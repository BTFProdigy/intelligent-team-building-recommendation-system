Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1121?1128
Manchester, August 2008
Diagnostic Evaluation of Machine Translation Systems Using Auto-
matically Constructed Linguistic Check-Points 
Ming Zhou1, Bo Wang2, Shujie Liu2, Mu Li1, Dongdong Zhang1, Tiejun Zhao2 
1Microsoft Research Asia 
Beijing, China 
{mingzhou,muli,dozhang} 
@microsoft.com 
 
2Harbin Institute of Technology 
Harbin, China 
{bowang,Shujieliu,tjzhao} 
@mtlab.hit.edu.cn 
 
 
?Abstract 
We present a diagnostic evaluation plat-
form which provides multi-factored eval-
uation based on automatically con-
structed check-points. A check-point is a 
linguistically motivated unit (e.g. an am-
biguous word, a noun phrase, a verb~obj 
collocation, a prepositional phrase etc.), 
which are pre-defined in a linguistic tax-
onomy. We present a method that auto-
matically extracts check-points from pa-
rallel sentences. By means of check-
points, our method can monitor a MT 
system in translating important linguistic 
phenomena to provide diagnostic evalua-
tion. The effectiveness of our approach 
for diagnostic evaluation is verified 
through experiments on various types of 
MT systems. 
1 Introduction 
Automatic MT evaluation is a crucial issue for 
MT system developers. The state-of-the-art me-
thods for automatic MT evaluation are using an 
n-gram based metric represented by BLEU (Pa-
pineni et al, 2002) and its variants. Ever since its 
invention, the BLEU score has been a widely 
accepted benchmark for MT system evaluation. 
Nevertheless, the research community has been 
aware of the deficiencies of the BLEU metric 
(Callison-Burch et al, 2006). For instance, 
BLEU fails to sufficiently capture the vitality of 
natural languages: all grams of a sentence are 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
treated equally ignoring their linguistic signific-
ance; only consecutive grams are considered ig-
noring the skipped grams of certain linguistic 
relations; candidate translation gets acknowl-
edged only if it uses exactly the same lexicon as 
the reference ignoring the variation in lexical 
choice. Furthermore, BLEU is useful for opti-
mizing and improving statistical MT systems but 
it has shown to be ineffective in comparing sys-
tems with different architectures (e.g., rule-based 
vs. phrase-based) (Callison-Burch et al,  2006).  
    Another common deficiency of the state-of-
the-art evaluation approaches is that they cannot 
clearly inform MT developers on the detailed 
strengths and flaws of an MT system, and there-
fore there is no way for us to understand the ca-
pability of certain modules of an MT system, and 
the capability of translating certain kinds of lan-
guage phenomena. For the purpose of system 
development, MT developers need a diagnostic 
evaluation approach to provide the feedback on 
the translation ability of an MT system with re-
gard to various important linguistic phenomena.     
    We propose a novel diagnostic evaluation ap-
proach. Instead of assigning a general score to an 
MT system we evaluate the capability of the sys-
tem in handling various important linguistic test 
cases called Check-Points. A check-point is a 
linguistically motivated unit, (e.g. an ambiguous 
word, a noun phrase, a verb~obj collocation, a 
prepositional phrase etc.) which are pre-defined 
in a linguistic taxonomy for diagnostic evalua-
tion. The reference of a check-point is its corres-
ponding part in the target sentence. The evalua-
tion is performed by matching the candidate 
translation corresponding to the references of the 
check-points. The extraction of the check-points 
is an automatic process using word aligner and 
parsers. We control the noise of the word aligner 
and parsers within tolerable scope by selecting 
1121
reliable subset of the check-points and weighting 
the references with confidence.  
    The check-points of various kinds extracted in 
this way have shown to be effective in perform-
ing diagnostic evaluation of MT systems. In ad-
dition, scores of check-points are also approved 
to be useful to improve the ranking of MT sys-
tems as additional features at sentence level and 
document level. 
The rest of the paper is structured in the fol-
lowing way:  Section 2 gives the overview of the 
process of the diagnostic evaluation. Section 3 
introduces the design of check-point taxonomy. 
Section 4 explains the details of construction of 
check-point database and the methods of reduc-
ing the noise of aligner and parsers. Section 5 
explains the matching approach. In Section 6, we 
introduce the experiments on different MT sys-
tems to demonstrate the capability of the diag-
nostic evaluation. In Section 7, we show that the 
check-points can be used to improve the current 
ranking methods of MT systems. Section 8 com-
pares our approach with related evaluation ap-
proaches. We conclude this work in Section 9. 
2 Overview of Diagnostic Evaluation 
In our implementation, we first build a check-
point database encoded in XML by associating a 
test sentence with qualified check-points it con-
tains. This process can be described as following 
steps: 
 
? Collect a large amount of parallel sen-
tences from the web or book collections. 
? Parse the sentences of source language 
and target language. 
? Perform the word alignments between 
each sentence pair. 
? For each category of check-points, extract 
the check-points from the parsed sentence 
pairs. 
? Determine the references of each check-
point in source language based on the 
word alignment.  
 
   With the extracted check-point database, the 
diagnostic evaluation of an MT system is per-
formed with the following steps: 
 
? The test sentences are selected from the 
database based on the selected categories 
of check-points to be evaluated. 
? For each check-point, we calculate the 
number of matched n-grams of the refer-
ences against the translated sentence of 
the MT system.  The credit of the MT sys-
tem in translating this check-point is ob-
tained after necessary normalization. 
? The credit of a category can be obtained 
by summing up the credits of all check-
points of this category. Then the credit of 
an MT system can be obtained by sum-
ming up the credits of all categories. 
? Finally, scores of system, category groups 
(e.g. Words), single category (e.g. Noun), 
and detail information of n-gram matching 
of each check-point are all provided to the 
developers to diagnose the MT system. 
3 Linguistic Check-Point Taxonomy 
The taxonomy of automatic diagnostic evaluation 
should be widely accepted so that the diagnostic 
results can be explained and shared with each 
other. We will also need to remove the sophisti-
cated categories that are out of the capability of 
current NLP tools to recognize.  
In light of this consideration, for Chinese-
English machine translation, we adopted the ma-
nual taxonomy introduced by (Lv, 2000; Liu, 
2002) and removed items that are beyond the 
capability of our parsers. The taxonomy includes 
typical check-pints at word, phrase and sentence 
levels. Some examples of the representative 
check-points at different levels are provided be-
low: 
 
?  Word level check-points: 
    a. Preposition word e.g., ?(in), ?(at) 
    b. Ambiguous word e.g., ?(play) 
    c. New word1 e.g., ??(Punk)  
?  Phrase level check-points: 
    a. Collocation. e.g., ??-??(fired ? food)  
    b. Repetitive word combination. e.g., ??
(have a look) 
    c. Subjective-predicate phrase e.g., ?*?, 
(he*said) 
     ? Sentence level check-points:  
a. ?BA? sentence 2 : ?? (BA)???? . 
(He took away the book.) 
b. ?BEI? sentence3????(BEI)???. 
(The vase was broken.)   
                                                 
1 New words are the terms extracted from web which can be 
a name entity or popular words emerging recently.   
2 In a ?BA? sentence, the object which normally follows the 
verb occurs preverbally, marked by word ?BA?. 
3 ?BEI? sentence is a kind of passive voice in Chinese 
marked by word ?BEI?. 
1122
    Our implementation of Chinese-English 
check-point taxonomy contains 22 categories and 
English-Chinese check-point taxonomy contains 
20 categories. Table 1 and 2 show the two 
taxonomies. In practice, any tag in parsers (e.g. 
NP) can be easily added as new category. 
 
Word level 
Ambiguous word New word Idiom 
Noun Verb Adjective 
Pronoun Adverb Preposition 
Quantifier Repetitive word Collocation 
Phrase level 
Subject-predicate 
phrase 
Predicate-object 
 phrase 
Preposition-
object phrase 
Measure phrase Location phrase  
Sentence level 
BA sentence BEI sentence SHI sentence 
YOU sentence Compound sentence 
Table 1:  Chinese check-point taxonomy 
 
Word level 
Noun Verb (with Tense) Modal verb 
Adjective Adverb Pronoun 
Preposition Ambiguous word Plurality 
Possessive Comparative & Superlative  degree 
Phrase level 
Noun phrase Verb phrase Adjective 
phrase 
Adverb phrase Preposition phrase  
Sentence level 
Attribute clause Adverbial clause Noun clause 
Hyperbaton  
Table 2: English check-point taxonomy 
4 Construction of Check-Point Data-
base 
Given a bilingual corpus with word alignment, 
the construction of check-point database consists 
of following two steps. First, the information of 
pos-tag, dependency structure and constituent 
structure can be identified with parsers. Then 
check-points of different categories are identified. 
Check-points of word-level categories such as 
Chinese idiom and English ambiguous words are 
extracted with human-made dictionaries, and the 
check-points of New-Word are extracted with a 
new word list mined from the web. A set of hu-
man-made rules are employed to extract certain 
categories involving sentence types such as com-
pound sentence.  
    Second, for a check-point, with the word 
alignment information, the corresponding target 
language portion is identified as the reference of 
this check-point. The following example illu-
strates the process of extracting check-points 
from a parallel sentence pair.  
?  A Chinese-English sentence pair: 
  ?????????. 
    They opposed the building of reserve funds. 
?  Word segmentation and pos-tagging: 
  ??/R ??/V ??/V ???/N ./P 
?  Parsing result (e.g.  a dependency result): 
    (SUB, 1/??, 0/??)  (OBJ, 1/??, 2/?
?) (OBJ, 2/??, 3/???) 
?  Word alignment: 
     (1; 1); (2; 2); (3; 4); (4; 6,7);   
?   The check-points in table 3 are extracted: 
 
Table 3: Example of check-point extraction 
 
    To extract the categories of check-points of 
different schema of syntactic analysis such as 
constitute structure and dependency structure, 
three parsers including a Chinese skeleton parser 
(a kind of dependency parser) (Zhou, 2000), 
Stanford statistical parser and Berkeley statistical 
parser (Klein 2003) are used to parse the Chinese 
and English sentences.  As explained in next sec-
tion, these multiple parsers are also used to select 
high confident check-points. To get word align-
ment, an existing tool GIZA++ (Och 2003) is 
used.  
4.1 Reducing the Noise of the Parser 
The reliability of the check-points mainly 
depends on the accuracy of the parsers. We can 
achieve high quality word level check-points 
with the state-of-the-art POS tagger (94% 
precision) and dictionaries of various purposes. 
For sentence level categories, the parser tags and 
manually compiled rules can also achieve 95% 
accuracy. For some kinds of categories at phrase 
level which parsers cannot produce high 
accuracy, we only select the check-points which 
can be identified by multiple parsers, that is, 
adopt the intersection of the parsers results. 
Table 4 shows the improvement brought by this 
approach. Column 1 and 2 shows the precision of 
6 major types of phrases in Stanford and 
Berkeley parser. Column 3 shows the precision 
of intersection and column 4 shows the reduction 
of the number of check-points when adopting the 
intersection results. The test corpus is a part of 
Category Check-point Reference 
New word ??? reserve funds 
Ambiguous word ?? building 
Predicate ? object 
phrase 
????? the building of  
reserve funds 
Subject-predicate 
phrase 
???? They opposed 
1123
Penn Chinese Treebank which is not contained in 
the training corpus of two statistical parsers. 
(Klein 2003).  
 
 Stf% Brk% Inter% Tpts redu% 
NP 87.37 86.03 95.83 17.06 
VP 87.34 82.87 95.23 19.68 
PP 90.60 88.56 96.00 11.50 
QP 98.12 92.90 99.21 6.31 
ADJP 91.95 90.87 96.41 10.20 
ADVP 95.21 94.25 92.64 3.92 
Table 4:  Precision of parsers and their intersec-
tion (Stf is Stanford, Brk is Berkelry) 
 
4.2 Alleviating the Impact of Alignment 
Noise 
Except for sentence level check-points whose 
references are the whole sentences and New 
Word, Idiom check-points whose references are 
extracted from dictionary, the quality of the ref-
erences are impacted by the alignment accuracy. 
To alleviate the noise of aligner we use the lexi-
cal dictionary to check the reliability of refer-
ences. Suppose c is a check-point, for each refer-
ence c.r of c we calculate the dictionary match-
ing degree DM(c.r) with the source side c.s of c: 
 
)1()).(
)).(,.(,1.0().( rcWordCnt
scDicrcCoCntMaxrcDM ?  
 
    Where Dic(x) is a word bag contains all words 
in the dictionary translations of each source word 
in x. CoCnt(x, y) is the count of the common 
words in x and y. WordCnt(x) is the count of 
words in x. Specially, if c.r is not obtained based 
on alignment DM(c.r) will be 1. Because the li-
mitation of dictionary, a zero DM score not al-
ways means the reference is completely wrong, 
so we force the DM score to be not less than a 
minimum value (e.g. 0.1). DM score will further 
be used in evaluation in section 5.  
    To better understand the reliability of the ref-
erences and explore whether increasing the num-
ber of check-points could also alleviate the im-
pact of noise, we built two check-point databases 
from a human-aligned corpus (with 60,000 sen-
tence pairs) and an automatically aligned corpus 
(using GIZA++) respectively and tested 10 dif-
ferent SMT systems4 with them. The Spearman 
correlation is calculated between two ranked lists 
of the 10 evaluation results against the two data-
                                                 
4 These systems are derived from an in-house phrase based 
SMT engine with different parameter sets. 
bases. A higher correlation score means that the 
impact of the mistakes in word alignment is 
weaker. The experiment is repeated on 6 subsets 
of the database with the size from 500 sentences 
to 16K sentences to check the impact of the cor-
pus size. 
    At system level, high correlations are found at 
different corpus sizes. At category level, correla-
tions are found to be low for some categories at 
small size and become higher at larger corpus 
size. The results indicate that the impact of the 
alignment quality can be ignored if the corpus 
size is at large scale. As the check-points can be 
extracted fully automatically, increasing the size 
of check-point database will not bring extra cost 
and efforts. Empirically, the proper scale is set to 
be 2000 or more sentences according to the Ta-
ble 6. 
 
Table 6: Impact of word alignment at different 
sizes of test corpus. 
5 Matching Check-Points for Evalua-
tion 
Evaluation can be carried out at multiple options: 
for certain linguistic category, a group of catego-
ries or entire taxonomy. For instance, in Chinese-
English translation task, if a MT developer 
would like to know the ability to translate idiom, 
then a number of parallel sentences containing 
idiom check-points are selected from the data-
base. Then the system translation sentences are 
matched to the references of the check-points of 
idioms.  
 500 1K 2K 4K 8K 16K 
Ambiguous 
word 
0.98 0.98 0.98 0.98 0.96 0.98 
Noun 0.93 0.99 0.99 0.89 0.8 0.86 
Verb 0.97 0.97 0.99 0.99 0.95 0.92 
Adjective 0.16 0.19 0.57 0.75 0.77 0.97 
Pronoun 0.96 1 0.93 0.99 0.97 0.99 
Adverb 0.38 0.77 0.8 0.96 0.72 0.84 
Preposition 0.56 0.86 0.9 0.9 0.97 0.96 
Quantifier 1 0.46 0.46 0.98 0.85 0.96 
Repetitive 
Word 
0.99 0.99 0.97 0.89 0.73 0.95 
Collocation 0.42 0.77 0.77 0.77 0.73 0.88 
Subject-
predicate 
phrase 
0.06 0.8 0.95 1 0.96 0.84 
Predicate-
object phrase 
0.84 0.96 0.78 0.7 0.78 0.88 
Preposition-
object phrase 
0.51 0.5 0.93 0.95 0.87 0.99 
Measure 
phrase 
0.91 0.67 0.95 0.95 0.87 0.97 
Location 
phrase 
0.62 0.54 0.55 0.55 0.85 0.89 
SYSTEM 0.95 0.95 0.98 0.99 0.97 0.98 
1124
To calculate the credit at different occasions of 
matching, similar to BLEU, we split each refer-
ence of a check-point into a set of n-grams and 
sum up the gains over all grams as the credit of 
this check-point. Especially, if the check-point is 
not consecutive we use a special token (e.g. ?*?) 
to represent a component which can be wildcard 
matched by any word sequence. We use the fol-
lowing examples to demonstrate the splitting and 
matching of grams.  
 
?  Consecutive check-point: 
    Check- point: ??? 
    Reference: playing a drum 
    Candidate translation:  He is playing a drum.  
    Matched n-grams: playing; a; drum; playing a; 
a drum; playing a drum  
 
?   Not consecutive check-point: 
    Check- point: ??*?   
    Reference: They*playing   
    Candidate translation: They are playing cop 
per drum. 
    Matched n-grams: They; playing; They * play-
ing 
    Additionally, to match word inflections, 3 dif-
ferent options of matching granularity are de-
fined as follows.  
?  Normal: matching with exact form. 
?  Lower-case: matching with lowercase. 
?  Stem: matching with the stem of the word. 
 
    For a check-point c and references set R of c, 
we select the r* in R which matches the transla-
tion best based on formula (2).  
 
 
 
    
 
When we calculate the recall of a set of check-
points C (C can be a single check-point, a cate-
gory or a category group), r* of each check-point 
c in C are merged into one reference set R* and 
the recall of C is obtained using formula (3) on 
R*. 
 
 
 
 
 
 
A penalty is also introduced to punish the re-
dundancy of candidate sentences, where length(T) 
is the average length of all translation sentences 
and length(R) is the average length of all refer-
ence sentences. 
 
 
 
 
 
Then, the final score of C will be: 
 
)5()(Re)( PenaltyCCScore ??
 
6 Experiments on MT System Diagnos-
es 
In this section, to demonstrate the ability of our 
approach in the diagnoses of MT systems, we 
apply diagnostic evaluation to 3 statistical MT 
(SMT) systems and a rule-based MT (RMT) sys-
tem respectively. We compare two SMT systems 
to understand the strength and shortcoming of 
each of them, and also compare a SMT system 
with the RMT system. The test corpus is NIST05 
test data with 54852 check-points. 
    First SMT system (system A) is an implemen-
tation of classical phrase based SMT. The second 
SMT system (system B) shares the same decoder 
with system A and introduces a preprocess to 
reorder the long phrases in source sentences ac-
cording to the syntax structure before decoding 
(Chiho Li et al, 2007). The third SMT system 
(system C) is a popular internet service and the 
RMT system (system D) is a popular commercial 
system.  
    In the first experiment, we diagnose the sys-
tem A and B and compare the results as shown in 
table 7. When evaluated using BLEU, system B 
achieved a 0.005 points increase on top of system 
A which is not a very significant difference. The 
diagnostic results in table 7 provide much richer 
information. The results indicate that two sys-
tems perform similar at the word level categories 
while at all phrase level categories, system B 
performs better. This result reflects the benefit 
from the reordering of complex phrases in sys-
tem B. Paired t-statistic score for each pair of 
category scores is also calculated by repeating 
the evaluation on a random copy of the test set 
with replacement (Koehn 2004). An absolute 
score beyond 2.17 of paired t-statistic means the 
difference of the samples is statistically signifi-
cant (above 95%). Table 8 and 9 show an in-
stance of the check-point and its evaluation in 
this experiment. 
)2()
)'(
)(
)((maxarg
'
*
?
?
??
??
? ?
?
??
rgramn
rgramn
Rr gramnCount
gramnMatch
rDMr
)4(
1
)()(
)(
)(
??
?
?
? ?
?
Otherwise
RlengthTlengthif
Tlength
Rlength
Penalty
)3(
))'()((
))()((
)Re(
*
*
' ''
'? ?
? ?
? ??
? ??
??
??
?
Rr rgran
Rr rgramn
gramnCountrDM
gramnMatchrDM
C
1125
 System A System B T score 
WORDs 
Idiom 0.1933 0.2370 13.38 
Adjective 0.5836 0.5577 -17.43 
Pronoun 0.7566 0.7344 -13.49 
Adverb 0.5365 0.5433 7.11 
Preposition 0.6529 0.6456 -6.21 
Repetitive word 0.3363 0.3958 9.86 
PHRASEs 
Subject-predicate 0.5117 0.5206 7.36 
Predicate-object 0.4041 0.4180 15.52 
Predicate-complement 0.4409 0.5125 9.51 
Measure phrase 0.5030 0.5092 3.56 
Location phrase 0.5245 0.5338 2.83 
GROUPs 
WORDs 0.4839 0.4855 2.03 
PHRASEs 0.4744 0.4964 13.97 
SYSTEM (Linguistic) 0.4263 0.4370 16.50 
SYSTEM (BLEU) 0.3564 0.3614 7.91 
Table 7: Diagnose of SMT systems 
 
Source Sentence ????????????????
??????? 
Category Preposition_Object_Phrase 
Check-Point ? ? ?? 
Reference 1 in this country  DM = 0.5 
Reference 2 in his country  DM = 0.5 
System A Translation but the prime minister of thailand Dex-
in vowed to continue in domestic the 
search. 
System B Translation but the prime minister of thailand Dex-
in vowed to continue the search in his 
country. 
Table 8: An instance of the check-point. 
 
 System A System B 
Ref 1: Match/Total 1/6 2/6 
Ref 2: Match/Total 1/6 6/6 
Score 0.17 1 
Table 9: N-gram matching rate and scores. 
 
Table 10: Diagnose of SMT and RMT. 
 
In the second experiment, we diagnose system 
C and D and compare the results. The BLEU 
score of system C is 0.3005 and system D is 
0.2606. Table 10 shows the diagnostic results on 
categories with significant differences. Scores 
calculated with 3 matching options described in 
section 5 are given (?Lower? means Lowercase. 
The scores are listed in the form ?SMT 
score/RMT score?). The diagnostic results indi-
cate that system C performs better on most cate-
gories than system D, but system D performs 
better on categories like idiom, pronoun and pre-
position. This result reveals a key difference be-
tween two types of MT systems: the SMT works 
well on the open categories that can be handled 
by context, while the RMT works well on closed 
categories which are easily translated by linguis-
tic rules. 
    As the results of two experiments demonstrate, 
the diagnostic evaluation provides rich informa-
tion of the capability of translating various im-
portant linguistic categories beyond a single sys-
tem score. It successfully distinguishes the spe-
cific difference between the MT systems whose 
system level performance is similar. It can also 
diagnose the MT system with different architec-
tures. Diagnostic evaluation tells the developers 
about the direction to improve the system. Along 
with the scores of categories, the diagnostic 
evaluation provides the system translation and 
references at every check-point so that the devel-
opers can trace and understand about how the 
MT system works on every single instance. 
7 Experiments on Ranking MT Systems 
Offering a general evaluation at system level is 
the major goal of state-of-the-art evaluation me-
thods including widely accepted n-gram metrics. 
The absence of linguistic knowledge in BLEU 
motivated many work to integrate linguistic fea-
tures into evaluation metric. In (Yang 2007), the 
evaluation of SMT systems is alternately formu-
lated as a ranking problem. Different linguistic 
features are combined with BLEU such as 
matching rate of dependency relations of transla-
tion candidates against the reference sentences. 
The experiments demonstrate that the dependen-
cy matching rate feature can increase the ranking 
accuracy in some cases. Compared to dependen-
cy structure, the linguistic categories in our ap-
proach showcase more extensive features. It 
would be interesting to see whether the linguistic 
categories can be used to further improve the 
ranking of SMT systems.  
    In experiments, we use the scores of linguistic 
categories, dependency matching rate, scores of 
BLEU and other popular metrics as ranking fea-
tures of MT systems and trained by Ranking 
SVM of SVMlight (Joachims, 1998). We per-
formed the ranking experiments on ACL 2005 
workshop data, ranking 7 MT translations with 
three-fold cross-validation both on sentence level 
and document level. The Spearman score is used 
Type Normal Lower Stem 
Ambiguous word 0.49/0.42 0.50/0.42 0.53/0.46 
New word 0.13/0.13 0.37/0.32 0.42/0.35 
Idiom 0.43/0.66 0.46/0.67 0.51/0.71 
Pronoun 0.60/0.68 0.69/0.75 0.66/0.75 
Preposition 0.38/0.42 0.42/0.45 0.43/0.46 
Collocation 0.66/0.54 0.66/0.55 0.70/0.56 
Subject-predicate 
phrase 
0.46/0.30 0.51/0.36 0.58/0.42 
Predicate-object 
phrase 
0.37/0.25 0.37/0.26 0.47/0.29 
Compound sentence 0.22/0.16 0.23/0.16 0.23/0.17 
1126
to calculate the correlation with human assess-
ments. Table 11 and 12 show the results of the 
different feature sets on sentence level and doc-
ument level respectively. 
As shown in experiment results linguistic cat-
egories (LC), when used alone, are better related 
with human assessments than BLEU and GTM. 
When combined with the baseline metrics 
(BLEU & NIST), LC scores further improve the 
correlation score, better than dependence match-
ing rate (DP). LC scores are obtained by match-
ing the exact form of the words as ME-
TEOR(exact) does. NIST+LC combination score 
is better than METEOR(exact) at sentence and 
document level, and also better than ME-
TEOR(exact&syn) (syn means wn_synonymy 
module in METEOR) at document level. This 
results indicate the ability of linguistic features in 
improving the performance of ranking task. 
 
 Mean Correlation 
BLEU 4 0.245 
NIST 5 0.307 
GTM (e=2) 0.251 
METEOR(exact) 0.306 
METEOR(exact&syn) 0.327 
DP 0.246 
LC 0.263 
BLEU+DP 0.270 
BLEU+ LC 0.288 
BLEU+ DP +LC 0.307 
NIST+ LC 0.322 
NIST+ DP +LC 0.333 
 Table11: Sentence level ranking (DP means 
dependency and LC means linguistic categories)  
 
 Mean Correlation 
BLEU 4 0.305 
NIST 5 0.373 
GTM (e=2) 0.327 
METEOR(exact) 0.363 
METEOR(exact&syn) 0.394 
DP 0.323 
LC 0.369 
BLEU+DP 0.325 
BLEU+ LC 0.387 
BLEU+ DP +LC 0.332 
NIST+ LC 0.409 
NIST+ DP +LC 0.359 
Table 12: Document level ranking 
8 Comparison with Related Work 
This work is inspired by (Yu, 1993) with many 
extensions. (Yu, 1993) proposed MTE evaluation 
system based on check-points for English-
Chinese machine translation systems with human 
craft linguistic taxonomy including 3,200 pairs of 
sentences containing 6 classes of check-points. 
Their check-points were manually constructed by 
human experts, therefore it will be costly to build 
new test corpus while the check-points in our 
approach are constructed automatically. Another 
limitation of their work is that only binary score 
is used for credits while we use n-gram matching 
rate which provides a broader coverage of differ-
ent levels of matching.   
    There are many recent work motivated by n-
gram based approach. (Callison-Burch et al, 
2006) criticized the inadequate accuracy of eval-
uation at the sentence level. (Lin and Och, 2004) 
used longest common subsequence and skip-
bigram statistics. (Banerjee and Lavie, 2005) cal-
culated the scores by matching the unigrams on 
the surface forms, stemmed forms and senses. 
(Liu et al, 2005) used syntactic features and un-
labeled head-modifier dependencies to evaluate 
MT quality, outperforming BLEU on sentence 
level correlations with human judgment. (Gime-
nez and Marquez, 2007) showed that linguistic 
features at more abstract levels such as depen-
dency relation may provide more reliable system 
rankings. (Yang et al, 2007) formulates MT 
evaluation as a ranking problems leading to 
greater correlation with human assessment at the 
sentence level.  
There are many differences between these n-
gram based methods and our approach. In n-
gram approach, a sentence is viewed as a collec-
tion of n-grams with different length without dif-
ferentiating the specific linguistic phenomena. In 
our approach, a sentence is viewed as a collec-
tion of check-points with different types and 
depth, conforming to a clear linguistic taxonomy. 
Furthermore, in n-gram approach, only one gen-
eral score at the system level is provided which 
make it not suitable for system diagnoses, while 
in our approach we can give scores of linguistic 
categories and provide much richer information 
to help developers to find the concrete strength 
and flaws of the system, in addition to the gener-
al score. The n-gram based metric is not very 
effective when comparing the systems with dif-
ferent architectures or systems with similar gen-
eral score, while our approach is more effective 
in both cases by digging into the multiple lin-
guistic levels and disclosing the latent differenc-
es of the systems. 
9 Conclusion and Future Work 
This paper presents an automatically diagnostic 
evaluation methods on MT based on linguistic 
check-points automatically constructed. In con-
trast with the metrics which only give a general 
score, our evaluation system can give developers 
1127
feedback about the faults and strength of an MT 
system regarding specific linguistic category or 
category group. Different with the existing work 
based on check-points, our work presents an ap-
proach to automatically generate the check-point 
database. We show that although there is some 
noise brought from word alignment and parsing, 
we can effectively alleviate the problem by refin-
ing the parser results, weighting the reference 
with confidence score and providing large quan-
tity of check-points.  
    The experiments demonstrate that this method 
can uncover the specific difference between MT 
systems with similar architectures and different 
architectures. It is also demonstrated that the lin-
guistic check-points can be used as new features 
to improve the ranking task of MT systems.   
    Although we present the diagnostic evaluation 
method with Chinese-English language pair, our 
approach can be applied to other language pair if 
syntax parser and word aligner are available. 
    The taxonomy used in current proposal is 
based on the human-made linguistic system. An 
interesting problem to be explored in the future is 
whether the taxonomy could be constructed au-
tomatically from the parsing results.  
References 
Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization 2005. 
Chris Callison-Burch, Miles Osborne, Philipp Koehn. 
2006. Re-evaluating the Role of Bleu in Machine 
Translation Research. In Proceedings of the Euro-
pean Chapter of the ACL 2006. 
Martin Chodorow, Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors, 
In 1st Meeting of the North America Chapter of the 
ACL, pp.140?147, 2000. 
Thorsten Joachims. 1998. Making Large-scale Sup-
port Vector Machine Learning Practical, In B. 
Scholkopf, C. Burges, A. Smola. Advances in Ker-
nel Methods: Support VectorMachines, MIT Press, 
Cambridge, MA, December. 
Jesus Gimenez and Llis Marquez. 2007. Linguistic 
features for automatic evaluation of heterogeneous 
MT systems, Workshop of statistical machine trans-
lation in conjunction with 45th ACL, 2007. 
Dan Klein, Christopher Manning. 2003. Accurate 
Unlexicalized Parsing, Proceedings of the 41th 
Meeting of the ACL, pp. 423-430. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of the 
EMNLP, Barcelona, Spain. 
Chiho Li, Minghui Li, Dongdong Zhang, Mu Li, 
Ming Zhou, Yi Guan. 2007. A Probabilistic Ap-
proach to Syntax-based Reor-dering for SMT. In 
Proceedings of the 45th  ACL, 2007. 
Chin-Yew Lin and Franz Josef Och. 2004. Automatic 
evaluation of machine translation quality using 
longest common subsequence and skip-bigram sta-
tistics. In Proceedings of the 42th ACL 2004.  
Ding Liu, Daniel Gildea. 2005. Syntactic Features for 
Evaluation of Machine Translation, ACL Work-
shop on Intrinsic and Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summariza-
tion. 
Shuxin Liu. 2002. Linguistics of Contemporary Chi-
nese Language (in Chinese), Advanced Education 
Publisher. 
Jiping Lv. 2000. Foundation of Mandarin Grammar 
(in Chinese), Shangwu Publisher. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els, Computational Linguistics, volume 29, number 
1, pp. 19-51 March 2003. 
Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation, In Proceedings of the 
ACL 2002. 
Shiwen Yu. 1993. Automatic evaluation of output 
quality for machine translation systems, In Pro-
ceedings of the evaluators? forum, April 21-24, 
1991, Les Rasses, Vaud, 1993.  
Yang Ye, Ming Zhou, Chinyew Lin. 2007. Sentence 
level machine translation evaluation as a ranking 
problem: one step aside from BLEU, In Workshop 
of statistical machine translation, in conjunction 
with 45th ACL, 2007. 
Ming Zhou. 2000, A Block-Based Robust Dependency 
Parser for Unrestricted Chinese Text. Proceedings 
of Second Chinese Language Processing Workshop, 
2000, held in conjunction with ACL, 2000.  
 
1128
Coling 2010: Poster Volume, pages 730?738,
Beijing, August 2010
Improved Discriminative ITG Alignment using  
Hierarchical Phrase Pairs and Semi-supervised Training 
?Shujie Liu*, ?Chi-Ho Li and ?Ming Zhou 
? School of Computer Science and Technology 
Harbin Institute of Technology 
shujieliu@mtlab.hit.edu.cn 
?Microsoft Research Asia 
{chl, mingzhou}@microsoft.com  
 
Abstract 
While ITG has many desirable properties 
for word alignment, it still suffers from 
the limitation of one-to-one matching. 
While existing approaches relax this li-
mitation using phrase pairs, we propose a 
ITG formalism, which even handles units 
of non-contiguous words, using both 
simple and hierarchical phrase pairs. We 
also propose a parameter estimation me-
thod, which combines the merits of both 
supervised and unsupervised learning, 
for the ITG formalism. The ITG align-
ment system achieves significant im-
provement in both word alignment quali-
ty and translation performance. 
1 Introduction 
Inversion transduction grammar (ITG) (Wu, 
1997) is an adaptation of CFG to bilingual 
parsing. It does synchronous parsing of two 
languages with phrasal and word-level alignment 
as by-product. One of the merits of ITG is that it 
is less biased towards short-distance reordering 
compared with other word alignment models 
such as HMM. For this reason ITG has gained 
more and more attention recently in the word 
alignment community (Zhang et al, 2005; 
Cherry et al, 2006; Haghighi et al, 2009)1. 
The basic ITG formalism suffers from the ma-
jor drawback of one-to-one matching. This limi-
tation renders ITG unable to produce certain 
alignment patterns (such as many-to-many 
                                                 
* This work has been done while the first author was visit-
ing Microsoft Research Asia. 
alignment for idiomatic expression). For this 
reason those recent approaches to ITG alignment 
introduce the notion of phrase (or block), de-
fined as sequence of contiguous words, into the 
ITG formalism (Cherry and Lin, 2007; Haghighi 
et al, 2009; Zhang et al, 2008). However, there 
are still alignment patterns which cannot be cap-
tured by phrases. A simple example is connec-
tive in Chinese/English. In English, two clauses 
are connected by merely one connective (like 
"although", "because") but in Chinese we need 
two connectives (e.g. There is a sentence pattern 
"??    ??   ?     although   ", where    
and     are variables for clauses). The English 
connective should then be aligned to two non-
contiguous Chinese connectives, and such 
alignment pattern is not available in either word-
level or phrase-level ITG. As hierarchical 
phrase-based SMT (Chiang, 2007) is proved to 
be superior to simple phrase-based SMT, it is 
natural to ask, why don?t we further incorporate 
hierarchical phrase pairs (henceforth h-phrase 
pairs) into ITG? In this paper we propose a ITG 
formalism and parsing algorithm using h-phrase 
pairs. 
The ITG model involves much more parame-
ters. On the one hand, each phrase/h-phrase pair 
has its own probability or score. It is not feasible 
to learn these parameters through discrimina-
tive/supervised learning since the repertoire of 
phrase pairs is much larger than the size of hu-
man-annotated alignment set. On the other hand, 
there are also a few useful features which cannot 
be estimated merely by unsupervised learning 
like EM. Inspired by Fraser et al (2006), we 
propose a semi-supervised learning algorithm 
which combines the merits of both discrimina-
730
tive training (error minimization) and approx-
imate EM (estimation of numerous parameters). 
The ITG model augmented with the learning 
algorithm is shown by experiment results to im-
prove significantly both alignment quality and 
translation performance.  
In the following, we will explain, step-by-step, 
how to incorporate hierarchical phrase pairs into 
the ITG formalism (Section 2) and in ITG pars-
ing (Section 3). The semi-supervised training 
method is elaborated in Section 4. The merits of 
the complete system are illustrated with the ex-
periments described in Section 5. 
2 ITG Formalisms 
2.1 W-ITG : ITG with only word pairs 
The simplest formulation of ITG contains three 
types of rules: terminal unary rules  ?    , 
where e and f represent words (possibly a null 
word, ?) in the English and foreign language 
respectively, and the binary rules  ?       and 
 ?      , which refer to that the component 
English and foreign phrases are combined in the 
same and inverted order respectively. From the 
viewpoint of word alignment, the terminal unary 
rules provide the links of word pairs, whereas 
the binary rules represent the reordering factor. 
Note also that the alignment between two phrase 
pairs is always composed of the alignment 
between word pairs (c.f. Figure 1(a) and (b)). 
The Figure 1 also shows ITG can handle the 
cases where two languages share the same 
(Figure 1(a)) and different (Figure 1(b)) word 
order 
?? XX,?X  (b)
]f,e[?X  (c) f1Xf3][e1Xe3,?X  (d)
X][X,?X  ( )
e2
e1
f1 f2 f1 f2
e2
e1
e2
e1
f1 f2
e2
e1
f1 f2 f3
e3
 
Figure 1. Four ways in which ITG can analyze a 
multi-word span pair. 
Such a formulation has two drawbacks. First 
of all, the simple ITG leads to redundancy if 
word alignment is the sole purpose of applying 
ITG. For instance, there are two parses for three 
consecutive word pairs, viz.               
      and                   . The problem of re-
dundancy is fixed by adopting ITG normal form. 
The ITG normal form grammar as used in this 
paper is described in Appendix A. 
The second drawback is that ITG fails to 
produce certain alignment patterns. Its constraint 
that a word is not allowed to align to more than 
one word is indeed a strong limitation as no 
idiom or multi-word expression is allowed to 
align to a single word on the other side. 
Moreover, its reordering constraint makes it 
unable to produce the ?inside-out? alignment 
pattern (c.f. Figure 2). 
f1      f2      f3      f4
e1     e2      e3      e4 
Figure 2. An example of inside-out alignment. 
2.2 P-ITG : ITG with Phrase Pairs 
A single word in one language is not always on a 
par with a single word in another language. For 
example, the Chinese word "??" is equivalent 
to two words in English ("white house"). This 
problem is even worsened by segmentation er-
rors (i.e. splitting a single word into more than 
one word). The one-to-one constraint in W-ITG 
is a serious limitation as in reality there are al-
ways segmentation or tokenization errors as well 
as idiomatic expressions. Therefore, researches 
like Cherry and Lin (2007), Haghighi et al 
(2009) and Zhang et al (2009) tackle this prob-
lem by enriching ITG, in addition to word pairs, 
with pairs of phrases (or blocks). That is, a se-
quence of source language word can be aligned, 
as a whole, to one (or a sequence of more than 
one) target language word. 
These methods can be subsumed under the 
term phrase-based ITG (P-ITG), which enhances 
W-ITG by altering the definition of a terminal 
production to include phrases:   ?      (c.f. 
Figure 1(c)).    stands for English phrase and 
   stands for foreign phrase. As an example, if 
there is a simple phrase pair <white house, ?
731
?>, then it is transformed into the ITG rule 
 ?  white house   ?? . 
An important question is how these phrase 
pairs can be formulated. Marcu and Wong (2002) 
propose a joint probability model which searches 
the phrase alignment space, simultaneously 
learning translations lexicons for words and 
phrases without consideration of potentially sub-
optimal word alignments and heuristic for phrase 
extraction. This method suffers from computa-
tional complexity because it considers all possi-
ble phrases and all their possible alignments. 
Birch et al (2006) propose a better and more 
efficient method of constraining the search space 
which does not contradict a given high confi-
dence word alignment for each sentence. Our P-
ITG collects all phrase pairs which are consistent 
with a word alignment matrix produced by a 
simpler word alignment model. 
2.3 HP-ITG : P-ITG with H-Phrase pairs 
P-ITG is the first enhancement of ITG to capture 
the linguistic phenomenon that more than one 
word of a language may function as a single unit, 
so that these words should be aligned to a single 
unit of another language. But P-ITG can only 
treat contiguous words as a single unit, and 
therefore cannot handle the single units of non-
contiguous words. Apart from sentence 
connectives as mentioned in Section 1, there is 
also the example that the single word ?since? in 
English corresponds to two non-adjacent words "
?" and "??" as shown the following sentence 
pair: 
?  ???  ?? ? ? ?? ? ?? . 
I have been ill since last weekend . 
No matter whether it is P-ITG or phrase-based 
SMT, the very notion of phrase pair is not help-
ful because this example is simply handled by 
enumerating all possible contiguous sequences 
involving the words "?" and "??", and thus 
subject to serious data sparseness. The lesson 
learned from hierarchical phrase-based SMT is 
that the modeling of non-contiguous word se-
quence can be very simple if we allow rules in-
volving h-phrase pairs, like: 
  ?  since    ?   ??  
where   is a placeholder for substituting a 
phrase pair like "???/last weekend". 
H-phrase pairs can also perform reordering, as 
illustrated by the well-known example from 
Chiang (2007),  ?   have    with        ?    
?     , for the following bilingual sentence 
fragment: 
?  ??  ?  ?? 
have diplomatic relations with North Korea 
The potential of intra-phrase reordering may also 
help us to capture those alignment patterns like 
the ?inside-out? pattern. 
All these merits of h-phrase pairs motivate a 
ITG formalism, viz. hierarchical phrase-based 
ITG (HP-ITG), which employs not only simple 
phrase pairs but also hierarchical ones. The ITG 
grammar is enriched with rules of the format: 
 ?      where   and    refer to either a phrase 
or h-phrase (c.f. Figure 1(d)) pair in English and 
foreign language respectively 2 . Note that, al-
though the format of HP-ITG is similar to P-ITG, 
it is much more difficult to handle rules with h-
phrase pairs in ITG parsing, which will be elabo-
rated in the next section. 
It is again an important question how to for-
mulate the h-phrase pairs. Similar to P-ITG, the 
h-phrase pairs are obtained by extracting the h-
phrase pairs which are consistent with a word 
alignment matrix produced by some simpler 
word alignment model. 
3 ITG Parsing 
Based on the rules, W-ITG word alignment is 
done in a similar way to chart parsing (Wu, 
1997). The base step applies all relevant terminal 
unary rules to establish the links of word pairs. 
The word pairs are then combined into span 
pairs in all possible ways. Larger and larger span 
pairs are recursively built until the sentence pair 
is built. 
Figure 3(a) shows one possible derivation for 
a toy example sentence pair with three words in 
each sentence. Each node (rectangle) represents 
a pair, marked with certain phrase category, of 
                                                 
2 Haghighi et al (2009) impose some rules which look like 
h-phrase pairs, but their rules are essentially h-phrase pairs 
with at most one ? ? only, added with the constraint that 
each ? ? covers only one word. 
732
foreign span (F-span) and English span (E-span) 
(the upper half of the rectangle) and the asso-
ciated alignment hypothesis (the lower half). 
Each graph like Figure 3(a) shows only one de-
rivation and also only one alignment hypothesis. 
The various derivations in ITG parsing can be 
compactly represented in hypergraph (Klein et 
al., 2001) like Figure 3(b). Each hypernode (rec-
tangle) comprises both a span pair (upper half) 
and the list of possible alignment hypotheses 
(lower half) for that span pair. The hyperedges 
show how larger span pairs are derived from 
smaller span pairs. Note that hypernode may 
have more than one alignment hypothesis, since 
a hypernode may be derived through more than 
one hyperedge (e.g. the topmost hypernode in 
Figure 3(b)). Due to the use of normal form, the 
hypotheses of a span pair are different from each 
other. 
In the case of P-ITG parsing, each span pair 
does not only examine all possible combinations 
of sub-span pairs using binary rules, but also 
checks if the yield of that span pair is exactly the 
same as that phrase pair. If so, then this span pair 
is treated as a valid leaf node in the parse tree. 
Moreover, in order to enable the parse tree pro-
duce a complete word aligned matrix as by-
product, the alignment links within the phrase 
pair (which are recorded when the phrase pair is 
extracted from a word aligned matrix produced 
by a simpler model) are taken as an alternative 
alignment hypothesis of that span pair. 
In the case of HP-ITG parsing, an ITG rule 
like  ?  have    with      ?   ?     (ori-
ginated from the hierarchical rule like  ?  <?
    ?   , have    with   >), is processed in the 
following manner: 1) Each span pair checks if it 
contains the lexical anchors: "have", "with","?" 
and "?"; 2) each span pair checks if the remain-
ing words in its yield can form two sub-span 
pairs which fit the reordering constraint among 
   and    (Note that span pairs of any category 
in the ITG normal form grammar can substitute 
for    or   ). 3) If both conditions hold, then the 
span pair is assigned an alignment hypothesis 
which combines the alignment links among the 
lexical anchors and those links among the sub-
span pairs. 
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e1,e2]/[f1,f2]
{e1/f2,e1/f1,
e2/f1,e2/f2}
A:[e1,e3]/[f1,f3]
{e1/f2,e1/f1,e2/f1,e2/f2,e3/f3} ,
 {e1/f1,e1/f3,e3/f1,e3/f3,e2,f2}
{e2/f2}
e1Xe3/f1Xf3:
[e1Xe3]/[f1Xf3]
{e1/f3,e1/f1,
e3/f3,e3/f1}
C:[e2,e2]/[f2,f2]
(c) 
e1               e2              e3
f1                f2               f3
(a) (b) 
e1               e2              e3
f1                f2               f3
A?[C,C] A?[e1Xe3/f1Xf3,C]
 
Figure 4. Phrase/h-phrase in hypergraph. 
 Figure 4(c) shows an example how to use 
phrase pair and h-phrase pairs in hypergraph.  
Figure 4(a) and  Figure 4(b) refer to alignment 
matrixes which cannot be generated by W-ITG, 
because of the one-to-one assumption.  Figure 
4(c) shows how the span pair [e1,e3]/[f1,f3] can 
be generated in two ways: one is combining a 
phrase pair and a word pair directly, and the oth-
er way is replacing the X in the h-phrase pair 
with a word pair. Here we only show how h-
phrase pairs with one variable be used during the 
B:[e1,e2]/[f1,f2]
{e1/f2,e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3}
(a) 
C:[e2,e2]/[f2,f2]
{e2/f2}
C:[e1,e1]/[f1,f1]
{e1/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
B:[e1,e2]/[f1,f2]
{e1/f2}
A:[e1,e2]/[f1,f2]
{e2/f2}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3} , 
{e1/f1,e2/f2,e3,f3}
(b)
B?<C,C> A?[C,C]
A?[A,C]A?[B,C]
 
Figure 3.  Example ITG parses in graph (a) and hypergraph (b). 
733
parsing, and h-phrase pairs with more than one 
variable can be used in a similar way. 
The original (unsupervised) ITG algorithm 
has complexity of O(n6). When extended to su-
pervised/discriminative framework, ITG runs 
even more slowly. Therefore all attempts to ITG 
alignment come with some pruning method. 
Zhang and Gildea (2005) show that Model 1 
(Brown et al, 1993) probabilities of the word 
pairs inside and outside a span pair are useful.  
Tic-tac-toe pruning algorithm (Zhang and Gildea, 
2005) uses dynamic programming to compute 
inside and outside scores for a span pair in O(n4). 
Tic-tac-toe pruning method is adopted in this 
paper. 
4 Semi-supervised Training 
The original formulation of ITG (W-ITG) is a 
generative model in which the ITG tree of a sen-
tence pair is produced by a set of rules. The pa-
rameters of these rules are trained by EM. Cer-
tainly it is difficult to add more non-independent 
features in such a generative model, and there-
fore Cherry et al (2006) and Haghighi et al 
(2009) used a discriminative model to incorpo-
rate features to achieve state-of-art alignment 
performance. 
4.1 HP-DITG : Discriminative HP-ITG 
We also use a discriminative model to assign 
score to an alignment candidate for a sentence 
pair (     ) as probability from a log-linear model 
(Liu et al, 2005; Moore, 2006): 
          
                    
                           
 (1) 
where each           is some feature about the 
alignment matrix, and each ? is the weight of the 
corresponding feature. The discriminative 
version of W-ITG, P-ITG, and HP-ITG are then 
called W-DITG, P-DITG, and HP-DITG 
respectively. 
There are two kinds of parameters in (1) to be 
learned. The first is the values of the features ?. 
Most features are indeed about the probabilities 
of the phrase/h-phrase pairs and there are too 
many of them to be trained from a labeled data 
set of limited size. Thus the feature values are 
trained by approximate EM. The other kind of 
parameters is feature weights ?, which are 
trained by an error minimization method. The 
discriminative training of ? and the approximate 
EM training of ? are integrated into a semi-
supervised training framework similar to EMD3 
(Fraser and Marcu, 2006). 
4.2 Discriminative Training of ? 
MERT (Och, 2003) is used to train feature 
weights ?. MERT estimates model parameters 
with the objective of minimizing certain measure 
of translation errors (or maximizing certain 
performance measure of translation quality) for a 
development corpus. Given an SMT system 
which produces, with model parameters   
 , the 
K-best candidate translations        
   for a 
source sentence   , and an error measure 
           of a particular candidate      with 
respect to the reference translation   , the 
optimal parameter values will be: 
   
        
  
 
             
   
 
   
  
       
  
 
                     
        
 
   
 
   
  
MERT for DITG applies the same equation 
for parameter tuning, with different interpreta-
tion of the components in the equation. Instead 
of a development corpus with reference transla-
tions, we have a collection of training samples, 
each of which is a sentence pair with annotated 
alignment result. The ITG parser outputs for 
each sentence pair a K-best list of alignment re-
sult        
   based on the current parameter 
values   
 . The MERT module for DITG takes 
alignment F-score of a sentence pair as the per-
formance measure. Given an input sentence pair 
and the reference annotated alignment, MERT 
aims to maximize the F-score of DITG-produced 
alignment.  
4.3 Approximate EM Training of ?  
Three kinds of features (introduced in section 
4.5 and 4.6) are calculated from training corpus 
given some initial alignment result: conditional 
probability of word pairs and two types of 
conditional probabilities for phrase/h-phrase. 
                                                 
3 For simplicity, we will also call our semi-supervised 
framework as EMD. 
734
The initial alignment result is far from perfect 
and so the feature values thus obtained are not 
optimized. There are too many features to be 
trained in supervised way. So, unsupervised 
training like EM is the best solution. 
When EM is applied to our model, the E-step 
corresponds to calculating the probability for all 
the ITG trees, and the M-step corresponds to re-
estimate the feature values. As it is intractable to 
handle all possible ITG trees, instead we use the 
Viterbi parse to update the feature values. In 
other words, the training is a kind of approx-
imate EM rather than EM. 
Word pairs are collected over Viterbi align-
ment and their conditional probabilities are esti-
mated by MLE. As to phrase/h-phrase, if they 
are handled in a similar way, then there will be 
data sparseness (as there are much fewer 
phrase/h-phrase pairs in Viterbi parse tree than 
needed for reliable parameter estimation). Thus, 
we collect all phrase/h-phrase pairs which are 
consistent with the alignment links. The condi-
tional probabilities are then estimated by MLE. 
4.4 Semi-supervised training 
Algorithm EMD (semi-supervised training) 
input development data dev, test data test, training 
data with initial alignment (train, align_train) 
output feature weights   and features . 
1: estimate initial features    with (train, align_train) 
2: get an initial weights    by MERT with the initial 
features   on dev. 
3: get the F-Measure    for          on test. 
4: for( =1;;  ++) 
5:  get the Viterbi alignment align_train for train 
using      and     
6:  estimate    with (train, align_train) 
7:  get new feature weights    by MERT with    
on dev. 
8:  get the F-Measure    for          on test. 
9:  if             then 
10:   break 
11: end for 
12: return      and     
Figure 5. Semi-supervised training for HP-DITG. 
The discriminative training (error minimiza-
tion) of feature weights   and the approximate 
EM learning of feature values  are integrated in 
a single semi-supervised framework. Given an 
initial estimation of  (estimated from an initial 
alignment matrix by some simpler word align-
ment model) and an initial estimation of  , the 
discriminative training process and the approx-
imate EM learning process are alternatively ite-
rated until there is no more improvement. The 
sketch of the semi-supervised training is shown 
in Figure 5. 
4.5 Features for word pairs 
The following features about alignment link are 
used in W-DITG: 
1) Word pair translation probabilities 
trained from HMM model (Vogel et al, 
1996) and IBM model 4 (Brown et al, 
1993). 
2) Conditional link probability (Moore, 
2006). 
3) Association score rank features (Moore et 
al., 2006). 
4) Distortion features: counts of inversion 
and concatenation. 
4.6 Features for phrase/h-phrase pairs 
For our HP-DITG model, the rule probabilities 
in both English-to-foreign and foreign-to-
English directions are estimated and taken as 
features, in addition to those features in W-
DITG, in the discriminative model of alignment 
hypothesis selection:  
1)           : The conditional probability of 
English phrase/h-phrase given foreign 
phrase/h-phrase. 
2)           : The conditional probability of 
foreign phrase/h-phrase given English 
phrase/h-phrase. 
The features are calculated as described in 
section 4.3. 
5 Evaluation 
Our experiments evaluate the performance of 
HP-DITG in both word alignment and transla-
tion in a Chinese-English setting, taking GI-
ZA++, BerkeleyAligner (henceforth BERK) 
(Haghighi, et al, 2009), W-ITG as baselines. 
Word alignment quality is evaluated by recall, 
precision, and F-measure, while translation per-
formance is evaluated by case-insensitive 
BLEU4. 
5.1 Experiment Data 
The small human annotated alignment set for 
discriminative training of feature weights is the 
same as that in Haghighi et al (2009). The 491 
735
sentence pairs in this dataset are adapted to our 
own Chinese word segmentation standard. 250 
sentence pairs are used as training data and the 
other 241 are test data. The large, un-annotated 
bilingual corpus for approximate EM learning of 
feature values is FBIS, which is also the training 
set for our SMT systems. 
In SMT experiments, our 5-gram language 
model is trained from the Xinhua section of the 
Gigaword corpus. The NIST?03 test set is used 
as our development corpus and the NIST?05 and 
NIST?08 test sets are our test sets.  We use two 
kinds of state-of-the-art SMT systems. One is a 
phrase-based decoder (PBSMT) with a MaxEnt-
based distortion model (Xiong, et al, 2006), and 
the other is an implementation of hierarchical 
phrase-based model (HPBSMT) (Chiang, 2007). 
The phrase/rule table for these two systems is 
not generated from the terminal node of HP-
DITG tree directly, but extracted from word 
alignment matrix (HP-DITG generated) using 
the same criterion as most phrase-based systems 
(Chiang, 2007). 
5.2 HP-DITG without EMD 
Our first experiment isolates the contribution of 
the various DITG alignment models from that of 
semi-supervised training. The feature values of 
the DITG models are estimated simply from 
IBM Model 4 using GIZA++. Apart from DITG, 
P-ITG, and HP-ITG as introduced in Section 2, 
we also include a variation, known as H-DITG, 
which covers h-phrase pairs but no simple 
phrase pairs at all. The experiment results are 
shown in Table 1. 
 Precision Recall F-Measure 
GIZA++ 0.826 0.807 0.816 
BERK 0.917 0.814 0.862 
W-DITG 0.912 0.745 0.820 
P-DITG 0.913 0.788 0.846 
H-DITG 0.913 0.781 0.842 
HP-DITG 0.915 0.795 0.851 
Table 1. Performance gains with features for 
HP-DITG. 
It is obvious that any form of ITG achieves 
better F-Measure than GIZA++. Without semi-
supervised training, however, our various DITG 
models cannot compete with BERK. Among the 
DITG models, it can be seen that precision is 
roughly the same in all cases, while W-ITG has 
the lowest recall, due to the limitation of one-to-
one matching. The improvement by (simple) 
phrase pairs is roughly the same as that by h-
phrase pairs. And it is not surprising that the 
combination of both kinds of phrases achieve the 
best result. 
Even HP-DITG does not achieve as high recall 
as BERK, it does produce promising alignment 
patterns that BERK fails to produce. For in-
stance, for the following sentence pair: 
?  ???  ?? ? ? ?? ? ?? . 
I have been ill since last weekend . 
Both GIZA++ and BERK produce the pattern 
in Figure 6(a), while HP-DITG produces the bet-
ter pattern in Figure 6(b) as it learns the h-phrase 
pair  since     ?   ?? . 
(b): HP-DITG
?           ???        ??
since          last       weekend
?           ???        ??
since          last       weekend
(a): BERK/Giza++
 
Figure 6. Partial alignment results. 
5.3 Alignment Quality of HP-DITG with 
EMD 
 Precision Recall F- Measure 
GIZA++ 0.826 0.807 0.816 
BERK 0.917 0.814 0.862 
EMD0 0.915 0.795 0.851 
EMD1 0.923 0.814 0.865 
EMD2 0.930 0.821 0.872 
EMD3 0.935 0.819 0.873 
Table 2. Semi-supervised Training Task on F-
Measure. 
The second experiment evaluates how the 
semi-supervised method of EMD improves HP-
DITG with respect to word alignment quality. 
The results are shown in Table 2. In the table, 
EMD0 refers to the HP-DITG model before any 
EMD training; EMD1 refers to the model after 
the first iteration of training, and so on. It is em-
pirically found that F-Measure is not improved 
after the third EMD iteration. 
It can be observed that EMD succeeds to help 
HP-DITG improves feature value and weight 
estimation iteratively. When semi-supervised 
736
training converges, the new HP-DITG model is 
better than before training by 2%, and better than 
BERK by 1%. 
5.4 Translation Quality of HP-DITG with 
EMD 
The third experiment evaluates the same 
alignment models in the last experiment but with 
respect to translation quality, measured by case-
insensitive BLEU4. The results are shown in 
Table 3. Note that the differences between 
EMD3 and the two baselines are statistically 
significant. 
 PBSMT HPBSMT 
05 08 05 08 
GIZA++ 33.43 23.89 33.59 24.39 
BERK 33.76 24.92 34.22 25.18 
EMD0 34.02 24.50 34.30 24.90 
EMD1 34.29 24.80 34.77 25.25 
EMD2 34.25 25.01 35.04 25.43 
EMD3 34.42 25.19 34.82 25.56 
Table 3. Semi-supervised Training Task on 
BLEU. 
It can be observed that EMD improves SMT 
performance in most iterations in most cases. 
EMD does not always improve BLEU score be-
cause the objective function of the discrimina-
tive training in EMD is about alignment F-
Measure rather than BLEU. And it is well 
known that the correlation between F-Measure 
and BLEU (Fraser and Marcu, 2007) is itself an 
intriguing problem. 
The best HP-DITG leads to more than 1 
BLEU point gain compared with GIZA++ on all 
datasets/MT models. Compared with BERK, 
EMD3 improves SMT performance significantly 
on NIST05 and slightly on NIST08. 
6 Conclusion and Future Work 
In this paper, we propose an ITG formalism 
which employs the notion of phrase/h-phrase 
pairs, in order to remove the limitation of one-to-
one matching. The formalism is proved to enable 
an alignment model to capture the linguistic fact 
that a single concept is expressed in several non-
contiguous words. Based on the formalism, we 
also propose a semi-supervised training method 
to optimize feature values and feature weights, 
which does not only improve the alignment qual-
ity but also machine translation performance 
significantly. Combining the formalism and 
semi-supervised training, we obtain better 
alignment and translation than the baselines of 
GIZA++ and BERK. 
A fundamental problem of our current frame-
work is that we fail to obtain monotonic incre-
ment of BLEU score during the course of semi-
supervised training. In the future, therefore, we 
will try to take the BLEU score as our objective 
function in discriminative training. That is to 
certain extent inspired by Deng et al (2008).  
Appendix A. The Normal Form Grammar 
Table 4 lists the ITG rules in normal form as 
used in this paper, which extend the normal form 
in Wu (1997) so as to handle the case of 
alignment to null. 
1     ?       
2     ?                                    
3     ?                         
     ?             
4     ?            
5     ?           
6    ?     
7     ?       ?     
8    ?                ?             
9    ?             ?          
Table 4. ITG Rules in Normal Form. 
In these rules,   is the Start symbol;   is the 
category for concatenating combination whereas 
  for inverted combination. Rules (2) and (3) are 
inherited from Wu (1997). Rules (4) divide the 
terminal category   into subcategories. Rule 
schema (6) subsumes all terminal unary rules for 
some English word   and foreign word  , and 
rule schemas (7) are unary rules for alignment to 
null. Rules (8) ensure all words linked to null are 
combined in left branching manner, while rules 
(9) ensure those words linked to null combine 
with some following, rather than preceding, 
word pair. (Note: Accordingly, all sentences 
must be ended by a special token      , other-
wise the last word(s) of a sentence cannot be 
linked to null.) If there are both English and for-
eign words linked to null, rule (5) ensures that 
those English words linked to null precede those 
foreign words linked to null. 
737
References 
Birch, Alexandra, Chris Callison-Burch, Miles Os-
borne and Phillipp Koehn. 2006. Constraining the 
Phrase-Based, Joint Probability Statistical Transla-
tion Model. Proceedings of the Workshop on Sta-
tistical Machine Translation. 
Brown, Peter F. Brown, Stephen A. Della Pietra, 
Vincent J. Della Peitra, Robert L. Mercer. 1993. 
The Mathematics of Statistical Machine Transla-
tion: Parameter Estimation. Computational Lin-
guistics, 19(2):263-311. 
Cherry, Colin and Dekang Lin. 2006. Soft Syntactic 
Constraints for Word Alignment through Discri-
minative Training. Proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the Association for 
Computational Linguistics.  
Cherry, Colin and Dekang Lin. 2007. Inversion 
Transduction Grammar for Joint Phrasal Transla-
tion Modeling. Proceedings of the Second Work-
shop on Syntax and Structure in Statistical Trans-
lation, Pages:17-24.  
Chiang, David. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics, 33(2). 
Deng, Yonggang, Jia Xu and Yuqing Gao. 2008. 
Phrase Table Training For Precision and Recall: 
What Makes a Good Phrase and a Good Phrase 
Pair?. Proceedings of the 7th International Confe-
rence on Human Language Technology Research 
and 46th Annual Meeting of the Association for 
Computational Linguistics, Pages:1017-1026. 
Fraser, Alexander, Daniel Marcu. 2006. Semi-
Supervised Training for StatisticalWord Align-
ment. Proceedings of the 21st International Confe-
rence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational 
Linguistics, Pages:769-776. 
Fraser, Alexander, Daniel Marcu. 2007. Measuring 
Word Alignment Quality for Statistical Machine 
Translation. Computational Linguistics, 33(3). 
Haghighi, Aria, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better Word Alignments with Super-
vised ITG Models. Proceedings of the Joint Confe-
rence of the 47th Annual Meeting of the ACL and 
the 4th International Joint Conference on Natural 
Language, Pages: 923-931. 
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. Proceedings of the 7th In-
ternational Workshop on Parsing Technologies, 
Pages:17-19 
Liu, Yang, Qun Liu and Shouxun Lin. 2005. Log-
linear models for word alignment. Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics, Pages: 81-88. 
Marcu, Daniel, William Wong. 2002. A Phrase-Based, 
Joint Probability Model for Statistical Machine 
Translation. Proceedings of 2002 Conference on 
Empirical Methods in Natural Language 
Processing, Pages:133-139. 
Moore, Robert, Wen-tau Yih, and Andreas Bode. 
2006. Improved Discriminative Bilingual Word 
Alignment. Proceedings of the 44rd Annual Meet-
ing of the Association for Computational Linguis-
tics, Pages: 513-520. 
Och, Franz Josef. 2003. Minimum error rate training 
in statistical machine translation. Proceedings of 
the 41rd Annual Meeting of the Association for 
Computational Linguistics, Pages:160-167. 
Och, Franz Josef and Hermann Ney. 2004. The 
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguistics, 
30(4) : 417-449. 
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in sta-
tistical translation. Proceedings of 16th Interna-
tional Conference on Computational Linguistics, 
Pages: 836-841. 
Wu, Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3). 
Xiong, Deyi, Qun Liu and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for 
statistical machine translation. Proceedings of the 
44rd Annual Meeting of the Association for Com-
putational Linguistics, Pages: 521-528. 
Zhang, Hao and Daniel Gildea. 2005. Stochastic Lex-
icalized Inversion Transduction Grammar for 
Alignment. Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics.  
Zhang, Hao, Chris Quirk, Robert Moore, and Daniel 
Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. 
Proceedings of the 46rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages: 
314-323. 
738
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 854?862, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Re-training Monolingual Parser Bilingually for Syntactic SMT 
 
?
Shujie Liu*, ?Chi-Ho Li, ?Mu Li and ?Ming Zhou 
?School of Computer Science and Technology 
Harbin Institute of Technology, Harbin, China 
shujieliu@mtlab.hit.edu.cn 
?Microsoft Research Asia, Beijing, China 
{chl, muli, mingzhou}@microsoft.com 
 
 
Abstract 
The training of most syntactic SMT approaches 
involves two essential components, word 
alignment and monolingual parser. In the 
current state of the art these two components 
are mutually independent, thus causing 
problems like lack of rule generalization, and 
violation of syntactic correspondence in 
translation rules. In this paper, we propose two 
ways of re-training monolingual parser with the 
target of maximizing the consistency between 
parse trees and alignment matrices. One is 
targeted self-training with a simple evaluation 
function; the other is based on training data 
selection from forced alignment of bilingual 
data. We also propose an auxiliary method for 
boosting alignment quality, by symmetrizing 
alignment matrices with respect to parse trees. 
The best combination of these novel methods 
achieves 3 Bleu point gain in an IWSLT task 
and more than 1 Bleu point gain in NIST tasks. 
1 Introduction 
There are many varieties in syntactic statistical 
machine translation (SSMT). Apart from a few 
attempts to use synchronous parsing to produce the 
tree structure of both source language (SL) and 
target language (TL) simultaneously, most SSMT 
approaches make use of monolingual parser to 
produce the parse tree(s) of the SL and/or TL 
sentences, and then link up the information of the 
two languages through word alignment. In the 
current state of the art, word aligner and 
monolingual parser are trained and applied 
separately. On the one hand, an average word 
aligner does not consider the syntax information of 
both languages, and the output links may violate 
syntactic correspondence. That is, some SL words 
yielded by a SL parse tree node may not be traced 
to, via alignment links, some TL words with 
legitimate syntactic structure. On the other hand, 
parser design is a monolingual activity and its 
impact on MT is not well studied (Ambati, 2008). 
Many good translation rules may thus be filtered 
by a good monolingual parser. 
In this paper we will focus on the translation 
task from Chinese to English, and the string-to-tree 
SSMT model as elaborated in (Galley et al2006). 
There are two kinds of translation rules in this 
model, minimal rules, and composed rules, which 
are composition of minimal rules. The minimal 
rules are extracted from a special kind of nodes, 
known as frontier nodes, on TL parse tree. The 
concept of frontier node can be illustrated by 
Figure 1, which shows two partial bilingual 
sentences with the corresponding TL sub-trees and 
word alignment links. The TL words yielded by a 
TL parse node can be traced to the corresponding 
SL words through alignment links. In the diagram, 
each parse node is represented by a rectangle, 
showing the phrase label, span, and complement 
span respectively. The span of a TL node   is 
defined as the minimal contiguous SL string that 
covers all the SL words reachable from  . The 
complement span of   is the union of spans of all 
the nodes that are neither descendants nor 
ancestors of   (c.f. Galley et al2006) . A frontier 
node is a node of which the span and the 
complement span do not overlap with each other. 
In the diagram, frontier nodes are grey in color. 
Frontier node is the key in the SSMT model, as it 
identifies the bilingual information which is 
consistent with both the parse tree and alignment 
matrix. 
There are two major problems in the SSMT 
model. The first one is the violation of syntactic 
854
structure by incorrect alignment links, as shown by 
the two dashed links in Figure 1(a). These two 
incorrect links hinder the extraction of a good 
minimal rule ???            ? and that of a 
good composed rule ??? , ?   NP(DT(the), 
NN(herdsmen), POS('s)) ?. By and large, incorrect 
alignment links lead to translation rules that are 
large in size, few in number, and poor in 
generalization ability (Fossum et al008). The 
second problem is parsing error, as shown in 
Figure 1(b). The incorrect POS tagging of the word 
?lectures" causes a series of parsing errors, 
including the absence of the noun phrase 
?NP(NN(propaganda), NN(lectures))?. These 
parsing errors hinder the extraction of good rules, 
such as ? ? ?   NP(NN(propaganda), 
NN(lectures)) ?. 
Note that in Figure 1(a), the parse tree is correct, 
and the incorrect alignment links might be fixed if 
the aligner takes the parse tree into consideration. 
Similarly, in Figure 1(b) some parsing errors might 
be fixed if the parser takes into consideration the 
correct alignment links about ?propaganda? and 
?lecture?. That is, alignment errors and parsing 
might be fixed if word aligner and parser are not 
mutually independent.  
In this paper, we emphasize more on the 
correction of parsing errors by exploiting 
alignment information. The general approach is to 
re-train a parser with parse trees which are the 
most consistent with alignment matrices. Our first 
strategy is to apply the idea of targeted self-
training (Katz-Brown et al2011) with the simple 
evaluation function of frontier set size. That is to 
re-train the parser with the parse trees which give 
rise to the largest number of frontier nodes. The 
second strategy is to apply forced alignment 
(Wuebker et al2010) to bilingual data and select 
the parse trees generated by our SSMT system for 
re-training the parser. Besides, although we do not 
invent a new word aligner exploiting syntactic 
information, we propose a new method to 
symmetrize the alignment matrices of two 
directions by taking parse tree into consideration.  
6
1-6
NNS
6
1-6
POS
4
1-3,5-6
NNIN
3
1-2,4-6
lived
1    
 in
2 
     the
3  
 herdsmen
4 
?s
 5 
  yurts
6  
   at
 7 
   night
8  
??
1
           ??
2
          ?
3
         ??
4
        ?
5
       ??
6
 
2 3 null1 4 6 6 6 1
VDB
2
1,3-6
DT
null1
1-6
6
1-6
IN
1
2-6
NN
1
2-6
NP
4-6
1-3,6
NP
4-6
1-3,6
NP
3-6
1-6
PP
1-6
2-6
PP
1-6
----
VP
a
1
large
2 
number
3
 of
4 
people
5 
coming
6
 to
7
 listen
8
 to
9
 their
10
 propaganda
11
 lectures
12
?
1
               ?
2
            ??
3
           ??
4
                ?
5
                 ?
6
               ??
7
7 7 5 6 1 4 4null null null null 3
null
1-7
DT
7
1-7
JJ
7
1-7
NN
5
1-4,6-7
IN
6
1-5,7
NNS
1
3-7
VBG
null
1-7
TO
null
1-7
VB
null
1-7
TO
3
1,4-7
PRP
4
1-7
NN
4
1-7
VP
7
1-6
NP
3-4
1,4-7
NP
3-4
1,4-7
PP
3-4
1,4-7
VP
3-4
1,4-7
VP
1-4
4-7
VP
6
1-5,7
NP
1-6
4,5,7
NP
1-6
4,7
PP
1-7
4
NP
1-7
----
S
 
(a)  (b) 
Figure 1. Two example partial bilingual sentences with word alignment and syntactic tree for the 
target sentence. All the nodes in gray are frontier nodes. Example (a) contains two error links (in dash 
line), and the syntactic tree for the target sentence of example (b) is wrong. 
 
855
2 Parser Re-training Strategies 
Most monolingual parsers used in SSMT are 
trained upon certain tree bank. That is, a parser is 
trained with the target of maximizing the 
agreement between its decision on syntactic 
structure and that decision in the human-annotated 
parse trees. As mentioned in Section 1, 
monolingual syntactic structure is not necessarily 
suitable for translation, and sometimes the 
bilingual information in word alignment may help 
the parser find out the correct structure. Therefore, 
it is desirable if there is a way to re-train a parser 
with bilingual information. 
What is needed includes a framework of parser 
re-training, and a data selection strategy that 
maximizes the consistency between parse tree and 
alignment matrix. Our two solutions will be 
introduced in the next two subsections respectively. 
2.1 Targeted Self-Training with Frontier Set 
Based Evaluation (TST-FS) 
The first solution is based on targeted self-training 
(TST) (Katz-Brown et al2011). In standard self-
training, the top one parse trees produced by the 
current parser are taken as training data for the 
next round, and the training objective is still the 
correctness of monolingual syntactic structure. In 
targeted self-training, the training objective shifts 
to certain external evaluation function. For each 
sentence, the n-best parse trees from the current 
parser are re-ranked in accordance with this 
external evaluation function, and the top one of the 
re-ranked candidates is then selected as training 
data for the next round. The key of targeted self-
training is the definition of this external evaluation 
function. 
As shown by the example in Figure 1(b), an 
incorrect parse tree is likely to hinder the 
extraction of good translation rules, because the 
number of frontier nodes in the incorrect tree is in 
general smaller than that in the correct tree. 
Consider the example in Figure 2, which is about 
the same partial bilingual sentence as in Figure 
1(b). Although both parse trees do not have the 
correct syntactic structure, the tree in Figure 2 has 
more frontier nodes, leads to more valid translation 
rules, and is therefore more preferable.  
This example suggests a very simple external 
evaluation function, viz. the size of frontier set. 
Given a bilingual sentence, its alignment matrix, 
and the N-best parse trees of the TL sentence, we 
will calculate the number of frontier nodes for each 
parse tree, and re-rank the parse trees in its 
descending order. The new top one parse tree is 
selected as the training data for the next round of 
targeted self-training of the TL parser. In the 
following we will call this approach as targeted 
self-training with frontier set based evaluation 
(TST-FS). 
Note that the size of the N-best list should be 
kept small. It is because sometimes a parse tree 
with an extremely mistaken structure happens to 
have perfect match with the alignment matrix, 
thereby giving rise to nearest the largest frontier set 
size. It is empirically found that a 5-best list of 
parse trees is already sufficient to significantly 
improve translation performance. 
2.2 Forced Alignment-based Parser Re-
Training (FA-PR) 
If we doubt that the parse tree from a monolingual 
parser is not appropriate enough for translation 
purpose, then it seems reasonable to consider using 
the parse tree produced by an SSMT system to re-
train the parser. A na?ve idea is simply to run an 
SSMT system over some SL sentences and retrieve 
the by-product TL parse trees for re-training the 
monolingual parser. The biggest problem of this 
na?ve approach is that the translation by an MT 
system is often a 'weird' TL sentence, and thus the 
associated parse tree is of little use in improving 
the parser. 
Forced alignment (Wuebker et al2010) of 
bilingual data is a much more promising approach. 
7
1-6
NP
3-4
1,5-7
NP
3-4
1,5-7
PP
3-4
1,5-7
VP
6
1-5,7
NP
1-6
3-5,7
NP
VP
1
3-7
1-6
3-4,7
PP
3-4
1,5-7
VP
1-7
3-4
NP
a1large2 number3 of4 people5 coming6 to7 listen8 to9 their10 propaganda11 lectures12
?1               ?2            ??3           ??4                ?5                 ?6               ??7
7 7 5 6 1 4 4null null null null 3
null
1-7
DT
7
1-7
JJ
7
1-7
NN
5
1-4,6-7
IN
6
1-5,7
NNS
1
3-7
VBG
null
1-7
TO
null
1-7
VB
null
1-7
TO
3
1,4-7
PRP
4
1-7
NN
4
1-7
VP
 
Figure 2. The parse tree selected by TST-FS for 
the example in Figure 1(b) 
856
When applied to SSMT, given a bilingual sentence, 
it performs phrase segmentation of the SL side, 
parsing of the TL side, and word alignment of the 
bilingual sentence, using the full translation system 
as in decoding. It finds the best decoding path that 
generates the TL side of the bilingual sentence, and 
the parse tree of the TL sentence is also obtained as 
a by-product. The parse trees from forced 
alignment are suitable for re-training the 
monolingual parser.  
Here is the simple iterative re-training algorithm. 
First we have a baseline monolingual parser and 
plug it into an SSMT system. Then perform forced 
alignment, using the SSMT system, of some 
bilingual data and obtain the parse trees as new 
training data for the parser. The new parser can 
then be applied again to do the second round of 
forced alignment. This iteration of forced 
alignment followed by parser re-training is kept 
going until some stopping criterion is met. In the 
following we will call this approach as forced 
alignment based parser re-training (FA-PR). 
Algorithm 1  Forced Alignment Based Parser Re-
Training (FA-PR) 
? step1:      ;                . 
? step2: Use parser      to parse target 
sentences of training data, and build a 
SSMT system      . 
? step3: Perform forced alignment on training 
data with      to get parse trees 
         for target sentence of training 
data. 
? step4: Train a new parser          with 
         . 
? step5:                       . 
? Step6: Go to step 2, until performance of      
on development data drops, or a preset 
limit is reached. 
There are a few important implementation 
details of FA-PR. Forced alignment is guaranteed 
to obtain a parse tree if all translation rules are kept 
and no pruning is performed during decoding. Yet 
in reality an average MT system applies pruning 
during translation model training and decoding, 
and a lot of translation rules will then be discarded. 
In order to have more parse trees be considered by 
forced alignment, we keep all translation rules and 
relax pruning constraints in the decoder, viz. 
enlarge the stack size of each cell in the chart from 
50 to 150.  
Another measure to guarantee the existence of a 
decoding path in forced alignment is to allow part 
of a SL or TL sentence translate to null. Consider 
the example in Figure 1(b). We also add a null 
alignment for any span of the source and target 
sentences to handle the null translation scenario. It 
is easy to add a null translation candidate for a 
span of the source sentence during decoding, but 
not easy for target spans. For example, suppose the 
best translation candidate for the source span " ? 1  
NP ? 5 ? 6 ?? 7" is "a large number of people 
coming NP", and the best translation candidate for 
"? 2 ?? 3 ?? 4" is "their propaganda lectures", 
there is no combination of candidates from two n-
best translation lists which can match a sequence in 
the given target part, so we add a translation 
candidate ("to listen to ") generated from null, 
whose syntactic label can be any label (decided 
according to the translated context, which is 
?ADJP? here).  The feature weights for the added 
null alignment are set to be very small, so as to 
avoid the competition with the normal candidates. 
In order to generate normal trees with not so many 
null alignment sub-trees for the target sentence 
(such trees are not suitable for parser re-training), 
only target spans with less than 4 words can align 
to null, and such null-aligned sub-tree can only be 
added  no more than 3 times.  
With all the mentioned modification of the 
forced alignment, the partial target tree generated 
using forced alignment for the example in Figure 
1(b) is shown in Figure 3. We can see that even 
a1large2 number3 of4 people5 coming6 to7 listen8 to9 their10 propaganda11 lectures12
7
1-6
NP
4
1-3,5-7
NP
6
1-5,7
NP
5-6
1-4,7
PP
5-7
1-4
NP
null
1-7
ADJP
3-4
1,5-7
NP
?1               ?2            ??3           ??4                ?5                 ?6               ??7
7 7 5 6 1 4 4null null null null 3
null
1-7
DT
7
1-7
JJ
7
1-7
NN
5
1-4,6-7
IN
6
1-5,7
NNS
1
3-7
VBG
null
1-7
TO
null
1-7
VB
null
1-7
TO
3
1,4-7
PRP
4
1-7
NN
4
1-7
VP
3-4
1,5-7
NP
1-4
5-7
NP
 
Figure 3. The parse tree selected by FA-PR for the 
example in Figure 1(b) 
 
857
with an incorrect sub-tree, more useful rules can be 
extracted, compared with the baseline sub-tree and 
the sub-tree generated from TST-FS. 
3  Word Alignment Symmetrization 
The most widely used word aligners in MT, like 
HMM and IBM Models (Och and Ney, 2003), are 
directional aligners. Such aligner produces one set 
of alignment matrices for the SL-to-TL direction 
and another set for the TL-to-SL direction. 
Symmetrization refers to the combination of these 
two sets of alignment matrices.  
The most popular method of symmetrization is 
intersect-diag-grow (IDG). Given a bilingual 
sentence and its two alignment matrices     and 
     IDG starts with all the links in        . 
Then IDG considers each link in           
          in turn. A link is added if its addition 
does not make some phrase pairs overlap. 
Although IDG is simple and efficient, and has been 
shown to be effective in phrase-based SMT, it is 
problematic in SSMT, as illustrated by the example 
in section 1. 
3.1 Intersect-Diag-Syntactic-Grow (IDSG) 
We propose a new symmetrization method, 
Intersect-Diag-Syntactic-Grow (IDSG), which is 
an adaptation of IDG but also taking syntactic 
information in consideration. It is sketched in 
Algorithm 2.  
Algorithm 2 Intersect-Diag-Syntactic-Grow  
? step1: Generate all the candidate links        
using IDG. 
? step2: Select the one which can generate the 
biggest frontier set: 
        
         
                          
? step3: Add   to  , and repeat step 1, until no 
new link can be added. 
Like IDG, IDSG starts with all the links in 
        and its main task is to add links selected 
from                         . IDSG is 
also subject to the constraints of IDG. The new 
criterion in link selection in IDSG is specified in 
Step 2. Given a parse tree of the TL side of the 
bilingual sentence, in each iteration IDSG 
considers the change of frontier set size caused by 
the addition of each link in       . The link 
leading to the maximum number of frontier nodes 
is added (and removed from       ). This process 
continues until no more links can be added. 
In sum, IDSG add links in an order which take 
syntactic structure into consideration, and the link 
with the least violation of the syntactic structure is 
added first. 
For the example in Figure 1(a), IDSG succeeds 
in discarding the two incorrect links, and produces 
the final alignment and frontier set as shown in 
Figure 4. Note that IDSG still fails to produce the 
correct link (the3, ?? 4), since this link does not 
appear in        at all. 
3.2 Combining TST-FS/FA-PR and IDSG 
Parser re-training aims to improve a parser with 
alignment matrix while IDSG aims to improve 
alignment matrix with parse tree. It is reasonable to 
combine them, and there are two alternatives of the 
combination, depending on the order of application. 
That is, we could either improve alignment matrix 
by IDSG and then re-train parser with the better 
alignment, or re-train parser and then improve 
alignment matrix with better syntactic information. 
Either alternative can be arranged into an iterative 
training routine, but empirically it is found that 
only one round of parser re-training before or after 
only one round of IDSG is already enough. 
6
1-5
NNS
5
1-4,6
POS
4
1-3,5-6
NNIN
3
1-2,4-6
lived
1    
 in
2 
     the
3  
 herdsmen
4 
?s
 5 
  yurts
6  
   at
 7 
   night
8  
??
1
           ??
2
          ?
3
         ??
4
        ?
5
       ??
6
 
2 3 null1 4 5 6 1 1
VDB
2
1,3-6
DT
null1
1-6
1
1-6
IN
1
1-6
NN
1
1-6
NP
4-5
1-3,6
NP
4-6
1-3
NP
3-6
1-2
PP
1
2-6
PP
1-6
----
VP
 
Figure 4, the alignment generated by IDSG for the 
example in Figure 1(a) 
858
4 Experiment 
In this section, we conduct experiments on Chinese 
to English translation task to test our proposed 
methods of parser re-training and word alignment 
symmetrization.  The evaluation method is the case 
insensitive IBM BLEU-4 (Papineni et al2002). 
Significant testing is carried out using bootstrap re-
sampling method proposed by Koehn (2004) with 
a 95% confidence level. 
4.1 Parser and SMT Decoder 
The syntactic parser we used in this paper is 
Berkley parser, with the grammar trained on WSJ 
corpus, and the training method follows Petrov and 
Klein (2007). Our SMT decoder is an in-house 
implementation of string-to-tree decoder. The 
features we used are standard used features, such 
as translation probabilities, lexical weights, 
language model probabilities and distortion 
probability. The feature weights are tuned using 
the minimum error rate training (MERT) (Och, 
2003). 
4.2 Experiment Data Setting and Baselines 
We test our method with two data settings: one is 
IWSLT data set, the other is NIST data set. 
 dev8+dialog dev9 
Baseline 50.58 49.85 
Table 1. Baselines for IWSLT data set 
 NIST'03 NIST'05 NIST'08 
Baseline 37.57 36.44 24.87 
Table 2. Baselines for NIST data set 
Our IWSLT data is the IWSLT 2009 dialog task 
data set. The training data include the BTEC and 
SLDB training data. The training data contains 81k 
sentence pairs, 655k Chinese words and 806k 
English words. The language model is 5-gram 
language model trained with the English sentences 
in the training data. We use the combination of 
dev8 and dialog as development set, and dev9 as 
test set. The TL sentences of the training data with 
the selected/generated trees are used as the training 
data to re-train the parser. To get the baseline of 
this setting, we run IDG to combine the bi-
direction alignment generated by Giza++ (Och 
Ney, 2003), and run Berkeley parser (Petrov and 
Klein, 2007) to parse the target sentences. With the 
baseline alignments and syntactic trees, we extract 
rules and calculate features. The baseline results 
are shown in Table 1. 
For the NIST data set, the bilingual training data 
we used is NIST 2008 training set excluding the 
Hong Kong Law and Hong Kong Hansard. The 
training data contains 354k sentence pairs, 8M 
Chinese words and 10M English words, and is also 
the training data for our parser re-training. The 
language model is 5-gram language model trained 
with the Giga-Word corpus plus the English 
sentences in the training data. The development 
data to tune the feature weights of our decoder is 
NIST 2003 evaluation set, and test sets are NIST 
2005 and 2008 evaluation sets. The baseline for 
NIST data is got in a similar way with for IWSLT, 
which are shown in Table 2 . 
4.3 Results of TST-FS/ FA-PR 
The parser re-training strategies TST-FS and FA-
PR are tested with two baselines, one is the default 
parser without any re-training and another is 
standard self-training (SST). All three re-training 
approaches are based on the same bilingual 
datasets as used in translation model training. The 
MT performances on IWSLT and NIST by the four 
approaches are shown in Table 3 and 4 
respectively. 
It can be seen that just standard self-training 
does improve translation performance, as re-
training on the TL side of bilingual data is a kind 
of domain adaptation (from WSJ to IWSLT/NIST). 
But targeted self-training achieves more noticeable 
improvement, almost twice as much as standard 
self-training. This confirms the value of word 
alignment information in parser re-training. Finally, 
the even larger improvement of FA-PR than TST-
FS shows that merely increasing the number of 
frontier nodes is not enough.  Some frontier nodes 
are of poor quality, and the frontier nodes found in 
forced alignment are more suitable.  
It can also be seen that the improvement in 
IWSLT is larger than that in NIST. The first reason 
is that both WSJ and NIST are of the news domain 
and of formal writing style, whereas IWSLT is of 
the tourist domain and of colloquial style. 
Therefore any improvement from the default parser, 
which is trained on WSJ, is expected to be smaller 
in the NIST case. Another reason is that, since the 
859
IWSLT dataset is much smaller, the impact of 
more and better rules is more obvious.  
Note that the figures in Table 3 and 4 are about 
parser re-training for only one iteration. It is found 
that, more iteration do not lead to further 
significant improvement. The forced alignment of 
bilingual training data does not obtain a full 
decoding path for every bilingual sentence. It is 
because, although all translation rules are kept, 
there is still pruning during decoding. Only 64% of 
the IWSLT dataset and 53% of the NIST dataset 
can be successfully forced-aligned. In general, the 
longer the bilingual sentence, the less likely forced 
alignment is successful, and that is why a lower 
proportion of NIST can be forced-aligned. 
4.4  Symmetrization 
The new symmetrization method IDSG is 
compared with the baseline method IDG. 
 dev8+dialog dev9 # Rules 
IDG 50.58 49.85 515K 
IDSG 
52.71 
(+2.31) 
51.80 
(+2.05) 
626K 
Table 5. MT performance of symmetrization 
methods on IWSLT data set. The results in bold 
type are significantly better than the performance 
of IDG. 
 NIST'03 NIST'05 NIST'08 #Rules 
IDG 37.57 36.44 24.87 3,376K 
IDSG 
38.15 
(+0.58) 
37.07 
(+0.63) 
25.67 
(+0.80) 
4,109K 
Table 6. MT performance of symmetrization 
methods on NIST data. The results in bold type are 
significantly better than the performance of IDG. 
As shown by the results in Table 5 and 6, IDSG 
enlarges the set of translation rules by more than 
20%, thereby improving translation performance 
significantly. As in parser re-training, the 
improvement in the IWSLT task is larger than that 
in the NIST task. Again, it is because the IWSLT 
dataset is very small and so the effect of rule table 
size is more obvious. 
4.5 Methods combined 
As mentioned in section 3.2, parser re-training and 
the new symmetrization method can be combined 
in two different ways, depending on the order of 
application. Table 7 and 8 show the experiment 
results of combining FA-PR with IDSG. 
It can be seen that either way of the combination 
is better than using FA-PR or IDSG alone. Yet 
there is no significant difference between the two 
kinds of combination.  
The best result is a gain of more than 3 Bleu 
points on IWSLT and that of more than 1 Bleu 
point on NIST.  
5 Related Works 
There are a lot of attempts in improving word 
alignment with syntactic information (Cherry and 
Lin, 2006; DeNero and Klein, 2007; Hermjackob, 
2009) and in improving parser with alignment 
information (Burkett and Klein, 2008). Yet strictly 
speaking all these attempts aim to improve the 
 dev8+dialog dev9 # Rules 
Baseline 50.58 49.85 515K 
SST 
52.04 
(+1.46) 
51.26 
(+1.41) 
574K 
TST-FS 
52.75 
(+2.17) 
52.51 
(+2.66) 
572K 
FA-PR 
53.31 
(+2.73) 
52.8 
(+2.95) 
591K 
Table 3. MT performance of parser re-training 
strategies on IWSLT data set. The results in 
bold type are significantly better than the 
baseline. 
 NIST'03 NIST'05 NIST'08 #Rules 
Baseline 37.57 36.44 24.87 3,376K 
SST 
37.98 
(+0.41) 
36.79 
(+0.35) 
25.30 
(+0.43) 
3,462K 
TST-FS 
38.42 
(+0.85) 
37.39 
(+0.95) 
25.79 
(+0.92) 
3,642K 
FA-PR 
38.74 
(+1.17) 
37.69 
(+1.25) 
25.89 
(+1.02) 
3,976K 
Table 4. MT performance of parser re-training 
strategies on NIST data set. The results in bold 
type are significantly better than the baseline. 
860
 dev8+dialog dev9 
# 
Rules 
Baseline 50.58 49.85 515K 
IDSG 
52.71 
(+2.31) 
51.80 
(+2.05) 
626K 
FA-PR 
53.31 
(+2.73) 
52.8 
(+2.95) 
591K 
IDSG  then 
FA-PR 
53.64 
(3.06) 
53.32 
(+3.47) 
602K 
FA-PR then 
IDSG 
53.81 
(+3.23) 
53.26 
(+3.41) 
597K 
Table 7. MT performance of the new methods 
on IWSLT data set. The results in bold type 
are significantly better than the baseline. 
 NIST'03 NIST'05 NIST'08 #Rules 
Baseline 37.57 36.44 24.87 3,376K 
IDSG 
38.15 
(+0.58) 
37.07 
(+0.63) 
25.67 
(+0.80) 
4,109K 
FA-PR 
38.74 
(+1.17) 
37.69 
(+1.25) 
25.89 
(+1.02) 
3,976K 
IDSG 
then 
FA-PR 
38.97 
(+1.40) 
37.95 
(+1.51) 
26.74 
(+1.87) 
4,557K 
FA-PR 
then 
IDSG 
38.90 
(+1.33) 
37.94 
(+1.50) 
26.52 
(+1.65) 
4,478K 
Table 8. MT performance of the new methods 
on NIST data set. The results in bold type are 
significantly better than the baseline. 
parser/aligner itself rather than the translation 
model.  
To improve the performance of syntactic 
machine translation, Huang and Knight (2006) 
proposed a method incorporating a handful of 
relabeling strategies to modify the syntactic trees 
structures. Ambati and Lavie (2008) restructured 
target parse trees to generate highly isomorphic 
target trees that preserve the syntactic boundaries 
of constituents aligned in the original parse trees. 
Wang et al(2010) proposed to use re-structuring 
and re-labeling to modify the parser tree. The re-
structuring method uses a binarization method to 
enable the reuse of sub-constituent structures, and 
the linguistic and statistical re-labeling methods to 
handle the coarse nonterminal problem, so as to 
enhance generalization ability. Different from the 
previous work of modifying tree structures with 
post-processing methods, our methods try to learn 
a suitable grammar for string-to-tree SMT models, 
and directly produce trees which are consistent 
with word alignment matrices.  
Instead of modifying the parse tree to improve 
machine translation performance, many methods 
were proposed to modify word alignment by taking 
syntactic tree into consideration, including deleting 
incorrect word alignment links by a discriminative 
model (Fossum et al2008), re-aligning sentence 
pairs using EM method with the rules extracted 
with initial alignment (Wang et al2010), and 
removing ambiguous alignment of functional 
words with constraint from chunk-level 
information during rule extraction (Wu et al
2011). Unlike all these pursuits, to generate a 
consistent word alignment, our method modifies 
the popularly used IDG symmetrization method to 
make it suitable for string-to-tree rule extraction, 
and our method is much simpler and faster than the 
previous works.  
6 Conclusion  
In this paper we have attempted to improve SSMT 
by reducing the errors introduced by the mutual 
independence between monolingual parser and 
word aligner. Our major contribution is the 
strategies of re-training parser with the bilingual 
information in alignment matrices. Either of our 
proposals of targeted self-training with frontier set 
size as evaluation function and forced alignment 
based re-training is more effective than baseline 
parser or standard self-training of parser. As an 
auxiliary method, we also attempted to improve 
alignment matrices by a new symmetrization 
method.  
In future, we will explore more alternatives in 
integrating parsing information and alignment 
information, such as discriminative word 
alignment using a lot of features from parser. 
References  
Vamshi Ambati and Alon Lavie. 2008. Improving 
syntax driven translation models by re-structuring 
divergent and non-isomorphic parse tree structures. 
In Student Research Workshop of the Eighth 
Conference of the Association for Machine 
Translation in the Americas, pages 235-244. 
861
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic parsing). In 
Proceedings of the Conference on Empirical 
Methods on Natural Language Processing, pages 
877-886. 
Colin Cherry and Dekang Lin. 2006. Soft syntactic 
constraints for word alignment through 
discriminative training. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics.  
John DeNero and Dan Klein. 2007. Tailing word 
alignment to syntactic machine translation. In 
Proceedings of the Association for Computational 
Linguistics, pages 17-24. 
Victoria Fossum, Kevin Knight, Steven Abney. 2008. 
Using syntax to improve word alignment precision 
for syntax-based machine translation. In Proceedings 
of the Third Workshop on Statistical Machine 
Translation, pages 44-52. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve Deneefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inference and training of 
context-rich syntactic translation models. In 
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the Association for Computational Linguistics, 
pages 961-968. 
Ulf Hermjackob. Improved word alignment with 
statistics and linguistic heuristics. In Proceedings of 
the Conference on Empirical Methods on Natural 
Language Processing, pages 229-237. 
Bryant Huang, Kevin Knight. 2006. Relabeling syntax 
trees to improve syntax-based machine translation 
quality. In Proceedings of the Human Technology 
Conference of the North American Chapter of the 
ACL, pages 240-247. 
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz 
Och, David Talbot, Hiroshi Ichikawa, Masakazu 
Seno, Hideto Kazawa. 2011. Training a parser for 
machine translation reordering. In Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing, pages 183-192. 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing, pages 388-395. 
Wei Wang, Jonathan May, Kevin Knight, Daniel Marcu. 
2010. Re-structuring, re-labeling, and re-alignment 
for syntax-Based machine translation. Computational 
Linguistics, 36(2). 
Xianchao Wu, Takuya Matsuzaki and Jun'ichi Tsujii. 
2011. Effective use of function words for rule 
generalization in forest-based translation. In 
Proceedings of the Association for Computational 
Linguistics, pages 22-31. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
160-167. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1). 
Joern Wuebker, Arne Mauser and Hermann Ney. 2010. 
Training phrase translation models with leaving-one-
out. In Proceedings of the Association for 
Computational Linguistics, pages 475-484. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, pages 
311-318. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of Human 
Language Technologies: The Annual Conference of 
the North American Chapter of the Association for 
Computational Linguistics, pages 404?411. 
 
862
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 426?435,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Collective Entity Linking with Stacking
Zhengyan He? Shujie Liu? Yang Song? Mu Li? Ming Zhou? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
songyangmagic@gmail.com wanghf@pku.edu.cn
Abstract
Entity disambiguation works by linking am-
biguous mentions in text to their correspond-
ing real-world entities in knowledge base. Re-
cent collective disambiguation methods en-
force coherence among contextual decisions
at the cost of non-trivial inference processes.
We propose a fast collective disambiguation
approach based on stacking. First, we train a
local predictor g0 with learning to rank as base
learner, to generate initial ranking list of can-
didates. Second, top k candidates of related
instances are searched for constructing expres-
sive global coherence features. A global pre-
dictor g1 is trained in the augmented feature
space and stacking is employed to tackle the
train/test mismatch problem. The proposed
method is fast and easy to implement. Exper-
iments show its effectiveness over various al-
gorithms on several public datasets. By learn-
ing a rich semantic relatedness measure be-
tween entity categories and context document,
performance is further improved.
1 Introduction
When extracting knowledge from natural language
text into a machine readable format, ambiguous
names must be resolved in order to tell which real-
world entity the name refers to. The task of linking
names to knowledge base is known as entity linking
or disambiguation (Ji et al, 2011). The resulting text
is populated with semantic rich links to knowledge
base like Wikipedia, and ready for various down-
stream NLP applications.
?Corresponding author
Previous researches have proposed several kinds
of effective approaches for this problem. Learning
to rank (L2R) approaches use hand-crafted features
f(d, e) to describe the similarity or dissimilarity be-
tween contextual document d and entity definition
e. L2R approaches are very flexible and expres-
sive. Features like name matching, context similar-
ity (Li et al, 2009; Zheng et al, 2010; Lehmann et
al., 2010) and category context correlation (Bunescu
and Pasca, 2006) can be incorporated with ease.
Nevertheless, decisions are made independently and
inconsistent results are found from time to time.
Collective approaches utilize dependencies be-
tween different decisions and resolve all ambiguous
mentions within the same context simultaneously
(Han et al, 2011; Hoffart et al, 2011; Kulkarni
et al, 2009; Ratinov et al, 2011). Collective ap-
proaches can improve performance when local ev-
idence is not confident enough. They often utilize
semantic relations across different mentions, and is
why they are called global approaches, while L2R
methods fall into local approaches (Ratinov et al,
2011). However, collective inference processes are
often expensive and involve an exponential search
space.
We propose a collective entity linking method
based on stacking. Stacked generalization (Wolpert,
1992) is a powerful meta learning algorithm that
uses two levels of learners. The predictions of the
first learner are taken as augmented features for the
second learner. The nice property of stacking is that
it does not restrict the form of the base learner. In
this paper, our base learner, an L2R ranker, is first
employed to generate a ranking list of candidates.
426
At the next level, we search for semantic coherent
entities from the top k candidates of neighboring
mentions. The second learner is trained on the aug-
mented feature space to enforce semantic coherence.
Stacking is employed to handle train/test mismatch
problem. Compared with existing collective meth-
ods, the inference process of our method is much
faster because of the simple form of its base learner.
Wikipedians annotate each entity with categories
which provide another source of valuable seman-
tic information. (Bunescu and Pasca, 2006) pro-
pose to generalize beyond context-entity correla-
tion s(d, e) with word-category correlation s(w, c).
However, this method works at word level, and does
not scale well to large number of categories. We
explore a representation learning technique to learn
the category-context association in latent semantic
space, which scales much better to large knowledge
base.
Our contributions are as follows: (1) We pro-
pose a fast and accurate stacking-based collective
entity linking method, which combines the benefits
of both coherence modeling of collective approaches
and expressivity of L2R methods. We show an
effective usage of ranking list as global features,
which is a key improvement for the global predictor.
(2) To overcome problems of scalability and shal-
low word-level comparison, we learn the category-
context correlation with recent advances of repre-
sentation learning, and show that this extra seman-
tic information indeed helps improve entity linking
performance.
2 Related Work
Most popular entity linking systems use the L2R
framework (Bunescu and Pasca, 2006; Li et al,
2009; Zheng et al, 2010; Lehmann et al, 2010).
Its discriminative nature gives the model enough
flexibility and expressivity. It can include any fea-
tures that describe the similarity or dissimilarity of
context d and candidate entity e. They often per-
form well even on small training set, with carefully-
designed features. This category falls into the local
approach as the decision processes for each mention
are made independently (Ratinov et al, 2011).
(Cucerzan, 2007) first suggests to optimize an ob-
jective function that is similar to the collective ap-
proach. However, the author adopts an approxi-
mation method because of the large search space
(which is O(nm) for a document with m mentions,
each with n candidates). Various other methods
like integer linear programming (Kulkarni et al,
2009), personalized PageRank (Han et al, 2011) and
greedy graph cutting (Hoffart et al, 2011) have been
explored in literature. Our method without stacking
resembles the method of (Ratinov et al, 2011) in
that they use the predictions of a local ranker to gen-
erate features for global ranker. The differences are
that we use stacking to train the local ranker to han-
dle the train/test mismatch problem and top k candi-
dates to generate features for the global ranker.
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that uses multiple learners out-
puts to augment the feature space of subsequent
learners. It utilizes a cross-validation strategy to ad-
dress the train set / testset label mismatch problem.
Various applications of stacking in NLP have been
proposed, such as collective document classification
(Kou and Cohen, 2007), stacked dependency parsing
(Martins et al, 2008) and joint Chinese word seg-
mentation and part-of-speech tagging (Sun, 2011).
(Kou and Cohen, 2007) propose stacked graphical
learning which captures dependencies between data
with relational template. Our method is inspired by
their approach. The difference is our base learner is
an L2R model. We search related entity candidates
in a large semantic relatedness graph, based on the
assumption that true candidates are often semanti-
cally correlated while false ones scattered around.
Wikipedians annotate entries in Wikipedia with
category network. This valuable information gener-
alizes entity-context correlation to category-context
correlation. (Bunescu and Pasca, 2006) utilize
category-word as features in their ranking model.
(Kataria et al, 2011) employ a hierarchical topic
model where each inner node in the hierarchy is a
category. Both approaches must rely on pruned cate-
gories because the large number of noisy categories.
We try to address this problem with recent advances
of representation learning (Bai et al, 2009), which
learns the relatedness of category and context in la-
tent continuous space. This method scales well to
potentially large knowledge base.
427
3 Method
In this section, we first introduce our base learner
and local features used; next, the stacking train-
ing strategy is given, followed by an explana-
tion of our global coherence model with aug-
mented feature space; finally we explain how to
learn category-context correlation with representa-
tion learning technique.
3.1 Base learner and local predictor g0
Entity linking is formalized as follows: given
an ambiguous name mention m with its con-
textual document d, a list of candidate entities
e1, e2, . . . , en(m) ? C(m) is generated for m, our
predictor g will generate a ranking score g(ei) for
each candidate ei. The ranking score will be used
to construct augmented features for the next level
learner, or used by our end system to select the an-
swer:
e? = arg max
e?C(m)
g(e) (1)
In an L2R framework, the model is often defined
as a linear combination of features. Here, our fea-
tures f?(d, e) are derived from document d and can-
didate e. The model is defined as g(e) = w?f?(d, e).
In our problem, we are given a list of training data
D = {(di, ei)}. We want to optimize the parameter
w?, such that the correct entity has a higher score over
negative ones. This is done via a preference learning
technique SVM rank, first introduced by (Joachims,
2002). The following margin based loss is mini-
mized w.r.t w?:
L = 1
2
?w??2 + C
?
?d,e? (2)
s.t. w?(f?(d, e)? f?(d, e?)) ? 1? ?d,e? (3)
?d,e? ? 0 (4)
where C is a trade-off between training error and
margin size; ? is slacking variable and loops over
all query documents d and negative candidates e? ?
C(m)? {e}.
This model is expressive enough to include any
form of features describing the similarity and dis-
similarity of d and e. We only include some typical
features seen in literature. The inclusion of these
features is not meant to be exhaustive. Our purpose
is to build a moderate model in which some of the
Surface matching:
1. mention string m exactly matches candidate
e, i.e. m = e
2. neither m is a substring of e nor e is a sub-
string of m
3. m ?= e and m is a substring of e
4. m ?= e and e is a substring of m
5. m ?= e and m is a redirect pointing to e in
Wikipedia
6. m ?= e and e starts with m
7. m ?= e and e ends with m
Context matching:
1. cosine similarity of TF-IDF score between
context and entire Wikipedia page of candidate
2. cosine similarity of TF-IDF score between
context and introduction of Wikipedia page
3. jaccard distance between context and entire
Wikipedia page of candidate
4. jaccard distance between context and intro-
duction of Wikipedia page
Popularity or prominence feature:
percentage of Wikipedia hyperlinks pointing to
e given mention m, i.e. P(e|m)
Category-context coherence model:
cat0 and cat1 (details in Section 3.4)
Table 1: Features for local predictor g0.
useful features like string matching and entity pop-
ularity cannot be easily expressed by collective ap-
proaches like (Hoffart et al, 2011; Han et al, 2011).
The features for level 0 predictor g0 are described
in Table 1. The reader can consult (Li et al, 2009;
Zheng et al, 2010; Lehmann et al, 2010) for further
reference.
3.2 Stacking training for global predictor g1
Stacked generalization (Wolpert, 1992) is a meta
learning algorithm that stacks two ?levels? of pre-
dictors. Level 0 includes one or more predictors
h(0)1 , h
(0)
2 , . . . , h
(0)
K : Rd ? R, each one is trained on
the original d-dimensional feature space. The level
1 predictor h(1) : Rd+K ? R is trained in the aug-
mented (d+K)-dimensional feature space, in which
predictions at level 0 are taken as extra features in
h(1).
(Kou and Cohen, 2007) proposed stacked graphi-
428
cal learning for learning and inference on relational
data. In stacked graphical learning, dependencies
among data are captured by relational template, with
which one searches for related instances of the cur-
rent instance. The augmented feature space does
not necessarily to be d + K. Instead, one can con-
struct any declarative feature with the original data
and predictions of related instances. For instance,
in collective document classification (Kou and Co-
hen, 2007) employ relational template to extract
documents that link to this document, then apply a
COUNT aggregator over each category on neighbor-
ing documents as level 1 features.
In our entity linking task, we use a single predic-
tor g0 trained with local features at level 0. Com-
pared with (Kou and Cohen, 2007), both g0 and g1
are L2R models rather than classifier. At level 1, for
each document-candidate entity pair, we use the re-
lational templateN (x) to find related entities for en-
tity x, and construct global features with some func-
tion G({g0(n)|n ? N (x)}) (details in Sec. 3.3).
The global predictor g1 receives as input the origi-
nal features plus G.
One problem is that if we use g0 trained on the en-
tire training set to predict related instances in train-
ing set, the accuracy can be somehow different (typ-
ically lower) for future unseen data. g1 with this pre-
diction as input doesn?t generalize well to test data.
This is known as train/test mismatch problem. To
mimic test time behavior, training is performed in a
cross-validation-like way. Let D be the entire train-
ing set:
1. Split D into L partitions {D1, . . . ,DL}
2. For each split Di:
2.1 Train an instance of g0 on D ?Di
2.2 Predict all related instances inDi with this
predictor g0
2.3 Augment feature space for x ? Di, with G
applied on predictions of N (x)
3. Train level 0 predictor g0 on entire D, for ex-
panding feature space for test data
4. Train level 1 predictor g1 on entire D, in the
augmented feature space.
In the next subsection, we will describe how to
construct global features from the predictions of g0
on neighbors N (x) with G.
3.3 Enforcing coherence with global features G
If one wants to identify the correct entity for an am-
biguous name, he would possibly look for related
entities in its surrounding context. However, sur-
rounding entities can also exhibit some degree of
ambiguity. In ideal cases, most true candidates are
inter-connected with semantic links while negative
candidates are scattered around (Fig. 1). Thus, we
ask the following question: Is there any highly rele-
vant entity to this candidate in context? Or, is there
any mention with highly relevant entity to this can-
didate in the top k ranking list of this mention? And
how many those mentions are? The reason to look
up top k candidates is to improve recall. g0 may not
perfectly rank related entity at the first place, e.g.
?Mitt Romney? in Figure 1.
Assume the ambiguous mention set is M . For
each mention mi ? M , we rank each entity ei,j ?
C(mi) by its score g0(ei,j). Denote its rank as
Rank(ei,j). For each entity e in the candidate set
E = {ei,j |?ei,j ? C(mi), ?mi ? M}, we search
related instances for e as follows:
1. search in E for entities with semantic related-
ness above a threshold ({0.1,0.3,0.5,0.7,0.9});
2. select those entities in step (1) with Rank(e)
less than or equal to k (k ? {1, 3, 5});
3. map entities in step (2) to unique set of men-
tions U , excluding current m, i.e. e ? C(m).
This process is relatively fast. It only involves a
sparse matrix slicing operation on the large pre-
computed semantic relatedness matrix in step (1),
and logical operation in step (2,3). The following
features are fired concerning the unique set U :
- if U is empty;
- if U is not empty;
- if the percentage |U |/|M | is above a threshold
(e.g. 0.3).
The above process generates a total of 45 (5?3?3)
global features.
429
Barack Obama Democratic Party (United States)
Mitt Romney
Republican Party (United States)
Obama, Fukui
Obama, Nagasaki
Democratic Party (Italy)
Democratic Party (Serbia)
Republican Party of Minnesota
Republicanism
Romney, West Virginia
HMS Romney (1694)
... ... ... ...
received national attention during his campaign  ...  with his vectory in the March   [[Obama|Barack Obama]]
[[Democratic Party|Democratic Party (United States)]] primary  ...  He was re-elected president in November
2012, defeating [[Republican|Republican Party (United States)]] nominee [[Romney|Mitt Romney]]
Figure 1: Semantic links for collective entity linking. Annotation [[mention|entity]] follows Wikipedia conventions.
Finally, the semantic relatedness measure of two
entities ei,ej is defined as the common in-links of ei
and ej in Wikipedia (Milne and Witten, 2008; Han
et al, 2011):
SR(ei, ej) = 1?
log(max(|A|, |B|))? log(|A ?B|)
log(|W |)? log(min(|A|, |B|))
(5)
where A and B are the set of in-links for entity ei
and ej respectively, andW is the set of all Wikipedia
pages.
Our method is a trade-off between exact collec-
tive inference and approximating related instance
with top ranked entities produced by g0. Most
collective approaches take all ambiguous mentions
into consideration and disambiguate them simulta-
neously, resulting in difficulty when inference in
large search space (Kulkarni et al, 2009; Hoffart
et al, 2011). Others resolve to some kinds of ap-
proximation. (Cucerzan, 2007) construct features as
the average of all candidates for one mention, in-
troducing considerable noise. (Ratinov et al, 2011)
also employ a two level architecture but only take
top 1 prediction for features. This most resembles
our approach, except we use stacking to tackle the
train/test mismatch problem, and construct different
set of features from top k candidates predicted by
g0. We will show in our experiments that this indeed
helps boost performance.
3.4 Learning category-context coherence
model cat
Entities in Wikipedia are annotated with rich se-
mantic structures. Category network provides us
with another valuable information for entity link-
ing. Take the mention ?Romney? as an exam-
ple, one candidate ?Mitt Romney? with category
?Republican party presidential nominee? co-occurs
frequently with context like ?election? and ?cam-
paign?, while another candidate ?Milton Romney?
with category ?Utah Utes football players? is fre-
quently observed with context like ?quarterback?
and ?backfield?. The category network forms a di-
rected acyclic graph (DAG). Some entities can share
category through the network, e.g. ?Barack Obama?
with category ?Democratic Party presidential nom-
inees? shares the category ?United States presiden-
tial candidates by party? with ?Mitt Romney? when
travelling two levels up the network.
(Bunescu and Pasca, 2006) propose to learn the
category-context correlation at word level through
category-word pair features. This method creates
sparsity problem and does not scale well because
the number of features grows linearly with both the
number of categories and the vocabulary size. More-
over, the category network is somewhat noisy, e.g.
travelling up four levels of the hierarchy can result
in over ten thousand categories, with many irrelevant
ones.
Rather than learning the correlation at word level,
we explore a representation learning method that
learns category-context correlation in the latent se-
mantic space. Supervised Semantic Indexing (SSI)
(Bai et al, 2009) is trained on query-document pairs
to predict their degree of matching. The compar-
ison is performed in the latent semantic space, so
that synonymy and polysemy are implicitly handled
by its inner mechanism. The score function between
query q and document d is defined as:
f(q, d) = qTWd (6)
430
where W is learned with supervision like click-
through data.
Given training data {(qi, di)}, training is done by
randomly sampling a negative target d?. The model
optimizes W such that f(q, d+) > f(q, d?). Thus,
the training objective is to minimize the following
margin-based loss function:
?
q,d+,d?
max(0, 1? f(q, d+) + f(q, d?)) (7)
which is also known as contrastive estimation
(Smith and Eisner, 2005).
W can become very large and inefficient when we
have a big vocabulary size. This is addressed by re-
placing W with its low rank approximation:
W = UTV + I (8)
here, the identity term I is a trade-off between the
latent space model and a vector space model. The
gradient step is performed with Stochastic Gradient
Descent (SGD):
U ?U + ?V (d+ ? d?)qT ,
if 1? f(q, d+) + f(q, d?) > 0 (9)
V ?V + ?Uq(d+ ? d?)T ,
if 1? f(q, d+) + f(q, d?) > 0. (10)
where ? is the learning rate.
The query and document are not necessary real
query and document. In our case, we treat our
problem as: given the occurring context of an en-
tity, retrieving categories corresponding to this en-
tity. Thus, we use context as query q and the cat-
egories of this candidate entity as d. We also treat
the definition page of an entity as its context, and
first train the model with definition pages, because
definition pages exhibit more focused topic. This
considerably accelerates the training process. To
reduce noise, We input the categories directly con-
nected with one entity as a word vector. The input
can be a TF-IDF vector or binary vector. We denote
model trained with normalized TF-IDF and with bi-
nary input as cat0 and cat1 respectively.
4 Experiments
4.1 Datasets
Previous researches have used diverse datasets for
evaluation, which makes it hard for comparison
with others? approaches. TAC-KBP has several
years of data for evaluating entity linking system,
but is not well suited for evaluating collective ap-
proaches. Recently, (Hoffart et al, 2011) anno-
tated a clean and much larger dataset AIDA 1 for
collective approaches evaluation based on CoNLL
2003 NER dataset. (Ratinov et al, 2011) also re-
fined previous work and contribute four publicly
available datasets 2. Thanks to their great works,
we have enough data to evaluate against. Accord-
ing to the setting of (Hoffart et al, 2011), we
split the AIDA dataset for train/development/test
with 946/216/231 documents. We train a separate
model on the Wikipedia training set for evaluating
ACE/QUAINT/WIKI dataset (Ratinov et al, 2011).
Table 2 gives a brief overview of the datasets used.
For knowledge base, we use the Wikipedia XML
dump 3 to extract over 3.3 million entities. We use
annotation from Wikipedia to build a name dictio-
nary from mention string m to entity e for can-
didate generation, including redirects, disambigua-
tion pages and hyperlinks, follows the approach of
(Cucerzan, 2007). For candidate generation, we
keep the top 30 candidates by popularity (Tbl. 1).
Note that our name dictionary is different from
(Ratinov et al, 2011) and has a much higher recall.
Since (Ratinov et al, 2011) evaluate on ?solvable?
mentions and we have no way to recover those men-
tions, we re-implement their global features and the
final scores are not directly comparable to theirs.
4.2 Methods under comparison
We compare our algorithm with several state-of-the-
art collective entity disambiguation systems. The
AIDA system proposed by (Hoffart et al, 2011) use
a greedy graph cutting algorithm that iteratively re-
move entities with low confidence scores. (Han et
al., 2011) employ personalized PageRank to prop-
agate evidence between different decisions. Both
algorithms use simple local features without dis-
criminative training. (Kulkarni et al, 2009) pro-
pose to use integer linear programming (ILP) for
inference. Except our re-implementation of Han?s
1available at http://www.mpi-inf.mpg.de/yago-naga/aida/
2http://cogcomp.cs.illinois.edu/Data, we don?t find the
MSNBC dataset in the zip file.
3available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
431
Dataset ndocs non-
NIL
identified solvable
AIDA dev 216 4791 4791 4707
AIDA test 231 4485 4485 4411
ACE 36 257 238 209(185)
AQUAINT 50 727 697 668(588)
Wikipedia 40 928 918 854(843)
Table 2: Number of mentions in each dataset. ?identi-
fied? means the mention exists in our name dictionary
and ?solvable? means the true entity are among the top 30
candidates by popularity. Number in parenthesis shows
the results of (Ratinov et al, 2011).
method, both AIDA and ILP solution are quite slow
at running time. The online demo of AIDA takes
over 10 sec to process one document with mod-
erate size, while the ILP solution takes around 2-
3 sec/doc. In contrast, our method takes only 0.3
sec/doc, and is easy to implement.
(Ratinov et al, 2011) also utilize a two layer
learner architecture. The difference is that their
method use top 1 candidate generated by local
learner for global feature generation , while we
search the top k candidates. Moreover, stacking is
used to tackle the train/test mismatch problem in
our model. We re-implement the global features of
(Ratinov et al, 2011) and use our local predictor
g0 for level 0 predictor. Note that we only imple-
ment their global features concerning common in-
links and inter-connection (totally 9 features) for fair
comparison because all other models don?t use com-
mon outgoing links for global coherence.
4.3 Settings
We implement SVM rank with an adaptation of lin-
ear SVM in scikit-learn (which is a wrapper of Li-
blinear). The category-context coherence model is
implemented with Numpy configured with Open-
Blas library, and we train this model on the entire
Wikipedia hyperlink annotation. It takes about 1.5d
for one pass over the entire dataset. The learning
rate ? is set to 1e-4 and training cost before update
is below 0.02.
Parameter tuning: there aren?t many parameters
to tune for both g0 and g1. The context document
window size is fixed as 100 for compatibility with
(Ratinov et al, 2011; Hoffart et al, 2011). The num-
ber of candidates is fixed to top 30 ranked by entity?s
popularity. Increase this value will generally boost
recall at the cost of lower precision.
We introduce the following default parameter for
global features in g1. The number of fold for stack-
ing is set to {1,5,10} (see Table 4, default is 10; 1
means no stacking, i.e. training g0 with all training
data and generating level 1 features for training data
directly with this g0). The number k for searching
neighboring entities with relational template is set
to {1,3,5,7} (e.g. in step 2 of Section 3.3 k = 5;
default is 5).
For category-context modeling, the vocabulary
sizes of context and category are set to top 10k and
6k unigrams by frequency. The latent dimension of
low rank approximation is set to 200.
Performance measures: For all non-NIL
queries, we evaluate performance with micro pre-
cision averaged over queries and macro precision
averaged over documents. Mean Reciprocal Rank
(MRR) is an information retrieval measure and is
defined as 1|Q|
?|Q|
i
1
ranki , where ranki is the rank
of correct answer in response to query i. For
ACE/AQUAINT/WIKI we also give the accuracy of
?solvable? mentions, but this is not directly compa-
rable to (Ratinov et al, 2011). Our name dictionary
is different from theirs and ours has a higher recall
rate (Tbl. 2). Hence, the ?solvable? set is different.
k recall k recall
1 78.56 6 96.31
2 89.59 7 97.04
3 93.01 8 97.37
4 94.97 9 97.62
5 95.78 10 97.81
Table 3: Top k recall for local predictor g0.
4.4 Discussions
Table 4 shows the evaluation results on AIDA
dataset and Table 5 shows results on datasets
ACE/AQUAINT/WIKI.
Effect of cat:The first group in Table 4 shows
some baseline features for comparison. We can see
even if the categories only carry incomplete and
noisy information about an entity, it performs much
432
Methods Devset Testset
micro
p@1
macro
p@1
MRR micro
p@1
macro
p@1
MRR
cosine 33.25 28.61 46.03 33.33 28.63 46.54
jaccard 44.71 36.56 57.76 45.66 36.89 57.08
cat0 54.75 47.14 67.70 61.52 54.72 72.55
cat1 60.15 54.64 72.98 65.46 61.04 76.84
popularity 69.21 67.59 79.26 69.07 72.63 79.45
g0 76.04 73.63 84.21 76.16 78.17 84.58
g0+global(Ratinov) 81.30 78.03 88.14 81.45 81.89 88.70
g1+1fold 82.01 78.52 88.90 83.59 83.58 90.05
g1+5fold 81.99 78.42 88.87 83.52 83.37 89.99
g1+10fold 82.01 78.53 88.91 83.59 83.55 90.03
g1+top1 81.65 78.76 88.51 81.81 82.55 89.06
g1+top3 82.20 78.64 88.98 83.52 83.34 89.94
g1+top5 82.01 78.57 88.90 83.63 83.76 90.05
g1+top7 82.05 78.40 88.90 83.75 83.58 90.08
g0+cat 79.36 76.14 86.66 79.64 80.47 87.32
g1+cat 82.24 78.49 89.02 84.88 84.49 90.65
g1+cat+all context 82.99 78.56 89.51 86.49 85.11 91.55
(Hoffart et al, 2011) - - - 82.29 82.02 -
(Shirakawa et al, 2011) - - - 81.40 83.57 -
(Kulkarni et al, 2009) - - - 72.87 76.74 -
(Han et al, 2011) - - - 78.97 75.77 -
Table 4: Performance on AIDA dataset. Maximal value in each group are highlighted with bold font. top k means up
to k candidates are used for searching related instances with relational template.
better than word level features. Group 5 in Table
4 shows cat information generally boosts perfor-
mance for both predictor g0 and g1.
Effect of stacking: Group 3 in Table 4 shows the
results with different fold in stacking training. 1 fold
means training g0 with all training data and directly
augment training data with this g0. Surprisingly, we
do not observe any substantial difference with vari-
ous fold size. We deduce it is possible the way we
fire global features with top k candidates that alle-
viates the problem of train/test mismatch when ex-
tending feature space for g1. Despite the ranking of
true entity can be lower in testset than in training
set, the semantic coherence information can still be
captured with searching over top k candidates.
Effect of top k global features: Group 4 in Table
4 shows the effect of k on g1 performance. Clearly,
increasing k generally improves precision and one
possible reason is the improvement in recall when
searching for related instances. Table 3 shows the
top k recall of local predictor g0. Further increasing
k does not show any improvement.
Our method benefits from such a searching strat-
egy, and consistently outperforms the global fea-
tures of (Ratinov et al, 2011). While their method
is a trade-off between expensive exact search over
all mentions and greedy assigning all mentions
with local predictor, we show this idea can be fur-
ther extended, somewhat like increasing the beam
search size without additional computational over-
head. The only exception is the ACE dataset, since
this dataset is so small, the difference translates to
only one mention. One may notice the improvement
on ACE/AQUAINT datasets is a little inconsistent.
These datasets are much smaller and the results only
differ within 4 mentions. Because these models are
433
Method micro
p@1
macro
p@1
MRR correct
/ solv-
able
ACE
g0 77.43 81.30 79.03 95.22
Ratinov 77.43 80.70 78.81 95.22
g1+5fold 77.04 79.85 78.96 94.74
g0+cat 77.82 81.48 79.31 95.69
g1+cat 77.43 80.16 79.25 95.22
AQUAINT
g0 84.46 84.69 87.49 91.92
Ratinov 85.14 85.29 87.90 92.66
g1+5fold 85.83 85.55 88.27 93.41
g0+cat 85.01 85.00 87.89 92.51
g1+cat 85.28 85.14 88.23 92.81
Wikipedia test
g0 83.19 84.30 86.63 90.40
Ratinov 84.48 85.96 87.62 91.80
g1+5fold 84.81 86.29 88.13 92.15
g0+cat 84.38 86.13 87.51 91.69
g1+cat 85.45 87.16 88.31 92.86
Table 5: Evaluation on ACE/AQUAINT/WIKI datasets.
trained on Wikipedia, the annotation style can be
quite different.
Finally, as we analyze the development set of
AIDA, we discover that some location entities rely
on more distant information across the context, as
we increase the context to the entire contextual doc-
ument, we can gain extra performance boost.
4.5 Error analysis
As we analyze the development set of AIDA, we find
some general problems with location names. Loca-
tion name generally is not part of the main topic
of one document. Thus, comparing context with
its definition is not realistic. Most of the time, we
can find some related location names in context; but
other times, it is not easily distinguished. For in-
stance, in ?France beats Turkey in men?s football...?
France refers to ?France national football team? but
our system links it to the country page ?France? be-
cause it is more popular. This can be addressed by
modeling finer context (Sen, 2012) or local syntac-
tic pattern (Hoffart et al, 2011). In other cases,
our system misclassifies ?New York City? for ?New
York? and ?Netherlands? for ?Holland? and ?Peo-
ple?s Republic of China? for ?China?, because in
all these cases, the latter ones are the most popu-
lar in Wikipedia. It is even hard for us humans to
tell the difference based only on context or global
coherence.
5 Conclusions
We propose a stacking based collective entity link-
ing method, which stacks a global predictor on top
of a local predictor to collect coherence information
from neighboring decisions. It is fast and easy to im-
plement. Our method trades off between inefficient
exact search and greedily assigning mention with lo-
cal predictor. It can be seen as searching related
entities with relational template in stacked graphi-
cal learning, with beam size k. Furthermore, we
adopt recent progress in representation learning to
learn category-context coherence model. It scales
better than existing approaches on large knowledge
base and performs comparison in the latent semantic
space. Combining these two techniques, our model
consistently outperforms all existing more sophisti-
cated collective approaches in our experiments.
Acknowledgments
This research was partly supported by Ma-
jor National Social Science Fund of China(No.
12&ZD227),National High Technology Research
and Development Program of China (863 Program)
(No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009).
References
B. Bai, J. Weston, D. Grangier, R. Collobert, O. Chapelle,
and K. Weinberger. 2009. Supervised semantic index-
ing. In The 18th ACM Conference on Information and
Knowledge Management (CIKM).
R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP-CoNLL, volume 6, pages 708?716.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
434
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, pages 765?774. ACM.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 782?792. Association for Computational Lin-
guistics.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the tac 2011
knowledge base population track. In Proceedings of
the Fourth Text Analysis Conference.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 133?142.
ACM.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hierar-
chical topic models. In Proceedings of KDD.
Zhenzhen Kou and William W Cohen. 2007. Stacked
graphical models for efficient inference in markov ran-
dom fields. In SDM.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
457?466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and Y. Shi.
2010. Lcc approaches to knowledge base population
at tac 2010. In Proc. TAC 2010 Workshop.
F. Li, Z. Zheng, F. Bu, Y. Tang, X. Zhu, and M. Huang.
2009. Thu quanta at tac 2009 kbp and rte track. In
Proceedings of Test Analysis Conference 2009 (TAC
09).
Andre? FT Martins, Dipanjan Das, Noah A Smith, and
Eric P Xing. 2008. Stacking dependency parsers. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 157?166. As-
sociation for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proceedings of the Annual Meeting of
the Association of Computational Linguistics (ACL).
P. Sen. 2012. Collective context-aware topic models
for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages
729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011.
Entity disambiguation based on a probabilistic
taxonomy. Technical report, Technical Report
MSR-TR-2011-125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 354?362. Asso-
ciation for Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In ACL, pages 1385?1394.
David H Wolpert. 1992. Stacked generalization. Neural
networks, 5(2):241?259.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan
Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
483?491, Los Angeles, California, June. Association
for Computational Linguistics.
435
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1055?1065,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Multi-domain Adaptation for SMT Using Multi-task Learning?
Lei Cui1, Xilun Chen2, Dongdong Zhang3, Shujie Liu3, Mu Li3, and Ming Zhou3
1Harbin Institute of Technology, Harbin, P.R. China
leicui@hit.edu.cn
2Cornell University, Ithaca, NY, U.S.
xlchen@cs.cornell.edu
3Microsoft Research Asia, Beijing, P.R. China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
Abstract
Domain adaptation for SMT usually adapts
models to an individual specific domain.
However, it often lacks some correlation
among different domains where common
knowledge could be shared to improve the
overall translation quality. In this paper, we
propose a novel multi-domain adaptation ap-
proach for SMT using Multi-Task Learning
(MTL), with in-domain models tailored for
each specific domain and a general-domain
model shared by different domains. The pa-
rameters of these models are tuned jointly via
MTL so that they can learn general knowledge
more accurately and exploit domain knowl-
edge better. Our experiments on a large-
scale English-to-Chinese translation task val-
idate that the MTL-based adaptation approach
significantly and consistently improves the
translation quality compared to a non-adapted
baseline. Furthermore, it also outperforms the
individual adaptation of each specific domain.
1 Introduction
Domain adaptation is an active topic in statisti-
cal machine learning and aims to alleviate the do-
main mismatch between training and testing data.
Like many machine learning tasks, Statistical Ma-
chine Translation (SMT) assumes that the data dis-
tributions of training and testing domains are sim-
ilar. However, this assumption does not hold for
real world SMT systems since training data for
SMT models may come from a variety of domains.
The translation quality is often unsatisfactory when
?This work was done while the first and second authors were
visiting Microsoft Research Asia.
translating texts from a specific domain using a gen-
eral model that is trained over a hotchpotch of bilin-
gual corpora. Therefore, domain adaptation is cru-
cial for SMT systems to achieve better performance.
Previous research on domain adaptation for SMT
includes data selection and weighting (Eck et al,
2004; Lu? et al, 2007; Foster et al, 2010; Moore and
Lewis, 2010; Axelrod et al, 2011), mixture mod-
els (Foster and Kuhn, 2007; Koehn and Schroeder,
2007; Sennrich, 2012; Razmara et al, 2012), and
semi-supervised transductive learning (Ueffing et
al., 2007), etc. Most of these methods adapt SMT
models to a specific domain according to testing data
and have achieved good performance. It is natural
that real world SMT systems should adapt the mod-
els to multiple domains because the input may be
heterogeneous, so that the overall translation qual-
ity can be improved. Although we can easily ap-
ply these methods to multiple domains individually,
it is difficult to use the common knowledge across
different domains. To leverage the common knowl-
edge, we need to devise a multi-domain adaptation
approach that jointly adapts the SMT models.
Multi-domain adaptation has been proved quite
effective in sentiment analysis (Dredze and Cram-
mer, 2008) and web ranking (Chapelle et al, 2011),
where the commonalities and differences across
multiple domains are explicitly addressed by Multi-
task Learning (MTL). MTL is an approach that
learns one target problem with other related prob-
lems at the same time, using a shared feature repre-
sentation. The key advantage of MTL is to enable
implicit data sharing and regularization. Therefore,
it often leads to a better model for each task. Anal-
ogously, we expect that the overall translation qual-
ity can be further improved by using an MTL-based
1055
TM1
S1 S2 S3
Multiple In-domain 
Translation Models
& Language Models
One General-domain 
Translation Model
& Language Model
T1
LM1
TM2
LM2
TM3
LM3
Domain-specific 
SMT Systems
In-domain
Training Data T2 T3
T
MTL-based Tuning
Entire
Training Data
TM-G
LM-G
TMN
LMN
TN
SN
...
...
...
...
Figure 1: An example with N pre-defined domains, where T is the entire training corpus. Ti is the in-domain training
data for the i-th domain selected from T using the bilingual cross-entropy based method (Axelrod et al, 2011). The
in-domain TMi and LMi are trained using the in-domain training data Ti. The general-domain models TM-G and LM-
G are trained using the entire training corpus T . Si is the domain-specific SMT system for the i-th domain, leveraging
the in-domain models and the general-domain models as features.
multi-domain adaptation approach.
In this paper, we use MTL to jointly adapt SMT
models to multiple domains. Specifically, we de-
velop multiple SMT systems based on mixture mod-
els, where each system is tailored for one specific
domain with an in-domain Translation Model (TM)
and an in-domain Language Model (LM). Mean-
while, all the systems share a same general-domain
TM and LM. These SMT systems are considered as
several related tasks with a shared feature represen-
tation, which fits well into a unified MTL frame-
work. With the MTL-based joint tuning, general
knowledge can be better learned by the general-
domain models, while domain knowledge can be
better exploited by the in-domain models as well.
By using a distributed stochastic learning approach
(Simianer et al, 2012), we can estimate the fea-
ture weights of multiple SMT systems at the same
time. Furthermore, we modify the algorithm to treat
in-domain and general-domain features separately,
which brings regularization to multiple SMT sys-
tems in an efficient way. Experimental results have
shown that our method can significantly improve the
translation quality on multiple domains over a non-
adapted baseline. Moreover, the MTL-based adap-
tation also outperforms the conventional individual
adaptation approach towards each domain.
The rest of the paper is organized as follows: The
proposed approach is explained in Section 2. Exper-
imental results are presented in Section 3. Section 4
introduces some related work. Section 5 concludes
the paper and suggests future research directions.
2 The Proposed Approach
Figure 1 gives an example with N pre-defined do-
mains to illustrate the main idea. There are three
steps in the training phase. First, in-domain train-
ing data is selected according to the pre-defined do-
mains (Section 2.1). Second, in-domain models and
general-domain models are trained to develop the
domain-specific SMT systems (Section 2.2). Third,
multiple domain-specific SMT systems are tuned
jointly by using an MTL-based approach (Section
2.3).
2.1 In-domain Data Selection
In the first step, in-domain bilingual data is selected
from all the bilingual data to train in-domain TMs.
We use the bilingual cross-entropy based approach
(Axelrod et al, 2011) to obtain the in-domain data:
[HI?src(s)?HG?src(s)]+[HI?tgt(t)?HG?tgt(t)] (1)
1056
where {s,t} is a bilingual sentence pair in the entire
bilingual corpus. HI?xxx(?) and HG?xxx(?) repre-
sent the cross-entropy of a string according to an in-
domain LM and a general-domain LM, respectively.
?xxx? denotes either the source language (src) or the
target language (tgt). HI?src(s)?HG?src(s) is the
cross-entropy difference of string s between the in-
domain and general-domain source-side LMs, and
HI?tgt(t) ? HG?tgt(t) is the cross-entropy differ-
ence of string t between the in-domain and general-
domain target-side LMs. This criterion biases to-
wards sentence pairs that are like the in-domain cor-
pus but unlike the general-domain corpus. There-
fore, the sentence pairs with lower scores (larger dif-
ferences) are presumed to be better.
Now, the question is how to find sufficient mono-
lingual data to train in-domain LMs. A straight-
forward solution is to collect the data from the in-
ternet. There are a large number of monolingual
webpages with domain information from web por-
tal sites1, which can be collected to train in-domain
LMs. In large-scale real world SMT systems, practi-
cal domain adaptation techniques should target more
domains rather than just one due to heterogeneous
input. Therefore, we use a web crawler to collect
monolingual webpages ofN domains from web por-
tal sites, for both the source language and the tar-
get language. The statistics of web-crawled data is
given in Section 3.1. We use the web-crawled mono-
lingual documents to train N in-domain source-side
LMs and N in-domain target-side LMs. Addition-
ally, we also train the source-side and target-side
general-domain LMs with all the web-crawled doc-
uments from different domains. Finally, these in-
domain and general-domain LMs are used to select
in-domain bilingual data for different domains ac-
cording to Formula (1).
2.2 SMT Systems with Mixture Models
In the second step, with the selected in-domain train-
ing data, we develop SMT systems based on mix-
ture models. In particular, we use the mixture model
based approach proposed by Koehn and Schroeder
1Many web portal sites contain domain information
for webpages, such as ?www.yahoo.com? in English and
?www.sina.com.cn? in Chinese and etc. The webpages are of-
ten categorized by human editors into different domains, such
as politics, sports, business, etc.
(2007). Specifically, we have developed N SMT
systems for N domains respectively, where each
system is a typical log-linear model. For each sys-
tem, the best translation candidate f? is given by:
f? = argmax
f
{P (f |e)} (2)
where the translation probability P (f |e) is given by:
P (f |e) ?
?
i
wi ? log ?i(f, e)
=
?
j?I
wj ? log ?j(f, e)
? ?? ?
In-domain
+
?
k?G
wk ? log ?k(f, e)
? ?? ?
General domain
(3)
where ?j(f, e) is the in-domain feature function and
wj is the corresponding feature weight. ?k(f, e) is
the general-domain feature function and wk is the
feature weight. The detailed feature description is
as follows:
In-domain features
? An in-domain TM, including phrase translation
probabilities and lexical weights for both direc-
tions (4 features)
? An in-domain target-side LM (1 feature)
? word count (1 feature)
? phrase count (1 feature)
? NULL penalty (1 feature)
? Number of hierarchical rules used (1 feature)
General-domain features
? A general-domain TM, including phrase trans-
lation probabilities and lexical weights for both
directions (4 features)
? A general-domain target-side LM (1 feature)
The feature description indicates that each SMT
system contains two TMs and two LMs. The in-
domain TMs are trained using the selected bilin-
gual training data according to Formula (1), and the
general-domain TM is trained using the entire bilin-
gual training data. For the LMs, we re-use the target-
side in-domain LMs and general-domain LM trained
1057
for data selection (Section 2.1). Compared with a
normal single-model system, the system with mix-
ture models can balance the contributions from the
general-domain and in-domain knowledge. Hence it
potentially benefits from both.
2.3 MTL-based Tuning
In the third step, the feature weights in multiple
domain-specific SMT systems are estimated. In-
stead of tuning each domain-specific system sepa-
rately, we treat different systems as related tasks and
tune them jointly in an MTL framework. There are
two main reasons for MTL-based tuning:
1. Domain-specific translation tasks share the
same general-domain LM and TM. MTL often
leads to better performance by leveraging com-
monalities among different tasks.
2. By enforcing that the general-domain LM and
TM perform equally across different domains,
MTL provides a kind of regularization to pre-
vent over-fitting.
Formally, the objective function of the proposed
MTL-based approach is described as follows:
min
W
{
N?
i=1
Loss(Ei, e?(Fi,wi))
}
(4)
where N is the number of pre-defined domains.
{Fi,Ei} is the in-domain development dataset for the
i-th domain. Fi denotes the source sentences and Ei
denotes the reference translations. wi is a D-length
feature weight column vector for the i-th domain,
where D is the dimension of the feature space. W is
a N -by-D matrix, representing [w1|w2| . . . |wN ]T .
e?(Fi,wi) are the best translations obtained for Fi
with parameters wi. Loss(?, ?) denotes the loss be-
tween the system?s output and the reference trans-
lations. The basic idea of the objective function is
to minimize the sum of loss functions for all the do-
mains, rather than one domain at a time. Therefore,
by adjusting the in-domain and general-domain fea-
ture weights, the translation quality is expected to be
good across different domains.
To effectively tune SMT systems jointly, we mod-
ify the asynchronous Stochastic Gradient Descend
(SGD) Algorithm (Simianer et al, 2012) to optimize
objective function (4). We follow the pairwise rank-
ing approach with the perceptron algorithm (Shen
and Joshi, 2005) to update feature weights. Let a
translation candidate be denoted by its feature vector
v ? RD, the pairwise preference for training is con-
structed by ranking two candidates according to the
smoothed sentence-level BLEU (Liang et al, 2006).
For a preference pair v[j]=(v(1), v(2)) where v(1) is
preferred, a hinge loss is used:
L(wi) = (??wi, v(1) ? v(2)?)+ (5)
where (x)+ = max(0, x) and ??, ?? denotes the in-
ner product of two vectors. With the perceptron al-
gorithm (Shen and Joshi, 2005), the gradient of the
hinge loss is:
?L(wi) =
{
v(2) ? v(1) if?wi, v(1) ? v(2)? ? 0
0 otherwise
(6)
The training instances for the discriminative
learning in pairwise ranking are made by comparing
the N-best list of the translation candidates scored
by the smoothed sentence-level BLEU (Liang et al,
2006). Following Simianer et al (2012), the N-best
list is divided into three bins: the top 10% (High),
the middle 80% (Middle), and the last 10% (Low).
These bins are used for pairwise ranking where the
translation preference pairs are built between the
candidates in High-Middle, Middle-Low, and High-
Low, but not the candidates within the same bin,
which is shown in Figure 2. The idea is to guar-
antee that the ranker is more discriminative to prefer
the good translations to the bad ones.
High: 10%
Middle: 80%
Low: 10%
N-best list
Figure 2: Training instances for pairwise ranking.
1058
Algorithm 1 Modified Asynchronous SGD
1: Distribute N domain-specific decoders to N ma-
chines
2: Initialize w1,w2, . . . ,wN ? 0
3: for epochs t? 0 . . . T ? 1 do
4: for all domains d ? {1 . . . N}: parallel do
5: ud,t,0,0 = wd
6: S = |Fd|
7: for all i ? {0 . . . S ? 1} do
8: Decode i-th sentence with ud,t,i,0
9: P = No. of pairs built from the N-best list
10: for all pairs v[j], j ? {0 . . . P ? 1} do
11: ud,t,i,j+1 ? ud,t,i,j ? ??L(ud,t,i,j)
12: end for
13: ud,t,i+1,0 ? ud,t,i,P
14: end for
15: end for
16: for all domains d ? {1 . . . N} do
17: wd = ud,t,S,0
18: end for
19: WG ? [wG1 | . . . |w
G
N ]
T
20: for all domains d ? {1 . . . N} do
21: for k ? 1 . . . |wGd | do
22: wGd [k] =
1
N
?N
n=1 W
G[n][k]
23: end for
24: wd ?
[wId
wGd
]
25: end for
26: end for
27: return w1,w2, . . . ,wN
Our modified algorithm is illustrated in Algorithm
1. Each column vector wi is further split into two
parts wIi and w
G
i , representing the In-domain and
General-domain feature weights respectively. In Al-
gorithm 1, we first distribute the domain-specific
SMT decoders to different machines and initialize
the feature weights (line 1-2). Typically, the SGD al-
gorithm runs in several iterations (In this study, we
set the number of epochs T to 20) (line 3). Multi-
ple SMT decoders run in parallel and each decoder
updates its feature weights individually using its in-
domain development data (line 4-15). For each do-
main, the domain-specific decoder translates each
in-domain development sentence and determines the
N-best translations (line 4-8). The preference pairs
are built and used to update the parameters by gra-
dient descent with ? = 0.0001 (line 9-13). Each
domain-specific decoder translates its in-domain de-
velopment data multiple times. After each itera-
tion, feature weights from all decoders are collected
(line 16-19). In contrast to the original algorithm
(Simianer et al, 2012), we only average the general-
domain feature weights wG1 , . . . ,w
G
N , but do not av-
erage the in-domain feature weights (line 20-25).
The reason is we hope to leverage the commonalities
among these systems. Meanwhile, general knowl-
edge is enforced to be conveyed equally across dif-
ferent domains. Finally, the algorithm returns all
the domain-specific feature weights w1,w2, . . . ,wN
that are used for testing (line 27).
After the joint MTL-based tuning, the feature
weights tailored for domain-specific SMT systems
are used to translate the testing data. We collect in-
domain testing data for each domain to evaluate the
domain-specific systems. Although this is not al-
ways the case in real applications where the testing
domain is known, this study mainly focuses on the
effectiveness of the MTL-based tuning approach.
3 Experiments
3.1 Data
We evaluated our MTL-based domain adaptation
approach on a large-scale English-to-Chinese ma-
chine translation task. The training data consisted
of two parts: monolingual data and bilingual data.
The monolingual data was used to train the source-
side and target-side LMs, both of which were used
for data selection in Section 2.1. In addition, the
target-side LMs were re-used in the SMT systems
as features. As mentioned in Section 2.1, we built a
web crawler to collect a large number of webpages
from web portal sites in English and Chinese respec-
tively. In the experiments, we mainly focused on six
popular domains, namely Business, Entertainment,
Health, Science & Technology, Sports, and Politics.
For both English and Chinese webpages, the HTML
tags were removed and the main content was ex-
tracted. The data statistics are shown in Table 1.
The bilingual data we used was mainly mined
from the web using the method proposed by Jiang
et al (2009), with a post-processing step using our
bilingual data cleaning method (Cui et al, 2013).
Therefore, the data quality is pretty good. In addi-
tion, we also used the English-Chinese parallel cor-
pus released by LDC2. In total, the bilingual data
2LDC2003E07, LDC2003E14, LDC2004E12,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,
1059
Domain
English Chinese
Docs Words Docs Words
Business 21M 10.4B 7.91M 2.73B
Ent. 18.3M 8.29B 4.16M 1.31B
Health 8.7M 4.73B 0.9M 0.42B
Sci&Tech 10.9M 5.33B 5.28M 1.6B
Sports 18.9M 9.58B 2.49M 0.59B
Politics 10.3M 5.56B 1.67M 0.39B
Table 1: Statistics of web-crawled monolingual data, in
numbers of documents and words (main content). ?M?
refers to million and ?B? refers to billion.
contained around 30 million sentence pairs, with
404M words in English and 329M words in Chi-
nese. For each domain, we used the cross-entropy
based method in Section 2.1 to rank the entire bilin-
gual data, and the top 10% sentence pairs from the
ranked bilingual data were selected as the in-domain
data to train the in-domain TM. Moreover, we pre-
pared 2,000 in-domain sentences for development
and 1,000 in-domain sentences for testing in each
domain. The details are shown in Table 2.
Domain
Train Dev Test
En Ch En Ch En Ch
Business 30M 28M 36K 35K 19K 19K
Ent. 25M 22M 21K 18K 13K 12K
Health 23M 20M 33K 33K 21K 22K
Sci&Tech 28M 26M 46K 45K 27K 27K
Sports 19M 16M 18K 14K 10K 9K
Politics 28M 24M 19K 17K 13K 12K
Table 2: Statistics of in-domain training, development
and testing data, in number of words.
3.2 Setup
An in-house hierarchical phrase-based SMT de-
coder was implemented for our experiments. The
CKY decoding algorithm was used and cube prun-
ing was performed with the same default parameter
settings as in Chiang (2007). We used a 100-best
list from the decoder for the pairwise ranking al-
gorithm. Translation models were trained over the
bilingual data that was automatically word-aligned
using GIZA++ (Och and Ney, 2003) in both direc-
tions, and the diag-grow-final heuristic was used to
LDC2006E34, LDC2006E85, LDC2006E92.
refine the symmetric word alignment. The phrase
tables were filtered to retain top-20 translation can-
didates for each source phrase for efficiency. An
in-house language modeling toolkit was used to
train the 4-gram language models with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) over
the web-crawled data. The evaluation metric for
the overall translation quality was case-insensitive
BLEU4 (Papineni et al, 2002). A statistical sig-
nificance test was performed using the bootstrap re-
sampling method (Koehn, 2004).
3.3 Baseline
We have two baselines. The first baseline is a non-
adapted Hiero using our implementation. It con-
tained the general-domain TM and LM, as well as
other standard features. In addition, the fix-discount
method (Foster et al, 2006) for phrase table smooth-
ing was also used. The system was general-domain
oriented and it was tuned by using MERT (Och,
2003) with a combination of six in-domain develop-
ment datasets. The second baseline is Google Online
Translation Service3. We obtained the English-to-
Chinese translations of the testing data from Google
Translation to have a more solid comparison.
Moreover, we also compared our method with the
adapted systems towards each domain individually
(Koehn and Schroeder, 2007). This is to demon-
strate the superiority of our MTL-based tuning ap-
proach across different domains.
3.4 Results
The end-to-end translation performance is shown in
Table 3. We found that the baseline has a similar
performance to Google Translation, with certain do-
mains performed even better (Business, Sci&Tech,
Sports, Politics). This demonstrates that the transla-
tion quality of our baseline is state-of-the-art. More-
over, we can answer three questions according to the
experimental results as follow:
First, is domain mismatch a significant prob-
lem for a real world SMT system? We used the
same system only with general-domain TM and LM,
but tuned towards each domain individually using
in-domain dev data. Table 3 shows that the setting
?[A] G-TM + G-LM? performs much better than
3http://translate.google.com
1060
Business Ent. Health Sci&Tech Sports Politics
[N] Baseline (G-TM + G-LM) 27.19 17.87 25.79 25.34 25.53 23.01
Google Translation 26.01 18.44 27.71 25.07 24.08 22.97
[A] G-TM + G-LM 29.58 19.08 28.80 26.84 30.28 25.64
[A] I-TM + I-LM 28.20 17.25 27.20 25.41 30.12 22.97
[A] (G+I)-TM + G-LM 29.45 19.22 28.93 27.01 31.01 25.40
[A] (G+I)-TM + I-LM 29.60 19.43 28.94 27.05 34.36 25.98
[A] (G+I)-LM + G-TM 29.66 19.50 29.00 27.10 33.60 26.03
[A] (G+I)-LM + I-TM 28.50 17.66 27.58 25.99 30.44 23.30
[A] (G+I)-TM + (G+I)-LM 29.82 19.53 29.03 26.94 33.77 26.09
[A,MTL] (G+I)-TM + (G+I)-LM 30.26 19.94 29.08 27.17 34.11 26.50
Table 3: End-to-end experimental results (BLEU4%) with large-scale training data (p < 0.05). ?[N]? means the system
is non-adapted and tuned using MERT on general-domain dev data. ?[A]? denotes that the system is adapted towards
each domain individually using MERT on in-domain dev data. ?[A,MTL]? indicates that the system was tuned using
our MTL-based approach on in-domain dev data. ?I-TM? and ?G-TM? denote the in-domain and general-domain
translation model. ?I-LM? and ?G-LM? denote the in-domain and general-domain language model. We also obtained
translations of the testing data using Google Translation for comparison.
the non-adapted baseline across all domains with at
least 1.2 BLEU points. In addition, the setting ?[A]
G-TM + G-LM? also outperforms Google Transla-
tion on all domains. Analogous to previous research,
this confirms that the domain mismatch indeed ex-
ists and the parameter estimation using in-domain
dev data is quite useful.
Second, does the mixture models based adap-
tation work for a variety of domains? We experi-
mented with different settings with multiple TMs or
LMs, or both. It is interesting to note that for large-
scale SMT systems, using in-domain models alone
is inferior to using the general models alone. The
setting ?[A] G-TM + G-LM? is better than the set-
ting ?[A] I-TM + I-LM? across different domains.
The reason is the data for general models has already
included the in-domain data and the data coverage is
much larger, thus the probability estimation is more
reliable and the translation quality is much better.
For the LM, the in-domain LM performs better
than the general-domain LM because our mono-
lingual data (Table 1) for each domain is already
sufficient for training an in-domain LM with good
performance. From Table 3, we observed that the
setting ?[A] (G+I)-TM + I-LM? outperforms ?[A]
(G+I)-TM + G-LM?, with the ?Sports? domain be-
ing the most significant. For the TM, the per-
formance of the in-domain TM is inferior to the
general-domain TM. The results show that the set-
ting ?[A] (G+I)-LM + G-TM? is significantly better
than ?[A] (G+I)-LM + I-TM?. The main reason is
the data coverage for in-domain TM is much smaller
than the general model. When each system uses two
TMs and two LMs, it consistently results in better
performance, indicating that mixture models are cru-
cial for domain adaptation in SMT.
Third, can MTL further improve the transla-
tion quality? We used the MTL-based approach to
jointly tune multiple domain-specific systems, lever-
aging the commonalities among different but related
tasks. From Table 3, the MTL-based approach sig-
nificantly improve the translation quality over the
non-adapted baseline, and also outperforms conven-
tional mixture models based methods. In particular,
the ?Sports? domain benefits the most from the in-
domain knowledge, which confirms that domain dis-
crepancy should be addressed and may bring large
improvements on certain domains.
3.5 Discussion
According to our experiments, only averaging over
the out-of-domain feature weights returned robust
and converged results. We do not have theoreti-
cally grounded guarantee. However, we observed
that the BLEU score of our method on DEV data
was slightly lower than that in the baseline system,
which indicates the out-of-domain features are less
over-fitting on the domain-specific DEV data since
1061
SOURSE A point begins with a player serving the ball. This means one player hits
the ball towards the other player. (The serve must be played from behind the
baseline and must
::::
land in the service box . Players get two attempts to make
a good serve.)
REF ? ? ? ? ???? ? ? ? ? ? ? ? ? ???? ? ? ????
??(??????????????????
::::
????? ??? ??
????????????)
[N] Baseline (G-TM + G-LM) ????????????????????????????
(???????????????
:::::::
??? ??? ????????
????????)
[A] (G+I)-TM + (G+I)-LM ???????????????? ????????(???
???????? ??? ?
:::::
????????????????
????)
[A,MTL](G+I)-TM + (G+I)-LM ????????????? ???????????(????
??????????
:::::::
??? ??? ????????????
????)
Table 4: Examples illustrating some different translations, where the Chinese phrases are translated from the English
phrases with the same symbols (e.g., underline, wavy-line, and box). The details are explained in Section 3.5.
we enforced them to play the same role across dif-
ferent domains. It seems that averaging the out-of-
domain feature weights can be considered as a kind
of regularization.
An example sentence from the Sports domain
with translations from different methods is shown
in Table 4. In this sentence, the baseline always
translates ?player? to ???? (game player), which
should be ???? (ball player). And, the base-
line translates ?serve? to ???? (work for), which
should be ???? (put the ball into play). The phrase
?service box? here means ?????, which denotes
the zone where the ball is to be served. However, the
baseline incorrectly splits them into two words, then
translates ?service? to ???? and ?box? to ???.
In contrast, the approaches with adapted models are
able to translate these words very well.
Both our MTL-based approach and the conven-
tional adaptation methods leverage the mixture mod-
els. A natural question is why our MTL-based ap-
proach performs better than the individual adapta-
tion. To answer this question, we looked into the
details of the tuning and decoding procedures in the
MTL-based approach. We observed that the BLEU
score on the development data for each system was
lower than the score when conducting individual
adaptation. Considering that the algorithm enforc-
ing the general features play the same role across
different domains, we suspect that MTL-based ap-
proach introduces a kind of regularization for each
domain-specific system. The regularization prevents
the general features from biasing towards certain do-
mains to the extreme. This property is quite impor-
tant for real world SMT systems. Usually, a sen-
tence is composed of some domain-specific words
and some general words, so it is often improper to
translate every word in the sentence using the in-
domain knowledge. For the example in Table 4,
the individual adaptation method ?[A] (G+I)-TM +
(G+I)-LM? translates ?land? to ???? (zone) im-
properly, because ???? appears more often in the
Sports text than the general-domain text. This shows
that the individual adaptation methods tend to over-
fit the in-domain development data. In contrast, the
MTL-based approach ?[A,MTL](G+I)-TM + (G+I)-
LM? just translates ?land? to ????? (fall on),
which is more appropriate.
4 Related Work
4.1 Domain Adaptation
One direction of domain adaptation explored the
data selection and weighting approach to improve
the performance of SMT on specific domains. Eck
1062
et al (2004) first decoded the testing data with a
general TM, and then used the translation results
to train an adapted LM, which was in turn used to
re-decode the testing data. Lu? et al (2007) tried
to weight the training data according to the similar-
ity with test data using information retrieval mod-
els, while Foster et al (2010) trained a discrimina-
tive model to estimate a weight for each sentence
in the training corpus. Other methods conducted
data selection based on cross-entropy (Moore and
Lewis, 2010), and Axelrod et al (2011) further ex-
tended their cross-entropy based method to the se-
lection of bilingual corpus in the hope that more rel-
evant corpus to the target domain could yield smaller
models with better performance. Other methods
included using semi-supervised transductive learn-
ing techniques to exploit the monolingual in-domain
data (Ueffing et al, 2007).
Adaptation methods also involved the utiliza-
tion of mixture models. Foster and Kuhn (2007)
explored a number of variants of utilizing multi-
ple TMs and LMs by interpolation. Koehn and
Schroeder (2007) used MERT to simultaneously
tune two TMs or LMs. Sennrich (2012) investi-
gated the TM perplexity minimization as a method
to set model weights in mixture modeling. In ad-
dition, inspired by system combination approaches,
Razmara et al (2012) used the ensemble decoding
method to mix multiple translation models, which
outperformed a variety of strong baselines.
Generally, most previous methods merely con-
ducted domain adaption for a single domain, rather
than multiple domains at the same time. One could
also simply build multiple SMT systems that were
adapted to multiple domains, but they were often
separated and not tuned together. So far, there has
been little research into the multi-domain adaptation
problem over mixture models for SMT systems, as
proposed in this paper.
4.2 Multi-task Learning
In machine learning, MTL is an approach to learn
one target problem with other related problems at
the same time. This often leads to a better model for
the main task because it allows the learner to use the
commonality among the tasks. MTL is performed
by learning tasks in parallel while using a shared
representation. Therefore, what is learned for each
task can help other tasks be learned better.
MTL was successfully applied in some Natu-
ral Language Processing (NLP) tasks. For exam-
ple, Blitzer et al (2006) extended the MTL ap-
proach (Ando and Zhang, 2005) to domain adapta-
tion tasks in part-of-speech tagging. Collobert and
Weston (2008) proposed using deep neural networks
to train a set of tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic roles labeling. They reported that jointly
learning these tasks led to superior performance.
MTL was also applied in sentiment analysis (Dredze
and Crammer, 2008) and web ranking (Chapelle
et al, 2011) to address the multi-domain learning
and adaptation. In SMT, Duh et al (2010) pro-
posed using MTL for N-best re-ranking on sparse
feature sets, where each N-best list corresponded to
a distinct task. Simianer et al (2012) proposed dis-
tributed stochastic learning with feature selection in-
spired by MTL. The distributed learning approach
outperformed several other training methods includ-
ing MIRA and SGD.
Inspired by these methods, we used MTL to tune
multiple SMT systems at the same time, where each
system was composed of in-domain and general-
domain models. Through a shared feature represen-
tation, the commonalities among the SMT systems
were better learned by the general models. In ad-
dition, domain-specific translation knowledge was
also better characterized by the in-domain models.
5 Conclusion and Future Work
In this paper, we propose an MTL-based approach to
address multi-domain adaptation for SMT. We first
use the cross-entropy based data selection method
to obtain in-domain bilingual data. After that, in-
domain TMs and LMs are trained for each domain-
specific SMT system. In addition, the general-
domain TM and LM are also trained and shared
across different systems. Finally, MTL is lever-
aged to tune multiple systems jointly. Experimen-
tal results have shown that our approach is quite
promising for the multi-domain adaptation problem,
and it brings significant improvement over both the
non-adapted baselines and the conventional domain
adaptation methods with mixture models.
We assume the domain information for testing
1063
data is known beforehand in this study. However,
this is not always the case for real world SMT sys-
tems. Therefore, to apply our approach in real appli-
cations, the domain information needs to be identi-
fied automatically. In the future, we will pre-define
more popular domains and develop automatic do-
main classifiers. For those domains that are iden-
tified with high confidence, we use the domain-
specific system to translate the texts. For other texts,
we use the general system to translate them. Fur-
thermore, since our approach is a general training
method, we may also combine this approach with
other domain adaptation methods to get more per-
formance improvement.
Acknowledgments
We are especially grateful to Nan Yang, Yajuan
Duan, Hong Sun and Danran Chen for the helpful
discussions. We also thank the anonymous review-
ers for their insightful comments.
References
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. The Journal of Machine Learning
Research, 6:1817?1853.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 355?362, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2011. Boosted multi-task learning. Machine
learning, 85(1-2):149?173.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference onMachine learn-
ing, pages 160?167. ACM.
Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and Ming
Zhou. 2013. Bilingual data cleaning for smt using
graph-based random walk. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 340?345,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 689?697, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best reranking
by multitask learning. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
ricsMATR, pages 375?383, Uppsala, Sweden, July.
Association for Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Proc.
of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53?61, Sydney, Australia, July. Association for
Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and
Qingsheng Zhu. 2009. Mining bilingual data from the
web with adaptively learnt patterns. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
870?878, Suntec, Singapore, August. Association for
Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Acous-
tics, Speech, and Signal Processing, 1995. ICASSP-
95., 1995 International Conference on, volume 1,
pages 181?184. IEEE.
1064
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 940?949, Jeju Island, Korea, July. Association
for Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 539?549, Avignon, France,
April. Association for Computational Linguistics.
Libin Shen and Aravind K Joshi. 2005. Ranking and
reranking with perceptron. Machine Learning, 60(1-
3):73?96.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11?21, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25?32, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
1065
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 316?324,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Discriminative Pruning for Discriminative ITG Alignment 
Shujie Liu?, Chi-Ho Li? and Ming Zhou? 
?School of Computer Science and Technology 
Harbin Institute of Technology, Harbin, China 
shujieliu@mtlab.hit.edu.cn 
?Microsoft Research Asia, Beijing, China 
{chl, mingzhou}@microsoft.com 
  
 
Abstract 
While Inversion Transduction Grammar (ITG) 
has regained more and more attention in recent 
years, it still suffers from the major obstacle of 
speed. We propose a discriminative ITG prun-
ing framework using Minimum Error Rate 
Training and various features from previous 
work on ITG alignment. Experiment results 
show that it is superior to all existing heuristics 
in ITG pruning. On top of the pruning frame-
work, we also propose a discriminative ITG 
alignment model using hierarchical phrase 
pairs, which improves both F-score and Bleu 
score over the baseline alignment system of 
GIZA++. 
1 Introduction 
Inversion transduction grammar (ITG) (Wu, 1997) 
is an adaptation of SCFG to bilingual parsing. It 
does synchronous parsing of two languages with 
phrasal and word-level alignment as by-product. 
For this reason ITG has gained more and more 
attention recently in the word alignment commu-
nity (Zhang and Gildea, 2005; Cherry and Lin, 
2006; Haghighi et al, 2009). 
A major obstacle in ITG alignment is speed. 
The original (unsupervised) ITG algorithm has 
complexity of O(n6). When extended to super-
vised/discriminative framework, ITG runs even 
more slowly. Therefore all attempts to ITG 
alignment come with some pruning method. For 
example, Haghighi et al (2009) do pruning based 
on the probabilities of links from a simpler 
alignment model (viz. HMM); Zhang and Gildea 
(2005) propose Tic-tac-toe pruning, which is 
based on the Model 1 probabilities of word pairs 
inside and outside a pair of spans. 
As all the principles behind these techniques 
have certain contribution in making good pruning 
decision, it is tempting to incorporate all these 
features in ITG pruning. In this paper, we pro-
pose a novel discriminative pruning framework 
for discriminative ITG. The pruning model uses 
no more training data than the discriminative ITG 
parser itself, and it uses a log-linear model to in-
tegrate all features that help identify the correct 
span pair (like Model 1 probability and HMM 
posterior). On top of the discriminative pruning 
method, we also propose a discriminative ITG 
alignment system using hierarchical phrase pairs.  
In the following, some basic details on the ITG 
formalism and ITG parsing are first reviewed 
(Sections 2 and 3), followed by the definition of 
pruning in ITG (Section 4). The ?Discriminative 
Pruning for Discriminative ITG? model (DPDI) 
and our discriminative ITG (DITG) parsers will 
be elaborated in Sections 5 and 6 respectively. 
The merits of DPDI and DITG are illustrated 
with the experiments described in Section 7.  
2 Basics of ITG 
The simplest formulation of ITG contains three 
types of rules: terminal unary rules ? ? ?/? , 
where ?  and ?  represent words (possibly a null 
word, ?) in the English and foreign language 
respectively, and the binary rules ? ?  ?,?  and 
? ?  ?,? , which refer to that the component 
English and foreign phrases are combined in the 
same and inverted order respectively. 
From the viewpoint of word alignment, the 
terminal unary rules provide the links of word 
pairs, whereas the binary rules represent the reor-
dering factor. One of the merits of ITG is that it 
is less biased towards short-distance reordering. 
Such a formulation has two drawbacks. First of 
all, it imposes a 1-to-1 constraint in word align-
ment. That is, a word is not allowed to align to 
more than one word. This is a strong limitation as 
no idiom or multi-word expression is allowed to 
align to a single word on the other side. In fact 
there have been various attempts in relaxing the 
1-to-1 constraint. Both ITG alignment 
316
approaches with and without this constraint will 
be elaborated in Section 6.  
Secondly, the simple ITG leads to redundancy 
if word alignment is the sole purpose of applying 
ITG. For instance, there are two parses for three 
consecutive word pairs, viz. [?/?? [?/?? ?/
??] ]  and [[?/?? ?/??] ?/??] . The problem of re-
dundancy is fixed by adopting ITG normal form. 
In fact, normal form is the very first key to speed-
ing up ITG. The ITG normal form grammar as 
used in this paper is described in Appendix A. 
3 Basics of ITG Parsing 
Based on the rules in normal form, ITG word 
alignment is done in a similar way to chart pars-
ing (Wu, 1997). The base step applies all relevant 
terminal unary rules to establish the links of word 
pairs. The word pairs are then combined into 
span pairs in all possible ways. Larger and larger 
span pairs are recursively built until the sentence 
pair is built.  
Figure 1(a) shows one possible derivation for a 
toy example sentence pair with three words in 
each sentence. Each node (rectangle) represents a 
pair, marked with certain phrase category, of for-
eign span (F-span) and English span (E-span) 
(the upper half of the rectangle) and the asso-
ciated alignment hypothesis (the lower half). 
Each graph like Figure 1(a) shows only one deri-
vation and also only one alignment hypothesis.  
The various derivations in ITG parsing can be 
compactly represented in hypergraph (Klein and 
Manning, 2001) like Figure 1(b). Each hypernode 
(rectangle) comprises both a span pair (upper half) 
and the list of possible alignment hypotheses 
(lower half) for that span pair. The hyperedges 
show how larger span pairs are derived from 
smaller span pairs. Note that a hypernode may 
have more than one alignment hypothesis, since a 
hypernode may be derived through more than one 
hyperedge (e.g. the topmost hypernode in Figure 
1(b)). Due to the use of normal form, the hypo-
theses of a span pair are different from each other.  
4 Pruning in ITG Parsing 
The ITG parsing framework has three levels of 
pruning: 
1) To discard some unpromising span pairs; 
2) To discard some unpromising F-spans 
and/or E-spans; 
3) To discard some unpromising alignment 
hypotheses for a particular span pair. 
The second type of pruning (used in Zhang et. 
al. (2008)) is very radical as it implies discarding 
too many span pairs. It is empirically found to be 
highly harmful to alignment performance and 
therefore not adopted in this paper.  
The third type of pruning is equivalent to mi-
nimizing the beam size of alignment hypotheses 
in each hypernode. It is found to be well handled 
by the K-Best parsing method in Huang and 
Chiang (2005). That is, during the bottom-up 
construction of the span pair repertoire, each span 
pair keeps only the best alignment hypothesis. 
Once the complete parse tree is built, the k-best 
list of the topmost span is obtained by minimally 
expanding the list of alignment hypotheses of 
minimal number of span pairs. 
The first type of pruning is equivalent to mi-
nimizing the number of hypernodes in a hyper-
graph. The task of ITG pruning is defined in this 
paper as the first type of pruning; i.e. the search 
for, given an F-span, the minimal number of E-
spans which are the most likely counterpart of 
that F-span.1 The pruning method should main-
tain a balance between efficiency (run as quickly 
as possible) and performance (keep as many cor-
rect span pairs as possible).  
                                                 
1 Alternatively it can be defined as the search of the minimal 
number of E-spans per F-span. That is simply an arbitrary 
decision on how the data are organized in the ITG parser.  
B:[e1,e2]/[f1,f2]
{e1/f2,e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3}
(a) 
C:[e2,e2]/[f2,f2]
{e2/f2}
C:[e1,e1]/[f1,f1]
{e1/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
B:[e1,e2]/[f1,f2]
{e1/f2}
A:[e1,e2]/[f1,f2]
{e2/f2}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3} , 
{e1/f1,e2/f2,e3,f3}
(b)
B?<C,C> A?[C,C]
A?[A,C]A?[B,C]
 
Figure 1:  Example ITG parses in graph (a) and hypergraph (b). 
317
A na?ve approach is that the required pruning 
method outputs a score given a span pair. This 
score is used to rank all E-spans for a particular 
F-span, and the score of the correct E-span 
should be in general higher than most of the in-
correct ones.  
5 The DPDI Framework 
DPDI, the discriminative pruning model pro-
posed in this paper, assigns score to a span pair 
 ? , ?  as probability from a log-linear model: 
? ? ?  =
???(  ???? ? , ? ? )
 ???(  ????(? , ? ?))?? ???  
 (1) 
where each ??(? ,? )  is some feature about the 
span pair, and each ? is the weight of the corres-
ponding feature. There are three major questions 
to this model:  
1) How to acquire training samples? (Section 
5.1) 
2) How to train the parameters ? ? (Section 5.2) 
3) What are the features? (Section 5.3) 
5.1 Training Samples 
Discriminative approaches to word alignment use 
manually annotated alignment for sentence pairs. 
Discriminative pruning, however, handles not 
only a sentence pair but every possible span pair. 
The required training samples consist of various 
F-spans and their corresponding E-spans. 
Rather than recruiting annotators for marking 
span pairs, we modify the parsing algorithm in 
Section 3 so as to produce span pair annotation 
out of sentence-level annotation. In the base step, 
only the word pairs listed in sentence-level anno-
tation are inserted in the hypergraph, and the re-
cursive steps are just the same as usual.  
If the sentence-level annotation satisfies the 
alignment constraints of ITG, then each F-span 
will have only one E-span in the parse tree. How-
ever, in reality there are often the cases where a 
foreign word aligns to more than one English 
word. In such cases the F-span covering that for-
eign word has more than one corresponding E-
spans. Consider the example in Figure 2, where 
the golden links in the alignment annotation are 
?1/?1, ?2/?1, and ?3/?2; i.e. the foreign word 
?1 aligns to both the English words ?1 and ?2. 
Therefore the F-span  ?1,?1  aligns to the E-
span  ?1, ?1  in one hypernode and to the E-span 
 ?2, ?2  in another hypernode. When such situa-
tion happens, we calculate the product of the in-
side and outside probability of each alignment 
hypothesis of the span pair, based on the proba-
bilities of the links from some simpler alignment 
model2. The E-span with the most probable hypo-
thesis is selected as the alignment of the F-span.  
A?[C,C]
C
w
:
[e1,e1]/[f1,f1]
{e1/f1}
C
e
:
[e1]/?
C
w
:
[e2,e2]/[f1,f1]
C
e
:
[e2]/?
C
w
:
[e3,e3]/[f2,f2]
C:
[e1,e2]/[f1,f1]
{e2/f1}
C:
[e2,e3]/[f2,f2]
{e3/f2}
A:
[e1,e3]/[f1,f2]
{e1/f1,e3/f2},{e2/f1,e3/f2}
C? [C
e
,C
w
]
A?[C,C]
C? [C
e
,C
w
]
{e1/f1} {e1/f1}
(a) (b)
[f1,f1]
[e1,e1]
[e1,e2]
[e2,e2]
[f2,f2]
[e2,e3]
[e3,e3]
[f1,f2] [e1,e3]
Figure 2: Training sample collection. 
Table (b) lists, for the hypergraph in (a), the candidate 
E-spans for each F-span. 
It should be noted that this automatic span pair 
annotation may violate some of the links in the 
original sentence-level alignment annotation. We 
have already seen how the 1-to-1 constraint in 
ITG leads to the violation. Another situation is 
the ?inside-out? alignment pattern (c.f. Figure 3). 
The ITG reordering constraint cannot be satisfied 
unless one of the links in this pattern is removed. 
f1      f2      f3      f4
e1     e2      e3      e4 
Figure 3: An example of inside-out alignment 
The training samples thus obtained are positive 
training samples. If we apply some classifier for 
parameter training, then negative samples are 
also needed. Fortunately, our parameter training 
does not rely on any negative samples. 
5.2 MERT for Pruning 
Parameter training of DPDI is based on Mini-
mum Error Rate Training (MERT) (Och, 2003), a 
widely used method in SMT. MERT for SMT 
estimates model parameters with the objective of 
minimizing certain measure of translation errors 
(or maximizing certain performance measure of 
translation quality) for a development corpus. 
Given an SMT system which produces, with 
                                                 
2 The formulae of the inside and outside probability of a 
span pair will be elaborated in Section 5.3. The simpler 
alignment model we used is HMM. 
318
model parameters ?1
?, the K-best candidate trans-
lations ? (??; ?1
?) for a source sentence ??, and an 
error measure ?(?? , ??,?) of a particular candidate 
??,? with respect to the reference translation ?? , 
the optimal parameter values will be:  
? 1
? = ??????
?1
?
 ? ?? , ? ??; ?1
?  
?
?=1
  
 = ??????
?1
?
  ? ?? , ??,? ?(? ??; ?1
? , ??,?)
?
?=1
?
?=1
   
DPDI applies the same equation for parameter 
tuning, with different interpretation of the com-
ponents in the equation. Instead of a development 
corpus with reference translations, we have a col-
lection of training samples, each of which is a 
pair of F-span (??) and its corresponding E-span 
(??). These samples are acquired from some ma-
nually aligned dataset by the method elaborated 
in Section 5.1. The ITG parser outputs for each fs  
a K-best list of E-spans ? ??; ?1
?  based on the 
current parameter values ?1
?.  
The error function is based on the presence and 
the rank of the correct E-span in the K-best list:  
?  ?? , ? ??; ?1
?  =  
????? ??  ?? ?? ? ? ??; ?1
? 
???????      ?????????          
  
(2)    
where ???? ??  is the (0-based) rank of the cor-
rect E-span ?? in the K-best list  ? ??; ?1
? . If  ?? is 
not in the K-best list at all, then the error is de-
fined to be ???????, which is set as -100000 in 
our experiments. The rationale underlying this 
error function is to keep as many correct E-spans 
as possible in the K-best lists of E-spans, and 
push the correct E-spans upward as much as 
possible in the K-best lists. 
This new error measure leads to a change in 
details of the training algorithm. In MERT for 
SMT, the interval boundaries at which the per-
formance or error measure changes are defined 
by the upper envelope (illustrated by the dash 
line in Figure 4(a)), since the performance/error 
measure depends on the best candidate transla-
tion. In MERT for DPDI, however, the error 
measure depends on the correct E-span rather 
than the E-span leading to the highest system 
score. Thus the interval boundaries are the inter-
sections between the correct E-span and all other 
candidate E-spans (as shown in Figure 4(b)). The 
rank of the correct E-span in each interval can 
then be figured out as shown in Figure 4(c). Fi-
nally, the error measure in each interval can be 
calculated by Equation (2) (as shown in Figure 
4(d)).  All other steps in MERT for DPDI are the 
same as that for SMT. 
??
m
f
m
 
-index
loss
?
k
-8
-9
-10
-8
-9
-100,000
gold
??
m
f
m
?
k
(a)
(b)
(c)
(d)
?
k
?
k
 
Figure 4: MERT for DPDI 
Part (a) shows how intervals are defined for SMT and 
part (b) for DPDI. Part (c) obtains the rank of correct 
E-spans in each interval and part (d) the error measure. 
Note that the beam size (max number of E-spans) for 
each F-span is 10. 
5.3 Features 
The features used in DPDI are divided into three 
categories:  
1) Model 1-based probabilities. Zhang and Gil-
dea (2005) show that Model 1 (Brown et al, 
1993; Och and Ney., 2000) probabilities of 
the word pairs inside and outside a span pair 
( ??1 , ??2 /[??1 ,??2]) are useful. Hence these 
two features: 
a) Inside probability (i.e. probability of 
word pairs within the span pair): 
????  ??1,?2 ??1,?2 
=   
1
 ?2 ? ?1 
??1 ?? ?? 
?? ?1,?2 ?? ?1,?2 
 
b) Outside probability (i.e. probability of 
the word pairs outside the span pair): 
????  ??1,?2 ??1,?2 
=   
1
 ? ? ?2 + ?1 
??1 ?? ?? 
? ? ?1,?2 ?? ?1,?2 
   
where ? is the length of the foreign sen-
tence. 
2) Heuristics. There are four features in this cat-
egory. The features are explained with the 
319
example of Figure 5, in which the span pair 
in interest is  ?2, ?3 /[?1,?2]. The four links 
are produced by some simpler alignment 
model like HMM. The word pair  ?2/?1  is 
the only link in the span pair. The links 
?4/?2  and ?3/?3 are inconsistent with the 
span pair.3  
f1      f2      f3      f4
e1     e2      e3      e4
 
Figure 5: Example for heuristic features 
a) Link ratio: 
2?#?????
???? +????
 
where #?????  is the number of links in 
the span pair, and ???? and ???? are the 
length of the foreign and English spans 
respectively. The feature value of the ex-
ample span pair is (2*1)/(2+2)=0.5. 
b) inconsistent link ratio: 
2?#????? ?????
???? +????
  
where #??????????  is the number of links 
which are inconsistent with the phrase 
pair according to some simpler alignment 
model (e.g. HMM). The feature value of 
the example is (2*2)/(2+2) =1.0. 
c) Length ratio: 
????
????
? ????????   
where ????????  is defined as the average 
ratio of foreign sentence length to Eng-
lish sentence length, and it is estimated to 
be around 1.15 in our training dataset. 
The rationale underlying this feature is 
that the ratio of span length should not be 
too deviated from the average ratio of 
sentence length. The feature value for the 
example is |2/2-1.15|=0.15. 
d) Position Deviation: ???? ? ????    
where ????  refers to the position of the 
F-span in the entire foreign sentence, and 
it is defined as 
1
2?
 ?????? + ???? , 
??????  /????  being the position of the 
first/last word of the F-span in the for-
eign sentence. ????  is defined similarly. 
The rationale behind this feature is the 
monotonic assumption, i.e. a phrase of 
the foreign sentence usually occupies 
roughly the same position of the equiva-
lent English phrase. The feature value for 
                                                 
3
 An inconsistent link connects a word within the phrase pair 
to some word outside the phrase pair. C.f. Deng et al (2008) 
the example is |(1+2)/(2*4)-(2+3)/(2*4)| 
=0.25. 
3) HMM-based probabilities. Haghighi et al 
(2009) show that posterior probabilities from 
the HMM alignment model is useful for 
pruning. Therefore, we design two new fea-
tures by replacing the link count in link ratio 
and inconsistent link ratio with the sum of the 
link?s posterior probability. 
6 The DITG Models 
The discriminative ITG alignment can be con-
ceived as a two-staged process. In the first stage 
DPDI selects good span pairs. In the second stage 
good alignment hypotheses are assigned to the 
span pairs selected by DPDI. Two discriminative 
ITG (DITG) models are investigated. One is 
word-to-word DITG (henceforth W-DITG), 
which observes the 1-to-1 constraint on align-
ment. Another is DITG with hierarchical phrase 
pairs (henceforth HP-DITG), which relaxes the 1-
to-1 constraint by adopting hierarchical phrase 
pairs in Chiang (2007).  
Each model selects the best alignment hypo-
theses of each span pair, given a set of features. 
The contributions of these features are integrated 
through a log linear model (similar to Liu et al, 
2005; Moore, 2005) like Equation (1). The dis-
criminative training of the feature weights is 
again MERT (Och, 2003). The MERT module 
for DITG takes alignment F-score of a sentence 
pair as the performance measure. Given an input 
sentence pair and the reference annotated align-
ment, MERT aims to maximize the F-score of 
DITG-produced alignment. Like SMT (and un-
like DPDI), it is the upper envelope which de-
fines the intervals where the performance meas-
ure changes. 
6.1 Word-to-word DITG 
The following features about alignment link are 
used in W-DITG: 
1) Word pair translation probabilities trained 
from HMM model (Vogel, et.al., 1996) 
and IBM model 4 (Brown et.al., 1993; 
Och and Ney, 2000). 
2) Conditional link probability (Moore, 2005). 
3) Association score rank features (Moore et 
al., 2006). 
4) Distortion features: counts of inversion 
and concatenation. 
5) Difference between the relative positions 
of the words. The relative position of a 
word in a sentence is defined as the posi-
320
tion of the word divided by sentence 
length.  
6) Boolean features like whether a word in 
the word pair is a stop word.  
6.2 DITG with Hierarchical Phrase Pairs 
The 1-to-1 assumption in ITG is a serious limita-
tion as in reality there are always segmentation or 
tokenization errors as well as idiomatic expres-
sions. Wu (1997) proposes a bilingual segmenta-
tion grammar extending the terminal rules by 
including phrase pairs. Cherry and Lin (2007) 
incorporate phrase pairs in phrase-based SMT 
into ITG, and Haghighi et al (2009) introduce 
Block ITG (BITG), which adds 1-to-many or 
many-to-1 terminal unary rules.  
It is interesting to see if DPDI can benefit the 
parsing of a more realistic ITG. HP-DITG ex-
tends Cherry and Lin?s approach by not only em-
ploying simple phrase pairs but also hierarchical 
phrase pairs (Chiang, 2007). The grammar is 
enriched with rules of the format: ?? ? ?/? ? 
where ? ?  and ? ?  refer to the English and foreign 
side of the i-th (simple/hierarchical) phrase pair 
respectively.  
As example, if there is a simple phrase pair 
??  ????? ?????,? ?? , then it is trans-
formed into the ITG rule ?? "North Korea"/
"? ??". During parsing, each span pair does 
not only examine all possible combinations of 
sub-span pairs using binary rules, but also checks 
if the yield of that span pair is exactly the same as 
that phrase pair. If so, then the alignment links 
within the phrase pair (which are obtained in 
standard phrase pair extraction procedure) are 
taken as an alternative alignment hypothesis of 
that span pair.  
For a hierarchical phrase pair like 
??  ?1 ?? ?2 ,?2 ? ?1 , it is transformed into 
the ITG rule  ?? "?1 ?? ?2"/"?2 ? ?1"  during 
parsing, each span pair checks if it contains the 
lexical anchors "of" and "?", and if the remain-
ing words in its yield can form two sub-span 
pairs which fit the reordering constraint among 
?1 and ?2. (Note that span pairs of any category 
in the ITG normal form grammar can substitute 
for ?1 or ?2 .) If both conditions hold, then the 
span pair is assigned an alignment hypothesis 
which combines the alignment links among the 
lexical anchors (???? ??/?)  and those links 
among the sub-span pairs.  
HP-ITG acquires the rules from HMM-based 
word-aligned corpus using standard phrase pair 
extraction as stated in Chiang (2007). The rule 
probabilities and lexical weights in both English-
to-foreign and foreign-to-English directions are 
estimated and taken as features, in addition to 
those features in W-DITG, in the discriminative 
model of alignment hypothesis selection.  
7 Evaluation 
DPDI is evaluated against the baselines of Tic-
tac-toe (TTT) pruning (Zhang and Gildea, 2005) 
and Dynamic Program (DP) pruning (Haghighi et 
al., 2009; DeNero et al, 2009) with respect to 
Chinese-to-English alignment and translation. 
Based on DPDI, HP-DITG is evaluated against 
the alignment systems GIZA++ and BITG. 
7.1 Evaluation Criteria 
Four evaluation criteria are used in addition to 
the time spent on ITG parsing. We will first eva-
luate pruning regarding the pruning decisions 
themselves. That is, the first evaluation metric, 
pruning error rate (henceforth PER), measures 
how many correct E-spans are discarded. The 
major drawback of PER is that not all decisions 
in pruning would impact on alignment quality, 
since certain F-spans are of little use to the entire 
ITG parse tree.  
An alternative criterion is the upper bound on 
alignment F-score, which essentially measures 
how many links in annotated alignment can be 
kept in ITG parse. The calculation of F-score up-
per bound is done in a bottom-up way like ITG 
parsing. All leaf hypernodes which contain a cor-
rect link are assigned a score (known as hit) of 1. 
The hit of a non-leaf hypernode is based on the 
sum of hits of its daughter hypernodes. The max-
imal sum among all hyperedges of a hypernode is 
assigned to that hypernode. Formally,  
??? ? ? , ?  = 
???
?,?,? 1 ,? 1 ,? 2 ,? 2
(??? ? ? 1, ? 1  + ???[? 2, ? 2]) 
??? ??  ?, ?  =  
1      ??  ?, ? ? ?
0        ????????? 
  
??? ?? = 0;??? ?? = 0 
where ?,?,? are variables for the categories in 
ITG grammar, and ? comprises the golden links 
in annotated alignment. ?? , ?? , ??  are defined in 
Appendix A. 
Figure 6 illustrates the calculation of the hit 
score for the example in Section 5.1/Figure 2. 
The upper bound of recall is the hit score divided 
by the total number of golden links. The upper 
321
ID pruning beam size pruning/total time cost PER F-UB F-score 
1 DPDI 10 72??/3?03?? 4.9% 88.5% 82.5% 
2 TTT 10 58??/2?38?? 8.6% 87.5% 81.1% 
3 TTT 20 53??/6?55?? 5.2% 88.6% 82.4% 
4 DP -- 11??/6?01?? 12.1% 86.1% 80.5% 
Table 1: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for W-DITG 
ID pruning beam size pruning/total time cost PER F-UB F-score 
1 DPDI 10 72??/5?18?? 4.9% 93.9% 87.0% 
2 TTT 10 58??/4?51?? 8.6% 93.0% 84.8% 
3 TTT 20 53??/12?5?? 5.2% 94.0% 86.5% 
4 DP -- 11??/15?39?? 12.1% 91.4% 83.6% 
Table 2: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG. 
bound of precision, which should be defined as 
the hit score divided by the number of links pro-
duced by the system, is almost always 1.0 in 
practice. The upper bound of alignment F-score 
can thus be calculated as well.  
A?[C,C]
C
w
:
[e1,e1]/[f1,f1]
hit=1
C
e
:
[e1]/?
C
w
:
[e2,e2]/[f1,f1]
C
e
:
[e2]/?
C
w
:
[e3,e3]/[f2,f2]
C:
[e1,e2]/[f1,f1]
hit=max{0+ }=1
C:
[e2,e3]/[f2,f2]
hit=max{0+1}=1
A:
[e1,e3]/[f1,f2]
hit=max{1+1,1+1}=2
C? [C
e
,C
w
]
A?[C,C]
C? [C
e
,C
w
]
hit=1 hit=1hit=0 hit=0
 
Figure 6: Recall Upper Bound Calculation 
Finally, we also do end-to-end evaluation us-
ing both F-score in alignment and Bleu score in 
translation. We use our implementation of hierar-
chical phrase-based SMT (Chiang, 2007), with 
standard features, for the SMT experiments.  
7.2 Experiment Data 
Both discriminative pruning and alignment need 
training data and test data. We use the manually 
aligned Chinese-English dataset as used in Hag-
highi et al (2009). The 491 sentence pairs in this 
dataset are adapted to our own Chinese word 
segmentation standard. 250 sentence pairs are 
used as training data and the other 241 are test 
data. The corresponding numbers of F-spans in 
training and test data are 4590 and 3951 respec-
tively.  
In SMT experiments, the bilingual training da-
taset is the NIST training set excluding the Hong 
Kong Law and Hong Kong Hansard, and our 5-
gram language model is trained from the Xinhua 
section of the Gigaword corpus. The NIST?03 
test set is used as our development corpus and the 
NIST?05 and NIST?08 test sets are our test sets.  
7.3 Small-scale Evaluation 
The first set of experiments evaluates the perfor-
mance of the three pruning methods using the 
small 241-sentence set. Each pruning method is 
plugged in both W-DITG and HP-DITG. IBM 
Model 1 and HMM alignment model are re-
implemented as they are required by the three 
ITG pruning methods.  
The results for W-DITG are listed in Table 1. 
Tests 1 and 2 show that with the same beam size 
(i.e. number of E-spans per F-span), although 
DPDI spends a bit more time (due to the more 
complicated model), DPDI makes far less incor-
rect pruning decisions than the TTT. In terms of 
F-score upper bound, DPDI is 1 percent higher. 
DPDI achieves even larger improvement in ac-
tual F-score. 
To enable TTT achieving similar F-score or F-
score upper bound, the beam size has to be 
doubled and the time cost is more than twice the 
original (c.f. Tests 1 and 3 in Table 1) . 
The DP pruning in Haghighi et.al. (2009) per-
forms much poorer than the other two pruning 
methods. In fact, we fail to enable DP achieve the 
same F-score upper bound as the other two me-
thods before DP leads to intolerable memory 
consumption. This may be due to the use of dif-
ferent HMM model implementations between our 
work and Haghighi et.al. (2009).  
Table 2 lists the results for HP-DITG. Roughly 
the same observation as in W-DITG can be made. 
In addition to the superiority of DPDI, it can also 
be noted that HP-DITG achieves much higher F-
score and F-score upper bound. This shows that 
322
hierarchical phrase is a powerful tool in rectify-
ing the 1-to-1 constraint in ITG. 
Note also that while TTT in Test 3 gets rough-
ly the same F-score upper bound as DPDI in Test 
1, the corresponding F-score is slightly worse. A 
possible explanation is that better pruning not 
only speeds up the parsing/alignment process but 
also guides the search process to focus on the 
most promising region of the search space. 
7.4 Large-scale End-to-End Experiment 
ID Prun-
ing 
beam 
size 
time 
cost 
Bleu-
05 
Bleu-
08 
1 DPDI 10 1092h 38.57 28.31 
2 TTT 10 972h 37.96 27.37 
3 TTT 20 2376h 38.13 27.58 
4 DP -- 2068h 37.43 27.12 
Table 3:  Evaluation of DPDI against TTT and 
DP for HP-DITG  
ID WA-
Model 
F-Score Bleu-05 Bleu-08 
1 HMM 80.1% 36.91 26.86 
2 Giza++ 84.2% 37.70 27.33 
3 BITG 85.9% 37.92 27.85 
4 HP-DITG 87.0% 38.57 28.31 
Table 4:  Evaluation of DPDI against HMM, Gi-
za++ and BITG 
Table 3 lists the word alignment time cost and 
SMT performance of different pruning methods.  
HP-DITG using DPDI achieves the best Bleu 
score with acceptable time cost. Table 4 com-
pares HP-DITG to HMM (Vogel, et al, 1996), 
GIZA++ (Och and Ney, 2000) and BITG (Hag-
highi et al, 2009). It shows that HP-DITG (with 
DPDI) is better than the three baselines both in 
alignment F-score and Bleu score. Note that the 
Bleu score differences between HP-DITG and the 
three baselines are statistically significant (Koehn, 
2004). 
An explanation of the better performance by 
HP-DITG is the better phrase pair extraction due 
to DPDI. On the one hand, a good phrase pair 
often fails to be extracted due to a link inconsis-
tent with the pair. On the other hand, ITG prun-
ing can be considered as phrase pair selection, 
and good ITG pruning like DPDI guides the sub-
sequent ITG alignment process so that less links 
inconsistent to good phrase pairs are produced. 
This also explains (in Tables 2 and 3) why DPDI 
with beam size 10 leads to higher Bleu than TTT 
with beam size 20, even though both pruning me-
thods lead to roughly the same alignment F-score.  
8 Conclusion and Future Work 
This paper reviews word alignment through ITG 
parsing, and clarifies the problem of ITG pruning. 
A discriminative pruning model and two discri-
minative ITG alignments systems are proposed. 
The pruning model is shown to be superior to all 
existing ITG pruning methods, and the HP-DITG 
alignment system is shown to improve state-of-
the-art alignment and translation quality.  
The current DPDI model employs a very li-
mited set of features. Many features are related 
only to probabilities of word pairs. As the success 
of HP-DITG illustrates the merit of hierarchical 
phrase pair, in future we should investigate more 
features on the relationship between span pair 
and hierarchical phrase pair. 
Appendix A. The Normal Form Grammar 
Table 5 lists the ITG rules in normal form as 
used in this paper, which extend the normal form 
in Wu (1997) so as to handle the case of align-
ment to null. 
1  ?  ? ?|?|? 
2  ?  ?  ? ? | ? ? | ? ? | ?? | ? ? | ? ?  
3  ?  ?  ? ? | ? ? | ? ? | ? ?  
  ?  ?   ? ? | ? ?  
4  ?  ? ?? |??? |???  
5  ?  ?  ???  ???   
6 ??  ? ?/? 
7 ??   ? ?/?;?? ? ?/? 
8 ??? ? ??| ???  ?? ;??? ? ?? | ???  ??  
9 ??? ?  ???  ??  ;??? ?  ???  ??   
 
Table 5: ITG Rules in Normal Form 
In these rules, ? is the Start symbol; ? is the 
category for concatenating combination whereas 
? for inverted combination. Rules (2) and (3) are 
inherited from Wu (1997). Rules (4) divide the 
terminal category ?  into subcategories. Rule 
schema (6) subsumes all terminal unary rules for 
some English word ?  and foreign word ? , and 
rule schemas (7) are unary rules for alignment to 
null. Rules (8) ensure all words linked to null are 
combined in left branching manner, while rules 
(9) ensure those words linked to null combine 
with some following, rather than preceding, word 
pair. (Note: Accordingly, all sentences must be 
ended by a special token  ??? , otherwise the 
last word(s) of a sentence cannot be linked to 
null.) If there are both English and foreign words 
linked to null, rule (5) ensures that those English 
323
words linked to null precede those foreign words 
linked to null. 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Peitra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 
19(2):263-311. 
Colin Cherry and Dekang Lin. 2006. Soft Syntactic 
Constraints for Word Alignment through Dis-
criminative Training. In Proceedings of ACL-
COLING.  
Colin Cherry and Dekang Lin. 2007. Inversion 
Transduction Grammar for Joint Phrasal 
Translation Modeling. In Proceedings of SSST, 
NAACL-HLT, Pages:17-24.  
David Chiang. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics, 33(2). 
John DeNero, Mohit Bansal, Adam Pauls, and Dan 
Klein. 2009. Efficient Parsing for Transducer 
Grammars. In Proceedings of NAACL, Pag-
es:227-235. 
Alexander Fraser and Daniel Marcu. 2006. Semi-
Supervised Training for StatisticalWord 
Alignment. In Proceedings of ACL, Pages:769-
776. 
Aria Haghighi, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better Word Alignments with Su-
pervised ITG Models. In Proceedings of ACL, 
Pages: 923-931. 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. In Proceedings of IWPT 2005, Pag-
es:173-180. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. In Proceedings of 
ACL. Pages: 440-447 
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of ACL,  Pages:160-167. 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. In Proceedings of IWPT, 
Pages:17-19 
Philipp Koehn. 2004. Statistical Significance Tests 
for Machine Translation Evaluation. In Pro-
ceedings of EMNLP,  Pages: 388-395. 
Yang Liu, Qun Liu and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceed-
ings of ACL, Pages: 81-88. 
Robert Moore. 2005. A Discriminative Framework 
for Bilingual Word Alignment. In Proceedings of 
EMNLP 2005, Pages: 81-88. 
Robert Moore, Wen-tau Yih, and Andreas Bode. 2006. 
Improved Discriminative Bilingual Word 
Alignment. In Proceedings of ACL, Pages: 513-
520. 
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of COL-
ING, Pages: 836-841. 
Stephan Vogel. 2005. PESA: Phrase Pair Extrac-
tion as Sentence Splitting.  In Proceedings of MT 
Summit. 
Dekai Wu. 1997. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Pa-
rallel Corpora. Computational Linguistics, 23(3). 
Hao Zhang and Daniel Gildea. 2005. Stochastic Lex-
icalized Inversion Transduction Grammar for 
Alignment. In Proceedings of ACL.  
Hao Zhang, Chris Quirk, Robert Moore, and Daniel 
Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In Proceedings of ACL, Pages: 314-323. 
 
324
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 302?310,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Translation Consensus with Structured Label Propagation 
 
?Shujie Liu*, ?Chi-Ho Li, ?Mu Li and ?Ming Zhou
? Harbin Institute of Technology ?Microsoft Research Asia 
 Harbin, China Beijing, China 
shujieliu@mtlab.hit.edu.cn 
 
{chl, muli, mingzhou}@microsoft.com 
 
 
Abstract 
In this paper, we address the issue for 
learning better translation consensus in 
machine translation (MT) research, and 
explore the search of translation consensus 
from similar, rather than the same, source 
sentences or their spans. Unlike previous 
work on this topic, we formulate the 
problem as structured labeling over a much 
smaller graph, and we propose a novel 
structured label propagation for the task. 
We convert such graph-based translation 
consensus from similar source strings into 
useful features both for n-best output re-
ranking and for decoding algorithm. 
Experimental results show that, our method 
can significantly improve machine 
translation performance on both IWSLT 
and NIST data, compared with a state-of-
the-art baseline.  
1 Introduction 
Consensus in translation has? gained more and 
more attention in recent years. The principle of 
consensus can be sketched as ?a translation 
candidate is deemed more plausible if it is 
supported by other translation candidates.? The 
actual formulation of the principle depends on 
whether the translation candidate is a complete 
sentence or just a span of it, whether the candidate 
is the same as or similar to the supporting 
candidates, and whether the supporting candidates 
come from the same or different MT system.  
                                                          
? This work has been done while the first author was visiting 
Microsoft Research Asia. 
Translation consensus is employed in those 
minimum Bayes risk (MBR) approaches where the 
loss function of a translation is defined with 
respect to all other translation candidates. That is, 
the translation with the minimal Bayes risk is the 
one to the greatest extent similar to other 
candidates. These approaches include the work of 
Kumar and Byrne (2004), which re-ranks the n-
best output of a MT decoder, and the work of 
Tromble et al (2008) and Kumar et al (2009), 
which does MBR decoding for lattices and 
hypergraphs.  
Others extend consensus among translations 
from the same MT system to those from different 
MT systems. Collaborative decoding (Li et al, 
2009) scores the translation of a source span by its 
n-gram similarity to the translations by other 
systems. Hypothesis mixture decoding (Duan et al, 
2011) performs a second decoding process where 
the search space is enriched with new hypotheses 
composed out of existing hypotheses from multiple 
systems. 
All these approaches are about utilizing 
consensus among translations for the same (span 
of) source sentence. It should be noted that 
consensus among translations of similar source 
sentences/spans is also helpful for good candidate 
selection. Consider the examples in Figure 1. For 
the source (Chinese) span ??? ? ?? ? ? ?, 
the MT system produced the correct translation for 
the second sentence, but it failed to do so for the 
first one. If the translation of the first sentence 
could take into consideration the translation of the 
second sentence, which is similar to but not 
exactly the same as the first one, the final 
translation output may be improved. 
Following this line of reasoning, a 
discriminative learning method is proposed to 
constrain the translation of an input sentence using 
302
the most similar translation examples from 
translation memory (TM) systems (Ma et al, 
2011). A classifier is applied to re-rank the n-best 
output of a decoder, taking as features the 
information about the agreement with those similar 
translation examples. Alexandrescu and Kirchhoff 
(2009) proposed a graph-based semi-supervised 
model to re-rank n-best translation output. Note 
that these two attempts are about translation 
consensus for similar sentences, and about re-
ranking of n-best output. It is still an open question 
whether translation consensus for similar 
sentences/spans can be applied to the decoding 
process. Moreover, the method in Alexandrescu 
and Kirchhoff (2009) is formulated as a typical and 
simple label propagation, which leads to very large 
graph, thus making learning and search inefficient. 
(c.f. Section 3.) 
In this paper, we attempt to leverage translation 
consensus among similar (spans of) source 
sentences in bilingual training data, by a novel 
graph-based model of translation consensus. 
Unlike Alexandrescu and Kirchhoff (2009), we 
reformulate the task of seeking translation 
consensus among source sentences as structured 
labeling. We propose a novel label propagation 
algorithm for structured labeling, which is much 
more efficient than simple label propagation, and 
derive useful MT decoder features out of it. We 
conduct experiments with IWSLT and NIST data, 
and experimental results show that, our method 
can improve the translation performance 
significantly on both data sets, compared with a 
state-of-the-art baseline. 
2 Graph-based Translation Consensus 
Our MT system with graph-based translation 
consensus adopts the conventional log-linear 
model. For the source string ? , the conditional 
probability of a translation candidate ? is defined 
as: 
???|?? ? exp ?? ???????, ???? ?? ?exp?? ????????, ???? ????????? (1)
where ?  is the feature vector, ?  is the feature 
weights, and ????  is the set of translation 
hypotheses in the search space.  
Based on the commonly used features, two 
kinds of feature are added to equation (1), one is 
graph-based consensus features, which are about 
consensus among the translations of similar 
sentences/spans; the other is local consensus 
features, which are about consensus among the 
translations of the same sentence/span. We 
develop a structured label propagation method, 
which can calculate consensus statistics from 
translation candidates of similar source 
sentences/spans. 
In the following, we explain why the standard, 
simple label propagation is not suitable for 
translation consensus, and then introduce how the 
problem is formulated as an instance of structured 
labeling, with the proposed structured label 
propagation algorithm, in section 3. Before 
elaborating how the graph model of consensus is 
constructed for both a decoder and N-best output 
re-ranking in section 5, we will describe how the 
consensus features and their feature weights can be 
trained in a semi-supervised way, in section 4. 
3 Graph-based Structured Learning 
In general, a graph-based model assigns labels to 
instances by considering the labels of similar 
instances. A graph is constructed so that each 
instance is represented by a node, and the weight 
of the edge between a pair of nodes represents the 
similarity between them. The gist of graph-based 
model is that, if two instances are connected by a 
strong edge, then their labels tend to be the same 
(Zhu, 2005). 
 
IWSLT Chinese to English Translation Task 
Src ? ??? ?? ? ?? ? ? ? 
Ref Do you have any tea under five 
hundred dollars ? 
Best1 Do you have any less than five 
hundred dollars tea ? 
Src ? ?? ?? ? ?? ? ? . 
Ref I would like some tea under five 
hundred dollars . 
Best1 I would like tea under five hundred 
dollars . 
Figure 1. Two sentences from IWSLT 
(Chinese to English) data set. "Src" stands for 
the source sentence, and "Ref" means the 
reference sentence. "Best1" is the final output 
of the decoder. 
303
In MT, the instances are source sentences or 
spans of source sentences, and the possible labels 
are their translation candidates. This scenario 
differs from the general case of graph-based model 
in two aspects. First, there are an indefinite, or 
even intractable, number of labels. Each of them is 
a string of words rather than a simple category. In 
the following we will call these labels as structured 
labels (Berlett et al, 2004). Second, labels are 
highly ?instance-dependent?. In most cases, for any 
two different (spans of) source sentences, however 
small their difference is, their correct labels 
(translations) are not exactly the same. Therefore, 
the principle of graph-based translation consensus 
must be reformulated as, if two instances (source 
spans) are similar, then their labels (translations) 
tend to be similar (rather than the same). 
Note that Alexandrescu and Kirchhoff (2009) do 
not consider translation as structured labeling. In 
their graph, a node does not represent only a 
source sentence but a pair of source sentence and 
its candidate translation, and there are only two 
possible labels for each node, namely, 1 (this is a 
good translation pair) and 0 (this is not a good 
translation pair). Thus their graph-based model is a 
normal example of the general graph-based model. 
The biggest problem of such a perspective is 
inefficiency. An average MT decoder considers a 
vast amount of translation candidates for each 
source sentence, and therefore the corresponding 
graph also contains a vast amount of nodes, thus 
rendering learning over a large dataset is infeasible. 
3.1 Label Propagation for General Graph-
based Models 
A general graph-based model is iteratively trained 
by label propagation, in which ??,?, the probability 
of label l for the node ?, is updated with respect to 
the corresponding probabilities for ??s neighboring 
nodes ???? . In Zhu (2005), the updating rule is 
expressed in a matrix calculation. For convenience, 
the updating rule is expressed for each label here: 
??,???? ? ? ???, ????,??
??????
 
 
(2)
where ???, ??,  the propagating probability, is 
defined as: 
???, ?? ? ??,?? ??,?????????  
 
(3)
??,?  defines the weight of the edge, which is a 
similarity measure between nodes ? and ?. 
Note that the graph contains nodes for training 
instances, whose correct labels are known. The 
probability of the correct label to each training 
instance is reset to 1 at the end of each iteration. 
With a suitable measure of instance/node similarity, 
it is expected that an unlabeled instance/node will 
find the most suitable label from similar labeled 
nodes.  
3.2 Structured Label Propagation for Graph-
based Learning 
In structured learning like MT, different instances 
would not have the same correct label, and so the 
updating rule (2) is no longer valid, as the value of 
??,?   should not be calculated based on ??,? . Here 
we need a new updating rule so that ??,?  can be 
updated with respect to ??,?? , where in general 
? ? ??. 
Let us start with the model in Alexandrescu and 
Kirchhoff (2009). According to them, a node in the 
graph represents the pair of some source 
sentence/span ??  and its translation candidate ?? . 
The updating rule (for the label 1 or 0) is: 
???,????? ? ? ????, ??, ???, ????????,????
???,????????,??
??4? 
where ????, ?? is the set of neighbors of the node 
??, ?). 
When the problem is reformulated as structured 
labeling, each node represents the source 
sentence/span only, and the translation candidates 
become labels. The propagating probability 
????, ??, ???, ????? has to be reformulated 
accordingly. A natural way is to decompose it into 
a component for nodes and a component for labels. 
Assuming that the two components are 
independent, then: 
????, ??, ???, ???? ? ????, ??? ????, ????????????5? 
where ????, ??? is the propagating probability from source sentence/span ?? to ? , and ????, ??? is that from translation candidate  ?? to ?.  
The set of neighbors ????, ?? of a pair ??, ?? 
has also to be reformulated in terms of the set of 
neighbors ???? of a source sentence/span ?: 
????, ?? ? ????, ???|?? ? ????, ?? ? ????????6? 
304
where???????is?the?set?of?translation?candidates?
for?source???.?The new updating rule will then be:?
??,???? ? ? ????, ??? ????, ??????,???
???????,????????
?
? ? ? ????, ??? ????, ??????,???
???????????????
?
? ? ????, ??? ? ????, ??????,???
???????????????
???7? 
The new rule updates the probability of a 
translation ?  of a source sentence/span ? with 
probabilities of similar translations ??s  of some 
similar source sentences/spans ??s.  
Propagation probability ????, ??? is as defined in equation (3), and ????, ??? is defined given some similarity measure ?????, ??? between labels ? and 
??: 
????, ??? ? ???
??, ???
? ?????, ????????????? ? ?????????????8? 
Note that rule (2) is a special case of rule (7), 
when ?????, ??? is defined as: 
?????, ??? ? ?
1
0
???????????
???? ? ???;
?????????;
 
4 Features and Training 
The last section sketched the structured label 
propagation algorithm. Before elaborating the 
details of how the actual graph is constructed, we 
would like to first introduce how the graph-based 
translation consensus can be used in an MT system. 
4.1 Graph-based Consensus Features  
The probability as estimated in equation (7) is 
taken as a group of new features in either a 
decoder or an n-best output re-ranker. We will call 
these features collectively as graph-based 
consensus features (GC): 
????, ?? ?????????????????????????????????????????????????????????????????9??
log?? ? ????, ??? ? ????, ??????,??
???????????????
??
Recall that, ???? refers to source sentences/spans 
which are similar with ? , and ?????  refers to 
translation candidates of ?? . ???,??  is initialized 
with the translation posterior of ?? given ?? .The 
translation posterior is normalized in the n-best list. 
For the nodes representing the training sentence 
pairs, this posterior is fixed. ? ????, ???  is the propagating probability in equation (8), with the 
similarity measure ?????, ??? defined as the Dice 
co-efficient over the set of all n-grams in ?  and 
those in ??. That is, 
?????, ??? ? ????????????, ????????? 
where ??????? is the set of n-grams in string ?, and ??????, ?? is the Dice co-efficient over sets ? 
and ?: 
??????, ?? ? 2|? ? ?||?| ? |?| 
We take 1 ? ? ? 4  for similarity between 
translation candidates, thus leading to four features. 
The other propagating probability ????, ??? , as defined in equation (3),  takes symmetrical 
sentence level BLEU as similarity measure1: 
??,?? ?
1
2 ???? ???????, ?
?? ? ??? ????????, ??? 
where ??? ???????, ???  is defined as follows (Liang et al, 2006): 
??? ???????, ??? ?? ? ? ??????, ?
??
2?????
?
???
????10? 
where ? ? ??????, ???  is the IBM BLEU score 
computed over i-grams for hypothesis ? using ?? 
as reference. 
In theory we could use other similarity measures 
such as edit distance, string kernel. Here simple n-
gram similarity is used for the sake of efficiency. 
4.2 Other Features 
In addition to graph-based consensus features, we 
also propose local consensus features, defined over 
the n-best translation candidates as: 
????, ?? ? log ? ? ????|?? ?????, ???
???????
?? (11)
                                                          
1 BLEU is not symmetric, which means, different scores are 
obtained depending on which one is reference and which one 
is hypothesis. 
305
where ????|??? is translation posterior. Like ?? , 
there are four features with respect to the value of 
n in n-gram similarity measure. 
We also use other fundamental features, such as 
translation probabilities, lexical weights, distortion 
probability, word penalty, and language model 
probability. 
4.3 Training Method 
When graph-based consensus is applied to an MT 
system, the graph will have nodes for training data, 
development (dev) data, and test data (details in 
Section 5). There is only one label/translation for 
each training data node. For each dev/test data 
node, the possible labels are the n-best translation 
candidates from the decoder. Note that there is 
mutual dependence between the consensus graph 
and the decoder. On the one hand, the MT decoder 
depends on the graph for the GC features. On the 
other hand, the graph needs the decoder to provide 
the translation candidates as possible labels, and 
their posterior probabilities as initial values of 
various ??,? . Therefore, we can alternatively 
update graph-based consensus features and feature 
weights in the log-linear model. 
Algorithm 1 Semi-Supervised Learning 
??? ? 0; 
??=??????????, ????, ????; 
while not converged do 
 ?? ? ?????????????, ??????, ????, ?????, ???. 
 ????? ? ????????????. 
 ???? ? ?????????, ????, ?????? 
end while 
return last (???,???) 
Algorithm 1 outlines our semi-supervised 
method for such alternative training. The entire 
process starts with a decoder without consensus 
features. Then a graph is constructed out of all 
training, dev, and test data. The subsequent 
structured label propagation provides ??  feature 
values to the MT decoder. The decoder then adds 
the new features and re-trains all the feature 
weights?by Minimum Error Rate Training (MERT) 
(Och, 2003). The decoder with new feature 
weights then provides new n-best candidates and 
their posteriors for constructing another consensus 
graph, which in turn gives rise to next round of 
MERT. This alternation of structured label 
propagation and MERT stops when the BLEU 
score on dev data converges, or a pre-set limit (10 
rounds) is reached. 
5 Graph Construction 
A technical detail is still needed to complete the 
description of graph-based consensus, namely, 
how the actual consensus graph is constructed. We 
will divide the discussion into two sections 
regarding how the graph is used.  
5.1 Graph Construction for Re-Ranking 
When graph-based consensus is used for re-
ranking the n-best outputs of a decoder, each node 
in the graph corresponds to a complete sentence. A 
separate node is created for each source sentence 
in training data, dev data, and test data. For any 
node from training data (henceforth training node), 
it is labeled with the correct translation, and ??,? is 
fixed as 1. If there are sentence pairs with the same 
source sentence but different translations, all the 
translations will be assigned as labels to that 
source sentence, and the corresponding 
probabilities are estimated by MLE. There is no 
edge between training nodes, since we suppose all 
the sentences of the training data are correct, and it 
is pointless to re-estimate the confidence of those 
sentence pairs. 
Each node from dev/test data (henceforth test 
node) is unlabeled, but it will be given an n-best 
list of translation candidates as possible labels 
from a MT decoder. The decoder also provides 
translation posteriors as the initial confidences of 
 
1, e1 a1 c b
2, e1 a1 b c
3, e2 a1 b c
E A B C
1, f1 b c d1
2, f1 d1 b c
3, f2 d1 b c
e1 a1 m n e1 a1 b n e1 d1 b n
0.5
0.5
0.75 0.5
 
Figure 2. A toy graph constructed for re-ranking.  
306
the labels. A test node can be connected to training 
nodes and other test nodes. If the source sentences 
of a test node and some other node are sufficiently 
similar, a similarity edge is created between them. 
In our experiment we measure similarity by 
symmetrical sentence level BLEU of source 
sentences, and 0.3 is taken as the threshold for 
edge creation.  
Figure 2 shows a toy example graph. Each node 
is depicted as rectangle with the upper half 
showing the source sentence and the lower half 
showing the correct or possible labels. Training 
nodes are in grey while test nodes are in white. 
The edges between the nodes are weighted by the 
similarities between the corresponding source 
sentences.   
5.2 Graph Construction for Decoding 
Graph-based consensus can also be used in the 
decoding algorithm, by re-ranking the translation 
candidates of not only the entire source sentence 
but also every source span. Accordingly the graph 
does not contain only the nodes for source 
sentences but also the nodes for all source spans. It 
is needed to find the candidate labels for each 
source span. 
It is not difficult to handle test nodes, since the 
purpose of MT decoder is to get al possible 
segmentations of a source sentence in dev/test data, 
search for the translation candidates of each source 
span, and calculate the probabilities of the 
candidates. Therefore, the cells in the search space 
of a decoder can be directly mapped as test nodes 
in the graph. 
 Training nodes can be handled similarly, by 
applying forced alignment. Forced alignment 
performs phrase segmentation and alignment of 
each sentence pair of the training data using the 
full translation system as in decoding (Wuebker et 
al., 2010). In simpler term, for each sentence pair 
in training data, a decoder is applied to the source 
side, and all the translation candidates that do not 
match any substring of the target side are deleted. 
The cells of in such a reduced search space of the 
decoder can be directly mapped as training nodes 
in the graph, just as in the case of test nodes. Note 
that, due to pruning in both decoding and 
translation model training, forced alignment may 
fail, i.e. the decoder may not be able to produce 
target side of a sentence pair. In such case we still 
map the cells in the search space as training nodes. 
Note also that the shorter a source span is, the 
more likely it appears in more than one source 
sentence. All the translation candidates of the same 
source span in different source sentences are 
merged. 
Edge creation is the same as that in graph 
construction for n-best re-ranking, except that two 
nodes are always connected if they are about a 
span and its sub-span. This exception ensures that 
shorter spans can always receive propagation from 
longer ones, and vice versa.  
Figure 3 shows a toy example. There is one 
node for the training sentence "E A M N" and two 
nodes for the test sentences "E A B C" and "F D B 
C". All the other nodes represent spans. The node 
"M N" and "E A" are created according to the 
forced alignment result of the sentence "E A M N". 
As we see, the translation candidates for "M N" 
and "E A" are not the sub-strings from the target 
sentence of "E A M N". There are two kinds of 
edges. Dash lines are edges connecting nodes of a 
span and its sub-span, such as the one between "E 
A B C" and "E". Solid lines are edges connecting 
nodes with sufficient source side n-gram similarity, 
such as the one between "E A M N" and "E A B 
C". 
 
 
Figure 3. A toy example graph for decoding. 
Edges in dash line indicate relation between a 
span and its sub-span, whereas edges of solid 
line indicate source side similarity. 
307
6 Experiments and Results 
In this section, graph-based translation consensus 
is tested on the Chinese to English translation tasks. 
The evaluation method is the case insensitive IBM 
BLEU-4 (Papineni et al, 2002). Significant testing 
is carried out using bootstrap re-sampling method 
proposed by Koehn (2004) with a 95% confidence 
level. 
6.1 Experimental Data Setting and Baselines 
We test our method with two data settings: one is 
IWSLT data set, the other is NIST data set. Our 
baseline decoder is an in-house implementation of 
Bracketing Transduction Grammar (Dekai Wu, 
1997) (BTG) in CKY-style decoding with a lexical 
reordering model trained with maximum entropy 
(Xiong et al, 2006). The features we used are 
commonly used features as standard BTG decoder, 
such as translation probabilities, lexical weights, 
language model, word penalty and distortion 
probabilities.  
Our IWSLT data is the IWSLT 2009 dialog task 
data set. The training data include the BTEC and 
SLDB training data. The training data contains 81k 
sentence pairs, 655k Chinese words and 806 
English words. The language model is 5-gram 
language model trained with the target sentences in 
the training data. The test set is devset9, and the 
development set for MERT comprises both 
devset8 and the Chinese DIALOG set. The 
baseline results on IWSLT data are shown in Table 
1. 
 devset8+dialog devset9 
Baseline 48.79 44.73 
Table 1. Baselines for IWSLT data 
For the NIST data set, the bilingual training data 
we used is NIST 2008 training set excluding the 
Hong Kong Law and Hong Kong Hansard. The 
training data contains 354k sentence pairs, 8M 
Chinese words and 10M English words. The 
language model is 5-gram language model trained 
with the Giga-Word corpus plus the English 
sentences in the training data. The development 
data utilized to tune the feature weights of our 
decoder is NIST?03 evaluation set, and test sets are 
NIST?05 and NIST?08 evaluation sets. The 
baseline results on NIST data are shown in Table 2. 
 NIST'03 NIST'05 NIST'08 
Baseline 38.57 38.21 27.52 
Table 2. Baselines for NIST data 
6.2 Experimental Result 
Table 3 shows the performance of our consensus-
based re-ranking and decoding on the IWSLT data 
set. To perform consensus-based re-ranking, we 
first use the baseline decoder to get the n-best list 
for each sentence of development and test data, 
then we create graph using the n-best lists and 
training data as we described in section 5.1, and 
perform semi-supervised training as mentioned in 
section 4.3. As we can see from Table 3, our 
consensus-based re-ranking (G-Re-Rank) 
outperforms the baseline significantly, not only for 
the development data, but also for the test data.  
Instead of using graph-based consensus 
confidence as features in the log-linear model, we 
perform structured label propagation (Struct-LP) to 
re-rank the n-best list directly, and the similarity 
measures for source sentences and translation 
candidates are symmetrical sentence level BLEU 
(equation (10)). Using Struct-LP, the performance 
is significantly improved, compared with the 
baseline, but not as well as G-Re-Rank. 
devset8+dialog devset9
Baseline 48.79 44.73 
Struct-LP 49.86 45.54 
G-Re-Rank 50.66 46.52 
G-Re-Rank-GC 50.23 45.96 
G-Re-Rank-LC 49.87 45.84 
G-Decode 51.20 47.31 
G-Decode-GC 50.46 46.21 
G-Decode-LC 50.11 46.17 
Table 3. Consensus-based re-ranking and decoding 
for IWSLT data set. The results in bold type are 
significantly better than the baseline. 
We use the baseline system to perform forced 
alignment procedure on the training data, and 
create span nodes using the derivation tree of the 
forced alignment. We also saved the spans of the 
sentences from development and test data, which 
will be used to create the responding nodes for 
consensus-based decoding. In such a way, we 
create the graph for decoding, and perform semi-
308
supervised training to calculate graph-based 
consensus features, and tune the weights for all the 
features we used. In Table 3, we can see that our 
consensus-based decoding (G-Decode) is much 
better than baseline, and also better than 
consensus-based re-ranking method. That is 
reasonable since the neighbor/local similarity 
features not only re-rank the final n-best output, 
but also the spans during decoding. 
To test the contribution of each kind of features, 
we first remove all the local consensus features 
and perform consensus-based re-ranking and 
decoding (G-Re-Rank-GC and G-Decode-GC), 
and then we remove all the graph-based consensus 
features to test the contribution of local consensus 
features (G-Re-Rank-LC and G-Decode-LC). 
Without the graph-based consensus features, our 
consensus-based re-ranking and decoding is 
simplified into a consensus re-ranking and 
consensus decoding system, which only re-rank 
the candidates according to the consensus 
information of other candidates in the same n-best 
list.  
From Table 3, we can see, the G-Re-Rank-LC 
and G-Decode-LC improve the performance of 
development data and test data, but not as much as 
G-Re-Rank and G-Decode do. G-Re-Rank-GC and 
G-Decode-GC improve the performance of 
machine translation according to the baseline. G-
Re-Rank-GC does not achieve the same 
performance as G-Re-Rank-LC does. Compared 
with G-Decode-LC, the performance with G-
Decode-GC is much better.  
 NIST'03 NIST'05 NIST'08
Baseline 38.57 38.21 27.52 
Struct-LP 38.79 38.52 28.06 
G-Re-Rank 39.21 38.93 28.18 
G-Re-Rank-GC 38.92 38.76 28.21 
G-Re-Rank-LC 38.90 38.65 27.88 
G-Decode 39.62 39.17 28.76 
G-Decode-GC 39.42 39.02 28.51 
G-Decode-LC 39.17 38.70 28.20 
Table 4. Consensus-based re-ranking and decoding 
for NIST data set. The results in bold type are 
significantly better than the baseline. 
We also conduct experiments on NIST data, and 
results are shown in Table 4. The consensus-based 
re-ranking methods are performed in the same way 
as for IWSLT data, but for consensus-based 
decoding, the data set contains too many sentence 
pairs to be held in one graph for our machine. We 
apply the method of Alexandrescu and Kirchhoff 
(2009) to construct separate graphs for each 
development and test sentence without losing 
global connectivity information. We perform 
modified label propagation with the separate 
graphs to get the graph-based consensus for n-best 
list of each sentence, and the graph-based 
consensus will be recorded for the MERT to tune 
the weights. 
From Table 4, we can see that, Struct-LP 
improves the performance slightly, but not 
significantly. Local consensus features (G-Re-
Rank-LC and G-Decode-LC) improve the 
performance slightly. The combination of graph-
based and local consensus features can improve 
the translation performance significantly on SMT 
re-ranking. With graph-based consensus features, 
G-Decode-GC achieves significant performance 
gain, and combined with local consensus features, 
G-Decode performance is improved farther. 
7 Conclusion and Future Work 
In this paper, we extend the consensus method by 
collecting consensus statistics, not only from 
translation candidates of the same source 
sentence/span, but also from those of similar ones. 
To calculate consensus statistics, we develop a 
novel structured label propagation method for 
structured learning problems, such as machine 
translation. Note that, the structured label 
propagation can be applied to other structured 
learning tasks, such as POS tagging and syntactic 
parsing. The consensus statistics are integrated into 
the conventional log-linear model as features. The 
features and weights are tuned with an iterative 
semi-supervised method. We conduct experiments 
on IWSLT and NIST data, and our method can 
improve the performance significantly. 
In this paper, we only tried Dice co-efficient of 
n-grams and symmetrical sentence level BLEU as 
similarity measures. In the future, we will explore 
other consensus features and other similarity 
measures, which may take document level 
information, or syntactic and semantic information 
into consideration. We also plan to introduce 
feature to model the similarity of the source 
309
sentences, which are reflected by only one score in 
our paper, and optimize the parameters with CRF 
model. 
References  
Andrei Alexandrescu, Katrin Kirchhoff. 2009. Graph-
based learning for statistical machine translation. In 
Proceedings of Human Language Technologies and 
Annual Conference of the North American Chapter 
of the ACL, pages 119-127. 
Peter L. Bertlett, Michael Collins, Ben Taskar and 
David McAllester. 2004. Exponentiated gradient 
algorithms for large-margin structured classification. 
In Proceedings of Advances in Neural Information 
Processing Systems. 
John DeNero, David Chiang, and Kevin Knight. 2009. 
Fast consensus decoding over translation forests. In 
Proceedings of the Association for Computational 
Linguistics, pages 567-575. 
John DeNero, Shankar Kumar, Ciprian Chelba and 
Franz Och. 2010. Model combination for machine 
translation. In Proceedings of the North American 
Association for Computational Linguistics, pages 
975-983. 
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 
2010. Mixture model-based minimum bayes risk 
decoding using multiple machine translation Systems. 
In Proceedings of the International Conference on 
Computational Linguistics, pages 313-321. 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing, pages 388-395. 
Shankar Kumar and William Byrne. 2004. Minimum 
bayes-risk decoding for statistical machine 
translation. In Proceedings of the North American 
Association for Computational Linguistics, pages 
169-176. 
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and 
Franz Och. 2009. Efficient minimum error rate 
training and minimum bayes-risk decoding for 
translation hypergraphs and lattices. In Proceedings 
of the Association for Computational Linguistics, 
pages 163-171. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009. Collaborative decoding: partial 
hypothesis re-ranking using translation consensus 
between decoders. In Proceedings of the Association 
for Computational Linguistics, pages 585-592. 
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and 
Ben Taskar. 2006. An end-to-end discriminative 
approach to machine translation. In Proceedings of 
the International Conference on Computational 
Linguistics and the ACL, pages 761-768 
Yanjun Ma, Yifan He, Andy Way, Josef van Genabith. 
2011. Consistent translation using discriminative 
learning: a translation memory-inspired approach. In 
Proceedings of the Association for Computational 
Linguistics, pages 1239-1248. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
160-167. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, pages 
311-318. 
Roy Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice minimum bayes-
risk decoding for statistical machine translation. In 
Proceedings of the Conference on Empirical 
Methods on Natural Language Processing, pages 
620-629. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3). 
Joern Wuebker, Arne Mauser and Hermann Ney. 2010. 
Training phrase translation models with leaving-one-
out. In Proceedings of the Association for 
Computational Linguistics, pages 475-484. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
521-528. 
Xiaojin Zhu. 2005. Semi-supervised learning with 
graphs. Ph.D. thesis, Carnegie Mellon University. 
CMU-LTI-05-192. 
 
 
310
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166?175,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Word Alignment Modeling with Context Dependent Deep Neural Network
Nan Yang1, Shujie Liu2, Mu Li2, Ming Zhou2, Nenghai Yu1
1University of Science and Technology of China, Hefei, China
2Microsoft Research Asia, Beijing, China
{v-nayang,shujliu,muli,mingzhou}@microsoft.com
ynh@ustc.edu.cn
Abstract
In this paper, we explore a novel bilin-
gual word alignment approach based on
DNN (Deep Neural Network), which has
been proven to be very effective in var-
ious machine learning tasks (Collobert
et al, 2011). We describe in detail
how we adapt and extend the CD-DNN-
HMM (Dahl et al, 2012) method intro-
duced in speech recognition to the HMM-
based word alignment model, in which
bilingual word embedding is discrimina-
tively learnt to capture lexical translation
information, and surrounding words are
leveraged to model context information
in bilingual sentences. While being ca-
pable to model the rich bilingual corre-
spondence, our method generates a very
compact model with much fewer parame-
ters. Experiments on a large scale English-
Chinese word alignment task show that the
proposed method outperforms the HMM
and IBM model 4 baselines by 2 points in
F-score.
1 Introduction
Recent years research communities have seen a
strong resurgent interest in modeling with deep
(multi-layer) neural networks. This trending topic,
usually referred under the name Deep Learning, is
started by ground-breaking papers such as (Hin-
ton et al, 2006), in which innovative training pro-
cedures of deep structures are proposed. Unlike
shallow learning methods, such as Support Vector
Machine, Conditional Random Fields, and Maxi-
mum Entropy, which need hand-craft features as
input, DNN can learn suitable features (represen-
tations) automatically with raw input data, given a
training objective.
DNN did not achieve expected success until
2006, when researchers discovered a proper way
to intialize and train the deep architectures, which
contains two phases: layer-wise unsupervised pre-
training and supervised fine tuning. For pre-
training, Restricted Boltzmann Machine (RBM)
(Hinton et al, 2006), auto-encoding (Bengio et al,
2007) and sparse coding (Lee et al, 2007) are pro-
posed and popularly used. The unsupervised pre-
training trains the network one layer at a time, and
helps to guide the parameters of the layer towards
better regions in parameter space (Bengio, 2009).
Followed by fine tuning in this region, DNN is
shown to be able to achieve state-of-the-art per-
formance in various area, or even better (Dahl et
al., 2012) (Kavukcuoglu et al, 2010). DNN also
achieved breakthrough results on the ImageNet
dataset for objective recognition (Krizhevsky et
al., 2012). For speech recognition, (Dahl et al,
2012) proposed context-dependent neural network
with large vocabulary, which achieved 16.0% rel-
ative error reduction.
DNN has also been applied in Natural Lan-
guage Processing (NLP) field. Most works con-
vert atomic lexical entries into a dense, low di-
mensional, real-valued representation, called word
embedding; Each dimension represents a latent as-
pect of a word, capturing its semantic and syntac-
tic properties (Bengio et al, 2006). Word embed-
ding is usually first learned from huge amount of
monolingual texts, and then fine-tuned with task-
specific objectives. (Collobert et al, 2011) and
(Socher et al, 2011) further apply Recursive Neu-
ral Networks to address the structural prediction
tasks such as tagging and parsing, and (Socher
et al, 2012) explores the compositional aspect of
word representations.
Inspired by successful previous works, we pro-
pose a new DNN-based word alignment method,
which exploits contextual and semantic similari-
ties between words. As shown in example (a) of
Figure 1, in word pair {?juda? ??mammoth?},
the Chinese word ?juda? is a common word, but
166
mammothwill be a
jiang shi yixiang juda gongcheng
job
(a)
? ? ?? ?? ??
A :farmer Yibula said
nongmin yibula shuo : ?
?
(b)
?? ??? ?
Figure 1: Two examples of word alignment
the English word ?mammoth? is not, so it is very
hard to align them correctly. If we know that
?mammoth? has the similar meaning with ?big?,
or ?huge?, it would be easier to find the corre-
sponding word in the Chinese sentence. As we
mentioned in the last paragraph, word embedding
(trained with huge monolingual texts) has the abil-
ity to map a word into a vector space, in which,
similar words are near each other.
For example (b) in Figure 1, for the word pair
{?yibula? ? ?Yibula?}, both the Chinese word
?yibula? and English word ?Yibula? are rare name
entities, but the words around them are very com-
mon, which are {?nongmin?, ?shuo?} for Chinese
side and {?farmer?, ?said?} for the English side.
The pattern of the context {?nongmin X shuo?
? ?farmer X said?} may help to align the word
pair which fill the variableX , and also, the pattern
{?yixiang X gongcheng?? ?a X job?} is helpful
to align the word pair {?juda???mammoth?} for
example (a).
Based on the above analysis, in this paper, both
the words in the source and target sides are firstly
mapped to a vector via a discriminatively trained
word embeddings, and word pairs are scored by a
multi-layer neural network which takes rich con-
texts (surrounding words on both source and target
sides) into consideration; and a HMM-like distor-
tion model is applied on top of the neural network
to characterize structural aspect of bilingual sen-
tences.
In the rest of this paper, related work about
DNN and word alignment are first reviewed in
Section 2, followed by a brief introduction of
DNN in Section 3. We then introduce the details
of leveraging DNN for word alignment, including
the details of our network structure in Section 4
and the training method in Section 5. The mer-
its of our approach are illustrated with the experi-
ments described in Section 6, and we conclude our
paper in Section 7.
2 Related Work
DNN with unsupervised pre-training was firstly
introduced by (Hinton et al, 2006) for MNIST
digit image classification problem, in which, RBM
was introduced as the layer-wise pre-trainer. The
layer-wise pre-training phase found a better local
maximum for the multi-layer network, thus led to
improved performance. (Krizhevsky et al, 2012)
proposed to apply DNN to do object recognition
task (ImageNet dataset), which brought down the
state-of-the-art error rate from 26.1% to 15.3%.
(Seide et al, 2011) and (Dahl et al, 2012) apply
Context-Dependent Deep Neural Network with
HMM (CD-DNN-HMM) to speech recognition
task, which significantly outperforms traditional
models.
Most methods using DNN in NLP start with a
word embedding phase, which maps words into
a fixed length, real valued vectors. (Bengio et
al., 2006) proposed to use multi-layer neural net-
work for language modeling task. (Collobert et al,
2011) applied DNN on several NLP tasks, such
as part-of-speech tagging, chunking, name entity
recognition, semantic labeling and syntactic pars-
ing, where they got similar or even better results
than the state-of-the-art on these tasks. (Niehues
and Waibel, 2012) shows that machine transla-
tion results can be improved by combining neural
language model with n-gram traditional language.
(Son et al, 2012) improves translation quality of
n-gram translation model by using a bilingual neu-
ral language model. (Titov et al, 2012) learns a
context-free cross-lingual word embeddings to fa-
cilitate cross-lingual information retrieval.
For the related works of word alignment, the
most popular methods are based on generative
models such as IBM Models (Brown et al, 1993)
and HMM (Vogel et al, 1996). Discriminative ap-
proaches are also proposed to use hand crafted fea-
tures to improve word alignment. Among them,
(Liu et al, 2010) proposed to use phrase and rule
pairs to model the context information in a log-
linear framework. Unlike previous discriminative
methods, in this work, we do not resort to any hand
crafted features, but use DNN to induce ?features?
from raw words.
167
3 DNN structures for NLP
The most important and prevalent features avail-
able in NLP are the words themselves. To ap-
ply DNN to NLP task, the first step is to trans-
form a discrete word into its word embedding, a
low dimensional, dense, real-valued vector (Ben-
gio et al, 2006). Word embeddings often implic-
itly encode syntactic or semantic knowledge of
the words. Assuming a finite sized vocabulary V ,
word embeddings form a (L?|V |)-dimension em-
bedding matrix WV , where L is a pre-determined
embedding length; mapping words to embed-
dings is done by simply looking up their respec-
tive columns in the embedding matrix WV . The
lookup process is called a lookup layer LT , which
is usually the first layer after the input layer in neu-
ral network.
After words have been transformed to their em-
beddings, they can be fed into subsequent classi-
cal network layers to model highly non-linear re-
lations:
zl = fl(M lzl?1 + bl) (1)
where zl is the output of lth layer, M l is a |zl| ?
|zl?1| matrix, bl is a |zl|-length vector, and fl
is an activation function. Except for the last
layer, fl must be non-linear. Common choices for
fl include sigmoid function, hyperbolic function,
?hard? hyperbolic function etc. Following (Col-
lobert et al, 2011), we choose ?hard? hyperbolic
function as our activation function in this work:
htanh(x) =
?
?
?
1 if x is greater than 1
?1 if x is less than -1
x otherwise
(2)
If probabilistic interpretation is desired, a softmax
layer (Bridle, 1990) can be used to do normaliza-
tion:
zli =
ezl?1i
|zl|?
j=1
ez
l?1
j
(3)
The above layers can only handle fixed sized in-
put and output. If input must be of variable length,
convolution layer and max layer can be used, (Col-
lobert et al, 2011) which transform variable length
input to fixed length vector for further processing.
Multi-layer neural networks are trained with
the standard back propagation algorithm (LeCun,
1985). As the networks are non-linear and the
task specific objectives usually contain many lo-
cal maximums, special care must be taken in the
optimization process to obtain good parameters.
Techniques such as layerwise pre-training(Bengio
et al, 2007) and many tricks(LeCun et al, 1998)
have been developed to train better neural net-
works. Besides that, neural network training also
involves some hyperparameters such as learning
rate, the number of hidden layers. We will address
these issues in section 4.
4 DNN for word alignment
Our DNN word alignment model extends classic
HMM word alignment model (Vogel et al, 1996).
Given a sentence pair (e, f), HMM word alignment
takes the following form:
P (a, e|f) =
|e|?
i=1
Plex(ei|fai)Pd(ai ? ai?1) (4)
where Plex is the lexical translation probability
and Pd is the jump distance distortion probability.
One straightforward way to integrate DNN
into HMM is to use neural network to compute
the emission (lexical translation) probability Plex.
Such approach requires a softmax layer in the neu-
ral network to normalize over all words in source
vocabulary. As vocabulary for natural languages
is usually very large, it is prohibitively expen-
sive to do the normalization. Hence we give up
the probabilistic interpretation and resort to a non-
probabilistic, discriminative view:
sNN (a|e, f) =
|e|?
i=1
tlex(ei, fai |e, f)td(ai, ai?1|e, f)
(5)
where tlex is a lexical translation score computed
by neural network, and td is a distortion score.
In the classic HMM word alignment model,
context is not considered in the lexical translation
probability. Although we can rewrite Plex(ei|fai)
to Plex(ei|context of fai) to model context, it in-
troduces too many additional parameters and leads
to serious over-fitting problem due to data sparse-
ness. As a matter of fact, even without any con-
texts, the lexical translation table in HMM al-
ready contains O(|Ve| ? |Vf |) parameters, where
|Ve| and Vf denote source and target vocabulary
sizes. In contrast, our model does not maintain
a separate translation score parameters for every
source-target word pair, but computes tlex through
a multi-layer network, which naturally handles
contexts on both sides without explosive growth
of number of parameters.
168
Input
Source window e Target window f
)( 323 bzM ??
)( 212 bzM ??
ii-1 i+1 j-1 j j+1
Lookup 
LT
0zLayer f1 1zLayer f2 2z
?? ??? ? farmer yibula said
)( 101 bzM ??htanh
htanh
Layer f3 ),|,( fefet jilex
Figure 2: Network structure for computing context
dependent lexical translation scores. The example
computes translation score for word pair (yibula,
yibulayin) given its surrounding context.
Figure 2 shows the neural network we used
to compute context dependent lexical transla-
tion score tlex. For word pair (ei, fj), we take
fixed length windows surrounding both ei and fj
as input: (ei? sw2 , . . . , ei+ sw2 , fj? tw2 , . . . , fj+ tw2 ),where sw, tw stand window sizes on source and
target side respectively. Words are converted to
embeddings using the lookup table LT , and the
catenation of embeddings are fed to a classic neu-
ral network with two hidden-layers, and the output
of the network is the our lexical translation score:
tlex(ei, fj |e, f)
= f3 ? f2 ? f1 ? LT (window(ei), window(fj))
(6)
f1 and f2 layers use htanh as activation functions,
while f3 is only a linear transformation with no
activation function.
For the distortion td, we could use a lexicalized
distortion model:
td(ai, ai?1|e, f) = td(ai ? ai?1|window(fai?1))
(7)
which can be computed by a neural network sim-
ilar to the one used to compute lexical transla-
tion scores. If we map jump distance (ai ? ai?1)
to B buckets, we can change the length of the
output layer to B, where each dimension in the
output stands for a different bucket of jump dis-
tances. But we found in our initial experiments
on small scale data, lexicalized distortion does not
produce better alignment over the simple jump-
distance based model. So we drop the lexicalized
distortion and reverse to the simple version:
td(ai, ai?1|e, f) = td(ai ? ai?1) (8)
Vocabulary V of our alignment model consists
of a source vocabulary Ve and a target vocabu-
lary Vf . As in (Collobert et al, 2011), in addition
to real words, each vocabulary contains a special
unknown word symbol ?unk? to handle unseen
words; two sentence boundary symbols ?s? and
?/s?, which are filled into surrounding window
when necessary; furthermore, to handle null align-
ment, we must also include a special null symbol
?null?. When fj is null word, we simply fill the
surrounding window with the identical null sym-
bols.
To decode our model, the lexical translation
scores are computed for each source-target word
pair in the sentence pair, which requires going
through the neural network (|e| ? |f|) times; af-
ter that, the forward-backward algorithm can be
used to find the viterbi path as in the classic HMM
model.
The majority of tunable parameters in our
model resides in the lookup table LT , which is
a (L ? (|Ve| + |Vf |))-dimension matrix. For a
reasonably large vocabulary, the number is much
smaller than the number of parameters in classic
HMM model, which is in the order of (|Ve|?|Vf |).
1
The ability to model context is not unique to
our model. In fact, discriminative word alignment
can model contexts by deploying arbitrary features
(Moore, 2005). Different from previous discrim-
inative word alignment, our model does not use
manually engineered features, but learn ?features?
automatically from raw words by the neural net-
work. (Berger et al, 1996) use a maximum en-
tropy model to model the bag-of-words context for
word alignment, but their model treats each word
as a distinct feature, which can not leverage the
similarity between words as our model.
5 Training
Although unsupervised training technique such as
Contrastive Estimation as in (Smith and Eisner,
2005), (Dyer et al, 2011) can be adapted to train
1In practice, the number of non-zero parameters in clas-
sic HMM model would be much smaller, as many words do
not co-occur in bilingual sentence pairs. In our experiments,
the number of non-zero parameters in classic HMM model
is about 328 millions, while the NN model only has about 4
millions.
169
our model from raw sentence pairs, they are too
computational demanding as the lexical transla-
tion probabilities must be computed from neu-
ral networks. Hence, we opt for a simpler su-
pervised approach, which learns the model from
sentence pairs with word alignment. As we do
not have a large manually word aligned corpus,
we use traditional word alignment models such as
HMM and IBM model 4 to generate word align-
ment on a large parallel corpus. We obtain bi-
directional alignment by running the usual grow-
diag-final heuristics (Koehn et al, 2003) on uni-
directional results from both directions, and use
the results as our training data. Similar approach
has been taken in speech recognition task (Dahl et
al., 2012), where training data for neural network
model is generated by forced decoding with tradi-
tional Gaussian mixture models.
Tunable parameters in neural network align-
ment model include: word embeddings in lookup
table LT , parametersW l, bl for linear transforma-
tions in the hidden layers of the neural network,
and distortion parameters sd of jump distance. We
take the following ranking loss with margin as our
training criteria:
loss(?) =
?
every (e,f)
max{0, 1? s?(a+|e, f) + s?(a?|e, f)}
(9)
where ? denotes all tunable parameters, a+ is
the gold alignment path, a? is the highest scor-
ing incorrect alignment path under ?, and s? is
model score for alignment path defined in Eq. 5
. One nuance here is that the gold alignment af-
ter grow-diag-final contains many-to-many links,
which cannot be generated by any path. Our solu-
tion is that for each source word alignment multi-
ple target, we randomly choose one link among all
candidates as the golden link.
Because our multi-layer neural network is in-
herently non-linear and is non-convex, directly
training against the above criteria is unlikely to
yield good results. Instead, we take the following
steps to train our model.
5.1 Pre-training initial word embedding with
monolingual data
Most parameters reside in the word embeddings.
To get a good initial value, the usual approach is
to pre-train the embeddings on a large monolin-
gual corpus. We replicate the work in (Collobert
et al, 2011) and train word embeddings for source
and target languages from their monolingual cor-
pus respectively. Our vocabularies Vs and Vt con-
tain the most frequent 100,000 words from each
side of the parallel corpus, and all other words are
treated as unknown words. We set word embed-
ding length to 20, window size to 5, and the length
of the only hidden layer to 40. Follow (Turian et
al., 2010), we randomly initialize all parameters
to [-0.1, 0.1], and use stochastic gradient descent
to minimize the ranking loss with a fixed learn-
ing rate 0.01. Note that embedding for null word
in either Ve and Vf cannot be trained from mono-
lingual corpus, and we simply leave them at the
initial value untouched.
Word embeddings from monolingual corpus
learn strong syntactic knowledge of each word,
which is not always desirable for word align-
ment between some language pairs like English
and Chinese. For example, many Chinese words
can act as a verb, noun and adjective without any
change, while their English counter parts are dis-
tinct words with quite different word embeddings
due to their different syntactic roles. Thus we
have to modify the word embeddings in subse-
quent steps according to bilingual data.
5.2 Training neural network based on local
criteria
Training the network against the sentence level
criteria Eq. 5 directly is not efficient. Instead, we
first ignore the distortion parameters and train neu-
ral networks for lexical translation scores against
the following local pairwise loss:
max{0, 1? t?((e, f)+|e, f) + t?((e, f)?|e, f)}
(10)
where (e, f)+ is a correct word pair, (e, f)? is a
wrong word pair in the same sentence, and t? is as
defined in Eq. 6 . This training criteria essentially
means our model suffers loss unless it gives cor-
rect word pairs a higher score than random pairs
from the same sentence pair with some margin.
We initialize the lookup table with embed-
dings obtained from monolingual training, and
randomly initialize all W l and bl in linear layers
to [-0.1, 0.1]. We minimize the loss using stochas-
tic gradient descent as follows. We randomly cy-
cle through all sentence pairs in training data; for
each correct word pair (including null alignment),
we generate a positive example, and generate two
negative examples by randomly corrupting either
170
side of the pair with another word in the sentence
pair. We set learning rate to 0.01. As there is no
clear stopping criteria, we simply run the stochas-
tic optimizer through parallel corpus for N itera-
tions. In this work, N is set to 50.
To make our model concrete, there are still
hyper-parameters to be determined: the window
size sw and tw, the length of each hidden layer
Ll. We empirically set sw and tw to 11, L1 to
120, and L2 to 10, which achieved a minimal loss
on a small held-out data among several settings we
tested.
5.3 Training distortion parameters
We fix neural network parameters obtained from
the last step, and tune the distortion parameters
sd with respect to the sentence level loss using
standard stochastic gradient descent. We use a
separate parameter for jump distance from -7 and
7, and another two parameters for longer for-
ward/backward jumps. We initialize all parame-
ters in sd to 0, set the learning rate for the stochas-
tic optimizer to 0.001. As there are only 17 param-
eters in sd, we only need to run the optimizer over
a small portion of the parallel corpus.
5.4 Tuning neural network based on sentence
level criteria
Up-to-now, parameters in the lexical translation
neural network have not been trained against the
sentence level criteria Eq. 5. We could achieve
this by re-using the same online training method
used to train distortion parameters, except that we
now fix the distortion parameters and let the loss
back-propagate through the neural networks. Sen-
tence level training does not take larger context in
modeling word translations, but only to optimize
the parameters regarding to the sentence level loss.
This tuning is quite slow, and it did not improve
alignment on an initial small scale experiment; so,
we skip this step in all subsequent experiment in
this work.
6 Experiments and Results
We conduct our experiment on Chinese-to-English
word alignment task. We use the manually aligned
Chinese-English alignment corpus (Haghighi et
al., 2009) which contains 491 sentence pairs as
test set. We adapt the segmentation on the Chinese
side to fit our word segmentation standard.
6.1 Data
Our parallel corpus contains about 26 million
unique sentence pairs in total which are mined
from web.
The monolingual corpus to pre-train word em-
beddings are also crawled from web, which
amounts to about 1.1 billion unique sentences for
English and about 300 million unique sentences
for Chinese. As pre-processing, we lowercase all
English words, and map all numbers to one spe-
cial token; and we also map all email addresses
and URLs to another special token.
6.2 Settings
We use classic HMM and IBM model 4 as our
baseline, which are generated by Giza++ (Och and
Ney, 2000). We train our proposed model from re-
sults of classic HMM and IBM model 4 separately.
Since classic HMM, IBM model 4 and our model
are all uni-directional, we use the standard grow-
diag-final to generate bi-directional results for all
models.
Models are evaluated on the manually aligned
test set using standard metric: precision, recall and
F1-score.
6.3 Alignment Result
It can be seen from Table 1, the proposed model
consistently outperforms its corresponding base-
line whether it is trained from alignment of classic
HMM or IBM model 4. It is also clear that the
setting prec. recall F-1
HMM 0.768 0.786 0.777
HMM+NN 0.810 0.790 0.798
IBM4 0.839 0.805 0.822
IBM4+NN 0.885 0.812 0.847
Table 1: Word alignment result. The first row
and third row show baseline results obtained by
classic HMM and IBM4 model. The second row
and fourth row show results of the proposed model
trained from HMM and IBM4 respectively.
results of our model also depends on the quality
of baseline results, which is used as training data
of our model. In future we would like to explore
whether our method can improve other word align-
ment models.
We also conduct experiment to see the effect
on end-to-end SMT performance. We train hier-
171
archical phrase model (Chiang, 2007) from dif-
ferent word alignments. Despite different align-
ment scores, we do not obtain significant differ-
ence in translation performance. In our C-E exper-
iment, we tuned on NIST-03, and tested on NIST-
08. Case-insensitive BLEU-4 scores on NIST-08
test are 0.305 and 0.307 for models trained from
IBM-4 and NN alignment results. The result is not
surprising considering our parallel corpus is quite
large, and similar observations have been made in
previous work as (DeNero and Macherey, 2011)
that better alignment quality does not necessarily
lead to better end-to-end result.
6.4 Result Analysis
6.4.1 Error Analysis
From Table 1 we can see higher F-1 score of our
model mainly comes from higher precision, with
recall similar to baseline. By analyzing the results,
we found out that for both baseline and our model,
a large part of missing alignment links involves
stop words like English words ?the?, ?a?, ?it? and
Chinese words ?de?. Stop words are inherently
hard to align, which often requires grammatical
judgment unavailable to our models; as they are
also extremely frequent, our model fully learns
their alignment patterns of the baseline models,
including errors. On the other hand, our model
performs better on low-frequency words, espe-
cially proper nouns. Take person names for ex-
ample. Most names are low-frequency words, on
which baseline HMM and IBM4 models show the
?garbage collector? phenomenon. In our model,
different person names have very similar word em-
beddings on both English side and Chinese side,
due to monolingual pre-training; what is more, dif-
ferent person names often appear in similar con-
texts. As our model considers both word embed-
dings and contexts, it learns that English person
names should be aligned to Chinese person names,
which corrects errors of baseline models and leads
to better precision.
6.4.2 Effect of context
To examine how context contribute to alignment
quality, we re-train our model with different win-
dow size, all from result of IBM model 4. From
Figure 3, we can see introducing context increase
the quality of the learned alignment, but the ben-
efit is diminished for window size over 5. On the
other hand, the results are quite stable even with
large window size 13, without noticeable over-
0.740.76
0.780.8
0.820.84
0.86
1 3 5 7 9 11 13
Figure 3: Effect of different window sizes on word
alignment F-score.
fitting problem. This is not surprising consider-
ing that larger window size only requires slightly
more parameters in the linear layers. Lastly, it
is worth noticing that our model with no context
(window size 1) performs much worse than set-
tings with larger window size and baseline IBM4.
Our explanation is as follows. Our model uses
the simple jump distance based distortion, which
is weaker than the more sophisticated distortions
in IBM model 4; thus without context, it does not
perform well compared to IBM model 4. With
larger window size, our model is able to produce
more accurate translation scores based on more
contexts, which leads to better alignment despite
the simpler distortions.
IBM4+NN F-1
1-hidden-layer 0.834
2-hidden-layer 0.847
3-hidden-layer 0.843
Table 3: Effect of different number of hidden lay-
ers. Two hidden layers outperform one hidden
layer, while three hidden layers do not bring fur-
ther improvement.
6.4.3 Effect of number of hidden layers
Our neural network contains two hidden layers be-
sides the lookup layer. It is natural to ask whether
adding more layers would be beneficial. To an-
swer this question, we train models with 1, 2 and
3 layers respectively, all from result of IBM model
4. For 1-hidden-layer setting, we set the hidden
layer length to 120; and for 3-hidden-layer set-
ting, we set hidden layer lengths to 120, 100, 10
respectively. As can be seen from Table 3, 2-
hidden-layer outperforms the 1-hidden-layer set-
ting, while another hidden layer does not bring
172
word good history british served labs zetian laggards
LM
bad tradition russian worked networks hongzhang underperformers
great culture japanese lived technologies yaobang transferees
strong practice dutch offered innovations keming megabanks
true style german delivered systems xingzhi mutuals
easy literature canadian produced industries ruihua non-starters
WA
nice historical uk offering lab hongzhang underperformers
great historic britain serving laboratories qichao illiterates
best developed english serve laboratory xueqin transferees
pretty record classic delivering exam fuhuan matriculants
excellent recording england worked experiments bingkun megabanks
Table 2: Nearest neighbors of several words according to their embedding distance. LM shows neighbors
of word embeddings trained by monolingual language model method; WA shows neighbors of word
embeddings trained by our word alignment model.
improvement. Due to time constraint, we have
not tuned the hyper-parameters such as length of
hidden layers in 1 and 3-hidden-layer settings, nor
have we tested settings with more hidden-layers.
It would be wise to test more settings to verify
whether more layers would help.
6.4.4 Word Embedding
Following (Collobert et al, 2011), we show some
words together with its nearest neighbors using the
Euclidean distance between their embeddings. As
we can see from Table 2, after bilingual training,
?bad? is no longer in the nearest neighborhood of
?good? as they hold opposite semantic meanings;
the nearest neighbor of ?history? is now changed
to its related adjective ?historical?. Neighbors of
proper nouns such as person names are relatively
unchanged. For example, neighbors of word
?zetian? are all Chinese names in both settings.
As Chinese language lacks morphology, the single
form and plural form of a noun in English often
correspond to the same Chinese word, thus it is
desirable that the two English words should have
similar word embeddings. While this is true for
relatively frequent nouns such as ?lab? and ?labs?,
rarer nouns still remain near their monolingual
embeddings as they are only modified a few times
during the bilingual training. As shown in last
column, neighborhood of ?laggards? still consists
of other plural forms even after bilingual training.
7 Conclusion
In this paper, we explores applying deep neu-
ral network for word alignment task. Our model
integrates a multi-layer neural network into an
HMM-like framework, where context dependent
lexical translation score is computed by neural
network, and distortion is modeled by a sim-
ple jump-distance scheme. Our model is dis-
criminatively trained on bilingual corpus, while
huge monolingual data is used to pre-train word-
embeddings. Experiments on large-scale Chinese-
to-English task show that the proposed method
produces better word alignment results, compared
with both classic HMM model and IBM model 4.
For future work, we will investigate more set-
tings of different hyper-parameters in our model.
Secondly, we want to explore the possibility of
unsupervised training of our neural word align-
ment model, without reliance of alignment result
of other models. Furthermore, our current model
use rather simple distortions; it might be helpful
to use more sophisticated model such as ITG (Wu,
1997), which can be modeled by Recursive Neural
Networks (Socher et al, 2011).
Acknowledgments
We thank anonymous reviewers for insightful
comments. We also thank Dongdong Zhang, Lei
Cui, Chunyang Wu and Zhenyan He for fruitful
discussions.
References
Yoshua Bengio, Holger Schwenk, Jean-Se?bastien
Sene?cal, Fre?deric Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. Inno-
vations in Machine Learning, pages 137?186.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
173
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. Advances in neural information
processing systems, 19:153.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and Trends R? in Machine Learning,
2(1):1?127.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
JS Bridle. 1990. Neurocomputing: Algorithms, archi-
tectures and applications, chapter probabilistic inter-
pretation of feedforward classification network out-
puts, with relationships to statistical pattern recogni-
tion.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE
Transactions on, 20(1):30?42.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proc. ACL.
Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A
Smith. 2011. Unsupervised word alignment with ar-
bitrary features. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 409?419. Association for Computational Lin-
guistics.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with su-
pervised itg models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 923?931. Association for Compu-
tational Linguistics.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527?1554.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Michae?l Mathieu, and Yann LeCun.
2010. Learning convolutional feature hierarchies for
visual recognition. Advances in Neural Information
Processing Systems, pages 1090?1098.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Yann LeCun, Le?on Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278?2324.
Yann LeCun. 1985. A learning scheme for asymmet-
ric threshold networks. Proceedings of Cognitiva,
85:599?604.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y Ng. 2007. Efficient sparse coding algo-
rithms. Advances in neural information processing
systems, 19:801.
Shujie Liu, Chi-Ho Li, and Ming Zhou. 2010. Dis-
criminative pruning for discriminative itg alignment.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL, vol-
ume 10, pages 316?324.
Y MarcAurelio Ranzato, Lan Boureau, and Yann Le-
Cun. 2007. Sparse feature learning for deep belief
networks. Advances in neural information process-
ing systems, 20:1185?1192.
Robert C Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzmann
machines. In Proceedings of the nineth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models.
Frank Seide, Gang Li, and Dong Yu. 2011. Conversa-
tional speech transcription using context-dependent
deep neural networks. In Proc. Interspeech, pages
437?440.
174
Noah A Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
354?362. Association for Computational Linguis-
tics.
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neu-
ral networks. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
volume 2, page 7.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39?48. Association for Compu-
tational Linguistics.
Ivan Titov, Alexandre Klementiev, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Urbana, 51:61801.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
175
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 30?34,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning Entity Representation for Entity Disambiguation
Zhengyan He? Shujie Liu? Mu Li? Ming Zhou? Longkai Zhang? Houfeng Wang??
? Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
? Microsoft Research Asia
hezhengyan.hit@gmail.com {shujliu,muli,mingzhou}@microsoft.com
zhlongk@qq.com wanghf@pku.edu.cn
Abstract
We propose a novel entity disambigua-
tion model, based on Deep Neural Net-
work (DNN). Instead of utilizing simple
similarity measures and their disjoint com-
binations, our method directly optimizes
document and entity representations for a
given similarity measure. Stacked Denois-
ing Auto-encoders are first employed to
learn an initial document representation in
an unsupervised pre-training stage. A su-
pervised fine-tuning stage follows to opti-
mize the representation towards the simi-
larity measure. Experiment results show
that our method achieves state-of-the-art
performance on two public datasets with-
out any manually designed features, even
beating complex collective approaches.
1 Introduction
Entity linking or disambiguation has recently re-
ceived much attention in natural language process-
ing community (Bunescu and Pasca, 2006; Han
et al, 2011; Kataria et al, 2011; Sen, 2012). It is
an essential first step for succeeding sub-tasks in
knowledge base construction (Ji and Grishman,
2011) like populating attribute to entities. Given
a sentence with four mentions, ?The [[Python]] of
[[Delphi]] was a creature with the body of a snake.
This creature dwelled on [[Mount Parnassus]], in
central [[Greece]].? How can we determine that
Python is an earth-dragon in Greece mythology
and not the popular programming language, Del-
phi is not the auto parts supplier, and Mount Par-
nassus is in Greece, not in Colorado?
A most straightforward method is to compare
the context of the mention and the definition of
candidate entities. Previous work has explored
many ways of measuring the relatedness of context
?Corresponding author
d and entity e, such as dot product, cosine similar-
ity, Kullback-Leibler divergence, Jaccard distance,
or more complicated ones (Zheng et al, 2010;
Kulkarni et al, 2009; Hoffart et al, 2011; Bunescu
and Pasca, 2006; Cucerzan, 2007; Zhang et al,
2011). However, these measures are often dupli-
cate or over-specified, because they are disjointly
combined and their atomic nature determines that
they have no internal structure.
Another line of work focuses on collective dis-
ambiguation (Kulkarni et al, 2009; Han et al,
2011; Ratinov et al, 2011; Hoffart et al, 2011).
Ambiguous mentions within the same context are
resolved simultaneously based on the coherence
among decisions. Collective approaches often un-
dergo a non-trivial decision process. In fact, (Rati-
nov et al, 2011) show that even though global ap-
proaches can be improved, local methods based on
only similarity sim(d, e) of context d and entity e
are hard to beat. This somehow reveals the impor-
tance of a good modeling of sim(d, e).
Rather than learning context entity associa-
tion at word level, topic model based approaches
(Kataria et al, 2011; Sen, 2012) can learn it in
the semantic space. However, the one-topic-per-
entity assumption makes it impossible to scale to
large knowledge base, as every entity has a sepa-
rate word distribution P (w|e); besides, the train-
ing objective does not directly correspond with
disambiguation performances.
To overcome disadvantages of previous ap-
proaches, we propose a novel method to learn con-
text entity association enriched with deep architec-
ture. Deep neural networks (Hinton et al, 2006;
Bengio et al, 2007) are built in a hierarchical man-
ner, and allow us to compare context and entity
at some higher level abstraction; while at lower
levels, general concepts are shared across entities,
resulting in compact models. Moreover, to make
our model highly correlated with disambiguation
performance, our method directly optimizes doc-
30
ument and entity representations for a fixed simi-
larity measure. In fact, the underlying representa-
tions for computing similarity measure add inter-
nal structure to the given similarity measure. Fea-
tures are learned leveraging large scale annotation
of Wikipedia, without any manual design efforts.
Furthermore, the learned model is compact com-
pared with topic model based approaches, and can
be trained discriminatively without relying on ex-
pensive sampling strategy. Despite its simplicity,
it beats all complex collective approaches in our
experiments. The learned similarity measure can
be readily incorporated into any existing collective
approaches, which further boosts performance.
2 Learning Representation for
Contextual Document
Given a mention string m with its context docu-
ment d, a list of candidate entities C(m) are gen-
erated form, for each candidate entity ei ? C(m),
we compute a ranking score sim(dm, ei) indicat-
ing how likely m refers to ei. The linking result is
e = argmaxei sim(dm, ei).
Our algorithm consists of two stages. In the pre-
training stage, Stacked Denoising Auto-encoders
are built in an unsupervised layer-wise fashion to
discover general concepts encoding d and e. In the
supervised fine-tuning stage, the entire network
weights are fine-tuned to optimize the similarity
score sim(d, e).
2.1 Greedy Layer-wise Pre-training
Stacked Auto-encoders (Bengio et al, 2007) is
one of the building blocks of deep learning. As-
sume the input is a vector x, an auto-encoder con-
sists of an encoding process h(x) and a decod-
ing process g(h(x)). The goal is to minimize the
reconstruction error L(x, g(h(x))), thus retaining
maximum information. By repeatedly stacking
new auto-encoder on top of previously learned
h(x), stacked auto-encoders are obtained. This
way we learn multiple levels of representation of
input x.
One problem of auto-encoder is that it treats all
words equally, no matter it is a function word or
a content word. Denoising Auto-encoder (DA)
(Vincent et al, 2008) seeks to reconstruct x given
a random corruption x? of x. DA can capture global
structure while ignoring noise as the author shows
in image processing. In our case, we input each
document as a binary bag-of-words vector (Fig.
1). DA will capture general concepts and ignore
noise like function words. By applying masking
noise (randomly mask 1 with 0), the model also
exhibits a fill-in-the-blank property (Vincent et
al., 2010): the missing components must be re-
covered from partial input. Take ?greece? for ex-
ample, the model must learn to predict it with
?python? ?mount?, through some hidden unit. The
hidden unit may somehow express the concept of
Greece mythology.
h(x)
g(h(x))
pythondragon delphicoding ... greecemountsnake phd
reconstruct input
reconstruct randomzero nodenot reconstruct
inactive
active, but mask out
active
Figure 1: DA and reconstruction sampling.
In order to distinguish between a large num-
ber of entities, the vocabulary size must be large
enough. This adds considerable computational
overhead because the reconstruction process in-
volves expensive dense matrix multiplication. Re-
construction sampling keeps the sparse property
of matrix multiplication by reconstructing a small
subset of original input, with no loss of quality of
the learned representation (Dauphin et al, 2011).
2.2 Supervised Fine-tuning
This stage we optimize the learned representation
(?hidden layer n? in Fig. 2) towards the ranking
score sim(d, e), with large scale Wikipedia an-
notation as supervision. We collect hyperlinks in
Wikipedia as our training set {(di, ei,mi)}, where
mi is the mention string for candidate generation.
The network weights below ?hidden layer n? are
initialized with the pre-training stage.
Next, we stack another layer on top of the
learned representation. The whole network is
tuned by the final supervised objective. The reason
to stack another layer on top of the learned rep-
resentation, is to capture problem specific struc-
tures. Denote the encoding of d and e as d? and
e? respectively, after stacking the problem-specific
layer, the representation for d is given as f(d) =
sigmoid(W ? d? + b), where W and b are weight
and bias term respectively. f(e) follows the same
31
encoding process.
The similarity score of (d, e) pair is defined as
the dot product of f(d) and f(e) (Fig. 2):
sim(d, e) = Dot(f(d), f(e)) (1)
<.,.>
f(d) f(e)
hidden layer n
stacked auto-encoder
sim(d,e)
Figure 2: Network structure of fine-tuning stage.
Our goal is to rank the correct entity higher
than the rest candidates relative to the context of
the mention. For each training instance (d, e), we
contrast it with one of its negative candidate pair
(d, e?). This gives the pairwise ranking criterion:
L(d, e) = max{0, 1? sim(d, e) + sim(d, e?)}
(2)
Alternatively, we can contrast with all its candi-
date pairs (d, ei). That is, we raise the similarity
score of true pair sim(d, e) and penalize all the
rest sim(d, ei). The loss function is defined as
negative log of softmax function:
L(d, e) = ? log exp sim(d, e)?
ei?C(m) exp sim(d, ei)
(3)
Finally, we seek to minimize the following train-
ing objective across all training instances:
L =
?
d,e
L(d, e) (4)
The loss function is closely related to con-
trastive estimation (Smith and Eisner, 2005),
which defines where the positive example takes
probability mass from. We find that by penaliz-
ing more negative examples, convergence speed
can be greatly accelerated. In our experiments, the
softmax loss function consistently outperforms
pairwise ranking loss function, which is taken as
our default setting.
However, the softmax training criterion adds
additional computational overhead when per-
forming mini-batch Stochastic Gradient Descent
(SGD). Although we can use a plain SGD (i.e.
mini-batch size is 1), mini-batch SGD is faster to
converge and more stable. Assume the mini-batch
size ism and the number of candidates is n, a total
of m ? n forward-backward passes over the net-
work are performed to compute a similarity ma-
trix (Fig. 3), while pairwise ranking criterion only
needs 2?m. We address this problem by grouping
training pairs with same mentionm into one mini-
batch {(d, ei)|ei ? C(m)}. Observe that if candi-
date entities overlap, they share the same forward-
backward path. Only m + n forward-backward
passes are needed for each mini-batch now.
Python (programming language)PythonidaePython (mythology)
... ...
... ...
... ...
d0
d1 ...dm ... =sim(d,e)
e0 e1 e2 en
Figure 3: Sharing path within mini-batch.
The re-organization of mini-batch is similar
in spirit to Backpropagation Through Structure
(BTS) (Goller and Kuchler, 1996). BTS is a vari-
ant of the general backpropagation algorithm for
structured neural network. In BTS, parent node
is computed with its child nodes at the forward
pass stage; child node receives gradient as the sum
of derivatives from all its parents. Here (Fig. 2),
parent node is the score node sim(d, e) and child
nodes are f(d) and f(e). In Figure 3, each row
shares forward path of f(d) while each column
shares forward path of f(e). At backpropagation
stage, gradient is summed over each row of score
nodes for f(d) and over each column for f(e).
Till now, our input simply consists of bag-of-
words binary vector. We can incorporate any
handcrafted feature f(d, e) as:
sim(d, e) = Dot(f(d), f(e)) + ~?~f(d, e) (5)
In fact, we find that with only Dot(f(d), f(e))
as ranking score, the performance is sufficiently
good. So we leave this as our future work.
32
3 Experiments and Analysis
Training settings: In pre-training stage, input
layer has 100,000 units, all hidden layers have
1,000 units with rectifier functionmax(0, x). Fol-
lowing (Glorot et al, 2011), for the first recon-
struction layer, we use sigmoid activation func-
tion and cross-entropy error function. For higher
reconstruction layers, we use softplus (log(1 +
exp(x))) as activation function and squared loss
as error function. For corruption process, we use a
masking noise probability in {0.1,0.4,0.7} for the
first layer, a Gaussian noise with standard devi-
ation of 0.1 for higher layers. For reconstruction
sampling, we set the reconstruction rate to 0.01. In
fine-tuning stage, the final layer has 200 units with
sigmoid activation function. The learning rate is
set to 1e-3. The mini-batch size is set to 20.
We run all our experiments on a Linux ma-
chine with 72GB memory 6 core Xeon CPU. The
model is implemented in Python with C exten-
sions, numpy configured with Openblas library.
Thanks to reconstruction sampling and refined
mini-batch arrangement, it takes about 1 day to
converge for pre-training and 3 days for fine-
tuning, which is fast given our training set size.
Datasets: We use half of Wikipedia 1 plain text
(?1.5M articles split into sections) for pre-training.
We collect a total of 40M hyperlinks grouped by
name string m for fine-tuning stage. We holdout
a subset of hyperlinks for model selection, and we
find that 3 layers network with a higher masking
noise rate (0.7) always gives best performance.
We select TAC-KBP 2010 (Ji and Grishman,
2011) dataset for non-collective approaches, and
AIDA 2 dataset for collective approaches. For both
datasets, we evaluate the non-NIL queries. The
TAC-KBP and AIDA testb dataset contains 1020
and 4485 non-NIL queries respectively.
For candidate generation, mention-to-entity dic-
tionary is built by mining Wikipedia structures,
following (Cucerzan, 2007). We keep top 30 can-
didates by prominence P (e|m) for speed consid-
eration. The candidate generation recall are 94.0%
and 98.5% for TAC and AIDA respectively.
Analysis: Table 1 shows evaluation results
across several best performing systems. (Han et
al., 2011) is a collective approach, using Person-
alized PageRank to propagate evidence between
1available at http://dumps.wikimedia.org/enwiki/, we use
the 20110405 xml dump.
2available at http://www.mpi-inf.mpg.de/yago-naga/aida/
different decisions. To our surprise, our method
with only local evidence even beats several com-
plex collective methods with simple word similar-
ity. This reveals the importance of context model-
ing in semantic space. Collective approaches can
improve performance only when local evidence is
not confident enough. When embedding our sim-
ilarity measure sim(d, e) into (Han et al, 2011),
we achieve the best results on AIDA.
A close error analysis shows some typical er-
rors due to the lack of prominence feature and
name matching feature. Some queries acciden-
tally link to rare candidates and some link to en-
tities with completely different names. We will
add these features as mentioned in Eq. 5 in future.
We will also add NIL-detection module, which is
required by more realistic application scenarios.
A first thought is to construct pseudo-NIL with
Wikipedia annotations and automatically learn the
threshold and feature weight as in (Bunescu and
Pasca, 2006; Kulkarni et al, 2009).
Methods micro
P@1
macro
P@1
TAC 2010 eval
Lcc (2010) (top1, noweb) 79.22 -
Siel 2010 (top2, noweb) 71.57 -
our best 80.97 -
AIDA dataset (collective approaches)
AIDA (2011) 82.29 82.02
Shirakawa et al (2011) 81.40 83.57
Kulkarni et al (2009) 72.87 76.74
wordsim (cosine) 48.38 37.30
Han (2011) +wordsim 78.97 75.77
our best (non-collective) 84.82 83.37
Han (2011) + our best 85.62 83.95
Table 1: Evaluation on TAC and AIDA dataset.
4 Conclusion
We propose a deep learning approach that auto-
matically learns context-entity similarity measure
for entity disambiguation. The intermediate rep-
resentations are learned leveraging large scale an-
notations of Wikipedia, without any manual effort
of designing features. The learned representation
of entity is compact and can scale to very large
knowledge base. Furthermore, experiment reveals
the importance of context modeling in this field.
By incorporating our learned measure into collec-
tive approach, performance is further improved.
33
Acknowledgments
We thank Nan Yang, Jie Liu and Fei Wang for helpful discus-
sions. This research was partly supported by National High
Technology Research and Development Program of China
(863 Program) (No. 2012AA011101), National Natural Sci-
ence Foundation of China (No.91024009) and Major Na-
tional Social Science Fund of China(No. 12&ZD227).
References
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle.
2007. Greedy layer-wise training of deep networks.
Advances in neural information processing systems,
19:153.
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of EACL, volume 6, pages 9?16.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings
of EMNLP-CoNLL, volume 6, pages 708?716.
Y. Dauphin, X. Glorot, and Y. Bengio. 2011.
Large-scale learning of embeddings with recon-
struction sampling. In Proceedings of the Twenty-
eighth International Conference on Machine Learn-
ing (ICML11).
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification: A
deep learning approach. In Proceedings of the 28th
International Conference on Machine Learning.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347?352. IEEE.
X. Han, L. Sun, and J. Zhao. 2011. Collective en-
tity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765?774. ACM.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
J. Hoffart, M.A. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of
named entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 782?792. Association for Com-
putational Linguistics.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and chal-
lenges. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1148?
1158, Portland, Oregon, USA, June. Association for
Computational Linguistics.
S.S. Kataria, K.S. Kumar, R. Rastogi, P. Sen, and S.H.
Sengamedu. 2011. Entity disambiguation with hier-
archical topic models. In Proceedings of KDD.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 457?
466. ACM.
J. Lehmann, S. Monahan, L. Nezda, A. Jung, and
Y. Shi. 2010. Lcc approaches to knowledge base
population at tac 2010. In Proc. TAC 2010 Work-
shop.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL).
P. Sen. 2012. Collective context-aware topic mod-
els for entity disambiguation. In Proceedings of the
21st international conference on World Wide Web,
pages 729?738. ACM.
M. Shirakawa, H. Wang, Y. Song, Z. Wang,
K. Nakayama, T. Hara, and S. Nishio. 2011. Entity
disambiguation based on a probabilistic taxonomy.
Technical report, Technical Report MSR-TR-2011-
125, Microsoft Research.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 354?
362. Association for Computational Linguistics.
P. Vincent, H. Larochelle, Y. Bengio, and P.A. Man-
zagol. 2008. Extracting and composing robust
features with denoising autoencoders. In Proceed-
ings of the 25th international conference on Ma-
chine learning, pages 1096?1103. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. The Journal of Machine Learn-
ing Research, 11:3371?3408.
W. Zhang, Y.C. Sim, J. Su, and C.L. Tan. 2011. Entity
linking with effective acronym expansion, instance
selection and topic modeling. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages
1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xi-
aoyan Zhu. 2010. Learning to link entities with
knowledge base. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 483?491, Los Ange-
les, California, June. Association for Computational
Linguistics.
34
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340?345,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingual Data Cleaning for SMT using Graph-based Random Walk?
Lei Cui?, Dongdong Zhang?, Shujie Liu?, Mu Li?, and Ming Zhou?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
leicui@hit.edu.cn
?Microsoft Research Asia, Beijing, China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
Abstract
The quality of bilingual data is a key factor
in Statistical Machine Translation (SMT).
Low-quality bilingual data tends to pro-
duce incorrect translation knowledge and
also degrades translation modeling per-
formance. Previous work often used su-
pervised learning methods to filter low-
quality data, but a fair amount of human
labeled examples are needed which are
not easy to obtain. To reduce the re-
liance on labeled examples, we propose
an unsupervised method to clean bilin-
gual data. The method leverages the mu-
tual reinforcement between the sentence
pairs and the extracted phrase pairs, based
on the observation that better sentence
pairs often lead to better phrase extraction
and vice versa. End-to-end experiments
show that the proposed method substan-
tially improves the performance in large-
scale Chinese-to-English translation tasks.
1 Introduction
Statistical machine translation (SMT) depends on
the amount of bilingual data and its quality. In
real-world SMT systems, bilingual data is often
mined from the web where low-quality data is in-
evitable. The low-quality bilingual data degrades
the quality of word alignment and leads to the in-
correct phrase pairs, which will hurt the transla-
tion performance of phrase-based SMT systems
(Koehn et al, 2003; Och and Ney, 2004). There-
fore, it is very important to exploit data quality in-
formation to improve the translation modeling.
Previous work on bilingual data cleaning often
involves some supervised learning methods. Sev-
eral bilingual data mining systems (Resnik and
?This work has been done while the first author was visit-
ing Microsoft Research Asia.
Smith, 2003; Shi et al, 2006; Munteanu and
Marcu, 2005; Jiang et al, 2009) have a post-
processing step for data cleaning. Maximum en-
tropy or SVM based classifiers are built to filter
some non-parallel data or partial-parallel data. Al-
though these methods can filter some low-quality
bilingual data, they need sufficient human labeled
training instances to build the model, which may
not be easy to acquire.
To this end, we propose an unsupervised ap-
proach to clean the bilingual data. It is intuitive
that high-quality parallel data tends to produce
better phrase pairs than low-quality data. Mean-
while, it is also observed that the phrase pairs that
appear frequently in the bilingual corpus are more
reliable than less frequent ones because they are
more reusable, hence most good sentence pairs are
prone to contain more frequent phrase pairs (Fos-
ter et al, 2006; Wuebker et al, 2010). This kind of
mutual reinforcement fits well into the framework
of graph-based random walk. When a phrase pair
p is extracted from a sentence pair s, s is consid-
ered casting a vote for p. The higher the number
of votes a phrase pair has, the more reliable of the
phrase pair. Similarly, the quality of the sentence
pair s is determined by the number of votes casted
by the extracted phrase pairs from s.
In this paper, a PageRank-style random walk al-
gorithm (Brin and Page, 1998; Mihalcea and Ta-
rau, 2004; Wan et al, 2007) is conducted to itera-
tively compute the importance score of each sen-
tence pair that indicates its quality: the higher the
better. Unlike other data filtering methods, our
proposed method utilizes the importance scores
of sentence pairs as fractional counts to calculate
the phrase translation probabilities based on Maxi-
mum Likelihood Estimation (MLE), thereby none
of the bilingual data is filtered out. Experimen-
tal results show that our proposed approach sub-
stantially improves the performance in large-scale
Chinese-to-English translation tasks.
340
2 The Proposed Approach
2.1 Graph-based random walk
Graph-based random walk is a general algorithm
to approximate the importance of a vertex within
the graph in a global view. In our method, the ver-
tices denote the sentence pairs and phrase pairs.
The importance of each vertex is propagated to
other vertices along the edges. Depending on dif-
ferent scenarios, the graph can take directed or
undirected, weighted or un-weighted forms. Start-
ing from the initial scores assigned in the graph,
the algorithm is applied to recursively compute the
importance scores of vertices until it converges, or
the difference between two consecutive iterations
falls below a pre-defined threshold.
2.2 Graph construction
Given the sentence pairs that are word-aligned
automatically, an undirected, weighted bipartite
graph is constructed which maps the sentence
pairs and the extracted phrase pairs to the ver-
tices. An edge between a sentence pair vertex and
a phrase pair vertex is added if the phrase pair can
be extracted from the sentence pair. Mutual re-
inforcement scores are defined on edges, through
which the importance scores are propagated be-
tween vertices. Figure 1 illustrates the graph struc-
ture. Formally, the bipartite graph is defined as:
G = (V,E)
where V = S ? P is the vertex set, S = {si|1 ?
i ? n} is the set of all sentence pairs. P =
{pj |1 ? j ? m} is the set of all phrase pairs
which are extracted from S based on the word
alignment. E is the edge set in which the edges
are between S and P , thereby E = {?si, pj?|si ?
S, pj ? P, ?(si, pj) = 1}.
?(si, pj) =
{
1 if pj can be extracted from si
0 otherwise
2.3 Graph parameters
For sentence-phrase mutual reinforcement, a non-
negative score r(si, pj) is defined using the stan-
dard TF-IDF formula:
r(si, pj) =
{ PF (si,pj)?IPF (pj)?
p??{p|?(si,p)=1} PF (si,p
?)?IPF (p?) if ?(si, pj) = 1
0 otherwise
Sentence Pair Vertices
Phrase Pair Vertices
s1
s2
s3
p1
p3
p4
p5
p6
p2
Figure 1: The circular nodes stand for S and
square nodes stand for P . The lines capture the
sentence-phrase mutual reinforcement.
where PF (si, pj) is the phrase pair frequency in
a sentence pair and IPF (pj) is the inverse phrase
pair frequency of pj in the whole bilingual corpus.
r(si, pj) is abbreviated as rij .
Inspired by (Brin and Page, 1998; Mihalcea
and Tarau, 2004; Wan et al, 2007), we com-
pute the importance scores of sentence pairs and
phrase pairs using a PageRank-style algorithm.
The weights rij are leveraged to reflect the rela-
tionships between two types of vertices. Let u(si)
and v(pj) denote the scores of a sentence pair ver-
tex and a phrase pair vertex. They are computed
iteratively by:
u(si) = (1?d)+d?
?
j?N(si)
rij?
k?M(pj) rkj
v(pj)
v(pj) = (1?d) +d?
?
j?M(pj)
rij?
k?N(si) rik
u(si)
where d is empirically set to the default value 0.85
that is same as the original PageRank, N(si) =
{j|?si, pj? ? E}, M(pj) = {i|?si, pj? ? E}.
The detailed process is illustrated in Algorithm 1.
Algorithm 1 iteratively updates the scores of sen-
tence pairs and phrase pairs (lines 10-26). The
computation ends when difference between two
consecutive iterations is lower than a pre-defined
threshold ? (10?12 in this study).
2.4 Parallelization
When the random walk runs on some large bilin-
gual corpora, even filtering phrase pairs that ap-
pear only once would still require several days of
CPU time for a number of iterations. To over-
come this problem, we use a distributed algorithm
341
Algorithm 1 Modified Random Walk
1: for all i ? {0 . . . |S| ? 1} do
2: u(si)(0) ? 1
3: end for
4: for all j ? {0 . . . |P | ? 1} do
5: v(pj)(0) ? 1
6: end for
7: ? ? Infinity
8: ? threshold
9: n? 1
10: while ? >  do
11: for all i ? {0 . . . |S| ? 1} do
12: F (si)? 0
13: for all j ? N(si) do
14: F (si)? F (si) + rij?
k?M(pj) rkj
? v(pj)(n?1)
15: end for
16: u(si)(n) ? (1? d) + d ? F (si)
17: end for
18: for all j ? {0 . . . |P | ? 1} do
19: G(pj)? 0
20: for all i ?M(pj) do
21: G(pj)? G(pj) + rij?
k?N(si) rik
? u(si)(n?1)
22: end for
23: v(pj)(n) ? (1? d) + d ?G(pj)
24: end for
25: ? ? max(4u(si)||S|?1i=1 ,4v(pj)||P |?1j=1 )26: n? n+ 1
27: end while
28: return u(si)(n)||S|?1i=0
based on the iterative computation in the Sec-
tion 2.3. Before the iterative computation starts,
the sum of the outlink weights for each vertex
is computed first. The edges are randomly par-
titioned into sets of roughly equal size. Each
edge ?si, pj? can generate two key-value pairs
in the format ?si, rij? and ?pj , rij?. The pairs
with the same key are summed locally and ac-
cumulated across different machines. Then, in
each iteration, the score of each vertex is up-
dated according to the sum of the normalized
inlink weights. The key-value pairs are gener-
ated in the format ?si, rij?
k?M(pj) rkj
? v(pj)? and
?pj , rij?
k?N(si) rik
? u(si)?. These key-value pairs
are also randomly partitioned and summed across
different machines. Since long sentence pairs usu-
ally extract more phrase pairs, we need to normal-
ize the importance scores based on the sentence
length. The algorithm fits well into the MapRe-
duce programming model (Dean and Ghemawat,
2008) and we use it as our implementation.
2.5 Integration into translation modeling
After sufficient number of iterations, the impor-
tance scores of sentence pairs (i.e., u(si)) are ob-
tained. Instead of simple filtering, we use the
scores of sentence pairs as the fractional counts to
re-estimate the translation probabilities of phrase
pairs. Given a phrase pair p = ?f? , e??, A(f?) and
B(e?) indicate the sets of sentences that f? and e?
appear. Then the translation probability is defined
as:
PCW(f? |e?) =
?
i?A(f?)?B(e?) u(si)? ci(f? , e?)?
j?B(e?) u(sj)? cj(e?)
where ci(?) denotes the count of the phrase or
phrase pair in si. PCW(f? |e?) and PCW(e?|f?) are
named as Corpus Weighting (CW) based transla-
tion probability, which are integrated into the log-
linear model in addition to the conventional phrase
translation probabilities (Koehn et al, 2003).
3 Experiments
3.1 Setup
We evaluated our bilingual data cleaning ap-
proach on large-scale Chinese-to-English machine
translation tasks. The bilingual data we used
was mainly mined from the web (Jiang et al,
2009)1, as well as the United Nations parallel cor-
pus released by LDC and the parallel corpus re-
leased by China Workshop on Machine Transla-
tion (CWMT), which contain around 30 million
sentence pairs in total after removing duplicated
ones. The development data and testing data is
shown in Table 1.
Data Set #Sentences Source
NIST 2003 (dev) 919 open test
NIST 2005 (test) 1,082 open test
NIST 2006 (test) 1,664 open test
NIST 2008 (test) 1,357 open test
CWMT 2008 (test) 1,006 open test
In-house dataset 1 (test) 1,002 web data
In-house dataset 2 (test) 5,000 web data
In-house dataset 3 (test) 2,999 web data
Table 1: Development and testing data used in the
experiments.
A phrase-based decoder was implemented
based on inversion transduction grammar (Wu,
1997). The performance of this decoder is simi-
lar to the state-of-the-art phrase-based decoder in
Moses, but the implementation is more straight-
forward. We use the following feature functions
in the log-linear model:
1Although supervised data cleaning has been done in the
post-processing, the corpus still contains a fair amount of
noisy data based on our random sampling.
342
dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3
baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19
(Wuebker et al, 2010) 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18
-0.25M 41.28 37.62 35.31 29.70 31.40 24.52 22.69 24.64
-0.5M 41.45 37.71 35.52 29.76 31.77 24.64 22.68 24.69
-1M 41.28 37.41 35.28 29.65 31.73 24.23 23.06 24.20
+CW 41.75 38.08 35.84 30.03 31.82 25.23 23.18 24.80
Table 2: BLEU(%) of Chinese-to-English translation tasks on multiple testing datasets (p < 0.05), where
?-numberM? denotes we simply filter number million low scored sentence pairs from the bilingual data
and use others to extract the phrase table. ?CW? means the corpus weighting feature, which incorporates
sentence scores from random walk as fractional counts to re-estimate the phrase translation probabilities.
? phrase translation probabilities and lexical
weights in both directions (4 features);
? 5-gram language model with Kneser-Ney
smoothing (1 feature);
? lexicalized reordering model (1 feature);
? phrase count and word count (2 features).
The translation model was trained over the
word-aligned bilingual corpus conducted by
GIZA++ (Och and Ney, 2003) in both directions,
and the diag-grow-final heuristic was used to re-
fine the symmetric word alignment. The language
model was trained on the LDC English Gigaword
Version 4.0 plus the English part of the bilingual
corpus. The lexicalized reordering model (Xiong
et al, 2006) was trained over the 40% randomly
sampled sentence pairs from our parallel data.
Case-insensitive BLEU4 (Papineni et al, 2002)
was used as the evaluation metric. The parame-
ters of the log-linear model are tuned by optimiz-
ing BLEU on the development data using MERT
(Och, 2003). Statistical significance test was per-
formed using the bootstrap re-sampling method
proposed by Koehn (2004).
3.2 Baseline
The experimental results are shown in Table 2. In
the baseline system, the phrase pairs that appear
only once in the bilingual data are simply dis-
carded because most of them are noisy. In ad-
dition, the fix-discount method in (Foster et al,
2006) for phrase table smoothing is also used.
This implementation makes the baseline system
perform much better and the model size is much
smaller. In fact, the basic idea of our ?one count?
cutoff is very similar to the idea of ?leaving-one-
out? in (Wuebker et al, 2010). The results show
?? ?? ? ? ??
uncharted waters
?? ?? ? ? ??
unexplored new areas
weijing tansuo de xin lingyu
Figure 2: The left one is the non-literal translation
in our bilingual corpus. The right one is the literal
translation made by human for comparison.
that the ?leaving-one-out? method performs al-
most the same as our baseline, thereby cannot
bring other benefits to the system.
3.3 Results
We evaluate the proposed bilingual data clean-
ing method by incorporating sentence scores into
translation modeling. In addition, we also com-
pare with several settings that filtering low-quality
sentence pairs from the bilingual data based on
the importance scores. The last N = { 0.25M,
0.5M, 1M } sentence pairs are filtered before the
modeling process. Although the simple bilin-
gual data filtering can improve the performance on
some datasets, it is difficult to determine the bor-
der line and translation performance is fluctuated.
One main reason is in the proposed random walk
approach, the bilingual sentence pairs with non-
literal translations may get lower scores because
they appear less frequently compared with those
literal translations. Crudely filtering out these data
may degrade the translation performance. For ex-
ample, we have a sentence pair in the bilingual
corpus shown in the left part of Figure 2. Although
the translation is correct in this situation, translat-
ing the Chinese word ?lingyu? to ?waters? appears
very few times since the common translations are
?areas? or ?fields?. However, simply filtering out
this kind of sentence pairs may lead to some loss
of native English expressions, thereby the trans-
343
lation performance is unstable since both non-
parallel sentence pairs and non-literal but parallel
sentence pairs are filtered. Therefore, we use the
importance score of each sentence pair to estimate
the phrase translation probabilities. It consistently
brings substantial improvements compared to the
baseline, which demonstrates graph-based random
walk indeed improves the translation modeling
performance for our SMT system.
3.4 Discussion
In (Goutte et al, 2012), they evaluated phrase-
based SMT systems trained on parallel data with
different proportions of synthetic noisy data. They
suggested that when collecting larger, noisy par-
allel data for training phrase-based SMT, clean-
ing up by trying to detect and remove incor-
rect alignments can actually degrade performance.
Our experimental results confirm their findings
on some datasets. Based on our method, some-
times filtering noisy data leads to unexpected re-
sults. The reason is two-fold: on the one hand,
the non-literal parallel data makes false positive in
noisy data detection; on the other hand, large-scale
SMT systems is relatively robust and tolerant to
noisy data, especially when we remove frequency-
1 phrase pairs. Therefore, we propose to integrate
the importance scores when re-estimating phrase
pair probabilities in this paper. The importance
scores can be considered as a kind of contribution
constraint, thereby high-quality parallel data con-
tributes more while noisy parallel data contributes
less.
4 Conclusion and Future Work
In this paper, we develop an effective approach
to clean the bilingual data using graph-based ran-
dom walk. Significant improvements on several
datasets are achieved in our experiments. For
future work, we will extend our method to ex-
plore the relationships of sentence-to-sentence and
phrase-to-phrase, which is beyond the existing
sentence-to-phrase mutual reinforcement.
Acknowledgments
We are especially grateful to Yajuan Duan, Hong
Sun, Nan Yang and Xilun Chen for the helpful dis-
cussions. We also thank the anonymous reviewers
for their insightful comments.
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107?
117.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapre-
duce: simplified data processing on large clusters.
Communications of the ACM, 51(1):107?113.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53?61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Cyril Goutte, Marine Carpuat, and George Foster.
2012. The impact of sentence alignment errors on
phrase-based machine translation performance. In
Proceedings of AMTA 2012, San Diego, California,
October. Association for Machine Translation in the
Americas.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 870?878, Suntec, Singapore, August.
Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003 Main Papers, pages
48?54, Edmonton, May-June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
404?411, Barcelona, Spain, July. Association for
Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
344
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A dom tree alignment model for mining paral-
lel data from the web. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 489?496, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552?559, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475?484, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 521?528,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
345
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111?121,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Bilingually-constrained Phrase Embeddings for Machine Translation
Jiajun Zhang
1
, Shujie Liu
2
, Mu Li
2
, Ming Zhou
2
and Chengqing Zong
1
1
National Laboratory of Pattern Recognition, CASIA, Beijing, P.R. China
{jjzhang,cqzong}@nlpr.ia.ac.cn
2
Microsoft Research Asia, Beijing, P.R. China
{shujliu,muli,mingzhou}@microsoft.com
Abstract
We propose Bilingually-constrained Re-
cursive Auto-encoders (BRAE) to learn
semantic phrase embeddings (compact
vector representations for phrases), which
can distinguish the phrases with differ-
ent semantic meanings. The BRAE is
trained in a way that minimizes the seman-
tic distance of translation equivalents and
maximizes the semantic distance of non-
translation pairs simultaneously. After
training, the model learns how to embed
each phrase semantically in two languages
and also learns how to transform semantic
embedding space in one language to the
other. We evaluate our proposed method
on two end-to-end SMT tasks (phrase ta-
ble pruning and decoding with phrasal se-
mantic similarities) which need to mea-
sure semantic similarity between a source
phrase and its translation candidates. Ex-
tensive experiments show that the BRAE
is remarkably effective in these two tasks.
1 Introduction
Due to the powerful capacity of feature learn-
ing and representation, Deep (multi-layer) Neural
Networks (DNN) have achieved a great success in
speech and image processing (Kavukcuoglu et al,
2010; Krizhevsky et al, 2012; Dahl et al, 2012).
Recently, statistical machine translation (SMT)
community has seen a strong interest in adapting
and applying DNN to many tasks, such as word
alignment (Yang et al, 2013), translation confi-
dence estimation (Mikolov et al, 2010; Liu et al,
2013; Zou et al, 2013), phrase reordering predic-
tion (Li et al, 2013), translation modelling (Auli et
al., 2013; Kalchbrenner and Blunsom, 2013) and
language modelling (Duh et al, 2013; Vaswani et
al., 2013). Most of these works attempt to im-
prove some components in SMT based on word
embedding, which converts a word into a dense,
low dimensional, real-valued vector representation
(Bengio et al, 2003; Bengio et al, 2006; Collobert
and Weston, 2008; Mikolov et al, 2013).
However, in the conventional (phrase-based)
SMT, phrases are the basic translation units. The
models using word embeddings as the direct in-
puts to DNN cannot make full use of the whole
syntactic and semantic information of the phrasal
translation rules. Therefore, in order to success-
fully apply DNN to model the whole translation
process, such as modelling the decoding process,
learning compact vector representations for the ba-
sic phrasal translation units is the essential and
fundamental work.
In this paper, we explore the phrase embedding,
which represents a phrase (sequence of words)
with a real-valued vector. In some previous works,
phrase embedding has been discussed from differ-
ent views. Socher et al (2011) make the phrase
embeddings capture the sentiment information.
Socher et al (2013a) enable the phrase embed-
dings to mainly capture the syntactic knowledge.
Li et al (2013) attempt to encode the reordering
pattern in the phrase embeddings. Kalchbrenner
and Blunsom (2013) utilize a simple convolution
model to generate phrase embeddings from word
embeddings. Mikolov et al (2013) consider a
phrase as an indivisible n-gram. Obviously, these
methods of learning phrase embeddings either fo-
cus on some aspects of the phrase (e.g. reordering
pattern), or impose strong assumptions (e.g. bag-
of-words or indivisible n-gram). Therefore, these
phrase embeddings are not suitable to fully repre-
sent the phrasal translation units in SMT due to the
lack of semantic meanings of the phrase.
Instead, we focus on learning phrase embed-
dings from the view of semantic meaning, so
that our phrase embedding can fully represent the
phrase and best fit the phrase-based SMT. As-
suming the phrase is a meaningful composition
111
of its internal words, we propose Bilingually-
constrained Recursive Auto-encoders (BRAE) to
learn semantic phrase embeddings. The core idea
behind is that a phrase and its correct translation
should share the same semantic meaning. Thus,
they can supervise each other to learn their seman-
tic phrase embeddings. Similarly, non-translation
pairs should have different semantic meanings,
and this information can also be used to guide
learning semantic phrase embeddings.
In our method, the standard recursive auto-
encoder (RAE) pre-trains the phrase embedding
with an unsupervised algorithm by minimizing the
reconstruction error (Socher et al, 2010), while
the bilingually-constrained model learns to fine-
tune the phrase embedding by minimizing the se-
mantic distance between translation equivalents
and maximizing the semantic distance between
non-translation pairs.
We use an example to explain our model. As
illustrated in Fig. 1, the Chinese phrase on the
left and the English phrase on the right are trans-
lations with each other. If we learn the embedding
of the Chinese phrase correctly, we can regard it
as the gold representation for the English phrase
and use it to guide the process of learning English
phrase embedding. In the other direction, the Chi-
nese phrase embedding can be learned in the same
way. This procedure can be performed with an
co-training style algorithm so as to minimize the
semantic distance between the translation equiva-
lents
1
. In this way, the result Chinese and English
phrase embeddings will capture the semantics as
much as possible. Furthermore, a transformation
function between the Chinese and English seman-
tic spaces can be learned as well.
With the learned model, we can accurately mea-
sure the semantic similarity between a source
phrase and a translation candidate. Accordingly,
we evaluate the BRAE model on two end-to-
end SMT tasks (phrase table pruning and decod-
ing with phrasal semantic similarities) which need
to check whether a translation candidate and the
source phrase are in the same meaning. In phrase
table pruning, we discard the phrasal translation
rules with low semantic similarity. In decoding
with phrasal semantic similarities, we apply the
semantic similarities of the phrase pairs as new
features during decoding to guide translation can-
1
For simplicity, we do not show non-translation pairs
here.
source phrase embedding ps  
?? ? ??? France and Russia 
target phrase embedding pt  
Figure 1: A motivation example for the BRAE
model.
didate selection. The experiments show that up to
72% of the phrase table can be discarded without
significant decrease on the translation quality, and
in decoding with phrasal semantic similarities up
to 1.7 BLEU score improvement over the state-of-
the-art baseline can be achieved.
In addition, our semantic phrase embeddings
have many other potential applications. For in-
stance, the semantic phrase embeddings can be
directly fed to DNN to model the decoding pro-
cess. Besides SMT, the semantic phrase embed-
dings can be used in other cross-lingual tasks (e.g.
cross-lingual question answering) and monolin-
gual applications such as textual entailment, ques-
tion answering and paraphrase detection.
2 Related Work
Recently, phrase embedding has drawn more and
more attention. There are three main perspectives
handling this task in monolingual languages.
One method considers the phrases as bag-of-
words and employs a convolution model to trans-
form the word embeddings to phrase embeddings
(Collobert et al, 2011; Kalchbrenner and Blun-
som, 2013). Gao et al (2013) also use bag-of-
words but learn BLEU sensitive phrase embed-
dings. This kind of approaches does not take the
word order into account and loses much informa-
tion. Instead, our bilingually-constrained recur-
sive auto-encoders not only learn the composition
mechanism of generating phrases from words, but
also fine tune the word embeddings during the
model training stage, so that we can induce the full
information of the phrases and internal words.
Another method (Mikolov et al, 2013) deals
with the phrases having a meaning that is not a
simple composition of the meanings of its indi-
vidual words, such as New York Times. They first
find the phrases of this kind. Then, they regard
these phrases as indivisible units, and learn their
embeddings with the context information. How-
112
ever, this kind of phrase embedding is hard to cap-
ture full semantics since the context of a phrase
is limited. Furthermore, this method can only ac-
count for a very small part of phrases, since most
of the phrases are compositional. In contrast, our
method attempts to learn the semantic vector rep-
resentation for any phrase.
The third method views any phrase as the mean-
ingful composition of its internal words. The re-
cursive auto-encoder is typically adopted to learn
the way of composition (Socher et al, 2010;
Socher et al, 2011; Socher et al, 2013a; Socher
et al, 2013b; Li et al, 2013). They pre-train the
RAE with an unsupervised algorithm. And then,
they fine-tune the RAE according to the label of
the phrase, such as the syntactic category in pars-
ing (Socher et al, 2013a), the polarity in sentiment
analysis (Socher et al, 2011; Socher et al, 2013b),
and the reordering pattern in SMT (Li et al, 2013).
This kind of semi-supervised phrase embedding is
in fact performing phrase clustering with respect
to the phrase label. For example, in the RAE-
based phrase reordering model for SMT (Li et
al., 2013), the phrases with the similar reorder-
ing tendency (e.g. monotone or swap) are close
to each other in the embedding space, such as the
prepositional phrases. Obviously, this kind meth-
ods of semi-supervised phrase embedding do not
fully address the semantic meaning of the phrases.
Although we also follow the composition-based
phrase embedding, we are the first to focus on
the semantic meanings of the phrases and propose
a bilingually-constrained model to induce the se-
mantic information and learn transformation of the
semantic space in one language to the other.
3 Bilingually-constrained Recursive
Auto-encoders
This section introduces the Bilingually-
constrained Recursive Auto-encoders (BRAE),
that is inspired by two observations. First, the
recursive auto-encoder provides a reasonable
composition mechanism to embed each phrase.
And the semi-supervised phrase embedding
(Socher et al, 2011; Socher et al, 2013a; Li et
al., 2013) further indicates that phrase embedding
can be tuned with respect to the label. Second,
even though we have no correct semantic phrase
representation as the gold label, the phrases
sharing the same meaning provide an indirect but
feasible way.
x1 x2 x3 x4 
y1=f(W(1)[x1; x2]+b) 
y2=f(W(1)[y1; x3]+b) 
y3=f(W(1)[y2; x4]+b) 
Figure 2: A recursive auto-encoder for a four-
word phrase. The empty nodes are the reconstruc-
tions of the input.
We will first briefly present the unsupervised
phrase embedding, and then describe the semi-
supervised framework. After that, we introduce
the BRAE on the network structure, objective
function and parameter inference.
3.1 Unsupervised Phrase Embedding
3.1.1 Word Vector Representations
In phrase embedding using composition, the word
vector representation is the basis and serves as the
input to the neural network. After learning word
embeddings with DNN (Bengio et al, 2003; Col-
lobert and Weston, 2008; Mikolov et al, 2013),
each word in the vocabulary V corresponds to a
vector x ? R
n
, and all the vectors are stacked into
an embedding matrix L ? R
n?|V |
.
Given a phrase which is an ordered list of m
words, each word has an index i into the columns
of the embedding matrix L. The index i is used to
retrieve the word?s vector representation using a
simple multiplication with a binary vector e which
is zero in all positions except for the ith index:
x
i
= Le
i
? R
n
(1)
Note that n is usually set empirically, such as n =
50, 100, 200. Throughout this paper, n = 3 is used
for better illustration as shown in Fig. 1.
3.1.2 RAE-based Phrase Embedding
Assuming we are given a phrase w
1
w
2
? ? ?w
m
,
it is first projected into a list of vectors
(x
1
, x
2
, ? ? ? , x
m
) using Eq. 1. The RAE learns
the vector representation of the phrase by recur-
sively combining two children vectors in a bottom-
up manner (Socher et al, 2011). Fig. 2 illustrates
an instance of a RAE applied to a binary tree, in
113
which a standard auto-encoder (in box) is re-used
at each node. The standard auto-encoder aims at
learning an abstract representation of its input. For
two children c
1
= x
1
and c
2
= x
2
, the auto-
encoder computes the parent vector y
1
as follows:
p = f(W
(1)
[c
1
; c
2
] + b
(1)
) (2)
Where we multiply the parameter matrix W
(1)
?
R
n?2n
by the concatenation of two children
[c
1
; c
2
] ? R
2n?1
. After adding a bias term b
(1)
,
we apply an element-wise activation function such
as f = tanh(?), which is used in our experiments.
In order to apply this auto-encoder to each pair of
children, the representation of the parent p should
have the same dimensionality as the c
i
?s.
To assess how well the parent?s vector repre-
sents its children, the standard auto-encoder recon-
structs the children in a reconstruction layer:
[c
?
1
; c
?
2
] = f
(2)
(W
(2)
p+ b
(2)
) (3)
Where c
?
1
and c
?
2
are reconstructed children, W
(2)
and b
(2)
are parameter matrix and bias term for re-
construction respectively, and f
(2)
= tanh(?).
To obtain the optimal abstract representation of
the inputs, the standard auto-encoder tries to min-
imize the reconstruction errors between the inputs
and the reconstructed ones during training:
E
rec
([c
1
; c
2
]) =
1
2
||[c
1
; c
2
]? [c
?
1
; c
?
2
]||
2
(4)
Given y
1
= p, we can use Eq. 2 again to com-
pute y
2
by setting the children to be [c
1
; c
2
] =
[y
1
;x
3
]. The same auto-encoder is re-used until
the vector of the whole phrase is generated.
For unsupervised phrase embedding, the only
objective is to minimize the sum of reconstruction
errors at each node in the optimal binary tree:
RAE
?
(x) = argmin
y?A(x)
?
s?y
E
rec
([c
1
; c
2
]
s
) (5)
Where x is the list of vectors of a phrase, andA(x)
denotes all the possible binary trees that can be
built from inputs x. A greedy algorithm (Socher
et al, 2011) is used to generate the optimal binary
tree y. The parameters ? = (W, b) are optimized
over all the phrases in the training data.
3.2 Semi-supervised Phrase Embedding
The above RAE is completely unsupervised and
can only induce general representations of the
Reco nstr uctio n Erro r  Pred ictio n Erro r  
W (1)  
W (2)  W (label)  
Figure 3: An illustration of a semi-supervised
RAE unit. Red nodes show the label distribution.
multi-word phrases. Several researchers extend
the original RAEs to a semi-supervised setting so
that the induced phrase embedding can predict a
target label, such as polarity in sentiment analysis
(Socher et al, 2011), syntactic category in parsing
(Socher et al, 2013a) and phrase reordering pat-
tern in SMT (Li et al, 2013).
In the semi-supervised RAE for phrase embed-
ding, the objective function over a (phrase, label)
pair (x, t) includes the reconstruction error and the
prediction error, as illustrated in Fig. 3.
E(x, t; ?) = ?E
rec
(x, t; ?)+(1??)E
pred
(x, t; ?)
(6)
Where the hyper-parameter ? is used to balance
the reconstruction and prediction error. For label
prediction, the cross-entropy error is usually used
to calculate E
pred
. By optimizing the above ob-
jective, the phrases in the vector embedding space
will be grouped according to the labels.
3.3 The BRAE Model
We know from the semi-supervised phrase embed-
ding that the learned vector representation can be
well adapted to the given label. Therefore, we can
imagine that learning semantic phrase embedding
is reasonable if we are given gold vector represen-
tations of the phrases.
However, no gold semantic phrase embedding
exists. Fortunately, we know the fact that the
two phrases should share the same semantic rep-
resentation if they express the same meaning. We
can make inference from this fact that if a model
can learn the same embedding for any phrase pair
sharing the same meaning, the learned embedding
must encode the semantics of the phrases and the
corresponding model is our desire.
As translation equivalents share the same se-
mantic meaning, we employ high-quality phrase
translation pairs as training corpus in this
work. Accordingly, we propose the Bilingually-
constrained Recursive Auto-encoders (BRAE),
114
Sour ce Recons truction Err o r  
Sour ce Prediction Err o r  
W s (1)  
W s (2)  W s (label)  
Ta rg et Recons truction Err o r  
W t (1)  
W t (2)  
W t (label)  Ta rg et Prediction Err o r  
Source Language Phrase Target Language Phrase 
Figure 4: An illustration of the bilingual-
constrained recursive auto-encoders. The two
phrases are translations with each other.
whose basic goal is to minimize the semantic dis-
tance between the phrases and their translations.
3.3.1 The Objective Function
Unlike previous methods, the BRAE model jointly
learns two RAEs (Fig. 4 shows the network struc-
ture): one for source language and the other for
target language. For a phrase pair (s, t), two kinds
of errors are involved:
1. reconstruction errorE
rec
(s, t; ?): how well
the learned vector representations p
s
and p
t
repre-
sent the phrase s and t respectively?
E
rec
(s, t; ?) = E
rec
(s; ?) + E
rec
(t; ?) (7)
2. semantic error E
sem
(s, t; ?): what is the
semantic distance between the learned vector rep-
resentations p
s
and p
t
?
Since word embeddings for two languages are
learned separately and locate in different vector
space, we do not enforce the phrase embeddings
in two languages to be in the same semantic vector
space. We suppose there is a transformation be-
tween the two semantic embedding spaces. Thus,
the semantic distance is bidirectional: the distance
between p
t
and the transformation of p
s
, and that
between p
s
and the transformation of p
t
. As a re-
sult, the overall semantic error becomes:
E
sem
(s, t; ?) = E
sem
(s|t, ?) + E
sem
(t|s, ?) (8)
Where E
sem
(s|t, ?) = E
sem
(p
t
, f(W
l
s
p
s
+ b
l
s
))
means the transformation of p
s
is performed as
follows: we first multiply a parameter matrix W
l
s
by p
s
, and after adding a bias term b
l
s
we apply
an element-wise activation function f = tanh(?).
Finally, we calculate their Euclidean distance:
E
sem
(s|t, ?) =
1
2
||p
t
? f(W
l
s
p
s
+ b
l
s
)||
2
(9)
E
sem
(t|s, ?) can be calculated in exactly the same
way. For the phrase pair (s, t), the joint error is:
E(s, t; ?) = ?E
rec
(s, t; ?) + (1??)E
sem
(s, t; ?)
(10)
The hyper-parameter ? weights the reconstruction
and semantic error. The final BRAE objective over
the phrase pairs training set (S, T ) becomes:
J
BRAE
=
1
N
?
(s,t)?(S,T )
E(s, t; ?)+
?
2
||?||
2
(11)
3.3.2 Max-Semantic-Margin Error
Ideally, we want the learned BRAE model can
make sure that the semantic error for the positive
example (a source phrase s and its correct transla-
tion t) is much smaller than that for the negative
example (the source phrase s and a bad translation
t
?
). However, the current model cannot guarantee
this since the above semantic error E
sem
(s|t, ?)
only accounts for positive ones.
We thus enhance the semantic error with both
positive and negative examples, and the corre-
sponding max-semantic-margin error becomes:
E
?
sem
(s|t, ?) = max{0, E
sem
(s|t, ?)
? E
sem
(s|t
?
, ?) + 1}
(12)
It tries to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously. Using the above error function, we need
to construct a negative example for each positive
example. Suppose we are given a positive exam-
ple (s, t), the correct translation t can be converted
into a bad translation t
?
by replacing the words
in t with randomly chosen target language words.
Then, a negative example (s, t
?
) is available.
3.3.3 Parameter Inference
Like semi-supervised RAE (Li et al, 2013), the
parameters ? in our BRAE model can also be di-
vided into three sets:
?
L
: word embedding matrix L for two lan-
guages (Section 3.1.1);
?
rec
: recursive auto-encoder parameter matrices
W
(1)
, W
(2)
, and bias terms b
(1)
, b
(2)
for two lan-
guages (Section 3.1.2);
?
sem
: transformation matrix W
l
and bias term
b
l
for two directions in semantic distance compu-
tation (Section 3.3.1).
115
To have a deep understanding of the parameters,
we rewrite Eq. 10:
E(s, t; ?) = ?(E
rec
(s; ?) + E
rec
(t; ?))
+ (1? ?)(E
?
sem
(s|t, ?) + E
?
sem
(t|s, ?))
= (?E
rec
(s; ?
s
) + (1? ?)E
?
sem
(s|t, ?
s
))
+ (?E
rec
(t; ?
t
) + (1? ?)E
?
sem
(t|s, ?
t
))
(13)
We can see that the parameters ? can be divided
into two classes: ?
s
for the source language and ?
t
for the target language. The above equation also
indicates that the source-side parameters ?
s
can be
optimized independently as long as the semantic
representation p
t
of the target phrase t is given to
compute E
sem
(s|t, ?) with Eq. 9. It is similar for
the target-side parameters ?
t
.
Assuming the target phrase representation p
t
is available, the optimization of the source-side
parameters is similar to that of semi-supervised
RAE. We apply the Stochastic Gradient Descent
(SGD) algorithm to optimize each parameter:
?
s
= ?
s
? ?
?J
s
??
s
(14)
In order to run SGD algorithm, we need to solve
two problems: one for parameter initialization and
the other for partial gradient calculation.
In parameter initialization, ?
rec
and ?
sem
for the
source language is randomly set according to a
normal distribution. For the word embedding L
s
,
there are two choices. First, L
s
is initialized ran-
domly like other parameters. Second, the word
embedding matrix L
s
is pre-trained with DNN
(Bengio et al, 2003; Collobert and Weston, 2008;
Mikolov et al, 2013) using large-scale unlabeled
monolingual data. We prefer to the second one
since this kind of word embedding has already
encoded some semantics of the words. In this
work, we employ the toolkit Word2Vec (Mikolov
et al, 2013) to pre-train the word embedding for
the source and target languages. The word em-
beddings will be fine-tuned in our BRAE model to
capture much more semantics.
The partial gradient for one instance is com-
puted as follows:
?J
s
??
s
=
?E(s|t, ?
s
)
??
s
+ ??
s
(15)
Where the source-side error given the target phrase
representation includes reconstruction error and
updated semantic error:
E(s|t, ?
s
) = ?E
rec
(s; ?
s
) + (1??)E
?
sem
(s|t, ?
s
)
(16)
Given the current ?
s
, we first construct the binary
tree (as illustrated in Fig. 2) for any source-side
phrase using the greedy algorithm (Socher et al,
2011). Then, the derivatives for the parameters in
the fixed binary tree will be calculated via back-
propagation through structures (Goller and Kuch-
ler, 1996). Finally, the parameters will be updated
using Eq. 14 and a new ?
s
is obtained.
The target-side parameters ?
t
can be optimized
in the same way as long as the source-side phrase
representation p
s
is available. It seems a para-
dox that updating ?
s
needs p
t
while updating ?
t
needs p
s
. To solve this problem, we propose an
co-training style algorithm which includes three
steps:
1. Pre-training: applying unsupervised phrase
embedding with standard RAE to pre-train the
source- and target-side phrase representations p
s
and p
t
respectively (Section 2.1.2);
2. Fine-tuning: with the BRAE model, us-
ing target-side phrase representation p
t
to update
the source-side parameters ?
s
and obtain the fine-
tuned source-side phrase representation p
?
s
, and
meanwhile using p
s
to update ?
t
and get the fine-
tuned p
?
t
, and then calculate the joint error over the
training corpus;
3. Termination Check: if the joint error
reaches a local minima or the iterations reach
the pre-defined number (25 is used in our exper-
iments), we terminate the training procedure, oth-
erwise we set p
s
= p
?
s
, p
t
= p
?
t
, and go to step
2.
4 Experiments
With the semantic phrase embeddings and the vec-
tor space transformation function, we apply the
BRAE to measure the semantic similarity between
a source phrase and its translation candidates in
the phrase-based SMT. Two tasks are involved in
the experiments: phrase table pruning that dis-
cards entries whose semantic similarity is very low
and decoding with the phrasal semantic similari-
ties as additional new features.
4.1 Hyper-Parameter Settings
The hyper-parameters in the BRAE model include
the dimensionality of the word embedding n in Eq.
1, the balance weight ? in Eq. 10, ?s in Eq. 11,
and the learning rate ? in Eq. 14.
For the dimensionality n, we have tried three
settings n = 50, 100, 200 in our experiments. We
116
empirically set the learning rate ? = 0.01. We
draw ? from 0.05 to 0.5 with step 0.05, and ?s
from {10
?6
, 10
?5
, 10
?4
, 10
?3
, 10
?2
}. The over-
all error of the BRAE model is employed to guide
the search procedure. Finally, we choose ? =
0.15, ?
L
= 10
?2
, ?
rec
= 10
?3
and ?
sem
= 10
?3
.
4.2 SMT Setup
We have implemented a phrase-based translation
system with a maximum entropy based reordering
model using the bracketing transduction grammar
(Wu, 1997; Xiong et al, 2006).
The SMT evaluation is conducted on Chinese-
to-English translation. Accordingly, our BRAE
model is trained on Chinese and English. The
bilingual training data from LDC
2
contains 0.96M
sentence pairs and 1.1M entity pairs with 27.7M
Chinese words and 31.9M English words. A 5-
gram language model is trained on the Xinhua por-
tion of the English Gigaword corpus and the En-
glish part of bilingual training data. The NIST
MT03 is used as the development data. NIST
MT04-06 and MT08 (news data) are used as the
test data. Case-insensitive BLEU is employed
as the evaluation metric. The statistical signif-
icance test is performed by the re-sampling ap-
proach (Koehn, 2004).
In addition, we pre-train the word embedding
with toolkit Word2Vec on large-scale monolingual
data including the aforementioned data for SMT.
The monolingual data contains 1.06B words for
Chinese and 1.12B words for English. To ob-
tain high-quality bilingual phrase pairs to train
our BRAE model, we perform forced decoding
for the bilingual training sentences and collect the
phrase pairs used. After removing the duplicates,
the remaining 1.12M bilingual phrase pairs (length
ranging from 1 to 7) are obtained.
4.3 Phrase Table Pruning
Pruning most of the phrase table without much
impact on translation quality is very important
for translation especially in environments where
memory and time constraints are imposed. Many
algorithms have been proposed to deal with this
problem, such as significance pruning (Johnson et
al., 2007; Tomeh et al, 2009), relevance prun-
ing (Eck et al, 2007) and entropy-based pruning
2
LDC category numbers: LDC2000T50, LDC2002L27,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2005T34.
(Ling et al, 2012; Zens et al, 2012). These algo-
rithms are based on corpus statistics including co-
occurrence statistics, phrase pair usage and com-
position information. For example, the signifi-
cance pruning, which is proven to be a very ef-
fective algorithm, computes the probability named
p-value, that tests whether a source phrase s and a
target phrase t co-occur more frequently in a bilin-
gual corpus than they happen just by chance. The
higher the p-value, the more likely of the phrase
pair to be spurious.
Our work has the same objective, but instead of
using corpus statistics, we attempt to measure the
quality of the phrase pair from the view of seman-
tic meaning. Given a phrase pair (s, t), the BRAE
model first obtains their semantic phrase represen-
tations (p
s
, p
t
), and then transforms p
s
into target
semantic space p
s
?
, p
t
into source semantic space
p
t
?
. We finally get two similarities Sim(p
s
?
, p
t
)
and Sim(p
t
?
, p
s
). Phrase pairs that have a low
similarity are more likely to be noise and more
prone to be pruned. In experiments, we discard
the phrase pair whose similarity in two directions
are smaller than a threshold
3
.
Table 1 shows the comparison results between
our BRAE-based pruning method and the signif-
icance pruning algorithm. We can see a common
phenomenon in both of the algorithms: for the first
few thresholds, the phrase table becomes smaller
and smaller while the translation quality is not
much decreased, but the performance jumps a lot
at a certain threshold (16 for Significance pruning,
0.8 for BRAE-based one).
Specifically, the Significance algorithm can
safely discard 64% of the phrase table at its thresh-
old 12 with only 0.1 BLEU loss in the overall
test. In contrast, our BRAE-based algorithm can
remove 72% of the phrase table at its threshold
0.7 with only 0.06 BLEU loss in the overall eval-
uation. When the two algorithms using a similar
portion of the phrase table
4
(35% in BRAE and
36% in Significance), the BRAE-based algorithm
outperforms the Significance algorithm on all the
test sets except for MT04. It indicates that our
BRAE model is a good alternative for phrase table
pruning. Furthermore, our model is much more in-
3
To avoid the situation that all the translation candidates
for a source phrase are pruned, we always keep the first 10
best according to the semantic similarity.
4
In the future, we will compare the performance by en-
forcing the two algorithms to use the same portion of phrase
table
117
Method Threshold PhraseTable MT03 MT04 MT05 MT06 MT08 ALL
Baseline 100% 35.81 36.91 34.69 33.83 27.17 34.82
BRAE
0.4 52% 35.94 36.96 35.00 34.71 27.77 35.16
0.5 44% 35.67 36.59 34.86 33.91 27.25 34.89
0.6 35% 35.86 36.71 34.93 34.63 27.34 35.05
0.7 28% 35.55 36.62 34.57 33.97 27.10 34.76
0.8 20% 35.06 36.01 34.13 33.04 26.66 34.04
Significance
8 48% 35.86 36.99 34.74 34.53 27.59 35.13
12 36% 35.59 36.73 34.65 34.17 27.16 34.72
16 25% 35.19 36.24 34.26 33.32 26.55 34.09
20 18% 35.05 36.09 34.02 32.98 26.37 33.97
Table 1: Comparison between BRAE-based pruning and Significance pruning of phrase table. Threshold
means similarity in BRAE and negative-log-p-value in Significance. ?ALL? combines the development
and test sets. Bold numbers denote that the result is better than or comparable to that of baseline. n = 50
is used for embedding dimensionality.
tuitive because it is directly based on the semantic
similarity.
4.4 Decoding with Phrasal Semantic
Similarities
Besides using the semantic similarities to prune
the phrase table, we also employ them as two in-
formative features like the phrase translation prob-
ability to guide translation hypotheses selection
during decoding. Typically, four translation prob-
abilities are adopted in the phrase-based SMT, in-
cluding phrase translation probability and lexical
weights in both directions. The phrase transla-
tion probability is based on co-occurrence statis-
tics and the lexical weights consider the phrase as
bag-of-words. In contrast, our BRAE model fo-
cuses on compositional semantics from words to
phrases. Therefore, the semantic similarities com-
puted using our BRAE model are complementary
to the existing four translation probabilities.
The semantic similarities in two directions
Sim(p
s
?
, p
t
) and Sim(p
t
?
, p
s
) are integrated into
our baseline phrase-based model. In order to in-
vestigate the influence of the dimensionality of the
embedding space, we have tried three different set-
tings n = 50, 100, 200.
As shown in Table 2, no matter what n is, the
BRAE model can significantly improve the trans-
lation quality in the overall test data. The largest
improvement can be up to 1.7 BLEU score (MT06
for n = 50). It is interesting that with dimen-
sionality growing, the translation performance is
not consistently improved. We speculate that us-
ing n = 50 or n = 100 can already distinguish
good translation candidates from bad ones.
4.5 Analysis on Semantic Phrase Embedding
To have a better intuition about the power of the
BRAE model at learning semantic phrase embed-
dings, we show some examples in Table 3. Given
the BRAE model and the phrase training set, we
search from the set the most semantically similar
English phrases for any new input English phrase.
The input phrases contain different number of
words. The table shows that the unsupervised
RAE can at most capture the syntactic property
when the phrases are short. For example, the
unsupervised RAE finds do not want for the in-
put phrase do not agree. When the phrase be-
comes longer, the unsupervised RAE cannot even
capture the syntactic property. In contrast, our
BRAE model learns the semantic meaning for
each phrase no matter whether it is short or rel-
atively long. This indicates that the proposed
BRAE model is effective at learning semantic
phrase embeddings.
5 Discussions
5.1 Applications of The BRAE model
As the semantic phrase embedding can fully rep-
resent the phrase, we can go a step further in the
phrase-based SMT and feed the semantic phrase
embeddings to DNN in order to model the whole
translation process (e.g. derivation structure pre-
diction). We will explore this direction in our fu-
ture work. Besides SMT, the semantic phrase em-
beddings can be used in other cross-lingual tasks,
such as cross-lingual question answering, since
the semantic similarity between phrases in differ-
ent languages can be calculated accurately.
In addition to the cross-lingual applications, we
believe the BRAE model can be applied in many
118
Method n MT03 MT04 MT05 MT06 MT08 ALL
Baseline 35.81 36.91 34.69 33.83 27.17 34.82
BRAE
50 36.43 37.64 35.35 35.53 28.59 35.84
+
100 36.45 37.44 35.58 35.42 28.57 36.03
+
200 36.34 37.35 35.78 34.87 27.84 35.62
+
Table 2: Experimental results of decoding with phrasal semantic similarities. n is the embedding dimen-
sionality. ?+? means that the model significantly outperforms the baseline with p < 0.01.
New Phrase Unsupervised RAE BRAE
military force
core force military power
main force military strength
labor force armed forces
at a meeting
to a meeting at the meeting
at a rate during the meeting
a meeting , at the conference
do not agree
one can accept do not favor
i can understand will not compromise
do not want not to approve
each people in this nation
each country regards every citizen in this country
each country has its all the people in the country
each other , and people all over the country
Table 3: Semantically similar phrases in the training set for the new phrases.
monolingual NLP tasks which depend on good
phrase representations or semantic similarity be-
tween phrases, such as named entity recognition,
parsing, textual entailment, question answering
and paraphrase detection.
5.2 Model Extensions
In fact, the phrases having the same meaning are
translation equivalents in different languages, but
are paraphrases in one language. Therefore, our
model can be easily adapted to learn semantic
phrase embeddings using paraphrases.
Our BRAE model still has some limitations.
For example, as each node in the recursive auto-
encoder shares the same weight matrix, the BRAE
model would become weak at learning the seman-
tic representations for long sentences with tens of
words. Improving the model to semantically em-
bed sentences is left for our future work.
6 Conclusions and Future Work
This paper has explored the bilingually-
constrained recursive auto-encoders in learning
phrase embeddings, which can distinguish phrases
with different semantic meanings. With the ob-
jective to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously, the learned model can semantically embed
any phrase in two languages and can transform
the semantic space in one language to the other.
Two end-to-end SMT tasks are involved to test
the power of the proposed model at learning the
semantic phrase embeddings. The experimental
results show that the BRAE model is remarkably
effective in phrase table pruning and decoding
with phrasal semantic similarities.
We have also discussed many other potential ap-
plications and extensions of our BRAE model. In
the future work, we will explore four directions.
1) we will try to model the decoding process with
DNN based on our semantic embeddings of the
basic translation units. 2) we are going to learn
semantic phrase embeddings with the paraphrase
corpus. 3) we will apply the BRAE model in other
monolingual and cross-lingual tasks. 4) we plan to
learn semantic sentence embeddings by automati-
cally learning different weight matrices for differ-
ent nodes in the BRAE model.
Acknowledgments
We thank Nan Yang for sharing the baseline
code and anonymous reviewers for their valu-
able comments. The research work has been
partially funded by the Natural Science Founda-
tion of China under Grant No. 61333018 and
61303181, and Hi-Tech Research and Develop-
ment Program (863 Program) of China under
Grant No. 2012AA011102.
119
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137?186.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language
Processing, 20(1):30?42.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 678?
683.
Matthias Eck, Stephen Vogal, and Alex Waibel. 2007.
Estimating phrase pair relevance for translation
model pruning. In MTSummit XI.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347?352.
John Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700?1709.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha?el Mathieu, and Yann L Cun.
2010. Learning convolutional feature hierarchies for
visual recognition. In Advances in neural informa-
tion processing systems, pages 1090?1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for itg-based translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Wang Ling, Joao Grac?a, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 962?971.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 791?801.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
120
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for
statistical machine translation. In Proceedings of
Summit XII, pages 144?151.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387?1392.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of ACL-
COLING, pages 505?512.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 972?983.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
121
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133?143,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Topic Representation for SMT with Neural Networks
?
Lei Cui
1
, Dongdong Zhang
2
, Shujie Liu
2
, Qiming Chen
3
, Mu Li
2
, Ming Zhou
2
, and Muyun Yang
1
1
School of Computer Science and Technology, Harbin Institute of Technology, Harbin, P.R. China
leicui@hit.edu.cn, ymy@mtlab.hit.edu.cn
2
Microsoft Research, Beijing, P.R. China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
3
Shanghai Jiao Tong University, Shanghai, P.R. China
simoncqm@gmail.com
Abstract
Statistical Machine Translation (SMT)
usually utilizes contextual information
to disambiguate translation candidates.
However, it is often limited to contexts
within sentence boundaries, hence broader
topical information cannot be leveraged.
In this paper, we propose a novel approach
to learning topic representation for paral-
lel data using a neural network architec-
ture, where abundant topical contexts are
embedded via topic relevant monolingual
data. By associating each translation rule
with the topic representation, topic rele-
vant rules are selected according to the dis-
tributional similarity with the source text
during SMT decoding. Experimental re-
sults show that our method significantly
improves translation accuracy in the NIST
Chinese-to-English translation task com-
pared to a state-of-the-art baseline.
1 Introduction
Making translation decisions is a difficult task in
many Statistical Machine Translation (SMT) sys-
tems. Current translation modeling approaches
usually use context dependent information to dis-
ambiguate translation candidates. For exam-
ple, translation sense disambiguation approaches
(Carpuat and Wu, 2005; Carpuat and Wu,
2007) are proposed for phrase-based SMT sys-
tems. Meanwhile, for hierarchical phrase-based
or syntax-based SMT systems, there is also much
work involving rich contexts to guide rule selec-
tion (He et al, 2008; Liu et al, 2008; Marton
and Resnik, 2008; Xiong et al, 2009). Although
these methods are effective and proven successful
in many SMT systems, they only leverage within-
?
This work was done while the first and fourth authors
were visiting Microsoft Research.
sentence contexts which are insufficient in explor-
ing broader information. For example, the word
driver often means ?the operator of a motor ve-
hicle? in common texts. But in the sentence ?Fi-
nally, we write the user response to the buffer, i.e.,
pass it to our driver?, we understand that driver
means ?computer program?. In this case, people
understand the meaning because of the IT topical
context which goes beyond sentence-level analy-
sis and requires more relevant knowledge. There-
fore, it is important to leverage topic information
to learn smarter translation models and achieve
better translation performance.
Topic modeling is a useful mechanism for dis-
covering and characterizing various semantic con-
cepts embedded in a collection of documents. At-
tempts on topic-based translation modeling in-
clude topic-specific lexicon translation models
(Zhao and Xing, 2006; Zhao and Xing, 2007),
topic similarity models for synchronous rules
(Xiao et al, 2012), and document-level translation
with topic coherence (Xiong and Zhang, 2013). In
addition, topic-based approaches have been used
in domain adaptation for SMT (Tam et al, 2007;
Su et al, 2012), where they view different topics
as different domains. One typical property of these
approaches in common is that they only utilize
parallel data where document boundaries are ex-
plicitly given. In this way, the topic of a sentence
can be inferred with document-level information
using off-the-shelf topic modeling toolkits such
as Latent Dirichlet Allocation (LDA) (Blei et al,
2003) or Hidden Topic Markov Model (HTMM)
(Gruber et al, 2007). Most of them also assume
that the input must be in document level. However,
this situation does not always happen since there is
considerable amount of parallel data which does
not have document boundaries. In addition, con-
temporary SMT systems often works on sentence
level rather than document level due to the effi-
ciency. Although we can easily apply LDA at the
133
sentence level, it is quite difficult to infer the topic
accurately with only a few words in the sentence.
This makes previous approaches inefficient when
applied them in real-world commercial SMT sys-
tems. Therefore, we need to devise a systematical
approach to enriching the sentence and inferring
its topic more accurately.
In this paper, we propose a novel approach to
learning topic representations for sentences. Since
the information within the sentence is insufficient
for topic modeling, we first enrich sentence con-
texts via Information Retrieval (IR) methods using
content words in the sentence as queries, so that
topic-related monolingual documents can be col-
lected. These topic-related documents are utilized
to learn a specific topic representation for each
sentence using a neural network based approach.
Neural network is an effective technique for learn-
ing different levels of data representations. The
levels inferred from neural network correspond to
distinct levels of concepts, where high-level rep-
resentations are obtained from low-level bag-of-
words input. It is able to detect correlations among
any subset of input features through non-linear
transformations, which demonstrates the superior-
ity of eliminating the effect of noisy words which
are irrelevant to the topic. Our problem fits well
into the neural network framework and we expect
that it can further improve inferring the topic rep-
resentations for sentences.
To incorporate topic representations as trans-
lation knowledge into SMT, our neural network
based approach directly optimizes similarities be-
tween the source language and target language in a
compact topic space. This underlying topic space
is learned from sentence-level parallel data in or-
der to share topic information across the source
and target languages as much as possible. Addi-
tionally, our model can be discriminatively trained
with a large number of training instances, without
expensive sampling methods such as in LDA or
HTMM, thus it is more practicable and scalable.
Finally, we associate the learned representation to
each bilingual translation rule. Topic-related rules
are selected according to distributional similarity
with the source text, which helps hypotheses gen-
eration in SMT decoding. We integrate topic simi-
larity features in the log-linear model and evaluate
the performance on the NIST Chinese-to-English
translation task. Experimental results demonstrate
that our model significantly improves translation
accuracy over a state-of-the-art baseline.
2 Background: Deep Learning
Deep learning is an active topic in recent years
which has triumphed in many machine learning
research areas. This technique began raising pub-
lic awareness in the mid-2000s after researchers
showed how a multi-layer feed-forward neural
network can be effectively trained. The train-
ing procedure often involves two phases: a layer-
wise unsupervised pre-training phase and a su-
pervised fine-tuning phase. For pre-training, Re-
stricted Boltzmann Machine (RBM) (Hinton et
al., 2006), auto-encoding (Bengio et al, 2006)
and sparse coding (Lee et al, 2006) are most fre-
quently used. Unsupervised pre-training trains the
network one layer at a time and helps guide the pa-
rameters of the layer towards better regions in pa-
rameter space (Bengio, 2009). Followed by fine-
tuning in this parameter region, deep learning is
able to achieve state-of-the-art performance in var-
ious research areas, including breakthrough results
on the ImageNet dataset for objective recognition
(Krizhevsky et al, 2012), significant error reduc-
tion in speech recognition (Dahl et al, 2012), etc.
Deep learning has also been successfully ap-
plied in a variety of NLP tasks such as part-of-
speech tagging, chunking, named entity recog-
nition, semantic role labeling (Collobert et al,
2011), parsing (Socher et al, 2011a), sentiment
analysis (Socher et al, 2011b), etc. Most NLP
research converts a high-dimensional and sparse
binary representation into a low-dimensional and
real-valued representation. This low-dimensional
representation is usually learned from huge
amount of monolingual texts in the pre-training
phase, and then fine-tuned towards task-specific
criterion. Inspired by previous successful re-
search, we first learn sentence representations us-
ing topic-related monolingual texts in the pre-
training phase, and then optimize the bilingual
similarity by leveraging sentence-level parallel
data in the fine-tuning phase.
3 Topic Similarity Model with Neural
Network
In this section, we explain our neural network
based topic similarity model in detail, as well as
how to incorporate the topic similarity features
into SMT decoding procedure. Figure 1 sketches
the high-level overview which illustrates how to
134
?? = ?(?) ?? = ?(?) 
cos(?? , ??) 
???(?, ?) 
 ?  ? 
 English document collection 
 ??  ?? 
Parallel sentence  
IR IR 
? ? 
 Chinese document collection 
Neural Network Training 
Data Preprocessing 
Figure 1: Overview of neural network based topic
similarity model.
learn topic representations using sentence-level
parallel data. Given a parallel sentence pair ?f, e?,
the first step is to treat f and e as queries, and
use IR methods to retrieve relevant documents to
enrich contextual information for them. Specifi-
cally, the ranking model we used is a Vector Space
Model (VSM), where the query and document are
converted into tf-idf weighted vectors. The most
relevant N documents d
f
and d
e
are retrieved and
converted to a high-dimensional, bag-of-words in-
put f and e for the representation learning
1
.
There are two phases in our neural network
training process: pre-training and fine-tuning. In
the pre-training phase (Section 3.1), we build two
neural networks with the same structure but differ-
ent parameters to learn a low-dimensional repre-
sentation for sentences in two different languages.
Then, in the fine-tuning phase (Section 3.2), our
model directly optimizes the similarity of two low-
dimensional representations, so that it highly cor-
relates to SMT decoding. Finally, the learned rep-
resentation is used to calculate similarities which
are integrated as features in SMT decoding proce-
dure (Section 3.3).
3.1 Pre-training using denoising
auto-encoder
In the pre-training phase, we leverage neural
network structures to transform high-dimensional
sparse vectors to low-dimensional dense vectors.
The topic similarity is calculated on top of the
learned dense vectors. This dense representation
should preserve the information from the bag-of-
1
We use f and e to denote the n-of-V vector converted
from the retrieved documents.
words input, meanwhile alleviate data sparse prob-
lem. Therefore, we use a specially designed mech-
anism called auto-encoder to solve this problem.
Auto-encoder (Bengio et al, 2006) is one of the
basic building blocks of deep learning. Assum-
ing that the input is a n-of-V binary vector x rep-
resenting the bag-of-words (V is the vocabulary
size), an auto-encoder consists of an encoding pro-
cess g(x) and a decoding process h(g(x)). The
objective of the auto-encoder is to minimize the
reconstruction error L(h(g(x)), x). Our goal is to
learn a low-dimensional vector which can preserve
information from the original n-of-V vector.
One problem with auto-encoder is that it treats
all words in the same way, making no distinguish-
ment between function words and content words.
The representation learned by auto-encoders tends
to be influenced by the function words, thereby it
is not robust. To alleviate this problem, Vincent et
al. (2008) proposed the Denoising Auto-Encoder
(DAE), which aims to reconstruct a clean, ?re-
paired? input from a corrupted, partially destroyed
vector. This is done by corrupting the initial in-
put x to get a partially destroyed version
?
x. DAE
is capable of capturing the global structure of the
input while ignoring the noise. In our task, for
each sentence, we treat the retrieved N relevant
documents as a single large document and convert
it to a bag-of-words vector x in Figure 2. With
DAE, the input x is manually corrupted by apply-
ing masking noise (randomly mask 1 to 0) and get-
ting
?
x. Denoising training is considered as ?filling
in the blanks? (Vincent et al, 2010), which means
the masking components can be recovered from
the non-corrupted components. For example, in
IT related texts, if the word driver is masked, it
should be predicted through hidden units in neural
networks by active signals such as ?buffer?, ?user
response?, etc.
In our case, the encoding process transforms
the corrupted input
?
x into g(
?
x) with two layers:
a linear layer connected with a non-linear layer.
Assuming that the dimension of the g(
?
x) is L,
the linear layer forms a L ? V matrix W which
projects the n-of-V vector to a L-dimensional hid-
den layer. After the bag-of-words input has been
transformed, they are fed into a subsequent layer
to model the highly non-linear relations among
words:
z = f(W
?
x + b) (1)
where z is the output of the non-linear layer, b is a
135
? ?? 
?(??) 
(?? ??) 
?(?? ?? ,?) 
Figure 2: Denoising auto-encoder with a bag-of-
words input.
L-length bias vector. f(?) is a non-linear function,
where common choices include sigmoid function,
hyperbolic function, ?hard? hyperbolic function,
rectifier function, etc. In this work, we use the
rectifier function as our non-linear function due to
its efficiency and better performance (Glorot et al,
2011):
rec(x) =
{
x if x > 0
0 otherwise
(2)
The decoding process consists of a linear layer
and a non-linear layer with similar network struc-
tures, but different parameters. It transforms the
L-dimensional vector g(
?
x) to a V -dimensional
vector h(g(
?
x)). To minimize reconstruction error
with respect to
?
x, we define the loss function as
the L2-norm of the difference between the uncor-
rupted input and reconstructed input:
L(h(g(
?
x)), x) = ?h(g(
?
x))? x?
2
(3)
Multi-layer neural networks are trained with the
standard back-propagation algorithm (Rumelhart
et al, 1988). The gradient of the loss function
is calculated and back-propagated to the previous
layer to update its parameters. Training neural net-
works involves many factors such as the learning
rate and the length of hidden layers. We will dis-
cuss the optimization of these parameters in Sec-
tion 4.
3.2 Fine-tuning with parallel data
In the fine-tuning phase, we stack another layer on
top of the two low-dimensional vectors to maxi-
mize the similarity between source and target lan-
guages. The similarity scores are integrated into
the standard log-linear model for making transla-
tion decisions. Since the vectors from DAE are
trained using information from monolingual train-
ing data independently, these vectors may be in-
adequate to measure bilingual topic similarity due
to their different topic spaces. Therefore, in this
stage, parallel sentence pairs are used to help con-
necting the vectors from different languages be-
cause they express the same topic. In fact, the ob-
jective of fine-tuning is to discover a latent topic
space which is shared by both languages as much
as possible. This shared topic space is particularly
useful when the SMT decoder tries to match the
source texts and translation candidates in the tar-
get language.
Given a parallel sentence pair ?f, e?, the DAE
learns representations for f and e respectively, as
z
f
= g(f) and z
e
= g(e) in Figure 1. We then take
two vectors as the input to calculate their similar-
ity. Consequently, the whole neural network can
be fine-tuned towards the supervised criteria with
the help of parallel data. The similarity score of
the representation pair ?z
f
, z
e
? is defined as the co-
sine similarity of the two vectors:
sim(f, e) = cos(z
f
, z
e
)
=
z
f
? z
e
?z
f
??z
e
?
(4)
Since a parallel sentence pair should have the
same topic, our goal is to maximize the similar-
ity score between the source sentence and target
sentence. Inspired by the contrastive estimation
method (Smith and Eisner, 2005), for each paral-
lel sentence pair ?f, e? as a positive instance, we
select another sentence pair ?f
?
, e
?
? from the train-
ing data and treat ?f, e
?
? as a negative instance. To
make the similarity of the positive instance larger
than the negative instance by some margin ?, we
utilize the following pairwise ranking loss:
L(f, e) = max{0, ? ? sim(f, e) + sim(f, e
?
)}
(5)
where ? =
1
2
? sim(f, f
?
). The rationale behind
this criterion is, the smaller sim(f, f
?
) is, the more
we should penalize negative instances.
To effectively train the model in this task, neg-
ative instances must be selected carefully. Since
different sentences may have very similar topic
distributions, we select negative instances that are
dissimilar with the positive instances based on the
following criteria:
1. For each positive instance ?f, e?, we select e
?
which contains at least 30% different content
words from e.
136
2. If we cannot find such e
?
, remove ?f, e? from
the training instances for network learning.
The model minimizes the pairwise ranking loss
across all training instances:
L =
?
?f,e?
L(f, e) (6)
We used standard back-propagation algorithm
to further fine-tune the neural network parameters
W and b in Equation (1). The learned neural net-
works are used to obtain sentence topic representa-
tions, which will be further leveraged to infer topic
representations of bilingual translation rules.
3.3 Integration into SMT decoding
We incorporate the learned topic similarity scores
into the standard log-linear framework for SMT.
When a synchronous rule ??, ?? is extracted from
a sentence pair ?f, e?, a triple instance I =
(??, ??, ?f, e?, c) is collected for inferring the
topic representation of ??, ??, where c is the count
of rule occurrence. Following (Chiang, 2007), we
give a count of one for each phrase pair occurrence
and a fractional count for each hierarchical phrase
pair. The topic representation of ??, ?? is then cal-
culated as the weighted average:
z
?
=
?
(??,??,?f,e?,c)?T
{c? z
f
}
?
(??,??,?f,e?,c)?T
{c}
(7)
z
?
=
?
(??,??,?f,e?,c)?T
{c? z
e
}
?
(??,??,?f,e?,c)?T
{c}
(8)
where T denotes all instances for the rule ??, ??,
z
?
and z
?
are the source-side and target-side topic
vectors respectively.
By measuring the similarity between the source
texts and bilingual translation rules, the SMT de-
coder is able to encourage topic relevant transla-
tion candidates and penalize topic irrelevant candi-
dates. Therefore, it helps to train a smarter transla-
tion model with the embedded topic information.
Given a source sentence s to be translated, we de-
fine the similarity as follows:
Sim(z
s
, z
?
) = cos(z
s
, z
?
) (9)
Sim(z
s
, z
?
) = cos(z
s
, z
?
) (10)
where z
s
is the topic representation of s. The
similarity calculated against z
?
or z
?
denotes the
source-to-source or the source-to-target similarity.
We also consider the topic sensitivity estimation
since general rules have flatter distributions while
topic-specific rules have sharper distributions. A
standard entropy metric is used to measure the sen-
sitivity of the source-side of ??, ?? as:
Sen(?) = ?
|z
?
|
?
i=1
z
?i
? log z
?i
(11)
where z
?i
is a component in the vector z
?
. The
target-side sensitivity Sen(?) can be calculated in
a similar way. The larger the sensitivity is, the
more topic-specific the rule manifests.
In addition to traditional SMT features, we add
new topic-related features into the standard log-
linear framework. For the SMT system, the best
translation candidate e? is given by:
e? = argmax
e
P (e|f) (12)
where the translation probability is given by:
P (e|f) ?
?
i
w
i
? log ?
i
(f, e)
=
?
j
w
j
? log ?
j
(f, e)
? ?? ?
Standard
+
?
k
w
k
? log ?
k
(f, e)
? ?? ?
Topic related
(13)
where ?
j
(f, e) is the standard feature function and
w
j
is the corresponding feature weight. ?
k
(f, e)
is the topic-related feature function and w
k
is the
feature weight. The detailed feature description is
as follows:
Standard features: Translation model, includ-
ing translation probabilities and lexical weights
for both directions (4 features), 5-gram language
model (1 feature), word count (1 feature), phrase
count (1 feature), NULL penalty (1 feature), num-
ber of hierarchical rules used (1 feature).
Topic-related features: rule similarity scores
(2 features), rule sensitivity scores (2 features).
4 Experiments
4.1 Setup
We evaluate the performance of our neural net-
work based topic similarity model on a Chinese-
to-English machine translation task. In neural net-
work training, a large number of monolingual doc-
uments are collected in both source and target lan-
guages. The documents are mainly from two do-
mains: news and weblog. We use Chinese and
137
English Gigaword corpus (Version 5) which are
mainly from news domain. In addition, we also
collect weblog documents with a variety of top-
ics from the web. The total data statistics are
presented in Table 1. These documents are built
in the format of inverted index using Lucene
2
,
which can be efficiently retrieved by the paral-
lel sentence pairs. The most relevant N docu-
ments are collected, where we experiment with
N = {1, 5, 10, 20, 50}.
Domain
Chinese English
Docs Words Docs Words
News 5.7M 5.4B 9.9M 25.6B
Weblog 2.1M 8B 1.2M 2.9B
Total 7.8M 13.4B 11.1M 28.5B
Table 1: Statistics of monolingual data, in num-
bers of documents and words (main content). ?M?
refers to million and ?B? refers to billion.
We implement a distributed framework to speed
up the training process of neural networks. The
network is learned with mini-batch asynchronous
gradient descent with the adaptive learning rate
procedure called AdaGrad (Duchi et al, 2011).
We use 32 model replicas in each iteration during
the training. The model parameters are averaged
after each iteration and sent to each replica for the
next iteration. The vocabulary size for the input
layer is 100,000, and we choose different lengths
for the hidden layer as L = {100, 300, 600, 1000}
in the experiments. In the pre-training phase, all
parallel data is fed into two neural networks re-
spectively for DAE training, where network pa-
rameters W and b are randomly initialized. In
the fine-tuning phase, for each parallel sentence
pair, we randomly select other ten sentence pairs
which satisfy the criterion as negative instances.
These training instances are leveraged to optimize
the similarity of two vectors.
In SMT training, an in-house hierarchical
phrase-based SMT decoder is implemented for our
experiments. The CKY decoding algorithm is
used and cube pruning is performed with the same
default parameter settings as in Chiang (2007).
The parallel data we use is released by LDC
3
. In
total, the datasets contain nearly 1.1 million sen-
tence pairs. Translation models are trained over
the parallel data that is automatically word-aligned
2
http://lucene.apache.org/
3
LDC2003E14, LDC2002E18, LDC2003E07,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34,
LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09
using GIZA++ in both directions, and the diag-
grow-final heuristic is used to refine symmetric
word alignment. An in-house language modeling
toolkit is used to train the 5-gram language model
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). The English monolingual data used
for language modeling is the same as in Table
1. The NIST 2003 dataset is the development
data. The testing data consists of NIST 2004,
2005, 2006 and 2008 datasets. The evaluation
metric for the overall translation quality is case-
insensitive BLEU4 (Papineni et al, 2002). The
reported BLEU scores are averaged over 5 times
of running MERT (Och, 2003). A statistical sig-
nificance test is performed using the bootstrap re-
sampling method (Koehn, 2004).
4.2 Baseline
The baseline is a re-implementation of the Hiero
system (Chiang, 2007). The phrase pairs that ap-
pear only once in the parallel data are discarded
because most of them are noisy. We also use
the fix-discount method in Foster et al (2006)
for phrase table smoothing. This implementation
makes the system perform much better and the
translation model size is much smaller.
We compare our method with the LDA-based
approach proposed by Xiao et al (2012). In (Xiao
et al, 2012), the topic of each sentence pair is ex-
actly the same as the document it belongs to. Since
some of our parallel data does not have document-
level information, we rely on the IR method to
retrieve the most relevant document and simulate
this approach. The PLDA toolkit (Liu et al, 2011)
is used to infer topic distributions, which takes
34.5 hours to finish.
4.3 Effect of retrieved documents and length
of hidden layers
We illustrate the relationship among translation
accuracy (BLEU), the number of retrieved docu-
ments (N ) and the length of hidden layers (L) on
different testing datasets. The results are shown in
Figure 3. The best translation accuracy is achieved
when N=10 for most settings. This confirms that
enriching the source text with topic-related doc-
uments is very useful in determining topic repre-
sentations, thereby help to guide the synchronous
rule selection. However, we find that as N be-
comes larger in the experiments, e.g. N=50, the
translation accuracy drops drastically. As more
documents are retrieved, less relevant information
138
0 5 10 20 5042
42.2
42.4
42.6
42.8
43
Number of Retrieved Documents (N)
BLE
U
NIST 2004
 
 L=100L=300L=600L=1000
0 5 10 20 5041
41.2
41.4
41.6
41.8
42
Number of Retrieved Documents (N)
BLE
U
NIST 2005
 
 L=100L=300L=600L=1000
0 5 10 20 5037.8
38
38.2
38.4
38.6
38.8
39
39.2
Number of Retrieved Documents (N)
BLE
U
NIST 2006
 
 L=100L=300L=600L=1000
0 5 10 20 5031
31.2
31.4
31.6
31.8
32
Number of Retrieved Documents (N)
BLE
U
NIST 2008
 
 L=100L=300L=600L=1000
Figure 3: End-to-end translation results (BLEU%) using all standard and topic-related features, with
different settings on the number of retrieved documents N and the length of hidden layers L.
is also used to train the neural networks. Irrel-
evant documents bring so many unrelated topic
words hence degrade neural network learning per-
formance.
Another important factor is the length of hid-
den layers L in the network. In deep learning, this
parameter is often empirically tuned with human
efforts. As shown in Figure 3, the translation accu-
racy is better when L is relatively small. Actually,
there is no obvious distinction of the performance
when L is less than 600. However, when L equals
1,000, the translation accuracy is inferior to other
settings. The main reason is that parameters in
the neural networks are too many to be effectively
trained. As we know when L=1000, there are a
total of 100, 000? 1, 000 parameters between the
linear and non-linear layers in the network. Lim-
ited training data prevents the model from getting
close to the global optimum. Therefore, the model
is likely to fall in local optima and lead to unac-
ceptable representations.
4.4 Effect of topic related features
We evaluate the performance of adding new topic-
related features to the log-linear model and com-
pare the translation accuracy with the method in
(Xiao et al, 2012). To make different methods
comparable, we set the dimension of topic rep-
resentation as 100 for all settings. This takes 10
hours in pre-training phase and 22 hours in fine-
tuning phase. Table 2 shows how the accuracy is
improved with more features added. The results
confirm that topic information is indispensable for
SMT since both (Xiao et al, 2012) and our neural
network based method significantly outperforms
the baseline system. Our method improves 0.86
BLEU points at most and 0.76 BLEU points on
average over the baseline. We observe that source-
side similarity is more effective than target-side
similarity, but their contributions are cumulative.
This proves that bilingually induced topic repre-
sentation with neural network helps the SMT sys-
tem disambiguate translation candidates. Further-
more, rule sensitivity features improve SMT per-
formance compared with only using similarity fea-
tures. Because topic-specific rules usually have a
larger sensitivity score, they can beat general rules
when they obtain the same similarity score against
the input sentence. Finally, when all new fea-
tures are integrated, the performance is the best,
preforming substantially better than (Xiao et al,
2012) with 0.39 BLEU points on average.
It is worth mentioning that the performance
of (Xiao et al, 2012) is similar to the settings
with N=1 and L=100 in Figure 3. This is not
simply coincidence since we can interpret their
approach as a special case in our neural net-
work method: when a parallel sentence pair has
139
Settings NIST 2004 NIST 2005 NIST 2006 NIST 2008 Average
Baseline 42.25 41.21 38.05 31.16 38.17
(Xiao et al, 2012) 42.58 41.61 38.39 31.58 38.54
Sim(Src) 42.51 41.55 38.53 31.57 38.54
Sim(Trg) 42.43 41.48 38.4 31.49 38.45
Sim(Src+Trg) 42.7 41.66 38.66 31.66 38.67
Sim(Src+Trg)+Sen(Src) 42.77 41.81 38.85 31.73 38.79
Sim(Src+Trg)+Sen(Trg) 42.85 41.79 38.76 31.7 38.78
Sim(Src+Trg)+Sen(Src+Trg) 42.95 41.97 38.91 31.88 38.93
Table 2: Effectiveness of different features in BLEU% (p < 0.05), with N=10 and L=100. ?Sim?
denotes the rule similarity feature and ?Sen? denotes rule sensitivity feature. ?Src? and ?Trg? means
utilizing source-side/target-side rule topic vectors to calculate similarity or sensitivity, respectively. The
?Average? setting is the averaged result of four datasets.
document-level information, that document will
be retrieved for training; otherwise, the most rel-
evant document will be retrieved from the mono-
lingual data. Therefore, our method can be viewed
as a more general framework than previous LDA-
based approaches.
4.5 Discussion
In this section, we give a case study to explain
why our method works. An example of transla-
tion rule disambiguation for a sentence from the
NIST 2005 dataset is shown in Figure 4. We find
that the topic of this sentence is about ?rescue af-
ter a natural disaster?. Under this topic, the Chi-
nese rule ??? X? should be translated to ?de-
liver X? or ?distribute X?. However, the baseline
system prefers ?send X? rather than those two can-
didates. Although the translation probability of
?send X? is much higher, it is inappropriate in this
context since it is usually used in IT texts. For
example, ?????, send emails?, ?????,
send messages? and ?????, send data?. In
contrast, with our neural network based approach,
the learned topic distributions of ?deliver X? or
?distribute X? are more similar with the input sen-
tence than ?send X?, which is shown in Figure 4.
The similarity scores indicate that ?deliver X? and
?distribute X? are more appropriate to translate the
sentence. Therefore, adding topic-related features
is able to keep the topic consistency and substan-
tially improve the translation accuracy.
5 Related Work
Topic modeling was first leveraged to improve
SMT performance in (Zhao and Xing, 2006; Zhao
and Xing, 2007). They proposed a bilingual
topical admixture approach for word alignment
and assumed that each word-pair follows a topic-
specific model. They reported extensive empir-
ical analysis and improved word alignment ac-
curacy as well as translation quality. Follow-
ing this work, (Xiao et al, 2012) extended topic-
specific lexicon translation models to hierarchical
phrase-based translation models, where the topic
information of synchronous rules was directly in-
ferred with the help of document-level informa-
tion. Experiments show that their approach not
only achieved better translation performance but
also provided a faster decoding speed compared
with previous lexicon-based LDA methods.
Another direction of approaches leveraged topic
modeling techniques for domain adaptation. Tam
et al (2007) used bilingual LSA to learn latent
topic distributions across different languages and
enforce one-to-one topic correspondence during
model training. They incorporated the bilingual
topic information into language model adaptation
and lexicon translation model adaptation, achiev-
ing significant improvements in the large-scale
evaluation. (Su et al, 2012) investigated the rela-
tionship between out-of-domain bilingual data and
in-domain monolingual data via topic mapping
using HTMM methods. They estimated phrase-
topic distributions in translation model adaptation
and generated better translation quality. Recently,
Chen et al (2013) proposed using vector space
model for adaptation where genre resemblance is
leveraged to improve translation accuracy. We
also investigated multi-domain adaptation where
explicit topic information is used to train domain
specific models (Cui et al, 2013).
Generally, most previous research has leveraged
conventional topic modeling techniques such as
LDA or HTMM. In our work, a novel neural net-
work based approach is proposed to infer topic
representations for parallel data. The advantage of
140
S
rc
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
R
ef
(1
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
al
so
be
gu
n
de
li
ve
ri
ng
ba
si
c
m
ed
ic
al
ki
ts
(2
)
th
e
un
ic
ef
ha
s
al
so
st
ar
te
d
to
di
st
ri
bu
te
ba
si
c
m
ed
ic
al
ki
ts
(3
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
al
so
be
gu
n
di
st
ri
bu
ti
ng
ba
si
c
m
ed
ic
al
ki
ts
(4
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
be
gu
n
de
li
ve
ri
ng
ba
si
c
m
ed
ic
al
ki
ts
B
as
el
in
e
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
be
ga
n
to
se
nd
ba
si
c
m
ed
ic
al
ki
ts
O
ur
s
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
be
gu
n
to
di
st
ri
bu
te
ba
si
c
m
ed
ic
al
ki
ts
T
ab
le
4:
A
ck
n
ow
le
d
gm
en
ts
T
he
ac
kn
ow
le
dg
m
en
ts
sh
ou
ld
go
im
m
ed
ia
te
ly
be
-
fo
re
th
e
re
fe
re
nc
es
.
D
o
no
t
nu
m
be
r
th
e
ac
kn
ow
l-
ed
gm
en
ts
se
ct
io
n.
D
o
no
t
in
cl
ud
e
th
is
se
ct
io
n
w
he
n
su
bm
it
ti
ng
yo
ur
pa
pe
r
fo
r
re
vi
ew
.
R
ef
er
en
ce
s
Y
os
hu
a
B
en
gi
o,
P
as
ca
l
L
am
bl
in
,
D
an
P
op
ov
ic
i,
an
d
H
ug
o
L
ar
oc
he
ll
e.
20
06
.
G
re
ed
y
la
ye
r-
w
is
e
tr
ai
n-
in
g
of
de
ep
ne
tw
or
ks
.
In
B
.
S
ch
o?l
ko
pf
,
J.
P
la
tt
,
an
d
T
.
H
of
fm
an
,
ed
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
a-
ti
on
P
ro
ce
ss
in
g
Sy
st
em
s
19
,
pa
ge
s
15
3?
16
0.
M
IT
P
re
ss
,C
am
br
id
ge
,M
A
.
Y
os
hu
a
B
en
gi
o.
20
09
.
L
ea
rn
in
g
de
ep
ar
ch
it
ec
tu
re
s
fo
r
ai
.
Fo
un
d.
Tr
en
ds
M
ac
h.
L
ea
rn
.,
2(
1)
:1
?1
27
,
Ja
n-
ua
ry
.
D
av
id
M
.
B
le
i,
A
nd
re
w
Y
.
N
g,
an
d
M
ic
ha
el
I.
Jo
rd
an
.
20
03
.
L
at
en
t
di
ri
ch
le
t
al
lo
ca
ti
on
.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
3:
99
3?
10
22
,M
ar
ch
.
M
ar
in
e
C
ar
pu
at
an
d
D
ek
ai
W
u.
20
07
.
C
on
te
xt
-
de
pe
nd
en
t
ph
ra
sa
l
tr
an
sl
at
io
n
le
xi
co
ns
fo
r
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
P
ro
ce
ed
in
gs
of
M
ac
hi
ne
Tr
an
s-
la
ti
on
Su
m
m
it
X
I,
pa
ge
s
73
?8
0.
D
av
id
C
hi
an
g.
20
07
.
H
ie
ra
rc
hi
ca
l
ph
ra
se
-b
as
ed
tr
an
s-
la
ti
on
.
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
,3
3(
2)
:2
01
?2
28
.
R
on
an
C
ol
lo
be
rt
,J
as
on
W
es
to
n,
L
e?o
n
B
ot
to
u,
M
ic
ha
el
K
ar
le
n,
K
or
ay
K
av
uk
cu
og
lu
,
an
d
P
av
el
K
uk
sa
.
20
11
.
N
at
ur
al
la
ng
ua
ge
pr
oc
es
si
ng
(a
lm
os
t)
fr
om
sc
ra
tc
h.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
12
:2
49
3?
25
37
,
N
ov
em
be
r.
G
.
E
.
D
ah
l,
D
on
g
Y
u,
L
i
D
en
g,
an
d
A
.
A
ce
ro
.
20
12
.
C
on
te
xt
-d
ep
en
de
nt
pr
e-
tr
ai
ne
d
de
ep
ne
ur
al
ne
tw
or
ks
fo
r
la
rg
e-
vo
ca
bu
la
ry
sp
ee
ch
re
co
gn
it
io
n.
Tr
an
s.
A
u-
di
o,
Sp
ee
ch
an
d
L
an
g.
P
ro
c.
,2
0(
1)
:3
0?
42
,J
an
ua
ry
.
Jo
hn
D
uc
hi
,
E
la
d
H
az
an
,
an
d
Y
or
am
S
in
ge
r.
20
11
.
A
da
pt
iv
e
su
bg
ra
di
en
t
m
et
ho
ds
fo
r
on
li
ne
le
ar
ni
ng
an
d
st
oc
ha
st
ic
op
ti
m
iz
at
io
n.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
12
:2
12
1?
21
59
,J
ul
y.
G
eo
rg
e
F
os
te
r,
R
ol
an
d
K
uh
n,
an
d
H
ow
ar
d
Jo
hn
so
n.
20
06
.
P
hr
as
et
ab
le
sm
oo
th
in
g
fo
r
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
20
06
C
on
fe
re
nc
e
on
E
m
pi
ri
ca
l
M
et
ho
ds
in
N
at
ur
al
L
an
gu
ag
e
P
ro
-
ce
ss
in
g,
pa
ge
s
53
?6
1,
S
yd
ne
y,
A
us
tr
al
ia
,
Ju
ly
.
A
s-
so
ci
at
io
n
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
A
m
it
G
ru
be
r,
M
ic
ha
lR
os
en
-z
vi
,a
nd
Y
ai
r
W
ei
ss
.
20
07
.
H
id
de
n
to
pi
c
m
ar
ko
v
m
od
el
s.
In
In
P
ro
ce
ed
in
gs
of
A
rt
ifi
ci
al
In
te
ll
ig
en
ce
an
d
St
at
is
ti
cs
.
Z
ho
ng
ju
n
H
e,
Q
un
L
iu
,
an
d
S
ho
ux
un
L
in
.
20
08
.
Im
-
pr
ov
in
g
st
at
is
ti
ca
lm
ac
hi
ne
tr
an
sl
at
io
n
us
in
g
le
xi
ca
l-
iz
ed
ru
le
se
le
ct
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
22
nd
In
-
te
rn
at
io
na
l
C
on
fe
re
nc
e
on
C
om
pu
ta
ti
on
al
L
in
gu
is
-
ti
cs
(C
ol
in
g
20
08
),
pa
ge
s
32
1?
32
8,
M
an
ch
es
te
r,
U
K
,
A
ug
us
t.
C
ol
in
g
20
08
O
rg
an
iz
in
g
C
om
m
it
te
e.
G
eo
ff
re
y
E
.
H
in
to
n,
S
im
on
O
si
nd
er
o,
an
d
Y
ee
-W
hy
e
T
eh
.
20
06
.
A
fa
st
le
ar
ni
ng
al
go
ri
th
m
fo
r
de
ep
be
li
ef
ne
ts
.
N
eu
ra
l
C
om
pu
t.
,1
8(
7)
:1
52
7?
15
54
,J
ul
y.
R
ei
nh
ar
d
K
ne
se
r
an
d
H
er
m
an
n
N
ey
.
19
95
.
Im
-
pr
ov
ed
ba
ck
in
g-
of
f
fo
r
m
-g
ra
m
la
ng
ua
ge
m
od
el
in
g.
In
A
co
us
ti
cs
,
Sp
ee
ch
,
an
d
Si
gn
al
P
ro
ce
ss
in
g,
19
95
.
IC
A
SS
P
-9
5.
,1
99
5
In
te
rn
at
io
na
lC
on
fe
re
nc
e
on
,v
ol
-
um
e
1,
pa
ge
s
18
1?
18
4.
IE
E
E
.
P
hi
li
pp
K
oe
hn
.
20
04
.
S
ta
ti
st
ic
al
si
gn
ifi
ca
nc
e
te
st
s
fo
r
m
ac
hi
ne
tr
an
sl
at
io
n
ev
al
ua
ti
on
.
In
D
ek
an
g
L
in
an
d
D
ek
ai
W
u,
ed
it
or
s,
P
ro
ce
ed
in
gs
of
E
M
N
L
P
20
04
,
pa
ge
s
38
8?
39
5,
B
ar
ce
lo
na
,
S
pa
in
,
Ju
ly
.
A
ss
oc
ia
ti
on
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
A
le
x
K
ri
zh
ev
sk
y,
Il
ya
S
ut
sk
ev
er
,
an
d
G
eo
ff
H
in
to
n.
20
12
.
Im
ag
en
et
cl
as
si
fi
ca
ti
on
w
it
h
de
ep
co
nv
ol
u-
ti
on
al
ne
ur
al
ne
tw
or
ks
.
In
P.
B
ar
tl
et
t,
F.
C
.N
.P
er
ei
ra
,
C
.J
.C
.
B
ur
ge
s,
L
.
B
ot
to
u,
an
d
K
.Q
.
W
ei
nb
er
ge
r,
ed
-
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
at
io
n
P
ro
ce
ss
in
g
Sy
st
em
s
25
,p
ag
es
11
06
?1
11
4.
H
on
gl
ak
L
ee
,
A
le
xi
s
B
at
tl
e,
R
aj
at
R
ai
na
,
an
d
A
n-
dr
ew
Y
.
N
g.
20
06
.
E
ffi
ci
en
t
sp
ar
se
co
di
ng
al
go
-
ri
th
m
s.
In
B
.
S
ch
o?l
ko
pf
,
J.
P
la
tt
,
an
d
T
.
H
of
fm
an
,
ed
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
at
io
n
P
ro
ce
ss
in
g
Sy
st
em
s
19
,p
ag
es
80
1?
80
8.
M
IT
P
re
ss
,C
am
br
id
ge
,
M
A
.
Q
un
L
iu
,
Z
ho
ng
ju
n
H
e,
Y
an
g
L
iu
,
an
d
S
ho
ux
un
L
in
.
20
08
.
M
ax
im
um
en
tr
op
y
ba
se
d
ru
le
se
le
ct
io
n
m
od
el
fo
r
sy
nt
ax
-b
as
ed
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
20
08
C
on
fe
re
nc
e
on
E
m
pi
ri
ca
l
M
et
ho
ds
in
N
at
ur
al
L
an
gu
ag
e
P
ro
ce
ss
in
g,
pa
ge
s
89
?9
7,
H
on
ol
ul
u,
H
aw
ai
i,
O
ct
ob
er
.
A
ss
oc
ia
ti
on
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
Z
hi
yu
an
L
iu
,
Y
uz
ho
u
Z
ha
ng
,
E
dw
ar
d
Y
.
C
ha
ng
,
an
d
M
ao
so
ng
S
un
.
20
11
.
P
ld
a+
:
P
ar
al
le
l
la
te
nt
di
ri
ch
le
t
al
lo
ca
ti
on
w
it
h
da
ta
pl
ac
em
en
ta
nd
pi
pe
li
ne
pr
oc
es
s-
in
g.
A
C
M
Tr
an
sa
ct
io
ns
on
In
te
ll
ig
en
t
Sy
st
em
s
an
d
Te
ch
no
lo
gy
,
sp
ec
ia
l
is
su
e
on
L
ar
ge
Sc
al
e
M
ac
hi
ne
L
ea
rn
in
g.
S
of
tw
ar
e
av
ai
la
bl
e
at
h
t
t
p
:
/
/
c
o
d
e
.
g
o
o
g
l
e
.
c
o
m
/
p
/
p
l
d
a
.
0
20
40
60
80
100
00.020.040.060.080.1???
???
???
???
???
???
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , de
liver 
X>
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , di
stribu
te X>
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , se
nd X>
S
et
ti
n
gs
N
IS
T
20
04
N
IS
T
20
05
N
IS
T
20
06
N
IS
T
20
08
A
ve
ra
ge
B
as
el
in
e
42
.2
5
41
.2
1
38
.0
5
31
.1
6
38
.1
7
(X
ia
o
et
al
.,
20
12
)
42
.5
8
41
.6
1
38
.3
9
31
.5
8
38
.5
4
S
im
(S
rc
)
42
.5
1
41
.5
5
38
.5
3
31
.5
7
38
.5
4
S
im
(T
rg
)
42
.4
3
41
.4
8
38
.4
31
.4
9
38
.4
5
S
im
(S
rc
+
T
rg
)
42
.7
41
.6
6
38
.6
6
31
.6
6
38
.6
7
S
im
(S
rc
+
T
rg
)+
S
en
(S
rc
)
42
.7
7
41
.8
1
38
.8
5
31
.7
3
38
.7
9
S
im
(S
rc
+
T
rg
)+
S
en
(T
rg
)
42
.8
5
41
.7
9
38
.7
6
31
.7
38
.7
8
S
im
(S
rc
+
T
rg
)+
S
en
(S
rc
+
T
rg
)
42
.9
5
41
.9
7
38
.9
1
31
.8
8
38
.9
3
T
ab
le
2:
E
ff
ec
ti
ve
ne
ss
of
di
ff
er
en
t
fe
at
ur
es
in
B
L
E
U
%
(p
<
0.
05
),
w
it
h
N
=
10
an
d
L
=
10
0.
?S
im
?
de
no
te
s
th
e
ru
le
si
m
il
ar
it
y
fe
at
ur
e
an
d
?S
en
?
de
no
te
s
ru
le
se
ns
it
iv
it
y
fe
at
ur
e.
?S
rc
?
an
d
?T
rg
?
m
ea
ns
ut
il
iz
in
g
so
ur
ce
-s
id
e/
ta
rg
et
-s
id
e
ru
le
to
pi
c
ve
ct
or
s
to
ca
lc
ul
at
e
si
m
il
ar
it
y
or
se
ns
it
iv
it
y,
re
sp
ec
ti
ve
ly
.
T
he
?A
ve
ra
ge
?
se
tt
in
g
is
th
e
av
er
ag
ed
re
su
lt
s
of
fo
ur
da
ta
se
ts
.
pa
re
d
w
it
h
on
ly
us
in
g
si
m
il
ar
it
y
fe
at
ur
es
.
B
ec
au
se
to
pi
c-
sp
ec
ifi
c
ru
le
s
us
ua
ll
y
ha
ve
a
la
rg
er
se
ns
it
iv
-
it
y
sc
or
e,
th
ey
ca
n
be
at
ge
ne
ra
l
ru
le
s
w
he
n
th
ey
ob
ta
in
th
e
sa
m
e
si
m
il
ar
it
y
sc
or
e
ag
ai
ns
t
th
e
in
pu
t
se
nt
en
ce
.
F
in
al
ly
,
w
he
n
al
l
ne
w
fe
at
ur
es
ar
e
in
-
te
gr
at
ed
,
th
e
pe
rf
or
m
an
ce
is
th
e
be
st
,
pr
ef
or
m
in
g
su
bs
ta
nt
ia
ll
y
be
tt
er
th
an
(X
ia
o
et
al
.,
20
12
)
w
it
h
0.
39
B
L
E
U
po
in
ts
on
av
er
ag
e.
O
ne
in
te
re
st
in
g
ob
se
rv
at
io
n
is
,t
he
pe
rf
or
m
an
ce
of
(X
ia
o
et
al
.,
20
12
)
is
qu
it
e
si
m
il
ar
to
th
e
se
t-
ti
ng
s
w
it
h
N
=
1
an
d
L
=
10
0
in
F
ig
ur
e
3.
T
hi
s
is
no
t
si
m
pl
y
co
in
ci
de
nc
e
si
nc
e
w
e
ca
n
in
te
rp
re
t
th
ei
r
ap
pr
oa
ch
as
a
sp
ec
ia
l
ca
se
in
ou
r
ne
ur
al
ne
t-
w
or
k
m
et
ho
d.
W
he
n
a
pa
ra
ll
el
se
nt
en
ce
pa
ir
ha
s
do
cu
m
en
t-
le
ve
l
in
fo
rm
at
io
n,
th
at
do
cu
m
en
t
w
il
l
be
re
tr
ie
ve
d
fo
r
tr
ai
ni
ng
.
O
th
er
w
is
e,
th
e
m
os
ts
im
-
il
ar
do
cu
m
en
t
w
il
l
be
ob
ta
in
ed
fr
om
th
e
m
on
ol
in
-
gu
al
da
ta
.
O
ur
m
et
ho
d
ca
n
be
vi
ew
ed
as
a
m
or
e
ge
ne
ra
l
fr
am
ew
or
k
th
an
pr
ev
io
us
L
D
A
-b
as
ed
ap
-
pr
oa
ch
es
.
4.
5
D
is
cu
ss
io
n
In
ou
r
ex
pe
ri
m
en
ts
,
In
pr
ev
io
us
L
D
A
-b
as
ed
m
et
ho
d,
if
a
do
cu
m
en
t
D
oc
co
nt
ai
ns
M
se
nt
en
ce
s,
al
l
M
se
nt
en
ce
s
w
il
l
sh
ar
e
th
e
sa
m
e
to
pi
c
di
st
ri
bu
ti
on
of
D
oc
.
A
l-
th
ou
gh
di
ff
er
en
t
se
nt
en
ce
s
m
ay
ex
pr
es
s
sl
ig
ht
ly
di
ff
er
en
t
im
pl
ic
at
io
ns
an
d
th
e
to
pi
c
w
il
l
ch
an
ge
,
th
e
co
nv
en
ti
on
al
L
D
A
-b
as
ed
ap
pr
oa
ch
do
es
no
t
ta
ke
th
e
to
pi
c
tr
an
si
ti
on
in
to
co
ns
id
er
at
io
n.
In
co
n-
tr
as
t,
ou
r
ap
pr
oa
ch
di
re
ct
ly
le
ar
ns
th
e
to
pi
c
re
p-
re
se
nt
at
io
n
w
it
h
an
ab
un
da
nc
y
of
re
la
te
d
do
cu
-
m
en
ts
.
In
ad
di
ti
on
al
to
th
e
or
ig
in
al
do
cu
m
en
tf
ro
m
w
hi
ch
th
e
se
nt
en
ce
is
ex
tr
ac
te
d,
th
e
IR
m
et
ho
d
al
so
re
tr
ie
ve
s
ot
he
r
re
le
va
nt
do
cu
m
en
ts
w
hi
ch
pr
o-
vi
de
co
m
pl
em
en
ta
ry
to
pi
c
in
fo
rm
at
io
n.
T
he
re
fo
re
,
th
e
to
pi
c
re
pr
es
en
ta
ti
on
s
le
ar
ne
d
ar
e
m
or
e
fi
ne
-
gr
ai
ne
d
an
d
th
us
m
or
e
ac
cu
ra
te
.
R
u
le
s
P
(?
|?
)
S
im
(z
s
,z
?
)
??
?
X
,d
el
iv
er
X
?
0.
02
37
0.
84
69
??
?
X
,d
is
tr
ib
ut
e
X
?
0.
05
46
0.
82
68
??
?
X
,s
en
d
X
?
0.
24
64
0.
61
19
T
ab
le
3:
D
ev
el
op
m
en
ta
nd
te
st
in
g
da
ta
us
ed
in
th
e
ex
pe
ri
m
en
ts
.
5
R
el
at
ed
W
or
k
T
op
ic
m
od
el
in
g
w
as
fi
rs
t
le
ve
ra
ge
d
to
im
pr
ov
e
S
M
T
pe
rf
or
m
an
ce
in
(Z
ha
o
an
d
X
in
g,
20
06
;Z
ha
o
an
d
X
in
g,
20
07
).
T
he
y
pr
op
os
ed
a
bi
li
ng
ua
l
to
pi
ca
l
ad
m
ix
tu
re
ap
pr
oa
ch
fo
r
w
or
d
al
ig
nm
en
t
an
d
as
su
m
ed
th
at
ea
ch
w
or
d-
pa
ir
fo
ll
ow
s
a
to
pi
c-
sp
ec
ifi
c
m
od
el
.
T
he
y
re
po
rt
ed
ex
te
ns
iv
e
em
pi
r-
ic
al
an
al
ys
is
an
d
im
pr
ov
ed
w
or
d
al
ig
nm
en
t
ac
-
cu
ra
cy
as
w
el
l
as
tr
an
sl
at
io
n
qu
al
it
y.
F
ol
lo
w
-
in
g
th
is
w
or
k,
(X
ia
o
et
al
.,
20
12
)
ex
te
nd
ed
to
pi
c-
sp
ec
ifi
c
le
xi
co
n
tr
an
sl
at
io
n
m
od
el
s
to
hi
er
ar
ch
ic
al
ph
ra
se
-b
as
ed
tr
an
sl
at
io
n
m
od
el
s,
w
he
re
th
e
to
pi
c
in
fo
rm
at
io
n
of
sy
nc
hr
on
ou
s
ru
le
s
w
as
di
re
ct
ly
in
-
fe
rr
ed
w
it
h
th
e
he
lp
of
do
cu
m
en
t-
le
ve
l
in
fo
rm
a-
ti
on
.
E
xp
er
im
en
ts
sh
ow
th
at
th
ei
r
ap
pr
oa
ch
no
t
on
ly
ac
hi
ev
ed
be
tt
er
tr
an
sl
at
io
n
pe
rf
or
m
an
ce
bu
t
al
so
pr
ov
id
ed
a
fa
st
er
de
co
di
ng
sp
ee
d
co
m
pa
re
d
w
it
h
pr
ev
io
us
le
xi
co
n-
ba
se
d
m
et
ho
ds
.
A
no
th
er
di
re
ct
io
n
of
ap
pr
oa
ch
es
le
ve
ra
ge
d
to
pi
c
m
od
el
in
g
te
ch
ni
qu
es
fo
r
do
m
ai
n
ad
ap
ta
ti
on
.
T
am
et
al
.
(2
00
7)
us
ed
bi
li
ng
ua
l
L
S
A
to
le
ar
n
la
te
nt
to
pi
c
di
st
ri
bu
ti
on
s
ac
ro
ss
di
ff
er
en
t
la
ng
ua
ge
s
an
d
en
fo
rc
e
on
e-
to
-o
ne
to
pi
c
co
rr
es
po
nd
en
ce
du
ri
ng
m
od
el
tr
ai
ni
ng
.
T
he
y
in
co
rp
or
at
ed
th
e
bi
li
ng
ua
l
to
pi
c
in
fo
rm
at
io
n
in
to
la
ng
ua
ge
m
od
el
ad
ap
ta
ti
on
an
d
le
xi
co
n
tr
an
sl
at
io
n
m
od
el
ad
ap
ta
ti
on
,
ac
hi
ev
-
in
g
si
gn
ifi
ca
nt
im
pr
ov
em
en
ts
in
th
e
la
rg
e-
sc
al
e
ev
al
ua
ti
on
.
(S
u
et
al
.,
20
12
)
in
ve
st
ig
at
ed
th
e
re
la
-
ti
on
sh
ip
be
tw
ee
n
ou
t-
of
-d
om
ai
n
bi
li
ng
ua
ld
at
a
an
d
in
-d
om
ai
n
m
on
ol
in
gu
al
da
ta
vi
a
to
pi
c
m
ap
pi
ng
us
-
Figure 4: An exampl from the NIST 2005 dataset. We ill strate the normalized topic repres ntations of
the source sentence and three ambiguous synchronous rules. Details are explained in Section 4.5.
our method is that it is applicable to both sentence-
level and doc ment-level SMT, since we do not
place any restricti ns on the input. In addition, our
method directly maximizes the similarity between
parallel sentence pairs, which is ideal for SMT de-
coding. Compared to document-level topic mod-
eling which uses the topic of a document for all
sentences within the document (Xiao et al, 2012),
our contributions are:
? We proposed a more general approach to
leveraging topic information for SMT by us-
ing IR methods to get a collection of related
documents, regardless of whether or not doc-
ument boundaries are explicitly given.
? We used neural networks to learn topic repre-
sentations more accurately, with more practi-
cable and scalable modeling techniques.
? We directly optimized bilingual topic simi-
larity in the deep learning framework with
the help of sentence-level parallel data, so
that the learned representation could be easily
used in SMT decoding procedure.
6 Conclusion and Future Work
In this paper, we propose a neural network based
approach to learning bilingual topic representa-
tion for SMT. We enrich contexts of parallel sen-
tence pairs with topic related monolingual data
and obtain a set of documents to represent sen-
tences. These documents are converted to a bag-
of-words input and fed into neural networks. The
learned low-dimensional vector is used to obtain
the topic representations of synchronous rules. In
SMT decoding, appropriate rules a e selected to
best match source texts according to their similar-
ity in the topic space. Experimental results show
that our approach is promising for SMT systems to
learn a better translation model. It is a significant
improvement over the state-of-the-art Hiero sys-
tem, as well as a conventional LDA-based method.
In the future research, we will extend our neural
network methods to address document-level trans-
lation, where topic transition between sentences is
a crucial problem to be solved. Since the transla-
tion of the current sentence is usually influenced
by the topic of previous sentences, we plan to
leverage recurrent neural networks to model this
phenomenon, where the history translation infor-
mation is naturally combined in the model.
Acknowledgments
We are grateful to the anonymous reviewers for
their insightful comments. We also thank Fei
Huang (BBN), Nan Yang, Yajuan Duan, Hong Sun
and Duyu Tang for the helpful discussions. This
work is supported by the National Natural Science
Foundation of China (Granted No. 61272384)
141
References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise train-
ing of deep networks. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153?160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Found. Trends Mach. Learn., 2(1):1?127, Jan-
uary.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
pages 387?394, Ann Arbor, Michigan, June. Asso-
ciation for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. Proceedings of Machine Trans-
lation Summit XI, pages 73?80.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1285?
1293, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,
Mu Li, and Ming Zhou. 2013. Multi-domain adap-
tation for SMT using multi-task learning. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1055?
1065, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech and Language
Processing, 20(1):30?42, January.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53?61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Amit Gruber, Michal Rosen-zvi, and Yair Weiss. 2007.
Hidden topic markov models. In In Proceedings of
Artificial Intelligence and Statistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527?1554, July.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In P. Bartlett, F.C.N. Pereira,
C.J.C. Burges, L. Bottou, and K.Q. Weinberger, ed-
itors, Advances in Neural Information Processing
Systems 25, pages 1106?1114.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y. Ng. 2006. Efficient sparse coding algo-
rithms. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 801?808. MIT Press, Cambridge,
MA.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
89?97, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
142
Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based transla-
tion. In Proceedings of ACL-08: HLT, pages 1003?
1011, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696?699. MIT
Press, Cambridge, MA, USA.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 354?362, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011a. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 151?161, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 459?468, Jeju Island, Korea,
July. Association for Computational Linguistics.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, December.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th International
Conference on Machine Learning, ICML ?08, pages
1096?1103, New York, NY, USA. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res., 11:3371?
3408, December.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for
hierarchical phrase-based translation. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 750?758, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
AAAI.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Bing Zhao and Eric P. Xing. 2007. Hm-bitam: Bilin-
gual topic exploration, word alignment, and trans-
lation. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1689?1696. MIT
Press, Cambridge, MA.
143
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491?1500,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Recursive Recurrent Neural Network
for Statistical Machine Translation
Shujie Liu
1
, Nan Yang
2
, Mu Li
1
and Ming Zhou
1
1
Microsoft Research Asia, Beijing, China
2
University of Science and Technology of China, Hefei, China
shujliu, v-nayang, muli, mingzhou@microsoft.com
Abstract
In this paper, we propose a novel recursive
recurrent neural network (R
2
NN) to mod-
el the end-to-end decoding process for s-
tatistical machine translation. R
2
NN is a
combination of recursive neural network
and recurrent neural network, and in turn
integrates their respective capabilities: (1)
new information can be used to generate
the next hidden state, like recurrent neu-
ral networks, so that language model and
translation model can be integrated natu-
rally; (2) a tree structure can be built, as
recursive neural networks, so as to gener-
ate the translation candidates in a bottom
up manner. A semi-supervised training ap-
proach is proposed to train the parameter-
s, and the phrase pair embedding is ex-
plored to model translation confidence di-
rectly. Experiments on a Chinese to En-
glish translation task show that our pro-
posed R
2
NN can outperform the state-
of-the-art baseline by about 1.5 points in
BLEU.
1 Introduction
Deep Neural Network (DNN), which essential-
ly is a multi-layer neural network, has re-gained
more and more attentions these years. With the
efficient training methods, such as (Hinton et al,
2006), DNN is widely applied to speech and im-
age processing, and has achieved breakthrough re-
sults (Kavukcuoglu et al, 2010; Krizhevsky et al,
2012; Dahl et al, 2012).
Applying DNN to natural language processing
(NLP), representation or embedding of words is
usually learnt first. Word embedding is a dense,
low dimensional, real-valued vector. Each dimen-
sion of the vector represents a latent aspect of
the word, and captures its syntactic and semantic
properties (Bengio et al, 2006). Word embedding
is usually learnt from large amount of monolin-
gual corpus at first, and then fine tuned for spe-
cial distinct tasks. Collobert et al (2011) propose
a multi-task learning framework with DNN for
various NLP tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic role labelling. Recurrent neural networks
are leveraged to learn language model, and they
keep the history information circularly inside the
network for arbitrarily long time (Mikolov et al,
2010). Recursive neural networks, which have the
ability to generate a tree structured output, are ap-
plied to natural language parsing (Socher et al,
2011), and they are extended to recursive neural
tensor networks to explore the compositional as-
pect of semantics (Socher et al, 2013).
DNN is also introduced to Statistical Machine
Translation (SMT) to learn several components
or features of conventional framework, includ-
ing word alignment, language modelling, transla-
tion modelling and distortion modelling. Yang et
al. (2013) adapt and extend the CD-DNN-HMM
(Dahl et al, 2012) method to HMM-based word
alignment model. In their work, bilingual word
embedding is trained to capture lexical translation
information, and surrounding words are utilized to
model context information. Auli et al (2013) pro-
pose a joint language and translation model, based
on a recurrent neural network. Their model pre-
dicts a target word, with an unbounded history of
both source and target words. Liu et al (2013) pro-
pose an additive neural network for SMT decod-
ing. Word embedding is used as the input to learn
translation confidence score, which is combined
with commonly used features in the convention-
al log-linear model. For distortion modeling, Li
et al (2013) use recursive auto encoders to make
full use of the entire merging phrase pairs, going
beyond the boundary words with a maximum en-
tropy classifier (Xiong et al, 2006).
1491
Different from the work mentioned above,
which applies DNN to components of conven-
tional SMT framework, in this paper, we propose
a novel R
2
NN to model the end-to-end decod-
ing process. R
2
NN is a combination of recursive
neural network and recurrent neural network. In
R
2
NN, new information can be used to generate
the next hidden state, like recurrent neural net-
works, and a tree structure can be built, as recur-
sive neural networks. To generate the translation
candidates in a commonly used bottom-up man-
ner, recursive neural networks are naturally adopt-
ed to build the tree structure. In recursive neural
networks, all the representations of nodes are gen-
erated based on their child nodes, and it is difficult
to integrate additional global information, such as
language model and distortion model. In order to
integrate these crucial information for better trans-
lation prediction, we combine recurrent neural net-
works into the recursive neural networks, so that
we can use global information to generate the next
hidden state, and select the better translation can-
didate.
We propose a three-step semi-supervised train-
ing approach to optimizing the parameters of
R
2
NN, which includes recursive auto-encoding
for unsupervised pre-training, supervised local
training based on the derivation trees of forced de-
coding, and supervised global training using ear-
ly update strategy. So as to model the transla-
tion confidence for a translation phrase pair, we
initialize the phrase pair embedding by leveraging
the sparse features and recurrent neural network.
The sparse features are phrase pairs in translation
table, and recurrent neural network is utilized to
learn a smoothed translation score with the source
and target side information. We conduct exper-
iments on a Chinese-to-English translation task
to test our proposed methods, and we get about
1.5 BLEU points improvement, compared with a
state-of-the-art baseline system.
The rest of this paper is organized as follows:
Section 2 introduces related work on applying
DNN to SMT. Our R
2
NN framework is introduced
in detail in Section 3, followed by our three-step
semi-supervised training approach in Section 4.
Phrase pair embedding method using translation
confidence is elaborated in Section 5. We intro-
duce our conducted experiments in Section 6, and
conclude our work in Section 7.
2 Related Work
Yang et al (2013) adapt and extend CD-DNN-
HMM (Dahl et al, 2012) to word alignment.
In their work, initial word embedding is firstly
trained with a huge mono-lingual corpus, then the
word embedding is adapted and fine tuned bilin-
gually in a context-depended DNN HMM frame-
work. Word embeddings capturing lexical trans-
lation information and surrounding words model-
ing context information are leveraged to improve
the word alignment performance. Unfortunately,
the better word alignment result generated by this
model, cannot bring significant performance im-
provement on a end-to-end SMT evaluation task.
To improve the SMT performance directly, Auli
et al (2013) extend the recurrent neural network
language model, in order to use both the source
and target side information to scoring translation
candidates. In their work, not only the target word
embedding is used as the input of the network, but
also the embedding of the source word, which is
aligned to the current target word. To tackle the
large search space due to the weak independence
assumption, a lattice algorithm is proposed to re-
rank the n-best translation candidates, generated
by a given SMT decoder.
Liu et al (2013) propose an additive neural net-
work for SMT decoding. RNNLM (Mikolov et al,
2010) is firstly used to generate the source and tar-
get word embeddings, which are fed into a one-
hidden-layer neural network to get a translation
confidence score. Together with other common-
ly used features, the translation confidence score
is integrated into a conventional log-linear model.
The parameters are optimized with developmen-
t data set using mini-batch conjugate sub-gradient
method and a regularized ranking loss.
DNN is also brought into the distortion mod-
eling. Going beyond the previous work using
boundary words for distortion modeling in BTG-
based SMT decoder, Li et al (2013) propose to ap-
ply recursive auto-encoder to make full use of the
entire merged blocks. The recursive auto-encoder
is trained with reordering examples extracted from
word-aligned bilingual sentences. Given the rep-
resentations of the smaller phrase pairs, recursive
auto-encoder can generate the representation of
the parent phrase pair with a re-ordering confi-
dence score. The combination of reconstruction
error and re-ordering error is used to be the objec-
tive function for the model training.
1492
3 Our Model
In this section, we leverage DNN to model the
end-to-end SMT decoding process, using a novel
recursive recurrent neural network (R
2
NN), which
is different from the above mentioned work ap-
plying DNN to components of conventional SMT
framework. R
2
NN is a combination of recur-
sive neural network and recurrent neural network,
which not only integrates the conventional glob-
al features as input information for each combina-
tion, but also generates the representation of the
parent node for the future candidate generation.
In this section, we briefly recall the recurren-
t neural network and recursive neural network in
Section 3.1 and 3.2, and then we elaborate our
R
2
NN in detail in Section 3.3.
3.1 Recurrent Neural Network
Recurrent neural network is usually used for
sequence processing, such as language model
(Mikolov et al, 2010). Commonly used sequence
processing methods, such as Hidden Markov
Model (HMM) and n-gram language model, only
use a limited history for the prediction. In HMM,
the previous state is used as the history, and for n-
gram language model (for example n equals to
3), the history is the previous two words. Recur-
rent neural network is proposed to use unbounded
history information, and it has recurrent connec-
tions on hidden states, so that history information
can be used circularly inside the network for arbi-
trarily long time.
??
?
???1
??
??
??
Figure 1: Recurrent neural network
As shown in Figure 1, the network contains
three layers, an input layer, a hidden layer, and an
output layer. The input layer is a concatenation of
h
t?1
and x
t
, where h
t?1
is a real-valued vec-
tor, which is the history information from time 0
to t? 1. x
t
is the embedding of the input word at
time t . Word embedding x
t
is integrated with
previous history h
t?1
to generate the current hid-
den layer, which is a new history vector h
t
. Based
on h
t
, we can predict the probability of the next
word, which forms the output layer y
t
. The new
history h
t
is used for the future prediction, and
updated with new information from word embed-
ding x
t
recurrently.
3.2 Recursive Neural Network
In addition to the sequential structure above, tree
structure is also usually constructed in various
NLP tasks, such as parsing and SMT decoding.
To generate a tree structure, recursive neural net-
works are introduced for natural language parsing
(Socher et al, 2011). Similar with recurrent neural
networks, recursive neural networks can also use
unbounded history information from the sub-tree
rooted at the current node. The commonly used
binary recursive neural networks generate the rep-
resentation of the parent node, with the represen-
tations of two child nodes as the input.
?[?,?] ?[?,?]
?[?,?]
?
?[?,?]
Figure 2: Recursive neural network
As shown in Figure 2, s
[l,m]
and s
[m,n]
are
the representations of the child nodes, and they are
concatenated into one vector to be the input of the
network. s
[l,n]
is the generated representation of
the parent node. y
[l,n]
is the confidence score of
how plausible the parent node should be created.
l,m, n are the indexes of the string. For example,
for nature language parsing, s
[l,n]
is the represen-
tation of the parent node, which could be a NP or
V P node, and it is also the representation of the
whole sub-tree covering from l to n .
3.3 Recursive Recurrent Neural Network
Word embedding x
t
is integrated as new input
information in recurrent neural networks for each
prediction, but in recursive neural networks, no ad-
ditional input information is used except the two
representation vectors of the child nodes. How-
ever, some global information , which cannot be
generated by the child representations, is crucial
1493
for SMT performance, such as language model s-
core and distortion model score. So as to integrate
such global information, and also keep the ability
to generate tree structure, we combine the recur-
rent neural network and the recursive neural net-
work to be a recursive recurrent neural network
(R
2
NN).
?[?,?] ?[?,?]
?[?,?]
?[?,?] ?[?,?]
?
?[?,?]
??[?,?]
Figure 3: Recursive recurrent neural network
As shown in Figure 3, based on the recursive
network, we add three input vectors x
[l,m]
for
child node [l,m] , x
[m,n]
for child node [m,n] ,
and x
[l,n]
for parent node [l, n] . We call them
recurrent input vectors, since they are borrowed
from recurrent neural networks. The two recurrent
input vectors x
[l,m]
and x
[m,n]
are concatenat-
ed as the input of the network, with the original
child node representations s
[l,m]
and s
[m,n]
. The
recurrent input vector x
[l,n]
is concatenated with
parent node representation s
[l,n]
to compute the
confidence score y
[l,n]
.
The input, hidden and output layers are calcu-
lated as follows:
x?
[l,n]
= x
[l,m]
./ s
[l,m]
./ x
[m,n]
./ s
[m,n]
(1)
s
[l,n]
j
= f(
?
i
x?
[l,n]
i
w
ji
) (2)
y
[l,n]
=
?
j
(s
[l,n]
./ x
[l,n]
)
j
v
j
(3)
where ./ is a concatenation operator in Equation
1 and Equation 3, and f is a non-linear function,
here we use HTanh function, which is defined
as:
HTanh(x) =
?
?
?
?
?
?1, x < ?1
x, ?1 ? x ? 1
1, x > 1
(4)
Figure 4 illustrates the R
2
NN architecture for
SMT decoding. For a source sentence ?laizi faguo
he eluosi de?, we first split it into phrases ?laiz-
i?, ?faguo he eluosi? and ?de?. We then check
whether translation candidates can be found in the
translation table for each span, together with the
phrase pair embedding and recurrent input vec-
tor (global features). We call it the rule match-
ing phase. For a translation candidate of the s-
pan node [l,m] , the black dots stand for the node
representation s
[l,m]
, while the grey dots for re-
current input vector x
[l,m]
. Given s
[l,m]
and
x
[l,m]
for matched translation candidates, conven-
tional CKY decoding process is performed using
R
2
NN. R
2
NN can combine the translation pairs
of child nodes, and generate the translation can-
didates for parent nodes with their representations
and plausible scores. Only the n-best translation
candidates are kept for upper combination, accord-
ing to their plausible scores.
??
laizi
??????
faguo he eluosi
?
de
coming from France and Russia NULL
Rule Match Rule Match Rule Match
coming from France and Russia
R2NN
coming from France and Russia
R2NN
Figure 4: R
2
NN for SMT decoding
We extract phrase pairs using the conventional
method (Och and Ney, 2004). The commonly used
features, such as translation score, language mod-
el score and distortion score, are used as the recur-
rent input vector x . During decoding, recurrent
input vectors x for internal nodes are calculat-
ed accordingly. The difference between our model
and the conventional log-linear model includes:
? R
2
NN is not linear, while the conventional
model is a linear combination.
? Representations of phrase pairs are automat-
ically learnt to optimize the translation per-
formance, while features used in convention-
al model are hand-crafted.
? History information of the derivation can be
recorded in the representation of internal n-
odes, while conventional model cannot.
1494
Liu et al (2013) apply DNN to SMT decoding,
but not in a recursive manner. A feature is learn-
t via a one-hidden-layer neural network, and the
embedding of words in the phrase pairs are used
as the input vector. Our model generates the rep-
resentation of a translation pair based on its child
nodes. Li et al (2013) also generate the repre-
sentation of phrase pairs in a recursive way. In
their work, the representation is optimized to learn
a distortion model using recursive neural network,
only based on the representation of the child n-
odes. Our R
2
NN is used to model the end-to-end
translation process, with recurrent global informa-
tion added. We also explore phrase pair embed-
ding method to model translation confidence di-
rectly, which is introduced in Section 5.
In the next two sections, we will answer the fol-
lowing questions: (a) how to train the model, and
(b) how to generate the initial representations of
translation pairs.
4 Model Training
In this section, we propose a three-step training
method to train the parameters of our proposed
R
2
NN, which includes unsupervised pre-training
using recursive auto-encoding, supervised local
training on the derivation tree of forced decoding,
and supervised global training using early update
training strategy.
4.1 Unsupervised Pre-training
We adopt the Recursive Auto Encoding (RAE)
(Socher et al, 2011) for our unsupervised pre-
training. The main idea of auto encoding is to
initialize the parameters of the neural network,
by minimizing the information lost, which means,
capturing as much information as possible in the
hidden states from the input vector.
As shown in Figure 5, RAE contains two part-
s, an encoder with parameter W , and a decoder
with parameter W
?
. Given the representations of
child nodes s
1
and s
2
, the encoder generates the
representation of parent node s . With the parent
node representation s as the input vector, the de-
coder reconstructs the representation of two child
nodes s
?
1
and s
?
2
. The loss function is defined as
following so as to minimize the information lost:
L
RAE
(s
1
, s
2
) =
1
2
(
?
?
s
1
? s
?
1
?
?
2
+
?
?
s
2
? s
?
2
?
?
2
)
(5)
where ??? is the Euclidean norm.
coming from France and Russia
??
laizi
??????
faguo he eluosi
coming from France and Russia
coming from France and Russia
?1 ?2
?
?1? ?2?
?
??
Figure 5: Recursive auto encoding for unsuper-
vised pre-training
The training samples for RAE are phrase pairs
{s
1
, s
2
} in translation table, where s
1
and
s
2
can form a continuous partial sentence pair in
the training data. When RAE training is done, on-
ly the encoding model W will be fine tuned in
the future training phases.
4.2 Supervised Local Training
We use contrastive divergence method to fine tune
the parameters W and V . The loss function
is the commonly used ranking loss with a margin,
and it is defined as follows:
L
SLT
(W,V, s
[l,n]
) = max(0, 1? y
[l,n]
oracle
+ y
[l,n]
t
)
(6)
where s
[l,n]
is the source span. y
[l,n]
oracle
is
the plausible score of a oracle translation result.
y
[l,n]
t
is the plausible score for the best transla-
tion candidate given the model parameters W and
V . The loss function aims to learn a model which
assigns the good translation candidate (the oracle
candidate) higher score than the bad ones, with a
margin 1.
Translation candidates generated by forced de-
coding (Wuebker et al, 2010) are used as ora-
cle translations, which are the positive samples.
Forced decoding performs sentence pair segmen-
tation using the same translation system as decod-
ing. For each sentence pair in the training data,
SMT decoder is applied to the source side, and
any candidate which is not the partial sub-string
of the target sentence is removed from the n-best
list during decoding. From the forced decoding
result, we can get the ideal derivation tree in the
decoder?s search space, and extract positive/oracle
translation candidates.
1495
4.3 Supervised Global Training
The supervised local training uses the n-
odes/samples in the derivation tree of forced de-
coding to update the model, and the trained model
tends to over-fit to local decisions. In this subsec-
tion, a supervised global training is proposed to
tune the model according to the final translation
performance of the whole source sentence.
Actually, we can update the model from the root
of the decoding tree and perform back propaga-
tion along the tree structure. Due to the inexac-
t search nature of SMT decoding, search errors
may inevitably break theoretical properties, and
the final translation results may be not suitable
for model training. To handle this problem, we
use early update strategy for the supervised glob-
al training. Early update is testified to be useful
for SMT training with large scale features (Yu et
al., 2013). Instead of updating the model using
the final translation results, early update approach
optimizes the model, when the oracle translation
candidate is pruned from the n-best list, meaning
that, the model is updated once it performs a unre-
coverable mistake. Back propagation is performed
along the tree structure, and the phrase pair em-
beddings of the leaf nodess are updated.
The loss function for supervised global training
is defined as follows:
L
SGT
(W,V, s
[l,n]
) = ? log(
?
y
[l,n]
oracle
exp (y
[l,n]
oracle
)
?
t?nbest
exp (y
[l,n]
t
)
)
(7)
where y
[l,n]
oracle
is the model score of a oracle trans-
lation candidate for the span [l, n] . Oracle transla-
tion candidates are candidates get from forced de-
coding. If the span [l, n] is not the whole source
sentence, there may be several oracle translation
candidates, otherwise, there is only one, which is
exactly the target sentence. There are much few-
er training samples than those for supervised local
training, and it is not suitable to use ranking loss
for global training any longer. We use negative
log-likelihood to penalize all the other translation
candidates except the oracle ones, so as to leverage
all the translation candidates as training samples.
5 Phrase Pair Embedding
The next question is how to initialize the phrase
pair embedding in the translation table, so as to
generate the leaf nodes of the derivation tree.
There are more phrase pairs than mono-lingual
words, but bilingual corpus is much more difficult
to acquire, compared with monolingual corpus.
Embedding #Data #Entry #Parameter
Word 1G 500K 20 ? 500K
Word Pair 7M (500K)
2
20 ? (500K)
2
Phrase Pair 7M (500K)
4
20 ? (500K)
4
Table 1: The relationship between the size of train-
ing data and the number of model parameters. The
numbers for word embedding is calculated on En-
glish Giga-Word corpus version 3. For word pair
and phrase pair embedding, the numbers are cal-
culated on IWSLT 2009 dialog training set. The
word count of each side of phrase pairs is limited
to be 2.
Table 1 shows the relationship between the size
of training data and the number of model parame-
ters. For word embedding, the training size is 1G
bits, and we may have 500K terms. For each ter-
m, we have a vector with length 20 as parameters,
so there are 20 ? 500K parameters totally. But
for source-target word pair, we may only have 7M
bilingual corpus for training (taking IWSLT data
set as an example), and there are 20 ?(500K)
2
parameters to be tuned. For phrase pairs, the sit-
uation becomes even worse, especially when the
limitation of word count in phrase pairs is relaxed.
It is very difficult to learn the phrase pair embed-
ding brute-forcedly as word embedding is learnt
(Mikolov et al, 2010; Collobert et al, 2011), s-
ince we may not have enough training data.
A simple approach to construct phrase pair em-
bedding is to use the average of the embeddings
of the words in the phrase pair. One problem is
that, word embedding may not be able to mod-
el the translation relationship between source and
target phrases at phrase level, since some phrases
cannot be decomposed. For example, the meaning
of ?hot dog? is not the composition of the mean-
ings of the words ?hot? and ?dog?. In this section,
we split the phrase pair embedding into two parts
to model the translation confidence directly: trans-
lation confidence with sparse features and trans-
lation confidence with recurrent neural network.
We first get two translation confidence vectors sep-
arately using sparse features and recurrent neu-
ral network, and then concatenate them to be the
phrase pair embedding. We call it translation con-
fidence based phrase pair embedding (TCBPPE).
1496
5.1 Translation Confidence with Sparse
Features
Large scale feature training has drawn more at-
tentions these years (Liang et al, 2006; Yu et al,
2013). Instead of integrating the sparse features
directly into the log-linear model, we use them as
the input to learn a phrase pair embedding. For
the top 200,000 frequent translation pairs, each of
them is a feature in itself, and a special feature is
added for all the infrequent ones.
The one-hot representation vector is used as the
input, and a one-hidden-layer network generates
a confidence score. To train the neural network,
we add the confidence scores to the convention-
al log-linear model as features. Forced decoding
is utilized to get positive samples, and contrastive
divergence is used for model training. The neu-
ral network is used to reduce the space dimension
of sparse features, and the hidden layer of the net-
work is used as the phrase pair embedding. The
length of the hidden layer is empirically set to 20.
5.2 Translation Confidence with Recurrent
Neural Network
?e
????1 ?
?(??)
???1
??
???
??
Figure 6: Recurrent neural network for translation
confidence
We use recurrent neural network to generate two
smoothed translation confidence scores based on
source and target word embeddings. One is source
to target translation confidence score and the other
is target to source. These two confidence scores
are defined as:
T
S2T
(s, t) =
?
i
log p(e
i
|e
i?1
, f
a
i
, h
i
) (8)
T
T2S
(s, t) =
?
j
log p(f
j
|f
j?1
, e
a?
j
, h
j
) (9)
where, f
a
i
is the corresponding target word
aligned to e
i
, and it is similar for e
a?
j
.
p(e
i
|e
i?1
, f
a
i
, h
i
) is produced by a recurrent net-
work as shown in Figure 6. The recurrent neural
network is trained with word aligned bilingual cor-
pus, similar as (Auli et al, 2013).
6 Experiments and Results
In this section, we conduct experiments to test our
method on a Chinese-to-English translation task.
The evaluation method is the case insensitive IB-
M BLEU-4 (Papineni et al, 2002). Significant
testing is carried out using bootstrap re-sampling
method proposed by (Koehn, 2004) with a 95%
confidence level.
6.1 Data Setting and Baseline
The data is from the IWSLT 2009 dialog task.
The training data includes the BTEC and SLDB
training data. The training data contains 81k sen-
tence pairs, 655K Chinese words and 806K En-
glish words. The language model is a 5-gram lan-
guage model trained with the target sentences in
the training data. The test set is development set
9, and the development set comprises both devel-
opment set 8 and the Chinese DIALOG set.
The training data for monolingual word embed-
ding is Giga-Word corpus version 3 for both Chi-
nese and English. Chinese training corpus con-
tains 32M sentences and 1.1G words. English
training data contains 8M sentences and 247M
terms. We only train the embedding for the top
100,000 frequent words following (Collobert et
al., 2011). With the trained monolingual word em-
bedding, we follow (Yang et al, 2013) to get the
bilingual word embedding using the IWSLT bilin-
gual training data.
Our baseline decoder is an in-house implemen-
tation of Bracketing Transduction Grammar (BT-
G) (Wu, 1997) in CKY-style decoding with a lex-
ical reordering model trained with maximum en-
tropy (Xiong et al, 2006). The features of the
baseline are commonly used features as standard
BTG decoder, such as translation probabilities,
lexical weights, language model, word penalty and
distortion probabilities. All these commonly used
features are used as recurrent input vector x in
our R
2
NN.
6.2 Translation Results
As we mentioned in Section 5, constructing phrase
pair embeddings from word embeddings may be
not suitable. Here we conduct experiments to ver-
1497
ify it. We first train the source and target word em-
beddings separately using large monolingual data,
following (Collobert et al, 2011). Using monolin-
gual word embedding as the initialization, we fine
tune them to get bilingual word embedding (Yang
et al, 2013).
The word embedding based phrase pair embed-
ding (WEPPE) is defined as:
Epp
web
(s, t) =
?
i
E
wms
(s
i
) ./
?
j
E
wbs
(s
j
)
./
?
k
E
wmt
(t
k
) ./
?
l
E
wbt
(t
l
) (10)
where ./ is a concatenation operator. s and
t are the source and target phrases. E
wms
(s
i
) and
E
wmt
(t
k
) are the monolingual word embeddings,
and E
wbs
(s
i
) and E
wbt
(t
k
) are the bilingual
word embeddings. Here the length of the word
embedding is also set to 20. Therefore, the length
of the phrase pair embedding is 20? 4 = 80 .
We compare our phrase pair embedding meth-
ods and our proposed R
2
NN with baseline system,
in Table 2. We can see that, our R
2
NN models
with WEPPE and TCBPPE are both better than the
baseline system. WEPPE cannot get significan-
t improvement, while TCBPPE does, compared
with the baseline result. TCBPPE is much better
than WEPPE.
Setting Development Test
Baseline 46.81 39.29
WEPPE+R
2
NN 47.23 39.92
TCBPPE+R
2
NN 48.70 ? 40.81 ?
Table 2: Translation results of our proposed R
2
NN
Model with two phrase embedding methods, com-
pared with the baseline. Setting ?WEPPE+R
2
NN?
is the result with word embedding based phrase
pair embedding and our R
2
NN Model, and
?TCBPPE+R
2
NN? is the result of translation con-
fidence based phrase pair embedding and our
R
2
NN Model. The results with ? are significantly
better than the baseline.
Word embedding can model translation rela-
tionship at word level, but it may not be power-
ful to model the phrase pair respondents at phrasal
level, since the meaning of some phrases cannot
be decomposed into the meaning of words. And
also, translation task is difference from other NLP
tasks, that, it is more important to model the trans-
lation confidence directly (the confidence of one
target phrase as a translation of the source phrase),
and our TCBPPE is designed for such purpose.
6.3 Effects of Global Recurrent Input Vector
In order to compare R
2
NN with recursive network
for SMT decoding, we remove the recurrent input
vector in R
2
NN to test its effect, and the results
are shown in Table 3. Without the recurrent input
vectors, R
2
NN degenerates into recursive neural
network (RNN).
Setting Development Test
WEPPE+R
2
NN 47.23 40.81
WEPPE+RNN 37.62 33.29
TCBPPE+R
2
NN 48.70 40.81
TCBPPE+RNN 45.11 37.33
Table 3: Experimental results to test the effects of
recurrent input vector. WEPPE /TCBPPE+RNN
are the results removing recurrent input vectors
with WEPPE /TCBPPE.
From Table 3 we can find that, the recurren-
t input vector is essential to SMT performance.
When we remove it from R
2
NN, WEPPE based
method drops about 10 BLEU points on devel-
opment data and more than 6 BLEU points on
test data. TCBPPE based method drops about 3
BLEU points on both development and test data
sets. When we remove the recurrent input vectors,
the representations of recursive network are gener-
ated with the child nodes, and it does not integrate
global information, such as language model and
distortion model, which are crucial to the perfor-
mance of SMT.
6.4 Sparse Features and Recurrent Network
Features
To test the contributions of sparse features and re-
current network features, we first remove all the
recurrent network features to train and test our
R
2
NN model, and then remove all the sparse fea-
tures to test the contribution of recurrent network
features.
Setting Development Test
TCBPPE+R
2
NN 48.70 40.81
SF+R
2
NN 48.23 40.19
RNN+R
2
NN 47.89 40.01
Table 4: Experimental results to test the effects of
sparse features and recurrent network features.
1498
The results are shown in Table 6.4. From the
results, we can find that, sparse features are more
effective than the recurrent network features a lit-
tle bit. The sparse features can directly model the
translation correspondence, and they may be more
effective to rank the translation candidates, while
recurrent neural network features are smoothed
lexical translation confidence.
7 Conclusion and Future Work
In this paper, we propose a Recursive Recur-
rent Neural Network(R
2
NN) to combine the re-
current neural network and recursive neural net-
work. Our proposed R
2
NN cannot only inte-
grate global input information during each com-
bination, but also can generate the tree struc-
ture in a recursive way. We apply our model to
SMT decoding, and propose a three-step semi-
supervised training method. In addition, we ex-
plore phrase pair embedding method, which mod-
els translation confidence directly. We conduc-
t experiments on a Chinese-to-English translation
task, and our method outperforms a state-of-the-
art baseline about 1.5 points BLEU.
From the experiments, we find that, phrase pair
embedding is crucial to the performance of SMT.
In the future, we will explore better methods for
phrase pair embedding to model the translation e-
quivalent between source and target phrases. We
will apply our proposed R
2
NN to other tree struc-
ture learning tasks, such as natural language pars-
ing.
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. Inno-
vations in Machine Learning, pages 137?186.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE
Transactions on, 20(1):30?42.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527?1554.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha?el Mathieu, and Yann LeCun.
2010. Learning convolutional feature hierarchies for
visual recognition. Advances in Neural Information
Processing Systems, pages 1090?1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 388?395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for ITG-based translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 567?
577, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 761?768. Association for Computational Lin-
guistics.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for s-
tatistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 791?801, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of the Annual Conference of Internation-
al Speech Communication Association, pages 1045?
1048.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
1499
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 26th Internation-
al Conference on Machine Learning (ICML), vol-
ume 2, page 7.
Richard Socher, John Bauer, and Christopher D Man-
ning. 2013. Parsing with compositional vector
grammars. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s, volume 1, pages 455?465.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475?484. Association for Computa-
tional Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for s-
tatistical machine translation. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, volume 44, page 521.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word alignment modeling with contex-
t dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1112?1123, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
1500
