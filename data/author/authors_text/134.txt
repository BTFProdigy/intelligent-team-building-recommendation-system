Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1?4,
New York, June 2006. c?2006 Association for Computational Linguistics
Factored Neural Language Models
Andrei Alexandrescu
Department of Comp. Sci. Eng.
University of Washington
andrei@cs.washington.edu
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
katrin@ee.washington.edu
Abstract
We present a new type of neural proba-
bilistic language model that learns a map-
ping from both words and explicit word
features into a continuous space that is
then used for word prediction. Addi-
tionally, we investigate several ways of
deriving continuous word representations
for unknown words from those of known
words. The resulting model significantly
reduces perplexity on sparse-data tasks
when compared to standard backoff mod-
els, standard neural language models, and
factored language models.
1 Introduction
Neural language models (NLMs) (Bengio et al,
2000) map words into a continuous representation
space and then predict the probability of a word
given the continuous representations of the preced-
ing words in the history. They have previously been
shown to outperform standard back-off models in
terms of perplexity and word error rate on medium
and large speech recognition tasks (Xu et al, 2003;
Emami and Jelinek, 2004; Schwenk and Gauvain,
2004; Schwenk, 2005). Their main drawbacks are
computational complexity and the fact that only dis-
tributional information (word context) is used to
generalize over words, whereas other word prop-
erties (e.g. spelling, morphology etc.) are ignored
for this purpose. Thus, there is also no principled
way of handling out-of-vocabulary (OOV) words.
Though this may be sufficient for applications that
use a closed vocabulary, the current trend of porting
systems to a wider range of languages (esp. highly-
inflected languages such as Arabic) calls for dy-
namic dictionary expansion and the capability of as-
signing probabilities to newly added words without
having seen them in the training data. Here, we in-
troduce a novel type of NLM that improves gener-
alization by using vectors of word features (stems,
affixes, etc.) as input, and we investigate deriving
continuous representations for unknown words from
those of known words.
2 Neural Language Models
P(w  | w    ,w     )t?2t?1t
M
i
h
o
Wih Who
d columns
|V| rows
d = continuous space size
V = vocabulary
n?2w
n?1w
Figure 1: NLM architecture. Each word in the context maps
to a row in the matrix M . The output is next word?s probability
distribution.
A standard NLM (Fig. 1) takes as input the previ-
ous n ? 1 words, which select rows from a continu-
ous word representation matrix M . The next layer?s
input i is the concatenation of the rows in M cor-
responding to the input words. From here, the net-
work is a standard multi-layer perceptron with hid-
den layer h = tanh(i ? Wih + bh) and output layer
o = h ? Who + bo. where bh,o are the biases on the
respective layers. The vector o is normalized by the
softmax function fsoftmax(oi) = eoiP|V |
k=1 e
ok
. Back-
propagation (BKP) is used to learn model parame-
1
ters, including the M matrix, which is shared across
input words. The training criterion maximizes the
regularized log-likelihood of the training data.
3 Generalization in Language Models
An important task in language modeling is to pro-
vide reasonable probability estimates for n-grams
that were not observed in the training data. This
generalization capability is becoming increasingly
relevant in current large-scale speech and NLP sys-
tems that need to handle unlimited vocabularies and
domain mismatches. The smooth predictor func-
tion learned by NLMs can provide good generaliza-
tion if the test set contains n-grams whose individ-
ual words have been seen in similar context in the
training data. However, NLMs only have a simplis-
tic mechanism for dealing with words that were not
observed at all: OOVs in the test data are mapped
to a dedicated class and are assigned the singleton
probability when predicted (i.e. at the output layer)
and the features of a randomly selected singleton
word when occurring in the input. In standard back-
off n-gram models, OOVs are handled by reserv-
ing a small fixed amount of the discount probabil-
ity mass for the generic OOV word and treating it
as a standard vocabulary item. A more powerful
backoff strategy is used in factored language models
(FLMs) (Bilmes and Kirchhoff, 2003), which view
a word as a vector of word features or ?factors?:
w = ?f1, f2, . . . , fk? and predict a word jointly
from previous words and their factors: A general-
ized backoff procedure uses the factors to provide
probability estimates for unseen n-grams, combin-
ing estimates derived from different backoff paths.
This can also be interpreted as a generalization of
standard class-based models (Brown et al, 1992).
FLMs have been shown to yield improvements in
perplexity and word error rate in speech recogni-
tion, particularly on sparse-data tasks (Vergyri et
al., 2004) and have also outperformed backoff mod-
els using a linear decomposition of OOVs into se-
quences of morphemes. In this study we use factors
in the input encoding for NLMs.
4 Factored Neural Language Models
NLMs define word similarity solely in terms of their
context: words are assumed to be close in the contin-
uous space if they co-occur with the same (subset of)
words. But similarity can also be derived from word
shape features (affixes, capitalization, hyphenation
etc.) or other annotations (e.g. POS classes). These
allow a model to generalize across classes of words
bearing the same feature. We thus define a factored
neural language model (FNLM) (Fig. 2) which takes
as input the previous n ? 1 vectors of factors. Dif-
ferent factors map to disjoint row sets of the ma-
trix. The h and o layers are identical to the standard
NLM?s. Instead of predicting the probabilities for
n?1f
2
f 1n?1
n?1f
3
?  |V  | rows
M
i
h
o
Wih Who
n?2
f1
3
n?2f
fn?2
2
P(c   | c    ,c      ) t t?1 t?2
P(w  |c   )t t
d columns
d = continuous space size
k
k
k
V  =vocabulary of factor k
Figure 2: FNLM architecture. Input vectors consisting of
word and feature indices are mapped to rows in M. The final
multiplicative layer outputs the word probability distribution.
all words at the output layer directly, we first group
words into classes (obtained by Brown clustering)
and then compute the conditional probability of each
word given its class: P (wt) = P (ct) ? P (wt|ct).
This is a speed-up technique similar to the hierarchi-
cal structuring of output units used by (Morin and
Bengio, 2005), except that we use a ?flat? hierar-
chy. Like the standard NLM, the network is trained
to maximize the log-likelihood of the data. We use
BKP with cross-validation on the development set
and L2 regularization (the sum of squared weight
values penalized by a parameter ?) in the objective
function.
5 Handling Unknown Factors in FNLMs
In an FNLM setting, a subset of a word?s factors may
be known or can be reliably inferred from its shape
although the word itself never occurred in the train-
ing data. The FNLM can use the continuous repre-
sentation for these known factors directly in the in-
put. If unknown factors are still present, new contin-
uous representations are derived for them from those
of known factors of the same type. This is done by
averaging over the continuous vectors of a selected
subset of the words in the training data, which places
the new item in the center of the region occupied by
2
the subset. For example, proper nouns constitute a
large fraction of OOVs, and using the mean of the
rows in M associated with words with a proper noun
tag yields the ?average proper noun? representation
for the unknown word. We have experimented with
the following strategies for subset selection: NULL
(the null subset, i.e. the feature vector components
for unknown factors are 0), ALL (average of all
known factors of the same type); TAIL (averaging
over the least frequently encountered factors of that
type up to a threshold of 10%); and LEAST, i.e. the
representation of the single least frequent factors of
the same type. The prediction of OOVs themselves
is unaffected since we use a factored encoding only
for the input, not for the output (though this is a pos-
sibility for future work).
6 Data and Baseline Setup
We evaluate our approach by measuring perplex-
ity on two different language modeling tasks. The
first is the LDC CallHome Egyptian Colloquial Ara-
bic (ECA) Corpus, consisting of transcriptions of
phone conversations. ECA is a morphologically
rich language that is almost exclusively used in in-
formal spoken communication. Data must be ob-
tained by transcribing conversations and is therefore
very sparse. The present corpus has 170K words
for training (|V | = 16026), 32K for development
(dev), 17K for evaluation (eval97). The data was
preprocessed by collapsing hesitations, fragments,
and foreign words into one class each. The corpus
was further annotated with morphological informa-
tion (stems, morphological tags) obtained from the
LDC ECA lexicon. The OOV rates are 8.5% (de-
velopment set) and 7.7% (eval97 set), respectively.
Model ECA (?102) Turkish (?102)
dev eval dev eval
baseline 3gram 4.108 4.128 6.385 6.438
hand-optimized FLM 4.440 4.327 4.269 4.479
GA-optimized FLM 4.325 4.179 6.414 6.637
NLM 3-gram 4.857 4.581 4.712 4.801
FNLM-NULL 5.672 5.381 9.480 9.529
FNLM-ALL 5.691 5.396 9.518 9.555
FNLM-TAIL 10% 5.721 5.420 9.495 9.540
FNLM-LEAST 5.819 5.479 10.492 10.373
Table 1: Average probability (scaled by 102) of known words
with unknown words in order-2 context
The second corpus consists of Turkish newspa-
per text that has been morphologically annotated and
disambiguated (Hakkani-Tu?r et al, 2002), thus pro-
viding information about the word root, POS tag,
number and case. The vocabulary size is 67510
(relatively large because Turkish is highly aggluti-
native). 400K words are used for training, 100K
for development (11.8% OOVs), and 87K for test-
ing (11.6% OOVs). The corpus was preprocessed by
removing segmentation marks (titles and paragraph
boundaries).
7 Experiments and Results
We first investigated how the different OOV han-
dling methods affect the average probability as-
signed to words with OOVs in their context. Ta-
ble 1 shows that average probabilities increase com-
pared to the strategy described in Section 3 as
well as other baseline models (standard backoff tri-
grams and FLM, further described below), with the
strongest increase observed for the scheme using the
least frequent factor as an OOV factor model. This
strategy is used for the models in the following per-
plexity experiments.
We compare the perplexity of word-based and
factor-based NLMs with standard backoff trigrams,
class-based trigrams, FLMs, and interpolated mod-
els. Evaluation was done with (the ?w/unk? column
in Table 2) and without (the ?no unk? column) scor-
ing of OOVs, in order to assess the usefulness of our
approach to applications using closed vs. open vo-
cabularies. The baseline Model 1 is a standard back-
off 3-gram using modified Kneser-Ney smoothing
(model orders beyond 3 did not improve perplex-
ity). Model 2 is a class-based trigram model with
Brown clustering (256 classes), which, when inter-
polated with the baseline 3-gram, reduces the per-
plexity (see row 3). Model 3 is a 3-gram word-based
NLM (with output unit clustering). For NLMs,
higher model orders gave improvements, demon-
strating their better scalability: for ECA, a 6-gram
(w/o unk) and a 5-gram (w/unk) were used; for Turk-
ish, a 7-gram (w/o unk) and a 5-gram (w/unk) were
used. Though worse in isolation, the word-based
NLMs reduce perplexity considerably when interpo-
lated with Model 1. The FLM baseline is a hand-
optimized 3-gram FLM (Model 5); we also tested
an FLM optimized with a genetic algorithm as de-
3
# Model ECA dev ECA eval Turkish dev Turkish eval
no unk w/unk no unk w/unk no unk w/unk no unk w/unk
1 Baseline 3-gram 191 176 183 172 827 569 855 586
2 Class-based LM 221 278 219 269 1642 1894 1684 1930
3 1) & 2) 183 169 178 167 790 540 814 555
4 Word-based NLM 208 341 204 195 1510 1043 1569 1067
5 1) & 4) 178 165 173 162 758 542 782 557
6 Word-based NLM 202 194 204 192 1991 1369 2064 1386
7 1) & 6) 175 162 173 160 754 563 772 580
8 hand-optimized FLM 187 171 178 166 827 595 854 614
9 1) & 8) 182 167 174 163 805 563 832 581
10 genetic FLM 190 188 181 188 761 1181 776 1179
11 1) & 10) 183 166 175 164 706 488 720 498
12 factored NLM 189 173 190 175 1216 808 1249 832
13 1) & 12) 169 155 168 155 724 487 744 500
14 1) & 10) & 12) 165 155 165 154 652 452 664 461
Table 2: Perplexities for baseline backoff LMs, FLMs, NLMs, and LM interpolation
scribed in (Duh and Kirchhoff, 2004) (Model 6).
Rows 7-10 of Table 2 display the results. Finally, we
trained FNLMs with various combinations of fac-
tors and model orders. The combination was opti-
mized by hand on the dev set and is therefore most
comparable to the hand-optimized FLM in row 8.
The best factored NLM (Model 7) has order 6 for
both ECA and Turkish. It is interesting to note that
the best Turkish FNLM uses only word factors such
as morphological tag, stem, case, etc. but not the
actual words themselves in the input. The FNLM
outperforms all other models in isolation except the
FLM; its interpolation with the baseline (Model 1)
yields the best result compared to all previous inter-
polated models, for both tasks and both the unk and
no/unk condition. Interpolation of Model 1, FLM
and FNLM yields a further improvement. The pa-
rameter values of the (F)NLMs range between 32
and 64 for d, 45-64 for the number of hidden units,
and 362-1024 for C (number of word classes at the
output layer).
8 Conclusion
We have introduced FNLMs, which combine neu-
ral probability estimation with factored word repre-
sentations and different ways of inferring continuous
word features for unknown factors. On sparse-data
Arabic and Turkish language modeling task FNLMs
were shown to outperform all comparable models
(standard backoff 3-gram, word-based NLMs) ex-
cept FLMs in isolation, and all models when inter-
polated with the baseline. These conclusions apply
to both open and closed vocabularies.
Acknowledgments
This work was funded by NSF under grant no. IIS-
0326276 and DARPA under Contract No. HR0011-
06-C-0023. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the views of these agencies.
References
Y. Bengio, R. Ducharme, and P. Vincent. 2000. A neural
probabilistic language model. In NIPS.
J.A. Bilmes and K. Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
HLT-NAACL.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics, 18(4).
K. Duh and K. Kirchhoff. 2004. Automatic learning of
language model structure. In COLING 2004.
A. Emami and F. Jelinek. 2004. Exact training of a neu-
ral syntactic language model. In ICASSP 2004.
D. Hakkani-Tu?r, K. Oflazer, and G. Tu?r. 2002. Statistical
morphological disambiguation for agglutinative lan-
guages. Journal of Computers and Humanities, 36(4).
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In AISTATS.
H. Schwenk and J.L. Gauvain. 2004. Neural network
language models for conversational speech recogni-
tion. In ICSLP 2004.
H. Schwenk. 2005. Training neural network language
models on very large corpora. In HLT/EMNLP.
D. Vergyri, K. Kirchhoff, K. Duh, and A. Stolcke.
2004. Morphology-based language modeling for ara-
bic speech recognition. In ICSLP.
P. Xu, A. Emami, and F. Jelinek. 2003. Training connec-
tionist models for the structured language model. In
EMNLP 2003.
4
Proceedings of NAACL HLT 2007, pages 204?211,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Data-Driven Graph Construction for Semi-Supervised Graph-Based
Learning in NLP
Andrei Alexandrescu
Dept. of Computer Science and Engineering
University of Washington
Seattle, WA, 98195
andrei@cs.washington.edu
Katrin Kirchhoff
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195
katrin@ee.washington.edu
Abstract
Graph-based semi-supervised learning has
recently emerged as a promising approach
to data-sparse learning problems in natu-
ral language processing. All graph-based
algorithms rely on a graph that jointly rep-
resents labeled and unlabeled data points.
The problem of how to best construct this
graph remains largely unsolved. In this
paper we introduce a data-driven method
that optimizes the representation of the
initial feature space for graph construc-
tion by means of a supervised classifier.
We apply this technique in the frame-
work of label propagation and evaluate
it on two different classification tasks, a
multi-class lexicon acquisition task and a
word sense disambiguation task. Signifi-
cant improvements are demonstrated over
both label propagation using conventional
graph construction and state-of-the-art su-
pervised classifiers.
1 Introduction
Natural Language Processing (NLP) applications
benefit from the availability of large amounts of an-
notated data. However, such data is often scarce,
particularly for non-mainstream languages. Semi-
supervised learning addresses this problem by com-
bining large amounts of unlabeled data with a small
set of labeled data in order to learn a classifica-
tion function. One class of semi-supervised learn-
ing algorithms that has recently attracted increased
interest is graph-based learning. Graph-based tech-
niques represent labeled and unlabeled data points
as nodes in a graph with weighted edges encoding
the similarity of pairs of samples. Various tech-
niques are then available for transferring class la-
bels from the labeled to the unlabeled data points.
These approaches have shown good performance in
cases where the data is characterized by an underly-
ing manifold structure and samples are judged to be
similar by local similarity measures. However, the
question of how to best construct the graph forming
the basis of the learning procedure is still an under-
investigated research problem. NLP learning tasks
present additional problems since they often rely on
discrete or heterogeneous feature spaces for which
standard similarity measures (such as Euclidean or
cosine distance) are suboptimal.
We propose a two-pass data-driven technique for
graph construction in the framework of label propa-
gation (Zhu, 2005). First, we use a supervised clas-
sifier trained on the labeled subset to transform the
initial feature space (consisting of e.g. lexical, con-
textual, or syntactic features) into a continuous rep-
resentation in the form of soft label predictions. This
representation is then used as a basis for measur-
ing similarity among samples that determines the
structure of the graph used for the second, semi-
supervised learning step. It is important to note that,
rather than simply cascading the supervised and the
semi-supervised learner, we optimize the combina-
tion with respect to the properties required of the
graph. We present several techniques for such op-
timization, including regularization of the first-pass
classifier, biasing by class priors, and linear combi-
204
nation of classifier predictions with known features.
The proposed approach is evaluated on a lexicon
learning task using the Wall Street Journal (WSJ)
corpus, and on the SENSEVAL-3 word sense dis-
ambiguation task. In both cases our technique sig-
nificantly outperforms our baseline systems (label
propagation using standard graph construction and
discriminatively trained supervised classifiers).
2 Background
Several graph-based learning techniques have re-
cently been developed and applied to NLP prob-
lems: minimum cuts (Pang and Lee, 2004), random
walks (Mihalcea, 2005; Otterbacher et al, 2005),
graph matching (Haghighi et al, 2005), and label
propagation (Niu et al, 2005). Here we focus on
label propagation as a learning technique.
2.1 Label propagation
The basic label propagation (LP) algorithm (Zhu and
Ghahramani, 2002; Zhu, 2005) has as inputs:
? a labeled set {(x1, y1), (x2, y2), . . . , (xn, yn)},
where xi are samples (feature vectors) and yi ?
{1, 2, . . . , C} are their corresponding labels;
? an unlabeled set {xn+1, . . . , xN};
? a distance measure d(i, j) i, j ? {1, . . . N} de-
fined on the feature space.
The goal is to infer the labels {yn+1, . . . , yN} for
the unlabeled set. The algorithm represents all N
data points as vertices in an undirected graph with
weighted edges. Initially, only the known data ver-
tices are labeled. The edge linking vertices i and j
has weight:
wij = exp
(
?d(i, j)
2
?2
)
(1)
where ? is a hyperparameter that needs to be empir-
ically chosen or learned separately. wij indicates the
label affinity of vertices: the larger wij is, the more
likely it is that i and j have the same label. The LP
algorithm constructs a row-normalized N ?N tran-
sition probability matrix P as follows:
Pij = P (i? j) =
wij
?N
k=1 wik
(2)
The algorithm probabilistically pushes labels from
the labeled nodes to the unlabeled nodes. To do so, it
defines the n?C hard labels matrix Y and the N?C
soft labels matrix f , whose first n rows are identical
to Y . The hard labels matrix Y is invariant through
the algorithm and is initialized with probability 1 for
the known label and 0 for all other labels:
Yic = ?(yi, C) (3)
where ? is Kronecker?s delta function. The algo-
rithm iterates as follows:
1. f ? ? P ? f
2. f ?[rows 1 to n] ? Y
3. If f ? ?= f , stop
4. f ? f ?
5. Repeat from step 1
In each iteration, step 2 fixes the known labels,
which might otherwise be overriden by propagated
labels. The resulting labels for each feature xi,
where i ? {n + 1, . . . , N}, are:
li = arg max
j=1,...,C
fij (4)
It is important that the distance measure is locally
accurate, i.e. nodes connected by an edge with a
high weight should have the same label. The global
distance is less relevant since label information will
be propagated from labeled points through the entire
space. This is why LP works well with a local dis-
tance measure that might be unsuitable as a global
distance measure.
Applications of LP include handwriting recogni-
tion (Zhu and Ghahramani, 2002), image classifi-
cation (Balcan et al, 2005) and retrieval (Qin et
al., 2005), and protein classification (Weston et al,
2003). In NLP, label propagation has been used for
word sense disambiguation (Niu et al, 2005), doc-
ument classification (Zhu, 2005), sentiment analy-
sis (Goldberg and Zhu, 2006), and relation extrac-
tion (Chen et al, 2006).
2.2 Graph construction
One of the main problems in LP, as well as other
graph-based learning techniques, is how to best con-
struct the graph. Currently, graph construction ?is
more of an art than science? (Zhu, 2005). Typically,
edge weights are derived from a simple Euclidean
or cosine distance measure, regardless of the nature
of the underlying features. Edges are then estab-
lished either by connecting all nodes, by applying
a single global threshold to the edge weights, or by
connecting each node to its k nearest neighbors ac-
cording to the edge weights. This procedure is often
suboptimal: Euclidean distance relies on a model of
normally distributed i.i.d. random variables; cosine
205
distance likewise assumes that the different feature
vector dimensions are uncorrelated. However, many
applications, particularly in NLP, rely on feature
spaces with correlated dimensions. Moreover, fea-
tures may have different ranges and different types
(e.g. continuous, binary, multi-valued), which en-
tails the need for normalization, binning, or scaling.
Finally, common distance measures do not take ad-
vantage of domain knowledge that might be avail-
able.
Some attempts have been made at improving the
standard method of graph construction. For in-
stance, in a face identification task (Balcan et al,
2005), domain knowledge was used to identify three
different edge sets based on time, color and face
features, associating a different hyperparameter with
each. The resulting graph was then created by super-
posing edge sets. Zhu (Zhu, 2005, Ch. 7) describes
graph construction using separate ? hyperparame-
ters for each feature dimension, and presents a data-
driven way (evidence maximization) for learning the
values of the parameters.
3 Data-driven graph construction
Unlike previous work, we propose to optimize the
feature representation used for graph construction
by learning it with a first-pass supervised classi-
fier. Under this approach, similarity of samples is
defined as similarity of the output values produced
by a classifier applied to the original feature repre-
sentation of the samples. This idea bears similar-
ity to classifier cascading (Alpaydin and Kaynak,
1998), where classifiers are trained around a rule-
exceptions paradigm; however, in our case, the clas-
sifiers work together, the first acting as a jointly op-
timized feature mapping function for the second.
1. Train a first-pass supervised classifier that out-
puts soft label predictions Zi for all sam-
ples i ? {1, . . . N}, e.g. a posterior prob-
ability distribution over target labels: Zi =
?pi1, pi2, . . . , piC?;
2. Apply postprocessing to Zi if needed.
3. Use vectors Zi and an appropriately chosen dis-
tance measure to construct a graph for LP.
4. Perform label propagation over the constructed
graph to find the labeling of the test samples.
The advantages of this procedure are:
? Uniform range and type of features: The out-
put from a first-pass classifier can produce well-
defined features, e.g. posterior probability distribu-
tions. This eliminates the problem of input features
of different ranges and types (e.g. binary vs. multi-
valued, continuous vs. categorical attributes) which
are often used in combination.
? Feature postprocessing: The transformation of
features into a different space also opens up pos-
sibilities for postprocessing (e.g. probability distri-
bution warping) depending on the requirements of
the second-pass learner. In addition, different dis-
tance functions (e.g. those defined on probability
spaces) can be used, which avoids violating assump-
tions made by metrics such as Euclidean and cosine
distance.
? Optimizing class separation: The learned repre-
sentation of labeled training samples might reveal
better clusters in the data than the original represen-
tation: a discriminatively-trained first pass classifier
will attempt to maximize the separation of samples
belonging to different classes. Moreover, the first-
pass classifier may learn a feature transformation
that suppresses noise in the original input space.
Difficulties with the proposed approach might arise
when the first-pass classifier yields confident but
wrong predictions, especially for outlier samples in
the original space. For this reason, the first-pass
classifier and the graph-based learner should not
simply be concatenated without modification, but
the first classifier should be optimized with respect
to the requirements of the second. In our case, the
choice of first-pass classifier and joint optimization
techniques are determined by the particular learning
task and are detailed below.
4 Tasks
4.1 Lexicon acquisition task
Our first task is a part-of-speech (POS) lexicon ac-
quisition task, i.e. the labels to be predicted are the
sets of POS tags associated with each word in a lex-
icon. Note that this is not a tagging task: we are not
attempting to identify the correct POS of each word
in running text. Rather, for each word in the vocab-
ulary, we attempt to infer the set of possible POS
tags. Our choice of this task is motivated by our
long-term goal of applying this technique to lexicon
acquisition for resource-poor languages: POS lexi-
206
cons are one of the most basic language resources,
which enable subsequent training of taggers, chun-
kers, etc. We assume that a small set of words can be
reliably annotated, and that POS-sets for the remain-
ing words can be inferred by semi-supervised learn-
ing. Rather than choosing a genuinely resource-poor
language for this task, we use the English Wall Street
Journal (WSJ) corpus and artificially limit the size
of the labeled set. This is because the WSJ corpus is
widely obtainable and allows easy replication of our
experiments.
We use sections 0-18 of the Wall Street Journal
corpus (N = 44, 492). Words have between 1 and
4 POS tags, with an average of 1.1 per word. The
number of POS tags is 36, and we treat every POS
combination as a unique class, resulting in C = 158
distinct labels. We use three different randomly se-
lected training sets of various sizes: 5000, 10000,
and 15000 words, representing about 11%, 22%, and
34% of the entire data set respectively; the rest of the
data was used for testing. In order to avoid experi-
mental bias, we run all experiments on five differ-
ent randomly chosen labeled subsets and report av-
erages and standard deviations. Due to the random
sampling of the data it is possible that some labels
never occur in the training set or only occur once.
We train our classifiers only on those labels that oc-
cur at least twice, which results in 60-63 classes. La-
bels not present in the training set will therefore not
be hypothesized and are guaranteed to be errors. We
delete samples with unknown labels from our unla-
beled set since their percentage is less than 0.5% on
average.
We use the following features to represent sam-
ples:
? Integer: the three-letter suffix of the word;
? Integer: The four-letter suffix of the word;
? Integer ? 4: The indices of the four most fre-
quent words that immediately precede the word
in the WSJ text;
? Boolean: word contains capital letters;
? Boolean: word consists only of capital letters;
? Boolean: word contains digits;
? Boolean: word contains a hyphen;
? Boolean: word contains other special charac-
ters (e.g. ?&?).
We have also experimented with shorter suffixes and
with prefixes but those features tended to degrade
performance.
4.2 SENSEVAL-3 word sense disambiguation
task
The second task is word sense disambiguation using
the SENSEVAL-3 corpus (Mihalcea et al, 2004), to
enable a comparison of our method with previously
published results. The goal is to disambiguate the
different senses of each of 57 words given the sen-
tences within which they occur. There are 7860 sam-
ples for training and 3944 for testing. In line with
existing work (Lee and Ng, 2002; Niu et al, 2005),
we use the following features:
? Integer ? 7: seven features consisting of the
POS of the previous three words, the POS of
the next three words, and the POS of the word
itself. We used the MXPOST tagger (Ratna-
parkhi, 1996) for POS annotation.
? Integer??variable length?: a bag of all words
in the surrounding context.
? Integer ? 15: Local collocations Cij (i, j are
the bounds of the collocation window)?word
combinations from the context of the word to
disambiguate. In addition to the 11 collocations
used in similar work (Lee and Ng, 2002), we
also used C?3,1, C?3,2, C?2,3, C?1,3.
Note that syntactic features, which have been used in
some previous studies on this dataset (Mohammad
and Pedersen, 2004), were not included. We apply a
simple feature selection method: a feature X is se-
lected if the conditional entropy H(Y |X) is above
a fixed threshold (1 bit) in the training set, and if X
also occurs in the test set (note that no label infor-
mation from the test data is used for this purpose).
5 Experiments
For both tasks we compare the performance of a su-
pervised classifier, label propagation using the stan-
dard input features and either Euclidean or cosine
distance, and LP using the output from a first-pass
supervised classifier.
5.1 Lexicon acquisition task
5.1.1 First-pass classifier
For this task, the first-pass classifier is a multi-
layer perceptron (MLP) with the topology shown
in Fig. 1. The input features are mapped to con-
207
x 2
x 4
x 1
x 3
P(y | x)
M
i
h
o
Wih Who
A
Figure 1: Architecture of first-pass supervised classifier (MLP)
for lexicon acquisition
.
tinuous values by a discrete-to-continuous mapping
layer M , which is itself learned during the MLP
training process. This layer connects to the hidden
layer h, which in turn is connected to the output
layer o. The entire network is trained via backprop-
agation. The training criterion maximizes the regu-
larized log-likelihood of the training data:
L = 1n
n
?
t=1
log P (yt|xt, ?) + R(?) (5)
The use of an additional continuous mapping layer
is similar to the use of hidden continuous word rep-
resentations in neural language modeling (Bengio et
al., 2000) and yields better results than a standard
3-layer MLP topology.
Problems caused by data scarcity arise when some
of the input features of the unlabeled words have
never been seen in the training set, resulting in un-
trained, randomly-initialized values for those fea-
ture vector components. We address this problem
by creating an approximation layer A that finds the
known input feature vector x? that is most similar
to x (by measuring the cosine similarity between
the vectors). Then xk is replaced with x?k, resulting
in vector x? = ?x1, . . . , xk?1, x?k, xk+1, . . . , xf ? that
has no unseen features and is closest to the original
vector.
5.1.2 LP Setup
We use a dense graph approach. The WSJ set
has a total of 44,492 words, therefore the P ma-
trix that the algorithm requires would have 44, 492?
44, 492 ?= 2? 109 elements. Due to the matrix size,
we avoid the analytical solution of the LP problem,
which requires inverting the P matrix, and choose
the iterative approach described above (Sec. 2.1) in-
stead. Convergence is stopped when the maximum
relative difference between each cell of f and the
corresponding cell of f ? is less than 1%.
Also for data size reasons, we apply LP in chunks.
While the training set stays in memory, the test
data is loaded in fixed-size chunks, labeled, and dis-
carded. This approach has yielded similar results
for various chunk sizes, suggesting that chunking is
a good approximation of whole-set label propaga-
tion.1 LP in chunks is also amenable to paralleliza-
tion: Our system labels different chunks in parallel.
We trained the ? hyperparameter by three-fold
cross-validation on the training data, using a geo-
metric progression with limits 0.1 and 10 and ratio
2. We set fixed upper limits of edges between an
unlabeled node and its labeled neighbors to 15, and
between an unlabeled node and its unlabeled neigh-
bors to 5. The approach of setting different limits
among different kinds of nodes is also used in re-
lated work (Goldberg and Zhu, 2006).
For graph construction we tested: (a) the original
discrete input representation with cosine distance;
(b) the classifier output features (probability distri-
butions) with the Jeffries-Matusita distance.
5.2 Combination optimization
The static parameters of the MLP (learning rate, reg-
ularization rate, and number of hidden units) were
optimized for the LP step by 5-fold cross-validation
on the training data. This process is important be-
cause overspecialization is detrimental to the com-
bined system: an overspecialized first-pass classi-
fier may output very confident but wrong predic-
tions for unseen patterns, thus placing such samples
at large distances from all correctly labeled sam-
ples. A strongly regularized neural network, by con-
trast, will output smoother probability distributions
for unseen patterns. Such outputs also result in a
smoother graph, which in turn helps the LP process.
Thus, we found that a network with only 12 hidden
units and relatively high R(?) in Eq. 5 (10% of the
weight value) performed best in combination with
LP (at an insignificant cost in accuracy when used
1In fact, experiments have shown that performance tends to
degrade for larger chunk sizes, suggesting that whole-set LP
might be affected by ?artifact? clusters that are not related to
the labels.
208
as an isolated classifier).
5.2.1 Results
We first conducted an experiment to measure the
smoothness of the underlying graph, S(G), in the
two LP experiments according to the following for-
mula: S(G) =
?
yi 6=yj ,(i>n?j>n)
wij (6)
where yi is the label of sample i. (Lower values are
better as they reflect less affinity between nodes of
different labels.) The value of S(G) was in all cases
significantly better on graphs constructed with our
proposed technique than on graphs constructed in
the standard way (see Table 1). Table 1 also shows
the performance comparison between LP over the
discrete representation and cosine distance (?LP?),
the neural network itself (?NN?), and LP over the
continuous representation (?NN+LP?), on all dif-
ferent subsets and for different training sizes. For
scarce labeled data (5000 samples) the neural net-
work, which uses a strictly supervised training pro-
cedure, is at a clear disadvantage. However, for a
larger training set the network is able to perform
more accurately than the LP learner that uses the
discrete features directly. The third, combined tech-
nique outperforms the first two significantly.2 The
differences are more pronounced for smaller train-
ing set sizes. Interestingly, the LP is able to extract
information from largely erroneous (noisy) distribu-
tions learned by the neural network.
5.3 Word Sense Disambiguation
We compare the performance of an SVM classifier,
an LP learner using the same input features as the
SVM, and an LP learner using the SVM outputs as
input features. To analyze the influence of train-
ing set size on accuracy, we randomly sample sub-
sets of the training data (25%, 50%, and 75%) and
use the remaining training data plus the test data
as unlabeled data, similarly to the procedure fol-
lowed in related work (Niu et al, 2005). The re-
sults are averaged over five different random sam-
plings. The samplings were chosen such that there
was at least one sample for each label in the training
set. SENSEVAL-3 sports multi-labeled samples and
2Significance was tested using a difference of proportions
significance test; the significance level is 0.01 or smaller in all
cases.
samples with the ?unknown? label. We eliminate all
samples labeled as unknown and retain only the first
label for the multi-labeled instances.
5.3.1 SVM setup
The use of SVM vs. MLP in this case was justi-
fied by the very small training data set. An MLP has
many parameters and needs a considerable amount
of data for effective training, so for this task with
only on the order of 102 training samples per classi-
fier, an SVM was deemed more appropriate. We use
the SVMlight package to build a set of binary clas-
sifiers in a one-versus-all formulation of the multi-
class classification problem. The features input to
each SVM consist of the discrete features described
above (Sec. 4.2) after feature selection. After train-
ing SVMs for each target label against the union of
all others, we evaluate the SVM approach against the
test set by using the winner-takes-all strategy: the
predicted label corresponds to the SVM that outputs
the largest value.
5.3.2 LP setup
Again we set up two LP systems: one using the
original feature space (after feature selection, which
benefited all of the tested systems) and one using the
SVM outputs. Both use a cosine distance measure.
The ? parameter (see Eq. 1) is optimized through
3-fold cross-validation on the training set.
5.4 Combination optimization
Unlike MLPs, SVMs do not compute a smooth out-
put distribution but base the classification decision
on the sign of the output values. In order to smooth
output values with a view towards graph construc-
tion we applied the following techniques:
1. Combining SVM predictions and perfect fea-
ture vectors: After training, the SVM actu-
ally outputs wrong label predictions for a small
number (? 5%) of training samples. These out-
puts could simply be replaced with the perfect
SVM predictions (1 for the true class, -1 else-
where) since the labels are known. However,
the second-pass learner might actually bene-
fit from the information contained in the mis-
classifications. We therefore linearly combine
the SVM predictions with the ?perfect? feature
209
Initial labels Model S(G) avg. Accuracy (%)
Set 1 Set 2 Set 3 Set 4 Set 5 Average
5000 NN ? 50.70 59.22 63.77 60.09 54.58 57.67 ? 4.55
LP 451.54 58.37 59.91 60.88 62.01 59.47 60.13 ? 1.24
NN+LP 409.79 58.03 63.91 66.62 65.93 57.76 62.45 ? 3.83
10000 NN ? 65.86 60.19 67.52 65.68 65.64 64.98 ? 2.49
LP 381.16 58.27 60.04 60.85 61.99 62.06 60.64 ? 1.40
NN+LP 315.53 69.36 64.73 69.50 70.26 67.71 68.31 ? 1.97
15000 NN ? 69.85 66.42 70.88 70.71 72.18 70.01 ? 1.94
LP 299.10 58.51 61.00 60.94 63.53 60.98 60.99 ? 1.59
NN+LP 235.83 70.59 69.45 69.99 71.20 73.45 70.94 ? 1.39
Table 1: Accuracy results of neural classification (NN), LP with discrete features (LP), and combined (NN+LP), over 5 random
samplings of 5000, 10000, and 15000 labeled words in the WSJ lexicon acquisition task. S(G) is the smoothness of the graph
vectors v that contain 1 at the correct label po-
sition and -1 elsewhere:
s?i = ?si + (1? ?)vi (7)
where si, s?i are the i?th input and output feature
vectors and ? a parameter fixed at 0.5.
2. Biasing uninformative distributions: For some
training samples, although the predicted class
label was correct, the outputs of the SVM were
relatively close to one another, i.e. the decision
was borderline. We decided to bias these SVM
outputs in the right direction by using the same
formula as in equation 7.
3. Weighting by class priors: For each training
sample, a corresponding sample with the per-
fect output features was added, thus doubling
the total number of labeled nodes in the graph.
These synthesized nodes are akin to the ?don-
gle? nodes (Goldberg and Zhu, 2006). The dif-
ference is that, while dongle nodes are only
linked to one node, our artificial nodes are
treated like any other node and as such can con-
nect to several other nodes. The role of the arti-
ficial nodes is to serve as authorities during the
LP process and to emphasize class priors.
5.4.1 Results
As before, we measured the smoothness of the
graphs in the two label propagation setups and found
that in all cases the smoothness of the graph pro-
duced with our method was better when compared
to the graphs produced using the standard approach,
as shown in Table 3, which also shows accuracy re-
sults for the SVM (?SVM? label), LP over the stan-
dard graph (?LP?), and label propagation over SVM
outputs (?SVM+LP?). The latter system consistently
performs best in all cases, although the most marked
gains occur in the upper range of labeled samples
percentage. The gain of the best data-driven LP over
the knowledge-based LP is significant in the 100%
and 75% cases.
# System Acc. (%)
1 htsa3 (Grozea, 2004) 72.9
2 IRST-kernels (Strapparava et al, 2004) 72.6
3 nusels (Lee et al, 2004) 72.4
4 SENSEVAL-3 contest baseline 55.2
5 Niu et al (Niu et al, 2005) LP/J-S 70.3
6 Niu et al LP/cosine 68.4
7 Niu et al SVM 69.7
Table 2: Accuracy results of other published systems on
SENSEVAL-3. 1-3 use syntactic features; 5-7 are directly com-
parably to our system.
For comparison purposes, Table 2 shows results
of other published systems against the SENSEVAL
corpus. The ?htsa3?, ?IRST-kernels?, and ?nusels?
systems were the winners of the SENSEVAL-3 con-
test and used extra input features (syntactic rela-
tions). The Niu et al work (Niu et al, 2005) is
most comparable to ours. We attribute the slightly
higher performance of our SVM due to our feature
selection process. The LP/cosine system is a system
similar to our LP system using the discrete features,
and the LP/Jensen-Shannon system is also similar
but uses a distance measure derived from Jensen-
Shannon divergence.
6 Conclusions
We have presented a data-driven graph construction
technique for label propagation that utilizes a first-
210
Initial labels Model S(G) avg. Accuracy (%)
Set 1 Set 2 Set 3 Set 4 Set 5 Average
25% SVM ? 62.94 62.53 62.69 63.52 62.99 62.93 ? 0.34
LP 44.71 63.27 61.84 63.26 62.96 63.30 62.93 ? 0.56
SVM+LP 39.67 63.39 63.20 63.95 63.68 63.91 63.63 ? 0.29
50% SVM ? 67.90 66.75 67.57 67.44 66.79 67.29 ? 0.45
LP 33.17 67.84 66.57 67.35 66.52 66.35 66.93 ? 0.57
SVM+LP 24.19 67.95 67.54 67.93 68.21 68.11 67.95 ? 0.23
75% SVM ? 69.54 70.19 68.75 69.80 68.73 69.40 ? 0.58
LP 29.93 68.87 68.65 68.58 68.42 67.19 68.34 ? 0.59
SVM+LP 16.19 69.98 70.05 69.69 70.38 68.94 69.81 ? 0.49
100% SVM ? 70.74
LP 21.72 69.69
SVM+LP 13.17 71.72
Table 3: Accuracy results of support vector machine (SVM), label propagation over discrete features (LP), and label propagation
over SVM outputs (SVM+LP), each trained with 25%, 50%, 75% (5 random samplings each), and 100% of the train set. The
improvements of SVM+LP are significant over LP in the 75% and 100% cases. S(G) is the graph smoothness
pass supervised classifier. The outputs from this
classifier (especially when optimized for the second-
pass learner) were shown to serve as a better repre-
sentation for graph-based semi-supervised learning.
Classification results on two learning tasks showed
significantly better performance compared to LP us-
ing standard graph construction and the supervised
classifier alone.
Acknowledgments This work was funded by
NSF under grant no. IIS-0326276. Any opinions,
findings and conclusions, or recommendations ex-
pressed herein are those of the authors and do not
necessarily reflect the views of this agency.
References
E. Alpaydin and C. Kaynak. 1998. Cascading classifiers. Ky-
bernetika, 34:369?374.
Balcan et al 2005. Person identification in webcam images. In
ICML Workshop on Learning with Partially Classified Train-
ing Data.
Y. Bengio, R. Ducharme, and P. Vincent. 2000. A neural prob-
abilistic language model. In NIPS.
J. Chen, D. Ji, C.L. Tan, and Z. Niu. 2006. Relation Extraction
Using Label Propagation Based Semi-supervised Learning.
In Proceedings of ACL, pages 129?136.
A. Goldberg and J. Zhu. 2006. Seeing stars when there aren?t
many stars: Graph-based semi-supervised learning for sen-
timent categorization. In HLT-NAACL Workshop on Graph-
based Algorithms for Natural Language Processing.
C. Grozea. 2004. Finding optimal parameter settings for high
performance word sense disambiguation. Proceedings of
Senseval-3 Workshop.
A. Haghighi, A. Ng, and C.D. Manning. 2005. Robust textual
inference via graph matching. Proceedings of EMNLP.
Y.K. Lee and H.T. Ng. 2002. An empirical evaluation of knowl-
edge sources and learning algorithms for word sense disam-
biguation. In Proceedings of EMNLP, pages 41?48.
Y.K. Lee, H.T. Ng, and T.K. Chia. 2004. Supervised Word
Sense Disambiguation with Support Vector Machines and
Multiple Knowledge Sources. SENSEVAL-3.
R. Mihalcea, T. Chklovski, and A. Killgariff. 2004. The
Senseval-3 English Lexical Sample Task. In Proceedings
of ACL/SIGLEX Senseval-3.
R. Mihalcea. 2005. Unsupervised large-vocabulary word sense
disambiguation with graph-based algorithms for sequence
data labeling. In Proceedings of HLT/EMNLP, pages 411?
418.
S. Mohammad and T. Pedersen. 2004. Complementarity of
Lexical and Simple Syntactic Features: The SyntaLex Ap-
proach to Senseval-3. Proceedings of the SENSEVAL-3.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005. Word
sense disambiguation using label propagation based semi-
supervised learning. In ACL ?05.
J. Otterbacher, G. Erkan, and D.R. Radev. 2005. Using Ran-
dom Walks for Question-focused Sentence Retrieval. Pro-
ceedings of HLT/EMNLP, pages 915?922.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL, pages 271?278.
T. Qin, T.-Y. Liu, X.-D. Zhang, W.-Y. Ma, and H.-J. Zhang.
2005. Subspace clustering and label propagation for active
feedback in image retrieval. In MMM, pages 172?179.
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proceedings of EMNLP, pages 133?142.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern
abstraction and term similarity for word sense disambigua-
tion: IRST at SENSEVAL-3. Proc. of SENSEVAL-3, pages
229?234.
J. Weston, C. Leslie, D. Zhou, A. Elisseeff, and W. Noble.
2003. Semi-supervised protein classification using cluster
kernels.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical report,
CMU-CALD-02.
Xiaojin Zhu. 2005. Semi-Supervised Learning with Graphs.
Ph.D. thesis, Carnegie Mellon University. CMU-LTI-05-
192.
211
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 119?127,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Graph-based Learning for Statistical Machine Translation
Andrei Alexandrescu
Dept. of Comp. Sci. Eng.
University of Washington
Seattle, WA 98195, USA
andrei@cs.washington.edu
Katrin Kirchhoff
Dept. of Electrical Eng.
University of Washington
Seattle, WA 98195, USA
katrin@ee.washington.edu
Abstract
Current phrase-based statistical machine
translation systems process each test sentence
in isolation and do not enforce global consis-
tency constraints, even though the test data
is often internally consistent with respect to
topic or style. We propose a new consistency
model for machine translation in the form
of a graph-based semi-supervised learning
algorithm that exploits similarities between
training and test data and also similarities
between different test sentences. The algo-
rithm learns a regression function jointly over
training and test data and uses the resulting
scores to rerank translation hypotheses. Eval-
uation on two travel expression translation
tasks demonstrates improvements of up to 2.6
BLEU points absolute and 2.8% in PER.
1 Introduction
Current phrase-based statistical machine translation
(SMT) systems commonly operate at the sentence
level?each sentence is translated in isolation, even
when the test data consists of internally coherent
paragraphs or stories, such as news articles. For
each sentence, SMT systems choose the translation
hypothesis that maximizes a combined log-linear
model score, which is computed independently of
all other sentences, using globally optimized com-
bination weights. Thus, similar input strings may
be translated in very different ways, depending on
which component model happens to dominate the
combined score for that sentence. This is illustrated
by the following example (from the IWSLT 2007
Arabic-English translation task):
Source 1: Asf lA ymknk *lk hnAk klfp HwAly vmAnyn
dwlAr lAlsAEp AlwAHdp
Ref: sorry you can?t there is a cost the charge is eighty
dollars per hour
1-best: i?m sorry you can?t there in the cost about eighty
dollars for a one o?clock
Source 2: E*rA lA ymknk t$gyl AltlfAz HtY tqlE
AlTA}rp
Ref: sorry you cannot turn the tv on until the plane has
taken off
1-best: excuse me i you turn tv until the plane departs
The phrase lA ymknk (you may not/you cannot)
is translated differently (and wrongly in the sec-
ond case) due to different segmentations and phrase
translations chosen by the decoder. Though differ-
ent choices may be sometimes appropriate, the lack
of constraints enforcing translation consistency of-
ten leads to suboptimal translation performance. It
would be desirable to counter this effect by encour-
aging similar outputs for similar inputs (under a suit-
ably defined notion of similarity, which may include
e.g. a context specification for the phrase/sentence).
In machine learning, the idea of forcing the out-
puts of a statistical learner to vary smoothly with the
underlying structure of the inputs has been formal-
ized in the graph-based learning (GBL) framework.
In GBL, both labeled (train) and unlabeled (test)
data samples are jointly represented as vertices in a
graph whose edges encode pairwise similarities be-
tween samples. Various learning algorithms can be
applied to assign labels to the test samples while en-
suring that the classification output varies smoothly
119
along the manifold defined by the graph. GBL has
been successfully applied to a range of problems in
computer vision, computational biology, and natu-
ral language processing. However, in most cases,
the learning tasks consisted of unstructured classi-
fication, where the input was represented by fixed-
length feature vectors and the output was one of a
finite set of discrete labels. In machine translation,
by contrast, both inputs and outputs consist of word
strings of variable length, and the number of possi-
ble outputs is not fixed and practically unlimited.
In this paper we propose a new graph-based learn-
ing algorithm with structured inputs and outputs to
improve consistency in phrase-based statistical ma-
chine translation. We define a joint similarity graph
over training and test data and use an iterative label
propagation procedure to regress a scoring function
over the graph. The resulting scores for unlabeled
samples (translation hypotheses) are then combined
with standard model scores in a log-linear transla-
tion model for the purpose of reranking. Our con-
tributions are twofold. First, from a machine trans-
lation perspective, we design and evaluate a global
consistency model enforcing that similar inputs re-
ceive similar translations. Second, from a machine
learning perspective, we apply graph-based learning
to a task with structured inputs and outputs, which
is a novel contribution in itself since previous ap-
plications of GBL have focused on predicting cat-
egorical labels. We evaluate our approach on two
machine translation tasks, the IWSLT 2007 Italian-
to-English and Arabic-to-English tasks, and demon-
strate significant improvements over the baseline.
2 Graph-Based Learning
GBL algorithms rely on a similarity graph consisting
of a set of nodes representing data samples xi (where
i ranges over 1, . . . , l labeled points and l+1, . . . , n
unlabeled points), and a set of weighted edges en-
coding pairwise similarities between samples. The
graph is characterized by a weight matrix W whose
elements Wij ? 0 are the similarity values for edges
between vertices xi and xj , and by its label vector
Y = (y1, . . . yl), yi ? {1, . . . , C} that defines la-
bels for the first l points. If there is no edge linking
nodes xi and xj , then Wij = 0. There is consider-
able freedom in choosing the weights. The similar-
ity measure used to compute the edge weights de-
termines the graph structure and is the most impor-
tant factor in successfully applying GBL. In most
applications of GBL, data samples are represented
by fixed-length feature vectors, and cosine similar-
ity or Euclidean distance-based measures are used
for edge weights.
Learning algorithms on similarity graphs include
e.g. min-cut (Blum and Chawla, 2001), spectral
graph transducer (Joachims, 2003), random walk-
based approaches (Szummer and Jaakkola, 2001),
and label propagation (Zhu and Ghahramani, 2002).
The algorithm proposed herein is based on the latter.
2.1 Label Propagation
Given a graph defined by a weight matrix W and
a label set Y , the basic label propagation algorithm
proceeds as follows:
1. Initialize the matrix P as Pij = Wij?WiiP
j Wij?Wii
2. Initialize a n? C matrix f with binary vectors
encoding the known labels for the first l rows:
fi = ?C(yi) ?i ? {1, 2, . . . , l}, where ?C(yi) is
the Kronecker vector of length C with 1 in po-
sition yi and 0 elsewhere. The remaining rows
of f can be zero.
3. f ? ? P ? f
4. Clamp already-labeled data rows: f ?i = ?C(yi)
?i ? {1, 2, . . . , l}
5. If f ? ?= f , stop.
6. f ? f ?
7. Repeat from step 3.
After convergence, f contains the solution in rows
l + 1 to n in the form of unnormalized label proba-
bility distributions. Hard labels can be obtained by
y?i = argmax
j?{1,...,C}
fij ?i ? {l + 1, . . . , n} (1)
The algorithm minimizes the following cost func-
tion (Zhu, 2005):
S =
C?
k=1
?
i>l ? j>l
Wij(fik ? fjk)2 (2)
S measures the smoothness of the learned function,
i.e., the extent to which the labeling allows large-
weight edges to link nodes of different labels. By
minimizing S , label propagation finds a labeling
120
that, to the extent possible, assigns similar soft labels
(identical hard labels) to nodes linked by edges with
large weights (i.e., highly similar samples). The
labeling decision takes into account not only sim-
ilarities between labeled and unlabeled nodes (as
in nearest-neighbor approaches) but also similarities
among unlabeled nodes. Label propagation has been
used successfully for various classification tasks,
e.g. image classification and handwriting recogni-
tion (Zhu, 2005). In natural language processing, la-
bel propagation has been used for document classifi-
cation (Zhu, 2005), word sense disambiguation (Niu
et al, 2005; Alexandrescu and Kirchhoff, 2007), and
sentiment categorization (Goldberg and Zhu, 2006).
3 Graph-Based Learning for Machine
Translation
Our goal is to exploit graph-based learning for im-
proving consistency in statistical phrase-based ma-
chine translation. Intuitively, a set of similar source
sentences should receive similar target-language
translations. This means that similarities between
training and test sentences should be taken into ac-
count, but also similarities between different test
sentences, which is a source of information currently
not exploited by standard SMT systems. To this
end we define a graph over the training and test sets
with edges between test and training sentences as
well as between different test sentences. In cases
where a test sentence does not have any connections
to training sentences but is connected to other test
sentences, helpful information about preferred trans-
lations can be propagated via these edges.
As mentioned above, the problem of machine
translation does not neatly fit into the standard
GBL framework. Given that our samples consist
of variable-length word strings instead of feature
vectors, the standard cosine or Euclidean-distance
based similarity measures cannot be used mean-
ingfully, and the number of possible ?labels??
correct translations?is unbounded and practically
very large. We thus need to modify both the graph
construction and the label propagation algorithms.
First, we handle the problem of unlimited out-
puts by applying GBL to rescoring only. In most
SMT systems, an N -best list (generated by a first de-
coding pass) approximates the search space of good
hypotheses reasonably well, provided N is large
enough. For all hypotheses of all sentences in the
test set (set we denote with H), the system learns a
ranking function r : H ? [0, 1]. Larger values of r
indicate better hypotheses. The corresponding loss
functional is
L(r) =?
i,j
Wij [r(xi)? r(xj)]2 (3)
L(r) measures the smoothness of r over the graph
by penalizing highly similar clusters of nodes that
have a high variance of r (in other words, simi-
lar input sentences that have very different transla-
tions). The smaller L(r), the ?smoother? r is over
the graph. Thus, instead of directly learning a clas-
sification function, we learn a regression function?
similar to (Goldberg and Zhu, 2006)?that is then
used for ranking the hypotheses.
3.1 Graph Construction
Each graph node represents a sentence pair (consist-
ing of source and target strings), and edge weights
represent the combined similarity scores computed
from comparing both the source sides and target
sides of a pair of nodes. Given a training set
with l source and target language sentence pairs
(s1, t1), . . . , (sl, tl) and a test set with l + 1, ..., n
source sentences, sl+1, . . . , sn, the construction of
the similarity graph proceeds as follows:
1. For each test sentence si, i = l + 1, . . . , n,
find a set Straini of similar training source
sentences and a set Stesti of similar test sen-
tences (excluding si and sentences identical to
it) by applying a string similarity function ? to
the source sides only and retaining sentences
whose similarity exceeds a threshold ?. Dif-
ferent ??s can be used for training vs. test sen-
tences; we use the same ? for both sets.
2. For each hypothesis hsi generated for si by a
baseline system, compute its similarity to the
target sides of all sentences in Straini . The
overall similarity is then defined by the com-
bined score
?ij = ? (?(si, sj), ?(hsi , tj)
) (4)
where i = l + 1, . . . n, j = 1, . . . , |Straini | and
? : R+ ? R+ ? R+ is an averaging function.
121
If ?ij > 0, establish graph nodes for hsi and tj
and link them with an edge of weight ?ij .
3. For each hypothesis hsi and each hypothe-
sis generated for each of the sentences sk ?
?testi , compute similarity on the target side and
use the combined similarity score as the edge
weight between nodes for hsi and hsk .
4. Finally,for each node xt representing a train-
ing sentence, assign r(xt) = 1 and also de-
fine its synthetic counterpart: a vertex x?t with
r(x?t) = 0. For each edge incident to xt of
weight Wth, define a corresponding edge of
weight 1?Wt?h.
The synthetic nodes and edges need to be added
to prevent the label propagation algorithm from con-
verging to the trivial solution that assigns r = 1 to
all points in the graph. This choice is theoretically
motivated?a similarity graph for regression should
have not only ?sources? (good nodes with high value
of r) but also ?sinks? (counterparts for the sources).
Figure 1 illustrates the connections of a test node.
Similarity Measure The similarity measure used
for comparing source and target sides is of prime
importance, as it determines the structure of the
graph. This has consequences for both computa-
tional efficiency (denser graphs require more com-
putation and memory) and the accuracy of the out-
come. A low similarity threshold results in a rich
graph with a large number of edges but possibly in-
troduces noise. A higher threshold leads to a small
graph emphasizing highly similar samples but with
too many disconnected components. The similarity
measure is also the means by which domain knowl-
edge can be incorporated into the graph construc-
tion process. Similarity may be defined at the level
of surface word strings, but may also include lin-
guistic information such as morphological features,
part-of-speech tags, or syntactic structures. Here,
we compare two similarity measures: the famil-
iar BLEU score (Papineni et al, 2002) and a score
based on string kernels. In using BLEU we treat
each sentence as a complete document. BLEU is not
symmetric?when comparing two sentences, differ-
ent results are obtained depending on which one is
considered the reference and which one is the hy-
pothesis. For computing similarities between train
and test translations, we use the train translation as
the reference. For computing similarity between two
test hypotheses, we compute BLEU in both direc-
tions and take the average. We note that more ap-
propriate distance measures are certainly possible.
Many previous studies, such as (Callison-Burch et
al., 2006), have pointed out drawbacks of BLEU,
and any other similarity measure could be utilized
instead. In particular, similarity measures that model
aspects of sentences that are ill handled by standard
phrase-based decoders (such as syntactic structure
or semantic information) could be useful here.
A more general way of computing similarity be-
tween strings is provided by string kernels (Lodhi et
al., 2002; Rousu and Shawe-Taylor, 2005), which
have been extensively used in bioinformatics and
email spam detection. String kernels map strings
into a feature space defined by all possible sub-
strings of the string up a fixed length k, and com-
puting the dot product between the resulting feature
vectors. Several variants of basic string kernels ex-
ist, notably those allowing gaps or mismatches, and
efficient implementations have been devised even
for large scale applications. Formally, we define a
sentence s as a concatenation of symbols from a fi-
nite alphabet ? (the vocabulary of the language) and
an embedding function from strings to feature vec-
tors, ? : ?? ? H. A kernel function K(s, t) com-
putes the distance between the resulting vectors for
two sentences s and t. In our case, the embedding
function is defined as
?ku(s) :=
?
i:u=s(i)
?|i| u ? ?k (5)
where k is the maximum length of substrings, |i| is
the length of i, and ? is a penalty parameter for each
gap encountered in the substring. K is defined as
K(s, t) =?
u
??u(s), ?u(t)?wu (6)
where w is a weight dependent on the length of the
substring u. Finally, the kernel score is normalized
by
?
K(s, s) ? K(t, t) to discourage long sentences
from being favored. Thus, our similarity measure is
a gapped, normalized string kernel, which is a more
general measure than BLEU in that is considers non-
contiguous substrings. We use a dynamic program-
ming implementation of string kernels (Rousu and
Shawe-Taylor, 2005).
122
For the combination of source-side and target-
side similarity scores (the function we denoted as ?)
we test two simple schemes, using either the ge-
ometric or the arithmetic mean of the individual
scores. In the first case, large edge weights only re-
sult when both source and target are close to each
other; the latter may produce high edge weights
when only one of them (typically the source score)
is high. More sophisticated combination schemes,
using e.g. weighted combination, could be used but
were not investigated in this study.
Scalability Poor scalability is often mentioned as
a drawback of graph-based learning. Straightfor-
ward implementations of GBL algorithms often rep-
resent the joint training and test data in working
memory and therefore do not scale well to large
data sets. However, we have developed several tech-
niques to improve scalability without impeding ac-
curacy. First, we construct separate graphs for each
test sentence without losing global connectivity in-
formation. The graph for a test sentence is com-
puted as the transitive closure of the edge set E over
the nodes containing all hypotheses for that test sen-
tence. This smaller graph does not affect the out-
come of the learning process for the chosen sentence
because in label propagation the learned value r(xi)
can be influenced by that of another node xj if and
only if xj is reachable from xi. In the worst the-
oretical case, the transitive closure could compre-
hend the entire graph, but in practice the edge set is
never that dense and can be easily pruned based on
the heuristic that faraway nodes connected through
low-weight edges have less influence on the result.
We use a simple embodiment of this heuristic in a
work-list approach: starting from the nodes of inter-
est (hypotheses for the focal sentence), we expand
the closure starting with the direct neighbors, which
have the largest influence; then add their neighbors,
which have less influence, and so forth. A thresh-
old on the number of added vertices limits undue
expansion while capturing either the entire closure
or a good approximation of it. Another practical
computational advantage of portioning work is that
graphs for different hypothesis sets can be trivially
created and used in parallel, whereas distributing
large matrix-vector multiplication is much more dif-
ficult (Choi, 1998). The disadvantage is that overall
1 0
1 0
. . . . . .
W2h
W1h 1?W1h1?W2h
Figure 1: Connections for hypothesis node xh. Similar-
ity edges with weights Wth link the node with train sen-
tences xt, for which r(xt) = 1. For each of these edges
we define a dissimilarity edge of weight 1?Wth, linking
the node with node x?t for which r(x?t) = 0. The vertex is
also connected to other test vertices (the dotted edges).
redundant computations are being made: incomplete
estimates of r are computed for the ancillary nodes
in the transitive closure and then discarded.
Second, we obtain a reduction in graph size of or-
ders of magnitude by collapsing all training vertices
of the same r that are connected to the same test
vertex into one and sum the edge weights. This is
equivalent to the full graph for learning purposes.
3.2 Propagation
Label propagation proceeds as follows:
1. Compute the transitive closure over the edges
starting from all hypothesis nodes of a given
sentence.
2. On the resulting graph, collapse all test-train
similarities for each test node by summing edge
weights. Obtain accumulated similarities in
row and column 1 of the similarity matrix W .
3. Normalize test-to-train weights such that?
j W1j =
?
j Wj1 = 1.
4. Initialize the matrix P as Pij = Wij1?Wi1+Pj Wij .
(The quantity 1?W1i in the denominator is the
weight of the dissimilarity edge.)
5. Initialize a column vector f of height n with
f1 = 1 (corresponding to node x1) and 0 in the
remaining positions.
6. f ? ? P ? f
7. Clamp f ?1: f ?1 = 1
8. If f ? ?= f , continue with step 11.
9. f ? f ?
10. Repeat from step 6.
11. The result r is in the slots of f that correspond
to the hypotheses of interest. Normalize per
sentence if needed, and rank in decreasing or-
der of r.
123
Convergence Our algorithm?s convergence proof
is similar to that for standard label propagation (Zhu,
2005, p. 6). We split P as follows:
P =
[ 0 PLU
PUL PUU
]
(7)
where PUL is a column vector holding global simi-
larities of test hypotheses with train sentences, PLU
is a horizontal vector holding the same similarities
(though PLU 6= P TUL due to normalization), and
PUU holds the normalized similarities between pairs
of test hypotheses. We also separate f :
f =
[ 1
fU
]
(8)
where we distinguish the first entry because it repre-
sents the training part of the data. With these nota-
tions, the iteration formula becomes:
f ?U = PUUfU + PUL (9)
Unrolling the iteration yields:
fU = limn??
[
(PUU )nf0U +
( n?
i=1
(PUU )i?1
)
PUL
]
It can be easily shown that the first term converges
to zero because of normalization in step 4 (Zhu,
2005). The sum in the second term converges to
(I ? PUU )?1, so the unique fixed point is:
fU = (I ? PUU )?1PUL (10)
Our system uses the iterative form. On the data sets
used, convergence took 61.07 steps on average.
At the end of the label propagation algorithm, nor-
malized scores are obtained for each N-best list (sen-
tences without any connections whatsoever are as-
signed zero scores). These are then used together
with the other component models in log-linear com-
bination. Combination weights are optimized on a
held-out data set.
4 Data and System
We evaluate our approach on the IWSLT 2007
Italian-to-English (IE) and Arabic-to-English (AE)
travel tasks. The first is a challenge task, where the
training set consists of read sentences but the de-
velopment and test data consist of spontaneous di-
alogues. The second is a standard travel expres-
sion translation task consisting entirely of read in-
put. For our experiments we chose the text input
(correct transcription) condition only. The data set
sizes are shown in Table 1. We split the IE develop-
ment set into two subsets of 500 and 496 sentences
each. The first set (dev-1) is used to train the system
parameters of the baseline system and as a training
set for GBL. The second is used to tune the GBL pa-
rameters. For each language pair, the baseline sys-
tem was trained with additional out-of-domain text
data: the Italian-English Europarl corpus (Koehn,
2005) in the case of the IE system, and 5.5M words
of newswire data (LDC Arabic Newswire, Multiple-
Translation Corpus and ISI automatically extracted
parallel data) in the case of the AE system.
Set # sent pairs # words # refs
IE train 26.5K 160K 1
IE dev-1 500 4308 1
IE dev-2 496 4204 1
IE eval 724 6481 4
AE train 23K 160K 1
AE dev4 489 5392 7
AE dev5 500 5981 7
AE eval 489 2893 6
Table 1: Data set sizes and reference translations count.
Our baseline is a standard phrase-based SMT
system based on a log-linear model with the fol-
lowing feature functions: two phrase-based trans-
lation scores, two lexical translation scores, word
count and phrase count penalty, distortion score,
and language model score. We use the Moses de-
coder (Koehn et al, 2007) with a reordering limit of
4 for both languages, which generates N -best lists
of up to 2000 hypotheses per sentence in a first pass.
The second pass uses a part-of-speech (POS) based
trigram model, trained on POS sequences generated
by a MaxEnt tagger (Ratnaparkhi, 1996). The lan-
guage models are trained on the English side using
SRILM (Stolcke, 2002) and modified Kneser-Ney
discounting for the first-pass models and Witten-
Bell discounting for the POS models. The baseline
system yields state-of-the-art performance.
124
Weighting dev-2 eval
none (baseline) 22.3/53.3 29.6/45.5
(a) 23.4/51.5 30.7/44.1
(b) 23.5/51.6 30.6/44.3
(c) 23.2/51.8 30.0/44.6
Table 2: GBL results (%BLEU/PER) on IE task
for different weightings of labeled-labeled vs. labeled-
unlabeled graph edges (BLEU-based similarity measure).
5 Experiments and Results
We started with the IE system and initially inves-
tigated the effect of only including edges between
labeled and unlabeled samples in the graph. This
is equivalent to using a weighted k-nearest neighbor
reranker that, for each hypothesis, computes average
similarity with its neighborhood of labeled points,
and uses the resulting score for reranking.
Starting with the IE task and the BLEU-based
similarity metric, we ran optimization experiments
that varied the similarity threshold and compared
sum vs. product combination of source and target
similarity scores, settling for ? = 0.7 and prod-
uct combination. We experimented with three dif-
ferent ways of weighting the contributions from
labeled-unlabeled vs. unlabeled-unlabeled edges:
(a) no weighting, (b) labeled-to-unlabeled edges
were weighted 4 times stronger than unlabeled-
unlabeled ones; and (c) labeled-to-unlabeled edges
were weighted 2 times stronger. The weighting
schemes do not lead to significantly different results.
The best result obtained shows a gain of 1.2 BLEU
points on the dev set and 1 point on the eval set, re-
flecting PER gains of 2% and 1.2%, respectively.
We next tested the string kernel based similarity
measure. The parameter values were 0.5 for the gap
penalty, a maximum substring length of k = 4, and
weights of 0, 0.1, 0.2, 0.7. These values were chosen
heuristically and were not tuned extensively due to
time constraints. Results (Table 3) show significant
improvements in PER and BLEU.
In the context of the BTEC challenge task it is
interesting to compare this approach to adding the
development set directly to the training set. Part of
the improvements may be due to utilizing kNN in-
formation from a data set that is matched to the test
System dev-2 eval
Baseline 22.3/53.3 29.6/45.5
GBL 24.3/51.0 32.2/42.7
Table 3: GBL results (%BLEU/PER) on IE tasks with
string-kernel based similarity measure.
set in terms of style. If this data were also used for
training the initial phrase table, the improvements
might disappear. We first optimized the log-linear
model combination weights on the entire dev07 set
(dev-1 and dev-2 in Table 1) before retraining the
phrase table using the combined train and dev07
data. The new baseline performance (shown in Ta-
ble 4) is much better than before, due to the im-
proved training data. We then added GBL to this
system by keeping the model combination weights
trained for the previous system, using the N-best
lists generated by the new system, and using the
combined train+dev07 set as a train set for select-
ing similar sentences. We used the GBL parameters
that yielded the best performance in the experiments
described above. As can be seen from Table 4, GBL
again yields an improvement of up to 1.2% absolute
in both BLEU and PER.
System BLEU (%) PER
Baseline 37.9 38.4
GBL 39.2 37.2
Table 4: Effect of GBL on IE system trained with
matched data (eval set).
For the AE task we used ? = 0.5; however, this
threshold was not tuned extensively. Results using
BLEU similarity are shown in Table 5. The best
result on the eval set yields an improvement of 1.2
BLEU points though only 0.2% reduction in PER.
Overall, results seem to vary with parameter settings
and nature of the test set (e.g. on dev5, used as a test
set, not for optimization, a surprisingly larger im-
provement in BLEU of 2.7 points is obtained!).
Overall, sentence similarities were observed to be
lower for this task. One reason may be that the AE
system includes statistical tokenization of the source
side, which is itself error-prone in that it can split the
same word in different ways depending on the con-
125
Method dev4 dev5 eval
Baseline 30.2/43.5 21.9/48.4 37.8/41.8
GBL 30.3/42.5 24.6/48.1 39.0/41.6
Table 5: AE results (%BLEU/PER, ? = 0.5)
text. Since our similarity measure is word-based,
this may cause similar sentences to fall below the
threshold. The string kernel does not yield any im-
provement over the BLEU-based similarity measure
on this task. One possible improvement would be to
use an extended string kernel that can take morpho-
logical similarity into account.
Example Below we give an actual example of a
translation improvement, showing the source sen-
tence, the 1-best hypotheses of the baseline system
and GBL system, respectively, the references, and
the translations of similar sentences in the graph
neighborhood of the current sentence.
Source: Al+ mE*rp Aymknk {ltqAT Swrp lnA
Baseline: i?m sorry could picture for us
GBL: excuse me could you take a picture of the us
Refs:
excuse me can you take a picture of us
excuse me could you take a photo of us
pardon would you mind taking a photo of us
pardon me could you take our picture
pardon me would you take a picture of us
excuse me could you take a picture of u
Similar sentences:
could you get two tickets for us
please take a picture for me
could you please take a picture of us
6 Related Work
GBL is an instance of semi-supervised learning,
specifically transductive learning. A different form
of semi-supervised learning (self-training) has been
applied to MT by (Ueffing et al, 2007). Ours is
the first study to explore a graph-based learning ap-
proach. In the machine learning community, work
on applying GBL to structured outputs is beginning
to emerge. Transductive graph-based regularization
has been applied to large-margin learning on struc-
tured data (Altun et al, 2005). However, scalability
quickly becomes a problem with these approaches;
we solve that issue by working on transitive closures
as opposed to entire graphs. String kernel represen-
tations have been used in MT (Szedmak, 2007) in
a kernel regression based framework, which, how-
ever, was an entirely supervised framework. Finally,
our approach can be likened to a probabilistic imple-
mentation of translation memories (Maruyana and
Watanabe, 1992; Veale and Way, 1997). Translation
memories are (usually commercial) databases of
segment translations extracted from a large database
of translation examples. They are typically used by
human translators to retrieve translation candidates
for subsequences of a new input text. Matches can
be exact or fuzzy; the latter is similar to the iden-
tification of graph neighborhoods in our approach.
However, our GBL scheme propagates similarity
scores not just from known to unknown sentences
but also indirectly, via connections through other un-
known sentences. The combination of a translation
memory and statistical translation was reported in
(Marcu, 2001); however, this is a combination of
word-based and phrase-based translation predating
the current phrase-based approach to SMT.
7 Conclusion
We have presented a graph-based learning scheme
to implement a consistency model for SMT that
encourages similar inputs to receive similar out-
puts. Evaluation on two small-scale translation tasks
showed significant improvements of up to 2.6 points
in BLEU and 2.8% PER. Future work will include
testing different graph construction schemes, in par-
ticular better parameter optimization approaches and
better string similarity measures. More gains can
be expected when using better domain knowledge
in constructing the string kernels. This may include
e.g. similarity measures that accommodate POS tags
or morphological features, or comparisons of the
syntax trees of parsed sentence. The latter could be
quite easily incorporated into a string kernel or the
related tree kernel similarity measure. Additionally,
we will investigate the effectiveness of this approach
on larger translation tasks.
Acknowledgments This work was funded by
NSF grant IIS-032676 and DARPA under Contract
No. HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the views of these agencies.
126
References
A. Alexandrescu and K. Kirchhoff. 2007. Data-Driven
Graph Construction for Semi-Supervised Graph-
Based Learning in NLP. In HLT.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Proceedings of NIPS 18.
A. Blum and S. Chawla. 2001. Learning from labeled
and unlabeled data using graph mincuts. Proc. 18th
International Conf. on Machine Learning, pages 19?
26.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In Proceedings of EACL.
Jaeyoung Choi. 1998. A new parallel matrix multi-
plication algorithm on distributed-memory concurrent
computers. Concurrency: Practice and Experience,
10(8):655?670.
A. Goldberg and J. Zhu. 2006. Seeing stars when
there aren?t many stars: Graph-based semi-supervised
learning for sentiment categorization. In HLT-NAACL
Workshop on Graph-based Algorithms for Natural
Language Processing.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Machine Translation
Summit X, pages 79?86, Phuket, Thailand.
H. Lodhi, J. Shawe-taylor, and N. Cristianini. 2002. Text
classification using string kernels. In Proceedings of
NIPS.
D. Marcu. 2001. Towards a unified approach to memory-
and statistical-based machine translation. In Proceed-
ings of ACL.
H. Maruyana and H. Watanabe. 1992. Tree cover search
algorithm for example-based translation. In Proceed-
ings of TMI, pages 173?184.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005.
Word sense disambiguation using label propagation
based semi-supervised learning method. In Proceed-
ings of ACL, pages 395?402.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proc.of (EMNLP).
J. Rousu and J. Shawe-Taylor. 2005. Efficient computa-
tion of gap-weighted string kernels on large alphabets.
Journal of Machine Learning Research, 6:1323?1344.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In ICSLP, pages 901?904.
Zhuoran Wang;John Shawe-Taylor;Sandor Szedmak.
2007. Kernel regression based machine translation. In
Proceedings of NAACL/HLT, pages 185?188. Associ-
ation for Computational Linguistics.
Martin Szummer and Tommi Jaakkola. 2001. Partially
labeled classification with markov random walks. In
Advances in Neural Information Processing Systems,
volume 14. http://ai.mit.edu/people/
szummer/.
N. Ueffing, G. Haffari, and A. Sarkar. 2007. Trans-
ductive learning for statistical machine translation. In
Proceedings of the ACL Workshop on Statistical Ma-
chine Translation.
T. Veale and A. Way. 1997. Gaijin: a template-
based bootstrapping approach to example-based ma-
chine translation. In Proceedings of News Methods in
Natural Language Processing.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU-CALD-02.
Xiaojin Zhu. 2005. Semi-Supervised Learning with
Graphs. Ph.D. thesis, Carnegie Mellon University.
CMU-LTI-05-192.
127
