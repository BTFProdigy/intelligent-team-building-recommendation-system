Proceedings of the ACL-HLT 2011 System Demonstrations, pages 32?37,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
MemeTube: A Sentiment-based Audiovisual System  
for Analyzing and Displaying Microblog Messages 
Cheng-Te Li1   Chien-Yuan Wang2   Chien-Lin Tseng2   Shou-De Lin1,2 
1 Graduate Institute of Networking and Multimedia 
2 Department of Computer Science and Information Engineering 
National Taiwan University, Taipei, Taiwan 
{d98944005, sdlin}@csie.ntu.edu.tw   {gagedark, moonspirit.wcy}@gmail.com 
Abstract 
Micro-blogging services provide platforms 
for users to share their feelings and ideas 
on the move. In this paper, we present a 
search-based demonstration system, called 
MemeTube, to summarize the sentiments 
of microblog messages in an audiovisual 
manner. MemeTube provides three main 
functions: (1) recognizing the sentiments of 
messages (2) generating music melody au-
tomatically based on detected sentiments, 
and (3) produce an animation of real-time 
piano playing for audiovisual display. Our 
MemeTube system can be accessed via: 
http://mslab.csie.ntu.edu.tw/memetube/ . 
1    Introduction 
Micro-blogging services such as Twitter1, Plurk2, 
and Jaiku3, are platforms that allow users to share 
immediate but short messages with friends. Gener-
ally, the micro-blogging services possess some 
signature properties that differentiate them from 
conventional weblogs and forum. First, microblogs 
deal with almost real-time messaging, including 
instant information, expression of feelings, and 
immediate ideas. It also provides a source of crowd 
intelligence that can be used to investigate com-
mon feelings or potential trends about certain news 
or concepts. However, this real-time property can 
lead to the production of an enormous number of 
messages that recipients must digest. Second, mi-
cro-blogging is time-traceable. The temporal in-
formation is crucial because contextual posts that 
appear close together are, to some extent, correlat-
ed. Third, the style of micro-blogging posts tends 
to be conversation-based with a sequence of re-
                                                          
1 http://www.twitter.com 
2 http://www.plurk.com 
3 http://www.jaiku.com/ 
sponses. This phenomenon indicates that the posts 
and their responses are highly correlated in many 
respects. Fourth, micro-blogging is friendship-
influenced. Posts from a particular user can also be 
viewed by his/her friends and might have an im-
pact on them (e.g. the empathy effect) implicitly or 
explicitly. Therefore, posts from friends in the 
same period may be correlated sentiment-wise as 
well as content-wise. 
We leverage the above properties to develop an 
automatic and intuitive Web application, Me-
meTube, to analyze and display the sentiments be-
hind messages in microblogs. Our system can be 
regarded as a sentiment-driven, music-based sum-
marization framework as well as a novel audiovis-
ual presentation of art. MemeTube is designed as a 
search-based tool. The system flow is as shown in 
Figure 1. Given a query (either a keyword or a user 
id), the system first extracts a series of relevant 
posts and replies based on keyword matching. 
Then sentiment analysis is applied to determine the 
sentiment of the posts. Next a piece of music is 
composed to reflect the detected sentiments. Final-
ly, the messages and music are fed into the anima-
tion generation model, which displays a piano 
keyboard that plays automatically. 
 
  
Figure 1: The system flow of our MemeTube. 
 
The contributions of this work can be viewed 
from three different perspectives. 
? From system perspective of view, we demo a 
novel Web-based system, MemeTube, as a kind 
of search-based sentiment presentation, musi-
32
calization, and visualization tool for microblog 
messages. It can serve as a real-time sentiment 
detector or an interactive microblog audio-
visual presentation system. 
? Technically, we integrate a language-model-
based classifier approach with a Markov-
transition model to exploit three kinds of in-
formation (i.e., contextual, response, and 
friendship information) for sentiment recogni-
tion. We also integrate the sentiment-detection 
system with a real-time rule-based harmonic 
music and animation generator to display 
streams of messages in an audiovisual format. 
? Conceptually, our system demonstrates that, 
instead of simply using textual tags to express 
sentiments, it is possible to exploit audio (i.e., 
music) and visual (i.e., animation) cues to pre-
sent microblog users? feelings and experiences. 
In this respect, the system can also serve as a 
Web-based art piece that uses NLP-
technologies to concretize and portray senti-
ments. 
2    Related Works 
Related works can be divided into two parts: sen-
timent classification in microblogs, and sentiment-
based audiovisual presentation for social media. 
For the first part, most of related literatures focus 
on exploiting different classification methods to 
separate positive and negative sentiments by a va-
riety of textual and linguistics features, as shown in 
Table 1. Their accuracy ranges from 60%~85% 
depending on different setups. The major differ-
ence between our work and existing approaches is 
that our model considers three kinds of additional 
information (i.e., contextual, response and friend-
ship information) for sentiment recognition.  
In recent years, a number of studies have inves-
tigated integrating emotions and music in certain 
media applications. For example, Ishizuka and 
Onisawa (2006) generated variations of theme mu-
sic to fit the impressions of story scenes represent-
ed by textual content or pictures. Kaminskas (2009) 
aligned music with user-selected points of interests 
for recommendation. Li and Shan (2007) produced 
painting slideshows with musical accompaniment. 
Hua et al (2004) proposed a Photo2Video system 
that allows users to specify incident music that ex-
presses their feelings about the photos. To the best 
of our knowledge, MemeTube is the first attempt 
to exploit AI techniques to create harmonic audio-
visual experiences and interactive emotion-based 
summarization for microblogs. 
 
Table 1: Summary of related works that 
detect sentiments in microblogs. 
3    Sentiment Analysis of Microblog Posts 
First, we develop a classification model as our 
basic sentiment recognition mechanism. Given a 
training corpus of posts and responses annotated 
with sentiment labels, we train an n-gram language 
model for each sentiment. Then, we use such mod-
el to calculate the probability that a post expresses 
the sentiment s associated with that model: 
????|??
? ?????,? ,??|?????????????????,? ,????, ??,
?
???
 
where w is the sequence of words in the post. We 
also use the common Laplace smoothing method. 
For each post p and each sentiment s?S, our 
classifier calculates the probability that such post 
expresses the sentiment ????|?? using Bayes rule: 
????|?? ? ?????????|?? 
????? is estimated directly by counting, while 
????|?? can be derived by using the learned lan-
 Features Methods 
Pak and Paroubek 
2010 
statistic counting of 
adjectives Naive Bayes 
Chen et al 2008 POS tags, emoticons SVM 
Lewin and Pribu-
la 2009 smileys, keywords 
Maximum Entropy, 
SVM 
Riley 2009 
n-grams, smileys, 
hashtags, replies, 
URLs, usernames, 
emoticons 
Naive Bayes,  
Prasad 2010 n-grams Na?ve Bayes 
Go et al 2009 
usernames, sequential 
patterns of keywords, 
POS tags, n-grams 
Naive Bayes, Max-
imum Entropy, 
SVM 
Li et al 2009 
several dictionaries 
about different kinds of 
keywords 
Keyword Matching
Barbosa and Feng 
2010 
retweets, hashtag, re-
plies, URLs, emoticons, 
upper cases 
SVM 
Sun et al 2010 keyword counting and Chinese dictionaries Naive Bayes, SVM
Davidov et al 
2010 
n-grams, word patterns, 
punctuation information k-Nearest Neighbor
Bermingham and 
Smeaton 2010 n-grams and POS tags 
Binary Classifica-
tion 
33
guage models. This allow us to produce a distribu-
tion of sentiments for a given post p, denoted as ??. 
However, the major challenge in the microblog 
sentiment detection task is that the length of each 
post is limited (i.e., posts on Twitter are limited to 
140 characters). Consequently, there might not be 
enough information for a sentiment detection sys-
tem to exploit. To solve this problem, we propose 
to utilize the three types of information mentioned 
earlier. We discuss each type in detail below. 
3.1   Response Factor 
We believe the sentiment of a post is highly corre-
lated with (but not necessary similar to) that of re-
sponses to the post. For example, an angry post 
usually triggers angry responses, but a sad post 
usually solicits supportive responses. We propose 
to learn the correlation patterns of sentiments from 
the data and use them to improve the recognition. 
To achieve such goal, from the data, we learn 
the probability ???????????????|??????????????????, which represents the conditional probability of a 
post given responses. Then we use such probability 
to construct a transition matrix ?? , where ????  =??????????????? ? ?	|	????????????????? ? ??.  With ??, we can generate the adjusted sentiment dis-tribution of the post ??? as: 
??? ? ? ?
? ????????????
? ? ?1 ? ????	, where ??  denotes the original sentiment distribu-
tion of the post, and ???  is the sentiment distribu-
tion of the ???  response determined by the 
abovementioned language model approach. In ad-
dition, ??? ? 1 ??????????? ? ???????  represents the weight of the response since it is preferable to as-
sign higher weights to closer responses. There is 
also a global parameter ? that determines how 
much the system should trust the information de-
rived from the responses to the post. If there is no 
response to a post, we simply assign ??? ? ??. 
3.2    Context Factor 
It is assumed that the sentiment of a microblog 
post is correlated with the author?s previous posts 
(i.e., the ?context? of the post). We also assume 
that, for each person, there is a sentiment transition 
matrix ??  that represents how his/her sentiments 
change over time. The ??, ???? element in ?? repre-sents the conditional probability from the senti-
ment of the previous post to that of the current post: 
???????????? ??? ? ?	|	?????????? ????? ? ??. 
The diagonal elements stand for the consistency 
of the emotion state of a person. Conceivably, a 
capricious person?s diagnostic ????  values will be lower than those of a calm person. The matrix ?? can be learned directly from the annotated data. 
Let ?? represent the detected sentiment distribu-tion of an existing post at time t. We want to adjust 
??  based on the previous posts from ? ? ??  to ? , where ?? is a given temporal threshold. The sys-
tem first extracts a set of posts from the same au-
thor posted from time ? ? ?? to ? and determines 
their sentiment distributions ????, ???, ? , ???? , where ? ? ?? ? ??, ??, ? , ?? ? ?  using the same classifier. Then, the system utilizes the following 
update equation to obtain an adjusted sentiment 
distribution ???: 
??? ? ? ?
? ????????????
? ? ?1 ? ????	, 
where ??? ? 1/?? ? ???. The parameters ???, ?, ? are defined similar to the previous case. If there is 
no post in the defined interval, the system will 
leave ?? unchanged. 
3.3    Friendship Factor 
We also assume that the friends? emotions are cor-
related with each other. This is because friends 
affect each other, and they are more likely to be in 
the same circumstances, and thus enjoy/suffer sim-
ilarly. Our hypothesis is that the sentiment of a 
post and the sentiments of the author?s friends? 
recent posts might be correlated. Therefore, we can 
treat the friends? recent posts in the same way as 
the recent posts of the author, and learn the transi-
tion matrix?? , where ???? ? ???????????????? ??? ?
?	|	???????????????	??????? ????? ? ??, and apply the tech-
nique proposed in the previous section to improve 
the recognition accuracy. 
However, it is not necessarily true that all 
friends have similar emotional patterns. One?s sen-
timent transition matrix ?? might be very different from that of the other, so we need to be careful 
when using such information to adjust our recogni-
tion outcomes. We propose to only consider posts 
from friends with similar emotional patterns. 
To achieve our goal, we first learn every user?s 
contextual sentiment transition matrix ?? from the data. In ??, each row represents a distribution that sums to one; therefore, we can compare two ma-
trixes ???  and ???  by averaging the symmetric KL-divergence of each row. That is, 
34
?????????????,???
? ??????????? ?????????, ??, ??????, ???. Two persons are considered as having similar 
emotion pattern if their contextual sentiment transi-
tion matrixes are similar. After a set of similar 
friends are identified, their recent posts (i.e., from 
? ? ??  to ? ) are treated in the same way as the 
posts by the author, and we use the method pro-
posed previously to fine-tune the recogni-
tion outcomes. 
4    Music Generation 
For each microblog post retrieved according to the 
query, we can derive its sentiment distribution (as 
a vector of probabilities) by using the above meth-
od. Next, the system transforms every sentiment 
distribution into an affective vector comprised of a 
valence value and an arousal value. The valence 
value represents the positive-to-negative sentiment, 
while the arousal value represents the intense-to-
silent level. 
We exploit the mapping from each type of sen-
timent to a two-dimensional affective vector based 
on the two-dimensional emotion model of Russell 
(1980). Using the model we extract the affective 
score vectors of the six emotions (see Table 2) 
used in our experiments. The mapping enables us 
to transform a sentiment distribution ??  into an 
affective score vector by weighted sum approach. 
For example, given a distribution of (Anger=20%, 
Surprise=20%,Disgust=10%, Fear=10%, Joy=10%, 
Sadness=30%), the two-dimensional affective vec-
tor can be computed as 0.2*(-0.25, 1) + 0.2*(0.5, 
0.75) + 0.1*(-0.75, -0.5) + 0.1*(-0.75, 0.5) + 
0.1*(1, 0.25) + 0.3*(-1, -0.25). Finally, the affec-
tive vector of each post will be summed to repre-
sent the sentiment of the given query in terms of 
the valence and arousal values. 
 
Table 2: Affective score vector for each sentiment label. 
Sentiment Label Affective Score Vector 
Anger (-0.25, 1) 
Surprise (0.5, 0.75) 
Disgust (-0.75, -0.5) 
Fear (-0.75, 0.5) 
Joy (1, 0.25) 
Sadness (-1, -0.25) 
 
Next the system transforms the affective vector 
into music elements through chord set selection 
(based on the valence value) and rhythm determi-
nation (based on the arousal value). For chord set 
selection, we design nine basic chord sets as {A, 
Am, Bm, C, D, Dm, Em, F, G}, where each chord 
set consists of some basic notes. The chord sets are 
used to compose twenty chord sequences. Half of 
the chord sequences are used for weakly positive to 
strongly positive sentiments and the other half are 
used for weakly negative to strongly negative sen-
timents. The valence value is therefore divided into 
twenty levels, and gradually shifts from strongly 
positive to strongly negative. The chord sets ensure 
that the resulting auditory presentation is in har-
mony (Hewitt 2008). For rhythm determination, 
we divide the arousal values into five levels to de-
cide the tempo/speed of the music. Higher arousal 
values generate music with a faster tempo while 
lower ones lead to slow and easy-listening music. 
 
 Figure 2: A snapshot of the proposed MemeTube. 
 
   Figure 3: The animation with automatic piano playing. 
5    Animation Generation 
In this final stage, our system produces real-time 
animation for visualization. The streams of mes-
sages are designed to flow as if they were playing a 
piece of a piano melody. We associate each mes-
sage with a note in the generated music. When a 
post message flows from right to left and touches a 
piano key, the key itself blinks once and the corre-
sponding tone of the key is produced. The message 
flow and the chord/rhythm have to be synchro-
nized so that it looks as if the messages themselves 
are playing the piano. The system also allows users 
to highlight the body of a message by moving the 
35
cursor over the flowing message. A snapshot is 
shown in Figure 2 and the sequential snapshots of 
the animation are shown in Figure 3. 
6    Evaluations on Sentiment Detection 
We collect the posts and responses from every ef-
fective user, users with more than 10 messages, of 
Plurk from January 31st to May 23rd, 2009. In order 
to create the diversity for the music generation sys-
tem, we decide to use six different sentiments, as 
shown in Table 2, rather than using only three sen-
timent types, positive, negative and neutral, as 
most of the systems in Table 1 have used. The sen-
timent of each sentence is labeled automatically 
using the emoticons. This is similar to what many 
people have proposed for evaluation (Davidov et al 
2010; Sun et al 2010; Bifet and Frank 2010; Go et 
al. 2009; Pak and Paroubek 2010; Chen et al 
2010). We use data from January 31st to April 30th 
as training set, May 1st to 23rd as testing data. For 
the purpose of observing the result of using the 
three factors, we filter the users without friends, 
the posts without responses, and the posts without 
previous post in 24 hour in testing data. We also 
manually label the sentiments on the testing data 
(totally 1200 posts, 200 posts for each sentiment). 
We use three metrics to evaluate our model: ac-
curacy, Root-Mean-Square Error for valence (de-
noted by RMSE(V)) and RMSE for arousal 
(denoted by RMSE(A)). The RMSE values are 
generated by comparing the affective vector of the 
predicted sentiment distribution with the affective 
vector of the answer. Our basic model reaches 
33.8% in accuracy, 0.78 in the RMSE(V) and 0.64 
in RMSE(A). Note that RMSE?0.5 means that 
there is roughly one quarter (25%) error in the va-
lence/arousal values as they range from [-1,1].  
Note that the main reason the accuracy is not ex-
tremely high is that we are dealing with 6 classes. 
When we combine angry, disgust, fear, and sad-
ness into one negative sense and the rest as posi-
tive senses, our system reaches 78.7% in accuracy, 
which is competitive to the state-of-the-art algo-
rithms as shown in the related work section. How-
ever, doing such generalization will lose the 
flexibility of producing more fruitful and diverse 
pieces of music. Therefore we choose more fine-
grained classes for our experiment. 
Figure 3 shows the results of exploiting the re-
sponse, context, and friendship. Note RMSE?0.5 
means that there is roughly one quarter (25%) error 
in the valence/arousal values as they range from [-
1,1]. The results show that considering all three 
additional factors can achieve the best results and 
decent improvement over the basic LM model. 
 
Table 3: The results after adding addition info 
(note that for RMSE, the lower value the better) 
 LM Response Context Friend Combine
Accuracy 33.8% 34.7% 34.8% 35.1% 36.5% 
RMSE(V) 0.784 0.683 0.684 0.703 0.679 
RMSE(A) 0.640 0.522 0.516 0.538 0.514 
7    System Demo 
We create video clips of five different queries for 
demonstration, which is downloadable from:  
http://mslab.csie.ntu.edu.tw/memetube/demo/. This 
demo page contains the resulting clips of four 
keyword queries (including football, volcano, 
Monday, big bang) and a user id query mstcgeek. 
Here we briefly describe each case. (1) The video 
for query term, football, was recorded on February 
7th 2011, results in a relatively positive and ex-
tremely intense atmosphere. It is reasonable be-
cause the NFL Super Bowl was played on 
February 6th, 2011. The valence value is not as 
high as the arousal value because some fans might 
not be very happy to see their favorite team losing 
the game. (2) The query, volcano, was also record-
ed on February 7th 2011. The resulting video ex-
presses negative valence and neutral arousal. After 
checking the posts, we have learned that it is be-
cause the Japanese volcano Mount Asama has con-
tinued to erupt. Some users are worried and 
discussed about the potential follow-up disasters. 
(3) The query Monday was performed on February 
6th 2011, which is a Sunday night. The negative 
valence reflects the ?blue Monday? phenomenon, 
which leads to some heavy, less smooth melody. (4) 
The term big bang turns out to be very positive on 
both valence and arousal, mainly because, besides 
its relatively neutral meaning in physics, this term 
also refers to a famous comic show that some peo-
ple in Plurk love to watch. We also use one user id 
as query: the user-id mstcgeek is the official ac-
count of Microsoft Taiwan. This user often uses 
cheery texts to share some videos about their prod-
ucts or provide some discounts of their product, 
which leads to relatively hyped music. 
36
8    Conclusion 
Microblog, as a daily journey and social network-
ing service, generally captures the dynamics of the 
change of feelings over time of the authors and 
their friends. In MemeTube, the affective vector is 
generated by aggregating the sentiment distribution 
of each post; thus, it represents the majority?s opin-
ion (or sentiment) about a topic. In this sense, our 
system can be regarded as providing users with an 
audiovisual experience to learn collective opinion 
of a particular topic. It also shows how NLP tech-
niques can be integrated with knowledge about 
music and visualization to create a piece of inter-
esting network art work. Note that MemeTube can 
be regarded as a flexible framework as well since 
each component can be further refined inde-
pendently. Therefore, our future works are three-
fold: For sentiment analysis, we will consider more 
sophisticated ways to improve the baseline accura-
cy and to aggregate individual posts into a collec-
tive consensus. For music generation, we plan to 
add more instruments and exploit learning ap-
proaches to improve the selection of chords. For 
visualization, we plan to add more interactions be-
tween music, sentiments, and users. 
Acknowledgements  
This work was supported by National Science Council, Na-
tional Taiwan University and Intel Corporation under Grants 
NSC99-2911-I-002-001, 99R70600, and 10R80800. 
References  
Barbosa, L., and Feng, J. 2010. Robust Sentiment Detec-
tion on Twitter from Biased and Noisy Data. In Pro-
ceedings of International Conference on Computational 
Linguistics (COLING?10), 36?44. 
Bermingham, A., and Smeaton, A. F. 2010. Classifying 
Sentiment in Microblogs: is Brevity an Advantage? In 
Proceedings of ACM International Conference on In-
formation and Knowledge Management (CIKM?10), 
1183?1186. 
Chen, M. Y.; Lin, H. N.; Shih, C. A.; Hsu, Y. C.; Hsu, P. 
Y.; and Hewitt, M. 2008. Music Theory for Computer 
Musicians. Delmar. 
Hsieh, S. K. 2010. Classifying Mood in Plurks. In Proceed-
ings of Conference on Computational Linguistics and 
Speech Processing (ROCLING 2010), 172?183. 
Davidov, D.; Tsur, O.; and Rappoport, A. 2010. Enhanced 
Sentiment Learning Using Twitter Hashtags and Smi-
leys. In Proceedings of International Conference on 
Computational Linguistics (COLING?10), 241?249. 
Go, A.; Bhayani, R.; and Huang, L. 2009. Twitter Senti-
ment Classification using Distant Supervision. Technical 
Report, Stanford University. 
Hua, X. S.; Lu, L.; and Zhang, H. J. 2004. Photo2Video - 
A System for Automatically Converting Photographic 
Series into Video. In Proceedings of ACM International 
Conference on Multimedia (MM?04), 708?715. 
Ishizuka, K., and Onisawa, T. 2006. Generation of Varia-
tions on Theme Music Based on Impressions of Story 
Scenes. In Proceedings of ACM International Confer-
ence on Game Research and Development, 129?136. 
Kaminskas, M. 2009. Matching Information Content with 
Music. In Proceedings of ACM International Confer-
ence on Recommendation System (RecSys?09), 405?
408. 
Lewin, J. S., and Pribula, A. 2009. Extracting Emotion 
from Twitter. Technical Report, Stanford University. 
Li, C. T., and Shan, M. K. 2007. Emotion-based Impres-
sionism Slideshow with Automatic Music Accompani-
ment. In Proceedings of ACM International Conference 
on Multimedia (MM?07), 839?842. 
Li, S.; Zheng, L.; Ren, X.; and Cheng, X. 2009. Emotion 
Mining Research on Micro-blog. In Proceedings of 
IEEE Symposium on Web Society, 71?75. 
Pak, A., and Paroubek, P. 2010. Twitter Based System: 
Using Twitter for Disambiguating Sentiment Ambigu-
ous Adjectives. In Proceedings of International Work-
shop on Semantic Evaluation, (ACL?10), 436?439. 
Pak, A., and Paroubek, P. 2010. Twitter as a Corpus for 
Sentiment Analysis and Opinion Mining. In Proceedings 
of International Conference on Language Resources and 
Evaluation (LREC?10), 1320?1326. 
Prasad, S. 2010. Micro-blogging Sentiment Analysis Using 
Bayesian Classification Methods. Technical Report, 
Stanford University. 
Riley, C. 2009. Emotional Classification of Twitter Mes-
sages. Technical Report, UC Berkeley. 
Russell, J. A. 1980. Circumplex Model of Affect. Journal 
of Personality and Social Psychology, 39(6):1161?1178. 
Strapparava, C., and Valitutti, A. 2004. Wordnet-affect: an 
Affective extension of wordnet. In Proceedings of Inter-
national Conference on Language Resources and Evalu-
ation, 1083?1086. 
Sun, Y. T.; Chen, C. L.; Liu, C. C.; Liu, C. L.; and Soo, V. 
W. 2010. Sentiment Classification of Short Chinese 
Sentences. In Proceedings of Conference on Computa-
tional Linguistics and Speech Processing 
(ROCLING?10), 184?198. 
Yang, C.; Lin, K. H. Y.; and Chen, H. H. 2007. Emotion 
Classification Using Web Blog Corpora. In Proceedings 
of IEEE/WIC/ACM International Conference on Web 
Intelligence (WI?07), 275?278. 
37
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 133?138,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
IMASS: An Intelligent Microblog Analysis and Summarization System 
 
Jui-Yu Weng Cheng-Lun Yang Bo-Nian Chen Yen-Kai Wang Shou-De Lin 
Department of Computer Science and Information Engineering 
National Taiwan University 
{r98922060,r99944042,f92025,b97081,sdlin}@csie.ntu.edu.tw 
 
Abstract 
This paper presents a system to summarize 
a Microblog post and its responses with the 
goal to provide readers a more constructive 
and concise set of information for efficient 
digestion. We introduce a novel two-phase 
summarization scheme. In the first phase, 
the post plus its responses are classified in-
to four categories based on the intention, 
interrogation, sharing, discussion and chat. 
For each type of post, in the second phase, 
we exploit different strategies, including 
opinion analysis, response pair identifica-
tion, and response relevancy detection, to 
summarize and highlight critical informa-
tion to display. This system provides an al-
ternative thinking about machine-
summarization: by utilizing AI approaches, 
computers are capable of constructing dee-
per and more user-friendly abstraction. 
1 Introduction 
As Microblog services such as Twitter have be-
come increasingly popular, it is critical to re-
consider the applicability of the existing NLP 
technologies on this new media sources. Take 
summarization for example, a Microblog user 
usually has to browse through tens or even hun-
dreds of posts together with their responses daily, 
therefore it can be beneficial if there is an intelli-
gent tool assisting summarizing those information.  
Automatic text summarization (ATS) has been 
investigated for over fifty years, but the majority of 
the existing techniques might not be appropriate 
for Microblog write-ups. For instance, a popular 
kind of approaches for summarization tries to iden-
tify a subset of information, usually in sentence 
form, from longer pieces of writings as summary 
(Das and Martins, 2007). Such extraction-based 
methods can hardly be applied to Microblog texts 
because many posts/responses contain only one 
sentence.  
Below we first describe some special characte-
ristics that deviates the Microblog summarization 
task from general text summarization.  
a. The number of sentences is limited, and sen-
tences are usually too short and casual to con-
tain sufficient structural information or cue 
phrases. Unlike normal blogs, there is a strict 
limitation on the number of characters for each 
post (e.g. 140 characters for Twitter and Plurk 
maximum). Microblog messages cannot be 
treated as complete documents so that we can-
not take advantage of the structural information. 
Furthermore, users tend to regard Microblog as 
a chatting board. They write casually with 
slangs, jargons, and incorrect grammar.  
b. Microblog posts can serve several different 
purposes. At least three different types of posts 
are observed in Microblogs, expressing feeling, 
sharing information, and asking questions. 
Structured language is not the only means to 
achieve those goals. For example, people 
sometimes use attachment, as links or files, for 
sharing, and utilize emoticons and pre-defined 
qualifiers to express their feelings.  The diver-
sity of content differ Microblogs from general 
news articles. Consequently, using one mold to 
fit all types of Microblog posts is not sufficient. 
Different summarization schemes for posts 
with different purposes are preferred.  
c. Posts and responses in Microblogs are more 
similar to a multi-persons dialogue corpus. One 
of the main purposes of a Microblog is to serve 
as the fast but not instant communication 
channel among multiple users. Due to the free-
chatting, multi-user characteristics, the topic of 
a post/response thread can drift quickly. Some-
times, the topic of discussion at the end of the 
thread is totally unrelated to that of the post. 
133
This paper introduces a framework that summariz-
es a post with its responses. Motivated by the ab-
ovementioned characteristics of Microblogs, we 
plan to use a two-phase summarization scheme to 
develop different summarization strategies for dif-
ferent type of posts (see Figure 1). In the first 
phase, a post will be automatically classified into 
several categories including interrogation, discus-
sion, sharing and chat based on the intention of the 
users. In the second phase, the system chooses dif-
ferent summarization components for different 
types of posts.  
The novelties of this system are listed below. 
1. Strategically, we propose an underlying 2-phase 
framework for summarizing Microblog posts. 
The system can be accessed online at 
http://mslab.csie.ntu.edu.tw/~fishyz/plurk/. 
2. Tactically, we argue that it is possible to inte-
grate post-intention classification, opinion anal-
ysis, response relevancy and response-pair 
mining to create an intelligent summarization 
framework for Microblog posts and responses. 
We also found that the content features are not 
as useful as the temporal or positional features 
for text mining in Microblog. 
3. Our work provides an alternative thinking about 
ATS. It is possible to go beyond the literal 
meaning of summarization to exploit advanced 
text mining methods to improve the quality and 
usability of a summarization system. 
2 Summarization Framework and Expe-
riments 
Below we discuss our two-phase summarization 
framework and the experiment results on each in-
dividual component. Note that our experiments 
were tested on the Plurk dataset, which is one of 
the most popular micro-blogging platforms in Asia. 
Our observation is that Microblog posts can 
have different purposes. We divide them into four 
categories, Interrogation, Sharing, Discussion, and 
Chat.  
The Interrogation posts are questions asked in 
public with the hope to obtain some useful answers 
from friends or other users. However, it is very 
common that some repliers do not provide mea-
ningful answers. The responses might serve the 
purpose for clarification or, even worse, have noth-
ing to do with the question.  Hence we believe the 
most appropriate summarization process for this 
kind of posts is to find out which replies really re-
spond to the question. We created a response re-
levance detection component to serve as its 
summarization mechanism. 
The Sharing posts are very frequently observed 
in Microblog as Microbloggers like to share inter-
esting websites, pictures, and videos with their 
friends. Other people usually write down their 
comments or feelings on the shared subjects in the 
responses. To summarize such posts, we obtain the 
statistics on how many people have positive, neu-
tral, and negative attitude toward the shared sub-
jects. We introduce the opinion analysis 
component that provides the analysis on whether 
the information shared is recommended by the res-
pondents. 
We also observe that some posts contain charac-
teristics of both Interrogation and Sharing. The 
users may share a hyperlink and ask for others? 
opinions at the same time. We create a category 
named Discussion for these posts, and apply both 
response ranking and opinion analysis engines on 
this type of posts.  
Finally, there are posts which simply act as the 
solicitation for further chat. For example, one user 
writes ?so sad?? and another replies ?what hap-
pened?. We name this type of posts/responses as 
Chat. This kind of posts can sometimes involve 
multiple persons and the topic may gradually drift 
to a different one. We believe the plausible sum-
marization strategy is to group different messages 
based on their topics. Therefore for Chat posts, we 
designed a response pair identification system to 
accomplish such goal. We group the related res-
ponses together for display, and the number of 
groups represents the number of different topics in 
this thread.  
Figure 1 shows the flow of our summarization 
 
Figure 1. System architecture  
134
framework. When an input post with responses 
comes in, the system first determines its intention, 
based on which the system adopts proper strategies 
for summarization. Below we discuss the technical 
parts of each sub-system with experiment results. 
2.1 Post Intention Classification 
This stage aims to classify each post into four cat-
egories, Interrogation, Sharing, Discussion, and 
Chat. One tricky issue is that the Discussion label 
is essentially a combination of interrogation and 
sharing labels. Therefore, simply treating it as an 
independent label and use a typical multi-label 
learning method can hurt the performance. We ob-
tain 76.7% (10-fold cross validation) in accuracy 
by training a four-class classifier using the 6-gram 
character language model. To improve the perfor-
mance, we design a decision-tree based framework 
that utilizes both manually-designed rules and dis-
criminant classification engine (see Figure 2). The 
system first checks whether the posts contains 
URLs or pointers to files, then uses a binary clas-
sifier to determine whether the post is interrogative.  
For the experiment, we manually annotate 6000 
posts consisting of 1840 interrogation, 2002 shar-
ing, 1905 chat, and 254 discussion posts. We train 
a 6-gram language model as the binary interroga-
tion classifier. Then we integrate the classifier into 
our system and test on 6000 posts to obtain a test-
ing accuracy of 82.8%, which is significantly bet-
ter than 76.7% with multi-class classification. 
2.2 Opinion Analysis 
Opinion analysis is used to evaluate public prefe-
rence on the shared subject. The system classifies 
responses into 3 categories, positive, negative, and 
neutral. 
Here we design a two-level classification 
framework using Na?ve-Bayes classifiers which 
takes advantage of the learned 6-gram language 
model probabilities as features. First of all, we 
train a binary classifier to determine if a post or a 
reply is opinionative. This step is called the subjec-
tivity test. If the answer is yes, we then use another 
binary classifier to decide if the opinion is positive 
or negative. The second step is called the polarity 
test.  
For subjectivity test, we manually annotate 3244 
posts, in which half is subjective and half is objec-
tive. The 10-fold cross validation shows average 
accuracy of 70.5%.  
For polarity test, we exploit the built-in emoti-
cons in Plurk to automatically extract posts with 
positive and negative opinions. We collect 10,000 
positive and 10,000 negative posts as training data 
to train a language model of Na?ve Bayes classifier, 
and evaluate on manually annotated data of 3121 
posts, with 1624 positive and 1497 negative to ob-
tain accuracy of 0.722. 
2.3 Response Pair Identification 
Conversation in micro-blogs tends to diverge into 
multiple topics as the number of responses grows. 
Sometimes such divergence may result in res-
ponses that are irrelevant to the original post, thus 
creating problems for summarization. Furthermore, 
because the messages are usually short, it is diffi-
cult to identify the main topics of these dialogue-
like responses using only keywords in the content 
for summarization. Alternatively, we introduce a 
subcomponent to identify Response Pairs in micro-
blogs. A Response Pair is a pair of responses that 
the latter specifically responds to the former. Based 
on those pairs we can then form clusters of mes-
sages to indicate different group of topics and mes-
 
Figure 2. The post classification procedure 
Feature Description Weight 
Backward Refe-
rencing  
Latter response content 
contains former respond-
er?s display name 
0.055 
Forward Refe-
rencing of user 
name 
Former response contains 
latter response?s author?s 
user name 
0.018 
Response position 
difference 
Number of responses in 
between responses 
0.13 
Content similarity Contents? cosine similari-
ty using n-gram models. 
0.025 
Response time 
difference 
Time difference between 
responses in seconds 
0.012 
Table 1. Feature set with their description and weights 
 
135
sages. 
Looking at the content of micro-blogs, we ob-
serve that related responses are usually adjacent to 
each other as users tend to closely follow whether 
their messages are responded and reply to the res-
ponses from others quickly. Therefore besides con-
tent features, we decide to add the temporal and 
ordering features (See Table 1) to train a classifier 
that takes a pair of messages as inputs and return 
whether they are related. By identifying the re-
sponse pairs, our summarization system is able to 
group the responses into different topic clusters 
and display the clusters separately. We believe 
such functionality can assist users to digest long 
Microblog discussions. 
For experiment, the model is trained using 
LIBSVM (Chang and Lin, 2001) (RBF kernel) 
with 6000 response pairs, half of the training set 
positive and half negative. The positive data can be 
obtained automatically based on Plurk?s built in 
annotation feature. Responses with @user_name 
string in the content are matched with earlier res-
ponses by the author, user_name. Based on the 
learned weights of the features, we observe that 
content feature is not very useful in determining 
the response pairs. In a Microblog dialogue, res-
pondents usually do not repeat the question nor 
duplicate the keywords. We also have noticed that 
there is high correlation between the responses re-
latedness and the number of other responses be-
tween them. For example, users are less likely to 
respond to a response if there have been many rep-
lies about this response already.  Statistical analy-
sis on positive training data shows that the average 
number of responses between related responses is 
2.3.  
We train the classifier using 6000 automatically-
extracted pairs of both positive and negative in-
stances. We manually annotated 1600 pairs of data 
for testing. The experiment result reaches 80.52% 
accuracy in identifying response pairs. The base-
line model which uses only content similarity fea-
ture reaches only 45% in accuracy.   
2.4 Response Relevance Detection 
For interrogative posts, we think the best summary 
is to find out the relevent responses as potential 
answers.  
 We introduce a response relevancy detection 
component for the problem. Similar to previous 
components, we exploit a supervised learning ap-
proach and the features? weights, learned by 
LIBSVM with RBF kernel, are shown in Table 2. 
Temporal and Positional Features 
A common assertion is that the earlier responses 
have higher probability to be the answers of the 
question. Based on the learned weights, it is not 
surprising that most important feature is the posi-
tion of the response in the response hierarchy. 
Another interesting finding by our system is that 
meaningful replies do not come right away. Res-
ponses posted within ten seconds are usually for 
chatting/clarification or ads from robots. 
Content Features 
We use the length of the message, the cosine simi-
larity of the post and the responses, and the occur-
rence of the interrogative words in response 
sentences as content features.  
Because the interrogation posts in Plurk are rela-
tively few, we manually find a total of 382 positive 
and 403 negative pairs for training and use 10-fold 
cross validation for evaluation. 
We implement the component using LIBSVM 
(RBF Kernel) classifier.  The baseline is to always 
select the first response as the only relevant answer. 
The results show that the accuracy of baseline 
reaches 67.4%, far beyond that of our system 
73.5%.  
3 System Demonstration 
In this section, we show some snapshots of our 
summarization system with real examples using 
Plurk dataset. Our demo system is designed as a 
Feature Weight 
Response position 0.170 
Response time difference 0.008 
Response length 0.003 
Occurrence of interrogative 
words 
0.023 
Content similarity 0.023 
Table 2. Feature set and their weights 
 
Figure 3. The IMASS interface 
136
search engine (see Figure 3). Given a query term, 
our system first returns several posts containing the 
query string under the search bar. When one of the 
posts is selected, it will generate a summary ac-
cording to the detected intention and show it in a 
pop-up frame. We have recorded a video demon-
strating our system. The video can be viewed at   
http://imss-acl11-demo.co.cc/. 
For interrogative posts, we perform the response 
relevancy detection. The summary contains the 
question and relevant answers. Figure 4 is an ex-
ample of summary of an interrogative post. We can 
see that responses other than the first and the last 
responses are filtered because they are less relevant 
to the question. 
 For sharing posts, the summary consists of two 
parts. A pie chart that states the percentage of each 
opinion group is displayed. Then the system picks 
three responses from the majority group or one re-
sponse from each group if there is no significant 
difference. Figure 5 is an example that most 
friends of the user dfrag give positive feedback to 
the shared video link. 
For discussion posts, we combine the response 
relevancy detection subsystem and the opinion 
analysis sub-system for summarization. The former 
first eliminates the responses that are not likely to 
be the answer of the post. The latter then generates 
a summary for the post and relevant responses. The 
result is similar to sharing posts. 
For chat posts, we apply the response pair iden-
tification component to generate the summary. In 
the example, Figure 6, the original Plurk post is 
about one topic while the responses diverge to one 
or more unrelated topics. Our system clearly sepa-
rates the responses into multiple groups. This re-
presentation helps the users to quickly catch up 
with the discussion flow. The users no longer have 
to read interleaving responses from different topics 
and guess which topic group a response is referring 
to.  
 
 
Figure 4. An example of interrogative post. 
 
Figure 6. An Example of chat post 
 
 
 
 
 
Figure 5. An example of sharing post. 
 
137
4 Related Work 
We have not seen many researches focusing on the 
issues of Microblog summarization. We found on-
ly one work that discusses about the issues of 
summarization for Microblogs (Sharifi et al, 2010). 
Their goal, however, is very different from ours as 
they try to summarize multiple posts and do not 
consider the responses. They propose the Phrase 
Reinforcement Algorithm to find the most com-
monly used phrase that encompasses the topic 
phrase, and use these phrases to compose the 
summary. They are essentially trying to solve a 
multi-document summarization problem while our 
problem is more similar to short dialog summariza-
tion because the dialogue nature of Microblogs is 
one of the most challenging part that we tried to 
overcome.  
In dialogue summarization, many researchers 
have pointed out the importance of detecting re-
sponse pairs in a conversation. Zechner (2001) per-
forms an in depth analysis and evaluation in the 
area of open domain spoken dialogue summariza-
tion. He uses decision tree classifier with lexical 
features like POS tags to identify questions and 
applies heuristic rules like maximum distance be-
tween speakers to extract answers. Shrestha and 
McKeown (2004) propose a supervised learning 
method to detect question-answer pairs in Email 
conversations.  Zhou and Hovy (2005) concen-
trates on summarizing dialogue-style technical in-
ternet relay chats using supervised learning 
methods. Zhou further clusters chat logs into sev-
eral topics and then extract some essential response 
pairs to form summaries. Liu et al (2006) propose 
to identify question paragraph via analyzing each 
participant?s status, and then use cosine measure to 
select answer paragraphs for online news dataset.  
The major differences between our components 
and the systems proposed by others lie in the selec-
tion of features. Due to the intrinsic difference be-
tween the writing styles of Microblog and other 
online sources, our experiments show that the con-
tent feature is not as useful as the position and 
temporal features. 
5 Conclusion 
In terms of length and writing styles, Microblogs 
possess very different characteristics than other 
online information sources such as web blogs and 
news articles. It is therefore not surprising that dif-
ferent strategies are needed to process Microblog 
messages. Our system uses an effective strategy to 
summarize the post/response by first determine the 
intention and then perform different analysis de-
pending on the post types. Conceptually, this work 
intends to convey an alternative thinking about 
machine-summarization. By utilizing text mining 
and analysis techniques, computers are capable of 
providing more intelligent summarization than in-
formation condensation. 
Acknowledgements  
This work was supported by National Science 
Council, National Taiwan University and Intel 
Corporation under Grants NSC99-2911-I-002-001, 
99R70600, and 10R80800. 
References  
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM : 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Dipanjan Das and Andr? F.T. Martins. 2007.  A Survey 
on Automatic Text Summarization. Literature Survey 
for the Language and Statistics II Course. CMU.  
Chuanhan Liu, Yongcheng Wang, and Fei Zheng. 2006. 
Automatic Text Summarization for Dialogue Style. 
In Proceedings of the IEEE International Conference 
on Information Acquisition. 274-278 
Beaux Sharifi, Mark A. Hutton, and Jugal Kalita. 2010. 
Summarizing Microblogs Automatically. In Proceed-
ings of the Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT). 685-688 
Lokesh Shrestha and Kathleen McKeown. 2004. Detec-
tion of Question-Answer Pairs in Email Conversa-
tions. In Proceedings of the 23rd International 
Conference on Computational Linguistics (COLING 
2010).  
Klaus Zechner. 2001. Automatic Generation of Concise 
Summaries of Spoken Dialogues in Unrestricted 
Domains. In Proceedings of the 24th ACM-SIGIR 
International Conference on Research and Develop-
ment in Information Retrieval. 199-207. 
Liang Zhou and Eduard Hovy. 2005. Digesting virtual 
geek culture: The summarization of technical internet 
relay chats, in Proceedings of  the 43rd Annual Meet-
ing of the Association for Computational Linguistics 
(ACL 2005). 298-305. 
138
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 344?348,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Latent Information to Predict Diffusions of Novel Topics on 
Social Networks 
Tsung-Ting Kuo1*, San-Chuan Hung1, Wei-Shih Lin1, Nanyun Peng1, Shou-De Lin1, 
Wei-Fen Lin2 
1Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan 
2MobiApps Corporation, Taiwan 
*d97944007@csie.ntu.edu.tw 
 
Abstract 
This paper brings a marriage of two seemly 
unrelated topics, natural language 
processing (NLP) and social network 
analysis (SNA). We propose a new task in 
SNA which is to predict the diffusion of a 
new topic, and design a learning-based 
framework to solve this problem. We 
exploit the latent semantic information 
among users, topics, and social connections 
as features for prediction. Our framework is 
evaluated on real data collected from public 
domain. The experiments show 16% AUC 
improvement over baseline methods. The 
source code and dataset are available at 
http://www.csie.ntu.edu.tw/~d97944007/dif
fusion/ 
1 Background 
The diffusion of information on social networks 
has been studied for decades. Generally, the 
proposed strategies can be categorized into two 
categories, model-driven and data-driven. The 
model-driven strategies, such as independent 
cascade model (Kempe et al, 2003), rely on 
certain manually crafted, usually intuitive, models 
to fit the diffusion data without using diffusion 
history. The data-driven strategies usually utilize 
learning-based approaches to predict the future 
propagation given historical records of prediction 
(Fei et al, 2011; Galuba et al, 2010; Petrovic et al, 
2011).  Data-driven strategies usually perform 
better than model-driven approaches because the 
past diffusion behavior is used during learning 
(Galuba et al, 2010). 
Recently, researchers started to exploit content 
information in data-driven diffusion models (Fei et 
al., 2011; Petrovic et al, 2011; Zhu et al, 2011). 
However, most of the data-driven approaches 
assume that in order to train a model and predict 
the future diffusion of a topic, it is required to 
obtain historical records about how this topic has 
propagated in a social network (Petrovic et al, 
2011; Zhu et al, 2011). We argue that such 
assumption does not always hold in the real-world 
scenario, and being able to forecast the propagation 
of novel or unseen topics is more valuable in 
practice. For example, a company would like to 
know which users are more likely to be the source 
of ?viva voce? of a newly released product for 
advertising purpose. A political party might want 
to estimate the potential degree of responses of a 
half-baked policy before deciding to bring it up to 
public. To achieve such goal, it is required to 
predict the future propagation behavior of a topic 
even before any actual diffusion happens on this 
topic (i.e., no historical propagation data of this 
topic are available). Lin et al also propose an idea 
aiming at predicting the inference of implicit 
diffusions for novel topics (Lin et al, 2011). The 
main difference between their work and ours is that 
they focus on implicit diffusions, whose data are 
usually not available. Consequently, they need to 
rely on a model-driven approach instead of a data-
driven approach. On the other hand, our work 
focuses on the prediction of explicit diffusion 
behaviors. Despite the fact that no diffusion data of 
novel topics is available, we can still design a data-
driven approach taking advantage of some explicit 
diffusion data of known topics. Our experiments 
show that being able to utilize such information is 
critical for diffusion prediction. 
2 The Novel-Topic Diffusion Model 
We start by assuming an existing social network G 
= (V, E), where V is the set of nodes (or user) v, 
and E is the set of link e. The set of topics is 
344
denoted as T. Among them, some are considered as 
novel topics (denoted as N), while the rest (R) are 
used as the training records.  We are also given a 
set of diffusion records D = {d | d = (src, dest, t)}, 
where src is the source node (or diffusion source), 
dest is the destination node, and t is the topic of the 
diffusion that belongs to R but not N. We assume 
that diffusions cannot occur between nodes without 
direct social connection; any diffusion pair implies 
the existence of a link e = (src, dest ?)  E. Finally, 
we assume there are sets of keywords or tags that 
relevant to each topic (including existing and novel 
topics). Note that the set of keywords for novel 
topics should be seen in that of existing topics. 
From these sets of keywords, we construct a topic-
word matrix TW = (P(wordj | topici))i,j of which the 
elements stand for the conditional probabilities that 
a word appears in the text of a certain topic. 
Similarly, we also construct a user-word matrix 
UW= (P(wordj | useri))i,j from these sets of 
keywords. Given the above information, the goal is 
to predict whether a given link is active (i.e., 
belongs to a diffusion link) for topics in N. 
2.1 The Framework 
The main challenge of this problem lays in that the 
past diffusion behaviors of new topics are missing. 
To address this challenge, we propose a supervised 
diffusion discovery framework that exploits the 
latent semantic information among users, topics, 
and their explicit / implicit interactions. Intuitively, 
four kinds of information are useful for prediction: 
? Topic information: Intuitively, knowing the 
signatures of a topic (e.g., is it about politics?) 
is critical to the success of the prediction. 
? User information: The information of a user 
such as the personality (e.g., whether this user 
is aggressive or passive) is generally useful. 
? User-topic interaction: Understanding the users' 
preference on certain topics can improve the 
quality of prediction. 
? Global information: We include some global 
features (e.g., topology info) of social network. 
Below we will describe how these four kinds of 
information can be modeled in our framework. 
2.2 Topic Information 
We extract hidden topic category information to 
model topic signature. In particular, we exploit the 
Latent Dirichlet Allocation (LDA) method (Blei et 
al., 2003), which is a widely used topic modeling 
technique, to decompose the topic-word matrix TW 
into hidden topic categories:  
                        TW = TH * HW 
, where TH is a topic-hidden matrix, HW is hidden-
word matrix, and h is the manually-chosen 
parameter to determine the size of hidden topic 
categories. TH indicates the distribution of each 
topic to hidden topic categories, and HW indicates 
the distribution of each lexical term to hidden topic 
categories. Note that TW and TH include both 
existing and novel topics.  We utilize THt,*, the row 
vector of the topic-hidden matrix TH for a topic t, 
as a feature set. In brief, we apply LDA to extract 
the topic-hidden vector THt,* to model topic 
signature (TG) for both existing and novel topics. 
Topic information can be further exploited. To 
predict whether a novel topic will be propagated 
through a link, we can first enumerate the existing 
topics that have been propagated through this link. 
For each such topic, we can calculate its similarity 
with the new topic based on the hidden vectors 
generated above (e.g., using cosine similarity 
between feature vectors). Then, we sum up the 
similarity values as a new feature: topic similarity 
(TS). For example, a link has previously 
propagated two topics for a total of three times 
{ACL, KDD, ACL}, and we would like to know 
whether a new topic, EMNLP, will propagate 
through this link. We can use the topic-hidden 
vector to generate the similarity values between 
EMNLP and the other topics (e.g., {0.6, 0.4, 0.6}), 
and then sum them up (1.6) as the value of TS. 
2.3 User Information 
Similar to topic information, we extract latent 
personal information to model user signature (the 
users are anonymized already). We apply LDA on 
the user-word matrix UW: 
UW = UM * MW 
, where UM is the user-hidden matrix, MW is the 
hidden-word matrix, and m is the manually-chosen 
size of hidden user categories. UM indicates the 
distribution of each user to the hidden user 
categories (e.g., age). We then use UMu,*, the row 
vector of UM for the user u, as a feature set. In 
brief, we apply LDA to extract the user-hidden 
vector UMu,* for both source and destination nodes 
of a link to model user signature (UG). 
345
2.4 User-Topic Interaction 
Modeling user-topic interaction turns out to be 
non-trivial. It is not useful to exploit latent 
semantic analysis directly on the user-topic matrix 
UR = UQ * QR , where UR represents how many 
times each user is diffused for existing topic R (R 
?
 T), because UR does not contain information of 
novel topics, and neither do UQ and QR. Given no 
propagation record about novel topics, we propose 
a method that allows us to still extract implicit 
user-topic information. First, we extract from the 
matrix TH (described in Section 2.2) a subset RH 
that contains only information about existing topics. 
Next we apply left division to derive another user-
hidden matrix UH: 
UH = (RH \ URT)T = ((RHT RH
 
)-1 RHT URT)T 
Using left division, we generate the UH matrix 
using existing topic information. Finally, we 
exploit UHu,*, the row vector of the user-hidden 
matrix UH for the user u, as a feature set. 
Note that novel topics were included in the 
process of learning the hidden topic categories on 
RH; therefore the features learned here do 
implicitly utilize some latent information of novel 
topics, which is not the case for UM. Experiments 
confirm the superiority of our approach. 
Furthermore, our approach ensures that the hidden 
categories in topic-hidden and user-hidden 
matrices are identical. Intuitively, our method 
directly models the user?s preference to topics? 
signature (e.g., how capable is this user to 
propagate topics in politics category?). In contrast, 
the UM mentioned in Section 2.3 represents the 
users? signature (e.g., aggressiveness) and has 
nothing to do with their opinions on a topic. In 
short, we obtain the user-hidden probability vector 
UHu,* as a feature set, which models user 
preferences to latent categories (UPLC). 
2.5 Global Features 
Given a candidate link, we can extract global 
social features such as in-degree (ID) and out-
degree (OD). We tried other features such as 
PageRank values but found them not useful. 
Moreover, we extract the number of distinct topics 
(NDT) for a link as a feature. The intuition behind 
this is that the more distinct topics a user has 
diffused to another, the more likely the diffusion 
will happen for novel topics. 
2.6 Complexity Analysis 
The complexity to produce each feature is as below: 
(1) Topic information: O(I * |T| * h * Bt) for LDA 
using Gibbs sampling, where I is # of the 
iterations in sampling, |T| is # of topics, and Bt 
is the average # of tokens in a topic. 
(2) User information: O(I * |V| * m * Bu) , where |V| is # of users, and Bu is the average # of 
tokens for a user. 
(3) User-topic interaction: the time complexity is 
O(h3 + h2 * |T| + h * |T| * |V|). 
(4) Global features: O(|D|), where |D| is # of 
diffusions. 
3 Experiments 
For evaluation, we try to use the diffusion records 
of old topics to predict whether a diffusion link 
exists between two nodes given a new topic.  
3.1 Dataset and Evaluation Metric 
We first identify 100 most popular topic (e.g., 
earthquake) from the Plurk micro-blog site 
between 01/2011 and 05/2011. Plurk is a popular 
micro-blog service in Asia with more than 5 
million users (Kuo et al, 2011). We manually 
separate the 100 topics into 7 groups. We use 
topic-wise 4-fold cross validation to evaluate our 
method, because there are only 100 available 
topics. For each group, we select 3/4 of the topics 
as training and 1/4 as validation. 
The positive diffusion records are generated 
based on the post-response behavior. That is, if a 
person x posts a message containing one of the 
selected topic t, and later there is a person y 
responding to this message, we consider a 
diffusion of t has occurred from x to y (i.e., (x, y, t) 
is a positive instance). Our dataset contains a total 
of 1,642,894 positive instances out of 100 distinct 
topics; the largest and smallest topic contains 
303,424 and 2,166 diffusions, respectively. Also, 
the same amount of negative instances for each 
topic (totally 1,642,894) is sampled for binary 
classification (similar to the setup in KDD Cup 
2011 Track 2). The negative links of a topic t are 
sampled randomly based on the absence of 
responses for that given topic. 
The underlying social network is created using 
the post-response behavior as well. We assume 
there is an acquaintance link between x and y if and 
346
only if x has responded to y (or vice versa) on at 
least one topic. Eventually we generated a social 
network of 163,034 nodes and 382,878 links. 
Furthermore, the sets of keywords for each topic 
are required to create the TW and UW matrices for 
latent topic analysis; we simply extract the content 
of posts and responses for each topic to create both 
matrices. We set the hidden category number h = m 
= 7, which is equal to the number of topic groups. 
We use area under ROC curve (AUC) to 
evaluate our proposed framework (Davis and 
Goadrich, 2006); we rank the testing instances 
based on their likelihood of being positive, and 
compare it with the ground truth to compute AUC. 
3.2 Implementation and Baseline 
After trying many classifiers and obtaining similar 
results for all of them, we report only results from 
LIBLINEAR with c=0.0001 (Fan et al, 2008) due 
to space limitation. We remove stop-words, use 
SCWS (Hightman, 2012) for tokenization, and  
MALLET (McCallum, 2002) and GibbsLDA++ 
(Phan and Nguyen, 2007) for LDA. 
There are three baseline models we compare the 
result with. First, we simply use the total number 
of existing diffusions among all topics between 
two nodes as the single feature for prediction. 
Second, we exploit the independent cascading 
model (Kempe et al, 2003), and utilize the 
normalized total number of diffusions as the 
propagation probability of each link. Third, we try 
the heat diffusion model (Ma et al, 2008), set 
initial heat proportional to out-degree, and tune the 
diffusion time parameter until the best results are 
obtained. Note that we did not compare with any 
data-driven approaches, as we have not identified 
one that can predict diffusion of novel topics.  
3.3 Results 
The result of each model is shown in Table 1. All 
except two features outperform the baseline. The 
best single feature is TS. Note that UPLC performs 
better than UG, which verifies our hypothesis that 
maintaining the same hidden features across 
different LDA models is better. We further conduct 
experiments to evaluate different combinations of 
features (Table 2), and found that the best one (TS 
+ ID + NDT) results in about 16% improvement 
over the baseline, and outperforms the combination 
of all features. As stated in (Witten et al, 2011), 
adding useless features may cause the performance 
of classifiers to deteriorate. Intuitively, TS captures 
both latent topic and historical diffusion 
information, while ID and NDT provide 
complementary social characteristics of users. 
 
Table 1: Single-feature results. 
 
Table 2: Feature combination results. 
4 Conclusions 
The main contributions of this paper are as below: 
1. We propose a novel task of predicting the 
diffusion of unseen topics, which has wide 
applications in real-world.  
2. Compared to the traditional model-driven or 
content-independent data-driven works on 
diffusion analysis, our solution demonstrates 
how one can bring together ideas from two 
different but promising areas, NLP and SNA, 
to solve a challenging problem. 
3. Promising experiment result (74% in AUC) 
not only demonstrates the usefulness of the 
proposed models, but also indicates that 
predicting diffusion of unseen topics without 
historical diffusion data is feasible. 
Acknowledgments 
This work was also supported by National Science 
Council, National Taiwan University and Intel 
Corporation under Grants NSC 100-2911-I-002-001, 
and 101R7501. 
Method Feature AUC
Baseline
Existing Diffusion 58.25%
Independent Cascade 51.53%
Heat Diffusion 56.08%
Learning
Topic Signature (TG) 50.80%
Topic Similarity (TS) 69.93%
User Signature (UG) 56.59%
User Preferences to
Latent Categories (UPLC) 61.33%
In-degree (ID) 65.55%
Out-degree (OD) 59.73%
Number of Distinct Topics (NDT) 55.42%
Method Feature AUC
Baseline Existing Diffusion 58.25%
Learning
ALL 65.06%
TS + UPLC + ID + NDT 67.67%
TS + UPLC + ID 64.80%
TS + UPLC + NDT 66.01%
TS + ID + NDT 73.95%
UPLC + ID + NDT 67.24%
347
References  
David M. Blei, Andrew Y. Ng & Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3.993-1022. 
Jesse Davis & Mark Goadrich. 2006. The relationship 
between Precision-Recall and ROC curves. 
Proceedings of the 23rd international conference on 
Machine learning, Pittsburgh, Pennsylvania. 
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang & Chih-Jen Lin. 2008. LIBLINEAR: A 
Library for Large Linear Classification. J. Mach. 
Learn. Res., 9.1871-74. 
Hongliang Fei, Ruoyi Jiang, Yuhao Yang, Bo Luo & 
Jun Huan. 2011. Content based social behavior 
prediction: a multi-task learning approach. 
Proceedings of the 20th ACM international 
conference on Information and knowledge 
management, Glasgow, Scotland, UK. 
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, 
Zoran Despotovic & Wolfgang Kellerer. 2010. 
Outtweeting the twitterers - predicting information 
cascades in microblogs. Proceedings of the 3rd 
conference on Online social networks, Boston, MA. 
Hightman. 2012. Simple Chinese Words Segmentation 
(SCWS). 
David Kempe, Jon Kleinberg & Eva Tardos. 2003. 
Maximizing the spread of influence through a social 
network. Proceedings of the ninth ACM SIGKDD 
international conference on Knowledge discovery 
and data mining, Washington, D.C. 
Tsung-Ting Kuo, San-Chuan Hung, Wei-Shih Lin, 
Shou-De Lin, Ting-Chun Peng & Chia-Chun Shih. 
2011. Assessing the Quality of Diffusion Models 
Using Real-World Social Network Data. Conference 
on Technologies and Applications of Artificial 
Intelligence, 2011. 
C.X. Lin, Q.Z. Mei, Y.L. Jiang, J.W. Han & S.X. Qi. 
2011. Inferring the Diffusion and Evolution of 
Topics in Social Communities. Proceedings of the 
IEEE International Conference on Data Mining, 
2011. 
Hao Ma, Haixuan Yang, Michael R. Lyu & Irwin King. 
2008. Mining social networks using heat diffusion 
processes for marketing candidates selection. 
Proceeding of the 17th ACM conference on 
Information and knowledge management, Napa 
Valley, California, USA. 
Andrew Kachites McCallum. 2002. MALLET: A 
Machine Learning for Language Toolkit. 
Sasa Petrovic, Miles Osborne & Victor Lavrenko. 2011. 
RT to Win! Predicting Message Propagation in 
Twitter. International AAAI Conference on Weblogs 
and Social Media, 2011. 
Xuan-Hieu Phan & Cam-Tu Nguyen. 2007. 
GibbsLDA++: A C/C++ implementation of latent 
Dirichlet alocation (LDA). 
Ian H. Witten, Eibe Frank & Mark A. Hall. 2011. Data 
Mining: Practical machine learning tools and 
techniques. San Francisco: Morgan Kaufmann 
Publishers Inc. 
Jiang Zhu, Fei Xiong, Dongzhen Piao, Yun Liu & Ying 
Zhang. 2011. Statistically Modeling the 
Effectiveness of Disaster Information in Social 
Media. Proceedings of the 2011 IEEE Global 
Humanitarian Technology Conference. 
 
 
348
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 145?150,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Online Plagiarism Detection Through Exploiting Lexical, Syntactic, and 
Semantic Information 
 
Wan-Yu Lin Nanyun Peng Chun-Chao Yen Shou-de Lin 
Graduate Institute of 
Networking and 
Multimedia, National 
Taiwan University 
Institute of 
Computational 
Linguistic, Peking 
University 
Graduate Institute of 
Networking and 
Multimedia, National 
Taiwan University 
Graduate Institute of 
Networking and 
Multimedia, National 
Taiwan University 
r99944016@csie
.ntu.edu.tw 
pengnanyun@pku
.edu.cn 
r96944016@csie
.ntu.edu.tw 
sdlin@csie.ntu
.edu.tw 
 
Abstract 
In this paper, we introduce a framework that 
identifies online plagiarism by exploiting lexical, 
syntactic and semantic features that includes 
duplication-gram, reordering and alignment of 
words, POS and phrase tags, and semantic 
similarity of sentences. We establish an ensemble 
framework to combine the predictions of each 
model. Results demonstrate that our system can 
not only find considerable amount of real-world 
online plagiarism cases but also outperforms 
several state-of-the-art algorithms and commercial 
software. 
Keywords 
Plagiarism Detection, Lexical, Syntactic, Semantic 
1. Introduction 
Online plagiarism, the action of trying to create a 
new piece of writing by copying, reorganizing or 
rewriting others? work identified through search 
engines, is one of the most commonly seen 
misusage of the highly matured web technologies. 
As implied by the experiment conducted by 
(Braumoeller and Gaines, 2001), a powerful 
plagiarism detection system can effectively 
discourage people from plagiarizing others? work. 
A common strategy people adopt for online-
plagiarism detection is as follows. First they 
identify several suspicious sentences from the 
write-up and feed them one by one as a query to a 
search engine to obtain a set of documents. Then 
human reviewers can manually examine whether 
these documents are truly the sources of the 
suspicious sentences. While it is quite 
straightforward and effective, the limitation of this 
strategy is obvious. First, since the length of search 
query is limited, suspicious sentences are usually 
queried and examined independently. Therefore, it 
is harder to identify document level plagiarism 
than sentence level plagiarism. Second, manually 
checking whether a query sentence plagiarizes 
certain websites requires specific domain and 
language knowledge as well as considerable 
amount of energy and time. To overcome the 
above shortcomings, we introduce an online 
plagiarism detection system using natural language 
processing techniques to simulate the above 
reverse-engineering approach. We develop an 
ensemble framework that integrates lexical, 
syntactic and semantic features to achieve this goal. 
Our system is language independent and we have 
implemented both Chinese and English versions 
for evaluation. 
2. Related Work 
Plagiarism detection has been widely discussed in 
the past decades (Zou et al, 2010). Table 1. 
summarizes some of them: 
Author 
Comparison 
Unit 
Similarity Function 
Brin et al, 
1995 
Word + 
Sentence 
Percentage of matching 
sentences. 
White and 
Joy, 2004 
Sentence 
Average overlap ratio of 
the sentence pairs using 2 
pre-defined thresholds. 
Niezgoda 
and Way, 
2006 
A human 
defined 
sliding 
window 
Sliding windows ranked 
by the average length per 
word. 
Cedeno and 
Rosso,  2009 
Sentence + 
n-gram 
Overlap percentage of n-
gram in the sentence pairs. 
145
Pera and Ng, 
2010 
Sentence 
A pre-defined 
resemblance function 
based on word correlation 
factor. 
Stamatatos, 
2011 
Passage 
Overlap percentage of 
stopword n-grams. 
Grman and 
Ravas, 2011 
Passage 
Matching percentage of 
words with given 
thresholds on both ratio 
and absolute number of 
words in passage. 
Table 1. Summary of related works 
Comparing to those systems, our system exploits 
more sophisticated syntactic and semantic 
information to simulate what plagiarists are trying 
to do. 
There are several online or charged/free 
downloadable plagiarism detection systems such as 
Turnitin, EVE2, Docol? c, and CATPPDS which 
detect mainly verbatim copy. Others such as 
Microsoft Plagiarism Detector (MPD), Safeassign, 
Copyscape and VeriGuide, claim to be capable of 
detecting obfuscations. Unfortunately those 
commercial systems do not reveal the detail 
strategies used, therefore it is hard to judge and 
reproduce their results for comparison. 
3. Methodology 
 
Figure 1. Detection Flow 
The data flow is shown above in Figure 1.  
3.1 Query a Search Engine 
We first break down each article into a series of 
queries to query a search engine. Several systems 
such as (Liu at al., 2007) have proposed a similar 
idea. The main difference between our method and 
theirs is that we send unquoted queries rather than 
quoted ones. We do not require the search results 
to completely match to the query sentence. This 
strategy allows us to not only identify the 
copy/paste type of plagiarism but also re-write/edit 
type of plagiarism.  
3.2 Sentence-based Plagiarism Detection 
Since not all outputs of a search engine contain an 
exact copy of the query, we need a model to 
quantify how likely each of them is the source of 
plagiarism. For better efficiency, our experiment 
exploits the snippet of a search output to represent 
the whole document. That is, we want to measure 
how likely a snippet is the plagiarized source of the 
query. We designed several models which utilized 
rich lexical, syntactic and semantic features to 
pursue this goal, and the details are discussed 
below.  
3.2.1 Ngram Matching (NM) 
One straightforward measure is to exploit the n-
gram similarity between source and target texts. 
We first enumerate all n-grams in source, and then 
calculate the overlap percentage with the n-grams 
in the target. The larger n is, the harder for this 
feature to detect plagiarism with insertion, 
replacement, and deletion. In the experiment, we 
choose n=2. 
3.2.2 Reordering of Words (RW) 
Plagiarism can come from the reordering of words. 
We argue that the permutation distance between S1 
and S2 is an important indicator for reordered 
plagiarism. The permutation distance is defined as 
the minimum number of pair-wise exchanging of 
matched words needed to transform a sentence, S2, 
to contain the same order of matched words as 
another sentence, S1. As mentioned in (S?rensena 
and Sevaux, 2005), the permutation distance can 
be calculated by the following expression  
? ?1, ?2 =   ???
?
?=?+1
??1
?=1   
where  
??? =  
1, ?? ?1 ? > ?1 ? ??? ?2 ? < ?2 ? 
          0, ?????????                                                   
  
S1(i) and S2(i) are indices of the i
th matched 
word in sentences S1 and S2 respectively and n is 
the number of matched words between  the 
sentences S1 and S2. Let ? =  
n2? n
2
 be the 
normalized term, which is the maximum possible 
distance between S1 and S2, then the reordering 
146
score of the two sentences, expressed as s(S1, S2), 
will be s S1 , S2  = 1 ?  
d S1 ,S2 
?
 
3.2.3 Alignment of Words (AW) 
Besides reordering, plagiarists often insert or 
delete words in a sentence. We try to model such 
behavior by finding the alignment of two word 
sequences. We perform the alignment using a 
dynamic programming method as mentioned in 
(Wagner and Fischer, 1975).  
However, such alignment score does not reflect 
the continuity of the matched words, which can be 
an important cue to identify plagiarism. To 
overcome such drawback, we modify the score as 
below. 
New Alignment Score =
 ??
|? |?1
?=1
|?|?1
 
where    ?? =
1
# ??  ?????  ???????  ?? ,??+1 +1
 
M is the list of matched words, and Mi is the i
th 
matched word in M. This implies we prefer fewer 
unmatched words in between two matched ones. 
3.2.4 POS and Phrase Tags of Words (PT, PP) 
Exploiting only lexical features can sometimes 
result in some false positive cases because two sets 
of matched words can play different roles in the 
sentences. See S1 and S2 in Table 2. as a possible 
false positive case. 
S1: The man likes the woman 
S2: The woman is like the man 
Word S1: Tag S2: Tag S1: Phrase S2: Phrase 
man NN NN NP PP 
like VBZ IN VP PP 
woman NN NN VP NP 
Table 2. An example of matched words with different 
tags and phrases 
Therefore, we further explore syntactic features 
for plagiarism detection. To achieve this goal, we 
utilize a parser to obtain POS and phrase tags of 
the words. Then we design an equation to measure 
the tag/phrase similarity. 
Sim = 
???  ???? ???  ?????  ??? ?  ?????????  ???  
???  ???? ???  ?????  
 
We paid special attention to the case that 
transforms a sentence from an active form to a 
passive-form or vice versa. A subject originally in 
a Noun Phrase can become a Preposition Phrase, 
i.e. ?by ??, in the passive form while the object in 
a Verb Phrase can become a new subject in a Noun 
Phrase. Here we utilize the Stanford Dependency 
provided by Stanford Parser to match the 
tag/phrase between active and passive sentences.  
3.2.5 Semantic Similarity (LDA) 
Plagiarists, sometimes, change words or phrases to 
those with similar meanings. While previous works 
(Y. Lin et al, 2006) often explore semantic 
similarity using lexical databases such as WordNet 
to find synonyms, we exploit a topic model, 
specifically latent Dirichlet alocation (LDA, D. M. 
Blei et al, 2003), to extract the semantic features 
of sentences. Given a set of documents represented 
by their word sequences, and a topic number n, 
LDA learns the word distribution for each topic 
and the topic distribution for each document which 
maximize the likelihood of the word co-occurrence 
in a document. The topic distribution is often taken 
as semantics of a document. We use LDA to obtain 
the topic distribution of a query and a candidate 
snippet, and compare the cosine similarity of them 
as a measure of their semantic similarity. 
3.3 Ensemble Similarity Scores 
Up to this point, for each snippet the system 
generates six similarity scores to measure the 
degree of plagiarism in different aspects. In this 
stage, we propose two strategies to linearly 
combine the scores to make better prediction. The 
first strategy utilizes each model?s predictability 
(e.g. accuracy) as the weight to linearly combine 
the scores. In other words, the models that perform 
better individually will obtain higher weights. In 
the second strategy we exploit a learning model (in 
the experiment section we use Liblinear) to learn 
the weights directly.  
3.4 Document Level Plagiarism Detection 
For each query from the input article, our system 
assigns a degree-of-plagiarism score to some 
plausible source URLs. Then, for each URL, the 
system sums up all the scores it obtains as the final 
score for document-level degree-of-plagiarism. We 
set up a cutoff threshold to obtain the most 
plausible URLs. At the end, our system highlights 
the suspicious areas of plagiarism for display. 
147
4. Evaluation 
We evaluate our system from two different angles. 
We first evalaute the sentence level plagirism 
detection using the PAN corpus in English. We 
then evaluate the capability of the full system to 
detect on-line plagiarism cases using annotated 
results in Chinese. 
4.1 Sentence-based Evaluations 
We want to compare our model with the state-of-
the-art methods, in particular the winning entries in 
plagiarism detection competition in PAN 1 . 
However, the competition in PAN is designed for 
off-line plagiarism detection; the entries did not 
exploit an IR system to search the Web like we do. 
Nevertheless, we can still compare the core 
component of our system, the sentence-based 
measuring model with that of other systems. To 
achieve such goal, we first randomly sampled 370 
documents from PAN-2011 external plagiarism 
corpus (M. Potthast et al, 2010) containing 2882 
labeled plagiarism cases.  
 To obtain high-quality negative examples for 
evaluation, we built a full-text index on the corpus 
using Lucene package. Then we use the suspicious 
passages as queries to search the whole dataset 
using Lucene. Since there is length limitation in 
Lucene (as well as in the real search engines), we 
further break the 2882 plagiarism cases into 6477 
queries. We then extract the top 30 snippets 
returned by the search engine as the potential 
negative candidates for each plagiarism case. Note 
that for each suspicious passage, there is only one 
target passage (given by the ground truth) that is 
considered as a positive plagiarism case in this data, 
and it can be either among these 30 cases or not. 
However, we union these 30 cases with the ground 
truth as a set, and use our (as well as the 
competitors?) models to rank the degree-of-
plagiarism for all the candidates. We then evaluate 
the rank by the area-under-PR-curve (AUC) score. 
We compared our system with the winning entry of 
PAN 2011 (Grman and Ravas, 2011) and the 
stopword ngram model that claims to perform 
better than this winning entry by Stamatatos (2011). 
The results of each individual model and ensemble 
using 5-fold cross validation are listed in Table 3. 
It shows that NM is the best individual model, and 
                                                                
1 The website of PAN-2011 is http://pan.webis.de/ 
an ensemble of three features outperforms the 
state-of-the-art by 26%. 
NM RW AW PT PP LDA 
0.876 0.596 0.537 0.551 0.521 0.596 
                                     (a) 
 Ours ensemble 
Pan-11 
Champion 
Stopword 
Ngram 
AUC 
0.882 
(NM+RW+PP) 
0.620 0.596 
                                     (b) 
Table 3. (a) AUC for each individual model (b) AUC of 
our ensemble and other state-of-the-art algorithms 
4.2 Evaluating the Full System 
To evaluate the overall system, we manually 
collect 60 real-world review articles from the 
Internet for books (20), movies (20), and music 
albums (20). Unfortunately for an online system 
like ours, there is no ground truth available for 
recall measure. We conduct two differement 
evalautions. First we use the 60 articles as inputs to 
our system, ask 5 human annotators to check 
whether the articles returned by our system can be 
considered as plagiarism. Among all 60 review 
articles, our system identifies a considerablely high 
number of copy/paste articles, 231 in total. 
However, identifying this type of plagiarism is 
trivial, and has been done by many similar tools. 
Instead we focus on the so-called smart-plagiarism 
which cannot be found through quoting a query in 
a search engine. Table 4. shows the precision of 
the smart-plagiarism articles returned by our 
system. The precision is very high and outperforms 
a commertial tool Microsoft Plagiarism Detector. 
 Book Movie Music 
Ours 
280/288 
(97%) 
88/110 
(80%) 
979/1033 
(95%) 
MPD 
44/53 
(83%) 
123/172 
(72%) 
120/161 
(75%) 
Table 4. Precision of Smart Plagiarism 
In the second evaluation, we first choose 30 
reviews randomly. Then we use each of them as 
queries into Google and retrieve a total of 5636 
pieces of snippet candidates. We then ask 63 
human beings to annotate whether those snippets 
represent plagiarism cases of the original review 
article. Eventually we have obtained an annotated 
148
dataset and found a total of 502 plagiarized 
candidates with 4966 innocent ones for evalaution. 
Table 5. shows the average AUC of 5-fold cross 
validation. The results show that our method 
outperforms the Pan-11 winner slightly, and much 
better than the Stopword Ngram.  
NM RW AW PT PP LDA 
0.904 0.778 0.874 0.734 0.622 0.581 
(a) 
 Ours ensemble 
Pan-11 
Champion 
Stopword 
Ngram 
AUC 
0.919 
(NM+RW+AW
+PT+PP+LDA) 
0.893 0.568 
(b) 
Table 5. (a) AUC for each individual model (b) AUC of 
our ensemble and other state-of-the-art algorithms 
4.3 Discussion 
There is some inconsistency of the performance of 
single features in these two experiments. The main 
reason we believe is that the plagiarism cases were 
created in very different manners. Plagiarism cases 
in PAN external source are created artificially 
through word insertions, deletions, reordering and 
synonym substitutions. As a result, features such as 
word alignment and reordering do not perform 
well because they did not consider the existence of 
synonym word replacement. On the other hand, 
real-world plagiarism cases returned by Google are 
those with matching-words, and we can find better 
performance for AW. 
The performances of syntactic and semantic 
features, namely PT, PP and LDA, are consistently 
inferior than other features. It is because they often 
introduce false-positives as there are some non-
plagiarism cases that might have highly overlapped 
syntactic or semantic tags. Nevertheless, 
experiments also show that these features can 
improve the overall accuracy in ensemble. 
We also found that the stopword Ngram model 
is not applicable universally. For one thing, it is 
less suitable for on-line plagiarism detection, as the 
length limitation for queries diminishes the 
usability of stopword n-grams. For another, 
Chinese seems to be a language that does not rely 
as much on stopwords as the latin languages do to 
maintain its syntax structure. 
Samples of our system?s finding can be found 
here, http://tinyurl.com/6pnhurz 
5. Online Demo System 
We developed an online demos system using 
JAVA (JDK 1.7). The system currently supports 
the detection of documents in both English and 
Chinese. Users can either upload the plain text file 
of a suspicious document, or copy/paste the 
content onto the text area, as shown below in 
Figure 2.  
 
Figure 2. Input Screen-Shot 
Then the system will output some URLs and 
snippets as the potential source of plagiarism. (see 
Figure 3.) 
Figure 3. Output Screen-Shot 
6. Conclusion 
Comparing with other online plagiarism 
detection systems, ours exploit more sophisticated 
features by modeling how human beings plagiarize 
online sources. We have exploited sentence-level 
plagiarism detection on lexical, syntactic and 
semantic levels. Another noticeable fact is that our 
approach is almost language independent. Given a 
parser and a POS tagger of a language, our 
framework can be extended to support plagiarism 
detection for that language.  
149
7. References 
Salha Alzahrani, Naomie Salim, and Ajith Abraham, 
?Understanding Plagiarism Linguistic Patterns, 
Textual Features and Detection Methods ? in IEEE 
Transactions on systems , man and cyberneticsPart C: 
Applications and reviews, 2011 
D. M.  Blei,  A.  Y. Ng, M. I. Jordan,  and J. Lafferty. 
Latent dirichlet alocation. Journal of Machine 
Learning Research, 3:2003, 2003. 
Bear F. Braumoeller and Brian J. Gaines. 2001. Actions 
Do Speak Louder Than Words: Deterring Plagiarism 
with the Use of Plagiarism-Detection Software. In 
Political Science & Politics, 34(4):835-839. 
Sergey Brin, James Davis, and Hector Garcia-molina. 
1995. Copy Detection Mechanisms for Digital 
Documents. In Proceedings of the ACM SIGMOD 
Annual Conference, 24(2):398-409.  
Alberto Barr?n Cede?o and Paolo Rosso. 2009. On 
Automatic Plagiarism Detection based on n-grams 
Comparison. In Proceedings of the 31th European 
Conference on IR Research on Advances in 
Information Retrieval, ECIR 2009, LNCS 5478:696-
700, Springer-Verlag, and Berlin Heidelberg,  
Jan Grman and Rudolf Ravas. 2011. Improved 
implementation for finding text similarities in large 
collections of data.In Proceedings of PAN 2011. 
NamOh Kang, Alexander Gelbukh, and SangYong Han. 
2006. PPChecker: Plagiarism Pattern Checker in 
Document Copy Detection. In Proceedings of TSD-
2006, LNCS, 4188:661-667. 
Yuhua Li, David McLean, Zuhair A. Bandar, James D. 
O? Shea, and Keeley Crockett. 2006. Sentence 
Similarity Based on Semantic Nets and Corpus 
Statistics. In Proceedings of the IEEE Transactions 
on Knowledge and Data Engineering, 18(8):1138-
1150. 
Yi-Ting Liu, Heng-Rui Zhang, Tai-Wei Chen, and Wei-
Guang Teng. 2007. Extending Web Search for 
Online Plagiarism Detection. In Proceedings of the 
IEEE International Conference on Information Reuse 
and Integration, IRI 2007. 
Caroline Lyon, Ruth Barrett, and James Malcolm. 2004. 
A Theoretical Basis to the Automated Detection of 
Copying Between Texts, and its Practical 
Implementation in the Ferret Plagiarism and 
Collusion Detector. In Proceedings of Plagiarism: 
Prevention, Practice and Policies 2004 Conference. 
Sebastian Niezgoda and Thomas P. Way. 2006. 
SNITCH: A Software Tool for Detecting Cut and 
Paste Plagiarism. In Proceedings of the 37th SIGCSE 
Technical Symposium on Computer Science 
Education, p.51-55. 
Maria Soledad Pera and Yiu-kai Ng. 2010. IOS Press 
SimPaD: A Word-Similarity Sentence-Based 
Plagiarism Detection Tool on Web Documents. In 
Journal on Web Intelligence and Agent Systems, 9(1). 
Xuan-Hieu Phan and Cam-Tu Nguyen. GibbsLDA++: 
A C/C++ implementation of latent Dirichlet 
allocation (LDA), 2007  
Martin Potthast, Benno Stein, Alberto Barr?n Cede?o, 
and Paolo Rosso. An Evaluation Framework for 
Plagiarism Detection. In 23rd International 
Conference on Computational Linguistics (COLING 
10), August 2010. Association for Computational 
Linguistics. 
Kenneth S?rensena and Marc Sevaux. 2005. 
Permutation Distance Measures for Memetic 
Algorithms with Population Management. In 
Proceedings of 6th Metaheuristics International 
Conference. 
Efstathios Stamatatos, "Plagiarism Detection Based on 
Structural Information" in Proceedings of the 20th 
ACM international conference on Information and 
knowledge management, CIKM'11 
Robert A. Wagner and Michael J. Fischer. 1975. The 
String-to-string correction problem. In Journal of the 
ACM, 21(1):168-173. 
Daniel R. White and Mike S. Joy. 2004. Sentence-Based 
Natural Language Plagiarism Detection. In Journal 
on Educational Resources in Computing JERIC 
Homepage archive, 4(4).  
Du Zou, Wei-jiang Long, and Zhang Ling. 2010. A 
Cluster-Based Plagiarism Detection Method. In Lab 
Report for PAN at CLEF 2010. 
 
 
 
 
 
 
 
 
 
 
150
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611?617,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Enriching Cold Start Personalized Language Model 
Using Social Network Information 
  Yu-Yang Huang
?
, Rui Yan*, Tsung-Ting Kuo
?
, Shou-De Lin
??
 
?
Graduate Institute of Computer Science and Information Engineering,  
National Taiwan University, Taipei, Taiwan 
?
Graduate Institute of Network and Multimedia,  
National Taiwan University, Taipei, Taiwan 
*Computer and Information Science Department,  
University of Pennsylvania, Philadelphia, PA 19104, U.S.A. 
{r02922050, d97944007, sdlin}@csie.ntu.edu.tw, ruiyan@seas.upenn.edu 
 
Abstract 
We introduce a generalized framework to enrich 
the personalized language models for cold start 
users. The cold start problem is solved with 
content written by friends on social network 
services. Our framework consists of a mixture 
language model, whose mixture weights are es-
timated with a factor graph. The factor graph is 
used to incorporate prior knowledge and heuris-
tics to identify the most appropriate weights. 
The intrinsic and extrinsic experiments show 
significant improvement on cold start users. 
1 Introduction 
Personalized language models (PLM) on social 
network services are useful in many aspects (Xue 
et al, 2009; Wen et al, 2012; Clements, 2007), 
For instance, if the authorship of a document is 
in doubt, a PLM may be used as a generative 
model to identify it. In this sense, a PLM serves 
as a proxy of one?s writing style. Furthermore, 
PLMs can improve the quality of information 
retrieval and content-based recommendation sys-
tems, where documents or topics can be recom-
mended based on the generative probabilities. 
However, it is challenging to build a PLM for 
users who just entered the system, and whose 
content is thus insufficient to characterize them. 
These are called ?cold start? users. Producing 
better recommendations is even more critical for 
cold start users to make them continue to use the 
system. Therefore, this paper focuses on how to 
overcome the cold start problem and obtain a 
better PLM for cold start users. 
The content written by friends on a social 
network service, such as Facebook or Twitter, is 
exploited. It can be either a reply to an original 
post or posts by friends. Here the hypothesis is 
that friends, who usually share common interests, 
tend to discuss similar topics and use similar 
words than non-friends. In other words, we be-
lieve that a cold start user?s language model can 
be enriched and better personalized by incorpo-
rating content written by friends. 
Intuitively, a linear combination of document-
level language models can be used to incorporate 
content written by friends. However, it should be 
noticed that some documents are more relevant 
than others, and should be weighted higher. To 
obtain better weights, some simple heuristics 
could be exploited. For example, we can measure 
the similarity or distance between a user lan-
guage model and a document language model. In 
addition, documents that are shared frequently in 
a social network are usually considered to be 
more influential, and could contribute more to 
the language model. More complex heuristics 
can also be derived. For instance, if two docu-
ments are posted by the same person, their 
weights should be more similar. The main chal-
lenge lies in how such heuristics can be utilized 
in a systematic manner to infer the weights of 
each document-level language model. 
In this paper, we exploit the information on 
social network services in two ways. First, we 
impose the social dependency assumption via a 
finite mixture model. We model the true, albeit 
unknown, personalized language model as a 
combination of a biased user language model and 
a set of relevant document language models. Due 
to the noise inevitably contained in social media 
content, instead of using all available documents, 
we argue that by properly specifying the set of 
relevant documents, a better personalized lan-
guage model can be learnt. In other words, each 
user language model is enriched by a personal-
ized collection of background documents. 
Second, we propose a factor graph model 
(FGM) to incorporate prior knowledge (e.g. the 
heuristics described above) into our model. Each 
611
mixture weight is represented by a random vari-
able in the factor graph, and an efficient algo-
rithm is proposed to optimize the model and infer 
the marginal distribution of these variables. Use-
ful information about these variables is encoded 
by a set of potential functions. 
The main contributions of this work are sum-
marized below: 
? To solve the cold start problem encountered 
when estimating PLMs, a generalized frame-
work based on FGM is proposed. We incorpo-
rate social network information into user lan-
guage models through the use of FGM. An it-
erative optimization procedure utilizing per-
plexity is presented to learn the parameters. 
To our knowledge, this is the first proposal to 
use FGM to enrich language models. 
? Perplexity is selected as an intrinsic evalua-
tion, and experiment on authorship attribution 
is used as an extrinsic evaluation. The results 
show that our model yields significant im-
provements for cold start users. 
2 Methodology 
2.1 Social-Driven Personalized Language 
Model 
The language model of a collection of documents 
can be estimated by normalizing the counts of 
words in the entire collection (Zhai, 2008). To 
build a user language model, one na?ve way is to 
first normalize word frequency ?(?, ?)  within 
each document, and then average over all the 
documents in a user?s document collection. The 
resulting unigram user language model is: 
??(?) =
1
|??|
?
?(?, ?)
|?|????
 
=
1
|??|
? ??(?)
????
 
(1) 
where ??(?) is the language model of a particu-
lar document, and ?? is the user?s document col-
lection. This formulation is basically an equal-
weighted finite mixture model. 
A simple yet effective way to smooth a lan-
guage model is to linearly interpolate with a 
background language model (Chen and Good-
man, 1996; Zhai and Lafferty, 2001). In the line-
ar interpolation method, all background docu-
ments are treated equally. The entire document 
collection is added to the user language model 
??(?) with the same interpolation coefficient. 
Our main idea is to specify a set of relevant 
documents for the target user using information 
embedded in a social network, and enrich the 
smoothing procedure with these documents. Let 
????  denote the content from relevant persons 
(e.g. social neighbors) of u1, our idea can be con-
cisely expressed as: 
??1
? (?) = ??1??1(?) + ? ??????(?)
???????
 (2) 
where ??? is the mixture weight of the language 
model of document di, and ??1 + ???? = 1 . 
Documents posted by irrelevant users are not 
included as we believe the user language model 
can be better personalized by exploiting the so-
cial relationship in a more structured way. In our 
experiment, we choose the first degree neighbor 
documents as ????. 
Also note that we have made no assumption 
about how the ?base? user language model 
??1(?) is built. In practice, it need not be models 
following maximum likelihood estimation, but 
any language model can be integrated into our 
framework to achieve a better refined model. 
Furthermore, any smoothing method can be ap-
plied to the language model without degrading 
the effectiveness. 
2.2 Factor Graph Model (FGM) 
Now we discuss how the mixture weights can be 
estimated. We introduce a factor graph model 
(FGM) to make use of the diverse information on 
a social network. Factor graph (Kschischang et 
al., 2006) is a bipartite graph consisting of a set 
of random variables and a set of factors which 
signifies the relationships among the variables. It 
is best suited in situations where the data is clear-
ly of a relational nature (Wang et al, 2012). The 
joint distribution of the variables is factored ac-
cording to the graph structure. Using FGM, one 
can incorporate the knowledge into the potential 
function for optimization and perform joint in-
ference over documents. As shown in Figure 1, 
the variables included in the model are described 
as follows: 
Candidate variables ?? = ??, ??? . The ran-
dom variables in the top layer stand for the de-
grees of belief that a document di should be in-
cluded in the PLM of the target user ?. 
Figure 1: A two-layered factor graph (FGM) 
proposed to estimate the mixture weights. 
612
Attribute variables xi. Local information is 
stored as the random variables in the bottom lay-
er. For example, x1 might represent the number 
of common friends between the author of a doc-
ument di and our target user. 
The potential functions in the FGM are: 
Attribute-to-candidate function. This poten-
tial function captures the local dependencies of a 
candidate variable to the relevant attributes. Let 
the candidate variable yi correspond to a docu-
ment di, the attribute-to-candidate function of yi 
is defined in a log-linear form: 
?(?? , ?) =
1
??
???{???(??, ?)} (3) 
where A is the set of attributes of either the doc-
ument di or target user u; f is a vector of feature 
functions which locally model the value of yi 
with attributes in A; ??  is the local partition 
function and ? is the weight vector to be learnt. 
In our experiment, we define the vector of 
functions as ? = ?????, ???? , ????, ????, ????
? as: 
? Similarity function ???? . The similarity be-
tween language models of the target user and 
a document should play an important role. We 
use cosine similarity between two unigram 
models in our experiments. 
? Document quality function ????. The out-of-
vocabulary (OOV) ratio is used to measure the 
quality of a document. It is defined as 
???? = 1 ?
|{?:? ? ?? ? ? ? ?}|
|??|
 (4) 
where ?  is the vocabulary set of the entire 
corpus, with stop words excluded. 
? Document popularity function ???? . This 
function is defined as the number of times di is 
shared to model the popularity of documents. 
? Common friend function ????. It is defined 
as the number of common friends between the 
target user u1 and the author of di. 
? Author friendship function ??? . Assuming 
that documents posted by a user with more 
friends are more influential, this function is 
defined as the number of friends of di?s author. 
Candidate-to-candidate function. This po-
tential function defines the correlation of a can-
didate variable yi with another candidate variable 
yj in the factor graph. The function is defined as 
?(?? , ??) =
1
???,?
???{???(??, ??)} (5) 
where g is a vector of feature functions indicat-
ing whether two variables are correlated. If we 
further denote the set of all related variables as 
?(??) , then for any candidate variable yi, we 
have the following brief expression: 
?(?? , ?(??)) = ? ?(?? , ??)
????(??)
 (6) 
For two candidate variables, let the corre-
sponding document be di and dj, respectively, we 
define the vector ? = ????? , ?????
? as: 
? User relationship function ????. We assume 
that two candidate variables have higher de-
pendency if they represent documents of the 
same author or the two authors are friends. 
The dependency should be even greater if two 
documents are similar. Let ?(?)  denote the 
author of a document d and ?[?] denote the 
closed neighborhood of a user u, we define 
???? = ?{?(??) ? ?[?(??)]} ? ???(?? , ??) (7) 
? Co-category function ????. For any two can-
didate variables, it is intuitive that the two var-
iables would have a higher correlation if di 
and dj are of the same category. Let ?(?) de-
note the category of document d, we define 
???? = ?{?(??) = ?(??)} ? ???(?? , ??) (8) 
2.3 Model Inference and Optimization 
Let Y and X be the set of all candidate variables 
and attribute variables, respectively. The joint 
distribution encoded by the FGM is given by 
multiplying all potential functions. 
?(?, ?) =??(??, ?)?(?? , ?(??))
?
 (9) 
The desired marginal distribution can be ob-
tained by marginalizing all other variables. Since 
under most circumstances, however, the factor 
graph is densely connected, the exact inference is 
intractable and approximate inference is required. 
After obtaining the marginal probabilities, the 
mixture weights ???  in Eq. 2 are estimated by 
normalizing the corresponding marginal proba-
bilities ?(??) over all candidate variables, which 
can be written as 
??? = (1 ? ??1)
?(??)
? ?(??)?:???????
 (10) 
where the constraint ??1 + ???? = 1 leads to a 
valid probability distribution for our mixture 
model. 
A factor graph is normally optimized by gra-
dient-based methods. Unfortunately, since the 
ground truth values of the mixture weights are 
not available, we are prohibited from using su-
pervised approaches. Here we propose a two-step 
iterative procedure to optimize our model. At 
613
first, all the model parameters (i.e. ?, ?, ??) are 
randomly initialized. Then, we infer the marginal 
probabilities of candidate variables. Given these 
marginal probabilities, we can evaluate the per-
plexity of the user language model on a held-out 
dataset, and search for better parameters. This 
procedure is repeated until convergence. Also, 
notice that by using FGM, we reduce the number 
of parameters from 1 + |????| to 1 + |?| + |?|, 
lowering the risk of overfitting. 
3 Experiments 
3.1 Dataset and Experiment Setup 
We perform experiments on the Twitter dataset 
collected by Galuba et al (2010). Twitter data 
have been used to verify models with different 
purposes (Lin et al, 2011; Tan et al, 2011). To 
emphasize on the cold start scenario, we random-
ly selected 15 users with about 35 tweets and 70 
friends as candidates for an authorship attribution 
task. Our experiment corpus consists of 4322 
tweets. All words with less than 5 occurrences 
are removed. Stop words and URLs are also re-
moved and all tweets are stemmed. We identify 
the 100 most frequent terms as categories. The 
size of the vocabulary set is 1377. 
We randomly partitioned the tweets of each 
user into training, validation and testing sets. The 
reported result is the average of 10 random splits. 
In all experiments, we vary the size of training 
data from 1% to 15%, and hold out the same 
number of tweets from each user as validation 
and testing data. The statistics of our dataset, 
given 15% training data, are shown in Table 1. 
 Loopy belief propagation (LBP) is used to ob-
tain the marginal probabilities of the variables 
(Murphy et al, 1999). Parameters are searched 
with the pattern search algorithm (Audet and 
Dennis, 2002). To not lose generality, we use the 
default configuration in all experiments. 
# of Max. Min. Avg. 
Tweets 70 19 35.4 
Friends 139 24 68.9 
Variables 467 97 252.7 
Edges 9216 231 3427.1 
Table 1: Dataset statistics 
3.2 Baseline Methods 
We compare our framework with two baseline 
methods. The first (?Cosine?) is a straightfor-
ward implementation that sets all mixture 
weights ??? to the cosine similarity between the 
probability mass vectors of the document and 
user unigram language models. The second 
(?PS?) uses the pattern search algorithm to per-
form constrained optimization over the mixture 
weights. As mentioned in section 2.3, the main 
difference between this method and ours 
(?FGM?) is that we reduce the search space of 
the parameters by FGM. Furthermore, social 
network information is exploited in our frame-
work, while the PS method performs a direct 
search over mixture weights, discarding valuable 
knowledge. 
Different from other smoothing methods that 
are usually mutually exclusive, any other 
smoothing methods can be easily merged into 
our framework. In Eq. 2, the base language 
model ??1(?) can be already smoothed by any 
techniques before being plugged into our frame-
work. Our framework then enriches the user lan-
guage model with social network information. 
We select four popular smoothing methods to 
demonstrate such effect, namely additive 
smoothing, absolute smoothing (Ney et al, 1995), 
Jelinek-Mercer smoothing (Jelinek and Mercer, 
1980) and Dirichlet smoothing (MacKay and 
Peto, 1994). The results of using only the base 
model (i.e. set ??? = 0 in Eq. 2) are denoted as 
?Base? in the following tables. 
Train % 
Additive Absolute 
Base Cosine PS FGM Base Cosine PS FGM 
1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** 
5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** 
10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** 
15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** 
Train % 
Jelinek-Mercer Dirichlet 
Base Cosine PS FGM Base Cosine PS FGM 
1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** 
5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2** 
10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0** 
15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4 
Table 2: Testing set perplexity. ** indicates that the best score among all methods is significantly bet-
ter than the next highest score, by t-test at a significance level of 0.05. 
614
3.3 Perplexity 
As an intrinsic evaluation, we first compute the 
perplexity of unseen sentences under each user 
language model. The result is shown in Table 2. 
Our method significantly outperforms all of 
the methods in almost all settings. We observe 
that the ?PS? method takes a long time to con-
verge and is prone to overfitting, likely because 
it has to search about a few hundred parameters 
on average. As expected, the advantage of our 
model is more apparent when the data is sparse. 
3.4 Authorship Attribution (AA) 
The authorship attribution (AA) task is chosen as 
the extrinsic evaluation metric. Here the goal is 
not about comparing with the state-of-the-art ap-
proaches in AA, but showing that LM-based ap-
proaches can benefit from our framework. 
To apply PLM on this task, a na?ve Bayes 
classifier is implemented (Peng et al, 2004). The 
most probable author of a document d is the one 
whose PLM yields the highest probability, and is 
determined by ?? = argmax?{? ??(?)??? }. 
The result is shown in Table 3. Our model im-
proves personalization and outperforms the base-
lines under cold start settings. When data is 
sparse, the ?PS? method tends to overfit the 
noise, while the ?Cosine? method contains too 
few information and is severely biased. Our 
method strikes a balance between model com-
plexity and the amount of information included, 
and hence performs better than the others. 
4 Related Work 
Personalization has long been studied in various 
textual related tasks. Personalized search is es-
tablished by modeling user behavior when using 
search engines (Shen et al, 2005; Xue et al, 
2009). Query language model could be also ex-
panded based on personalized user modeling 
(Chirita et al, 2007). Personalization has also 
been modeled in many NLP tasks such as sum-
marization (Yan et al, 2011) and recommenda-
tion (Yan et al, 2012). Different from our pur-
pose, these models do not aim at exploiting so-
cial media content to enrich a language model. 
Wen et al (2012) combines user-level language 
models from a social network, but instead of fo-
cusing on the cold start problem, they try to im-
prove the speech recognition performance using 
a mass amount of texts on social network. On the 
other hand, our work explicitly models the more 
sophisticated document-level relationships using 
a probabilistic graphical model. 
5 Conclusion 
The advantage of our model is threefold. First, 
prior knowledge and heuristics about the social 
network can be adapted in a structured way 
through the use of FGM. Second, by exploiting a 
well-studied graphical model, mature inference 
techniques, such as LBP, can be applied in the 
optimization procedure, making it much more 
effective and efficient. Finally, different from 
most smoothing methods that are mutually ex-
clusive, any other smoothing method can be in-
corporated into our framework to be further en-
hanced. Using only 1% of the training corpus, 
our model can improve the perplexity of base 
models by as much as 40% and the accuracy of 
authorship attribution by at most 15%. 
6 Acknowledgement 
This work was sponsored by AOARD grant 
number No. FA2386-13-1-4045 and National 
Science Council, National Taiwan University 
and Intel Corporation under Grants NSC102-
2911-I-002-001 and NTU103R7501 and grant 
102-2923-E-002-007-MY2, 102-2221-E-002-170, 
101-2628-E-002-028-MY2. 
Train % 
Additive Absolute 
Base Cosine PS FGM Base Cosine PS FGM 
1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27** 
5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53** 
10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87** 
15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60** 
Train % 
Jelinek-Mercer Dirichlet 
Base Cosine PS FGM Base Cosine PS FGM 
1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67** 
5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.93 
10% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.53 
15% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40 
Table 3: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is 
significantly better than the next highest score, by t-test at a significance level of 0.05. 
615
Reference 
Charles Audet and J. E. Dennis, Jr. 2002. Analysis of 
generalized pattern searches. SIAM J. on Optimiza-
tion, 13(3):889?903, August. 
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language 
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics, 
ACL ?96, pages 310?318, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 
Paul Alexandru Chirita, Claudiu S. Firan, and Wolf-
gang Nejdl. 2007. Personalized query expansion 
for the web. In Proceedings of the 30th Annual In-
ternational ACM SIGIR Conference on Research 
and Development in Information Retrieval, 
SIGIR ?07, pages 7?14, New York, NY, USA. 
ACM. 
Maarten Clements. 2007. Personalization of social 
media. In Proceedings of the 1st BCS IRSG Con-
ference on Future Directions in Information Access, 
FDIA?07, pages 14?14, Swinton, UK, UK. British 
Computer Society. 
Wojciech Galuba, Karl Aberer, Dipanjan Chakraborty, 
Zoran Despotovic, and Wolfgang Kellerer. 2010. 
Outtweeting the twitterers - predicting information 
cascades in microblogs. In Proceedings of the 3rd 
Conference on Online Social Networks, WOSN?10, 
pages 3?3, Berkeley, CA, USA. USENIX Associa-
tion. 
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters 
from sparse data. In In Proceedings of the Work-
shop on Pattern Recognition in Practice, pages 
381?397, Amsterdam, The Netherlands: North-
Holland, May. 
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 
2006. Factor graphs and the sum-product algorithm. 
IEEE Trans. Inf. Theor., 47(2):498?519, Septem-
ber. 
Jimmy Lin, Rion Snow, and William Morgan. 2011. 
Smoothing techniques for adaptive online language 
models: Topic tracking in tweet streams. In Pro-
ceedings of the 17th ACM SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining, KDD ?11, pages 422?429, New York, NY, 
USA. ACM. 
David J.C. MacKay and Linda C. Bauman Peto. 1994. 
A hierarchical dirichlet language model. Natural 
Language Engineering, 1:1?19. 
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. 
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the 
Fifteenth Conference on Uncertainty in Artificial 
Intelligence, UAI?99, pages 467?475, San Francis-
co, CA, USA. Morgan Kaufmann Publishers Inc. 
Hermann Ney, Ute Essen, and Reinhard Kneser. 1995. 
On the estimation of ?small? probabilities by leav-
ing-one-out. IEEE Trans. Pattern Anal. Mach. In-
tell., 17(12):1202?1212, December. 
Fuchun Peng, Dale Schuurmans, and Shaojun Wang. 
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Inf. Retr., 7(3-4):317?345, 
September. 
Xuehua Shen, Bin Tan, and ChengXiang Zhai. 2005. 
Implicit user modeling for personalized search. In 
Proceedings of the 14th ACM International Con-
ference on Information and Knowledge Manage-
ment, CIKM ?05, pages 824?831, New York, NY, 
USA. ACM. 
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, 
Ming Zhou, and Ping Li. 2011. User-level senti-
ment analysis incorporating social networks. In 
Proceedings of the 17th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and 
Data Mining, KDD ?11, pages 1397?1405, New 
York, NY, USA. ACM. 
Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie 
Tang. 2012. Cross-lingual knowledge linking 
across wiki knowledge bases. In Proceedings of the 
21st International Conference on World Wide Web, 
WWW ?12, pages 459?468, New York, NY, USA. 
ACM. 
Tsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, and 
Lin-Shan Lee. 2012. Personalized language model-
ing by crowd sourcing with social network data for 
voice access of cloud applications. In Spoken Lan-
guage Technology Workshop (SLT), 2012 IEEE, 
pages 188?193. 
Gui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. 
2009. User language model for collaborative per-
sonalized search. ACM Trans. Inf. Syst., 
27(2):11:1?11:28, March. 
Rui Yan, Jian-Yun Nie, and Xiaoming Li. 2011. 
Summarize what you are interested in: An optimi-
zation framework for interactive personalized 
summarization. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1342?1351, Strouds-
burg, PA, USA. Association for Computational 
Linguistics. 
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012. 
Tweet recommendation with graph co-ranking. In 
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Long Pa-
pers - Volume 1, ACL ?12, pages 516?525, 
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics. 
ChengXiang Zhai. 2008. Statistical Language Models 
for Information Retrieval. Now Publishers Inc., 
Hanover, MA, USA. 
616
Chengxiang Zhai and John Lafferty. 2001. A study of 
smoothing methods for language models applied to 
ad hoc information retrieval. In Proceedings of the 
24th Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval, SIGIR ?01, pages 334?342, New York, NY, 
USA. ACM. 
 
617
