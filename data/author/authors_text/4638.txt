Term Recognition Using Technical Dictionary Hierarchy 
 
Jong-Hoon Oh, KyungSoon Lee, and Key-Sun Choi 
Computer Science Dept., Advanced Information TechnologyResearch Center (AITrc), and 
Korea Terminology Research Center for Language and Knowledge Engineering (KORTERM) 
Korea Advanced Institute of Science & Technology (KAIST)  
Kusong-Dong, Yusong-Gu Taejon, 305-701 Republic of Korea  
{rovellia,kslee,kschoi}@world.kaist.ac.kr  
 
 
 
Abstract  
In recent years, statistical approaches on 
ATR (Automatic Term Recognition) have 
achieved good results. However, there are 
scopes to improve the performance in 
extracting terms still further. For example, 
domain dictionaries can improve the 
performance in ATR. This paper focuses on 
a method for extracting terms using a 
dictionary hierarchy. Our method produces 
relatively good results for this task. 
Introduction 
In recent years, statistical approaches on ATR 
(Automatic Term Recognition) (Bourigault, 
1992; Dagan et al 1994; Justeson and Katz, 
1995; Frantzi, 1999) have achieved good results. 
However, there are scopes to improve the 
performance in extracting terms still further. For 
example, the additional technical dictionaries 
can be used for improving the accuracy in 
extracting terms. Although, the hardship on 
constructing an electronic dictionary was major 
obstacles for using an electronic technical 
dictionary in term recognition, the increasing 
development of tools for building electronic 
lexical resources makes a new chance to use 
them in the field of terminology. From these 
endeavour, a number of electronic technical 
dictionaries (domain dictionaries) have been 
acquired.  
Since newly produced terms are usually made 
out of existing terms, dictionaries can be used as 
a source of them. For example, ?distributed 
database? is composed of ?distributed? and 
?database? that are terms in a computer science 
domain. Further, concepts and terms of a domain 
are frequently imported from related domains. 
For example, the term ?Geographical 
Information System (GIS)? is used not only in a 
computer science domain, but also in an 
electronic domain. To use these properties, it is 
necessary to build relationships between 
domains. The hierarchical clustering method 
used in the information retrieval offers a good 
means for this purpose. A dictionary hierarchy 
can be constructed by the hierarchical clustering 
method. The hierarchy helps to estimate the 
relationships between domains. Moreover the 
estimated relationships between domains can be 
used for weighting terms in the corpus. For 
example, a domain of electronics may have a 
deep relationship to that of computer science. As 
a result, terms in the dictionary of electronics 
domain have a higher probability to be terms of 
computer science domain than terms in the 
dictionary of others do (Felber, 1984).  
The recent works on ATR identify the 
candidate terms using shallow syntactic 
information and score the terms using statistical 
measure such as frequency. The candidate terms 
are ranked by the score and are truncated by the 
thresholds. However, the statistical method 
solely may not give accurate performance in 
case of small sized corpora or very specialized 
domains, where the terms may not appear 
repeatedly in the corpora. 
In our approach, a dictionary hierarchy is 
used to avoid these limitations. In the next 
section, we describe the overall method 
description. In section 2, section 3, and section 4, 
we describe primary methods and its details. In 
section 5, we describe experiments and results 
1 Method Description 
 
The description of the proposed method is 
shown in figure 1. There are three main steps in 
our method. In the first stage, candidate terms 
that are complex nominal are extracted by a 
linguistic filter and a dictionary hierarchy is 
constructed. In the second stage, candidate terms 
are scored by each weighting scheme. In 
dictionary weighing scheme, candidate terms are 
scored based on the kind of domain dictionary 
where terms appear. In statistical weighting 
scheme, terms are scored by their frequency in 
the given corpus. In transliterated word 
weighting scheme, terms are scored by the 
number of transliterated foreign words in the 
terms. In the third stage, each weight is 
normalized and combined to Term weight 
(Wterm), and terms are extracted by Term weight.   
Figure 1. The method description 
2 Dictionary Hierarchy 
2.1 Resource 
Field 
Agrochemical, Aerology, Physics, Biology, 
Mathematics, Nutrition, Casting, Welding, 
Dentistry, Medical, Electronical engineering, 
Computer science, Electronics, Chemical 
engineering, Chemistry.... and so on. 
Table 1. The fragment of a list: dictionaries of 
domains used for constructing the hierarchy. 
A dictionary hierarchy is constructed using 
bi-lingual dictionaries (English to Korean) of the 
fifty-seven domains. Table 1 lists the domains 
that are used for constructing the dictionary 
hierarchy. The dictionaries belong to domains of 
science and technology. Moreover, terms that do 
not appear in any dictionary (henceforth we call 
them unregistered terms) are complemented by a 
domain tagged corpus. We use a corpus, called 
ETRI-KEMONG test collection, with the 
documents of seventy-six domains to 
complement unregistered terms and to eliminate 
common term.  
2.2 Constructing Dictionary Hierarchy  
The clustering method is used for constructing 
a dictionary hierarchy. The clustering is a 
statistical technique to generate a category 
structure using the similarity between 
documents (Anderberg, 1973). Among the 
clustering methods, a reciprocal nearest 
neighbor (RNN) algorithm (Murtaugh, 1983) 
based on a hierarchical clustering model is used, 
since it joins the cluster minimizing the increase 
in the total within-group error sum of squares at 
each stage and tends to make a symmetric 
hierarchy (Lorr, 1983). The algorithm to form a 
cluster can be described as follows:  
 
1. Determine all inter-object (or 
inter-dictionary) dissimilarity. 
2. Form cluster from two closest objects 
(dictionaries) or clusters. 
3. Recalculate dissimilarities between new 
cluster created in the step2 and other 
object (dictionary) or cluster already 
made. (all other inter-point dissimilarities 
are unchanged). 
4. Return to Step2, until all objects 
(including cluster) are in the one cluster. 
 
In the algorithm, all objects are treated as a 
vector such as Di = (xi1, xi2, ... , xiL ). In the step 
1, inter-object dissimilarity is calculated based 
on the Euclidian distance. In the step2, the 
closest object is determined by a RNN. For 
given object i and object j, we can define that 
there is a RNN relationship between i and j 
when the closest object of i is object j and the 
closest object of j is object i. This is the reason 
why the algorithm is called a RNN algorithm. A 
dictionary hierarchy is constructed by the 
algorithm, as shown in figure 2. There are ten 
domains in the hierarchy ? this is a fragment of 
whole hierarchy. 
 
Technical
Dictionaries
Domain 
tagged
Documents 
?.A CB D ?.
Constructing  
hierarchy
POS-tagged
Corpus Linguistic filter
Abbreviation and
Translation pairs
extraction
Candidate term
Frequency based
Weighing
Transliterated
Word detection
Transliterated word
Based Weighting
Complement 
Unregistered Term
Scoring by hierarchy
Eliminate
Common Word
Dictionary based 
Weighting
Statistical
Weight
Transliterated
Word Weight
Dictionary
Weight
Term Recognition
 
Figure 2. The fragment of whole dictionary 
hierarchy : The hierarchy shows that domains 
clustered in the terminal node such as chemical 
engineering and chemistry are highly related. 
2.3 Scoring Terms Using Dictionary 
Hierarchy 
The main idea for scoring terms using the 
hierarchy is based on the premise that terms in 
the dictionaries of the target domain and terms 
in the dictionary of the domain related to the 
target domain act as a positive indicator for 
recognizing terms. Terms in the dictionaries of 
the domains that are not related to the target 
domain act as a negative indicator for 
recognizing terms. We apply the premise for 
scoring terms using the hierarchy. There are 
three steps to calculate the score. 
 
1. Calculating the similarity between the 
domains using the formula (2.1) (Maynard 
and Ananiadou, 1998) 
 
where  
Depthi: the depth of the domaini node in the 
hierarchy 
Commonij: the depth of the deepest node 
sharing between the domaini and the 
domainj in the path from the root. 
 
In the formula (2.1), the depth of the node 
is defined as a distance from the root ? the 
depth of a root is 1. For example, let the 
parent node of C1 and C8 be the root of 
hierarchy in figure 2. The similarity between 
?Chemistry? and ?Chemical engineering? is 
calculated as shown below in table 2: 
 
Domain Chemistry Chemical 
Engineering 
Path from 
the root 
Root->C8-> 
C9->Chemistry 
Root->C8->C9-> 
Chemical 
Engineering 
Depthi 4 4 
Common ij 3 3 
Similarity 
ij 
2*3/(4+4) =0.75 2*3/(4+4) =0.75 
Table 2. Similarityij  calculation: The table shows 
an example in caculating similarity using formula 
(2.1). In the example, Chemical engineering 
domain and Chemistry domain are used. Path, 
Depth, and Common are calculated according to 
figure 1. Then similarity between domains are 
determined to 0.75. 
2.Term scoring by distance between a target 
domain and domains where terms appear: 
 
  
where  
N: the number of dictionaries where a 
term appear  
Similarityti: the similarity between the 
target domain and the domain dictionary 
where a term appears  
 
For example, in figure 2, let the target 
domain be physics and a term ?radioactive? 
appear in physics, chemistry and astronomy 
domain dictionaries. Then similarity between 
physics and the domains where the term 
?radioactive? appears can be estimated by 
formula (2.1) as shown below. Finally, 
Score(radioactive) is calculated by formula 
(2.2) ? score is (0.4+1+0.7)/3.:  
 
N 3 
similarity physics-chemistry 0.4 
similarity physics-physics 1 
similarity physics-astronomy 0.7 
Score(radioactive) 2.1*1/3 = 0.7  
Table 3. Scoring terms based on similarity 
between domains 
 
3. Complementing unregistered terms and 
common terms by domain tagged corpora.  
 
)1.2(2
ji
ij
ij depthdepth
Commonsimilarity
+
?
=
)2.2(1)(
1
?
=
=
N
i
tisimilarityNtermScore
 
where 
W: the number of words in the term ??? 
dofi: the number of domain that words in 
the term appear in the domain tagged 
corpus. 
 
Consider two exceptional possible cases. First, 
there are unregistered terms that are not 
contained in any dictionaries. Second, some 
commonly used terms can be used to describe a 
special concept in a specific domain dictionary.  
Since an unregistered term may be a newly 
created term of domains, it should be considered 
as a candidate term. In contrast with an 
unregistered term, common terms should be 
eliminated from candidate terms. Therefore, the 
score calculated in the step 2 should be 
complemented for these purposes. In our method, 
the domain tagged corpus (ETRI 1997) is used. 
Each word in the candidate terms ? they are 
composed of more than one word ? can appear 
in the domain tagged corpus. We can count the 
number of domains where the word appears. If 
the number is large, we can determine that the 
word have a tendency to be a common word. If 
the number is small, we can determine that the 
word have a high probability to be a valid term. 
In this paper, the score calculated by the 
dictionary hierarchy is called Dictionary Weight 
(WDic). 
3. Statistical Method 
The statistical method is divided into two 
elements. The first element, the Statistical 
Weight, is based on the frequencies of terms. 
The second element, the Transliterated word 
Weight, which is based on the number of 
transliterated foreign word in the candidate term. 
This section describes the above two elements.  
3.1. Statistical Weight: Frequency Based 
Weight 
In the Statistical Weight, not only 
abbreviation pairs and translation pairs in a 
parenthetical expression but also frequencies of 
terms are considered. Abbreviation pairs and 
translation pairs are detected using the following 
simple heuristics: 
 
For a given parenthetical expression A(B), 
1. Check on a fact that A and B are 
abbreviation pairs. The capital letter of A is 
compared with that of B. If the half of the 
capital letter are matched for each other 
sequentially, A and B are determined to 
abbreviation pairs (Hisamitsu et. al, 1998). 
For example, ?ISO? and ?International 
Standardization Organization? is detected as 
an abbreviation in a parenthetical expression 
?ISO (International Standardization 
Organization)?. 
 
2. Check on a fact that A and B are translation 
pairs. Using the bi-lingual dictionary, it is 
determined. 
 
After detecting abbreviation pairs and 
translation pairs, the Statistical Weight (WStat) of 
the terms is calculated by the formula (3.1). 
 
where  
?: a candidate term 
|?|: the length of a term??? 
S (?): abbreviation and translation pairs of 
??? 
T(?): The set of candidate terms that nest 
??? 
f(?): the frequency of ?? ? 
C(T(?)): The number of elements in T(?) 
 
In the formula (3.1), the nested relation is 
defined as follows: let A and B be a candidate 
term. If A contains B, we define that A nests B.  
The formula implies that abbreviation pairs 
and translation pairs related to ??? is counted as 
well as ??? itself and productivity of words in 
the nested expression containing ??? gives more 
weight, when the generated expression contains 
???. Moreover, formula (1) deals with a single- 
word term, since an abbreviation such as GUI 
(Graphical User Interface) is single word term 
and English multi-word term usually translated 
to Korean single-word term ? (e.g. distributed 
database => bunsan deitabeisu) 
)3.2(*)1)(()( 1W
dof
ScoreW
W
i
i
Dic
?
=+= ??
( )
??
?
?
??
?
?
?
??
?
?
?
??
?
?
?
??
?
?
?
??
?
?
?
+?
?
=
?
?
?
??
?
??
}{)(
)(
}{)(
)1.3())((
)(
)(
)(
)(
???
??
???
?
?
??
???
?
S
T
S
Stat
otherwiseTC
f
f
nestedisiff
W
3.2 Transliterated word Weight: By 
Automatic Extraction of Transliterated 
words 
Technical terms and concepts are created in 
the world that must be translated or transliterated. 
Transliterated terms are one of important clues 
to identify the terms in the given domain. We 
observe dictionaries of computer science and 
chemistry domains to investigate the 
transliterated foreign words. In the result of 
observation, about 53% of whole entries in a 
dictionary of a computer science domain are 
transliterated foreign words and about 48% of 
whole entries in a dictionary of a chemistry 
domain are transliterated foreign words. Because 
there are many possible transliterated forms and 
they are usually unregistered terms, it is difficult 
to detect them automatically.  
In our method, we use HMM (Hidden Markov 
Model) for this task (Oh, et al, 1999). The main 
idea for extracting a foreign word is that the 
composition of foreign words would be different 
from that of pure Korean words, since the 
phonetic system for the Korean language is 
different from that of the foreign language. 
Especially, several English consonants that 
occur frequently in English words, such as 
?p?, ?t?, ?c?, and ?f?, are transliterated into Korean 
consonants ?p?, ?t?, ?k?, and ?p? respectively. 
Since these consonants of Korean are not used in 
pure Korean words frequently, this property can 
be used as an important clue for extracting a 
foreign word from Korean. For example, in a 
word, ?si-seu-tem? (system), the syllable ?tem? 
have a high probability to be a syllable of 
transliterated foreign word, since the consonant 
of ?t? in the syllable ?tem? is usually not used in 
a pure Korean word. Therefore, the consonant 
information which is acquired from a corpus can 
be used to determine whether a syllable in the 
given term is likely to be the part of a foreign 
word or not.  
Using HMM, a syllable is tagged with ?K? or 
?F?. A syllable tagged with ?K? means that it is 
part of a pure Korean word. A syllable tagged 
with ?F? means that it is part of a transliterated 
word. For example, ?si-seu-tem-eun (system is)? 
is tagged with  ?si/F + seu/F + tem/F + eun/K?. 
We use consonant information to detect a 
transliterated word like lexical information in 
part-of-speech-tagging. The formula (3.2) is 
used for extracting a transliterated word and the 
formula (3.3) is used for calculating the 
Transliterated Word Weight (WTrl). The formula 
(3.3) implies that terms have more transliterated 
foreign words than common words do. 
 
where  
si: i-th consonant in the given word. 
ti: i-th tag (?F? or ?K?) of the syllable in the 
given word. 
 
 
where  
|?| is the number of words in the term ? 
trans(?) is the number of transliterated 
words in the term ? 
4.Term Weighting 
The three individual weights described above 
are combined according to the following 
formula (4.1) called Term Weight (WTerm) for 
identifying the relevant terms.  
 
Where 
?: a candidate term ??? 
f,g,h : normalization function 
?+?+? = 1 
 
In the formula (4.1), the three individual 
weights are normalized by the function f, g, and 
h respectively and weighted parameter ?,?, and 
?. The parameter ?,?, and ? are determined by 
experiment with the condition ?+?+? = 1. Each 
value which is used in this paper is ?=0.6, ? 
=0.1, and ?=0.3 respectively. 
 
)3.3()()(
?
?
?
transWTrl =
)2.3()|(),|(
)|()()()|(
13
21
121
??
???
???
???
?
=
??
==
??
n
i
ii
n
i
iii tsptttp
ttptpSPSTP
)1.4())(())((
))(()(
????
???
StatTrl
Dicterm
WhWg
WfW
?+?
+?=
5. Experiment 
The proposed method is tested on a corpus of 
computer science domains, called the KT test 
collection. The collection contains 4,434 
documents and 67,253 words and contains 
documents about the abstract of the paper (Park. 
et al, 1996). It was tagged with a part-of-speech 
tagger for evaluation. We examined the 
performance of the Dictionary Weight (WDic) to 
show its usefulness. Moreover, we examined 
both the performance of the C-value that is 
based on the statistical method (Frantzi. et al, 
1999) and the performance of the proposed 
method. 
5.1 Evaluation Criteria 
Two domain experts manually carry out the 
assessment of the list of terms extracted by the 
proposed method. The results are accepted as the 
valid term when both of the two experts agree on 
them. This prevents the evaluation from being 
carried out subjectively, when one expert 
assesses the results. The results are evaluated by 
a precision rate. A precision rate means that the 
proportion of correct answers to the extracted 
results by the system. 
5.2 Evaluation by Dictionary Weight 
(WDic) 
In this section, the evaluation is performed 
using only WDic to show the usefulness of a 
dictionary hierarchy to recognize the relevant 
terms The Dictionary Weight is based on the 
premise that the information of the target 
domain is a good indicator for identifying terms. 
The term in the dictionaries of the target domain 
and the domain related to the target domain acts 
as a positive indicator for recognizing terms. 
The term in the dictionaries of the domains, 
which are not related to the target domain acts as 
a negative indicator for recognizing terms. The 
dictionary hierarchy is constructed to estimate 
the similarity between one domain and another. 
 
 Top 10% Bottom 10% 
The Valid Term 94% 54.8% 
Non-Term 6% 45.2% 
Table 4.  terms and non-terms by Dictionary 
Weight 
The result, depicted in table 4, can be 
interpreted as follows: In the top 10% of the 
extracted terms, 94% of them are the valid terms 
and 6% of them are non-terms. In the bottom 
10% of the extracted terms, 54.8% of them are 
the valid terms and 45.2% of them are non-terms. 
This means that the relevant terms are much 
more than non-terms in the top 10% of the result, 
while non-terms are much more than the 
relevant terms in the bottom 10% of the result.  
 
The results are summarized as follow:  
 
!"According as a term has a high 
Dictionary Weight (WDic), it is apt 
to be valid. 
!"More valid terms have a high 
Dictionary Weight (WDic) than 
non-terms do 
 
5.3 Overall Performance 
Table 5 and figure 3 show the performance of 
the proposed method and of the C-value method. 
By dividing the ranked lists into 10 equal 
sections, the results are compared. Each section 
contains the 1291 terms and is evaluated 
independently. 
 
C-value The proposed 
method 
Section # of 
term 
Precision # of 
term 
Precision 
1 1181 91.48% 1241 96.13% 
2 1159 89.78% 1237 95.82% 
3 1207 93.49% 1213 93.96% 
4 1192 92.33% 1174 90.94% 
5 1206 93.42% 1154 89.39% 
6 981 75.99% 1114 86.29% 
7 934 72.35% 1044 80.87% 
8 895 69.33% 896 69.40% 
9 896 69.40% 780 60.42% 
10 578 44.77% 379 29.36% 
Table 5.  Precision rates of C-value and the 
proposed method : Section contain 1291 terms and 
precision is evaluated independently. For example, 
in section 1, since there are 1291 candidate terms 
and 1241 relevant terms by the proposed method, 
the precision rate in section 1 is 96.13% . 
The result can be interpreted as follows. In the 
top sections, the proposed method shows the 
higher precision rate than the C-value does. The 
distribution of valid terms is also better for the 
proposed method, since there is a downward 
tendency from section 1 to section 10. This 
implies that the terms with higher weight scored 
by our method have a higher probability to be 
valid terms. Moreover, the precision rate of our 
method shows the rapid decrease from section 6 
to section 10. This indicates that most of valid 
terms are located in the top sections. 
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Section
Pre
cis
ion
The Proposed method C-value
Figure 2. The performance of C-value and the 
proposed method in each section 
The results can be summarized as follow : 
 
!"The proposed method extracts a valid 
term more accurate than C-value does. 
!"Most of the valid terms are in the top 
section extracted by the proposed 
method. 
Conclusion 
In this paper, we have described a method for 
term extraction using a dictionary hierarchy. It is 
constructed by clustering method and is used for 
estimating the relationships between domains. 
Evaluation shows improvement over the C-value. 
Especially, our approach can distinguish the 
valid terms efficiently ? there are more valid 
terms in the top sections and less valid terms in 
the bottom sections. Although the method 
targets Korean, it can be applicable to English 
by slight change on the Tweight (WTrl).  
However, there are many scopes for further 
extensions of this research. The problems of 
non-nominal terms (Klavans and Kan, 1998), 
term variation (Jacquemin et al, 1997), and  
relevant contexts (Maynard and Ananiadou, 
1998), can be considered for improving the 
performance. Moreover, it is necessary to apply 
our method to practical NLP systems, such as an 
information retrieval system and a 
morphological analyser. 
Acknowledgements 
KORTERM is sponsored by the Ministry of Culture 
and Tourism under the program of King Sejong 
Project. Many fundamental researches are supported 
by the fund of Ministry of Science and Technology 
under a project of plan STEP2000. And this work 
was partially supported by the KOSEF through the 
?Multilingual Information Retrieval? project at the 
AITrc. 
References  
Anderberg, M.R. (1973) Cluster Analysis for 
Applications. New York: Academic 
Bourigault, D. (1992) Surface grammatical analysis 
for the extraction of terminological noun phrases. 
In Proceedings of the 14th International Conference 
on Computational Linguistics, COLING?92 pp. 
977-981. 
Dagan, I. and K. Church. (1994) Termight: 
Identifying and terminology In Proceedings of the 
4th Conference on Applied Natural Language 
Processing, Stuttgart/Germany, 1994. Association 
for Computational Linguistics. 
ETRI (1997) Etri-Kemong set 
Felber Helmut (1984) Terminology Manual, 
International Information Centre for Terminology 
(Infoterm) 
Frantzi, K.T. and S.Ananiadou (1999) The 
C-value/NC-value domain independent method for 
multi-word term extraction. Journal of Natural 
Language Processing, 6(3) pp. 145-180 
Hisamitsu, Toru and Yoshiki Niwa (1998) Extraction 
of useful terms from parenthetical expressions by 
using simple rules and statistical measures. In First 
Workshop on Computational Terminology 
Computerm?98, pp 36-42 
Jacquemin, C., Judith L.K. and Evelyne, T. (1997) 
Expansion of Muti-word Terms for indexing and 
Retrieval Using Morphology and Syntax, 35th 
Annual Meeting of the Association for 
Computational Linguistics, pp 24-30 
Justeson, J.S. and S.M. Katz (1995) Technical 
terminology : some linguistic properties and an 
algorithm for identification in text. Natural 
Language Engineering, 1(1) pp. 9-27  
Klavans, J. and Kan M.Y (1998) Role of Verbs in 
Document Analysis, In Proceedings of the 17th 
International Conference on Computational 
Linguistics, COLING?98 pp. 680-686. 
Lauriston, A. (1996) Automatic Term Recognition : 
performance of Linguistic and Statistical 
Techniques. Ph.D. thesis, University of Manchester 
Institute of Science and Technology. 
Lorr, M. (1983) Cluster Analysis and Its Application, 
Advances in Information System Science,8 , 
pp.169-192 
Murtagh, F. (1983) A Survey of Recent Advances in 
Hierarchical Clustering Algorithms, Computer 
Journal, 26, 354-359 
Maynard, D. and Ananiadou, S. (1998) Acquiring 
Context Information for Term Disambiguation In 
First Workshop on Computational Terminology 
Computerm?98, pp 86-90 
Oh, J.H. and K.S. Choi (1999) Automatic extraction 
of a transliterated foreign word using hidden 
markov model , In Proceedings of the 11th Korean 
and Processing of Korean Conference pp. 137-141 
(In Korean). 
Park, Y.C., K.S. Choi, J.K.Kim and Y.H. Kim (1996). 
Development of the KT test collection for 
researchers in information retrieval. In the 23th 
KISS Spring Conference (in Korean) 
  
Implicit Ambiguity Resolution Using Incremental Clustering in 
Korean-to-English Cross-Language Information Retrieval 
 
Kyung-Soon Lee1, Kyo Kageura1, Key-Sun Choi2 
1 NII (National Institute of Informatics) 
2-1-2 Hitotsubashi, Chiyoda-ku, 
Tokyo, 101-8430, Japan 
{kslee, kyo}@nii.ac.jp 
2 Division of Computer Science, KAIST 
373-1 Kusung Yusong 
Daejeon, 305-701, Korea 
kschoi@cs.kaist.ac.kr 
 
Abstract  
This paper presents a method to implicitly 
resolve ambiguities using dynamic 
incremental clustering in Korean-to-English 
cross-language information retrieval. In the 
framework we propose, a query in Korean is 
first translated into English by looking up 
Korean-English dictionary, then documents 
are retrieved based on the vector space 
retrieval for the translated query terms. For 
the top-ranked retrieved documents, 
query-oriented document clusters are 
incrementally created and the weight of each 
retrieved document is re-calculated by using 
clusters. In experiment on TREC-6 CLIR 
test collection, our method achieved 28.29% 
performance improvement for translated 
queries without ambiguity resolution for 
queries. This corresponds to 97.27% of the 
monolingual performance for original 
queries. When we combine our method with 
query ambiguity resolution, our method 
even outperforms the monolingual retrieval. 
1 Introduction 
This paper describes a method of applying 
dynamic incremental clustering to the implicit 
resolution of query ambiguities in 
Korean-to-English cross-language information 
retrieval. The method uses the clusters of 
retrieved documents as a context for 
re-weighting each retrieved document and for 
re-ranking the retrieved documents. 
Cross-language information retrieval (CLIR) 
enables users to retrieve documents written in a 
language different from a query language. The 
methods used in CLIR fall into two categories:  
statistical approaches and translation approaches. 
Statistical methods establish cross-lingual 
associations without language translation 
(Dumais et al 1997; Rehder et al 1997; Yang et 
al, 1998). They require large-scale bilingual 
corpora. In translation approach, either queries 
or documents are translated. Though document 
translation is possible when high quality 
machine translation systems are available (Kwon 
et al 1997; Oard and Hackett, 1997), it is not 
very practical. Query translation methods (Hull 
and Grefenstette, 1996; Davis, 1996; Eichmann 
et al 1998; Yang et al 1998; Jang et al 1999; 
Chun, 2000) based on bilingual dictionaries, 
multilingual ontology or thesaurus are much 
more practical. Many researches adopt 
dictionary-based query translation because it is 
simpler and practical, given the wide availability 
of bilingual or multilingual dictionaries. In order 
to achieve a high performance CLIR using 
dictionary-based query translation, however, it is 
necessary to solve the problem of increased 
ambiguities of query terms. One way of 
resolving query ambiguities is to use the 
statistics, such as mutual information (Church 
and Hanks, 1990), to measure associations of 
query terms, on the basis of existing corpora 
(Jang et al 1999). 
Document clusters, widely adopted in various 
applications such as browsing and viewing of 
document results (Hearst and Pedersen, 1996) or 
topic detection (Allan et al 1998), also reflect 
the association of terms and documents. Lee et 
al (2001) showed that incorporating a document 
re-ranking method based on document clusters 
into the vector space retrieval achieved the 
significant improvement in monolingual IR, as it 
contributed to resolving ambiguities caused by 
polysemous query terms. 
The noise or ambiguity produced by 
dictionary-based query translation in CLIR is 
much larger than the polysemous ambiguities in 
monolingual IR. For example, a Korean term 
???[eun-haeng]? is a polysemous term with 
two meanings: ?bank? and ?ginkgo?. The English 
term ?bank? itself is polysemous, so the 
translated query ends up having magnified 
ambiguities. We will show that the method we 
propose, i.e. implicit ambiguity resolution using 
incremental clustering, is highly effective in 
dealing with the increased query ambiguities in 
CLIR. 
2 Implicit ambiguity resolution using 
incremental clustering 
Figure 1 shows the overall architecture of our 
system which incorporates implicit ambiguity 
resolution method based on query-oriented 
document clusters. In the system, a query in 
Korean is first translated into English by looking 
up dictionaries, and documents are retrieved 
based on the vector space retrieval for the 
translated query. For the top-ranked retrieved 
documents, document clusters are incrementally 
created and the weight of each retrieved 
document is re-calculated by using clusters with 
preference. This phase is the core of our implicit 
ambiguity resolution method. Below, we will 
describe each module in the system. 
2.1 Dictionary-based query translation and 
ambiguities 
Queries are written in natural language in 
Korean. We first apply morphological analysis 
and part-of-speech (POS) tagging to a query, 
and select keywords based on the POS 
information. For each keyword, we look up 
Korean-English dictionaries, and all the English 
translations in the dictionaries are chosen as 
query terms. We used a general-purpose 
bilingual dictionary and technical bilingual 
dictionaries (Chun, 2000). All in all, they have 
282,511 Korean entries and 505,003 English 
translations. 
Since a term can have multiple translations, 
the list of translated query terms can contain 
terms of different meanings as well as synonyms. 
While synonyms can improve retrieval 
effectiveness, terms with different meanings 
produced from the same original term can 
degrade retrieval performance tremendously. 
At this stage, we can apply statistical 
ambiguity resolution method based on mutual 
information. In the experiment below, we will 
examine two cases, i.e. with and without 
ambiguity resolution at this stage. 
2.2 Document retrieval based on vector space 
retrieval model 
For the query, documents are retrieved based on 
the vector space retrieval method. This method 
simply checks the existence of query terms, and 
calculates similarities between the query and 
documents. The query-document similarity of 
each document is calculated by vector inner 
product of the query and document vectors: 
di
t
i
qi wwdqsimD ?= ?
=1
),(              (1) 
where query and document weight, qiw and diw , 
are calculated by ntc-ltn weighting scheme  
which yields the best retrieval result in Lee et al
(2001) among several weighting schemes used 
in SMART system (Salton, 1989). 
As the translated query can contain noises, 
non-relevant documents may have higher ranks 
than relevant documents. 
Figure 1. System architecture of implicit 
ambiguity resolution by incremental clustering. 
English Query with ambiguities
TREC AP-news collection
Korean-English 
Dictionaries
Korean Query
retrieved top N docs
Dictionary-based
Query Translation
Vector Space Retrieval
Each  document view
re-ranked results
?
Incremental Clusters
Document context view
Reflecting context
to each document 
2.3 Query-oriented incremental clustering for 
implicit ambiguity resolution 
In order to exclude non-relevant documents 
from higher ranks, we take top N documents to 
create clusters incrementally and dynamically, 
and use similarities between the clusters and the 
query to re-rank the documents. Basic idea is: 
Each cluster created by clustering of retrieved 
documents can be seen as giving a context of the 
documents belonging to the cluster; by 
calculating the similarity between each cluster 
and the query, therefore, we can spot the 
relevant context of the query; documents that 
belong to more relevant context or cluster are 
likely to be relevant to the query. 
It should be noted here that the static global 
clustering is not practical in the current setup, 
because it takes much computational time and 
the document space is too sparse (see Anick and 
Vaithyanathan (1997) for the comparison of 
static and dynamic clustering). 
2.3.1 Dynamic incremental centroid clustering 
We make clusters based on incremental centroid 
method. There are a few variations in the 
agglomerative clustering method. The 
agglomerative centroid method joins the pair of 
clusters with the most similar centroid at each 
stage (Frakes and Baeza-Yates, 1992).  
Incremental centroid clustering method is 
straightforward. The input document of 
incremental clustering proceeds according to the 
ranks of the top-ranked N documents resulted 
from vector space retrieval for a query. 
Document and cluster centroid are represented 
in vectors. For the first input document (rank 1), 
create one cluster whose member is itself. For 
each consecutive document (rank 2, ..., N), 
compute cosine similarity between the document 
and each cluster centroid in the already created 
clusters. If the similarity between the document 
and a cluster is above a threshold, then add the 
document to the cluster as a member and update 
cluster centroid. Otherwise, create a new cluster 
with this document. Note that one document can 
be a member of several clusters as shown in 
Figure 2 (sold lines show that the document 
belongs to the cluster). 
2.3.2 Cluster preference 
Similarities between the clusters and the query, 
or query-cluster similarities, are calculated by 
the combination of the query inclusion ratio and 
vector inner product between the query vector 
and the centroid vectors of the clusters. 
ci
t
i
qi
q wwq
ccqsimC ??= ?
=1
),(          (2) 
where |q| is the number of terms in the query, 
|cq| is the number of query terms included in a 
cluster centroid, |cq|/|q| is the query inclusion 
ratio for the cluster. The documents included in 
the same cluster have the same query-cluster 
similarity. 
Cluster preferences are influenced by the 
query inclusion ratio, which prefers the cluster 
whose centroid includes more various query 
terms. Thus incorporating this information into 
the weighting of each document means adding 
information which is related to the behavior of 
terms in documents as well as the association of 
terms and documents into the evaluation of the 
relevance of each document; it therefore has the 
effect of ambiguity resolution. 
2.4 Reflecting cluster information to the 
documents 
Using the query-cluster similarity, we 
re-calculate the relevance of each document 
according to the following equation: 
 
),(),(),( cqsimCMAXdqsimDdqsim cd??= (3) 
 
where simD(q,d) is a query-document similarity 
by vector space retrieval as defined in equation 
(1) and simC(q,c) is a query-cluster similarity of 
a document d defined in equation (2). Since each 
document can be a member of several clusters, 
we assign the highest query-cluster similarity 
value to the document. The new document 
similarity, sim(q,d), is calculated by 
multiplication of a query-cluster similarity and a 
query-document similarity. Based on this new 
Figure 2. Incremental centroid clustering in order 
of the top-ranked N documents 
 
similarity sim(q,d), we re-rank the retrieved 
documents. In the equation, we tried to use 
weighted sum of a query-document similarity 
and a query-cluster similarity. The combination 
by multiplication showed better performances 
than that of weighted sum. 
Through this procedure, we can effectively 
take into account the contexts of all the terms in 
a document as well as of the query terms. Thus, 
even if a document which has a low 
query-document similarity can have a high 
query-cluster similarity thanks to the effect of 
neighboring documents in the same cluster. The 
reverse can be true as well. 
3 Experiments  
3.1 Experimental environment 
We evaluated our method on TREC-6 CLIR test 
collection which contains 242,918 English 
documents (AP news from 1988 to 1990) and 24 
English queries. English queries are translated to 
Korean queries manually. We use title field of 
queries which consist of three fields such as title, 
description and narrative. 
In dictionary-based query translation, one 
query term has multiple translations. Table 3 
shows the degree of ambiguities. 
The number of Korean query terms 47 
The number of translated terms 149 
The average number of translations 3.2 
Table 1. The degree of ambiguities for 24 queries. 
In our experiment, we only use 14 queries 
which consist of more than one term to observe 
real effects of our method. This is because, if a 
query consists of more than one term, human 
can select the correct meaning of the term by its 
neighbours. But if a query consists of one term 
such as ?bank? and it is polysemous, no one can 
resolve ambiguities without considering 
additional external information. The rest 10 
queries which consist of one term are used to 
decide a threshold in incremental clustering. 
We use SMART system (Salton, 1989) 
developed at Cornell as a vector space retrieval. 
3.2 Results 
The retrieval effectiveness was evaluated using 
the 11-point average precision metric. 
We compared our method with original 
English queries, with translated queries with 
ambiguities, and with translated queries with the 
best translation after disambiguation. The 
followings are the brief descriptions for 
comparison methods: 
1) monolingual: the performance of vector 
space retrieval system for original English 
queries as the monolingual baseline. 
2) tall_base: the performance of vector space 
retrieval system for translated English 
queries which have all possible translations 
in bilingual dictionaries without ambiguity 
resolution. 
3) tall_rerank: the performance of proposed 
method using dynamic incremental clusters 
for the retrieved documents of tall_base. 
4) tone_base: the performance of vector space 
retrieval system for translated queries with 
the best translations for each query term 
after ambiguity resolution based on mutual 
information. 
5) tone_rerank: the performance of proposed 
method using dynamic incremental clusters 
for the retrieved documents of tone_base. 
 
?tall_rerank? and ?tone_rerank? use our 
implicit disambiguation method. The number of 
top N documents used in dynamic incremental 
clustering is 300 and thresholds for incremental 
centroid clustering are set as 0.41 which are 
learned from training 10 queries with one term 
in both tall_rerank and tone_rerank. 
The main objective of this paper is to observe 
the performance change by incremental clusters 
for translated queries with ambiguities (tall_base 
and tall_rerank). 
 
Comparison 11-pt avg. C/M Change 
 precision (%) (%) 
1) monolingual 0.2858 100 - 
2) tall_base 0.2167 75.82 - 
3) tall_rerank 0.2780 97.27 +28.29 
4) tone_base 0.2559 89.54 - 
5) tone_rerank 0.3026 105.87 +18.25 
Table 2. The retrieval effectiveness for comparison 
methods. 
To observe the effect of clusters, we 
compared the results after disambiguation based 
on mutual information (tone_base and 
tone_rerank). We selected the best translation 
based on mutual information among all 
translation terms. Mutual information MI(x,y) is 
defined as following (Church and Hanks, 1990): 
)()(
),(log)()(
),(log),( 22 yfxf
yxfN
ypxp
yxpyxMI ?==  (4) 
where f(x) and f(y) are frequency of term x and  
term y, respectively. Co-occurrence frequency of 
term x and term y, f(x,y), is taken in window size 
6 for AP 1988 news documents. 
 
The 11-point average precision value, 
corresponding result to monolingual (C/M), and 
performance change are summarized in Table 2. 
The retrieval effectiveness of tall_rerank is 
0.2780, corresponding to 97.27% of 
monolingual performance. The performance of 
tone_rerank yields 0.3026 (105.87%). This is 
even better than the monolingual performance. 
The performance of our implicit ambiguity 
resolution method for all translations 
(tall_rerank) shows 8.63% improvement 
compared with that of ambiguity resolution 
based on mutual information (tone_base). The 
proposed method achieved 28% improvement 
for all translation queries and 18% for best 
translation queries compared with the vector 
space retrieval.  Our method after 
disambiguation (tone_rerank) using mutual 
information improved about 39.6% over vector 
space retrieval for all translations queries 
(tall_base). 
The cluster-based implicit disambiguation 
method, therefore, is more effective for 
performance improvement than the simple query 
disambiguation method based on mutual 
information; if used together, it shows yet 
further improvement. 
3.3 Result analysis 
We examined the effects of our method for a 
query with ambiguities increased after bilingual 
dictionary-based term translation. 
The Korean query is ????[ja-dong-cha] 
??[gong-gi] ??[o-yeom]? whose original 
English query is ?automobile air pollution?. The 
translated query with all the possible translations 
in Korean-English dictionaries for this query is 
as follows: 
In this query, the term ???? is polysemous 
which has several meanings such as <air>, 
<atmosphere>, <jackstone>, <co-occurrence>, 
and <bowl>. This is the cause of degrading 
system performance. 
 
146 clusters were created for the retrieved 300 
documents of this query. The token number of 
documents in the clusters was 435. The 
distribution of cluster members is shown in 
Figure 3. Most non-relevant documents had a 
tendency to make singleton cluster, and most 
relevant documents made large group clusters. 
 
We examined inside the clusters how to see 
cluster give effects to resolve ambiguity and 
reflect context. Cluster C4 in Figure 3 has 60 
members, which contains 56 relevant documents 
and 4 non-relevant documents, among 209 
relevant documents for this query. This cluster 
centroid includes following terms related to the 
query: 
car 0.069 
automobile 0.127 
air 0.082 
atmosphere 0.018 
pollution 0.196 
contamination 0.064 
 
??? 
[ja-dong-cha] 
car, automobile, autocar, 
motorcar 
?? 
[gong-gi] 
air, atmosphere, empty vessel, 
bowl, jackstone, pebble, marbles 
??[o-yeom]  contamination, pollution 
C2
C4 C17
0
10
20
30
40
50
60
70
80
90
100
0 20 40 60 80 100 120 140
cluster ID
# o
f m
em
ber
# o
f m
em
ber
Figure 3. The distribution of cluster members 
for the query with translation ambiguities. 
Although this centroid includes a noise term 
?atmosphere?, its weight is low. The other terms 
are appropriate to the query; they are synonyms. 
Since all of the query terms are included in the 
centroid, query inclusion ratio is 1 and all 
synonyms affect positively to the vector inner 
product value. Therefore, since this cluster 
preference is high, the ranks of all documents in 
this cluster changed higher. The cluster 
performed as a context of the documents 
relevant to the query. Cluster C85 is a singleton 
whose centroid includes one of three query 
terms: 
bowl 0.101 
marble 0.191 
Since query inclusion ratio is low, the cluster 
preference is low. Therefore this cluster?s effect 
is weak to the document. 
 
Figure 4 presents the rank changes, calculated 
by subtracting ranks by our method (tall_rerank) 
from those by vector space retrieval (tall_base) 
for each relevant document of the ambiguous 
query. The ranks of most documents are 
changed higher through cluster analysis, 
although the ranks of some documents are 
changed lower. Figure 5 shows recall/precision 
curves for the performances of original English 
query (monolingual; 0.6783 in 11-pt avg. 
precision), translated query without 
disambiguation (tall_base; 0.5635), and our 
method (tall_rerank; 0.6622). For increased 
query ambiguity, we could achieve 97.62% 
performance compared to the monolingual 
retrieval.  
These results indicate that cluster analysis 
help to resolve ambiguity. Thus, we could 
effectively take into account the context of all 
the terms in a document as well as the query 
terms. 
4 Conclusion 
We have proposed the method of applying 
dynamic incremental clustering to the implicit 
resolution of query ambiguities in 
Korean-to-English cross-language information 
retrieval. The method used the clusters of 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00.10.20.30.40.50.60.70.80.91.0
recall
pr
ec
isi
on
monolingual
tall_base
tall_rerank
 Figure 5. The performance comparison for the ambiguous query. 
Ra
nk
 ch
an
ge
s  
(ra
nk
 of
 ta
ll_
ba
se
  ?
ran
k o
f t
all
_re
ran
k )
-120
-80
-40
0
40
80
120
160
1 1
7 7
1 9
1 8
8
2 9
9 9
9
5 4
7 1
1
6 0
4 0
7
7 2
6 8
3
9 1
4 2
5
1 0
0 6
2 9
1 1
0 8
2 2
1 1
8 1
0 2
1 2
4 9
9 5
1 2
8 7
4 9
1 3
3 2
5 4
1 3
9 7
7 0
1 4
2 5
8 4
1 5
2 7
6 2
1 6
9 3
2 3
1 7
4 0
9 4
1 7
7 5
7 2
1 7
8 6
9 2
1 7
9 7
1 6
1 8
2 6
5 9
1 8
6 1
1 5
1 9
6 0
7 7
1 9
6 9
8 5
2 0
7 8
1 2
2 2
4 2
5 6
2 2
7 4
6 4
2 2
9 4
7 5
2 3
3 7
0 8
Relevant document ID for the query
Ra
nk
 ch
an
ge
s  
(ra
nk
 of
 ta
ll_
ba
se
  ?
ran
k o
f t
all
_re
ran
k )
 
Figure 4. The rank changes of tall_rerank from rank of tall_base for each relevant document of the query.  
retrieved documents as a context for 
re-weighting each retrieved document and for 
re-ranking the retrieved documents. 
Our method was evaluated on TREC-6 CLIR 
test collection. This method achieved 28.29% 
performance improvement for translated queries 
without ambiguity resolution. This corresponds 
to 97.27% of the monolingual performance. 
When our method was used with the query 
ambiguity resolution method based on mutual 
information, it showed 105.87% performance 
improvement of the monolingual retrieval. 
These results indicate that cluster analysis help 
to resolve ambiguity greatly, and each cluster 
itself provide a context for a query. 
Our method is a language independent model 
which can be applied to any language retrieval. 
We expect that our method will further 
improve the results, although further research is 
needed on combining a method to improve recall 
such as query expansion and relevance feedback. 
 
References 
Allan, J. Carbonell, J., Doddington, G. Yamron. J. 
and Yang, Y. (1998) Topic Detection and Tracking 
Pilot Study: Final Report. In Proc. of the DARPA 
Broadcast News Transcription and Understanding 
Workshop, pp.194-218. 
Anick, P.G. and Vaithyanathan, S. (1997) Exploiting 
Clustering and Phrases for Context-Based 
Information Retrieval. In Proc. of 20th ACM 
SIGIR Conference (SIGIR?97). 
Chun, J.H. (2000) Resolving Ambiguity and English 
Query Supplement using Parallel Corpora on 
Korean-English CLIR system. MS thesis, Dept. of 
Computer Science, KAIST (in Korean). 
Church, K.W. and Hanks P. (1990) Word Association 
Norms Mutual Information and Lexicography. 
Computational Linguistics, 16(1), pp.23-29.` 
Davis, M. (1996) New experiments in cross-language 
text retrieval at NMSU's computing research lab. In 
Proc. of the fifth Text Retrieval Conference 
(TREC-5). 
Dumais, S.T., Letsche, T.A., Littman, M.L. and 
Landauer, T.K. (1997) Automatic cross-language 
retrieval using latent semantic indexing. In Proc. of 
AAAI Symposium on Cross-Language Text and 
Speech Retrieval. 
Eichmann, D., Ruiz, M.E. and Srinivasan, P. (1998) 
Cross-Language Information Retrieval with the 
UMLS Metathesaurus. . In Proc. of the 21th ACM 
SIGIR Conference (SIGIR?98). 
Frakes, W.B., and Baeza-Yates, R. (1992) 
Information Retrieval: data structures & algorithms. 
New Jersey: Prentice Hall, pp.435-436. 
Gilarranz, J., Gonzalo, J. and Verdejo, F. (1997) An 
Approach to Conceptual Text Retrieval Using the 
EuroWordNet Multilingual Semantic Database. In 
Proc. of AAAI Spring Symposium on 
Cross-Language Text and Speech Retrieval. 
Hearst, M.A. and Pedersen, J.O. (1996) Reexamining 
the Cluster Hypothesis: Scatter/Gather on Retrieval 
Results. In Proc. of 19th ACM SIGIR Conference 
(SIGIR?96). 
Hull, D.A. and Grefenstette, G. (1996) Querying 
across languages: a dictionary-based approach to 
multilingual information retrieval. In Proc. of the 
19th ACM SIGIR Conference (SIGIR?96). 
Jang, M.G., Myaeng, S.H. and Park, S.H. (1999) 
Using Mutual Information to Resolve Query 
Translation Ambiguities and Query Term 
Weighting. In Proc. of the 37th Annual Meeting of 
the Association for Computational Linguistics. 
Kwon, O-W., Kang, I.S., Lee, J-H and Lee, G.B.  
(1997) Cross-Language Text Retrieval Based on 
Document Translation Using Japanese-to-Korean 
MT system. In Proc. of NLPRS'97, pp. 101-106. 
Lee, K.S., Park, Y.C., Choi, K.S. (2001) Re-ranking 
model based on document clusters. Information 
Processing and Management, 37(1), pp. 1-14. 
Oard, D.,W. and Hackett, P. (1997) Document 
Translation for the Cross-Language Text Retrieval 
at the University of Maryland. In  Proc. of the 
Sixth Text REtrieval Conference (TREC-6). 
Rehder, B., Littman, M.L., Dumais, S. and Landauer, 
T.K. (1997) Automatic 3-language cross-language 
information retrieval with latent semantic indexing. 
In Proc. of the Sixth Text REtrieval Conference 
(TREC-6). 
Salton, G. (1989) Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of 
Information by Computer. Addison-Wesley, 
Reading, Pennsylvania. 
Voorhees, E.M. (1986) Implementing agglomerative 
hierarchic clustering algorithms for use in 
document retrieval. Information Processing & 
Management, 22(6), pp. 465-476. 
Yang, Y., Carbonell, J.G., Brown, R.D. and 
Frederking, R.E. (1998) Translingual Information 
Retrieval: Learning from Bilingual Corpora. AI 
Journal special issue, pp. 323-345. 
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 340?349,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Contrasting Opposing Views of News Articles on Contentious Issues  
 
 
Souneil Park1, KyungSoon Lee2, Junehwa Song1   
1Korea Advanced Institute of  
Science and Technology 
2Chonbuk National  
University 
  
291 Daehak-ro, Yuseong-gu, 664-14 1ga Deokjin-dong Jeonju,   
Daejeon, Republic of Korea Jeonbuk, Republic of Korea   
{spark,junesong}@nclab.kaist.ac.kr selfsolee@chonbuk.ac.kr   
 
 
 
 
 
Abstract 
We present disputant relation-based meth-
od for classifying news articles on conten-
tious issues. We observe that the disputants 
of a contention are an important feature for 
understanding the discourse. It performs 
unsupervised classification on news articles 
based on disputant relations, and helps 
readers intuitively view the articles through 
the opponent-based frame. The readers can 
attain balanced understanding on the con-
tention, free from a specific biased view. 
We applied a modified version of HITS al-
gorithm and an SVM classifier trained with 
pseudo-relevant data for article analysis. 
1 Introduction 
The coverage of contentious issues of a community 
is an essential function of journalism. Contentious 
issues continuously arise in various domains, such 
as politics, economy, environment; each issue in-
volves diverse participants and their different com-
plex arguments. However, news articles are 
frequently biased and fail to fairly deliver conflict-
ing arguments of the issue. It is difficult for ordi-
nary readers to analyze the conflicting arguments 
and understand the contention; they mostly per-
ceive the issue passively, often through a single 
article. Advanced news delivery models are re-
quired to increase awareness on conflicting views. 
In this paper, we present disputant relation-
based method for classifying news articles on con-
tentious issues. We observe that the disputants of a 
contention, i.e., people who take a position and 
participate in the contention such as politicians, 
companies, stakeholders, civic groups, experts, 
commentators, etc., are an important feature for 
understanding the discourse. News producers pri-
marily shape an article on a contention by selecting 
and covering specific disputants (Baker. 1994). 
Readers also intuitively understand the contention 
by identifying who the opposing disputants are.  
The method helps readers intuitively view the 
news articles through the opponent-based frame. It 
performs classification in an unsupervised manner: 
it dynamically identifies opposing disputant groups 
and classifies the articles according to their posi-
tions. As such, it effectively helps readers contrast 
articles of a contention and attain balanced under-
standing, free from specific biased viewpoints. 
The proposed method differs from those used in 
related tasks as it aims to perform classification 
under the opponent-based frame. Research on sen-
timent classification and debate stance recognition 
takes a topic-oriented view, and attempts to per-
form classification under the ?positive vs. negative? 
or ?for vs. against? frame for the given topic, e.g., 
positive vs. negative about iPhone.  
However, such frames are often not appropriate 
for classifying news articles of a contention. The 
coverage of a contention often spans over different 
topics (Miller. 2001). For the contention on the 
health care bill, an article may discuss the enlarged 
coverage whereas another may discuss the increase 
of insurance premiums. In addition, we observe 
that opposing arguments of a contention are often 
complex to classify under these frames. For exam-
340
ple, in a political contention on holding a referen-
dum on the Sejong project1, the opposition parties 
strongly opposed and criticized the president office. 
Meanwhile, the president office argued that they 
were not considering holding the referendum and 
the contention arose from a misunderstanding. In 
such a case, it is difficult to classify any argument 
to the ?positive? category of the frame.  
We demonstrate that the opponent-based frame 
is clear and effective for contrasting opposing 
views of contentious issues. For the contention on 
the referendum, ?president office vs. opposition 
parties? provides an intuitive frame to understand 
the contention. The frame does not require the 
documents to discuss common topics nor the op-
posing arguments to be positive vs. negative.  
Under the proposed frame, it becomes important 
to analyze which side is more centrally covered in 
an article. Unlike debate posts or product reviews 
news articles, in general, do not take a position 
explicitly (except a few types such as editorials). 
They instead quote a specific side, elaborate them, 
and provide supportive facts. On the other hand, 
the opposing disputants compete for news cover-
age to influence more readers and gain support 
(Miller et al 2001). Thus, the method focuses on 
identifying the disputants of each side and classify-
ing the articles based on the side it covers. 
We applied a modified version of HITS algo-
rithm to identify the key opponents of an issue, and 
used disputant extraction techniques combined 
with an SVM classifier for article analysis. We 
observe that the method achieves acceptable per-
formance for practical use with basic language re-
sources and tools, i.e., Named Entity Recognizer 
(Lee et al 2006), POS tagger (Shim et al 2002), 
and a translated positive/negative lexicon. As we 
deal with non-English (Korean) news articles, it is 
difficult to obtain rich resources and tools, e.g., 
WordNet, dependency parser, annotated corpus 
such as MPQA. When applied to English, we be-
lieve the method could be further improved by 
adopting them.  
2 Background and Related Work 
Research has been made on sentiment classifica-
tion in document-level (Turney et al, 2002, Pang 
et al, 2002, Seki et al 2008, Ounis et al 2006). It 
aims to automatically identify and classify the sen-
                                                          
1 http://www.koreatimes.co.kr/www/news/nation/2010/07/116_61649.html 
timent of documents into positive or negative. 
Opinion summarization aims a similar goal, to 
identify different opinions on a topic and generate 
summaries of them. Paul et al (2010) developed an 
unsupervised method for generating summaries of 
contrastive opinions on a common topic. These 
works make a number of assumptions that are dif-
ficult to apply to the discourse of contentious news 
issues. They assume that the input documents have 
a common opinion target, e.g., a movie. Many of 
them primarily deal with documents which explic-
itly reveal opinions on the selected target, e.g., 
movie reviews. They usually apply one static clas-
sification frame, positive vs. negative, to the topic.   
The discourse of contentious issues in news arti-
cles show different characteristics from that stud-
ied in the sentiment classification tasks. First, the 
opponents of a contentious issue often discuss dif-
ferent topics, as discussed in the example above. 
Research in mass communication has showed that 
opposing disputants talk across each other, not by 
dialogue, i.e., they martial different facts and inter-
pretations rather than to give different answers to 
the same topics (Schon et al, 1994). 
Second, the frame of argument is not fixed as 
?positive vs. negative?. We frequently observed 
both sides of a contention articulating negative ar-
guments attacking each other. The forms of argu-
ments are also complex and diverse to classify 
them as positive or negative; for example, an ar-
gument may just neglect the opponent?s argument 
without positive or negative expressions, or em-
phasize a different discussion point.  
In addition, a position of a contention can be 
communicated without explicit expression of opin-
ion or sentiment. It is often conveyed through ob-
jective sentences that include carefully selected 
facts. For example, a news article can cast a nega-
tive light on a government program simply by cov-
ering the increase of deficit caused by it. 
A number of works deal with debate stance 
recognition, which is a closely related task. They 
attempt to identify a position of a debate, such as 
ideological (Somasundaran et al, 2010, Lin et al, 
2006) or product comparison debate (So-
masundaran et al, 2009). They assume a debate 
frame, which is similar to the frame of the senti-
ment classification task, i.e., for vs. against the de-
bate topic. All articles of a debate in their corpus 
cover a coherent debate topic, e.g., iPhone vs. 
Blackberry, and explicitly express opinions for or 
341
against to the topic, e.g., for or against iPhone or 
Blackberry. The proposed methods assume that the 
debate frame is known apriori. This debate frame 
is often not appropriate for contentious issues for 
similar reasons as the positive/negative frame. In 
contrast, our method does not assume a fixed de-
bate frame, and rather develops one based on the 
opponents of the contention at hand. 
The news corpus is also different from the de-
bate corpus. News articles of a contentious issue 
are more diverse than debate articles conveying 
explicit argument of a specific side. There are 
news articles which cover both sides, facts without 
explicit opinions, and different topics unrelated to 
the arguments of either side.  
Several works have used the relation between 
speakers or authors for classifying their debate 
stance (Thomas et al, 2006, Agrawal et al, 2003). 
However, these works also assume the same debate 
frame and use the debate corpus, e.g., floor debates 
in the House of Representatives, online debate fo-
rums. Their approaches are also supervised, and 
require training data for relation analysis, e.g., vot-
ing records of congresspeople.  
3 Argument Frame Comparison  
Establishing an appropriate argument frame is im-
portant. It provides a framework which enable 
readers to intuitively understand the contention. It 
also determines how classification methods should 
classify articles of the issue. 
We conducted a user study to compare the op-
ponent-based frame and the positive (for) vs. nega-
tive (against) frame. In the experiment, multiple 
human annotators classified the same set of news 
articles under each of the two frames. We com-
pared which frame is clearer for the classification, 
and more effective for exposing opposing views. 
We selected 14 contentious issues from Naver 
News (a popular news portal in Korea) issue ar-
chive. We randomly sampled about 20 articles per 
each issue, for a total of 250 articles. The selected 
issues range over diverse domains such as politics, 
local, diplomacy, economy; to name a few for ex-
ample, the contention on the 4 river project, of 
which the key opponents are the government vs. 
catholic church; the entrance of big retailers to the 
supermarket business, of which the key opponents 
are the small store owners vs. big retail companies; 
the refusal to approve an integrated civil servants? 
union, of which the key opponents are government 
vs. Korean government employees? union.  
We use an internationally known contention, i.e., 
the dispute about the Cheonan sinking incident, as 
an example to give more details on the disputants. 
Our data set includes 25 articles that were pub-
lished after the South Korea?s announcement of 
their investigation result. Many disputants appear 
in the articles, e.g., South Korean Government, 
South Korea defense secretary, North Korean 
Government, United States officials, Chinese ex-
perts, political parties of South Korea, etc.  
Three annotators performed the classification. 
All of them were students. For impartiality, two of 
them were recruited from outside the team, who 
were not aware of this research.  
The annotators performed two subtasks for clas-
sification. As for the positive vs. negative frame, 
first, we asked them to designate the main topic of 
the contention. Second, they classified the articles 
which mainly deliver arguments for the topic to the 
?positive? category and those delivering arguments 
against the topic to the ?negative? category. The 
articles are classified to the ?Other? category if 
they do not deal with the main topic nor cover pos-
itive or negative arguments.  
As for the opponent-based frame, first, we asked 
them to designate the competing opponents. Se-
cond, we asked to classify articles to a specific side 
if the articles cover only the positions, arguments, 
or information supportive of that side or if they 
cover information detrimental or criticism to its 
opposite side. Other articles were classified to the 
?Other? category. Examples of this category in-
clude articles covering both sides fairly, describing 
general background or implications of the issue. 
Issue #
Free-marginal kappa
Issue #
Free-marginal kappa
Pos.-Neg. Opponent Pos.-Neg. Opponent
1 0.83 0.67 8 0.26 0.58 
2 0.57 0.48 9 0.07 1.00 
3 0.44 0.95 10 0.48 0.84 
4 0.75 0.87 11 0.71 0.86 
5 0.36 0.64 12 0.71 0.71 
6 0.30 0.70 13 0.63 0.79 
7 0.18 0.96 14 0.48 0.87 
Avg. 0.50 0.78 
 
Table 1. Inter-rater agreement result. 
The agreement in classification was higher for 
the opponent-based frame in most issues. This in-
dicates that the annotators could apply the frame 
more clearly, resulting in smaller difference be-
tween them. The kappa measure was 0.78 on aver-
342
age. The kappa measure near 0.8 indicates a sub-
stantial level of agreement, and the value can be 
achieved, for example, when 8 or 9 out of 10 items 
are annotated equally (Table 1). 
In addition, fewer articles were classified to the 
?Other? category under the opponent-based frame. 
The annotators classified about half of the articles 
to this category under the positive vs. negative 
frame whereas they classified about 35% to the 
category under the opponent-based frame. This is 
because the frame is more flexible to classify di-
verse articles of an issue, such as those covering 
arguments on different points, and those covering 
detrimental facts to a specific side without explicit 
positive or negative arguments. 
The kappa measure was less than 0.5 for near 
half of the issues under the positive-negative frame. 
The agreement was low especially when the main 
topic of the contention was interpreted differently 
among the annotators; the main topic was inter-
preted differently for issue 3, 7, 8, and 9. Even 
when the topic was interpreted identically, the an-
notators were confused in judging complex argu-
ments either as positive or negative. One annotator 
commented that ?it was confusing as the argu-
ments were not clearly for or against the topic of-
ten. Even when a disputant was assumed to have a 
positive attitude towards the topic, the disputant?s 
main argument was not about the topic but about 
attacking the opponent? The annotators all agreed 
that the opponent-based frame is more effective to 
understand the contention. 
4 Disputant relation-based method 
Disputant relation-based method adopts the oppo-
nent-based frame for classification. It attempts to 
identify the two opposing groups of the issue at 
hand, and analyzes whether an article more reflects 
the position of a specific side. The method is based 
on the observation that there exists two opposing 
groups of disputants, and the groups compete for 
news coverage. They strive to influence readers? 
interpretation, evaluation of the issue and gain 
support from them (Miller et al 2001). In this 
competing process, news articles may give more 
chance of speaking to a specific side, explain or 
elaborate them, or provide supportive facts of that 
side (Baker 1994).  
The proposed method is performed in three 
stages: the first stage, disputant extraction, extracts 
the disputants appearing in an article set; the se-
cond stage, disputant partition, partitions the ex-
tracted disputants into two opposing groups; lastly, 
the news classification stage classifies the articles 
into three categories, i.e., two for the articles bi-
ased to each group, and one for the others. 
4.1 Disputant Extraction 
In this stage, the disputants who participate in the 
contention have to be extracted. We utilize that 
many disputants appear as the subject of quotes in 
the news article set. The articles actively quote or 
cover their action in order to deliver the contention 
lively. We used straight forward methods for ex-
traction of subjects. The methods were effective in 
practice as quotes of articles frequently had a regu-
lar pattern. 
The subjects of direct and indirect quotes are ex-
tracted. The sentences including an utterance in-
side double quotes are considered as direct quotes. 
 The sentences which convey an utterance with-
out double quotes, and those describing the action 
of a disputant are considered as indirect quotes 
(See the translated example 1 below). The indirect 
quotes are identified based on the morphology of 
the ending word. The ending word of the indirect 
quotes frequently has a verb as its root or includes 
a verbalization suffix. Other sentences, typically, 
those describing the reporter?s interpretation or 
comments are not considered as quotes. (See ex-
ample sentence 2. The ending word of the original 
sentence is written in boldface). 
(1) The government clarified that there won?t be 
any talks unless North Korea apologizes for 
the attack.  
(2) The government?s belief is that a stern re-
sponse is the only solution for the current crisis 
A named entity combined with a topic particle 
or a subject particle is identified as the subject of 
these quotes. We detect the name of an organiza-
tion, person, or country using the Korean Named 
Entity Recognizer (Lee et al 2006). A simple 
anaphora resolution is conducted to identify sub-
jects also from abbreviated references or pronouns 
in subsequent quotes.  
4.2 Disputant Partitioning 
We develop key opponent-based partitioning 
method for disputant partitioning. The method first 
identifies two key opponents, each representing 
343
one side, and uses them as a pivot for partitioning 
other disputants. The other disputants are divided 
according to their relation with the key opponents, 
i.e., which key opponent they stand for or against. 
The intuition behind the method is that there 
usually exists key opponents who represent the 
contention, and many participants argue about the 
key opponents whereas they seldom recognize and 
talk about minor disputants. For instance, in the 
contention on ?investigation result of the Cheonan 
sinking incident?, the government of North Korea 
and that of South Korea are the key opponents; 
other disputants, such as politicians, experts, civic 
group of South Korea, the government of U.S., and 
that of China, mostly speak about the key oppo-
nents. Thus, it is effective to analyze where the 
disputants stand regarding their attitude toward the 
key opponents. 
Selecting key opponents: In order to identify 
the key opponents of the issue, we search for the 
disputants who frequently criticize, and are also 
criticized by other disputants. As the key oppo-
nents get more news coverage, they have more 
chance to articulate their argument, and also have 
more chance to face counter-arguments by other 
disputants. 
This is done in two steps. First, for each dispu-
tant, we analyze whom he or she criticizes and by 
whom he or she is criticized. The method goes 
through each sentence of the article set and search-
es for both disputant?s criticisms and the criticisms 
about the disputant. Based on the criticisms, it ana-
lyzes relationships among disputants. 
A sentence is considered to express the dispu-
tant?s criticism to another disputant if the follow-
ing holds: 1) the sentence is a quote, 2) the 
disputant is the subject of the quote, 3) another 
disputant appears in the quote, and 4) a negative 
lexicon appears in the sentence.  
On the other hand, if the disputant is not the sub-
ject but appears in the quote, the sentence is con-
sidered to express a criticism about the disputant 
made by another disputant (See example 3. The 
disputants are written in italic, and negative words 
are in boldface.).  
(3) the government defined that ?the attack of 
North Korea is an act of invasion and also a 
violation of North-South Basic Agreement? 
The negative lexicon we use is carefully built 
from the Wilson lexicon (Wilson et al 2005). We 
translated all the terms in it using the Google trans-
lation, and manually inspected the translated result 
to filter out inappropriate translations and the terms 
that are not negative in the Korean context. 
Second, we apply an adapted version of HITS 
graph algorithm to find major disputants. For this, 
the criticizing relationships obtained in the first 
step are represented in a graph. Each disputant is 
modeled as a node, and a link is made from a criti-
cizing disputant to a criticized disputant.  
South Korea 
government
North Korea 
government
Ministry of 
Defense
China
Opposition
party
(A: 0.3, H: 0.2)
(A: 0, H: 0.1)
(A: 0.28, H: 0.15)
(A: 0, H: 0.1)
A: Authority score
H: Hub score
 
Figure 1. Example HITS graph illustration  
Originally, the HITS algorithm (Kleinberg, 
1999) is designed to rate Web pages regarding the 
link structure. The feature of the algorithm is that it 
separately models the value of outlinks and inlinks. 
Each node, i.e., a web page, has two scores: the 
authority score, which reflects the value of inlinks 
toward itself, and the hub score, which reflects the 
value of its outlinks to others. The hub score of a 
node increases if it links to nodes with high author-
ity score, and the authority score increases if it is 
pointed by many nodes with high hub score. 
We adopt the HITS algorithm due to above fea-
ture. It enables us to separately measure the signif-
icance of a disputant?s criticism (using the hub 
score) and the criticism about the disputant (using 
the authority score). We aim to find the nodes 
which have both high hub score and high authority 
score; the key opponents will have many links to 
others and also be pointed by many nodes.  
The modified HITS algorithm is shown in Fig-
ure 2. We make some adaptation to make the algo-
rithm reflect the disputants? characteristics. The 
initial hub score of a node is set to the number of 
quotes in which the corresponding disputant is the 
subject. The initial authority score is set to the 
number of quotes in which the disputant appears 
but not as the subject. In addition, the weight of 
each link (from a criticizing disputant to a criti-
cized disputant) is set to the number of sentences 
that express such criticism.  
We select the nodes which show relatively high 
hub score and high authority score compared to 
other nodes. We rank the nodes according to the 
sum of hub and authority scores, and select from 
344
the top ranking node. The node is not selected if its 
hub or authority score is zero. The selection is fin-
ished if more than two nodes are selected and the 
sum of hub and authority scores is less than half of 
the sum of the previously selected node.  
Modified HITS(G,W,k) 
G = <V, E> where
V is a set of vertex, a vertex v
i
represents a disputant
E is a set of edges, an edge e
ij
represents a criticizing quote 
from disputant i to j
W = {w
ij
| weight of edge e
ij
}
For all v
i
V
Auth
1
(v
i
) = # of quotes of  which the subject is disputant i
Hub
1 
(v
i
) = # of quotes of  which disputant i appears, but
not as the subject
F t = 1 to k:
Auth
t+1
(v
i
) = 
Hub
t+1 
(v
i
) = 
Normalize Auth
t+1
(v
i
) and Hub
t+1 
(v
i
)
 
Figure 2. Algorithm of the Modified HITS 
More than two disputants can be selected if 
more than one disputant is active from a specific 
side. In such cases, we choose the two disputants 
whose criticizing relationship is the strongest 
among the selected ones, i.e., the two who show 
the highest ratio of criticism between them.  
Partitioning minor disputants: Given the two 
key opponents, we partition the rest of disputants 
based on their relations with the key opponents. 
For this, we identify whether each disputant has 
positive or negative relations with the key oppo-
nents. The disputant is classified to the side of the 
key opponent who shows more positive relations. 
If the disputant shows more negative relations, the 
disputant is classified to the opposite side.  
We analyze the relationship not only from the 
article set but also from the web news search re-
sults. The minor disputants may not be covered 
importantly in the article set; hence, it can be diffi-
cult to obtain sufficient data for analysis. The web 
news search results provide supplementary data for 
the analysis of relationships. 
We develop four features to capture the positive 
and negative relationships between the disputants.  
1) Positive Quote Rate (PQRab): Given two dis-
putants (a key opponent a, and a minor disputant b), 
the feature measures the ratio of positive quotes 
between them. A sentence is considered as a posi-
tive quote if the following conditions hold: the sen-
tence is a direct or indirect quote, the two 
disputants appear in the sentence, one is the subject 
of the quote, and a positive lexicon appears in the 
sentence. The number of such sentences is divided 
by the number of all quotes in which the two dis-
putants appear and one appears as the subject.  
2) Negative Quote Rate (NQRab): This feature is 
an opposite version of PQR. It measures the ratio 
of negative quotes between the two disputants. The 
same conditions are considered to detect negative 
quotes except that negative lexicon is used instead 
of positive lexicon.  
3) Frequency of Standing Together (FSTab): 
This feature attempts to capture whether the two 
disputants share a position, e.g., ?South Korea and 
U.S. both criticized North Korea for?? It counts 
how many times they are co-located or connected 
with the conjunction ?and? in the sentences. 
4) Frequency of Division (FDab): This feature is 
an opposite version of the FST. It counts how 
many times they are not co-located in the sentences. 
The same features are also calculated from the 
web news search results; we collect news articles 
of which the title includes the two disputants, i.e., a 
key opponent a and a minor disputant b.  
The calculation method of PQR and NQR is 
slightly adapted since the titles are mostly not 
complete sentences. For PQR (NQR), it counts the 
titles which the two disputants appear with a posi-
tive (negative) lexicon. The counted number is di-
vided by the number of total search results. The 
calculation method of FST and FD is the same ex-
cept that they are calculated from the titles. 
We combine the features obtained from web 
news search with the corresponding ones obtained 
from the article set by calculating a weighted sum. 
We currently give equal weights.  
The disputants are partitioned by the following 
rule: given a minor disputant a, and the two key 
opponents b and c, 
classify a to b?s side if, 
(PQRab ? NQRab) > (PQRac ? NQRac) or  
((FSTab > FDab) and (FSTac = 0)); 
classify a to c?s side if, 
(PQRac ? NQRac) > (PQRab ? NQRab) or  
((FSTac > FDac) and (FSTab = 0)); 
classify a to other, otherwise. 
4.3 Article Classification 
Each news article of the set is classified by analyz-
ing which side is importantly covered. The method 
classifies the articles into three categories, either to 
one of the two sides or the category ?other?.  
345
We observed that the major components which 
shape an article on a contention are quotes from 
disputants and journalists? commentary. Thus, our 
method considers two points for classification: first, 
from which side the article?s quotes came; second, 
for the rest of the article?s text, the similarity of the 
text to the arguments of each side. 
As for the quotes of an article, the method calcu-
lates the proportion of the quotes from each side 
based on the disputant partitioning result. As for 
the rest of the sentences, a similarity analysis is 
conducted with an SVM classifier. The classifier 
takes a sentence as input, determines its class to 
one of the three categories, i.e., one of the two 
sides, or other. It is trained with the quotes from 
each side (tf.idf of unigram and bigram is used as 
features). The same number of quotes from each 
side is used for training. The training data is pseu-
do-relevant: it is automatically obtained based on 
the partitioning result of the previous stage.  
An article is classified to a specific side if more 
of its quotes are from that side and more sentences 
are similar to that side: given an article a, and the 
two sides b and c,  
classify a to b  if
  
classify a to c  if 
 
classify a to other, otherwise. 
where SU: number of all sentences of the article 
Qi: number of quotes from the side i. 
Qij: number of quotes from either side i or j. 
Si: number of sentences classified to i by SVM. 
Sij:: number of sentences classified to either i or j. 
We currently set the parameters heuristically. 
We set 0.7 and 0.6 for the two parameters ? and ? 
respectively. Thus, for an article written purely 
with quotes, the article is classified to a specific 
side if more than 70% of the quotes are from that 
side. On the other hand, for an article which does 
not include quotes from any side, more than 60% 
of the sentences have to be determined similar to a 
specific side?s quotes. We set a lower value for ? 
to classify articles with less number of biased sen-
tences (Articles often include non-quote sentences 
unrelated to any side to give basic information).  
5 Evaluation and Discussion  
Our evaluation of the method is twofold: first, we 
evaluate the disputant partitioning results, second, 
the accuracy of classification. The method was 
evaluated using the same data set used for the clas-
sification frame comparison experiment.  
A gold result was created through the three hu-
man annotators. To evaluate the disputant parti-
tioning results, we had the annotators to extract the 
disputants of each issue, divide them into opposing 
two groups. We then created a gold partitioning 
result, by taking a union of the three annotators? 
results. A gold classification is also created from 
the classification of the annotators. We resolved 
the disagreements between the annotators? results 
by following the decision of the majority. 
5.1 Evaluation of Disputant Partitioning 
We evaluated the partitioning result of the two op-
posing groups, denoted as G1 and G2. The perfor-
mance is measured using precision and recall. 
Table 2 presents the results. The precision of the 
partitioning was about 70% on average. The false 
positives were mostly the disputants who appear 
only a few times both in the article set and the 
news search results. As they appeared rarely, there 
was not enough data to infer their position. The 
effect of these false positives in article classifica-
tion was limited.  
The recall was slightly lower than precision. 
This was mainly because some disputants were 
omitted in the disputant extraction stage. The NER 
we used occasionally missed the names of unpopu-
lar organizations, e.g., civic groups, and the extrac-
tion rule failed to capture the subject in some 
complex sentences. However, most disputants who 
frequently appear in the article set were extracted 
and partitioned appropriately. 
 
Table 2. Disputant Partitioning Result 
5.2 Evaluation of Article Classification 
We evaluate our method and compare it with two 
unsupervised methods below. 
Similarity-based clustering (Sim.): The meth-
od implements a typical method. It clusters articles 
of an issue into three groups based on text similari
346
Issue 
#
Method wF
Group 1 Group 2 Other
Issue 
#
Method wF
Group 1 Group 2 Other
F P R F P R F P R F P R F P R F P R
1
DrC 0.47 0.64 0.47 1.00 0.62 1.00 0.44 N/A 0.00 0.00 
8
DrC 0.90 0.86 0.75 1.00 1.00 1.00 1.00 0.86 1.00 0.75 
QbC 0.50 0.62 0.47 0.89 0.71 1.00 0.55 N/A 0.00 0.00 QbC 0.48 0.57 0.50 0.67 0.57 0.50 0.67 0.33 0.50 0.25 
Sim. 0.27 0.20 1.00 0.11 0.20 1.00 0.11 0.47 0.30 1.00 Sim. 0.56 0.67 0.67 0.67 0.50 0.40 0.67 0.50 1.00 0.33 
2
DrC 0.65 0.67 0.62 0.73 0.86 1.00 0.75 0.53 0.57 0.50 
9
DrC 0.77 N/A 0.00 N/A 0.57 0.50 0.67 0.82 1.00 0.70 
QbC 0.65 0.76 0.80 0.73 0.60 0.50 0.75 0.53 0.57 0.50 QbC 0.79 N/A 0.00 N/A 0.67 0.67 0.67 0.82 1.00 0.70 
Sim. 0.37 0.63 0.48 0.91 N/A 0.00 0.00 0.22 1.00 0.13 Sim. 0.49 N/A 0.00 N/A 0.00 0.00 0.00 0.63 0.67 0.60 
3
DrC 0.72 0.57 0.40 1.00 0.67 1.00 0.50 0.86 0.75 1.00 
10
DrC 0.66 0.71 0.56 1.00 0.73 1.00 0.57 0.40 0.50 0.33 
QbC 0.74 0.57 0.40 1.00 0.75 1.00 0.60 0.77 0.71 0.83 QbC 0.72 0.77 0.63 1.00 0.77 0.83 0.71 0.50 1.00 0.33 
Sim. 0.59 N/A 0.00 0.00 0.70 0.62 0.80 0.60 0.75 0.50 Sim. 0.40 0.33 1.00 0.20 0.44 1.00 0.29 0.40 0.25 1.00 
4
DrC 0.80 0.82 0.69 1.00 0.86 1.00 0.75 0.57 0.67 0.50 
11
DrC 0.61 0.73 0.80 0.67 0.50 0.43 0.60 0.57 0.67 0.50 
QbC 0.81 0.90 0.82 1.00 0.86 1.00 0.75 0.44 0.40 0.50 QbC 0.39 0.62 0.57 0.67 0.20 0.20 0.20 0.29 0.33 0.25 
Sim. 0.67 0.80 1.00 0.67 0.80 0.67 1.00 N/A 0.00 0.00 Sim. 0.47 0.63 0.46 1.00 0.33 1.00 0.20 0.40 1.00 0.25 
5
DrC 0.60 0.63 0.50 0.83 0.71 0.83 0.63 0.33 0.50 0.25 
12
DrC 0.67 0.29 0.20 0.50 0.67 0.67 0.67 0.77 1.00 0.63 
QbC 0.55 0.40 0.50 0.33 0.71 0.67 0.75 0.44 0.40 0.50 QbC 0.38 0.33 0.25 0.50 0.44 0.33 0.67 0.36 0.47 0.25 
Sim. 0.51 0.63 0.46 1.00 0.67 1.00 0.50 N/A 0.00 0.00 Sim. 0.43 N/A 0.00 0.00 0.55 0.38 1.00 0.50 0.75 0.38 
6
DrC 0.89 N/A 0.00 N/A 0.89 1.00 0.80 0.89 1.00 0.80 
13
DrC 0.65 0.79 0.69 0.92 0.33 1.00 0.20 0.67 1.00 0.50 
QbC 0.50 N/A 0.00 N/A 0.50 0.67 0.40 0.50 0.67 0.40 QbC 0.59 0.75 0.75 0.75 0.33 1.00 0.20 0.29 0.20 0.50 
Sim. 0.55 N/A 0.00 N/A 0.77 0.63 1.00 0.33 1.00 0.20 Sim. 0.54 0.71 0.63 0.83 0.33 1.00 0.20 N/A 0.00 0.00 
7
DrC 0.48 0.67 1.00 0.50 0.71 0.55 1.00 N/A N/A 0.00 
14
DrC 0.61 0.77 0.77 0.77 0.50 0.57 0.44 0.25 0.20 0.33 
QbC 0.48 0.67 1.00 0.50 0.62 0.53 0.73 0.17 0.20 0.14 QbC 0.66 0.83 0.75 0.92 0.53 0.67 0.44 0.33 0.33 0.33 
Sim. 0.44 0.40 0.27 0.75 0.57 0.60 0.55 0.25 1.00 0.14 Sim. 0.37 0.29 1.00 0.17 0.60 0.43 1.00 N/A 0.00 0.00 
Issue 
#
Total G1 G2 Other
1 24 9 9 6
2 23 11 4 8
3 18 2 10 6
4 25 9 12 4
5 18 5 9 4
6 10 0 5 5
7 22 4 11 7
8 10 3 3 4
9 13 0 3 10
10 15 5 7 3
11 15 6 5 4
12 13 2 3 8
13 19 12 5 2
14 25 13 10 2
 
*N/A: The metric could not be calculated in some cases. This happened when no articles were classified to a category.  
Table 3. Number of articles of each issue and group (left), and classification performance (right) 
ty. It uses tf.idf of unigram and bigram as features, 
and cosine similarity as the similarity measure. 
We used the K-means clustering algorithm.  
Quote-based classification (QbC.): The meth-
od is a partial implementation of our method. The 
disputant extraction and disputant partitioning is 
performed identically; however, it classifies news 
articles merely based on quotes. An article is clas-
sified to one of the two opposing sides if more 
than 70% of the quotes are from that side, or to 
the ?other? category otherwise.  
Results: We evaluated the classification result 
of the three categories, the two groups G1 and G2, 
and the category Other. The performance is meas-
ured using precision, recall, and f-measure. We 
additionally used the weighted f-measure (wF) to 
aggregate the f-measure of the three categories. It 
is the weighted average of the three f-measures. 
The weight is proportional to the number of arti-
cles in each category of the gold result. 
The disputant relation-based method (DrC) per-
formed better than the two comparison methods. 
The overall average of the weighted f-measure 
among issues was 0.68, 0.59, and 0.48 for the DrC, 
QbC, and Sim. method, respectively (See Table 3). 
The performance of the similarity-based clustering 
was lower than that of the other two in most issues.  
A number of works have reported that text sim-
ilarity is reliable in stance classification in politi-
cal domains. These experiments were conducted 
in political debate corpus (Lin et al 2006). How-
ever, news article set includes a number of articles 
covering different topics irrelevant to the argu-
ments of the disputants. For example, there can be 
an article describing general background of the 
contention. Similarity-based clustering approach 
reacted sensitively to such articles and failed to 
capture the difference of the covered side.  
Quote-based classification performs better than 
similarity-based approach as it classifies articles 
primarily based on the quoted disputants. The per-
formance is comparable to DrC in many issues. 
The method performs similarly to DrC if most 
articles of an issue include many qutes. DrC per-
forms better for other issues which include a 
number of articles with only a few quotes. 
Error analysis: As for our method, we ob-
served three main reasons of misclassification.  
1) Articles with few quotes: Although the pro-
posed method better classifies such articles than 
the quote-based classification, there were some 
misclassifications. There are sentences that are not 
directly related to the argument of any side, e.g., 
plain description of an event, summarizing the 
development of the issue, etc. The method made 
errors while trying to decide to which side these 
sentences are close to. Detecting such sentences 
and avoiding decisions for them would be one 
way of improvement. Research on classification 
347
of subjective and objective sentences would be 
helpful (Wiebe et al 99).  
2) Article criticizing the quoted disputants: There 
were some articles criticizing the quoted dispu-
tants. For example, an article quoted the president 
frequently but occasionally criticized him between 
the quotes. The method misclassified such articles 
as it interpreted that the article is mainly deliver-
ing the president?s argument.  
3) Errors in disputant partitioning: Some misclas-
sifications were made due to the errors in the dis-
putant partitioning stage, specifically, those who 
were classified to a wrong side. Articles which 
refer to such disputants many times were misclas-
sified.  
6 Conclusion 
We study the problem of classifying news articles 
on contentious issues. It involves new challenges 
as the discourse of contentious issues is complex, 
and news articles show different characteristics 
from commonly studied corpus, such as product 
reviews. We propose opponent-based frame, and 
demonstrate that it is a clear and effective classifi-
cation frame to contrast arguments of contentious 
issues. We develop disputant relation-based clas-
sification and show that the method outperforms a 
text similarity-based approach.  
Our method assumes polarization for conten-
tious issues. This assumption was valid for most 
of the tested issues. For a few issues, there were 
some participants who do not belong to either 
side; however, they usually did not take a particu-
lar position nor make strong arguments. Thus, the 
effect on classification performance was limited. 
Discovering and developing methods for issues 
which involve more than two disputants groups is 
a future work. 
References  
Rakesh Agrawal, Sridhar Rajagopalan, Rama-
krishnan Srikant, and Yirong Xu. 2003. Mining 
newsgroups using networks arising from social 
behavior. In Proceedings of WWW. 
Baker, B. 1994. How to Identify, Expose and Cor-
rect Liberal Media Bias. Media Research Cen-
ter. 
Mohit Bansal, Claire Cardie, and Lillian Lee. 
2008. The power of negative thinking: Exploit-
ing label disagreement in the min-cut 
classification framework. In Proceedings of the 
22nd International Conference on Computa-
tional Linguistics (COLING-2008). 
Jon M. Kleinberg. 1999. Authoritative sources in 
a hyperlinked environment. In Journal of ACM, 
46(5): 604-632. 
Landis JR, Koch G. 1977. The measurement of 
observer agreement for categorical data. Bio-
metrics 33:159-174.  
Changki Lee, Yi-Gyu Hwang, Hyo-Jung Oh, Soo-
jong Lim, Jeong Heo, Chung-Hee Lee, Hyeon-
Jin Kim, Ji-Hyun Wang, Myung-Gil Jang. 2006. 
Fine-Grained Named Entity Recognition using 
Conditional Random Fields for Question An-
swering,  In Proceedings of Human & Cogni-
tive Language Technology (HCLT), pp. 
268~272. (in Korean) 
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and 
Alexander Hauptmann. 2006. Which side are 
you on? Identifying perspectives at the docu-
ment and sentence levels. In Proceedings of the 
10th Conference on Computational Natural 
Language Learning (CoNLL-2006), pages 109?
116, New York. 
Mark M. Miller and Bonnie P. Riechert. 2001. 
Spiral Opportunity and Frame Resonance: 
Mapping Issue Cycle in News and Public Dis-
course. In Framing Public Life: Perspectives on 
Media and our Understanding of the Social 
World, NJ: Lawrence Erlbaum Associates. 
I Ounis, M de Rijke, C Macdonald, G Mishne, and 
I Soboroff. 2006. Overview of the TREC-2006 
Blog Track. In Proceedings of TREC.  
Pang, Bo, Lillian Lee, and Shivakumar Vaithya-
nathan. 2002. Thumbs up? Sentiment Classifi-
cation using Machine Learning Techniques, 
Proceedings of the 2002 Conference on Empir-
ical Methods in Natural Language Processing 
(EMNLP). 
Paul, M. J., Zhai, C., Girju, R. 2010. Summarizing 
Contrastive Viewpoints in Opinionated Text. In 
Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing 
(EMNLP).  
348
Schon, D.A., and Rien, M. 1994. Frame reflec-
tion: Toward the resolution of intractable policy 
controversies. New York: Basic Books.  
Y. Seki, D. Evans, L. Ku, L. Sun, H. Chen, and N. 
Kando. 2008. Overview of Multilingual Opin-
ion Analysis Task at NTCIR-7. In Proceedings 
of 7th NTCIR Evaluation Workshop, pages 185-
203 
Kwangseob Shim and Jaehyung Yang. 2002. 
MACH : A Supersonic Korean Morphological 
Analyzer, Proceedings of the 19th International 
Conference on Computational Linguistics 
(COLING-2002), pp.939-945.  
Swapna Somasundaran and Janyce Wiebe. 2009. 
Recognizing stances in online debates. In Pro-
ceedings of the Joint Conference of the 47th 
Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 226?
234, Suntec, Singapore, August. Association 
for Computational Linguistics. 
Swapna Somasundaran and Janyce Wiebe. 2010. 
Recognizing stances in ideological online de-
bates. In Proceedings of the NAACL HLT 2010 
Workshop on Computational Approaches to 
Analysis and Generation of Emotion in Text 
(CAAGET ?10). 
Matt Thomas, Bo Pang, and Lillian Lee. 2006. 
Get outthe vote: Determining support or oppo-
sition from congressional floor-debate tran-
scripts. In Proceedings of the 2006 Conference 
on Empirical Methods in Natural Language 
Processing, pages 327?335, Sydney, Australia, 
July. Association for Computational Linguistics. 
Turney, Peter D. 2002. Thumbs up or thumbs 
down? Semantic orientation applied to unsu-
pervised classification of reviews,  Proceedings 
of ACL-02, Philadelphia, Pennsylvania, 417-
424 
Wiebe, Janyce M., Bruce, Rebecca F., & O'Hara, 
Thomas P. 1999. Development and use of a 
gold standard data set for subjectivity classifi-
cations. In Proc. 37th Annual Meeting of the 
Assoc. for Computational Linguistics (ACL-99). 
June, pp. 246-253. 
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. 
Recognizing contextual polarity in phrase-level 
sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). 
  
 
349
