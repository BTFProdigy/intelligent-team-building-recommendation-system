Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 434?437,
Prague, June 2007. c?2007 Association for Computational Linguistics
UPV-WSD : Combining different WSD Methods
by means of Fuzzy Borda Voting
Davide Buscaldi and Paolo Rosso
DSIC, Dpto. Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
Valencia, Spain
{dbuscaldi,prosso}@dsic.upv.es
Abstract
This paper describes the WSD system devel-
oped for our participation to the SemEval-1.
It combines various methods by means of a
fuzzy Borda voting. The fuzzy Borda vote-
counting scheme is one of the best known
methods in the field of collective decision
making. In our system the different disam-
biguation methods are considered as experts
that give a preference ranking for the senses
a word can be assigned. Then the prefer-
ences are evaluated using the fuzzy Borda
scheme in order to select the best sense. The
methods we considered are the sense fre-
quency probability calculated over SemCor,
the Conceptual Density calculated over both
hyperonyms and meronyms hyerarchies in
WordNet, the extended Lesk by Banerjee
and Pedersen, and finally a method based on
WordNet domains.
1 Introduction
One of the lessons learned from our previous experi-
ence at Senseval-31 (Buscaldi et al, 2004; Vazquez
et al, 2004) is that the integration of different sys-
tems usually works better than a standalone system.
In our opinion this reflects the reality where humans
do not apply always the same rule in order to disam-
biguate the same ambigue word; for instance, if we
consider the sentences ?He hit a home run? and ?The
thermometer hit 100 degrees?, in the first case the
sport domain helps in determining the right sense for
1http://www.senseval.org
hit, whereas in the latter the disambiguation is car-
ried out mostly depending on the fact that the subject
of the sentence is an object.
The combination of distinct methods represents
itself a major problem. If the methods return dif-
ferent answers, how can we select the best one? In
this sense the available choices are the following:
? Rule-based selection: a set of rules that can be
both hand-made or automatically learned from
examples;
? Probability-based: the output of the methods is
normalized in the range [0, 1] and is considered
as a probability. Then the values are multiplied
in order to obtain the sense with a maximum
probability.
? Vote-based: the output of the methods is con-
sidered as a weighted vote. Then a voting
scheme is used in order to obtain the most voted
sense.
In our previous participation with the R2D2 project
(Vazquez et al, 2004) the selection was rule-based,
with hand-made rules that attempted to take into ac-
count the reliability of the various method. We sub-
sequently attempted to learn automatically the rules,
but the results of these experiments did not allow to
determine clearly which method was to be used in
each context.
Working with probabilities can be problematic
due to the null probabilities that make necessary the
adoption of smoothing techniques. Therefore, we
opted for a voting scheme, in this case the fuzzy
Borda (Nurmi, 2001; Garc??a Lapresta and Mart??nez
434
Panero, 2002), one of the best known methods in
the field of collective decision making. With this
scheme the disambiguation methods are considered
as experts providing a preference ranking over the
sense of the word.
The methods we choose as experts are the sense
probability calculated over SemCor, the Conceptual
Density algorithm by (Rosso et al, 2003), the ex-
tended Lesk by (Banerjee and Pedersen, 2002), and
an algorithm that takes into account the domains of
the word to be disambiguated and the context words.
In the following sections we describe in detail the
fuzzy Borda scheme and each WSD expert.
2 The Fuzzy Borda voting scheme
The original Borda vote-counting scheme was in-
troduced in 1770 by Jean Charles de Borda, and
adopted by the French Academy of Sciences with
the purpose of selecting its members. In the classical
Borda count each expert gives a mark to each alter-
native, according to the number of alternatives worse
than it. The fuzzy variant (Nurmi, 2001; Garc??a
Lapresta and Mart??nez Panero, 2002) is a natural ex-
tension that allows the experts to show numerically
how much some alternatives are preferred to the oth-
ers, evaluating their preference intensities from 0 to
1.
Let R1, R2, . . . , Rm be the fuzzy prefer-
ence relations of m experts over n alternatives
x1, x2, . . . , xn. For each expert k we obtain a
matrix of preference intensities:
?
?
?
?
?
rk11 rk12 . . . rk1n
rk21 rk22 . . . rk2n
. . . . . . . . . . . .
rkn1 rkn2 . . . rknn
?
?
?
?
?
where each rkij = ?Rk(xi, xj), with ?Rk : X?X ?
[0, 1] being the membership function of Rk. The
number rkij ? [0, 1] is considered as the degree of
confidence with which the expert k prefers xi to xj .
The final value assigned by the expert k to each al-
ternative xi is:
rk(xi) =
n
?
j=1,rkij>0.5
rkij (1)
which coincides with the sum of the entries greater
than 0.5 in the i-th row in the preference matrix. The
threshold 0.5 ensure the relation Rkto be an ordinary
preference relation (Garc??a Lapresta and Mart??nez
Panero, 2002).
Therefore, the definitive fuzzy Borda count for an
alternative xi is obtained as the sum of the values
assigned by each expert:
r(xi) =
m
?
k=1
rk(xi) (2)
In order to fill the preference matrix with the
correct confidence values, the output weights
w1, w2, . . . , wn of each expert k are transformed to
fuzzy confidence values by means of the following
transformation:
rkij =
wi
wi + wj
(3)
An example of how fuzzy Borda is used to combine
the votes in order to obtain the right sense of the
target word is shown in Section 4.
3 WSD Experts
We considered five experts in order to carry out
the disambiguation process. Sense probability and
the extended lesk were available for every word,
while the Conceptual Density was calculated only
for nouns. Therefore, all the experts were available
only for the nouns. For each expert different con-
texts were taken into account, depending on the spe-
cific characteristics of each expert.
3.1 Sense Probability
This expert is the simplest one: its votes are calcu-
lated using only the frequency count in SemCor of
the WordNet senses of the word. The transformation
of the frequency counts to the preference ranking is
done according to Formula (3). Zero frequency are
normalized to 1.
3.2 Conceptual Density
Conceptual Density (CD) was originally introduced
by (Agirre and Rigau, 1996). It is computed on
WordNet subhierarchies, determined by the hyper-
nymy (or is-a) relationship. Our formulation (Rosso
et al, 2003) of the Conceptual Density of a WordNet
subhierarchy s is:
CD(m, f, n) = m?
(m
n
)
(4)
435
Where m are the relevant synsets in the subhierar-
chy, n is the total number of synsets in the subhierar-
chy.The relevant synsets are both the synsets of the
word to be disambiguated and those of the context
words.
The WSD system based on this formula par-
ticipated at the Senseval-3 competition as the
CIAOSENSO system (Buscaldi et al, 2004), ob-
taining 75.3% in precision over nouns in the all-
words task (baseline: 70.1%). These results were
obtained with a context window of two nouns, the
one preceding and the one following the word. In
Senseval-3 the WSD system took also into account
the frequency of senses depending on their rank. In
SemEval-1 we do not, because of the presence of the
Sense Probability expert.
The CD-based expert uses a context of two nouns
for the disambiguation process too. The weights
from Formula (4) are used for computing the fuzzy
confidence values that are used to fill the preference
matrix after they are transformed according to For-
mula (3).
A second CD-based expert exploits the holonymy,
or part-of relationship instead of hyperonymy. This
expert uses as context all the nouns in the sentence
of the word to be disambiguated.
3.3 Extended Lesk
This expert is based on the algorithm by (Banerjee
and Pedersen, 2002), a WordNet-enhanced version
of the well-known dictionary-based algorithm pro-
posed by (Lesk, 1986). The original Lesk was based
on the comparison of the gloss of the word to be dis-
ambiguated with the context words and their glosses.
This enhancement consists in taking into account
also the glosses of concepts related to the word to
be disambiguated by means of various WordNet re-
lationships. Then similarity between a sense of the
word and the context is calculated by means of over-
laps. The word is assigned the sense obtaining the
best overlap match with the glosses of the context
words and their related synsets.
The weights used as input for Formula (3) are the
similarity values between the senses of the world
and the context words. The context for this ex-
pert consists of 4 WordNet words (disregarding their
Part-Of-Speech) located in the same sentence of the
word to be disambiguated, i.e., words with POS
noun, verb, adjective or adverb that can be found in
WordNet.
3.4 WordNet Domains
This expert uses WordNet Domains (Magnini and
Cavaglia`, 2000) in order to provide the system with
domain-awareness. All WordNet words in the same
sentence of the target word are used as context. The
weight for each sense is obtained by counting the
number of times the same domain of the sense ap-
pears in the context (all senses of context words are
considered). We decided to not take into account the
?factotum? domain.
4 Example
In this example we will consider only the sense
probability and extended Lesk experts for simplic-
ity.
Let us consider the following phrase: ?And he has
kept mum on how his decision might affect a bid
for United Airlines , which includes a big stake by
British Airways PLC.? with affect as target word.
We can observe that in WordNet the verb affect has
5 senses. The sense count values are 43 for the first
sense, 11 for the second, 4 for both the third and the
fourth one, and 0 for the last one. We decided to nor-
malize the cases with 0 occurrences to 1. After ap-
plying the transformation (3) to the sense counts, we
obtain the following preference matrix for the sense
probability expert:
?
?
?
?
?
?
?
0.5 0.80 0.91 0.91 0.98
0.20 0.5 0.73 0.73 0.92
0.09 0.27 0.5 0.5 0.8
0.09 0.27 0.5 0.5 0.8
0.02 0.08 0.2 0.2 0.5
?
?
?
?
?
?
?
Therefore, the final fuzzy Borda counts by the
sense probability expert are 3.60 for affect(1),
2.38 for affect(2), 0.8 for affect(3) and
affect(4), and 0 for affect(5), obtained
from the sum of the rows where the value is greater
than 0.5.
The extended Lesk expert calculates the following
similarity scores for thesenses of affect, with context
words decision, might, bid and include: respectively
107, 70, 35, 63 and 71 for senses 1 to 5. After apply-
ing the transformation (3) to the weights, we obtain
436
the preference matrix for this expert:
?
?
?
?
?
?
?
0.5 0.60 0.75 0.63 0.60
0.40 0.5 0.67 0.53 0.49
0.25 0.33 0.5 0.36 0.33
0.37 0.47 0.64 0.5 0.47
0.40 0.51 0.67 0.53 0.5
?
?
?
?
?
?
?
In this case the final fuzzy Borda counts are 2.58 for
the first sense, 1.2 for sense 2, 0 for sense 3, 0.64
and 1.71 for senses 4 and 5 respectively.
Finally, the sum of Borda counts of every expert
for each sense (see Table 4) are used to disambiguate
the word.
sense no: 1 2 3 4 5
expert 1 3.60 2.38 0.80 0.80 0
expert 2 2.58 1.20 0 0.64 1.71
total: 6.18 3.58 0.80 1.44 1.71
Table 1: Borda Count for the verb affect in the ex-
ample phrase.
5 Results
The system was not tested before SemEval. Our par-
ticipation was limited to the All-Word and Coarse-
Grained tasks (without the sense inventory provided
by the organizers). The results are compared to the
best system and the MFS (Most Frequent Sense)
baseline. We calculated also the partial results over
nouns in the all word task, obtaining that the MFS
baseline in this case is about 0.633, whereas our sys-
tem obtains 0.520.
task upv-wsd MFS best system
coarse-grained 0.786 0.789 0.832
awt 0.420 0.471 0.537
Table 2: Recall obtained by our system (upv-wsd)
in each task we participated in, compared with the
most frequent sense baseline and the best system in
the task.
6 Conclusions
The combination of different systems allowed us to
attain higher recall than with our previous system
used in Senseval-3. However, overall results were
not as good as expected. Partial results over the
nouns show that the CD expert did not perform as
in the Senseval-3 and that the CD formula needs to
include sense frequency ranking in order to achieve
a good performance. As a further work we plan to
add a weight reflecting the reliability of each expert.
Acknowledgements
We would like to thank the TIN2006-15265-C06-04 research
project for partially supporting this work. We would also like to
thank Prof. Eugene Levner of the Holon Institute of Technology
for inspiring us to use the fuzzy Borda voting scheme.
References
Eneko Agirre and German Rigau. 1996. Word sense dis-
ambiguation using conceptual density. In COLING,
pages 16?22.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proceedings of CICLing 2002, pages 136?
145, London, UK. Springer-Verlag.
Davide Buscaldi, Paolo Rosso, and Francesco Masulli.
2004. The upv-unige-CIAOSENSO WSD System.
In Proc. of Senseval-3 Workshop, Barcelona (Spain),
July. ACL.
Jose? Luis Garc??a Lapresta and Miguel Mart??nez Panero.
2002. Borda Count Versus Approval Voting: A Fuzzy
Approach. Public Choice, 112(1-2):167?184.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proc. of SIGDOC
?86, pages 24?26.
Bernardo Magnini and Gabriela Cavaglia`. 2000. Inte-
grating Subject Field Codes into WordNet. In Proc. of
the 2nd LREC Conference, pages 1413?1418, Athens,
Greece.
Hannu Nurmi. 2001. Resolving Group Choice Para-
doxes Using Probabilistic and Fuzzy Concepts. Group
Decision and Negotiation, 10(2):177?199.
Paolo Rosso, Francesco Masulli, Davide Buscaldi, Fer-
ran Pla, and Antonio Molina. 2003. Automatic noun
sense disambiguation. In Proc. of CICLing 2003,
pages 273?276.
Sonia Vazquez, Rafael Romero, Armando Suarez, An-
dres Montoyo, Manuel Garc??a, M. Teresa Martin,
M. Angel Garc??a, Alfonso Uren?a, Davide Buscaldi,
Paolo Rosso, Antonio Molina, Ferran Pla, and Encarna
Segarra. 2004. The R2D2 Team at SENSEVAL-3. In
Proc. of Senseval-3 Workshop.
437
The upv-unige-CIAOSENSO WSD System
Davide Buscaldi
  
, Paolo Rosso

, Francesco Masulli

 
DISI, Universita` di Genova, Italy

DSIC, Universidad Polite?cnica de Valencia, Spain

INFM-Genova and Dip. di Informatica, Universita` di Pisa, Italy

dbuscaldi, prosso  @dsic.upv.es

masulli@disi.unige.it
Abstract
The CIAOSENSO WSD system is based on Con-
ceptual Density, WordNet Domains and frequences
of WordNet senses. This paper describes the upv-
unige-CIAOSENSO WSD system, we participated
in the english all-word task with, and its versions
used for the english lexical sample and the Word-
Net gloss disambiguation tasks. In the last an ad-
ditional goal was to check if the disambiguation of
glosses, that has been performed during our tests on
the SemCor corpus, was done properly or not.
Introduction
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density (Agirre and
Rigau, 1995), frequencies of WordNet senses, and
WordNet Domains (Magnini and Cavagli a`, 2000).
Conceptual Density (CD) is a measure of the cor-
relation among the sense of a given word and its
context. The foundation of this measure is the Con-
ceptual Distance, defined as the length of the short-
est path which connects two concepts in a hierar-
chical semantic net. The starting point for our work
was the CD formula of Agirre and Rigau (Agirre
and Rigau, 1995), which compares areas of sub-
hierarchies. The noun sense disambiguation in the
CIAOSENSO WSD system is performed by means
of a formula combining Conceptual Density with
WordNet sense frequency (Rosso et al, 2003).
WordNet Domains is an extension of WordNet
1.6, developed at ITC-irst1, where each synset has
been annotated with at least one domain label,
selected from a set of about two hundred labels
hierarchically organized (Magnini and Cavagli a`,
2000). Since the lexical resource used by the upv-
unige-CIAOSENSO WSD system is WordNet 2.0
(WN2.0), it has been necessary to map the synsets
of WordNet Domains from version 1.6 to the ver-
sion 2.0. This has been done in a fully automated
way, by using the WordNet mappings for nouns and
1Istituto per la Ricerca Scientifica e Tecnologica, Trento,
Italy
verbs, and by checking the similarity of synset terms
and glosses for adjectives and adverbs. Some do-
mains have also been assigned by hand in some
cases, when necessary.
1 Noun Sense Disambiguation
In our upv-unige-CIAOSENSO WSD system the
noun sense disambiguation is carried out by means
of the formula presented in (Rosso et al, 2003),
which gave good results for the disambiguation of
nouns over the SemCor corpus (precision 0.815).
This formula has been derived from the original
Conceptual Density formula described in (Agirre
and Rigau, 1995):

	First Joint Conference on Lexical and Computational Semantics (*SEM), pages 552?556,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
IRIT: Textual Similarity Combining Conceptual Similarity with an N-Gram
Comparison Method
Davide Buscaldi, Ronan Tournier, Nathalie Aussenac-Gilles and Josiane Mothe
IRIT
118 Route de Narbonne
Toulouse (France)
{davide.buscaldi,ronan.tournier}@irit.fr,
{nathalie.aussenac,josiane.mothe}@irit.fr
Abstract
This paper describes the participation of the
IRIT team to SemEval 2012 Task 6 (Seman-
tic Textual Similarity). The method used con-
sists of a n-gram based comparison method
combined with a conceptual similarity mea-
sure that uses WordNet to calculate the sim-
ilarity between a pair of concepts.
1 Introduction
The system used for the participation of the IRIT
team (composed by members of the research groups
SIG and MELODI) to the Semantic Textual Similar-
ity (STS) task (Agirre et al, 2012) is based on two
sub-modules:
? a module that calculates the similarity between
sentences using n-gram based similarity;
? a module that calculates the similarity between
concepts in the two sentences, using a concept
similarity measure and WordNet (Miller, 1995)
as a resource.
In Figure 1, we show the structure of the sys-
tem and the connections between the main compo-
nents. The input phrases are passed on one hand
directly to the n-gram similarity module, and on the
other they are annoted with the Stanford POS Tag-
ger (Toutanova et al, 2003). All nouns and verbs are
extracted from the tagged phrases and WordNet is
searched for synsets corresponding to the extracted
nouns and nouns associated to the verbs by the de-
rived terms relationship. The synsets are the con-
cepts used by the conceptual similarity module to
Phrases 
N-gram similarity module 
POS Tagger 
Google Web 1T 
WordNet 
Concept similarity module 
Score 
Geometric Average and Normalisation 
Concept Extraction 
Figure 1: Schema of the system.
calculate the concept similarity. Each module cal-
culates a similarity score using its own method; the
final similarity value is calculated as the geometric
average between the two scores, multiplied by 5 in
order to comply with the task specifications.
The n-gram based similarity relies on the idea
that two sentences are semantically related if they
contain a long enough sub-sequence of non-empty
terms. Google Web 1T (Brants and Franz, 2006)
has been used to calculate term idf, which is used
as a measure of the importance of the terms. The
conceptual similarity is based on the idea that, given
an ontology, two concepts are semantically similar
if their distance from a common ancestor is small
enough. We used three different measures: the Wu-
Palmer similarity measure (Wu and Palmer, 1994)
and two ?Proxigenea? measures (Dudognon et al,
2010). In the following we will explain in detail how
552
each similarity module works.
2 N-Gram based Similarity
N-gram based similarity is based on the Clustered
Keywords Positional Distance (CKPD) model pro-
posed in (Buscaldi et al, 2009). This model was
originally proposed for passage retrieval in the field
of Question Answering (QA), and it has been im-
plemented in the JIRS system1. In (Buscaldi et al,
2006), JIRS showed to be able to obtain a better an-
swer coverage in the Question Answering task than
other traditional passage retrieval models based on
Vector Space Model, such as Lucene2. The model
has been adapted for this task by calculating the idf
weights for each term using the frequency value pro-
vided by Google Web 1T.
The similarity between a text fragment (or pas-
sage) p and another text fragment q is calculated as:
Sim(p, q) =
?
?x?Q
h(x, P )
1
d(x, xmax)
?n
i=1 wi
(1)
Where P is the set of n-grams with the highest
weight in p, where all terms are also contained in q;
Q is the set of all the possible j-grams in q and n
is the total number of terms in the longest passage.
The weights for each term and each n-gram are cal-
culated as:
? wi calculates the weight of the term tI as:
wi = 1?
log(ni)
1 + log(N)
(2)
Where ni is the frequency of term ti in the
Google Web 1T collection, and N is the fre-
quency of the most frequent term in the Google
Web 1T collection.
? the function h(x, P ) measures the weight of
each n-gram and is defined as:
h(x, Pj) =
{ ?j
k=1 wk if x ? Pj
0 otherwise
(3)
1http://sourceforge.net/projects/jirs/
2http://lucene.apache.org/
Where wk is the weight of the k-th term (see
Equation 2) and j is the number of terms that
compose the n-gram x;
? 1d(x,xmax) is a distance factor which reduces the
weight of the n-grams that are far from the
heaviest n-gram. The function d(x, xmax) de-
termines numerically the value of the separa-
tion according to the number of words between
a n-gram and the heaviest one. That function is
defined as show in Equation 4 :
d(x, xmax) = 1 + k? ln(1 + L) (4)
Where k is a factor that determines the impor-
tance of the distance in the similarity calcula-
tion and L is the number of words between a
n-gram and the heaviest one (see Equation 3).
In our experiments, k was set to 0.1, a default
value used in JIRS.
For instance, given the following two sentences:
?Mr. President, enlargement is essential for the con-
struction of a strong and united European continent?
and ?Mr. President, widening is essential for the
construction of a strong and plain continent of Eu-
rope?, the longest n-grams shared by the two sen-
tences are: ?Mr. President?, ?is essential for the
construction of a strong and?, ?continent?.
term w(term)
Mr 0.340
President 0.312
is 0.159
essential 0.353
for 0.153
the 0.104
construction 0.332
of 0.120
a 0.139
strong 0.329
and 0.121
continent 0.427
of 0.120
Europe 0.308
widening 0.464
Table 1: Term weights (idf) calculated using the fre-
quency for each term in Google Web 1T unigrams set.
553
Figure 2: Visualisation of depth calculation.
The weights have been calculated with Formula
2, using the frequencies from Google Web 1T. The
weights for each of the longest n-grams are 0.652,
1.809 and 0.427 respectively; their sum is 2.888
which divided by all the term weights contained
in the sentence gives 0.764 which is the similarity
score between the two sentences as calculated by the
n-gram based method.
3 Conceptual Similarity
Given Cp and Cq as the sets of concepts contained in
sentence p and q, respectively, with |Cp| ? |Cq|, the
conceptual similarity between p and q is calculated
as:
ss(p, q) =
?
c1?Cp
max
c2?Cq
s(c1, c2)
|Cp|
(5)
where s(c1, c2) is a concept similarity measure.
Concept similarity can be calculated by different
ways. Wu and Palmer introduced in (Wu and
Palmer, 1994) a concept similarity measure defined
as:
s(c1, c2) =
2 ? d(c0)
d(c1) + d(c2)
(6)
c0 is the most specific concept that is present both
in the synset path of c1 and c2 (see Figure 2 for de-
tails). The function returning the depth of a concept
is noted with d.
3.1 ProxiGenea
By making an analogy between a family tree and
the concept hierarchy in WordNet, (Dudognon et al,
2010; Ralalason, 2010) proposed a concept similar-
ity measure based on the principle of evaluating the
proximity between two members of the same fam-
ily. The measure has been named ?ProxiGenea?
(from the french Proximite? Ge?ne?alogique, genealog-
ical proximity). We took into account three versions
of the ProxiGenea measure:
pg1(c1, c2) =
d(c0)2
d(c1) ? d(c2)
(7)
This measure is very similar to the Wu-Palmer sim-
ilarity measure, but it emphasizes the distances be-
tween concepts;
pg2(c1, c2) =
d(c0)
d(c1) + d(c2)? d(c0)
(8)
In this measure, the more are the elements which are
not shared between the paths of c1 and c2, the more
the score decreases. However, if the elements are
placed more deeply in the ontology, the decrease is
less important.
pg3(c1, c2) =
1
1 + d(c1) + d(c2)? 2 ? d(c0)
(9)
In Table 2 we show the weights that have been
calculated for each concept, using all the above sim-
ilarity measures, and the concept that provided the
maximum weight. No Word Sense Disambiguation
process is carried out; therefore, the scores are cal-
culated taking into account all the possible senses
for the word. If the same concept is present in both
sentences, it obtains always a score of 1. In the other
cases, the maximum similarity value obtained with
any other concept is retained.
From the example in Table 2 we can see that Wu-
Palmer tends to give to the concepts a higher simi-
larity value than Proxigenea3.
The final score for the above example is cal-
culated as the geometric mean between the scores
obtained in Table 2 and 0.764 obtained from the
n-gram based similarity module, multiplied by 5.
Therefore, for each similarity measure, the final
scores of the example are, respectively: 4.029,
3.869, 3.921 and 3.703. The correct similarity value,
according to the gold standard, was 4.600.
554
c1, c2 wp pg1 pg2 pg3
Mr
1.000 1.000 1.000 1.000
Mr
President
1.000 1.000 1.000 1.000
President
construction
1.000 1.000 1.000 1.000
construction
continent
1.000 1.000 1.000 1.000
continent
Europe
0.400 0.160 0.250 0.143
continent
widening
0.737 0.544 0.583 0.167
enlargement
score 0.850 0.784 0.805 0.718
Table 2: Maximum conceptual similarity weights using
the different formulae for the concepts in the example.
c1: first concept, c2: concept for which the maximum
similarity value was calculated. wp: Wu-Palmer similar-
ity; pgX : Proxigenea similarity. score is the result of (5).
4 Evaluation
Before the official runs we carried out an evalua-
tion to select the best similarity measures over the
training set provided by the organisers. The results
of this evaluation are shown in Table 3. The mea-
sure selected is the normalised Pearson correlation
(Agirre et al, 2012). We evaluated also the use of
the product instead of the geometric mean for the
combination of the two scores.
Geometric mean
MSRpar MSRvid SMT-Eur All
pg1 0.489 0.602 0.587 0.559
pg2 0.490 0.596 0.586 0.558
pg3 0.470 0.657 0.552 0.560
wp 0.494 0.572 0.592 0.552
Scalar product
MSRpar MSRvid SMT-Eur All
pg1 0.469 0.601 0.487 0.519
pg2 0.471 0.597 0.487 0.518
pg3 0.447 0.637 0.459 0.514
wp 0.476 0.577 0.492 0.515
Table 3: Results on training corpus, comparison of dif-
ferent conceptual similarity measures and combination
method. Top: geometric mean, bottom: product.
We used these results to select the final config-
urations for our participation to the STS task: we
selected to exclude Proxigenea 2 and to use the ge-
ometric mean to combine the scores of the n-gram
based similarity module and the conceptual similar-
ity module. Wu-Palmer similarity allowed to obtain
the best results on two train sets but Proxigenea 3
was the similarity measure that obtained the best av-
erage score thanks to the good result on MSRvid.
The official results obtained by our system are
shown in Table 4, with the ranking obtained for each
test set. We could observe that the system was well
r best pg3 pg1 wp
MSRPar 60 0.734 0.417 0.429 0.433
MSRvid 58 0.880 0.673 0.612 0.583
SMTeur 7 0.567 0.518 0.495 0.486
OnWN 64 0.727 0.553 0.539 0.532
SMTnews 55 0.608 0.369 0.361 0.348
All 58 0.677 0.520 0.501 0.490
Table 4: Results obtained on each test set, grouped by
conceptual similarity method. r indicates the ranking
among all the participants teams.
behind the best system in most test sets, except for
SMTeur. This was expected since our system does
not use a machine learning approach and is com-
pletely unsupervised, while the best systems used
supervised learning. We observed also that the be-
haviour of the concept similarity measures was dif-
ferent from the behaviour on the training sets. In the
competition, the best results were always obtained
with Proxigenea3 instead of Wu-Palmer, except for
the MSRpar test set.
In Table 4 we extrapolated the results for the com-
posing methods and compared them with the result
obtained after their combination. We used the pg3
configuration for the conceptual similarity measure.
From these results, we can observe that MSRvid
was a test set where the conceptual similarity alone
would have resulted better than the combination of
scores, while SMT-news was the test set where the
CKPD measure obtained the best results in compar-
ison to the result obtained by the conceptual simi-
larity alone. It was quite surprising to observe such
a good result for a method that does not take into
account any information about the structure of the
sentences, actually viewing them as ?bags of con-
555
Combined pg3 CKPD
MSRPar 0.417 0.412 0.417
MSRvid 0.673 0.777 0.548
SMTeuroparl 0.518 0.486 0.467
OnWN 0.553 0.544 0.505
SMTnews 0.369 0.266 0.408
Table 5: Results obtained for each test set using only the
conceptual similarity measure (pg3) and only the struc-
tural similarity measure (CKPD), compared to the re-
sult obtained by the complete system (Combined).
cepts?. This is probably due to the fact that SMT-
news is a corpus composed of automatically trans-
lated sentences, where structural similarity is an im-
portant clue for determining overall semantic sim-
ilarity. On the other hand, MSRvid sentences are
very short, and CKPD is in most cases unable to cap-
ture the semantic similarity.
5 Conclusions
The proposed method combined a measure of struc-
tural similarity and a measure of conceptual simi-
larity based on WordNet. With the participation to
this task, we were interested in studying the differ-
ences between different conceptual similarity mea-
sures and in determining whether they can be used
to effectively measure the semantic similarity of text
fragments. The obtained results showed that Proxi-
genea 3 allowed us to obtain the best results, indicat-
ing that under the test conditions and with WordNet
as a resource it overperforms the Wu-Palmer mea-
sure. Further studies may be required in order to
determine if these results can be generalised to other
collections and in using different ontologies. We are
also interested in comparing the method to the Lin
concept similarity measure (Lin, 1998) which takes
into account also the importance of the local root
concept.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gon-
zalez. 2012. A pilot on semantic textual similarity.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2012), Montreal, Quebec,
Canada.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Davide Buscaldi, Jose? Manuel Go?mez, Paolo Rosso, and
Emilio Sanchis. 2006. N-gram vs. keyword-based
passage retrieval for question answering. In CLEF,
pages 377?384.
Davide Buscaldi, Paolo Rosso, Jose? Manuel Go?mez, and
Emilio Sanchis. 2009. Answering questions with an
n-gram based passage retrieval engine. Journal of In-
telligent Information Systems (JIIS), 34(2):113?134.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxige?ne?a : Une mesure
de similarite? conceptuelle. In Proceedings of the Col-
loque Veille Strate?gique Scientifique et Technologique
(VSST 2010).
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, ICML
?98, pages 296?304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41, November.
Bachelin Ralalason. 2010. Repre?sentation multi-facette
des documents pour leur acce`s se?mantique. Ph.D. the-
sis, Universite? Paul Sabatier, Toulouse, September. in
French.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, ACL ?94, pages 133?138, Stroudsburg, PA,
USA. Association for Computational Linguistics.
556
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 162?168, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic
Analysis, ESA and Information Retrieval based Features
Davide Buscaldi, Joseph Le Roux,
Jorge J. Garc??a Flores
Laboratoire d?Informatique de Paris Nord,
CNRS, (UMR 7030)
Universite? Paris 13, Sorbonne Paris Cite?,
F-93430, Villetaneuse, France
{buscaldi,joseph.le-roux,jgflores}
@lipn.univ-paris13.fr
Adrian Popescu
CEA, LIST,
Vision & Content
Engineering Laboratory
F-91190 Gif-sur-Yvette, France
adrian.popescu@cea.fr
Abstract
This paper describes the system used by the
LIPN team in the Semantic Textual Similarity
task at *SEM 2013. It uses a support vector re-
gression model, combining different text sim-
ilarity measures that constitute the features.
These measures include simple distances like
Levenshtein edit distance, cosine, Named En-
tities overlap and more complex distances like
Explicit Semantic Analysis, WordNet-based
similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
1 Introduction
The Semantic Textual Similarity task (STS) at
*SEM 2013 requires systems to grade the degree of
similarity between pairs of sentences. It is closely
related to other well known tasks in NLP such as tex-
tual entailment, question answering or paraphrase
detection. However, as noticed in (Ba?r et al, 2012),
the major difference is that STS systems must give a
graded, as opposed to binary, answer.
One of the most successful systems in *SEM
2012 STS, (Ba?r et al, 2012), managed to grade pairs
of sentences accurately by combining focused mea-
sures, either simple ones based on surface features
(ie n-grams), more elaborate ones based on lexical
semantics, or measures requiring external corpora
such as Explicit Semantic Analysis, into a robust
measure by using a log-linear regression model.
The LIPN-CORE system is built upon this idea of
combining simple measures with a regression model
to obtain a robust and accurate measure of tex-
tual similarity, using the individual measures as fea-
tures for the global system. These measures include
simple distances like Levenshtein edit distance, co-
sine, Named Entities overlap and more complex dis-
tances like Explicit Semantic Analysis, WordNet-
based similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
The paper is organized as follows. Measures are
presented in Section 2. Then the regression model,
based on Support Vector Machines, is described in
Section 3. Finally we discuss the results of the sys-
tem in Section 4.
2 Text Similarity Measures
2.1 WordNet-based Conceptual Similarity
(Proxigenea)
First of all, sentences p and q are analysed in or-
der to extract all the included WordNet synsets. For
each WordNet synset, we keep noun synsets and put
into the set of synsets associated to the sentence, Cp
and Cq, respectively. If the synsets are in one of the
other POS categories (verb, adjective, adverb) we
look for their derivationally related forms in order
to find a related noun synset: if there is one, we put
this synsets in Cp (or Cq). For instance, the word
?playing? can be associated in WordNet to synset
(v)play#2, which has two derivationally related
forms corresponding to synsets (n)play#5 and
(n)play#6: these are the synsets that are added
to the synset set of the sentence. No disambiguation
process is carried out, so we take all possible mean-
ings into account.
GivenCp andCq as the sets of concepts contained
in sentences p and q, respectively, with |Cp| ? |Cq|,
162
the conceptual similarity between p and q is calcu-
lated as:
ss(p, q) =
?
c1?Cp
max
c2?Cq
s(c1, c2)
|Cp|
(1)
where s(c1, c2) is a conceptual similarity measure.
Concept similarity can be calculated by different
ways. For the participation in the 2013 Seman-
tic Textual Similarity task, we used a variation of
the Wu-Palmer formula (Wu and Palmer, 1994)
named ?ProxiGenea? (from the french Proximite?
Ge?ne?alogique, genealogical proximity), introduced
by (Dudognon et al, 2010), which is inspired by the
analogy between a family tree and the concept hi-
erarchy in WordNet. Among the different formula-
tions proposed by (Dudognon et al, 2010), we chose
the ProxiGenea3 variant, already used in the STS
2012 task by the IRIT team (Buscaldi et al, 2012).
The ProxiGenea3 measure is defined as:
s(c1, c2) =
1
1 + d(c1) + d(c2)? 2 ? d(c0)
(2)
where c0 is the most specific concept that is present
both in the synset path of c1 and c2 (that is, the Least
Common Subsumer or LCS). The function returning
the depth of a concept is noted with d.
2.2 IC-based Similarity
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik?s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity metric:
sjc(c1, c2) =
1
IC(c1) + IC(c2)? 2 ? IC(c0)
(3)
where IC is the information content introduced by
(Resnik, 1995) as IC(c) = ? logP (c).
The similarity between two text segments T1 and
T2 is therefore determined as:
sim(T1, T2) =
1
2
?
?
?
?
w?{T1}
max
w2?{T2}
ws(w,w2) ? idf(w)
?
w?{T1}
idf(w)
+
?
w?{T2}
max
w1?{T1}
ws(w,w1) ? idf(w)
?
w?{T2}
idf(w)
?
?
?(4)
where idf(w) is calculated as the inverse document
frequency of word w, taking into account Google
Web 1T (Brants and Franz, 2006) frequency counts.
The semantic similarity between words is calculated
as:
ws(wi, wj) = max
ci?Wi,cjinWj
sjc(ci, cj). (5)
where Wi and Wj are the sets containing all synsets
in WordNet corresponding to word wi and wj , re-
spectively. The IC values used are those calcu-
lated by Ted Pedersen (Pedersen et al, 2004) on the
British National Corpus1.
2.3 Syntactic Dependencies
We also wanted for our systems to take syntac-
tic similarity into account. As our measures are
lexically grounded, we chose to use dependen-
cies rather than constituents. Previous experiments
showed that converting constituents to dependen-
cies still achieved best results on out-of-domain
texts (Le Roux et al, 2012), so we decided to use
a 2-step architecture to obtain syntactic dependen-
cies. First we parsed pairs of sentences with the
LORG parser2. Second we converted the resulting
parse trees to Stanford dependencies3.
Given the sets of parsed dependenciesDp andDq,
for sentence p and q, a dependency d ? Dx is a
triple (l, h, t) where l is the dependency label (for in-
stance, dobj or prep), h the governor and t the depen-
dant. We define the following similarity measure be-
tween two syntactic dependencies d1 = (l1, h1, t1)
and d2 = (l2, h2, t2):
dsim(d1, d2) = Lev(l1, l2)
?
idfh ? sWN (h1, h2) + idft ? sWN (t1, t2)
2
(6)
where idfh = max(idf(h1), idf(h2)) and idft =
max(idf(t1), idf(t2)) are the inverse document fre-
quencies calculated on Google Web 1T for the gov-
ernors and the dependants (we retain the maximum
for each pair), and sWN is calculated using formula
2, with two differences:
? if the two words to be compared are antonyms,
then the returned score is 0;
1
http://www.d.umn.edu/?tpederse/similarity.html
2
https://github.com/CNGLdlab/LORG-Release
3We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
163
? if one of the words to be compared is not in
WordNet, their similarity is calculated using
the Levenshtein distance.
The similarity score between p and q, is then cal-
culated as:
sSD(p, q) = max
?
?
?
?
di?Dp
max
djinDq
dsim(di, dj)
|Dp|
,
?
di?Dq
max
djinDp
dsim(di, dj)
|Dq|
?
?
?
(7)
2.4 Information Retrieval-based Similarity
Let us consider two texts p and q, an Information Re-
trieval (IR) system S and a document collection D
indexed by S. This measure is based on the assump-
tion that p and q are similar if the documents re-
trieved by S for the two texts, used as input queries,
are ranked similarly.
Let be Lp = {dp1 , . . . , dpK} and Lq =
{dq1 , . . . , dqK}, dxi ? D the sets of the top K docu-
ments retrieved by S for texts p and q, respectively.
Let us define sp(d) and sq(d) the scores assigned by
S to a document d for the query p and q, respectively.
Then, the similarity score is calculated as:
simIR(p, q) = 1?
?
d?Lp?Lq
?
(sp(d)?sq(d))2
max(sp(d),sq(d))
|Lp ? Lq|
(8)
if |Lp ? Lq| 6= ?, 0 otherwise.
For the participation in this task we indexed a
collection composed by the AQUAINT-24 and the
English NTCIR-85 document collections, using the
Lucene6 4.2 search engine with BM25 similarity.
The K value was empirically set to 20 after some
tests on the STS 2012 data.
2.5 ESA
Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) represents meaning as a
4
http://www.nist.gov/tac/data/data_desc.html#AQUAINT-2
5
http://metadata.berkeley.edu/NTCIR-GeoTime/
ntcir-8-databases.php
6
http://lucene.apache.org/core
weighted vector of Wikipedia concepts. Weights
are supposed to quantify the strength of the relation
between a word and each Wikipedia concept using
the tf-idf measure. A text is then represented as a
high-dimensional real valued vector space spanning
all along the Wikipedia database. For this particular
task we adapt the research-esa implementation
(Sorg and Cimiano, 2008)7 to our own home-made
weighted vectors corresponding to a Wikipedia
snapshot of February 4th, 2013.
2.6 N-gram based Similarity
This feature is based on the Clustered Keywords Po-
sitional Distance (CKPD) model proposed in (Bus-
caldi et al, 2009) for the passage retrieval task.
The similarity between a text fragment p and an-
other text fragment q is calculated as:
simngrams(p, q) =
?
?x?Q
h(x, P )
1
d(x, xmax)
?n
i=1wi
(9)
Where P is the set of n-grams with the highest
weight in p, where all terms are also contained in q;
Q is the set of all the possible n-grams in q and n
is the total number of terms in the longest passage.
The weights for each term and each n-gram are cal-
culated as:
? wi calculates the weight of the term tI as:
wi = 1?
log(ni)
1 + log(N)
(10)
Where ni is the frequency of term ti in the
Google Web 1T collection, and N is the fre-
quency of the most frequent term in the Google
Web 1T collection.
? the function h(x, P ) measures the weight of
each n-gram and is defined as:
h(x, Pj) =
{ ?j
k=1wk if x ? Pj
0 otherwise
(11)
7
http://code.google.com/p/research-esa/
164
Where wk is the weight of the k-th term (see
Equation 10) and j is the number of terms that
compose the n-gram x;
? 1d(x,xmax) is a distance factor which reduces the
weight of the n-grams that are far from the
heaviest n-gram. The function d(x, xmax) de-
termines numerically the value of the separa-
tion according to the number of words between
a n-gram and the heaviest one:
d(x, xmax) = 1 + k? ln(1 + L) (12)
where k is a factor that determines the impor-
tance of the distance in the similarity calcula-
tion and L is the number of words between a
n-gram and the heaviest one (see Equation 11).
In our experiments, k was set to 0.1, the default
value in the original model.
2.7 Other measures
In addition to the above text similarity measures, we
used also the following common measures:
2.7.1 Cosine
Given p = (wp1 , . . . , wpn) and q =
(wq1 , . . . , wqn) the vectors of tf.idf weights asso-
ciated to sentences p and q, the cosine distance is
calculated as:
simcos(p,q) =
n?
i=1
wpi ? wqi
?
n?
i=1
wpi2 ?
?
n?
i=1
wqi2
(13)
The idf value was calculated on Google Web 1T.
2.7.2 Edit Distance
This similarity measure is calculated using the
Levenshtein distance as:
simED(p, q) = 1?
Lev(p, q)
max(|p|, |q|)
(14)
where Lev(p, q) is the Levenshtein distance be-
tween the two sentences, taking into account the
characters.
2.7.3 Named Entity Overlap
We used the Stanford Named Entity Recognizer
by (Finkel et al, 2005), with the 7 class model
trained for MUC: Time, Location, Organization,
Person, Money, Percent, Date. Then we calculated a
per-class overlap measure (in this way, ?France? as
an Organization does not match ?France? as a Loca-
tion):
ONER(p, q) =
2 ? |Np ?Nq|
|Np|+ |Nq|
(15)
where Np and Nq are the sets of NEs found, respec-
tively, in sentences p and q.
3 Integration of Similarity Measures
The integration has been carried out using the
?-Support Vector Regression model (?-SVR)
(Scho?lkopf et al, 1999) implementation provided
by LIBSVM (Chang and Lin, 2011), with a radial
basis function kernel with the standard parameters
(? = 0.5).
4 Results
In order to evaluate the impact of the different fea-
tures, we carried out an ablation test, removing one
feature at a time and training a new model with the
reduced set of features. In Table 2 we show the re-
sults of the ablation test for each subset of the *SEM
2013 test set; in Table 1 we show the same test on the
whole test set. Note: the results have been calculated
as the Pearson correlation test on the whole test set
and not as an average of the correlation scores cal-
culated over the composing test sets.
Feature Removed Pearson Loss
None 0.597 0
N-grams 0.596 0.10%
WordNet 0.563 3.39%
SyntDeps 0.602 ?0.43%
Edit 0.584 1.31%
Cosine 0.596 0.10%
NE Overlap 0.603 ?0.53%
IC-based 0.598 ?0.10%
IR-Similarity 0.510 8.78%
ESA 0.601 ?0.38%
Table 1: Ablation test for the different features on the
whole 2013 test set.
165
FNWN Headlines OnWN SMT
Feature Pearson Loss Pearson Loss Pearson Loss Pearson Loss
None 0.404 0 0.706 0 0.694 0 0.301 0
N-grams 0.379 2.49% 0.705 0.12% 0.698 ?0.44% 0.289 1.16%
WordNet 0.376 2.80% 0.695 1.09% 0.682 1.17% 0.278 2.28%
SyntDeps 0.403 0.08% 0.699 0.70% 0.679 1.49% 0.284 1.62%
Edit 0.402 0.19% 0.689 1.70% 0.667 2.72% 0.286 1.50%
Cosine 0.393 1.03% 0.683 2.38% 0.676 1.80% 0.303 ?0.24%
NE Overlap 0.410 ?0.61% 0.700 0.67% 0.680 1.37% 0.285 1.58%
IC-based 0.391 1.26% 0.699 0.75% 0.669 2.50% 0.283 1.76%
IR-Similarity 0.426 ?2.21% 0.633 7.33% 0.589 10.46% 0.249 5.19%
ESA 0.391 1.22% 0.691 1.57% 0.702 ?0.81% 0.275 2.54%
Table 2: Ablation test for the different features on the different parts of the 2013 test set.
FNWN Headlines OnWN SMT ALL
N-grams 0.285 0.532 0.459 0.280 0.336
WordNet 0.395 0.606 0.552 0.282 0.477
SyntDeps 0.233 0.409 0.345 0.323 0.295
Edit 0.220 0.536 0.089 0.355 0.230
Cosine 0.306 0.573 0.541 0.244 0.382
NE Overlap 0.000 0.216 0.000 0.013 0.020
IC-based 0.413 0.540 0.642 0.285 0.421
IR-based 0.067 0.598 0.628 0.241 0.541
ESA 0.328 0.546 0.322 0.289 0.390
Table 3: Pearson correlation calculated on individual features.
The ablation test show that the IR-based feature
showed up to be the most effective one, especially
for the headlines subset (as expected), and, quite sur-
prisingly, on the OnWN data. In Table 3 we show
the correlation between each feature and the result
(feature values normalised between 0 and 5): from
this table we can also observe that, on average, IR-
based similarity was better able to capture the se-
mantic similarity between texts. The only exception
was the FNWN test set: the IR-based similarity re-
turned a 0 score 178 times out of 189 (94.1%), indi-
cating that the indexed corpus did not fit the content
of the FNWN sentences. This result shows also the
limits of the IR-based similarity score which needs
a large corpus to achieve enough coverage.
4.1 Shared submission with INAOE-UPV
One of the files submitted by INAOE-UPV,
INAOE-UPV-run3 has been produced using seven
features produced by different teams: INAOE, LIPN
and UMCC-DLSI. We contributed to this joint sub-
mission with the IR-based, WordNet and cosine fea-
tures.
5 Conclusions and Further Work
In this paper we introduced the LIPN-CORE sys-
tem, which combines semantic, syntactic an lexi-
cal measures of text similarity in a linear regression
model. Our system was among the best 15 runs for
the STS task. According to the ablation test, the best
performing feature was the IR-based one, where a
sentence is considered as a query and its meaning
represented as a set of documents indexed by an IR
system. The second and third best-performing mea-
sures were WordNet similarity and Levenshtein?s
edit distance. On the other hand, worst perform-
ing similarity measures were Named Entity Over-
lap, Syntactic Dependencies and ESA. However, a
correlation analysis calculated on the features taken
one-by-one shows that the contribution of a feature
166
on the overall regression result does not correspond
to the actual capability of the measure to represent
the semantic similarity between the two texts. These
results raise the methodological question of how to
combine semantic, syntactic and lexical similarity
measures in order to estimate the impact of the dif-
ferent strategies used on each dataset.
Further work will include richer similarity mea-
sures, like quasi-synchronous grammars (Smith and
Eisner, 2006) and random walks (Ramage et al,
2009). Quasi-synchronous grammars have been
used successfully for paraphrase detection (Das and
Smith, 2009), as they provide a fine-grained model-
ing of the alignment of syntactic structures, in a very
flexible way, enabling partial alignments and the in-
clusion of external features, like Wordnet lexical re-
lations for example. Random walks have been used
effectively for paraphrase recognition and as a fea-
ture for recognizing textual entailment. Finally, we
will continue analyzing the question of how to com-
bine a wide variety of similarity measures in such a
way that they tackle the semantic variations of each
dataset.
Acknowledgments
We would like to thank the Quaero project and the
LabEx EFL8 for their support to this work.
References
[Ba?r et al2012] Daniel Ba?r, Chris Biemann, Iryna
Gurevych, and Torsten Zesch. 2012. Ukp: Computing
semantic textual similarity by combining multiple
content similarity measures. In Proceedings of the
6th International Workshop on Semantic Evaluation,
held in conjunction with the 1st Joint Conference
on Lexical and Computational Semantics, pages
435?440, Montreal, Canada, June.
[Brants and Franz2006] Thorsten Brants and Alex Franz.
2006. Web 1t 5-gram corpus version 1.1.
[Buscaldi et al2009] Davide Buscaldi, Paolo Rosso,
Jose? Manuel Go?mez, and Emilio Sanchis. 2009. An-
swering questions with an n-gram based passage re-
trieval engine. Journal of Intelligent Information Sys-
tems (JIIS), 34(2):113?134.
[Buscaldi et al2012] Davide Buscaldi, Ronan Tournier,
Nathalie Aussenac-Gilles, and Josiane Mothe. 2012.
8http://www.labex-efl.org
Irit: Textual similarity combining conceptual simi-
larity with an n-gram comparison method. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012), Montreal, Que-
bec, Canada.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector
machines. ACM Transactions on Intelligent Systems
and Technology, 2:27:1?27:27. Software available
at http://www.csie.ntu.edu.tw/?cjlin/
libsvm.
[Das and Smith2009] Dipanjan Das and Noah A. Smith.
2009. Paraphrase identification as probabilistic quasi-
synchronous recognition. In Proc. of ACL-IJCNLP.
[Dudognon et al2010] Damien Dudognon, Gilles Hubert,
and Bachelin Jhonn Victorino Ralalason. 2010.
Proxige?ne?a : Une mesure de similarite? conceptuelle.
In Proceedings of the Colloque Veille Strate?gique Sci-
entifique et Technologique (VSST 2010).
[Finkel et al2005] Jenny Rose Finkel, Trond Grenager,
and Christopher Manning. 2005. Incorporating non-
local information into information extraction systems
by gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 363?370, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit semantic
analysis. In Proceedings of the 20th international joint
conference on Artifical intelligence, IJCAI?07, pages
1606?1611, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
[Jiang and Conrath1997] J.J. Jiang and D.W. Conrath.
1997. Semantic similarity based on corpus statistics
and lexical taxonomy. In Proc. of the Int?l. Conf. on
Research in Computational Linguistics, pages 19?33.
[Le Roux et al2012] Joseph Le Roux, Jennifer Foster,
Joachim Wagner, Rasul Samad Zadeh Kaljahi, and
Anton Bryl. 2012. DCU-Paris13 Systems for the
SANCL 2012 Shared Task. In The NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1?4, Montre?al, Canada,
June.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence - Volume 1, AAAI?06, pages 775?
780. AAAI Press.
[Pedersen et al2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Wordnet::similarity:
measuring the relatedness of concepts. In Demon-
stration Papers at HLT-NAACL 2004, HLT-NAACL?
167
Demonstrations ?04, pages 38?41, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Ramage et al2009] Daniel Ramage, Anna N. Rafferty,
and Christopher D. Manning. 2009. Random walks
for text semantic similarity. In Proceedings of the
2009 Workshop on Graph-based Methods for Natural
Language Processing, pages 23?31. The Association
for Computer Linguistics.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Scho?lkopf et al1999] Bernhard Scho?lkopf, Peter
Bartlett, Alex Smola, and Robert Williamson. 1999.
Shrinking the tube: a new support vector regression
algorithm. In Proceedings of the 1998 conference on
Advances in neural information processing systems II,
pages 330?336, Cambridge, MA, USA. MIT Press.
[Smith and Eisner2006] David A. Smith and Jason Eisner.
2006. Quasi-synchronous grammars: Alignment by
soft projection of syntactic dependencies. In Proceed-
ings of the HLT-NAACL Workshop on Statistical Ma-
chine Translation, pages 23?30, New York, June.
[Sorg and Cimiano2008] Philipp Sorg and Philipp Cimi-
ano. 2008. Cross-lingual Information Retrieval with
Explicit Semantic Analysis. In Working Notes for the
CLEF 2008 Workshop.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting on Association
for Computational Linguistics, ACL ?94, pages 133?
138, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
168
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400?405,
Dublin, Ireland, August 23-24, 2014.
LIPN: Introducing a new Geographical Context Similarity Measure and a
Statistical Similarity Measure Based on the Bhattacharyya Coefficient
Davide Buscaldi, Jorge J. Garc??a Flores, Joseph Le Roux, Nadi Tomeh
Laboratoire d?Informatique de Paris Nord, CNRS (UMR 7030)
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
{buscaldi,jgflores,joseph.le-roux,nadi.tomeh}@lipn.univ-paris13.fr
Bel
?
em Priego Sanchez
Laboratoire LDI (Lexique, Dictionnaires, Informatique)
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
LKE, FCC, BUAP, San Manuel, Puebla, Mexico
belemps@gmail.com
Abstract
This paper describes the system used by
the LIPN team in the task 10, Multilin-
gual Semantic Textual Similarity, at Sem-
Eval 2014, in both the English and Span-
ish sub-tasks. The system uses a sup-
port vector regression model, combining
different text similarity measures as fea-
tures. With respect to our 2013 partici-
pation, we included a new feature to take
into account the geographical context and
a new semantic distance based on the
Bhattacharyya distance calculated on co-
occurrence distributions derived from the
Spanish Google Books n-grams dataset.
1 Introduction
After our participation at SemEval 2013 with
LIPN-CORE (Buscaldi et al., 2013) we found that
geography has an important role in discriminating
the semantic similarity of sentences (especially in
the case of newswire). If two events happened in
a different location, their semantic relatedness is
usually low, no matter if the events are the same.
Therefore, we worked on a similarity measure able
to capture the similarity between the geographic
contexts of two sentences. We tried also to rein-
force the semantic similarity features by introduc-
ing a new measure that calculates word similari-
ties on co-occurrence distributions extracted from
Google Books bigrams. This measure was intro-
duced only for the Spanish runs, due to time con-
straints. The regression model used to integrate
the features was the ?-Support Vector Regression
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
model (?-SVR) (Sch?olkopf et al., 1999) imple-
mentation provided by LIBSVM (Chang and Lin,
2011), with a radial basis function kernel with the
standard parameters (? = 0.5). We describe all
the measures in Section 2; the results obtained by
the system are detailed in Section 3.
2 Similarity Measures
In this section we describe the measures used as
features in our system. The description of mea-
sures already used in our 2013 participation is less
detailed than the description of the new ones. Ad-
ditional details on the measures may be found in
(Buscaldi et al., 2013). When POS tagging and
NE recognition were required, we used the Stan-
ford CoreNLP
1
for English and FreeLing
2
3.1 for
Spanish.
2.1 WordNet-based Conceptual Similarity
This measure has been introduced in order to mea-
sure similarities between concepts with respect to
an ontology. The similarity is calculated as fol-
lows: first of all, words in sentences p and q are
lemmatised and mapped to the related WordNet
synsets. All noun synsets are put into the set of
synsets associated to the sentence, C
p
and C
q
, re-
spectively. If the synsets are in one of the other
POS categories (verb, adjective, adverb) we look
for their derivationally related forms in order to
find a related noun synset: if there exists one, we
put this synset in C
p
(or C
q
). No disambigua-
tion process is carried out, so we take all possible
meanings into account.
Given C
p
and C
q
as the sets of concepts con-
tained in sentences p and q, respectively, with
1
http://www-nlp.stanford.edu/software/corenlp.shtml
2
http://nlp.lsi.upc.edu/freeling/
400
|C
p
| ? |C
q
|, the conceptual similarity between p
and q is calculated as:
ss(p, q) =
?
c
1
?C
p
max
c
2
?C
q
s(c
1
, c
2
)
|C
p
|
where s(c
1
, c
2
) is a conceptual similarity mea-
sure. Concept similarity can be calculated in dif-
ferent ways. We used a variation of the Wu-Palmer
formula (Wu and Palmer, 1994) named ?Proxi-
Genea3?, introduced by (Dudognon et al., 2010),
which is inspired by the analogy between a family
tree and the concept hierarchy in WordNet. The
ProxiGenea3 measure is defined as:
s(c
1
, c
2
) =
1
1 + d(c
1
) + d(c
2
)? 2 ? d(c
0
)
where c
0
is the most specific concept that is
present both in the synset path of c
1
and c
2
(that is,
the Least Common Subsumer or LCS). The func-
tion returning the depth of a concept is noted with
d.
2.2 IC-based Similarity
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik?s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity met-
ric. This measure is more precise than the one
introduced in the previous subsection because it
takes into account also the importance of concepts
and not only their relative position in the hierarchy.
We refer to (Buscaldi et al., 2013) and (Mihalcea
et al., 2006) for a detailed description of the mea-
sure. The idf weights for the words were calcu-
lated using the Google Web 1T (Brants and Franz,
2006) frequency counts, while the IC values used
are those calculated by Ted Pedersen (Pedersen et
al., 2004) on the British National Corpus
3
.
2.3 Syntactic Dependencies
This measure tries to capture the syntactic simi-
larity between two sentences using dependencies.
Previous experiments showed that converting con-
stituents to dependencies still achieved best results
on out-of-domain texts (Le Roux et al., 2012), so
we decided to use a 2-step architecture to obtain
syntactic dependencies. First we parsed pairs of
sentences with the LORG parser
4
. Second we con-
3
http://www.d.umn.edu/ tpederse/similarity.html
4
https://github.com/CNGLdlab/LORG-Release
verted the resulting parse trees to Stanford depen-
dencies
5
.
Given the sets of parsed dependencies D
p
and
D
q
, for sentence p and q, a dependency d ? D
x
is a triple (l, h, t) where l is the dependency label
(for instance, dobj or prep), h the governor and
t the dependant. The similarity measure between
two syntactic dependencies d
1
= (l
1
, h
1
, t
1
) and
d
2
= (l
2
, h
2
, t
2
) is the levenshtein distance be-
tween the labels l
1
and l
2
multiplied by the aver-
age of idf
h
? s
WN
(h
1
, h
2
) and idf
t
? s
WN
(t
1
, t
2
),
where idf
h
and idf
t
are the inverse document fre-
quencies calculated on Google Web 1T for the
governors and the dependants (we retain the max-
imum for each pair), respectively, and s
WN
is cal-
culated using formula ??. NOTE: This measure
was used only in the English sub-task.
2.4 Information Retrieval-based Similarity
Let us consider two texts p and q, an IR system S
and a document collection D indexed by S. This
measure is based on the assumption that p and q
are similar if the documents retrieved by S for the
two texts, used as input queries, are ranked simi-
larly.
Let be L
p
= {d
p
1
, . . . , d
p
K
} and L
q
=
{d
q
1
, . . . , d
q
K
}, d
x
i
? D the sets of the top K
documents retrieved by S for texts p and q, respec-
tively. Let us define s
p
(d) and s
q
(d) the scores as-
signed by S to a document d for the query p and
q, respectively. Then, the similarity score is calcu-
lated as:
sim
IR
(p, q) = 1?
?
d?L
p
?L
q
?
(s
p
(d)?s
q
(d))
2
max(s
p
(d),s
q
(d))
|L
p
? L
q
|
if |L
p
? L
q
| 6= ?, 0 otherwise.
For the participation in the English sub-task we
indexed a collection composed by the AQUAINT-
2
6
and the English NTCIR-8
7
document collec-
tions, using the Lucene
8
4.2 search engine with
BM25 similarity. The Spanish index was cre-
ated using the Spanish QA@CLEF 2005 (agencia
EFE1994-95, El Mundo 1994-95) and multiUN
5
We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
6
http://www.nist.gov/tac/data/data desc.html#AQUAINT-
2
7
http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-
databases.php
8
http://lucene.apache.org/core
401
(Eisele and Chen, 2010) collections. The K value
was set to 70 after a study detailed in (Buscaldi,
2013).
2.5 N-gram Based Similarity
This measure tries to capture the fact that similar
sentences have similar n-grams, even if they are
not placed in the same positions. The measure is
based on the Clustered Keywords Positional Dis-
tance (CKPD) model proposed in (Buscaldi et al.,
2009) for the passage retrieval task.
The similarity between a text fragment p and
another text fragment q is calculated as:
sim
ngrams
(p, q) =
?
?x?Q
h(x, P )
?
n
i=1
w
i
d(x, x
max
)
Where P is the set of the heaviest n-grams in p
where all terms are also contained in q; Q is the
set of all the possible n-grams in q, and n is the
total number of terms in the longest sentence. The
weights for each term w
i
are calculated as w
i
=
1 ?
log(n
i
)
1+log(N)
where n
i
is the frequency of term
t
i
in the Google Web 1T collection, and N is the
frequency of the most frequent term in the Google
Web 1T collection. The weight for each n-gram
(h(x, P )), with |P | = j is calculated as:
h(x, P ) =
{
?
j
k=1
w
k
if x ? P
0 otherwise
The function d(x, x
max
) determines the minimum
distance between a n-gram x and the heaviest one
x
max
as the number of words between them.
2.6 Geographical Context Similarity
We observed that in many sentences, especially
those extracted from news corpora, the compati-
bility of the geographic context between the sen-
tences is an important clue to determine if the sen-
tences are related or not. This measure tries to
measure if the two sentences refer to events that
took place in the same geographical area. We built
a database of geographically-related entities, using
geo-WordNet (Buscaldi and Rosso, 2008) and ex-
panding it with all the synsets that are related to a
geographically grounded synset. This implies that
also adjectives and verbs may be used as clues for
the identification of the geographical context of a
sentence. For instance, ?Afghan? is associated to
?Afghanistan?, ?Sovietize? to ?Soviet Union?, etc.
The Named Entities of type PER (Person) are also
used as clues: we use Yago
9
to check whether the
NE corresponds to a famous leader or not, and in
the affirmative case we include the related nation
to the geographical context of the sentence. For in-
stance, ?Merkel? is mapped to ?Germany?. Given
G
p
and G
q
the sets of places found in sentences p
and q, respectively, the geographical context simi-
larity is calculated as follows:
sim
geo
(p, q) = 1?log
K
?
?
?
1 +
?
x?G
p
min
y?G
q
d(x, y)
max(|G
p
|, |G
q
|)
?
?
?
Where d(x, y) is the spherical distance in Km. be-
tween x and y, and K is a normalization factor set
to 10000 Km. to obtain similarity values between
1 and 0.
2.7 2-grams ?Spectral? Distance
This measure is used to calculate the seman-
tic similarity of two words on the basis of their
context, according to the distributional hypothe-
sis. The measure exploits bi-grams in the Google
Books n-gram collection
10
and is based on the dis-
tributional hypothesis, that is, ?words that tend to
appear in similar contexts are supposed to have
similar meanings?. Given a word w, we calcu-
late the probability of observing a word x know-
ing that it is preceded by w as p(x|w) = p(w ?
x)/p(w) = c(?wx?)/c(?w?), where c(?wx?) is
the number of bigrams ?w x? observed in Google
Books (counting all publication years) 2-grams
and c(?w?) is the number of occurrences of w ob-
served in Google Books 1-grams. We calculate
also the probability of observing a word y know-
ing that it is followed by w as p(y|w) = p(w ?
y)/p(w) = c(?yw?)/c(?w?). In such a way, we
may obtain for a word w
i
two probability distri-
butions D
w
i
p
and D
w
i
f
that can be compared to the
distributions obtained in the same way for another
word w
j
. Therefore, we calculate the distance of
two words comparing the distribution probabilities
built in this way, using the Bhattacharyya coeffi-
cient:
9
http://www.mpi-inf.mpg.de/yago-naga/yago/
10
https://books.google.com/ngrams/datasets
402
sf
(w
i
, w
j
) = ? log
(
?
x?X
?
D
w
i
f
(x) ?D
w
j
f
(x)
)
s
p
(w
i
, w
j
) = ? log
(
?
x?X
?
D
w
i
p
(x) ?D
w
j
p
(x)
)
the resulting distance between w
i
and w
j
is cal-
culated as the average between s
f
(w
i
, w
j
) and
s
p
(w
i
, w
j
). All words in sentence p are compared
to the words of sentence q using this similarity
value. The words that are semantically closer are
paired; if a word cannot be paired (average dis-
tance with any of the words in the other sentence
> 10), then it is left unpaired. The value used as
the final feature is the averaged sum of all distance
scores.
2.8 Other Measures
In addition to the above text similarity measures,
we used also the following common measures:
Cosine
Cosine distance calculated between
p = (w
p
1
, . . . , w
p
n
) and q = (w
q
1
, . . . , w
q
n
), the
vectors of tf.idf weights associated to sentences
p and q, with idf values calculated on Google Web
1T.
Edit Distance
This similarity measure is calculated using the
Levenshtein distance on characters between the
two sentences.
Named Entity Overlap
This is a per-class overlap measure (in this way,
?France? as an Organization does not match
?France? as a Location) calculated using the Dice
coefficient between the sets of NEs found, respec-
tively, in sentences p and q.
3 Results
3.1 Spanish
In order to train the Spanish model, we trans-
lated automatically all the sentences in the English
SemEval 2012 and 2013 using Google Translate.
We also built a corpus manually using definitions
from the RAE
11
(Real Academia Espa?nola de la
Lengua). The definitions were randomly extracted
and paired at different similarity levels (taking into
11
http://www.rae.es/
account the Dice coefficient calculated on the def-
initions bag-of-words). Three annotators gave in-
dependently their similarity judgments on these
paired definitions. A total of 200 definitions were
annotated for training. The official results for the
Spanish task are shown in Table 1. In Figure 1 we
show the results obtained by taking into account
each individual feature as a measure of similarity
between texts. These results show that the combi-
nation was always better than the single features
(as expected), and the feature best able to capture
semantic similarity alone was the cosine distance.
In Table 2 we show the results of the ablation
test, which shows that the features that most con-
tributed to improve the results were the IR-based
similarity for the news dataset and the cosine dis-
tance for the Wikipedia dataset. The worst feature
was the NER overlap (not taking into account it
would have allowed us to gain 2 places in the final
rankings).
Wikipedia News Overall
LIPN-run1 0.65194 0.82554 0.75558
LIPN-run2 0.71647 0.8316 0.7852
LIPN-run3 0.71618 0.80857 0.77134
Table 1: Spanish results (Official runs).
The differences between the three submit-
ted runs are only in the training set used.
LIPN-run1 uses all the training data available
together, LIPN-run3 uses a training set com-
posed by the translated news for the news dataset
and the RAE training set for the Wikipedia dataset;
finally, the best run LIPN-run2 uses the same
training sets of run3 together to build a single
model.
3.2 English
Our participation in the English task was ham-
pered by some technical problems which did not
allow us to complete the parsing of the tweet data
in time. As a consequence of this and some er-
rors in the scripts launched to finalize the experi-
ments, the submitted results were incomplete and
we were able to detect the problem only after the
submission. We show in Table 3 the official re-
sults of run1 with the addition of the results on the
OnWN dataset calculated after the participation to
the task.
403
Figure 1: Spanish task: results taking into account the individual features as semantic similarity mea-
sures.
Ablated feature Wikipedia News Overall diff
LIPN-run2 (none) 0.7165 0.8316 0.7852 0.00%
1:CKPD 0.7216 0.8318 0.7874 0.22%
2:WN 0.7066 0.8277 0.7789 ?0.63%
3:Edit Dist 0.708 0.8242 0.7774 ?0.78%
4:Cosine 0.6849 0.8235 0.7677 ?1.75%
5:NER overlap 0.7338 0.8341 0.7937 0.85%
6:Mihalcea-JC 0.7103 0.8301 0.7818 ?0.34%
7:IRsim 0.7161 0.8026 0.7677 ?1.74%
8:geosim 0.7185 0.8325 0.7865 0.14%
9:Spect. Dist 0.7243 0.8311 0.7880 0.28%
Table 2: Spanish task: ablation test.
Dataset Correlation
Complete (official + OnWN) 0.6687
Complete (only official) 0.5083
deft-forum 0.4544
deft-news 0.6402
headlines 0.6527
images 0.8094
OnWN (unofficial) 0.8039
tweet-news 0.5507
Table 3: English results (Official run + unofficial
OnWN).
4 Conclusions and Future Work
The introduced measures were studied on the
Spanish subtask, observing a limited contribu-
tion from geographic context similarity and spec-
tral distance. The IR-based measure introduced
in 2013 proved to be an important feature for
newswire-based datasets as in the 2013 English
task, even when trained on a training set derived
from automatic translation, which include many
errors. Our participation in the English subtask
was inconclusive due to the technical faults experi-
enced to produce our results. We will nevertheless
take into account the lessons learned in this partic-
ipation for future ones.
Acknowledgements
Part of this work has been carried out with the sup-
port of LabEx-EFL (Empirical Foundation of Lin-
guistics) strand 5 (computational semantic analy-
sis). We are also grateful to CoNACyT (Consejo
NAcional de Ciencia y Tecnologia) for support to
404
this work.
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Davide Buscaldi and Paolo Rosso. 2008. Geo-
WordNet: Automatic Georeferencing of WordNet.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco.
Davide Buscaldi, Paolo Rosso, Jos?e Manuel G?omez,
and Emilio Sanchis. 2009. Answering ques-
tions with an n-gram based passage retrieval engine.
Journal of Intelligent Information Systems (JIIS),
34(2):113?134.
Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flo-
res, and Adrian Popescu. 2013. Lipn-core: Seman-
tic text similarity using n-grams, wordnet, syntac-
tic analysis, esa and information retrieval based fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 162?168,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Davide Buscaldi. 2013. Une mesure de similarit?e
s?emantique bas?ee sur la recherche d?information. In
5`eme Atelier Recherche d?Information SEmantique
- RISE 2013, pages 81?91, Lille, France, July.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxig?en?ea : Une
mesure de similarit?e conceptuelle. In Proceedings of
the Colloque Veille Strat?egique Scientifique et Tech-
nologique (VSST 2010).
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868?2872. Euro-
pean Language Resources Association (ELRA), 5.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int?l. Conf. on Research in Computa-
tional Linguistics, pages 19?33.
Joseph Le Roux, Jennifer Foster, Joachim Wagner,
Rasul Samad Zadeh Kaljahi, and Anton Bryl.
2012. DCU-Paris13 Systems for the SANCL 2012
Shared Task. In The NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), pages 1?4, Montr?eal, Canada, June.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intelli-
gence - Volume 1, AAAI?06, pages 775?780. AAAI
Press.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Bernhard Sch?olkopf, Peter Bartlett, Alex Smola, and
Robert Williamson. 1999. Shrinking the tube: a
new support vector regression algorithm. In Pro-
ceedings of the 1998 conference on Advances in neu-
ral information processing systems II, pages 330?
336, Cambridge, MA, USA. MIT Press.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
405
