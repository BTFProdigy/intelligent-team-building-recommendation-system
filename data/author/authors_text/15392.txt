Proceedings of NAACL HLT 2009: Short Papers, pages 61?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Bayesian Networks for Semantic Frame Composition
in a Spoken Dialog System
Marie-Jean Meurs, Fabrice Lefe`vre and Renato de Mori
Universite? d?Avignon et des Pays de Vaucluse
Laboratoire Informatique d?Avignon (EA 931), F-84911 Avignon, France.
{marie-jean.meurs,fabrice.lefevre,renato.demori}@univ-avignon.fr
Abstract
A stochastic approach based on Dynamic
Bayesian Networks (DBNs) is introduced for
spoken language understanding. DBN-based
models allow to infer and then to compose
semantic frame-based tree structures from
speech transcriptions. Experimental results on
the French MEDIA dialog corpus show the
appropriateness of the technique which both
lead to good tree identification results and can
provide the dialog system with n-best lists of
scored hypotheses.
1 Introduction
Recent developments in Spoken Dialog Systems
(SDSs) have renewed the interest for the extrac-
tion of rich and high-level semantics from users?
utterances. Shifting every SDS component from
hand-crafted to stochastic is foreseen as a good op-
tion to improve their overall performance by an in-
creased robustness to speech variabilities. For in-
stance stochastic methods are now efficient alter-
natives to rule-based techniques for Spoken Lan-
guage Understanding (SLU) (He and Young, 2005;
Lefe`vre, 2007).
The SLU module links up the automatic speech
recognition (ASR) module and the dialog manager.
From the user?s utterance analysis, it derives a repre-
sentation of its semantic content upon which the di-
alog manager can decide the next best action to per-
form, taking into account the current dialog context.
In this work, the overall objective is to increase the
relevancy of the semantic information used by the
system. Generally the internal meaning representa-
tion is based on flat concept sets obtained by either
keyword spotting or conceptual decoding. In some
cases a dialog act can be added on top of the concept
set. Here we intend to consider an additional se-
mantic composition step which will capture the ab-
stract semantic structures conveyed by the basic con-
cept representation. A frame formalism is applied to
specify these nested structures. As such structures
do not rely on sequential constraints, pure left-right
branching semantic parser (such as (He and Young,
2005)) will not apply in this case.
To derive automatically such frame meaning rep-
resentations we propose a system based on a two
decoding step process using dynamic Bayesian net-
works (DBNs) (Bilmes and Zweig, 2002): first ba-
sic concepts are derived from the user?s utterance
transcriptions, then inferences are made on sequen-
tial semantic frame structures, considering all the
available previous annotation levels (words and con-
cepts). The inference process extracts all possible
sub-trees (branches) according to lower level infor-
mation (generation) and composes the hypothesized
branches into a single utterance-span tree (composi-
tion). A hand-craft rule-based approach is used to
derive the seed annotated training data. So both ap-
proaches are not competing and the stochastic ap-
proach is justified as only the DBN system is able
to provide n-best lists of tree hypotheses with confi-
dence scores to a stochastic dialog manager (such as
the very promising POMDP-based approaches).
The paper is organized as follows. The next sec-
tion presents the semantic frame annotation on the
MEDIA corpus. Then Section 3 introduces the DBN-
based models for semantic composition and finally
Section 4 reports on the experiments.
61
HOTEL LOCATION
location_event
LODGING
lodging_hotel lodging_location
frame
frame frame
FE FE
FE
Figure 1: Frames, FEs and relations associated to the se-
quence ?staying in a hotel near the Festival de Cannes?
2 Semantic Frames on the MEDIA corpus
MEDIA is a French corpus of negotiation di-
alogs among users and a tourist information phone
server (Bonneau-Maynard et al, 2005). The corpus
contains 1,257 dialogs recorded using a Wizard of
Oz system. The semantic corpus is annotated with
concept-value pairs corresponding to word segments
with the addition of specifier tags representing some
relations between concepts. The annotation utilizes
83 basic concepts and 19 specifiers.
Amongst the available semantic representations,
the semantic frames (Lowe et al, 1997) are probably
the most suited to the task, mostly because of their
ability to represent negotiation dialogs. Semantic
frames are computational models describing com-
mon or abstract situations involving roles, the frame
elements (FEs). The FrameNet project (Fillmore et
al., 2003) provides a large frame database for En-
glish. As no such resource exists for French, we
elaborated a frame ontology to describe the semantic
knowledge of the MEDIA domain. The MEDIA on-
tology is composed of 21 frames and 86 FEs. All are
described by a set of manually defined patterns made
of lexical units and conceptual units (frame and FE
evoking words and concepts). Figure 1 gives the an-
notation of word sequence ?staying in a hotel near
the Festival de Cannes?. The training data are auto-
matically annotated by a rule-based process. Pattern
matching triggers the instantiation of frames and
FEs which are composed using a set of logical rules.
Composition may involve creation, modification or
deletion of frame and FE instances. About 70 rules
are currently used. This process is task-oriented and
is progressively enriched with new rules to improve
its accuracy. A reference frame annotation for the
training corpus is established in this way and used
for learning the parameters of the stochastic models
introduced in the next section.
concept concept
conecone
concepttrans concepttrans
FrameFEFrameFE
Frame-FE transFrame-FE trans
Frame trans Frame trans
concept concept
pntrapntra
conecone
concepttrans concepttrans
psps
FE transFE trans
Figure 2: Frames, FEs as one or 2 unobserved variables
concept concept
conecone
conceptrtasnF concepttasnF
masEemasEe
masEertasnFmasEertasnF
concept concept
pntrapntra
conecone
concepttasnF
concepttasnF
masEertasnF masEertasnF
psps
m-rtasnFm-rtasnF
Figure 3: 2-level decoding of frames and FEs
3 DBN-based Frame Models
The generative DBN models used in the system are
depicted on two time slices (two words) in figures 2
and 3. In practice, a regular pattern is repeated suffi-
ciently to fit the entire word sequence. Shaded nodes
are observed variables whereas empty nodes are hid-
den. Plain lines represent conditional dependencies
between variables and dashed lines indicate switch-
ing parents (variables modifying the conditional re-
lationship between others). An example of a switch-
ing parent is given by the trans nodes which in-
fluence the frame and FE nodes: when trans node
is null the frame or FE stays the same from slice to
slice, when trans is 1 a new frame or FE value is
predicted based on the values of its parent nodes in
the word sequence using frame (or FE) n-grams.
In the left DBN model of Figure 2 frames and FEs
are merged in a single compound variable. They
are factorized in the right model using two variables
jointly decoded. Figure 3 shows the 2-level model
where frames are first decoded then used as observed
values in the FE decoding step. Merging frames and
FEs into a variable reduces the decoding complex-
ity but leads to deterministic links between frames
62
and FEs. With their factorization, on the contrary, it
is possible to deal with the ambiguities in the frame
and FE links. During the decoding step, every com-
bination is tested, even not encountered in the train-
ing data, by means of a back-off technique. Due
to the increase in model complexity, a sub-optimal
beam search is applied for decoding. In this way,
the 2-level approach reduces the complexity of the
factored approach while preserving model general-
ization.
Because all variables are observed at training
time, the edge?s conditional probability tables are
directly derived from observation counts. To im-
prove their estimates, factored language models
(FLMs) are used along with generalized parallel
backoff (Bilmes and Kirchhoff, 2003). Several FLM
implementations of the joint distributions are used
in the DBN models, corresponding to the arrows in
Figures 2 and 3. In the FLMs given below, n is the
history length (n = 1 for bigrams), the uppercase
and lowercase letters FFE, F , FE, C and W re-
spectively stand for frame/FE (one variable), frame,
FE, concept and word variables:
? Frame/FE compound variable:
P (FFE) '?nk=0 P (ffek|ffek?1);
P (C|FFE) '?nk=0 P (ck|ck?1, ffek);
P (W |C,FFE) '?nk=0 P (wk|wk?1, ck, ffek).
? Frame and FE variables, joint decoding:
P (F ) '?nk=0 P (fk|fk?1);
P (FE|F ) '?nk=0 P (fek|fek?1, fk);
P (C|FE,F ) '?nk=0 P (ck|ck?1, fek, fk);
P (W |C,FE, F ) '?nk=0 P (wk|wk?1, ck, fek, fk).
? Frame and FE variables, 2-level decoding:
? First stage: same as frame/FE compound variables
but only decoding frames
? Second stage: same as joint decodind but frames are
observed
P (F? ) '?nk=0 P (f?k|f?k?1);
P (FE|F? ) '?nk=0 P (fek|fek?1, f?k);
P (C|F? , FE) '?nk=0 P (ck|ck?1, f?k, fek);
P (W |C, F? , FE) '?nk=0 P (wk|wk?1, ck, f?k, fek).
Variables with hat have observed values.
Due to the frame hierarchical representation,
some overlapping situations can occurred when de-
termining the frame and FE associated to a concept.
To address this difficulty, a tree-projection algorithm
is performed on the utterance tree-structured frame
annotation and allows to derive sub-branches associ-
ated to a concept (possibly more than one). Starting
from a leaf of the tree, a compound frame/FE class
is obtained by aggregating the father vertices (either
frames or FEs) as long as they are associated to the
same concept (or none). The edges are defined both
by the frame?FE attachments and the FE?frame
sub-frame relations.
Thereafter, either the branches are considered di-
rectly as compound classes or the frame and FE in-
terleaved components are separated to produce two
class sets. These compound classes are considered
in the decoding process then projected back after-
wards to recover the two types of frame?FE con-
nections. However, some links are lost because de-
coding is sequential. A set of manually defined rules
is used to retrieve the missing connections from the
set of hypothesized branches. Theses rules are sim-
ilar to those used in the semi-automatic annotation
of the training data but differ mostly because the
available information is different. For instance, the
frames cannot anymore be associated to a particular
word inside a concept but rather to the whole seg-
ment. The training corpus provides the set of frame
and FE class sequences on which the DBN parame-
ters are estimated.
4 Experiments and Results
The DBN-based composition systems were evalu-
ated on a test set of 225 speakers? turns manually
annotated in terms of frames and FEs. The rule-
based system was used to perform a frame annota-
tion of the MEDIA data. On the test set, an aver-
age F-measure of 0.95 for frame identification con-
firms the good reliability of the process. The DBN
model parameters were trained on the training data
using jointly the manual transcriptions, the manual
concept annotations and the rule-based frame anno-
tations.
Experiments were carried out on the test set under
three conditions varying the input noise level:
? REF (reference): speaker turns manually tran-
scribed and annotated;
? SLU: concepts decoded from manual transcrip-
tions using a DBN-based SLU model comparable
to (Lefe`vre, 2007) (10.6% concept error rate);
? ASR+SLU: 1-best hypotheses of transcriptions
63
Inputs REF SLU ASR + SLU
DBN models Frames FE Links Frames FE Links Frames FE Links
frame/FEs p?/r? 0.91/0.93 0.91/0.86 0.93/0.98 0.87/0.82 0.91/0.83 0.93/0.98 0.86/0.80 0.90/0.86 0.92/0.98
(compound) F?-m 0.89 0.86 0.92 0.81 0.82 0.92 0.78 0.84 0.92
frames and FEs p?/r? 0.92/0.92 0.92/0.85 0.94/0.98 0.88/0.81 0.92/0.83 0.93/0.97 0.87/0.79 0.90/0.86 0.94/0.97
(2 variables) F?-m 0.90 0.86 0.94 0.80 0.83 0.91 0.78 0.84 0.93
frames then FEs p?/r? 0.92/0.94 0.91/0.82 0.92/0.98 0.88/0.86 0.91/0.80 0.92/0.97 0.87/0.81 0.89/0.82 0.93/0.98
(2-level) F?-m 0.91 0.83 0.93 0.83 0.80 0.90 0.79 0.80 0.92
Table 1: Precision (p?), Recall (r?) and F-measure (F?-m) on the MEDIA test set for the DBN-based frame composition
systems.
generated by an ASR system and concepts decoded
using them (14.8% word error rate, 24.3% concept
error rate).
All the experiments reported in the paper were per-
formed using GMTK (Bilmes and Zweig, 2002),
a general purpose graphical model toolkit and
SRILM (Stolcke, 2002), a language modeling
toolkit.
Table 1 is populated with the results on the test
set for the DBN-based frame composition systems
in terms of precision, recall and F-measure. For the
FE figures, only the reference FEs corresponding to
correctly identified frames are considered. Only the
frame and FE names are considered, neither their
constituents nor their order matter. Finally, results
are given for the sub-frame links between frames
and FEs. Table 1 shows that the performances of the
3 DBN-based systems are quite comparable. Any-
how the 2-level system can be considered the best
as besides its good F-measure results, it is also the
most efficient model in terms of decoding complex-
ity. The good results obtained for the sub-frame
links confirm that the DBN models combined with a
small rule set can be used to generate consistent hi-
erarchical structures. Moreover, as they can provide
hypotheses with confidence scores they can be used
in a multiple input/output context (lattices and n-best
lists) or in a validation process (evaluating and rank-
ing hypotheses from other systems).
5 Conclusion
This work investigates a stochastic process for gen-
erating and composing semantic frames using dy-
namic Bayesian networks. The proposed approach
offers a convenient way to automatically derive se-
mantic annotations of speech utterances based on
a complete frame and frame element hierarchical
structure. Experimental results, obtained on the ME-
DIA dialog corpus, show that the performance of the
DBN-based models are definitely good enough to be
used in a dialog system in order to supply the dialog
manager with a rich and thorough representation of
the user?s request semantics. Though this can also
be obtained using a rule-based approach, the DBN
models alone are able to derive n-best lists of se-
mantic tree hypotheses with confidence scores. The
incidence of such outputs on the dialog manager de-
cision accuracy needs to be asserted.
Acknowledgment
This work is supported by the 6th Framework Re-
search Program of the European Union (EU), LUNA
Project, IST contract no 33549,www.ist-luna.eu
References
J. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In NAACL
HLT.
J. Bilmes and G. Zweig. 2002. The graphical models
toolkit: An open source software system for speech
and time-series processing. In IEEE ICASSP.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
D. Mostefa, and the Media consortium. 2005. Seman-
tic annotation of the MEDIA corpus for spoken dialog.
In ISCA Eurospeech.
C.J. Fillmore, C.R. Johnson, and M.R.L. Petruck. 2003.
Background to framenet. International Journal of
Lexicography, 16.3:235?250.
Y. He and S. Young. 2005. Spoken language understand-
ing using the hidden vector state model. Speech Com-
munication, 48(3-4):262?275.
F. Lefe`vre. 2007. Dynamic bayesian networks and dis-
criminative classifiers for multi-stage semantic inter-
pretation. In IEEE ICASSP.
J.B. Lowe, C.F. Baker, and C.J. Fillmore. 1997. A frame-
semantic approach to semantic annotation. In SIGLEX
Workshop: Why, What, and How?
A. Stolcke. 2002. Srilm an extensible language model-
ing toolkit. In IEEE ICASSP.
64
Sentence Interpretation using Stochastic Finite State
Transducers
Frederic Bchet, Christian Raymond and Renato De Mori
LIA CNRS BP 1228 , 84911 Avignon Cedex 9 - France
 
frederic.bechet, christian.raymond, renato.demori  @lia.univ-avignon.fr
Abstract
An effective way of representing the meaning of a utterance is with frame structures in which
a type of sentence is represented by a set of property/value slots. Properties can types of verbs and
cases and values are extracted from a sentence and should respect constraints represented by case
relations and selectional restrictions involving word senses organized in type hierarchies.
Properties and values can be obtained as the output of Stochastic Finite State Transducers
(SFST) based on property specific language models combined with generic n-gam models. In this
way, sentence interpretation and recognition are carried out by the same search process.
LM adaptation can be performed by dynamically modifying the probability of each SFST
based on system expectations. Phrases accepted by different SFSTs may share words, especially if
different SFST recognize constituents of the same frame. For this reason, search for the most likely
interpretation has to consider promising (possibly overlapping) hypotheses generated by SFSTs and
the best combination of them into an acceptable semantic structure.
Using different types of acoustic confidence measures and indices of consistency, it is possible
to evaluate the probability that each semantic component that has been hypothesized is correct.
These probabilities can be used by the dialogue strategy to decide about specific clarification and
confirmation actions.
SFSTs can be constructed using semi-automatic learning procedures, including the manual
analysis of a limited number of cases followed by the automatic generation of examples by analogy
or the retrieval of analogous examples from existing corpora of data.
Strategies for clarification and confirmation actions can be learned using classification and
regression trees.
1
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 48?55,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Experiments on the France Telecom 3000 Voice Agency corpus: academic
research on an industrial spoken dialog system?
Ge?raldine Damnati
France Te?le?com R&D
TECH/SSTP/RVA
2 av. Pierre Marzin
22307 Lannion Cedex 07, France
geraldine.damnati@orange-ftgroup.com
Fre?de?ric Be?chet Renato De Mori
LIA
University of Avignon
AGROPARC, 339 ch. des Meinajaries
84911 Avignon Cedex 09, France
frederic.bechet,renato.demori
@univ-avignon.fr
Abstract
The recent advances in speech recognition
technologies, and the experience acquired
in the development of WEB or Interac-
tive Voice Response interfaces, have facil-
itated the integration of speech modules
in robust Spoken Dialog Systems (SDS),
leading to the deployment on a large scale
of speech-enabled services. With these
services it is possible to obtain very large
corpora of human-machine interactions by
collecting system logs. This new kinds of
systems and dialogue corpora offer new
opportunities for academic research while
raising two issues: How can academic re-
search take profit of the system logs of
deployed SDS in order to build the next
generation of SDS, although the dialogues
collected have a dialogue flow constrained
by the previous SDS generation? On the
other side, what immediate benefits can
academic research offer for the improve-
ment of deployed system? This paper ad-
dresses these aspects in the framework of
the deployed France Telecom 3000 Voice
Agency service.
?This work is supported by the 6th Framework Research
Programme of the European Union (EU), Project LUNA,
IST contract no 33549. The authors would like to thank
the EU for the financial support. For more information
about the LUNA project, please visit the project home-page,
www.ist-luna.eu .
1 Introduction
Since the deployment on a very large scale of the
AT&T How May I Help You? (HMIHY) (Gorin et
al., 1997) service in 2000, Spoken Dialogue Sys-
tems (SDS) handling a very large number of calls are
now developed from an industrial point of view. Al-
though a lot of the remaining problems (robustness,
coverage, etc.) are still spoken language process-
ing research problems, the conception and the de-
ployment of such state-of-the-art systems mainly re-
quires knowledge in user interfaces.
The recent advances in speech recognition tech-
nologies, and the experience acquired in the devel-
opment of WEB or Interactive Voice Response inter-
faces have facilitated the integration of speech mod-
ules in robust SDS.
These new SDS can be deployed on a very large
scale, like the France Telecom 3000 Voice Agency
service considered in this study. With these services
it is possible to obtain very large corpora of human-
machine interactions by collecting system logs. The
main differences between these corpora and those
collected in the framework of evaluation programs
like the DARPA ATIS (Hemphill et al, 1990) or the
French Technolangue MEDIA (Bonneau-Maynard
et al, 2005) programs can be expressed through the
following dimensions:
? Size. There are virtually no limits in the
amount of speakers available or the time
needed for collecting the dialogues as thou-
sands of dialogues are automatically processed
every day and the system logs are stored.
Therefore Dialog processing becomes similar
48
to Broadcast News processing: the limit is not
in the amount of data available, but rather in the
amount of data that can be manually annotated.
? Speakers. Data are from real users. The speak-
ers are not professional ones or have no reward
for calling the system. Therefore their behav-
iors are not biased by the acquisition protocols.
Spontaneous speech and speech affects can be
observed.
? Complexity. The complexity of the services
widely deployed is necessarily limited in order
to guarantee robustness with a high automation
rate. Therefore the dialogues collected are of-
ten short dialogues.
? Semantic model. The semantic model of such
deployed system is task-oriented. The inter-
pretation of an utterance mostly consists in the
detection of application-specific entities. In an
application like the France Telecom 3000 Voice
Agency service this detection is performed by
hand-crafted specific knowledge.
The AT&T HMIHY corpus was the first large dia-
logue corpus, obtained from a deployed system, that
has the above mentioned characteristics. A service
like the France Telecom 3000 Voice Agency service
has been developed by a user interface development
lab. This new kind of systems and dialogue corpora
offer new opportunities for academic research that
can be summarized as follows:
? How can academic research take profit of the
system logs of deployed SDS in order to build
the next generation of SDS, although the di-
alogues collected have a dialogue flow con-
strained by the previous SDS generation?
? On the other side, what immediate benefits can
academic research offer for the improvement
of deployed system, while waiting for the next
SDS generation?
This paper addresses these aspects in the frame-
work of the deployed FT 3000 Voice Agency ser-
vice. Section 3 presents how the ASR process can
be modified in order to detect and reject Out-Of-
Domain utterances, leading to an improvement in
the understanding performance without modifying
the system. Section 4 shows how the FT 3000 cor-
pus can be used in order to build stochastic models
that are the basis of a new Spoken Language Un-
derstanding strategy, even if the current SLU system
used in the FT 3000 service is not stochastic. Sec-
tion 5 presents experimental results obtained on this
corpus justifying the need of a tighter integration be-
tween the ASR and the SLU models.
2 Description of the France Telecom 3000
Voice Agency corpus
The France Telecom 3000 (FT3000) Voice Agency
service, the first deployed vocal service at France
Telecom exploiting natural language technologies,
has been made available to the general public in Oc-
tober 2005. FT3000 service enables customers to
obtain information and purchase almost 30 differ-
ent services and access the management of their ser-
vices. The continuous speech recognition system re-
lies on a bigram language model. The interpretation
is achieved through the Verbateam two-steps seman-
tic analyzer. Verbateam includes a set of rules to
convert the sequence of words hypothesized by the
speech recognition engine into a sequence of con-
cepts and an inference process that outputs an inter-
pretation label from a sequence of concepts.
2.1 Specificities of interactions
Given the main functionalities of the application,
two types of dialogues can be distinguished. Some
users call FT 3000 to activate some services they
have already purchased. For such demands, users
are rerouted toward specific vocal services that are
dedicated to those particular tasks. In that case, the
FT3000 service can be seen as a unique automatic
frontal desk that efficiently redirects users. For such
dialogues the collected corpora only contain the in-
teraction prior to rerouting. It can be observed in that
case that users are rather familiar to the system and
are most of the time regular users. Hence, they are
more likely to use short utterances, sometimes just
keywords and the interaction is fast (between one or
two dialogue turns in order to be redirected to the
demanded specific service).
Such dialogues will be referred as transit dia-
logues and represent 80% of the calls to the FT3000
49
service. As for the 20% other dialogues, referred to
as other, the whole interaction is proceeded within
the FT3000 application. They concern users that are
more generally asking for information about a given
service or users that are willing to purchase a new
service. For these dialogues, the average utterance
length is higher, as well as the average number of
dialogue turns.
other transit
# dialogues 350 467
# utterances 1288 717
# words 4141 1454
av. dialogue length 3.7 1.5
av. utterance length 3.2 2.0
OOV rate (%) 3.6 1.9
disfluency rate (%) 2.8 2.1
Table 1: Statistics on the transit and other dialogues
As can be observed in table 1 the fact that users
are less familiar with the application in the other dia-
logues implies higher OOV rate and disfluency rate1.
An important issue when designing ASR and SLU
models for such applications that are dedicated to
the general public is to be able to handle both naive
users and familiar users. Models have to be robust
enough for new users to accept the service and in
the meantime they have to be efficient enough for
familiar users to keep on using it. This is the reason
why experimental results will be detailed on the two
corpora described in this section.
2.2 User behavior and OOD utterances
When dealing with real users corpora, one has to
take into account the occurrence of Out-Of-Domain
(OOD) utterances. Users that are familiar with a ser-
vice are likely to be efficient and to strictly answer
the system?s prompts. New users can have more di-
verse reactions and typically make more comments
about the system. By comments we refer to such
cases when a user can either be surprised what am
I supposed to say now?, irritated I?ve already said
that or even insulting the system. A critical aspect
for other dialogues is the higher rate of comments
uttered by users. For the transit dialogues this phe-
nomenon is much less frequent because users are fa-
1by disfluency we consider here false starts and filled pauses
miliar to the system and they know how to be effi-
cient and how to reach their goal. As shown in ta-
ble 2, 14.3% of the other dialogues contain at least
one OOD comment, representing an overall 10.6%
of utterances in these dialogues.
other transit
# dialogues 350 467
# utterances 1288 717
# OOD comments 137 24
OOD rate (%) 10.6 3.3
dialogues with OOD (%) 14.3 3.6
Table 2: Occurrence of Out-Of-Domain comments
on the transit and other dialogues
Some utterances are just comments and some con-
tain both useful information and comments. In the
next section, we propose to detect these OOD se-
quences and to take this phenomenon into account
in the global SLU strategy.
3 Handling Out-Of-Domain utterances
The general purpose of the proposed strategy is to
detect OOD utterances in a first step, before entering
the Spoken Language Understanding (SLU) mod-
ule. Indeed standard Language Models (LMs) ap-
plied to OOD utterances are likely to generate erro-
neous speech recognition outputs and more gener-
ally highly noisy word lattices from which it might
not be relevant and probably harmful to apply SLU
modules.
Furthermore, when designing a general interac-
tion model which aims at predicting dialogue states
as proposed in this paper, OOD utterances are as
harmful for state prediction as can be an out-of-
vocabulary word for the prediction of the next word
with an n-gram LM.
This is why we propose a new composite LM that
integrates two sub-LMs: one LM for transcribing in-
domain phrases, and one LM for detecting and delet-
ing OOD phrases. Finally the different SLU strate-
gies proposed in this paper are applied only to the
portions of signal labeled as in-domain utterances.
50
3.1 Composite Language Model for decoding
spontaneous speech
As a starting point, the comments have been manu-
ally annotated in the training data in order to easily
separate OOD comment segments from in-domain
ones. A specific bigram language model is trained
for these comment segments. The comment LM was
designed from a 765 words lexicon and trained on
1712 comment sequences.
This comment LM, called LMOOD has been in-
tegrated in the general bigram LMG. Comment
sequences have been parsed in the training corpus
and replaced by a OOD tag. This tag is added to
the general LM vocabulary and bigram probabilities
P ( OOD |w) and P (w| OOD ) are trained along
with other bigram probabilities (following the prin-
ciple of a priori word classes). During the decoding
process, the general bigram LM probabilities and the
LMOOD bigram probabilities are combined.
3.2 Decision strategy
Given this composite LM, a decision strategy is ap-
plied to select those utterances for which the word
lattice will be processed by the SLU component.
This decision is made upon the one-best speech
recognition hypotheses and can be described as fol-
lows:
1. If the one-best ASR output is a single OOD
tag, the utterance is simply rejected.
2. Else, if the one-best ASR output contains an
OOD tag along with other words, those words
are processed directly by the SLU component,
following the argument that the word lattice for
this utterance is likely to contain noisy infor-
mation.
3. Else (i.e. no OOD tag in the one-best ASR
output), the word-lattice is transmitted to fur-
ther SLU components.
It will be shown in the experimental section that
this pre-filtering step, in order to decide whether a
word lattice is worth being processed by the higher-
level SLU components, is an efficient way of pre-
venting concepts and interpretation hypothesis to be
decoded from an uninformative utterance.
3.3 Experimental setup and evaluation
The models presented are trained on a corpus col-
lected thanks to the FT3000 service. It contains real
dialogues from the deployed service. The results
presented are obtained on the test corpus described
in section 2.
The results were evaluated according to 3 crite-
ria: the Word Error Rate (WER), the Concept Error
Rate (CER) and the Interpretation Error Rate (IER).
The CER is related to the correct translation of an
utterance into a string of basic concepts. The IER is
related to the global interpretation of an utterance
in the context of the dialogue service considered.
Therefore this last measure is the most significant
one as it is directly linked to the performance of the
dialogue system.
IER all other transit
size 2005 717 1288
LMG 16.5 22.3 13.0
LMG + OOD 15.0 18.6 12.8
Table 3: Interpretation error rate according to the
Language Model
Table 3 presents the IER results obtained with the
strategy strat1 with 2 different LMs for obtaining
W? : LMG which is the general word bigram model;
and LMG + OOD which is the LM with the OOD com-
ment model. As one can see, a very significant im-
provement, 3.7% absolute, is achieved on the other
dialogues, which are the ones containing most of
the comments. For the transit dialogues a small im-
provement (0.2%) is also obtained.
4 Building stochastic SLU strategies
4.1 The FT3000 SLU module
The SLU component of the FT3000 service consid-
ered in this study contains two stages:
1. the first one translates a string of words W =
w1, . . . , wn into a string of elementary con-
cepts C = c1, . . . , cl by means of hand-written
regular grammars;
2. the second stage is made of a set of about 1600
inference rules that take as input a string of con-
cepts C and output a global interpretation ? of
51
a message. These rules are ordered and the
first match obtained by processing the concept
string is kept as the output interpretation.
These message interpretations are expressed by an
attribute/value pair representing a function in the vo-
cal service.
The models used in these two stages are manually
defined by the service designers and are not stochas-
tic. We are going now to present how we can use a
corpus obtained with such models in order to define
an SLU strategy based on stochastic processes.
4.2 Semantic knowledge representation
The actual FT3000 system includes semantic knowl-
edge represented by hand-written rules. These rules
can also be expressed in a logic form. For this rea-
son, some basic concepts are now described with the
purpose of showing how logic knowledge has been
integrated in a first probabilistic model and how it
can be used in a future version in which optimal poli-
cies can be applied.
The semantic knowledge of an application is a
knowledge base (KB) containing a set of logic for-
mulas. Formulas return truth and are constructed
using constants which represent objects and may be
typed, variables, functions which are mappings from
tuples of objects to objects and predicates which
represent relations among objects. An interpretation
specifies which objects, functions and relations in
the domain are represented by which symbol. Basic
inference problem is to determine whether KB |= F
which means that KB entails a formula F .
In SLU, interpretations are carried on by binding
variables and instantiating objects based on ASR re-
sults and inferences performed in the KB. Hypothe-
ses about functions and instantiated objects are writ-
ten into a Short Term Memory (STM).
A user goal is represented by a conjunction of
predicates. As dialogue progresses, some predi-
cates are grounded by the detection of predicate tags,
property tags and values. Such a detection is made
by the interpretation component. Other predicates
are grounded as a result of inference. A user goal G
is asserted when all the atoms of its conjunction are
grounded and asserted true.
Grouping the predicates whose conjunction is the
premise for asserting a goal Gi is a process that goes
through a sequence of states: S1(Gi), S2(Gi), . . .
Let ?ik be the content of the STM used for as-
serting the predicates grounded at the k-th turn of a
dialogue. These predicates are part of the premise
for asserting the i-th goal.
Let Gi be an instance of the i-th goal asserted after
grounding all the predicates in the premise.
?ik can be represented by a composition from a
partial hypothesis ?ik? 1 available at turn k ? 1, the
machine action ak?1 performed at turn k ? 1 and
the semantic interpretation ?ik i.e.:
?ik = ?
(
?ik, ak?1,?ik?1
)
Sk(Gi) is an information state that can lead to a
user?s goal Gi and ?ik is part of the premise for as-
serting Gi at turn k.
State probability can be written as follows:
P (Sk(Gi)|Yk) = P
(
Gi|?ik
)
P
(
?ik|Yk
) (1)
where P
(
Gi|?ik
)
is the probability that Gi is the
type of goal that corresponds to the user interac-
tion given the grounding predicates in ?ik. Yk is the
acoustic features of the user?s utterance at turn k.
Probabilities of states can be used to define a be-
lief of the dialogue system.
A first model allowing multiple dialog state se-
quence hypothesis is proposed in (Damnati et al,
2007). In this model each dialog state correspond
to a system state in the dialog automaton. In order
to deal with flexible dialog strategies and following
previous work (Williams and Young, 2007), a new
model based on a Partially Observable Markov De-
cision Process (POMDP) is currently studied.
If no dialog history is taken into account,
P
(
?ik|Y
)
comes down to P
(
?ik|Y
)
, ?ik being a
semantic attribute/value pair produced by the Ver-
bateam interpretation rules.
The integration of this semantic decoding process
in the ASR process is presented in the next section.
5 Optimizing the ASR and SLU processes
With the stochastic models proposed in section 4,
different strategies can be built and optimized. We
are interested here in the integration of the ASR and
SLU processes. As already shown by previous stud-
ies (Wang et al, 2005), the traditional sequential ap-
proach that first looks for the best sequence of words
52
W? before looking for the best interpretation ?? of an
utterance is sub-optimal. Performing SLU on a word
lattice output by the ASR module is an efficient way
of integrating the search for the best sequence of
words and the best interpretation. However there are
real-time issues in processing word lattices in SDS,
and therefore they are mainly used in research sys-
tems rather than deployed systems.
In section 3 a strategy is proposed for selecting
the utterances for which a word lattice is going to be
produced. We are going now to evaluate the gain in
performance that can be obtained thanks to an inte-
grated approach on these selected utterances.
5.1 Sequential vs. integrated strategies
Two strategies are going to be evaluated. The first
one (strat1) is fully sequential: the best sequence of
word W? is first obtained with
W? = argmax
W
P (W |Y )
Then the best sequence of concepts C? is obtained
with
C? = argmax
C
P (C|W? )
Finally the interpretation rules are applied to C? in
order to obtain the best interpretation ??.
The second strategy (strat2) is fully integrated: ??
is obtained by searching at the same time for W? and
C? and ??. In this case we have:
?? = argmax
W,C,?
P (?|C)P (C|W )P (W |Y )
The stochastic models proposed are implemented
with a Finite State Machine (FSM) paradigm thanks
to the AT&T FSM toolkit (Mohri et al, 2002).
Following the approach described in (Raymond
et al, 2006), the SLU first stage is implemented by
means of a word-to-concept transducer that trans-
lates a word lattice into a concept lattice. This con-
cept lattice is rescored with a Language Model on
the concepts (also encoded as FSMs with the AT&T
GRM toolkit (Allauzen et al, 2003)).
The rule database of the SLU second stage is en-
coded as a transducer that takes as input concepts
and output semantic interpretations ?. By applying
this transducer to an FSM representing a concept lat-
tice, we directly obtain a lattice of interpretations.
The SLU process is therefore made of the com-
position of the ASR word lattice, two transducers
(word-to-concepts and concept-to-interpretations)
and an FSM representing a Language Model on the
concepts. The concept LM is trained on the FT3000
corpus.
This strategy push forward the approach devel-
opped at AT&T in the How May I Help You? (Gorin
et al, 1997) project by using richer semantic mod-
els than call-types and named-entities models. More
precisely, the 1600 Verbateam interpretation rules
used in this study constitute a rich knowledge base.
By integrating them into the search, thanks to the
FSM paradigm, we can jointly optimize the search
for the best sequence of words, basic concepts, and
full semantic interpretations.
For the strategy strat1 only the best path is kept in
the FSM corresponding to the word lattice, simulat-
ing a sequential approach. For strat2 the best inter-
pretation ?? is obtained on the whole concept lattice.
error WER CER IER
strat1 40.1 24.4 15.0
strat2 38.2 22.5 14.5
Table 4: Word Error Rate (WER), Concept Error
Rate (CER) and Interpretation Error Rate (IER) ac-
cording to the SLU strategy
The comparison among the two strategies is given
in table 4. As we can see a small improvement is ob-
tained for the interpretation error rate (IER) with the
integrated strategy (strat2). This gain is small; how-
ever it is interesting to look at the Oracle IER that
can be obtained on an n-best list of interpretations
produced by each strategy (the Oracle IER being the
lowest IER that can be obtained on an n-best list of
hypotheses with a perfect Oracle decision process).
This comparison is given in Figure 1. As one can
see a much lower Oracle IER can be achieved with
strat2. For example, with an n-best list of 5 interpre-
tations, the lowest IER is 7.4 for strat1 and only 4.8
for strat2. This is very interesting for dialogue sys-
tems as the Dialog Manager can use dialogue con-
text information in order to filter such n-best lists.
53
 4
 5
 6
 7
 8
 9
 10
 1  2  3  4  5  6  7  8  9  10
O
ra
cl
e 
IE
R
size of the n-best list of interpretations
sequential search (strat1)
integrated search (strat2)
Figure 1: Oracle IER according to an n-best list of interpretations for strategies strat1 and strat2
5.2 Optimizing WER, CER and IER
Table 4 also indicates that the improvements ob-
tained on the WER and CER dimensions don?t al-
ways lead to similar improvements in IER. This is
due to the fact that the improvements in WER and
CER are mostly due to a significant reduction in the
insertion rates of words and concepts. Because the
same weight is usually given to all kinds of errors
(insertions, substitutions and deletions), a decrease
in the overall error rate can be misleading as inter-
pretation strategies can deal more easily with inser-
tions than deletions or substitutions. Therefore the
reduction of the overall WER and CER measures is
not a reliable indicator of an increase of performance
of the whole SLU module.
level 1-best Oracle hyp.
WER 33.7 20.0
CER 21.2 9.7
IER 13.0 4.4
Table 5: Error rates on words, concepts and interpre-
tations for the 1-best hypothesis and for the Oracle
hypothesis of each level
These results have already been shown for WER
by previous studies like (Riccardi and Gorin, 1998)
IER
from word Oracle 9.8
from concept Oracle 7.5
interpretation Oracle 4.4
Table 6: IER obtained on Oracle hypotheses com-
puted at different levels.
or more recently (Wang et al, 2003). They are il-
lustrated by Table 5 and Table 6. The figures shown
in these tables were computed on the subset of utter-
ances that were passed to the SLU component. Ut-
terances for which an OOD has been detected are
discarded. In Table 5 are displayed the error rates
obtained on words, concepts and interpretations both
on the 1-best hypothesis and on the Oracle hypothe-
sis (the one with the lowest error rate in the lattice).
These Oracle error rates were obtained by looking
for the best hypothesis in the lattice obtained at the
corresponding level (e.g. looking for the best se-
quence of concepts in the concept lattice). As for Ta-
ble 6, the mentioned IER are the one obtained when
applying SLU to the Oracles hypotheses computed
for each level. As one can see the lowest IER (4.4)
is not obtained on the hypotheses with the lowest
WER (9.8) or CER (7.5).
54
6 Conclusion
This paper presents a study on the FT3000 corpus
collected from real users on a deployed general pub-
lic application. Two problematics are addressed:
How can such a corpus be helpful to carry on re-
search on advanced SLU methods eventhough it has
been collected from a more simple rule-based dia-
logue system? How can academic research trans-
late into short-term improvements for deployed ser-
vices? This paper proposes a strategy for integrating
advanced SLU components in deployed services.
This strategy consists in selecting the utterances for
which the advanced SLU components are going to
be applied. Section 3 presents such a strategy that
consists in filtering Out-Of-Domain utterances dur-
ing the ASR first pass, leading to significant im-
provement in the understanding performance.
For the SLU process applied to in-domain utter-
ances, an integrated approach is proposed that looks
simultaneously for the best sequence of words, con-
cepts and interpretations from the ASR word lat-
tices. Experiments presented in section 5 on real
data show the advantage of the integrated approach
towards the sequential approach. Finally, section 4
proposes a unified framework that enables to define
a dialogue state prediction model that can be applied
and trained on a corpus collected through an already
deployed service.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?03), Sap-
poro, Japan.
Helene Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Se-
mantic annotation of the french media dialog corpus.
In Proceedings of the European Conference on Speech
Communication and Technology (Eurospeech), Lis-
boa, Portugal.
Geraldine Damnati, Frederic Bechet, and Renato
De Mori. 2007. Spoken Language Understanding
strategies on the France Telecom 3000 voice agency
corpus. In Proceedings of the International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), Honolulu, USA.
A. L. Gorin, G. Riccardi, and J.H. Wright. 1997. How
May I Help You ? In Speech Communication, vol-
ume 23, pages 113?127.
Charles T. Hemphill, John J. Godfrey, and George R.
Doddington. 1990. The ATIS spoken language sys-
tems pilot corpus. In Proceedings of the workshop on
Speech and Natural Language, pages 96?101, Hidden
Valley, Pennsylvania.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88.
Christian Raymond, Frederic Bechet, Renato De Mori,
and Geraldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48,3-4:288?304.
Giuseppe Riccardi and Allen L. Gorin. 1998. Language
models for speech recognition and understanding. In
Proceedings of the International Conference on Spo-
ken Langage Processing (ICSLP), Sidney, Australia.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is word
error rate a good indicator for spoken language under-
standing accuracy? In Automatic Speech Recognition
and Understanding workshop - ASRU?03, St. Thomas,
US-Virgin Islands.
Ye-Yi Wang, Li Deng, and Alex Acero. 2005. Spoken
language understanding. In Signal Processing Maga-
zine, IEEE, volume 22, pages 16?31.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer, Speech and Language, 21:393?
422.
55
On the use of confidence for statistical decision in dialogue strategies
Christian Raymond1 Fre?de?ric Be?chet1 Renato de Mori1 Ge?raldine Damnati2
1 LIA/CNRS, University of Avignon France 2 France Telecom R&D, Lannion, France
christian.raymond,frederic.bechet,renato.demori@lia.univ-avignon.fr
geraldine.damnati@rd.francetelecom.com
Abstract
This paper describes an interpretation and deci-
sion strategy that minimizes interpretation er-
rors and perform dialogue actions which may
not depend on the hypothesized concepts only,
but also on confidence of what has been rec-
ognized. The concepts introduced here are ap-
plied in a system which integrates language
and interpretation models into Stochastic Finite
State Transducers (SFST). Furthermore, acous-
tic, linguistic and semantic confidence mea-
sures on the hypothesized word sequences are
made available to the dialogue strategy. By
evaluating predicates related to these confi-
dence measures, a decision tree automatically
learn a decision strategy for rescoring a n-best
list of candidates representing a user?s utter-
ance. The different actions that can be then per-
formed are chosen according to the confidence
scores given by the tree.
1 Introduction
There is a wide consensus in the scientific community
that human-computer dialogue systems based on spoken
natural language make mistakes because the Automatic
Speech Recognition (ASR) component may not hypothe-
size some of the pronounced words and the various levels
of knowledge used for recognizing and reasoning about
conceptual entities are imprecise and incomplete. In spite
of these problems, it is possible to make useful applica-
tions with dialogue systems using spoken input if suit-
able interpretation and decision strategies are conceived
that minimize interpretation errors and perform dialogue
actions which may not depend on the hypothesized con-
cepts only, but also on confidence of what has been rec-
ognized.
This paper introduces some concepts developed for
telephone applications in the framework of stochastic
models for interpretation and dialogue strategies, a good
overview of which can be found in (Young, 2002).
The concepts introduced here are applied in a system
which integrates language and interpretation models into
Stochastic Finite State Transducers (SFST). Furthermore,
acoustic, linguistic and semantic confidence measures on
the hypothesized word sequences are made available to
the dialogue strategy. A new way of using them in the
dialogue decision process is proposed in this paper.
Most of the Spoken language Understanding Systems
(SLU) use semantic grammars with semantic tags as non-
terminals (He and Young, 2003) with rules for rewriting
them into strings of words.
The SFSTs of the system used for the experiments de-
scribed here, represent knowledge for the basic building
blocks of a frame-based semantic grammar. Each block
represents a property/value relation. Different SFSTs
may share words in the same sentence. Property/value
hypotheses are generated with an approach described in
(Raymond et al, 2003) and are combined into a sentence
interpretation hypothesis in which the same word may
contribute to more than one property/value pair. The di-
alogue strategy has to evaluate the probability that each
component of each pair has been correctly hypothesized
in order to decide to perform an action that minimizes the
risk of user dissatisfaction.
2 Overview of the decoding process
The starting point for decoding is a lattice of word
hypotheses generated with an n-gram language model
(LM). Decoding is a search process which detects com-
binations of specialized SFSTs and the n-gram LM. The
output of the decoding process consists of a n-best list
of conceptual interpretations ?. An interpretation ? is
a set of property/value pairs sj = (cj, vj) called con-
cepts. cj is the concept tag and vj is the concept value
of sj . Each concept tag cj is represented by a SFST and
can be related either to the dialogue application (phone
number, date, location expression, etc.) or to the dia-
logue management (confirmation, contestation, etc.). To
each string of words recognized by a given SFST cj is
associated a value vj representing a normalized value
for the concept detected. For example, to the word
phrase: on July the fourteenth, detected by a
SFST dedicated to process dates, is associated the value:
????/07/14.
The n-best list of interpretations output by the decod-
ing process is structured according to the different con-
cept tag strings that can be found in the word lattice. To
each concept tag string is attached another n-best list on
the concept values. This whole n-best is called a struc-
tured n-best. After presenting the statistical model used
in this study, we will describe the implementation of this
decoding process.
3 Statistical model
The contribution of a sequence of words W to a concep-
tual structure ? is evaluated by the posterior probability
P (? | Y ), where Y is the description of acoustic features.
Such a probability is computed as follows:
P (? | Y ) =
?
W?SW P (Y | W )P (? | W )
?P (W )?
?
W?SW P (Y | W )P (W )?
(1)
where P (Y | W ) is provided by the acoustic models,
P (W ) is computed with the LM. Exponents ? and ? are
respectively a semantic and a syntactic fudge factor. SW
corresponds to the set of word strings that can be found in
the word lattice. P (? | W ) is computed by considering
that thus:
P (? | W ) = P (s1 | W ).
J
?
j=2
P (sj | sj?11 W ) (2)
P (sj | sj?11 W ) ? P (sj | W )
If the conceptual component sj is hypothesized with
a sentence pattern pij(W ) recognized in W and pik(W )
triggers a pair sk and there is a training set with which
the probabilities P (pik(W ) | sk) ?k, can be estimated,
then the posterior probability can be obtained as follows:
P (sj | W ) =
P (pij(W ) | sj)P (sj)
?K
k=1 P (pik(W ) | sk)P (sk)
(3)
where P (sk) is a unigram probability of conceptual
components.
4 Structured N-best list
N-best lists are generally produced by simply enumerat-
ing the n best paths in the word graphs produced by Au-
tomatic Speech Recognition (ASR) engines. The scores
used in such graphs are usually only a combination of
acoustic and language model scores, and no other linguis-
tic levels are involved. When an n-best word hypothesis
list is generated, the differences between the hypothesis
i and the hypothesis i+1 are often very small, made of
only one or a few words. This phenomenon is aggravated
when the ASR word graph contains a low confidence
area, due for example to an Out-Of-Vocabulary word, to
a noisy input or to a speech disfluency.
This is the main weakness of this approach in a Spoken
Dialogue context: not all words are important to the Di-
alogue Manager, and all the n-best word hypotheses that
differ only between each other because of some speech
disfluency effects can be considered as equals. That?s
why it is important to generate not only a n-best list of
word hypotheses but rather a n-best list of interpretations,
each of them corresponding to a different meaning from
the Dialogue Manager point of view.
We propose here a method for directly extracting such
a structured n-best from a word lattice output by an ASR
engine. This method relies on operations between Finite
State Machines and is implemented thanks to the AT&T
FSM toolkit (see (Mohri et al, 2002) for more details).
4.1 Word-to-Concept transducer
Each concept ck of the dialogue application is associated
with an FSM. These FMSs are called acceptors (Ak for
the concept ck). In order to process strings of words that
don?t belong to any concept, a filler model, called AF
is used. Because the same string of words can?t belong
to both a concept model and the background text, all the
paths contained in the acceptors Ak are removed from the
filler model AF in the following way:
AF = ? ? ?
m
?
k=1
Ak
where ? is the word lexicon of the application and m
is the number of concepts used.
All these acceptors are now turned into transducers
that take words as input symbols and start or end con-
cept tags as output symbols. Indeed, all acceptors Ak be-
come transducers Tk where the first transition emits the
symbol <Ck> and the last transition the symbol </Ck>.
Similarly the filler model becomes the transducer Tbk
which emits the symbols <BCK> and </BCK>. Except
these start and end tags, no other symbols are emitted: all
words in the concept or background transducers emit an
empty symbol.
Finally all these transducers are linked together in a
single model called Tconcept as presented in figure 1.
FILLER
out=<BCK>
in=<start>
out=<>
in=<> in=<>
out=</BCK>
in=<end>
out=<>
out=<>
in=<start>
out=<>
in=w1
in=w2
out=<>
in=<end>
out=<>
in=<>
out=</C1>out=<C1>
in=<>
out=<C2>
in=<>
out=</C2>
in=<>
in=<>
out=<Cn>
in=<>
out=</Cn>
in=<>
out=<>
in=<>
out=<>in=<>
out=<>
SFST C1
SFST Cn
SFST C2
Figure 1: Word-to-Concept Transducer
4.2 Processing the ASR word lattice
The ASR word lattice is coded by an FSM: an acceptor L
where each transition emits a word. The cost function for
a transition corresponds to the acoustic score of the word
emitted.
The first step in the word lattice processing consists
of rescoring each transition of L by means of a 3-gram
Language Model (LM) in order to obtain the probabili-
ties P (W ) of equation 1. This is done by composing the
word lattice with a 3-gram LM also coded as an FSM (see
(Allauzen et al, 2003) for more details about statistical
LMs and FSMs).
The resulting FSM is then composed with the trans-
ducer TConcept in order to obtain the word-to-concept
transducer L?. A path in L? corresponds to a word string
if only the input symbols of the transducer are considered
and its score is the one expressed by equation 1; simi-
larly by considering only the output symbols, a path in L?
corresponds to a concept tag string.
The structured n-best list is directly obtained from L?:
by extracting the n-best concept tag strings (output label
paths) we obtain an n-best list on the conceptual interpre-
tations. The score of each conceptual interpretation is the
sum of all the word strings (input label paths) in the word
lattice producing the same interpretation.
Finally, for every conceptual interpretations C kept at
the previous step, a local n-best list on the word strings is
calculated by selecting in L? the best paths outputting the
string C .
The resulting structured n-best is illustrated by the fol-
lowing example. If we keep the 2 best conceptual in-
terpretations C1, C2 of a transducer L? and, for each of
these, the 2 best word strings, we obtain:
1 : C1 = <c1_1,c1_2,..,c1_x>
1.1 : W1.1 = <v1.1_1,v1.1_2,..,v1.1_x>
1.2 : W1.2 = <v1.2_1,v1.2_2,..,v1.2_x>
2 : C2 = <c2_1,c2_2,..,c2_y>
2.1 : W2.1 = <v2.1_1,v2.1_2,..,v2.1_y>
2.2 : W2.2 = <v2.2_1,v2.2_2,..,v2.2_y>
where <ci_1,ci_2,..,ci_y> is the conceptual
interpretation at the rank i in the n-best list; Wi.j is the
word string ranked j of interpretation i; and vi.j_k
is the concept value of the kth concept ci_k of the jth
word string of interpretation i.
5 Use of correctness probabilities
In order to select a particular interpretation ? (concep-
tual interpretation + concept values) from the structured
n-best list, we are now interested in computing the proba-
bility that ? is correct, given a set of confidence measures
M : P (? | M ). The choice of the confidence measures
determines the quality of the decision strategy. Those
used in this study are briefly presented in the next sec-
tions.
5.1 Confidence measures
5.1.1 Acoustic confidence measure (AC)
This confidence measure relies on the comparison of
the acoustic likelihood provided by the speech recogni-
tion model for a given hypothesis to the one that would
be provided by a totally unconstrained phoneme loop
model. In order to be consistent with the general model,
the acoustic units are kept identical and the loop is over
context dependent phonemes. This confidence measure
is used at the utterance level and at the concept level (see
(Raymond et al, 2003) for more details).
5.1.2 Linguistic confidence measure (LC)
In order to assess the impact of the absence of ob-
served trigrams as a potential cause of recognition errors,
a Language Model consistency measure is introduced.
This measure is simply, for a given word string candi-
date, the ratio between the number of trigrams observed
in the training corpus of the Language Model vs. the total
number of trigrams in the same word string. Its computa-
tion is very fast and the confidence scores obtained from
it give interesting results as presented in (Este`ve et al,
2003).
5.1.3 Semantic confidence measure (SC)
Several studies have shown that text classification tools
(like Support Vector Machines or Boosting algorithms)
can be an efficient way of labeling an utterance transcrip-
tion with a semantic label such as a call-type (Haffner et
al., 2003) in a Spoken Dialogue context. In our case, the
semantic labels attached to an utterance are the different
concepts handled by the Dialogue Manager. One classi-
fier is trained for each concept tag in the following way:
Each utterance of a training corpus is labeled with a
tag, manually checked, indicating if a given concept oc-
curs or not in the utterance. In order to let the classi-
fier model the context of occurrence of a concept rather
than its value we removed most of the concept headwords
from the list of criterion used by the classifier.
During the decision process, if the interpretation eval-
uated contains 2 concepts c1 and c2, then the classifiers
corresponding to c1 and c2 are used to give to the utter-
ance a confidence score of containing these two concepts.
The text classifier used in the experimental section
is a decision-tree classifier based on the Semantic-
Classification-Trees introduced for the ATIS task
by (Kuhn and Mori, 1995) and used for semantic disam-
biguation in (Be?chet et al, 2000).
5.1.4 Rank confidence measure (R)
To the previous confidence measures we added the
rank of each candidate in its n-best. This rank contains
two numbers: the rank of the interpretation of the utter-
ance and the rank of the utterance among those having
the same interpretation.
5.2 Decision Tree based strategy
As the dependencies of these measures are difficult to es-
tablish, their values are transformed into symbols by vec-
tor quantization (VQ) and conjunctions of these symbols
expressing relevant statistical dependencies are obtained
by a decision tree which is trained with a development
set of examples. At the leaves probabilities P (M |?) are
obtained when ? represents any correct hypothesis, the
case in which only the properties have been correctly rec-
ognized or both properties and values have errors. With
these probabilities we are now able to estimate P (? | M )
in the following way:
P (? | M) = 1
1 + P (M |??)P (??)P (M |?)P (?)
(4)
where ?? indicates that the interpretation in question is
incorrect and P (M |??) = 1 ? P (M |?).
6 From hypotheses to actions
Once concepts have been hypothesized, a dialog system
has to decide what action to perform. Let A = aj be
the set of actions a system can perform. Some of them
can be requests for clarification or repetition. In partic-
ular, the system may request the repetition of the entire
utterance. Performing an action has a certain risk and the
decision about the action to perform has to be the one that
minimizes the risk of user dissatisfaction.
It is thus possible that some or all the hypothesized
components of a conceptual structure ? do not corre-
spond to the user intention because the word sequence
W based on which the conceptual hypothesis has been
generated contains some errors. In particular, there are
requests for clarification or repetition which should be
performed right after the interpretation of an utterance in
order to reduce the stress of the user. It is important to
notice that actions consisting in requests for clarification
or repetition mostly depend on the probability that the in-
terpretation of an utterance is correct, rather than on the
utterance interpretation.
The decoding process described in section 2 provides
a number of hypotheses containing a variable number of
pairs sj = (cj, vj) based on the score expressed by equa-
tion 1.
P (? | M ) is then computed for these hypotheses. The
results can be used to decide to accept an interpretation
or to formulate a clarification question which may imply
more hypotheses.
For simplification purpose, we are going to consider
here only two actions: accepting the hypothesis with the
higher P (? | M ) or rejecting it. The risk associated to the
acceptation decision is called ?fa and corresponds to the
cost of a false acceptation of an incorrect interpretation.
Similarly the risk associated to the rejection decision is
called ?fr and corresponds to the cost of a false rejection
of a correct interpretation. In a spoken dialogue context,
?fa is supposed to be higher than ?fr .
The choice of the action to perform is determined by
a threshold ? on P (? | M ). This threshold is tuned on
a development corpus by minimizing the total risk R ex-
pressed as follows:
R = ?fa ?
Nfa
Ntotal
+ ?fr ?
Nfr
Ntotal
(5)
Nfa and Nfr are the numbers of false acceptation and
false rejection decisions on the development corpus for a
given value of ?. Ntotal is the total number of examples
available for tuning the strategy.
The final goal of the strategy is to make negligible Nfa
and the best set of confidence measures is the one that
minimizes Nfr . In fact, the cost of these cases is lower
because the corresponding action has to be a request for
repetition.
Instead of simply discarding an utterance if P (? | M )
is below ?, another strategy we are investigating consists
of estimating the probability that the conceptual interpre-
tation alone (without the concept values) is correct. This
probability can be estimated the same way as P (? | M )
and can be used to choose a third kind of actions: accept-
ing the conceptual meaning of an utterance but asking for
clarifications about the values of the concepts.
A final decision about the strategy to be adopted should
be based on statistics on system performance to be col-
lected and updated after deploying the system on the tele-
phone network.
7 Experiments
7.1 Application domain
The application domain considered in this study is a
restaurant booking application developed at France Tele-
com R&D. At the moment, we only consider in our strat-
egy the most frequent concepts related to the application
domain: PLACE, PRICE and FOOD TYPE. They can be
described as follows:
? PLACE: an expression related to a restaurant loca-
tion (eg. a restaurant near Bastille);
? PRICE: the price range of a restaurant (eg. less than
a hundred euros);
? FOOD TYPE: the kind of food requested by the
caller (eg. an Indian restaurant).
These entities are expressed in the training corpus by
short sequences of words containing three kinds of to-
ken: head-words like Bastille, concept related words like
restaurant and modifier tokens like near.
A single value is associated to each concept entity
simply be adding together the head-words and some
modifier tokens. For example, the values associated to
the three contexts presented above are: Bastille ,
less+hundred+euros and indian.
In the results section a concept detected is considered a
success only if the tag exists in the reference corpus and if
both values are identical. It?s a binary decision process:
a concept can be considered as a false detection even if
the concept tag is correct and if the value is partially cor-
rect. The measure on the errors (insertion, substitution,
deletion) of these concept/value tokens is called in this
paper the Understanding Error Rate, by opposition to the
standard Word Error Rate measure where all words are
considered equals.
7.2 Experimental setup
Experiments were carried out on a dialogue corpus pro-
vided by France Telecom R&D. The task has a vocabu-
lary of 2200 words. The language model used is made
of 44K words. For this study we selected utterances cor-
responding to answers to a prompt asking for the kind
of restaurant the users were looking for. This corpus has
been cut in two: a development corpus containing 511
utterances and a test corpus containing 419 utterances.
This development corpus has been used to train the deci-
sion tree presented in section 5.2. The Word Error Rate
on the test corpus is 22.7%.
7.3 Evaluation of the rescoring strategy
Table 1 shows the results obtained with a rescoring strat-
egy that selects, from the structured n-best list, the hy-
pothesis with the highest P (? | M ). The baseline re-
sults are obtained with a standard maximum-likelihood
approach choosing the hypothesis maximizing the proba-
bility P (? | Y ) of equation 1. No rejection is performed
in this experiment.
The size of the n-best lists was set to 12 items: the first
4 candidates of the first 3 interpretations in the structured
n-best list. The gain obtained after rescoring is very sig-
nificant and justify our 2-step approach that first extract
an n-best list of interpretations thanks to P (? | Y ) and
then choose the one with the highest confidence accord-
ing to a large set of confidence measures M . This gain
can be compared to the one obtained on the Word Error
Rate measure: the WER drops from 21.6% to 20.7% af-
ter rescoring on the development corpus and from 22.7%
to 22.5% on the test corpus. It is clear here that the
WER measure is not an adequate measure in a Spoken
Dialogue context as a big reduction in the Understanding
Error Rate might have very little effect on the Word Error
Rate.
Corpus baseline rescoring UER reduction %
Devt. 15.0 12.4 17.3%
Test 17.7 14.5 18%
Table 1: Understanding Error Rate results with and with-
out rescoring on structured n-best lists (n=12) (no rejec-
tion)
7.4 Evaluation of the decision strategy
In this experiment we evaluate the decision strategy con-
sisting of accepting or rejecting an hypothesis ? thanks to
a threshold on the probability P (? | M ). Figure 2 shows
the curve UER vs. utterance rejection on the development
and test corpora. As we can see very significant improve-
ments can be achieved with very little utterance rejection.
For example, at a 5% utterance rejection operating point,
the UER on the development corpus drops from 15.0% to
8.6% (42.6% relative improvement) and from 17.7% to
11.4% (35.6% relative improvement).
By using equation 5 for finding the operating point
minimizing the risk fonction (with a cost ?fa = 1.5 ?
?fr) on the development corpus we obtain:
? on the development corpus: UER=6.5 utterance re-
jection=13.1
? on the test corpus: UER=9.6 utterance rejec-
tion=15.9
46
8
10
12
14
16
18
0 5 10 15 20
un
de
rst
an
din
g e
rro
r r
ate
utterance rejection (%)
devt
test
Figure 2: Understanding Error Rate vs. utterance rejec-
tion on the development and test corpora
8 Conclusion
This paper describes an interpretation and decision strat-
egy that minimizes interpretation errors and perform dia-
logue actions which may not depend on the hypothesized
concepts only, but also on confidence of what has been
recognized. The first step in the process consists of gen-
erating a structured n-best list of conceptual interpreta-
tions of an utterance. A set of confidence measures is
then used in order to rescore the n-best list thanks to a de-
cision tree approach. Significant gains in Understanding
Error Rate are achieved with this rescoring method (18%
relative improvement). The confidence score given by the
tree can also be used in a decision strategy about the ac-
tion to perform. By using this score, significant improve-
ments in UER can be achieved with very little utterance
rejection. For example, at a 5% utterance rejection op-
erating point, the UER on the development corpus drops
from 15.0% to 8.6% (42.6% relative improvement) and
from 17.7% to 11.4% (35.6% relative improvement). Fi-
nally the operating point for a deployed dialogue system
can be chosen by explicitly minimizing a risk function on
a development corpus.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In 41st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?03), Sapporo,
Japan.
Fre?de?ric Be?chet, Alexis Nasr, and Franck Genet. 2000.
Tagging unknown proper names using decision trees.
In 38th Annual Meeting of the Association for Compu-
tational Linguistics, Hong-Kong, China, pages 77?84.
Yannick Este`ve, Christian Raymond, Renato De Mori,
and David Janiszek. 2003. On the use of linguistic
consistency in systems for human-computer dialogs.
IEEE Transactions on Speech and Audio Processing,
(Accepted for publication, in press).
Patrick Haffner, Gokhan Tur, and Jerry Wright. 2003.
Optimizing SVMs for complex call classification. In
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP?03, Hong-Kong.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In Automatic Speech
Recognition and Understanding workshop - ASRU?03,
St. Thomas, US-Virgin Islands.
R. Kuhn and R. De Mori. 1995. The application of se-
mantic classification trees to natural language under-
standing. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17(449-460).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88.
Christian Raymond, Yannick Este`ve, Fre?de?ric Be?chet,
Renato De Mori, and Ge?raldine Damnati. 2003. Belief
confirmation in spoken dialogue systems using confi-
dence measures. In Automatic Speech Recognition and
Understanding workshop - ASRU?03, St. Thomas, US-
Virgin Islands.
Steve Young. 2002. Talking to machines (statisti-
cally speaking). In International Conference on Spo-
ken Language Processing, ICSLP?02, pages 113?120,
Denver, CO.
Conceptual Language Models for Dialog systems 
Renato De Mori 
LIA CNRS  BP 1228  
84911 Avignon Cedex 9 - France 
renato.demori, @lia.univ-avignon.fr 
Frederic B?chet 
LIA CNRS  BP 1228  
84911 Avignon Cedex 9 - France  
frederic.bechet, @lia.univ-avignon.fr 
 
1 
2 
Introduction 
The purpose of computer speech understanding is to 
find conceptual representations from signs coded into 
the speech signal. 
 
Contrary to speech interpretation by humans in which 
the same discourse may be interpreted differently by 
different subjects, for practical applications of computer 
understanding the result of interpretation should be 
unique for a given signal. Usually it is represented by an 
object which is an instance of class corresponding to a 
semantic structure which can be fairly complex even if 
it is built with instances of conceptual constituents be-
longing to a small set of major ontological categories.  
 
The mapping process that leads to a semantic interpreta-
tion can be derived manually because human interpreta-
tion of sentences can be completely explained with a 
logical formalism or it can be inferred by machine 
learning algorithms in order to ensure a large coverage 
of possible sentence patterns. Theories and practical 
implementations of these approaches are proposed in 
[1],[2]. 
 
 Limitations of coverage in the manual approach and in 
precision of machine learning can be reduced by making 
manually a detailed analysis of a limited number of ex-
amples and generalizing each analysis with automatic 
methods. In particular, a well structured lexicon can be 
very useful, in which the meaning of words is repre-
sented together with suggestions of possible syntactic 
and conceptual structures.  
 
Word associations found with networks of word rela-
tions [3] can also be useful for suggesting compositions 
of semantic constituents into conceptual structures. 
Thus, given an observed example, other examples can 
be manually derived and generalized automatically. 
 
Computer understanding of a spoken sentences is prob-
lem solving activity whose central engine is a search 
process involving various types of models.  
 
Searching for concepts can be combined with searching 
for words. This suggests that statistical language models 
(LMs) could be adapted based on expectations of con-
cepts predicted by a system belief. With this perspec-
tive, it is important to notice that, while the observation 
of only certain words may be sufficient for hypothesiz-
ing a conceptual structure, complete details of word 
phrases expressing a conceptual structure have to be 
known in order to adapt a generic LM to the expectation 
of such a structure.  
 
This paper introduces a search method and a learning 
paradigm based on the just introduced considerations.  
 
The search engine built with this method finds the best 
common path between the system knowledge repre-
sented by the composition of Stochastic Finite State 
Transducers (SFST) and a Stochastic Finite State 
Automaton (SFSA) representing the lattice of word hy-
potheses generated by an Automatic Speech Recogni-
tion System (ASR).  
Hypothesis evaluation and search 
Let a dialogue system have a belief which generates 
expectations B about conceptual structures.  
 
Expectation uncertainty is represented by a probability 
distribution P(B) which is non-zero for a set of concep-
tual structures expected at a given time. Thus for a gen-
eral concept structure ? and a description Y of the 
speech signal, one gets:   
 
)B,Y,(Pmax)B,Y,(P)Y,(P
)Y,(Pmaxarg)Y|(Pmaxarg*
BB
????=?
?=?=?
??  
 
{ })B,,W(P)W|Y(Pmaxarg*
)B,,W(P)W|Y(Pmax
)B,Y,,W(P)B,Y,(P
B,W,
W
W
???
?
???=?
?
  (1) 
 
)B(P)B|W(P)BW|(P)B,W,(P ?=?  
 
A general concept structure ? can be represented as a 
string of parenthesized terminals and non-terminals.  
 
These expressions can be decomposed into chunks. A 
sentence may contain only one or more chunks of an 
incomplete structure, Thus, a system should be able to 
generate interpretation hypotheses about parts of a con-
ceptual structure. In this case, symbol ? makes refer-
ence only to a set of components.  
 
Probability P(?|BW) can be simply set equal to 0 for a 
conceptual structure which cannot be inferred from W. 
If the conceptual structure is part of the expectations of 
system beliefs and can be inferred unambiguously from 
W, then P(?|BW) as in many practical applications in-
cluding the one considered in this paper, then P(?|BW). 
Otherwise, let [  be the sequence of con-
cept symbols corresponding to the preterminal symbols 
in ?. Probability P(?|BW) can be expressed as follows: 
]c....c...c1 ??
 
{ }?
==?
?
=? ???
??
2
111
1
BW]c...c[|cP)BW|c(P
)BW|]c....c...c([P)BW|(P
 (2) 
 
At least, for some values of ? the probability { }BW]c...c[|cP 11 ???  is one for a class of applica-
tions. 
 
Let ? be the set of conceptual components, chunks of 
them or conceptual structures known to the system. Ex-
pectations derived from the system belief can be 
grouped into a set B1. Let B2 the complement of B1 
w.r.t. ? and F be a filler structure representing all the 
conceptual structures not in the application or just ig-
nored by ignorance of the system knowledge. B1, B2 
and F are the possible values for B in the (1) and their 
probabilities P(B) can be established subjectively or by 
evaluating counts for user responses consistent with the 
belief, consistent with the application but not with the 
belief and inconsistent with the application knowledge. 
 
Probability P(W|B) is that of an LM which is adapted to 
the system belief. It can be obtained with an LM built in 
the following way.  
 
Each conceptual structure or part of it ? is represented 
by a finite-state network N(?). 
 
All the networks corresponding to structures in B1 are 
connected in parallel in a single structure with associ-
ated a probability P(B1). A similar structure is built for 
the automata corresponding to structures in B2. A filler 
F is also considered containing a network derived by a 
trigram LM. A network N(?) is obtained by the con-
catenation of finite-state automata C(?) inferred with 
the procedure described in the next section representing 
chunks of knowledge with fillers F. These automata 
output components of conceptual structures. 
 
A search is performed by finding the most likely com-
mon path in the network and in the automaton derived 
from a lattice of word hypotheses generated by the 
speech recognizer with the generic trigram LM. System 
belief make vary the topology of the network by dy-
namically changing the composition of  sets B1 and B2. 
Network recompilation can be avoided by just putting 
all the N(?) in parallel and dynamically assigning each 
network of B1 a probability : 
 
1B
)1B(P)](N[P =?    (3) 
where 1B  indicates the number of elements in B1.  
Probabilities of networks in B2 are assigned in a similar 
way. 
A word sequence W always corresponds to a path in F 
and may correspond to one or more conceptual struc-
tures represented by paths in networks in B1 and B2. In 
the second case, the likelihood of W in F will be much 
lower than the likelihood in B1 or B2 because phrases 
recognized by the chunk automata of the network are 
boosted as it will be shown later. Thus the best path for 
W, in this case, will go through a network whose auto-
mata produce as output the components of a conceptual 
structure.  
 
3 Knowledge inference 
Usually, when an application is developed, an even 
small training corpus is available. 
 
Semantic categories and functions are manually derived 
for an application. They can be modified when the ap-
plication is deployed in order to correct errors or add 
missing constituents.  
 
A number of words in the lexicon have lexical entries 
containing their syntactic category, syntactic constructs 
which can appear in the same sentence, semantic fea-
tures and constructs they can be part of. When one of 
these words is encountered in the training corpus, it is 
considered as a trigger for the semantic categories con-
tained in its lexical entry. The association between 
words and semantic features is part of the semantic 
knowledge of the system. 
 
The presence of a category in the sentence under analy-
sis can be verified manually or by deriving it from the 
parse tree of the sentence. As lexical entries, grammars 
and rules for deriving semantic structures from parse 
trees may be imprecise or incomplete, a single example 
can be carefully examined and validated manually.  
 
Once a single example is available with a detailed syn-
tactic and semantic analysis, it can be generalized. A 
sentence may contain a complete or partial semantic 
structure or just one component concept. Let ? represent 
such a semantic interpretation. Furthermore, each struc-
ture may correspond to a pattern made of phrases and 
fillers of the sentence represented by a sequence of 
words W. Semantic Classification Trees (SCT) pro-
posed in [1] can be used for automatically deriving sen-
tence patterns corresponding to conceptual structures.  
 
The purpose of learning is to build or modify a SFST 
that accepts a sequence of words and output a semantic 
interpretation ?. 
 
The initial analysis of an example starts by using a tag-
ger for replacing words with their preterminal syntactic 
categories.  
 
Then, semantic tags are automatically associated with 
sequences of syntactic tags manually or using the se-
mantic knowledge. A tag expression made of syntactic 
and semantic tags is obtained in this way as a represen-
tation for of ?. As a by-product, expressions for the 
constituents of and components of  ? are built and 
added to the semantic knowledge.  
 
Generalization of the example uses a phrase generator to 
produce sequences of words from the tag expression. 
These sequences of words enrich the finite state transla-
tor which has to map word sequences into the concep-
tual structure ?.  
 
Further generalization can be obtained by inferring 
synonyms with a WordNet. If generalization has pro-
vided erroneous sequences of words, these sequences 
can be removed  by manual inspection or when it is ob-
served that the system has made an interpretation error 
because of them. With a similar procedure, new se-
quences of words can be added to the automaton for ?.  
 
Once it has been found that a word (noun or verb) con-
tributed to hypothesize a concept in the semantic struc-
ture, the concept is added as semantic feature in the 
lexical entry of the word.  
 
In summary learning of semantic knowledge follows the 
following steps: 
 
1 Set the semantic categories for the application. 
 
2 Set the lexical entries for the words that are semanti-
cally relevant for the application. 
 
3 For every analyzed sentence  
? if semantic interpretation is correct   then do 
nothing, 
? if a phrase is misplaced in the representation of 
a semantic structure then remove it, 
? if a phrase is missed in the representation of a 
semantic structure, but the corresponding tag 
expressions is present in the semantic knowl-
edge, then the phrase is added to the corre-
sponding SFST, 
? if the tag expression does not exist in the se-
mantic knowledge, then it is built and se-
quences of words are generated from it with 
the above outlined generalization procedure. 
 
A set of SFST is built in this way. They are added to the 
LM to provide concept specific components and to pro-
duce semantic interpretations at the same time with a 
translation process.  
 
References 
 
[1] Kuhn R.  and De Mori R. (1995). The Application of 
Semantic Classification Trees to Natural Language Un-
derstanding. IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 17 : 449-460.  
 
[2] Pieraccini R., Levin E., and Lee C.-H. (1991). Sto-
chastic Representation of Conceptual Structure in the 
ATIS Task. Proceedings of the, 1991 Speech and Natu-
ral Language Workshop, 121-124, Morgan Kaufmann 
publ, Los Altos, CA. 
 
[3] Vossen P. Diez-Orzas P. and Peters W., (1997) 
The multilingual design of EuroWordnet. 
Proc ACL/EACL workshop on automatic information 
extraction and building of lexical semantic resources for 
NLP applications, Madrid, 1997. 
 
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 443?454,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
An I-vector Based Approach to Compact Multi-Granularity Topic Spaces
Representation of Textual Documents
Mohamed Morchid
?
, Mohamed Bouallegue
?
, Richard Dufour
?
,
Georges Linar
`
es
?
, Driss Matrouf
?
and Renato de Mori
??
?
LIA, University of Avignon, France
?
McGill University, School of Computer Science, Montreal, Quebec, Canada
{firstname.lastname}@univ-avignon.fr
rdemori@cs.mcgill.ca
Abstract
Various studies highlighted that topic-
based approaches give a powerful spo-
ken content representation of documents.
Nonetheless, these documents may con-
tain more than one main theme, and their
automatic transcription inevitably contains
errors. In this study, we propose an orig-
inal and promising framework based on a
compact representation of a textual docu-
ment, to solve issues related to topic space
granularity. Firstly, various topic spaces
are estimated with different numbers of
classes from a Latent Dirichlet Allocation.
Then, this multiple topic space representa-
tion is compacted into an elementary seg-
ment, called c-vector, originally developed
in the context of speaker recognition. Ex-
periments are conducted on the DECODA
corpus of conversations. Results show the
effectiveness of the proposed multi-view
compact representation paradigm. Our
identification system reaches an accuracy
of 85%, with a significant gain of 9 points
compared to the baseline (best single topic
space configuration).
1 Introduction
Automatic Speech Recognition (ASR) systems
frequently fail on noisy conditions and high Word
Error Rates (WER) make the analysis of the au-
tomatic transcriptions difficult. Speech analyt-
ics suffer from these transcription issues that may
be overcome by improving the ASR robustness
and/or the tolerance of speech analytic systems to
ASR errors. This paper proposes a new method
to improve the robustness of speech analytics by
combining a semantic multi-model approach and
a noise reduction technique based on the i-vector
paradigm.
This method is evaluated in the application
framework of the RATP call centre (Paris Public
Transportation Authority), focusing on the theme
identification task (Bechet et al., 2012).
Telephone conversations are a particular case
of human-human interaction whose automatic
processing raises problems, especially due to the
speech recognition step required to obtain the
transcription of the speech contents. First, the
speaker?s behavior may be unexpected and the
training/test mismatch may be very large. Second,
the speech signal may be strongly impacted by
various sources of variability: environment and
channel noises, acquisition devices, etc.
Telephone conversation issues
Topics are related to the reason why the customer
called. Various classes corresponding to the
main customer?s requests are considered (lost and
founds, traffic state, timelines, etc). In addition
to classical issues in such adverse conditions,
the topic-identification system should deal with
problems related to class proximity. For example,
a lost & found request is related to itinerary
(where was the object lost?) or timeline (when?),
that could appear in most of the classes. In fact,
these conversations involve a relatively small
set of basic concepts related to transportation
issues. Figure 1 shows an example of a dialogue
which is manually labeled by the agent as an
issue related to an infraction. However, words
in bold suggest that this conversation could be
related to a transportation card. Thus, we assume
that a dialogue representation should be seen as
a multi-view problem to substantiate the claims
regarding the multi-theme representation of a
given dialogue.
On the other hand, multi-view approaches in-
troduce additional variability due to the diversity
of the views. This variability is also due to the
vocabulary used by both agent and customer
443
Agent: Hello
Customer: Hello
Agent: Speaking ...
Customer: I call you because 
I was fined today, but I still 
have an imagine card 
suitable for zone 1 [...] I forgot 
to use my navigo card for 
zone 2
Agent: You did not use 
your navigo card, that is 
why they give you a fine not 
for a zone issue [...]
Customer: Thanks, bye
Agent: bye
Agent
Customer
Transportation
cards
Figure 1: Example of a dialogue from the DE-
CODA corpus labeled by the agent as an infraction
issue which contains more than one theme (infrac-
tion + transportation cards).
during a telephone conversation. Indeed, an
agent have to follow an predefined scenario of
conversation. Thus, the agent can find the main
reason for the call which corresponds to the theme.
Proposed solutions
An efficient way to tackle both ASR robustness
and class ambiguity could be to map dialogues
into a topic space abstracting the ASR outputs.
Then, dialogue categorization is achieved in this
topic space. Numerous unsupervised methods for
topic-space estimation were proposed in the past.
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) has been largely used for speech analytics;
one of its main drawbacks is the tuning of the
model, that involves various meta-parameters
such as the number of classes (that determines
the model granularity), word distribution meth-
ods, temporal spans. . . If the decision process is
highly dependent on these features, the system?s
performance could be quite unstable.
Classically, this abstract representation involves
selecting the right number of classes composing
the topic space. This decision is crucial since
topic model perplexity, which expresses its qual-
ity, is highly dependent on this feature. Further-
more, the multi-theme context of the study (see
Figure 1) involves a more complex dialogue rep-
resentation. In this paper, we propose to deal with
these two drawbacks by using a compact represen-
tation from multiple topic spaces. This model is
based on a robust multi-view representation of the
textual documents.
A multi-view representation of a dialogue intro-
duces both a relevant variability needed to repre-
sent different contexts of the dialogue, and a noisy
variability related to topic space processing. Thus,
a topic-based representation of a dialogue is built
from the dialogue content itself. For this reason,
the mapping process of a dialogue into several
topic spaces generates a noisy variability related to
the difference between the dialogue and the con-
tent of each class. In the same way, the relevant
variability comes from the common content be-
tween the dialogue and the classes composing the
topic space.
We propose to reduce the noisy variability by
using a factor analysis technique, which was ini-
tially developed in the domain of speaker identifi-
cation. In this field, the factor analysis paradigm
is used as a decomposition model that enables to
separate the representation space into two sub-
spaces containing respectively useful and useless
information. The general Joint Factor Analysis
(JFA) paradigm (Kenny et al., 2008) considers
multiple variabilities that may be cross-dependent.
Therefore, JFA representation allows us to com-
pensate the variability within sessions of a same
speaker. This representation is an extension of the
GMM-UBM (Gaussian Mixture Model-Universal
Background Model) models (Reynolds and Rose,
1995). (Dehak et al., 2011) extract a compact
super-vector (called an i-vector) from the GMM
super-vector. The aim of the compression pro-
cess (i-vector extraction) is to represent the super-
vector variability in a low dimensional space. Al-
though this compact representation is widely used
in speaker recognition systems, this method has
not been used yet in the field of text classification.
In this paper, we propose to apply factor anal-
ysis to compensate noisy variabilities due to the
multiplication of LDA models. Furthermore, a
normalization approach to condition dialogue rep-
resentations (multi-model and i-vector) is pre-
sented. The two methods showed improvements
for speaker verification: within Class Covariance
Normalization (WCCN) (Dehak et al., 2011) and
Eigen Factor Radial (EFR) (Bousquet et al., 2011).
The latter includes length normalization (Garcia-
Romero and Espy-Wilson, 2011). Both methods
dilate the total variability space as a means of re-
ducing the within-class variability. In our multi-
model representation, the within class variability
is redefined according to both dialogue content
444
(vocabulary) and topic space characteristics (word
distributions among the topics). Thus, the speaker
is represented by a theme, and the speaker session
is a set of topic-based representations (frames) of
a dialogue (session).
The paper is organized as follows. Section 2
presents previous related works. The dialogue rep-
resentation is described in Section 3. Section 4 in-
troduces the i-vector compact representation and
presents its application to text documents. Sec-
tions 5 and 6 report experiments and results. The
last section concludes and proposes some perspec-
tives.
2 Related work
In the past, several approaches considered a
text document as a mixture of latent topics.
These methods, such as Latent Semantic Analysis
(LSA) (Deerwester et al., 1990; Bellegarda, 1997),
Probabilistic LSA (PLSA) (Hofmann, 1999) or
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003), build a higher-level representation of the
document in a topic space. ? Document is
then considered as a bag-of-words (Salton, 1989)
where the word order is not taken into account.
These methods have demonstrated their perfor-
mance on various tasks, such as sentence (Belle-
garda, 2000) or keyword (Suzuki et al., 1998) ex-
traction.
In opposition to a multinomial mixture model,
LDA considers that a theme is associated to each
occurrence of a word composing the document,
rather than associate a topic to the complete doc-
ument. Therefore, a document can change topics
from a word to another one. However, word oc-
currences are connected by a latent variable which
controls the global match of the distribution of
the topics in the document. These latent topics
are characterized by a distribution of associated
word probabilities. PLSA and LDA models have
been shown to generally outperform LSA on IR
tasks (Hofmann, 2001). Moreover, LDA provides
a direct estimate of the relevance of a topic given
a word set. In this paper, probabilities of hidden
topic features, estimated with LDA, are considered
for possibly capturing word dependencies express-
ing the semantic contents of a given conversation.
Topic-based approaches involve defining a
number of topics composing the topic space. The
choice of the ?right? number of topics is a crucial
step, especially when the documents may contain
multiple themes. Many studies have tried to find
a relevant method to deal with this issue. (Arun et
al., 2010) proposed to use a Singular Value De-
composition (SVD) to represent the separability
between the words contained in the vocabulary.
Then, if the singular values of the topic-word ma-
trix M equal the norm of the rows of M, this means
that the vocabulary is well separated among the
topics. This method has to be evaluated with the
Kullback-Liebler divergence metric for each topic
space. However, this process would be time con-
suming for thousands of representations of a dia-
logue.
(Teh et al., 2004) proposed the Hierarchical
Dirichlet Process (HDP) method to find the ?right?
number of topics by assuming that the data has
a hierarchical structure. The HDP models were
then compared to the LDA ones on the same
dataset. (Zavitsanos et al., 2008) presented a
method to learn the right depth of an ontology de-
pending of the number of topics of LDA models.
The study presented by (Cao et al., 2009) is quite
similar to (Teh et al., 2004). The authors consider
the average correlation between pairs of topics at
each stage as the right number of topics.
All these methods assume that a document can
have only one representation since they consider
that finding the optimal topic model is the best so-
lution. Another solution would be to consider a set
of topic models to represent a document. Nonethe-
less, a multi-topic-based representation of a dia-
logue can involve a noisy variability due to the
mapping of a dialogue in each topic space. Indeed,
a dialogue does not share its content (i.e. words)
with each class composing the topic space. Thus,
a variability is added during the mapping pro-
cess. Another weakness of the multi-view repre-
sentation is the relation between classes in a topic
space. (Blei and Lafferty, 2006) show that classes
into a LDA topic space are correlated. More-
over, (Li and McCallum, 2006) consider a class
as a node of an acyclic graph and as a distribu-
tion over other classes contained in the same topic
space.
3 Multi-view representation of automatic
dialogue transcriptions in a
homogeneous space
The purpose of the considered application is the
identification of the major theme of a human-
human telephone conversation in the customer
445
care service (CCS) of the RATP Paris transporta-
tion system. The approach considered in this pa-
per focuses on modeling the variability between
different dialogues expressing the same theme t.
For this purpose, it is important to select relevant
features that represent semantic contents for the
theme of a dialogue. An attractive set of features
for capturing possible semantically relevant word
dependencies is obtained with Latent Dirichlet Al-
location (LDA) (Blei et al., 2003), as described in
section 2.
Given a training set of conversations D, a hid-
den topic space is derived and a conversation d
is represented by its probability in each topic of
the hidden space. Estimation of these probabili-
ties is affected by a variability inherent to the es-
timation of the model parameters. If many hidden
spaces are considered and features are computed
for each hidden space, it is possible to model the
estimation variability together with the variability
of the linguistic expression of a theme by different
speakers in different real-life situations. Even if
the purpose of the application is theme identifica-
tion and a training corpus annotated with themes is
available, supervised LDA (Griffiths and Steyvers,
2004) is not suitable for the proposed approach.
LDA is used only for producing different feature
sets used involved in statistical variability models.
In order to estimate the parameters of differ-
ent hidden spaces, a set of discriminative words
V is constructed as described in (Morchid et al.,
2014a). Each theme t contains a set of specific
words. Note that the same word may appear in
several discriminative word sets. All the selected
words are then merged without repetition to form
V .
Several techniques, such as Variational Meth-
ods (Blei et al., 2003), Expectation-propagation
(Minka and Lafferty, 2002) or Gibbs Sam-
pling (Griffiths and Steyvers, 2004), have been
proposed for estimating the parameters describ-
ing a LDA hidden space. Gibbs Sampling is
a special case of Markov-chain Monte Carlo
(MCMC) (Geman and Geman, 1984) and gives
a simple algorithm for approximate inference in
high-dimensional models such as LDA (Heinrich,
2005). This overcomes the difficulty to directly
and exactly estimate parameters that maximize the
likelihood of the whole data collection defined as:
p(W |
??
? ,
??
? ) =
?
w?W
p(
??
w |
??
? ,
??
? ) for the whole
data collection W knowing the Dirichlet parame-
ters
??
? and
??
? .
Gibbs Sampling allows us both to estimate the
LDA parameters in order to represent a new dia-
logue d with the r
th
topic space of size n, and to
obtain a feature vector V
z
r
d
of the topic representa-
tion of d. The j
th
feature V
z
r
j
d
= P (z
r
j
|d) (where
1 ? j ? n) is the probability of topic z
r
j
to be
generated by the unseen dialogue d in the r
th
topic
space of size n (see Figure 2) and V
w
z
r
j
= P (w|z
r
j
)
is the vector representation of a word into r.
Agent: Hello
Customer: Hello
Agent: Speaking ...
Customer: I call you because I 
was fined today, but I still have an 
imagine card suitable for zone 1 
[...] I forgot to use my navigo card 
for zone 2
Agent: You did not use your 
navigo card, that is why they give 
you a fine not for a zone issue [...]
Customer: Thanks, bye
Agent: bye
Agent
Customer
Conversations agent/customer 
customer care service of the 
Paris transportation system
TOPIC 1
P(w|z)           w
0.03682338236708009   card
0.026680126910873955 month
0.026007114700509565 navigo
0.01615229304874531   old
0.015527353139121238 agency
0.014229401019132776 euros
0.013123738102105566 imagine
TOPIC n
P(w|z)           w
0.06946564885496183   card
0.04045801526717557  fine
0.016793893129770993 transport
0.01603053435114504   woman
0.01450381679389313  fined
0.013740458015267175 a?e
0.012977099236641221 infraction
...
P(z |d) P(z |d)
...
1
n
Agent: Hello
Customer: Hello
Agent: Speaking ...
Customer: I call you because I 
was fined today, but I still have an 
imagine card suitable for zone 1 
[...] I forgot to use my navigo card 
for zone 2
Agent: You did not use your 
navigo card, that is why they give 
you a fine not for a zone issue [...]
Customer: Thanks, bye
Agent: bye
Agent
Customer
Conversations agent/customer 
customer care service of the 
Paris transportation system
TOPIC 1
P(w|z)           w
0.03682338236708009   card
0.026680126910873955 month
0.0260 7114700509565 navigo
0.01615229304874531   old
0.015527353139121238 agency
0.014229401019132776 euros
0.013123738102105566 imagine
TOPIC n
P(w|z)           w
0.06946564885496183   card
0.0404580152 717557  fine
0.016793893129770993 transport
0.01603053435114504   woman
0.01450381679389313  fined
0.013740458015267175 a?e
0.012977099236641221 infraction
...
P(z |d) P(z |d)
...
1
n
Figure 2: Example of a dialogue d mapped into a
topic space of size n.
In the LDA technique, topic z
j
, j is drawn
from a multinomial over ? which is drawn from
a Dirichlet distribution over
??
? . Thus, a set of
p topic spaces are learned using LDA by varying
the number of topics n to obtain p topic spaces of
size n. The number of topics n varies from 10 to
3, 010. Thus, a set of 3, 000 topic spaces is esti-
mated. This is high enough to generate, for each
dialogue, many feature sets for estimating the pa-
rameters of a variability model.
The next process allows us to obtain a homo-
geneous representation of transcription d for the
r
th
topic space r. The feature vector V
z
m
d
of
d is mapped to the common vocabulary space
V composed with a set of |V | discriminative
words (Morchid et al., 2014a) of size 166, to ob-
tain a new feature vector V
w
d,r
= {P (w|d)
r
}
w?V
446
of size |V | for the r
th
topic space r of size nwhere
the i
th
(0 ? i ? |V |) feature is:
V
w
i
d,r
= P (w
i
|d)
=
n
?
j=1
P (w
i
|z
r
j
)P (z
r
j
|d)
=
n
?
j=1
V
w
i
z
r
j
? V
z
r
j
d
=
?
???
V
w
i
z
r
,
???
V
z
r
d
?
where ??, ?? is the inner product, ? being the fre-
quency of the term w
i
in d, V
w
i
z
r
j
= P (w
i
|z
j
) and
V
z
r
j
d
= P (z
j
|d) evaluated using Gibbs Sampling
in the topic space r.
4 Compact multi-view representation
In this section, an i-vector-based method to
represent automatic transcriptions is presented.
Initially introduced for speaker recognition, i-
vectors (Kenny et al., 2008) have become very
popular in the field of speech processing and re-
cent publications show that they are also reli-
able for language recognition (Mart?nez et al.,
2011) and speaker diarization (Franco-Pedroso et
al., 2010). I-vectors are an elegant way of re-
ducing the imput space dimensionality while re-
taining most of the relevant information. The
technique was originally inspired by the Joint
Factor Analysis framework (Kenny et al., 2007).
Hence, i-vectors convey the speaker characteris-
tics among other information such as transmission
channel, acoustic environment or phonetic content
of speech segments. The next sections describe
the i-vector extraction process, the application of
this compact representation to textual documents
(called c-vector), and the vector transformation
with the EFR method and the Mahalanobis met-
ric.
4.1 Total variability space definition
I-vector extraction could be seen as a probabilistic
compression process that reduces the dimension-
ality of speech super-vectors according to a linear-
Gaussian model. The speech (of a given speech
recording) super-vector m
s
of concatenated GMM
means is projected in a low dimensionality space,
named Total Variability space, with:
m
(h,s)
= m+ Tx
(h,s)
, (1)
where m is the mean super-vector of the UBM
1
.
T is a low rank matrix (MD ? R), where M is
the number of Gaussians in the UBM and D is the
cepstral feature size, which represents a basis of
the reduced total variability space. T is named To-
tal Variability matrix; the components of x
(h,s)
are
the total factors which represent the coordinates of
the speech recording in the reduced total variabil-
ity space called i-vector (i for identification).
4.2 From i-vector speaker identification to
c-vector textual document classification
The proposed approach uses i-vectors to model
transcription representation through each topic
space in a homogeneous vocabulary space. These
short segments are considered as basic semantic-
based representation units. Indeed, vector V
w
d
rep-
resents a segment or a session of a transcription d.
In the following, (d, r) will indicate the dialogue
representation d in the topic space r. In our model,
the segment super-vector m
(d,r)
of a transcription
d knowing a topic space r is modeled:
m
(d,r)
= m+ Tx
(d,r)
(2)
where x
(d,r)
contains the coordinates of the topic-
based representation of the dialogue in the re-
duced total variability space called c-vector (c for
classification).
Let N
(d,r)
and X
(d,r)
be two vectors containing
the zero order and first order dialogue statistics re-
spectively. The statistics are estimated against the
UBM:
N
r
[g] =
?
t?r
?
g
(t); {X
(d,r)
}
[g]
=
?
t?(d,r)
?
g
(t) ? t
(3)
where ?
g
(t) is the a posteriori probability of Gaus-
sian g for the observation t. In the equation,
?
t?(d,r)
represents the sum over all the frames be-
longing to the dialogue d.
Let X
(d,r)
be the state dependent statistics de-
fined as follows:
{X
(d,r)
}
[g]
={X
(d,r)
}
[g]
?m
[g]
?
?
(d,r)
N
(d,r)
[g]
(4)
Let L
(d,r)
be a R ? R matrix, and B
(d,r)
a vector
1
The UBM is a GMM that represents all the possible ob-
servations.
447
Algorithm 1: Estimation algorithm of T and
latent variable x.
For each dialogue d mapped into the topic
space r: x
(d,r)
? 0, T? random ;
Estimate statistics: N
(d,r)
, X
(d,r)
(eq.3);
for i = 1 to nb iterations do
for all d and r do
Center statistics: X
(d,r)
(eq.4);
Estimate L
(d,r)
and B
(d,r)
(eq.5);
Estimate x
(d,r)
(eq.6);
end
Estimate matrix T (eq. 7 and 8) ;
end
of dimension R, both defined as:
L
(d,r)
= I +
?
g?UBM
N
(d,r)
[g] ? {T}
t
[g]
??
?1
[g]
? {T}
[g]
B
(d,r)
=
?
g?UBM
{T}
t
[g]
??
?1
g
? {X
(d,r)
}
[g]
,
(5)
By using L
(d,r)
and B
(d,r)
, x
(d,r)
can be obtained
using the following equation:
x
(d,r)
= L
?1
(d,r)
? B
(d,r)
(6)
The matrix T can be estimated line by line, with
{T}
i
[g]
being the i
th
line of {T}
[g]
then:
T
i
[g]
= LU
?1
g
? RU
i
g
, (7)
where RU
i
g
and LU
g
are given by:
LU
g
=
?
(d,r)
L
?1
(d,r)
+ x
(d,r)
x
t
(d,r)
? N
(d,r)
[g]
RU
i
g
=
?
(d,r)
{X
(d,r)
}
[i]
[g]
? x
(d,r)
(8)
Algorithm 1 presents the method adopted to es-
timate the multi-view variability dialogue matrix
with the above developments where the standard
likelihood function can be used to assess the con-
vergence. One can refer to (Matrouf et al., 2007)
to find out more about the implementation of the
factor analysis.
C-vector representation suffers from 3 raised c-
vector issues: (i) the c-vectors x of equation 2
have to be theoretically distributed among the nor-
mal distribution N (0, I), (ii) the ?radial? effect
should be removed, and (iii) the full rank total
factor space should be used to apply discriminant
transformations. The next section presents a solu-
tion to these 3 problems.
4.3 C-vector standardization
A solution to standardize c-vectors has been de-
veloped in (Bousquet et al., 2011). The authors
proposed to apply transformations for training and
test transcription representations. The first step is
to evaluate the empirical mean x and covariance
matrix V of the training c-vector. Covariance ma-
trix V is decomposed by diagonalization into:
PDP
T
(9)
where P is the eigenvector matrix of V and D is the
diagonal version of V. A training i-vector x
(d,r)
is
transformed in x
?
(d,r)
as follows:
x
?
(d,r)
=
D
?
1
2
P
T
(x
(d,r)
? x)
?
(x
(d,r)
? x)
T
V
?1
(x
(d,r)
? x)
(10)
The numerator is equivalent by rotation to
V
?
1
2
(x
(d,r)
? x) and the Euclidean norm of x
?
(d,r)
is equal to 1. The same transformation is applied
to the test c-vectors, using the training set parame-
ters x and mean covariance Vas estimations of the
test set of parameters.
Figure 3 shows the transformation steps: Fig-
ure 3-(a) is the original training set; Figure 3-
(b) shows the rotation applied to the initial train-
ing set around the principal axes of the total vari-
ability when P
T
is applied; Figure 3-(c) shows
the standardization of c-vectors when D
?
1
2
is
applied; and finally, Figure 3-(d) shows the c-
vector x
?
(d,r)
on the surface area of the unit hyper-
sphere after a length normalization by a division
of
?
(x
(d,r)
? x)
T
V
?1
(x
(d,r)
? x).
5 Experimental Protocol
The proposed c-vector representation of automatic
transcriptions is evaluated in the context of the
theme identification of a human-human telephone
conversation in the customer care service (CCS)
of the RATP Paris transportation system. The met-
ric used to identify of the best theme is the Maha-
lanobis metric.
5.1 Theme identification task
The DECODA project corpus (Bechet et al., 2012)
was designed to perform experiments on the iden-
tification of conversation themes. It is composed
of 1,514 telephone conversations, corresponding
to about 74 hours of signal, split into a training
448
l ll
l
l ll
l
lll
l
l
l
l
l
l ll
l
l
l
l l l
l
l
l
l
l
ll
l
ll
ll ll
ll l
l
l
l
l l
?3 ?2 ?1 0 1 2 3
?
2
?
1
0
1
2
a.Inital: M
l ll
ll
ll l
ll
l
l
ll ll l lll
l
l
l
ll
l
l ll ll
l
ll
ll l ll
ll lll l
?3 ?2 ?1 0 1 2 3
?
2
?
1
0
1
2
b.Rotation: Pt
l
l
l
l
l ll l
ll
l
l
ll ll l lll
l
l
l
ll
l
l l
l ll
l
ll
ll
l
l
l
l
ll lll l
?3 ?2 ?1 0 1 2 3
?
2
?
1
0
1
2
c.Standardization: D?1 2
l l l
ll
ll l
l l
l ll
l
l
l l
l
ll llll l
l
?3 ?2 ?1 0 1 2 3
?
2
?
1
0
1
2
d.Norm: (x ? x)tV?1(x ? x)
Figure 3: Effect of the standardization with the EFR algorithm.
set (740 dialogues), a development set (175 dia-
logues) and a test set (327 dialogues), and manu-
ally annotated with 8 conversation themes: prob-
lems of itinerary, lost and found, time schedules,
transportation cards, state of the traffic, fares, in-
fractions and special offers.
An LDA model allowed us to elaborate 3,000
topics spaces by varying the number of topics from
10 to 3,010. A topic space having less than 10
topics is not suitable for a corpus of more than 700
dialogues (training set). For each theme {C
i
}
8
i=1
,
a set of 50 specific words is identified. All the
selected words are then merged without repetition
to compose V , which is made of 166 words. The
topic spaces are made with the LDA Mallet Java
implementation
2
.
The LIA-Speeral ASR system (Linar`es et al.,
2007) is used for the experiments. Acoustic model
parameters were estimated from 150 hours of
speech in telephone conditions. The vocabulary
contains 5,782 words. A 3-gram language model
(LM) was obtained by adapting a basic LM with
the training set transcriptions. A ?stop list? of 126
words
3
was used to remove unnecessary words
(mainly function words), which results in a Word
Error Rate (WER) of 33.8% on the training, 45.2%
on the development, and 49.5% on the test. These
2
http://mallet.cs.umass.edu/
3
http://code.google.com/p/stop-words/
high WER are mainly due to speech disfluencies
and to adverse acoustic environments (for exam-
ple, calls from noisy streets with mobile phones).
5.2 Mahalanobis metric
Given a new observation x, the goal of the task is
to identify the theme belonging to x. Probabilistic
approaches ignore the process by which c-vectors
were extracted and they pretend instead they were
generated by a prescribed generative model. Once
a c-vector is obtained from a dialogue, its repre-
sentation mechanism is ignored and it is regarded
as an observation from a probabilistic generative
model. The Mahalanobis scoring metric assigns a
dialogue d with the most likely theme C. Given
a training dataset of dialogues, let W denote the
within dialogue covariance matrix defined by:
W =
K
?
k=1
n
t
n
W
k
=
1
n
K
?
k=1
n
t
?
i=0
(
x
k
i
? x
k
)(
x
k
i
? x
k
)
t
(11)
where W
k
is the covariance matrix of the k
th
theme C
k
, n
t
is the number of utterances for the
theme C
k
, n is the total number of dialogues, and
x
k
is the centroid (mean) of all dialogues x
k
i
ofC
k
.
449
Each dialogue does not contribute to the co-
variance in an equivalent way. For this reason,
the term
n
t
n
is introduced in equation 11. If ho-
moscedasticity (equality of the class covariances)
and Gaussian conditional density models are as-
sumed, a new observation x from the test dataset
can be assigned to the most likely themeC
k
Bayes
us-
ing the classifier based on the Bayes decision rule:
C
k
Bayes
= argmax
k
{N (x | x
k
,W)}
= argmax
k
{
?
1
2
(x? x
k
)
t
W
?1
(x? x
k
) + a
k
}
where W is the within theme covariance ma-
trix defined in eq. 11; N denotes the normal dis-
tribution and a
k
= log (P (C
k
)). It is noted that,
with these assumptions, the Bayesian approach is
similar to Fisher?s geometric approach: x is as-
signed to the class of the nearest centroid, accord-
ing to the Mahalanobis metric (Xing et al., 2002)
of W
?1
:
C
k
Bayes
= argmax
k
{
?
1
2
||x? x
k
||
2
W
?1
+ a
k
}
6 Experiments and results
The proposed c-vector approach is applied to
the same classification task and corpus proposed
in (Morchid et al., 2014a; Morchid et al., 2014b;
Morchid et al., 2013) (state-of-the-art in text clas-
sification in (Morchid et al., 2014a)). Experiments
are conducted using the multiple topic spaces esti-
mated with an LDA approach. From these mul-
tiple topic spaces, a classical way is to find the
one that reaches the best performance. Figure 4
presents the theme classification performance ob-
tained on the development and test sets using vari-
ous topic-based representation configurations with
the EFR normalization algorithm (baseline).
For sake of comparison, experiments are con-
ducted using the automatic transcriptions only
(ASR) only. The conditions indicated by the ab-
breviations between parentheses are considered
for the development (Dev) and the test (Test) sets.
Only homogenous conditions (ASR for both
training and validations sets) are considered in this
study. Authors in (Morchid et al., 2014a) notice
that results collapse dramatically when heteroge-
nous conditions are employed (TRS or TRS+ASR
for training set and ASR for validation set).
First of all, we can see that this baseline ap-
proach reached a classification accuracy of 83%
and 76%, respectively on the development and the
test sets. However, we note that the classifica-
tion performance is rather unstable, and may com-
pletely change from a topic space configuration to
another. The gap between the lower and the higher
classification results is also important, with a dif-
ference of 25 points on the development set (the
same trend is observed on the test set). As a result,
finding the best topic space size seems crucial for
this classification task, particularly in the context
of highly imperfect automatic dialogue transcrip-
tions containing more than one theme.
The topic space that yields the best accuracy
with the baseline method (n = 15 topics) is pre-
sented in Figure 5. This figure presents each of the
15 topics and their 10 most representative words
(highest P (w|z)). Several topics contain more or
less the same representative words, such as topics
3, 6 and 9. This figure points out some interesting
topics that allow us to distinguish a theme from the
others. For example:
? topics 2, 10 and 15 represent some words re-
lated to itinerary problems,
? the transportation cards theme is mostly rep-
resented in topic 4 and 15 (Imagine and Nav-
igo are names of transportation cards),
? the words which represent the time schedules
theme are contained in topic 5,6,7 and less in
topic 9,
? state of the traffic could be discussed with
words such as: departure, line, service, day.
These words and others are contained in topic
13,
? topics 4 and 12 are related to the infractions
theme with to words fine, pass, zone or ticket,
? but topic 12 could be related to theme fares
or special offers as well .
Table 1 presents results obtained with the pro-
posed c-vector approach coupled with the EFR al-
gorithm. We can firstly note that this compact rep-
resentation allows it to outperform the best topic
space configuration (baseline), with a gain of 9.4
points on the development data and of 9 points on
the test data. Moreover, if we consider the differ-
ent c-vector configurations with the development
450
15 500 1500 2000 2497 3010
50
60
70
80
90
Max = 83.3
Min = 58.6
(a) Accuracy with the development set
Items by varying the granularity 10 ? n ? 3010
A
c
c
u
r
a
c
y
(
%
)
15 500 825 1500 2000 3010
50
60
70
80
90
Max = 76.0
Min = 56.8
(b) Accuracy with the test set
Items by varying the granularity 10 ? n ? 3010
A
c
c
u
r
a
c
y
(
%
)
Figure 4: Theme classification accuracies using various topic-based representations with EFR normal-
ization (baseline) on the development and test sets (X-coordinates start at 10 indeed, but to show the best
configuration point (15), the origine (10) has been removed).
Table 1: Theme classification accuracy (%) with different c-vectors and GMM-UBM sizes.
DEV TEST
c-vector Number of Gaussians in GMM-UBM
size 32 64 128 256 32 64 128 256
60 88.8 86.5 91.2 90.6 85.0 82.6 83.5 84.7
100 91.2 92.4 92.4 87.7 86.0 85.0 83.5 84.7
120 89.5 92.2 89.5 87.7 85.0 83.5 85.4 84.1
Table 2: Maximum (Max), minimum (Min) and Difference (Max ?Min) theme classification accu-
racies (%) using the baseline and the proposed c-vector approaches.
Max Min Difference
Method DEV TEST DEV TEST DEV TEST
baseline 83.3 76.0 58.6 56.8 14.7 20.8
c-vector 92.4 85.0 86.5 82.6 5.9 2.4
and test sets, the gap between accuracies is much
smaller: classification accuracy does not go be-
low 82.6%, while it reached 56% for the worst
topic-based configuration. Indeed, as shown in Ta-
ble 2, the difference between the maximum and
the minimum theme classification accuracies is of
20% using the baseline approach while it is only
of 2.4% using the c-vector method.
We can conclude that this original c-vector ap-
proach allows one to better handle the variabilities
451
w P(w|z)
TOPIC 1
line
bag
metro
lost
hours
name
found
thing
object
instant
0.028
0.027
0.018
0.017
0.015
0.013
0.012
0.012
0.011
0.009
w P(w|z)
TOPIC 2
bus
direction
road
stop
sixty
five
three
go
hour
station
0.038
0.027
0.022
0.021
0.018
0.017
0.016
0.013
0.012
0.010
w P(w|z)
TOPIC 3
bus
hours
twenty
four
minutes
onto
old
know
sunday
line
0.024
0.023
0.019
0.014
0.014
0.013
0.010
0.010
0.010
0.008
w P(w|z)
TOPIC 4
card
pass
navigo
month
euro
go
agency
mail
fine
address
0.040
0.032
0.024
0.022
0.021
0.018
0.016
0.012
0.010
0.009
w P(w|z)
TOPIC 5
line
know
station
traffic
hour
say
level
time
today
instant
0.029
0.027
0.024
0.021
0.017
0.015
0.014
0.014
0.012
0.011
w P(w|z)
TOPIC 6
bus
hour
hundred
line
ten
old
mister
morning
lost
bag
0.030
0.021
0.020
0.019
0.018
0.016
0.015
0.015
0.014
0.012
w P(w|z)
TOPIC 7
ticket
say
old
bus
issue
never
always
time
validate
normal
0.026
0.017
0.016
0.015
0.015
0.014
0.014
0.013
0.012
0.011
w P(w|z)
TOPIC 8
saint
plus
say
road
level
station
train
city
four
far
0.018
0.017
0.013
0.013
0.012
0.011
0.011
0.010
0.010
0.009
w P(w|z)
TOPIC 9
hour
four
ten
bus
hundred
miss
zero
line
five
six
0.041
0.039
0.037
0.036
0.024
0.024
0.022
0.020
0.018
0.017
w P(w|z)
TOPIC 10
station
saint
direction
orly
take
madame
metro
line
north
bus
0.041
0.036
0.024
0.020
0.015
0.015
0.013
0.012
0.012
0.011
w P(w|z)
TOPIC 11
madame
service
address
mail
metro
paris
old
stop
lac
dock
0.028
0.027
0.022
0.021
0.020
0.019
0.018
0.018
0.016
0.015
w P(w|z)
TOPIC 12
paris
euro
zone
ticket
fare
card
buy
station
noisy
week
0.025
0.025
0.017
0.015
0.014
0.014
0.013
0.013
0.010
0.010
w P(w|z)
TOPIC 13
service
old
line
madame
mister
ask
internet
departure
day
client
0.034
0.027
0.018
0.017
0.014
0.014
0.013
0.013
0.012
0.011
w P(w|z)
TOPIC 14
bus
direction
metro
line
stop
madame
saint
old
road
door
0.040
0.023
0.022
0.017
0.016
0.015
0.015
0.014
0.014
0.014
w P(w|z)
TOPIC 15
number
integral
card
agency
imagine
subscription
navigo
old
eleven
call
0.040
0.030
0.024
0.023
0.018
0.018
0.017
0.014
0.013
0.012
Figure 5: Topic space (15 topics) that obtains the best accuracy with the baseline system (see Fig. 4).
contained in dialogue conversations: in a classi-
fication context, better accuracy can be obtained
and the results can be more consistent when vary-
ing the c-vector size and the number of Gaussians.
7 Conclusions
This paper presents an original multi-view repre-
sentation of automatic speech dialogue transcrip-
tions, and a fusion process with the use of a factor
analysis method called i-vector. The first step of
the proposed method is to represent a dialogue in
multiple topic spaces of different sizes (i.e. num-
ber of topics). Then, a compact representation
of the dialogue from the multiple views is pro-
cessed to compensate the vocabulary and the vari-
ability of the topic-based representations. The ef-
fectiveness of the proposed approach is evaluated
in a classification task of theme dialogue identifi-
cation. Thus, the architecture of the system iden-
tifies conversation themes using the i-vector ap-
proach. This compact representation was initially
developed for speaker recognition and we showed
that it can be successfully applied to a text clas-
sification task. Indeed, this solution allowed the
system to obtain better classification accuracy than
with the use of the classical best topic space con-
figuration. In fact, we highlighted that this original
compact version of all topic-based representations
of dialogues, called c-vector in this work, coupled
with the EFR normalization algorithm, is a better
solution to deal with dialogue variabilities (high
word error rates, bad acoustic conditions, unusual
word vocabulary, etc). This promising compact
representation allows us to effectively solve both
the difficult choice of the right number of topics
and the multi-theme representation issue of partic-
ular textual documents. Finally, the classification
accuracy reached 85% with a gain of 9 points com-
pared to usual baseline (best topic space configu-
ration). In a future work, we plan to evaluate this
new representation of textual documents in other
information retrieval tasks, such as keyword ex-
traction or automatic summarization systems.
Acknowledgements
We thank the anonymous reviewers for their help-
ful comments. This work was funded by the
SUMACC and ContNomina projects supported by
the French National Research Agency (ANR) un-
der contracts ANR-10-CORD-007 and ANR-12-
BS02-0009.
452
References
R. Arun, Venkatasubramaniyan Suresh, C.E.
Veni Madhavan, and Musti Narasimha Murty.
2010. On finding the natural number of topics
with latent dirichlet allocation: Some observations.
In Advances in Knowledge Discovery and Data
Mining, pages 391?402. Springer.
Frederic Bechet, Benjamin Maza, Nicolas Bigouroux,
Thierry Bazillon, Marc El-Beze, Renato De Mori,
and Eric Arbillot. 2012. Decoda: a call-
centre human-human spoken conversation corpus.
LREC?12.
Jerome R. Bellegarda. 1997. A latent semantic analy-
sis framework for large-span language modeling. In
Fifth European Conference on Speech Communica-
tion and Technology.
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279?1296.
David M. Blei and John Lafferty. 2006. Correlated
topic models. Advances in neural information pro-
cessing systems, 18:147.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research, 3:993?1022.
Pierre-Michel Bousquet, Driss Matrouf, and Jean-
Franc?ois Bonastre. 2011. Intersession compensa-
tion and scoring methods in the i-vectors space for
speaker recognition. In Interspeech, pages 485?488.
Juan Cao, Tian Xia, Jintao Li, Yongdong Zhang, and
Sheng Tang. 2009. A density-based method for
adaptive lda model selection. Neurocomputing,
72(7):1775?1781.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391?407.
Najim Dehak, Patrick J. Kenny, R?eda Dehak, Pierre
Dumouchel, and Pierre Ouellet. 2011. Front-end
factor analysis for speaker verification. IEEE Trans-
actions on Audio, Speech, and Language Process-
ing, 19(4):788?798.
Javier Franco-Pedroso, Ignacio Lopez-Moreno, Doro-
teo T Toledano, and Joaquin Gonzalez-Rodriguez.
2010. Atvs-uam system description for the audio
segmentation and speaker diarization albayzin 2010
evaluation. In FALA VI Jornadas en Tecnologa del
Habla and II Iberian SLTech Workshop, pages 415?
418.
Daniel Garcia-Romero and Carol Y Espy-Wilson.
2011. Analysis of i-vector length normalization in
speaker recognition systems. In Interspeech, pages
249?252.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, (6):721?741.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Gregor Heinrich. 2005. Parameter estimation
for text analysis. Web: http://www. arbylon.
net/publications/text-est. pdf.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI ? 99, page 21. Citeseer.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42(1):177?196.
Patrick Kenny, Gilles Boulianne, Pierre Ouellet, and
Pierre Dumouchel. 2007. Joint factor analysis ver-
sus eigenchannels in speaker recognition. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(4):1435?1447.
Patrick Kenny, Pierre Ouellet, Najim Dehak, Vishwa
Gupta, and Pierre Dumouchel. 2008. A study of in-
terspeaker variability in speaker verification. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 16(5):980?988.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations.
Georges Linar`es, Pascal Noc?era, Dominique Massonie,
and Driss Matrouf. 2007. The lia speech recogni-
tion system: from 10xrt to 1xrt. In Text, Speech and
Dialogue, pages 302?308. Springer.
David Mart?nez, Oldrich Plchot, Luk?as Burget, On-
drej Glembek, and Pavel Matejka. 2011. Language
recognition in ivectors space. Interspeech, pages
861?864.
Driss Matrouf, Nicolas Scheffer, Benoit G.B. Fauve,
and Jean-Francois Bonastre. 2007. A straightfor-
ward and efficient implementation of the factor anal-
ysis model for speaker verification. In Interspeech,
pages 1242?1245.
Thomas Minka and John Lafferty. 2002. Expectation-
propagation for the generative aspect model. In
Proceedings of the Eighteenth conference on Uncer-
tainty in artificial intelligence, pages 352?359. Mor-
gan Kaufmann Publishers Inc.
Mohamed Morchid, Georges Linar`es, Marc El-Beze,
and Renato De Mori. 2013. Theme identification in
telephone service conversations using quaternions of
speech features. In Interspeech. ISCA.
453
Mohamed Morchid, Richard Dufour, Pierre-Michel
Bousquet, Mohamed Bouallegue, Georges Linar`es,
and Renato De Mori. 2014a. Improving dialogue
classification using a topic space representation and
a gaussian classifier based on the decision rule. In
ICASSP. IEEE.
Mohamed Morchid, Richard Dufour, and Georges
Linar`es. 2014b. A LDA-Based Topic Classification
Approach from Highly Imperfect Automatic Tran-
scriptions. In LREC.
Douglas A. Reynolds and Richard C. Rose. 1995.
Robust text-independent speaker identification using
gaussian mixture speaker models. IEEE Transac-
tions on Speech and Audio Processing, 3(1):72?83.
Gerard Salton. 1989. Automatic text processing: the
transformation. Analysis and Retrieval of Informa-
tion by Computer.
Yoshimi Suzuki, Fumiyo Fukumoto, and Yoshihiro
Sekiguchi. 1998. Keyword extraction using term-
domain interdependence for dictation of radio news.
In 17th international conference on Computational
linguistics, volume 2, pages 1272?1276. ACL.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2004. Sharing clusters among
related groups: Hierarchical dirichlet processes. In
NIPS.
Eric P. Xing, Michael I. Jordan, Stuart Russell, and
Andrew Ng. 2002. Distance metric learning with
application to clustering with side-information. In
Advances in neural information processing systems,
pages 505?512.
Elias Zavitsanos, Sergios Petridis, Georgios Paliouras,
and George A. Vouros. 2008. Determining auto-
matically the size of learned ontologies. In ECAI,
volume 178, pages 775?776.
454
