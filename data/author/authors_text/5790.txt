Coling 2008: Companion volume ? Posters and Demonstrations, pages 165?168
Manchester, August 2008
Multilingual Mobile-Phone Translation Services for World Travelers
Michael Paul, Hideo Okuma, Hirofumi Yamamoto, Eiichiro Sumita,
Shigeki Matsuda, Tohru Shimizu, Satoshi Nakamura
? NICT Spoken Language Communication Group
? ATR Spoken Language Communication Research Labs
Hikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, Japan
Michael.Paul@nict.go.jp
Abstract
This demonstration introduces two new
multilingual translation services for mo-
bile phones. The first translation service
provides state-of-the-art text-to-text trans-
lations of Japanese as well as English con-
versational spoken language in the travel
domain into 17 languages using statistical
machine translation technologies trained
automatically from a large-scale multilin-
gual corpus. The second demonstration
is a speech translation service between
Japanese and English for real environ-
ments. It is based on distributed speech
recognition with noise suppression. Flexi-
ble interfaces between internal and exter-
nal speech translation resources ease the
portability of the system to other languages
and enable real-time location-free commu-
nication world-wide.
1 Introduction
Spoken language translation technologies attempt
to bridge the language barriers between people
with different native languages who each want
to engage in conversation by using their mother-
tongue. The importance of these technologies is
increasing due to increases in the number of op-
portunities for cross-language communication in
face-to-face conversation, especially in the domain
of tourism.
We demonstrate two multilingual translation
services for mobile phones that are built on corpus-
based speech recognition and translation technolo-
gies. These services enable smooth and location-
free communication in real environments covering
the major languages of most nations (see Figure 1).
c
?NICT/ATR, 2008. Licensed under the Creative
Commons Attribution-Noncommercial-Share Alike 3.0 Un-
ported license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
Figure 1: Global Language Coverage
The first multilingual translation service de-
scribed in this paper is a text-to-text translation
service that enables users to translate Japanese
and English conversational spoken language sen-
tences in the travel domain into 17 other languages.
The system?s core components consist of a mul-
tilingual, sentence-aligned spoken language cor-
pus covering 18 of the major world languages
and state-of-the-art statistical machine translation
(SMT) engines that are trained automatically from
this corpus covering 306 (=18x17) translation di-
rections. A graphical user-interface (GUI) allows
24x7 world-wide access to the translation service
(see Section 2).
The second multilingual translation service is an
extension of the text-based translation service that
additionally provides speech recognition capabil-
ities. This is the first commercial speech transla-
tion service in the world. The system is based on
distributed speech recognition and operates as fol-
lows: (1) front-end processing (noise suppression,
feature extraction, and feature parameter compres-
sion) is carried out on the mobile phone, (2) back-
end processing (recognition, translation) is done
on a server and (3) translation results are sent back
and displayed on the mobile phone (see Section 3).
2 Multilingual Text Translation Service
(MTTS)
The multilingual text translation service for mo-
bile phones can be accessed via ?http://atr-
langue.jp/smlt? or by using the QR code in Figure 2
that also illustrates the graphical user interface of
165
Figure 2: QR Code and GUI of MTTS
the translation service. Two different modes are
distinguished: (1) the multilingual mode where the
input is translated into all 17 languages simultane-
ously and the translation results are displayed side-
by-side and (2) the bilingual mode where a single
language out of 17 languages can be selected as the
target language of the Japanese or English input
text translation. The bilingual mode also features
back-translation functionality, i.e., a reverse trans-
lation of the generated translation output into the
source language, that enables immediate feedback
on the quality of the translation output. In order to
solve font problems of mobile phones, the trans-
lated sentences are rendered on the server side and
an image is sent and displayed in the mobile phone.
2.1 Multilingual Travel Conversation Corpus
The translation engines used for the translation ser-
vice are trained on the Basic Travel Expressions
Corpus (ATR-BTEC) which is a collection of sen-
tences that travel experts consider useful for people
Table 1: Language Characteristics
Language Order Segments Morphology
Arabic (ar) SVO phrase rich
Danish (da) SVO words medium
German (de) SVO words medium
English (en) SVO words poor
Spanish (es) SVO words medium
French (fr) SVO words medium
Indonesian (id) SVO words rich
Italian (it) SVO words medium
Japanese (ja) SOV none poor
Korean (ko) SOV phrase poor
Malay (ms) SVO words rich
Dutch (nl) SVO words medium
Portuguese (pt) SVO words medium
Brazilian (pt-b) SVO words medium
Portuguese
Russian (ru) SVO words rich
Thai (th) SVO none none
Vietnamese (vi) SVO phrase none
Chinese (zh) SVO none none
going abroad and cover a large variety of topics in
travel situations like shopping or stay (Kikui et al,
2006). The multilingual corpus consists of 160K
sentences for each of the 18 languages, aligned
at the sentence-level. The characteristics of all
ATR-BTEC corpus languages are summarized in
Table 1. These languages differ largely in word or-
der (SVO, SOV), segmentation unit (phrase, word,
none), and morphology (poor, medium, rich). Con-
cerning word segmentation, the corpora were pre-
processed using simple tokenization tools for all
European languages and language-specific word-
segmentation tools for languages like Chinese,
Japanese, Korean, or Thai that do not use white-
space to separate word/phrase tokens. All data sets
were lower-cased and punctuation marks were re-
moved.
2.2 Statistical Machine Translation Engines
Phrase-based statistical machine translation ap-
proaches continue to dominate the field of machine
translation. The translation service makes use of
state-of-the-art phrase-based SMT systems within
the framework of feature-based exponential mod-
els containing the following features:
? Phrase translation probability
? Inverse phrase translation probability
? Lexical weighting probability
? Inverse lexical weighting probability
? Phrase penalty
? Language model probability
? Simple distance-based distortion model
? Word penalty
166
Table 2: Language Model Perplexity
Lang Entropy Total Eval Data
uage Entropy Words Vocab
ar 5.73 21,663 3,780 1,067
da 5.66 17,411 3,077 884
de 5.58 16,698 2,995 910
en 4.53 14,370 3,169 807
es 5.35 15,622 2,919 943
fr 4.77 16,793 3,521 929
id 6.09 18,145 2,977 908
it 5.52 16,078 2,914 956
ja 4.03 15,080 3,745 929
ko 4.21 15,011 3,567 943
ms 6.43 19,144 2,977 909
nl 5.66 17,609 3,110 909
pt-b 5.73 16,981 2,962 932
pt 5.54 16,064 2,900 946
ru 6.20 16,040 2,587 1,143
th 5.12 20,230 3,953 738
vi 4.84 19,531 4,034 792
zh 5.11 14,748 2,887 944
The basic framework within which all the MT
systems were constructed is shown in Figure 3.
SourceL a n g ua g eI n p ut
T a rg etL a n g ua g eO ut p ut
D ecod i n g  A l g ori t h margmax P ( s rc | t rg) *  P ( t rg)
T ra n s l a t i onM od el s L a n g ua g eM od el s
st a t i st i c a la n a l y si s
P a ra l l elT ex t  C orp ora M on ol i n g ua lT ex t  C orp ora
Figure 3: SMT Framework
Translation examples from the respective bilin-
gual text corpus are aligned in order to extract
phrasal equivalences and to calculate the bilingual
feature probabilities. Monolingual features like the
language model probability are trained on mono-
lingual text corpora of the target language whereby
standard word alignment and language modeling
tools were used. For decoding, the CleopATRa
decoder (Finch et al, 2007), a multi-stack phrase-
based SMT decoder is used.
2.3 Evaluation
In order to get an idea of how difficult the trans-
lation tasks are, we trained standard 5gram lan-
guage models on 160K sentence pairs and eval-
uated the entropy and total entropy, i.e., the en-
tropy multiplied by word counts, of each language
on an evaluation data set of 510 sentences each.
Table 2 shows that the total entropy of European
Table 3: Automatic Evaluation Results
BLEU (%) METEOR (%)
en-* *-en ja-* *-ja en-* *-en ja-* *-ja
ar 18.21 51.01 13.03 46.09 40.90 69.01 37.52 58.02
da 59.70 70.90 45.94 55.34 75.08 82.56 64.41 65.83
de 56.48 69.25 41.99 59.20 74.01 81.48 63.69 69.61
en ? ? 61.56 68.53 ? ? 78.19 75.39
es 65.22 73.82 51.77 63.24 78.15 85.28 68.30 72.17
fr 64.69 71.04 52.36 63.16 79.28 83.05 71.14 72.82
id 48.35 59.69 40.59 57.24 66.82 75.83 62.33 69.00
it 56.80 70.43 43.45 60.77 72.41 82.96 62.35 70.70
ja 68.53 61.56 ? ? 75.39 78.19 ? ?
ko 37.00 58.82 69.96 85.10 57.89 75.92 83.25 89.73
ms 40.99 57.63 36.13 55.84 61.08 74.75 58.73 67.33
nl 57.46 72.85 41.43 59.70 75.88 84.52 63.42 72.19
pt-b 59.99 69.41 46.50 58.07 72.77 80.70 64.68 69.14
pt 62.81 70.25 48.24 59.20 75.65 83.32 67.38 68.32
ru 44.46 61.23 36.08 55.13 66.41 73.75 60.59 64.55
th 46.49 51.35 43.75 50.85 62.47 73.12 60.25 62.91
vi 55.18 57.42 50.86 55.07 71.04 73.98 68.67 70.81
zh 53.08 59.33 51.68 69.43 69.85 74.68 65.88 77.62
languages like Danish, German, English, Span-
ish, etc. does not differ much. Moreover, lan-
guages with phrasal segments and/or rich morphol-
ogy like Arabic, Malay, Russian or Vietnamese
have a high total entropy and thus can be expected
to be more difficult to translate. This is confirmed
by the translation experiments in which the eval-
uation data sets were translated using the servers
translation engines and the translation quality was
evaluated using the standard automatic evaluation
metrics BLEU (Papineni et al, 2002) and ME-
TEOR (Banerjee and Lavie, 2005) where scores
range between 0 (worst) and 1 (best). Besides Ko-
rean (single references only), all languages were
evaluated using 16 reference translations. The
evaluation results in Table 3 show that closely
related language pairs like Japanese-Korean or
Portuguese-Brazilian can be translated very accu-
rately, whereas translations into languages with
high total entropy are of lower quality.
3 Multilingual Speech Translation
Service (MSTS)
The speech translation service1 can be accessed via
?http://www.atr-trek.co.jp/contents html? or using
the QR code in Figure 4 that also illustrates the
graphical user interface of the translation service.
After connecting to the top page, the translation
service is activated by selecting the ?Translation?
option. In order to achieve robust speech recogni-
1The speech translation service for Japanese?English on
Docomo 905i mobile phones started November 2007.
167
Speak!
Figure 4: QR Code and GUI of MSTS
tion, the service features a push-to-talk function-
ality, i.e., the user (1) presses the key to start the
service (2) speaks freely into the integrated micro-
phone of the mobile phone, and (3) presses the key
again after the speech input is finished. Fast and
accurate front-end and back-end processing algo-
rithms enable high-speed speech translation of the
input. Both, the speech recognition results as well
as the translation results are sent back to and dis-
played on the mobile phone.
3.1 Multilingual Speech Corpus
Similar to the statistical machine translation ap-
proach introduced in Section 2.2, the speech recog-
nition components are based on large-sized mul-
tilingual speech corpora. For Japanese, speech
recordings of 4000 speakers were collected result-
ing on a total of 200 hours of speech. For English,
almost the same amount of speech data were col-
lected from 500 speakers in North America (300
speakers), the UK (100 speakers), and Australia
(100 speakers).
3.2 Distributed Speech Recognition
The speech interface is based on distributed speech
recognition (DSR) that is integrated as a client-
server architecture compatible with the ETSI ES
202 050 standards. The usage of Speech Trans-
lation Markup Language (STML) enables flexible
connections between internal and external speech
translation resources like speech recognition and
translation servers via a network. Figure 5 illus-
trates the architecture of the utilized DSR system.
The front-end processing includes noise suppres-
sion, feature extraction and feature parameter com-
pression and is carried out on the mobile phone.
The data stream is then sent via internet to the ap-
plication service provider (ASP) for back-end pro-
cessing, i.e. speech recognition and statistical ma-
chine translation. The recognition and translation
results are sent back to the mobile phone for dis-
play to the user.
Back-e n dN e t w o r kF r o n t -e n d
A p p l i cat i o n  S e r v i ce  P r o v i d e r  ( A S P )  A p p l i t i o n  S e r v i e  P r o v i d e r  ( A S P )  
ETSI ES 202  050c o m p a t i b l e  B i t -s t r e a mD a t a  ( 4 . 8 k b i t s / s )
Sp e e c h  R e c o g n i t i o n  a n d  Tr a n s l a t i o n  R e s u l t s
Speechr eco g n i t i o n L a n g u a g et r a n s l a t i o n
Figure 5: MSTS Architecture
4 Conclusion
This paper introduced the first commercial speech
translation service in the world. State-of-the-
art spoken language translation technologies (dis-
tributed speech recognition with noise suppres-
sion, multilingual statistical machine translation)
are implemented into a flexible client-server archi-
tecture that covers the major languages of most
countries and enables users to communicate in real
environments all over the world using their own
mobile phones.
5 Acknowledgments
This work is partly supported by the Grant-in-Aid
for Scientific Research (C) and the Special Coordi-
nation Funds for Promoting Science and Technol-
ogy of the Ministry of Education, Culture, Sports,
Science and Technology, Japan.
References
Banerjee, S. and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 65?72, Ann Arbor, Michigan.
Finch, A., E. Denoual, H. Okuma, M. Paul, H. Ya-
mamoto, K. Yasuda, R. Zhang, and E. Sumita.
2007. The NICT/ATR Speech Translation System
for IWSLT 2007. In Proc. of the IWSLT, pages 103?
110, Trento, Italy.
Kikui, G., S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech
translation. IEEE Transactions on Audio, Speech
and Language Processing, 14(5):1674?1682.
Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proc. of the 40th ACL, pages
311?318, Philadelphia, USA.
168
Development of Indonesian Large Vocabulary Continuous Speech
Recognition System within A-STAR Project
Sakriani Sakti1,2, Eka Kelana3, Hammam Riza4, Shinsuke Sakai1,2
Konstantin Markov1,2, Satoshi Nakamura1,2
1National Institute of Information and Communications Technology, Japan
2ATR Spoken Language Communication Research Laboratories, Japan
3R&D Division, PT Telekomunikasi Indonesia, Indonesia
4Agency for the Assessment and Application of Technology, BPPT, Indonesia
{sakriani.sakti,shinsuke.sakai,konstantin.markov,satoshi.nakamura}@atr.jp,
eka k@telkom.co.id, hammam@iptek.net.id
Abstract
The paper outlines the development of a
large vocabulary continuous speech recog-
nition (LVCSR) system for the Indonesian
language within the Asian speech transla-
tion (A-STAR) project. An overview of the
A-STAR project and Indonesian language
characteristics will be briefly described. We
then focus on a discussion of the develop-
ment of Indonesian LVCSR, including data
resources issues, acoustic modeling, lan-
guage modeling, the lexicon, and accuracy
of recognition. There are three types of In-
donesian data resources: daily news, tele-
phone application, and BTEC tasks, which
are used in this project. They are available in
both text and speech forms. The Indonesian
speech recognition engine was trained using
the clean speech of both daily news and tele-
phone application tasks. The optimum per-
formance achieved on the BTEC task was
92.47% word accuracy.
1 A-STAR Project Overview
The A-STAR project is an Asian consortium that
is expected to advance the state-of-the-art in multi-
lingual man-machine interfaces in the Asian region.
This basic infrastructure will accelerate the devel-
opment of large-scale spoken language corpora in
Asia and also facilitate the development of related
fundamental information communication technolo-
gies (ICT), such as multi-lingual speech translation,
Figure 1: Outline of future speech-technology ser-
vices connecting each area in the Asian region
through network.
multi-lingual speech transcription, and multi-lingual
information retrieval.
These fundamental technologies can be applied to
the human-machine interfaces of various telecom-
munication devices and services connecting Asian
countries through the network using standardized
communication protocols as outlined in Fig. 1. They
are expected to create digital opportunities, improve
our digital capabilities, and eliminate the digital di-
vide resulting from the differences in ICT levels in
each area. The improvements to borderless commu-
nication in the Asian region are expected to result
in many benefits in everyday life including tourism,
business, education, and social security.
The project was coordinated together by the Ad-
vanced Telecommunication Research (ATR) and the
National Institute of Information and Communica-
tions Technology (NICT) Japan in cooperation with
several research institutes in Asia, such as the Na-
tional Laboratory of Pattern Recognition (NLPR) in
China, the Electronics and Telecommunication Re-
search Institute (ETRI) in Korea, the Agency for the
Assessment and Application Technology (BPPT)
in Indonesia, the National Electronics and Com-
puter Technology Center (NECTEC) in Thailand,
the Center for Development of Advanced Comput-
ing (CDAC) in India, the National Taiwan Univer-
sity (NTU) in Taiwan. Partners are still being sought
for other languages in Asia.
More details about the A-STAR project can be
found in (Nakamura et al, 2007).
2 Indonesian Language Characteristic
The Indonesian language, or so-called Bahasa In-
donesia, is a unified language formed from hun-
dreds of languages spoken throughout the Indone-
sian archipelago. Compared to other languages,
which have a high density of native speakers, In-
donesian is spoken as a mother tongue by only 7%
of the population, and more than 195 million people
speak it as a second language with varying degrees
of proficiency. There are approximately 300 eth-
nic groups living throughout 17,508 islands, speak-
ing 365 native languages or no less than 669 di-
alects (Tan, 2004). At home, people speak their own
language, such as Javanese, Sundanese or Balinese,
even though almost everybody has a good under-
standing of Indonesian as they learn it in school.
Although the Indonesian language is infused with
highly distinctive accents from different ethnic lan-
guages, there are many similarities in patterns across
the archipelago. Modern Indonesian is derived from
the literary of the Malay dialect. Thus, it is closely
related to the Malay spoken in Malaysia, Singapore,
Brunei, and some other areas.
Unlike the Chinese language, it is not a tonal
language. Compared with European languages, In-
donesian has a strikingly small use of gendered
words. Plurals are often expressed by means of word
repetition. It is also a member of the agglutina-
tive language family, meaning that it has a complex
range of prefixes and suffixes, which are attached to
base words. Consequently, a word can become very
long.
More details on Indonesian characteristics can be
found in (Sakti et al, 2004).
3 Indonesian Phoneme Set
The Indonesian phoneme set is defined based on In-
donesian grammar described in (Alwi et al, 2003).
A full phoneme set contains 33 phoneme symbols in
total, which consists of 10 vowels (including diph-
thongs), 22 consonants, and one silent symbol. The
vowel articulation pattern of the Indonesian lan-
guage, which indicates the first two resonances of
the vocal tract, F1 (height) and F2 (backness), is
shown in Fig. 2.
 
 
High
 
 
 
          
Mid
 
 
                     
Low
 
i
 
Front        Central      Back 
e
 
e2
 
u
 
o 
a
 
Figure 2: Articulatory pattern of Indonesian vowels.
It consists of vowels, i.e., /a/ (like ?a? in ?father?),
/i/ (like ?ee? in ?screen?), /u/ (like ?oo? in ?soon?),
/e/ (like ?e? in ?bed?), /e2/ (a schwa sound, like ?e?
in ?learn?), /o/ (like ?o? in ?boss?), and four diph-
thongs, /ay/, /aw/, /oy/ and /ey/. The articulatory pat-
tern for Indonesian consonants can be seen in Table
1.
4 Indonesian Data Resources
Three types of Indonesian data resources available
in both text and speech forms were used here. The
first two resources were developed or processed by
the R&D Division of PT Telekomunikasi Indone-
sia (R&D TELKOM) in collaboration with ATR as
continuation of the APT project (Sakti et al, 2004),
while the third one was developed by ATR under the
A-STAR project in collaboration with BPPT. They
are described in the following.
Table 1: Articulatory pattern of Indonesian consonants.
Bilabial Labiodental Dental/Alveolar Palatal Velar Glotal
Plosives p, b t, d k, g
Affricates c, j
Fricatives f s, z sy kh h
Nasal m n ny ng
Trill r
Lateral l
Semivowel w y
4.1 Text Data
The three text corpora are:
1. Daily News Task
There is already a raw source of Indonesian
text data, which has been generated by an In-
donesian student (Tala, 2003). The source is a
compilation from ?KOMPAS? and ?TEMPO?,
which are currently the largest and most widely
read Indonesian newspaper and magazine. It
consists of more than 3160 articles with about
600,000 sentences. R&D TELKOM then fur-
ther processed them to generate a clean text
corpus.
2. Telephone Application Task
About 2500 sentences from the telephone
application domain were also generated by
R&D TELKOM, and were derived from some
daily dialogs from telephone services, includ-
ing tele-home security, billing information ser-
vices, reservation services, status tracking of
e-Government services, and also hearing im-
paired telecommunication services (HITSs).
3. BTEC Task
The ATR basic travel expression corpus
(BTEC) has served as the primary source
for developing broad-coverage speech transla-
tion systems (Kikui et al, 2003). The sen-
tences were collected by bilingual travel ex-
perts from Japanese/English sentence pairs in
travel domain ?phrasebooks?. BTEC has also
been translated into several languages includ-
ing French, German, Italian, Chinese and Ko-
rean. Under the A-STAR project, there are also
plans to collect synonymous sentences from the
different languages of the Asian region. ATR
has currently successfully collected an Indone-
sian version of BTEC tasks, which consists of
160,000 sentences (with about 20,000 unique
words) of a training set and 510 sentences of a
test set with 16 references per sentence. There
are examples of BTEC English sentences and
synonymous Indonesian sentences in Table 2.
Table 2: Examples of English-Indonesian bilingual
BTEC sentences.
English Indonesian
Good Evening Selamat Malam
I like strong coffee Saya suka kopi yang kental
Where is the boarding Di manakah pintu
gate? keberangkatan berada?
How much is this? Harganya berapa?
Thank you Terima kasih
4.2 Speech Data
The three speech corpora are:
1. Daily News Task
From the text data of the news task described
above, we selected phonetically-balanced sen-
tences, then recorded the speech utterances.
Details on the phonetically-balanced sentences,
the recording set-up, speaker criteria, and
speech utterances are described in what fol-
lows:
? Phonetically-Balanced Sentences
We selected phonetically-balanced sen-
tences using the greedy search algorithm
(Zhang and S.Nakamura, 2003), resulting
in 3168 sentences in total (see Table 3).
Table 3: Number of phonetically-balanced sentences
resulting from greedy search algorithm.
Phone # Units # Sentences
Monophones 33 6
Left Biphones 809 240
Right Biphones 809 242
Triphones 9667 2978
Total 3168
? Recording Set-Up
Speech recording was done by R&D
TELKOM in Bandung, Java, Indonesia. It
was conducted in parallel for both clean
and telephone speech, recorded at respec-
tive sampling frequency of 16 kHz and 8
kHz. The system configuration is outlined
in Fig. 3.
Sennheizer
microphone
Microphone
pre-amplifier
& ADC
PC
Phone
PABX
R&D TELKOM
building
Phone USB
Microphone
pre-amplifier
& ADC
USB
Sound-proofed room
Figure 3: Recording set-up.
? Speaker Criteria
The project will require a lot of time,
money, and resources to collect all of
the possible languages and dialects of
the tribes recognized in Indonesia. In
this case, R&D TELKOM only focused
on the major ethnic accents in Bandung
area where the actual telecommunication
services will be implemented. Four
main accents were selected, including:
Batak, Javanese, Sundanese, and standard
Indonesian (no accent) with appropriate
distributions as outlined in Fig. 4. Both
genders are evenly distributed and the
speakers? ages are also distributed as out-
lined in Fig. 5. The largest percentage is
those aged 20-35 years who are expected
to use the telecommunication services
more often.
Figure 4: Accent distribution of 400 speakers in
daily news and telephone application tasks.
Figure 5: Age distribution of 400 speakers in daily
news and telephone application tasks.
? Speech Utterances
The total number of speakers was 400
(200 males and 200 females). Each
speaker uttered 110 sentences resulting
in a total of 44,000 speech utterances or
about 43.35 hours of speech.
2. Telephone Application Task
The utterances in speech of 2500 telephone
application sentences were recorded by R&D
TELKOM in Bandung, Indonesia using the
same recording set-up as that for the news task
corpus. The total number of speakers, as well
as appropriate distributions for age and accent,
were also kept the same. Each speaker uttered
100 sentences resulting in a total of 40,000 ut-
terances (36.15 hours of speech).
3. BTEC Task
From the test set of the BTEC text data pre-
viously described, 510 sentences of one refer-
ence were selected and the recordings of speech
were then done by ATR in Jakarta, Indone-
sia. BPPT helped to evaluate the preliminary
recordings. For this first version, we only se-
lected speakers who spoke standard Indonesian
(no accent). There were 42 speakers (20 males
and 22 females) and each speaker uttered the
same 510 BTEC sentences, resulting in a total
of 21,420 utterances (23.4 hours of speech).
5 Indonesian Speech Recognizer
The Indonesian LVCSR system was developed us-
ing the ATR speech recognition engine. The clean
speech of both daily news and telephone applica-
tion tasks were used as the training data, while the
BTEC task was used as an evaluation test set. More
details on the parameter set-up, acoustic modeling,
language modeling, pronunciation dictionary and
recognition accuracy will be described in the follow-
ing.
5.1 Parameter Set-up
The experiments were conducted using feature ex-
traction parameters, which were a sampling fre-
quency of 16 kHz, a frame length of a 20-ms Ham-
ming window, a frame shift of 10 ms, and 25 dimen-
sional MFCC features (12-order MFCC, ? MFCC
and ? log power).
5.2 Segmentation Utterances
Segmented utterances according to labels are usu-
ally used as a starting point in speech recognition
systems for training speech models. Automatic seg-
mentation is mostly used since it is efficient and less
time consuming. It is basically produced by forced
alignment given the transcriptions. In this case, we
used an available Indonesian phoneme-based acous-
tic model developed using the English-Indonesian
cross language approach (Sakti et al, 2005).
5.3 Acoustic Modeling
Three states were used as the initial HMM for each
phoneme. A shared state HMnet topology was then
obtained using a successive state splitting (SSS)
training algorithm based on the minimum descrip-
tion length (MDL) optimization criterion (Jitsuhiro
et al, 2004). Various MDL parameters were eval-
uated, resulting in context-dependent triphone sys-
tems having different version of total states. i.e.,
1,277 states, 1,944 states and 2,928 states. All tri-
phone HMnets were also generated with three dif-
ferent versions of Gaussian mixture components per
state, i.e., 5, 10, and 15 mixtures.
5.4 Language Modeling
Word bigram and trigram language models were
trained using the 160,000 sentences of the BTEC
training set, yielding a trigram perplexity of 67.0 and
an out-of-vocabulary (OOV) rate of 0.78% on the
510 sentences of the BTEC test set. This high per-
plexity could be due to agglutinative words in the
Indonesian language.
5.5 Pronunciation Dictionary
About 40,000 words from an Indonesian pronun-
ciation dictionary were manually developed by In-
donesian linguists and this was owned by R&D
TELKOM. This was derived from the daily news
and telephone application text corpora, which con-
sisted of 30,000 original Indonesian words plus
8,000 person and place names and also 2,000 of for-
eign words. Based on these pronunciations, we then
included additional words derived from the BTEC
sentences.
5.6 Recognition Accuracy
The performance of the Indonesian speech recog-
nizer with different versions of total states and Gaus-
sian mixture components per state is graphically
depicted in Fig. 6. On average, they achieved
92.22% word accuracy. The optimum performance
was 92.47% word accuracy at RTF=0.97 (XEON 3.2
GHz), which was obtained by the model with 1.277
total states and 15 Gaussian mixture components per
state.
Figure 6: Recognition accuracy of Indonesian
LVCSR on BTEC test set.
6 Conclusion
We have presented the results obtained from the pre-
liminary stages of an Indonesian LVCSR system.
The optimum performance achieved was 92.47%
word accuracy at RTF=0.97. A future development
will be to implement it on a real speech-to-speech
translation system using computer terminals (tablet
PCs). To further refine the system, speaker adap-
tation as well as environmental or noise adaptation
needs to be done in the near future.
References
H. Alwi, S. Dardjowidjojo, H. Lapoliwa, and A.M. Moe-
liono. 2003. Tata Bahasa Baku Bahasa Indonesia (In-
donesian Grammar). Balai Pustaka, Jakarta, Indone-
sia.
T. Jitsuhiro, T. Matsui, and S. Nakamura. 2004. Au-
tomatic generation of non-uniform HMM topologies
based on the MDL criterion. IEICE Trans. Inf. & Syst.,
E87-D(8):2121?2129.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003. Creating corpora for speech-to-speech trans-
lation. In Proc. EUROSPEECH, pages 381?384,
Geneva, Switzerland.
S. Nakamura, E. Sumita, T. Shimizu, S. Sakti, S. Sakai,
J. Zhang, A. Finch, N. Kimura, and Y. Ashikari. 2007.
A-star: Asia speech translation consortium. In Proc.
ASJ Autumn Meeting, page to appear, Yamanashi,
Japan.
S. Sakti, P. Hutagaol, A. Arman, and S. Nakamura. 2004.
Indonesian speech recognition for hearing and speak-
ing impaired people. In Proc. ICSLP, pages 1037?
1040, Jeju, Korea.
S. Sakti, K. Markov, and S.Nakamura. 2005. Rapid de-
velopment of initial indonesian phoneme-based speech
recognition using cross-language approach. In Proc.
Oriental COCOSDA, pages 38?43, Jakarta, Indonesia.
F. Tala. 2003. A Study of Stemming Effects on Infor-
mation Retrieval in Bahasa Indonesia. Ph.D. thesis,
The Information and Language System (ILPS) Group,
Informatics Institute, University of Amsterdam, Ams-
terdam, Netherland.
J. Tan. 2004. Bahasa indonesia: Between faqs and facts.
http://www.indotransnet.com/article1.html.
J. Zhang and S.Nakamura. 2003. An efficient algorithm
to search for a minimum sentence set for collecting
speech database. In Proc. ICPhS, pages 3145?3148,
Barcelona, Spain.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 25?28,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
NICT-ATR Speech-to-Speech Translation System 
Eiichiro Sumita Tohru Shimizu Satoshi Nakamura 
 
National Institute of Information and Communications Technology  
& 
ATR Spoken Language Communication Research Laboratories 
2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, Japan 
eiichiro.sumita, tohru.shimizu & satoshi.nakamura@atr.jp 
 
Abstract 
This paper describes the latest version of 
speech-to-speech translation systems de-
veloped by the team of NICT-ATR for over 
twenty years. The system is now ready to 
be deployed for the travel domain. A new 
noise-suppression technique notably im-
proves speech recognition performance. 
Corpus-based approaches of recognition, 
translation, and synthesis enable coverage 
of a wide variety of topics and portability 
to other languages.  
1 Introduction 
Speech recognition, speech synthesis, and machine 
translation research started about half a century 
ago. They have developed independently for a long 
time until speech-to-speech translation research 
was proposed in the 1980?s. The feasibility of 
speech-to-speech translation was the focus of re-
search at the beginning because each component 
was difficult to build and their integration seemed 
more difficult. After groundbreaking work for two 
decades, corpus-based speech and language proc-
essing technology have recently enabled the 
achievement of speech-to-speech translation that is 
usable in the real world.  
This paper introduces (at ACL 2007) the state-
of-the-art speech-to-speech translation system de-
veloped by NICT-ATR, Japan. 
2 SPEECH-TO-SPEECH TRANSLA-
TION SYSTEM 
A speech-to-speech translation system is very large 
and complex. In this paper, we prefer to describe 
recent progress. Detailed information can be found 
in [1, 2, 3] and their references. 
2.1 Speech recognition 
To obtain a compact, accurate model from corpora 
with a limited size, we use MDL-SSS [4] and 
composite multi-class N-gram models [5] for 
acoustic and language modeling, respectively. 
MDL-SSS is an algorithm that automatically de-
termines the appropriate number of parameters ac-
cording to the size of the training data based on the 
Maximum Description Length (MDL) criterion. 
Japanese, English, and Chinese acoustic models 
were trained using the data from 4,200, 532, and 
536 speakers, respectively. Furthermore, these 
models were adapted to several accents, e.g., US 
(the United States), AUS (Australia), and BRT 
(Britain) for English. A statistical language model 
was trained by using large-scale corpora (852 k 
sentences of Japanese, 710 k sentences of English, 
510 k sentences of Chinese) drawn from the travel 
domain. 
Robust speech recognition technology in noisy 
situations is an important issue for speech transla-
tion in real-world environments. An MMSE 
(Minimum mean square error) estimator for log 
Mel-spectral energy coefficients using a GMM 
(Gaussian Mixture Model) [6] is introduced for 
suppressing interference and noise and for attenu-
ating reverberation. 
Even when the acoustic and language models 
are trained well, environmental conditions such as 
variability of speakers, mismatches between the 
training and testing channels, and interference 
from environmental noise may cause recognition 
errors. These utterance recognition errors can be 
rejected by tagging them with a low confidence 
value. To do this we introduce generalized word 
25
posterior probability (GWPP)-based recognition 
error rejection for the post processing of the speech 
recognition [7, 8]. 
2.2 Machine translation 
The translation modules are automatically con-
structed from large-scale corpora: (1) TATR, a 
phrase-based SMT module and (2) EM, a simple 
memory-based translation module. EM matches a 
given source sentence against the source language 
parts of translation examples. If an exact match is 
achieved, the corresponding target language sen-
tence will be output. Otherwise, TATR is called up. 
In TATR, which is built within the framework of 
feature-based exponential models, we used the fol-
lowing five features: phrase translation probability 
from source to target; inverse phrase translation 
probability; lexical weighting probability from 
source to target; inverse lexical weighting prob-
ability; and phrase penalty. 
Here, we touch on two approaches of TATR: 
novel word segmentation for Chinese, and lan-
guage model adaptation.  
We used a subword-based approach for word 
segmentation of Chinese [9]. This word segmenta-
tion is composed of three steps. The first is a dic-
tionary-based step, similar to the word segmenta-
tion provided by LDC. The second is a subword-
based IOB tagging step implemented by a CRF  
tagging model. The subword-based IOB tagging 
achieves a better segmentation than character-
based IOB tagging. The third step is confidence-
dependent disambiguation to combine the previous 
two results. The subword-based segmentation was 
evaluated with two different data from the Sighan 
Bakeoff and the NIST machine translation evalua-
tion workshop. With the data of the second Sighan 
Bakeoff1, our segmentation gave a higher F-score 
than the best published results. We also evaluated 
this segmentation in a translation scenario using 
the data of NIST translation evaluation 2  2005, 
where its BLEU score3 was 1.1% higher than that 
using the LDC-provided word segmentation. 
The language model that is used plays an impor-
tant role in SMT. The effectiveness of the language 
                                                 
                                                
1 http://sighan.cs.uchicago.edu/bakeoff2005/ 
2http://www.nist.gov/speech/tests/mt/mt05eval_official_
results_release_20050801_v3.html 
3http://www.nist.gov/speech/tests/mt/resources/scoring.
htm 
model is significant if the test data happen to have 
the same characteristics as those of the training 
data for the language models. However, this coin-
cidence is rare in practice. To avoid this perform-
ance reduction, a topic adaptation technique is of-
ten used. We applied this adaptation technique to 
machine translation. For this purpose, a ?topic? is 
defined as clusters of bilingual sentence pairs. In 
the decoding, for a source input sentence, f, a topic 
T is determined by maximizing P(f|T). To maxi-
mize P(f|T) we select cluster T that gives the high-
est probability for a given translation source sen-
tence f. After the topic is found, a topic-dependent 
language model P(e|T) is used instead of P(e), the 
topic-independent language model. The topic-
dependent language models were tested using 
IWSLT06 data 4 . Our approach improved the 
BLEU score between 1.1% and 1.4%. The paper of 
[10] presents a detailed description of this work.  
2.3 Speech synthesis 
An ATR speech synthesis engine called XIMERA 
was developed using large corpora (a 110-hour 
corpus of a Japanese male, a 60-hour corpus of a 
Japanese female, and a 20-hour corpus of a Chi-
nese female). This corpus-based approach makes it 
possible to preserve the naturalness and personality 
of the speech without introducing signal processing 
to the speech segment [11]. XIMERA?s HMM 
(Hidden Markov Model)-based statistical prosody 
model is automatically trained, so it can generate a 
highly natural F0 pattern [12]. In addition, the cost 
function for segment selection has been optimized 
based on perceptual experiments, thereby improv-
ing the naturalness of the selected segments [13]. 
3 EVALUATION 
3.1 Speech and language corpora 
We have collected three kinds of speech and lan-
guage corpora: BTEC (Basic Travel Expression 
Corpus), MAD (Machine Aided Dialog), and FED 
(Field Experiment Data) [14, 15, 16, and 17]. The 
BTEC Corpus includes parallel sentences in two 
languages composed of the kind of sentences one 
might find in a travel phrasebook. MAD is a dialog 
corpus collected using a speech-to-speech transla-
tion system. While the size of this corpus is rela-
tively limited, the corpus is used for adaptation and 
 
4 http://www.slt.atr.jp/IWSLT2006/ 
26
evaluation. FED is a corpus collected in Kansai 
International Airport uttered by travelers using the 
airport. 
3.2 Speech recognition system 
The size of the vocabulary was about 35 k in ca-
nonical form and 50 k with pronunciation varia-
tions. Recognition results are shown in Table 1 for 
Japanese, English, and Chinese with a real-time 
factor5 of 5. Although the speech recognition per-
formance for dialog speech is worse than that for 
read speech, the utterance correctness excluding 
erroneous recognition output using GWPP [8] was 
greater than 83% in all cases. 
 
 BTEC MAD FED 
Characteristics Read speech 
Dialog 
speech 
(Office) 
Dialog 
speech 
(Airport)
# of speakers 20 12 6
# of utterances 510 502 155
# of word tokens 4,035 5,682 1,108
Average length 7.9 11.3 7.1
Perplexity 18.9 23.2 36.2
Japanese 94.9 92.9 91.0
English 92.3 90.5 81.0Word ac-curacy 
Chinese 90.7 78.3 76.5
All 82.4 62.2 69.0Utterance 
correct-
ness 
Not re-
jected 87.1 83.9 91.4
Table 1 Evaluation of speech recognition 
3.3 Machine Translation 
The mechanical evaluation is shown, where there 
are sixteen reference translations. The performance 
is very high except for English-to-Chinese (Table 
2). 
 
 BLEU
Japanese-to-English 0.6998
English-to-Japanese 0.7496
Japanese-to-Chinese 0.6584
Chinese-to-Japanese 0.7400
English-to-Chinese 0.5520
Chinese-to-English 0.6581
Table 2 Mechanical evaluation of translation 
                                                 
5 The real time factor is the ratio to an utterance time. 
The translation outputs were ranked A (perfect), 
B (good), C (fair), or D (nonsense) by professional 
translators. The percentage of ranks is shown in 
Table 3. This is in accordance with the above 
BLEU score. 
 
 A AB ABC
Japanese-to-English 78.4 86.3 92.2
English-to-Japanese 74.3 85.7 93.9
Japanese-to-Chinese 68.0 78.0 88.8
Chinese-to-Japanese 68.6 80.4 89.0
English-to-Chinese 52.5 67.1 79.4
Chinese-to-English 68.0 77.3 86.3
Table 3 Human Evaluation of translation 
4 System presented at ACL 2007 
The system works well in a noisy environment and 
translation can be performed for any combination 
of Japanese, English, and Chinese languages. The 
display of the current speech-to-speech translation 
system is shown below.  
 
 
 
Figure 1  Japanese-to-English Display of NICT-
ATR Speech-to-Speech Translation System 
 
5 CONCLUSION 
This paper presented a speech-to-speech transla-
tion system that has been developed by NICT-ATR 
for two decades. Various techniques, such as noise 
suppression and corpus-based modeling for both 
speech processing and machine translation achieve 
robustness and portability.  
The evaluation has demonstrated that our system 
is both effective and useful in a real-world envi-
ronment. 
27
References  
[1] S. Nakamura, K. Markov, H. Nakaiwa, G. Kikui, H. 
Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E. 
Sumita, and S. Yamamoto. The ATR multilingual 
speech-to-speech translation system. IEEE Trans. on 
Audio, Speech, and Language Processing, 14, No. 
2:365?376, 2006. 
[2] T. Shimizu, Y. Ashikari, E. Sumita, H. Kashioka, 
and S. Nakamura, ?Development of client-server 
speech translation system on a multi-lingual speech 
communication platform,? Proc. of the International 
Workshop on Spoken Language Translation, pp. 213-
216, Kyoto, Japan, 2006. 
[3] R. Zhang, H. Yamamoto, M. Paul, H. Okuma, K. 
Yasuda, Y. Lepage, E. Denoual, D. Mochihashi, A. 
Finch, and E. Sumita, ?The NiCT-ATR Statistical 
Machine Translation System for the IWSLT 2006 
Evaluation,? Proc. of the International Workshop on 
Spoken Language Translation, pp. 83-90, Kyoto, Ja-
pan , 2006. 
[4] T. Jitsuhiro, T. Matsui, and S. Nakamura. Automatic 
generation of non-uniform context-dependent HMM 
topologies based on the MDL criterion. In Proc. of 
Eurospeech, pages 2721?2724, 2003. 
[5] H. Yamamoto, S. Isogai, and Y. Sagisaka. Multi-
class composite N-gram language model. Speech 
Communication, 41:369?379, 2003. 
[6] M. Fujimoto and Y. Ariki. Combination of temporal 
domain SVD based speech enhancement and GMM 
based speech estimation for ASR in noise - evalua-
tion on the AURORA II database and tasks. In Proc. 
of Eurospeech, pages 1781?1784, 2003. 
[7] F. K. Soong, W. K. Lo, and S. Nakamura. Optimal 
acoustic and language model weight for minimizing 
word verification errors. In Proc. of ICSLP, pages 
441?444, 2004 
[8] W. K. Lo and F. K. Soong. Generalized posterior 
probability for minimum error verification of recog-
nized sentences. In Proc. of ICASSP, pages 85?88, 
2005. 
[9] R. Zhang, G. Kikui, and E. Sumita, ?Subword-based 
tagging by conditional random fields for Chinese 
word segmentation,? in Companion volume to the 
proceedings of the North American chapter of the 
Association for Computational Linguistics (NAACL), 
2006, pp. 193?196. 
[10] H. Yamamoto and E. Sumita, ?Online language 
model task adaptation for statistical machine transla-
tion (in Japanese),? in FIT2006, Fukuoka, Japan, 
2006, pp. 131?134. 
[11] H. Kawai, T. Toda, J. Ni, and M. Tsuzaki. XI-
MERA: A new TTS from ATR based on corpus-
based technologies. In Proc. of 5th ISCA Speech 
Synthesis Workshop, 2004. 
[12] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobaya-
shi, and T. Kitamura. Speech parameter generation 
algorithms for HMM-based speech synthesis. In Proc. 
of ICASSP, pages 1215?1218, 2000. 
[13] T. Toda, H. Kawai, and M. Tsuzaki. Optimizing 
sub-cost functions for segment selection based on 
perceptual evaluation in concatenative speech syn-
thesis. In Proc. of ICASSP, pages 657?660, 2004. 
[14] T. Takezawa and G. Kikui. Collecting machine ?
translation-aided bilingual dialogs for corpus-based 
speech translation. In Proc. of Eurospeech, pages 
2757?2760, 2003. 
[15] G. Kikui, E. Sumita, T. Takezawa, and S. Yama-
moto. Creating corpora for speech-to-speech transla-
tion. In Proc. Of Eurospeech, pages 381?384, 2003. 
[16] T. Takezawa and G. Kikui. A comparative study on 
human communication behaviors and linguistic char-
acteristics for speech-to-speech translation. In Proc. 
of LREC, pages 1589?1592, 2004. 
[17] G. Kikui, T. Takezawa, M. Mizushima, S. Yama-
moto, Y. Sasaki, H. Kawai, and S. Nakamura. Moni-
tor experiments of ATR speech-to-speech translation 
system. In Proc. of Autumn Meeting of the Acousti-
cal Society of Japan, pages 1?7?10, 2005, in Japa-
nese. 
28
Proceedings of NAACL HLT 2009: Short Papers, pages 221?224,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On the Importance of Pivot Language Selection
for Statistical Machine Translation
Michael Paul??, Hirofumi Yamamoto??, Eiichiro Sumita? and Satoshi Nakamura?
? NICT, Hikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, Japan
? Kinki University School of Science and Engineering, Higashi-Osaka City, 577-8502, Japan
Michael.Paul@nict.go.jp
Abstract
Recent research on multilingual statistical machine
translation focuses on the usage of pivot languages
in order to overcome resource limitations for certain
language pairs. Due to the richness of available lan-
guage resources, English is in general the pivot lan-
guage of choice. In this paper, we investigate the
appropriateness of languages other than English as
pivot languages. Experimental results using state-of-
the-art statistical machine translation techniques to
translate between twelve languages revealed that the
translation quality of 61 out of 110 language pairs
improved when a non-English pivot language was
chosen.
1 Introduction
The translation quality of state-of-the-art, phrase-based
statistical machine translation (SMT) approaches heavily
depends on the amount of bilingual language resources
available to train the statistical models. For frequently
used language pairs like French-English or Chinese-
English, large-sized text data sets are readily available.
There exist several data collection initiatives like the Lin-
guistic Data Consortium1, the European Language Re-
source Association2, or the GSK3, amassing and distribut-
ing large amounts of textual data. However, for less fre-
quently used language pairs, e.g., most of the Asian lan-
guages, only a limited amount of bilingual resources are
available, if at all.
In order to overcome such language resource limita-
tions, recent research on multilingual SMT focuses on the
usage of pivot languages. Instead of a direct translation
between two languages where only a limited amount of
bilingual resources is available, the pivot translation ap-
proach makes use of a third language that is more appro-
priate due to the availability of more bilingual corpora
and/or its relatedness towards either the source or the tar-
get language. Several pivot translation techniques like
cascading, phrase-table combination, or pseudo corpus
generation have already been proposed (cf. Section 2).
However, for most recent research efforts, English is
the pivot language of choice due to the richness of avail-
1LDC: http://www.ldc.upenn.edu
2ELRA: http://www.elra.info
3GSK: http://www.gsk.or.jp/catalog.html
able language resources. For example, the Europarl cor-
pus is exploited in (Utiyama and Isahara, 2007) for com-
paring pivot translation approaches between French, Ger-
man and Spanish via English. Other research efforts tried
to exploit the closeness between specific language pairs
to generate high-quality translation hypotheses in the first
step to minimize the pivot detoriation effects, e.g., for
Catalan-to-English translations via Spanish (Gispert and
Marino, 2006).
This paper investigates the appropriateness of lan-
guages other than English as pivot languages to support
future research on machine translation between under-
resourced language pairs. Pivot translation experiments
using state-of-the-art SMT techniques are carried out to
translate between twelve of the major world languages
covering Indo-European as well as Asian languages and
the effects of selecting a non-English language as the
pivot language are discussed in Section 3.
2 Pivot Translation
Pivot translation is a translation from a source language
(SRC) to a target language (TRG) through an intermedi-
ate pivot (or bridging) language (PVT). Within the SMT
framework, the following coupling strategies have al-
ready been investigated:
1. cascading of two translation systems where the first
MT engine translates the source language input into
the pivot language and the second MT engine takes
the obtained pivot language output as its input and
translates it into the target language.
2. pseudo corpus approach that (i) creates a ?noisy?
SRC-TRG parallel corpus by translating the pivot
language parts of the SRC-PVT and PVT-TRG train-
ing resources into the target language using an SMT
engine trained on the PVT-TRG and PVT-SRC lan-
guage resources, respectively, and (ii) directly trans-
lates the source language input into the target lan-
guage using a single SMT engine that is trained on
the obtained SRC-TRG language resources (Gispert
and Marino, 2006).
3. phrase-table composition in which the translation
models of the SRC-PVT and PVT-TRG translation en-
gines are combined to a new SRC-TRG phrase-table
by merging SRC-PVT and PVT-TRG phrase-table en-
tries with identical pivot language phrases and mul-
221
tiplying posterior probabilities (Utiyama and Isa-
hara, 2007; Wu and Wang, 2007).
4. bridging at translation time where the coupling is
integrated into the SMT decoding process by model-
ing the pivot text as a hidden variable and assuming
independence between source and target sentences
(Bertoldi et al, 2008).
3 Pivot Language Selection
The effects of using different pivot languages are inves-
tigated using the multilingual Basic Travel Expressions
Corpus (BTEC), which is a collection of sentences that
bilingual travel experts consider useful for people going
to or coming from another country. For the pivot transla-
tion experiments, we selected twelve of the major world
languages covered by BTEC, favoring languages that are
actively being researched on, i.e.,Chinese (zh), English
(en), French (fr), German (de), Hindi (hi), Indonesian
(id), Japanese (ja), Korean (ko), Malay (ms), Spanish
(es), Thai (th), and Vietnamese (vi). These languages
differ largely in word order (SVO, SOV), segmentation
unit (phrase, word, none), and degree of inflection (high,
moderate, light). All data sets were case-sensitive with
punctuation marks preserved.
However, in a real-world application, identical lan-
guage resources covering three or more languages are not
necessarily to be expected. In order to avoid a trilingual
scenario for the pivot translation experiments described
in this paper, the 160k sentence-aligned BTEC corpus
was randomly split into two subsets of 80k sentences
each, whereby the first set of sentence pairs was used to
train the source-to-pivot translation models (80ksp) and
the second subset of sentence pairs was used to train the
pivot-to-target translation models (80kpt). Table 1 sum-
marizes the characteristics of the BTEC corpus data sets
used for the training (train) of the SMT models, the tun-
ing of model weights (dev), and the evaluation of transla-
tion quality (eval). Besides the number of sentences (sen)
and the vocabulary (voc), the sentence length (len) is also
given, as the average number of words per sentence.
For the training of the SMT models, standard word
alignment (Och and Ney, 2003) and language modeling
(Stolcke, 2002) tools were used. Minimum error rate
training (MERT) was used to tune the decoder?s param-
eters, and performed on the dev set using the technique
proposed in (Och and Ney, 2003). For the translation,
an in-house multi-stack phrase-based decoder compara-
ble to MOSES was used. For the evaluation of trans-
lation quality, we applied standard automatic evaluation
metrics, i.e., BLEU (Papineni et al, 2002) and METEOR
(Banerjee and Lavie, 2005). For the experimental results
in this paper, the given scores are calculated as the aver-
age of the respective BLEU and METEOR scores obtained
for each system output and are listed as percent figures.
Table 1: Language Resources
BTEC train dev eval
Corpus 80ksp 80kpt set set
# of sen 80,000 80,000 1,000 1,000
en voc 12,264 11,047 1,262 1,292
len 7.8 7.2 7.1 7.2
de voc 19,593 17,324 1,486 1,491
len 7.4 6.8 6.7 6.8
es voc 16,317 14,807 1,486 1,511
len 7.6 7.1 7.0 7.2
fr voc 15,319 13,663 1,455 1,466
len 7.8 7.3 7.1 7.3
hi voc 26,096 19,906 1,558 1,588
len 8.1 7.6 7.4 7.5
id voc 14,585 13,224 1,433 1,394
len 7.0 6.5 6.3 6.4
ja voc 13,868 12,517 1,407 1,408
len 8.8 8.2 8.1 8.2
ko voc 13,546 12,281 1,366 1,365
len 8.3 7.8 7.7 7.8
ms voc 15,113 13,616 1,459 1,438
len 7.1 6.6 6.4 6.5
th voc 6,103 5,603 1,081 1,053
len 8.1 7.4 7.3 7.4
vi voc 7,980 7,335 1,245 1,267
len 9.4 8.7 8.5 8.6
zh voc 11,084 10,159 1,312 1,301
len 7.1 6.6 6.4 6.5
In order to get an idea of how difficult the translation
task for the different languages is supposed to be, the
automatic evaluation scores for the direct translation ap-
proach using the 80ksp language resources are summa-
rized in Section 3.1. The effects of the pivot language se-
lection are discussed in Section 3.2 using the pivot trans-
lation method of cascading two SMT systems. In addition,
the dependency between selecting the optimal pivot lan-
guage for a given language pair and the amount of avail-
able training resources are described in Section 3.3.
3.1 Direct Translation Results
The automatic evaluation scores for all source and target
language pair combinations of the direct translation ap-
proach are given in Table 2. For each target language, the
highest evaluation scores are marked in boldface and the
lowest scores are marked in typewriter mode.
The highest translation quality was achieved for the
Japanese?Korean, Indonesian?Malay, and Span-
ish?English translation tasks. In addition, relatively
high evaluation scores were achieved for Japanese
?Chinese and for translations from English into Ger-
man, French, Hindi, Thai, and Vietnamese. On the other
hand, the most difficult translation tasks were those hav-
ing Korean or Chinese as the source language.
3.2 Pivot Translation Results
The automatic evaluation scores for all pivot translation
combinations are summarized in Table 3 whereby for
each source-target language pair, the results of the exper-
iments using (i) English (en) and (ii) the best performing
language (best) as the pivot language are listed.
Comparing the results of the pivot translation experi-
ments towards the direct translation results, we can see
222
Table 2: Translation Quality of Direct Translation Approach
SRC de en es fr hi id ja ko ms th vi zh
de ? 74.24 56.22 49.78 63.25 69.31 54.09 50.88 69.33 66.83 67.17 51.59
en 63.31 ? 64.30 56.10 66.43 73.46 55.64 54.15 73.66 70.57 72.64 53.18
es 58.98 76.43 ? 53.53 63.60 70.46 55.37 51.41 70.46 67.69 69.15 52.03
fr 55.45 72.24 57.25 ? 61.70 68.58 55.17 52.15 68.72 65.03 65.97 52.83
hi 52.89 67.82 50.69 45.53 ? 68.65 52.94 50.93 68.14 66.44 66.88 51.31
id 52.75 67.58 52.06 46.00 62.43 ? 55.52 52.90 88.69 67.20 68.01 52.77
ja 35.43 51.65 37.82 32.70 46.94 52.90 ? 78.73 53.26 54.14 51.45 67.83
ko 32.65 50.12 36.97 31.62 44.67 53.51 78.88 ? 51.75 52.35 51.34 63.19
ms 53.16 68.17 53.06 45.30 63.36 91.12 54.88 52.18 ? 67.79 67.93 53.23
th 49.66 64.53 50.16 42.70 59.40 66.58 53.82 50.81 65.76 ? 65.90 52.22
vi 52.59 69.16 53.17 45.60 61.19 68.39 52.95 50.68 69.44 67.64 ? 51.29
zh 34.18 49.79 37.13 31.16 44.33 52.72 65.64 62.23 52.46 51.88 51.09 ?
Table 4: Pivot Language Selection
PVT usage (%) PVT usage (%)
en 49 (44.5) ko 12 (10.9)
ms 16 (14.5) zh 2 ( 1.8)
id 16 (14.5) es 1 ( 0.9)
ja 14 (12.7)
that in general the pivot translation approach performs
worse than the direct translation approach due to the ef-
fect of error chaining, i.e., translation errors of the SRC-
PVT engine cause a degradation in translation quality
of the PVT-TRG system output. However, for language
pairs like Korean?German, Japanese? Indonesian
and German/Spanish?Korean, the best pivot transla-
tion system outperforms the direct translation approach
slightly. This phenomenon is caused mainly by the high
SRC-PVT (PVT-TRG) translation quality in combination
with a better PVT-TRG (SRC-PVT) performance compared
to the direct SRC-TRG system output results.
Besides the automatic evaluation scores, Table 3 lists
also the optimal pivot language for each source-target
language pair in boldface. The experimental results show
that English is indeed the best pivot language when trans-
lating between languages, like German, Spanish, French,
Hindi, Thai, and Vietnamese, whose direct translation
performance from/into English is high. For these six
languages, all language pair combinations achieved the
highest scores using the English pivot translation ap-
proach. In contrast, English is the pivot language of
choice for only 16.2% (11 out of 68) of the language pairs
when translating from/into Japanese, Korean, Indone-
sian, or Malay. In the remaining cases, the language with
the highest direct translation scores is in general selected
as the optimal pivot language, i.e., Japanese for Korean,
Malay for Indonesian and vice versa. For Chinese, the
choice of the optimal pivot language varies largely de-
pending on the language direction. However, the selec-
tion of the optimal pivot language is not symmetric for
34.5% of the language pairs, i.e., a different optimal pivot
language was obtained for the SRC-TRG compared to the
TRG-SRC translation task. This indicates that the choice
of the optimal pivot language depends on the relatedness
of the SRC and PVT languages as well as the relatedness
of the PVT and TRG languages.
The distribution of the optimal pivot language selection
Table 5: Pivot Selection Shifts for 10k vs. 80k Training Data
10k 80k pivot translation 10k 80k pivot translation
PVT PVT language pair PVT PVT language pair
ko en ja-fr, ja-de, ja-vi ko ms ja-id
ko zh-fr, zh-es, zh-hi en vi-zh
ja ko-vi, zh-vi, zh-th es fr-zh
ms id-fr en ja fr-ko, hi-ko, vi-ko
ja es ko-hi id zh-ms
ja id ko-ms,th-zh ms ko id-ja
en es-ms,hi-zh,hi-ja es fr-ja
en de-ja,es-ja
for all language pairs is given in Table 4. The figures
show that the English pivot approach still achieves the
highest scores for the majority of the examined language
pairs. However, in 55.5% (61 out of 110) of the cases, a
non-English pivot language, mainly Malay, Indonesian,
Japanese, or Korean, is to be preferred.
3.3 Training Data Size Dependency
In order to investigate the dependency between selecting
the optimal pivot language for a given language pair and
the amount of available training resources, we repeated
the pivot translation experiments described in Section 3.2
for statistical models trained on subsets of 10k sentences
randomly extracted from the 80ksp and the 80kpt cor-
pora, respectively.
The results showed that 75.5% of the pivot language
selections are identical for small (10k) and large (80k)
training data sets. For the remaining 27 out of 110 trans-
lation tasks, Table 5 lists how the optimal pivot language
selection changed. In the case of small training data sets,
the pivot language is closely related (in terms of high
direct translation quality) to the source language. How-
ever, for larger training data sets, the focus shifts towards
closely related target languages. Therefore, the higher
the translation quality of the pivot translation task is, the
more dependend the selection of the optimal pivot lan-
guage is on the system performance of the PVT-TRG task.
4 Conclusion
In this paper, the effects of using non-English pivot lan-
guages for translations between twelve major world lan-
guages were compared to the standard English pivot
translation approach. The experimental results revealed
that English was indeed more frequently (45.5% out of
223
Table 3: Translation Quality of Pivot Translation Approach
SRC PVT de es fr hi id ja ko ms th vi zh
de en ? 54.69 47.01 60.48 66.42 52.53 51.10 66.47 65.06 66.08 50.46
best ? (en) 54.69 (en) 47.01 (en) 60.48 (ms) 66.92 (ko) 52.67 (en) 51.10 (en) 66.47 (en) 65.06 (en) 66.08 (en) 50.46
es en 55.37 ? 48.75 60.24 68.10 52.68 51.80 67.54 65.59 66.99 51.08
best (en) 55.37 ? (en) 48.75 (en) 60.24 (ms) 69.29 (ko) 53.10 (en) 51.80 (id) 68.37 (en) 65.59 (en) 66.99 (en) 51.08
fr en 52.03 53.88 ? 58.27 65.59 52.51 51.19 65.43 62.47 64.34 50.12
best (en) 52.03 (en) 53.88 ? (en) 58.27 (ms) 67.25 (ko) 53.06 (ja) 51.81 (en) 65.43 (en) 62.47 (en) 64.34 (ms) 50.35
hi en 48.56 48.69 41.71 ? 63.01 50.21 48.96 63.13 62.08 62.48 48.12
best (en) 48.56 (en) 48.69 (en) 41.71 ? (ms) 65.43 (id) 51.09 (ja) 49.06 (id) 65.54 (en) 62.08 (en) 62.48 (id) 48.71
id en 48.97 49.48 42.56 57.41 ? 51.30 50.19 72.94 62.40 64.60 49.45
best (ms) 49.19 (ms) 50.16 (en) 42.56 (ms) 60.30 ? (ko) 54.12 (ja) 51.54 (en) 72.94 (ms) 64.51 (ms) 65.51 (ms) 51.82
ja en 33.43 36.61 31.20 44.27 52.31 ? 56.34 51.34 52.57 50.97 52.85
best (en) 33.43 (ko) 36.88 (en) 31.20 (ko) 44.96 (ms) 53.13 ? (zh) 60.99 (ko) 51.37 (ko) 52.65 (en) 50.97 (ko) 62.65
ko en 31.52 34.50 29.01 43.23 50.70 54.43 ? 49.83 50.74 49.97 51.66
best (ja) 33.23 (ja) 36.18 (ja) 31.20 (es) 44.24 (ja) 52.21 (zh) 60.10 ? (id) 51.79 (ja) 51.98 (en) 49.97 (ja) 62.74
ms en 49.64 49.71 42.39 57.85 73.25 51.01 49.52 ? 62.64 64.09 49.22
best (id) 51.14 (id) 50.95 (id) 43.87 (id) 60.76 (en) 73.25 (id) 54.56 (ja) 50.94 ? (id) 65.99 (id) 66.97 (id) 52.46
th en 46.57 46.61 39.83 55.41 61.88 50.54 48.75 61.09 ? 61.50 47.75
best (en) 46.57 (en) 46.61 (en) 39.83 (en) 55.41 (ms) 63.22 (ko) 51.37 (ja) 50.39 (id) 62.36 ? (en) 61.50 (id) 48.72
vi en 49.87 50.17 43.04 57.42 64.94 50.68 49.45 64.60 62.50 ? 48.12
best (en) 49.87 (en) 50.17 (en) 43.04 (en) 57.42 (ms) 67.14 (ko) 51.86 (ja) 49.48 (id) 66.57 (en) 62.50 ? (ms) 48.86
zh en 32.26 35.29 28.35 43.20 50.11 53.27 52.53 49.20 51.54 49.92 ?
best (en) 32.26 (en) 35.29 (en) 28.35 (en) 43.20 (ms) 52.18 (ko) 61.96 (ja) 60.64 (ja) 49.71 (en) 51.54 (en) 49.92 ?
110 language pairs) selected as the best pivot language
over any other examined language. However, its usage is
limited to translations between Indo-European languages
and some Asian languages like Thai or Vietnamese. Oth-
erwise, the English pivot approach is largely outper-
formed by using Asian languages as the pivot languages,
especially Japanese, Malay, Indonesian, or Korean.
The analysis of the results revealed that the selection of
the optimal pivot language largely depends on the SRC-
PVT and PVT-TRG translation performance, i.e., for small
training corpora, the relationship between source/pivot
languages seems to be more important, whereas the se-
lection criteria moves towards the relationship between
pivot/target languages for larger amounts of training data
and thus for MT engines of higher translation quality.
In order to explore the question of pivot selection fur-
ther and arrive at firmer conclusions, future work will
have to investigate in detail what kind of features are im-
portant in selecting a pivot language for a given language
pair. Besides the translation quality of SMT engines, au-
tomatic metrics to measure the relatedness of a language
pair should also be taken into account to find optimal
pivot languages. For example, (Birch et al, 2008) pro-
poses features like amount of reordering, the morpholog-
ical complexity of the target language, and historical re-
latedness of the two languages as strong predictors for
the variability of SMT system performance.
In addition, concerning the question of how the pivot
language selection criteria depends on the choice of the
pivot translation method, future work will also have to
investigate the effects of pivot language selection for the
other pivot translation approaches described in Section 2.
Based on these findings, we plan to determine the con-
tribution of different language characteristics on the sys-
tem performance automatically to obtain useful indica-
tors that could be used to train statistical classification
models to predict the best pivot language for a new lan-
guage pair and improve the usability of machine transla-
tion between under-resourced languages further.
Acknowledgment
This work is partly supported by the Grant-in-Aid for Sci-
entific Research (C) Number 19500137 and ?Construc-
tion of speech translation foundation aiming to overcome
the barrier between Asian languages?, the Special Coor-
dination Funds for Promoting Science and Technology of
the Ministry of Education, Culture, Sports, Science and
Technology, Japan.
References
S. Banerjee and A. Lavie. 2005. METEOR: An Automatic
Metric for MT Evaluation. In Proc. of the ACL, pages 65?
72, Ann Arbor, US.
N. Bertoldi, M. Barbaiani, M. Federico, and R. Cattoni. 2008.
Phrase-Based SMT with Pivot Languages. In Proc. of the
IWSLT, pages 143?149, Hawaii, US.
A. Birch, M. Osborne, and P. Koehn. 2008. Predicting Success
in MT. In Proc. of the EMNLP, pages 744?753, Hawaii, US.
A. Gispert and J. Marino. 2006. Catalan-English SMT without
Parallel Corpus: Bridging through Spanish. In Proc. of 5th
LREC, pages 65?68, Genoa, Italy.
F. Och and H. Ney. 2003. A Systematic Comparison of
Statistical Alignment Models. Computational Linguistics,
29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU:
a Method for Automatic Evaluation of Machine Translation.
In Proc. of the 40th ACL, pages 311?318, Philadelphia, US.
A. Stolcke. 2002. SRILM an extensible language modeling
toolkit. In Proc. of ICSLP, pages 901?904, Denver.
M. Utiyama and H. Isahara. 2007. A comparison of pivot meth-
ods for phrase-based SMT. In Proc. of HLT, pages 484?491,
New York, US.
H. Wu and H. Wang. 2007. Pivot Language Approach
for Phrase-Based SMT. In Proc. of ACL, pages 856?863,
Prague, Czech Republic.
224
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 32?39,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Dialogue Acts to Construct Dialogue Systems for Consulting
Kiyonori Ohtake Teruhisa Misu Chiori Hori Hideki Kashioka Satoshi Nakamura
MASTAR Project, National Institute of Information and Communications Technology
Hikaridai, Keihanna Science City, JAPAN
kiyonori.ohtake (at) nict.go.jp
Abstract
This paper introduces a new corpus of con-
sulting dialogues, which is designed for
training a dialogue manager that can han-
dle consulting dialogues through sponta-
neous interactions from the tagged dia-
logue corpus. We have collected 130 h
of consulting dialogues in the tourist guid-
ance domain. This paper outlines our tax-
onomy of dialogue act annotation that can
describe two aspects of an utterances: the
communicative function (speech act), and
the semantic content of the utterance. We
provide an overview of the Kyoto tour
guide dialogue corpus and a preliminary
analysis using the dialogue act tags.
1 Introduction
This paper introduces a new dialogue corpus for
consulting in the tourist guidance domain. The
corpus consists of speech, transcripts, speech act
tags, morphological analysis results, dependency
analysis results, and semantic content tags. In this
paper, we describe the current status of a dialogue
corpus that is being developed by our research
group, focusing on two types of tags: speech act
tags and semantic content tags. These speech act
and semantic content tags were designed to ex-
press the dialogue act of each utterance.
Many studies have focused on developing spo-
ken dialogue systems. Their typical task do-
mains included the retrieval of information from
databases or making reservations, such as airline
information e.g., DARPA Communicator (Walker
et al, 2001) and train information e.g., ARISE
(Bouwman et al, 1999) and MASK (Lamel et al,
2002). Most studies assumed a definite and con-
sistent user objective, and the dialogue strategy
was usually designed to minimize the cost of in-
formation access. Other target tasks include tutor-
ing and trouble-shooting dialogues (Boye, 2007).
In such tasks, dialogue scenarios or agendas are
usually described using a (dynamic) tree structure,
and the objective is to satisfy all requirements.
In this paper, we introduce our corpus, which is
being developed as part of a project to construct
consulting dialogue systems, that helps the user in
making a decision. So far, several projects have
been organized to construct speech corpora such
as CSJ (Maekawa et al, 2000) for Japanese. The
size of CSJ is very large, and a great part of the
corpus consists of monologues. Although, CSJ
includes some dialogues, the size of dialogues is
not enough to construct a dialogue system via re-
cent statistical techniques. In addition, relatively
to consulting dialogues, the existing large dialogue
corpora covered very clear tasks in limited do-
mains.
However, consulting is a frequently used and
very natural form of human interaction. We of-
ten consult with a sales clerk while shopping or
with staff at a concierge desk in a hotel. Such dia-
logues usually form part of a series of information
retrieval dialogues that have been investigated in
many previous studies. They also contains various
exchanges, such as clarifications and explanations.
The user may explain his/her preferences vaguely
by listing examples. The server would then sense
the user?s preferences from his/her utterances, pro-
vide some information, and then request a deci-
sion.
It is almost impossible to handcraft a scenario
that can handle such spontaneous consulting dia-
logues; thus, the dialogue strategy should be boot-
strapped from a dialogue corpus. If an extensive
dialogue corpus is available, we can model the
dialogue using machine learning techniques such
as partially observable Markov decision processes
(POMDPs) (Thomson et al, 2008). Hori et al
(2008) have also proposed an efficient approach to
organize a dialogue system using weighted finite-
state transducers (WFSTs); the system obtains the
32
Table 2: Overview of Kyoto tour guide dialogue
corpus
dialogue type F2F WOZ TEL
# of dialogues 114 80 62
# of guides 3 2 2
avg. # of utterance 365.4 165.2 324.5/ dialogue (guide)
avg. # of utterance 301.7 112.9 373.5/ dialogue (tourist)
structure of the transducers and the weight for
each state transitions from an annotated corpus.
Thus, the corpus must be sufficiently rich in in-
formation to describe the consulting dialogue to
construct the statistical dialogue manager via such
techniques.
In addition, a detailed description would be
preferable when developing modules that focus
on spoken language understanding and generation
modules. In this study, we adopt dialogue acts
(DAs) (Bunt, 2000; Shriberg et al, 2004; Banga-
lore et al, 2006; Rodriguez et al, 2007; Levin et
al., 2002) for this information and annotate DAs in
the corpus.
In this paper, we describe the design of the Ky-
oto tour guide dialogue corpus in Section 2. Our
design of the DA annotation is described in Sec-
tion 3. Sections 4 and 5 respectively describe two
types of the tag sets, namely, the speech act tag
and the semantic content tag.
2 Kyoto Tour Guide Dialogue Corpus
We are currently developing a dialogue corpus
based on tourist guidance for Kyoto City as the tar-
get domain. Thus far, we have collected itinerary
planning dialogues in Japanese, in which users
plan a one-day visit to Kyoto City. There are
three types of dialogues in the corpus: face-to-
face (F2F), Wizard of OZ (WOZ), and telephonic
(TEL) dialogues. The corpus consists of 114 face-
to-face dialogues, 80 dialogues using the WOZ
system, and 62 dialogues obtained from telephone
conversations with the interface of the WOZ sys-
tem.
The overview of these three types of dialogues
is shown in Table 2. Each dialogue lasts for almost
30 min. Most of all the dialogues have been man-
ually transcribed. Table 2 also shows the average
number of utterances per a dialogue.
Each face-to-face dialogue involved a profes-
sional tour guide and a tourist. Three guides, one
male and two females, were employed to collect
the dialogues. All three guides were involved in
almost the same number of dialogues. The guides
used maps, guidebooks, and a PC connected to the
internet.
In the WOZ dialogues, two female guides were
employed. Each of them was participated in 40
dialogues. The WOZ system consists of two in-
ternet browsers, speech synthesis program, and
an integration program for the collaborative work.
Collaboration was required because in addition to
the guide, operators were employed to operate the
WOZ system and support the guide. Each of the
guide and operators used own computer connected
each other, and they collaboratively operate the
WOZ system to serve a user (tourist).
In the telephone dialogues, two female guides
who are the same for the WOZ dialogues were
employed. In these dialogues, we used the WOZ
system, but we did not need the speech synthesis
program. The guide and a tourist shared the same
interface in different rooms, and they could talk to
each other through the hands-free headset.
Dialogues to plan a one-day visit consist of sev-
eral conversations for choosing places to visit. The
conversations usually included sequences of re-
quests from the users and provision of information
by the guides as well as consultation in the form of
explanation and evaluation. It should be noted that
in this study, enabling the user to access informa-
tion is not an objective in itself, unlike information
kiosk systems such as those developed in (Lamel
et al, 2002) or (Thomson et al, 2008). The objec-
tive is similar to the problem-solving dialogue of
the study by Ferguson and Allen (1998), in other
words, accessing information is just an aspect of
consulting dialogues.
An example of dialogue via face-to-face com-
munication is shown in Table 1. This dialogue is
a part of a consultation to decide on a sightseeing
spot to visit. The user asks about the location of a
spot, and the guide answers it. Then, the user pro-
vides a follow-up by evaluating the answer. The
task is challenging because there are many utter-
ances that affect the flow of the dialogue during a
consultation. The utterances are listed in the order
of their start times with the utterance ids (UID).
From the column ?Time? in the table, it is easy to
see that there are many overlaps.
33
Table 1: Example dialogue from the Kyoto tour guide dialogue corpus
UID Time (ms) Speaker Transcript Speech act tag** Semantic content tag
56 76669?78819 User
Ato (And,)
WH?Question Where
null
Ohara ga (Ohara is) (activity),location
dono henni (where) (activity),(demonstrative),interr
narimasuka (I?d like to know) (activity),predicate
57 80788?81358 Guide kono (here) State Answer?56 (demonstrative),kosoahendesune (is around) (demonstrative),noun
58 81358?81841 Guide Ohara ha (Ohara) State Inversion location
59 81386?82736 User Chotto (a bit) State Evaluation?57 (transp),(cost),(distance),adverb-phrasehanaresugitemasune (is too far) (transp),(cost),(distance),predicate
60 83116?83316 Guide A (Yeah,) Pause Grabber null
61 83136?85023 User
Kore demo (it)
Y/N?Question
null
ichinichi dewa (in a day) (activity),(planning),duration
doudeshou (Do you think I can do) (activity),(planning),(demonstrative),interr
62 83386?84396 Guide Soudesune (right.) State Acknowledgment?59 null
63 85206?87076 Guide
Ichinichi (One day)
State AffirmativeAnswer?61
(activity),(planning),(entity),day-window
areba (is) (activity),(planning),predicate
jubuN (enough) (consulting),(activity),adverb-phrase
ikemasu (to enjoy it.) (consulting),(activity),action
64 88392?90072 Guide
Oharamo (Ohara is)
State Opinion
(activity),location
sugoku (very) (recommendation),(activity),adverb-phrase
kireidesuyo (a beautiful spot) (recommendation),(activity),predicate
65 89889?90759 User Iidesune (that would be nice.) State Acknowledgment?64 (consulting),(activity),predicateEvaluation?64
* Tags are concatenated using a delimiter ? ? and omitting null values.
The number following the ??? symbol denotes the target utterance of the function.
3 Annotation of Communicative
Function and Semantic Content in DA
We annotate DAs in the corpus in order to de-
scribe a user?s intention and a system?s (or the tour
guide?s) action. Recently, several studies have ad-
dressed multilevel annotation of dialogues (Levin
et al, 2002; Bangalore et al, 2006; Rodriguez et
al., 2007); in our study, we focus on the two as-
pects of a DA indicated by Bunt (2000). One is the
communicative function that corresponds to how
the content should be used in order to update the
context, and the other is a semantic content that
corresponds to what the act is about. We consider
both of them important information to handle the
consulting dialogue. We designed two different
tag sets to annotate DAs in the corpus. The speech
act tag is used to capture the communicative func-
tions of an utterance using domain-independent
multiple function layers. The semantic content tag
is used to describe the semantic contents of an ut-
terance using domain-specific hierarchical seman-
tic classes.
4 Speech Act Tags
In this section, we introduce the speech act (SA)
tag set that describes communicative functions of
utterances. As the base units for tag annotation,
we adopt clauses that are detected by applying
the clause boundary annotation program (Kash-
ioka and Maruyama, 2004) to the transcript of the
dialogue. Thus, in the following discussions, ?ut-
terance? denotes a clause.
4.1 Tag Specifications
There are two major policies in SA annotation.
One is to select exactly one label from the tag set
(e.g., the AMI corpus1). The other is to annotate
with as many labels as required. MRDA (Shriberg
et al, 2004) and DIT++ (Bunt, 2000) are defined
on the basis of the second policy. We believe that
utterances are generally multifunctional and this
multifunctionality is an important aspect for man-
aging consulting dialogues through spontaneous
interactions. Therefore, we have adopted the latter
policy.
By extending the MRDA tag set and DIT++, we
defined our speech act tag set that consists of six
layers to describe six groups of function: Gen-
eral, Response, Check, Constrain, ActionDiscus-
sion, and Others. A list of the tag sets (excluding
the Others layer is shown in Table 3. The General
layer has two sublayers under the labels, Pause
and WH-Question, respectively. The two sublay-
ers are used to elaborate on the two labels, respec-
tively. A tag of the General layer must be labeled
to an utterance, but the other layer?s tags are op-
tional, in other words, layers other than the Gen-
eral layer can take null values when there is no tag
which is appropriate to the utterance. In the practi-
cal annotation, the most appropriate tag is selected
from each layer, without taking into account any
of the other layers.
The descriptions of the layers are as follows:
General: It is used to represent the basic form
1http://corpus.amiproject.org
34
Table 3: List of speech act tags and their occurrence in the experiment
Tag Percentage(%) Tag Percentage(%) Tag Percentage(%) Tag Percentage(%)User Guide User Guide User Guide User Guide
(General) (Response) (ActionDiscussion) (Constrain)
Statement 45.25 44.53 Acknowledgment 19.13 5.45 Opinion 0.52 2.12 Reason 0.64 2.52
Pause 12.99 15.05 Accept 4.68 6.25 Wish 1.23 0.05 Condition 0.61 3.09
Backchannel 26.05 9.09 PartialAccept 0.02 0.10 Request 0.22 0.19 Elaboration 0.28 4.00
Y/N-Question 3.61 2.19 AffirmativeAnswer 0.08 0.20 Suggestion 0.16 1.12 Evaluation 1.35 2.01
WH-Question 1.13 0.40 Reject 0.25 0.11 Commitment 1.15 0.29 (Check)
Open-Question 0.32 0.32 PartialReject 0.04 0.03 RepetitionRequest 0.07 0.03
OR?after-Y/N 0.05 0.02 NegativeAnswer 0.10 0.10 UnderstandingCheck 0.19 0.20
OR-Question 0.05 0.03 Answer 1.16 2.57 DoubleCheck 0.36 0.15
Statement== 9.91 27.79 ApprovalRequest 2.01 1.07
of the unit. Most of the tags in this layer
are used to describe forward-looking func-
tions. The tags are classified into three large
groups: ?Question,? ?Fragment,? and ?State-
ment.? ?Statement==? denotes the continua-
tion of the utterance.
Response: It is used to label responses directed
to a specific previous utterance made by the
addressee.
Check: It is used to label confirmations that are
along a certain expected response.
Constrain: It is used to label utterances that re-
strict or complement the target of the utter-
ance.
ActionDiscussion: It is used to label utterances
that pertain to a future action.
Others: It is used to describe various functions of
the utterance, e.g., Greeting, SelfTalk, Wel-
come, Apology, etc.
In the General layer, there are two sublayers:? (1)
the Pause sublayer that consists of Hold, Grabber,
Holder, and Releaser and (2) the WH sublayer that
labels the WH-Question type.
It should be noted that this taxonomy is in-
tended to be used for training spoken dialogue sys-
tems. Consequently, it contains detailed descrip-
tions to elaborate on the decision-making process.
For example, checks are classified into four cat-
egories because they should be treated in various
ways in a dialogue system. UnderstandingCheck
is often used to describe clarifications; thus, it
should be taken into account when creating a di-
alogue scenario. In contrast, RepetitionRequest,
which is used to request that the missed portions
of the previous utterance be repeated, is not con-
cerned with the overall dialogue flow.
An example of an annotation is shown in Table
1. Since the Response and Constrain layers are not
necessarily directed to the immediately preceding
utterance, the target utterance ID is specified.
4.2 Evaluation
We performed a preliminary annotation of the
speech act tags in the corpus. Thirty dialogues
(900 min, 23,169 utterances) were annotated by
three labellers. When annotating the dialogues, we
took into account textual information, audio infor-
mation, and contextual information The result was
cross-checked by another labeller.
4.2.1 Distributional Statistics
The frequencies of the tags, expressed as a per-
centages, are shown in Table 3. In the General
layer, nearly half of the utterances were Statement.
This bias is acceptable because 66% of the utter-
ances had tag(s) of other layers.
The percentages of tags in the Constrain layer
are relatively higher than those of tags in the other
layers. They are also higher than the percentages
of the corresponding tags of MRDA (Shriberg
et al, 2004) and SWBD-DAMSL(Jurafsky et al,
1997).
These statistics characterize the consulting dia-
logue of sightseeing planning, where explanations
and evaluations play an important role during the
decision process.
4.2.2 Reliability
We investigated the reliability of the annotation.
Another two dialogues (2,087 utterances) were an-
notated by three labelers and the agreement among
them was examined. These results are listed in Ta-
ble 4. The agreement ratio is the average of all the
combinations of the three individual agreements.
In the same way, we also computed the average
Kappa statistic, which is often used to measure the
agreement by considering the chance rate.
A high concordance rate was obtained for the
General layer. When the specific layers and sub-
layers are taken into account, Kappa statistic was
35
Table 4: Agreement among labellers
General layer All layers
Agreement ratio 86.7% 74.2%
Kappa statistic 0.74 0.68
0.68, which is considered a good result for this
type of task. (cf. (Shriberg et al, 2004) etc.)
4.2.3 Analysis of Occurrence Tendency
during Progress of Episode
We then investigated the tendencies of tag occur-
rence through a dialogue to clarify how consult-
ing is conducted in the corpus. We annotated the
boundaries of episodes that determined the spots
to visit in order to carefully investigate the struc-
ture of the decision-making processes. In our cor-
pus, users were asked to write down their itinerary
for a practical one day tour. Thus, the beginning
and ending of an episode can be determined on the
basis of this itinerary.
As a result, we found 192 episodes. We selected
122 episodes that had more than 50 utterances,
and analyzed the tendency of tag occurrence. The
episodes were divided into five segments so that
each segment had an equal number of utterances.
The tendency of tag occurrence is shown in Figure
1. The relative occurrence rate denotes the number
of times the tags appeared in each segment divided
by the total number of occurrences throughout the
dialogues. We found three patterns in the tendency
of occurrence. The tags corresponding to the first
pattern frequently appear in the early part of an
episode; this typically applies to Open-Question,
WH-Question, and Wish. The tags of the sec-
ond pattern frequently appear in the later part, this
typically applies to Evaluation, Commitment, and
Opinion. The tags of the third pattern appear uni-
formly over an episode, e.g., Y/N-Question, Ac-
cept, and Elaboration. These statistics characterize
the dialogue flow of sightseeing planning, where
the guide and the user first clarify the latter?s in-
terests (Open, WH-Questions), list and evaluate
candidates (Evaluation), and then the user makes
a decision (Commitment).
This progression indicates that a session (or di-
alogue phase) management is required within an
episode to manage the consulting dialogue, al-
though the test-set perplexity2 , which was calcu-
2The perplexity was calculated by 10-fold cross validation
of the 30 dialogues.







    
	






	



	



	

	
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 70?75,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Construction of Chinese Segmented and POS-tagged  Conversational Corpora 
and Their Evaluations on Spontaneous Speech Recognitions 
 
Xinhui Hu, Ryosuke Isotani, Satoshi Nakamura 
National Institute of Information and Communications Technology, Japan 
{xinhui.hu, ryosuke.isotani, satoshi.nakamura}@nict.go.jp 
 
Abstract 
The performance of a corpus-based language and 
speech processing system depends heavily on the 
quantity and quality of the training corpora. Although 
several famous Chinese corpora have been developed, 
most of them are mainly written text. Even for some 
existing corpora that contain spoken data, the quantity 
is insufficient and the domain is limited. In this paper, 
we describe the development of Chinese conversational 
annotated textual corpora currently being used in the 
NICT/ATR speech-to-speech translation system. A total 
of 510K manually checked utterances provide 3.5M 
words of Chinese corpora. As far as we know, this is 
the largest conversational textual corpora in the 
domain of travel. A set of three parallel corpora is 
obtained with the corresponding pairs of Japanese and 
English words from which the Chinese words are 
translated. Evaluation experiments on these corpora 
were conducted by comparing the parameters of the 
language models, perplexities of test sets, and speech 
recognition performance with Japanese and English. 
The characteristics of the Chinese corpora, their 
limitations, and solutions to these limitations are 
analyzed and discussed.  
 
1. Introduction 
 
In corpus-based machine translation and speech 
recognition, the performance of the language model 
depends heavily on the size and quality of the corpora. 
Therefore, the corpora are indispensable for these 
studies and applications. In recent decades, corpus 
development has seen rapid growth for many 
languages such as English, Japanese, and Chinese. For 
Chinese, since there are no plain delimiters among the 
words, the creation of a segmented and part-of-speech 
(POS)-tagged corpus is the initial step for most 
statistical language processes. Several such Chinese 
corpora have been developed since the 1990s. The two 
most typical are People?s Daily corpus (referred to as 
PKU), jointly developed by the Institute of 
Computational Linguistics of Peking University and 
the Fujitsu Research & Development Center [1], and 
the Sinica Corpus (referred to as Sinica) developed by 
the Institute of Information Science and the CKIP 
Group in Academia Sinica of Taiwan [2]. The former 
is based on the People?s Daily newspaper in 1998. It!
uses standard articles of news reports. The latter is a 
balanced corpus collected from different areas and 
classified according to five criteria: genre, style, mode, 
topic, and source. Although conversational text is also 
contained in this corpus, it has only 75K of utterances 
and the domains are limited to a few fields, such as 
academia and economics, and the style is mostly in 
address and seldom in conversation. 
Since the features of conversation differ from written 
text, especially in news articles, the development of a 
segmented and POS-tagged corpus of conversational 
language is promising work for spontaneous speech 
recognition and speech-to-speech translation.  
In the Spoken Communication Group of NICT, in 
order to study corpus-based speech translation 
technologies for the real world, a set of corpora on 
travel conversation has been built for Japanese, 
English, and Chinese [3]. These corpora are elaborately 
designed and constructed on the basis of the concept of 
variety in samples, situations, and expressions. Now 
these corpora have been used in the NICT speech-to-
speech translation (S2ST) system [8] and other 
services. 
In this paper, we introduce our work on this Chinese 
corpora development and applications in S2ST speech 
recognition using these corpora. In Section 2, we 
provide a brief description of the contents of the NICT 
corpora, then describe how the Chinese data were 
obtained. In Section 3, we illustrate the specifications 
for the segmentation and POS tagging designed for 
these corpora. Here, we explain the guidelines of 
segmentation and POS tagging, placing particular 
emphasis on the features of conversation and speech 
recognition application. In Section 4, we outline the 
development procedures and explain our methods of 
how to get the segmented and POS-tagged data. Some 
statistical characteristics of the corpora will be shown 
here. In Section 5, evaluation experiments of speech 
recognition utilizing these corpora are reported by 
comparing the results using the same data sets of 
70
Japanese and English. Finally, in Section 6, we discuss 
the performance of the corpora, the problems that 
remain in the corpora, and give our ideas concerning 
future work. 
 
2. Current NICT Chinese Corpora on 
Travel Dialog Domain 
 
At NICT, in order to deal with various conversational 
cases of S2ST research, several kinds of corpora were 
elaborately designed and constructed [3]. Table 1 gives 
a brief description of the data sets related to the 
development of the Chinese corpora. Each corpus 
shown in this table was collected using different 
methods, for different application purposes, and was 
categorized into different domains. 
Table 1. NICT Corpora Used for Chinese Processing 
Name Collecting Method Uttr. Domain 
SLDB 
Bilingual conversation 
evolved by 
interpreters. 
16K 
Dialogues 
with the 
front desk 
clerk at a 
hotel 
MAD 
Bilingual conversation 
evolved by a machine 
translation system. 
19K 
General 
dialogues  
on travel 
BTEC Text in guidebooks for overseas travelers 475K 
General 
dialogues 
on travel 
 
The SLDB (Spoken Language Database) is a 
collection of transcriptions of spoken language 
between two people speaking different languages and 
mediated by a professional interpreter. 
In comparison, the MAD (Machine Translation Aid 
Dialogue) is a similar collection, but it uses our S2ST 
system instead of an interpreter. 
The BTEC (Basic Travel Expression Corpus) is a 
collection of Japanese sentences and their English 
translations written by bilingual travel experts. This 
corpus covers such topics related to travel as shopping, 
hotel or restaurant reservations, airports, lost and 
found, and so on. 
The original data of the above corpora were 
developed in the form of English-to-Japanese 
translation pairs. The Chinese versions are mainly 
translated from the Japanese, but a small portion of 
BTEC (namely, BTEC4, about 70K of utterances) was 
translated from English. Every sentence in these 
corpora has an equivalent in the other two languages, 
and they share a common header (ID), except for the 
language mark. All the data in these three languages 
constitute a set of parallel corpora. The following 
shows examples of sentences in the three languages: 
Chn.: BTEC1\jpn067\03870\zh\\\\??????? 
Eng.: BTEC1\jpn067\03870\en\\\\I'd like to have some strong 
coffee. 
Jap.:BTEC1\jpn067\03870\ja\\\\???????????? 
 
3. Specifications of Segmentation and Part-
of-Speech Tagging 
 
By mainly referring to the PKU and taking into 
account the characteristics of conversational data, we 
made our definitions for segmentation units and POS 
tags. Here, we explain the outlines of these definitions, 
then illustrate the segmentation and POS-tagging items 
relating to those considerations on conversations. 
 
3.1. Guidelines of the Definitions 
 
(1) Compatibility with the PKU and Taking into 
account the Demand of Speech Recognition of 
S2ST 
Since the specification of segmentations and POS-
tagging proposed by the PKU [4] has its palpability 
and maneuverability and is close to China?s national 
standard [5] on segmentation and close to the 
specification on POS tags recommended by the 
National Program of China, namely, the 973-project 
[6], we mainly followed PKU?s specification. We 
adopted the concept of ?segmentation unit,? i.e., words 
with disyllable, trisyllable, some compound words, and 
some phrases were regarded as segmentation units. The 
morpheme character (word) was also regarded as an 
independent unit. 
However, we made some adjustments to these 
specifications. In the speech recognition phase of S2ST 
to deal with data sparseness, the word for ?training? 
needed to be shortened. So a numeral  was divided into 
syllabic units, while both the PKU and the Sinica took 
the whole number as a unit. For the same reason, the 
directional verbs (????), such as ??, ????
????? and ?? ,? which generally follow 
another verb and express action directions, were 
divided from the preceding verb. The modal auxiliary 
verbs (????), such as ?????and ?,? which 
often precede another verb were separated and tagged 
with an individual tag set. Because the numeral can be 
easily reunited as an integrated unit, such a processing 
method for numerals does not harm the translation 
phase of S2ST. Moreover, if the directional verb and 
the modal auxiliary verb can be identified, they will 
help the syntactic analysis and improve the translation 
phrase. These two kinds of verbs, together with ?? 
(be)? and ??  (have)? are more frequently used in 
71
colloquial conversations than in written text, so we 
took them as an individual segmentation unit and 
assigned a POS tag to each. The special processes for 
these kinds of words aim at reflecting the features of 
spoken language and improve the performance of the 
S2ST system. 
 
(2) Ability for Future Expansion 
Although the corpora were developed for speech 
recognition in S2ST system, it is desirable that they 
can be used in other fields when necessary. This 
reflects in both segmentation and POS-tagging. In 
segmentation, the compound words with definitive 
suffix or prefix are divided, so they can be combined 
easily when necessary. In POS-tagging, the nouns and 
verbs are mainly further categorized into several sub-
tags. We selected about 40 POS tags for our corpora, 
as shown in Table 1 in the Appendix. With such scale 
of tag sets, it is regarded to be suitable for 
language model of ASR. When necessary, it is 
also easy to choice an adequate tag set from it to 
meet the needs of other tasks. 
 
(3) Relation with the Corpora of Other Languages 
in NICT 
The original data of the corpora are in Japanese or 
English. It is meaningful to build connections at the 
morphological level among these trilingual parallel 
corpora at least for ?named entities.? For example, we 
adopted the same segmentation units as in Japanese, 
and we subcategorized these words into personal 
names, organization names, location names, and drink 
and food names and assigned them each an individual 
tag. Personal names were further divided into family 
names and first names for Chinese, Japanese, and 
Western names. These subclasses are useful in 
language modeling, especially in the travel domain.  
 
3.2. Some Explanations on Segmentation and 
POS-tagging 
 
(1) About Segmentation 
In our definition of a segmentation unit, words longer 
than 4 Hanzis (Chinese characters) were generally 
divided into their syntactic units. Idioms and some 
greeting phrases were also regarded as segmentation 
units. For example: ???/?????/???/??
? /.? Semantic information was also used to judge 
segmentation unit. For example:  
 
? ?/ ?/ ?/ ?/ ?/ ?/ ??/ ?/ (Tell me the best 
restaurant around here.) 
? ??/ ?/ ??/ ??/ ?/ ?/ ??/ ?/ (I'd like a 
hotel that is not too expensive.) 
 For segmenting compound words with different 
structures, we constituted detailed items to deal with 
them. These structures include ?coordinated (??)?
modifying (?? ), verb-object (?? ), subject-
predicate (??), and verb-complement (??).? The 
main consideration for these was to divide them 
without changing the original meaning. For those 
words that have a strong ability to combine with 
others, we generally separated them from the others. 
This was due to the consideration that if it were done in 
another way, it would result in too many words. For 
example, in the verb-object (?? ) structure, ?? 
(buy)? can combine with many nouns to get 
meaningful words or phrases, such as ???  (buy 
book), ?? (buy meat)??? (buy ticket)?and ??
?  (buy clothes).? We prescribed separating such 
active characters or words, no matter how frequently 
they are used in the real world, to ensure that the 
meaning did not change and ambiguity did not arise. 
So the above phrases should be separated in following 
forms: ??/ ?/ (buy book), ?/ ?/ (buy meat)??/ ?/ 
(buy ticket)?and ?/ ?? /buy clothes).?  
 For the directional verbs, we generally separated 
them from their preceding verbs. For example: 
?/ ??/ ?/ ?/ ??/ ??/ ?/ ?/ (Is it all right to move 
to another seat?) 
?/ ?/ ?/ ?/ ???/ ??/ ?/ ???/ ?/ (Please keep 
this suitcase until one o'clock.) 
Prefix and appendix were commonly separated from 
the root words. For example: 
??/ ?/ ?/ ?/ ??/ ?/ ?/ (Are all students going to 
Kyoto?) 
?/ ?/ ??/ ??/ ?/. (I do free-lance work.) 
 
(2) About POS-Tagging 
The POS tag sets are shown in Table 1 in the Appendix. 
The POS tagging was conducted by the grammar 
function based on how the segmentation unit behaves 
in a sentence. 
 
4. Procedure of Developing the Chinese 
Corpora 
 
The segmented and POS-tagged data were obtained in 
two steps. The first step was to get the raw segmented 
and POS-tagged data automatically by computer. The 
second was to check the raw segmented and POS-
tagged data manually. 
 
(1) Getting Raw Segmented and POS-Tagged Data 
The text data were segmented and POS tagged by 
using the language model shown in formula (1). 
)|()|()1()ww|(w)( 2-i1-iiii2-i1-ii cccPcwPPLP ?? ?+=    (1) 
72
Here iw  denotes the word at the ith position of a 
sentence, and ic  stands for the class to which the word 
iw  belongs. The class we used here is a POS-tag set, 
and  ?  is set 0.9. 
The initial data for training the model were from the 
Sinica due to their balanced characteristics. The 
annotated data were added to the training data when 
producing new data. When the annotated data reached 
a given quantity (here, the BTEC1 was finished, and 
the total words in the corpora exceeded 1M), the Sinica 
data were not used for training. We have conducted an 
experiment with this model for an open test text of 510 
utterances from BTEC, and the segmentation and POS-
tagging accuracy was more than 95%. Furthermore, 
proper noun information was extracted from Japanese 
corpora and marked in the corresponding lines of the 
Chinese segmented and POS-tagged data. 
 
(2) Manual Annotation 
The manual annotations were divided into two phases. 
The first was a line-by-line check of the raw segmented 
and POS-tagged data. The second was to check the 
consistency. The consistency check was conducted in 
the following manner: 
? Find the candidates having differences between the 
manually checked data and the automatically 
segmented and POS-tagged data.  
? Pick up the candidates having a high frequency of 
updating in the above step, and build an 
inconsistency table. The candidates in this table are 
the main objects of the later checks.  
? Check the same sentences with different 
segmentations and POS tags. 
? List all words having multiple POS tags and their 
frequencies. Determine the infrequent ones as 
distrustful candidates and add them into the 
inconsistency tables. 
The released annotated data were appended with a 
header ID for each token (pair of word entry and POS 
tag) in an utterance including a start marker and end 
marker, shown as follows: 
BTEC1\jpn067\03870\zh\\\00010\||||UTT-START|||| 
BTEC1\jpn067\03870\zh\\\00020\?|?||?|r|||| 
BTEC1\jpn067\03870\zh\\\00030\?|?||?|vw|||| 
BTEC1\jpn067\03870\zh\\\00040\?|?||?|v|||| 
BTEC1\jpn067\03870\zh\\\00050\?|?||?|a|||| 
BTEC1\jpn067\03870\zh\\\00060\??|??||??|n|||| 
BTEC1\jpn067\03870\zh\\\00070\?||||UTT-END|||| 
Table 2 shows some of the statistics for the 510K 
utterances in Table 1 for different languages.  
 
Table 2. Some Statistics of Each Corpora in NICT 
 Utter.
Ave. 
words 
/Uttr. 
Words Vocab.
Chinese 510K 6.95 3.50M 47.3K 
Japanese 510K 8.60 4.30M 45.5K 
English 510K 7.74 3.80M 32.9K 
 
 Figure 1 shows the distributions of utterance length 
(words in an utterance) for 3 languages among the 
510K annotated data. From Figure 1, we know that the 
Chinese has the fewest words in an utterance, followed 
by English, with the Japanese having the most.  
 
 
Figure 1. Distribution of utterance length  
 
5. Evaluation Experiments 
 
To verify the effectiveness of the developed Chinese 
textual corpora, we built a language model for speech 
recognition using these corpora. For comparisons with 
other languages, including Japanese and English, we 
also built language models for these two languages 
using the same training sets. Meanwhile, the same test 
set of each language was selected for speech 
recognition.  
 
5.1. Data Sets for Language Models and 
Speech Recognitions  
 
For simplicity, we adopted word 2-gram and word 3-
gram for evaluating perplexities and speech 
recognition performance. The training data were 
selected from the 510K utterances in Table 1, while the 
test sets were also extracted from them, but they are 
guaranteed not to exist in the training sets. In 
evaluations of perplexity, 1524 utterances (a total of 
three sets) were chosen as the test set. In evaluation of 
recognition, 510 utterances were chosen as test set. For 
Japanese and English, the same data sets were also 
chosen for comparisons. 
Distribution of Utterance Length
0%
2%
4%
6%
8%
10%
12%
14%
16%
18%
2 4 6 8 10 12 14
   
   
  >
15
length(words)
Pe
rc
en
ta
ge
Japanese
English
Chinese
73
 
Figure 2. Ratio of 2-gram items with low occurrence 
 
5.2. Comparisons of Language Models 
 
Using the above utterances in the training sets, a word 
2-gram and a 3-gram were built respectively for each 
language. The distributions of items inside these 
models were investigated. Figure 2 shows the ratios of 
2-gram?s items which have low occurrences (from 1 to 
6) in the 2-gram model.  
Compared with the other two languages, the Chinese 
has the biggest vocabulary. Moreover, it also has a 
large amount of low-frequency 1-gram, 2-gram, and 3-
gram items. For example, more than 60% of its 2-gram 
entries appear only once. This can be regarded that the 
Chinese has more varieties when expressing a same 
meaning than the other two languages. It is also partly 
due to bias occurred in the translation process, 
compared to the original languages. So the probability 
computations in 2 or 3-gram related to these entries 
were estimated by using a smoothing approach, so the 
accuracy is not high.  
Table 3 shows average sentence entropies (ASE) of 
the test sets to the 3-gram models. The ASE is obtained 
as follows: (1) first to get the  product of average word 
entropy and the total word count in test set. (2) then 
divide the product by the total sentences in the test set.  
From the table, we know the Chinese has the maximal 
sentence entropy (or maximal perplexity) among the 
three languages. This means that when predicate a 
sentence in the recognition process, Chinese requires a 
much bigger search space than the other two languages.   
 
Table 3. Average Sentence Entropy of the Test Sets to 3-
gram Models 
 Chinese Japanese English
Vocab. of Test Set 10,030 12,344 10,840
Ave. Sen. Entropy  294.58 165.80 202.92
Word Perplexity 45.0 20.1 28.5
 
5.3. Comparison of Speech Recognition 
Performances 
 
 
Figure 3. Word recognition accuracies of 3 languages 
 
The 2-gram language model was used for decoding 
recognition lattice, while the 3-gram model was used  
for rescoring process. The recognition results are 
shown in  Figure 3. Here, WordID  refers to the word?s 
outer layer (entry) together with its POS tag, other 
information like conjugation of verbs, declension of 
nouns, etc., while the surface word contains only its 
outer layer, no POS tag is contained  in this case . 
The difference in word accuracy of speech 
recognition between these two forms is about 2% for 
Chinese, and 1% for English and Japanese. 
 
6. Summary 
 
This paper described the development of Chinese 
conversational segmented and POS-tagged corpora that 
are used for spontaneous speech recognition in S2ST 
system. While referring mainly to the PKU?s 
specifications, we defined ours by taking into account 
the needs of S2ST. About 510K utterances, or about 
3.5M words of conversational Chinese data, are 
contained in these corpora. As far as we know, they are 
presently the biggest ones in the domain of travel, with 
a style of conversations. Moreover, a parallel corpus 
was obtained using these 510K pairs of utterances of 
Chinese, Japanese, and English. These corpora now 
play a big role in spontaneous language and speech 
processing, and are used in the NICT/ATR Chinese-
Japanese-English Speech-to-Speech Translation 
System [8] and other communication services. 
However, according to our evaluations in this paper, 
there are still some difference in performance among 
Chinese and other languages, especially Japanese. 
There is still some room to improve the quality of these 
corpora mainly because the Chinese text data were 
translated from other languages, mainly Japanese, with 
a few words from English. There is some bias in 
expression, especially for the transliterations of proper 
nouns. For examples, ?Los Angles? is translated as ??
??,???,???, and ???.?also, some 
utterances are not like those spoken by native speakers, 
0
10
20
30
40
50
60
70
1 2 3 4 5 6
Chinese
Japanese
English
Ratio of 2-gram Item [%]
occurrence
81
87.96
93.87
82.89
89.29
94.67
70
75
80
85
90
95
100
Chinese English Japanese
W
or
d 
A
cc
ur
ac
y
Word Accuracy for Each Language
WordID
Surface Word
74
like sentence of ????????? ? which 
corresponds to the original sentence of  ??????
????(I appreciate your kindness).?    
For future work, while continuing to improve the 
consistency of the corpora, we will expand the Chinese 
corpora from external data resource, such as Web sites 
and LDC databases, to extract original Chinese 
spontaneous text data.  
 
7. References 
[1] H.M. Duan, J. Song, G.W. Xu, G.X. Hu and S.W. Yu, 
?The Development of a Large-scale Tagged Chinese 
Corpus and Its Applications.? http://icl.pku.edu.cn/icl_tr 
[2] C.R. Huang, and K.J. Chen, ?Introduction to Sinica 
Corpus,? CKIP Technical Report 95-02/98-04, 
http://www.sinica.edu.tw/SinicaCorpus 
[3] G. Kikui, E. Sumita, T. Takezawa, S. Yamamoto, 
?Creating Corpora for Speech-to-Speech Translation.? 8th 
European Conference on Speech Communication and 
Technology, Vol.1, pp.381-384, Sep., 2003 
[4] S.W. Yu, X.F. Zhu, and H.M. Duan, ?The Guideline for 
Segmentation and Part-Of-Speech Tagging on Very Large 
Scale Corpus of Contemporary Chinese.? 
http://icl.pku.edu.cn/icl_tr 
[5] The National Standard of PRC, ?Standardization of 
Segmentation for Contemporary Chinese.? GB13715, 
1992. 
[6] Institute of Applied Linguistics of the Ministry of 
Education, China, ?Specification on Part-of-Speech 
Tagging of Contemporary Chinese for Information 
Processing (Draft).? 2002. 
[7] H. Yamamoto, S. Isogai, and Y. Sagisaka, ?Multi-class 
Composite N-gram Language Model,? Speech 
Communication, 2003, Vol.41, pp369-379. 
[8] T. Shimizu, Y. Ashikari, E. Sumita, J.S. Zhang, S. 
Nakamura, ?NICT/ATR Chinese-Japanese-English 
Speech-to-Speech Translation System.? Tsinghua Science 
and Technology, Vol.13, No.4, pp540-544, Aug. 2008. 
 
Appendix Table 1. Chinese POS Tag Table 
 
POS Tag 
Description 
POS Tag 
Description 
Chinese English Chinese English 
a ??? Adjective 
n
nppx ????? Chinese  family name 
b ??? Non-predicate  adjective nppm ????? 
Chinese  
first name 
c ?? Conjunction nppxj ????? Japanese  family name 
d ?? Adverb nppmj ????? Japanese  first name 
de ???? Attributive nppxw ??????? Western  family name 
e ?? Interjection nppmw ??????? Western  first name 
g ??? Morpheme Word npl ?? Place 
h ??? Prefix npo ??? Organization 
i 
??, 
??? Idiom npfd ???? Drink and food 
j ??? Abbreviation o ??? Onomatopoeia 
k ??? Suffix p ?? Preposition 
m 
m ?? Numeral q ?? Quantifier 
ma ???? Numeral Classifier r ?? Pronoun 
mb ??? Approximate  numeral u ?? Auxiliary 
n 
n ???? Noun 
v
v ???? Verb 
nd ??? Directional locality v1 ??????? 
Auxiliary  
verb 
ns ??? Space word v2 ????? Verb ?Have? 
nt ??? Time word vt ???? Directional verb 
nx 
???, 
?? 
Numeric, 
character string vw ???? Modal verb 
np ???? Proper noun w ???? Punctuation 
npp ?? Personal name y ???? Modal particle 
75
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1124?1132, Dublin, Ireland, August 23-29 2014.
Discriminative Language Models as a Tool for
Machine Translation Error Analysis
Koichi Akabe Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
{akabe.koichi.zx8, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose a new method for effective error analysis of machine translation (MT)
systems. In previous work on error analysis of MT, error trends are often shown by frequency.
However, if we attempt to perform a more detailed analysis based on frequently erroneous word
strings, the word strings also often occur in correct translations, and analyzing these correct sen-
tences decreases the overall efficiency of error analysis. In this paper, we propose the use of
regularized discriminative language models (LMs) to allow for more focused MT error analysis.
In experiments, we demonstrate that our method is more efficient than frequency-based analysis,
and examine differences across systems, language pairs, and evaluation measures. 1
1 Introduction
Accuracy of Statistical Machine Translation (SMT) systems is continually increasing, but systems are
now more complex than ever before. As a result, not all effects of making modifications to a system are
known without actually making the modification and generating translations. Therefore, in the process
of developing an SMT system, it is common to evaluate actual translations to identify problems to make
improvements. This process is time consuming, as it is often necessary to analyze a large number of
translations to get an overall grasp of the system?s error trends. In addition, many sentences will contain
no errors, or only errors from the long tail that are not representative of the system as a whole. On the
other hand, if we are able to detect and rank important errors automatically, we will likely be able to find
representative errors of the SMT system more efficiently.
Previous work has proposed methods for automatic error analysis of MT systems based on automati-
cally separating errors into classes and sorting these classes by frequency (Vilar et al., 2006; Popovic and
Ney, 2011). These classes cover common mistakes of MT systems, e.g. conjugation, reordering, word
deletion, and insertion. This makes it possible to view overall error trends, but when the goal of analysis
is to identify errors to make some concrete improvement to the system, it is often necessary to perform a
more focused analysis, looking at actual errors made by a particular language pair or system. We show
examples of errors types that are informative, but are language- or task-specific, and not covered by pre-
vious methods in Figure 1. In this example, the type given by more standard error typologies is indicated
by ?Traditional type,? but we would prefer a more detailed analysis such as ?Fine-grained type,? would
allows us to take specific steps to fix the machine translation system (such as ensuring that Wikipedia
titles are not punctuated, or normalizing full-width characters to half-width). These fine-grained types
are difficult to conceive without actually observing the MT system output, but if we are able to group ac-
tual errors into fine-grained classes based on, for example, lexical clues, this sort of analysis will become
possible and more efficient.
Previous research on improving the efficiency of error analysis has generally focused on grouping error
types by frequency, but try to apply such frequency-based techniques to individual errors, selected errors
1Our implementation is available open-source at https://github.com/vbkaisetsu/dlm-analyzer
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1124
Src ?? ?? ??
Ref the academic exchange agreement
MT academic exchange agreement .
Traditional type Insertion error
Fine-grained type Insertion error (unneeded period)
Src ?? ? ?? ? ???? ? ???? ? ? ?? ?? ?
Ref prince kakugyoho -lrb- 1075 - 1105 -rrb- ninna-ji monzeki
MT imperial prince kakugyo -lrb-???? - 1105 -rrb- : ninna-ji temple ruins
Traditional type Replacement error or Unknown word
Fine-grained type Unknown word (number) or Half-/Full-width error
Figure 1: Example of errors in Japanese to English translation, classified into traditional, or more fine-
grained and useful classes.
1-gram 2-gram
the 61 (BOS) the 42
, 47 . (EOS) 41
and 43 , and 32
of 42 of the 27
: 42 in the 21
Table 1: Frequently occurring erroneous n-grams
are often dominated by frequently occurring linguistic phenomena that are not necessarily indicative
of translation errors. To show examples of this problem, in Table 1 we provide a list of erroneous n-
grams that were produced by an MT system (described in Section 4.1) but not contained in the respective
references. From this table, we can see that frequently occurring erroneous n-grams are simply n-grams
that frequently occur in English, and because of this we cannot discover characteristic errors of the system
for improvement just from this information.
In this paper, we propose a new method that uses regularized discriminative LMs to solve the above
problem. Discriminative LMs are LMs trained to fix common output errors of a particular system. From
the viewpoint of error analysis, if we train a discriminative LM using n-gram features and examine the
weights learned by this model, n-grams with large negative or positive weights will be indicative of pat-
terns that are over- or under-produced by the MT system. Because the weights are specifically trained to
fix errors, it is likely that these patterns will be more informative than mistakes that are simply frequently
occurring. We can also use a number of features of discriminative LMs to perform a more focused and
efficient analysis. For example, if we perform training with L1 regularization, many features will be
removed and only important patterns will remain in the model. Additionally, we can focus on specific
varieties of errors by changing the evaluation measure used for training the LMs.
In our experiments, we validate the effectiveness of error analysis based on discriminative LMs. We
perform a manual evaluation of the n-gram patterns discovered by random selection, by frequency-based
analysis, and by the proposed method. As a result, the proposed method is more effective at identifying
errors than other methods.
2 Discriminative Language Models
In this section, we first introduce the discriminative LM used in our method. As a target for our analysis,
we have input sentences F = {F
1
, . . . , F
K
}, n-best outputs ?E = { ?E
1
, . . . ,
?
E
K
} of an MT system, and
reference translations R = {R
1
, . . . , R
K
}. Discriminative LMs define feature vectors ?(E
i
) for each
candidate in ?E
k
= {E
1
, E
2
, . . . , E
I
}, and calculate inner products w ? ?(E
i
) as scores.
To train the weight vector w, we first calculate evaluation scores of all candidates using a sentence-
level evaluation measure EV such as BLEU+1 (Lin and Och, 2004) given the reference sentence R
k
.
1125
We choose the sentence with the highest evaluation EV as an oracle E?
k
. Oracles are chosen for each
n-best, and we train w so that the oracle?s score becomes higher than the other candidates.
2.1 Structured Perceptron
While there are a number of methods for training discriminative LMs, we follow Roark et al. (2007)
in using the structured perceptron as a simple and effective method for LM training. The structured
perceptron is a widely used on-line learning method that examines one training instance and updates
the weight vector using the difference between feature vectors generated from the oracle E? and the
hypothesis ?E calculated by the current model. For each iteration, w is updated using the difference
between E? and ?E. If ?E is equal to E?, the difference becomes 0, so no update is performed. This
process is run for all F sequentially, and iterated until weights converge or we reach a fixed iteration
limit N . We show the above procedure in Algorithm 1.
Algorithm 1 Structured perceptron training of the discriminative LM
for n = 1 to N do
for all ?E ? ?E do
E
?
? arg max
E?
?E
EV (E)
?
E ? arg max
E?
?E
w ? ?(E)
w ? w + ?(E
?
)? ?(
?
E)
end for
end for
2.2 Learning Sparse Discriminative LMs
While the structured perceptron is a simple and effective method for learning discriminative LMs, it also
has no bias towards reducing the number of features used in the model. However, if we add a bias towards
learning smaller models, we can keep only salient features (Tsuruoka et al., 2009).
In our work, we use L1 regularization to add this bias. L1 regularization gives a penalty to w pro-
portional to the L1 norm ?w?
1
=
?
i
|w
i
|, pushing a large number of elements in w to 0, so ineffective
features are removed from the model.
To train L1 regularized discriminative LMs, we use the forward-backward splitting (FOBOS) algo-
rithm proposed by Duchi and Singer (2009). FOBOS splits update and regularization, and lazily calcu-
lates the regularization upon using the weight to improve efficiency.
2.3 Features of Discriminative LMs
In the LM, we used the following three features:
1. System score feature ?
s
: As our goal is fixing the output of the system, we add this feature to allow
a default ordering of n-bests by score.
2. n-gram feature?
n
: We add a binary feature counting the frequency of eachn-gram in the hypothesis.
The weights of these features will be the main target of our analysis.
3. Hypothesis length feature ?
l
: If the evaluation measure has a penalty for the number of words, this
allows us to adjust it.
In this work, we do not use other features, but our method theoretically allows for addition of other
features such as POS tags or syntactic information, which could also potentially be used as a target for
analysis.
3 Discriminative LMs for Error Analysis
In this section, we describe how to incorporate information from discriminative LMs into manual error
analysis.
1126
Error types
Replacement (Context dependent)
(Context independent)
Insertion
Deletion
Reordering
Conjugation
Polarity
Unknown words
Table 2: Error categories for annotation
Src ? ??? ? ? ?? ?
Ref kyo-chan -lrb- city bus -rrb-
MT <s> kyoto chan -lrb- kyoto city bus -rrb- </s>
Rules SYMP ( x0:SYM SYMP ( NP ( NN ( ??? ) NN ( ???? ) ) x1:SYM ) )?
x0 ?kyoto? ?city? ?bus? x1
Eval Insertion error
Src ?? ?? ?? 13 ?
Ref there are 13 open patents .
MT <s> the number of public patent 13 cases </s>
Rules NP ( NP ( x0:NN x1:NN ) NN ( ???? ) )? ?number? ?of? x0 x1
NN ( ???? )? ?public?
Eval Context-dependent replacement error
Figure 2: Example of the evaluation sheet. Boxed words are chosen n-grams.
3.1 Focused Error Analysis of MT output
We first define the following general framework for focused analysis of errors in MT output. Using this,
we can find error trends of chosen n-grams:
1. Automatically choose potentially erroneous n-grams in the MT output.
2. Select one or more 1-best translations that contain each chosen n-gram.
3. Show selected translations to an annotator with the selected n-gram highlighted.
4. The annotator looks at the indicated n-gram, and marks whether or not by examining the n-gram
whether they were able to identify an error in the MT output. If the answer is ?yes,? the annotator
additionally indicates which variety of error was found according to Table 2.
A part of the actual evaluation sheet is shown in Fig. 2. The first four rows are the input, and the final
row is the annotator?s evaluation.
3.2 Selection of Target n-grams
We can think of the following three methods for choosing potentially erroneous n-grams:
Random: n-grams that are selected randomly. This corresponds to the standardmethod of error analysis,
where sentences are randomly sampled and analyzed.
1127
Sent Words
English Japanese
Train 330k 5.91M 6.09M
Dev 1166 24.3k 26.8k
Test 1160 26.7k 28.5k
Table 3: Data size of KFTT
Frequency: n-grams that are most frequently over-generated (occur in the hypothesis, but not in the
references). This corresponds to a focused version of the frequency-based automatic error analysis
methods of Vilar et al. (2006) and Popovic and Ney (2011).
LM: n-grams that have the lowest weight according to the discriminative LM. This is our proposed
method.
In particular, for discriminative LMs, n-gram features that have large positive or negative weights
indicate n-grams that are under-generated or over-generated by the system. Therefore, by examining
high-weighted or low-weighted n-grams, it is likely that we will be able to get a grasp of the system
mistakes. When performing actual evaluation, we want to analyze n-grams with 1-best translations.
Almost high-weighted n-grams are only contained in oracle translations, and not contained in 1-best
translation. Therefore, we use low-weighted n-grams for evaluation. If the discriminative LM is properly
trained, low-weighted n-grams will often correspond to actual errors.
3.3 System Comparison
When developing MT systems, it is common to not only evaluate a single system, but also compare
multiple systems, such as when comparing a new system with baselines.
To do this in the current work, we create discriminative LMs from n-bests generated by multiple
translation systems, and choose representative n-grams using the proposed method. Then we examine
the selected n-grams in context and then compare the result of this analysis.
4 Experiments
We evaluate the effectiveness of our method by performing a manual evaluation over three translation
systems, two translation directions, and two evaluation measures.
4.1 Experiment Setup
For each MT system, we use Japanese-English data from the KFTT (Neubig, 2011) as a corpus. The size
of the corpus is shown in Table 3. In our experiment, we use a forest-to-string (f2s) system trained using
the Travatar toolkit (Neubig, 2013) for single system evaluation. For system comparison, we compare the
above f2s system with a phrase based (pbmt) system and a hierarchical phrase based (hiero) system
built using Moses (Koehn et al., 2007).
The f2s system is built using Nile2 for making word alignments, and syntax trees generated with
Egret3. pbmt andhiero are built usingGIZA++ (Och andNey, 2003) for word alignments. Each system
is optimized using MERT (Och, 2003) with BLEU (Papineni et al., 2002) as an evaluation measure. For
single system evaluation, we also use the reordering-oriented evaluation metric RIBES (Isozaki et al.,
2010) as additional metric for training the discriminative LM.
For training discriminative LMs, our method uses the structured perceptron with 100 iterations and
FOBOS for L1 regularization as described in Section 2.2. The regularization factor is chosen from the
range 10?6-10?2 to give the highest performance on the KFTT test data.
LMs are trained using 500-bests from each MT system and features described in Section 2.3. We use
1-grams to 3-grams as n-gram features.
2http://code.google.com/p/nile/
3http://code.google.com/p/egret-parser/
1128
System BLEU(dev) BLEU(test)
Original LM applied Original LM applied
pbmt 0.2929 0.3521 0.2460 0.2485
hiero 0.2953 0.3859 0.2616 0.2562
f2s 0.2958 0.3887 0.2669 0.2676
Table 4:  Translation accuracy of each system, without LMs and with LMs
Method Ja? En En? Ja
Random 0.46 0.37
Frequency 0.30 0.31
LM 0.55 0.48
Table 5: Precision of top 30 n-grams that select errors in both directions
We show translation accuracies of each system before and after training in Table 4. From this table, we
can see that the LM increases the accuracy of all dev data, but it does not necessarily have a large effect
for the test data. The main reason for this is because the development set used to train the LM is relatively
small, at only 1166 sentences. However, as our goal in this paper is to perform error analysis on set of
data which we already have parallel references (in this case, the development set), the generalization
ability of the model is not necessarily fundamental to our task at hand. We directly identify the ability to
identify errors in the next section.
4.2 Evaluation of Error Identification Ability
This section evaluates the ability of our method to identify errors in MT output. As we are proposing
our method as a tool for manual analysis of MT output, it is necessary to perform manual evaluation to
ensure that our method is identifying locations that are actually erroneous according to human subjective
evaluation. To measure the accuracy of each method, we perform an evaluation as described in Section
3.1 and use the precision of selectedn-grams (the percentage of selectedn-grams for which then annotator
indicated that an error actually existed) as our evaluation measure. The annotator is an MT specialist who
is proficient in English and Japanese. The order of the evaluation sentences is shuffled so the annotator
can not determine which method was responsible for choosing each n-gram.
0 10 20 30 40 50 60 70 80 90 1000.0
0.5
1.0
# of selected n-grams
Precisi
on
FrequencyLMRandom
Figure 3: Precision of n-grams that select errors (Japanese to English)
We show the precision results for each number of selected n-grams over three methods for Japanese-
English translation in Fig. 3, and the precision of the top 30 n-grams in both directions in Table 5. From
1129
n-gram Weight Examples
-rrb- of -7.50950 Src ?? ?? ? ?? ? ? ? ? ? ? ??? ?? ? ? ? ?? ? ????
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ?
Ref his achievements were evaluated by emperor go-daigo , and he was awarded the
letter -lrb- ? -rrb- , which came from the emperor ?s real name takaharu -lrb-
?? -rrb- , so he changed the letter in his name from ??? ? to ??? ? .
MT <s> it is regarded as a valor in the fall of the bakufu , and was the first character of
takaharu , imina -lrb-???? -rrb- of emperor godaigo , and changed his name
to takauji . </s>
Eval Reordering error
<s> the first -6.55510 (Only contained in other candidates in n-bests)
senior -6.52024 Src ?? ? ? ?? ?? ? ?? ? ? ? ?? ?? ? ?? ?? ?? ? ?
?? ?
Ref kyoryukai-this organization consists of teachers of junior high , high , and other
schools who are ryukoku university graduates .
MT <s> graduates of?? association - ryukoku university , and is a organization con-
sisting of teachers such as senior . </s>
Eval Context independent replacement error
the ko clan -6.52021 Src ?? ? ? ? ? ? ?? ? ? ? ??? ? ?? ? ?? ? ? ? ? ?
? ? ?
Ref in this fighting , takewakamaru , the son of takauji ?s concubine , was killed .
MT <s> on this occasion , was killed during the confusion??? , the son of a concu-
bine of the ko clan . </s>
Eval Context dependent replacement error
foundation of -6.50773 Src ?? ?? ? ? ? ? ? ? ? ?? ?? ? ? ? ? ?? ? ? ? ?
? ? ? ?? ? ?? ?
Ref the family name comes from the fact that the kujo family lived in kujo-den , which
was located in kyoto kujo and said to have been built by fujiwara no mototsune .
MT <s> the origin of the family name that lived in kujo dono , which was located in
kyoto kujo is said to be a foundation of fujiwara no mototsune . </s>
Eval Context dependent replacement error
Table 6: Top 5 erroneous n-grams learned by the discriminative LM and examples. Boxes on MT indi-
cates the selected n-gram, and boxes in Src and Ref indicate the corresponding words.
these results, we can see that each method is able to detect erroneous n-grams, but the proposed method
achieves a precision that outperforms other methods.
To demonstrate why this is the case, in Table 6 we show examples, in context, of potentially erroneous
n-grams chosen by our proposed method. Compared to the baseline n-grams in Table 1, we can see that
these n-grams are not limited to frequently occurring n-grams in English, and are more likely to have a
high probability of indicating actual errors.
In addition, to give a better idea of the prominence of the selected n-grams, in Table 7, we show
the mean number of locations of the KFTT test data that contain the top 100 n-grams selected by each
method. We can see that randomly selected n-grams are rarely contained in the separate test set, while
the proposed method tends to select n-grams that are more frequent than random, and thus have a better
chance of generalizing.
4.3 Effect of Evaluation Measure Choice
We can also hypothesize that by varying the evaluation measure used in training the LM, we can select
different varieties of errors for analysis. To test this, we compare analysis results obtained using one
1130
Method Ja? En En? Ja
Random 1.1 1.5
Frequency 381.0 432.6
LM 6.2 14.0
Table 7: Mean number of occurrences of selected n-grams in the test set
Type +BLEU +RIBES
Actual Error 0.55 0.41
Replacement (Context dependent) 0.36 0.30
(Context independent) 0.15 0
Insertion 0.17 0.25
Deletion 0.18 0.10
Reordering 0.14 0.27
Conjugation 0 0.08
Polarity 0 0
Unknown words 0 0
Table 8: Error statistics found when optimizing different metrics. Bold indicates the higher score.
LM optimized with BLEU and another with RIBES, which is a reordering-oriented evaluation metric.
We show a breakdown of the identified errors in Table 8. From this table, we can see that the BLEU-
optimized LM is able to detect more deletion errors than the RIBES-optimized LM. This is a natural result,
as the BLEU metric puts a heavier weight on the brevity penalty assigned to shorter translations. On the
other hand, the RIBES-optimized LM detects more reordering errors than the BLEU-optimized LM. The
RIBES metric is sensitive to reordering errors, and thus reordering errors will cause larger decreases in
RIBES. From this experiment, we can see that it is possible to focus on different error types by using
different metrics in the optimization of the LM.
4.4 Result of System Comparison
Finally, we examine whether discriminative LMs allow us to grasp characteristic errors for system com-
parison. Similarly with single system analysis, we generated the top 30 potentially erroneous n-grams
for pbmt, hiero, and f2s in two directions, and evaluated them manually. The result is listed in Table
9. From this table, we can see that pbmt and hiero count reordering errors as one of the three most
frequent types, while f2s does not, especially for English to Japanese. This is consistent with common
knowledge that syntactic information can be used to improve reordering accuracy. We can also see in-
sertion is a problem when translating into English, and conjugation is a problem when translating into
morphologically-rich Japanese. While these are only general trends, they largely match with intuition,
even after analysis of only the top 30 n-grams.
5 Conclusion
In this paper, we proposed a new method for efficiently analyzing the output of MT systems using L1
regularized discriminative LMs, and evaluate its effectiveness. As a result, weights trained by discrim-
inative LMs are more effective at identifying errors than n-grams chosen either randomly or by error
frequency. This indicates that our method allows an MT system engineer to inspect fewer sentences in
the course of identifying characteristic errors of the MT system.
The overall framework of using discriminative LMs in error analysis opens up a number of directions
for future work, and there are a number of additional points we plan to analyze in the future. For example,
while it is clear that the proposedmethod allows errors to be identifiedmore efficiently, it is still necessary
to quantify the overall benefit of having an MT expert use the result of this error analysis to improve
1131
Type Ja? En En? Ja
pbmt hiero f2s pbmt hiero f2s
Actual Error 0.58 0.60 0.55 0.81 0.64 0.48
Replacement (Context dependent) 0.41 0.33 0.36 0.10 0.17 0.52
(Context independent) 0.03 0.08 0.15 0.55 0.03 0.12
Insertion 0.26 0.22 0.17 0.06 0.13 0.15
Deletion 0.10 0.09 0.18 0.07 0.14 0.06
Reordering 0.13 0.28 0.14 0.19 0.32 0.04
Conjugation 0.07 0 0 0.04 0.20 0.12
Polarity 0 0 0 0 0.01 0
Unknown words 0 0 0 0 0 0
Table 9: Error statistics of three systems with in both directions. Bold scores are the top 3 most occuring
error types in each system.
an MT system. In addition, we plan on examining the effect of using larger training data for the LM,
incorporating different features based on POS patterns or syntactic features, and using more sophisticated
training methods.
References
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. In
Journal of Machine Learning Research, volume 10.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation
of translation quality for distant language pairs. In Proc. EMNLP, pages 944?952.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177?180.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for
machine translation. In Proc. COLING, pages 501?507.
Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.
Graham Neubig. 2013. Travatar: A forest-to-string machine translation engine based on tree transducers. In Proc.
ACL.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation
of machine translation. In Proc. ACL, pages 311?318.
Maja Popovic and Hermann Ney. 2011. Towards automatic error analysis of machine translation output. In
Computational Linguistics, pages 657?688.
Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer
Speech & Language, 21(2):373?392.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-
regularized log-linear models with cumulative penalty. In Proc. ACL, pages 477?485.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney. 2006. Error analysis of statistical machine trans-
lation output. In Proc. LREC, pages 697?702.
1132
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1706?1717, Dublin, Ireland, August 23-29 2014.
Reinforcement Learning of Cooperative Persuasive Dialogue Policies
using Framing
Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
Nara Institute of Science and Technology (NAIST), Nara, Japan
{takuya-h,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
Abstract
In this paper, we apply reinforcement learning for automatically learning cooperative persuasive
dialogue system policies using framing, the use of emotionally charged statements common in
persuasive dialogue between humans. In order to apply reinforcement learning, we describe a
method to construct user simulators and reward functions specifically tailored to persuasive dia-
logue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate
the learned policy and the effect of framing through experiments both with a user simulator and
with real users. The experimental evaluation indicates that applying reinforcement learning is
effective for construction of cooperative persuasive dialogue systems which use framing.
1 Introduction
With the basic technology supporting dialogue systems maturing, there has been more interest in recent
years about dialogue systems that move beyond the traditional task-based or chatter bot frameworks. In
particular there has been increasing interest in dialogue systems that engage in persuasion or negotiation
(Georgila and Traum, 2011; Georgila, 2013; Paruchuri et al., 2009; Heeman, 2009; Mazzotta and de
Rosis, 2006; Mazzotta et al., 2007; Nguyen et al., 2007; Guerini et al., 2003). We concern ourselves
with cooperative persuasive dialogue systems (Hiraoka et al., 2013), which try to satisfy both the user
and system goals. For these types of systems, creating a system policy that both has persuasive power
and is able to ensure that the user is satisfied is the key to the system?s success.
In recent years, reinforcement learning has gained much attention in the dialogue research community
as an approach for automatically learning optimal dialogue policies. The most popular framework for
reinforcement learning in dialogue models is based on Markov decision processes (MDP) and partially
observable Markov decision processes (POMDP). In these frameworks, the system gets a reward repre-
senting the degree of success of the dialogue. Reinforcement learning enables the system to learn a policy
maximizing the reward. Traditional reinforcement learning requires thousands of dialogues, which are
difficult to collect with real users. Therefore, a user simulator which simulates the behavior of real users
is used for generating training dialogues. Most research in reinforcement learning for dialogue system
policies has been done in slot-filling dialogue, where the system elicits information required to provide
appropriate services for the user (Levin et al., 2000; Williams and Young, 2007).
There is also ongoing research on applying reinforcement learning to persuasion and negotiation
dialogues, which are different from slot-filling dialogue (Georgila and Traum, 2011; Georgila, 2013;
Paruchuri et al., 2009; Heeman, 2009). In slot-filling dialogue, the system is required to perform the
dialogue to achieve the user goal, eliciting some information from a user to provide an appropriate ser-
vice. A reward corresponding to the achievement of the user?s goal is given to the system. In contrast,
in persuasive dialogue, the system convinces the user to take some action achieving the system goal.
Thus, in this setting, reward corresponding to the achievement of both the user?s and the system?s goal is
given to the system. The importance of each goal will vary depending on the use case of the system. For
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1706
example, a selfish system could be rewarded with an emphasis on only achievement of the system goal,
and a cooperative system could be rewarded with an emphasis on achievement of both of the goals. In
addition, negotiation dialogue could be considered as a kind of the persuasive dialogue where the user
also tries to convince the system to achieve the user?s goal.
In this paper, our research purpose is learning better policies for cooperative persuasive dialogue sys-
tems using framing. We focus on learning a policy that tries to satisfy both the user and system goals. In
particular, two elements in this work set it apart from previous works:
? We introduce framing (Irwin et al., 2013), which is known to be important for persuasion and a
key concept of this paper, as a system action. Framing uses emotionally charged words to explain
particular alternatives. In the context of research that applies reinforcement learning to persuasive
(or negotiation) dialogue, this is the first work that considers framing as a system action.
? We use a human-to-human persuasive dialogue corpus of Hiraoka et al. (2014) to train predictive
models for achievement of a human persuadee?s and a human persuader?s goals, and introduce these
models to reward calculation to enable the system to learn a policy reflecting knowledge of human
persuasion.
To achieve our research purpose, we construct a POMDP where the reward function and user simulator
are learned from a corpus of human persuasive dialogue. We define system actions based on framing and
general dialogue acts. In addition, the system dialogue state (namely, belief state) is defined for tracking
the system?s rewards. Then, we evaluate the effect of framing and learning a system policy. Experimental
evaluation is done through a user simulator and real users.
2 Reinforcement learning
Reinforcement learning is a machine learning technique for learning a system policy. The policy is a
mapping function from a dialogue state to a particular system action. In reinforcement learning, the
policy is learned by maximizing the reward function. Reinforcement learning is often applied to models
based on the framework of MDP or POMDP.
In this paper, we follow a POMDP-based approach. A POMDP is defined as a tuple
?S,A, P,R,O,Z, ?, b
0
? where S is the set of states (representing different contexts) which the system
may be in (the system?s world), A is the set of actions of the system, P : S ?A ? P (S,A) is the set of
transition probabilities between states after taking an action, R : S?A ? ? is the reward function, O is
a set of observations that the system can receive about the world, Z is a set of observation probabilities
Z : S ? A ? Z(S,A), and ? a discount factor weighting longterm rewards. At any given time step i
the world is in some unobserved state s
i
? S. Because s
i
is not known exactly, we keep a distribution
over states called a belief state b, thus b(s
i
) is the probability of being in state s
i
, with initial belief state
b
0
. When the system performs an action ?
i
? A based on b, following a policy pi : S ? A, it receives
a reward r
i
(s
i
, ?
i
) ? ? and transitions to state s
i+1
according to P (s
i+1
|s
i
, ?
i
) ? P . The system then
receives an observation o
i+1
according to P (o
i+1
|s
i+1
, ?
i
). The quality of the policy pi followed by the
agent is measured by the expected future reward also called Q-function, Q
pi
: S ?A ? ?.
In this framework, it is critical to be able to learn a good policy function. In order to do so, we use
Neural fitted Q Iteration (Riedmiller, 2005) for learning the system policy. Neural fitted Q Iteration is an
offline value-based method, and optimizes the parameters to approximate the Q-function. Neural fitted
Q Iteration repeatedly performs 1) sampling training experience using a POMDP through interaction and
2) training a Q-function approximator using training experience. Neural fitted Q Iteration uses a multi-
layered perceptron as the Q-function approximator. Thus, even if the Q-function is complex, Neural
fitted Q Iteration can approximate the Q-function better than using a linear approximation function
1
.
3 Persuasive dialogue corpus
In this section, we give a brief overview of Hiraoka et al. (2014)?s persuasive dialogue corpus between
human participants that we will use to estimate the models described in later sections.
1
In a preliminary experiment, we found that Neural fitted Q Iteration had high performance compared to using the linear
approximation of the Q-function in this domain.
1707
Table 1: The beginning of a dialogue from the cor-
pus (translated from Japanese)
Speaker Transcription GPF Tag
Cust Well, I am looking for a camera, PROPQ
do you have camera B?
Sales Yes, we have camera B. ANSWER
Sales Did you already take a look at
it somewhere? PROPQ
Cust Yes. On the Internet. ANSWER
Sales It is very nice. Don?t you think? PROPQ
Cust Yes, that?s right, yes. INFORM
Table 2: Sytem and user?s GPF tags
Inform Answer Question PropQ
SetQ Commisive Directive
Table 3: An example of positive framing
(Camera A is) able to achieve performance
of comparable single-lens cameras
and can fit in your pocket, this is a point.
3.1 Outline of persuasive dialogue corpus
As a typical example of persuasive dialogue, the corpus consists of dialogues between a salesperson
(persuader) and customer (persuadee). The salesperson attempts to convince the customer to purchase a
particular product (decision) from a number of alternatives (decision candidates). This type of dialogue
is defined as ?sales dialogue.? More concretely, the corpus assumes a situation where the customer is in
an appliance store looking for a camera, and the customer must decide which camera to purchase from 5
alternatives.
Prior to recording, the salesperson is given the description of the 5 cameras and instructed to try to
convince the customer to purchase a specific camera (the persuasive target). In this corpus, the persuasive
target is camera A, and this persuasive target is invariant over all subjects. The customer is also instructed
to select one preferred camera from the catalog of the cameras, and choose one aspect of the camera that
is particularly important in making their decision (the determinant). During recording, the customer and
the salesperson converse and refer to the information in the camera catalog as support for their dialogues.
The customer can close the dialogue whenever they want, and choose to buy a camera, not buy a camera,
or reserve their decision for a later date.
The corpus includes a role-playing dialogue with participants consisting of 3 salespeople from 30 to
40 years of age and 19 customers from 20 to 40 years of age. All salespeople have experience working
in an appliance store. The total number of dialogues is 34, and the total time is about 340 minutes. Table
1 show an example transcript of the beginning of one dialogue. Further examples are shown in Table 8
in the appendix.
3.2 Annotated dialogue acts
Each utterance is annotated with two varieties of tags, the first covering dialogue acts in general, and the
rest covering framing.
As a tag set to represent traditional dialogue acts, we use the general-purpose functions (GPF) defined
by the ISO international standard for dialogue act annotation (ISO24617-2, 2010). All annotated GPF
tags are defined to be one of the tags in this set (Table 2).
More relevant to this work is the framing annotation. Framing uses emotionally charged words to
explain particular alternatives. It has been suggested that humans generally evaluate decision candidates
by selecting based on several determinants weighted by the user?s preference, and that framing is an
effective way of increasing persuasive power. This corpus focuses on negative/positive framing (Irwin
et al., 2013; Mazzotta and de Rosis, 2006), with negative framing using negative words and positive
framing using positive words.
In the corpus, framing is defined as a tuple ?a, p, r? where a represents the target alternative, p takes
value NEG if the framing is negative, and POS if the framing is positive, and r represents whether the
framings contains a reference to the persuadees preferred determinant (for example, the performance or
price of a camera), taking the value TRUE if contained, and FALSE if not contained. The user?s preferred
determinant is annotated based on the results of a questionnaire.
Table 3 shows an example of positive framing (p=POS) about the performance of Camera A (a=A). In
this example, the customer answered that his preference is the price of camera, and this utterance does
1708
Figure 1: Dynamic Bayesian network of the user simulator. Each node represents a variable, and each
edge represents a probabilistic dependency. The system cannot observe the shaded variables.
not contain any description of price. Thus, r=NO is annotated. Further examples of positive and negative
framing are shown in Tables 9 and 10 in the appendix.
In this paper, we re-perform annotation of the framing tags and evaluate inter-annotator agreement,
which is slightly improved from Hiraoka et al. (2014). Two annotators are given the description and
examples of tags (e.g. what a positive word is), and practice with these manuscripts prior to annotation.
In corpus annotation, at first, each annotator independently chooses the framing sentences. Then, framing
tags are independently annotated to all utterances chosen by the two annotators. The inter-annotator
agreement of framing polarity is 96.9% (kappa=0.903).
4 User simulator
In this section, we describe a statistical dialogue model for the user (customer in Section 3). This model
is used to simulate the system?s conversational partner in applying reinforcement learning.
The user simulator estimates two aspects of the conversation:
1. The user?s general dialogue act.
2. Whether the preferred determinant has been conveyed to the user (conveyed preferred determinant;
CPD).
The users? general dialogue act is represented by using GPF. For example, in Table 1, PROPQ, ANSWER,
and INFORM appear as the user?s dialogue act. In our research, the user simulator chooses one GPF
described in Table 2 or None representing no response at each turn. CPD represents that the user
has been convinced that the determinant in the persuader?s framing satisfies the user?s preference. For
example, in Table 3, the ?performance? is contained in the clerk?s positive framing for camera A. If the
persuadee is convinced that the decision candidate satisfies his/her preference based on this framing,
we say that CPD has occurred (r=YES)
2
. In our research, the user simulator models CPD for each of
the 5 cameras. This information is required to calculate reward described in the following Section 5.1.
Specifically, GPF and CPD are used for calculating naturalness and persuasion success, which are part
of the reward function.
The user simulator is based on an order one Markov chain, and Figure 1 shows its dynamic Bayesian
network. The user?s GPF G
t+1
user
and CPD C
t+1
alt
at turn t + 1 are calculated by the following equations.
P (G
t+1
user
|G
t
user
, F
t
sys
, G
t
sys
, S
alt
) (1)
P (C
t+1
alt
|C
t
alt
, F
t
sys
, G
t
sys
, S
alt
) (2)
G
t
sys
represents the system GPF at time t. F
t
sys
represents the system framing at t. These two variables
correspond to system actions, and are explained in Section 5.2. G
t
user
represents the user?s GPF at t.
C
t
alt
represents the CPD at t. S
alt
represents the users?s original evaluation of the alternatives. In our
2
Note that the persuader does not necessarily know if r=YES because the persuader is not certain of the user?s preferred
determinants.
1709
research, this is the camera that the user selected as a preferred camera at the beginning of the dialogue
3
.
We use the persuasive dialogue corpus described in Section 3 for training the user simulator, considering
the customer in the corpus as the user and the salesperson in the corpus as the system. In addition, we
use logistic regression for learning Equations (1) and (2).
5 Learning cooperative persuasion policies
Now that we have introduced the user model, we describe the system?s dialogue management. In par-
ticular, we describe the reward, system action, and belief state, which are required for reinforcement
learning.
5.1 Reward
We follow Hiraoka et al. (2014) in defining a reward function according to three factors: user satisfac-
tion, system persuasion success, and naturalness. As described in Section 1, we focus on developing
cooperative persuasive dialogue systems. Therefore, the system must perform dialogue to achieve both
the system and user goals. In our research, we define three elements of the reward function as follows:
Satisfaction The user?s goal is represented by subjective user satisfaction. The reason why we use
satisfaction is that the user?s goal is not necessarily clear for the system (and system creator) in
persuasive dialogue. For example, some users may want the system to recommend appropriate
alternatives, while some users may want the system not to recommend, but only give information
upon the user?s request. As the goal is different for each user, we use abstract satisfaction as a
measure, and leave it to each user how to evaluate achievement of the goal.
Persuasive success The system goal is represented by persuasion success. Persuasion success represents
whether the persuadee finally chooses the persuasive target (in this paper, camera A) at the end of
the dialogue. Persuasion success takes the value SUCCESS when the customer decides to purchase
the persuasive target at the end of dialogue, and FAILURE otherwise.
Naturalness In addition, we use naturalness as one of the rewards. This factor is known to enhance the
learned policy performance for real users (Meguro et al., 2011).
The reward at each turn t is calculated with the following equation
4
.
r
t
= (Sat
t
user
+ PS
t
sys
+ N
t
)/3 (3)
Sat
t
user
represents a 5 level score of the user?s subjective satisfaction (1: Not satisfied?3: Neutral?
5: Satisfied) at turn t scaled into the range between 0 and 1. PS
t
sys
represents persuasion success (1:
SUCCESS?0: FAILURE) at turn t. N
t
represents bi-gram likelihood of the dialogue between system and
user at turn t as follows.
N
t
= P (F
t
sys
, G
t
sys
, G
t
user
|F
t?1
sys
, G
t?1
sys
, G
t?1
user
) (4)
In our research, Sat and PS are calculated with a predictive model constructed from the human per-
suasion dialogue corpus described in Section 3. In constructing these predictive models, the persuasion
results (i.e. persuasion success and persuadee?s satisfaction) at the end of dialogue are given as the su-
pervisory signal, and the dialogue features in Table 4 are given as the input. In the reward calculation,
the dialogue features used by the predictive model are calculated by information generated from the dia-
logue of the user simulator and the system. Table 4 shows all features used for reward calculation at each
turn
5
. Note that, for the calculating TOTAL TIME, average speaking time corresponding to speakers and
dialogue acts is added at each turn.
3
Preliminary experiments indicated that the user behaved differently depending on the first selection of the camera, thus we
introduce this variable to the user simulator.
4
We also optimized the policy in the case where the reward (Equation (3)) is given only when dialogue is closed. However,
the convergence of the learning was much longer, and the performance was relatively bad.
5
Originally, there are more dialogue features for the predictive model. However as in previous research, we choose signifi-
cant dialogue features by step-wise feature selection (Terrell and Bilge, 2012).
1710
Table 4: Features for calculating reward. These
features are also used as the system belief state.
Sat
user
Frequency of system commisive
Frequency of system question
PS
sys
Total time
C
alt
(for each 6 cameras)
S
alt
(for each 6 cameras)
N System and user current GPF
System and user previous GPF
System framing
Table 5: System framing. Pos represents positive
framing and Neg represents negative framing. A, B,
C, D, E represent camera names.
Pos A Pos B Pos C Pos D Pos E None
Neg A Neg B Neg C Neg D Neg E
Table 6: System action.
<None, ReleaseTurn> <None, CloseDialogue>
<Pos A, Inform> <Pos A, Answer>
<Neg A, Inform> <Pos B, Inform>
<Pos B, Answer> <Pos E, Inform>
<None, Inform> <None, Answer>
<None, Question> <None, Commissive>
<None, Directive>
5.2 Action
The system?s action ?F
sys
, G
sys
? is a framing/GPF pair. These pairs represent the dialogue act of the
salesperson, and are required for reward calculation (Section 5.1). There are 11 types of framing (Table
5), and 9 types of GPF which are expanded by adding RELEASETURN and CLOSEDIALOGUE to the
original GPF sets (Table 2). The number of all possible GPF/framing pairs is 99, and some pairs have not
appeared in the original corpus. Therefore, we reduce the number of actions by filtering. We construct
a unigram model of the salesperson?s dialogue acts P (F
sales
, G
sales
) from the original corpus, then
exclude pairs for which the likelihood is below 0.005
6
. As a result, the 13 pairs shown in Table 6
remained
7
. We use these pairs as the system actions.
5.3 Belief state
The current system belief state is represented by the features used for reward calculation (Table 4) and
the reward calculated at previous turn. Namely, the features for the reward calculation and calculated
reward are also used as the next input of the system policy. Note that the system cannot directly observe
C
alt
, thus the system estimates it through the dialogue by using the following equation.
P (
?
C
t+1
alt
|
?
C
t
alt
, F
t
sys
, G
t
sys
, S
alt
) (5)
where
?
C
t+1
alt
represents the estimated CPD at t + 1.
?
C
t
alt
represents the estimated CPD at t. The other
variables are the same as those in Equation (2). In contrast, we assume that the system can observe
G
user
and S
alt
. G
user
is not usually observable because traditional dialogue systems have automatic
speech recognition/Spoken language understanding errors. However, in this work, we use Wizard of Oz
in place of automatic speech recognition/Spoken language understanding (Section 6.2). Thus, we can
ignore these factors
8
.
6 Experimental evaluation
In this section, we describe the evaluation of the proposed method for learning cooperative persuasive
dialogue policies. Especially, we focus on examining how the learned policy with framing is effective
for persuasive dialogue. The evaluation is done both using a user simulator and real users.
6
We chose this threshold by trying values from 0.001 to 0.01 with incrementation of 0.001. We select the threshold that
resulted in the number of actions closest to previous work (Georgila, 2013).
7
Cameras C and D are not popular, and don?t appear frequently in the human persuasive dialogue corpus, and are therefore
excluded in filtering.
8
In addition to this reason, the G
user
is not so essential to our research (GPF is general dialogue act), and we want to focus
the CPD. This is the other reason that we assume that G
user
is observable.
1711
Figure 2: Average reward of each system. Error bars represents 95% confidence intervals. Rew repre-
sents the reward, Sat represents the user satisfaction, PS represents persuasion success, and Nat represents
naturalness.
6.1 Policy learning and evaluation using the user simulator
For evaluating the effectiveness of framing and learning the policy through the user simulator, we prepare
the following 3 policies.
Random A baseline where the action is randomly output from all possible actions.
NoFraming A baseline where the action is output based on the policy which is learned using only
GPFs. For constructing the actions, we remove actions whose framing is not None from the actions
described in Section 5.2. The policy is a greedy policy, and selects the action with the highest Q-
value.
Framing The proposed method where the action is output based on the policy learned with all actions
described in Section 5.2 including framing. The policy is also a greedy policy.
For learning the policy, we use Neural fitted Q Iteration (Section 2). For applying Neural fitted Q
Iteration, we use the Pybrain library (Schaul et al., 2010). We set the discount factor ? of learning to 0.9,
and the number of nodes in the hidden layer of the neural network for approximating the Q-function to
the sum of number of belief states and actions (i.e. Framing: 53, NoFraming: 47). The policy in learning
is the ?-greedy policy (? = 0.3). These conditions follow the default Pybrain settings. We consider 50
dialogues as one epoch, and update the parameters of the neural network at each epoch. Learning is
finished when number of epochs reaches 200 (10000 dialogues), and the policy with the highest average
reward is used for evaluation.
We evaluate the system on the basis of average reward per dialogue with the user simulator. For
calculating average reward, 1000 dialogues are performed with each policy.
Experimental results (Figure 2) indicate that 1) performance is greatly improved by learning and 2)
framing is somewhat effective for the user simulator. Learned policies (Framing, NoFraming) get a
higher reward than Random. Particularly, both of the learned policies better achieve user satisfaction than
Random. On the other hand, only Framing is able to achieve better persuasion success than Random.
This result indicates that framing is effective for persuasive success. In contrast, naturalness of Framing
is not improved from Random. One of the reasons for this is that variance of Nat is smaller than those
of the other factors, and the optimization algorithm favored the other two factors which had a higher
variance.
6.2 Real user evaluation based on Wizard of Oz
To test whether the gains shown on the user simulator will carry over to an actual dialogue scenario, we
perform an experiment with real human users. In addition to the policies described in Section 6.1, we
add the following policy.
Human An oracle where the action is output based on human selection. In this research, the first author
(who has no formal sales experience, but experience of about 1 year in analysis of camera sales
dialogue) selects the action.
1712
Figure 3: The experimental environment based on Wizard of Oz. The rectangle represents information,
and the cylinder represents a system module. The information flow (dashed line) in the experiment
through the user simulator is also shown for comparison.
Experimental evaluation is conducted, based on the Wizard of Oz framework. In the experiment, the
wizard plays the salesperson, and the evaluator plays the customer. Dialogue is performed between the
wizard and the evaluator. The wizard and evaluator are divided by a partition, and the evaluator cannot
see or detect what the wizard is doing. The evaluator selects his/her preferred camera from the catalog
before starting evaluation. Then, the evaluator starts the dialogue with the wizard who is obeying one
of the policies (Figure 3). In particular, dialogue between wizard and evaluator proceeds based on the
following steps.
1. The evaluator talks to the wizard using the mic. In this step, the evaluators can close the dialogue if
they want.
2. The wizard listens to the evaluator?s utterance, translating the utterance into the appropriate G
user
.
Then, the wizard inputs G
user
to the policy module.
3. The policy module decides action sequences (F
sys
, G
sys
) based on G
user
, then outputs the action to
the utterance database module. This module is constructed from the camera sales corpus (Section
3).
4. The utterance database module searches for similar sentences that match the history of input actions
and G
user
so far, then outputs the top 6 similar utterances to the wizard.
5. The wizard generates the system utterance (Text) using the retrieved sentences. The wizard selects
one sentence which best matches the context
9
. If the wizard determines the sentence is hard to
understand, the wizard can correct the sentence to be more natural.
6. The wizard inputs the system utterance to text-to-speech, then waits for the next evaluator utterance
(back to step 1).
Finally, the evaluator answers the following questionnaire for calculating the evaluation measures in
Section 5.1.
Satisfaction The evaluator?s subjective satisfaction defined as a 5 level score of customer satisfaction
(1: Not satisfied?3: Neutral?5: Satisfied).
Final decision The camera that the customer finally wants to buy.
We use SofTalk (cncc, 2010) as text-to-speech software.
Evaluation criteria are basically same to those of previous section (described in Section 5.1). Note
that in the previous section, Sat
user
and PS
sys
are estimated from the simulated dialogue. In contrast
to the previous section, Sat
user
and PS
sys
are calculated from the result of the real user?s questionnaire
9
Note that the wizard is not allowed to create the utterance with complete freedom, and selects an utterance from the
utterance database even when Human policy is used.
1713
Figure 4: Evaluation results for real users. Error bars represent 95% confidence intervals. Rew represents
the reward, Sat represents the user satisfaction, PS represents persuasion success, and Nat represents
naturalness.
Table 7: Part of a dialogue between Framing and an evaluator (translated from Japanese)
Speaker Transcription Fra GPF
Wiz Which pictures do you want to take? Far or near? None QUESTION
Wiz Camera B has 20x zoom, and this is good. Pos B ANSWER
Wiz How about it? RELEASET
Eva I think B sounds good. ANSWER
Wiz Yes, B is popular with zoom, Pos B INFORM
Wiz But, A has extremely good performance.
Camera A has almost the same parts as a single lens camera,
and is more reasonably priced than a single lens-camera. Pos A ANSWER
Wiz How about it? RELEASET
(described in the previous paragraph)
10
based on the definition of Sat
user
and Sat
user
in Section 6.1. The
naturalness is automatically calculated by the system, in the same manner as described in the previous
section. Finally, reward is calculated considering Sat
user
, PS
sys
and naturalness according to Equation
3.
Participants consist of 13 evaluators (3 female, 10 male) and one wizard. Evaluators perform one
dialogue with the wizard obeying each policy (a total of 4 dialogues) in random order.
Experimental results (Figure 4) indicate that framing is effective in persuasive dialogues with real
users, and that the reward of Framing is higher than NoFraming and Random, and almost equal to
Human. In addition, the score of NoFraming is almost equal to Random. This indicates that despite the
fact that it performed relatively well in the simulation experiment, NoFraming is not an effective policy
for real users. In addition, the score of NoFraming is lower than the score given by the user simulator.
In particular, persuasion success is drastically decreased. This indicates that framing is important for
persuasion.
We can see that some features in human persuasive dialogue appear in the dialogue between users
and the wizard using the Framing policy. An example of a typical dialogue of Framing is shown in
Table 7. The first feature is that the system also recommends camera B when the system does positive
framing of camera A, which is the persuasive target. This feature was found by Hiraoka et al. (2014) to
be an indicator of persuasion success in the camera sales corpus. The second feature is that the system
asks the user about the user?s profile at the first stage of the dialogue. This feature is often found when
user satisfaction is high. The second feature also appeared in the dialogue with NoFraming. However,
NoFraming does not use framing, and asks the user to make a decision (DIRECTIVE). An example
utterance from the DIRECTIVE class is ?Please, decide (which camera you want to buy) after seeing the
catalog?.
Considering the evaluation result of the previous section, we can see that Sat and PS differ between
the user simulator and the real users (p < .05). While the general trend of showing improvements for
10
Note that, though systems estimate the satisfaction and evaluator?s decision at each turn for the belief state, the human
evaluator answers the questionnaire only when the dialogue is closed.
1714
satisfaction and persuasive success is identical in Figures 2 and 4, the systems are given excessively high
Sat in simulation. In addition, systems (especially Framing) are given underestimated PS in simulation.
One of the reasons for this is that the property of dialogue features for the predictive model for reward
differs from previous research (Hiraoka et al., 2014). In this paper, dialogue features for the predictive
model are calculated at each turn. In addition, persuasion success and user satisfaction are successively
calculated at each turn. In contrast, in previous research, the predictive model was constructed with
dialogue features calculated at end of the dialogue. Therefore, it is not guaranteed that the predictive
model estimates appropriate persuasion success and user satisfaction at each turn. Another reason is
that the simulator is not sufficiently accurate to use for reflecting real user?s behavior. Compared to
other works (Meguro et al., 2010; Misu et al., 2012), we are using a relatively small sized corpus for
training the user simulator. Therefore, the user simulator cannot be trained to accurately imitate real user
behavior. Improving the user simulator is an important challenge for future work.
7 Related work
There are a number of related works that apply reinforcement learning to persuasion and negotiation
dialogue. Georgila and Traum (2011) apply reinforcement learning to negotiation dialogue using user
simulators divided into three types representing individualist, collectivist, and altruist. Dialogue between
a florist and a grocer are assumed as an example of negotiation dialogue. In addition, Georgila (2013)
also applies reinforcement learning to two-issue negotiation dialogue where participants have a party,
and decide both the date and food type. A handcrafted user simulator is used for learning the policy
of each participant. Heeman (2009) models negotiation dialogue, assuming a furniture layout task, and
Paruchuri et al. (2009) model negotiation dialogue, assuming the dialogue between a seller and buyer.
Our research differs from these in three major ways. The first is that we use framing, positive or
negative statements about the particular item, which is known to be important for persuasion (Irwin et
al., 2013). By considering framing, the system has the potential to be more persuasive. While there is
one previous example of persuasive dialogue using framing (Mazzotta et al., 2007), this system does not
use an automatically learned policy, relying on handcrafted rules. In contrast, in our research, we apply
reinforcement learning to learn the system policy automatically.
In addition, in these previous works, rewards and belief states are defined with heuristics. In contrast,
in our research, reward is defined on the basis of knowledge of human persuasive dialogue. In particular,
we calculate the reward and belief state using the predictive model of Hiraoka et al. (2014) for estimating
persuasion success and user satisfaction using dialogue features. In the real world, it is unclear what
factors are important for achieving the dialogue goal in many persuasive situations. By considering these
predictions as knowledge of human persuasion, the system can identify the important factors in human
persuasion and can track the achievement of the goal based on these.
Finally, these works do not evaluate the learned policy, or evaluate only in simulation. In contrast, we
evaluate the learned policy with real users.
8 Conclusion
We apply reinforcement learning for learning cooperative persuasive dialogue system policies using
framing. In order to apply reinforcement learning, a user simulator and reward function is constructed
based on a human persuasive dialogue corpus. Then, we evaluate the learned policy and effect of fram-
ing using a user simulator and real users. Experimental evaluation indicates that applying reinforcement
learning is effective for construction of cooperative persuasive dialogue systems that use framing.
In the future, we plan to construct a fully automatic persuasive dialogue system using framing. In this
research, automatic speech recognition, spoken language understanding and natural language generation
are performed by a human Wizard. We plan to implement these modules and evaluate system perfor-
mance. In addition, in this research, corpus collection and evaluation are done in a role-playing situation.
Therefore, we plan to evaluate the system policies in a more realistic situation. We also plan to consider
non-verbal information (Nouri et al., 2013) for estimating persuasive success and user satisfaction.
1715
References
cncc. 2010. SofTalk. http://www35.atwiki.jp/softalk/.
Kallirroi Georgila and David Traum. 2011. Reinforcement learning of argumentation dialogue policies in negoti-
ation. Proceedings of INTERSPEEECH.
Kallirroi Georgila. 2013. Reinforcement learning of two-issue negotiation dialogue policies. Proceedings of the
SIGDIAL.
Marco Guerini, Oliviero Stock, and Massimo Zancanaro. 2003. Persuasion model for intelligent interfaces.
Proceedings of the IJCAI Workshop on Computational Models of Natural Argument.
Peter A. Heeman. 2009. Representing the reinforcement learning state in a negotiation dialogue. Proceedings of
ASRU.
Takuya Hiraoka, Yuki Yamauchi, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2013.
Dialogue management for leading the conversation in persuasive dialogue systems. Proceedings of ASRU.
Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. 2014. Construction and
analysis of a persuasive dialogue corpus. Proceedings of IWSDS.
Levin Irwin, Sandra L. Schneider, and Gary J. Gaeth. 2013. All frames are not created equal: A typology and
critical analysis of framing effects. Organizational behavior and human decision processes 76.2.
ISO24617-2, 2010. Language resource management-Semantic annotation frame work (SemAF), Part2: Dialogue
acts. ISO.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-machine interaction
for learning dialog strategies. Proceedings of ICASSP.
Irene Mazzotta and Fiorella de Rosis. 2006. Artifices for persuading to improve eating habits. AAAI Spring
Symposium: Argumentation for Consumers of Healthcare.
Irene Mazzotta, Fiorella de Rosis, and Valeria Carofiglio. 2007. PORTIA: a user-adapted persuasion system in the
healthy-eating domain. Intelligent Systems.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Minami, and Kohji Dohsaka. 2010. Controlling listening-
oriented dialogue using partially observable Markov decision processes. Proceedings of COLING.
Toyomi Meguro, Yasuhiro Minami, Ryuichiro Higashinaka, and Kohji Dohsaka. 2011. Wizard of oz evaluation of
listening-oriented dialogue control using pomdp. Proceedings of ASRU.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and David Traum. 2012. Reinforcement learning of question-
answering dialogue policies for virtual museum guides. Proceedings of the 13th Annual Meeting of SigDial.
Hien Nguyen, Judith Masthoff, and Pete Edwards. 2007. Persuasive effects of embodied conversational agent
teams. Proceedings of HCI.
Elnaz Nouri, Sunghyun Park, Stefan Scherer, Jonathan Gratch, Peter Carnevale, Louis-Philippe Morency, and
David Traum. 2013. Prediction of strategy and outcome as negotiation unfolds by using basic verbal and
behavioral features. Proceedings of INTERSPEECH.
Praveen Paruchuri, Nilanjan Chakraborty, Roie Zivan, Katia Sycara, Miroslav Dudik, and Geoff Gordon. 2009.
POMDP based negotiation modeling. Proceedings of the first MICON.
Martin Riedmiller. 2005. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement
learning method. Machine Learning: ECML.
Tom Schaul, Justin Bayer, Daan Wierstra, Yi Sun, Martin Felder, Frank Sehnke, Thomas R?uckstie?, and J?urgen
Schmidhuber. 2010. Pybrain. The Journal of Machine Learning Research.
Allison Terrell and Mutlu Bilge. 2012. A regression-based approach to modeling addressee backchannels. Pro-
ceedings of the 13th Annual Meeting of SIGDIAL.
Jason D. Williams and Steve Young. 2007. Scaling POMDPs for spoken dialog management. IEEE Transactions
on Audio, Speech, and Language Processing.
1716
Appendix
Table 8: The summary of one dialogue in the corpus (translated from Japanese)
Speaker Transcription GPF Tag
Customer Hello. INFORM
Customer I?m looking for a camera for traveling. Do you have any recommendations? PROPQ
Clerk What kind of pictures do you want to take? SETQ
Customer Well, I?m the member of a tennis club,
and want to take a picture of landscapes or tennis. ANSWER
Clerk O.K. You want the camera which can take both far and near. Don?t you? PROPQ
Clerk Well, have you used a camera before? PROPQ
Customer I have used a digital camera. But the camera was cheap and low resolution. ANSWER
Clerk I see. I see. Camera A is a high resolution camera.
A has extremely good resolution compared with other cameras.
Although this camera does not have a strong zoom,
its sensor is is almost the same as a single-lens camera. INFORM
Customer I see. INFORM
Clerk For a single lens camera,
buying only the lens can cost 100 thousand yen.
Compared to this, this camera is a bargain. INFORM
Customer Ah, I see. INFORM
Customer But, it?s a little expensive. right? PROPQ
Customer Well, I think, camera B is good at price. INFORM
Clerk Hahaha, yes, camera B is reasonably priced. ANSWER
Clerk But its performance is low compared with camera A. INFORM
Customer If I use the two cameras will I be able to tell the difference? PROPQ
Clerk Once you compare the pictures taken by these cameras,
you will understand the difference immediately.
The picture itself is very high quality.
But, camera B and E are lower resolution,
and the picture is a little bit lower quality. ANSWER
Customer Is there also difference in normal size pictures? PROPQ
Clerk Yes, whether the picture is small or large, there is a difference ANSWER
Customer Considering A has single-lens level performance, it is surely reasonable. INFORM
Clerk I think so too. INFORM
Clerk The general price of a single-lens is about 100 or 200 thousand yen.
Considering these prices, camera A is a good choice. INFORM
Customer Certainly, I?m interested in this camera. INFORM
Clerk Considering its performance, it is a bargain. INFORM
Customer I think I?ll go home, compare the pictures, and think a little more. COMMISIVE
Clerk I see. Thank you. DIRECTIVE
Table 9: Example positive framing of a salesperson?s utterance ?a
i
= B, p
i
= POS, r
i
= YES?. In this
example, the customer has indicated price as the preferred determinant.
Hahaha, yes, camera B is reasonably priced.
Table 10: Example negative framing of a salesperson?s utterance ?a
i
= B, p
i
= NEG, r
i
= NO?. In this
example, the customer has indicated price as the preferred determinant.
But, considering the long term usage, you might care about picture quality.
You might change your mind if you only buy a small camera (Camera B).
1717
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 128?132,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Acquiring a Dictionary of Emotion-Provoking Events
Hoa Trong Vu
?,?
, Graham Neubig
?
, Sakriani Sakti
?
, Tomoki Toda
?
, Satoshi Nakamura
?
?
Graduate School of Information Science, Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
?
Vietnam National University, University of Engineering and Technology
E3 Building - 144 Xuan Thuy Street, Cau Giay, Hanoi, Vietnam
Abstract
This paper is concerned with the discov-
ery and aggregation of events that provoke
a particular emotion in the person who
experiences them, or emotion-provoking
events. We first describe the creation of a
small manually-constructed dictionary of
events through a survey of 30 subjects.
Next, we describe first attempts at auto-
matically acquiring and aggregating these
events from web data, with a baseline from
previous work and some simple extensions
using seed expansion and clustering. Fi-
nally, we propose several evaluation meas-
ures for evaluating the automatically ac-
quired events, and perform an evaluation
of the effectiveness of automatic event ex-
traction.
1 Introduction
?You look happy today, did something good hap-
pen?? This is a natural question in human dia-
logue, and most humans could think of a variety of
answers, such as ?I met my friends? or ?I passed a
test.? In this work, we concern ourselves with cre-
ating resources that answer this very question, or
more formally ?given a particular emotion, what
are the most prevalent events (or situations, con-
texts) that provoke it??
1
Information about these
emotion-provoking events is potentially useful for
emotion recognition (recognizing emotion based
on events mentioned in a dialogue), response gen-
eration (providing an answer to emotion-related
questions), and answering social-science related
questions (discovering events that affect the emo-
tion of a particular segment of the population).
1
This is in contrast to existing sentiment lexicons (Riloff
et al., 2003; Valitutti, 2004; Esuli and Sebastiani, 2006; Ve-
likovich et al., 2010; Mohammad and Turney, 2013), which
only record the sentiment orientation of particular words
(such as ?meet? or ?friend?), which, while useful, are less dir-
ectly connected to the emotions than the events themselves.
While there is very little previous research on
this subject, one previous work of note by Tok-
uhisa et al. (2008) focused on emotion-provoking
events purely from the viewpoint of emotion re-
cognition. They used large corpus of examples
collected from the Web using manual patterns to
build a k-nearest-neighbors emotion classifier for
dialog systems and found that the classifier sig-
nificantly outperforms baseline methods. This
method provides both an inspiration and a baseline
for our work, but still lacks in that it makes no
attempt to measure the quality of the extracted
events, aggregate similar events, or rank events by
prevalence, all essential factors when attempting
to use extracted events for applications other than
simple emotion recognition.
In this paper, we describe work on creat-
ing prevalence-ranked dictionaries of emotion-
provoking events through both manual labor and
automatic information extraction. To create a
manual dictionary of events, we perform a sur-
vey asking 30 participants to describe events that
caused them to feel a particular emotion, and
manually cleaned and aggregated the results into
a ranked list. Next, we propose several methods
for extracting events automatically from large data
from the Web, which will allow us to increase the
coverage over the smaller manually created dic-
tionary. We start with Tokuhisa et al. (2008)?s pat-
terns as a baseline, and examine methods for im-
proving precision and coverage through the use of
seed expansion and clustering. Finally, we dis-
cuss evaluation measures for the proposed task,
and perform an evaluation of the automatically ex-
tracted emotion-provoking events. The acquired
events will be provided publicly upon acceptance
of the paper.
2 Manual Creation of Events
In order to create a small but clean set of gold-
standard data for each emotion, we first performed
128
Emotions Words
happiness happy, glad
sadness sad, upset
anger angry, irritated
fear afraid, scared
surprise surprised, astonished
disgust disgusted, terrible
Table 2: Seed words for each emotion.
a survey on emotion-provoking events. We did so
by asking a total of 30 subjects (a mixture of male
and female from 20-40 years of age) to write down
five events that provoke each of five emotions:
happiness, sadness, anger, fear, and surprise. As
these events created according to this survey still
have a large amount of lexical variation, we manu-
ally simplify them to their core and merge together
events that have similar meanings.
Finally, for each emotion we extract all the
events that are shared by more than one person. It
should be noted that this will not come anywhere
close to covering the entirety of human emotion,
but as each event is shared by at least two people
in a relatively small sample, any attempt to create
a comprehensive dictionary of emotion-provoking
events should at least be able to cover the pairs in
this collection. We show the most common three
events for each emotion in Table 1.
3 Automatic Extraction of Events
We also performed experiments attempting to
automatically extract and aggregate events from
Web data. As a starting point, we follow Tokuhisa
et al. (2008) in defining a single reliable pattern as
a starting point for event extraction:
I am EMOTION that EVENT
As this pattern is a relatively reliable indicator that
the event is correct, most events extracted by this
pattern will actually be emotion-provoking events.
For instance, this pattern will be matched with the
sentence ?I am happy that my mother is feeling
better?, in which my mother is feeling better cer-
tainly causes happiness.
For the EMOTION placeholder, we take into ac-
count 6 emotions - happiness, sadness, anger, fear,
disgust, and surprise - argued by Ekman (1992) to
be the most basic. We manually create a short list
of words that can be inserted into the above pattern
appropriately, as shown in Table 2.
For the EVENT placeholder, we allow any string
of words, but it is necessary to choose the scope
of the string that is referring to the emotion-
provoking event. To this end, we use a syntactic
parser and set a hard restriction that all events must
be a subtree having root tag S and containing at
least one noun phrase and one verb phrase.
Given these two restrictions, these patterns
provide us with high quality event-emotion pairs,
but the method is still lacking in two respects, lack
of coverage and lack of ability to aggregate sim-
ilar events. As both of these are essential to cre-
ating a high-quality and non-redundant dictionary
of events, we make two simple extensions to the
extraction process as follows.
3.1 Pattern Expansion
Pattern expansion, or bootstrapping algorithms are
widely used in the information extraction field
(Ravichandran and Hovy, 2002). In particular Es-
presso (Pantel and Pennacchiotti, 2006) is known
as a state-of-the-art pattern expansion algorithm
widely used in acquiring relationships between
entities. We omit the details of the algorithm
for space concerns, but note that applying the al-
gorithm to our proposed task is relatively straight-
forward, and allows us to acquire additional pat-
terns that may be matched to improve the cover-
age over the single seed pattern. We do, however,
make two changes to the algorithm. The first is
that, as we are interested in extracting events in-
stead of entities, we impose the previously men-
tioned restriction of one verb phrase and one noun
phrase over all events extracted by the patterns.
The second is that we perform normalization of
events to reduce their variability, namely removing
all function words, replacing proper nouns with
special symbol, and lemmatizing words.
3.2 Grouping events
The second improvement we perform is group-
ing the extracted events together. Grouping has a
number of potential practical advantages, as noted
frequently in previous work (Becker et al., 2011).
The first is that by grouping similar events to-
gether, we can relieve sparsity issues to some
extent by sharing statistics among the events in
a single group. The second is that aggregating
events together allows humans to browse the lists
more efficiently by reducing the number of re-
dundant entries. In preliminary experiments, we
attempted several clustering methods and even-
129
Emotions Events
happiness meeting friends going on a date getting something I want
sadness someone dies/gets sick someone insults me people leave me alone
anger someone insults me someone breaks a promise someone is too lazy
fear thinking about the future taking a test walking/driving at night
surprise seeing a friend unexpectedly someone comes to visit receiving a gift
Table 1: The top three events for each emotion.
tually settled on hierarchical agglomerative clus-
tering and the single-linkage criterion using co-
sine similarity as a distance measure (Gower and
Ross, 1969). Choosing the stopping criterion for
agglomerative clustering is somewhat subjective,
in many cases application dependent, but for the
evaluation in this work, we heuristically choose
the number of groups so the average number of
events in each group is four, and leave a further
investigation of the tuning to future work.
4 Evaluation Measures
Work on information extraction typically uses ac-
curacy and recall of the extracted information as
an evaluation measure. However, in this work, we
found that it is difficult to assign a clear-cut dis-
tinction between whether an event provokes a par-
ticular emotion or not. In addition, recall is diffi-
cult to measure, as there are essentially infinitely
many events. Thus, in this section, we propose two
new evaluation measures to measure the precision
and recall of the events that we recovered in this
task.
To evaluate the precision of the events extrac-
ted by our method, we focus on the fact that an
event might provoke multiple emotions, but usu-
ally these emotions can be ranked in prominence
or appropriateness. This is, in a way, similar to the
case of information retrieval, where there may be
many search results, but some are more appropri-
ate than others. Based on this observation, we fol-
low the information retrieval literature (Voorhees,
1999) in adapting mean reciprocal rank (MRR) as
an evaluation measure of the accuracy of our ex-
traction. In our case, one event can have multiple
emotions, so for each event that the system out-
puts, we ask an annotator to assign emotions in
descending order of prominence or appropriate-
ness, and assess MRR with respect to these ranked
emotions.
2
We also measure recall with respect to the
2
In the current work we did not allow annotators to assign
?ties? between the emotions, but this could be accommodated
in the MRR framework.
manually created dictionary described in Section
2, which gives us an idea of what percent of com-
mon emotions we were able to recover. It should
be noted that in order to measure recall, it is ne-
cessary to take a matching between the events out-
put by the system and the events in the previously
described list. While it would be ideal to do this
automatically, this is difficult due to small lexical
variations between the system output and the list.
Thus, for the current work we perform manual
matching between the system hypotheses and the
references, and hope to examine other ways of
matching in future work.
5 Experiments
In this section, we describe an experimental eval-
uation of the accuracy of automatic extraction of
emotion-provoking events.
5.1 Experimental Setup
We use Twitter
3
as a source of data, as it is it
provides a massive amount of information, and
also because users tend to write about what they
are doing as well as their thoughts, feelings and
emotions. We use a data set that contains more
than 30M English tweets posted during the course
of six weeks in June and July of 2012. To remove
noise, we perform a variety of preprocessing, re-
moving emoticons and tags, normalizing using
the scripts provided by Han and Baldwin (2011),
and Han et al. (2012). CoreNLP
4
was used to
get the information about part-of-speech, syntactic
parses, and lemmas.
We prepared four systems for comparison. As a
baseline, we use a method that only uses the ori-
ginal seed pattern mentioned in Section 3 to ac-
quire emotion-provoking events. We also evalu-
ate expansions to this method with clustering, with
pattern expansion, and with both.
We set a 10 iteration limit on the Espresso al-
gorithm and after each iteration, we add the 20
3
http://www.twitter.com
4
http://nlp.stanford.edu/software/
corenlp.shtml
130
Methods MRR Recall
Seed 46.3 (?5.0) 4.6 (?0.5)
Seed + clust 57.2 (?7.9) 8.5 (?0.9)
Espresso 49.4 (?2.8) 8.0 (?0.5)
Espresso + clust 71.7 (?2.9) 15.4 (?0.8)
Table 3: MRR and recall of extracted data (with
standard deviation for 3 annotators).
most reliable patterns to the pattern set, and in-
crease the seed set by one third of its size. These
values were set according to a manual inspection
of the results for several settings, before any eval-
uation was performed.
We examine the utility of each method accord-
ing to the evaluation measures proposed in Sec-
tion 4 over five emotions, happiness, sadness, an-
ger, fear, and surprise.
5
To measure MRR and
recall, we used the 20 most frequent events or
groups extracted by each method for these five
emotions, and thus all measures can be interpreted
as MRR@20 and recall@20. As manual annota-
tion is required to calculate both measures, we ac-
quired results for 3 annotators and report the aver-
age and standard deviation.
5.2 Experimental Results
The results are found in Table 3. From these res-
ults we can see that clustering the events causes a
significant gain on both MRR and recall, regard-
less of whether we use Espresso or not. Looking
at the results for Espresso, we see that it allows for
small boost in recall when used on its own, due
to the fact that the additional patterns help recover
more instances of each event, making the estimate
of frequency counts more robust. However, Es-
presso is more effective when used in combination
with clustering, showing that both methods are
capturing different varieties of information, both
of which are useful for the task.
In the end, the combination of pattern expansion
and clustering achieves an MRR of 71.7% and re-
call of 15.4%. While the MRR could be deemed
satisfactory, the recall is still relatively low. One
reason for this is that due to the labor-intensive
manual evaluation, it is not realistic to check many
more than the top 20 extracted events for each
emotion, making automatic evaluation metrics the
top on the agenda for future work.
5
We exclude disgust, as the seed only matched 26 times
over entire corpus, not enough for a reasonable evaluation.
Emotions MRR Recall
happiness 93.9 23.1
sadness 76.9 10.0
anger 76.5 14.0
fear 48.3 24.3
surprise 59.6 0.0
Table 4: Average MRR and recall by emotion for
the Espresso + clustering method.
However, even without considering this, we
found that the events extracted from Twitter
were somewhat biased towards common, everyday
events, or events regarding love and dating. On the
other hand, our annotators produced a wide vari-
ety of events including both everyday events, and
events that do not happen every day, but leave a
particularly strong impression when encountered.
This can be seen particularly in the accuracy and
recall results by emotion for the best system shown
in Table 4. We can see that for some emotions we
achieved recall approaching 25%, but for surprise
we didn?t manage to extract any of the emotions
created by the annotators at all, instead extracting
more mundane events such as ?surprised I?m not
fat yet? or ?surprised my mom hasn?t called me
yet.? Covering the rare, but important events is an
interesting challenge for expansions to this work.
6 Conclusion and Future Work
In this paper we described our work in creat-
ing a dictionary of emotion-provoking events, and
demonstrated results for four varieties of auto-
matic information extraction to expand this dic-
tionary. As this is the first attempt at acquiring dic-
tionaries of emotion-provoking events, there are
still many future directions that deserve further in-
vestigation. As mentioned in the experimental dis-
cussion, automatic matching for the evaluation of
event extraction, and ways to improve recall over
rarer but more impressive events are necessary.
There are also many improvements that could be
made to the extraction algorithm itself, including
more sophisticated clustering and pattern expan-
sion algorithms. Finally, it would be quite interest-
ing to use the proposed method as a tool for psy-
chological inquiry, including into the differences
between events that are extracted from Twitter and
other media, or the differences between different
demographics.
131
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identific-
ation on Twitter. In Proceedings of the Fifth Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM11).
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3-4):169?200.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Con-
ference on Language Resources and Evaluation,
pages 417?422.
John C Gower and GJS Ross. 1969. Minimum span-
ning trees and single linkage cluster analysis. Ap-
plied statistics, pages 54?64.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation diction-
ary for microblogs. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 421?432, Jeju Island, Korea,
July. Association for Computational Linguistics.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Patrick Pantel and Marco Pennacchiotti. 2006. Es-
presso: leveraging generic patterns for automatic-
ally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 113?120.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 41?47.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL
2003-Volume 4, pages 25?32. Association for Com-
putational Linguistics.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive ex-
amples extracted from the web. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08, pages
881?888.
Ro Valitutti. 2004. Wordnet-affect: an affective ex-
tension of wordnet. In In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 1083?1086.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 777?785.
Ellen M Voorhees. 1999. The trec-8 question an-
swering track report. In Proceedings of TREC,
volume 99, pages 77?82.
132
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 551?556,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Optimizing Segmentation Strategies for Simultaneous Speech Translation
Yusuke Oda Graham Neubig Sakriani Sakti Tomoki Toda Satoshi Nakamura
Graduate School of Information Science
Nara Institute of Science and Technology
Takayama, Ikoma, Nara 630-0192, Japan
{oda.yusuke.on9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
Abstract
In this paper, we propose new algorithms
for learning segmentation strategies for si-
multaneous speech translation. In contrast
to previously proposed heuristic methods,
our method finds a segmentation that di-
rectly maximizes the performance of the
machine translation system. We describe
two methods based on greedy search and
dynamic programming that search for the
optimal segmentation strategy. An experi-
mental evaluation finds that our algorithm
is able to segment the input two to three
times more frequently than conventional
methods in terms of number of words,
while maintaining the same score of auto-
matic evaluation.
1
1 Introduction
The performance of speech translation systems
has greatly improved in the past several years,
and these systems are starting to find wide use in
a number of applications. Simultaneous speech
translation, which translates speech from the
source language into the target language in real
time, is one example of such an application. When
translating dialogue, the length of each utterance
will usually be short, so the system can simply
start the translation process when it detects the end
of an utterance. However, in the case of lectures,
for example, there is often no obvious boundary
between utterances. Thus, translation systems re-
quire a method of deciding the timing at which
to start the translation process. Using estimated
ends of sentences as the timing with which to start
translation, in the same way as a normal text trans-
lation, is a straightforward solution to this problem
(Matusov et al, 2006). However, this approach
1
The implementation is available at
http://odaemon.com/docs/codes/greedyseg.html.
impairs the simultaneity of translation because the
system needs to wait too long until the appearance
of a estimated sentence boundary. For this reason,
segmentation strategies, which separate the input
at appropriate positions other than end of the sen-
tence, have been studied.
A number of segmentation strategies for simul-
taneous speech translation have been proposed in
recent years. F?ugen et al (2007) and Bangalore et
al. (2012) propose using prosodic pauses in speech
recognition to denote segmentation boundaries,
but this method strongly depends on characteris-
tics of the speech, such as the speed of speaking.
There is also research on methods that depend on
linguistic or non-linguistic heuristics over recog-
nized text (Rangarajan Sridhar et al, 2013), and it
was found that a method that predicts the location
of commas or periods achieves the highest perfor-
mance. Methods have also been proposed using
the phrase table (Yarmohammadi et al, 2013) or
the right probability (RP) of phrases (Fujita et al,
2013), which indicates whether a phrase reorder-
ing occurs or not.
However, each of the previously mentioned
methods decides the segmentation on the basis
of heuristics, so the impact of each segmenta-
tion strategy on translation performance is not di-
rectly considered. In addition, the mean number
of words in the translation unit, which strongly af-
fects the delay of translation, cannot be directly
controlled by these methods.
2
In this paper, we propose new segmentation al-
gorithms that directly optimize translation perfor-
mance given the mean number of words in the
translation unit. Our approaches find appropri-
ate segmentation boundaries incrementally using
greedy search and dynamic programming. Each
boundary is selected to explicitly maximize trans-
2
The method using RP can decide relative frequency of
segmentation by changing a parameter, but guessing the
length of a translation unit from this parameter is not trivial.
551
lation accuracy as measured by BLEU or another
evaluation measure.
We evaluate our methods on a speech transla-
tion task, and we confirm that our approaches can
achieve translation units two to three times as fine-
grained as other methods, while maintaining the
same accuracy.
2 Optimization Framework
Our methods use the outputs of an existing ma-
chine translation system to learn a segmentation
strategy. We define F = {f
j
: 1 ? j ? N},
E = {e
j
: 1 ? j ? N} as a parallel corpus
of source and target language sentences used to
train the segmentation strategy. N represents the
number of sentences in the corpus. In this work,
we consider sub-sentential segmentation, where
the input is already separated into sentences, and
we want to further segment these sentences into
shorter units. In an actual speech translation sys-
tem, these sentence boundaries can be estimated
automatically using a method like the period es-
timation mentioned in Rangarajan Sridhar et al
(2013). We also assume the machine translation
system is defined by a function MT (f) that takes
a string of source words f as an argument and re-
turns the translation result
?
e.
3
We will introduce individual methods in the fol-
lowing sections, but all follow the general frame-
work shown below:
1. Decide the mean number of words ? and the
machine translation evaluation measure EV
as parameters of algorithm. We can use an
automatic evaluation measure such as BLEU
(Papineni et al, 2002) as EV . Then, we cal-
culate the number of sub-sentential segmen-
tation boundaries K that we will need to in-
sert into F to achieve an average segment
length ?:
K := max
(
0,
?
?
f?F |f |
?
?
?N
)
. (1)
2. Define S as a set of positions in F in which
we will insert segmentation boundaries. For
example, if we will segment the first sentence
after the third word and the third sentence af-
ter the fifth word, then S = {?1, 3? , ?3, 5?}.
3
In this work, we do not use the history of the language
model mentioned in Bangalore et al (2012). Considering this
information improves the MT performance and we plan to
include this in our approach in future work.
Figure 1: Concatenated translation MT (f ,S).
Based on this representation, choose K seg-
mentation boundaries in F to make the set
S
?
that maximizes an evaluation function ?
as below:
S
?
:= arg max
S?{S
?
:|S
?
|=K}
?(S;F , E , EV,MT ).
(2)
In this work, we define ? as the sum of the
evaluation measure for each parallel sentence
pair ?f
j
,e
j
?:
?(S) :=
N
?
j=1
EV (MT (f
j
,S), e
j
), (3)
where MT (f ,S) represents the concatena-
tion of all partial translations {MT (f
(n)
)}
given the segments S as shown in Figure 1.
Equation (3) indicates that we assume all
parallel sentences to be independent of each
other, and the evaluation measure is calcu-
lated for each sentence separately. This lo-
cality assumption eases efficient implementa-
tion of our algorithm, and can be realized us-
ing a sentence-level evaluation measure such
as BLEU+1 (Lin and Och, 2004).
3. Make a segmentation model M
S
?
by treating
the obtained segmentation boundaries S
?
as
positive labels, all other positions as negative
labels, and training a classifier to distinguish
between them. This classifier is used to de-
tect segmentation boundaries at test time.
Steps 1. and 3. of the above procedure are triv-
ial. In contrast, choosing a good segmentation ac-
cording to Equation (2) is difficult and the focus
of the rest of this paper. In order to exactly solve
Equation (2), we must perform brute-force search
over all possible segmentations unless we make
some assumptions about the relation between the
? yielded by different segmentations. However,
the number of possible segmentations is exponen-
tially large, so brute-force search is obviously in-
tractable. In the following sections, we propose 2
552
I ate lunch but she left
Segments already selected at the k-th iteration
? = 0.5 ? = 0.8
(k+1)-th segment
? = 0.7
Figure 2: Example of greedy search.
Algorithm 1 Greedy segmentation search
S
?
? ?
for k = 1 to K do
S
?
? S
?
?
{
arg max
s

?S
?
?(S
?
? {s})
}
end for
return S
?
methods that approximately search for a solution
to Equation (2).
2.1 Greedy Search
Our first approximation is a greedy algorithm that
selects segmentation boundaries one-by-one. In
this method, k already-selected boundaries are left
unchanged when deciding the (k+1)-th boundary.
We find the unselected boundary that maximizes ?
and add it to S:
S
k+1
= S
k
?
{
arg max
s

?S
k
?(S
k
? {s})
}
. (4)
Figure 2 shows an example of this process for a
single sentence, and Algorithm 1 shows the algo-
rithm for calculating K boundaries.
2.2 Greedy Search with Feature Grouping
and Dynamic Programming
The method described in the previous section
finds segments that achieve high translation per-
formance for the training data. However, because
the translation system MT and evaluation mea-
sureEV are both complex, the evaluation function
? includes a certain amount of noise. As a result,
the greedy algorithm that uses only ? may find a
segmentation that achieves high translation perfor-
mance in the training data by chance. However,
these segmentations will not generalize, reducing
the performance for other data.
We can assume that this problem can be solved
by selecting more consistent segmentations of the
training data. To achieve this, we introduce a con-
straint that all positions that have similar charac-
teristics must be selected at the same time. Specif-
ically, we first group all positions in the source
I ate lunch but she left
PRP VBD NN CC PRP VBD
I ate an apple and an orange
PRP VBD DT NN CC DT NN
WORD:
 POS:
WORD:
 POS:
Group
PRP+VBD
Group
NN+CC
Group
DT+NN
Figure 3: Grouping segments by POS bigrams.
sentences using features of the position, and intro-
duce a constraint that all positions with identical
features must be selected at the same time. Figure
3 shows an example of how this grouping works
when we use the POS bigram surrounding each
potential boundary as our feature set.
By introducing this constraint, we can expect
that features which have good performance over-
all will be selected, while features that have rela-
tively bad performance will not be selected even if
good performance is obtained when segmenting at
a specific location. In addition, because all posi-
tions can be classified as either segmented or not
by evaluating whether the corresponding feature is
in the learned feature set or not, it is not necessary
to train an additional classifier for the segmenta-
tion model when using this algorithm. In other
words, this constraint conducts a kind of feature
selection for greedy search.
In contrast to Algorithm 1, which only selected
one segmentation boundary at once, in our new
setting there are multiple positions selected at one
time. Thus, we need to update our search algo-
rithm to handle this setting. To do so, we use
dynamic programming (DP) together with greedy
search. Algorithm 2 shows ourGreedy+DP search
algorithm. Here, c(?;F) represents the number
of appearances of ? in the set of source sentences
F , and S(F ,?) represents the set of segments de-
fined by both F and the set of features ?.
The outer loop of the algorithm, like Greedy,
iterates over all S of size 1 to K. The inner loop
examines all features that appear exactly j times
in F , and measures the effect of adding them to
the best segmentation with (k ? j) boundaries.
2.3 Regularization by Feature Count
Even after we apply grouping by features, it
is likely that noise will still remain in the less
frequently-seen features. To avoid this problem,
we introduce regularization into the Greedy+DP
algorithm, with the evaluation function ? rewrit-
553
Algorithm 2 Greedy+DP segmentation search
?
0
? ?
for k = 1 to K do
for j = 0 to k ? 1 do
?
?
? {? : c(?;F) = k ? j ? ?
Segmentation for Efficient Supervised Language Annotation with an
Explicit Cost-Utility Tradeoff
Matthias Sperber1, Mirjam Simantzik2, Graham Neubig3, Satoshi Nakamura3, Alex Waibel1
1Karlsruhe Institute of Technology, Institute for Anthropomatics, Germany
2Mobile Technologies GmbH, Germany
3Nara Institute of Science and Technology, AHC Laboratory, Japan
matthias.sperber@kit.edu, mirjam.simantzik@jibbigo.com, neubig@is.naist.jp
s-nakamura@is.naist.jp, waibel@kit.edu
Abstract
In this paper, we study the problem of manu-
ally correcting automatic annotations of natu-
ral language in as efficient a manner as pos-
sible. We introduce a method for automati-
cally segmenting a corpus into chunks such
that many uncertain labels are grouped into
the same chunk, while human supervision
can be omitted altogether for other segments.
A tradeoff must be found for segment sizes.
Choosing short segments allows us to reduce
the number of highly confident labels that are
supervised by the annotator, which is useful
because these labels are often already correct
and supervising correct labels is a waste of
effort. In contrast, long segments reduce the
cognitive effort due to context switches. Our
method helps find the segmentation that opti-
mizes supervision efficiency by defining user
models to predict the cost and utility of su-
pervising each segment and solving a con-
strained optimization problem balancing these
contradictory objectives. A user study demon-
strates noticeable gains over pre-segmented,
confidence-ordered baselines on two natural
language processing tasks: speech transcrip-
tion and word segmentation.
1 Introduction
Many natural language processing (NLP) tasks re-
quire human supervision to be useful in practice,
be it to collect suitable training material or to meet
some desired output quality. Given the high cost of
human intervention, how to minimize the supervi-
sion effort is an important research problem. Previ-
ous works in areas such as active learning, post edit-
(a) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
(b) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
(c) It was a bright cold (they) in (apron), and (a) clocks
were striking thirteen.
Figure 1: Three automatic transcripts of the sentence ?It
was a bright cold day in April, and the clocks were strik-
ing thirteen?, with recognition errors in parentheses. The
underlined parts are to be corrected by a human for (a)
sentences, (b) words, or (c) the proposed segmentation.
ing, and interactive pattern recognition have inves-
tigated this question with notable success (Settles,
2008; Specia, 2011; Gonza?lez-Rubio et al., 2010).
The most common framework for efficient anno-
tation in the NLP context consists of training an NLP
system on a small amount of baseline data, and then
running the system on unannotated data to estimate
confidence scores of the system?s predictions (Set-
tles, 2008). Sentences with the lowest confidence
are then used as the data to be annotated (Figure 1
(a)). However, it has been noted that when the NLP
system in question already has relatively high accu-
racy, annotating entire sentences can be wasteful, as
most words will already be correct (Tomanek and
Hahn, 2009; Neubig et al., 2011). In these cases, it
is possible to achieve much higher benefit per anno-
tated word by annotating sub-sentential units (Fig-
ure 1 (b)).
However, as Settles et al. (2008) point out, sim-
ply maximizing the benefit per annotated instance
is not enough, as the real supervision effort varies
169
Transactions of the Association for Computational Linguistics, 2 (2014) 169?180. Action Editor: Eric Fosler-Lussier.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
1 3 5 7 9 11 13 15 17 19
0
2
4
6
Segment length
A
vg
. t
im
e 
/ i
ns
ta
nc
e 
[s
ec
]
 
 
Transcription task
Word segmentation task
Figure 2: Average annotation time per instance, plotted
over different segment lengths. For both tasks, the effort
clearly increases for short segments.
greatly across instances. This is particularly impor-
tant in the context of choosing segments to annotate,
as human annotators heavily rely on semantics and
context information to process language, and intu-
itively, a consecutive sequence of words can be su-
pervised faster and more accurately than the same
number of words spread out over several locations in
a text. This intuition can also be seen in our empiri-
cal data in Figure 2, which shows that for the speech
transcription and word segmentation tasks described
later in Section 5, short segments had a longer anno-
tation time per word. Based on this fact, we argue
it would be desirable to present the annotator with
a segmentation of the data into easily supervisable
chunks that are both large enough to reduce the num-
ber of context switches, and small enough to prevent
unnecessary annotation (Figure 1 (c)).
In this paper, we introduce a new strategy for nat-
ural language supervision tasks that attempts to op-
timize supervision efficiency by choosing an appro-
priate segmentation. It relies on a user model that,
given a specific segment, predicts the cost and the
utility of supervising that segment. Given this user
model, the goal is to find a segmentation that mini-
mizes the total predicted cost while maximizing the
utility. We balance these two criteria by defining a
constrained optimization problem in which one cri-
terion is the optimization objective, while the other
criterion is used as a constraint. Doing so allows
specifying practical optimization goals such as ?re-
move as many errors as possible given a limited time
budget,? or ?annotate data to obtain some required
classifier accuracy in as little time as possible.?
Solving this optimization task is computationally
difficult, an NP-hard problem. Nevertheless, we
demonstrate that by making realistic assumptions
about the segment length, an optimal solution can
be found using an integer linear programming for-
mulation for mid-sized corpora, as are common for
supervised annotation tasks. For larger corpora, we
provide simple heuristics to obtain an approximate
solution in a reasonable amount of time.
Experiments over two example scenarios demon-
strate the usefulness of our method: Post editing
for speech transcription, and active learning for
Japanese word segmentation. Our model predicts
noticeable efficiency gains, which are confirmed in
experiments with human annotators.
2 Problem Definition
The goal of our method is to find a segmentation
over a corpus of word tokens wN1 that optimizes
supervision efficiency according to some predictive
user model. The user model is denoted as a set of
functions ul,k(wba) that evaluate any possible sub-
sequence wba of tokens in the corpus according to
criteria l2L, and supervision modes k2K.
Let us illustrate this with an example. Sperber et
al. (2013) defined a framework for speech transcrip-
tion in which an initial, erroneous transcript is cre-
ated using automatic speech recognition (ASR), and
an annotator corrects the transcript either by correct-
ing the words by keyboard, by respeaking the con-
tent, or by leaving the words as is. In this case,
we could define K={TYPE, RESPEAK, SKIP}, each
constant representing one of these three supervision
modes. Our method will automatically determine
the appropriate supervision mode for each segment.
The user model in this example might evaluate ev-
ery segment according to two criteria L, a cost crite-
rion (in terms of supervision time) and a utility cri-
terion (in terms of number of removed errors), when
using each mode. Intuitively, respeaking should be
assigned both lower cost (because speaking is faster
than typing), but also lower utility than typing on a
keyboard (because respeaking recognition errors can
occur). The SKIP mode denotes the special, unsuper-
vised mode that always returns 0 cost and 0 utility.
Other possible supervision modes include mul-
tiple input modalities (Suhm et al., 2001), several
human annotators with different expertise and cost
170
(Donmez and Carbonell, 2008), and correction vs.
translation from scratch in machine translation (Spe-
cia, 2011). Similarly, cost could instead be ex-
pressed in monetary terms, or the utility function
could predict the improvement of a classifier when
the resulting annotation is not intended for direct hu-
man consumption, but as training data for a classifier
in an active learning framework.
3 Optimization Framework
Given this setting, we are interested in simulta-
neously finding optimal locations and supervision
modes for all segments, according to the given cri-
teria. Each resulting segment will be assigned ex-
actly one of these supervision modes. We de-
note a segmentation of the N tokens of corpus wN1
into M?N segments by specifying segment bound-
ary markers sM+11 =(s1=1, s2, . . . , sM+1=N+1).
Setting a boundary marker si=a means that we
put a segment boundary before the a-th word to-
ken (or the end-of-corpus marker for a=N+1).
Thus our corpus is segmented into token sequences
[(wsj , . . . , wsj+1 1)]Mj=1. The supervision modes
assigned to each segment are denoted by mj . We
favor those segmentations that minimize the cumu-
lative valuePMj=1[ul,mj (wsj+1sj )] for each criterion l.
For any criterion where larger values are intuitively
better, we flip the sign before defining ul,mj (wsj+1sj )
to maintain consistency (e.g. negative number of er-
rors removed).
3.1 Multiple Criteria Optimization
In the case of a single criterion (|L|=1), we obtain
a simple, single-objective unconstrained linear opti-
mization problem, efficiently solvable via dynamic
programming (Terzi and Tsaparas, 2006). However,
in practice one usually encounters several compet-
ing criteria, such as cost and utility, and here we
will focus on this more realistic setting. We balance
competing criteria by using one as an optimization
objective, and the others as constraints.1 Let crite-
1This approach is known as the bounded objective function
method in multi-objective optimization literature (Marler and
Arora, 2004). The very popular weighted sum method merges
criteria into a single efficiency measure, but is problematic in
our case because the number of supervised tokens is unspec-
ified. Unless the weights are carefully chosen, the algorithm
might find, e.g., the completely unsupervised or completely su-
(at)% (what?s)% a% bright% ?%
[RESPEAK:1.5/2]/
[SKIP:0/0]/
1/ cold%2/ 3/ 4/ 5/ 6/
[TYPE:2/5]/[TYPE:1/4]/
[TYPE:1/4]/
[RESPEAK:0/3]/[SKIP:0/0]/
Figure 3: Excerpt of a segmentation graph for an ex-
ample transcription task similar to Figure 1 (some edges
are omitted for readability). Edges are labeled with their
mode, predicted number of errors that can be removed,
and necessary supervision time. A segmentation scheme
might prefer solid edges over dashed ones in this exam-
ple.
rion l0 be the optimization objective criterion, and
let Cl denote the constraining constants for the cri-
teria l 2 L l0 = L \ {l0}. We state the optimization
problem:
min
M ;sM+11 ;mM1
MX
j=1
?
ul0,mj
 
wsj+1sj
 ?
s.t.
MX
j=1
?
ul,mj
 
wsj+1sj
 ?
? Cl (8l 2 L l0)
This constrained optimization problem is difficult
to solve. In fact, the NP-hard multiple-choice knap-
sack problem (Pisinger, 1994) corresponds to a spe-
cial case of our problem in which the number of seg-
ments is equal to the number of tokens, implying
that our more general problem is NP-hard as well.
In order to overcome this problem, we refor-
mulate search for the optimal segmentation as a
resource-constrained shortest path problem in a di-
rected, acyclic multigraph. While still not efficiently
solvable in theory, this problem is well studied in
domains such as vehicle routing and crew schedul-
ing (Irnich and Desaulniers, 2005), and it is known
that in many practical situations the problem can
be solved reasonably efficiently using integer linear
programming relaxations (Toth and Vigo, 2001).
In our formalism, the set of nodes V represents
the spaces between neighboring tokens, at which the
algorithm may insert segment boundaries. A node
with index i represents a segment break before the
i-th token, and thus the sequence of the indices in
a path directly corresponds to sM+11 . Edges E de-
note the grouping of tokens between the respective
pervised segmentation to be most ?efficient.?
171
nodes into one segment. Edges are always directed
from left to right, and labeled with a supervision
mode. In addition, each edge between nodes i and j
is assigned ul,k(wj 1i ), the corresponding predicted
value for each criterion l 2 L and supervision mode
k 2 K, indicating that the supervision mode of the
j-th segment in a path directly corresponds to mj .
Figure 3 shows an example of what the result-
ing graph may look like. Our original optimization
problem is now equivalent to finding the shortest
path between the first and last nodes according to
criterion l0, while obeying the given resource con-
straints. According to a widely used formulation for
the resource constrained shortest path problem, we
can defineEij as the set of competing edges between
i and j, and express this optimization problem with
the following integer linear program (ILP):
min
x
X
i,j2V
X
k2Eij
xijkul0,k(sj 1i ) (1)
s.t.
X
i,j2V
X
k2Eij
xijkul,k(sj 1i ) ? Cl
(8l 2 L l0)
(2)
X
i2V
k2Eij
xijk =
X
i2V
k2Eij
xjik
(8j 2 V \{1, n})
(3)
X
j2V
k2E1j
x1jk = 1 (4)
X
i2V
k2Ein
xink = 1 (5)
xijk 2 {0, 1} (8xijk 2 x) (6)
The variables x={xijk|i, j 2 V , k 2 Eij} denote
the activation of the k?th edge between nodes i and
j. The shortest path according to the minimization
objective (1), that still meets the resource constraints
for the specified criteria (2), is to be computed. The
degree constraints (3,4,5) specify that all but the first
and last nodes must have as many incoming as out-
going edges, while the first node must have exactly
one outgoing, and the last node exactly one incom-
ing edge. Finally, the integrality condition (6) forces
all edges to be either fully activated or fully deacti-
vated. The outlined problem formulation can solved
directly by using off-the-shelf ILP solvers, here we
employ GUROBI (Gurobi Optimization, 2012).
3.2 Heuristics for Approximation
In general, edges are inserted for every supervision
mode between every combination of two nodes. The
search space can be constrained by removing some
of these edges to increase efficiency. In this study,
we only consider edges spanning at most 20 tokens.
For cases in which larger corpora are to be anno-
tated, or when the acceptable delay for delivering re-
sults is small, a suitable segmentation can be found
approximately. The easiest way would be to parti-
tion the corpus, e.g. according to its individual doc-
uments, divide the budget constraints evenly across
all partitions, and then segment each partition inde-
pendently. More sophisticated methods might ap-
proximate the Pareto front for each partition, and
distribute the budgets in an intelligent way.
4 User Modeling
While the proposed framework is able to optimize
the segmentation with respect to each criterion, it
also rests upon the assumption that we can provide
user models ul,k(wj 1i ) that accurately evaluate ev-
ery segment according to the specified criteria and
supervision modes. In this section, we discuss our
strategies for estimating three conceivable criteria:
annotation cost, correction of errors, and improve-
ment of a classifier.
4.1 Annotation Cost Modeling
Modeling cost requires solving a regression prob-
lem from features of a candidate segment to annota-
tion cost, for example in terms of supervision time.
Appropriate input features depend on the task, but
should include notions of complexity (e.g. a confi-
dence measure) and length of the segment, as both
are expected to strongly influence supervision time.
We propose using Gaussian process (GP) regres-
sion for cost prediction, a start-of-the-art nonpara-
metric Bayesian regression technique (Rasmussen
and Williams, 2006)2. As reported on a similar
task by Cohn and Specia (2013), and confirmed by
our preliminary experiments, GP regression signifi-
cantly outperforms popular techniques such as sup-
2Code available at http://www.gaussianprocess.org/gpml/
172
port vector regression and least-squares linear re-
gression. We also follow their settings for GP, em-
ploying GP regression with a squared exponential
kernel with automatic relevance determination. De-
pending on the number of users and amount of train-
ing data available for each user, models may be
trained separately for each user (as we do here), or
in a combined fashion via multi-task learning as pro-
posed by Cohn and Specia (2013).
It is also crucial for the predictions to be reliable
throughout the whole relevant space of segments.
If the cost of certain types of segments is system-
atically underpredicted, the segmentation algorithm
might be misled to prefer these, possibly a large
number of times.3 An effective trick to prevent such
underpredictions is to predict the log time instead of
the actual time. In this way, errors in the critical low
end are penalized more strongly, and the time can
never become negative.
4.2 Error Correction Modeling
As one utility measure, we can use the number of
errors corrected, a useful measure for post editing
tasks over automatically produced annotations. In
order to measure how many errors can be removed
by supervising a particular segment, we must es-
timate both how many errors are in the automatic
annotation, and how reliably a human can remove
these for a given supervision mode.
Most machine learning techniques can estimate
confidence scores in the form of posterior probabil-
ities. To estimate the number of errors, we can sum
over one minus the posterior for all tokens, which
estimates the Hamming distance from the reference
annotation. This measure is appropriate for tasks in
which the number of tokens is fixed in advance (e.g.
a part-of-speech estimation task), and a reasonable
approximation for tasks in which the number of to-
kens is not known in advance (e.g. speech transcrip-
tion, cf. Section 5.1.1).
Predicting the particular tokens at which a human
will make a mistake is known to be a difficult task
(Olson and Olson, 1990), but a simplifying constant
3For instance, consider a model that predicts well for seg-
ments of medium size or longer, but underpredicts the supervi-
sion time of single-token segments. This may lead the segmen-
tation algorithm to put every token into its own segment, which
is clearly undesirable.
human error rate can still be useful. For example,
in the task from Section 2, we may suspect a certain
number of errors in a transcript segment, and predict,
say, 95% of those errors to be removed via typing,
but only 85% via respeaking.
4.3 Classifier Improvement Modeling
Another reasonable utility measure is accuracy of a
classifier trained on the data we choose to annotate
in an active learning framework. Confidence scores
have been found useful for ranking particular tokens
with regards to how much they will improve a clas-
sifier (Settles, 2008). Here, we may similarly score
segment utility as the sum of its token confidences,
although care must be taken to normalize and cali-
brate the token confidences to be linearly compara-
ble before doing so. While the resulting utility score
has no interpretation in absolute terms, it can still be
used as an optimization objective (cf. Section 5.2.1).
5 Experiments
In this section, we present experimental results ex-
amining the effectiveness of the proposed method
over two tasks: speech transcription and Japanese
word segmentation.4
5.1 Speech Transcription Experiments
Accurate speech transcripts are a much-demanded
NLP product, useful by themselves, as training ma-
terial for ASR, or as input for follow-up tasks like
speech translation. With recognition accuracies
plateauing, manually correcting (post editing) auto-
matic speech transcripts has become popular. Com-
mon approaches are to identify words (Sanchez-
Cortina et al., 2012) or (sub-)sentences (Sperber et
al., 2013) of low confidence, and have a human edi-
tor correct these.
5.1.1 Experimental Setup
We conduct a user study in which participants
post-edited speech transcripts, given a fixed goal
word error rate. The transcription setup was such
that the transcriber could see the ASR transcript of
parts before and after the segment that he was edit-
ing, providing context if needed. When imprecise
time alignment resulted in segment breaks that were
4Software and experimental data can be downloaded from
http://www.msperber.com/research/tacl-segmentation/
173
slightly ?off,? as happened occasionally, that context
helped guess what was said. The segment itself was
transcribed from scratch, as opposed to editing the
ASR transcript; besides being arguably more effi-
cient when the ASR transcript contains many mis-
takes (Nanjo et al., 2006; Akita et al., 2009), prelim-
inary experiments also showed that supervision time
is far easier to predict this way. Figure 4 illustrates
what the setup looked like.
We used a self-developed transcription tool to
conduct experiments. It presents our computed seg-
ments one by one, allows convenient input and play-
back via keyboard shortcuts, and logs user interac-
tions with their time stamps. A selection of TED
talks5 (English talks on technology, entertainment,
and design) served as experimental data. While
some of these talks contain jargon such as medi-
cal terms, they are presented by skilled speakers,
making them comparably easy to understand. Initial
transcripts were created using the Janus recognition
toolkit (Soltau et al., 2001) with a standard, TED-
optimized setup. We used confusion networks for
decoding and obtaining confidence scores.
For reasons of simplicity, and better compara-
bility to our baseline, we restricted our experiment
to two supervision modes: TYPE and SKIP. We
conducted experiments with 3 participants, 1 with
several years of experience in transcription, 2 with
none. Each participant received an explanation on
the transcription guidelines, and a short hands-on
training to learn to use our tool. Next, they tran-
scribed a balanced selection of 200 segments of
varying length and quality in random order. This
data was used to train the user models.
Finally, each participant transcribed another 2
TED talks, with word error rate (WER) 19.96%
(predicted: 22.33%). We set a target (predicted)
WER of 15% as our optimization constraint,6 and
minimize the predicted supervision time as our ob-
jective function. Both TED talks were transcribed
once using the baseline strategy, and once using the
proposed strategy. The order of both strategies was
reversed between talks, to minimize learning bias
due to transcribing each talk twice.
The baseline strategy was adopted according to
5www.ted.com
6Depending on the level of accuracy required by our final
application, this target may be set lower or higher.
Sperber et al. (2013): We segmented the talk into
natural, subsentential units, using Matusov et al.
(2006)?s segmenter, which we tuned to reproduce
the TED subtitle segmentation, producing a mean
segment length of 8.6 words. Segments were added
in order of increasing average word confidence, until
the user model predicted a WER<15%. The second
segmentation strategy was the proposed method,
similarly with a resource constraint of WER<15%.
Supervision time was predicted via GP regres-
sion (cf. Section 4.1), using segment length, au-
dio duration, and mean confidence as input features.
The output variable was assumed subject to addi-
tive Gaussian noise with zero mean, a variance of
5 seconds was chosen empirically to minimize the
mean squared error. Utility prediction (cf. Section
4.2) was based on posterior scores obtained from
the confusion networks. We found it important to
calibrate them, as the posteriors were overconfident
especially in the upper range. To do so, we automat-
ically transcribed a development set of TED data,
grouped the recognized words into buckets accord-
ing to their posteriors, and determined the average
number of errors per word in each bucket from an
alignment with the reference transcript. The map-
ping from average posterior to average number of
errors was estimated via GP regression. The result
was summed over all tokens, and multiplied by a
constant human confidence, separately determined
for each participant.7
5.1.2 Simulation Results
To convey a better understanding of the poten-
tial gains afforded by our method, we first present a
simulated experiment. We assume a transcriber who
makes no mistakes, and needs exactly the amount of
time predicted by a user model trained on the data of
a randomly selected participant. We compare three
scenarios: A baseline simulation, in which the base-
line segments are transcribed in ascending order of
confidence; a simulation using the proposed method,
in which we change the WER constraint in small in-
crements; finally, an oracle simulation, which uses
7More elaborate methods for WER estimation exist, such as
by Ogawa et al. (2013), but if our method achieves improve-
ments using simple Hamming distance, incorporating more so-
phisticated measures will likely achieve similar, or even better
accuracy.
174
(3) SKIP: ?nineteen forty six until today you see the green?
(4) TYPE: <annotator types: ?is the traditional?>
(5) SKIP: ?Interstate conflict?
(6) TYPE: <annotator types: ?the ones we used to?>
(7) SKIP: . . .
Figure 4: Result of our segmentation method (excerpt).
TYPE segments are displayed empty and should be tran-
scribed from scratch. For SKIP segments, the ASR tran-
script is displayed to provide context. When annotating a
segment, the corresponding audio is played back.
0 10 20 30 40 50 60
0
5
10
15
20
25
Post editing time [min]
R
es
ul
tin
g 
W
ER
 [%
]
 
 
Baseline
Proposed
Oracle
Figure 5: Simulation of post editing on example TED
talk. The proposed method reduces the WER consider-
ably faster than the baseline at first, later both converge.
The much superior oracle simulation indicates room for
further improvement.
the proposed method, but uses a utility model that
knows the actual number of errors in each segment.
For each supervised segment, we simply replace the
ASR output with the reference, and measure the re-
sulting WER.
Figure 5 shows the simulation on an example
TED talk, based on an initial transcript with 21.9%
WER. The proposed method is able to reduce the
WER faster than the baseline, up to a certain point
where they converge. The oracle simulation is even
faster, indicating room for improvement through
better confidence scores.
5.1.3 User Study Results
Table 1 shows the results of the user study. First,
we note that the WER estimation by our utility
model was off by about 2.5%: While the predicted
improvement in WER was from 22.33% to 15.0%,
the actual improvement was from 19.96% to about
12.5%. The actual resulting WER was consistent
Participant Baseline ProposedWER Time WER Time
P1 12.26 44:05 12.18 33:01
P2 12.75 36:19 12.77 29:54
P3 12.70 52:42 12.50 37:57
AVG 12.57 44:22 12.48 33:37
Table 1: Transcription task results. For each user, the
resultingWER [%] after supervision is shown, along with
the time [min] they needed. The unsupervised WER was
19.96%.
across all users, and we observe strong, consistent
reductions in supervision time for all participants.
Prediction of the necessary supervision time was ac-
curate: Averaged over participants, 45:41 minutes
were predicted for the baseline, 44:22 minutes mea-
sured. For the proposed method, 32:11 minutes were
predicted, 33:37 minutes measured. On average,
participants removed 6.68 errors per minute using
the baseline, and 8.93 errors per minute using the
proposed method, a speed-up of 25.2%.
Note that predicted and measured values are not
strictly comparable: In the experiments, to provide
a fair comparison participants transcribed the same
talks twice (once using baseline, once the proposed
method, in alternating order), resulting in a notice-
able learning effect. The user model, on the other
hand, is trained to predict the case in which a tran-
scriber conducts only one transcription pass.
As an interesting finding, without being informed
about the order of baseline and proposed method,
participants reported that transcribing according to
the proposed segmentation seemed harder, as they
found the baseline segmentation more linguistically
reasonable. However, this perceived increase in dif-
ficulty did not show in efficiency numbers.
5.2 Japanese Word Segmentation Experiments
Word segmentation is the first step in NLP for lan-
guages that are commonly written without word
boundaries, such as Japanese and Chinese. We ap-
ply our method to a task in which we domain-adapt a
word segmentation classifier via active learning. In
this experiment, participants annotated whether or
not a word boundary occurred at certain positions in
a Japanese sentence. The tokens to be grouped into
segments are positions between adjacent characters.
175
5.2.1 Experimental Setup
Neubig et al. (2011) have proposed a pointwise
method for Japanese word segmentation that can be
trained using partially annotated sentences, which
makes it attractive in combination with active learn-
ing, as well as our segmentation method. The
authors released their method as a software pack-
age ?KyTea? that we employed in this user study.
We used KyTea?s active learning domain adaptation
toolkit8 as a baseline.
For data, we used the Balanced Corpus of Con-
temporary Written Japanese (BCCWJ), created by
Maekawa (2008), with the internet Q&A subcor-
pus as in-domain data, and the whitepaper subcor-
pus as background data, a domain adaptation sce-
nario. Sentences were drawn from the in-domain
corpus, and the manually annotated data was then
used to train KyTea, along with the pre-annotated
background data. The goal (objective function) was
to improve KyTea?s classification accuracy on an in-
domain test set, given a constrained time budget of
30 minutes. There were again 2 supervision modes:
ANNOTATE and SKIP. Note that this is essentially a
batch active learning setup with only one iteration.
We conducted experiments with one expert with
several years of experience with Japanese word seg-
mentation annotation, and three non-expert native
speakers with no prior experience. Japanese word
segmentation is not a trivial task, so we provided
non-experts with training, including explanation of
the segmentation standard, a supervised test with
immediate feedback and explanations, and hands-on
training to get used to the annotation software.
Supervision time was predicted via GP regression
(cf. Section 4.1), using the segment length and mean
confidence as input features. As before, the output
variable was assumed subject to additive Gaussian
noise with zero mean and 5 seconds variance. To ob-
tain training data for these models, each participant
annotated about 500 example instances, drawn from
the adaptation corpus, grouped into segments and
balanced regarding segment length and difficulty.
For utility modeling (cf. Section 4.3), we first nor-
malized KyTea?s confidence scores, which are given
in terms of SVM margin, using a sigmoid function
(Platt, 1999). The normalization parameter was se-
8http://www.phontron.com/kytea/active.html
lected so that the mean confidence on a development
set corresponded to the actual classifier accuracy.
We derive our measure of classifier improvement for
correcting a segment by summing over one minus
the calibrated confidence for each of its tokens. To
analyze how well this measure describes the actual
training utility, we trained KyTea using the back-
ground data plus disjoint groups of 100 in-domain
instances with similar probabilities and measured
the achieved reduction of prediction errors. The cor-
relation between each group?s mean utility and the
achieved error reduction was 0.87. Note that we ig-
nore the decaying returns usually observed as more
data is added to the training set. Also, we did not
attempt to model user errors. Employing a con-
stant base error rate, as in the transcription scenario,
would change segment utilities only by a constant
factor, without changing the resulting segmentation.
After creating the user models, we conducted the
main experiment, in which each participant anno-
tated data that was selected from a pool of 1000
in-domain sentences using two strategies. The first,
baseline strategy was as proposed by Neubig et al.
(2011). Queries are those instances with the low-
est confidence scores. Each query is then extended
to the left and right, until a word boundary is pre-
dicted. This strategy follows similar reasoning as
was the premise to this paper: To decide whether or
not a position in a text corresponds to a word bound-
ary, the annotator has to acquire surrounding context
information. This context acquisition is relatively
time consuming, so he might as well label the sur-
rounding instances with little additional effort. The
second strategy was our proposed, more principled
approach. Queries of both methods were shuffled
to minimize bias due to learning effects. Finally, we
trained KyTea using the results of both methods, and
compared the achieved classifier improvement and
supervision times.
5.2.2 User Study Results
Table 2 summarizes the results of our experi-
ment. It shows that the annotations by each partic-
ipant resulted in a better classifier for the proposed
method than the baseline, but also took up consider-
ably more time, a less clear improvement than for
the transcription task. In fact, the total error for
time predictions was as high as 12.5% on average,
176
Participant Baseline ProposedTime Acc. Time Acc.
Expert 25:50 96.17 32:45 96.55
NonExp1 22:05 95.79 26:44 95.98
NonExp2 23:37 96.15 31:28 96.21
NonExp3 25:23 96.38 33:36 96.45
Table 2: Word segmentation task results, for our ex-
pert and 3 non-expert participants. For each participant,
the resulting classifier accuracy [%] after supervision is
shown, along with the time [min] they needed. The unsu-
pervised accuracy was 95.14%.
where the baseline method tended take less time than
predicted, the proposed method more time. This is
in contrast to a much lower total error (within 1%)
when cross-validating our user model training data.
This is likely due to the fact that the data for train-
ing the user model was selected in a balanced man-
ner, as opposed to selecting difficult examples, as
our method is prone to do. Thus, we may expect
much better predictions when selecting user model
training data that is more similar to the test case.
Plotting classifier accuracy over annotation time
draws a clearer picture. Let us first analyze the re-
sults for the expert annotator. Figure 6 (E.1) shows
that the proposed method resulted in consistently
better results, indicating that time predictions were
still effective. Note that this comparison may put the
proposed method at a slight disadvantage by com-
paring intermediate results despite optimizing glob-
ally.
For the non-experts, the improvement over the
baseline is less consistent, as can be seen in Fig-
ure 6 (N.1) for one representative. According to
our analysis, this can be explained by two factors:
(1) The non-experts? annotation error (6.5% on av-
erage) was much higher than the expert?s (2.7%),
resulting in a somewhat irregular classifier learn-
ing curve. (2) The variance in annotation time
per segment was consistently higher for the non-
experts than the expert, indicated by an average
per-segment prediction error of 71% vs. 58% rela-
tive to the mean actual value, respectively. Infor-
mally speaking, non-experts made more mistakes,
and were more strongly influenced by the difficulty
of a particular segment (which was higher on av-
erage with the proposed method, as indicated by a
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
0 10 20 30
0.955
0.965
 
 
Prop.
Basel
N.1E.1 N.2E.2 N.3E.3 N.4E.4
Annotation time [min.]
Classifier Accu
racy
.
Figure 6: Classifier improvement over time, depicted for
the expert (E) and a non-expert (N). The graphs show
numbers based on (1) actual annotations and user mod-
els as in Sections 4.1 and 4.3, (2) error-free annotations,
(3) measured times replaced by predicted times, and (4)
both reference annotations and replaced time predictions.
lower average confidence).9
In Figures 6 (2-4) we present a simulation experi-
ment in which we first pretend as if annotators made
no mistakes, then as if they needed exactly as much
time as predicted for each segment, and then both.
This cheating experiment works in favor of the pro-
posed method, especially for the non-expert. We
may conclude that our segmentation approach is ef-
fective for the word segmentation task, but requires
more accurate time predictions. Better user models
will certainly help, although for the presented sce-
nario our method may be most useful for an expert
annotator.
9Note that the non-expert in the figure annotated much faster
than the expert, which explains the comparable classification
result despite making more annotation errors. This is in contrast
to the other non-experts, who were slower.
177
5.3 Computational Efficiency
Since our segmentation algorithm does not guar-
antee polynomial runtime, computational efficiency
was a concern, but did not turn out problematic.
On a consumer laptop, the solver produced seg-
mentations within a few seconds for a single docu-
ment containing several thousand tokens, and within
hours for corpora consisting of several dozen doc-
uments. Runtime increased roughly quadratically
with respect to the number of segmented tokens. We
feel that this is acceptable, considering that the time
needed for human supervision will likely dominate
the computation time, and reasonable approxima-
tions can be made as noted in Section 3.2.
6 Relation to Prior Work
Efficient supervision strategies have been studied
across a variety of NLP-related research areas, and
received increasing attention in recent years. Ex-
amples include post editing for speech recogni-
tion (Sanchez-Cortina et al., 2012), interactive ma-
chine translation (Gonza?lez-Rubio et al., 2010), ac-
tive learning for machine translation (Haffari et al.,
2009; Gonza?lez-Rubio et al., 2011) and many other
NLP tasks (Olsson, 2009), to name but a few studies.
It has also been recognized by the active learn-
ing community that correcting the most useful parts
first is often not optimal in terms of efficiency, since
these parts tend to be the most difficult to manually
annotate (Settles et al., 2008). The authors advocate
the use of a user model to predict the supervision ef-
fort, and select the instances with best ?bang-for-the-
buck.? This prediction of supervision effort was suc-
cessful, and was further refined in other NLP-related
studies (Tomanek et al., 2010; Specia, 2011; Cohn
and Specia, 2013). Our approach to user modeling
using GP regression is inspired by the latter.
Most studies on user models consider only super-
vision effort, while neglecting the accuracy of hu-
man annotations. The view on humans as a perfect
oracle has been criticized (Donmez and Carbonell,
2008), since human errors are common and can
negatively affect supervision utility. Research on
human-computer-interaction has identified the mod-
eling of human errors as very difficult (Olson and
Olson, 1990), depending on factors such as user ex-
perience, cognitive load, user interface design, and
fatigue. Nevertheless, even the simple error model
used in our post editing task was effective.
The active learning community has addressed the
problem of balancing utility and cost in some more
detail. The previously reported ?bang-for-the-buck?
approach is a very simple, greedy approach to com-
bine both into one measure. A more theoretically
founded scalar optimization objective is the net ben-
efit (utility minus costs) as proposed by Vijaya-
narasimhan and Grauman (2009), but unfortunately
is restricted to applications where both can be ex-
pressed in terms of the same monetary unit. Vijaya-
narasimhan et al. (2010) and Donmez and Carbonell
(2008) use a more practical approach that specifies a
constrained optimization problem by allowing only
a limited time budget for supervision. Our approach
is a generalization thereof and allows either specify-
ing an upper bound on the predicted cost, or a lower
bound on the predicted utility.
The main novelty of our presented approach is
the explicit modeling and selection of segments of
various sizes, such that annotation efficiency is opti-
mized according to the specified constraints. While
some works (Sassano and Kurohashi, 2010; Neubig
et al., 2011) have proposed using subsentential seg-
ments, we are not aware of any previous work that
explicitly optimizes that segmentation.
7 Conclusion
We presented a method that can effectively choose
a segmentation of a language corpus that optimizes
supervision efficiency, considering not only the ac-
tual usefulness of each segment, but also the anno-
tation cost. We reported noticeable improvements
over strong baselines in two user studies. Future user
experiments with more participants would be desir-
able to verify our observations, and allow further
analysis of different factors such as annotator ex-
pertise. Also, future research may improve the user
modeling, which will be beneficial for our method.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n 287658 Bridges Across the Language
Divide (EU-BRIDGE).
178
References
Yuya Akita, Masato Mimura, and Tatsuya Kawahara.
2009. Automatic Transcription System for Meetings
of the Japanese National Congress. In Interspeech,
pages 84?87, Brighton, UK.
Trevor Cohn and Lucia Specia. 2013. Modelling Anno-
tator Bias with Multi-task Gaussian Processes: An Ap-
plication to Machine Translation Quality Estimation.
In Association for Computational Linguistics Confer-
ence (ACL), Sofia, Bulgaria.
Pinar Donmez and Jaime Carbonell. 2008. Proactive
Learning : Cost-Sensitive Active Learning with Mul-
tiple Imperfect Oracles. In Conference on Information
and Knowledge Management (CIKM), pages 619?628,
Napa Valley, CA, USA.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2010. Balancing User Effort and
Translation Error in Interactive Machine Translation
Via Confidence Measures. In Association for Compu-
tational Linguistics Conference (ACL), Short Papers
Track, pages 173?177, Uppsala, Sweden.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart??nez, and Fran-
cisco Casacuberta. 2011. An active learning scenario
for interactive machine translation. In International
Conference on Multimodal Interfaces (ICMI), pages
197?200, Alicante, Spain.
Gurobi Optimization. 2012. Gurobi Optimizer Refer-
ence Manual.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active Learning for Statistical Phrase-based
Machine Translation. In North American Chapter
of the Association for Computational Linguistics -
Human Language Technologies Conference (NAACL-
HLT), pages 415?423, Boulder, CO, USA.
Stefan Irnich and Guy Desaulniers. 2005. Shortest Path
Problems with Resource Constraints. In Column Gen-
eration, pages 33?65. Springer US.
Kikuo Maekawa. 2008. Balanced Corpus of Contem-
porary Written Japanese. In International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 101?102, Hyderabad, India.
R. Timothy Marler and Jasbir S. Arora. 2004. Survey
of multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369?395, April.
EvgenyMatusov, ArneMauser, and Hermann Ney. 2006.
Automatic Sentence Segmentation and Punctuation
Prediction for Spoken Language Translation. In Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 158?165, Kyoto, Japan.
Hiroaki Nanjo, Yuya Akita, and Tatsuya Kawahara.
2006. Computer Assisted Speech Transcription Sys-
tem for Efficient Speech Archive. In Western Pacific
Acoustics Conference (WESPAC), Seoul, Korea.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise Prediction for Robust , Adapt-
able Japanese Morphological Analysis. In Associa-
tion for Computational Linguistics: Human Language
Technologies Conference (ACL-HLT), pages 529?533,
Portland, OR, USA.
Atsunori Ogawa, Takaaki Hori, and Atsushi Naka-
mura. 2013. Discriminative Recognition Rate Esti-
mation For N-Best List and Its Application To N-Best
Rescoring. In International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pages 6832?
6836, Vancouver, Canada.
Judith Reitman Olson and Gary Olson. 1990. The
Growth of Cognitive Modeling in Human-Computer
Interaction Since GOMS. Human-Computer Interac-
tion, 5(2):221?265, June.
Fredrik Olsson. 2009. A literature survey of active ma-
chine learning in the context of natural language pro-
cessing. Technical report, SICS Sweden.
David Pisinger. 1994. A Minimal Algorithm for the
Multiple-Choice Knapsack Problem. European Jour-
nal of Operational Research, 83(2):394?410.
John C. Platt. 1999. Probabilistic Outputs for Sup-
port Vector Machines and Comparisons to Regularized
Likelihood Methods. In Advances in Large Margin
Classifiers, pages 61?74. MIT Press.
Carl E. Rasmussen and Christopher K.I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT
Press, Cambridge, MA, USA.
Isaias Sanchez-Cortina, Nicolas Serrano, Alberto San-
chis, and Alfons Juan. 2012. A prototype for Inter-
active Speech Transcription Balancing Error and Su-
pervision Effort. In International Conference on Intel-
ligent User Interfaces (IUI), pages 325?326, Lisbon,
Portugal.
Manabu Sassano and Sadao Kurohashi. 2010. Using
Smaller Constituents Rather Than Sentences in Ac-
tive Learning for Japanese Dependency Parsing. In
Association for Computational Linguistics Conference
(ACL), pages 356?365, Uppsala, Sweden.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active Learning with Real Annotation Costs. In
Neural Information Processing Systems Conference
(NIPS) - Workshop on Cost-Sensitive Learning, Lake
Tahoe, NV, United States.
Burr Settles. 2008. An Analysis of Active Learning
Strategies for Sequence Labeling Tasks. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1070?1079, Honolulu, USA.
Hagen Soltau, Florian Metze, Christian Fu?gen, and Alex
Waibel. 2001. A One-Pass Decoder Based on Poly-
morphic Linguistic Context Assignment. In Auto-
matic Speech Recognition and Understanding Work-
179
shop (ASRU), pages 214?217, Madonna di Campiglio,
Italy.
Lucia Specia. 2011. Exploiting Objective Annota-
tions for Measuring Translation Post-editing Effort. In
Conference of the European Association for Machine
Translation (EAMT), pages 73?80, Nice, France.
Matthias Sperber, Graham Neubig, Christian Fu?gen,
Satoshi Nakamura, and Alex Waibel. 2013. Efficient
Speech Transcription Through Respeaking. In Inter-
speech, pages 1087?1091, Lyon, France.
Bernhard Suhm, Brad Myers, and Alex Waibel. 2001.
Multimodal error correction for speech user inter-
faces. Transactions on Computer-Human Interaction,
8(1):60?98.
Evimaria Terzi and Panayiotis Tsaparas. 2006. Efficient
algorithms for sequence segmentation. In SIAM Con-
ference on Data Mining (SDM), Bethesda, MD, USA.
Katrin Tomanek and Udo Hahn. 2009. Semi-Supervised
Active Learning for Sequence Labeling. In Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP), pages 1039?1047, Singapore.
Katrin Tomanek, Udo Hahn, and Steffen Lohmann.
2010. A Cognitive Cost Model of Annotations Based
on Eye-Tracking Data. In Association for Compu-
tational Linguistics Conference (ACL), pages 1158?
1167, Uppsala, Sweden.
Paolo Toth and Daniele Vigo. 2001. The Vehicle Routing
Problem. Society for Industrial & Applied Mathemat-
ics (SIAM), Philadelphia.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2009. Whats It Going to Cost You?: Predicting Ef-
fort vs. Informativeness for Multi-Label Image Anno-
tations. In Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 2262?2269, Miami
Beach, FL, USA.
Sudheendra Vijayanarasimhan, Prateek Jain, and Kristen
Grauman. 2010. Far-sighted active learning on a bud-
get for image and video recognition. In Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 3035?3042, San Francisco, CA, USA, June.
180
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221?224,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Modeling Spoken Decision Making Dialogue
and Optimization of its Dialogue Strategy
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi Nakamura
MASTAR Project, NICT
Kyoto, Japan.
teruhisa.misu@nict.go.jp
Abstract
This paper presents a spoken dialogue frame-
work that helps users in making decisions.
Users often do not have a definite goal or cri-
teria for selecting from a list of alternatives.
Thus the system has to bridge this knowledge
gap and also provide the users with an appro-
priate alternative together with the reason for
this recommendation through dialogue. We
present a dialogue state model for such deci-
sion making dialogue. To evaluate this model,
we implement a trial sightseeing guidance sys-
tem and collect dialogue data. Then, we opti-
mize the dialogue strategy based on the state
model through reinforcement learning with a
natural policy gradient approach using a user
simulator trained on the collected dialogue
corpus.
1 Introduction
In many situations where spoken dialogue interfaces
are used, information access by the user is not a goal in
itself, but a means for decision making (Polifroni and
Walker, 2008). For example, in a restaurant retrieval
system, the user?s goal may not be the extraction of
price information but to make a decision on candidate
restaurants based on the retrieved information.
This work focuses on how to assist a user who is
using the system for his/her decision making, when
he/she does not have enough knowledge about the tar-
get domain. In such a situation, users are often un-
aware of not only what kind of information the sys-
tem can provide but also their own preference or fac-
tors that they should emphasize. The system, too, has
little knowledge about the user, or where his/her inter-
ests lie. Thus, the system has to bridge such gaps by
sensing (potential) preferences of the user and recom-
mend information that the user would be interested in,
considering a trade-off with the length of the dialogue.
We propose a model of dialogue state that consid-
ers the user?s preferences as well as his/her knowledge
about the domain changing through a decision making
dialogue. A user simulator is trained on data collected
with a trial sightseeing system. Next, we optimize
the dialogue strategy of the system via reinforcement
learning (RL) with a natural policy gradient approach.
2 Spoken decision making dialogue
We assume a situation where a user selects from a given
set of alternatives. This is highly likely in real world
situations; for example, the situation wherein a user se-
lects one restaurant from a list of candidates presented
Choose the optimal spot
1. Cherry 
Blossoms
2. Japanese
Garden
3. Easy
Access
Kinkakuji-
Temple
Ryoanji-
Temple
Nanzenji-
Temple
?
?????
Goal
Criteria
Alternatives
(choices)
?????
p1 p2 p3
v11 v12 v13
? ?
Figure 1: Hierarchy structure for sightseeing guidance
dialogue
by a car navigation system. In this work, we deal with
a sightseeing planning task where the user determines
the sightseeing spot to visit, with little prior knowledge
about the target domain. The study of (Ohtake et al,
2009), which investigated human-human dialogue in
such a task, reported that such consulting usually con-
sists of a sequence of information requests from the
user, presentation and elaboration of information about
certain spots by the guide followed by the user?s evalu-
ation. We thus focus on these interactions.
Several studies have featured decision support sys-
tems in the operations research field, and the typical
method that has been employed is the Analytic Hierar-
chy Process (Saaty, 1980) (AHP). In AHP, the problem
is modeled as a hierarchy that consists of the decision
goal, the alternatives for achieving it, and the criteria
for evaluating these alternatives. An example hierarchy
using these criteria is shown in Figure 1.
For the user, the problem of making an optimal de-
cision can be solved by fixing a weight vector P
user
=
(p
1
, p
2
, . . . , p
M
) for criteria and local weight matrix
V
user
= (v
11
, v
12
, . . . , v
1M
, . . . , v
NM
) for alterna-
tives in terms of the criteria. The optimal alternative
is then identified by selecting the spot k with the maxi-
mum priority of
?
M
m=1
p
m
v
km
. In typical AHP meth-
ods, the procedure of fixing these weights is often con-
ducted through pairwise comparisons for all the possi-
ble combinations of criteria and spots in terms of the
criteria, followed by weight tuning based on the re-
sults of these comparisons (Saaty, 1980). However, this
methodology cannot be directly applied to spoken dia-
logue systems. The information about the spot in terms
of the criteria is not known to the users, but is obtained
only via navigating through the system?s information.
In addition, spoken dialogue systems usually handle
several candidates and criteria, making pairwise com-
parison a costly affair.
We thus consider a spoken dialogue framework that
estimates the weights for the user?s preference (po-
tential preferences) as well as the user?s knowledge
221
about the domain through interactions of information
retrieval and navigation.
3 Decision support system with spoken
dialogue interface
The dialogue system we built has two functions: an-
swering users? information requests and recommend-
ing information to them. When the system is requested
to explain about the spots or their determinants, it ex-
plains the sightseeing spots in terms of the requested
determinant. After satisfying the user?s request, the
system then provides information that would be helpful
in making a decision (e.g., instructing what the system
can explain, recommending detailed information of the
current topic that the user might be interested in, etc.).
Note that the latter is optimized via RL (see Section 4).
3.1 Knowledge base
Our back-end DB consists of 15 sightseeing spots as al-
ternatives and 10 determinants described for each spot.
We select determinants that frequently appear in the di-
alogue corpus of (Ohtake et al, 2009) (e.g. cherry blos-
soms, fall foliage). The spots are annotated in terms of
these determinants if they apply to them. The value of
the evaluation e
nm
is ?1? when the spot n applies to the
determinant m and ?0? when it does not.
3.2 System initiative recommendation
The content of the recommendation is determined
based on one of the following six methods:
1. Recommendation of determinants based on the
currently focused spot (Method 1)
This method is structured on the basis of the user?s
current focus on a particular spot. Specifically, the
system selects several determinants related to the
current spot whose evaluation is ?1? and presents
them to the user.
2. Recommendation of spots based on the cur-
rently focused determinant (Method 2)
This method functions on the basis of the focus on
a certain specific determinant.
3. Open prompt (Method 3)
The system does not make a recommendation, and
presents an open prompt.
4. Listing of determinants 1 (Method 4)
This method lists several determinants to the user in
ascending order from the low level user knowledge
K
sys
(that the system estimates). (K
sys
, P
sys
, p
m
and Pr(p
m
= 1) are defined and explained in Sec-
tion 4.2.)
5. Listing of determinants 2 (Method 5)
This method also lists the determinants, but the or-
der is based on the user?s high preference P
sys
(that
the system estimates).
6. Recommendation of user?s possibly preferred
spot (Method 6)
The system recommends a spot as well as the de-
terminants that the users would be interested in
based on the estimated preference P
sys
. The sys-
tem selects one spot k with a maximum value of
?
M
m=1
Pr(p
m
= 1) ? e
k,m
. This idea is based
on collaborative filtering which is often used for
recommender systems (Breese et al, 1998). This
method will be helpful to users if the system suc-
cessfully estimates the user?s preference; however,
it will be irrelevant if the system does not.
We will represent these recommendations
through a dialogue act expression, (ca
sys
{sc
sys
}),
which consists of a communicative act ca
sys
and the semantic content sc
sys
. (For exam-
ple Method1{(Spot
5
), (Det
3
,Det
4
,Det5)},
Method3{NULL,NULL}, etc.)
4 Optimization of dialogue strategy
4.1 Models for simulating a user
We introduce a user model that consists of a tuple of
knowledge vector K
user
, preference vector P
user
, and
local weight matrix V
user
. In this paper, for simplic-
ity, a user?s preference vector or weight for determi-
nants P
user
= (p
1
, p
2
, . . . , p
M
) is assumed to con-
sist of binary parameters. That is, if the user is in-
terested in (or potentially interested in) the determi-
nant m and emphasizes it when making a decision,
the preference p
m
is set to ?1?. Otherwise, it is set
to ?0?. In order to represent a state that the user has
potential preference, we introduce a knowledge param-
eter K
user
= (k
1
, k
2
, . . . , k
M
) that shows if the user
has the perception that the system is able to handle or
he/she is interested in the determinants. k
m
is set to
?1? if the user knows (or is listed by system?s recom-
mendations) that the system can handle determinant m
and ?0? when he/she does not. For example, the state
that the determinant m is the potential preference of a
user (but he/she is unaware of that) is represented by
(k
m
= 0, p
m
= 1). This idea is in contrast to previous
research which assumes some fixed goal observable by
the user from the beginning of the dialogue (Schatz-
mann et al, 2007). A user?s local weight v
nm
for spot
n in terms of determinant m is set to ?1?, when the
system lets the user know that the evaluation of spots is
?1? through recommendation Methods 1, 2 and 6.
We constructed a user simulator that is based on
the statistics calculated through an experiment with the
trial system (Misu et al, 2010) as well as the knowl-
edge and preference of the user. That is, the user?s com-
municative act cat
user
and the semantic content sct
user
for the system?s recommendation at
sys
are generated
based on the following equation:
Pr(cat
user
, sct
user
|cat
sys
, sct
sys
,K
user
,P
user
)
= Pr(cat
user
|cat
sys
)
?Pr(sct
user
|K
user
,P
user
, cat
user
, cat
sys
, sct
sys
)
This means that the user?s communicative act ca
user
is sampled based on the conditional probability of
Pr(cat
user
|cat
sys
) in (Misu et al, 2010). The seman-
tic content sc
user
is selected based on the user?s pref-
erence P
user
under current knowledge about the de-
terminants K
user
. That is, the sc is sampled from the
determinants within the user?s knowledge (k
m
= 1)
based on the probability that the user requests the de-
terminant of his/her preference/non-preference, which
is also calculated from the dialogue data of the trial sys-
tem.
4.2 Dialogue state expression
We defined the state expression of the user in the pre-
vious section. However the problem is that for the
system, the state (P
user
,K
user
,V
user
) is not observ-
able, but is only estimated from the interactions with
the user. Thus, this model is a partially observable
Markov decision process (POMDP) problem. In or-
der to estimate unobservable properties of a POMDP
222
 
Priors of the estimated state:
- Knowledge: K
sys
= (0.22, 0.01, 0.02, 0.18, . . . )
- Preference: P
sys
= (0.37, 0.19, 0.48, 0.38, . . . )
Interactions (observation):
- System recommendation:
a
sys
= Method1{(Spot
5
), (Det
1
, Det
3
, Det4)}
- User query:
a
user
= Accept{(Spot
5
), (Det
3
)}
Posterior of the estimated state:
- Knowledge: K
sys
= (1.00, 0.01, 1.00, 1.00, . . . )
- Preference: P
sys
= (0.26, 0.19, 0.65, 0.22, . . . )
User?s knowledge acquisition:
- Knowledge: K
user
? {k
1
= 1, k
3
= 1, k
4
= 1}
- Local weight: V
user
? {v
51
= 1, v
53
= 1, v
54
=
1}
 
Figure 2: Example of state update
and handle the problem as an MDP, we introduce
the system?s inferential user knowledge vector K
sys
or probability distribution (estimate value) K
sys
=
(Pr(k
1
= 1), P r(k
2
= 1), . . . , P r(k
M
= 1)) and
that of preference P
sys
= (Pr(p
1
= 1), P r(p
2
=
1), . . . , P r(p
M
= 1)).
The dialogue state DSt+1 or estimated user?s dia-
logue state of the step t+1 is assumed to be dependent
only on the previous state DSt, as well as the interac-
tions It = (at
sys
, at
user
).
The estimated user?s state is represented as a prob-
ability distribution and is updated by each interac-
tion. This corresponds to representing the user types
as a probability distribution, whereas the work of (Ko-
matani et al, 2005) classifies users to several discrete
user types. The estimated user?s preference P
sys
is up-
dated when the system observes the interaction It. The
update is conducted based on the following Bayes? the-
orem using the previous state DSt as a prior.
Pr(p
m
= 1|It) =
Pr(I
t
|p
m
=1)Pr(p
m
=1)
Pr(I
t
|p
m
=1)Pr(p
m
=1)+Pr(I
t
|(p
m
=0))Pr(1?Pr(p
m
=1))
Here, Pr(It|p
m
= 1), P r(It|(p
m
= 0) to the right
side was obtained from the dialogue corpus of (Misu et
al., 2010). This posterior is then used as a prior in the
next state update using interaction It+1. An example
of this update is illustrated in Figure 2.
4.3 Reward function
The reward function that we use is based on the num-
ber of agreed attributes between the user preference
and the decided spot. Users are assumed to determine
the spot based on their preference P
user
under their
knowledge K
user
(and local weight for spots V
user
)
at that time, and select the spot k with the maximum
priority of
?
m
k
k
? p
k
? v
km
. The reward R is then
calculated based on the improvement in the number of
agreed attributes between the user?s actual (potential)
preferences and the decided spot k over the expected
agreement by random spot selection.
R =
M
?
m=1
p
m
? e
k,m
?
1
N
N
?
n=1
M
?
m=1
p
m
? e
n,m
For example, if the decided spot satisfies three prefer-
ences and the average agreement of the agreement by
random selection is 1.3, then the reward is 1.7.
4.4 Optimization by reinforcement learning
The problem of system recommendation generation is
optimized through RL. The MDP (S, A, R) is defined
as follows. The state parameter S = (s
1
, s
2
, . . . , s
I
) is
generated by extracting the features of the current dia-
logue state DSt. We use the following 29 features 1.
1. Parameters that indicate the # of interactions from
the beginning of the dialogue. This is approximated by
five parameters using triangular functions. 2. User?s
previous communicative act (1 if at?1
user
= x
i
, other-
wise 0). 3. System?s previous communicative act (1 if
at?1
sys
= y
j
, otherwise 0). 4. Sum of the estimated user
knowledge about determinants (?N
n=1
Pr(k
n
= 1)).
5. Number of presented spot information. 6. Expecta-
tion of the probability that the user emphasizes the de-
terminant in the current state (Pr(k
n
= 1)? Pr(p
n
=
1)) (10 parameters). The action set A consists of the
six recommendation methods shown in subsection 3.2.
Reward R is given by the reward function of subsection
4.3.
A system action a
sys
(ca
sys
) is sampled based on the
following soft-max (Boltzmann) policy.
?(a
sys
= k|S) = Pr(a
sys
= k|S,?)
=
exp(
?
I
i=1
s
i
? ?
ki
)
?
J
j=1
exp(
?
I
i=1
s
i
? ?
ji
)
Here, ? = (?
11
, ?
12
, . . . ?
1I
, . . . , ?
JI
) consists of J (#
actions) ? I (# features) parameters. The parameter
?
ji
works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. This ? is the target of optimization by RL.
We adopt the Natural Actor Critic (NAC) (Peters and
Schaal, 2008), which adopts a natural policy gradient
method as the policy optimization method.
4.5 Experiment by dialogue simulation
For each simulated dialogue session, a simulated user
(P
user
,K
user
,V
user
) is sampled. A preference vector
P
user
of the user is generated so that he/she has four
preferences. As a result, four parameters in P
user
are
?1? and the others are ?0?. This vector is fixed through-
out the dialogue episode. This sampling is conducted
based on the rate proportional to the percentage of users
who emphasize it for making decisions (Misu et al,
2010). The user?s knowledge K
user
is also set based
on the statistics of the ?percentage of users who stated
the determinants before system recommendation?. For
each determinant, we sample a random valuable r that
ranges from ?0? to ?1?, and k
m
is set to ?1? if r is
smaller than the percentage. All the parameters of
local weights V
user
are initialized to ?0?, assuming
that users have no prior knowledge about the candi-
date spots. As for system parameters, the estimated
user?s preference P
sys
and knowledge K
sys
are ini-
tialized based on the statistics of our trial system (Misu
et al, 2010).
We assumed that the system does not misunderstand
the user?s action. Users are assumed to continue a di-
alogue session for 20 turns2, and episodes are sampled
using the policy ? at that time and the user simulator
1Note that about half of them are continuous variables and
that the value function cannot be denoted by a lookup table.
2In practice, users may make a decision at any point once
they are satisfied collecting information. And this is the rea-
son why we list the rewards in the early dialogue stage in
223
Table 1: Comparison of reward with baseline methods
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)
B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)
Table 2: Comparison of reward with discrete dialogue
state expression
Reward (?std)
State T = 5 T = 10 T = 15 T = 20
PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)
Table 3: Effect of estimated preference and knowledge
Reward (?std)
Policy T = 5 T = 10 T = 15 T = 20
Pref+Know0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)
Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)
No Pref or
Know
0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)
of subsection 4.1. In each turn, the system is rewarded
using the reward function of subsection 4.3. The pol-
icy (parameter ?) is updated using NAC in every 2,000
dialogues.
4.6 Experimental result
The policy was fixed at about 30,000 dialogue
episodes. We analyzed the learned dialogue policy by
examining the value of weight parameter ?. We com-
pared the parameters of the trained policy between ac-
tions3. The weight of the parameters that represent the
early stage of the dialogue was large in Methods 4 and
5. On the other hand, the weight of the parameters that
represent the latter stage of the dialogue was large in
Methods 2 and 6. This suggests that in the trained pol-
icy, the system first bridges the knowledge gap between
the user, estimates the user?s preference, and then, rec-
ommends specific information that would be useful to
the user.
Next, we compared the trained policy with the fol-
lowing baseline methods.
1. No recommendation (B1)
The system only provides the requested informa-
tion and does not generate any recommendations.
2. Random recommendation (B2)
The system randomly chooses a recommendation
from six methods.
The comparison of the average reward between the
baseline methods is listed in Table 1. Note that the ora-
cle average reward that can be obtained only when the
user knows all knowledge about the knowledge base
(it requires at least 50 turns) was 1.45. The reward by
the strategy optimized by NAC was significantly better
than that of baseline methods (n = 500, p < .01).
We then compared the proposed method with the
case where estimated user?s knowledge and preference
are represented as discrete binary parameters instead of
probability distributions (PDs). That is, the estimated
user?s preference p
m
of determinant m is set to ?1?
when the user requested the determinant, otherwise it
is ?0?. The estimated user?s knowledge k
m
is set to
the following subsections. In our trial system, the dialogue
length was 16.3 turns with a standard deviation of 7.0 turns.
3The parameters can be interpreted as the size of the con-
tribution for selecting the action.
?1? when the system lets the user know the determi-
nant, otherwise it is ?0?. Another dialogue strategy was
trained using this dialogue state expression. This result
is shown in Table 2. The proposed method that rep-
resents the dialogue state as a probability distribution
outperformed (p < .01 (T=15,20)) the method using a
discrete state expression.
We also compared the proposed method with the
case where either one of estimated preference or
knowledge was used as a feature for dialogue state in
order to carefully investigate the effect of these factors.
In the proposed method, expectation of the probabil-
ity that the user emphasizes the determinant (Pr(k
n
=
1) ? Pr(p
n
= 1)) was used as a feature of dialogue
state. We evaluated the performance of the cases where
the estimated knowledge Pr(k
n
= 1) or estimated
preference Pr(p
n
= 1) was used instead of the expec-
tation of the probability that the user emphasizes the
determinant. We also compared with the case where
no preference/knowledge feature was used. This result
is shown in Table 3. We confirmed that significant im-
provement (p < .01 (T=15,20)) was obtained by taking
into account the estimated knowledge of the user.
5 Conclusion
In this paper, we presented a spoken dialogue frame-
work that helps users select an alternative from a list of
alternatives. We proposed a model of dialogue state for
spoken decision making dialogue that considers knowl-
edge as well as preference of the user and the system,
and its dialogue strategy was trained by RL. We con-
firmed that the learned policy achieved a better recom-
mendation strategy over several baseline methods.
Although we dealt with a simple recommendation
strategy with a fixed number of recommendation com-
ponents, there are many possible extensions to this
model. The system is expected to handle a more com-
plex planning of natural language generation. We also
need to consider errors in speech recognition and un-
derstanding when simulating dialogue.
References
J. Breese, D. Heckerman, and C. Kadie. 1998. ?empirical
analysis of predictive algorithms for collaborative filter-
ing?. In ?Proc. the 14th Annual Conference on Uncer-
tainty in Artificial Intelligence?, pages 43?52.
K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005.
User Modeling in Spoken Dialogue Systems to Generate
Flexible Guidance. User Modeling and User-Adapted In-
teraction, 15(1):169?183.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and
S. Nakamura. 2010. Construction and Experiment of a
Spoken Consulting Dialogue System. In Proc. IWSDS.
K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.
2009. Annotating Dialogue Acts to Construct Dialogue
Systems for Consulting. In Proc. The 7th Workshop on
Asian Language Resources, pages 32?39.
J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neuro-
computing, 71(7-9):1180?1190.
J. Polifroni and M. Walker. 2008. Intensional Summaries
as Cooperative Responses in Dialogue: Automation and
Evaluation. In Proc. ACL/HLT, pages 479?487.
T. Saaty. 1980. The Analytic Hierarchy Process: Planning,
Priority Setting, Resource Allocation. Mcgraw-Hill.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
HLT/NAACL.
224
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 259?265,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Toward Construction of Spoken Dialogue System
that Evokes Users? Spontaneous Backchannels
Teruhisa Misu, Etsuo Mizukami, Yoshinori Shiga, Shinichi Kawamoto?,
Hisashi Kawai and Satoshi Nakamura
National Institute of Information and Communications Technology (NICT), Kyoto, Japan.
teruhisa.misu@nict.go.jp
Abstract
This paper addresses a first step toward a
spoken dialogue system that evokes user?s
spontaneous backchannels. We construct
an HMM-based dialogue-style text-to-speech
(TTS) system that generates human-like cues
that evoke users? backchannels. A spoken
dialogue system for information navigation
was implemented and the TTS was evaluated
in terms of evoked user backchannels. We
conducted user experiments and demonstrated
that the user backchannels evoked by our TTS
are more informative for the system in detect-
ing users? feelings than those by conventional
reading-style TTS.
1 Introduction
One of the most enduring problems in spoken di-
alogue systems research is realizing a natural dia-
logue in a human-human form. One direction re-
searchers have been utilizing spontaneous nonverbal
and paralinguistic information. For example,
This paper focuses on backchannels, one of the
most common forms of para-linguistic information
in human-human dialogue. In particular, we focus
on users? verbal feedback, such as ?uh-huh? (called
Aizuchi in Japanese), and non-verbal feedback in the
form of nods. Such backchannels are very com-
mon phenomena, and considered to be used to fa-
cilitate smooth human-human communications. In
this regard, Maynard (Maynard, 1986) indicated that
such backchannels are listener?s signals to let the
speaker continue speaking (continuer), to indicate
that the listener understands and consents. It was
also hypothesized that humans detect feelings ex-
pressed via backchannels, and the correlation be-
tween backchannel patterns and user interests was
examined (Kawahara et al, 2008). These studies in-
dicate that detection of spontaneous user backchan-
? currently with Japan Advanced Institute of Science and
Technology (JAIST)
nels can benefit spoken dialogue systems by provid-
ing informative cues that reflect the user?s situation.
For instance, if a spoken dialogue system can detect
user?s backchannels, it can facilitate smooth turn-
taking. The system can also detect user?s feelings
and judge if it should continue the current topic or
change it.
Despite these previous studies and decades of
analysis on backchannels, few practical dialogue
systems have made use of them. This is proba-
bly due to the fact that users do not react as spon-
taneously to dialogue systems as they do to other
humans. We presume one of the reasons for this
is the unnatural intonation of synthesized speech.
That is, conventional speech synthesizers do not pro-
vide users with signs to elicit backchannels; an ap-
propriate set of lexical, acoustic and prosodic cues
(or backchannel-inviting cues (A. Gravano and J.
Hirschberg, 2009)), which tends to precede the lis-
tener?s backchannels in human-human communica-
tion. Though recorded human speech can provide
such cues, it is costly to re-record system?s speech
every time system scripts are updated. In this work,
we therefore tackle the challenge of constructing
dialogue-style text-to-speech (TTS) system that in-
spires users to make spontaneous backchannels un-
der the hypothesis of:
People will give more spontaneous backchannels to
a spoken dialogue system that makes more spontaneous
backchannel-inviting cues than a spoken dialogue system
that makes less spontaneous ones.
which is derived from the Media Equation (Reeves
and Nass, 1996).
2 Related Works
A number of studies have aimed at improving
the naturalness of TTS. Though most of these
have focused on means of realizing a clear and
easy-to-listen-to reading-style speech, some at-
tempts have been made at spontaneous conversa-
tional speech. Andersson (Andersson et al, 2010)
and Marge (Marge et al, 2010) focused on lexi-
259
cal phenomena such as lexical filler and acknowl-
edgments in spontaneous speech, and showed that
inserting them improves the naturalness of human-
computer dialogues. In this work, we tackle con-
structing a natural dialogue-style TTS system focus-
ing on prosodic phenomena such as intonation and
phoneme duration.
In the field of conversation analysis, many studies
analyzed backchannels in human-human dialogue
focusing on lexical and non-verbal cues (Koiso et
al., 1998; Ward and Tsukahara, 2000; A. Gravano
and J. Hirschberg, 2009). For instance these cues
were examined in preceding utterances, such as in
part-of-speech tags, length of pause, power contour
pattern, and F
0
contour pattern around the end of
the Inter-Pausal Units (IPUs). (A. Gravano and J.
Hirschberg, 2009) showed that when several of the
above cues occur simultaneously, the likelihood of
occurrence of a backchannel will increase.
Several studies also utilized the above findings
for spoken dialogue systems. Okato (Okato et al,
1996) and Fujie (Fujie et al, 2005) trained models to
predict backchannels, and implemented spoken di-
alogue systems that make backchannels. Our goal
differs in that it is to inspire users to give backchan-
nels.
3 Construction of Spoken Dialogue TTS
3.1 Spoken Dialogue Data collection for TTS
In order to make spontaneous dialogue-style TTS
that can evoke backchannels, we construct a spon-
taneous dialogue-style speech corpus that contains
backchannel-inviting cues, and then train an HMM
acoustic model for synthesis.
We collected our training data by dubbing a script
of our Kyoto Sightseeing Guidance Spoken Dia-
logue Corpus (Misu et al, 2009), a set of itinerary-
planning dialogues in Japanese. In the dialogue
task, the expert guide has made recommendations on
sightseeing spots and restaurants until has decided
on a plan for the day. With the guide?s recommen-
dations, many users give spontaneous backchannels.
We made a set of dialogue scripts from the corpus,
and asked voice actors to act them out.
When preparing the dialogue script for dubbing,
we first removed fillers and backchannels from the
transcripts of the dialogue corpus. We then anno-
tated the guide?s end of the IPUs, where the the
user made backchannels, with #. A sample dialogue
script is shown in Figure 6. We asked two profes-
sional voice actresses to duplicate the spoken dia-
logue of the script, with playing the role of the tour
guide, and the other as the tourist, sitting face-to-
face. During the recording, we asked the tour guide
role to read the scenario with intonation so that the
tourist role would spontaneously make backchan-
nels at the points marked with #. The tourist was
allowed to make backchannels at will at any pause
segments the guide made. We recorded 12 dialogue
sessions in total. The speech data was manually la-
beled, and 239.3 minutes of tour guide utterances,
which are used to train our HMM for the TTS sys-
tem, were collected. The training data is comple-
mented by the ATR 503 phonetically balanced sen-
tence set (Abe et al, 1990), so as to cover deficien-
cies in the phoneme sequence. The sentence set is
collected from news articles, and data consists of
43.1 minutes of reading-style speech.
3.2 Analysis of Collected Speech Data
Before training the HMM, we analyzed the collected
spoken dialogue data to confirm if the recorded di-
alogue speech data contained backchannel-inviting
prosodic cues. We compared prosodic features of
the dialogue speech data with those of the reading-
style speech data (phonetically balanced sentences
that we collected). Following the findings of a pre-
vious study (Koiso et al, 1998), we investigated the
duration, F
0
contour pattern and power contour pat-
tern of the final phoneme of the IPUs1.
In conversation analysis of Japanese, the F
0
con-
tour pattern label of the final phoneme is often used.
While the contour pattern is usually manually la-
beled, we roughly determined the patterns based on
the following procedure. We first normalized the log
F
0
scale using all utterances so that it has zero mean
and one standard deviation (z-score: z = (x??)/?).
We then divided each final phoneme of the IPU into
former and latter parts, and calculated the F
0
slope
of each segment by linear regression. By combina-
tion of following three patterns, we defined nine F
0
contour patterns for the final phonemes of the IPUs.
The pattern of the segment was judged as rise if the
slope was larger than a threshold ?. If the slope was
less than the threshold??, the pattern was judged as
fall. Otherwise, it was judged as flat. Here, ? was
empirically set to 5.0. The power contour patterns
of the IPUs were estimated by a similar procedure.
We analyzed 3,311 IPUs that were not followed
1For this study, we define an IPU as a maximal sequence
of words surrounded by silence longer than 200 ms. This unit
usually coincides with one Japanese phrasal unit.
260
Table 1: Prosodic analysis of final phonemes of IPUs
(dialogue script vs. newsarticle script)
dialogue newsarticle
dur. phoneme [msec] 177.1 (? 83.6) 119.4 (? 31.3)
average (? standard deviation)
F
0
power
pattern dialogue news dialogue news
rise-rise 3.7 % 10.4 % 0.0 % 0.0 %
rise-flat 2.6 % 2.1 % 0.0 % 0.0 %
rise-fall 18.8 % 3.2 % 0.0 % 0.0 %
flat-rise 4.8 % 11.5 % 0.0 % 0.0 %
flat-flat 3.5 % 1.8 % 0.0 % 9.2 %
flat-fall 12.6 % 2.7 % 13.6 % 0.1 %
fall-rise 29.2 % 47.0 % 0.0 % 0.0 %
fall-flat 7.7 % 9.0 % 86.0 % 90.7 %
fall-fall 17.1 % 12.3 % 0.0 % 0.0 %
by a turn-switch in the dialogue-style speech data
and 645 non-sentence-end IPUs in the reading-
style speech data. The prosodic features of final
phonemes of these IPUs are listed in Table 1.
According to a study (Koiso et al, 1998), in which
prosodic features of IPUs followed by a turn-hold
with backchannel, without backchannel and turn-
switch were compared, a long duration in the final
phoneme is a speaker?s typical sign to keep floor.
The same study also reported that the flat-fall and
rise-fall pattern of F
0
and power are more likely
to be followed by a backchannel than a turn-hold
without a backchannel and turn-switch. In our col-
lected speech corpus, there were actually signifi-
cant (p < 0.01) differences in the duration of the
final phoneme between that in the dialogue-style
speech and in reading-style speech. There was
also significant (p < 0.01) difference in the oc-
currence probability of the above two prosodic pat-
terns between dialogue-style speech and reading-
style speech data. These figures indicate that
as a whole the collected dialogue-style data con-
tains more backchannel-inviting cues than collected
reading-style speech data.
We trained HMM for our TTS system Ximera
using the HMM-based Speech Synthesis System
(HTS) (Zen et al, 2007). We adopted mel log spec-
trum approximation (MLSA) filter-based vocod-
ing (SPTK, 2011), a quint-phone-based phoneme
set and five state HMM-based acoustic modeling.
All training data including reading-style speech data
were used for model training.
4 User Experiment
4.1 Dialogue System used for Experiment
To evaluate our TTS system based on users? reac-
tions, a sightseeing guidance spoken dialogue sys-
Figure 1: Screen shot of the dialogue system
tem that assist users in making decision was im-
plemented. The system can explain six sightseeing
spots in Kyoto. The system provides responses to
user requests for explanation about a certain spot.
Each descriptive text on a sightseeing spot consists
of 500 (?1%) characters, 30 phrases. The text is
synthesized using section 3 TTS2. We set the speech
rate of our TTS as nine phoneme per second.
A display is used to present photos of the tar-
get sightseeing spot and an animated 3D desktop
avatar named Hanna. Figure 1 shows the GUI
the user sees. The avatar can express its status
through several motions. For example, when the
user begins speaking, it can express the state of
listening using the listener?s motion, as shown in
the figure. A sample dialogue with the system is
shown in Table 7. A video (with English subtitles)
of an sample dialogue with a user can be seen at
http://mastarpj.nict.go.jp/?xtmisu/video/TTS.wmv.
To compare the effectiveness of our TTS in
evoking users? spontaneous backchannels, we con-
structed a comparison system that adopts a conven-
tional reading-style TTS system. An HMM model
was trained using 10-hour reading-style speech by
another professional female narrator. Other settings,
such as the descriptive text and avatar agent, were
the same as those of the base system.
4.2 Comparison of Prosodic Features of the
Synthesized Speech
Prior to the experiments, we investigated the
prosodic features of the final phoneme of IPUs in
the synthesized explanations on six spots to confirm
if they contain backchannel-inviting cues. The re-
sults are given in Table 2.
Tendencies in the duration of the final phoneme
and prosody pattern distribution of the synthesized
2The descriptive texts are not included in the training data.
261
Table 2: Prosodic analysis of final phonemes of IPUs
(dialogue-style TTS vs. reading-style TTS)
dialogue synth. reading synth.
dur. phoneme [msec] 172.9 (? 29.6) 126.1 (? 19.1)
average (? standard deviation)
F
0
power
pattern dialogue reading dialogue reading
rise-rise 5.4 % 0.0 % 0.0 % 0.0 %
rise-flat 2.0 % 0.0 % 1.7 % 0.0 %
rise-fall 23.5 % 0.0 % 46.3 % 5.3 %
flat-rise 5.0 % 0.0 % 0.0 % 0.0 %
flat-flat 1.7 % 0.0 % 4.0 % 9.2 %
flat-fall 15.8 % 0.0 % 22.8 % 18.1 %
fall-rise 15.8 % 0.0 % 0.7 % 0.0 %
fall-flat 3.4 % 0.0 % 7.0 % 0.0 %
fall-fall 27.5 % 100.0 % 17.4 % 76.5 %
speech by the dialogue-style TTS system were simi-
lar to that of recorded dialogue speech, suggests that
the constructed dialogue-style TTS system can du-
plicate the backchannel-inviting cues of the recorded
original speech. The synthesized dialogue-style
speech also contained much more rise-fall and flat-
fall patterns in F
0
and power than that generated by
the reading-style TTS system. The average dura-
tion of the final phoneme was also longer. Consider-
ing the fact that the speech data was generated from
the same script, this indicates that the synthesized
speech by the dialogue-style TTS system contains
more backchannel-inviting features than that by the
reading-style TTS system.
4.3 Experimental Setup
We evaluated the TTS systems using 30 subjects
who had not previously used spoken dialogue sys-
tems. Subjects were asked to use the dialogue sys-
tem in two settings; dialogue-style TTS system and
reading-style TTS system. The experiment was con-
ducted in a small (about 2 m2) soundproof room
with no one else present.
We instructed the subjects to speak with the avatar
agent Hanna (not with the system). We also told
them that the avatar agent was listening to their
speech at all times using the microphone, and was
observing their reactions using the camera above the
display3. Subjects were given the task of acquiring
information about three candidate sightseeing spots
in Kyoto shown on the display and then selecting
one that they liked. An example dialogue with the
system is shown in Table 7. A video (with English
subtitles) showing a real user dialogue can be seen
at http://mastarpj.nict.go.jp/?xtmisu/video/exp.avi.
3The system did not actually sense the subjects? reactions.
Table 3: Questionnaire items
1. Overall, which speech was better?
2. Which speech had easier-to-understand explanations?
3. For which speech did you feel compelled to give
backchannels?
4. Which speech was more appropriate for this system?
5. Which speech had more human-like explanation?
(a) both
(b) dialogue style
(c) reading style
(d) neither
#5#4
#3#2
#1
Figure 2: Questionnaire results
After the subject selected from candidate spots,
we changed the TTS system settings and instructed
the user to have another dialogue session selecting
one of another three spots. Considering the effects of
the order, the subjects were divided into four groups;
the first group (Group 1) used the system in the order
of ?Spot list A with dialogue-style speech ? Spot
list B with reading-style speech,? the second group
(Group 2) worked in reverse order. Groups 3 and 4
used a system alternating the order of the spot sets.
5 Experimental Results
5.1 Questionnaire Results
After the experiments, subjects were asked to fill in
a questionnaire about the system. Table 3 shows the
questionnaire items. The subjects selected (a) both
are good, (b) dialogue-style speech was better, (c)
reading-style speech was better, or (d) neither were
good. Figure 2 shows the results.
The dialogue-style speech generally earned
higher ratings, but reading-style was slightly higher
in items #2 and #5. This tendency is likely at-
tributable to the fact that the dialogue-style speech
had worse clarity and naturalness than reading-style.
The mean opinion score (MOS), which is often used
to measure clarity and naturalness of TTS, of the
dialogue-style TTS was in fact 2.79, worse than 3.74
for the reading-style.
5.2 Analysis of Frequency of Backchannels
We analyzed the number of backchannels that users
made during the dialogue session. We manually
annotated subjects? verbal feedbacks, such as ?uh-
huh? and nodding of the head using the recorded
video. Out of 30 subjects, 26 gave some form of
262
Table 4: Percentages and average number of users who made backchannels
TTS % users made BCs # average BCs taken
Group 1: (Dialogue? Reading) Dialogue-style 100.0% (50.0%, 100.0%) 30.4 (1.8, 28.6)
(Spot list A? Spot list B) Reading-style 100.0% (50.0%, 87.5%) 26.1 (3.1, 23.0)
Group 2: (Reading? Dialogue) Dialogue-style 75.0% (25.0%, 62.5%) 12.7 (0.5, 12.2)
(Spot list A? Spot list B) Reading-style 75.0% (25.0%, 62.5%) 12.9 (1.3, 11.6)
Group 3: (Dialogue? Reading) Dialogue-style 100.0% (28.6%, 100.0%) 14.0 (0.4, 13.6)
(Spot list B? Spot list A) Reading-style 100.0% (0%, 100.0%) 19.3 (0, 19.3)
Group 4: (Reading? Dialogue) Dialogue-style 87.5% (42.9%, 87.5%) 28.2 (4.7, 23,5)
(Spot list B? Spot list A) Reading-style 100.0% (71.4%, 87.5%) 24.8 (6.5, 18.3)
All: Dialogue-style 86.7% (36.7%, 86.7%) 21.1 (1.7, 19.4)
Reading-style 90.0% (40.0%, 83.3%) 20.6 (2.4, 18.2)
Total backchannel (verbal feedback [Aizuchi], nodding)
backchannel to the system. Table 4 shows the per-
centages and average number of times subjects gave
backchannels. Many users made more backchannels
using the dialogue-style TTS system. Despite the
significant difference in questionnaire item #3, there
were no significant differences in the average num-
ber of users? backchannels.
5.3 Informativeness of Backchannels
We then evaluated the TTS in terms of the informa-
tiveness of evoked backchannels. The spontaneous
prosodic pattern of the backchannels is expected
to suggest positive/negative feelings on regarding
the recommended candidate. One promising use
of backchannels in our application is for detecting
users? feelings about the currently focused on spot,
and choosing to continue the explanation on the cur-
rent topic if the user seems interested, or otherwise
change the topic. We therefore label backchannels
made during the systems explanation of the spot
that the user finally selected as ?positive? and those
made during the explanations of the other two spots
as ?negative? and consider distinguishing between
them. In human-human dialogues, it was confirmed
that when a user responds promptly, the majority of
responses are positive, and more backchannels also
suggest positive responses (Kawahara et al, 2008).
We investigated the informativeness of the
backchannels based on their classification rate, or
whether the system can distinguish positive and neg-
ative backchannels, using 10-fold cross-validation.
That is, the backchannels evoked by the dialogue-
style TTS system were divided into 10 groups and
nine were used for training and the other for classi-
fication tests. We trained decision trees using J4.8
algorithm using timing, frequency, total frequency
throughout the session and type of backchannel (ver-
bal feedback or nod) as the feature set. The classifi-
cation error cost of the positive sample was set to (#
negative samples / # positive samples) considering
the difference in the number of positive and nega-
tive samples. Ten trials were conducted by chang-
ing the test set and the average classification rate
was calculated. The classification rate of backchan-
nels evoked by the system with dialogue-style TTS
was 71.4%, The confusion matrix of the classifi-
cation is shown below. We obtained precisions of
62.8% in the classification of the positive backchan-
nels, and 73.2% in that of the negative backchan-
nels. The rates are significantly higher than chance
rates of 33.5% and 66.5%. This result indicates
the backchannels evoked by the dialogue-style TTS
were informative for the system.
Table 5: Confusion matrix of classification
? classified as positive negative
? label
positive 76 141
negative 45 386
The classification rate of the reading-style TTS
system was calculated in the same way. The av-
erage classification rate of backchannels evoked by
reading-style TTS was a significantly lower 47.4%,
meaning they were not informative at all.
These results suggest that our dialogue-style TTS
system can evoke more spontaneous and informative
backchannels that reflects users? intentions than the
conventional reading-style one. This classification
rate is not completely satisfactory, but we expect that
users? feeling can be detected after observing several
backchannels. We also believe that we can estimate
users? interest more precisely by combining verbal
information of dialogue acts (Misu et al, 2010).
6 Conclusions
This paper presented our first steps toward a spoken
dialogue system that evokes users? spontaneous lis-
tener?s reactions. We constructed a dialogue-style
TTS and confirmed that by generating human-like
backchannel-inviting cues, the system can evoke
user?s spontaneous backchannels, which are infor-
mative for the system.
263
References
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proc. In-
terspeech, pages 1019?1022.
M. Abe, Y. Sagisaka, T. Umeda, and H. Kuwabara. 1990.
Speech Database User?s Manual. ATR Technical Re-
port TR-I-0166.
S. Andersson, K. Georgila, D. Traum, and R. Clark
M. Aylett. 2010. Prediction and Realisation of Con-
versational Characteristics by Utilising Spontaneous
Speech for Unit Selection. In Proc. Speech Prosody.
S. Fujie, K. Fukushima, and T. Kobayashi. 2005. Back-
channel feedback generation using linguistic and non-
linguistic information and its application to spoken di-
alogue system. In Proc. Interspeech, pages 889?892.
T. Kawahara, M. Toyokura, T. Misu, and C. Hori. 2008.
Detection of Feeling Through Back-Channels in Spo-
ken Dialogue. In Proc. Interspeech, pages 1696?1696.
H. Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, and
Y. Den. 1998. An Analysis of Turn-Taking and
Backchannels based on Prosodic and Syntactic Fea-
tures in Japanese Map Task Dialogue. Language and
Speech, 41(3-4):295?322.
M. Marge, J. Miranda, A. Black, and A. I. Rudnicky.
2010. Towards Improving the Naturalness of Social
Conversations with Dialogue Systems. In Proc. SIG-
DIAL, pages 91?94.
S. Maynard. 1986. On back-channel behavior in
japanese and english casual conversation. Linguistics,
24(6):1079?1108.
T. Misu, K. Ohtake, C. Hori, H. Kashioka, and S. Naka-
mura. 2009. Annotating Communicative Function
and Semantic Content in Dialogue Act for Construc-
tion of Consulting Dialogue Systems. In Proc. Inter-
speech.
Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake, Chiori
Hori, Hideki Kashioka, Hisashi Kawai, and Satoshi
Nakamura. 2010. Dialogue Strategy Optimization
to Assist User?s Decision for Spoken Consulting Di-
alogue Systems. In Proc. IEEE-SLT, pages 342?347.
Y. Okato, K. Kato, M. Yamamoto, and S. Itahashi. 1996.
Insertion of interjectory response based on prosodic
information. In Proc. of IEEE Workshop Interac-
tive Voice Technology for Telecommunication Applica-
tions, pages 85?88.
B. Reeves and C. Nass. 1996. The Media Equation: How
People Treat Computers, Television, and New Media
Like Real People and Places. Cambridge University
Press.
SPTK, 2011. Speech Signal Processing Toolkit (SPTK).
http://sp-tk.sourceforge.net/.
N. Ward and W. Tsukahara. 2000. Prosodic fea-
tures which cue backchannel responses in English and
Japanese. Journal of Pragmatics, 32(8):1177?1207.
H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A.W.
Black, and K. Tokuda. 2007. The HMM-based speech
synthesis system version 2.0. In Proc. ISCA SSW6.
A Appendix
264
Table 6: Example dialogue scenario used for recording (translation of Japanese)
Guide And Kyomizu-dera.
Guide Yeah, it is very famous for.
Guide There is this like wooden balcony#, and you can?t see the whole city#, but you can see a
good part of the city from there. And it is a beautiful view.
User It is nice. You have been there?
Guide Yes, many a times.
Guide Unfortunately, in winter it is not very green#, and there are not any flowers.
User So if there is snow will they close the stairs or will they - we are just going to be careful
walking the steps?
Guide You have to be careful going up.
Guide Basically, it?s a slope it?s a slope#, so if it?s very icy - if there is, you know, like black
ice#, you might want to rethink hiking up that slope. But usually it is okay# in the winter.
I mean, Kyomizu-dera and this whole area, like every tourist eventually makes their way
here#, whether they are Japanese or foreign.
Guide So, I am sure the paths should be clear.
Guide Yeah, that is a good spot too.
Table 7: Example dialogue with the system (translation of Japanese)
System Hello, my name is Hanna, a tour guide for sightseeing in Kyoto. Nice to meet you. Please
say ?Okay? when you have decided which spot you would like to know about first.
User Okay.
System Which spot would you like to know about?
User Tell me about Shinnyo-do temple.
System Sure, I?ll explain Shinnyo-do temple.
System Shinnyo-do is a Buddhist Tendai Temple in Sakyo-ku, Kyoto City and the formal name is
Shinsho Gokuraku-ji. You may take the bus directly from Kyoto Station for about thirty
minutes, and the fee is two hundred twenty yen. The admission fee for the temple is five
hundred yen. The temple was established in nine hundred eighty-four A.D., when Amida
Nyorai-zo; a principle image from Mt. Hiei was placed in the Imperial villa of this area.
System Shall I continue?
User Sure.
System The temple is famous, especially for its coloring of autumn leaves; Red Maple stand on both
sides of the stone-paved approach, and it appears as though the temple is placed inside a
maple forest. . . .
. . .
System That?s all about this spot.
System Which spot would you like to know about next?
User Daitoku-ji please.
. . .
265
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 88?96,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Linguistic and Acoustic Features for Automatic Identification of Autism
Spectrum Disorders in Children?s Narrative
Hiroki Tanaka, Sakriani Sakti, Graham Neubig, Tomoki Toda, Satoshi Nakamura
Graduate School of Information Science, Nara Institute of Science and Technology
{hiroki-tan, ssakti, neubig, tomoki, s-nakamura}@is.naist.jp
Abstract
Autism spectrum disorders are develop-
mental disorders characterised as deficits
in social and communication skills, and
they affect both verbal and non-verbal
communication. Previous works measured
differences in children with and without
autism spectrum disorders in terms of
linguistic and acoustic features, although
they do not mention automatic identifi-
cation using integration of these features.
In this paper, we perform an exploratory
study of several language and speech fea-
tures of both single utterances and full nar-
ratives. We find that there are charac-
teristic differences between children with
autism spectrum disorders and typical de-
velopment with respect to word categories,
prosody, and voice quality, and that these
differences can be used in automatic clas-
sifiers. We also examine the differences
between American and Japanese children
and find significant differences with re-
gards to pauses before new turns and lin-
guistic cues.
1 Introduction
Autism spectrum disorders (ASD) are develop-
mental disorders, first described by Kanner and
Asperger in 1943 and 1944 respectively (Kanner,
1943; Asperger, 1944). The American Psychi-
atric Association defines the two characteristics of
ASD as: 1) persistent deficits in social communi-
cation and social interaction across multiple con-
texts, and 2) restricted, repetitive patterns of be-
havior, interests, or activities (American Psychi-
atric Association, 2013). In particular, the former
deficits in social communication are viewed as the
most central characteristic of ASD. Thus, quanti-
fying the degree of social communication skills is
a necessary component of understanding the na-
ture of ASD, creating systems for automatic ASD
screening, and early intervention methods such as
social skills training and applied behaviour analy-
sis (Wallace et al., 1980; Lovaas et al., 1973).
There are a number of studies finding differ-
ences between people with ASD and people with
typical development (TD). In terms of deficits in
social communication, there have been reports de-
scribing atypical usage of gestures (Ashley and
Inge-Marie, 2010), frequency of eye-contact and
laughter (Geraldine et al., 1990), prosody (Mc-
Cann and Peppe, 2003; Rhea et al., 2005), voice
quality (Asgari et al., 2013), delay responses
(Heeman et al., 2010), and unexpected words
(Rouhizadeh et al., 2013). In this paper, we par-
ticularly focus on the cues of ASD that appear in
children?s language and speech
In the case of language, Newton et al. (2009)
analyze blogs of people with ASD and TD, and
found that people with ASD have larger variation
of usage of words describing social processes, al-
though there are no significant differences in other
word categories. In the case of speech, people with
ASD tend to have prosody that differs from that
of their peers (Kanner, 1943), although McCann
and Peppe (2003) note that prosody in ASD is an
under-researched area and that where research has
been undertaken, findings often conflict. Since
then, there have been various studies analyzing
and modeling prosody in people with ASD (Daniel
et al., 2012; Kiss et al., 2013; Santen et al., 2013;
Van et al., 2010). For example, Kiss et al. (2012)
find several significant differences in the pitch
characteristics of ASD, and report that automatic
classification utilizing these features achieves ac-
curacy well above chance level. To our knowl-
edge, there is no previous work integrating both
language and speech features to identify differ-
ences between people with ASD and TD. How-
ever, it has been noted that differences in person-
88
ality traits including introversion/extroversion can
be identified using these features (Mairesse et al.,
2007).
In this paper, we perform a comprehensive anal-
ysis of language and speech features mentioned in
previous works, as well as novel features specific
to this work. In addition, while previous works an-
alyzed differences between people with ASD and
TD, we additionally investigate whether it is possi-
ble to automatically distinguish between children
with ASD or TD using both language and speech
features and a number of classification methods.
We focus on narratives, where the children serving
as our subjects tell a memorable story to their par-
ent (Davis et al., 2004). Here, the use of narrative
allows us to consider not only single-sentence fea-
tures, but also features considering interaction as-
pects between the child and parent such as pauses
before new turns and overall narrative-specific fea-
tures such as words per minute and usage of un-
expected words. Given this setting, we perform
a pilot study examining differences between chil-
dren with ASD and TD, the possibilities of auto-
matic classification between ASD and TD, and the
differences between American and Japanese chil-
dren.
2 Data Description
As a target for our analysis, we first collected a
data set of interactions between Japanese children
and their parents. In collecting the data, we fol-
lowed the procedure used in the creation of the
USC Rachel corpus (Mower et al., 2011). The data
consists of four sessions: doh (free play), jenga (a
game), narrative, and natural conversation. The
first child-parent interaction is free play with the
parent. The child and parent are given play doh,
Mr. Potato Head, and blocks. The second child-
parent interaction is a jenga game. Jenga is a game
in which the participants must remove blocks, one
at a time, from a tower. The game ends when the
tower falls. The third child-parent interaction is a
narrative task. The child and parent are asked to
explain stories in which they experienced a mem-
orable emotion. The final child-parent interaction
is a natural conversation without a task. These
child-parent interactions are recorded and will en-
able comparison of the child?s interaction style and
communication with their parent. Each session
continues for 10 minutes. During interaction, a pin
microphone and video camera record the speech
and video of the child and the parent.
In this paper, we use narrative data of four chil-
dren with ASD (male: 3, female: 1) and two
children with TD (male: 1, female: 1) as an ex-
ploratory study. The intelligence quotient (IQ) for
all subjects is above 70, which is often used as
a threshold for diagnosis of intellectual disabil-
ity. Each subject?s age and diagnosis as ASD/TD
is provided in Table 1. In the narrative session,
each child and parent speaks ?a memorable story?
for 5 minutes in turn, and the listener responds to
the speaker?s story by asking questions. After 5
minutes, the experimenter provides directions to
change the turn.
Table 1: Subjects? age and diagnosis
Subject A1 A2 A3 A4 T1 T2
Age 10 10 10 13 10 12
Diagnosis ASD ASD ASD ASD TD TD
In this paper, we analyze the child-speaking turn
of the narrative session in which the parent re-
sponds to the child?s utterances. All utterances are
transcribed based on USC Rachel corpus manual
(Mower et al., 2011) to facilitate comparison with
this existing corpus. In the transcription manual, if
the speaker pauses for more than one second, the
speech is transcribed as separate utterances. In this
paper, we examine two segment levels, the first
treating each speech segment independently, and
the second handling a whole narrative as the tar-
get. When handling each segment independently,
we use a total of 116 utterances for both children
with ASD and TD.
3 Single Utterance Level
In this section, we describe language and speech
features and analysis of these characteristics to-
wards automatic classification of utterances based
on whether they were spoken by children with
ASD or TD. We hypothesize that based on the fea-
tures extracted from the speech signal we are ca-
pable to classify children with ASD and TD on a
speech segment level, as well as on narrative level
after temporally combining all the segment-based
decisions.
3.1 Feature Extraction
We extract language and speech features based
on those proposed by (Mairesse et al., 2007) and
89
(Hanson, 1995). Extracted features are summa-
rized in Table 2. We also add one feature not cov-
ered in previous work counting the number of oc-
currences of laughter.
Table 2: Description of language and speech fea-
tures.
Language Features
Words per sentence (WPS)
General descriptor Words with more than 6 letters
Occurrences of laughter
Sentence structure
Percentage of pronouns, conjunctions,
negations, quantifiers, numbers
Psychological proc.
Percentage of words describing social,
affect, cognitive, perceptual,
and biological
Personal concerns
Percentage of words describing work,
achievement, leisure, and home
Paralinguistic
Percentage of assent,
disfluencies, and fillers
Speech Features
Pitch Statistics of sd and cov
Intensity Statistics of sd and cov
Speech rate Words per voiced second
Amplitude of a3
Voice quality Difference of the h1 and the h2
Difference of the h1 and the a3
3.1.1 Language Features
We use the linguistic inquiry and word count
(LIWC) (Pennebaker et al., 2007), which is a tool
to categorize words, to extract language features.
Because a Japanese version of LIWC is not avail-
able and there is no existing similar resource for
Japanese, we implement the following procedures
to automatically establish correspondences be-
tween LIWC categories and transcribed Japanese
utterances. First, we use Mecab
1
for part-of-
speech tagging in Japanese utterances, translate
each word into English using the WWWJDIC
2
dictionary, and finally determine the LIWC cate-
gory corresponding to the English word. Among
the language features described in Table 2, we
calculate sentence structures, psychological pro-
cesses, and personal concerns using LIWC, and
other features using Mecab. Here, we do not
consider language-dependent features and subcat-
egories of LIWC.
1
https://code.google.com/p/mecab/
2
http://www.edrdg.org/cgi-bin/wwwjdic/wwwjdic?1C
3.1.2 Speech Features
For speech feature extraction, we use the Snack
sound toolkit
3
. Here, we consider fundamental
frequency, power, and voice quality, which are ef-
fective features according to previous works (Mc-
Cann and Peppe, 2003; Hanson, 1995). We do
not extract mean values of fundamental frequency
and power because those features are strongly re-
lated to individuality. Thus, we extract statistics
of standard deviation (fsd, psd) and coefficient of
variation (fcov, pcov) for fundamental frequency
and power. We calculate speech rate, which is a
feature dividing the number of words by the num-
ber of voiced seconds. Voice quality is also com-
puted using: the amplitude of the third formant
(a3), the difference between the first harmonic and
the second harmonic (h1h2), and the difference
between the first harmonic and the third formant
(h1a3) (Hanson, 1995).
3.1.3 Projection Normalization
For normalization, we simply project all feature
values to a range of [0, 1], where 0 corresponds
to the smallest observed value and 1 to the largest
observed value across all utterances. For utterance
i, we define the value of the jth feature as v
ij
and
define p
ij
=
v
ij
?min
j
max
j
?min
j
, where p
ij
is the feature
value after normalisation.
3.2 Characteristics of Language and Speech
Features
In this section, we report the result of a t-test, prin-
cipal component analysis, factor analysis, and de-
cision tree using the normalised features. We use
R
4
for statistical analysis.
Table 3 shows whether utterances of children
with ASD or TD have a greater mean on the cor-
responding feature. The results indicate that the
children with ASD more frequently use words
with more than 6 letters (e.g. complicated words),
assent (e.g. ?uh-huh,? or ?un? in Japanese), and
fillers (e.g. ?umm,? or ?eh? in Japanese) signif-
icantly more than the children with TD. In con-
trast, the children with TD more frequently use the
words words categorized as social (e.g. friend), af-
fect (e.g. enjoy), and cognitive (e.g. understand)
significantly more than the children with ASD. In
addition, there are differences in terms of funda-
mental frequency variations and voice quality (e.g.
3
http://www.speech.kth.se/snack/
4
http://www.r-project.org
90
Table 3: Difference of mean values between ASD and TD based on language and speech features from
children?s utterances. Each table cell notes which of the two classes has the greater mean on the corre-
sponding feature (*: p < 0.01, **: p < 0.005).
WPS 6 let. laughter adverb pronoun conjunctions negations quantifiers numbers social
- ASD* - - - - - - - TD**
affect cognitive perceptual biological relativity work achievement leisure home assent
TD** TD* - - - - - - - ASD**
nonfluent fillers fsd fcov psd pcov speech rate a3 h1h2 h1a3
- ASD* TD** TD* - - - - - ASD**
h1a3). In particular, we observe that the children
with ASD tend to use monotonous intonation as
reported in (Kanner, 1943). We do not confirm a
significant differences in other features.
Next, we use principal component analysis and
factor analysis to find features that have a large
contribution based on large variance values. As
a result of principal component analysis, features
about fundamental frequency, power, and h1a3
have large variance in the first component, and the
feature counting perceptual words also has large
value in the second component. To analyze a dif-
ferent aspect of principal component analysis with
rotated axes, we use factor analysis with the vari-
max rotation method. Figure 1 shows the result of
factor analysis indicating that features regarding
fundamental frequency and power have large vari-
ance. In addition, other features such as speech
rate, a3, and h1a3 also have large variance. Here,
we can see that for features such as statistics of
fundamental frequency (fsd and fcov) and power
(psd and pcov), the correlation coefficient between
these features are over 80% (p < 0.01). For cor-
related features, we use only standard deviation in
the following sections.
We also analyze important features to distin-
guish between children with ASD and TD by us-
ing a decision tree. Figure 2 shows the result of a
decision tree with 10 leaves indicating that speech
features fill almost all of the leaves (e.g. fsd is a
most useful feature to distinguish between ASD
and TD). In terms of the language features, we
confirm that WPS and perceptual words are im-
portant for classification.
3.3 Classification
In this section, we examine the possibility of au-
tomatic identification of whether an utterance be-
longs to a speaker with ASD or TD. Based on
the previous analysis, we prepare the following
Figure 1: Factor analysis with varimax rotation
method. First and second factors are indicated.
feature sets: 1) language features (Language), 2)
speech features (Speech), 3) all features (All), 4)
important features according to the t-test, princi-
pal component analysis, factor analysis, and de-
cision tree (Selected), 5) important features ac-
cording to the t-test that are not highly correlated
(T-Uncor). The feature set of T-Uncor is as fol-
lows: 6 let., social, affect, cognitive, fillers, as-
sent, fed, and h1a3. We also show the chance
rate, which is a baseline of 50% because the num-
ber of utterances in each group is the same, and
measure accuracy with 10-fold cross-validation
and leave-one-speaker-out cross-validation using
naive Bayes (NB) and support vector machines
with a linear kernel (SVM). In the case of leave-
one-speaker-out cross-validation, we use T-Uncor
because the number of utterances without one
speaker is too small to train using high dimen-
sional feature sets.
Table 4 shows the result indicating that accu-
91
racies with almost all feature sets and classifiers
are over 65%. The SVM with Selected achieves
the best performance for the task of 10-fold cross-
validation, and The SVM with T-Uncor achieves
66.7% for the task of leave-one-speaker-out. The
accuracy for the task of leave-one-speaker-out on
each speaker A1 to T2 is as follows: 78%, 60%,
53%, 51%, 82%, and 78%.
Table 4: Accuracy using Naive Bayes and SVM
classifiers. The p-value of the t-test is measured
compared to baseline (chance rate) (?: p < 0.1, *:
p < 0.01)
Feature set Accuracy [%]
Baseline NB SVM
Language 62.2? 70.3*
Speech 57.6 67.6*
All 50.0 65.0? 68.8*
Selected 67.4* 71.9*
T-Uncor 67.8? 68.1?
Per-Speaker 50.0 65.5? 66.7?
4 Narrative Level
In this section, we focus on the features of en-
tire narratives, which allows us to examine other
features of child-parent interaction for a better un-
derstanding of ASD and classification in children
with ASD and TD. Each following subsection de-
scribes the procedure of feature extraction and
analysis of characteristics at the narrative level.
We consider pauses before new turns and unex-
pected words, which are mentioned in previous
works, as well as words per minute.
4.1 Pauses Before New Turns
Heeman et al., (2010) reported that children with
ASD tend to delay responses to their parent more
than children with TD in natural conversation. In
this paper, we examine whether a similar result is
found in interactive narrative. We denote values
of pauses before new turns as time between the
end of the parent?s utterance and the start of the
child?s utterance. We do not consider overlap of
utterances. We test goodness of fit of pauses to a
gamma and an exponential distribution based on
(Theodora et al., 2013), because the later is a spe-
cial case of gamma with a unity shape parameter,
using the Kolmogorov-Smirnov test.
Figure 3 shows a fitting of pauses to gamma
or exponential distributions, and we select a bet-
2 4 6 8 10
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
Pauses before new turns (sec)
E
x
p
o
n
e
n
t
i
a
l
/
G
a
m
m
a
 
p
r
o
b
a
b
i
l
i
t
y
 
v
a
l
u
e
s
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
0
0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
TD
ASD
Figure 3: Gamma/Exponential pause distributions
with parameters computed using Maximum Like-
lihood Estimation (MLE) for children with ASD
and TD.
ter fitted distribution. All subjects significantly fit
(p > 0.6). As shown in Figure 3, we confirm that
children with ASD tend to delay responses to their
parent compared with children with TD. To reflect
this information in our following experiments in
automatic identification of ASD in narrative, we
extract the expectation value of the exponential
distribution
Heeman et al., (2010) also reported the rela-
tionship of the parent?s previous utterance?s type
(question or non-question) and the child?s pauses.
We examine the relationship between the parent?s
previous question?s type and pauses before new
turns. For each of the children?s utterances, we
label the parent?s utterance that directly precedes
as either ?open question,? ?closed question,? or
?non-question?, and we calculate pause latency.
Closed-questions are those which can be answered
by a simple ?yes? or ?no,? while open-questions
are those which require more thought and more
than a simple one-word answer. As shown in Table
5, children with ASD tend to delay responses to
their parent to a greater extent than children with
TD. We found no difference between open and
closed questions, although a difference between
questions and non-questions is observed. These
results are consistent with those of previous work
(Heeman et al., 2010) in terms of differences be-
tween questions and non-questions.
92
|fsd < 0.366375
fcov < 0.308899
WPS < 0.0543478
fcov < 0.204553
psd < 0.306304
pcov < 0.46429
fsd < 0.513756
perceptual < 0.07
pcov < 0.230634
a
t
t
a a
t
a
t
a
a
Figure 2: Decision tree with 10 leaves (a: ASD, t: TD).
Table 5: Relationship of pauses before new turns
and parents? question types. The mean value and
standard deviation are shown.
Question type TD ASD
Closed-question 0.47 (0.46) 1.61 (1.87)
Open-question 0.43 (0.34) 1.76 (1.51)
Non-question 0.95 (1.18) 2.60 (3.64)
4.2 Words Per Minute
We analyze words per minute (WPM) in children
with ASD and TD to clarify the relationship be-
tween ASD and frequency of speech. We use a
total of 5 minutes of data in each narrative, and
thus the total number of words are divided by 5 to
calculate WPM. Table 6 shows the result. The data
in this table indicates that some children with ASD
have a significantly lower speaking rate than oth-
ers with TD, but it is not necessarily the case that
ASD will result in a low speaking rate such as the
case of Asperger?s syndrome (Asperger, 1944).
4.3 Unexpected Words
Characteristics of ASD include deficits in social
communication, and these deficits affect inappro-
Table 6: Mean value of words per minute.
Subj. Averaged WPM
A1 18.25
A2 86.75
A3 23.75
A4 115.5
T1 99.25
T2 103.5
priate usage of words (Rouhizadeh et al., 2013).
We evaluate these unexpected words using two
measures, term frequency-inverse document fre-
quency (TF-IDF) and log odds ratio. We use
the following formulation to calculate TF-IDF for
each child?s narrative i and each word in that nar-
rative j, where c
ij
is the count of word j in narra-
tive i. f
j
is the number of narratives from the full
data of child narratives containing that word j, and
D is the total number of narratives (Rouhizadeh et
al., 2013).
tf ? idf
ij
= (1 + log c
ij
) log
D
f
j
The log odds ratio, another measure used in in-
93
formation retrieval and extraction tasks, is the ratio
between the odds of a particular word, j, appear-
ing in a child?s narrative, i. Letting the probabil-
ity of a word appearing in a narrative be p
1
and
the probability of that word appearing in all other
narratives be p
2
, we can express the odds ratio as
follows:
odds ratio =
odds(p
1
)
odds(p
2
)
=
p
1
/(1? p
1
)
p
2
/(1? p
2
)
A large TF-IDF and log odds score indicates
that the word j is very specific to the narrative
i, which in turn suggests that the word might be
unexpected or inappropriate. In addition, because
the overall amount of data included in the narra-
tives is too small to robustly analyze these statis-
tics for all words, we also check for the presence
of each word in Japanese WordNet
5
and deter-
mine that if it exists in WordNet it is likely a com-
mon (expected) word. Table 7 shows the result
of TF-IDF, log odds ratio, and their summation,
and we confirm that there is no difference between
children with ASD and TD. This result is differ-
ent from that of previous work (Rouhizadeh et al.,
2013). The children in that study were all telling
the same story, and one possible explanation for
this is due to the fact that in this work we do
not use language-constricted data such as narrative
retelling, and thus differences due to individuality
are more prevalent.
Table 7: TF-IDF, log odds ratio, and their summa-
tion.
Subj. TF-IDF Log-odds T+L
A1 0.50 1.01 1.52
A2 0.58 0.49 1.08
A3 0.66 1.23 1.89
A4 0.66 0.31 0.96
T1 0.74 0.49 1.23
T2 0.62 0.44 1.06
4.4 Classification
In this section, we examine the possibility of auto-
matic classification of whether an interactive nar-
rative belongs to children with ASD or TD. Be-
cause of the total number of subjects is small (n=4
for ASD, n=2 for TD), we perform classification
5
http://www.omomimi.com/wnjpn/
with a K-NN classifier with K=1 nearest neigh-
bour. As features, we compute the features men-
tioned in Section 3.1, and use the average over all
utterances as the features for the entire narrative.
Finally, we use pauses before new turns (expecta-
tion value of the exponential distribution), WPM,
TF-IDF, log odds ratio, 6 let., social, affect, cogni-
tive, assent, fillers, fsd, h1a3, and calculate accu-
racy with leave-one-speaker-out cross-validation.
As a result, we achieved an accuracy of 100%
in classification between ASD and TD on the full-
narrative level, which shows that these features
are effective to some extent to distinguish children
with ASD and TD. However, with only a total of 6
children, our sample size is somewhat small, and
thus experiments with a larger data set will be nec-
essary to draw more firm conclusions.
5 Data Comparison
As all our preceding experiments have been per-
formed on data for Japanese child-parent pairs, it
is also of interest to compare these results with
data of children and parents from other cultures.
In particular, we refer to the USC Rachel corpus
(Mower et al., 2011) (the subjects are nine chil-
dren with ASD) for comparison. Using the USC
Rachel corpus, there is a report mentioning the re-
lationship of parent?s and child?s linguistic infor-
mation and pauses before new turns (Theodora et
al., 2013). In this paper, we follow this work us-
ing Japanese data. The USC Rachel corpus in-
cludes a session of child-parent interaction, and
the same transcription standard is used. We ex-
tract pauses before new turns, and short and long
pauses are differentiated based on the 70th per-
centile of latency values for each child individu-
ally. We investigate the relationship between the
parent and child?s language information based on
features used in Section 3.1, and short and long
pauses.
Table 8 and 9 show significantly greater mean
values performed using bootstrap significance
testing on the means of the two pause types. By
observing the values in the table, we can see
that the trends are similar for both American and
Japanese children. However, in terms of WPS,
there is a difference. The American ASD chil-
dren have greater means for WPS in the case of
long pauses, while Japanese children have greater
means for WPS in the case of short pauses. We
analyze these differences in detail.
94
Table 8: In the case of USC Rachel corpus, boot-
strap on difference of means between short (S) and
long (L) pauses based on linguistic features from
child?s and parent?s utterances (?: p < 0.1, *: p <
0.01). Each table cell notes which of the two types
of pauses has greater mean on the corresponding
feature.
Subj.
Child Parent
WPS conj. affect nonflu. adverb cogn. percept.
S1 L* L* S* - L* L* L*
S2 L* L* S? L* L* L* L*
S3 L* L? - S? L* L* L*
S4 - - - L* L* L* L*
S5 L? - - - L* L* L*
S6 L* - S* - L* L* -
S7 L? - S? - L? - -
S8 L* - - - L* L* L*
S9 - - - S? L* L* L*
Table 9: Bootstrap for pause differences in the
Japanese corpus.
Subj.
Child Parent
WPS conj. affect nonflu. adverb cogn. percept.
A1 S* - - - S* L* -
A2 S? - S* - L* L* L*
A3 S? - - - L* L* L*
A4 S* - - - - - -
In the Japanese corpus, we observe that WPS is
larger in the case of short pauses. As we noticed
that the child often utters only a single word for
responses that follow a long pause, we analyzed
the content of these single word utterances. As
shown in Figure 4, for example, A1 tends to use
a word related to assent when latency is long, and
A4 tends to use a word related to filler, assent or
others when latency is long. Though there are in-
dividual differences, we confirm that the Japanese
children with ASD examined in this study tend
to delay their responses before uttering one word.
These characteristics may be related to the parent?s
question types and the child?s cognitive process,
and thus we need to examine these possibilities in
detail.
6 Conclusion
In this work, we focused on differentiation of chil-
dren with ASD and TD in terms of social com-
munication, particularly focusing on language and
speech features. Using narrative data, we exam-
ined several features on both the single utterance
A1 A2 A3 A4
Others
Laugh
Filler
Assent
Subject
P
e
r
c
e
n
t
a
g
e
 
o
f
 
o
n
e
?
w
o
r
d
 
r
e
s
p
o
n
c
e
s
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
Figure 4: The language category of one-word re-
sponses in the case of a long pause.
level and the narrative level. We examined fea-
tures mentioned in a number of previous works, as
well as a few novel features. We confirmed about
70% accuracy in an evaluation over single utter-
ances, and some narrative features also proved to
have a correlation with ASD.
For future directions, we plan to perform larger
scale experiments to examine the potential of these
features for automated ASD screening. Given the
results of this, we plan to move to applications in-
cluding the development of dialogue systems for
automatic ASD screening and social skills train-
ing.
Acknowledgments
We would like to thank the participants, children
and their parents, in this study. We also thank
Dr. Hidemi Iwasaka for his advice and support as
clinician in pediatrics. A part of this study was
conducted in Signal Analysis and Interpretation
Laboratory (SAIL), University of Southern Cali-
fornia. This study is supported by JSPS KAKEN
24240032.
References
American Psychiatric Association. 2013. The Diag-
nostic and Statistical Manual of Mental Disorders:
DSM 5.
Asgari, Meysam, Alireza Bayestehtashk, and Izhak
Shafran. 2013. Robust and Accurate Featuers for
Detecting and Diagnosing Autism Spectrum Disor-
ders. Proceedings of Interspeech, 191?194.
95
Asperger, H.. 1944. Die ,,Autistischen Psychopathen?
im Kindesalter. European Archives of Psychiatry
and Clinical Neuroscience, 117: 76?136.
Bone, D., Black, M. P., Lee, C. C., Williams, M.
E., Levitt, P., Lee, S., and Narayanan, S.. 2012.
Spontaneous-Speech Acoustic-Prosodic Features of
Children with Autism and the Interacting Psycholo-
gist. Proceedings of Interspeech.
Chaspari, T., Gibson, D. B., Lee, C.-C., and Narayanan,
S. S. 2013. Using physiology and language cues for
modeling verbal response latencies of children with
ASD. Proceedings of ICASSP, 3702?3706.
Davis, Megan, Kerstin Dautenhahn, CL Nehaniv, and
SD Powell. 2004. Towards an Interactive Sys-
tem Facilitating Therapeutic Narrative Elicitation in
Autism. Proceedings of NILE.
Dawson, Geraldine, Deborah Hill, Art Spencer, Larry
Galpert, and Linda Watson.. 1990. Affective ex-
changes between young autistic children and their
mothers. Journal of Abnormal Child Psychology,
18: 335?345.
de Marchena, A. and Inge-Marie E.. 2010. Conversa-
tional gestures in autism spectrum disorders: asyn-
chrony but not decreased frequency. Autism Re-
search, 3: 311?322.
Hanson M. H.. 1995. Glottal characteristics of female
speakers. Harvard University, Ph.D. dissertation.
Heeman, P. A., Lunsford, R., Selfridge, E., Black, L.,
and Van Santen, J.. 2010. Autism and interactional
aspects of dialogue. Proceedings of SIGDIAL, 249?
252.
Kanner, L.. 1943. Autistic disturbances of affective
contact. Nervous Child, 2: 217?250.
Kiss, G. and van Santen, J. P. H.. 2013. Estimating
Speaker-Specific Intonation Patterns Using the Lin-
ear Alignment Model. Proceedings of Interspeech
354?358.
Kiss, G., van Santen, J. P. H., Prud?hommeaux, E. T.,
and Black, L. M.. 2012. Quantitative Analysis of
Pitch in Speech of Children with Neurodevelopmen-
tal Disorders. Proceedings of Interspeech.
Lovaas, O Ivar, Robert Koegel, James Q Simmons, and
Judith Stevens Long. 1973. Some generalisation
and follow-up measures on autistic children in be-
haviour therapy. Journal of Applied Behavior Anal-
ysis, 6: 131?166.
Mairesse, Francois, Marilyn A Walker, Matthias R
Mehl, and Roger K Moore. 2007. Using Linguis-
tic cues for the automatic recognition of personality
in conversation and text. Journal of Artificial Intel-
ligence Research, 30: 457?500.
McCann, J. and Sue, P.. 2003. Prosody in autism
spectrum disorders: a critical review. International
Journal of Language & Communication Disorders,
38(4): 325?350.
Mower, E., Black, M. P., Flores, E., Williams, M., and
Narayanan, S.. 2011. Rachel: Design of an emo-
tionally targeted interactive agent for children with
autism. Proceedings of IEEE ICME, 1?6.
Newton, A. T., Kramer, A. D. I., and McIntosh, D. N..
2009. Autism online: a comparison of word usage
in bloggers with and without autism spectrum disor-
ders. Proceedings of SIGCHI, 463?466.
Paul, Rhea, Amy Augustyn, Ami Klin, and Fred R
Volkmar. 2005. Perception and production of
prosody by speakers with autism spectrum disor-
ders. Journal of Autism and Developmental Disor-
ders, 35: 205?220.
Pennebaker, James W, Martha E Francis, and Roger J
Booth. 2005. Linguistic inquiry and word count:
LIWC [Computer software] Austin, TX: liwc. net.
Rouhizadeh Masoud, Prud?hommeaux Emily, Roark
Brian, and van Santen Jan. 2013. Distributional se-
mantic models for the evaluation of disordered lan-
guage. Proceedings of NAACL-HLT, 709?714.
Santen, Jan PH, Richard W Sproat, and Alison Pres-
manes Hill. 2013. Quantifying repetitive speech
in autism spectrum disorders and language impair-
ment. Autism Research, 6: 372?383.
Sharda, Megha, T Padma Subhadra, Sanchita Sahay,
Chetan Nagaraja, Latika Singh, Ramesh Mishra,
Amit Sen, Nidhi Singhal, Donna Erickson, and Nan-
dini C Singh. 2010. Sounds of melody?Pitch pat-
terns of speech in autism. Neuroscience letters, 478:
42?45.
Van Santen, Jan PH, Emily T Prud?hommeaux, Lois
M Black, and Margaret Mitchell. 2010. Compu-
tational prosodic markers for autism. Autism, 14:
215?236.
Wallace, Charles J, Connie J Nelson, Robert Paul
Liberman, Robert A Aitchison, David Lukoff, John
P Elder, and Chris Ferris. 1980. A review and cri-
tique of social skills training with schizophrenic pa-
tients. Schizophrenia Bulletin, 6:42?63.
96
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 34?42,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Rule-based Syntactic Preprocessing
for Syntax-based Machine Translation
Yuto Hatakoshi, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
Nara Institute of Science and Technology
Graduate School of Information Science
Takayama, Ikoma, Nara 630-0192, Japan
{hatakoshi.yuto.hq8,neubig,ssakti,tomoki,s-nakamura}@is.naist.jp
Abstract
Several preprocessing techniques using
syntactic information and linguistically
motivated rules have been proposed to im-
prove the quality of phrase-based machine
translation (PBMT) output. On the other
hand, there has been little work on similar
techniques in the context of other trans-
lation formalisms such as syntax-based
SMT. In this paper, we examine whether
the sort of rule-based syntactic preprocess-
ing approaches that have proved beneficial
for PBMT can contribute to syntax-based
SMT. Specifically, we tailor a highly suc-
cessful preprocessing method for English-
Japanese PBMT to syntax-based SMT,
and find that while the gains achievable are
smaller than those for PBMT, significant
improvements in accuracy can be realized.
1 Introduction
In the widely-studied framework of phrase-based
machine translation (PBMT) (Koehn et al., 2003),
translation probabilities between phrases consist-
ing of multiple words are calculated, and trans-
lated phrases are rearranged by the reordering
model in the appropriate target language order.
While PBMT provides a light-weight framework
to learn translation models and achieves high
translation quality in many language pairs, it does
not directly incorporate morphological or syntac-
tic information. Thus, many preprocessing meth-
ods for PBMT using these types of information
have been proposed. Methods include preprocess-
ing to obtain accurate word alignments by the divi-
sion of the prefix of verbs (Nie?en and Ney, 2000),
preprocessing to reduce the errors in verb conju-
gation and noun case agreement (Avramidis and
Koehn, 2008), and many others. The effectiveness
of the syntactic preprocessing for PBMT has been
supported by these and various related works.
In particular, much attention has been paid to
preordering (Xia and McCord, 2004; Collins et
al., 2005), a class of preprocessing methods for
PBMT. PBMT has well-known problems with lan-
guage pairs that have very different word order,
due to the fact that the reordering model has dif-
ficulty estimating the probability of long distance
reorderings. Therefore, preordering methods at-
tempt to improve the translation quality of PBMT
by rearranging source language sentences into an
order closer to that of the target language. It?s of-
ten the case that preordering methods are based
on rule-based approaches, and these methods have
achieved great success in ameliorating the word
ordering problems faced by PBMT (Collins et al.,
2005; Xu et al., 2009; Isozaki et al., 2010b).
One particularly successful example of rule-
based syntactic preprocessing is Head Finalization
(Isozaki et al., 2010b), a method of syntactic pre-
processing for English to Japanese translation that
has significantly improved translation quality of
English-Japanese PBMT using simple rules based
on the syntactic structure of the two languages.
The most central part of the method, as indicated
by its name, is a reordering rule that moves the
English head word to the end of the corresponding
syntactic constituents to match the head-final syn-
tactic structure of Japanese sentences. Head Final-
ization also contains some additional preprocess-
ing steps such as determiner elimination, parti-
cle insertion and singularization to generate a sen-
tence that is closer to Japanese grammatical struc-
ture.
In addition to PBMT, there has also recently
been interest in syntax-based SMT (Yamada and
Knight, 2001; Liu et al., 2006), which translates
using syntactic information. However, few at-
tempts have been made at syntactic preprocessing
for syntax-based SMT, as the syntactic informa-
tion given by the parser is already incorporated
directly in the translation model. Notable excep-
34
tions include methods to perform tree transforma-
tions improving correspondence between the sen-
tence structure and word alignment (Burkett and
Klein, 2012), methods for binarizing parse trees to
match word alignments (Zhang et al., 2006), and
methods for adjusting label sets to be more ap-
propriate for syntax-based SMT (Hanneman and
Lavie, 2011; Tamura et al., 2013). It should be
noted that these methods of syntactic preprocess-
ing for syntax-based SMT are all based on auto-
matically learned rules, and there has been little in-
vestigation of the manually-created linguistically-
motivated rules that have proved useful in prepro-
cessing for PBMT.
In this paper, we examine whether rule-based
syntactic preprocessing methods designed for
PBMT can contribute anything to syntax-based
machine translation. Specifically, we examine
whether the reordering and lexical processing of
Head Finalization contributes to the improvement
of syntax-based machine translation as it did for
PBMT. Additionally, we examine whether it is
possible to incorporate the intuitions behind the
Head Finalization reordering rules as soft con-
straints by incorporating them as a decoder fea-
ture. As a result of our experiments, we demon-
strate that rule-based lexical processing can con-
tribute to improvement of translation quality of
syntax-based machine translation.
2 Head Finalization
Head Finalization is a syntactic preprocessing
method for English to Japanese PBMT, reducing
grammatical errors through reordering and lexi-
cal processing. Isozaki et al. (2010b) have re-
ported that translation quality of English-Japanese
PBMT is significantly improved using a transla-
tion model learned by English sentences prepro-
cessed by Head Finalization and Japanese sen-
tences. In fact, this method achieved the highest
results in the large scale NTCIR 2011 evaluation
(Sudoh et al., 2011), the first time a statistical ma-
chine translation (SMT) surpassed rule-based sys-
tems for this very difficult language pair, demon-
strating the utility of these simple syntactic trans-
formations from the point of view of PBMT.
2.1 Reordering
The reordering process of Head Finalization uses
a simple rule based on the features of Japanese
grammar. To convert English sentence into
John hit a ball
John hita ball
NN VBD DT NN
NP
VP
NP
S
VBD
VP
NP
S
DT NN
NP
NN
Original English
Head Final English
Add Japanese Particles
John hita ballva0 va2
Singularize, 
Eliminate Determiners
John hita ballva0 va2
Reordering
Figure 1: Head Finalization
Japanese word order, the English sentence is first
parsed using a syntactic parser, and then head
words are moved to the end of the corresponding
syntactic constituents in each non-terminal node
of the English syntax tree. This helps replicate
the ordering of words in Japanese grammar, where
syntactic head words come after non-head (depen-
dent) words.
Figure 1 shows an example of the application
of Head Finalization to an English sentence. The
head node of the English syntax tree is connected
to the parent node by a bold line. When this node
is the first child node, we move it behind the de-
pendent node in order to convert the English sen-
tence into head final order. In this case, moving
the head node VBD of black node VP to the end of
this node, we can obtain the sentence ?John a ball
hit? which is in a word order similar to Japanese.
2.2 Lexical Processing
In addition to reordering, Head Finalization con-
ducts the following three steps that do not affect
word order. These steps do not change the word
35
ordering, but still result in an improvement of
translation quality, and it can be assumed that the
effect of this variety of syntactic preprocessing is
not only applicable to PBMT but also other trans-
lation methods that do not share PBMT?s problems
of reordering such as syntax-based SMT. The three
steps included are as follows:
1. Pseudo-particle insertion
2. Determiner (?a?, ?an?, ?the?) elimination
3. Singularization
The motivation for the first step is that in con-
trast to English, which has relatively rigid word
order and marks grammatical cases of many noun
phrases according to their position relative to the
verb, Japanese marks the topic, subject, and object
using case marking particles. As Japanese parti-
cles are not found in English, Head Finalization
inserts ?pseudo-particles? to prevent a mistransla-
tion or lack of particles in the translation process.
In the pseudo-particle insertion process (1), we in-
sert the following three types of pseudo-particles
equivalent to Japanese case markers ?wa? (topic),
?ga? (subject) or ?wo? (object).
? va0: Subject particle of the main verb
? va1: Subject particle of other verbs
? va2: Object particle of any verb
In the example of Figure 1, we insert the topic par-
ticle va0 behind of ?John?, which is a subject of a
verb ?hit? and object particle va2 at the back of
object ?ball.?
Another source of divergence between the two
languages stems from the fact that Japanese does
not contain determiners or makes distinctions be-
tween singular and plural by inflection of nouns.
Thus, to generate a sentence that is closer to
Japanese, Head Finalization eliminates determin-
ers (2) and singularizes plural nouns (3) in addi-
tion to the pseudo-particle insertion.
In Figure 1, we can see that applying these
three processes to the source English sentence re-
sults in the sentence ?John va0 (wa) ball va2 (wo)
hit? which closely resembles the structure of the
Japanese translation ?jon wa bo-ru wo utta.?
3 Syntax-based Statistical Machine
Translation
Syntax-based SMT is a method for statistical
translation using syntactic information of the sen-
tence (Yamada and Knight, 2001; Liu et al., 2006).
By using translation patterns following the struc-
ture of linguistic syntax trees, syntax-based trans-
lations often makes it possible to achieve more
grammatical translations and reorderings com-
pared with PBMT. In this section, we describe
tree-to-string (T2S) machine translation based on
synchronous tree substitution grammars (STSG)
(Graehl et al., 2008), the variety of syntax-based
SMT that we use in our experiments.
T2S captures the syntactic relationship between
two languages by using the syntactic structure of
parsing results of the source sentence. Each trans-
lation pattern is expressed as a source sentence
subtree using rules including variables. The fol-
lowing example of a translation pattern include
two noun phrases NP
0
and NP
1
, which are trans-
lated and inserted into the target placeholders X
0
and X
1
respectively. The decoder generates the
translated sentence in consideration of the proba-
bility of translation pattern itself and translations
of the subtrees of NP
0
and NP
1
.
S((NP
0
) (VP(VBD hit) (NP
1
)))
? X
0
wa X
1
wo utta
T2S has several advantages over PBMT. First,
because the space of translation candidates is re-
duced using the source sentence subtree, it is often
possible to generate translations that are more ac-
curate, particularly with regards to long-distance
reordering, as long as the source parse is correct.
Second, the time to generate translation results is
also reduced because the search space is smaller
than PBMT. On the other hand, because T2S gen-
erates translation results using the result of auto-
matic parsing, translation quality highly depends
on the accuracy of the parser.
4 Applying Syntactic Preprocessing to
Syntax-based Machine Translation
In this section, we describe our proposed method
to apply Head Finalization to T2S translation.
Specifically, we examine two methods for incor-
porating the Head Finalization rules into syntax-
based SMT: through applying them as preprocess-
ing step to the trees used in T2S translation, and
36
through adding reordering information as a feature
of the translation patterns.
4.1 Syntactic Preprocessing for T2S
We applied the two types of processing shown in
Table 1 as preprocessing for T2S. This is similar
to preprocessing for PBMTwith the exception that
preprocessing for PBMT results in a transformed
string, and preprocessing for T2S results in a trans-
formed tree. In the following sections, we elabo-
rate on methods for applying these preprocessing
steps to T2S and some effects expected therefrom.
Table 1: Syntactic preprocessing applied to T2S
Preprocessing Description
Reordering Reordering based on Japanese
typical head-final grammatical
structure
Lexical Processing Pseudo-particle insertion, deter-
miner elimination, singulariza-
tion
4.1.1 Reordering for T2S
In the case of PBMT, reordering is used to change
the source sentence word order to be closer to
that of the target, reducing the burden on the rel-
atively weak PBMT reordering models. On the
other hand, because translation patterns of T2S
are expressed by using source sentence subtrees,
the effect of reordering problems are relatively
small, and the majority of reordering rules spec-
ified by hand can be automatically learned in a
well-trained T2S model. Therefore, preordering
is not expected to cause large gains, unlike in the
case of PBMT.
However, it can also be thought that preordering
can still have a positive influence on the translation
model training process, particularly by increasing
alignment accuracy. For example, training meth-
ods for word alignment such as the IBM or HMM
models (Och and Ney, 2003) are affected by word
order, and word alignment may be improved by
moving word order closer between the two lan-
guages. As alignment accuracy plays a important
role in T2S translation (Neubig and Duh, 2014), it
is reasonable to hypothesize that reordering may
also have a positive effect on T2S. In terms of the
actual incorporation with the T2S system, we sim-
ply follow the process in Figure 1, but output the
reordered tree instead of only the reordered termi-
nal nodes as is done for PBMT.
John hit a ball
NN VBD DT NN
NP
VP
NP
S
Original English
NN VBD NN VA
NP
VP
NP
S
VA
John hit ball va2va0
Lexical Processing
Figure 2: A method of applying Lexical Process-
ing
4.1.2 Lexical Processing for T2S
In comparison to reordering, Lexical Processing
may be expected to have a larger effect on T2S,
as it will both have the potential to increase align-
ment accuracy, and remove the burden of learning
rules to perform simple systematic changes that
can be written by hand. Figure 2 shows an ex-
ample of the application of Lexical Processing to
transform not strings, but trees.
In the pseudo-particle insertion component,
three pseudo particles ?va0,? ?va1,? and ?va2? (as
shown in Section 2.2) are added in the source En-
glish syntax tree as terminal nodes with the non-
terminal node ?VA?. As illustrated in Figure 2, par-
ticles are inserted as children at the end of the cor-
responding NP node. For example, in the figure
the topic particle ?va0? is inserted after ?John,?
subject of the verb ?hit,? and the object particle
?va2? is inserted at the end of the NP for ?ball,?
the object.
In the determiner elimination process, terminal
nodes ?a,? ?an,? and ?the? are eliminated along
with non-terminal node DT. Determiner ?a? and
its corresponding non-terminal DT are eliminated
in the Figure 2 example.
Singularization, like in the processing for
PBMT, simply changes plural noun terminals to
their base form.
4.2 Reordering Information as Soft
Constraints
As described in section 4.1.1, T2S work well on
language pairs that have very different word order,
but is sensitive to alignment accuracy. On the other
hand, we know that in most cases Japanese word
order tends to be head final, and thus any rules that
do not obey head final order may be the result of
bad alignments. On the other hand, there are some
cases where head final word order is not applica-
ble (such as sentences that contain the determiner
37
?no,? or situations where non-literal translations
are necessary) and a hard constraint to obey head-
final word order could be detrimental.
In order to incorporate this intuition, we add
a feature (HF-feature) to translation patterns that
conform to the reordering rules of Head Final-
ization. This gives the decoder ability to discern
translation patterns that follow the canonical re-
ordering patterns in English-Japanese translation,
and has the potential to improve translation quality
in the T2S translation model.
We use the log-linear approach (Och, 2003) to
add the Head Finalization feature (HF-feature). As
in the standard log-linear model, a source sen-
tence f is translated into a target language sen-
tence e, by searching for the sentence maximizing
the score:
?
e = arg max
e
w
T
? h(f ,e). (1)
where h(f , e) is a feature function vector. w is
a weight vector that scales the contribution from
each feature. Each feature can take any real value
which is useful to improve translation quality, such
as the log of the n-gram language model proba-
bility to represent fluency, or lexical/phrase trans-
lation probability to capture the word or phrase-
wise correspondence. Thus, if we can incorporate
the information about reordering expressed by the
Head Finalization reordering rule as a features in
this model, we can learn weights to inform the de-
coder that it should generally follow this canonical
ordering.
Figure 3 shows a procedure of Head Finaliza-
tion feature (HF-feature) addition. To add the
HF-feature to translation patterns, we examine
the translation rules, along with the alignments
between target and source terminals and non-
terminals. First, we apply the Reordering to the
source side of the translation pattern subtree ac-
cording to the canonical head-final reordering rule.
Second, we examine whether the word order of the
reordered translation pattern matches with that of
the target translation pattern for which the word
alignment is non-crossing, indicating that the tar-
get string is also in head-final word order. Finally,
we set a binary feature (h
HF
(f , e) = 1) if the tar-
get word order obeys the head final order. This
feature is only applied to translation patterns for
which the number of target side words is greater
than or equal to two.
VP
VBD NP
hit x0:NP
x0 wo
Source side of
translation pattern
Target side of
translation pattern
VP
NP VBD
hitx0:NP
1. Apply Reordering to 
source translation pattern
2. Add HF-feature
if word alignment is 
non-crossing
utta
Word alignment 
x0 woTarget side of
translation pattern
utta
Reordered
translation pattern
Figure 3: Procedure of HF-feature addition
Table 2: The details of NTCIR7
Dataset Lang Words Sentences
Average
length
train
En 99.0M 3.08M 32.13
Ja 117M 3.08M 37.99
dev
En 28.6k 0.82k 34.83
Ja 33.5k 0.82k 40.77
test
En 44.3k 1.38k 32.11
Ja 52.4k 1.38k 37.99
5 Experiment
In our experiment, we examined how much each
of the preprocessing steps (Reordering, Lexical
Processing) contribute to improve the translation
quality of PBMT and T2S. We also examined the
improvement in translation quality of T2S by the
introduction of the Head Finalization feature.
5.1 Experimental Environment
For our English to Japanese translation experi-
ments, we used NTCIR7 PATENT-MT?s Patent
corpus (Fujii et al., 2008). Table 2 shows the
details of training data (train), development data
(dev), and test data (test).
As the PBMT and T2S engines, we used the
Moses (Koehn et al., 2007) and Travatar (Neubig,
2013) translation toolkits with the default settings.
38
Enju (Miyao and Tsujii, 2002) is used to parse En-
glish sentences and KyTea (Neubig et al., 2011) is
used as a Japanese tokenizer. We generated word
alignments using GIZA++ (Och and Ney, 2003)
and trained a Kneser-Ney smoothed 5-gram LM
using SRILM (Stolcke et al., 2011). Minimum
Error Rate Training (MERT) (Och, 2003) is used
for tuning to optimize BLEU. MERT is replicated
three times to provide performance stability on test
set evaluation (Clark et al., 2011).
We used BLEU (Papineni et al., 2002) and
RIBES (Isozaki et al., 2010a) as evaluation mea-
sures of translation quality. RIBES is an eval-
uation method that focuses on word reordering
information, and is known to have high correla-
tion with human judgement for language pairs that
have very different word order such as English-
Japanese.
5.2 Result
Table 3 shows translation quality for each com-
bination of HF-feature, Reordering, and Lexical
Processing. Scores in boldface indicate no sig-
nificant difference in comparison with the con-
dition that has highest translation quality using
the bootstrap resampling method (Koehn, 2004)
(p < 0.05).
For PBMT, we can see that reordering plays an
extremely important role, with the highest BLEU
and RIBES scores being achieved when using Re-
ordering preprocessing (line 3, 4). Lexical Pro-
cessing also provided a slight performance gain
for PBMT.When we applied Lexical Processing to
PBMT, BLEU and RIBES scores were improved
(line 1 vs 2), although this gain was not significant
when Reordering was performed as well.
Overall T2S without any preprocessing
achieved better translation quality than all con-
ditions of PBMT (line 1 of T2S vs line 1-4 of
PBMT). In addition, BLEU and RIBES score of
T2S were clearly improved by Lexical Processing
(line 2, 4, 6, 8 vs line 1, 3, 5, 7), and these scores
are the highest of all conditions. On the other
hand, Reordering and HF-Feature addition had no
positive effect, and actually tended to slightly hurt
translation accuracy.
5.3 Analysis of Preprocessing
With regards to PBMT, as previous works on
preordering have already indicated, BLEU and
RIBES scores were significantly improved by Re-
ordering. In addition, Lexical Processing also con-
Table 5: Optimized weight of HF-feature in each
condition
HF-feature Reordering
Word Weight of
Processing HF-feature
+ - - -0.00707078
+ - + 0.00524676
+ + - 0.156724
+ + + -0.121326
tributed to improve translation quality of PBMT
slightly. We also investigated the influence
that each element of Lexical Processing (pseudo-
particle insertion, determiner elimination, singu-
larization) had on translation quality, and found
that the gains were mainly provided by particle
insertion, with little effect from determiner elim-
ination or singularization.
Although Reordering was effective for PBMT,
it did not provide any benefit for T2S. This in-
dicates that T2S can already conduct long dis-
tance word reordering relatively correctly, and
word alignment quality was not improved as much
as expected by closing the gap in word order be-
tween the two languages. This was verified by a
subjective evaluation of the data, finding very few
major reordering issues in the sentences translated
by T2S.
On the other hand, Lexical Processing func-
tioned effectively for not only PBMT but also T2S.
When added to the baseline, lexical processing on
its own resulted in a gain of 0.57 BLEU, and 0.99
RIBES points, a significant improvement, with
similar gains being seen in other settings as well.
Table 4 demonstrates a typical example of the
improvement of the translation result due to Lex-
ical Processing. It can be seen that translation
performance of particles (indicated by underlined
words) was improved. The underlined particle is
in the direct object position of the verb that corre-
sponds to ?comprises? in English, and thus should
be given the object particle ?? wo? as in the refer-
ence and the system using Lexical Processing. On
the other hand, in the baseline system the genitive
?? to? is generated instead due to misaligned par-
ticles being inserted in an incorrect position in the
translation rules.
5.4 Analysis of Feature Addition
Our experimental results indicated that translation
quality is not improved by HF-feature addition
(line 1-4 vs line 5-8). We conjecture that the rea-
son why HF-feature did not contribute to an im-
39
Table 3: Translation quality by combination of HF-feature, Reordering, and Lexical Processing. Bold
indicates results that are not statistically significantly different from the best result (39.60 BLEU in line
4 and 79.47 RIBES in line 2).
ID
PBMT T2S
HF-feature Reordering Lexical Processing BLEU RIBES BLEU RIBES
1 - - - 32.11 69.06 38.94 78.48
2 - - + 33.16 70.19 39.51 79.47
3 - + - 37.62 77.56 38.44 78.48
4 - + + 37.77 77.71 39.60 79.26
5 + - - ? ? 38.74 78.33
6 + - + ? ? 39.29 79.23
7 + + - ? ? 38.48 78.44
8 + + + ? ? 39.38 79.21
Table 4: Improvement of translation results due to Lexical Processing
Source another connector 96 , which is matable with this cable connector 90 , comprises a plurality of
male contacts 98 aligned in a row in an electrically insulative housing 97 as shown in the figure .
Reference ????????????????????????????????????
?????????????????????????????????
- Lexical Processing ???????????????????????????????????
???????????????????????????????????
???
+ Lexical Processing ???????????????????????????????????
???????????????????????????????????
??
provement in translation quality is that the reorder-
ing quality achieved by T2S translation was al-
ready sufficiently high, and the initial feature led
to confusion in MERT optimization.
Table 5 shows the optimized weight of the HF
feature in each condition. From this table, we can
see that in two of the conditions positive weights
are learned, and in two of the conditions negative
weights are learned. This indicates that there is no
consistent pattern of learning weights that corre-
spond to our intuition that head-final rules should
receive higher preference.
It is possible that other optimization methods,
or a more sophisticated way of inserting these fea-
tures into the translation rules could help alleviate
these problems.
6 Conclusion
In this paper, we analyzed the effect of applying
syntactic preprocessing methods to syntax-based
SMT. Additionally, we have adapted reordering
rules as a decoder feature. The results showed
that lexical processing, specifically insertion of
pseudo-particles, contributed to improving trans-
lation quality, and it was effective as preprocessing
for T2S.
It should be noted that this paper, while demon-
strating that the simple rule-based syntactic pro-
cessing methods that have been useful for PBMT
can also contribute to T2S in English-Japanese
translation, more work is required to ensure that
this will generalize to other settings. A next step in
our inquiry is the generalization of these results to
other proposed preprocessing techniques and other
language pairs. In addition, we would like to try
two ways described below. First, it is likely that
other tree transformations, for example changing
the internal structure of the tree by moving chil-
dren to different nodes, would help in cases where
it is common to translate into highly divergent syn-
tactic structures between the source and target lan-
guages. Second, we plan to investigate other ways
of incorporating the preprocessing rules as a soft
constraints, such as using n-best lists or forests to
enode many possible sentence interpretations.
References
Eleftherios Avramidis and Philipp Koehn. 2008. En-
riching morphologically poor languages for statisti-
cal machine translation. In Annual Meeting of the
40
Association for Computational Linguistics (ACL),
pages 763?770.
David Burkett and Dan Klein. 2012. Transforming
trees to improve syntactic convergence. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 863?872.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer
instability. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 176?181.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 531?
540.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2008. Overview of the
patent translation task at the NTCIR-7 workshop. In
Proceedings of the 7th NTCIR Workshop Meeting,
pages 389?400.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, pages 391?427.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Workshop on Syntax and Structure in
Statistical Translation, pages 98?106.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple
reordering rule for SOV languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 609?
616.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the second international conference on Hu-
man Language Technology Research, pages 292?
297.
Graham Neubig and Kevin Duh. 2014. On the ele-
ments of an accurate tree-to-string machine transla-
tion system. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143?
149.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 529?533.
Graham Neubig. 2013. Travatar: A forest-to-string
machine translation engine based on tree transduc-
ers. Annual Meeting of the Association for Compu-
tational Linguistics (ACL), page 91.
Sonja Nie?en and Hermann Ney. 2000. Improving
SMT quality with morpho-syntactic analysis. In
Proceedings of the 18th conference on Computa-
tional linguistics-Volume 2, pages 1081?1085.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, pages 19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and out-
look. In IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU), page 5.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,
Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki,
and Jun ?ichi Tsujii. 2011. NTT-UT statistical ma-
chine translation in NTCIR-9 PatentMT. In Pro-
ceedings of NTCIR, pages 585?592.
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita, Hi-
roya Takamura, and Manabu Okumura. 2013. Part-
of-speech induction in dependency trees for statisti-
cal machine translation. In Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 841?851.
41
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In International Conference on Computa-
tional Linguistics (COLING), page 508.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In North
American Chapter of the Association for Computa-
tional Linguistics, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 523?530.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In North American Chapter of the
Association for Computational Linguistics, pages
256?263.
42
