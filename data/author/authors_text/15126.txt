Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 626?635,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Discriminative Sample Selection for Statistical Machine Translation?
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard, and Prem Natarajan
Raytheon BBN Technologies
10 Moulton Street
Cambridge, MA, U.S.A.
{sanantha,rprasad,stallard,prem}@bbn.com
Abstract
Production of parallel training corpora for the
development of statistical machine translation
(SMT) systems for resource-poor languages
usually requires extensive manual effort. Ac-
tive sample selection aims to reduce the la-
bor, time, and expense incurred in produc-
ing such resources, attaining a given perfor-
mance benchmark with the smallest possible
training corpus by choosing informative, non-
redundant source sentences from an available
candidate pool for manual translation. We
present a novel, discriminative sample selec-
tion strategy that preferentially selects batches
of candidate sentences with constructs that
lead to erroneous translations on a held-out de-
velopment set. The proposed strategy supports
a built-in diversity mechanism that reduces
redundancy in the selected batches. Simu-
lation experiments on English-to-Pashto and
Spanish-to-English translation tasks demon-
strate the superiority of the proposed approach
to a number of competing techniques, such
as random selection, dissimilarity-based se-
lection, as well as a recently proposed semi-
supervised active learning strategy.
1 Introduction
Resource-poor language pairs present a significant
challenge to the development of statistical machine
translation (SMT) systems due to the latter?s depen-
dence on large parallel texts for training. Bilingual
human experts capable of producing the requisite
?Distribution Statement ?A? (Approved for Public Release,
Distribution Unlimited)
data resources are often in short supply, and the task
of preparing high-quality parallel corpora is labori-
ous and expensive. In light of these constraints, an
attractive strategy is to construct the smallest pos-
sible parallel training corpus with which a desired
performance benchmark may be achieved.
Such a corpus may be constructed by selecting the
most informative instances from a large collection
of source sentences for translation by a human ex-
pert, a technique often referred to as active learn-
ing. A SMT system trained with sentence pairs thus
generated is expected to perform significantly better
than if the source sentences were chosen using, say,
a na??ve random sampling strategy.
Previously, Eck et al (2005) described a selec-
tion strategy that attempts to maximize coverage by
choosing sentences with the highest proportion of
previously unseen n-grams. Depending on the com-
position of the candidate pool with respect to the
domain, this strategy may select irrelevant outliers.
They also described a technique based on TF-IDF to
de-emphasize sentences similar to those that have al-
ready been selected, thereby encouraging diversity.
However, this strategy is bootstrapped by random
initial choices that do not necessarily favor sentences
that are difficult to translate. Finally, they worked
exclusively with the source language and did not use
any SMT-derived features to guide selection.
Haffari et al (2009) proposed a number of fea-
tures, such as similarity to the seed corpus, transla-
tion probability, n-gram and phrase coverage, etc.,
that drive data selection. They also proposed a
model in which these features combine linearly to
predict a rank for each candidate sentence. The
626
top-ranked sentences are chosen for manual transla-
tion. However, this approach requires that the pool
have the same distributional characteristics as the
development sets used to train the ranking model.
Additionally, batches are chosen atomically. Since
similar or identical sentences in the pool will typi-
cally meet the selection criteria simultaneously, this
can have the undesired effect of choosing redundant
batches with low diversity.
The semi-supervised active learning strategy pro-
posed by Ananthakrishnan et al (2010) uses multi-
layer perceptrons (MLPs) to rank candidate sen-
tences based on various features, including domain
representativeness, translation difficulty, and batch
diversity. A greedy, incremental batch construction
technique encourages diversity. While this strat-
egy was shown to be superior to random as well
as n-gram based dissimilarity selection, its coarse
granularity (reducing a candidate sentence to a low-
dimensional feature vector for ranking) makes it un-
suitable for many situations. In particular, it is seen
to have little or no benefit over random selection
when there is no logical separation of the candidate
pool into ?in-domain? and ?out-of-domain? subsets.
This paper introduces a novel, active sample se-
lection technique that identifies translation errors on
a held-out development set, and preferentially se-
lects candidate sentences with constructs that are
incorrectly translated in the former. A discrimina-
tive pairwise comparator function, trained on the
ranked development set, is used to order candidate
sentences and pick sentences that provide maximum
potential reduction in translation error. The feature
functions that power the comparator are updated af-
ter each selection to encourage batch diversity. In
the following sections, we provide details of the pro-
posed sample selection approach, and describe sim-
ulation experiments that demonstrate its superiority
over a number of competing strategies.
2 Error-Driven Active Learning
Traditionally, unsupervised selection strategies have
dominated the active learning literature for natural
language processing (Hwa, 2004; Tang et al, 2002;
Shen et al, 2004). Sample selection for SMT has
followed a similar trend. The work of Eck et al
(2005) and most of the techniques proposed by Haf-
fari et al (2009) fall in this category. Notable ex-
ceptions include the linear ranking model of Haf-
fari et al (2009) and the semi-supervised selection
technique of Ananthakrishnan et al (2010), both of
which use one or more held-out development sets to
train and tune the sample selector. However, while
the former uses the posterior translation probability
and the latter, a sentence-level confidence score as
part of the overall selection strategy, current active
learning techniques for SMT do not explicitly target
the sources of error.
Error-driven active learning attempts to choose
candidate instances that potentially maximize error
reduction on a reference set (Cohn et al, 1996;
Meng and Lee, 2008). In the context of SMT, this
involves decoding a held-out development set with
an existing baseline (seed) SMT system. The selec-
tion algorithm is then trained to choose, from the
candidate pool, sentences containing constructs that
give rise to translation errors on this set. Assum-
ing perfect reference translations and word align-
ment in subsequent SMT training, these sentences
provide maximum potential reduction in translation
error with respect to the seed SMT system. It is a su-
pervised approach to sample selection. We assume
the following are available.
? A seed parallel corpus S for training the initial
SMT system.
? A candidate pool of monolingual source sen-
tences P from which samples must be selected.
? A held-out development set D for training the
selection algorithm and for tuning the SMT.
? A test set T for evaluating SMT performance.
We further make the following reasonable as-
sumptions: (a) the development set D and the test
set T are drawn from the same distribution and (b)
the candidate pool P consists of both in- and out-
of-domain source sentences, as well as an allowable
level of redundancy (similar or identical sentences).
Using translation errors on the development set to
drive sample selection has the following advantages
over previously proposed active learning strategies
for SMT.
? The seed training corpus S need not be derived
from the same distribution as D and T. The seed
SMT system can be trained with any available
627
parallel corpus for the specified language pair.
This is very useful if, as is often the case, lit-
tle or no in-domain training data is available to
bootstrap the SMT system. This removes a criti-
cal restriction present in the semi-supervised ap-
proach of Ananthakrishnan et al (2010).
? Sentences chosen are guaranteed to be relevant
to the domain, because selection is based on n-
grams derived from the development set. This
alleviates potential problems with approaches
suggested by Eck et al (2005) and several tech-
niques used by Haffari et al (2009), where ir-
relevant outliers may be chosen simply because
they contain previously unseen n-grams, or are
deemed difficult to translate.
? The proposed technique seeks to minimize
held-out translation error rather than maximize
training-set coverage. This is the more intuitive,
direct approach to sample selection for SMT.
? Diversity can be encouraged by preventing n-
grams that appear in previously selected sen-
tences from playing a role in choosing subse-
quent sentences. This provides an efficient alter-
native to the cumbersome ?batch diversity? fea-
ture proposed by Ananthakrishnan et al (2010).
The proposed implementation of error-driven ac-
tive learning for SMT, discriminative sample selec-
tion, is described in the following section.
3 Discriminative Sample Selection
The goal of active sample selection is to induce an
ordering of the candidate instances that satisfies an
objective criterion. Eck et al (2005) ordered can-
didate sentences based on the frequency of unseen
n-grams. Haffari et al (2009) induced a ranking
based on unseen n-grams, translation difficulty, etc.,
as well as one that attempted to incrementally max-
imize BLEU using two held-out development sets.
Ananthakrishnan et al (2010) attempted to order the
candidate pool to incrementally maximize source n-
gram coverage on a held-out development set, sub-
ject to difficulty and diversity constraints.
In the case of error-driven active learning, we at-
tempt to learn an ordering model based on errors
observed on the held-out development set D. We
achieve this in an innovative fashion by casting the
ranking problem as a pairwise sentence compari-
son problem. This approach, inspired by Ailon and
Mohri (2008), involves the construction of a binary
classifier functioning as a relational operator that can
be used to order the candidate sentences. The pair-
wise comparator is trained on an ordering of D that
ranks constituent sentences in decreasing order of
the number of translation errors. The comparator is
then used to rank the candidate pool in decreasing
order of potential translation error reduction.
3.1 Maximum-Entropy Pairwise Comparator
Given a pair of source sentences (u, v), we define,
adopting the notation of Ailon and Mohri (2008), the
pairwise comparator h(u, v) as follows:
h(u, v) =
{
1, u < v
0, u >= v
(1)
In Equation 1, the binary comparator h(u, v)
plays the role of the ?less than? (?<?) relational op-
erator, returning 1 if u is preferred to v in an or-
dered list, and 0 otherwise. As detailed in Ailon and
Mohri (2008), the comparator must satisfy the con-
straint that h(u, v) and h(v, u) be complementary,
i.e. h(u, v) + h(v, u) = 1 to avoid ambiguity. How-
ever, it need not satisfy the triangle inequality.
We implement h(u, v) as a combination of dis-
criminative maximum entropy classifiers triggered
by feature functions drawn from n-grams of u and v.
We define p(u, v) as the conditional posterior prob-
ability of the Bernoulli event u < v given (u, v) as
shown in Equation 2.
p(u, v) = Pr(u < v | u, v) (2)
In our implementation, p(u, v) is the output of
a binary maximum-entropy classifier trained on the
development set. However, this implementation
poses two problems.
First, if we use constituent n-grams of u and v
as feature functions to trigger the classifier, there is
no way to distinguish between (u, v) and (v, u) as
they will trigger the same feature functions. This
will result in identical values for p(u, v) and p(v, u),
a contradiction. We resolve this issue by intro-
ducing a set of ?complementary? feature functions,
which are formed by simply appending a recogniz-
able identifier to the existing n-gram feature func-
628
u: how are you
v: i am going
f(u) = {how:1, are:1, you:1, how*are:2, are*you:2, how*are*you:3}
f(v) = {i:1, am:1, going:1, i*am:2, am*going:2, i*am*going:3}
f ?(u) = {!how:1, !are:1, !you:1, !how*are:2, !are*you:2, !how*are*you:3}
f ?(v) = {!i:1, !am:1, !going:1, !i*am:2, !am*going:2, !i*am*going:3}
Table 1: Standard and complementary trigram feature functions for a source pair (u, v).
tions. Then, to evaluate p(u, v), for instance, we
invoke the classifier with standard feature functions
for u and complementary feature functions for v.
Similarly, p(v, u) is evaluated by triggering comple-
mentary feature functions for u and standard feature
functions for v. Table 1 illustrates this with a simple
example.
Note that each feature function is associated with
a real value, whose magnitude is an indicator of its
importance. In our implementation, an n-gram fea-
ture function (standard or complementary) receives
a value equal to its length. This is based on our intu-
ition that longer n-grams play a more important role
in dictating SMT performance.
Second, the introduction of complementary trig-
gers implies that evaluation of p(u, v) and p(v, u)
now involves disjoint sets of feature functions. Thus,
p(u, v) is not guaranteed to satisfy the complemen-
tarity condition imposed on h(u, v), and therefore
cannot directly be used as the binary pairwise com-
parator. We resolve this by normalizing across the
two possible permutations, as follows:
h?(u, v) = p(u, v)p(u, v) + p(v, u) (3)
h?(v, u) = p(v, u)p(u, v) + p(v, u) (4)
Since h?(u, v) + h?(v, u) = 1, the complemen-
tarity constraint is now satisfied, and h(u, v) is just
a binarized (thresholded) version of h?(u, v). Thus,
the binary pairwise comparator can be constructed
from the permuted classifier outputs.
3.2 Training the Pairwise Comparator
Training the maximum-entropy classifier for the
pairwise comparator requires a set of target labels
and input feature functions, both of which are de-
rived from the held-out development set D. We be-
gin by decoding the source sentences in D with the
seed SMT system, followed by error analysis using
the Translation Edit Rate (TER) measure (Snover
et al, 2006). TER measures translation quality by
computing the number of edits (insertions, substitu-
tions, and deletions) and shifts required to transform
a translation hypothesis to its corresponding refer-
ence. We then rank D in decreasing order of the
number of post-shift edits, i.e. the number of in-
sertions, substitutions, and deletions after the shift
operation is completed. Since shifts are often due to
word re-ordering issues within the SMT decoder (es-
pecially for phrase-based systems), we do not con-
sider them as errors for the purpose of ranking D.
Sentences at the top of the ordered list D? contain
the maximum number of translation errors.
For each pair of sentences (u, v) : u < v in D?,
we generate two training entries. The first, signify-
ing that u appears before v in D?, assigns the label
true to a trigger list consisting of standard feature
functions derived from u, and complementary fea-
ture functions derived from v. The second, reinforc-
ing this observation, assigns the label false to a trig-
ger list consisting of complementary feature func-
tions from u, and standard feature functions from v.
The labeled training set (feature:label pairs) for the
comparator can be expressed as follows:
?(u, v) ? D? : u < v,
{f(u) f ?(v)} : true
{f ?(u) f(v)} : false
Thus, if there are d sentences in D?, we obtain a
total of d(d? 1) labeled examples to train the com-
parator. We use the standard L-BFGS optimization
629
algorithm (Liu and Nocedal, 1989) to estimate the
parameters of the maximum entropy model.
3.3 Greedy Discriminative Selection
The discriminatively-trained pairwise comparator
can be used as a relational operator to sort the candi-
date pool P in decreasing order of potential transla-
tion error reduction. A batch of pre-determined size
K can then be selected from the top of this list to
augment the existing SMT training corpus. Assum-
ing the pool contains N candidate sentences, and
given a fast sorting algorithm such as Quicksort, the
complexity of this strategy is O(N logN). Batches
can be selected iteratively until a specified perfor-
mance threshold is achieved.
A potential downside of this approach reveals it-
self when there is redundancy in the candidate pool.
Since the batch is selected in a single atomic opera-
tion from the sorted candidates, and because similar
or identical sentences will typically occupy the same
range in the ordered list, it is likely that this approach
will result in batches with low diversity. Whereas
we desire diverse batches for better coverage and ef-
ficient use of manual translation resources. This is-
sue was previously addressed in Shen et al (2004) in
the context of named-entity recognition, where they
used a two-step procedure to first select the most in-
formative and representative samples, followed by a
diversity filter. Ananthakrishnan et al (2010) used a
greedy, incremental batch construction strategy with
an integrated, explicit batch diversity feature as part
of the ranking model. Based on these ideas, we de-
sign a greedy selection strategy using the discrimi-
native relational operator.
Rather than perform a full sort on P, we sim-
ply invoke the minh(u,v)(? ? ? ) function to find the
sentence that potentially minimizes translation er-
ror. The subscript indicates that our implementation
of this function utilizes the discriminative relational
operator trained on the development set D. The best
choice sentence s is then added to our batch at the
current position (we begin with an empty batch). We
then remove the standard and complementary fea-
ture functions f(s) and f ?(s) triggered by s from the
global pool of feature functions obtained from D,
so that they do not play a role in the selection of
subsequent sentences for the batch. Subsequently,
a candidate sentence that is similar or identical to
Algorithm 1 Greedy Discriminative Selection
B? ()
for k = 1 to K do
s? minh(u,v)(P)
B(k)? s
P? P? {s}
f(D)? f(D)? f(s)
f ?(D)? f ?(D)? f ?(s)
end for
return B
s will not be preferred, because the feature func-
tions that previously caused it to rank highly will
no longer trigger. Algorithm 1 summarizes our se-
lection strategy in pseudocode. Since each call to
minh(u,v)(? ? ? ) is O(N), the overall complexity of
greedy discriminative selection is O(K ?N).
4 Experiments and Results
We conduct a variety of simulation experiments
with multiple language pairs (English-Pashto and
Spanish-English) and different data configurations
in order to demonstrate the utility of discrimina-
tive sample selection in the context of resource-poor
SMT. We also compare the performance of the pro-
posed strategy to numerous competing active and
passive selection methods as follows:
? Random: Source sentences are uniformly sam-
pled from the candidate pool P.
? Similarity: Choose sentences from P with the
highest fraction of n-gram overlap with the seed
corpus S.
? Dissimilarity: Select sentences from P with the
highest proportion of n-grams not seen in the
seed corpus S (Eck et al, 2005; Haffari et al,
2009).
? Longest: Pick the longest sentences from the
candidate pool P.
? Semi-supervised: Semi-supervised active learn-
ing with greedy incremental selection (Anan-
thakrishnan et al, 2010).
? Discriminative: Choose sentences that po-
tentially minimize translation error using a
maximum-entropy pairwise comparator (pro-
posed method).
630
Identical low-resource initial conditions are ap-
plied to each selection strategy so that they may be
objectively compared. A very small seed corpus S is
sampled from the available parallel training data; the
remainder serves as the candidate pool. Following
the literature on active learning for SMT, our simula-
tion experiments are iterative. A fixed-size batch of
source sentences is constructed from the candidate
pool using one of the above selection strategies. We
then look up the corresponding translations from the
candidate targets (simulating an expert human trans-
lator), augment the seed corpus with the selected
data, and update the SMT system with the expanded
training corpus. The selected data are removed from
the candidate pool. This select-update cycle is then
repeated for either a fixed number of iterations or
until a specified performance benchmark is attained.
At each iteration, we decode the unseen test set T
with the most current SMT configuration and eval-
uate translation performance in terms of BLEU as
well as coverage (defined as the fraction of untrans-
latable source words in the target hypotheses).
We use a phrase-based SMT framework similar to
Koehn et al (2003) for all experiments.
4.1 English-Pashto Simulation
Our English-Pashto (E2P) data originates from a
two-way collection of spoken dialogues, and con-
sists of two parallel sub-corpora: a directional E2P
corpus and a directional Pashto-English (P2E) cor-
pus. Each sub-corpus has its own independent train-
ing, development, and test partitions. The direc-
tional E2P training, development, and test sets con-
sist of 33.9k, 2.4k, and 1.1k sentence pairs, respec-
tively. The directional P2E training set consists of
76.5k sentence pairs. The corpus was used as-is, i.e.
no length-based filtering or redundancy-reduction
(i.e. removal of duplicates, if any) was performed.
The test-set BLEU score with the baseline E2P SMT
system trained from all of the above data was 9.5%.
We obtained a seed training corpus by randomly
sampling 1,000 sentence pairs from the directional
E2P training partition. The remainder of this set, and
the entire reversed P2E training partition were com-
bined to create the pool (109.4k sentence pairs). In
the past, we have observed that the reversed direc-
tional P2E data gives very little performance gain
in the E2P direction even though its vocabulary is
similar, and can be considered ?out-of-domain? as
far as the E2P translation task is concerned. Thus,
our pool consists of 30% in-domain and 70% out-
of-domain sentence pairs, making for a challeng-
ing active learning problem. A pool training set of
10k source sentences is sampled from this collection
for the semi-supervised selection strategy, leaving us
with 99.4k candidate sentences, which we use for all
competing techniques. The data configuration used
in this simulation is identical to Ananthakrishnan et
al. (2010), allowing us to compare various strategies
under the same conditions. We simulated a total of
20 iterations with batches of 200 sentences each; the
original 1,000 sample seed corpus grows to 5,000
sentence pairs and the end of our simulation.
Figure 1(a) illustrates the variation in BLEU
scores across iterations for each selection strategy.
The proposed discriminative sample selection tech-
nique performs significantly better at every iteration
than random, similarity, dissimilarity, longest, and
semi-supervised active selection. At the end of 20
iterations, the BLEU score gained 3.21 points, a rel-
ative improvement of 59.3%. This was followed by
semi-supervised active learning, which improved by
2.66 BLEU points, a 49.2% relative improvement.
Table 2 summarizes the total number of words se-
lected by each strategy, as well as the total area
under the BLEU curve with respect to the base-
line. The latter, labeled BLEUarea and expressed in
percent-iterations, is a better measure of the over-
all performance of each strategy across all iterations
than comparing BLEU scores at the final iteration.
Figure 1(b) shows the variation in coverage (per-
centage of untranslatable source words in target
hypotheses) for each selection technique. Here,
discriminative sample selection was better than all
other approaches except longest-sentence selection.
4.2 Spanish-English Simulation
The Spanish-English (S2E) training corpus was
drawn from the Europarl collection (Koehn, 2005).
To prevent length bias in selection, the corpus was
filtered to only retain sentence pairs whose source
ranged between 7 and 15 words (excluding punc-
tuation). Additionally, redundancy was reduced by
removing all duplicate sentence pairs. After these
steps, we obtained approximately 253k sentence
pairs for training. The WMT10 held-out develop-
631
(a) Variation in BLEU (E2P)
(b) Variation in coverage (E2P)
Figure 1: Simulation results for E2P data selection.
632
(a) Variation in BLEU (S2E)
(b) Variation in coverage (S2E)
Figure 2: Simulation results for S2E data selection.
633
Method E2P size E2P BLEUarea S2E size S2E BLEUarea
Random 58.1k 26.4 26.5k 45.0
Similarity 30.7k 21.9 24.7k 13.2
Dissimilarity 39.2k 12.4 24.2k 54.9
Longest 173.0k 27.5 39.6k 48.3
Semi-supervised 80.0k 34.1 27.6k 45.6
Discriminative 109.1k 49.6 31.0k 64.5
Table 2: Source corpus size (in words) and BLEUarea after 20 sample selection iterations.
ment and test sets (2k and 2.5k sentence pairs, re-
spectively) were used to tune our system and eval-
uate performance. Note that this data configuration
is different from that of the E2P simulation in that
there is no logical separation of the training data into
?in-domain? and ?out-of-domain? sets. The baseline
S2E SMT system trained with all available data gave
a test-set BLEU score of 17.2%.
We randomly sampled 500 sentence pairs from
the S2E training partition to obtain a seed train-
ing corpus. The remainder, after setting aside an-
other 10k source sentences for training the semi-
supervised strategy, serves as the candidate pool. We
again simulated a total of 20 iterations, except in
this case, we used batches of 100 sentences in an at-
tempt to obtain smoother performance trajectories.
The training corpus grows from 500 sentence pairs
to 2,500 as the simulation progresses.
Variation in BLEU scores and coverage for the
S2E simulation are illustrated in Figures 2(a) and
2(b), respectively. Discriminative sample selection
outperformed all other selection techniques across
all iterations of the simulation. After 20 iterations,
we obtained a 4.51 point gain in BLEU, a rela-
tive improvement of 142.3%. The closest com-
petitor was dissimilarity-based selection, which im-
proved by 4.38 BLEU points, a 138.1% relative
improvement. The proposed method also outper-
formed other selection strategies in improving cov-
erage, with significantly better results especially in
the early iterations. Table 2 summarizes the number
of words chosen, and BLEUarea, for each strategy.
5 Conclusion and Future Directions
Building SMT systems for resource-poor language
pairs requires significant investment of labor, time,
and money for the development of parallel training
corpora. We proposed a novel, discriminative sam-
ple selection strategy that can help lower these costs
by choosing batches of source sentences from a large
candidate pool. The chosen sentences, in conjunc-
tion with their manual translations, provide signifi-
cantly better SMT performance than numerous com-
peting active and passive selection techniques.
Our approach hinges on a maximum-entropy pair-
wise comparator that serves as a relational operator
for comparing two source sentences. This allows us
to rank the candidate pool in decreasing order of po-
tential reduction in translation error with respect to
an existing seed SMT system. The discriminative
comparator is coupled with a greedy, incremental se-
lection technique that discourages redundancy in the
chosen batches. The proposed technique diverges
from existing work on active sample selection for
SMT in that it uses machine learning techniques in
an attempt to explicitly reduce translation error by
choosing sentences whose constituents were incor-
rectly translated in a held-out development set.
While the performance of competing strategies
varied across language pairs and data configurations,
discriminative sample selection proved consistently
superior under all test conditions. It provides a pow-
erful, flexible, data selection front-end for rapid de-
velopment of SMT systems. Unlike some selection
techniques, it is also platform-independent, and can
be used as-is with a phrase-based, hierarchical, syn-
tactic, or other SMT framework.
We have so far restricted our experiments to simu-
lations, obtaining expert human translations directly
from the sequestered parallel corpus. We are now
actively exploring the possibility of linking the sam-
ple selection front-end to a crowd-sourcing back-
end, in order to obtain ?non-expert? translations us-
ing a platform such as the Amazon Mechanical Turk.
634
References
Nir Ailon and Mehryar Mohri. 2008. An efficient reduc-
tion of ranking to classification. In COLT ?08: Pro-
ceedings of the 21st Annual Conference on Learning
Theory, pages 87?98.
Sankaranarayanan Ananthakrishnan, Rohit Prasad, David
Stallard, and Prem Natarajan. 2010. A semi-
supervised batch-mode active learning strategy for
improved statistical machine translation. In CoNLL
?10: Proceedings of the 14th International Conference
on Computational Natural Language Learning, pages
126?134, July.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4(1):129?
145.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based in N-gram frequency and TF-IDF. In Proceed-
ings of IWSLT, Pittsburgh, PA, October.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-
chine translation. In NAACL ?09: Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 415?423,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30:253?276.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit X:
Proceedings of the 10th Machine Translation Summit,
pages 79?86.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528.
Qinggang Meng and Mark Lee. 2008. Error-driven
active learning in growing radial basis function net-
works for early robot learning. Neurocomputing, 71(7-
9):1449?1461.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In ACL ?04: Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 589?596, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Ac-
tive learning for statistical natural language parsing.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
120?127, Morristown, NJ, USA. Association for Com-
putational Linguistics.
635
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 445?449,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
On-line Language Model Biasing for Statistical Machine Translation
Sankaranarayanan Ananthakrishnan, Rohit Prasad and Prem Natarajan
Raytheon BBN Technologies
Cambridge, MA 02138, U.S.A.
{sanantha,rprasad,pnataraj}@bbn.com
Abstract
The language model (LM) is a critical com-
ponent in most statistical machine translation
(SMT) systems, serving to establish a proba-
bility distribution over the hypothesis space.
Most SMT systems use a static LM, inde-
pendent of the source language input. While
previous work has shown that adapting LMs
based on the input improves SMT perfor-
mance, none of the techniques has thus far
been shown to be feasible for on-line sys-
tems. In this paper, we develop a novel mea-
sure of cross-lingual similarity for biasing the
LM based on the test input. We also illustrate
an efficient on-line implementation that sup-
ports integration with on-line SMT systems by
transferring much of the computational load
off-line. Our approach yields significant re-
ductions in target perplexity compared to the
static LM, as well as consistent improvements
in SMT performance across language pairs
(English-Dari and English-Pashto).
1 Introduction
While much of the focus in developing a statistical
machine translation (SMT) system revolves around
the translation model (TM), most systems do not
emphasize the role of the language model (LM). The
latter generally follows a n-gram structure and is es-
timated from a large, monolingual corpus of target
sentences. In most systems, the LM is independent
of the test input, i.e. fixed n-gram probabilities de-
termine the likelihood of all translation hypotheses,
regardless of the source input.
The views expressed are those of the author and do not reflect the official policy or position of
the Department of Defense or the U.S. Government.
Some previous work exists in LM adaptation for
SMT. Snover et al (2008) used a cross-lingual infor-
mation retrieval (CLIR) system to select a subset of
target documents ?comparable? to the source docu-
ment; bias LMs estimated from these subsets were
interpolated with a static background LM. Zhao
et al (2004) converted initial SMT hypotheses to
queries and retrieved similar sentences from a large
monolingual collection. The latter were used to
build source-specific LMs that were then interpo-
lated with a background model. A similar approach
was proposed by Kim (2005). While feasible in off-
line evaluations where the test set is relatively static,
the above techniques are computationally expensive
and therefore not suitable for low-latency, interac-
tive applications of SMT. Examples include speech-
to-speech and web-based interactive translation sys-
tems, where test inputs are user-generated and pre-
clude off-line LM adaptation.
In this paper, we present a novel technique for
weighting a LM corpus at the sentence level based
on the source language input. The weighting scheme
relies on a measure of cross-lingual similarity evalu-
ated by projecting sparse vector representations of
the target sentences into the space of source sen-
tences using a transformation matrix computed from
the bilingual parallel data. The LM estimated from
this weighted corpus boosts the probability of rele-
vant target n-grams, while attenuating unrelated tar-
get segments. Our formulation, based on simple
ideas in linear algebra, alleviates run-time complex-
ity by pre-computing the majority of intermediate
products off-line.
Distribution Statement ?A? (Approved for Public Release, Distribution Unlimited)
445
2 Cross-Lingual Similarity
We propose a novel measure of cross-lingual simi-
larity that evaluates the likeness between an arbitrary
pair of source and target language sentences. The
proposed approach represents the source and target
sentences in sparse vector spaces defined by their
corresponding vocabularies, and relies on a bilingual
projection matrix to transform vectors in the target
language space to the source language space.
Let S = {s1, . . . , sM} and T = {t1, . . . , tN} rep-
resent the source and target language vocabularies.
Let u represent the candidate source sentence in a
M -dimensional vector space, whose mth dimension
um represents the count of vocabulary item sm in the
sentence. Similarly, v represents the candidate tar-
get sentence in a N -dimensional vector space. Thus,
u and v are sparse term-frequency vectors. Tra-
ditionally, the cosine similarity measure is used to
evaluate the likeness of two term-frequency repre-
sentations. However, u and v lie in different vector
spaces. Thus, it is necessary to find a projection of
v in the source vocabulary vector space before sim-
ilarity can be evaluated.
Assuming we are able to compute a M ? N -
dimensional bilingual word co-occurrence matrix ?
from the SMT parallel corpus, the matrix-vector
product u? = ?v is a projection of the target sen-
tence in the source vector space. Those source terms
of the M -dimensional vector u? will be emphasized
that most frequently co-occur with the target terms
in v. In other words, u? can be interpreted as a ?bag-
of-words? translation of v.
The cross-lingual similarity between the candi-
date source and target sentences then reduces to the
cosine similarity between the source term-frequency
vector u and the projected target term-frequency
vector u?, as shown in Equation 2.1:
S(u,v) = 1?u??u??u
T u?
= 1?u???v?u
T?v (2.1)
In the above equation, we ensure that both u and
u? are normalized to unit L2-norm. This prevents
over- or under-estimation of cross-lingual similarity
due to sentence length mismatch.
We estimate the bilingual word co-occurrence
matrix ? from an unsupervised, automatic word
alignment induced over the parallel training corpus
P. We use the GIZA++ toolkit (Al-Onaizan et al,
1999) to estimate the parameters of IBM Model
4 (Brown et al, 1993), and combine the forward
and backward Viterbi alignments to obtain many-to-
many word alignments as described in Koehn et al
(2003). The (m,n)th entry ?m,n of this matrix is
the number of times source word sm aligns to target
word tn in P.
3 Language Model Biasing
In traditional LM training, n-gram counts are evalu-
ated assuming unit weight for each sentence. Our
approach to LM biasing involves re-distributing
these weights to favor target sentences that are ?sim-
ilar? to the candidate source sentence according to
the measure of cross-lingual similarity developed in
Section 2. Thus, n-grams that appear in the trans-
lation hypothesis for the candidate input will be as-
signed high probability by the biased LM, and vice-
versa.
Let u be the term-frequency representation of the
candidate source sentence for which the LM must be
biased. The set of vectors {v1, . . . ,vK} similarly
represent the K target LM training sentences. We
compute the similarity of the source sentence u to
each target sentence vj according to Equation 3.1:
?j = S(u,vj)
= 1?u???vj?
uT?vj (3.1)
The biased LM is estimated by weighting n-gram
counts collected from the jth target sentence with
the corresponding cross-lingual similarity ?j . How-
ever, this is computationally intensive because: (a)
LM corpora usually consist of hundreds of thou-
sands or millions of sentences; ?j must be eval-
uated at run-time for each of them, and (b) the
entire LM must be re-estimated at run-time from
n-gram counts weighted by sentence-level cross-
lingual similarity.
In order to alleviate the run-time complexity of
on-line LM biasing, we present an efficient method
for obtaining biased counts of an arbitrary target
446
n-gram t. We define ct =
[
c1t , . . . , cKt
]T to be
the indicator-count vector where cjt is the unbi-
ased count of t in target sentence j. Let ? =
[?1, . . . , ?K ]T be the vector representing cross-
lingual similarity between the candidate source sen-
tence and each of the K target sentences. Then, the
biased count of this n-gram, denoted by C?(t), is
given by Equation 3.2:
C?(t) = cTt ?
=
K
?
j=1
1
?u???vj?
cjtuT?vj
= 1?u?u
T
K
?
j=1
1
??vj?
cjt?vj
= 1?u?u
Tbt (3.2)
The vector bt can be interpreted as the projection
of target n-gram t in the source space. Note that bt is
independent of the source input u, and can therefore
be pre-computed off-line. At run-time, the biased
count of any n-gram can be obtained via a simple
dot product. This adds very little on-line time com-
plexity because u is a sparse vector. Since bt is tech-
nically a dense vector, the space complexity of this
approach may seem very high. In practice, the mass
of bt is concentrated around a very small number of
source words that frequently co-occur with target n-
gram t; thus, it can be ?sparsified? with little or no
loss of information by simply establishing a cutoff
threshold on its elements. Biased counts and proba-
bilities can be computed on demand for specific n-
grams without re-estimating the entire LM.
4 Experimental Results
We measure the utility of the proposed LM bias-
ing technique in two ways: (a) given a parallel test
corpus, by comparing source-conditional target per-
plexity with biased LMs to target perplexity with the
static LM, and (b) by comparing SMT performance
with static and biased LMs. We conduct experi-
ments on two resource-poor language pairs commis-
sioned under the DARPA Transtac speech-to-speech
translation initiative, viz. English-Dari (E2D) and
English-Pashto (E2P), on test sets with single as well
as multiple references.
Data set E2D E2P
TM Training 138k pairs 168k pairs
LM Training 179k sentences 302k sentences
Development 3,280 pairs 2,385 pairs
Test (1-ref) 2,819 pairs 1,113 pairs
Test (4-ref) - 564 samples
Table 1: Data configuration for perplexity/SMT experi-
ments. Multi-reference test set is not available for E2D.
LM training data in words: 2.4M (Dari), 3.4M (Pashto)
4.1 Data Configuration
Parallel data were made available under the Transtac
program for both language pairs evaluated in this pa-
per. We divided these into training, held-out devel-
opment, and test sets for building, tuning, and evalu-
ating the SMT system, respectively. These develop-
ment and test sets provide only one reference trans-
lation for each source sentence. For E2P, DARPA
has made available to all program participants an
additional evaluation set with multiple (four) refer-
ences for each test input. The Dari and Pashto mono-
lingual corpora for LM training are a superset of tar-
get sentences from the parallel training corpus, con-
sisting of additional untranslated sentences, as well
as data derived from other sources, such as the web.
Table 1 lists the corpora used in our experiments.
4.2 Perplexity Analysis
For both Dari and Pashto, we estimated a static
trigram LM with unit sentence level weights that
served as a baseline. We tuned this LM by varying
the bigram and trigram frequency cutoff thresholds
to minimize perplexity on the held-out target sen-
tences. Finally, we evaluated test target perplexity
with the optimized baseline LM.
We then applied the proposed technique to es-
timate trigram LMs biased to source sentences in
the held-out and test sets. We evaluated source-
conditional target perplexity by computing the to-
tal log-probability of all target sentences in a par-
allel test corpus against the LM biased by the cor-
responding source sentences. Again, bigram and
trigram cutoff thresholds were tuned to minimize
source-conditional target perplexity on the held-out
set. The tuned biased LMs were used to compute
source-conditional target perplexity on the test set.
447
Eval set Static Biased Reduction
E2D-1ref-dev 159.3 137.7 13.5%
E2D-1ref-tst 178.3 156.3 12.3%
E2P-1ref-dev 147.3 130.6 11.3%
E2P-1ref-tst 122.7 108.8 11.3%
Table 2: Reduction in perplexity using biased LMs.
Witten-Bell discounting was used for smoothing
all LMs. Table 2 summarizes the reduction in target
perplexity using biased LMs; on the E2D and E2P
single-reference test sets, we obtained perplexity re-
ductions of 12.3% and 11.3%, respectively. This in-
dicates that the biased models are significantly better
predictors of the corresponding target sentences than
the static baseline LM.
4.3 Translation Experiments
Having determined that target sentences of a parallel
test corpus better fit biased LMs estimated from the
corresponding source-weighted training corpus, we
proceeded to conduct SMT experiments on both lan-
guage pairs to demonstrate the utility of biased LMs
in improving translation performance.
We used an internally developed phrase-based
SMT system, similar to Moses (Koehn et al, 2007),
as a test-bed for our translation experiments. We
used GIZA++ to induce automatic word alignments
from the parallel training corpus. Phrase translation
rules (up to a maximum source span of 5 words)
were extracted from a combination of forward and
backward word alignments (Koehn et al, 2003).
The SMT decoder uses a log-linear model that com-
bines numerous features, including but not limited to
phrase translation probability, LM probability, and
distortion penalty, to estimate the posterior proba-
bility of target hypotheses. We used minimum error
rate training (MERT) (Och, 2003) to tune the feature
weights for maximum BLEU (Papineni et al, 2001)
on the development set. Finally, we evaluated SMT
performance on the test set in terms of BLEU and
TER (Snover et al, 2006).
The baseline SMT system used the static trigram
LM with cutoff frequencies optimized for minimum
perplexity on the development set. Biased LMs
(with n-gram cutoffs tuned as above) were estimated
for all source sentences in the development and test
Test set BLEU 100-TER
Static Biased Static Biased
E2D-1ref-tst 14.4 14.8 29.6 30.5
E2P-1ref-tst 13.0 13.3 28.3 29.4
E2P-4ref-tst 25.6 26.1 35.0 35.8
Table 3: SMT performance with static and biased LMs.
sets, and were used to decode the corresponding in-
puts. Table 3 summarizes the consistent improve-
ment in BLEU/TER across multiple test sets and
language pairs.
5 Discussion and Future Work
Existing methods for target LM biasing for SMT
rely on information retrieval to select a comparable
subset from the training corpus. A foreground LM
estimated from this subset is interpolated with the
static background LM. However, given the large size
of a typical LM corpus, these methods are unsuitable
for on-line, interactive SMT applications.
In this paper, we proposed a novel LM biasing
technique based on linear transformations of target
sentences in a sparse vector space. We adopted a
fine-grained approach, weighting individual target
sentences based on the proposed measure of cross-
lingual similarity, and by using the entire, weighted
corpus to estimate a biased LM. We then sketched an
implementation that improves the time and space ef-
ficiency of our method by pre-computing and ?spar-
sifying? n-gram projections off-line during the train-
ing phase. Thus, our approach can be integrated
within on-line, low-latency SMT systems. Finally,
we showed that biased LMs yield significant reduc-
tions in target perplexity, and consistent improve-
ments in SMT performance.
While we used phrase-based SMT as a test-bed
for evaluating translation performance, it should be
noted that the proposed LM biasing approach is in-
dependent of SMT architecture. We plan to test its
effectiveness in hierarchical and syntax-based SMT
systems. We also plan to investigate the relative
usefulness of LM biasing as we move from low-
resource languages to those for which significantly
larger parallel corpora and LM training data are
available.
448
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Josef Och,
David Purdy, Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation: Final report.
Technical report, JHU Summer Workshop.
Peter E. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: parameter
estimation. Computational Linguistics, 19:263?311.
Woosung Kim. 2005. Language Model Adaptation for
Automatic Speech Recognition and Statistical Machine
Translation. Ph.D. thesis, The Johns Hopkins Univer-
sity, Baltimore, MD.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 311?318, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?08, pages 857?866, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
449
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 697?701,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Incremental Topic-Based Translation Model Adaptation for
Conversational Spoken Language Translation
Sanjika Hewavitharana, Dennis N. Mehay, Sankaranarayanan Ananthakrishnan
and Prem Natarajan
Speech, Language and Multimedia Business Unit
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{shewavit,dmehay,sanantha,pnataraj}@bbn.com
Abstract
We describe a translation model adapta-
tion approach for conversational spoken
language translation (CSLT), which en-
courages the use of contextually appropri-
ate translation options from relevant train-
ing conversations. Our approach employs
a monolingual LDA topic model to de-
rive a similarity measure between the test
conversation and the set of training con-
versations, which is used to bias trans-
lation choices towards the current con-
text. A significant novelty of our adap-
tation technique is its incremental nature;
we continuously update the topic distribu-
tion on the evolving test conversation as
new utterances become available. Thus,
our approach is well-suited to the causal
constraint of spoken conversations. On
an English-to-Iraqi CSLT task, the pro-
posed approach gives significant improve-
ments over a baseline system as measured
by BLEU, TER, and NIST. Interestingly,
the incremental approach outperforms a
non-incremental oracle that has up-front
knowledge of the whole conversation.
1 Introduction
Conversational spoken language translation
(CSLT) systems facilitate communication be-
tween subjects who do not speak the same
language. Current systems are typically used to
achieve a specific task (e.g. vehicle checkpoint
search, medical diagnosis, etc.). These task-driven
Disclaimer: This paper is based upon work supported by the
DARPA BOLT program. The views expressed here are those
of the authors and do not reflect the official policy or position
of the Department of Defense or the U.S. Government.
Distribution Statement A (Approved for Public Release,
Distribution Unlimited)
conversations typically revolve around a set of
central topics, which may not be evident at the
beginning of the interaction. As the conversation
progresses, however, the gradual accumulation of
contextual information can be used to infer the
topic(s) of discussion, and to deploy contextually
appropriate translation phrase pairs. For example,
the word ?drugs? will predominantly translate
into Spanish as ?medicamentos? (medicines) in a
medical scenario, whereas the translation ?drogas?
(illegal drugs) will predominate in a law enforce-
ment scenario. Most CSLT systems do not take
high-level global context into account, and instead
translate each utterance in isolation. This often
results in contextually inappropriate translations,
and is particularly problematic in conversational
speech, which usually exhibits short, spontaneous,
and often ambiguous utterances.
In this paper, we describe a novel topic-based
adaptation technique for phrase-based statistical
machine translation (SMT) of spoken conversa-
tions. We begin by building a monolingual la-
tent Dirichlet alocation (LDA) topic model on the
training conversations (each conversation corre-
sponds to a ?document? in the LDA paradigm).
At run-time, this model is used to infer a topic
distribution over the evolving test conversation up
to and including the current utterance. Transla-
tion phrase pairs that originate in training conver-
sations whose topic distribution is similar to that
of the current conversation are given preference
through a single similarity feature, which aug-
ments the standard phrase-based SMT log-linear
model. The topic distribution for the test conver-
sation is updated incrementally for each new utter-
ance as the available history grows. With this ap-
proach, we demonstrate significant improvements
over a baseline phrase-based SMT system as mea-
sured by BLEU, TER and NIST scores on an
English-to-Iraqi CSLT task.
697
2 Relation to Prior Work
Domain adaptation to improve SMT performance
has attracted considerable attention in recent years
(Foster and Kuhn, 2007; Finch and Sumita, 2008;
Matsoukas et al, 2009). The general theme is to
divide the training data into partitions representing
different domains, and to prefer translation options
for a test sentence from training domains that most
resemble the current document context. Weak-
nesses of this approach include (a) assuming the
existence of discrete, non-overlapping domains;
and (b) the unreliability of models generated by
segments with little training data.
To avoid the need for hard decisions about do-
main membership, some have used topic modeling
to improve SMT performance, e.g., using latent
semantic analysis (Tam et al, 2007) or ?biTAM?
(Zhao and Xing, 2006). In contrast to our source
language approach, these authors use both source
and target information.
Perhaps most relevant are the approaches of
Gong et al (2010) and Eidelman et al (2012),
who both describe adaptation techniques where
monolingual LDA topic models are used to ob-
tain a topic distribution over the training data, fol-
lowed by dynamic adaptation of the phrase table
based on the inferred topic of the test document.
While our proposed approach also employs mono-
lingual LDA topic models, it deviates from the
above methods in the following important ways.
First, the existing approaches are geared towards
batch-mode text translation, and assume that the
full document context of a test sentence is always
available. This assumption is incompatible with
translation of spoken conversations, which are in-
herently causal. Our proposed approach infers
topic distributions incrementally as the conversa-
tion progresses. Thus, it is not only consistent
with the causal requirement, but is also capable
of tracking topical changes during the course of a
conversation.
Second, we do not directly augment the trans-
lation table with the inferred topic distribution.
Rather, we compute a similarity between the cur-
rent conversation history and each of the training
conversations, and use this measure to dynami-
cally score the relevance of candidate translation
phrase pairs during decoding.
3 Corpus Data and Baseline SMT
We use the DARPA TransTac English-Iraqi par-
allel two-way spoken dialogue collection to train
both translation and LDA topic models. This data
set contains a variety of scenarios, including med-
ical diagnosis; force protection (e.g. checkpoint,
reconnaissance, patrol); aid, maintenance and in-
frastructure, etc.; each transcribed from spoken
bilingual conversations and manually translated.
The SMT parallel training corpus contains ap-
proximately 773K sentence pairs (7.3M English
words). We used this corpus to extract transla-
tion phrase pairs from bidirectional IBM Model
4 word alignment (Och and Ney, 2003) based on
the heuristic approach of (Koehn et al, 2003). A
4-gram target LM was trained on all Iraqi Ara-
bic transcriptions. Our phrase-based decoder is
similar to Moses (Koehn et al, 2007) and uses
the phrase pairs and target LM to perform beam
search stack decoding based on a standard log-
linear model, the parameters of which were tuned
with MERT (Och, 2003) on a held-out develop-
ment set (3,534 sentence pairs, 45K words) using
BLEU as the tuning metric. Finally, we evaluated
translation performance on a separate, unseen test
set (3,138 sentence pairs, 38K words).
Of the 773K training sentence pairs, about
100K (corresponding to 1,600 conversations) are
marked with conversation boundaries. We use the
English side of these conversations for training
LDA topic models. All other sentence pairs are
assigned to a ?background conversation?, which
signals the absence of the topic similarity feature
for phrase pairs derived from these instances. All
of the development and test set data were marked
with conversation boundaries. The training, devel-
opment and test sets were partitioned at the con-
versation level, so that we could model a topic
distribution for entire conversations, both during
training and during tuning and testing.
4 Incremental Topic-Based Adaptation
Our approach is based on the premise that biasing
the translation model to favor phrase pairs origi-
nating in training conversations that are contextu-
ally similar to the current conversation will lead
to better translation quality. The topic distribution
is incrementally updated as the conversation his-
tory grows, and we recompute the topic similarity
between the current conversation and the training
conversations for each new source utterance.
698
4.1 Topic modeling with LDA
We use latent Dirichlet alocation, or LDA, (Blei et
al., 2003) to obtain a topic distribution over con-
versations. For each conversation di in the train-
ing collection (1,600 conversations), LDA infers a
topic distribution ?di = p(zk|di) for all latent top-
ics zk = {1, ...,K}, where K is the number of
topics. In this work, we experiment with values
of K ? {20, 30, 40}. The full conversation his-
tory is available for training the topic models and
estimating topic distributions in the training set.
At run-time, however, we construct the con-
versation history for the tuning and test sets in-
crementally, one utterance at a time, mirroring a
real-world scenario where our knowledge is lim-
ited to the utterances that have been spoken up to
that point in time. Thus, each development/test ut-
terance is associated with a different conversation
history d?, for which we infer a topic distribution
?d? = p(zk|d?) using the trained LDA model. We
use Mallet (McCallum, 2002) for training topic
models and inferring topic distributions.
4.2 Topic Similarity Computation
For each test utterance, we are able to infer the
topic distribution ?d? based on the accumulated
history of the current conversation. We use this
to compute a measure of similarity between the
evolving test conversation and each of the train-
ing conversations, for which we already have topic
distributions ?di . Because ?di and ?d? are proba-
bility distributions, we use the Jensen-Shannon di-
vergence (JSD) to evaluate their similarity (Man-
ning and Schu?tze, 1999). The JSD is a smoothed
and symmetric version of Kullback-Leibler diver-
gence, which is typically used to compare two
probability distributions. We define the similar-
ity score as sim(?di , ?d?) = 1? JSD(?di ||?d?).1
Thus, we obtain a vector of similarity scores in-
dexed by the training conversations.
4.3 Integration with the Decoder
We provide the SMT decoder with the similar-
ity vector for each test utterance. Additionally,
the SMT phrase table tracks, for each phrase pair,
the set of parent training conversations (including
the ?background conversation?) from which that
phrase pair originated. Using this information, the
decoder evaluates, for each candidate phrase pair
1JSD(?di ||?d?) ? [0, 1] when defined using log2.
REFERENCE TRANSCRIPTIONS
SYSTEM BLEU? TER? NIST?
Baseline 19.32 58.66 6.22
incr20 19.39 58.44 6.26*
incr30 19.36 58.32* 6.26
incr40 19.68* 58.19* 6.28*
conv20 19.60* 58.36* 6.27*
conv30 19.48 58.38* 6.27*
conv40 19.50 58.33* 6.28*
ASR TRANSCRIPTIONS
SYSTEM BLEU? TER? NIST?
Baseline 16.92 62.57 5.75
incr20 16.99 62.28* 5.77
incr30 16.96 62.33* 5.78
incr40 17.31* 61.97* 5.83*
conv20 17.29* 62.28* 5.81*
conv30 17.12 62.19* 5.80*
conv40 17.00 62.14* 5.79*
Table 1: Stemmed results on 3,138-utterance test
set. Asterisked results are significantly better than
the baseline (p ? 0.05) using 1,000 iterations
of paired bootstrap re-sampling (Koehn, 2004).
(Key: incrN = incremental LDA with N topics;
convN = non-incremental, whole-conversation
LDA with N topics.)
X ? Y added to the search graph, its topic simi-
larity score as follows:
FX?Y = max
i?Par(X?Y )
sim(?di , ?d?) (1)
where Par(X ? Y ) is the set of training con-
versations from which the candidate phrase pair
originated. Phrase pairs from the ?background
conversation? only are assigned a similarity score
FX?Y = 0.00. In this way we distill the in-
ferred topic distributions down to a single feature
for each candidate phrase pair. We add this fea-
ture to the log-linear translation model with its
own weight, which is tuned with MERT. The in-
tuition behind this feature is that the lower bound
of suitability of a candidate phrase pair should be
directly proportional to the similarity between its
most relevant conversational provenance and the
current context. Phrase pairs which only occur in
the background conversation are not directly pe-
nalized, but contribute nothing to the topic simi-
larity score.
699
Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. The x-axis
indicates the utterance number. The y-axis indicates a topic?s rank at each utterance.
5 Experimental Setup and Results
The baseline English-to-Iraqi phrase-based SMT
system was built as described in Section 3. This
system translated each utterance independently,
ignoring higher-level conversational context.
For the topic-adapted system, we compared
translation performance with a varying number of
LDA topics. In intuitive agreement with the ap-
proximate number of scenario types known to be
covered by our data set, a range of 20-40 topics
yielded the best results. We compared the pro-
posed incremental topic tracking approach to a
non-causal oracle approach that had up-front ac-
cess to the entire source conversations at run-time.
In all cases, we compared translation perfor-
mance on both clean-text and automatic speech
recognition (ASR) transcriptions of the source ut-
terances. ASR transcriptions were generated using
a high-performance two-pass HMM-based sys-
tem, which delivered a word error rate (WER) of
10.6% on the test set utterances.
Table 1 summarizes test set performance in
BLEU (Papineni et al, 2001), NIST (Doddington,
2002) and TER (Snover et al, 2006). Given the
morphological complexity of Iraqi Arabic, com-
puting string-based metrics on raw output can
be misleadingly low and does not always reflect
whether the core message was conveyed. Since
the primary goal of CSLT is information transfer,
we present automatic results that are computed af-
ter stemming with an Iraqi Arabic stemmer.
We note that in all settings (incremental
and non-causal oracle) our adaptation approach
matches or significantly outperforms the baseline
across multiple evaluation metrics. In particular,
the incremental LDA system with 40 topics is the
top-scoring system in both clean-text and ASR set-
tings. In the ASR setting, which simulates a real-
world deployment scenario, this system achieves
improvements of 0.39 (BLEU), -0.6 (TER) and
0.08 (NIST).
6 Discussion and Future Directions
We have presented a novel, incremental topic-
based translation model adaptation approach that
obeys the causality constraint imposed by spoken
conversations. This approach yields statistically
significant gains in standard MT metric scores.
We have also demonstrated that incremental
adaptation on an evolving conversation performs
better than oracle adaptation based on the com-
plete conversation history. Although this may
seem counter-intuitive, Figure 1 gives clues as to
why this happens. This figure illustrates the rank
trajectory of four LDA topics as the incremen-
tal conversation grows. The accompanying text
shows excerpts from the conversation. We indi-
cate (in superscript) the topic identity of most rele-
vant words in an utterance that are associated with
that topic. At the first utterance, the top-ranked
topic is ?5?, due to the occurrence of ?captain?
in the greeting. As the conversation evolves, we
note that this topic become less prominent. The
conversation shifts to a discussion on ?windows?,
raising the prominence of topic ?4?. Finally, topic
?3? becomes prominent due to the presence of the
700
words ?project? and ?contract?. Thus, the incre-
mental approach is able to track the topic trajecto-
ries in the conversation, and is able to select more
relevant phrase pairs than oracle LDA, which esti-
mates one topic distribution for the entire conver-
sation.
In this work we have used only the source lan-
guage utterance in inferring the topic distribution.
In a two-way CLST system, we also have access
to SMT-generated back-translations in the Iraqi-
English direction. As a next step, we plan to use
SMT-generated English translation of Iraqi utter-
ances to improve topic estimation.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers - Volume 2, ACL
?12, pages 115?119, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on
Statistical Machine Translation, StatMT ?08, pages
208?215, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 128?135, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhengxian Gong, Yu Zhang, and Guodong Zhou.
2010. Statistical machine translation based on LDA.
In Universal Communication Symposium (IUCS),
2010 4th International, pages 286?290.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395, Barcelona, Spain, July.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 708?717, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings AMTA, pages 223?231, August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, December.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?06).
701
