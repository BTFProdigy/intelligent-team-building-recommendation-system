Probabilistic Sentence Reduction Using Support Vector Machines
Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi
Bao Tu Ho and Masaru Fukushi
Japan Advanced Institute of Science and Technology
1-8, Tatsunokuchi, Ishikawa, 923-1211, JAPAN
{nguyenml, shimazu, hori, bao, mfukushi}@jaist.ac.jp
Abstract
This paper investigates a novel application of sup-
port vector machines (SVMs) for sentence reduction.
We also propose a new probabilistic sentence reduc-
tion method based on support vector machine learn-
ing. Experimental results show that the proposed
methods outperform earlier methods in term of sen-
tence reduction performance.
1 Introduction
The most popular methods of sentence reduc-
tion for text summarization are corpus based
methods. Jing (Jing 00) developed a method
to remove extraneous phrases from sentences
by using multiple sources of knowledge to de-
cide which phrases could be removed. However,
while this method exploits a simple model for
sentence reduction by using statistics computed
from a corpus, a better model can be obtained
by using a learning approach.
Knight and Marcu (Knight and Marcu 02)
proposed a corpus based sentence reduction
method using machine learning techniques.
They discussed a noisy-channel based approach
and a decision tree based approach to sentence
reduction. Their algorithms provide the best
way to scale up the full problem of sentence re-
duction using available data. However, these al-
gorithms require that the word order of a given
sentence and its reduced sentence are the same.
Nguyen and Horiguchi (Nguyen and Horiguchi
03) presented a new sentence reduction tech-
nique based on a decision tree model without
that constraint. They also indicated that se-
mantic information is useful for sentence reduc-
tion tasks.
The major drawback of previous works on
sentence reduction is that those methods are
likely to output local optimal results, which may
have lower accuracy. This problem is caused by
the inherent sentence reduction model; that is,
only a single reduced sentence can be obtained.
As pointed out by Lin (Lin 03), the best sen-
tence reduction output for a single sentence is
not approximately best for text summarization.
This means that ?local optimal? refers to the
best reduced output for a single sentence, while
the best reduced output for the whole text is
?global optimal?. Thus, it would be very valu-
able if the sentence reduction task could gener-
ate multiple reduced outputs and select the best
one using the whole text document. However,
such a sentence reduction method has not yet
been proposed.
Support Vector Machines (Vapnik 95), on the
other hand, are strong learning methods in com-
parison with decision tree learning and other
learning methods (Sholkopf 97). The goal of
this paper is to illustrate the potential of SVMs
for enhancing the accuracy of sentence reduc-
tion in comparison with previous work. Accord-
ingly, we describe a novel deterministic method
for sentence reduction using SVMs and a two-
stage method using pairwise coupling (Hastie
98). To solve the problem of generating mul-
tiple best outputs, we propose a probabilistic
sentence reduction model, in which a variant of
probabilistic SVMs using a two-stage method
with pairwise coupling is discussed.
The rest of this paper will be organized as
follows: Section 2 introduces the Support Vec-
tor Machines learning. Section 3 describes the
previous work on sentence reduction and our
deterministic sentence reduction using SVMs.
We also discuss remaining problems of deter-
ministic sentence reduction. Section 4 presents
a probabilistic sentence reduction method using
support vector machines to solve this problem.
Section 5 discusses implementation and our ex-
perimental results; Section 6 gives our conclu-
sions and describes some problems that remain
to be solved in the future.
2 Support Vector Machine
Support vector machine (SVM)(Vapnik 95) is a
technique of machine learning based on statisti-
cal learning theory. Suppose that we are given
l training examples (xi, yi), (1 ? i ? l), where
xi is a feature vector in n dimensional feature
space, yi is the class label {-1, +1 } of xi. SVM
finds a hyperplane w.x + b = 0 which correctly
separates the training examples and has a max-
imum margin which is the distance between two
hyperplanes w.x+ b ? 1 and w.x+ b ? ?1. The
optimal hyperplane with maximum margin can
be obtained by solving the following quadratic
programming.
min 12 ?w?+ C0
l?
i
?i
s.t. yi(w.xi + b) ? 1? ?i
?i ? 0
(1)
where C0 is the constant and ?i is a slack vari-
able for the non-separable case. In SVM, the
optimal hyperplane is formulated as follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(2)
where ?i is the Lagrange multiple, and
K(x?, x??) is a kernel function, the SVM calcu-
lates similarity between two arguments x? and
x??. For instance, the Polynomial kernel func-
tion is formulated as follow:
K(x?, x??) = (x?.x??)p (3)
SVMs estimate the label of an unknown ex-
ample x whether the sign of f(x) is positive or
not.
3 Deterministic Sentence Reduction
Using SVMs
3.1 Problem Description
In the corpus-based decision tree approach, a
given input sentence is parsed into a syntax tree
and the syntax tree is then transformed into a
small tree to obtain a reduced sentence.
Let t and s be syntax trees of the original sen-
tence and a reduced sentence, respectively. The
process of transforming syntax tree t to small
tree s is called ?rewriting process? (Knight and
Marcu 02), (Nguyen and Horiguchi 03). To
transform the syntax tree t to the syntax tree
s, some terms and five rewriting actions are de-
fined.
An Input list consists of a sequence of words
subsumed by the tree t where each word in the
Input list is labelled with the name of all syntac-
tic constituents in t. Let CSTACK be a stack
that consists of sub trees in order to rewrite a
small tree. Let RSTACK be a stack that con-
sists of sub trees which are removed from the
Input list in the rewriting process.
? SHIFT action transfers the first word from the
Input list into CSTACK. It is written mathe-
matically and given the label SHIFT.
? REDUCE(lk,X) action pops the lk syntactic
trees located at the top of CSTACK and com-
bines them in a new tree, where lk is an integer
and X is a grammar symbol.
? DROP X action moves subsequences of words
that correspond to syntactic constituents from
the Input list to RSTACK.
? ASSIGN TYPE X action changes the label of
trees at the top of the CSTACK. These POS
tags might be different from the POS tags in
the original sentence.
? RESTORE X action takes the X element in
RSTACK and moves it into the Input list,
where X is a subtree.
For convenience, let configuration be a status
of Input list, CSTACK and RSTACK. Let cur-
rent context be the important information in a
configuration. The important information are
defined as a vector of features using heuristic
methods as in (Knight and Marcu 02), (Nguyen
and Horiguchi 03).
The main idea behind deterministic sentence
reduction is that it uses a rule in the current
context of the initial configuration to select a
distinct action in order to rewrite an input sen-
tence into a reduced sentence. After that, the
current context is changed to a new context and
the rewriting process is repeated for selecting
an action that corresponds to the new context.
The rewriting process is finished when it meets
a termination condition. Here, one rule corre-
sponds to the function that maps the current
context to a rewriting action. These rules are
learned automatically from the corpus of long
sentences and their reduced sentences (Knight
and Marcu 02), (Nguyen and Horiguchi 03).
3.2 Example
Figure 1 shows an example of applying a se-
quence of actions to rewrite the input sentence
(a, b, c, d, e), when each character is a word. It
illustrates the structure of the Input list, two
stacks, and the term of a rewriting process based
on the actions mentioned above. For example,
in the first row, DROP H deletes the sub-tree
with its root node H in the Input list and stores
it in the RSTACK. The reduced tree s can be
obtained after applying a sequence of actions
as follows: DROP H; SHIFT; ASSIGN TYPE K;
DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2
F; RESTORE H; SHIFT; ASSIGN TYPE D; RE-
DUCE 2G. In this example, the reduced sentence
is (b, e, a).
Figure 1: An Example of the Rewriting Process
3.3 Learning Reduction Rules Using
SVMs
As mentioned above, the action for each config-
uration can be decided by using a learning rule,
which maps a context to an action. To obtain
such rules, the configuration is represented by
a vector of features with a high dimension. Af-
ter that, we estimate the training examples by
using several support vector machines to deal
with the multiple classification problem in sen-
tence reduction.
3.3.1 Features
One important task in applying SVMs to text
summarization is to define features. Here, we
describe features used in our sentence reduction
models.
The features are extracted based on the cur-
rent context. As it can be seen in Figure 2, a
context includes the status of the Input list and
the status of CSTACK and RSTACK. We de-
fine a set of features for a current context as
described bellow.
Operation feature
The set of features as described in (Nguyen and
Horiguchi 03) are used in our sentence reduction
models.
Original tree features
These features denote the syntactic constituents
Figure 2: Example of Configuration
that start with the first unit in the Input list.
For example, in Figure 2 the syntactic con-
stituents are labels of the current element in the
Input list from ?VP? to the verb ?convince?.
Semantic features
The following features are used in our model as
semantic information.
? Semantic information about current words
within the Input list; these semantic types
are obtained by using the named entities such
as Location, Person, Organization and Time
within the input sentence. To define these
name entities, we use the method described in
(Borthwick 99).
? Semantic information about whether or not the
word in the Input list is a head word.
? Word relations, such as whether or not a word
has a relationship with other words in the sub-
categorization table. These relations and the
sub-categorization table are obtained using the
Commlex database (Macleod 95).
Using the semantic information, we are able to
avoid deleting important segments within the
given input sentence. For instance, the main
verb, the subject and the object are essential
and for the noun phrase, the head noun is essen-
tial, but an adjective modifier of the head noun
is not. For example, let us consider that the
verb ?convince? was extracted from the Com-
lex database as follows.
convince
NP-PP: PVAL (?of?)
NP-TO-INF-OC
This entry indicates that the verb ?convince?
can be followed by a noun phrase and a preposi-
tional phrase starting with the preposition ?of?.
It can be also followed by a noun phrase and a
to-infinite phrase. This information shows that
we cannot delete an ?of? prepositional phrase
or a to-infinitive that is the part of the verb
phrase.
3.3.2 Two-stage SVM Learning using
Pairwise Coupling
Using these features we can extract training
data for SVMs. Here, a sample in our training
data consists of pairs of a feature vector and
an action. The algorithm to extract training
data from the training corpus is modified using
the algorithm described in our pervious work
(Nguyen and Horiguchi 03).
Since the original support vector machine
(SVM) is a binary classification method, while
the sentence reduction problem is formulated as
multiple classification, we have to find a method
to adapt support vector machines to this prob-
lem. For multi-class SVMs, one can use strate-
gies such as one-vs all, pairwise comparison or
DAG graph (Hsu 02). In this paper, we use the
pairwise strategy, which constructs a rule for
discriminating pairs of classes and then selects
the class with the most winning among two class
decisions.
To boost the training time and the sentence
reduction performance, we propose a two-stage
SVM described below.
Suppose that the examples in training data
are divided into five groups m1,m2, ...,m5 ac-
cording to their actions. Let Svmc be multi-
class SVMs and let Svmc-i be multi-class SVMs
for a group mi. We use one Svmc classifier to
identify the group to which a given context e
should be belong. Assume that e belongs to
the group mi. The classifier Svmc-i is then used
to recognize a specific action for the context e.
The five classifiers Svmc-1, Svmc-2,..., Svmc-5
are trained by using those examples which have
actions belonging to SHIFT, REDUCE, DROP,
ASSIGN TYPE and RESTORE.
Table 1 shows the distribution of examples in
five data groups.
3.4 Disadvantage of Deterministic
Sentence Reductions
The idea of the deterministic algorithm is to
use the rule for each current context to select
the next action, and so on. The process termi-
nates when a stop condition is met. If the early
steps of this algorithm fail to select the best ac-
Table 1: Distribution of example data on five
data groups
Name Number of examples
SHIFT-GROUP 13,363
REDUCE-GROUP 11,406
DROP-GROUP 4,216
ASSIGN-GROUP 13,363
RESTORE-GROUP 2,004
TOTAL 44,352
tions, then the possibility of obtaining a wrong
reduced output becomes high.
One way to solve this problem is to select mul-
tiple actions that correspond to the context at
each step in the rewriting process. However,
the question that emerges here is how to deter-
mine which criteria to use in selecting multiple
actions for a context. If this problem can be
solved, then multiple best reduced outputs can
be obtained for each input sentence and the best
one will be selected by using the whole text doc-
ument.
In the next section propose a model for se-
lecting multiple actions for a context in sentence
reduction as a probabilistic sentence reduction
and present a variant of probabilistic sentence
reduction.
4 Probabilistic Sentence Reduction
Using SVM
4.1 The Probabilistic SVM Models
Let A be a set of k actions A =
{a1, a2...ai, ..., ak} and C be a set of n con-
texts C = {c1, c2...ci, ..., cn} . A probabilistic
model ? for sentence reduction will select an
action a ? A for the context c with probability
p?(a|c). The p?(a|c) can be used to score ac-
tion a among possible actions A depending the
context c that is available at the time of deci-
sion. There are several methods for estimating
such scores; we have called these ?probabilistic
sentence reduction methods?. The conditional
probability p?(a|c) is estimated using a variant
of probabilistic support vector machine, which
is described in the following sections.
4.1.1 Probabilistic SVMs using
Pairwise Coupling
For convenience, we denote uij = p(a = ai|a =
ai?aj , c). Given a context c and an action a, we
assume that the estimated pairwise class prob-
abilities rij of uij are available. Here rij can
be estimated by some binary classifiers. For
instance, we could estimate rij by using the
SVM binary posterior probabilities as described
in (Plat 2000). Then, the goal is to estimate
{pi}ki=1 , where pi = p(a = ai|c), i = 1, 2, ..., k.For this propose, a simple estimate of these
probabilities can be derived using the following
voting method:
pi = 2
?
j:j 6=i
I{rij>rji}/k(k ? 1)
where I is an indicator function and k(k? 1) is
the number of pairwise classes. However, this
model is too simple; we can obtain a better one
with the following method.
Assume that uij are pairwise probabilities of
the model subject to the condition that uij =
pi/(pi+pj). In (Hastie 98), the authors proposed
to minimize the Kullback-Leibler (KL) distance
between the rij and uij
l(p) =
?
i 6=j
nijrij log rijuij (4)
where rij and uij are the probabilities of a pair-
wise ai and aj in the estimated model and in
our model, respectively, and nij is the number
of training data in which their classes are ai or
aj . To find the minimizer of equation (6), they
first calculate
?l(p)
?pi =
?
i 6=j
nij(?rijpi +
1
pi + pj ).
Thus, letting ?l(p) = 0, they proposed to find
a point satisfying
?
j:j 6=i
nijuij =
?
j:j 6=i
nijrij ,
k?
i=1
pi = 1,
where i = 1, 2, ...k and pi > 0.
Such a point can be obtained by using an algo-
rithm described elsewhere in (Hastie 98). We
applied it to obtain a probabilistic SVM model
for sentence reduction using a simple method as
follows. Assume that our class labels belong to
l groups: M = {m1,m2...mi, ...,ml} , where l
is a number of groups and mi is a group e.g.,
SHIFT, REDUCE ,..., ASSIGN TYPE. Then
the probability p(a|c) of an action a for a given
context c can be estimated as follows.
p(a|c) = p(mi|c)? p(a|c,mi) (5)
where mi is a group and a ? mi. Here, p(mi|c)
and p(a|c,mi) are estimated by the method in
(Hastie 98).
4.2 Probabilistic sentence reduction
algorithm
After obtaining a probabilistic model p, we then
use this model to define function score, by which
the search procedure ranks the derivation of in-
complete and complete reduced sentences. Let
d(s) = {a1, a2, ...ad} be the derivation of a small
tree s, where each action ai belongs to a set of
possible actions. The score of s is the product
of the conditional probabilities of the individual
actions in its derivation.
Score(s) =
?
ai?d(s)
p(ai|ci) (6)
where ci is the context in which ai was decided.
The search heuristic tries to find the best re-
duced tree s? as follows:
s? = argmax? ?? ?
s?tree(t)
Score(s) (7)
where tree(t) are all the complete reduced trees
from the tree t of the given long sentence. As-
sume that for each configuration the actions
{a1, a2, ...an} are sorted in decreasing order ac-
cording to p(ai|ci), in which ci is the context
of that configuration. Algorithm 1 shows a
probabilistic sentence reduction using the top
K-BFS search algorithm. This algorithm uses
a breadth-first search which does not expand
the entire frontier, but instead expands at most
the top K scoring incomplete configurations in
the frontier; it is terminated when it finds M
completed reduced sentences (CL is a list of re-
duced trees), or when all hypotheses have been
exhausted. A configuration is completed if and
only if the Input list is empty and there is one
tree in the CSTACK. Note that the function
get-context(hi, j) obtains the current context of
the jth configuration in hi, where hi is a heap at
step i. The function Insert(s,h) ensures that the
heap h is sorted according to the score of each
element in h. Essentially, in implementation we
can use a dictionary of contexts and actions ob-
served from the training data in order to reduce
the number of actions to explore for a current
context.
5 Experiments and Discussion
We used the same corpus as described in
(Knight and Marcu 02), which includes 1,067
pairs of sentences and their reductions. To
evaluate sentence reduction algorithms, we ran-
domly selected 32 pairs of sentences from our
parallel corpus, which is refered to as the test
corpus. The training corpus of 1,035 sentences
extracted 44,352 examples, in which each train-
ing example corresponds to an action. The
SVM tool, LibSVM (Chang 01) is applied to
train our model. The training examples were
Algorithm 1 A probabilistic sentence reduction
algorithm
1: CL={Empty};
i = 0; h0={ Initial configuration}
2: while |CL| < M do
3: if hi is empty then4: break;5: end if6: u =min(|hi|, K)7: for j = 1 to u do8: c=get-context(hi, j)
9: Select m so that
m?
i=1
p(ai|c) < Q is maximal
10: for l=1 to m do11: parameter=get-parameter(al);12: Obtain a new configuration s by performing action al
with parameter
13: if Complete(s) then
14: Insert(s, CL)
15: else16: Insert(s, hi+1)17: end if18: end for19: end for20: i = i + 1
21: end while
divided into SHIFT, REDUCE, DROP, RE-
STORE, and ASSIGN groups. To train our
support vector model in each group, we used
the pairwise method with the polynomial ker-
nel function, in which the parameter p in (3)
and the constant C0 in equation (1) are 2 and
0.0001, respectively.
The algorithms (Knight and Marcu 02) and
(Nguyen and Horiguchi 03) served as the base-
line1 and the baseline2 for comparison with the
proposed algorithms. Deterministic sentence re-
duction using SVM and probabilistic sentence
reduction were named as SVM-D and SVMP, re-
spectively. For convenience, the ten top reduced
outputs using SVMP were called SVMP-10. We
used the same evaluation method as described
in (Knight and Marcu 02) to compare the pro-
posed methods with previous methods. For this
experiment, we presented each original sentence
in the test corpus to three judges who are spe-
cialists in English, together with three sentence
reductions: the human generated reduction sen-
tence, the outputs of the proposed algorithms,
and the output of the baseline algorithms.
The judges were told that all outputs were
generated automatically. The order of the out-
puts was scrambled randomly across test cases.
The judges participated in two experiments. In
the first, they were asked to determine on a scale
from 1 to 10 how well the systems did with re-
spect to selecting the most important words in
the original sentence. In the second, they were
asked to determine the grammatical criteria of
reduced sentences.
Table 2 shows the results of English language
sentence reduction using a support vector ma-
chine compared with the baseline methods and
with human reduction. Table 2 shows compres-
sion rates, and mean and standard deviation re-
sults across all judges, for each algorithm. The
results show that the length of the reduced sen-
tences using decision trees is shorter than using
SVMs, and indicate that our new methods out-
perform the baseline algorithms in grammatical
and importance criteria. Table 2 shows that the
Table 2: Experiment results with Test Corpus
Method Comp Gramma Impo
Baseline1 57.19% 8.60? 2.8 7.18? 1.92
Baseline2 57.15% 8.60? 2.1 7.42? 1.90
SVM-D 57.65% 8.76? 1.2 7.53? 1.53
SVMP-10 57.51% 8.80? 1.3 7.74? 1.39
Human 64.00% 9.05? 0.3 8.50? 0.80
first 10 reduced sentences produced by SVMP-
10 (the SVM probabilistic model) obtained the
highest performances. We also compared the
computation time of sentence reduction using
support vector machine with that in previous
works. Table 3 shows that the computational
times for SVM-D and SVMP-10 are slower than
baseline, but it is acceptable for SVM-D.
Table 3: Computational times of performing re-
ductions on test-set. Average sentence length
was 21 words.
Method Computational times (sec)
Baseline1 138.25
SVM-D 212.46
SVMP-10 1030.25
We also investigated how sensitive the pro-
posed algorithms are with respect to the train-
ing data by carrying out the same experi-
ment on sentences of different genres. We
created the test corpus by selecting sentences
from the web-site of the Benton Foundation
(http://www.benton.org). The leading sen-
tences in each news article were selected as the
most relevant sentences to the summary of the
news. We obtained 32 leading long sentences
and 32 headlines for each item. The 32 sen-
tences are used as a second test for our methods.
We use a simple ranking criterion: the more the
words in the reduced sentence overlap with the
words in the headline, the more important the
sentence is. A sentence satisfying this criterion
is called a relevant candidate.
For a given sentence, we used a simple
method, namely SVMP-R to obtain a re-
duced sentence by selecting a relevant candi-
date among the ten top reduced outputs using
SVMP-10.
Table 4 depicts the experiment results for
the baseline methods, SVM-D, SVMP-R, and
SVMP-10. The results shows that, when ap-
plied to sentence of a different genre, the per-
formance of SVMP-10 degrades smoothly, while
the performance of the deterministic sentence
reductions (the baselines and SVM determinis-
tic) drops sharply. This indicates that the prob-
abilistic sentence reduction using support vector
machine is more stable.
Table 4 shows that the performance of
SVMP-10 is also close to the human reduction
outputs and is better than previous works. In
addition, SVMP-R outperforms the determin-
istic sentence reduction algorithms and the dif-
ferences between SVMP-R?s results and SVMP-
10 are small. This indicates that we can ob-
tain reduced sentences which are relevant to the
headline, while ensuring the grammatical and
the importance criteria compared to the origi-
nal sentences.
Table 4: Experiment results with Benton Cor-
pus
Method Comp Gramma Impo
Baseline1 54.14% 7.61? 2.10 6.74? 1.92
Baseline2 53.13% 7.72? 1.60 7.02? 1.90
SVM-D 56.64% 7.86? 1.20 7.23? 1.53
SVMP-R 58.31% 8.25? 1.30 7.54? 1.39
SVMP-10 57.62% 8.60? 1.32 7.71? 1.41
Human 64.00% 9.01? 0.25 8.40? 0.60
6 Conclusions
We have presented a new probabilistic sentence
reduction approach that enables a long sentence
to be rewritten into reduced sentences based on
support vector models. Our methods achieves
better performance when compared with earlier
methods. The proposed reduction approach can
generate multiple best outputs. Experimental
results showed that the top 10 reduced sentences
returned by the reduction process might yield
accuracies higher than previous work. We be-
lieve that a good ranking method might improve
the sentence reduction performance further in a
text.
References
A. Borthwick, ?A Maximum Entropy Approach
to Named Entity Recognition?, Ph.D the-
sis, Computer Science Department, New York
University (1999).
C.-C. Chang and C.-J. Lin, ?LIB-
SVM: a library for support vec-
tor machines?, Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
H. Jing, ?Sentence reduction for automatic
text summarization?, In Proceedings of the
First Annual Meeting of the North Ameri-
can Chapter of the Association for Compu-
tational Linguistics NAACL-2000.
T.T. Hastie and R. Tibshirani, ?Classification
by pairwise coupling?, The Annals of Statis-
tics, 26(1): pp. 451-471, 1998.
C.-W. Hsu and C.-J. Lin, ?A comparison of
methods for multi-class support vector ma-
chines?, IEEE Transactions on Neural Net-
works, 13, pp. 415-425, 2002.
K. Knight and D. Marcu, ?Summarization be-
yond sentence extraction: A Probabilistic ap-
proach to sentence compression?, Artificial
Intelligence 139: pp. 91-107, 2002.
C.Y. Lin, ?Improving Summarization Perfor-
mance by Sentence Compression ? A Pi-
lot Study?, Proceedings of the Sixth Inter-
national Workshop on Information Retrieval
with Asian Languages, pp.1-8, 2003.
C. Macleod and R. Grishman, ?COMMLEX
syntax Reference Manual?; Proteus Project,
New York University (1995).
M.L. Nguyen and S. Horiguchi, ?A new sentence
reduction based on Decision tree model?,
Proceedings of 17th Pacific Asia Conference
on Language, Information and Computation,
pp. 290-297, 2003
V. Vapnik, ?The Natural of Statistical Learning
Theory?, New York: Springer-Verlag, 1995.
J. Platt,? Probabilistic outputs for support vec-
tor machines and comparison to regularized
likelihood methods,? in Advances in Large
Margin Classifiers, Cambridege, MA: MIT
Press, 2000.
B. Scholkopf et al ?Comparing Support Vec-
tor Machines with Gausian Kernels to Radius
Basis Function Classifers?, IEEE Trans. Sig-
nal Procesing, 45, pp. 2758-2765, 1997.
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 9?16,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Empirical Study of Vietnamese Noun Phrase Chunking with
Discriminative Sequence Models
Le Minh Nguyen
School of Information Science, JAIST
nguyenml@jaist.ac.jp
Huong Thao Nguyen and Phuong Thai Nguyen
College of Technology, VNU
{thaonth, thainp}@vnu.edu.vn
Tu Bao Ho and Akira Shimazu
Japan Advanced Institute of Science and Technology
{bao,shimazu}@jaist.ac.jp
Abstract
This paper presents an empirical work
for Vietnamese NP chunking task. We
show how to build an annotation corpus of
NP chunking and how discriminative se-
quence models are trained using the cor-
pus. Experiment results using 5 fold cross
validation test show that discriminative se-
quence learning are well suitable for Viet-
namese chunking. In addition, by em-
pirical experiments we show that the part
of speech information contribute signifi-
cantly to the performance of there learning
models.
1 Introduction
Many Natural Language Processing applications
(i.e machine translation) require syntactic infor-
mation and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages(i.e English, Japanese, Chines). In
the case of Vietnamese, currently most researchers
have focused on word segmentation and part of
speech tagging. For example, Nghiem et al
(Nghiem, Dinh, Nguyen, 2008) has developed a
Vietnamese POS tagging. Tu (Tu, Phan, Nguyen,
Ha, 2006) (Nguyen, Romary, Rossignol, Vu,
2006)(Dien, Thuy, 2006) have developed Viet-
namese word segmentation.
The processing of building tools and annotated
data for other fundamental tasks such as chunk-
ing and syntactic parsing are currently developed.
This can be viewed as a bottleneck for develop-
ing NLP applications that require a deeper under-
standing of the language. The requirement of de-
veloping such tools motives us to develop a Viet-
namese chunking tool. For this goal, we have
been looking for an annotation corpus for conduct-
ing a Vietnamese chunking using machine learn-
ing methods. Unfortunately, at the moment, there
is still no common standard annotated corpus for
evaluation and comparison regarding Vietnamese
chunking.
In this paper, we aim at discussing on how
we can build annotated data for Vietnamese text
chunking and how to apply discriminative se-
quence learning for Vietnamese text chunking. We
choose discriminative sequence models for Viet-
namese text chunking because they have shown
very suitable methods for several languages(i.e
English, Japanese, Chinese) (Sha and Pereira,
2005)(Chen, Zhang, and Ishihara, 2006) (Kudo
and Matsumoto, 2001). These presentative dis-
criminative models which we choose for conduct-
ing empirical experiments including: Conditional
Random Fields (Lafferty, McCallum, and Pereira,
2001), Support Vector Machine (Vapnik, 1995)
and Online Prediction (Crammer et al 2006). In
other words, because Noun Phrase chunks appear
most frequently in sentences. So, in this paper
we focus mainly on empirical experiments for the
tasks of Vietnamese NP chunking.
We plan to answer several major questions by
using empirical experiments as follows.
? Whether or not the discriminative learning
models are suitable for Vietnamese chunking
problem?
? We want to know the difference of SVM,
Online Learning, and Conditional Random
Fields for Vietnamese chunking task.
? Which features are suitable for discriminative
learning models and how they contribute to
the performance of Vietnamese text chunk-
ing?
The rest of this paper is organized as follows:
Section 2 describes Vietnamese text chunking with
discriminative sequence learning models. Section
3 shows experimental results and Section 4 dis-
9
cusses the advantage of our method and describes
future work.
2 Vietnamese NP Chunking with
Discriminative Sequence Learning
Noun Phrase chunking is considered as the task
of grouping a consecutive sequence of words into
a NP chunk lablel. For example: ?[NP Anh Ay
(He)] [VP thich(likes)] [NP mot chiec oto(a car)]
?
Before describing NP chunking tasks, we
summarize the characteristic of Vietnamese lan-
guage and the background of Conditional Ran-
dom Fields, Support Vector Machine, and Online
Learning. Then, we present how to build the an-
notated corpus for the NP chunking task.
2.1 The characteristic of Vietnamese Words
Vietnamese syllables are elementary units that
have one way of pronunciation. In documents,
they are usually delimited by white-space. Be-
ing the elementary units, Vietnamese syllables are
not undivided elements but a structure. Generally,
each Vietnamese syllable has all five parts: first
consonant, secondary vowel, main vowel, last con-
sonant and a tone mark. For instance, the sylla-
ble tu.n (week) has a tone mark (grave accent), a
first consonant (t), a secondary vowel (u), a main
vowel () and a last consonant (n). However, except
for main vowel that is required for all syllables,
the other parts may be not present in some cases.
For example, the syllable anh (brother) has no tone
mark, no secondary vowel and no first consonant.
In other case, the syllable hoa (flower) has a sec-
ondary vowel (o) but no last consonant.
Words in Vietnamese are made of one or more
syllables which are combined in different ways.
Based on the way of constructing words from syl-
lables, we can classify them into three categories:
single words, complex words and reduplicative
words (Mai,Vu, Hoang, 1997).
The past of speechs (Pos) of each word in Viet-
namese are mainly sketched as follows.
A Noun Phrase (NP) in Vietnamese consists of
three main parts as follows: the noun center, the
prefix part, and the post fix part. The prefix and
postfix are used to support the meaning of the NP.
For example in the NP ?ba sinh vien nay?, the
noun center is ?sinh vien?, and the prefix is ?ba
(three)?, the postfix is ?nay?.
Vietnamese Tag Equivalent to English Tag
CC Coordinating conjunction)
CD Cardinal number)
DT Determiner)
V Verb
P Preposition
A Adjective
LS List item marker
MD Modal
N Noun
Table 1: Part of Speeches in Vietnamese
2.2 The Corpus
We have collected more than 9,000 sentences from
several web-sites through the internet. After that,
we then applied the segmentation tool (Tu, Phan,
Nguyen, Ha, 2006) to segment each sentences
into a sequence of tokens. Each sequence of
tokens are then represented using the format of
CONLL 2000. The details are sketched as follows.
Each line in the annotated data consists of
three columns: the token (a word or a punc-
tuation mark), the part-of-speech tag of the to-
ken, and the phrase type label (label for short)
of the token. The label of each token indicates
whether the token is outside a phrase (O), starts
a phrase (B-?PhraseType?), or continues a phrase
(I-?PhraseType?).
In order to save time for building annotated
data, we made a set of simple rules for automat-
ically generating the chunking data as follows. If
a word is not a ?noun?, ?adjective?, or ?article? it
should be assigned the label ?O?. The consecu-
tive words are NP if they is one of type as follows:
?noun noun?; ?article noun?, ?article noun adjec-
tive?. After generating such as data, we ask an
expert about Vietnamese linguistic to correct the
data. Finally, we got more than 9,000 sentences
which are annotated with NP chunking labels.
Figure 1 shows an example of the Vietnamese
chunking corpus.
2.3 Discriminative Sequence Learning
In this section, we briefly introduce three dis-
criminative sequence learning models for chunk-
ing problems.
2.3.1 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty,
McCallum, and Pereira, 2001) are undirected
graphical models used to calculate the conditional
10
Figure 1: An Example of the Vietnamese chunk-
ing corpus
probability of values on designated output nodes,
given values assigned to other designated input
nodes for data sequences. CRFs make a first-order
Markov independence assumption among output
nodes, and thus correspond to finite state machine
(FSMs).
Let o = (o1, o2, . . . , oT ) be some observed in-
put data sequence, such as a sequence of words in
a text (values on T input nodes of the graphical
model). Let S be a finite set of FSM states, each is
associated with a label l such as a clause start po-
sition. Let s = (s1, s2, . . . , sT ) be some sequences
of states (values on T output nodes). CRFs de-
fine the conditional probability of a state sequence
given an input sequence to be
P?(s|o) = 1Zo exp
( T?
t=1
F (s, o, t)
)
(1)
where Zo =
?
s exp
(?T
t=1 F (s, o, t)
)
is a nor-
malization factor over all state sequences. We de-
note ? to be the Kronecker-?. Let F (s, o, t) be the
sum of CRFs features at time position t:
?
i
?ifi(st?1, st, t) +
?
j
?jgj(o, st, t) (2)
where fi(st?1, st, t) = ?(st?1, l?)?(st, l) is a
transition feature function which represents se-
quential dependencies by combining the label l?
of the previous state st?1 and the label l of the
current state st, such as the previous label l? =
AV (adverb) and the current label l = JJ (adjec-
tive). gj(o, st, t) = ?(st, l)xk(o, t) is a per-state
feature function which combines the label l of cur-
rent state st and a context predicate, i.e., the binary
function xk(o, t) that captures a particular prop-
erty of the observation sequence o at time position
t. For instance, the current label is JJ and the cur-
rent word is ?conditional?.
Training CRFs is commonly performed by max-
imizing the likelihood function with respect to
the training data using advanced convex optimiza-
tion techniques like L-BFGS. Recently, there are
several works apply Stochastic Gradient Descent
(SGD) for training CRFs models. SGD has been
historically associated with back-propagation al-
gorithms in multilayer neural networks.
And inference in CRFs, i.e., searching the most
likely output label sequence of an input observa-
tion sequence, can be done using Viterbi algo-
rithm.
2.3.2 Support Vector Machines
Support vector machine (SVM)(Vapnik, 1995)
is a technique of machine learning based on sta-
tistical learning theory. The main idea behind
this method can be summarized as follows. Sup-
pose that we are given l training examples (xi, yi),
(1 ? i ? l), where xi is a feature vector in n di-
mensional feature space, and yi is the class label
{-1, +1 } of xi.
SVM finds a hyperplane w.x+b = 0 which cor-
rectly separates training examples and has maxi-
mum margin which is the distance between two
hyperplanes w ? x + b ? 1 and w ? x + b ? ?1.
Finally, the optimal hyperplane is formulated as
follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(3)
where ?i is the Lagrange multiple, and K(x?, x??)
is called a kernel function, which calculates sim-
ilarity between two arguments x? and x??. For in-
stance, the Polynomial kernel function is formu-
lated as follows:
K(x?, x??) = (x? ? x??)p (4)
SVMs estimate the label of an unknown example
x whether the sign of f(x) is positive or not.
Basically, SVMs are binary classifier, thus we
must extend SVMs to multi-class classifier in or-
11
der to classify three or more classes. The pair-
wise classifier is one of the most popular meth-
ods to extend the binary classification task to that
of K classes. Though, we leave the details to
(Kudo and Matsumoto, 2001), the idea of pairwise
classification is to build K.(K-1)/2 classifiers con-
sidering all pairs of classes, and final decision is
given by their weighted voting. The implementa-
tion of Vietnamese text chunking is based on Yam-
cha (V0.33)1.
2.3.3 Online Passive-Aggressive Learning
Online Passive-Aggressive Learning (PA) was
proposed by Crammer (Crammer et al 2006) as
an alternative learning algorithm to the maximize
margin algorithm. The Perceptron style for nat-
ural language processing problems as initially pro-
posed by (Collins, 2002) can provide to state of
the art results on various domains including text
segmentation, syntactic parsing, and dependency
parsing. The main drawback of the Perceptron
style algorithm is that it does not have a mech-
anism for attaining the maximize margin of the
training data. It may be difficult to obtain high
accuracy in dealing with hard learning data. The
online algorithm for chunking parsing in which
we can attain the maximize margin of the training
data without using an optimization technique. It
is thus much faster and easier to implement. The
details of PA algorithm for chunking parsing are
presented as follows.
Assume that we are given a set of sentences
xi and their chunks yi where i = 1, ..., n. Let
the feature mapping between a sentence x and
a sequence of chunk labels y be: ?(x, y) =
?1(x, y),?2(x, y), ...,?d(x, y) where each fea-
ture mapping ?j maps (x, y) to a real value. We
assume that each feature ?(x, y) is associated with
a weight value. The goal of PA learning for chunk-
ing parsing is to obtain a parameter w that min-
imizes the hinge-loss function and the margin of
learning data.
Algorithm 1 shows briefly the Online Learning
for chunking problem. The detail about this al-
gorithm can be referred to the work of (Crammer
et al 2006). In Line 7, the argmax value is com-
puted by using the Viterbi algorithm which is sim-
ilar to the one described in (Collins, 2002). Algo-
rithm 1 is terminated after T round.
1Yamcha is available at
http://chasen.org/ taku/software/yamcha/
Input: S = (xi; yi), i = 1, 2, ..., n in which1
xi is the sentence and yi is a sequence of
chunks
Aggressive parameter C2
Output: the model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt.?(xt, yt))7
Suffer loss: lt =
wt.?(xt, y?t )? wt.?(xt, yt) +
??(yt, y?t )
Set:?t = lt||?(xt,y?t )??(xt,yt)||28
Update:9
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for NP chunking.
2.3.4 Feature Set
Feature set is designed through features template
which is shown in Table 2. All edge features obey
the first-order Markov dependency that the label
(l) of the current state depends on the label (l?)
of the previous state (e.g., ?l = I-NP? and ?l? =
B-NP?). Each observation feature expresses how
much influence a statistic (x(o, i)) observed sur-
rounding the current position i has on the label
(l) of the current state. A statistic captures a par-
ticular property of the observation sequence. For
instance, the observation feature ?l = I-NP? and
?word?1 is the? indicates that the label of the cur-
rent state should be I-NP (i.e., continue a noun
phrase) if the previous word is the. Table 2 de-
scribes both edge and observation feature tem-
plates. Statistics for observation features are iden-
tities of words, POS tags surrounding the current
position, such as words and POS tags at ?2, ?1,
1, 2.
We also employ 2-order conjunctions of the cur-
rent word with the previous (w?1w0) or the next
word (w0w1), and 2-order and 3-order conjunc-
tions of two or three consecutive POS tags within
the current window to make use of the mutual de-
pendencies among singleton properties. With the
feature templates shown in Table 2 and the feature
rare threshold of 1 (i.e., only features with occur-
rence frequency larger than 1 are included into the
discriminative models)
12
Edge feature templates
Current state: si Previous state: si?1
l l?
Observation feature templates
Current state: si Statistic (or context predicate) templates: x(o, i)
l w?2; w?1; w0; w1; w2; w?1w0; w0w1;
t?2; t?1; t0; t1; t2;
t?2t?1; t?1t0; t0t1; t1t2; t?2t?1t0;
t?1t0t1; t0t1t2
Table 2: Feature templates for phrase chunking
3 Experimental Results
We evaluate the performance of using several se-
quence learning models for the Vietnamese NP
chunking problem. The data of more than 9,000
sentences is evaluated using an empirical experi-
ment with 5 fold cross validation test. It means
we used 1,800 and 7,200 sentences for testing
and training the discriminative sequence learning
models, respectively. Note that the evaluation
method is used the same as CONLL2000 did. We
used Precision, Recall, and F-Measure in which
Precision measures how many chunks found by
the algorithm are correct and the recall is per-
centage of chunks defined in the corpus that were
found by the chunking program.
Precision = #correct?chunk#numberofchunks
Recall = #correct?chunks#numerofchunksinthecorpus
F?measure =2? Precision? RecallPrecision + Recall
To compute the scores in our experiments, we
utilized the evaluation tool (conlleval.pl) which is
available in CONLL 2000 (Sang and Buchholz,
2000, ).
Figure 2 shows the precision scores of three
methods using 5 Folds cross validation test. It
reports that the CRF-LBFGS attain the highest
score. The SVMs and CRF-SGD are comparable
to CRF-LBFGS. The Online Learning achieved
the lowest score.
Figure 3 shows the recall scores of three CRFs-
LBFGS, CRFs-SGD, SVM, and Online Learning.
The results show that CRFs-SGD achieved the
highest score while the Online Learning obtained
the lowest score in comparison with others.
Figure 4 and Figure 5 show the F-measure and
accuracy scores using 5 Folds Cross-validation
Figure 2: Precision results in 5 Fold cross valida-
tion test
Test. Similar to these results of Precision and Re-
call, CRFs-LBFGS was superior to the other ones
while the Online Learning method obtained the
lowest result.
Table 3 shows the comparison of three discrim-
inative learning methods for Vietnamese Noun
Phrase chunking. We compared the three se-
quence learning methods including: CRFs using
the LBFGS method, CRFs with SGD, and On-
line Learning. Experiment results show that the
CRFs-LBFGS is the best in comparison with oth-
ers. However, the computational times when train-
ing the data is slower than either SGD or Online
Learning. The SGD is faster than CRF-LBFS ap-
proximately 6 times. The SVM model obtained a
comparable results with CRFs models and it was
superior to Online Learning. It yields results that
were 0.712% than Online Learning. However, the
SVM?s training process take slower than CRFs
and Online Learning. According to our empirical
investigation, it takes approximately slower than
CRF-SGF, CRF-LBFGS as well as Online Learn-
ing.
13
Figure 3: Recall result in 5 Fold cross validation
test
Figure 4: The F-measure results of 5 Folds Cross-
validation Test
Note that we used FlexCRFs (Phan, Nguyen,
Tu , 2005) for Conditional Random Fields us-
ing LBFGS, and for Stochastic Gradient Descent
(SGD) we used SGD1.3 which is developed by
Leon Bottou 2.
Methods Precision Recall F1
CRF-LBGS 80.85 81.034 80.86
CRF-SGD 80.74 80.66 80.58
Online-PA 80.034 80.13 79.89
SVM 80.412 80.982 80.638
Table 3: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, SVM, Online-PA)
In order to investigate which features are ma-
jor effect on the discriminative learning models for
Vietnamese Chunking problems, we conduct three
experiments as follows.
2http://leon.bottou.org/projects/sgd
Figure 5: The accuracy scores of four methods
with 5 Folds Cross-validation Test
? Cross validation test for three modes without
considering the edge features
? Cross validation test for three models without
using POS features
? Cross validation test for three models without
using lexical features
? Cross validation test for three models without
using ?edge features template? features
Note that the computational time of training
SVMs model is slow, so we skip considering fea-
ture selection for SVMs. We only consider feature
selection for CRFs and Online Learning.
Feature Set LBFGS SGD Online
Full-Features 80.86 80.58 79.89
Without-Edge 80.91 78.66 80.13
Without-Pos 62.264 62.626 59.572
Without-Lex 77.204 77.712 75.576
Table 4: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, Online-PA)
Table 4 shows that the Edge features have an
impact to the CRF-SGD model while it do not
affect to the performance of CRFs-LBFGS and
Online-PA learning. Table 4 also indicates that
the POS features are severed as important features
regarding to the performance of all discrimina-
tive sequence learning models. As we can see,
if one do not use POS features the F1-score of
each model is decreased more than 20%. We also
remark that the lexical features contribute an im-
portant role to the performance of Vietnamese text
14
Figure 6: F-measures of three methods with different feature set
chunking. If we do not use lexical features the
F1-score of each model is decreased till approxi-
mately 3%. In conclusion, the POS features signif-
icantly effect on the performance of the discrimi-
native sequence models. This is similar to the note
of (Chen, Zhang, and Ishihara, 2006).
Figure 6 reports the F-Measures of using dif-
ferent feature set for each discriminative models.
Note that WPos, WLex, and WEdge mean without
using Pos features, without using lexical features,
and without using edge features, respectively. As
we can see, the CRF-LBFGs always achieved the
best scores in comparison with the other ones and
the Online Learning achieved the lowest scores.
4 Conclusions
In this paper, we report an investigation of devel-
oping a Vietnamese Chunking tool. We have con-
structed an annotation corpus of more than 9,000
sentences and exploiting discriminative learning
models for the NP chunking task. Experimen-
tal results using 5 Folds cross-validation test have
showed that the discriminative models are well
suitable for Vietnamese phrase chunking. Con-
ditional random fields show a better performance
in comparison with other methods. The part of
speech features are known as the most influence
features regarding to the performances of discrim-
inative models on Vietnamese phrases chunking.
What our contribution is expected to be useful
for the development of Vietnamese Natural Lan-
guage Processing. Our results and corpus can be
severed as a very good baseline for Natural Lan-
guage Processing community to develop the Viet-
namese chunking task.
There are still room for improving the perfor-
mance of Vietnamese chunking models. For ex-
ample, more attention on features selection is nec-
essary. We would like to solve this in future work.
Acknowledgments
The constructive comments and helpful sugges-
tions from three anonymous reviewers are greatly
appreciated. This paper is supported by JAIST
Grant for Research Associates and a part from a
national project named Building Basic Resources
and Tools for Vietnamese Language and Speech
Processing, KC01.01/06-10.
References
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.
K. Crammer et al 2006. Online Passive-Aggressive
Algorithm. Journal of Machine Learning Research,
2006
W. Chen, Y. Zhang, and H. Ishihara 2006. An em-
pirical study of Chinese chunking. In Proceedings
COLING/ACL 2006
Dinh Dien, Vu Thuy 2006. A maximum entropy
approach for vietnamese word segmentation. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2006: 248-253
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In the proceed-
15
ings of International Conference on Machine Learn-
ing (ICML), pp.282-289, 2001
N.C. Mai, D.N. Vu, T.P. Hoang. 1997. Foundations
of linguistics and Vietnamese. Education Publisher
(1997) 142. 152
Thi Minh Huyen Nguyen, Laurent Romary, Mathias
Rossignol, Xuan Luong Vu. 2006. A lexicon
for Vietnamese language processing. Language Re-
seourse Evaluation (2006) 40:291-309.
Minh Nghiem, Dien Dinh, Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2008: 128?133.
X.H. Phan, M.L. Nguyen, C.T. Nguyen. Flex-
CRFs: Flexible Conditional Random Field Toolkit.
http://flexcrfs.sourceforge.net, 2005
T. Kudo and Y. Matsumoto. 2001. Chunking with
Support Vector Machines. The Second Meeting of
the North American Chapter of the Association for
Computational Linguistics (2001)
F. Sha and F. Pereira. 2005. Shallow Parsing with
Conditional Random Fields. Proceedings of HLT-
NAACL 2003 213-220 (2003)
C.T. Nguyen, T.K. Nguyen, X.H. Phan, L.M. Viet-
namese Word Segmentation with CRFs and SVMs:
An Investigation. 2006. The 20th Pacific Asia Con-
ference on Language, Information, and Computation
(PACLIC), 1-3 November, 2006, Wuhan, China
Tjong Kim Sang and Sabine Buchholz. 2000. Intro-
duction to the CoNLL-2000 Shared Task: Chunk-
ing. Proceedings of CoNLL-2000 , Lisbon, Portugal,
2000.
V. Vapnik. 1995. The Natural of Statistical Learning
Theory. New York: Springer-Verlag, 1995.
16
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143?150
Manchester, August 2008
A Tree-to-String Phrase-based Model for Statistical Machine Translation
Thai Phuong Nguyen
College of Technology
Vietnam National University, Hanoi
thainp@vnu.edu.vn
Akira Shimazu1, Tu-Bao Ho2, Minh Le Nguyen1, and Vinh Van Nguyen1
1School of Information Science
2School of Knowledge Science
Japan Advanced Institute of Science and Technology
{shimazu,bao,nguyenml,vinhnv}@jaist.ac.jp
Abstract
Though phrase-based SMT has achieved high
translation quality, it still lacks of generaliza-
tion ability to capture word order differences
between languages. In this paper we describe
a general method for tree-to-string phrase-
based SMT. We study how syntactic trans-
formation is incorporated into phrase-based
SMT and its effectiveness. We design syntac-
tic transformation models using unlexicalized
form of synchronous context-free grammars.
These models can be learned from source-
parsed bitext. Our system can naturally make
use of both constituent and non-constituent
phrasal translations in the decoding phase. We
considered various levels of syntactic analy-
sis ranging from chunking to full parsing.
Our experimental results of English-Japanese
and English-Vietnamese translation showed
a significant improvement over two baseline
phrase-based SMT systems.
1 Introduction
Based on the kind of linguistic information which
is made use of, syntactic SMT can be divided into
four types: tree-to-string, string-to-tree, tree-to-tree,
and hierarchical phrase-based. The tree-to-string ap-
proach (Collins et al, 2005; Nguyen and Shimazu,
2006; Liu et al, 2006 and 2007) supposes that syn-
tax of the source language is known. This approach
can be applied when a source language parser is
available. The string-to-tree approach (Yamada and
Knight, 2001; Galley et al, 2006) focuses on syntactic
modelling of the target language in cases it has syn-
tactic resources such as treebanks and parsers. The
tree-to-tree approach models the syntax of both lan-
guages, therefore extra cost is required. The fourth
approach (Chiang, 2005) constraints phrases under
context-free grammar structure without any require-
ment of linguistic annotation.
In this paper, we present a tree-to-string phrase-
based method which is based on synchronous CFGs.
This method has two important properties: syntactic
transformation is used in the decoding phase includ-
ing a word-to-phrase tree transformation model and
a phrase reordering model; phrases are the basic unit
of translation. Since we design syntactic transforma-
tion models using un-lexicalized synchronous CFGs,
the number of rules is small1. Previous studies on
tree-to-string SMT are different from ours. Collins
et al Collins et al (2005) used hand crafted rules to
carry out word reordering in the preprocessing phase
but not decoding phase. Nguyen and Shimazu (2006)
presented a more general method in which lexicalized
syntactic reordering models based on PCFGs can be
learned from source-parsed bitext and then applied in
the preprocessing phase. Liu et al (2006) changed the
translation unit from phrases to tree-to-string align-
ment templates (TATs) while we do not. TATs was
represented as xRs rules while we use synchronous
CFG rules. In order to overcome the limitation that
TATs can not capture non-constituent phrasal transla-
tions, Liu et al (2007) proposed forest-to-string rules
while our system can naturally make use of such kind
of phrasal translation by word-to-phrase tree transfor-
mation.
We carried out experiments with two language
pairs English-Japanese and English-Vietnamese. Our
system achieved significant improvements over
Pharaoh, a state-of-the-art phrase-based SMT system.
We also analyzed the dependence of translation qual-
ity on the level of syntactic analysis (shallow or deep).
Figure 1 shows the architecture of our system. The
input of this system is a source-language tree and the
output is a target-language string. This system uses
all features of conventional phrase-based SMT as in
(Koehn et al, 2003). There are two new features in-
cluding a word-to-phrase tree transformation model
and a phrase reordering model. The decoding algo-
1See Section 6.2.
143
rithm is a tree-based search algorithm.
Figure 1: A syntax-directed phrase-based SMT archi-
tecture.
2 Translation Model
We use an example of English-Vietnamese translation
to demonstrate the translation process as in Figure 2.
Now we describe a tree-to-string SMT model based
on synchronous CFGs. The translation process is:
Figure 2: The translation process.
T
1
? T
2
? T
3
? T
4
(1)
where T
1
is a source tree, T
2
is a source phrase tree,
T
3
is a reordered source phrase tree, and T
4
is a target
phrase tree.
Using the first order chain rule, the join probability
over variables (trees) in graphical representation 1 is
approximately calculated by:
P (T
1
, T
2
, T
3
, T
4
) = P (T
1
)?P (T
2
|T
1
)?P (T
3
|T
2
)?P (T
4
|T
3
)
(2)
P (T
1
) can be omitted since only one syntactic tree
is used. P (T
2
|T
1
) is a word-to-phrase tree transfor-
mation model we describe later. P (T
3
|T
2
) is a re-
ordering model. P (T
4
|T
3
) can be calculated using a
phrase translation model and a language model. This
is the fundamental equation of our study represented
in this paper. In the next section, we will describe how
to transform a word-based CFG tree into a phrase-
based CFG tree.
3 Word-to-Phrase Tree Transformation
3.1 Penn Treebank?s Tree Structure
According to this formalism, a tree is represented by
phrase structure. If we extract a CFG from a tree or
set of trees, there will be two possible rule forms:
? A ? ? where ? is a sequence of nonterminals
(syntactic categories).
? B ? ? where ? is a terminal symbol (or a word
in this case).
We consider an example of a syntactic tree and a
simple CFG extracted from that tree.
Sentence: ?I am a student?
Syntactic tree: (S (NP (NN I)) (VP (VBP am) (NP (DT a) (NN
student))))
Rule set: S ? NP VP; VP ? VBP NP; NP ? NN | DT NN; NN
? I | student;
VBP ? am; DT ? a
However, we are considering phrase-based transla-
tion. Therefore the right hand side of the second rule
form must be a sequence of terminal symbols (or a
phrase) but not a single symbol (a word). Suppose
that the phrase table contains a phrase ?am a student?
which leads to the following possible tree structure:
Phrase segmentation: ?I | am a student?
Syntactic tree: (S (NP (NN I)) (VP (VBP am a student)))
Rule set: S ? NP VP; VP ? VBP; NP ? NN; NN ? I; VBP ?
am a student
We have to find out some way to transform a CFG
tree into a tree with phrases at leaves. In the next sub-
section we propose such an algorithm.
3.2 An Algorithm for Word-to-Phrase Tree
Transformation
Table 1 represents our algorithm to transform a CFG
tree to a phrase CFG tree. When designing this algo-
rithm, our criterion is to preserve the original struc-
ture as much as possible. This algorithm includes two
steps. There are a number of notions concerning this
algorithm:
? A CFG rule has a head symbol on the right hand
side. Using this information, head child of a
node on a syntactic tree can be determined.
144
+ Input: A CFG tree, a phrase segmentation
+ Output: A phrase CFG tree
+ Step 1: Allocate phrases to leaf nodes in a top-down manner: A phrase is allocated to head word of a node if the
phrase contains the head word. This head word is then considered as the phrase head.
+ Step 2: Transform the syntactic tree by replacing leaf nodes by their allocated phrase and removing all nodes whose
span is a substring of phrases.
Table 1: An algorithm to transform a CFG tree to a phrase CFG tree.
? If a node is a pre-terminal node (containing POS
tag), its head word is itself. If a node is an in-
ner node (containing syntactic constituent tag),
its head word is retrieved through the head child.
? Word span of a node is a string of its leaves. For
instance, word span of subtree (NP (PRP$ your)
(NN class)) is ?your class?.
Now we consider an example depicted in Figure 3
and 4. Head children are tagged with functional label
H. There are two phrases: ?is a? and ?in your class?.
After the Step 1, the phrase ?is a? is attached to (VBZ
is). The phrase ?in your class? is attached to (IN in).
In Step 2, the node (V is) is replaced by (V ?is a?) and
(DT a) is removed from its father NP. Similarly, (IN
in) is replaced by (IN ?in your class?) and the subtree
NP on the right is removed.
S
[is]
NP
[Fred]
VP-H
[is]
VBZ-H NP[student]NNP-H
is NP-H[student]
DT NN-H
PP
[in]
IN-H NP[class]
PRP$ NN-H
Fred
a student in
your class
{is a}
{in your class}
Figure 3: Tree transformation - step 1. Solid arrows
show the allocation process of ?is a?. Dotted arrows
demonstrate the allocation process of ?in your class?
The proposed algorithm has some properties. We
state these properties without presenting proof2.
? Uniqueness: Given a CFG tree and a phrase seg-
mentation, by applying Algorithm 1, one and
only one phrase tree is generated.
2Proofs are simple.
Figure 4: Tree transformation - step 2.
? Constituent subgraph: A phrase CFG tree is
a connected subgraph of input tree if leaves are
ignored.
? Flatness: A phrase CFG tree is flatter than input
tree.
? Outside head: The head of a phrase is always a
word whose head outside the phrase. If there is
more than one word satisfying this condition, the
word at the highest level is chosen.
? Dependency subgraph: Dependency graph of a
phrase CFG tree is a connected subgraph of in-
put tree?s dependency graph if there exist no de-
tached nodes.
The meaning of uniqueness property is that our al-
gorithm is a deterministic procedure. The constituent-
subgraph property will be employed in the next sec-
tion for an efficient decoding algorithm. When a syn-
tactic tree is transformed, a number of subtrees are
replaced by phrases. The head word of a phrase is the
contact point of that phrase with the remaining part
of a sentence. From the dependency point of view, a
head word should depend on an outer word rather than
an inner word. About dependency-subgraph property,
when there is a detached node, an indirect dependency
will become a direct one. In any cases, there is no
145
change in dependency direction. We can observe de-
pendency trees in Figure 5. The first two trees are
source dependency tree and phrase dependency tree
of the previous example. The last one corresponds to
the case in which a detached node exists.
Fred is
ROOT
student in your classa
Fred is a
ROOT
student in your class
Fred is a student
ROOT
in your class
Figure 5: Dependency trees. The third tree corre-
sponds with phrase segmentation: ?Fred | is a student
| in your class?
3.3 Probabilistic Word-to-Phrase Tree
Transformation
We have proposed an algorithm to create a phrase
CFG tree from a pair of CFG tree and phrase seg-
mentation. Two questions naturally arise: ?is there
a way to evaluate how good a phrase tree is?? and ?is
such an evaluation valuable?? Note that phrase trees
are the means to reorder the source sentence repre-
sented as phrase segmentations. Therefore a phrase
tree is surely not good if no right order can be gen-
erated. Now the answer to the second question is
clear. We need an evaluation method to prevent our
program from generating bad phrase trees. In other
words, good phrase trees should be given a higher pri-
ority.
We define the phrase tree probability as the product
of its rule probability given the original CFG rules:
P (T
?
) =
?
i
P (LHS
i
? RHS
?
i
|LHS
i
? RHS
i
)
(3)
where T ? is a phrase tree whose CFG rules are
LHS
i
? RHS
?
i
. LHS
i
? RHS
i
are origi-
nal CFG rules. RHS?
i
are subsequences of RHS
i
.
Since phrase tree rules should capture changes made
by the transformation from word to phrase, we use
?+? to represent an expansion and ?-? to show an
overlap. These symbol will be added to a nonter-
minal on the side having a change. In the previ-
ous example, since a head noun in the word tree
has been expanded on the right, the correspond-
ing symbol in phrase tree is NN-H+. A nonter-
minal X can become one of the following symbols
X,?X,+X,X?, X+,?X?,?X+,+X?,+X+.
Conditional probabilities are computed in a sepa-
rate training phase using a source-parsed and word-
aligned bitext. First, all phrase pairs consistent with
the word alignment are collected. Then using this
phrase segmentation and syntactic trees we can gener-
ate phrase trees by word-to-phrase tree transformation
and extract rules.
4 Phrase Reordering Model
Reordering rules are represented as SCFG rules
which can be un-lexicalized or source-side lexicalized
(Nguyen and Shimazu, 2006). In this paper, we used
un-lexicalized rules. We used a learning algorithm
as in (Nguyen and Shimazu, 2006) to learn weighted
SCFGs. The training requirements include a bilingual
corpus, a word alignment tool, and a broad coverage
parser of the source language. The parser is a con-
stituency analyzer which can produce parse tree in
Penn Tree-bank?s style. The model is applicable to
language pairs in which the target language is poor
in resources. We used phrase reorder rules whose ?+?
and ?-? symbols are removed.
5 Decoding
A source sentence can have many possible phrase seg-
mentations. Each segmentation in combination with a
source tree corresponds to a phrase tree. A phrase-tree
forest is a set of those trees. A naive decoding algo-
rithm is that for each segmentation, a phrase tree is
generated and then the sentence is translated. This al-
gorithm is very slow or even intractable. Based on
the constituent-subgraph property of the tree trans-
formation algorithm, the forest of phrase trees will
be packed into a tree-structure container whose back-
bone is the original CFG tree.
5.1 Translation Options
A translation option encodes a possibility to translate
a source phrase (at a leaf node of a phrase tree) to
another phrase in target language. Since our decoder
uses a log-linear translation model, it can exploit var-
ious features of translation options. We use the same
features as (Koehn et al, 2003). Basic information of
a translation option includes:
? source phrase
? target phrase
? phrase translation score (2)
146
? lexical translation score (2)
? word penalty
Translation options of an input sentence are col-
lected before any decoding takes place. This allows a
faster lookup than consulting the whole phrase trans-
lation table during decoding. Note that the entire
phrase translation table may be too big to fit into
memory.
5.2 Translation Hypotheses
A translation hypothesis represents a partial or full
translation of an input sentence. Initial hypotheses
correspond to translation options. Each translation
hypothesis is associated with a phrase-tree node. In
other words, a phrase-tree node has a collection of
translation hypotheses. Now we consider basic infor-
mation contained in a translation hypothesis:
? the cost so far
? list of child hypotheses
? left language model state and right language
model state
5.3 Decoding Algorithm
First we consider structure of a syntactic tree. A tree
node contains fields such as syntactic category, child
list, and head child index. A leaf node has an ad-
ditional field of word string. In order to extend this
structure to store translation hypotheses, a new field
of hypothesis collection is appended. A hypothe-
sis collection contains translation hypotheses whose
word spans are the same. Actually, it corresponds to
a phrase-tree node. A hypothesis collection whose
word span is [i
1
, i
2
] at a node whose tag is X ex-
presses that:
? There is a phrase-tree node (X, i
1
, i
2
).
? There exist a phrase [i
1
, i
2
] or
? There exist a subsequence of X?s child list:
(Y
1
, j
0
, j
1
), (Y
2
, j
1
+1, j
2
), ..., (Y
n
, j
n?1
+1, j
n
)
where j
0
= i
1
and j
n
= i
2
? Suppose that [i, j] is X?s span, then [i
1
, i
2
] is a
valid phrase node?s span if and only if: i
1
<= i
or i < i
1
<= j and there exist a phrase [i
0
, i
1
?
1] overlapping X?s span at [i, i
1
? 1]. A similar
condition is required of j.
Table 2 shows our decoding algorithm. Step 1 dis-
tributes translation options to leaf nodes using a pro-
cedure similar to Step 1 of algorithm in Table 1. Step
Corpus Size Training Development Testing
Conversation 16,809 15,734 403 672
Reuters 57,778 55,757 1,000 1,021
Table 3: Corpora and data sets.
English Vietnamese
Sentences 16,809
Average sent. len. 8.5 8.0
Words 143,373 130,043
Vocabulary 9,314 9,557
English Japanese
Sentences 57,778
Average sent. len. 26.7 33.5
Words 1,548,572 1,927,952
Vocabulary 31,702 29,406
Table 4: Corpus statistics of translation tasks.
2 helps check valid subsequences in Step 3 fast. Step
3 is a bottom-up procedure, a node is translated if all
of its child nodes have been translated. Step 3.1 calls
syntactic transformation models. After reordered in
Step 3.2, a subsequence will be translated in Step 3.3
using a simple monotonic decoding procedure result-
ing in new translation hypotheses. We used a beam
pruning technique to reduce the memory cost and to
accelerate the computation.
6 Experimental Results
6.1 Experimental Settings
We used Reuters3, an English-Japanese bilingual cor-
pus, and Conversation, an English-Vietnamese corpus
(Table 4). These corpora were split into data sets as
shown in Table 3. Japanese sentences were analyzed
by ChaSen4, a word-segmentation tool.
A number of tools were used in our experiments.
Vietnamese sentences were segmented using a word-
segmentation program (Nguyen et al, 2003). For
learning phrase translations and decoding, we used
Pharaoh (Koehn, 2004), a state-of-the-art phrase-
based SMT system which is available for research
purpose. For word alignment, we used the GIZA++
tool (Och and Ney, 2000). For learning language
models, we used SRILM toolkit (Stolcke, 2002). For
MT evaluation, we used BLEU measure (Papineni et
al., 2001) calculated by the NIST script version 11b.
For the parsing task, we used Charniak?s parser (Char-
niak, 2000). For experiments with chunking (or shal-
low parsing), we used a CRFs-based chunking tool 5
to split a source sentence into syntactic chunks. Then
a pseudo CFG rule over chunks is built to generate a
two-level syntactic tree. This tree can be used in the
3http://www2.nict.go.jp/x/x161/members/mutiyama/index.html
4http://chasen.aist-nara.ac.jp/chasen/distribution.html.en
5http://crfpp.sourceforge.net/
147
+ Input: A source CFG tree, a translation-option collection
+ Output: The best target sentence
+ Step 1: Allocate translation options to hypothesis collections at leaf nodes.
+ Step 2: Compute overlap vector for all nodes.
+ Step 3: For each node, if all of its children have been translated, then for each valid
sub-sequence of child list, carry out the following steps:
+ Step 3.1: Retrieve transformation rules
+ Step 3.2: Reorder the sub-sequence
+ Step 3.3: Translate the reordered sub-sequence and update corresponding
hypothesis collections
Table 2: A bottom-up dynamic-programming decoding algorithm.
Corpus CFG PhraseCFG W2PTT Reorder
Conversation 2,784 2,684 8,862 2,999
Reuters 7,668 5,479 13,458 7,855
Table 5: Rule induction statistics.
Corpus Pharaoh PB system SD system SD system
(chunking) (full-parsing)
Conversation 35.47 35.66 36.85 37.42
Reuters 24.41 24.20 20.60 25.53
Table 6: BLEU score comparison between phrase-
based SMT and syntax-directed SMT. PB=phrase-
based; SD=syntax-directed
same way as trees produced by Charniak?s parser.
We built a SMT system for phrase-based log-linear
translation models. This system has two decoders:
beam search and syntax-based. We implemented the
algorithm in Section 5 for the syntax-based decoder.
We also implemented a rule induction module and a
module for minimum error rate training. We used the
system for our experiments reported later.
6.2 Rule Induction
In Table 5, we report statistics of CFG rules,
phrase CFG rules, word-to-phrase tree transformation
(W2PTT) rules, and reordering rules. All counted
rules were in un-lexicalized form. Those numbers are
very small in comparison with the number of phrasal
translations (up to hundreds of thousands on our cor-
pora). There were a number of ?un-seen? CFG rules
which did not have a corresponding reordering rule.
A reason is that those rules appeared once or several
times in the training corpus; however, their hierarchi-
cal alignments did not satisfy the conditions for in-
ducing a reordering rule since word alignment is not
perfect (Nguyen and Shimazu, 2006). Another reason
is that there were CFG rules which required nonlocal
reordering. This may be an issue for future research:
a Markovization technique for SCFGs.
6.3 BLEU Scores
Table 6 shows a comparison of BLEU scores be-
tween Pharaoh, our phrase-based SMT system, and
our syntax-directed (SD) SMT system with chunking
and full parsing respectively. On both Conversation
corpus and Reuters corpus: The BLEU score of our
phrase-based SMT system is comparable to that of
Pharaoh; The BLEU score of our SD system with full
parsing is higher than that of our phrase-based sys-
tem. On Conversation corpus, our SD system with
chunking has a higher performance in terms of BLEU
score than our phrase-based system. Using sign test
(Lehmann, 1986), we verified the improvements are
statistically significant. However, on Reuters corpus,
performance of the SD system with chunking is much
lower than the phrase-based system?s. The reason is
that in English-Japanese translation, chunk is a too
shallow syntactic structure to capture word order in-
formation. For example, a prepositional chunk of-
ten includes only preposition and adverb, therefore
such information does not help reordering preposi-
tional phrases.
6.4 The Effectiveness of the W2PTT Model
Without this feature, BLEU scores decreased around
0.5 on both corpora. We now consider a linguistically
motivated example of English-Vietnamese translation
to show that phrase segmentation can be evaluated
through phrase tree scoring. This example was ex-
tracted from Conversation test set.
English sentence: for my wife ?s mother
Vietnamese word order: for mother ?s wife my
Phrase segmentation 1: for my wife | ?s | mother
P1=P(PP?IN+ -NP | PP?IN NP)xP(-NP?-NP NN | NP?NP
NN)xP(-NP?POS | NP?PRP$ NN
POS)=log(0.00001)+log(0.14)+log(0.048)=-5-0.85-1.32=-7.17
Phrase segmentation 2: for | my wife ?s | mother
P2=P(PP?IN NP | PP?IN NP)xP(NP?NP NN | NP?NP
NN) xP(NP?POS | NP?PRP$ NN POS)
=log(0.32)+log(0.57)+log(0.048)=-0.5-0.24-1.32=-2.06
The first phrase segmentation is bad (or even un-
acceptable) since the right word order can not be
achieved from this segmentation by phrase reorder-
ing and word reordering within phrases. The second
phrase segmentation is much better. Source syntax
tree and phrase trees are shown in Figure 6. The first
phrase tree has a much smaller probability (P1=-7.17)
than the second (P2=-2.06).
148
Figure 6: Two phrase trees.
Corpus Level-1 Level-2 Level-3 Level-4 Full
Conversation 36.85 36.91 37.11 37.23 37.42
Reuters 20.60 22.76 24.49 25.12 25.53
Table 7: BLEU score with different syntactic levels.
Level-i means syntactic transformation was applied to
tree nodes whose level smaller than or equal to i. The
level of a pre-terminal node (POS tag) is 0. The level
of an inner node is the maximum of its children?s lev-
els.
6.5 Levels of Syntactic Analysis
Since in practice, chunking and full parsing are often
used, in Table 6, we showed translation quality of the
two cases. It is interesting if we can find how syn-
tactic analysis can affect BLEU score at more inter-
mediate levels (Table 7). On the Conversation corpus,
using syntax trees of level-1 is effective in comparison
with baseline. The increase of syntactic level makes a
steady improvement in translation quality. Note that
when we carried out experiments with chunking (con-
sidered as level-1 syntax) the translation speed (in-
cluding chunking) of our tree-to-string system was
much faster than baseline systems?. This is an option
for developing applications which require high speed
such as web translation.
7 Related Works
7.1 A Comparison of Syntactic SMT Methods
To advance the state of the art, SMT system design-
ers have experimented with tree-structured transla-
tion models. The underlying computational models
were synchronous context-free grammars and finite-
state tree transducers which conceptually have a bet-
ter expressive power than finite-state transducers. We
create Tables 8 and 9 in order to compare syntac-
tic SMT methods including ours. The first row is a
baseline phrasal SMT approach. The second column
in Table 8 only describes input types because the out-
put is often string. Syntactic SMT methods are dif-
ferent in many aspects. Methods which make use of
phrases (in either explicit or implicit way) can beat
the baseline approach (Table 8) in terms of BLEU
metric. Two main problems these models aim to deal
with are word order and word choice. In order to ac-
complish this purpose, the underlying formal gram-
mars (including synchronous context-free grammars
and tree transducers) can be fully lexicalized or un-
lexicalized (Table 9).
7.2 Non-constituent Phrasal Translations
Liu et al (2007) proposed forest-to-string rules to
capture non-constituent phrasal translation while our
system can naturally make use of such kind of phrasal
translation by using word-to-phrase tree transforma-
tion. Liu et al (2007) also discussed about how
the phenomenon of non-syntactic bilingual phrases
is dealt with in other SMT methods. Galley et al
(2006) handled non-constituent phrasal translation by
traversing the tree upwards until reaches a node that
subsumes the phrase. Marcu et al (2006) reported
that approximately 28% of bilingual phrases are non-
syntactic on their English-Chinese corpus. They pro-
posed using a pseudo nonterminal symbol that sub-
sumes the phrase and corresponding multi-headed
syntactic structure. One new xRs rule is required to
explain how the new nonterminal symbol can be com-
bined with others. This technique brought a signif-
icant improvement in performance to their string-to-
tree noisy channel SMT system.
8 Conclusions
We have presented a general tree-to-string phrase-
based method. This method employs a syntax-based
reordering model in the decoding phase. By word-
to-phrase tree transformation, all possible phrases
are considered in translation. Our method does
not suppose a uniform distribution over all possible
phrase segmentations as (Koehn et al, 2003) since
each phrase tree has a probability. We believe that
other kinds of translation unit such as n-gram (Jos
et al, 2006), factored phrasal translation (Koehn and
Hoang, 2007), or treelet (Quirk et al, 2005) can be
used in this method. We would like to consider this
problem as a future study. Moreover we would like to
use n-best trees as the input of our system. A number
149
Method Input Theoretical Decoding style Linguistic Phrase Performance
model information usage
Koehn et al (2003) string FSTs beam search no yes baseline
Yamada and Knight (2001) string SCFGs parsing target no not better
Melamed (2003) string SCFGs parsing both sides no not better
Chiang (2005) string SCFGs parsing no yes better
Quirk et al (2005) dep. tree TTs parsing source yes better
Galley et al (2006) string TTs parsing target yes better
Liu et al (2006) tree TTs tree transf. source yes better
Our work tree SCFGs tree transf. source yes better
Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous
Context-Free Grammar; TT=Tree Transducer.
Method Rule form Rule function Rule lexicalization level
Koehn et al (2003) no no no
Yamada and Knight (2001) SCFG rule reorder and function-word ins./del. unlexicalized
Melamed (2003) SCFG rule reorder and word choice full
Chiang (2005) SCFG rule reorder and word choice full
Quirk et al (2005) Treelet pair word choice full
Galley et al (2006) xRs rule reorder and word choice full
Liu et al (2006) xRs rule reorder and word choice full
Our work SCFG rule reorder unlexicalized
Table 9: A comparison of syntactic SMT methods (part 2). xRs is a kind of rule which maps a syntactic pattern
to a string, for example VP(AUX(does), RB(not),x
0
:VB) ? ne, x
0
, pas. In the column Rule lexicalization
level: full=lexicalization using vocabularies of both source language and target language.
of non-local reordering phenomena such as adjunct
attachment should be handled in the future.
References
Charniak, E. 2000. A maximum entropy inspired parser.
In Proceedings of HLT-NAACL.
Galley, M., Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, Ignacio Thayer 2006. Scal-
able Inference and Training of Context-Rich Syntactic
Translation Models. In Proceedings of ACL.
Jos B. Mario, Rafael E. Banchs, Josep M. Crego, Adri de
Gispert, Patrik Lambert, Jos A. R. Fonollosa, Marta R.
Costa-juss. 2006. N-gram-based Machine Translation.
Computational Linguistics, 32(4): 527?549.
Koehn, P. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of AMTA.
Koehn, P. and Hieu Hoang. 2007. Factored Translation
Models. In Proceedings of EMNLP.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
Lehmann, E. L. 1986. Testing Statistical Hypotheses (Sec-
ond Edition). Springer-Verlag.
Liu, Y., Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL.
Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007.
Forest-to-String Statistical Translation Rules. In Pro-
ceedings of ACL.
Marcu, D., Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceed-
ings of EMNLP.
Melamed, I. D. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Nguyen, Thai Phuong and Akira Shimazu. 2006. Improv-
ing Phrase-Based Statistical Machine Translation with
Morphosyntactic Transformation. Machine Translation,
20(3): 147?166.
Nguyen, Thai Phuong, Nguyen Van Vinh and Le Anh
Cuong. 2003. Vietnamese Word Segmentation Using
Hidden Markov Model. In Proceedings of International
Workshop for Computer, Information, and Communica-
tion Technologies in Korea and Vietnam.
Och, F. J. and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL.
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Report.
Quirk, C., A. Menezes, and C. Cherry. 2005. Dependency
treelet translation: Syntactically informed phrasal SMT.
In Proceedings of ACL.
Stolcke, A. 2002. SRILM - An Extensible Language Mod-
eling Toolkit. In Proc. Intl. Conf. Spoken Language
Processing.
Yamada, K. and K. Knight. 2001. A syntax-based statisti-
cal translation model. In Proceedings of ACL.
150
