Description of the HKU Chinese Word Segmentation System 
for Sighan Bakeoff 2005 
Guohong Fu 
Department of Linguistics 
The University of Hong Kong
Pokfulam Road, Hong Kong
ghfu@hkucc.hku.hk
Kang-Kwong Luke 
Department of Linguistics 
The University of Hong Kong
Pokfulam Road, Hong Kong
kkluke@hkusua.hku.hk
Percy Ping-Wai WONG  
Department of Linguistics 
The University of Hong Kong
Pokfulam Road, Hong Kong
wongpw@hkusua.hku.hk
Abstract 
In this paper, we describe in brief our 
system for the Second International 
Chinese Word Segmentation Bakeoff 
sponsored by the ACL-SIGHAN. We 
participated in all tracks at the bakeoff. 
The evaluation results show our system 
can achieve an F measure of 0.940-
0.967 for different testing corpora. 
1 Introduction 
Word segmentation is very important for Chi-
nese text processing, which is aiming at recog-
nizing the implicit word boundaries in plain 
Chinese text. Over the past decades, great pro-
gress has been made with Chinese word 
segmentation technology. However, two 
difficulties still face us while developing a 
practical segmentation system for large open 
applications, i.e. the resolution of ambiguous 
segmentation and the identification of unknown 
or out-of-vocabulary (OOV) words. 
In order to resolve the above two problems, 
we developed a purely statistical Chinese word 
segmentation system using a two-stage strategy. 
We participated in eight tracks at the Second 
International Chinese Word Segmentation 
Bakeoff sponsored by the ACL-SIGHAN, and 
tested our system on different testing corpora. 
The scored results show that our system is effec-
tive for most of ambiguous segmentation and 
unknown words in Chinese text. In this paper, 
we make a summary of this work and give some 
analysis on the results. 
The rest of this paper is organized as follows: 
First in Section 2, we describe in brief a two-
stage strategy for Chinese word segmentation. 
Then in Section 3, we give details about the set-
tings or configuration of our system for different 
testing tracks, particularly the training data and 
the dictionaries used in our system. Finally, we 
report the results of our system at this bakeoff in 
Section 4, and give our conclusions on this work 
in Section 5. 
2 Overview of the System 
In practice, our system works in two major steps 
as follows: 
The first step is a process of known word 
segmentation, which aims to segment an input 
sequence of Chinese characters into a sequence 
of known words that are listed in the system dic-
tionary. In our current system, we apply a 
known word bigram model to perform this task 
(Fu and Luke, 2003; Fu and Luke, 2004; Fu and 
Luke, 2005).  
Actually, known word segmentation is a 
process of disambiguation. Given a Chinese 
character string 
ncccC L21= , there are usually 
multiple possible segmentations of known words 
mwwwW L21=  according to the system diction-
ary. The task of known word segmentation is to 
find a proper segmentation 
mwwwW L21? =  that 
maximizes the probability ?
=
?
m
i
ii wwP
1
1 )|( , i.e. 
?
=
?
?=
m
i
ii
WW
wwPCWPW
1
1 )|(maxarg)|(maxarg?  (1) 
The second step is actually a tagging task on 
the sequence of known words acquired in the 
first step, which intends to detect unknown 
165
words or out-of-vocabulary (OOV) words in the 
input. In this process, each known word yielded 
in the first step will be further assigned a proper 
tag that indicates whether the known word is an 
independent segmented word by itself or a be-
ginning/middle/ending component of an OOV 
word  (Fu and Luke, 2004). In order to improve 
our system, part-of-speech information is also 
introduced in some tracks such as the PKU open 
test and the AS open test. Furthermore, a 
lexicalized HMM tagger is developed to per-
form this task (Fu and Luke, 2004).  
Given a sequence of known words 
nwwwW L21= , the lexicalized HMM tagger 
attempt to find an appropriate sequence of tags 
ntttT L21? =  that maximizes the conditional prob-
ability )|( WTP , namely 
  
?
=
???
?
=
n
i
iiiiii
T
T
twtPtwwP
WTPT
1
111 ),|(),|(maxarg
)|(maxarg?
     (2) 
3 Settings for Different Tracks 
Table 1. Training corpora for different tracks 
Table 1 presents the corpora used to train our 
system for different tracks. In the Academia 
Sinica (AS) open test and the Peking University 
(PKU) open test, our system is trained respec-
tively using the Sinica Corpus (3.0) and the PFR 
Corpus. In all other tests, including all closed 
tests, City University of Hong Kong (CityU) 
open test and Microsoft Research (MSR) open 
test, we trained our system using the relevant 
training corpora provided for the bakeoff.  
Track Training Corpus Word 
counts
AS-O The Sinica Corpus 3.0  5,692,150
AS-C AS corpus for Bakeoff 2005 5,449,698
CityU-O CityU corpus for Bakeoff 2005 1,455,629
CityU-C CityU corpus for Bakeoff 2005 1,455,629
MSR-O MSR corpus for Bakeoff 2005 2,368,391
MSR-C MSR corpus for Bakeoff 2005 2,368,391
PKU-O The PFR Corpus  7,286,960
PKU-C PKU corpus for Bakeoff 2005 1,109,947
Table 2 shows all the dictionaries used in our 
system for different tracks.   
In the closed test, the system dictionaries are 
derived automatically from the relevant training 
corpora for this bakeoff by using the following 
three criteria: (1) Each character in the training 
corpus is taken as an independent entry and col-
lected into the relevant system dictionary. (2) A 
standard Chinese word in the training corpus 
will enter to the relevant dictionary if it has four 
or less Chinese characters within it, and at the 
same time, its counts of occurrence in the corpus 
is observed to be larger than a threshold. In our 
current system, the threshold is set to 10 for the 
AS closed test and 5 for other closed tests. (3) 
For non-standard Chinese words such as nu-
meral expressions, English words and punctua-
tions, if they consist of multiple characters, they 
will be not included in the system dictionary. 
As for the open test, some other dictionaries 
are applied. As can be seen from Table 2, the 
CKIP Lexicon and Chinese Grammar is used in 
both AS and CityU open test, and the Gram-
matical Knowledge-base of Contemporary Chi-
nese developed by the Peking University is 
utilized in both PKU and MSR open test.  
   
Track System dictionary # of 
entries
AS-O The CKIP Lexicon and Chinese Grammar 84K
AS-C Automatically extracted from AS 
corpus for Bakeoff 2005 30K
CityU-O
The CKIP Lexicon and Chinese 
Grammar (without part-of-
speech)
84K
CityU-C Automatically extracted from CityU corpus for Bakeoff 2005 22K
MSR-O 
The Grammatical Knowledge-
base of Contemporary Chinese
(without part-of-speech) 
65K
MSR-C Automatically extracted from MSR corpus for Bakeoff 2005 17K
PKU-O The Grammatical Knowledge-base of Contemporary Chinese 65K
PKU-C Automatically extracted from PKU corpus for Bakeoff 2005 17K
Table 2. System dictionaries for different tracks 
It should be noted that part-of-speech infor-
mation is also utilized in the AS open test and 
the PKU open test, because part-of-speech in-
formation proved to be informative in identify-
ing OOV words in Chinese text (Fu and Luke, 
2004). Therefore, the training corpora for the 
two tests are tagged with part-of-speech, and 
entries in the relevant dictionaries are defined 
with their potential part-of-speech categories.   
166
4 The Scored Results 
In Bakeoff 2005, six measures are employed to 
score the performance of a word segmentation 
system, namely recall (R), precision (P), the 
evenly-weighted F-measure (F), out-of-
vocabulary (OOV) rate for the test corpus, recall 
with respect to OOV words (ROOV) or in-
vocabulary words (Riv).  
In order to achieve a consistent evaluation of 
our system in both the closed test and the open 
test, OOV is defined in this paper as the set of 
words in the test corpus but not occurring in 
both the training corpus and the system diction-
ary. Furthermore, the additional two rates, i.e. 
OOV-C and OOV-D are used to denote the out-
of-vocabulary rate with respect to the training 
corpus and the out-of-vocabulary rate with 
respect to the system dictionary, respectively. At 
the same time, the precision with regard to in-
vocabulary words (Piv) and OOV words (POOV) 
are also computed in this paper to give a more 
complete evalution of our system in unknown 
word identification. 
Track OOV-C OOV-D OOV 
AS-O 0.043 0.096 0.039 
AS-C 0.043 0.096 0.043 
CityU-O 0.074 0.140 0.049 
CityU-C 0.074 0.127 0.074 
MSR-O 0.026 0.076 0.023 
MSR-C 0.026 0.087 0.026 
PKU-O 0.038 0.070 0.033 
PKU-C 0.058 0.091 0.058 
Table 3. OOV rates for different tracks 
Track F R P Riv Piv ROOV POOV
AS-O 0.946 0.955 0.938 0.972 0.947 0.532 0.660
AS-C 0.940 0.947 0.934 0.966 0.949 0.523 0.566
CityU-O 0.941 0.944 0.938 0.962 0.956 0.592 0.599
CityU-C 0.939 0.944 0.933 0.969 0.952 0.626 0.677
MSR-O 0.967 0.969 0.966 0.978 0.977 0.586 0.537
MSR-C 0.962 0.962 0.962 0.972 0.977 0.592 0.499
PKU-O 0.962 0.959 0.965 0.963 0.970 0.835 0.816
PKU-C 0.944 0.943 0.944 0.961 0.958 0.656 0.700
Table 4. Scores for different tracks 
The OOV rates and scores of our system are 
summarized respectively in Table 3 and Table 4. 
The results show that our system can achieve a 
F-measure of 0.940-0.967 for different testing 
corpora while the relevant OOV rates are from 
0.023 to 0.074.  
Although our system has achieved a promis-
ing performance, there is still much to be done 
to improve it. Firstly, our system is purely statis-
tics-based, it cannot yield correct segmentations 
for all non-standard words (NSWs) such as nu-
meral expressions and English strings in Chi-
nese text. Secondly, known word segmentation 
and unknown word identification are taken as 
two independent stages in our system. This 
strategy is obviously simple and more easily 
applicable (Fu and Luke, 2003). Although the 
known word bigram model can partly resolve 
this problem, it is not always effective for some 
complicated strings that contains a mixture of 
ambiguities and unknown words, such as ?31
? and the fragment ?? in the sen-
tence ?	
?.   
5 Conclusions 
This paper presents a two-stage statistical word 
segmentation system for Chinese. We partici-
pated in all testing tracks at the second Sighan 
bakeoff. The scored results show that our system 
can achieve a F-measure of 0.940-0.967 as a 
whole for different corpora. This indicates that 
the proposed system is effective for most am-
biguous segmentations and unknown words in 
Chinese test. For future work, we hope to im-
prove our system by incorporating some pattern 
rules to handle complicated ambiguous frag-
ments and non-standard words in Chinese text. 
References 
Guohong Fu, and Kang-Kwong Luke. 2003. A two-
stage statistical word segmentation system for 
Chinese. In: Proceedings of the 2nd SIGHAN 
Workshop on Chinese Language Processing, Sap-
poro, Japan, 156-159. 
Guohong Fu, and Kang-Kwong Luke. 2004. Chinese 
unknown word identification as known word tag-
ging. In: Proceedings of the Third IEEE Interna-
tional Conference on Machine Learning and 
Cybernetics (ICMLC 2004), Shanghai, China, 
2612-2617. 
Guohong Fu, and Kang-Kwong Luke. 2005. Chinese 
unknown word identification using class-based 
LM. Lecture Notes in Computer Science (IJCNLP 
2004), 3248: 704-713. 
167
   
A two-stage statistical word segmentation system for Chinese 
Guohong Fu 
Dept of Linguistics 
 The University of Hong Kong 
Pokfulam Road, Hong Kong 
ghfu@hkucc.hku.hk 
K.K. Luke 
Dept of Linguistics 
The University of Hong Kong 
Pokfulam Road, Hong Kong 
kkluke@hkusua.hku.hk 
 
Abstract 
In this paper we present a two-stage 
statistical word segmentation system for 
Chinese based on word bigram and word-
formation models. This system was 
evaluated on Peking University corpora at 
the First International Chinese Word 
Segmentation Bakeoff. We also give 
results and discussions on this evaluation. 
1 Introduction 
Word segmentation is very important for Chinese 
language processing, which aims to recognize the 
implicit word boundaries in Chinese text. During 
the past decades, great success has been achieved in 
Chinese word segmentation (Nie, et al 1995; Yao, 
1997; Fu and Wang, 1999; Wang et al 2000; 
Zhang, et al 2002). However, there still remain two 
difficult problems, i.e. ambiguity resolution and 
unknown word (so-called OOV word) identification, 
while developing a practical segmentation system 
for large open applications. 
In this paper, we present a two-stage statistical 
word segmentation system for Chinese. In the first 
stage, we employ word bigram model to segment 
known words (viz. the words included in the system 
dictionary) in input. In the second stage, we develop 
a hybrid algorithm to perform unknown word 
identification incorporating word contextual 
information, word-formation patterns and word 
juncture model.   
The rest of this paper is organized as follows: 
Section 2 presents a word bigram solution for 
known word segmentation. Section 3 describes a 
hybrid approach for unknown word identification. 
In section 4, we report the results of our system at 
the SIGHAN evaluation program, and in the final 
section we give our conclusions on this work.  
2 The first stage: Segmentation of known 
words 
In a sense, known word segmentation is a process 
of disambiguation. In our system, we use word 
bigram language models and Viterbi algorithm 
(1967) to resolve word boundary ambiguities in 
known word segmentation. 
For a particular input Chinese character string 
ncccC L21= , there is usually more than one 
possible segmentation mwwwW L21= according to 
given system dictionary. Word bigram segmentation 
aims to find the most appropriate segmentation 
mwwwW L21? =  that maximizes the probability 
?
=
?
m
i
iir wwP
1
1)|( , i.e.  
    ?
=
?
?=
m
i
iir
W
r
W
wwPCWPW
1
1)|(maxarg)|(maxarg?     (1) 
where )|( 1?iir wwP  is the probability that word lw  
will occur given previous word 1?iw , which can be 
easily estimated from segmented corpus using 
maximum likelihood estimation (MLE), i.e. 
              
)(
)(
)|(
1
1
1
?
?
?
?
i
ii
iir wCount
wwCount
wwP              (2) 
To avoid the problem of data sparseness in MLE, 
here we apply the linear interpolation technique 
(Jelinek and Mercer, 1980) to smooth the estimated 
word bigram probabilities.  
3 The second stage: Unknown word 
identification 
The second stage mainly concerns unknown words 
segmentation that remains unresolved in first stage. 
This section describes a hybrid algorithm for 
unknown word identification, which can incorporate 
word juncture model, word-formation patterns and 
contextual information. To avoid the complicated 
normalization of the probabilities of different 
dimensions, the simple superposition principle is 
also used in merging these models. 
3.1 Word juncture model 
Word juncture model score an unknown word by 
assigning word juncture type. Obviously, most 
unknown words appear as a string of known words 
after segmentation in first stage. Therefore, 
unknown word identification can be viewed as a 
process of re-assigning correct word juncture type 
to each known word pair in input. Given a known 
word string nwwwW L21= , between each word pair 
)11(1 ???+ niww ii  is a word juncture. In general, 
there are two types of junctures in unknown word 
identification, namely word boundary (denoted by 
Bt ) and non-word boundary (denoted by Nt ). 
Let )( 1+ii wwt denote the type of a word juncture 
1+ii ww , and ))(( 1+iir wwtP denote the relevant 
conditional probability, then 
              
)(
))(())((
1
1
1
+
+
+ =
ii
ii
def
iir wwCount
wwtCountwwtP              (3) 
Thus, the word juncture probability )( UCJM wP of a 
particular unknown word jiiU wwww L1+=  
)1( nji ??? can be calculated by  
 ?
?
=
++? ??=
1
111 ))(())(())(()(
j
il
llNrjjBriiBrUCJM cctPwwtPwwtPwP  (4) 
In a sense, word juncture model mirrors the affinity 
of known word pairs in forming an unknown word. 
For a word juncture ),( 1+ii ww , the larger the 
probability ))(( 1+iiNr wwtP , the more likely the two 
words are merged together into one new word. 
3.2 Word-formation patterns 
Word-formation pattern model scores an unknown 
word according to the probability of how each 
internal known word contributes to its formation. In 
general, a known word w  may take one of the 
following four patterns while forming a word: (1) 
w itself is a word. (2) w is the beginning of an 
unknown word. (3) w is at the middle of an 
unknown word. (4) w appears at the end of an 
unknown word.  For convenience, we use S , B , 
M and E  to denote the four patterns respectively. 
Let )(wpttn denote a particular pattern of w  in an 
unknown word and ))(( wpttnPr  denote the relevant 
probability, then  
                
)(
))(())((
wCount
wpttnCountwpttnP
def
r =              (5) 
Obviously, 1))(( =?
pttn
r wpttnP . And  1- ))(( wSPr  is the 
word-formation power of the known word w . 
Let  )( Upttn wP be the overall word-formation 
pattern probability of a certain unknown word 
lU wwww L21=  , then 
               ?
?
=
Ui ww
irUpttn wpttnPwP ))(()(              (6) 
Theoretically speaking, a known word can take any 
pattern while forming an unknown word. But it is 
not even in probability for different known words 
and different patterns. For example, the word ? 
(xing4, nature) is more likely to act as the suffix of 
words. According to our investigation on the 
training corpus, the character? appears at the end 
of a multiword in more than 93% of cases.  
3.3 Hybrid algorithm for unknown word 
identification 
Current algorithm for unknown word identification 
consists of three major components: (1) an 
unknown word extractor firstly extracts a fragment 
of known words nwww L21 that that may have 
unknown words based on the related word-
formation power and word juncture probability and 
its left and right contextual word Lw , Rw from the 
output of the first stage.  (2) A candidate word 
constructor then generates a lattice of all possible 
new segmentations }|{ 21 mUU xxxWW L=  that may 
involve unknown words from the extracted 
fragment. (3) A decoder finally incorporates word 
juncture model )( UWJM WP , word-formation 
patterns )( Upttn WP  and word bigram probability 
)( Ubigram WP to score these candidates, and then 
applies the Viterbi algorithm again to find the best 
new segmentation mU xxxW L21? =  that has the 
maximum score: 
      
}))|()()(({maxarg
)}()()({maxarg?
,,1
1?
=
?
++=
++=
ni
iiriCJMipttn
W
UbigramUCJMUpttn
W
U
xxPxPxP
WPWPWPW
U
U
L
           (7) 
where Lwx =0  and Rn wx =+1 . Let Uw  denote any 
unknown word in the training corpus. If ix  is an 
unknown word, then
)(
)(
)|(
1
1
1
?
?
?
?
=
i
w
Ui
iir xCount
wxCount
xxP U .  
4 Experiments 
Our system participated in both closed and open 
tests on Peking University corpora at the First 
International Chinese Word Segmentation Bakeoff.  
This section reports the results and discussions on 
its evaluation. 
4.1 Measures 
In the evaluation program of the First International 
Chinese Word Segmentation Bakeoff, six measures 
are employed to score the performance of a word 
segmentation system, namely test recall (R), test 
precision (denoted by P), the balanced F-measure 
(F), the out-of-vocabulary (OOV) rate for the test 
corpus, the recall on OOV words (ROOV), and the 
recall on in-vocabulary (Riv) words. OOV is defined 
as the set of words in the test corpus not occurring 
in the training corpus in the closed test, and the set 
of words in the test corpus not occurring in the 
lexicon used in the open test. 
4.2 Experimental lexicons and corpora 
As shown in Table 1, we only used the training data 
from Peking University corpus to train our system 
in both the open and closed tests. As for the 
dictionary, we compiled a dictionary for the closed 
test from the training corpus, which contained 55, 
226 words, and used a dictionary in the open test 
that contained about 65, 269 words.  
 
Items # words in 
lexicon 
# train. 
words  
# test. 
words 
Closed 55,226 1,121,017 17,194 
Open 65,269 1,121,017 17,194 
Table 1: Experimental lexicons and corpora 
4.3 Experimental results and discussion 
 
Items F R P OOV ROOV Riv 
Closed 0.939 0.936 0.942 0.069 0.675 95.5 
Open 0.937 0.933 0.941 0.094 0.762 95.0 
Table 2: Test results on PK corpus 
 
Segmentation speed: There are in all about 28,458 
characters in the test corpus. It takes about 3.21 
and 3.07 seconds in all for our system to perform 
full segmentation (including known word 
segmentation and unknown word identification) on 
the closed and open test corpus respectively, 
running on an ACER notebook (TM632XC-P4M). 
This indicates that our system is able to process 
about 531,925~556,182 characters per minute.  
Results and discussions: The results for the closed 
and open test are presented in Table 2. We can 
draw some conclusions from these results.  
Firstly, the overall performance of our system is 
very stable in both the closed and open tests. As 
shown in Table 2, the out-of-vocabulary (OOV) 
rate is 6.9% in the closed test and 9.4% in the open 
test. However, the overall test F-measure drops by 
only 0.2 percent in the open test, compared with the 
closed test.  
Secondly, our approach can handle most unknown 
words in the input. As can be seen from Table 2, 
the recall on OOV words are 67.5% the closed-test 
and 76.2% in the open-test. Wang et al(2000) and 
Yao (1997) have proposed a character juncture 
model and word-formation patterns for Chinese 
unknown word identification. However, their 
approaches can only work for the unknown words 
that are made up of pure monosyllable character in 
that they are character-based methods. To address 
this problem, we introduce both word juncture 
model and word-based word-formation patterns into 
our system. As a result, our system can deal with 
different unknown words that consist of different 
known words, including monosyllable characters 
and multiword.  
Although our system is effective for most 
ambiguities and unknown words in the input, it has 
its inherent deficiencies. Firstly, to avoid data 
sparseness, we do not differentiate known words 
and unknown words while estimating word juncture 
models and word-formation patterns from the 
training corpus. This simplification may introduce 
some noises into these models for identifying 
unknown words. Our further investigations show 
that the precision on OOV words is still very low, 
i.e. 67.1% for closed test and 72.5% for open test. 
As a result, our system may yield a number of 
mistaken unknown words in the processing. 
Secondly, we regard known word segmentation and 
unknown word identification as two independent 
stages in our system. This strategy is obviously 
simple and more easily applicable. However, it does 
not work while the input contains a mixture of 
ambiguities and unknown words.  For example, 
there was a sentence ?????????? in 
the test corpus, where, the string ???? is a 
fragment mixed with ambiguity and unknown 
words. The correct segmentation should be ??/?
?/, where??(Zhonghang, the Bank of China) is 
a abbreviation of organization name, and ?? 
(Changge) is a place name. Actually, this fragment 
is segmented as?/??/?/ in the first stage of our 
system. However, the unknown word identification 
stage does not have a mechanism to split the word
??(Hangzhang, president) and  finally resulted in 
wrong segmentation.  
5 Conclusions 
This paper presents a two-stage statistical word 
segmentation system for Chinese. In the first stage, 
word bigram model and Viterbi algorithm are 
applied to perform known word segmentation on 
input plain text, and then a hybrid approach is 
employed in the second stage to incorporate word 
bigram probabilities, word juncture model and 
word-based word-formation patterns to detect OOV 
words. The experiments on Peking University 
corpora have shown that the present system based 
on fairly simple word bigram and word-formation 
models can achieve a F-score of 93.7% or above. In 
future work, we hope to improve our strategies on 
estimating word juncture model and word-formation 
patterns and develop an integrated segmentation 
technique that can perform known word 
segmentation and unknown word identification at 
one time. 
Acknowledgments 
We would like to thank all colleagues of the First 
International Chinese Word Segmentation Bakeoff 
for their evaluations of the results and the Institute 
of Computational Linguistics, Peking University for 
providing the experimental corpora. 
References 
Fu, Guohong and Xiaolong Wang. 1999. Unsupervised 
Chinese word segmentation and unknown word 
identification. In: Proceedings of NLPRS?99, 
Beijing, China, 32-37. 
Jelinek, Frederick, and Robert L. Mercer. 1980. 
Interpolated estimation of Markov source parameters 
from sparse data. In: Proceedings of Workshop on 
Pattern Recognition in Practice, Amsterdam, 381-
397. 
Nie, Jian-Yuan, M.-L. Hannan and W.-Y. Jin. 1995. 
Unknown word detection and segmentation of 
Chinese using statistical and heuristic knowledge. 
Communication of COLIPS, 5(1&2): 47-57.  
Viterbi, A.J. 1967. Error bounds for convolutional 
codes and an asymmetrically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, IT-13(2): 260-269. 
Wang, Xiaolong, Fu Guohong, Danial S.Yeung, James 
N.K.Liu, and Robert Luk. 2000. Models and 
algorithms of Chinese word segmentation. In: 
Proceedings of the International Conference on 
Artificial Intelligence (IC-AI?2000), Las Vegas, 
Nevada, USA, 1279-1284. 
Yao, Yuan. 1997. Statistics Based approaches towards 
Chinese language processing. Ph.D. thesis. National 
University of Singapore. 
Zhang, Hua-Ping, Qun Liu, Hao Zhang, and Xue-Qi 
Cheng. 2002. Automatic recognition of Chinese 
unknown words based on roles tagging. In: 
Proceedings of The First SIGHAN Workshop on 
Chinese Language Processing, Taiwan, 71-77. 
