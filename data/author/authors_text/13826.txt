Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89?97,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Collecting a Motion-Capture Corpus of American Sign Language  for Data-Driven Generation Research 
Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu 
Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu 
 
 
Abstract 
American Sign Language (ASL) generation software can improve the accessibility of in-formation and services for deaf individuals with low English literacy.  The understand-ability of current ASL systems is limited; they have been constructed without the benefit of annotated ASL corpora that encode detailed human movement.  We discuss how linguistic challenges in ASL generation can be ad-dressed in a data-driven manner, and we de-scribe our current work on collecting a motion-capture corpus. To evaluate the qual-ity of our motion-capture configuration, cali-bration, and recording protocol, we conducted an evaluation study with native ASL signers. 1 Introduction American Sign Language (ASL) is the primary means of communication for about one-half mil-lion deaf people in the U.S. (Mitchell et al, 2006).  ASL has a distinct word-order, syntax, and lexicon from English; it is not a representation of English using the hands.  Although reading is part of the curriculum for deaf students, lack of auditory ex-posure to English during the language-acquisition years of childhood leads to lower literacy for many adults.  In fact, the majority of deaf high school graduates in the U.S. have only a fourth-grade (age 10) English reading level (Traxler, 2000).   1.1 Applications of ASL Generation Research Most technology used by the deaf does not address this literacy issue; many deaf people find it diffi-
cult to read the English text on a computer screen or on a television with closed-captioning. Software to present information in the form of animations of ASL could make information and services more accessible to deaf users, by displaying an animated character performing ASL, rather than English text.  While writing systems for ASL have been proposed (Newkirk, 1987; Sutton, 1998), none is widely used in the Deaf community.  Thus, an ASL generation system cannot produce text output; the system must produce an animation of a human character performing sign language.  Coordinating the simultaneous 3D movements of parts of an animated character?s body is challenging, and few researchers have attempted to build such systems.   Prior work can be divided into two areas: scripting and generation/translation. Scripting sys-tems allow someone who knows sign language to ?word process? an animation by assembling a se-quence of signs from a lexicon and adding facial expressions.  The eSIGN project created tools for content developers to build sign databases and as-semble scripts of signing for web pages (Ken-naway et al, 2007).  Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (dis-cussed in section 4).  Others study generation or machine translation (MT) of sign language (Chiu et al, 2007; Elliot & Glauert, 2008; Fotinea et al, 2008; Huenerfauth, 2006; Karpouzis et al, 2007; Marshall & Safar, 2005; Shionome et al, 2005; Sumihiro et al, 2000; van Zijl & Barker, 2003). Experimental evaluations of the understandabil-ity of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compre-
89
hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al, 2008).  Errors include a lack of smooth inter-sign transitions, lack of grammatically-required facial expressions, and inaccurate sign perform-ances related to morphological inflection of signs. While current ASL animation systems have limitations, there are several advantages in present-ing sign language content in the form of animated virtual human characters, rather than videos: ? Generation or MT software planning ASL sen-tences cannot just concatenate videos of ASL.  Using video clips, it is difficult to produce smooth transitions between signs, subtle mo-tion variations in sign performances, or proper combinations of facial expressions with signs. ? If content must be frequently modified or up-dated, then a video performance would need to be largely re-recorded for each modification.  Whereas, an animation (scripted by a human author) could be further edited or modified. ? Because the face is used to indicate important information in ASL, a human must reveal his or her identity when producing an ASL video. Instead, a virtual human character could per-form sentences scripted by a human author. ? For wiki-style applications in which multiple authors are collaborating on information con-tent, ASL videos would be distracting: the per-son performing each sentence may differ.  A virtual human would be more uniform. ? Animations can be appealing to children for use in educational applications.  ? Animations allow ASL to be viewed at differ-ent angles, at different speeds, or by different virtual humans ? depending on the preferences of the user.  This can enable education applica-tions in which students learning ASL can prac-tice their ASL comprehension skills. 1.2 ASL is Challenging for NLP Research Natural Language Processing (NLP) researchers often apply techniques originally designed for one language to another, but research is not commonly ported to sign languages. One reason is that with-out a written form for ASL, NLP researchers must produce animation and thus address several issues: ? Timing: An ASL performance?s speed consists of: the speed of individual sign performances, 
the transitional time between signs, and the in-sertion of pauses during signing ? all of which are based on linguistic factors such as syntactic boundaries, repetition of signs in a discourse, and the part-of-speech of signs (Grosjean et al, 1979). ASL animations whose speed and paus-ing are incorrect are significantly less under-standable to ASL signers (Huenerfauth, 2009). ? Spatial Reference: Signers arrange invisible placeholders in the space around their body to represent objects or persons under discussion (Meier, 1990). To perform personal, posses-sive, or reflexive pronouns that refer to these entities, signers later point to these locations. Signers may not repeat the identity of these en-tities again; so, their conversational partner must remember where they have been placed.  An ASL generator must select which entities should be assigned 3D locations (and where). ? Inflection: Many verbs change their motion paths to indicate the 3D location where a spa-tial reference point has been established for their subject, object, or both (Padden, 1988). Generally, the motion paths of these inflecting verbs change so that their direction goes from the subject to the object (Figure 1); however, their paths are more complex than this.  Each verb has a standard motion path that is affected by the subject?s and the object?s 3D locations.  When a verb is inflected in this way, the signer does not need to overtly state the subject/object of a sentence. An ASL generator must produce appropriately inflected verb paths based on the layout of the spatial reference points. 
(a.)  
(b.)  Figure 1: An ASL inflecting verb ?BLAME?:  (a.) (person on left) blames (person on right),  (b.) (person on right) blames (person on left). 
90
? Coarticulation: As in speech production, the surrounding signs in a sentence affect finger, hand, and body movements.  ASL generators that use overly simple interpolation rules to produce these coarticulation effects yield un-natural and non-fluent ASL animation output. ? Non-Manuals: Head-tilt and eye-gaze indicate the 3D location of a verb?s subject and object (or other information); facial expressions also indicate negation, questions, topicalization, and other essential syntactic phenomena not conveyed by the hands (Neidle et al, 2000). Animations without proper facial expressions (and proper timing relative to manual signs) cannot convey the proper meaning of ASL sen-tences in a fluent and understandable manner. ? Evaluation: With no standard written form for ASL, string-based metrics cannot be used to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native sign-ers, prevent English environmental influences (that affect signer?s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al, 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al, 2000; Veale et al, 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and dis-cusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2).  Unfortunately, most prior work is not ?data-driven,? i.e. not based on statistical modeling of corpora, the dominant successful modern NLP ap-proach. The sign language generation research that has thus far been the most data-driven includes: ? Some researchers have used motion-capture (see section 3) to build lexicons of animations of individual signs, e.g. (Cox et al, 2002). However, their focus is recording a single cita-tion form of each sign, not creating annotated corpora of full sentences or discourse. Single-
sign recordings do not enable researchers to examine the Timing, Coarticulation, Spatial Reference, Non-Manuals, or Inflection phe-nomena (section 1.2), which operate over mul-tiple signs or sentences in an ASL discourse. ? Other researchers have examined how statisti-cal MT techniques could be used to translate from a written language to a sign language. Morrissey and Way (2005) discuss an exam-ple-based MT architecture for Irish Sign Lan-guage, and Stein et al (2006) apply simple statistical MT approaches to German Sign Language. Unfortunately, the sign language ?corpora? used in these studies consist of tran-scriptions of the sequence of signs performed, not recordings of actual human performances.  A transcription does not capture subtleties in the 3D movements of the hands, facial move-ments, or speed of an ASL performance.  Such information is needed in order to address the Spatial Reference, Inflection, Coarticulation, Timing, or Non-Manuals issues (section 1.2). ? Seguoat and Braffort (2009) derive models of coarticulation for French Sign Language based on a semi-automated ?rotoscoping? annotation of hand location from videos of signing. 1.4 Prior Sign Language Corpora Resources The reason why most prior ASL generation re-search has not been data-driven is that sufficiently detailed and annotated sign language corpora are in short supply and are time-consuming to construct. Without a writing system in common use, it is not possible to harvest some naturally arising source of ASL ?text?; instead, it is necessary to record the performance of a signer (through video or a mo-tion-capture suit).  Human signers must then tran-scribe and annotate this data by adding time-stamped linguistic details. For ASL (Neidle et al, 2000) and European sign languages (Bungeroth et al, 2006; Crasborn et al, 2004, 2006; Efthimiou & Fotinea, 2007), signers have been videotaped and experts marked time spans when events occur ? e.g. the right hand is performing the sign ?CAT? during time index 250-300 milliseconds, and the eyebrows are raised during time index 270-300. Such annotation is time-consuming to add; the largest ASL corpus has a few thousand sentences.   In order to learn how to control the movements of an animated virtual human based on a corpus, 
91
we need precise hand locations and joint angles of the human signer?s body throughout the perform-ance.  Asking humans to write down 3D angles and coordinates is time-consuming and inexact; some researchers have used computer vision techniques to model the signers? movements (see survey in (Loeding et al, 2004)).  Unfortunately, the com-plex shape of hands/face, rapid speed, and frequent occlusion of parts of the body during ASL limit the accuracy of vision-based recognition; it is not yet a reliable way to build a 3D model of a signer for a corpus.  Motion-capture technology (discussed in section 3) is required for this level of detail.   2 Research Goals & Focus of This Paper To address the lack of sufficiently detailed and linguistically annotated ASL corpora, we have be-gun a multi-year project to collect and annotate a motion-capture corpus of ASL (section 3). Digital 3D body movement and handshape data collected from native signers will become a permanent re-search resource for study by NLP researchers and ASL linguists. This corpus will allow us to create new ASL generation technologies in a data-driven manner by analyzing the subtleties in the motion data and its relationship to the linguistic structure. Specifically, we plan to model where signers tend to place spatial reference points around them in space. We also plan to uncover patterns in the mo-tion paths of inflecting verbs and model how they relate to layout of spatial references points. These models could be used in ASL generation software or could be used to partially automate with work of humans using ASL-scripting systems. To evaluate our ASL models, native signers will be asked to judge ASL animations produced using them. There are several unique aspects of our research: ? We use a novel combination of hand, body, head, and eye motion-tracking technologies and simultaneous video recordings (section 3). ? We collect multi-sentence single-signer ASL discourse, and we annotate novel linguistic in-formation (relevant to spatial reference points). ? We involve ASL signers in the research in several ways: as evaluators of our generation software, as research assistants conducting evaluation studies, and as corpus annotators. This paper will focus on the first of these as-pects of our project. Specifically, section 4 will 
examine the following research question: Have we successfully configured and calibrated our motion-capture equipment so that we are recording good-quality data that will be useful for NLP research?   Since the particular combination of motion-capture equipment we are using is novel and be-cause there have not been prior motion-capture-based ASL corpora projects, section 4 will evaluate whether the data we are collecting is of sufficient quality to drive ASL animations of a virtual human character.  In corpus-creation projects for tradi-tional written/spoken languages, researchers typi-cally gather text, audio, or (sometimes) video of human performances.  The quality of the gathered recordings is typically easier to verify and evalu-ate; for motion-capture data collected with a com-plex configuration of equipment, a more complex experimental design is necessary (section 4). 3 Our Motion-Capture Configuration The first stage of our research is to accurately and efficiently record 3D motion-capture data from ASL signers.  Assuming an ASL signer?s pelvis bone is stationary in 3D space, we want to record movement data for the upper body.  We are inter-ested in the shapes of each hand; the 3D location of the hands; the 3D orientation of the palms; joint angles for the wrists, elbows, shoulders, clavicle, neck, and waist; and a vector representing the eye-gaze aim.  We are using a customized configura-tion of several commercial motion-capture devices (as shown in Figure 2, worn by a human signer): ? Two Immersion CyberGloves?: The 22 flexi-ble sensor strips sewn into each of these spandex gloves record finger joint angles so that we can record the signer?s handshapes.  These gloves are ideal for recording ASL be-cause they are flexible and lightweight.  Hu-mans viewing a subject wearing the gloves are able to discern ASL fingerspelling and signing. ? Applied Science Labs H6 eye-tracker: This lightweight head-mounted eye-tracker with a near-eye camera records a signer?s eye gaze di-rection. A camera on the headband aims down, and a small clear plastic panel in front of the cheek reflects the image of the subject?s eye. When combined with the head tracking infor-mation from the IS-900 system below, the H6 identifies a 3D vector of eye-gaze in a room. 
92
? Intersense IS-900: This acoustical/intertial mo-tion-capture system uses a ceiling-mounted ul-trasonic speaker array and a set of directional microphones on a small sensor to record the location and orientation of the signer?s head.  A sensor sits atop the helmet shown in Figure 2a. IS-900 data is used to compensate for head movement when calculating eye-gaze direction with the Applied Science Labs H6 eye-tracker.  ? Animazoo IGS-190: This spandex bodysuit is covered with soft Velcro to which small sen-sors attach.  A sensor placed on each segment of the human?s body records inertial and mag-netic information.  Subjects wearing the suit stand facing north with their arms down at their sides at the beginning of the recording session; given this known starting pose and di-rection, the system calculates joint angles for the wrists, elbows, shoulders, clavicle, neck, and waist. We do not record leg/foot informa-tion in our corpus. Prior to recording data, we photograph subjects standing in a cube-shaped rig of known size; this allows us to identify bone lengths of the human subject, which are needed for the IGS-190 system to accurately calculate joint angles from the sensor data. Motion-capture recording sessions are video-taped to facilitate later linguistic analysis and an-notation. Videotaping the session also facilitates the ?clean up? of the motion-capture data in post-processing, during which algorithms are applied to adjust synchronization of different sensors or re-move ?jitter? or other noise artifacts from the re-cording.  Three digital high-speed video cameras 
film front view, facial close-up, and side views of the signer ? a setup that has been used in video-based ASL-corpora-building projects (Neidle et al, 2000). The front view is similar to Figure 2a (but wider). The facial close-up view is useful when later identifying specific non-manual facial expres-sions during ASL performances, which are essen-tial to correctly understanding and annotating the collected data. To facilitate synchronizing the three video files during post-processing, a strobe is flashed once at the start of the recording session. A ?blue screen? curtain hangs on the back and side walls of the motion-capture studio.  If future computer vision researchers wish to use this corpus to study ASL recognition from video, it is useful to have solid color walls for ?chroma key? back-ground removal.  Photographic studio lighting with spectra compatible with the eye-tracking system is used to support high-quality video recording.   During data collection, a native ASL signer (called the ?prompter?) sits directly behind the front-view camera to engage the participant wear-ing the suit (the ?performer?) in natural conversa-tion. While the corpus we are collecting consists of unscripted single-signer discourse, prior ASL cor-pora projects have identified the importance of sur-rounding signers with an ASL-centric environment during data collection (Neidle et al, 2000). English influence in the studio must be minimized to pre-vent signers from inadvertently code-switching to an English-like form of signing.  Thus, it is impor-tant that a native signer acts as the prompter, who conversationally suggests topics for the performer to discuss (to be recorded as part of the corpus).   
a.  b.  c.  Figure 2: (a) Motion-capture equipment configuration, (b) animation produced from motion-capture data (shown in evaluation study), and (c) animation produced using Sign Smith (shown in evaluation study). 
93
In our first year, we have collected and anno-tated 58 passages from 6 signers (40 minutes). We prefer to collect multi-sentence passages discuss-ing varied numbers of topics and with few ?classi-fier predicates,? phenomena that aren?t our current research focus.  In (Huenerfauth & Lu, 2010), we discuss details of: the genre of discourse we re-cord, our target linguistic phenomena to capture (spatial reference points and inflected verbs), the types of linguistic annotation added to the corpus, and the effectiveness of different ?prompts? used to elicit the desired type of spontaneous discourse.  This paper focuses on verifying the quality of the motion-capture data we can record using our current equipment configuration and protocols. We want to measure how well we have compensated for several possible sources of error in recordings: ? If a sensor connection is temporarily lost, then data gaps occur. We have selected equipment that does not require line-of-sight connections and tried to arrange the studio to avoid fre-quent dropping of any wireless connections. ? We ask subjects to perform a quick head movement and distinctive eye blink pattern at the beginning of the recording session to facili-tate ?synchronization? of the various motion-capture data streams during post-processing. ? Electronic and physical properties of sensors can lead to ?noise? in the data, which we at-tempt to remove with smoothing algorithms. ? Differences between the bone lengths of the human and the ?virtual skeleton? of the ani-mated character being recorded could lead to ?retargeting? errors, in which the body poses of the human do not match the recording.  We must be careful in the measurement of the bone lengths of the human participant and in the design of the virtual animation skeleton. ? To compensate for differences in how equip-ment sits on the body on different occasions or on different humans, we must set ?calibration? values; e.g., we designed a novel protocol for efficiently and accurately calibrating gloves for ASL signers (Lu & Huenerfauth, 2009).   4 Evaluating Our Collected Motion Data If a speech synthesis researcher were using a novel microphone technology to record audio perform-ances from human speakers to build a corpus, that 
researcher would want to experimentally confirm that the audio recordings were of high enough quality for research.  Even when perfectly clear audio recordings of human speech are recorded in a corpus, the automatic speech synthesis models trained on this data are not perfect.  Degradations in the quality of the corpus would yield even lower quality speech synthesis systems.  In the same way, it is essential that we evaluate the quality of the ASL motion-capture data we are collecting. In an earlier study, we sought to collect motion-data from humans and directly produce animations from them as an ?upper baseline? for an experi-mental study (Huenerfauth, 2006). We were not analyzing the collected data or using it for data-driven generation, we merely wanted the data to directly drive an animation of a virtual human character as a ?virtual puppet.? This earlier project used a different configuration of motion-capture equipment, including an earlier version of Cyber-Gloves? and an optical motion-capture system that required line-of-sight connections between infrared emitters on the signer?s body and cameras around the room.  Unfortunately, the data collected was so poor that the animations produced from the mo-tion-capture were not an ?upper? baseline ? in fact, they were barely understandable to native signers.  Errors arose from dropped connections, poor cali-bration, and insufficient removal of data noise. We have selected different equipment and have designed better protocols for recording high quality ASL data since that earlier study ? to compensate for the ?noise,? ?retargeting,? ?synchronization,? and ?calibration? issues mentioned in section 3.  However, we know that under some recording conditions, the quality of collected motion-capture data is so poor that ?virtual puppet? animations synthesized from it are not understandable. We expect that an even higher level of data quality is needed for a motion-capture corpus, which will be analyzed and manipulated in order to synthesize novel ASL animations from it.  Therefore, we con-ducted a study (discussed below) to evaluate the quality of our current motion-capture configura-tion.  As in our past study, we use the motion-capture data to directly control the body move-ments of a virtual human ?puppet.?  We then ask native ASL signers to evaluate the understandabil-ity and naturalness of the resulting animations (and compare them to some baseline animations pro-duced using ASL-animation scripting software).   
94
In our prior work, a native ASL signer designed a set of ASL stories and corresponding compre-hension questions for use in evaluation studies (Huenerfauth, 2009). The stories? average length is approximately 70 signs, and they consist of news stories, encyclopedia articles, and short narratives. We produced animations of each using Sign Smith Studio (SSS), commercial ASL-animation script-ing software (Vcom3D, 2010). Signs from SSS?s lexicon are placed on a timeline, and linguistically appropriate facial expressions are added. The soft-ware synthesizes an animation of a virtual human performing the story (Figure 2c). In earlier work, we designed algorithms for determining sign-speed and pause-insertion in ASL animations based on linguistic features of the sentence. We conducted a study to compare animations with default timing settings (uniform pauses and speed) and anima-tions governed by our timing algorithm ? at vari-ous speeds. The use of our timing algorithm yielded ASL animations that native signers found more understandable (Huenerfauth, 2009). We are reusing these stories and animations as baselines for comparison in a new evaluation study (below).   While we are collecting unscripted passages in our corpus, it is easier to compare the quality of different versions of animations when using a common set of scripted stories. Thus, we used the script from 10 of the stories above, and each was performed by a native signer, a 22-year-old male who learned ASL prior to age 2. He wore the full set of motion-capture equipment, and we followed the same calibration process and protocols as we do when recording ASL passages for our corpus. The signer rehearsed and memorized each story; ?cue cards? were also available when recording. 
Autodesk MotionBuilder software was used to produce a virtual human whose movements were driven by the motion-capture data (see Figure 2b). While our corpus contains video of facial expres-sion, our motion-capture equipment does not digit-ize it; so, the virtual human character has no facial movements. The recorded signer moved at an av-erage speed of 1.12 signs/second, and so for com-parison, we selected the version of the scripted ASL animations with the closest speed from our earlier study: 1.2 signs/second. (Since the scripted animations are slightly slower and include linguis-tic facial expressions, we expected them to receive higher understandability scores than our motion-capture animations.)  In our earlier work, we pro-duced two versions of each scripted story: one with default timing and one with our novel timing algo-rithm. Both versions are used as baselines for comparison in this new study; thus, we compare three versions of the same set of 10 ASL stories. Using questions designed to screen for native ASL signers developed in prior work (Huenerfauth et al, 2008), we recruited 12 participants to evalu-ate the ASL animations. A native ASL signer con-ducted the studies, in which participants viewed an animation and were then asked two types of ques-tions after each: (1) ten-point Likert-scale ques-tions about the ASL animation?s grammatical correctness, understandability, and naturalness of movement and (2) multiple-choice comprehension questions about basic facts from the story. The comprehension questions were presented in the form of scripted ASL animations (produced in SSS), and answer choices were presented in the form of clip-art images (so that strong English lit-eracy was not necessary). Identical questions were 
 Figure 3: Evaluation and comprehension scores (asterisks mark significant pairwise differences). 
95
used to evaluate the motion-capture animations and the scripted animations.  Examples of the questions are included in (Huenerfauth, 2009). Figure 3 displays results of the Likert-scale sub-jective questions and comprehension-question suc-cess scores for the three types of animations evaluated in this study. The scripted animations using our timing algorithm have higher compre-hension scores, but the motion-capture animations have higher naturalness scores.  All of the other scores for the animations are quite similar. Statisti-cally significant differences are marked with an asterisk (p<0.05, Mann-Whitney pairwise compari-sons with Bonferroni-corrected p-values). Non-parameteric tests were selected because the Likert-scale responses were not normally distributed. 5 Conclusion and Future Research Goals The research question addressed by this paper was whether our motion-capture configuration and re-cording protocols enabled us to collect motion-data of sufficient quality for data-driven ASL genera-tion research. In our study, the evaluation scores of the animations driven by the motion-capture data were similar to those of animations produced using state-of-the-art ASL animation scripting software.  This is a promising result, especially considering the slightly faster speed and lack of facial expres-sion information in the motion-capture animations.  While this suggests that the data we are collecting is of good quality, the real test will be when this corpus is used in future research.  If we can build useful ASL-animation generation software based on analysis of this corpus, then we will know that we have sufficient quality of motion-capture data. 5.1 Our Long-Term Research Goal: Making ASL Accessible to More NLP Researchers It is our goal to produce high-quality broad-coverage ASL generation software, which would benefit many deaf individuals with low English literacy.  However, this ambition is too large for any one team; for this technology to become real-ity, ASL must become a language commonly stud-ied by NLP researchers.  For this reason, we seek to build ASL software, models, and experimental techniques to serve as a resource for other NLP researchers.  Our goal is to make ASL ?accessible? to the NLP community.  By developing tools to address some of the modality-specific and spatial 
aspects of ASL, we can make it easier for other researchers to transfer their new NLP techniques to ASL. The goal is to ?normalize? ASL in the eyes of the NLP community.  Bridging NLP and ASL research will not only benefit deaf users: ASL will push the limits of current NLP techniques and will thus benefit other work in the field of NLP.  Sec-tion 1.2 listed six challenges for ASL NLP re-search; we address several of these in our research: We have conducted many experimental studies in which signers evaluate the understandability and naturalness of ASL animations (Huenerfauth et al, 2008; Huenerfauth, 2009).  To begin to address the Evaluation issue (section 1.2), we have published best-practices, survey materials, and experimental protocols for effectively evaluating ASL animation systems through the participation of native signers. We have also published baseline comprehension scores for ASL animations.  We will continue to produce such resources in future work. Our earlier work on timing algorithms for ASL animations (mentioned in section 4) was based on data reported in the linguistics literature (Grosjean et al, 1979).  In future work, we want to learn tim-ing models directly from our collected corpus ? to further address the Timing issue (section 1.2). To address the issues of Spatial Reference and Inflection (section 1.2), we plan on analyzing our ASL corpus to build models that can predict where in 3D space signers establish spatial reference points.  Further, we will analyze our corpus to ana-lyze how certain ASL verbs are inflected based on the 3D location of their subject and object. We want to build a parameterized lexicon of ASL verbs: given a 3D location for subject and object, we want to predict a 3D motion-path for the char-acter?s hands for a specific performance of a verb. While addressing the issues of Coarticulation and Non-Manuals (section 1.2) are not immediate research priorities, we believe our ASL corpus may also be useful in building computational models of these phenomena for data-driven ASL generation. Acknowledgments This material is based upon work supported by the National Science Foundation (Award #0746556), Siemens (Go PLM Academic Grant), and Visage Technologies AB (free academic license for soft-ware).  Jonathan Lamberton, Wesley Clarke, Kel-sey Gallagher, Amanda Krieger, and Aaron Pagan assisted with ASL data collection and experiments. 
96
References J. Bungeroth, D. Stein, P. Dreuw, M. Zahedi, H. Ney. 2006. A German sign language corpus of the domain weather report. Proc. LREC 2006 workshop on rep-resentation & processing of sign languages. Y.H. Chiu, C.H. Wu, H.Y. Su, C.J. Cheng. 2007. Joint optimization of word alignment and epenthesis gen-eration for Chinese to Taiwanese sign synthesis. IEEE Trans Pattern Anal Mach Intell 29(1):28-39. S. Cox, M. Lincoln, J. Tryggvason, M. Nakisa, M. Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to aid communication with deaf people. Proc. ASSETS. O. Crasborn, E. van der Kooij, D. Broeder, H. Brugman. 2004. Sharing sign language corpora online: propos-als for transcription and metadata categories. Proc. LREC 2004 workshop on representation & process-ing of sign languages, pp. 20-23. O. Crasborn, H. Sloetjes, E. Auer, and P. Wittenburg. 2006. Combining video and numeric data in the analysis of sign languages within the ELAN annota-tion software. Proc. LREC 2006 workshop on repre-sentation & processing of sign languages, 82-87. E. Efthimiou, S.E. Fotinea. 2007. GSLC: creation and annotation of a Greek sign language corpus for HCI. Proc. HCI International. R. Elliot, J. Glauert. 2008. Linguistic modeling and lan-guage-processing technologies for avatar-based sign language presentation. Universal Access in the In-formation Society 6(4):375-391. S.E. Fotinea, E. Efthimiou, G. Caridakis, K. Karpouzis. 2008. A knowledge-based sign synthesis architecture. Univ. Access in Information Society 6(4):405-418. F. Grosjean, L. Grosjean, H. Lane. 1979. The patterns of silence: Performance structures in sentence produc-tion. Cognitive Psychology 11:58-81. M. Huenerfauth. 2006. Generating American sign lan-guage classifier predicates for English-to-ASL ma-chine translation, dissertation, U. of Pennsylvania. M. Huenerfauth, L. Zhao, E. Gu, J. Allbeck. 2008. Evaluation of American sign language generation by native ASL signers. ACM Trans Access Comput 1(1):1-27. M. Huenerfauth. 2009. A linguistically motivated model for speed and pausing in animations of American sign language. ACM Trans Access Comput 2(2):1-31. M. Huenerfauth, P. Lu. 2010. Annotating spatial refer-ence in a motion-capture corpus of American sign language discourse. Proc. LREC 2010 workshop on representation & processing of sign languages. K. Karpouzis, G. Caridakis, S.E. Fotinea, E. Efthimiou. 2007. Educational resources and implementation of a Greek sign language synthesis architecture. Comput-ers & Education 49(1):54-74. J. Kennaway, J. Glauert, I. Zwitserlood. 2007. Providing signed content on Internet by synthesized animation. ACM Trans Comput-Hum Interact 14(3):15. 
B. Loeding, S. Sarkar, A. Parashar, A. Karshmer. 2004. Progress in automated computer recognition of sign language, Proc. ICCHP, 1079-1087. P. Lu, M. Huenerfauth. 2009. Accessible motion-capture glove calibration protocol for recording sign language data from deaf subjects. Proc. ASSETS. I. Marshall, E. Safar. 2005. Grammar development for sign language avatar-based synthesis. Proc. UAHCI. R. Meier. 1990. Person deixis in American sign lan-guage. In: S. Fischer & P. Siple (eds.), Theoretical issues in sign language research, vol. 1: Linguistics.  Chicago: University of Chicago Press, 175-190. R. Mitchell, T. Young, B. Bachleda, M. Karchmer. 2006. How many people use ASL in the United States? Sign Language Studies 6(3):306-335. S. Morrissey, A. Way. 2005. An example-based ap-proach to translating sign language. Proc. Workshop on Example-Based Machine Translation, 109-116. C. Neidle, D. Kegl, D. MacLaughlin, B. Bahan, & R.G. Lee. 2000. The syntax of ASL: functional categories and hierarchical structure. Cambridge: MIT Press. D. Newkirk. 1987. SignFont Handbook. San Diego: Emerson and Associates. C. Padden. 1988. Interaction of morphology & syntax in American sign language. Outstanding dissertations in linguistics, series IV. New York: Garland Press. J. Segouat, A. Braffort. 2009. Toward the study of sign language coarticulation: methodology proposal. Proc Advances in Comput.-Human Interactions, 369-374. T. Shionome, K. Kamata, H. Yamamoto, S. Fischer. 2005. Effects of display size on perception of Japa-nese sign language---Mobile access in signed lan-guage. Proc. Human-Computer Interaction, 22-27. D. Stein, J. Bungeroth, H. Ney. 2006. Morpho-syntax based statistical methods for sign language transla-tion. Proc. European Association for MT, 169-177. K. Sumihiro, S. Yoshihisa, K. Takao. 2000. Synthesis of sign animation with facial expression and its effects on understanding of sign language. IEIC Technical Report 100(331):31-36. V. Sutton. 1998. The Signwriting Literacy Project. In Impact of Deafness on Cognition AERA Conference. C. Traxler. 2000. The Stanford achievement test, ninth edition: national norming and performance standards for deaf and hard-of-hearing students. J. Deaf Studies and Deaf Education 5(4):337-348. L. van Zijl, D. Barker. 2003. South African sign lan-guage MT system. Proc. AFRIGRAPH, 49-52.  VCom3D. 2010. Sign Smith Studio.  http://www.vcom3d.com/signsmith.php T. Veale, A. Conway, B. Collins. 1998. Challenges of cross-modal translation: English to sign translation in ZARDOZ system. Machine Translation 13:81-106. L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler, M. Palmer. 2000. A machine translation system from English to American sign language. Proc. AMTA. 
97
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 66?74,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
 
 
Learning a Vector-Based Model of American Sign Language  Inflecting Verbs from Motion-Capture Data 
Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu 
Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu  Abstract 
American Sign Language (ASL) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents. Using motion-capture data recorded from human signers, we model how the mo-tion-paths of verb signs vary based on the lo-cation of their subject and object.  This model yields a lexicon for ASL verb signs that is pa-rameterized on the 3D locations of the verb?s arguments; such a lexicon enables more real-istic and understandable ASL animations.  A new model presented in this paper, based on identifying the principal movement vector of the hands, shows improvement in modeling ASL verb signs, including when trained on movement data from a different human signer. 1 Introduction American Sign Language (ASL) is a primary means of communication for over 500,000 people in the U.S. (Mitchell et al, 2006).  As a natural language that is not merely an encoding of English, ASL has a distinct syntax, word order, and lexicon. Someone can be fluent in ASL yet have significant difficulty reading English; in fact, due to various educational factors, the majority of deaf high school graduates (age 18+) in the U.S. have a fourth-grade (age 10) English reading level or low-er (Traxler, 2000).  This leads to accessibility chal-lenges for deaf adults when faced with English text on computers, video captions, or other sources.  
Technologies for automatically generating com-puter animations of ASL can make information and services accessible to deaf people with lower English literacy. While videos of sign language are feasible to produce in some contexts, animated avatars are more advantageous than video when the information content is often modified, the con-tent is generated or translated automatically, or signers scripting a message in ASL wish to pre-serve anonymity.  This paper focuses on ASL and producing accessible sign language animations for people who are deaf in the U.S., but many of the linguistic issues, literacy rates, and animation tech-nologies discussed within are also applicable to other sign languages used internationally.   2 Use Of Space, Inflected Verbs ASL signers can associate entities or concepts they are discussing with arbitrary locations in space (Liddle, 2003; Lillo-Martin, 1991; McBurney, 2002; Meier, 1990).  After an entity is first men-tioned, a signer may point to a 3D location in space around his/her body; to refer to this entity again, the signer (or his/her conversational partner) can point to this location. Many linguists have studied this pronominal use of space (Klima et al 1979; Liddell, 2003; McBurney, 2002; Meier, 1990). Some argue that signers tend to pick 3D locations on a semi-circular arc floating at chest height in front of their torso (McBurney, 2002; Meier, 1990); others argue that signers pick 3D locations at dif-ferent heights and distances from their body (Lid-dell, 2003).  Regardless, there are an infinite number of locations where entities may be associ-ated for pronominal reference; as discussed below, this also means that there are a potentially infinite number of ways for some verbs to be performed: a finite fixed lexicon for ASL is not sufficient. 
66
  
While ASL verbs have a standard citation form, many can be inflected to indicate the 3D location in space at which their subject and/or object have been associated (Liddell, 2003; Neidle et al, 2000; Padden, 1988).  Linguists refer to such verbs as ?inflecting? (Padden, 1988), ?indicating? (Liddell, 2003), or ?agreeing? verbs (Cormier, 2002). We use the term ?inflecting verbs? in this paper.  When they appear in a sentence, their standard motion path may be modified such that the movement or orientation goes from the 3D location of their sub-ject and toward the 3D location of their object (or more complex effects).  The resulting performance is a synthesis of the verb?s standard lexical motion path and the 3D locations associated with the sub-ject and object. Because the verb sign indicates its subject and/or object, the names of the subject and object may not be otherwise expressed in the sen-tence.  If the signer chooses to mention them in the sentence, it is legal to use the citation-form (unin-flected) version of the verb, but the resulting sen-tences tend to appear less fluent.  In prior studies, we have found that native ASL signers who view ASL animations report that those that include spa-tially inflected verbs and entities associated with locations in space are easier to understand (than those which lack spatial pronominal reference and lack verb inflection) (Huenerfauth and Lu, 2012).  Fig. 1 shows the ASL verb EMAIL, which in-flects for its subject and object locations.  Some ASL verbs do not inflect or inflect for their ob-ject?s location only (Liddell, 2003; Padden, 1988).  There are other categories of ASL verbs (e.g., ?de-picting,? ?locative,? or ?classifier?) whose move-ments convey complex spatial information and other forms of verb inflection (e.g., for temporal aspect); these are not the focus of this paper.  
 Fig. 1. Two inflected versions of the ASL verb EMAIL: (top) subject associated with location on left and object on right, (bottom) subject on right and object on left. 
3 Related Work on Sign Animation Given how the association of entities with loca-tions in space affects how signs are performed, it is not possible to pre-store all possible combinations of all the signs the system may need.  For pointing signs, inflecting verbs, and other space-affected signs, successful ASL systems must synthesize a specific instance of the sign as needed. Few sign language animation researchers have studied spa-tial inflection of verbs. There are two major types of ASL animation research: scripting software (El-liott et al, 2008; Traxler, 2000) or generation software (e.g., Fotinea et al, 2008; Huenerfauth, 2006; Marshall and Safar, 2005; VCom3D, 2012) as surveyed previously by (Huenerfauth and Han-son, 2009). Unfortunately, current generation and scripting systems for sign language animations typically do not make extensive use of spatial loca-tions to represent entities under discussion, the output of these systems looks much like the anima-tions without space use and without verb inflection that we evaluated in (Huenerfauth and Lu, 2012). For instance, Sign Smith Studio (VCom3D, 2012), a commercially available scripting system for ASL, contains a single uninflected version of most ASL verbs in its dictionary. To produce an inflected form of a verb, a user must use an ac-companying piece of software to precisely pose a character?s hands to produce a verb sign; this sig-nificantly slows down the process of scripting an ASL animation. One British Sign Language anima-tion generator (Marshall and Safar, 2005) can as-sociate entities under discussion with a finite number of locations in space (approximately 6). Its repertoire also includes a few verbs whose sub-ject/object are positioned at these locations. How-ever, most of the verbs handled by their system involved relatively simple motion paths for the hands from subject to object locations, and the sys-tem did not allow for the arrangement of pronomi-nal reference points at arbitrary locations in space.   Toro (Toro, 2004; 2005) focused on ASL in-flected verbs; they analyzed the videos of human signers to note the 2D hand locations in the image for different verbs. Next, they wrote animation code for planning motion paths for the hands based on their observations. A limitation of this work is that asking humans to look for hand locations in a video and write down angles and coordinates is 
67
  
inexact; further, a human looked for patterns in the data ? machine learning approaches were not used.  There are some sign language animation re-searchers who have used modeling techniques ap-plied to human motion data.  Researchers studying coarticulation for French Sign Language (LSF) animations (Segouat & Braffort, 2009) digitally analyzed the movements of human signers in video and trained mathematical models of the move-ments between signs, which could be used during animation synthesis.  Because collecting data from human via video requires researchers to estimate movements from a 2D image, it is more accurate and efficient to use motion-capture sensors.  Du-arte et al collected data via motion capture in their SignCom project for LSF (Duarte and Gibet, 2011), and they reassembled elements of the recordings to synthesize novel animations.  4 Our Prior Modeling Research The goal of our research is to construct computa-tional models of ASL verbs that can automate the work of human users of scripting software or be used within generation. Given the name of the verb, the location in space associated with verb?s subject, and the location associated with the object, our software should access its parameterized lexi-con of ASL verb signs to synthesize the specific inflected form needed for a sentence.  Our tech-nique for building these parameterized lexicon en-tries for each verb is data-driven: based on samples of sign language motion from human signers. Spe-cifically, we record a list of examples of each verb for a variety of arrangements of the verb?s subject and object around the signer?s body.  Fig. 2. shows how we identified 7 locations on an arc around the signer; we then collected examples of each verb for all possible combinations of these seven locations for subject and object. Table 1 lists the ASL verbs modeled in our prior work (Huenerfauth and Lu, 2010; Lu and Huenerfauth, 2011).   
 Fig. 2. Front & top view of arc positions around the signer. 
Table 1: Five ASL Verbs We Have Modeled Verb Inflection Type Description ASK Subject & Object The signer moves an extended index finger from the ?asker? (subject) to the ?person being asked? (object).  During the movement, the finger bends into a hooked shape. (ASL ?1? to ?X? handshape.) GIVE Subject & Object In this two-handed version of the sign, the signer moves two hands as a pair from the ?giver? (subject) toward the ?recipient? (object). (Both hands have an ASL ?flat-O? handshape.) MEET Subject & Object Signer moves two index fingers towards each other (pointing upward) to ?meet? at some point in the middle. (ASL ?1? handshape.) SCOLD Object Only The signer ?wags? (bounces up and down while pointing) an extended index finger at the ?person being scolded? (object).  (ASL ?1? handshape.) TELL Object Only The signer moves an extended index finger from the mouth/chin toward the ?person being told? (object). (ASL ?1? handshape.)  For verbs inflected for both subject and object location (MEET, GIVE), our training data con-tained 42 examples for all non-reflexive combina-tions of the 7 arc positions.  For verbs inflected for object location only (TELL, SCOLD, ASK), 7 ex-amples were collected.  While we focused on these five verbs as examples, we intend for our lexicon building methodology to be generalizable to other verbs and other sign languages. In our early work (Huenerfauth and Lu, 2010), we collected samples of inflected verbs by asking a native ASL signer with animation experience to produce these verbs using the Gesture Builder sign creation software (VCom3D, 2012).  In later work, we collected more natural/accurate data by using motion-capture equipment to record a human signer per-forming a verb for various arrangements of sub-ject/object in space (Lu and Huenerfauth, 2011).   Regardless of the data source, we extracted the hand position for each keyframe for each verb.  (A keyframe is an important moment for a movement; a straight-line path can be represented merely by its beginning and end.)  Thus, for a two-handed verb (e.g., GIVE) that is inflected for both subject and object, we collected 504 location values: 42 examples x 2 keyframes x 2 hands x 3 (x, y, z) val-ues.  Next, we fit third-order polynomial models for each dimension (x, y, z) of the hand position at each keyframe ? parameterized on the arc locations of the verb?s subject and object for that instance in the training data (Huenerfauth and Lu, 2010).   At this point, we could use the model to synthe-size novel ASL verb sign instances (properly in-flected for different locations of subject and object, 
68
  
including combinations not present in the training data) by predicting the location of the hand for each of the keyframes of a verb, given the location of the verb?s subject and object on the arc.  Our animation software is keyframe based, and it uses inverse kinematics and motion interpolation to syn-thesize a full animation from a list of hand location targets for specific keyframe times during the ani-mation. Additional details appear in (Huenerfauth and Lu, 2010; Lu and Huenerfauth, 2011).   To evaluate our models in prior work, we con-ducted a variety of user-based and distance-metric-based evaluations. For instance, we showed native ASL signer participants animations of short ASL stories that contained verbs (some versions pro-duced by our model, and some produced by a hu-man animator) to measure whether the stories containing our modeled verbs were easily under-stood, as measured on comprehension questions or side-by-side subjective evaluations (Huenerfauth and Lu, 2010).  No significant differences in com-prehension or evaluation scores were observed in these prior studies, indicating that the ASL anima-tions synthesized from our model had similar qual-ity to verb signs produced by a human animator. 5 Collecting More Verb Examples In prior work, we used motion-capture data from only a single human signer performing many in-flected forms of five ASL verbs.  For this paper, we asked two additional signers to perform exam-ples of each inflected form of the five verbs.  This section summarizes the collection methodology, described in detail in (Lu and Huenerfauth, 2011).  During a videotaped 90-minute recording session, each native ASL signer wore a set of motion-capture sensors while performing a set of ASL verb signs, for various given arrangements of the subject and object in the signing space. We use an Intersense IS-900 motion capture system with an overhead ultrasonic speaker array and hand, head, and torso mounted sensors with directional micro-phones and gyroscope to record location (x, y, z) and orientation (roll, pitch, yaw) data for the hands, torso, and head of the signer during the study. We placed colored targets around the perim-eter of the laboratory at precise angles, relative to where the signer was seated, corresponding to the points on the arc in Fig. 2.  Fig. 4. shows how we set up the laboratory during the data collection 
with 10cm colored paper squares were attached to the walls; the two squares visible in Fig. 4 corre-spond to arc positions 0.9 and 0.6 in Fig. 2. These squares served as ?targets? for the signer to use as ?subject? and ?object? when performing various inflected verb forms.  
 Fig. 4. This three-quarter view illustrates the layout of the laboratory during the motion capture data collection; the signer is facing a camera (off-screen to the right). Sitting behind the camera is another signer conversing with him. Another native ASL signer sitting behind the video camera prompted the performer to produce each inflected verb form by pointing to the colored squares for the subject and the object for each of the 42 samples we wanted to record for each verb. At the beginning of the session, the signer was asked to make several large arm movements and hand claps (Fig. 5) to facilitate the later synchroni-zation of the motion capture stream with the video data and scaling the data from the recorded human to match the body size of the VCom3D avatar.  
 Fig. 5. Arm movements the signer was asked to perform to facilitate calibration of the collected motion capture data. 
  Fig. 6. The signer signed the number that corresponded to each verb example being performed (left) and a close-up view of the hand-mounted sensor used in the study (right). 
69
  
Occasionally during the recording session (and whenever the signer made a mistake and needed to repeat a sign), the signer was asked to sign the se-quence number of the verb example being recorded (Fig. 6); this facilitated later analysis of the video.  We needed to identify timecodes in the motion capture data stream that correspond to the begin-ning and ending keyframes of each verb recorded. We asked a native ASL signer to view the video after the recording session to identify the time in-dex (video frame number) that corresponded to the start and end movement of each verb sign that we recorded. (If we had modeled signs with more complex motion paths, we might have needed more than two keyframes.) These time codes were used to extract hand location (x, y, z) data from the motion capture stream for each hand for each keyframe for each verb example that was recorded.  6 Modeling the Verb Path as a Vector Although experimental evaluations of verb models produced in prior work based on motion-capture data from a single human signer were positive (Lu and Huenerfauth, 2011), this may not have been a realistic test.  When constructing a large-scale sign language animation system, it may not be possible to gather all of the needed training examples for all of the verbs for a large lexicon from a single signer.  For instance, if you wish to learn performances of a verb from examples of the inflected form of that verb that happen to appear in a corpus, then you would likely need to mix data recorded from mul-tiple signers to produce your training data set for learning the inflected verb animation model. The challenge of using data from multiple sign-ers is that an ASL verb performance consists of: (1) non-meaningful/idiosyncratic variation in how dif-ferent people perform a verb (or how one person performs a verb on different occasions) and (2) meaningful/essential aspects of how a verb should be performed (that should be rather invariant across different signers or different occasions).  We prefer a model that captures the essential na-ture of the verb but not the signer-specific elements; models attuned too much to the specifics of a sin-gle human?s performance may overfit to that one individual?s version of the verb (or that one occa-sion when the signer performed).  Further, while motion-capture data recorded from humans with different body proportions can be somewhat re-
scaled to fit the animated character?s body size to be used by the sign language animation system, no ?retargeting? algorithm is perfect. If signer-specific idiosyncrasies are captured in the verb animation model, then the variation in data sources used when building a large-scale sign language anima-tion project may be apparent in its output.  Our prior modeling technique explicitly learned the starting and ending location of the hands for each instance of a verb based on a human signer?s movements.  However, when different signers per-form a verb (e.g., GIVE with subject at arc position -0.6 and object at 0.3), they may not select exactly the same point in 3D space for their hands to start and stop.  What is common across all of the varia-tions in the performance is the overall direction that the hands move through the signing space.  We can find empirical evidence for this intuition if we compare motion-capture data of the three dif-ferent signers we recorded (section 5) performing the same ASL inflecting verbs.  When we calculate Euclidean distance between different signer?s start-ing location and their ending locations of the hands for identical verb examples, we see inter-signer variability (Fig. 7).  If we instead calculate the Eu-clidean distance between the vector (direction and magnitude) of the hand movement from the start to the ending location between signers, we see much smaller inter-signer variability (Fig. 7). Section 7 explains the scale and formula used for the dis-tance metrics in Fig. 7 and elsewhere in this paper. 
 Fig. 7.  Inter-signer variability in ASL verb signs, re-ported using a ?point? or ?vector? distance metric. 
70
  
Using these results as intuition, we present a new model of ASL inflecting verbs in this paper, based on this ?vector? approach to modeling the movement of the signer?s hands through space.  We assume that what is essential to a human?s per-formance of an inflected ASL verb is the direction that the hands travel through space, not the specific starting and ending locations in space.  Thus, we model each verb example as a tuple of values: the difference between the x-, the y-, and the z-axis values for the starting and ending location of the hand.  (The model has three parameters for a one-handed sign and six parameters for a two-handed sign.)  Using this model, we followed a similar polynomial fitting technique summarized in sec-tion 4 ? except that we are now modeling a smaller number of parameters ? our new ?vector? model uses only three values per hand (deltax, deltay, deltaz), instead of six per hand in our prior ?point? model, which represented start and end location of the hand as (xstart, ystart, zstart, xend, yend, zend).  This new model can then be used to synthesize animations of ASL verb signs for given subject and object arc positions around the signer ? the differ-ence from our prior work is that these new models only represent the movement vector for the hands, not their specific starting and ending locations.   The purpose of building a model of a verb is that we wish to use it as a parameterized lexical entry in a sign language animation synthesis sys-tem; thus, we must explain how the model can be used to synthesize a novel verb example, given its input parameters (the arc position of the subject and the object of the verb).  While our new vector model predicts the motion vector for the hands, this is not enough; we need starting and ending locations for the hands (an infinite number of which are possible for a given vector).  Thus, we need a way to select a starting location for the hands for a specific verb instance (and then based on the vector, we would know the ending location).   We observe that, for a given verb, there are some locations in the signing space that are likely for the signer?s hands to occupy and some regions that are less likely.  Some motion paths through the signing space travel through high-likelihood ?pop-ular? regions of the signing space, and some, through less likely regions.  Thus, we can build a Gaussian mixture model of the likelihood that a hand might occupy a specific location in the sign-ing space during a particular ASL verb.  For a giv-
en motion vector, one possible starting point in the signing space will lead to a path that travels through a maximally likely region of the signing space.  Thus, we can search possible starting points for the hands for a given vector and identify an optimal path for the hands given a Gaussian mix-ture model of hand location likelihood.   Fig. 8 shows a (two-dimensional) illustration of our approach for selecting a starting location for the hand when synthesizing a verb.  The concentric groups of ovals in the image represent the compo-nent Gaussians in the mixture model, which was fit on the data from the locations that one hand occu-pied during a signer?s performances of a verb.  Given the vector (direction and magnitude) for the hand?s motion path for a verb (predicted by our model), we can systematically search the signing space for all possible starting locations for the hand ? to identify the starting location that yields a path through the signing space with maximum probabil-ity (as predicted by the Gaussian model).  The ar-rows shown in Fig. 8 represent a few possible paths for the hand given several possible starting locations, and one of these arrows travels a path through the model with maximum probability. 
 Fig. 8. This 2D diagram illustrates how the starting lo-cation for the hand can be selected that yields a path through the mixture model with maximum probability.   Specifically, for each signer, for each hand, for each verb, we used the recorded motion-capture data stream between the start-times and end-times of all of the verb examples as training data, and then we fit a 3D Gaussian mixture model for each, to represent the probability that the hand would occupy each location in the signing space during that verb.  We used a model with 6 component Gaussians for modeling the signing space for each of the verbs SCOLD, GIVE, ASK, and MEET. Due to the fast movement (and thus short clips of recorded motion-capture data) for the verb TELL, we only had sufficient data to fit a 5-component 
71
  
Gaussian model for the locations of the hand dur-ing this verb (TELL is a one-handed verb).  When we need to synthesize a verb, then we use our vec-tor model to predict a movement vector for the hands, and then we perform a grid search through the signing space (in the x, y, and z dimensions) to identify an optimal starting location for the hand.  If run-time efficiency is a concern, optimization or estimation methods could be applied to this search. In summary, the vector direction and magnitude of the hands are based on a model that is parame-terized on: the verb, the location of the subject on an arc around the signer, and the location of the object on this arc.  When a specific instance of a verb must be synthesized, a starting point for the hand is selected that maximizes the probability of the entire trajectory of the hands through space, based on a Gaussian mixture model specific to that verb (but not parameterized on any specific sub-ject/object locations in space). All instances of the verb in the training data were used to train the mix-ture model, due to data sparseness considerations. 7 Distance Metric Evaluation  Because the premise of this paper is that models of ASL verbs based on a motion vector representation would do a better job of capturing the essential aspects of a verb?s motion path across signers, we conducted an inter-signer cross-validation of our new model. We built separate models on the data from each of our three signers, and then we com-pared the resulting model?s predictions for all 42 verb instances collected from the other two signers.  For comparison purposes, we also trained three models (one per signer) using the ?point?-based model from our prior work (Lu and Huenerfauth, 2011).   Fig. 9 presents the results; the values of each bar are the average ?error? for each synthe-sized verb example for all five ASL verbs in Table 1.  The error score for a verb example is the aver-age of four values: (1) Euclidean distance between the start location of the right hand as predicted by the model and the start location of the right hand of the human signer data being used for evaluation, (2) same for the end location for the right hand, (3) same for the start location for the left hand, and (4) same for end location for the left hand. Fig. 9 shows that the new ?vector? model has lower error scores than our older ?point? model presented in prior work.  To interpret the Euclidean 
distance value, it is useful to know that the scale of the coordinate space used for the verb model is set such that shoulder width of a signer would be 1.0.  As a baseline for comparison, the average inter-signer variation (based on the values shown in Fig. 7) is also plotted in Fig. 9. 
 Fig. 9. Evaluation of the ?Point? and ?Vector? models for all five ASL verbs listed in Table 1. Next, we wanted to compare the two models under two assumptions: (1) it may not be possible to gather a large number of examples of a verb from a single signer and (2) it may be necessary to mix data from multiple signers when assembling a training data set for a verb model.  For instance, these conditions would hold if a researcher were using examples of a verb performance extracted from a multi-signer corpus to assemble a training set.  Due to the limited size of most sign language corpora (and the many possible combinations of subject and object position in the signing space), a training set gathered in this manner would likely contain a relatively small number of training ex-amples ? possibly gathered from multiple signers. To test the models under these conditions, we assembled three training data sets ? using the data from our three recorded signers.  Each data set in-cluded 22 examples of the performance of an ASL inflected verb for a subset of the various possible combinations of subject and object locations in the signing space ? with half of the examples from one signer and half from another.  After training a model on each data set, then the model was evalu-ated against the 42 examples of each verb perfor-mance recorded from the third signer (who was not part of the training data used for that model).  This process was repeated for a total of three times (for all combinations of the data from the three sign-
72
  
ers).  For comparison purposes, we also trained three models (one based on each of the three two-signer data sets) using the ?point?-based model from our prior work (Lu and Huenerfauth, 2011). Fig. 10 shows the results for two of the verbs in Table 1 (ASK and GIVE); the ?vector? model has lower error scores than our older ?point? model. 
 Fig. 10. Evaluation of the ?Point? and ?Vector? models trained on a small ?mixed? data set from two signers. Examples of animations of the ASL verbs syn-thesized using each of these models are on our lab website: http://latlab.cs.qc.cuny.edu/slpat2012/ 8 Conclusion And Future Work This paper presented and evaluated a new method of constructing a lexicon of ASL verb signs whose motion path depends on the location in the signing space associated with the verb?s subject and object.  We used motion capture data from multiple signers to evaluate whether our new models do a better job of capturing the signer-invariant and occasion-invariant aspect of an ASL inflected verb?s move-ment, compared to our prior modeling approach.  The parameterized models of ASL verb move-ments produced in this paper could be used to syn-thesize a desired verb instance for a potentially infinite number of arrangements of the subject and object of the verb in the signing space ? based on the collection of a finite number of examples of a verb performance from a human signer.  
Using this technique, generation software could include flexible lexicons that can be used to syn-thesize an infinite variety of inflecting verb in-stances, and scripting software could more easily enable users to include inflecting verbs in a sen-tence (without requiring the user to create a custom animations of a body movement for a particular inflected verb sign). While this paper demonstrates our method on five ASL verbs, this technique should be applicable to more ASL verbs, more ASL signs parameterized on spatial locations, and signs in other sign languages used internationally.  In this paper, we studied a set of ASL verbs with relatively simple motion-paths (consisting of straight line movements, which therefore only re-quired two keyframes per verb); in future work, we may analyze verbs with more complex movements of the hands.  Further, our vector models represent the magnitude (length) of the hands? motion path through space; in future work, we may explore techniques for rescaling these vector lengths.  In future work, we will also use hand orientation data from our motion capture sessions to synthesize hand orientation for sign animations.  We also plan to experiment with modeling how the timing of keyframes varies with subject/object positions.   Finally, we also plan on conducting a user-based evaluation study using animations synthe-sized by the models presented in this paper ? to determine if native ASL signers who view anima-tions containing such verbs find them to be more grammatical, understandable, and natural.   Acknowledgments This material is based upon work supported in part by the US. National Science Foundation under award number 0746556 and award number 1065009, by The City University of New York PSC-CUNY Research Award Program, by Sie-mens A&D UGS PLM Software through a Go PLM Academic Grant, and by Visage Technolo-gies AB through a free academic license for char-acter animation software. Jonathan Lamberton assisted with the recruitment of participants and the conduct of experimental sessions. Kenya Bry-ant, Wesley Clarke, Kelsey Gallagher, Amanda Krieger, Giovanni Moriarty, Aaron Pagan, Jaime Penzellna, Raymond Ramirez, and Meredith Turtletaub have also assisted with data collection and contributed their ASL expertise to the project. 
73
  
References  Cormier, K. 2002. Grammaticalization of Indexic Signs: How American Sign Language Expresses Numerosi-ty. Ph.D. Dissertation, University of Texas at Austin. Cox, S., M. Lincoln, J. Tryggvason, M. Nakisa, M. Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to aid communication with deaf people. In Proceedings of Assets '02, 205-212.  Duarte, K., and Gibet, S. Presentation of the SignCom Project. In Proceedings of the First International Workshop on Sign Language Translation and Avatar Technology, Berlin, Germany, 10-11 Jan 2011. Elliott, R., Glauert, J., Kennaway, J., Marshall, I., Safar, E. 2008. Linguistic modeling and language-processing technologies for avatar-based sign lan-guage presentation. Univ Access Inf Soc 6(4), 375-391. Berlin: Springer. Fotinea, S.E., Efthimiou, E., Caridakis, G., Karpouzis K. 2008. A knowledge-based sign synthesis architec-ture. Univ Access Inf Soc 6(4):405-418. Berlin: Springer. Huenerfauth, M. 2006. Generating American Sign Lan-guage classifier predicates for English-to-ASL ma-chine translation, dissertation, U. of Pennsylvania. Huenerfauth, M., Hanson, V. 2009. Sign language in the interface: access for deaf signers. In C. Stephanidis (ed.), Universal Access Handbook. NJ: Erlbaum. 38.1-38.18. Huenerfauth, M., Zhao, L., Gu, E., Allbeck, J.  2008. Evaluation of American sign language generation by native ASL signers. ACM Trans Access Comput 1(1):1-27. Huenerfauth, M., Lu, P. 2010. Annotating spatial refer-ence in a motion-capture corpus of American Sign Language discourse. In Proc. LREC 2010 workshop on representation & processing of sign languages. Huenerfauth, M., Lu, P. 2010. Modeling and synthesiz-ing spatially inflected verbs for American sign lan-guage animations. In Proceedings of the 12th international ACM SIGACCESS conference on Computers and accessibility (ASSETS '10). ACM, New York, NY, USA, 99-106. Huenerfauth, M, P. Lu. (2012. in press). Effect of spa-tial reference and verb inflection on the usability of American sign language animation. In Univ Access Inf Soc. Berlin: Springer. Klima, E., U. Bellugi. 1979. The Signs of Language. Harvard University Press, Cambridge, MA. Liddell, S. 2003. Grammar, Gesture, and Meaning in American Sign Language. UK: Cambridge U. Press. Lillo-Martin, D. 1991. Universal Grammar and Ameri-can Sign Language: Setting the Null Argument Pa-rameters. Kluwer Academic Publishers, Dordrecht. Lu, P., Huenerfauth, M. 2011. Synthesizing American Sign Language Spatially Inflected Verbs from Mo-
tion-Capture Data. Second International Workshop on Sign Language Translation and Avatar Technolo-gy (SLTAT), in conjunction with ASSETS 2011, Dundee, Scotland. Marshall, I., E. Safar. 2005. Grammar development for sign language avatar-based synthesis. In Proc. UAHCI?05. McBurney, S.L. 2002. Pronominal reference in signed and spoken language. In R.P. Meier, K. Cormier, D. Quinto-Pozos (eds.) Modality and Structure in Signed and Spoken Languages. UK: Cambridge U. Press, 329-369. Meier, R. 1990. Person deixis in American sign lan-guage. In S. Fischer, P. Siple (eds.) Theoretical issues in sign language research. Chicago: University of Chicago Press, 175-190. Mitchell, R., Young, T., Bachleda, B., & Karchmer, M. 2006. How many people use ASL in the United States? Why estimates need updating. Sign Lang Studies, 6(3):306-335. Neidle, C., D. Kegl, D. MacLaughlin, B. Bahan, R.G. Lee. 2000. The syntax of ASL: functional categories and hierarchical structure. Cambridge: MIT Press. Padden, C. 1988. Interaction of morphology & syntax in American Sign Language. New York: Garland Press. Segouat, J., A. Braffort. 2009. Toward the study of sign language coarticulation: methodology proposal. In Proc. Advances in Computer-Human Interactions, 369-374. Toro, J. 2004. Automated 3D animation system to in-flect agreement verbs. Proc. 6th High Desert Linguis-tics Conf. Toro, J. 2005. Automatic verb agreement in computer synthesized depictions of American Sign Language. Ph.D. dissertation, Depaul University, Chicago, IL. Traxler, C. 2000. The Stanford achievement test, 9th edition: national norming and performance standards for deaf & hard-of-hearing students. J Deaf Stud & Deaf Educ 5(4):337-348. VCom3D. 2012. Homepage. http://www.vcom3d.com/ Zhao, L., Kipper, K., Schuler, W., Vogler, C., Badler, N., Palmer, M. 2000. A machine translation system from English to American Sign Language. In Proc. AMTA?00, pp. 293-300. 
74
