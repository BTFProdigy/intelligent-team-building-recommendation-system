Annotation of Multiword Expressions in the Prague Dependency Treebank
Eduard Bejc?ek, Pavel Stran??k and Pavel Schlesinger
Institute of Formal and Applied Linguistics
Charles University, Prague, Czech Republic
{bejcek,stranak,schlesinger}@ufal.mff.cuni.cz
Abstract
In this article we want to demonstrate that
annotation of multiword expressions in the
Prague Dependency Treebank is a well de-
fined task, that it is useful as well as feasible,
and that we can achieve good consistency of
such annotations in terms of inter-annotator
agreement. We show a way to measure agree-
ment for this type of annotation. We also ar-
gue that some automatic pre-annotation is
possible and it does not damage the results.
1 Motivation
Various projects involving lexico-semantic annota-
tion have been ongoing for many years. Among those
there are the projects of word sense annotation, usu-
ally for creating training data for word sense disam-
biguation. However majority of these projects have
only annotated very limited number of word senses
(cf. Kilgarriff (1998)). Even among those that aim
towards ?all words? word-sense annotation, multi-
word expressions (MWE) are not annotated adequa-
tely (see (Mihalcea, 1998) or (Hajic? et al, 2004)),
because for their successful annotation a method-
ology allowing identification of new MWEs during
annotation is required. Existing dictionaries that in-
clude MWEs concentrate only on the most frequent
ones, but we argue that there are many more MWEs
that can only be identified (and added to the dictio-
nary) by annotation.
There are various projects for identification of na-
med entities (for an overview see (?evc??kov? et al,
2007)). We explain below (mainly in Section 2) why
we consider named entities to be concerned with lex-
ical meaning. At this place we just wish to recall that
these projects only select some specific parts of text
and provide information only for these. They do not
aim for full lexico-semantic annotation of texts.
There is also another group of projects that have to
tackle the problem of lexical meaning, namely tree-
banking projects that aim to develop a deeper layer
of annotation in adition to a surface syntactic layer.
This deeper layer is generally agreed to concern lex-
ical meaning. Therefore the units of this layer cannot
be words anymore, they should be lexias.
Lexia is defined by Filipec and C?erm?k (1986)
as equivalent to a ?monosemic lexeme? of (Filipec,
1994) or a ?lexical unit? of (Cruse, 1986): ?a pair
of a single sense and a basic form (plus its derived
forms) with relatively stable semantic properties?.
We work with the Prague Dependency Treebank
(PDT, see Hajic? (2005)), which has in addition to
the morphemic and the surface syntactic layers also
the tectogrammatical layer. The latter has been con-
strued as the layer of the (literal) meaning of the sen-
tence and thus should be composed of lexias (lexical
units) and the relations between their occurrences.1
On the tectogrammatical layer only the autose-
mantic words form nodes in a tree (t-nodes). Synse-
mantic (function) words are represented by various
attributes of t-nodes. Each t-node has a lemma: an at-
tribute whose value is the node?s basic lexical form.
Currently t-nodes, and consequently their t-lemmas,
are still visibly derived from the morphological di-
vision of text into tokens. This preliminary handling
1With a few exceptions, such as personal pronouns (that co-
refer to other lexias) or coordination heads.
793
has always been considered unsatisfactory in FGD.2
There is a clear goal to distinguish t-lemmas through
their senses, but this process has not been completed
so far.
Our project aims at improving the current state
of t-lemmas. Our goal is to assign each t-node a
t-lemma that would correspond to a lexia, i.e. that
would really distinguish the t-node?s lexical mean-
ings. To achieve this goal, in the first phase of the
project, which we report on in this paper, we iden-
tify multiword expressions and create a lexicon of
the corresponding lexias.
2 Introduction
We annotate all occurrences of MWEs (including
named entities, see below) in PDT 2.0. When we
speak of multiword expressions we mean ?idiosyn-
cratic interpretations that cross word boundaries?
(Sag et al, 2002). We understand multiword expres-
sions as a type of lexias. We distinguish also a spe-
cial type of MWEs, for which we are mainly inter-
ested in its type, rather than individual lexias, during
the annotation: named entities (NE).3 Treatment of
NEs together with other MWEs is important, be-
cause syntactic functions are more or less arbitrary
inside a NE (consider an address with phone num-
bers, etc.) and so is the assignment of semantic roles.
That is why we need each NE to be combined into a
single node, just like we do it with MWEs in general.
For the purpose of annotation we have built a repos-
itory of lexias corresponding to MWEs, which we
call SemLex. We have built it using entries from
some existing dictionaries and it is being enriched
during the annotation in order to contain every lexia
that was annotated. We explain this in detail in Sec-
tion 4.1.
3 Current state of MWEs in PDT 2.0
During the annotation of valency that is a part of
the tectogrammatical layer of PDT 2.0 the t-lemmas
2Functional Generative Description (FGD, (Sgall et al,
1986; Hajic?ov? et al, 1998)) is a framework for system-
atic description of a language, that the PDT project is based
upon. In FGD units of the t-layer are construed equivalently to
monosemic lexemes (lexias) and are combined into dependency
trees, based on syntactic valency of the lexias.
3NEs can in general be also single-word, but in this phase of
our project we are only interested in multiword expressions, so
when we say NE in this paper, we always mean multiword.
that correspond to lexias have been basically iden-
tified for all the verbs and some nouns and adjec-
tives. The resulting valency lexicon is called PDT-
VALLEX (Hajic? et al, 2003) and we can see it as
a repository of lexias based on verbs, adjectives and
nouns in PDT that have valency. 4
This is a starting point for having t-nodes corre-
sponding to lexias. However in the current state it is
not fully sufficient even for verbs, mainly because
parts of MWEs are not joined into one node. Parts
of frames marked as idiomatic are still represented
by separate t-nodes in a tectogrammatical tree. Ver-
bal phrasemes are also split into 2 nodes, where the
nominal part is governed by the verb. Non-verbal id-
ioms have not been annotated at all.
Below we give an example of the current state:
an idiom meaning ?in a blink (of an eye)? ? literally
?*what not-see? (Figure 1).
Figure 1: ?Co nevide?t? (in a blink)
4 Methodology
4.1 Building SemLex
Each entry we add into SemLex is considered to be
a lexia. We have also added 9 special entries to iden-
tify NE types, so we do not need to add the expres-
sions themselves. These types are derived from NE
classification by (?evc??kov? et al, 2007). Some fre-
quent names of persons, institutions or other objects
(e.g. film titles) are being added into SemLex dur-
ing annotation (while keeping the information about
a NE type), because this allows for their following
occurrences to be pre-annotated automatically (see
Section 5). For others, like addresses or bibliographic
4It is so because in PDT-VALLEX valency is not the only
criterion for distinguishing frames (=meanings). Two words
with the same morphological lemma and valency frame are as-
signed two different frames if their meaning differs. Thus the
PDT-VALLEX frames correspond to lexias.
794
entries, it makes but little sense, because they most
probably will not reappear during the annotation.
Currently (for the first stage of lexico-semantic
annotation of PDT) SemLex contains only lexias cor-
responding to MWEs. Its base has been composed of
MWEs extracted from Czech WordNet (Smr?, 2003),
Eurovoc (Eurovoc, 2007) and SC?FI (C?erm?k et al,
1994).5 Currently there are over 30,000 multi-word
lexias in SemLex and more are being added during
annotations.
The entries added by annotators must be lexias as
defined above. Annotators define their ?sense? infor-
mally (as much as possible) and we extract an exam-
ple of usage and the basic form from the annotation
automatically. The ?sense? information shall be re-
vised by a lexicographer, based on annotated occur-
rences.
4.2 Annotation
PDT 2.0 uses PML (Pajas and ?te?p?nek, 2005),
which is an application of XML that utilises a stand-
off annotation scheme. We have extended the PDT-
PML with a new schema for so-called s-files. We
use these files to store all of our annotation without
altering the PDT itself. These s-files are very sim-
ple: basically each of them consists of a list of s-
nodes. Each s-node corresponds to an occurrence of
a MWE and it is composed of a link to the entry in
SemLex and a list of identifiers of t-nodes that cor-
respond to this s-node.
Our annotation program reads in a tectogrammati-
cal representation (t-file) and calls TrEd (Pajas, 2007)
to generate plain text. This plain text (still linked to
the tectogrammatical representation) is presented to
the annotator. While the annotator marks MWEs al-
ready present in SemLex or adds new MWEs into
SemLex, tree representations of these MWEs extrac-
ted from underlying t-trees are added into their Sem-
Lex entries via TrEd scripts.
5 Pre-annotation
Because MWEs tend to occur repeatedly in a text,
we have decided to test pre-annotation both for the
speed improvement and for improving the consis-
tency of annotations. On the assumption that all oc-
5Slovn?k c?esk? frazeologie a idiomatiky (Dictionary of
Czech Phraseology and Idiomatics)
currences of a MWE share the same tree structure,
while there are no restrictions on the surface word
order other than those imposed by the tree structure
itself we have decided to employ four types of pre-
annotation:
A) External pre-annotation provided by our col-
league (see Hn?tkov? (2002)). With each MWE a
set of rules is associated that limits possible forms
and surface word order of parts of a MWE. This ap-
proach was devised for corpora that are not syntac-
tically annotated.
B) Our one-time pre-annotation with those lexias
from SemLex that were already used in annotation,
and thus have a tree structure as a part of their entry.
C) Dynamic pre-annotation as in B, only with the
SemLex entries that have been recently added by the
annotator.
D) When an annotator tags an occurrence of a
MWE in the text, other occurrences of this MWE
in the article are identified automatically.6
(A) was executed once for all of the PDT. (B) is
performed each time we merge lexias added by an-
notators into the main SemLex. We carry out this
annotation in one batch for all PDT files remaining
to annotate. (C) should be done for each file while
it is being opened in LexemAnn GUI. (D) happens
each time the annotator adds a new lexia into Sem-
Lex and uses it to annotate an occurrence in the text.
In subsequent files instances of this lexia are already
annotated in step (C), and later even in (B).
After the pilot annotation without pre-annotation
(D) we have compared instances of the same tags
and found that 10.5% of repeated lexias happened
to have two different trees. After closer examination
this 10.5% group is negligible because these cases
are caused by ellipses, variations in lexical form such
as diminutives etc., or wrong lemmatisation, rather
than inconsistencies in the tree structure. These cases
show us some issues of PDT 2.0, for instance:
? ji?n? ? Ji?n? Korea [southern ? South Korea] ?
wrong lemmatisation
6This is exactly what happens: 1) Tree structure of the se-
lected MWE is identified via TrEd 2) The tree structure is added
to the lexeme?s entry in SemLex 3) All the sentences in the
given file are searched for the same MWE using its tree structure
(via TrEd) 4) Other occurrences returned by TrEd are tagged
with this MWE?s ID, but these occurrences receive an attribute
?auto?, which identifies them (both in the s-files and visually in
the annotation tool) as annotated automatically.
795
? obchodn? r?editel ? r?editelka [managing direc-
tor ? man ? woman] ? in future these should
have one t-lemma and gender should be speci-
fied by an attribute of a t-node.
We have not found any case that would show that
there is such a MWE that its structure cannot be rep-
resented by a single tectogrammatical tree. 1.1% of
all occurences were not connected graphs, but this
happened due to errors in data and to coordination.
This corroborates our assumption that (disregarding
errors) all occurrences of a MWE share the same
tree structure. As a result, we started storing the tree
structures in the SemLex entries and employ them in
pre-annotation (D). This also allows us to use pre-
annotations (B) and (C), but we have decided not
to use them at the moment, in order to be able to
evaluate each pre-annotation step separately. Thus
the following section reports on the experiments that
employ pre-annotation (A) and (D).
6 Analysis of Annotations
Two annotators already started to use (and test) the
tool we have developed. They both have got the same
texts. The text is generated from the t-trees and pre-
sented as a plain text with pre-annotated words mark-
ed by colour labels. Annotators add their tags in the
form of different colour labels and they can delete
the pre-annotated tags. In this experiment data con-
sists of approx. 120,000 tokens that correspond to
100,000 t-nodes. Both annotators have marked about
15,200 t-nodes (~15%) as parts of MWEs. annotator
A has grouped them into 7,263 MWEs and annota-
tor B into 6,888. So the average length of a MWE is
2.2 t-nodes.
The ratio of general named entities versus Sem-
Lex lexias was 52:48 for annotator A and 49:51 in
case of annotator B. Annotator B used 10% more
lexias than annotatorA (3,279 and 3,677), while they
both used almost the same number of NEs. Some
comparison is in the Table 1.
type of MWE A B
SemLex lexias 3,677 3,279
Named Entities 3,553 3,587
- person/animal 1130 1137
- institution 842 772
Table 1: Annotated instances of significant types of
MWEs
Both annotators also needed to add missing en-
tries to the originally compiled SemLex or to edit
existing entries. annotatorA added 722 entries while
the annotator B added 861. They modified 796 and
809 existing entries, respectively.
6.1 Inter-anntator Agreement
In this section our primary goal is to assess whether
with our current methodology we produce reliable
annotation of MWEs. To that end we measure the
amount of inter-annotator agreement that is above
chance. There are, however, a few sources of com-
plications in measuring this agreement:
? Each tag of a MWE identifies a subtree of a tec-
togrammatical tree (represented on the surface by a
set of marked words). This allows for partial agree-
ment of tags at the beginning, at the end, but also in
the middle of a surface interval (in a sentence).
? A disagreement of the annotators on the tag is
still an agreement on the fact that this t-node is a part
of a MWE and thus should be tagged. This means we
have to allow for partial agreement on a tag.
? There is not any clear upper bound as to how
many (and how long) MWEs are there in texts.
? There is not a clear and simple way to esti-
mate the amount of the agreement by chance, be-
cause it must include the partial agreements men-
tioned above.
Since we want to keep our agreement calculation
as simple as possible but we also need to take into
account the problems above, we have decided to start
from pi as defined in (Artstein and Poesio, 2007) and
to make a few adjustments to allow for types of par-
tial agreement and estimated maximal agreement.
Because we do not know how many MWEs there
are in our texts, we need to calculate the agreement
over all t-nodes, rather than the t-nodes that ?should
be annotated?. This also means, that the theoretical
maximal agreement (upper bound) U , cannot be 1.
If it was 1, it would be saying that all nodes are part
of a MWE.
Since we know that U < 1 but we do not know
it?s exact value, we use the estimated upper bound
U? (see Equation 1). Because we calculate U? over all
t-nodes, we need to account not only for agreement
on tagging a t-node, but also for agreement, that the
t-node is not a part of a MWE, therefore it is not
796
tagged.7
If N is the number of all t-nodes in our data and
nA?B is the number of t-nodes annotated by at least
one annotator, then we estimate U? as follows:
U? =
nA?B
N
+ 0.052 ?
N ? nA?B
N
= 0.215 (1)
The weight 0.052 used for scoring the t-nodes that
were not annotated is explained below. Because U?
includes all the disagreements of the annotators, we
believe that the real upper bound U lies somewhat
below it and the agreement value 0.215 is not some-
thing that should (or could) be achieved. This is how-
ever based on the assumption that the data we have
not yet seen have similar ratio of MWEs as the data
we have used.
To account for partial agreement we divide the t-
nodes into 5 classes c and assign each class a weight
w as follows:
c1 If the annotators agree on the exact tag from Sem-
Lex, we get maximum information: w = 1
c2 If they agree, that the t-node is a part of a NE or
they agree it is a part of some lexia from Sem-
Lex, but they do not agree which NE or which
lexia, we estimate we get about a half of the in-
formation compared to c1: w = 0.5
c3 If they agree that the t-node is a part of a MWE,
but disagree whether a NE or a lexia from Sem-
Lex, it is again half the information compared to
c2, so w = 0.25
c4 If they agree that the t-node is not a part of a
MWE, w = 0.052. This low value of w accounts
for frequency of t-nodes that are not a part of a
MWE, as estimated from data: Agreement on not
annotating provides the same amount of infor-
mation as agreement on annotating, but we have
to take into account higher frequency of t-nodes
that are not annotated:
c4 = c3 ?
P
annotated
P
not annotated
= 0.25 ?
12797
61433
? 0.052
c5 If the annotators do not agree whether to anno-
tate a t-node or not, w = 0.
The number of t-nodes (n) and weights w per class
c are given in Table 2.
7If we did not do this, there would be no difference between
t-nodes, that were not tagged (annotators agreed they are not a
part of a MWE) and the t-nodes that one annotator tagged and
the other did not (i.e. they disagreed).
Agreement Disagreement
Agreement on annotation Not annotation
Agreement on NE / lexia
Full agreement
class c 1 2 3 4 5
t-nodes n 10,527 2,365 389 83,287 3,988
weight w 1 0.5 0.25 0.052 0
Table 2: The agreement per class and the associated
weights
Now that we have estimated the upper bound of
agreement U? and the weights w for all t-nodes we
can calculate our weighted version of pi:
piw =
Ao ?Ae
U? ?Ae
Ao is the observed agreement of annotators and
Ae is the agreement expected by chance (which is
similar to a baseline). piw is thus a simple ratio of our
observed agreement above chance and maximum a-
greement above chance.
Weights w come into account in calculation ofAo
and Ae.
We calculate Ao by multiplying the number of t-
nodes in each category c by that category?s weight
w, summing these 5 weighted sums and dividing this
sum of all the observed agreement in the data by
the total number of t-nodes: Ao = 1N
?5
c=1 ncwc =
0.160.
Ae is the probability of agreement expected by
chance over all t-nodes. This means it is the sum of
the weighted probabilities of all the combinations of
all the tags that can be obtained by a pair of annota-
tors. Every possible combination of tags (including
not tagging a t-node) falls into one of the categories
c and thus gets the appropriate weight w. Calculat-
ing the value of Ae depends not only on values of
w (see Table 2), but also on the fact that SemLex is
composed of 9 entries for NE types and over 30,000
entries for individual lexias. Based on this we have
obtained Ae = 0.047.
The resulting piw is then
piw =
Ao ?Ae
U? ?Ae
=
0.160? 0.047
0.215? 0.047
= 0.6760
When we analyse the cases of disagreement and
partial agreement we find that most of it has to do
with SemLex lexias rather than NEs. This is mostly
due to imperfectness of the dictionary and its size
(annotators could not explore each of almost 30,000
797
of SemLex entries). Our current methodology, which
relies too much on searching the SemLex, is also to
blame. This should, however, improve by employing
pre-annotation (B) and (C).
One more reason for disagreement consists in the
fact that there are cases, for which non-trivial knowl-
edge of the world is needed: ?Jang Di Pertuan Agong
Sultan Azlan ??h, the sultan of the state of Perak,
[ . . . ] flew back to Perak.? Is ?Sultan Azlan ??h? still
a part of the name or is it (or a part of it) a title?
The last important reason of disagreement is sim-
ple: both annotators identify the same part of text
as MWE instances, but while searching the SemLex
they choose different lexias as the tags. This can be
rectified by:
? Removing duplicate entries from SemLex (cur-
rently there are many close identical entries orig-
inating from Eurovoc and Czech WordNet).
? Imploring improved pre-annotation B and C, as
mentioned above.
7 Conclusion
We have annotated multi-word lexias and named en-
tities in a part of PDT 2.0. We use tectogrammati-
cal tree structures of MWEs for the automatic pre-
annotation. In the analysis of inter-annotator agree-
ment we show that a weighted measure that accounts
for partial agreement as well as the estimation of
maximal agreement is needed.
The resulting piw = 0.6760 is statistically sig-
nificant and should gradually improve as we clean
up the annotation lexicon, more entries can be pre-
annotated automatically, and further types of pre-
annotation are employed.
8 Acknowledgement
This work has been supported by grants 1ET2011205-
05 of Grant Agency of the Academy of Science of
the Czech Republic, projects MSM0021620838 and
LC536 of the Ministry of Education and 201/05/H014
of the Czech Science Foundation.
References
Ron Artstein and Massimo Poesio. 2007. Inter-coder agree-
ment for computational linguistics. Submitted to Computa-
tional Linguistics.
F. C?erm?k, V. C?erven?, M. Churav?, and J. Machac?. 1994.
Slovn?k c?esk? frazeologie a idiomatiky. Academia.
D.A. Cruse. 1986. Lexical Semantics. Cambridge University
Press.
Eurovoc. 2007. http://europa.eu/eurovoc/.
Josef Filipec and Franti?ek C?erm?k. 1986. C?esk? lexikologie.
Academia.
Josef Filipec. 1994. Lexicology and lexicography: Develop-
ment and state of the research. In P. A. Luelsdorff, editor,
The Prague School of Structural and Functional Linguistics,
pages 163?183, Amsterdam/Philadelphia. J. Benjamins.
Jan Hajic?, Jarmila Panevov?, Zden?ka Ure?ov?, Alevtina B?-
mov?, Veronika Kol?r?ov?, and Petr Pajas. 2003. PDT-
VALLEX. In Joakim Nivre and Erhard Hinrichs, editors,
Proceedings of The Second Workshop on Treebanks and Lin-
guistic Theories, volume 9 of Mathematical Modeling in
Physics, Engineering and Cognitive Sciences, pages 57?68,
Vaxjo, Sweden. Vaxjo University Press.
Jan Hajic?, Martin Holub, Marie Huc??nov?, Martin Pavl?k, Pavel
Pecina, Pavel Stran??k, and Pavel Martin ?id?k. 2004.
Validating and improving the Czech WordNet via lexico-
semantic annotation of the Prague Dependency Treebank. In
LREC 2004, Lisbon.
Jan Hajic?, 2005. Insight into Slovak and Czech Corpus Lin-
guistics, chapter Complex Corpus Annotation: The Prague
Dependency Treebank, pages 54?73. Veda Bratislava, Slo-
vakia.
Eva Hajic?ov?, Barbara H. Partee, and Petr Sgall. 1998. Topic-
focus articulation, tripartite structures, and semantic con-
tent, volume 71 of Studies in Linguistics and Philosophy.
Kluwer, Dordrecht.
Milena Hn?tkov?. 2002. Znac?kov?n? fraz?mu? a idiomu? v
C?esk?m n?rodn?m korpusu s pomoc? Slovn?ku c?esk? fraze-
ologie a idiomatiky. Slovo a slovesnost.
A. Kilgarriff. 1998. Senseval: An exercise in evaluating word
sense disambiguation programs. In Proc. LREC, pages 581?
588, Granada.
Rada Mihalcea. 1998. Semcor semantically tagged corpus.
Petr Pajas and Jan ?te?p?nek. 2005. A Generic XML-Based For-
mat for Structured Linguistic Annotation and Its Application
to Prague DependencyTreebank 2.0. Technical Report TR-
2005-29, ?FAL MFF UK, Prague, Czech Rep.
Petr Pajas. 2007. TrEd. http://ufal.mff.cuni.cz/?pajas/
tred/index.html.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain
in the neck for nlp. In Third International Conference, CI-
CLing.
Magda ?evc??kov?, Zdene?k ?abokrtsk?, and Oldr?ich Kru?za.
2007. Zpracov?n? pojmenovan?ch entit v c?esk?ch textech
(treatment of named entities in czech texts). Technical Re-
port TR-2007-36, ?FAL MFF UK, Prague, Czech Republic.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?. 1986. The
Meaning of the Sentence in Its Semantic and Pragmatic As-
pects. Academia/Reidel Publ. Comp., Praha/Dordrecht.
Pavel Smr?. 2003. Quality control for wordnet development.
In Petr Sojka, Karel Pala, Pavel Smr?, Christiane Fellbaum,
and Piek Vossen, editors, Proceedings of the Second Inter-
national WordNet Conference?GWC 2004, pages 206?212.
Masaryk University Brno, Czech Republic.
798
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 106?115,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Syntactic Identification of Occurrences of Multiword Expressions in Text
using a Lexicon with Dependency Structures
Eduard Bejc?ek, Pavel Stran?a?k, Pavel Pecina
Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, 118 00 Praha 1, Czechia
{bejcek,stranak,pecina}@ufal.mff.cuni.cz
Abstract
We deal with syntactic identification of oc-
currences of multiword expression (MWE)
from an existing dictionary in a text corpus.
The MWEs we identify can be of arbitrary
length and can be interrupted in the surface
sentence. We analyse and compare three ap-
proaches based on linguistic analysis at a vary-
ing level, ranging from surface word order to
deep syntax. The evaluation is conducted us-
ing two corpora: the Prague Dependency Tree-
bank and Czech National Corpus. We use the
dictionary of multiword expressions SemLex,
that was compiled by annotating the Prague
Dependency Treebank and includes deep syn-
tactic dependency trees of all MWEs.
1 Introduction
Multiword expressions (MWEs) exist on the inter-
face of syntax, semantics, and lexicon, yet they are
almost completely absent from major syntactic the-
ories and semantic formalisms. They also have inter-
esting morphological properties and for all these rea-
sons, they are important, but challenging for Natural
Language Processing (NLP). Recent advances show
that taking MWEs into account can improve NLP
tasks such as dependency parsing (Nivre and Nils-
son, 2004; Eryig?it et al, 2011), constituency parsing
(Arun and Keller, 2005), text generation (Hogan et
al., 2007), or machine translation (Carpuat and Diab,
2010).
The Prague Dependency Treebank (PDT) of
Czech and the associated lexicon of MWEs Sem-
Lex1 offer a unique opportunity for experimentation
1http://ufal.mff.cuni.cz/lexemann/mwe/semlex.zip
with MWEs. In this paper, we focus on identifica-
tion of their syntactic structures in the treebank us-
ing various levels of linguistic analysis and match-
ing algorithms.2 We compare approaches operating
on manually and automatically annotated data with
various depth of annotation from two sources: the
Prague Dependency Treebank and Czech National
Corpus (CNC).
The remainder of the paper is organised as fol-
lows. Section 2 describes the state of the art of in
acquisition and identification of MWEs. Section 3
explains what we consider a MWE. In Section 4
we describe the data used for our experiments. Sec-
tion 5 gives the details of our experiments, and in
Section 6 we analyse and discuss the results. Con-
clusions from the analysis are drawn in Section 7.
2 Processing of Multiword Expressions
and Related Work
Automatic processing of multiword expressions in-
cludes two distinct (but interlinked) tasks. Most of
the effort has been put into acquisition of MWEs
appearing in a particular text corpus into a lexi-
con of MWEs (types) not necessarily linked with
their occurrences (instances) in the text. The best-
performing methods are usually based on lexical as-
sociation measures that exploit statistical evidence
of word occurrences and co-occurrences acquired
from a corpus to determine degree of lexical asso-
ciation between words (Pecina, 2005). Expressions
that consist of words with high association are then
2We do not aim at disambiguating the occurrences as figura-
tive or literal. We have not observed enough literal uses to sub-
stantiate working on this step. There are bigger improvements
to be gained from better identification of syntactic occurrences.
106
denoted as MWEs. Most of the current approaches
are limited to bigrams despite the fact that higher-
order MWEs are quite common.
The task of identification of MWE occurrences
expects a list of MWEs as the input and identifies
their occurrences (instances) in a corpus. This may
seem to be a trivial problem. However, the complex
nature of this phenomenon gives rise to problems on
all linguistic levels of analysis: morphology, syntax,
and semantics.
In morphologically complex languages, a single
MWE can appear in a number of morphological
variants, which differ in forms of their individual
components; and at the same time, a sequence of
words whose base forms match with base forms
of components of a given MWE do not neces-
sarily represent an instance of this MWE (Praco-
val dnem i noc?? / He?s been working day and night
vs. Ti dva byli jako den a noc / Those two were as
day and night).
MWEs differ in the level of syntactic fixedness.
On the one hand, certain MWEs can be modified
by inserting words in between their components
or by changing word order. Such expressions can
only be identified by matching their syntactic struc-
tures, but only if a reliable syntactic information is
available in both the lexion and text (Po pr?evratu
padaly hlavy / After the coup, heads were rolling
vs. Hlavy zkorumpovany?ch na?me?stku? budou padat
jedna za druhou / One head of a corrupt deputy
will be rolling after the other). On the other hand,
some MWEs can appear only as fixed expressions
with no modifications allowed. In that case, the syn-
tactic matching approach can miss-indicate their in-
stances because of an inserted word or altered word
order (Vys?s??? spolec?nost / High society vs. *Vys?s??? bo-
hats??? spolec?nost / High rich society).
From the semantic point of view, MWEs are of-
ten characterized by more or less non-compositional
(figurative) meaning. Their components, however,
can also occur with the same syntax but composi-
tional (literal) semantics, and therefore not acting
as MWEs (Jedinou branku dal az? v posledn?? minute?
za?pasu / He scored his only goal in the last minute of
the match. vs. Rozhodc??? dal branku zpe?t na sve? m??sto
/ The referee put a goal back to its place).
Automatic discrimination between figurative and
literal meaning is a challenging task similar to
word sense disambiguation which has been stud-
ied extensively: Katz and Giesbrecht (2006), Cook
et al (2007), Hashimoto and Kawahara (2008), Li
and Sporleder (2009), and Fothergill and Baldwin
(2011). Seretan (2010) includes MWE identification
(based on a lexicon) in a syntactic parser and reports
an improvement of parsing quality. As a by-product,
the parser identified occurrences of MWEs from a
lexicon. Similarly, Green et al (2013) embed identi-
fication of some MWEs in a Tree Substitution Gram-
mar and achieve improvement both in parsing qual-
ity and MWE identification effectiveness. None of
these works, however, attempt to identify all MWEs,
regardless their length or complexity, which is the
main goal of this paper.
3 Definition of Multiword Expressions
We can use the rough definition of MWEs put for-
ward by Sag et al (2002): ?idiosyncratic interpreta-
tions that cross word boundaries (or spaces)?. We
can also start from their ? or Bauer?s (1983) ? ba-
sic classification of MWEs as lexicalised or insti-
tutionalised phrases, where lexicalised phrases in-
clude some syntactic, semantic or lexical (i.e. word
form) element, that is idiosyncratic. Institutionalised
phrases are syntactically and semantically compo-
sitional, but still require a particular lexical choice,
e.g. disallowing synonyms (mobile phone, but not
*movable phone).
We need to make just one small adjustment to the
above: ?phrase? above must be understood as a sub-
tree, i.e. it can have holes in the surface sentence, but
not in terms of a dependency tree.
In reality there is no clear boundary, in particu-
lar between the institutional phrases and other collo-
cations. Like many other traditional linguistic cate-
gories, cf. Manning (2003), this phenomenon seems
to be more continuous than categorial.
For the purpose of this paper, however, it is not
important at all. We simply try to find all instances
of the expressions (subtrees) from a lexicon in a text,
whatever form the expression may take in a sen-
tence.
4 Data
In this work we use two datasets: Czech National
Corpus (CNC), version SYN2006-PUB, and the
107
Prague Dependency Treebank (PDT), version 2.5.
We run and compare results of our experiments on
both manual annotation of PDT, and automatic anal-
ysis of both PDT and CNC (see Section 5.3). We
also make use of SemLex, a lexicon of MWEs in
the PDT featuring their dependency structures that
is described in Section 4.3.
4.1 Corpora ? Czech National Corpus and
Prague Dependency Treebank
CNC is a large3 corpus of Czech. Its released ver-
sions are automatically segmented and they contain
automatic morphological tagging (Hajic?, 2004).
PDT (Bejc?ek et al, 2011) is a smaller news-
domain corpus based on a subset of the news section
of CNC. It contains approx. 0.8 million words that
have three layers of annotation: morphological, ana-
lytical (surface syntax), and tectogrammatical (deep
syntax).
Annotation of a sentence on the morphological
layer consists of attaching morphological lemma
and tag to the tokens. A sentence at the analytical
layer is represented as a rooted ordered tree with la-
belled nodes. The dependency relation between two
nodes is captured by an edge with a functional label.
On the tectogrammatical layer only content words
form nodes in a tree (t-nodes).4 Auxiliary words are
represented by various attributes of t-nodes, as they
do not have their own lexical meaning, but rather
modify the meaning of the content words. Each t-
node has a t-lemma: an attribute whose value is the
node?s basic lexical form, and a dependency func-
tion that relates it to its parent. Figure 1 shows the
relations between the neighbouring layers of PDT.
4.2 MWE in Prague Dependency Treebank 2.5
In the Functional Generative Description (Sgall et
al., 1986, FGD)5 the tectogrammatical layer is con-
strued as a layer of the linguistic meaning of text.
This meaning is composed by means of ?deep?
(tecto-grammatical) syntax from single-meaning-
carrying units: monosemic lexemes.
3It contains 200 mil. words in SYN2000, 600 mil. in
SYN2006-PUB; http://www.korpus.cz.
4with a few exceptions (personal pronouns or coord. heads)
5FGD is a framework for systematic description of a lan-
guage, that the PDT project is based upon.
Byl         by        ?el       do     lesa       .
Byl          by                   do                 lesa
?el
AuxV          AuxV                     AuxP                   
  b?t             b?t            j?t            do        les             .  VpYS---XR-AA---      Vc-------------       VpYS---XR-AA---    RR?2----------    NNIS2-----A----        Z:------------- 
Adv
Pred
AuxS
j?tPRED
#PersPronACT  lesDIR3a
tree.r
f
a/aux.
rf
a/aux
.rf
a/lex.
rf
a/aux
.rf
a/lex.
rf
t - lay
er
a - lay
e r
m - la
yer
   .AuxK
Figure 1: A visualisation of the annotation schema of
PDT. Lit.: ?[He] would have gone into forest.?
In order to better facilitate this concept of t-layer,
all multiword expressions in the release of PDT 2.5
(Bejc?ek et al, 2011) have been annotated and they
are by default displayed as single units, although
their inner structure is still retained.
A lexicon of the MWEs has been compiled. A
simple view of the result of this annotation is given
in the Figure 2. A detailed description can be found
in Bejc?ek and Stran?a?k (2010), and Stran?a?k (2010).
The MWEs in PDT 2.5 include both multiword lex-
emes (phrasemes, idioms) and named entities (NEs).
In the present work we ignore the named entities,
concentrating on the lexemes. Some NEs (names of
persons, geographical entities) share characteristics
of multiword lexemes, other NEs do not (addresses,
bibliographic information).
We build on the PDT 2.5 data and MWE lexicon
SemLex (Section 4.3) to evaluate the approach with
various automatic methods for detection of MWEs.
4.3 Lexicon of MWEs ? SemLex
SemLex is the lexicon of all the MWEs annotators
identified during the preparation of PDT 2.5 t-layer.
In the PDT 2.5 these instances of MWEs can then be
displayed as single nodes and all the MWEs them-
selves are compiled in the SemLex lexicon. The lex-
icon itself is freely available. See http://ufal.
mff.cuni.cz/lexemann/mwe/. Length (size)
108
Can word sense disambiguation help statistical machine translation?
help
disambiguation
sense
word
translation
machine
statistical
#root
help
WSD MT
#root
statistical
Word sense disambiguation
Machine translation ?
BASIC_FORM: Word sense disambiguation
TREE_STRUCT: disambiguation?sense?word
LEMMATIZED: ?
?
SemLex
Figure 2: An illustration of changes in t-trees in PDT 2.5;
every MWE forms a single node and has its lexicon entry
distribution of MWEs in PDT 2.5 is given in Table 1.
There are three attributes of SemLex entries cru-
cial for our task:
BASIC FORM ? The basic form of a MWE. In
many languages including Czech it often contains
word forms in other than the basic form for the given
word on its own. E.g. ?vysoke? uc?en??? contains a
neuter suffix of the adjective ?vysoky?? (high) be-
cause of the required agreement in gender with the
noun, whereas the traditional lemma of adjectives in
Czech is in the masculine form.
LEMMATIZED ? ?Lemmatised BASIC FORM?,
i.e. take the basic form of an entry and substitute
each form with its morphological lemma. This at-
tribute is used for the identification of MWEs on the
morphological layer. For more details see Section 5.
TREE STRUCT (TS) ? A simplified tectogram-
matical dependency tree structure of an entry. Each
node in this tree structure has only two attributes: its
tectogrammatical lemma, and a reference to its ef-
fective parent.
4.4 Enhancing SemLex for the Experiments
SemLex contains all the information we use for the
identification of MWEs on t-layer.6 It also contains
basic information we use for MWE identification on
m-layer: the basic form and the lemmatized form of
each entry. For the experiments with MWE iden-
tification on analytical (surface syntactic) layer we
6Automatic identification of MWES was, after all, one of
the reasons for its construction.
a) len types instances
2 7063 18914
3 1260 2449
4 305 448
5 100 141
6 42 42
7 16 15
8 4 5
9 4 3
11 1 0
12 2 2
b) len types instances
18 148 534
2 7444 19490
3 843 1407
4 162 244
5 34 32
6 13 8
7 3 1
8 4 1
9 1 1
10 0 0
Table 1: Distribution of MWE length in terms of words (a)
and t-nodes (b) in SemLex (types) and PDT (instances).
need to add some information about the surface syn-
tactic structures of MWEs. Given the annotated oc-
currences of MWEs in the t-layer and links from
t-layer to a-layer, the extraction is straightforward.
Since one tectogrammatical TS can correspond to
several analytical TSs that contain auxiliaries and
use morphological lemmas, we add a list of a-layer
TSs with their frequency in data to each SemLex en-
try (MWE). In reality the difference between t-layer
and a-layer is unfortunately not as big as one could
expect. Lemmas of t-nodes still often include even
minute morphological variants, which goes against
the vision of tectogrammatics, as described in Sgall
et al (1986).7 Our methods would benefit from more
unified t-lemmas, see also Section 6.2.
5 Methodology of Experiments
SemLex ? with its almost 8,000 types of MWEs and
their 22,000 instances identified in PDT ? allows us
to measure accuracy of MWE identification on vari-
ous layers, since it is linked with the different layers
of PDT 2.5. In this section, we present the method
for identification of MWEs on t-layer in compari-
son with identification on a-layer and m-layer. The
7These variants are unified in FGD theory, but time consum-
ing to annotate in practice. Therefore, this aspect was left out
from the current version of PDT.
8Indeed, there are expressions that are multiword, but
?single-node?. E.g.: the preposition in bez va?ha?n?? (without hes-
itation) does not have its own node on t-layer; the phrase na
spra?vnou m??ru (lit.: into correct scale) is already annotated as
one phrasal node in PDT with the lemma ?na spra?vnou m??ru?;
the verbal expression ume?t si pr?edstavit (can imagine) has again
only one node for reflexive verb ?pr?edstavit si? plus an attribute
for the ability (representing ?ume?t? as explained in Section 4.1).
109
idea of using tectogrammatical TS for identification
is that with a proper tectogrammatical layer (as it
is proposed in FGD, i.e. with correct lemmatisation,
added nodes in place of ellipses, etc.), this approach
should have the highest Precision.
Our approach to identification of MWEs in this
work is purely syntactic. We simply try to find
MWEs from a lexicon in any form they may take
(including partial ellipses in coordination, etc.). We
do not try to exploit semantics, instead we want to
put a solid baseline for future work which may do
so, as mentioned in Section 2.
5.1 MWE Identification on t-layer
We assume that each occurrence of a given MWE
has the same t-lemmas and the same t-layer struc-
ture anywhere in the text. During the manual con-
struction of SemLex, these tectogrammatical ?tree
structures? (TSs) were extracted from PDT 2.5 and
inserted into the lexicon. In general this approach
works fine and for majority of MWEs only one TS
was obtained. For the MWEs with more than one TS
in data we used the most frequent one. These cases
are due to some problems of t-layer, not deficiencies
of the theoretical approach. See section 6.2 for the
discussion of the problems.
These TSs are taken one by one and we try to find
them in the tectogrammatical structures of the input
sentences. Input files are processed in parallel. The
criteria for matching are so far only t-lemmas and
topology of the subtree.9 Comparison of tree struc-
tures is done from the deepest node and we consider
only perfect matches of structure and t-lemmata.
5.2 MWE Identification on a-layer and m-layer
We use identification of MWE occurrences on a-
layer and m-layer mainly for comparison with our
approach based on the t-layer.
9It is not sufficient, though. Auxiliary words that are ig-
nored on t-layer are occasionally necessary for distinguishing
MWE from similar group of nodes. (E.g. ?v tomto sme?ru? (?in
this regard?) is an MWE whereas ?o tomto sme?ru? (?about
this direction?) is not.) There are also attributes in t-layer that
are?although rarely?important for distinguishing the mean-
ing. (E.g. words typeset in bold in ?Leonardo dal svy?m go?lem
signa?l.? (?Leonardo signalled by his goal.?) compose exactly
the same structure as in ?Leonardo dal go?l.? (?Leonardo scored
a goal.?). I.e., the dependency relation is ?dal governs go?l? in
both cases. The difference is in the dependency function of go?l:
it is either MEANS or DIRECT OBJECT (CPHR).)
We enhance SemLex with a-tree structures as ex-
plained in Section 4.4, and then a-layer is processed
in the same manner as t-layer: analytical TS is taken
from the SemLex and the algorithm tries to match it
to all a-trees. Again, if more than one TS is offered
in lexicon, only the most frequent one is used for
searching.
MWE identification on the m-layer is based on
matching lemmas (which is the only morphological
information we use). The process is parametrised
by a width of a window which restricts the maxi-
mum distance (in a sentence) of MWE components
to span (irrespective of their order) measured in the
surface word order. However, in the setting which
does not miss any MWE in a sentence (100% Re-
call), this parameter is set to the whole sentence and
the maximum distance is not restricted at all.
The algorithm processes each sentence at a time,
and tries to find all lemmas the MWE consists of,
running in a cycle over all MWEs in SemLex. This
method naturally over-generates ? it correctly finds
all MWEs that have all their words present in the sur-
face sentence with correct lemmatisation (high Re-
call), but it also marks words as parts of some MWE
even if they appear at the opposite ends of the sen-
tence by complete coincidence (false positives, low
Precision).
In other experiments, the window width varies
from two to ten and MWE is searched for within a
limited context.
5.3 Automatic Analysis of Data Sets
The three MWE identification methods are applied
on three corpora:
? manually annotated PDT: This is the same
data, from which the lexicon was created. Results
evaluated on the same data can be seen only as num-
bers representing the maximum that can be obtained.
? automatically annotated PDT: These are the
same texts (PDT), but their analysis (morphological,
analytical as well as tectogrammatical) started from
scratch. Results can be still biased ? first, there are
no new lexemes that did not appear during annota-
tion (that is as if we had a complete lexicon); second,
it should be evaluated only on eval part of the data ?
see discussion in Section 6.1.
? automatically annotated CNC: Automatic
analysis from scratch on different sentences. The
110
layer/span PDT/man PDT/auto CNC/auto
tecto 61.99 / 95.95 / 75.32 63.40 / 86.32 / 73.11 44.44 / 58.00 / 50.33
analytical 66.11 / 88.67 / 75.75 66.09 / 81.96 / 73.18 45.22 / 60.00 / 51.58
morpho / 2 67.76 / 79.96 / 73.36 67.77 / 79.26 / 73.07 51.85 / 56.00 / 53.85
3 62.65 / 90.50 / 74.05 62.73 / 89.80 / 73.86 46.99 / 60.00 / 52.70
4 58.84 / 92.03 / 71.78 58.97 / 91.29 / 71.65 42.83 / 61.33 / 50.48
5 56.46 / 92.94 / 70.25 56.59 / 92.16 / 70.12 40.09 / 61.33 / 48.49
6 54.40 / 93.29 / 68.81 54.64 / 92.51 / 68.70 38.27 / 61.33 / 47.13
7 52.85 / 93.42 / 67.51 53.01 / 92.64 / 67.43 36.99 / 61.33 / 46.15
8 51.39 / 93.46 / 66.32 51.57 / 92.68 / 66.27 35.59 / 61.33 / 45.04
9 50.00 / 93.46 / 65.15 50.18 / 92.68 / 65.11 34.67 / 61.33 / 44.30
10 48.57 / 93.46 / 63.92 48.71 / 92.68 / 63.86 33.84 / 61.33 / 43.64
? 35.12 / 93.51 / 51.06 35.16 / 92.72 / 50.99 22.70 / 62.00 / 33.24
P / R / F P / R / F P / R / F
Table 2: Evaluation of all our experiments in terms of Precision (P), Recall (R) and F1 score (F) in percent. Experiments
on the m-layer are shown for different widths of window (see Section 5.2).
disadvantage here is the absence of gold data. Man-
ual evaluation of results has to be accomplished.
For the automatic analysis we use the modular
NLP workflow system Treex (Popel and Z?abokrtsky?,
2010). Both datasets were analysed by the standard
Treex scenario ?Analysis of Czech? that includes the
following major blocks:
1) standard rule-based Treex segmentation and to-
kenisation
2) morphology (Hajic?, 2004) and Featurama tag-
ger (Spousta, 2011) trained on the train part of
the PDT
3) MST Parser with an improved set of features by
Nova?k and Z?abokrtsky? (2007)
4) and t-trees structure provided by standard rule-
based Treex block.
6 Results
Effectiveness of our methods of identification of
MWE occurrences is presented in Table 2. Numbers
are given as percentages of Precision and Recall The
first two columns show the results of the evaluation
against gold data in PDT 2.5, the third column re-
flects the manual evaluation on 546 sentences. The
results obtained for PDT (the first two columns) are
also visualised in Figure 3.
The important issue to be decided when evaluat-
ing MWE identification is whether partial match be-
tween automatic identification and gold data MWE
is to be counted. Because of cases containing el-
lipses (see Section 6.2), it can happen that longer
MWE is used for annotation of its subset in text.10
We do not want to penalise automatic identification
(either performing this behaviour or confronted with
it in the gold data), so we treated subset as a match.
Another decision is that although the MWEs can-
not be nested in gold data, we accept it for automatic
identification. Since one word can belong to several
MWEs, the Recall rises, while Precision declines.11
6.1 Discussion of Results
The automatically parsed part of the CNC consists
of 546 sentences. Thus the third column in Table 2
represents evaluation on a much smaller data set.
During manual annotation of this data carried out
by one annotator (different from those who anno-
tated PDT data, but using the same methodology and
a tool), 163 occurences of MWEs were found. Out
10Let us say, only elliptic term Ministry of Industry is seen
in the data (instead of the full name Ministry of Industry and
Trade) annotated by the full-term lexicon entry. Whenever Min-
istry of Industry and Trade is spotted in the test data, its first
part is identified. Should that be qualified as a mistake when
confronted with the gold annotation of the whole term? The as-
signed lexicon entry is the same ? only the extent is different.
11For example, annotator had to choose only one MWE to an-
notate in vla?dn?? na?vrh za?kona o dani z pr???jmu (lit.: government
proposal of the Law on Income Tax), while it is allowed to auto-
matically identify vla?dn?? na?vrh za?kona, za?kon o dani and dan? z
pr???jmu together with the whole phrase. Recall for this example
is 1, whereas Precision is 0.25.
111
 Str?nka 1
78 80 82 84 86 88 90 92 94 96 98
30
35
40
45
50
55
60
65
70
m-layer a-layer t-layer
Recall
Pre
cis
ion
 
Str?nka 1
78 80 82 84 86 88 90 92 94 96 98
30
35
40
45
50
55
60
65
70
m-layer a-layer t-layer
Recall
Pre
cis
ion
Figure 3: Precision?Recall scores of identification of MWE structures on manually/automatically annotated PDT.
of them, 46 MWEs were out-of-vocabulary expres-
sions: they could not be found by automatic prece-
dure using the original SemLex lexicon.
Note that results obtained using automatically
parsed PDT are very close to those for manual data
on all layers (see Table 2). The reasons need to be
analysed in more detail. Our hypotheses are:
? M-layer identification reaches the same results
on both data. It is caused by the fact that the ac-
curacy of morphological tagging is comparable to
manual morphological annotation: 95.68% (Spous-
tova?, 2008).
? Both a- and t-parsers have problems mostly in
complex constructions such as coordinations, that
very rarely appear inside MWEs.
There are generally two issues that hurt our accu-
racy and that we want to improve to get better re-
sults. First, better data can help. Second, the method
can always be improved. In our case, all data are
annotated?we do nothing on plain text?and it can
be expected that with a better parser, but also possi-
bly a better manual annotation we can do better, too.
The room for improvement is bigger as we go deeper
into the syntax: data are not perfect on the a-layer
(both automatically parsed and gold data) and on
the significantly more complex t-layer it gets even
worse. By contrast, the complexity of methods and
therefore possible improvements go in the opposite
direction. The complexity of tectogrammatic anno-
tation results in a tree with rich, complex attributes
of t-nodes, but simple topology and generalised lem-
mas. Since we only use tree topology and lemmas,
the t-layer method can be really simple. It is slightly
more complex on the a-layer (with auxiliary nodes,
for example); and finally on the m-layer there is vir-
tually unlimited space for experiments and a lot of
literature on that problem. As we can see, these two
issues (improving data and improving the method)
complement each other with changing ratio on indi-
vidual layers.
It is not quite clear from Table 2 that MWE iden-
tification should be done on the t-layer, because it is
currently far from our ideal. It is also not clear that it
should be done on the m-layer, because it seems that
the syntax is necessary for this task.
6.2 Error Analysis and Possible Improvements
There are several reasons, why the t-layer results are
not clearly better:
1. our representation of tree structures proved a
bit too simple,
2. there are some deficiencies in the current t-
layer parser, and
3. t-layer in PDT has some limitations relative to
the ideal tectogrammatical layer.
Ad 1. We thought the current SemLex implemen-
tation of simple tree structures would be sufficient
for our purpose, but it is clear now that it is too
simple and results in ambiguities. At least auxiliary
words and some further syntactico-semantic infor-
mation (such as tectogrammatical functions) should
be added to all nodes in these TSs.
Ad 2. Current tectogrammatical parser does not
do several things we would like to use. E.g. it cannot
112
properly generate t-nodes for elided parts of coordi-
nated MWEs that we need in order to have the same
TS of all MWE occurrences (see below).
Ad 3. The total of 771 out of 8,816 SemLex en-
tries, i.e. 8.75%, have been used with more than one
tectogrammatical tree structure in the PDT 2.5. That
argues against our hypothesis (stated in Section 5.1)
and cause false negatives in the output, since we cur-
rently search for only one TS. In this part we analyze
two of the most important sources of these inconsis-
tent t-trees and possible improvements:
? Gender opposites, diminutives and lemma vari-
ations. These are currently represented by variations
of t-lemma. We believe that they should rather be
represented by attributes of t-nodes that could be
roughly equivalent to some of the lexical functions
in the Meaning-text theory (see Mel?c?uk (1996)).
This should be tackled in some future version of
PDT. Once resolved it would allow us to identify
following (and many similar) cases automatically.
? obchodn?? r?editel vs. obchodn?? r?editelka
(lit.: managing director-man vs. managing
director-woman)
? rodinny? du?m vs. rodinny? domek
(lit.: family house vs. family little-house; but
the diminutive domek does not indicate that the
house is small)
? obc?ansky? za?kon vs. obc?ansky? za?kon??k
(lit.: citizen law vs. citizen law-codex, meaning
the same thing in modern Czech)
These cases were annotated as instances of the same
MWE, with a vision of future t-lemmas disregard-
ing this variation. Until that happens, however, we
cannot identify the MWEs with these variations au-
tomatically using the most frequent TS only.
? Elided parts of MWEs in coordinations. Al-
though t-layer contains many newly established t-
nodes in place of elided words, not all t-nodes
needed for easy MWE annotation were there. This
decision resulted in the situation, when some MWEs
in coordinations cannot be correctly annotated, esp.
in case of coordination of several multiword lexemes
like inz?eny?rska?, monta?z?n?? a stavebn?? spolec?nost (en-
gineering, assembling and building company), there
is only one t-node for company. Thus the MWE
inz?eny?rska? spolec?nost / engineering company is not
in PDT 2.5 data and cannot be found by the t-layer
identification method. It can, however, be found by
the m-layer surface method, provided the window is
large enough and MWEs can overlap.
7 Conclusions
Identification of occurrences of multiword expres-
sions in text has not been extensively studied yet
although it is very important for a lot of NLP ap-
plications. Our lexicon SemLex is a unique resource
with almost 9 thousand MWEs, each of them with
a tree-structure extracted from data. We use this re-
source to evaluate methods for automatic identifica-
tion of MWE occurrences in text based on matching
syntactic tree structures (tectogrammatical ? deep-
syntactic, and analytical ? surface-syntactic trees)
and sequences of lemmas in the surface sentence.
The theoretically ideal approach based on tec-
togrammatical layer turned out not to perform bet-
ter, mainly due to the imperfectness of the t-layer
implemented in PDT and also due to the low ac-
curacy of automatic tectogrammatical parser. It still
shows very high Recall, as expected ? due to sim-
ple topology of the trees ? however Precision is not
ideal. Morphology-based MWE identification guar-
antees high Recall (especially when no limits are put
on the MWE component distance) but Precision of
this approach is rather low. On the other hand, if the
maximum distance is set to 4?5 words we get a very
interesting trade-off between Precision and Recall.
Using analytical layer (and thus introducing surface
syntax to the solution) might be a good approach for
many applications, too. It provides high Precision as
well as reasonable Recall.
Acknowledgements
This research was supported by the Czech Sci-
ence Foundation (grant n. P103/12/G084 and
P406/2010/0875). This work has been using lan-
guage resources developed and/or stored and/or dis-
tributed by the LINDAT-Clarin project of the Min-
istry of Education of the Czech Republic (project
LM2010013). We want to thank to our colleagues
Michal Nova?k, Martin Popel and Ondr?ej Dus?ek for
providing the automatic annotation of the PDT and
CNC data.
113
References
Abhishek Arun and Frank Keller. 2005. Lexicaliza-
tion in crosslinguistic probabilistic parsing: The case
of French. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 306?313, Ann Arbor, Michigan.
Laurie Bauer. 1983. English Word-formation. Cam-
bridge Textbooks in Linguistics. Cambridge Univer-
sity Press.
Eduard Bejc?ek and Pavel Stran?a?k. 2010. Annotation of
multiword expressions in the Prague dependency tree-
bank. Language Resources and Evaluation, (44):7?
21.
Eduard Bejc?ek, Jarmila Panevova?, Jan Popelka, Lenka
Smejkalova?, Pavel Stran?a?k, Magda S?evc???kova?, Jan
S?te?pa?nek, Josef Toman, Zdene?k Z?abokrtsky?, and
Jan Hajic?. 2011. Prague dependency tree-
bank 2.5. http://hdl.handle.net/11858/
00-097C-0000-0006-DB11-8. Data.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in statis-
tical machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 242?245, Strouds-
burg, PA, USA.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: exploiting syntactic forms
for the automatic identification of idiomatic expres-
sions in context. In Proceedings of the Workshop on a
Broader Perspective on Multiword Expressions, MWE
?07, pages 41?48.
Gu?ls?en Eryig?it, Tugay I?lbay, and Ozan Arkan Can. 2011.
Multiword expressions in statistical dependency pars-
ing. In Proceedings of the Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
SPMRL ?11, pages 45?55, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Fothergill and Timothy Baldwin. 2011. Flesh-
ing it out: A supervised approach to MWE-token and
MWE-type classification. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 911?919, Chiang Mai, Thailand.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum.
Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 992?1001.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units in
history-based probabilistic generation. In EMNLP-
CoNLL, pages 267?276. ACL.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19.
Linlin Li and Caroline Sporleder. 2009. Classifier com-
bination for contextual idiom detection without la-
belled data. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 315?323.
Christopher D. Manning, 2003. Probabilistic Linguistics,
chapter Probabilistic Syntax, pages 289?341. MIT
Press, Cambridge, MA.
Igor Mel?c?uk. 1996. Lexical functions: A tool for the
description of lexical relations in a lexicon. In Leo
Wanner, editor, Lexical Functions in Lexicography and
Natural Language Processing, volume 31 of Studies
in Language Companion Series, pages 37?102. John
Benjamins.
Joachim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Dias, G., Lopes, J. G. P. and
Vintar, S. (eds.) MEMURA 2004 - Methodologies and
Evaluation of Multiword Units in Real-World Applica-
tions, Workshop at LREC 2004, pages 39?46, Lisbon,
Portugal.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Fea-
ture engineering in maximum spanning tree depen-
dency parser. In Va?clav Matous?ek and Pavel Mautner,
editors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th International Conference on Text,
Speech and Dialogue, volume 4629 of Lecture Notes
in Computer Science, pages 92?98, Berlin / Heidel-
berg. Springer.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL Student Research Workshop, pages 13?18, Ann
Arbor, Michigan.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natural
Language Processing (IceTAL 2010), volume 6233 of
LNCS, pages 293?304, Berlin / Heidelberg. Iceland
Centre for Language Technology (ICLT), Springer.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
114
expressions: A pain in the neck for NLP. In Com-
putational Linguistics and Intelligent Text Process-
ing: Third International Conference, CICLing, vol-
ume 2276/2002 of Lecture Notes in Computer Science.
Springer Berlin / Heidelberg.
Violeta Seretan. 2010. Syntax-Based Collocation Ex-
traction, volume 44 of Text, Speech and Language
Technology. Springer.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. Academia/Reidel Publ. Comp.,
Praha/Dordrecht.
Miroslav Spousta. 2011. Featurama. http://
sourceforge.net/projects/featurama/.
Software.
Drahom??ra ?johanka? Spoustova?. 2008. Combining sta-
tistical and rule-based approaches to morphological
tagging of Czech texts. The Prague Bulletin of Math-
ematical Linguistics, 89:23?40.
Pavel Stran?a?k. 2010. Annotation of Multiword Expres-
sions in The Prague Dependency Treebank. Ph.D. the-
sis, Charles University in Prague.
115
