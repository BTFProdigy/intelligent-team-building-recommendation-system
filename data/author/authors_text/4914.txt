Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 590?599, Prague, June 2007. c?2007 Association for Computational Linguistics
Experimental Evaluation of LTAG-based Features
for Semantic Role Labeling
Yudong Liu and Anoop Sarkar
Simon Fraser University
Burnaby, BC, Canada
{yudongl,anoop}@cs.sfu.ca
Abstract
This paper proposes the use of Lexical-
ized Tree-Adjoining Grammar (LTAG) for-
malism as an important additional source
of features for the Semantic Role Labeling
(SRL) task. Using a set of one-vs-all Sup-
port Vector Machines (SVMs), we evalu-
ate these LTAG-based features. Our exper-
iments show that LTAG-based features can
improve SRL accuracy significantly. When
compared with the best known set of fea-
tures that are used in state of the art SRL sys-
tems we obtain an improvement in F-score
from 82.34% to 85.25%.
1 Introduction
Semantic Role Labeling (SRL) aims to identify and
label all the arguments for each predicate occurring
in a sentence. It involves identifying constituents in
the sentence that represent the predicate?s arguments
and assigning pre-specified semantic roles to them.
[A0seller Ports of Call Inc.] reached agreements to
[Vverb sell] [A1thing its remaining seven aircraft]
[A2buyer to buyers that weren?t disclosed] .
is an example of SRL annotation from the PropBank
corpus (Palmer et al, 2005), where the subscripted
information maps the semantic roles A0, A1, A2
to arguments for the predicate sell as defined in the
PropBank Frame Scheme. For SRL, high accuracy
has been achieved by:
(i) proposing new types of features (see Table 1 in
Section 3 for previously proposed features),
(ii) modeling the predicate frameset by capturing de-
pendencies between arguments (Gildea and Juraf-
sky, 2002; Pradhan et al, 2004; Toutanova et al,
2005; Punyakanok et al, 2005a),
(iii) dealing with incorrect parser output by using
more than one parser (Pradhan et al, 2005b).
Our work in this paper falls into category (i). We
propose several novel features based on Lexicalized
Tree Adjoining Grammar (LTAG) derivation trees
in order to improve SRL performance. To show
the usefulness of these features, we provide an ex-
perimental study comparing LTAG-based features
with the standard set of features and kernel meth-
ods used in state-of-the-art SRL systems. The LTAG
formalism provides an extended domain of locality
in which to specify predicate-argument relationships
and also provides the notion of a derivation tree.
These two properties of LTAG make it well suited
to address the SRL task.
SRL feature extraction has relied on various syn-
tactic representations of input sentences, such as
syntactic chunks (Hacioglu et al, 2004) and full
syntactic parses (Gildea and Jurafsky, 2002). In
contrast with features from shallow parsing, previ-
ous work (Gildea and Palmer, 2002; Punyakanok et
al., 2005b) has shown the necessity of full syntactic
parsing for SRL. In order to generalize the path fea-
ture (see Table 1 in Section 3) which is probably the
most salient (while being the most data sparse) fea-
ture for SRL, previous work has extracted features
from other syntactic representations, such as CCG
derivations (Gildea and Hockenmaier, 2003) and de-
pendency trees (Hacioglu, 2004) or integrated fea-
tures from different parsers (Pradhan et al, 2005b).
To avoid explicit feature engineering on trees, (Mos-
chitti, 2004) used convolution kernels on selective
portions of syntactic trees. In this paper, we also
compare our work with tree kernel based methods.
Most SRL systems exploit syntactic trees as the
main source of features. We would like to take this
one step further and show that using LTAG deriva-
590
SNP VP
MD(will) VP
V(join) PP
S
VP
V(join)
VP
MD(will) VP?
S
VP
MD(will)
VP
V(join)?1(join)
NP ?1(will) PP
?2(will)
NP ?3(join)
PP
?1: ?1:
?2: ?3:
?1: ?2:
Figure 1: A parse tree schematic, and two plausible
LTAG derivation trees for it: derivation tree ?1 uses
elementary trees ?1 and ?1 while ?2 uses ?2 and ?3.
tion trees as an additional source of features can im-
prove both argument identification and classification
accuracy in SRL.
2 Using LTAG-based Features in SRL
We assume some familiarity with Lexicalized Tree-
Adjoining Grammar (LTAG); (Joshi and Schabes,
1997) is a good introduction to this formalism. A
LTAG is defined to be a set of lexicalized elementary
trees (etree for short), of which there are two types,
initial trees and auxiliary trees. Typically etrees
can be composed through two operations into parse
trees, substitution and adjunction. We use sister ad-
junction which is commonly used in LTAG statisti-
cal parsers to deal with the relatively flat Penn Tree-
bank trees (Chiang, 2000). The tree produced by
composing the etrees is the derived/parse tree and
the tree that records the history of composition is the
derivation tree.
A reasonable way to define SRL features is to pro-
vide a strictly local dependency (i.e. within a sin-
gle etree) between predicate and argument. There
have been many different proposals on how to main-
tain syntactic locality (Xia, 1999; Chen and Vijay-
Shanker, 2000) and SRL locality (Chen and Ram-
bow, 2003; Shen and Joshi, 2005) when extract-
ing LTAG etrees from a Treebank. These proposed
methods are exemplified by the derivation tree ?1 in
Fig. 1. However, in most cases they can only provide
a local dependency between predicate and argument
for 87% of the argument constituents (Chen and
Rambow, 2003), which is too low to provide high
SRL accuracy. In LTAG-based statistical parsers,
high accuracy is obtained by using the Magerman-
Collins head-percolation rules in order to provide
the etrees (Chiang, 2000). This method is exem-
plified by the derivation tree ?2 in Fig. 1. Compar-
ing ?1 with ?2 in Fig. 1 and assuming that join is
the predicate and the NP is the potential argument,
the path feature as defined over the LTAG deriva-
tion tree ?2 is more useful for the SRL task as it dis-
tinguishes between main clause and non-finite em-
bedded clause predicates. This alternative derivation
tree also exploits the so-called extended domain of
locality (Joshi and Schabes, 1997) (the examples in
Section 2.1 show this clearly). In this paper, we cru-
cially rely on features defined on LTAG derivation
trees of the latter kind. We use polynomial kernels
to create combinations of features defined on LTAG
derivation trees.
2.1 LTAG-based Feature Extraction
In order to create training data for the LTAG-based
features, we convert the Penn Treebank phrase struc-
ture trees into LTAG derivations. First, we prune the
Treebank parse tree using certain constraints. Then
we decompose the pruned parse trees into a set of
LTAG elementary trees and obtain a derivation tree.
For each constituent in question, we extract features
from the LTAG derivation tree. We combine these
features with the standard features used for SRL
and train an SVM classifier on the combined LTAG
derivation plus SRL annotations from the PropBank
corpus.
For the test data, we report on results using the
gold-standard Treebank data, and in addition we also
report results on automatically parsed data using the
Charniak parser (Charniak, 2000) as provided by the
CoNLL 2005 shared task. We did this for three rea-
sons: (i) our results are directly comparable to those
who have used the Charniak parses distributed with
the CoNLL 2005 data-set; (ii) we avoid the possi-
bility of a better parser identifying a larger number
of argument constituents and thus leading to bet-
ter results, which is orthogonal to the discrimina-
tive power of our proposed LTAG-based features;
and (iii) the quality of LTAG derivation trees de-
pends indirectly on the quality of head dependen-
cies recovered by the parser and it is a well-known
folklore result (see Table 3 in (McDonald et al,
591
2005)) that applying the head-percolation heuristics
on parser output produces better dependencies when
compared to dependencies directly recovered by the
parser (whether the parser is an LTAG parser or a
lexicalized PCFG parser).
2.1.1 Pruning Parse Trees
Given a parse tree, the pruning component iden-
tifies the predicate in the tree and then only admits
those nodes that are sisters to the path from the pred-
icate to the root. It is commonly used in the SRL
community (cf. (Xue and Palmer, 2004)) and our ex-
periments show that 91% of the SRL targets can be
recovered despite this aggressive pruning. We make
two enhancements to the pruned Propbank tree: we
enrich the sister nodes with head information, a part-
of-speech tag and word pair: ?t, w? and PP nodes are
expanded to include the NP complement of the PP
(including head information). The target SRL node
is still the PP. Figure 2 is a pruned parse tree for a
sentence from the PropBank.
2.1.2 Decompositions of Parse Trees
After pruning, the pruned tree is decom-
posed around the predicate using standard head-
percolation based heuristic rules1 to convert a Tree-
bank tree into an LTAG derivation tree. Figure 3
shows the resulting etrees after decomposition. Fig-
ure 4 is the derivation tree for the entire pruned tree.
Each node in this derivation tree represents an etree
in Figure 3. In our model we make an independence
assumption that each SRL is assigned to each con-
stituent independently, conditional only on the path
from the predicate etree to the argument etree in the
derivation tree. Different etree siblings in the LTAG
derivation tree do not influence each other in our cur-
rent models.
2.1.3 LTAG-based Features
We defined 5 LTAG feature categories: predicate
etree-related features (P for short), argument etree-
related features (A), subcategorization-related fea-
tures (S), topological relation-related features (R),
intermediate etree-related features (I). Since we
consider up to 6 intermediate etrees between the
predicate and the argument etree, we use I-1 to I-6
to represent these 6 intermediate trees respectively.
1using http://www.isi.edu/?chiang/software/treep/treep.html
S
NP
NNP-H
Inc.
VP-H
VBD-H
reached
NP
NNS-H
agreements
S
VP-H
TO-H
to
VP
VB-H
sell
NP
NN-H
aircraft
PP
TO-H
to
NP
NNS-H
buyers
Figure 2: The pruned tree for the sentence ?Ports of
Call Inc. reached agreements to sell its remaining
seven aircraft to buyers that weren?t disclosed.?
VP
VB
sell
NP
NN
aircraft
PP
TO
to
S
VP
TO
to
e0: e1: e2: e3:
NP
NNS
agreements
S
VP
VBD
reached
NP
NNP
Inc.
e4: e5: e6:
Figure 3: Elementary trees after decomposition of
the pruned tree.
Category P: Predicate etree & its variants Pred-
icate etree is an etree with predicate, such as e0 in
Figure 3. This new feature complements the pred-
icate feature in the standard SRL feature set. One
variant is to remove the predicate lemma. Another
variant is a combination of predicate tree w/o predi-
cate lemma&POS and voice. In addition, this variant
combined with predicate lemma comprises another
new feature. In the example, these three variants are
(VP(VB)) and (VP) active and (VP) active sell re-
spectively.
Category A: Argument etree & its variants Anal-
ogous to the predicate etree, the argument etree is an
etree with the target constituent and its head. Similar
592
e5(reached)
e6(Inc.) e4(agreements)
e3(to)
e0(sell)
e1(aircraft) e2(to)
Figure 4: LTAG derivation tree for Figure 2.
to predicate etree related features, argument etree,
argument etree with removal of head word, combi-
nation of argument etree w/o head POS&head word
and head Named Entity (NE) label (if any) are con-
sidered. For example, in Figure 3, these 3 features
for e6 are e6, (NP(NNP)) and (NP) LOC with head
word ?Inc.? having NE label ?LOC?.
Category S: Index of current argument etree in
subcat frame of predicate etree Sub-categorization
is a standard feature that denotes the immediate ex-
pansion of the predicate?s parent. For example, it
is V NP PP for predicate sell in the given sentence.
For argument etree e1 in Figure 3, the index feature
value is 1 since it is the very first element in the (or-
dered) subcat sequence.
Category R:
Relation type between argument etree & predi-
cate etree This feature is a combination of position
and modifying relation. Position is a binary valued
standard feature to describe if the argument is before
or after the predicate in a parse tree. For each argu-
ment etree and intermediate etree, we consider three
types of modifying relations they may have with the
predicate etree: modifying (value 1), modified (value
2) and neither (value 3). From Figure 4, we can see
e1 modifies e0 (predicate tree). So their modifying
relation type value is 1; Combining this value with
the position value, this feature for e1 is ?1 after?.
Attachment point of argument etree This fea-
ture describes where the argument etree is sister-
adjoined/adjoined to the predicate etree, or the other
way around. For e1 in the example, VP in the predi-
cate tree is the attachment point.
Distance This feature is the number of intermediate
etrees between argument etree and predicate etree in
the derivation tree. In Figure 4, the distance from e4
to the predicate etree is 1 since only one intermediate
etree e3 is between them.
Category I:
Intermediate etree related features Intermediate
etrees are those etrees that are located between the
predicate etree and argument etrees. The set of fea-
tures we propose for each intermediate etree is quite
similar to those for argument etrees except we do
not consider the named-entity label for head words
in this case.
Relation type of intermediate etree & predicate
etree.
Attachment point of intermediate etree.
Distance between intermediate etree and predicate
etree.
Up to 6 intermediate etrees are considered and the
category I features are extracted for each of them (if
they exist).
Each etree represents a linguistically meaningful
fragment. The features of relation type, attachment
point as well as the distance characterize the topo-
logical relations among the relevant etrees. In par-
ticular, the attachment point and distance features
can explicitly capture important information hidden
in the standard path feature. The intermediate tree
related features can give richer contextual informa-
tion between predicate tree and argument trees. We
added the subcat index feature to be complemen-
tary to the sub-cat and syntactic frame features in
the standard feature set.
3 Standard Feature Set
Our standard feature set is a combination of features
proposed by (Gildea and Jurafsky, 2002), (Surdeanu
et al, 2003; Pradhan et al, 2004; Pradhan et al,
2005b) and (Xue and Palmer, 2004). All features
listed in Table 1 are used for argument classifica-
tion in our baseline system; and features with aster-
isk are not used for argument identification2. We
compare this baseline SRL system with a system
that includes a combination of these features with
the LTAG-based features. Our baseline uses all fea-
tures that have been used in the state-of-the-art SRL
systems and as our experimental results show, these
standard features do indeed obtain state-of-the-art
2This is a standard idea in the SRL literature: removing fea-
tures more useful for classification, e.g. named entity features,
makes the classifier for identification more accurate.
593
Table 1: Standard features adopted by a typical SRL
system. Features with asterisk ? are not used for ar-
gument identification.
Basic features from (Gildea and Jurafsky, 2002)
? predicate lemma and voice
? phrase type and head word
? path from phrase to predicate 1
? position: phrase relative to predicate: before or after
? sub-cat records the immediate structure that expands from
predicate?s parent
2
Additional features proposed by (Surdeanu et al 2003;
Pradhan et al, 2004, 2005)
? predicate POS
? head word POS
? first/last word/POS
? POS of word immediately before/after phrase
? path length 1
? LCA(Lowest Common Ancestor) path from phrase to its
lowest common ancestor with predicate
? punctuation immediately before/after phrase?
? path trigrams?: up to 9 are considered
? head word named entity label such as ?PER, ORG,
LOC??
? content word named entity label for PP parent node?
Additional features proposed by (Xue and Palmer, 2004)
? predicate phrase type
? predicate head word
? voice position
? syntactic frame?
1 In Fig. 2 NNS?NP?S?VP?VB is the path from the con-
stituent NNS(agreements) to the predicate VB(sell) and the
path length is 4.
2 This feature is different from the frame feature which usu-
ally refers to all the semantic participants for the particular
predicate.
accuracy on the SRL task. We will show that adding
LTAG-based features can improve the accuracy over
this very strong baseline.
4 Experiments
4.1 Experimental Settings
Training data (PropBank Sections 2-21) and test
data (PropBank Section 23) are taken from CoNLL-
2005 shared task3 All the necessary annotation in-
formation such as predicates, parse trees as well as
Named Entity labels is part of the data. The ar-
3http://www.lsi.upc.es/?srlconll/.
gument set we consider is {A0, A1, A2, A3, A4,
AM} where AM is a generalized annotation of all
adjuncts such as AM-TMP, AM-LOC, etc., where
PropBank function tags like TMP or LOC in AM-
TMP, AM-LOC are ignored (a common setting for
SRL, see (Xue and Palmer, 2004; Moschitti, 2004)).
We chose these labels for our experiments because
they have sufficient training/test data for the per-
formance comparison and provide sufficient counts
for accurate significance testing. However, we also
provide the evaluation result on the test set for full
CoNLL-2005 task (all argument types).
We use SVM-light4 (Joachims, 1999) with a poly-
nomial kernel (degree=3) as our binary classifier for
argument classification. We applied a linear kernel
to argument identification because the training cost
of this phase is extremely computationally expen-
sive. We use 30% of the training samples to fine tune
the regularization parameter c and the loss-function
cost parameter j for both stages of argument identifi-
cation and classification. With parameter validation
experiments, we set c = 0.258 and j = 1 for the ar-
gument identification learner and c = 0.1 and j = 4
for the argument classification learner.
The classification performance is evaluated using
Precision/Recall/F-score (p/r/f) measures. We ex-
tracted all the gold labels of A0-A4 and AM with
the argument constituent index from the original test
data as the ?gold output?. When we evaluate, we
contrast the output of our system with the gold out-
put and calculate the p/r/f for each argument type.
Our evaluation criteria which is based on predict-
ing the SRL for constituents in the parse tree is based
on the evaluation used in (Toutanova et al, 2005).
However, we also predict and evaluate those Prop-
Bank arguments which do not have a corresponding
constituent in the gold parse tree or the automatic
parse tree: the missing constituent case. We also
evaluate discontinuous PropBank arguments using
the notation used in the CoNLL-2005 data-set but
we do not predict them. This is contrast with some
previous studies where the problematic cases have
been usually discarded or the largest constituents in
the parse tree that almost capture the missing con-
stituent cases are picked as being the correct answer.
Note that, in addition to the constituent based evalu-
4http://svmlight.joachims.org/
594
Gold Standard Charniak Parser
std std+ltag std std+ltag
p(%) 95.66 96.79 87.71 89.11
r(%) 94.36 94.59 84.86 85.51
f(%) 95.00 95.68 86.26 87.27?
Table 2: Argument identification results on test data
ation, in Section 4.4 we also provide the evaluation
of our model on the CoNLL-2005 data-set.
Because the main focus of this work is to evaluate
the impact of the LTAG-based features, we did not
consider the frameset or a distribution over the en-
tire argument set or apply any inference/constraints
as a post-processing stage as most current SRL sys-
tems do. We focus our experiments on showing the
value added by introducing LTAG-based features to
the SRL task over and above what is currently used
in SRL research.
4.2 Argument Identification
Table 2 shows results on argument identification (a
binary classification of constituents into argument or
non-argument). To fully evaluate the influence of the
LTAG-based features, we report the identification re-
sults on both Gold Standard parses and on Charniak
parser output (Charniak, 2000)5.
As we can see, after combing the LTAG-based
features with the standard features, F-score in-
creased from 95.00% to 95.68% with Gold-standard
parses; and from 86.26% to 87.27% with the Char-
niak parses (a larger increase). We can see LTAG-
based features help in argument identification for
both cases. This result is better than (Xue and
Palmer, 2004), and better on gold parses com-
pared to (Toutanova et al, 2005; Punyakanok et al,
2005b).
4.3 Argument Classification
Based on the identification results, argument clas-
sification will assign the semantic roles to the ar-
gument candidates. For each argument of A0-A4
and AM, a ?one-vs-all? SVM classifier is trained on
both the standard feature set (std) and the augmented
feature set (std+ltag). Table 3 shows the classifi-
cation results on the Gold-standard parses with the
5We use the parses supplied with the CoNLL-2005 shared
task for reasons of comparison.
gold argument identification; Table 4 and 5 show the
classification results on the Charniak parser with the
gold argument identification and the automatic ar-
gument identification respectively. Scores for multi-
class SRL are calculated based on the total number
of correctly predicted labels, total number of gold
labels and the number of labels in our prediction for
this argument set.
class std(p/r/f)% std+ltag(p/r/f)%
A0
96.69 96.71 96.71 96.77
96.70 96.74
A1
93.82 93.30 97.30 94.87
93.56 96.07
A2
87.05 79.98 92.43 81.42
83.37 86.58
A3
94.44 68.79 97.69 73.41
79.60 83.33
A4
96.55 82.35 94.11 78.43
88.89 85.56
AM
98.41 96.61 98.67 97.88
97.50 98.27
multi- 95.35 93.62 97.15 94.70
class 94.48 95.91
Table 3: Argument classification results on Gold-
standard parses with gold argument boundaries
4.4 Discussion
From the results shown in the tables, we can see that
by adding the LTAG-based features, the overall per-
formance of the systems is improved both for argu-
ment identification and for argument classification.
Table 3 and 4 show that with the gold argu-
ment identification, the classification for each class
in {A0, A1, A2, A3, AM} consistently benefit from
LTAG-based features. Especially for A3, LTAG-
based features lead to more than 3 percent improve-
ment. But for A4 arguments, the performance drops
3 percent in both cases. As we noticed in Table
5, which presents the argument classification results
on Charniak parser output with the automatic ar-
gument identification, the prediction accuracy for
classes A0, A1, A3, A4 and AM is improved, but
drops a little for A2.
In addition, we also evaluated our feature set
on the full CoNLL 2005 shared task. The over-
595
class std(p/r/f)% std+ltag(p/r/f)%
A0
96.04 92.92 96.07 92.92
94.46 94.47
A1
90.64 85.71 94.64 86.67
88.11 90.48
A2
84.46 75.72 89.26 75.22
79.85 81.64
A3
87.50 62.02 87.10 68.35
72.59 76.60
A4
90.00 79.12 90.54 73.62
84.21 81.21
AM
95.14 85.54 96.60 86.51
90.09 91.27
multi- 93.25 86.45 94.71 87.15
class 89.72 90.77
Table 4: Argument classification results on Charniak
parser output with gold argument boundaries
all performance using LTAG features increased from
74.41% to 75.31% in terms of F-score on the full ar-
gument set. Our accuracy is most closely compara-
ble to the 78.63% accuracy achieved on the full task
by (Pradhan et al, 2005a). However, (Pradhan et
al., 2005a) uses some additional information since it
deals with incorrect parser output by using multiple
parsers. The 79.44% accuracy obtained by the top
system in CoNLL 2005 (Punyakanok et al, 2005a)
is not directly comparable since their system used
the more accurate n-best parser output of (Charniak
and Johnson, 2005). In addition their system also
used global inference. Our focus in this paper was
to propose new LTAG features and to evaluate im-
pact of these features on the SRL task.
We also compared our proposed feature set
against predicate/argument features (PAF) proposed
by (Moschitti, 2004). We conducted an experiment
using SVM-light-TK-1.2 toolkit6. The PAF tree ker-
nel is combined with the standard feature vectors by
a linear operator. With settings of Table 5, its multi-
class performance (p/r/f)% is 83.09/80.18/81.61
with linear kernel and 85.36/81.79/83.53 with poly-
nomial kernel (degree=3) over std feature vectors.
6http://ai-nlp.info.uniroma2.it/moschitti/TK1.2-
software/Tree-Kernel.htm
class std(p/r/f)% std+ltag(p/r/f)%
A0
86.50 86.18 88.17 87.70
86.34 87.93?
A1
78.73 83.82 88.78 85.22
81.19 86.97?
A2
85.40 73.93 83.11 75.42
79.25 79.08
A3
85.71 60.76 85.71 68.35
71.11 76.06?
A4
84.52 78.02 89.47 74.72
81.15 81.43
AM
80.47 82.11 83.87 81.54
81.29 82.69?
multi- 81.79 82.90 86.04 84.47
class 82.34 85.25?
Table 5: Argument classification results on Charniak
parser output with automatic argument boundaries
4.5 Significance Testing
To assess the statistical significance of the im-
provements in accuracy we did a two-tailed sig-
nificance test on the results of both Table 2 and
5 where Charniak?s parser outputs were used.
We chose SIGF7, which is an implementation
of a computer-intensive, stratified approximate-
randomization test (Yeh, 2000). The statistical dif-
ference is assessed on SRL identification, classifica-
tion for each class (A0-A4, AM) and the full SRL
task (overall performance). In Table 2 and 5, we la-
beled numbers under std+ltag that are statistically
significantly better from those under std with aster-
isk. The significance tests show that for identifica-
tion and full SRL task, the improvements are statis-
tically significant with p value of 0.013 and 0.0001
at a confidence level of 95%. The significance test
on each class shows that the improvement by adding
LTAG-based features is statistically significant for
class A0, A1, A3 and AM. Even though in Table 5
the performance of A2 appears to be worse it is not
significantly so, and A4 is not significantly better. In
comparison, the performance of PAF did not show
significantly better than std with p value of 0.593 at
the same confidence level of 95%.
7http://www.coli.uni-saarland.de/?pado/sigf/index.html
596
full full?P full?R full?S full?A full?I std
id 90.5 90.6 90.0 90.5 90.5 90.1 89.6
A0 84.5 84.3 84.6 84.5 84.3 83.5 84.2
A1 89.8 90.1 89.4 89.3 89.6 89.3 88.9
A2 84.2 84.2 84.0 83.7 83.6 83.6 84.9
A3 76.7 80.7 75.1 76.0 75.6 76.7 78.6
A4 80.0 83.3 80.0 79.6 80.0 80.0 79.2
AM 82.8 83.3 82.9 82.8 82.6 83.1 82.4
Table 6: Impact of each LTAG feature category (P, R, S, A, I defined in Section 2.1.3) on argument classi-
fication and identification on CoNLL-2005 development set (WSJ Section 24). full denotes the full feature
set, and we use ?? to denote removal of a feature category of type ?. For example, full?P is the feature set
obtained by removing all P category features. std denotes the standard feature set.
5 Analysis of the LTAG-based features
We analyzed the drop in performance when a partic-
ular type of LTAG feature category is removed from
the full set of LTAG features (we use the broad cat-
egories P, R, S, A, I as defined in Section 2.1.3).
Table 6 shows how much performance is lost (or
gained) when a particular type of LTAG feature is
dropped from the full set.
These experiments were done on the development
set from CoNLL-2005 shared task, using the pro-
vided Charniak parses. All the SVM models were
trained using a polynomial kernel with degree 3. It
is clear that the S, A, I category features help in most
cases and P category features hurt in most cases,
including argument identification. It is also worth
noting that the R and I category features help most
for identification. This vindicates the use of LTAG
derivations as a way to generalize long paths in the
parse tree between the predicate and argument. Al-
though it seems LTAG features have negative impact
on prediction of A3 arguments on this development
set, dropping the P category features can actually
improve performance over the standard feature set.
In contrast, for the prediction of A2 arguments, none
of the LTAG feature categories seem to help.
Note that since we use a polynomial kernel in the
full set, we cannot rule out the possibility that a fea-
ture that improves performance when dropped may
still be helpful when combined in a non-linear ker-
nel with features from other categories. However,
this analysis on the development set does indicate
that overall performance may be improved by drop-
ping the P feature category. We plan to examine this
effect in future work.
6 Related Work
There has been some previous work in SRL that uses
LTAG-based decomposition of the parse tree. (Chen
and Rambow, 2003) use LTAG-based decomposi-
tion of parse trees (as is typically done for statis-
tical LTAG parsing) for SRL. Instead of extracting
a typical ?standard? path feature from the derived
tree, (Chen and Rambow, 2003) uses the path within
the elementary tree from the predicate to the con-
stituent argument. Under this frame, they only re-
cover semantic roles for those constituents that are
localized within a single etree for the predicate, ig-
noring cases that occur outside the etree. As stated
in their paper, ?as a consequence, adjunct seman-
tic roles (ARGM?s) are basically absent from our
test corpus?; and around 13% complement seman-
tic roles cannot be found in etrees in the gold parses.
In contrast, we recover all SRLs by exploiting more
general paths in the LTAG derivation tree. A simi-
lar drawback can be found in (Gildea and Hocken-
maier, 2003) where a parse tree pathwas defined in
terms of Combinatory Categorial Grammar (CCG)
types using grammatical relations between predicate
and arguments. The two relations they defined can
only capture 77% arguments in Propbank and they
had to use a standard path feature as a replacement
when the defined relations cannot be found in CCG
derivation trees. In our framework, we use interme-
diate sub-structures from LTAG derivations to cap-
ture these relations instead of bypassing this issue.
597
Compared to (Liu and Sarkar, 2006), we have
used a more sophisticated learning algorithm and a
richer set of syntactic LTAG-based features in this
task. In particular, in this paper we built a strong
baseline system using a standard set of features and
did a thorough comparison between this strong base-
line and our proposed system with LTAG-based fea-
tures. The experiments in (Liu and Sarkar, 2006)
were conducted on gold parses and it failed to show
any improvements after adding LTAG-based fea-
tures. Our experimental results show that LTAG-
based features can help improve the performance of
SRL systems. While (Liu and Sarkar, 2006) propose
some new features for SRL based on LTAG deriva-
tions, we propose several novel features and in ad-
dition they do not show that their features are useful
for SRL.
Our approach shares similar motivations with the
approach in (Shen and Joshi, 2005) which uses Prop-
Bank information to recover an LTAG treebank as if
it were hidden data underlying the Penn Treebank.
However their goal was to extract an LTAG grammar
using PropBank information from the Treebank, and
not the SRL task.
Features extracted from LTAG derivations are dif-
ferent and provide distinct information when com-
pared to predicate-argument features (PAF) or sub-
categorization features (SCF) used in (Moschitti,
2004) or even the later use of argument spanning
trees (AST) in the same framework. The adjunc-
tion operation of LTAG and the extended domain of
locality is not captured by those features as we have
explained in detail in Section 2.
7 Conclusion and Future Work
In this paper we show that LTAG-based features
improve on the best known set of features used in
current SRL prediction systems: the F-score for
argument identification increased from 86.26% to
87.27% and from 82.34% to 85.25% for the SRL
task. The analysis of the impact of each LTAG fea-
ture category shows that the intermediate etrees are
important for the improvement. In future work we
plan to explore the impact that different types of
LTAG derivation trees have on this SRL task, and ex-
plore the use of tree kernels defined over the LTAG
derivation tree. LTAG derivation tree kernels were
previously used for parse re-ranking by (Shen et al,
2003). Our work also provides motivation to do SRL
and LTAG parsing simultaneously.
Acknowledgements
This research was partially supported by NSERC,
Canada (RGPIN: 264905). We would like to thank
Aravind Joshi, Libin Shen, and the anonymous re-
viewers for their comments.
References
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
ACL-2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In EMNLP-2003.
J. Chen and K. Vijay-Shanker. 2000. Automated Extrac-
tion of TAGs from the Penn Treebank. In Proc. of the
6th International Workshop on Parsing Technologies
(IWPT-2000), Italy.
D. Chiang. 2000. Statistical parsing with an automati-
cally extracted tree adjoining grammars. In ACL-2000.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In EMNLP-2003.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
58(3):245?288.
D. Gildea and M. Palmer. 2002. The necessity of parsing
for predicate argument recognition. In ACL-2002.
K. Hacioglu, S. Pradhan, W. Ward, J. Martin, and D. Ju-
rafsky. 2004. Semantic role labeling by tagging syn-
tactic chunks. In CoNLL-2004 Shared Task.
K. Hacioglu. 2004. Semantic role labeling using depen-
dency trees. In COLING-2004.
T. Joachims. 1999. Making large-scale svm learning
practical. Advances in Kernel Methods - Support Vec-
tor Machines.
A. Joshi and Y. Schabes. 1997. Tree-adjoining gram-
mars. Handbook of Formal Languages, 3.
Y. Liu and A. Sarkar. 2006. Using LTAG-Based Features
for Semantic Role Labeling. In TAG+8-2006.
598
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large-Margin Training of Dependency Parsers. In
ACL-2005.
A. Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In ACL-2004.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2004. Shallow Semantic Parsing Using
Support Vector Machines. In HLT-NAACL-2004.
S. Pradhan, K. Hacioglu, W. Ward, J. Martin, and D. Ju-
rafsky. 2005a. Semantic role chunking combin-
ing complementary syntactic views. In CoNLL-2005
Shared Task.
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2005b. Semantic role labeling using dif-
ferent syntactic views. In ACL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005a. Gener-
alized inference with multiple semantic role labeling
systems (shared task paper). In CoNLL-2005.
V. Punyakanok, D. Roth, and W. Yih. 2005b. The neces-
sity of syntactic parsing for semantic role labeling. In
IJCAI-2005.
L. Shen and A. Joshi. 2005. Building an LTAG Tree-
bank. Technical Report Technical Report MS-CIS-05-
15,5, CIS Department, University of Pennsylvania.
L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
based features in parse reranking. In EMNLP-2003.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In ACL-2003.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005.
Joint learning improves semantic role labeling. In
ACL-2005.
F. Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of 5th Natural Lan-
guage Processing Pacific Rim Symposium (NLPRS-
99), Beijing, China.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP-2004.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In COLING-2000.
599
Proceedings of NAACL HLT 2007, Companion Volume, pages 97?100,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploiting Rich Syntactic Information for Relation
Extraction from Biomedical Articles?
Yudong Liu and Zhongmin Shi and Anoop Sarkar
School of Computing Science
Simon Fraser University
{yudongl,zshi1,anoop}@cs.sfu.ca
Abstract
This paper proposes a ternary relation
extraction method primarily based on
rich syntactic information. We identify
PROTEIN-ORGANISM-LOCATION re-
lations in the text of biomedical articles.
Different kernel functions are used with
an SVM learner to integrate two sources
of information from syntactic parse trees:
(i) a large number of syntactic features
that have been shown useful for Seman-
tic Role Labeling (SRL) and applied here
to the relation extraction task, and (ii) fea-
tures from the entire parse tree using a
tree kernel. Our experiments show that the
use of rich syntactic features significantly
outperforms shallow word-based features.
The best accuracy is obtained by combin-
ing SRL features with tree kernels.
1 Introduction
Biomedical functional relations (relations for short)
state interactions among biomedical substances. For
instance, the PROTEIN-ORGANISM-LOCATION
(POL) relation that we study in this paper provides
information about where a PROTEIN is located in
an ORGANISM, giving a valuable clue to the bi-
ological function of the PROTEIN and helping to
identify suitable drug, vaccine and diagnostic tar-
gets. Fig. 1 illustrates possible locations of proteins
in Gram+ and Gram? bacteria. Previous work in
biomedical relation extraction task (Sekimizu et al,
1998; Blaschke et al, 1999; Feldman et al, 2002)
suggested the use of predicate-argument structure by
taking verbs as the center of the relation ? in con-
trast, in this paper we directly link protein named en-
tities (NEs) to their locations; in other related work,
(Claudio et al, 2006) proposed an approach that
?This research was partially supported by NSERC, Canada.
cytoplasm cytoplasm 
Gram+ Gram- 
cytoplasmic 
membrane 
cell wall 
periplasm 
outer 
membrane secreted inner membrane 
Figure 1: Illustration of bacterial locations
solely considers the shallow semantic features ex-
tracted from sentences.
For relation extraction in the newswire domain,
syntactic features have been used in a generative
model (Miller et al, 2000) and in a discriminative
log-linear model (Kambhatla, 2004). In comparison,
we use a much larger set of syntactic features ex-
tracted from parse trees, many of which have been
shown useful in SRL task. Kernel-based methods
have also been used for relation extraction (Zelenko
et al, 2003; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005) on various syntactic represen-
tations, such as dependency trees or constituency-
based parse trees. In contrast, we explore a much
wider variety of syntactic features in this work. To
benefit from both views, a composite kernel (Zhang
et al, 2006) integrates the flat features from enti-
ties and structured features from parse trees. In our
work, we also combine a linear kernel with a tree
kernel for improved performance.
2 SRL Features for Information Extraction
Fig. 2 shows one example illustrating the ternary re-
lation we are identifying. In this example, ?Exoen-
zyme S? is a PROTEIN name, ?extracellular? a LO-
CATION name and ?Pseudomonas aeruginosa? an
ORGANISM name. Our task is to identify if there
exists a ?PROTEIN-ORGANISM-LOCATION? re-
lation among these three NEs.
To simplify the problem, we first reduce the POL
97
SPROTEIN/NP
PROTEIN/NNP
Exoenzyme
PROTEIN/NNP-H
S
VP-H
VBZ-H
is
NP
NP-H
DT
an
LOCATION/JJ
extracellular
NN-H
product
PP
IN-H
of
ORGANISM/NP
ORGANISM/FW
Pseudomonas
ORGANISM/FW-H
aeruginosa
Figure 2: An example of POL ternary relation in a parse tree
ternary relation extraction problem into two binary
relation extraction problems. Specifically, we split
the POL ternary relation into binary relations as: (1)
PO: PROTEIN and ORGANISM, and (2) PL: PRO-
TEIN and LOCATION.
The ORGANISM-LOCATION relation is ignored
because it does not consider the PROTEIN and is
less meaningful than the PO and PL relations. Based
on this simplification, and following the idea of
SRL, we take the PROTEIN name in the role of the
predicate (verb) and the ORGANISM/LOCATION
name as its argument candidates in question. Then
the problem of identifying the binary relations of PO
and PL has been reduced to the problem of argu-
ment classification problem given the predicate and
the argument candidates. The reason we pick PRO-
TEIN names as predicates is that we assume PRO-
TEIN names play a more central role in linking the
binary relations to the final ternary relations.
Compared to a corpus for the standard SRL task,
there are some differences in this task: first is the
relative position of PROTEIN names and ORGAN-
ISM/LOCATION names. Unlike the case in SRL,
where arguments locate either before or after the
predicate, in this application it is possible that one
NE is embedded in another. A second difference is
that a predicate in SRL scenario typically consists of
only one word; here a PROTEIN name can contain
up to 8 words.
We do not use PropBank data in our model at all.
All of our training data and test data is annotated by
domain expert biologists and parsed by Charniak-
Johnson?s parser (released in 2006). When there is
a misalignment between the NE and the constituent
in the parse tree, we insert a new NP parent node for
the NE.
3 System Description
Figure 3: High-level system architecture
Fig. 3 shows the system overview. The input to
our system consists of titles and abstracts that are
extracted from MEDLINE records. These extracted
sentences have been annotated with the NE infor-
mation (PROTEIN, ORGANISM and LOCATION).
The Syntactic Annotator parses the sentences and in-
serts the head information to the parse trees by using
the Magerman/Collins head percolation rules. The
main component of the system is our SRL-based
relation extraction module, where we first manu-
ally extract features along the path from the PRO-
TEIN name to the ORGANISM/LOCATION name
and then train a binary SVM classifier for the binary
relation extraction. Finally, we fuse the extracted
binary relations into a ternary relation. In contrast
with our discriminative model, a statistical parsing
based generative model (Shi et al, 2007) has been
proposed for a related task on this data set where the
NEs and their relations are extracted together and
used to identify which NEs are relevant in a particu-
lar sentence. Since our final goal is to facilitate the
biologists to generate the annotated corpus, in future
98
? each word and its Part-of-Speech (POS) tag of PRO name
? head word (hw) and its POS of PRO name
? subcategorization that records the immediate structure that
expands from PRO name. Non-PRO daughters will be elim-
inated
? POS of parent node of PRO name
? hw and its POS of the parent node of PRO name
? each word and its POS of ORG name (in the case of ?PO ?
relation extraction).
? hw and its POS of ORG name
? POS of parent node of ORG name
? hw and its POS of the parent node of ORG name
? POS of the word immediately before/after ORG name
? punctuation immediately before/after ORG name
? feature combinations: hw of PRO name hw of ORG name,
hw of PRO name POS of hw of ORG name, POS of hw of
PRO name POS of hw of ORG name
? path from PRO name to ORG name and the length of the
path
? trigrams of the path. We consider up to 9 trigrams
? lowest common ancestor node of PRO name and ORG
name along the path
? LCA (Lowest Common Ancestor) path that is from ORG
name to its lowest common ancestor with PRO name
? relative position of PRO name and ORG name. In parse
trees, we consider 4 types of positions that ORGs are relative
to PROs: before, after, inside, other
Table 1: Features adopted from the SRL task. PRO:
PROTEIN; ORG: ORGANISM
work we plan to take the relevant labeled NEs from
the generative model as our input.
Table 1 and Table 2 list the features that are used
in the system.
4 Experiments and Evaluation
4.1 Data set
Our experimental data set is derived from a small
expert-curated corpus, where the POL relations and
relevant PROTEIN, ORGANISM and LOCATION
NEs are labeled. It contains ?150k words, 565 rela-
tion instances for POL, 371 for PO and 431 for PL.
4.2 Systems and Experimental Results
We built several models to compare the relative util-
ity of various types of rich syntactic features that
we can exploit for this task. For various represen-
tations, such as feature vectors, trees and their com-
binations, we applied different kernels in a Support
Vector Machine (SVM) learner. We use Joachims?
? subcategorization that records the immediate structure that
expands from ORG name. Non-ORG daughters will be elim-
inated
? if there is an VP node along the path as ancestor of ORG
name
? if there is an VP node as sibling of ORG name
? path from PRO name to LCA and the path length (L1)
? path from ORG name to LCA and the path length (L2)
? combination of L1 and L2
? sibling relation of PRO and ORG
? distance between PRO name and ORG name in the sen-
tence. ( 3 valued: 0 if nw (number of words) = 0; 1 if 0 <
nw <= 5; 2 if nw > 5)
? combination of distance and sibling relation
Table 2: New features used in the SRL-based rela-
tion extraction system.
SVM light1 with default linear kernel to feature vec-
tors and Moschetti?s SVM-light-TK-1.22 with the
default tree kernel. The models are:
Baseline1 is a purely word-based system, where
the features consist of the unigrams and bigrams
between the PROTEIN name and the ORGAN-
ISM/LOCATION names inclusively, where the stop-
words are selectively eliminated.
Baseline2 is a naive approach that assumes that any
example containing PROTEIN, LOCATION names
has the PL relation. The same assumption is made
for PO and POL relations.
PAK system uses predicate-argument structure ker-
nel (PAK) based method. PAKwas defined in (Mos-
chitti, 2004) and only considers the path from the
predicate to the target argument, which in our set-
ting is the path from the PROTEIN to the ORGAN-
ISM or LOCATION names.
SRL is an SRL system which is adapted to use our
new feature set. A default linear kernel is applied
with SVM learning.
TRK system is similar to PAK system except that
the input is an entire parse tree instead of a PAK
path.
TRK+SRL combines full parse trees and manually
extracted features and uses the kernel combination.
1http://svmlight.joachims.org/
2http://ai-nlp.info.uniroma2.it/moschitti/TK1.2-
software/Tree-Kernel.htm
99
Method PL PO POL
Measure Prec Rec F Acc Prec Rec F Acc Prec Rec F Acc
Baseline1 98.1 61.0 75.3 60.6 88.4 59.7 71.3 58.5 57.1 90.9 70.1 56.3
Baseline2 61.9 100.0 76.5 61.9 48.8 100.0 65.6 48.9 59.8 100.0 74.8 59.8
PAK 71.0 71.0 71.0 64.6 69.0 66.7 67.8 61.8 66.0 69.9 67.9 62.6
SRL 72.9 77.1 74.9 70.3 66.0 71.0 68.4 64.5 70.6 67.5 69.0 65.8
TRK 69.8 81.6 75.3 72.0 64.2 84.1 72.8 72.0 79.6 66.2 72.3 71.3
TRK+SRL 74.9 79.4 77.1 72.8 73.9 78.1 75.9 72.6 75.3 74.5 74.9 71.8
Table 3: Percent scores of Precision/Recall/F-score/Accuracy for identifying PL, PO and POL relations.
4.3 Fusion of Binary relations
We predict the POL ternary relation by fusing PL
and PO binary relations if they belong to the same
sentence and have the same PROTEIN NE. The pre-
diction is made by the sum of confidence scores
(produced by the SVM) of the PL and PO relations.
This is similar to the postprocessing step in SRL task
in which the semantic roles assigned to the argu-
ments have to realize a legal final semantic frame
for the given predicate.
4.4 Discussion
Table 3 shows the results using 5-fold cross valida-
tion. We report figures on ternary relation extraction
and extraction of the two binary relations. Compari-
son between the PAK model and SRL model shows
that manually specified features are more discrimi-
native for binary relation extraction; they boost pre-
cision and accuracy for ternary relation extraction.
In contrast to the SRL model for binary relation ex-
traction, the TRK model obtains lower recall but
higher precision. The combination of SRL with the
TRK system gives best overall accuracy of 71.8%
outperforming shallow word based features.
5 Conclusion
In this paper we explored the use of rich syntac-
tic features for the relation extraction task. In con-
trast with the previously used set of syntactic fea-
tures for this task, we use a large number of fea-
tures originally proposed for the Semantic Role La-
beling task. We provide comprehensive experiments
using many different models that use features from
parse trees. Using rich syntactic features by com-
bining SRL features with tree kernels over the en-
tire tree obtains 71.8% accuracy which significantly
outperforms shallow word-based features which ob-
tains 56.3% accuracy.
References
C. Blaschke, M. Andrade, C. Ouzounis, and A. Valencia. 1999.
Automatic extraction of biological information from scien-
tific text: Protein-protein interactions. In AAAI-ISMB 1999.
R. C. Bunescu and R. J. Mooney. 2005. A shortest path depen-
dency kernel for relation extraction. In Proc. HLT/EMNLP-
2005.
G. Claudio, A. Lavelli, and L. Romano. 2006. Exploiting
Shallow Linguistic Information for Relation Extraction from
Biomedical Literature. In Proc. EACL 2006.
A. Culotta and J. Sorensen. 2004. Dependency tree kernels for
relation extraction. In Proc. ACL-2004.
R. Feldman, Y. Regev, M. Finkelstein-Landau, E. Hurvitz, and
B. Kogan. 2002. Mining biomedical literature using infor-
mation extraction. Current Drug Discovery.
N. Kambhatla. 2004. Combining lexical, syntactic, and seman-
tic features with maximum entropy models for information
extraction. In Proc. ACL-2004 (poster session).
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 2000. A
novel use of statistical parsing to extract information from
text. Proc. NAACL-2000.
A. Moschitti. 2004. A study on convolution kernels for shallow
semantic parsing. In Proc. ACL-2004.
T. Sekimizu, H.S. Park, and J. Tsujii. 1998. Identifying the
interaction between genes and gene products based on fre-
quently seen verbs in medline abstracts. In Genome Infor-
matics. 62-71.
Z. Shi, A. Sarkar, and F. Popowich. 2007. Simultaneous Iden-
tification of Biomedical Named-Entity and Functional Re-
lation UsingStatistical Parsing Techniques. In NAACL-HLT
2007 (short paper).
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning
Research.
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A Composite
Kernel to Extract Relations between Entities with Both Flat
and Structured Features. In Proc. ACL-2006.
100
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 127?132,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using LTAG-Based Features for Semantic Role Labeling
Yudong Liu and Anoop Sarkar
Computing Science Department
Simon Fraser University
British Columbia, Canada, V5A 1S6
yudongl,anoop@cs.sfu.ca
Abstract
Semantic role labeling (SRL) methods
typically use features from syntactic parse
trees. We propose a novel method that
uses Lexicalized Tree-Adjoining Gram-
mar (LTAG) based features for this task.
We convert parse trees into LTAG deriva-
tion trees where the semantic roles are
treated as hidden information learned by
supervised learning on annotated data de-
rived from PropBank. We extracted var-
ious features from the LTAG derivation
trees and trained a discriminative decision
list model to predict semantic roles. We
present our results on the full CoNLL 2005
SRL task.
1 Introduction
Semantic role labeling (SRL) is a natural exten-
sion of the syntactic parsing task. In SRL, par-
ticular syntactic constituents in a parse tree for a
sentence are identified with semantic roles. The
labels assigned to various types of arguments and
adjuncts differ in different annotation schemes.
In this paper, we use the PropBank corpus of
predicate-argument structures (Palmer, Gildea and
Kingsbury, 2005). We assume we are given a syn-
tactic parse tree and a particular predicate in the
sentence for which we then identify the arguments
and adjuncts and their labels. In this paper we
compare two models for the identification of se-
mantic role labels in a parse tree: A model that
uses a path in the parse tree (or the derived tree in
TAG terminology) and various associated features
related to this, and we compare this model with a
model that converts the syntactic parse tree into
a Lexicalized Tree-Adjoining Grammar (LTAG)
derivation tree and uses features extracted from the
elementary trees and the LTAG derivation tree.
In each model the features of that model are
used in a discriminative model for semantic role
labeling. The model is a simple decision list
learner that uses tree patterns extracted from the
LTAG derivation trees in order to classify con-
stituents into their semantic roles. We present re-
sults on the full CoNLL 2005 SRL task (Carreras
and Ma`rquez, 2005) a dataset built by combining
the Treebank and parser data with the PropBank
annotations.
2 Background about SRL
A semantic role is defined to be the relationship
that a syntactic constituent has with the predicate.
For example, the following sentence, taken from
the PropBank corpus, shows the annotation of se-
mantic roles:
[A0 Late buying] [V gave] [A2 the Paris
Bourse] [A1 a parachute] [AM-TMP after its
free fall early in the day].
Here, the arguments for the predicate gave are
defined in the PropBank Frame Scheme (Palmer,
Gildea and Kingsbury, 2005) as:
V: verb A2: beneficiary
A0: giver AM-TMP: temporal
A1: thing given
Recognizing and labeling semantic argu-
ments is a key task for answering ?Who?,
?When?,?What?, ?Where?, ?Why?, etc. questions
in Information Extraction, Question Answering,
Summarization (Melli et al 2005), and, in general,
in all NLP tasks in which some kind of semantic
interpretation is needed.
Most previous research treats the semantic role
labeling task as a classification problem, and di-
vides it into two phases: argument identification
and argument classification. Argument identifi-
cation involves classifying each syntactic element
in a sentence into either an argument or a non-
argument. Argument classification involves clas-
sifying each argument identified into a specific se-
mantic role. A variety of machine learning meth-
ods have been applied to this task. One of the most
important steps in building an accurate classifier is
feature selection. Different from the widely used
127
feature functions that are based on the syntactic
parse tree (Gildea and Jurafsky, 2002), we explore
the use of LTAG-based features in a simple dis-
criminative decision-list learner.
3 LTAG Based Feature Extraction
In this section, we introduce the main components
of our system. First, we do a pruning on the given
parse trees with certain constraints. Then we de-
compose the pruned parse trees into a set of LTAG
elementary trees. For each constituent in question,
we extract features from its corresponding deriva-
tion tree. We train using these features in a deci-
sion list model.
3.1 Pruning the Parse Trees
Given a parse tree, the pruning component identi-
fies the predicate in the tree and then only admits
those nodes that are sisters to the path from the
predicate to the root. It is commonly used in the
SRL community (cf. (Xue and Palmer, 2004)) and
our experiments show that 91% of the SRL targets
can be recovered despite this aggressive pruning.
There are two advantages to this pruning: the ma-
chine learning method used for prediction of SRLs
is not overwhelmed with a large number of non-
SRL nodes; and the process is far more efficient
as 80% of the target nodes in a full parse tree are
pruned away in this step. We make two enhance-
ments to the pruned Propbank tree: we enrich the
sister nodes with their head information, which is
a part-of-speech tag and word pair: ?t, w? and PP
nodes are expanded to include the NP complement
of the PP (including the head information). Note
that the target SRL node is still the PP. Figure 1
shows the pruned parse tree for a sentence from
PropBank section 24.
3.2 LTAG-based Decomposition
As next step, we decompose the pruned tree
around the predicate using standard head-
percolation based heuristic rules1 to convert a
Treebank tree into a LTAG derivation tree. We
do not use any sophistical adjunct/argument or
other extraction heuristics using empty elements
(as we don?t have access to them in the CoNLL
2005 data). Also, we do not use any substitution
nodes in our elementary trees: instead we exclu-
sively use adjunction or sister adjunction for the
attachment of sub-derivations. As a result the
1using http://www.isi.edu/?chiang/software/treep/treep.html
root node in an LTAG derivation tree is a spinal
elementary tree and the derivation tree provides
the path from the predicate to the constituent in
question. Figure 2 shows the resulting elementary
tree after decomposition of the pruned tree. For
each of the elementary trees we consider their
labeling in the derivation tree to be their semantic
role labels from the training data. Figure 3 is the
derivation tree for the entire pruned tree.
Note that the LTAG-based decomposition of the
parse tree allows us to use features that are distinct
from the usual parse tree path features used for
SRL. For example, the typical parse tree feature
from Figure 2 used to identify constituent (NP (NN
terminal)) as A0 would be the parse tree fragment:
NP ? NP ? SBAR ? S ? V P ? S ? V P ?
V BG cover (the arrows signify the path through
the parse tree). Using the LTAG-based decompo-
sition means that our SRL model can use any fea-
tures from the derivation tree such as in Figure 2,
including the elementary tree shapes.
3.3 Decision List Model for SRL
Before we train or test our model, we convert
the training, development and test data into LTAG
derivation trees as described in the previous sec-
tion. In our model we make an independence as-
sumption that each semantic role is assigned to
each constituent independently, conditional only
on the path from the predicate elementary tree
to the constituent elementary tree in the deriva-
tion tree. Different elementary tree siblings in the
LTAG derivation tree do not influence each other
in our current models. Figure 4 shows the differ-
ent derivation trees for the target constituent (NP
(NN terminal)): each providing a distinct semantic
role labeling for a particular constituent. We use
a decision list learner for identifying SRLs based
on LTAG-based features. In this model, LTAG el-
ementary trees are combined with some distance
information as features to do the semantic role la-
beling. The rationale for using a simple DL learner
is given in (Gildea and Jurafsky, 2002) where es-
sentially it based on their experience with the set-
ting of backoff weights for smoothing, it is stated
that the most specific single feature matching the
training data is enough to predict the SRL on test
data. For simplicity, we only consider one inter-
mediate elementary tree (if any) at one time in-
stead of multiple intermediate trees along the path
from the predicate to the argument.
128
SNP
PRP-H
He
VP-H
VBZ-H
backflips
PP
IN-H
into
NP
NP-H
NN-H
terminal
,
,
SBAR
WHNP-H
WDT-H
which
S
VP-H
VBZ-H
explodes
,
,
S
VP-H
VBG-H
covering
NP
NN-H
face
PP
IN-H
with
NP
NNS-H
microchips
Figure 1: The pruned tree for the sentence ?He backflips into a desktop computer terminal, which ex-
plodes, covering Huntz Hall ?s face with microchips.?
S
VP-H
VBG-H
cover
NP
NN-H
face
PP
IN-H
with
NP
NNS-H
microchips
S
VP-H
VBZ-H
explodes
,
,
SBAR
WHNP-H
WDT-H
which
predicate: A1: A2: NULL: NULL: R-A0:
NP
NP-H
NN-H
terminal
PP
IN-H
into
S
VP-H
VBZ-H
backflips
NP
PRP-H
He
A0: NULL: NULL: NULL:
Figure 2: The resulting elementary trees after decomposition of the pruned tree.
129
S(backflips)
NP(he) PP(into)
NP(terminal)
,(,) SBAR(which)
S(explodes)
,(,) S(cover)
NP(face) PP(with)
Figure 3: The LTAG derivation tree (with no semantic role labels) corresponding to the pruned tree.
A0: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
NULL: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
A1: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
NULL: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
A0: NP-NP(NN,terminal)
R-A0: SBAR-WHNP(WDT,which)
AM-ADV: S-VP(VBZ,explodes)
predicate: S-VPH(VBG,cover)
Figure 4: Different LTAG derivation trees corresponding to different assignments of semantic roles to
constituents. The constituent in question is (NP (NN terminal)).
NP
NP
NN
terminal
SBAR
S
VP
S
VP
VBG
cover
VP
VBG
cover
PP
IN
with
NP
NNS
microchips
SBAR
WHNP
WDT
which
S
VP
S
VP
VBG
cover
VP
VBZ
explodes
S
VP
VBG
cover
Figure 5: Tree patterns in tree pattern matching
130
The input to the learning algorithm is labeled
examples of the form (xi, yi). yi is the label (either
NULL for no SRL, or the SRL) of the ith example.
xi is a feature vector ?P,A,Dist, Position,R-
type, ti ? tI , Distti?, where P is the predicate
elementary tree, A is the tree for the constituent
being labeled with a SRL, tI is a set of interme-
diate elementary trees between the predicate tree
and the argument tree. Each P,A, I tree consists
of the elementary tree template plus the tag, word
pair: ?t, w?.
All possible combinations of fully-
lexicalized/postag/un-lexicalized elementary
trees are used for each example. Dist and Distti
denote the distance to the predicate from the
argument tree and the intermediate elementary
tree respectively. Position is interpreted as the
position that the target is relative to the predicate.
R-type denotes the relation type of the predicate
and the target constituent. 3 types are defined: if
the predicate dominates (directly or undirectly)
the argument in the derivation tree, we have the
relation of type-1; if the other way around, the
argument dominates (directly or undirectly) the
predicate then we have the relation of type-2; and
finally type-3 means that neither the predicate
or the argument dominate each other in the
derivation tree and instead are dominated (again,
directly or indirectly) by another elementary tree.
The output of the learning algorithm is a func-
tion h(x, y) which is an estimate of the conditional
probability p(y | x) of seeing SRL y given pat-
tern x. h is interpreted as a decision list of rules
x ? y ranked by the score h(x, y). In testing,
we simply pick the first rule that matches the par-
ticular test example x. We trained different mod-
els using the same learning algorithm. In addition
to the LTAG-based method, we also implemented
a pattern matching based method on the derived
(parse) tree using the same model. In this method,
instead of considering each intermediate elemen-
tary tree between the predicate and the argument,
we extract the whole path from the predicate to the
argument. So the input is more like a tree than a
discrete feature vector. Figure 5 shows the patterns
that are extracted from the same pruned tree.
4 Experiments and Results
We use the PropBank corpus of predicate-
argument structures (Palmer, Gildea and Kings-
bury, 2005) as our source of annotated data for the
dev = Sec24 p(%) r(%) f(%)
test = Sec23
M1: dev 78.42 77.03 77.72
M1: test 80.52 79.40 79.96
M2: dev 81.11 79.39 80.24
M2: test 83.47 81.82 82.64
M3: dev 80.98 79.56 80.26
M3: test 81.86 83.34 82.60
Table 1: Results on the CoNLL 2005 shared task
using gold standard parse trees. M1 is the LTAG-
based model, M2 is the derived tree pattern match-
ing Model, M3 is a hybrid model
SRL task. However, there are many different ways
to evaluate performance on the PropBank, leading
to incomparable results. To avoid such a situation,
in this paper we use the CoNLL 2005 shared SRL
task data (Carreras and Ma`rquez, 2005) which
provides a standard train/test split, a standard
method for training and testing on various prob-
lematic cases involving coordination. However, in
some cases, the CoNLL 2005 data is not ideal for
the use of LTAG-based features as some ?deep? in-
formation cannot be recovered due to the fact that
trace information and other empty categories like
PRO are removed entirely from the training data.
As a result some of the features that undo long-
distance movement via trace information in the
TreeBank as used in (Chen and Rambow, 2003)
cannot be exploited in our model. Our results are
shown in Table 1. Note that we test on the gold
standard parse trees because we want to compare
a model using features from the derived parse trees
to the model using the LTAG derivation trees.
5 Related Work
In the community of SRL researchers (cf. (Gildea
and Jurafsky, 2002; Punyakanok, Roth and Yih,
2005; Pradhan et al 2005; Toutanova et al,
2005)), the focus has been on two different aspects
of the SRL task: (a) finding appropriate features,
and (b) resolving the parsing accuracy problem by
combining multiple parsers/predictions. Systems
that use parse trees as a source of feature func-
tions for their models have typically outperformed
shallow parsing models on the SRL task. Typi-
cal features extracted from a parse tree is the path
from the predicate to the constituent and various
generalizations based on this path (such as phrase
type, position, etc.). Notably the voice (passive or
131
active) of the verb is often used and recovered us-
ing a heuristic rule. We also use the passive/active
voice by labeling this information into the parse
tree. However, in contrast with other work, in this
paper we do not focus on the problem of parse ac-
curacy: where the parser output may not contain
the constituent that is required for recovering all
SRLs.
There has been some previous work in SRL
that uses LTAG-based decomposition of the parse
tree and we compare our work to this more
closely. (Chen and Rambow, 2003) discuss a
model for SRL that uses LTAG-based decompo-
sition of parse trees (as is typically done for sta-
tistical LTAG parsing). Instead of using the typi-
cal parse tree features used in typical SRL models,
(Chen and Rambow, 2003) uses the path within
the elementary tree from the predicate to the con-
stituent argument. They only recover seman-
tic roles for those constituents that are localized
within a single elementary tree for the predicate,
ignoring cases that occur outside the elementary
tree. In contrast, we recover all SRLs regardless
of locality within the elementary tree. As a result,
if we do not compare the machine learning meth-
ods involved in the two approaches, but rather the
features used in learning, our features are a natural
generalization of (Chen and Rambow, 2003).
Our approach is also very akin to the approach
in (Shen and Joshi, 2005) which uses PropBank
information to recover an LTAG treebank as if it
were hidden data underlying the Penn Treebank.
This is similar to our approach of having several
possible LTAG derivations representing recovery
of SRLs. However, (Shen and Joshi, 2005) do
not focus on the SRL task, and in both of these
instances of previous work using LTAG for SRL,
we cannot directly compare our performance with
theirs due to differing assumptions about the task.
6 Conclusion and Future Work
In this paper, we proposed a novel model for
SRL using features extracted from LTAG deriva-
tion trees. A simple decision list learner is applied
to train on the tree patterns containing new fea-
tures. This simple learning method enables us to
quickly explore new features for this task. How-
ever, this work is still preliminary: a lot of addi-
tional work is required to be competitive with the
state-of-the-art SRL systems. In particular, we do
not deal with automatically parsed data yet, which
leads to a drop in our performance. We also do not
incorporate various other features commonly used
for SRL, as our goal in this paper was to make a
direct comparison between simple pattern match-
ing features on the derived tree and compare them
to features from LTAG derivation trees.
References
X. Carreras and L. Ma`rquez 2005. Introduction to
the CoNLL-2005 Shared Task. In Proc. of CoNLL
2005.
J. Chen and O. Rambow. 2003. Use of Deep Linguis-
tic Features for the Recognition and Labeling of Se-
mantic Arguments. In Proceedings of the 2003 Con-
ference on Empirical Methods in Natural Language
Processing, Sapporo, Japan, 2003.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
58(3):245?288
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
G. Melli and Y. Wang and Y. Liu and M. Kashani and Z.
Shi and B. Gu and A. Sarkar and F. Popowich 2005.
Description of SQUASH, the SFU Question An-
swering Summary Handler for the DUC-2005 Sum-
marization Task. In Proceeding of Document Un-
derstanding Conference (DUC-2005)
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Semantic Role Chunking Com-
bining Complementary Syntactic Views, In Pro-
ceedings of the 9th Conference on Natural Language
Learning (CoNLL 2005), Ann Arbor, MI, 2005.
V. Punyakanok, D. Roth, and W Yih. 2005. Gener-
alized Inference with Multiple Semantic Role La-
beling Systems (shared task paper). Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL) pp. 181-184
Ruppenhofer, Josef, Collin F. Baker and Charles J. Fill-
more. 2002. The FrameNet Database and Soft-
ware Tools. In Braasch, Anna and Claus Povlsen
(eds.), Proceedings of the Tenth Euralex Interna-
tional Congress. Copenhagen, Denmark. Vol. I: 371-
375.
L. Shen and A. Joshi. 2005. Building an LTAG Tree-
bank. Technical Report MS-CIS-05-15, CIS Depart-
ment, University of Pennsylvania.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005.
Joint learning improves semantic role labeling. ACL
2005
N. Xue and M. Palmer. 2004. Calibrating Features
for Semantic Role Labeling, In Proceedings of
EMNLP-2004. Barcelona, Spain.
132
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Exploration of the LTAG-Spinal Formalism and Treebank
for Semantic Role Labeling
Yudong Liu and Anoop Sarkar
School of Computing Science
Simon Fraser University
{yudongl,anoop}@cs.sfu.ca
Abstract
LTAG-spinal is a novel variant of tradi-
tional Lexicalized Tree Adjoining Gram-
mar (LTAG) introduced by (Shen, 2006).
The LTAG-spinal Treebank (Shen et al,
2008) combines elementary trees ex-
tracted from the Penn Treebank with Prop-
bank annotation. In this paper, we present
a semantic role labeling (SRL) system
based on this new resource and provide an
experimental comparison with CCGBank
and a state-of-the-art SRL system based
on Treebank phrase-structure trees. Deep
linguistic information such as predicate-
argument relationships that are either im-
plicit or absent from the original Penn
Treebank are made explicit and accessible
in the LTAG-spinal Treebank, which we
show to be a useful resource for semantic
role labeling.
1 Introduction
Semantic Role Labeling (SRL) aims to identify
and label all the arguments for each predicate in
a sentence. Specifically, it involves identifying
portions of the sentence that represent the pred-
icate?s arguments and assigning pre-specified se-
mantic roles to them.
[A0seller Ports of Call Inc.] reached agreements to
[Vverb sell] [A1thing its remaining seven aircraft]
[A2buyer to buyers that weren?t disclosed] .
is an example of SRL annotation from the Prop-
Bank corpus (Palmer et al, 2005), where the sub-
scripted information maps the semantic roles A0,
A1 and A2 to arguments for the predicate sell as
defined in the PropBank Frame Scheme.
The availability of annotated corpora like Prop-
Bank and FrameNet (Fillmore et al, 2001) have
provided rapid development of research into
SRL (Gildea and Jurafsky, 2002; Gildea and
Palmer, 2002; Surdeanu et al, 2003; Chen and
Rambow, 2003; Gildea and Hockenmaier, 2003;
Xue and Palmer, 2004; Pradhan et al, 2004; Prad-
han et al, 2005). The shared tasks in CoNLL-
2004 (Carreras and Ma`rquez, 2004), CoNLL-
2005 (Carreras and Ma`rquez, 2005) and CoNLL-
2008 (Surdeanu et al, 2008) were all focused on
SRL.
SRL systems (Gildea and Jurafsky, 2002;
Gildea and Palmer, 2002) have extensively used
features defined over Penn Treebank phrase-
structure trees. Other syntactic representations
such as CCG derivations (Gildea and Hocken-
maier, 2003) and dependency trees (Hacioglu,
2004; Surdeanu et al, 2008) have also been ex-
plored. It has been previously noted that LTAG,
which has the useful property of extended domain
of locality (EDL), is well-suited to address the
SRL task, c.f. (Chen and Rambow, 2003; Liu and
Sarkar, 2007). However, LTAG elementary trees
were extracted from the derived parse trees by
using Magerman-Collins style head-percolation
based heuristic rules (Liu and Sarkar, 2007). The
LTAG-spinal Treebank (Shen et al, 2008) pro-
vided a corpus of derivation trees where elemen-
tary trees were extracted from the Penn Tree-
bank in combination with the Propbank predicate-
argument annotation. The LTAG-spinal Treebank
can be used to overcome some of the limitations of
the previous work on SRL using LTAG: (Liu and
Sarkar, 2007) uses LTAG-based features extracted
from phrase-structure trees as an additional source
of features and combined them with features from
a phrase-structure based SRL framework; (Chen
and Rambow, 2003) only considers those comple-
ment/adjunct semantic roles that can be localized
in LTAG elementary trees, which leads to a loss
of over 17% instances of semantic roles even from
gold-standard trees.
The LTAG-spinal formalism was initially pro-
posed for automatic treebank extraction and sta-
tistical parsing (Shen and Joshi, 2005). However,
its Propbank-guided treebank extraction process
further strengthens the connection between the
LTAG-spinal and semantic role labeling. In this
paper, we present an SRL system that was built to
1
explore the utility of this new formalism, its Tree-
bank and the output of its statistical parser. Ex-
periments show that our LTAG-spinal based SRL
system achieves very high precision on both gold-
standard and automatic parses, and significantly
outperforms the one using CCGbank. More im-
portantly, it shows that LTAG-spinal is an useful
resource for semantic role labeling, with the po-
tential for further improvement.
2 LTAG-spinal, its Treebank and Parsers
This section gives a brief introduction of LTAG-
spinal formalism, its Treebank that is extracted
with the help of Propbank annotation, and its two
statistical parsers that are trained on the Tree-
bank. Predicate-argument relations encoded in the
LTAG-spinal treebank will also be discussed to il-
lustrate its compatibility with Propbank and their
potential utility for the SRL task.
2.1 LTAG-spinal
The LTAG-spinal formalism (Shen et al, 2008)
is a variant of Lexicalized Tree Adjoining Gram-
mar (LTAG) (Abeille? and Rambow, 2001). Com-
pared to traditional LTAG, the two types of ele-
mentary trees (e-tree for short), initial and auxil-
iary trees, are in spinal form with no substitution
nodes for arguments appearing in the predicate e-
tree: a spinal initial tree is composed of a lexi-
cal spine from the root to the anchor, and noth-
ing else; a spinal auxiliary tree is composed of a
lexical spine and a recursive spine from the root
to the foot node. For example, in Figure 1 (from
(Shen et al, 2008)), the lexical spine for the auxil-
iary tree is B1, .., Bi, .., Bn, the recursive spine is
B1, .., Bi, .., B?1 . Two operations attachment and
adjunction are defined in LTAG-spinal where ad-
junction is the same as adjunction in the traditional
LTAG; attachment stems from sister adjunction as
defined in Tree Insertion Grammar (TIG) (Schabes
and Shieber, 1994), which corresponds to the case
where the root of an initial tree is taken as a child
of another spinal e-tree. The two operations are
applied to LTAG-spinal e-tree pairs resulting in an
LTAG derivation tree which is similar to a depen-
dency tree (see Figure 2). In Figure 2, e-tree an-
chored with continue is the only auxiliary tree; all
other e-trees are initial trees. The arrow is directed
from parent to child, with the type of operation
labeled on the arc. The operation types are: att
denotes attachment operation; adj denotes adjunc-
tion operation. The sibling nodes may have differ-
An
B1A1
Bn
initial: auxiliary:
B1*
Bi
Figure 1: Spinal elementary trees
ent landing site along the parent spine. For ex-
ample, among the child nodes of stabilize e-tree,
to e-tree has VP as landing site; while even has S
as landing site. Such information, on some level,
turns out to be helpful to differentiate the semantic
role played by the different child nodes.
So far, we can see that in contrast with tradi-
tional LTAG where arguments refer to obligatory
constituents only, subcategorization frames and
argument-adjunct distinction are underspecified
in LTAG-spinal. Since argument-adjunct disam-
biguation is one of the major challenges faced by
LTAG treebank construction, LTAG-spinal works
around this issue by leaving the disambiguation
task for further deep processing, such as seman-
tic role labeling.
LTAG-spinal is weakly equivalent to traditional
LTAG with adjunction constraints1 (Shen, 2006).
The Propbank (Palmer et al, 2005) is an an-
notated corpus of verb subcategorization and al-
ternations which was created by adding a layer
of predicate-argument annotation over the phrase
structure trees in the Penn Treebank. The LTAG-
spinal Treebank is extracted from the Penn Tree-
bank by exploiting Propbank annotation. Specif-
ically, as described in (Shen et al, 2008), a Penn
Treebank syntax tree is taken as an LTAG-spinal
derived tree; then information from the Penn Tree-
bank and Propbank is merged using tree transfor-
mations. For instance, LTAG predicate coordina-
tion and instances of adjunction are recognized
using Propbank annotation. LTAG elementary
trees are then extracted from the transformed Penn
Treebank trees recursively, using the Propbank an-
notation and a Magerman-Collins style head per-
colation table.
This guided extraction process allows syntax
and semantic role information to be combined in
LTAG-spinal derivation trees. For example, the
1null adjunction (NA), obligatory adjunction (OA) and se-
lective adjunction (SA)
2
Figure 2: An example of LTAG-spinal sub-derivation tree, from LTAG-spinal Treebank Section 22
Figure 3: Three examples of LTAG-spinal derivation trees where predicates and their Propbank style
argument labels are given. These examples are from LTAG-spinal Treebank Section 22.
Penn Treebank does not differentiate raising verbs
and control verbs, however, based on the Propbank
information, LTAG-spinal makes this distinction
explicit. Thus, the error of taking a subject ar-
gument which is not semantically an argument of
the raising verb can be avoided. Another prop-
erty of LTAG-spinal Treebank extraction lies in the
flexibility and simplicity of the treatment of pred-
icate coordination (see (Shen et al, 2008)). Fig-
ure 3 shows three examples of Propbank annota-
tion as decorations over the LTAG-spinal deriva-
tion trees. In each derivation tree, each node is
associated with LTAG-spinal e-trees. Each argu-
ment (A0, A1, etc.) is referred to as A and the
predicate is called P . In most cases, the argument
is found locally in the derivation tree due to the
extended domain of locality in e-trees. Thus, most
arguments are identified by the pattern P ? A or
P ? A. The next section contains a discussion of
such patterns in more detail.
Two statistical parsers have been developed
by Libin Shen specifically for training on the
LTAG-spinal treebank: a left-to-right incremental
parser (Shen and Joshi, 2005) and a bidirectional
incremental parser (Shen and Joshi, 2008). If one
compares the output of these two parsers, the left-
to-right parser produces full LTAG-spinal deriva-
tion trees (including all the information about
specific elementary trees used in the derivation
and the attachment information within the e-trees)
while the bidirectional parser produces derivation
trees without information about elementary trees
or attachment points (similar to output from a de-
pendency parser). In this paper, we use the left-
to-right incremental parser for its richer output
because our SRL system uses feature functions
that use information about the elementary trees in
the derivation tree and the attachment points be-
tween e-trees. The landing site of child node along
the parent spine is useful for identifying different
types of arguments in SRL. For example, assume
the parent spine is ?S-VP-VB-anchor? (the root la-
bel is S, and ?anchor? is where the lexical item is
inserted). Along with direction information, the
landing site label ?S? is likely to be a good indi-
cator for argument A0 (subject) while the landing
site label ?VP? could be a good indicator for ?A1?
(object). In this sense, the incremental left-to-
right parser is preferable for semantic role label-
ing. However, having been developed earlier than
the bidirectional parser, the incremental parser ob-
tains 1.2% less in dependency accuracy compared
to the bidirectional parser (Shen and Joshi, 2008).
2.2 Predicate-argument relations in the
LTAG-spinal Treebank
The Propbank-guided extraction process for
LTAG-spinal treebank naturally creates a close
connection between these two resources. To ex-
amine the compatibility of the LTAG-spinal Tree-
bank with Propbank, (Shen et al, 2008) provides
the frequency for specific types of paths from
the predicate to the argument in the LTAG-spinal
derivation trees from the LTAG-spinal Treebank.
The 8 most frequent patterns account for 95.5%
of the total predicate-argument pairs of the LTAG-
spinal Treebank, of which 88.4% are directly con-
nected pairs. These statistics not only provide em-
3
Path Pattern Number Percent
1 P?A 8294 81.3
2 P?A, V?A 720 7.1
3 P?Px?A 437 4.3
4 P?Coord?Px?A 216 2.1
5 P?Ax?Py?A 84 0.82
6 P?Coord?Px?A 40 0.39
7 P?Px?Py?A 13 0.13
total recovered w/ patterns 9804 96.1
total 10206 100.0
Table 1: Distribution of the 7 most frequent
predicate-argument pair patterns in LTAG-spinal
Treebank Section 22. P : predicate, A: argument,
V : modifying verb, Coord: predicate coordina-
tion.
pirical justification for the notion of the extended
domain of locality (EDL) in LTAG-spinal (Shen et
al., 2008), they also provide motivation to explore
this Treebank for the SRL task.
We collected similar statistics from Treebank
Section 22 for the SRL task, shown in Table 1,
where 7 instead of 8 patterns suffice in our setting.
Each pattern describes one type of P(redicate)-
A(rgument) pair with respect to their dependency
relation and distance in the LTAG-spinal deriva-
tion tree. The reason that we combine the two pat-
terns P?A and V?A into one is that from SRL
perspective, they are equivalent in terms of the de-
pendency relation and distance between the pred-
icate. Each token present in the patterns, such as
P, Px, Py, V, A, Ax and Coord, denotes a spinal
e-tree in the LTAG-spinal derivation tree.
To explain the patterns more specifically, take
the LTAG-spinal sub-derivation tree in Figure 2
as an example, Assume P(redicate) in question is
stabilize then (stabilize ? even), (stabilize ?
if), (stabilize ? Street), (stabilize ? continue),
(stabilize ? to) all belong to pattern 1; but only
(stabilize ? Street) is actual predicate-argument
pair. Similarly, when take continue as P, the
predicate-argument pair (continue ? stabilize)
belongs to pattern 2, where stabilize corresponds
to A(rgument) in the pattern; (continue, Street) in
(Street ? stabilize ? continue) is an example of
pattern 3, where stabilize corresponds to Px and
Street corresponds to A in the pattern 3 schema.
Pattern 4 denotes the case where argument (A) is
shared between coordinated predicates (P and Px);
The main difference of pattern 5-7 exists where
the sibling node of A(rgument) is categorized into:
predicate (Px) in pattern 7, predicate coordination
node (Coord) in pattern 6 and others (Ax) in pat-
tern 5. We will retain this difference instead of
merging it since the semantic relation between P
and A varies based on these differences. Example
sentences for other (rarer) patterns can be found
in (Shen et al, 2008).
3 LTAG-spinal based SRL System De-
scription
In this section, we describe our LTAG-spinal based
SRL system. So far, we have studied LTAG-spinal
formalism, its treebank and parsers. In particular,
the frequency distribution of the seven most seen
predicate-argument pair patterns in LTAG-spinal
Treebank tells us that predicate-argument relation-
ships typical to semantic role labeling are often lo-
cal in LTAG-spinal derivation trees.
Pruning, argument identification and argument
classification ? the 3-stage architecture now stan-
dard in SRL systems is also used in this paper.
Specifically, for the sake of efficiency, nodes with
high probability of being NULL (non-argument)
should be filtered at the beginning; usually filter-
ing is done based on some heuristic rules; after the
pruning stage, argument identification takes place
with the goal of classifying the pruning-survival
nodes into argument and non-argument; for those
nodes that have been classified as arguments, ar-
gument classification component will further label
them with different argument types, such as A0,
A1, etc. Argument identification and classifica-
tion are highly ambiguous tasks and are usually
accomplished using a machine learning method.
For our LTAG-spinal based SRL system, we
first collect the argument candidates for each pred-
icate from the LTAG-spinal derivation tree. For
each candidate, features are extracted to capture
the predicate-argument relations. Binary classi-
fiers for identification and classification are trained
using SVMs and combined in a one-vs-all model.
The results are evaluated using precision/recall/f-
score.
3.1 Candidate Locations for Arguments
In SRL systems that perform role labeling of con-
stituents in a phrase-structure tree, statistics show
that after pruning, ?98% of the SRL argument
nodes are retained in the gold-standard trees in
the Penn Treebank, which provides a high upper-
bound for the recall of the SRL system. Pruning
away unnecessary nodes using a heuristic makes
4
learning easier as well, as many of the false posi-
tives are pruned away leading to a more balanced
binary classification problem during the seman-
tic role identification and classification steps. We
need a similar heuristic over LTAG-spinal nodes
that will have high coverage with respect to SRL
arguments and provide a high upper-bound for re-
call.
As previously shown that the seven most fre-
quent predicate-argument pair patterns that are
used to describe the specific types of paths from
the predicate to the argument account for?96% of
the total number of predicate-argument pairs in the
LTAG-spinal Treebank. These patterns provide a
natural candidate selection strategy for our SRL.
Table 2 shows a similar oracle test applied to the
output of the LTAG-spinal parser on Section 22.
The total drop in oracle predicate-argument iden-
tifiation drops 10.5% compared to gold-standard
trees. 9.8% is lost from patterns 1 and 2. If ex-
clude those pairs that belong to pattern i in tree-
bank but belong to pattern j (i 6= j) in automatic
parses (so the pattern exists but is the wrong one
for that constituent), the number drops to 81.6%
from 85.6%. This indicates that in terms of the
impact of the syntactic parser errors for SRL, the
LTAG-spinal parser will suffer even more than the
phase structure parser. An alternative is to exhaus-
tively search for predicate-argument pairs without
considering patterns, which we found introduces
too much noise in the learner to be feasible. Thus,
the predicate-argument pairs selected through this
phase are considered as argument candidates for
our SRL system.
3.2 Features
Based on the patterns, features are defined on
predicate-argument pairs from LTAG derivation
Path Pattern Number Percent
1 P?A 7441 72.9
2 P?A, V?A 583 5.7
3 P?Px?A 384 3.8
4 P?Coord?Px?A 180 1.76
5 P?Ax?Py?A 75 0.73
6 P?Coord?Px?A 48 0.47
7 P?Px?Py?A 22 0.21
total recovered w/ patterns 8733 85.6
total 10206 100.0
Table 2: Distribution of the 7 patterns in LTAG-
spinal parser output for Section 22.
tree, mainly including predicate e-trees, argument
e-trees, intermediate e-trees and their ?topological
relationships? such as operation, spine node, rel-
ative position and distance. The following are the
specific features used in our classifiers:
Features from predicate e-tree and its variants
predicate lemma, POS tag of predicate, predicate
voice, spine of the predicate e-tree, 2 variants of
predicate e-tree: replacing anchor in the spine
with predicate lemma, replacing anchor POS in
the spine with voice. In Figure 2, if take stabi-
lize as predicate, these two variants are S-VP-VB-
stabilize and S-VP-VB-active respectively.
Features from argument e-tree and its variants
argument lemma, POS tag of argument, Named
Entity (NE) label of the argument, spine of the ar-
gument e-tree, 2 variants of argument e-tree: re-
placing anchor in the spine with argument lemma,
replacing anchor POS with NE label if any, label
of root node of the argument spine. In Figure 2,
if take stabilize as predicate, and Street as argu-
ment, the two variants are XP-NNP-street and XP-
ORGANIZATION2 respectively.
PP content word of argument e-tree if the root
label of the argument e-tree is PP, anchor of the
last daughter node. NE variant of this feature: re-
place its POS with the NE label if any.
Features from the spine node (SP1) spine node is
the landing site between predicate e-tree and argu-
ment e-tree. Features include the index along the
host spine3, label of the node, operation involved
(att or adj).
Relative position of predicate and argument in the
sentence: before/after.
Order of current child node among its siblings.
In pattern 1, predicate e-tree is parent, and argu-
ment e-tree is child. This feature refers to the order
of argument e-tree among its siblings nodes (with
predicate e-tree as parent).
Distance of predicate e-tree and argument tree in
the LTAG derivation tree: For example, for pattern
1 and 2, the distance has value 0; for pattern 3, the
distance has value 1.
Pattern ID valued 1-7. (see Table 1 and Table 2)
Combination of position and pattern ID, combi-
nation of distance and pattern ID, combination of
2XP-NNP is a normalized e-tree form used in (Shen et
al., 2008) for efficiency and to avoid the problem of sparse
data over too many e-trees.
3it can either be predicate e-tree or argument e-tree. For
example, for pattern P?A, the A(rgument) e-tree is the host
spine.
5
position and order.
Features from intermediate predicate e-tree
same features as predicate e-tree features.
Features from spine node of intermediate pred-
icate e-tree and argument e-tree (SP2) for
predicate-argument pairs of pattern 3-7. These
features are similar to the SP1 features but instead
between intermediate predicate e-tree and argu-
ment e-tree.
Relative position between predicate e-tree and in-
termediate e-tree.
Combination relative positions of argument e-tree
and intermediate predicate e-tree + relative posi-
tion of argument e-tree and predicate e-tree.
The features listed above are used to represent
each candidate constituent (or node) in the LTAG-
spinal derivation tree in training and test data. In
both cases, we identify SRLs for nodes for each
predicate. In training each node comes with the
appropriate semantic role label, or NULL if it does
not have any (for the predicate). In test data,
we first identify nodes as arguments using these
features (ARG v.s. NULL classification) and then
classify a node identified as an argument with the
particular SRL using one-vs-all binary classifica-
tion.
4 Experiments
4.1 Data Set
Following the usual convention for parsing and
SRL experiments, LTAG-spinal Treebank Section
2-21 is used for training and Section 23 for test-
ing. Propbank argument set is used which includes
numbered arguments A0 to A5 and 13 adjunct-like
arguments. 454 sentences in the Penn Treebank
are skipped from the LTAG-spinal Treebank (Shen
et al, 2008)4, which results in 115 predicate-
argument pairs ignored in the test set.
We applied SVM-light (Joachims, 1999) with
default linear kernel to feature vectors. 30% of
the training samples are used to fine tune the reg-
ularization parameter c and the loss-function cost
parameter j for both argument identification and
classification. With parameter validation experi-
ments, we set c = 0.1 and j = 1 for {A0, AM-
4Based on (Shen et al, 2008), the skipped 454 sentences
amount to less than 1% of the total sentences. 314 of these
454 sentences have gapping structures. Since PTB does not
annotate the trace of deleted predicates, additional manual
annotation is required to handle these sentences. For the rest
of the 146 sentences, abnormal structures are generated due
to tagging errors.
NEG}, c = 0.1, j = 2 for {A1, A2, A4, AM-
EXT} and c = 0.1 and j = 4 for the rest.
For comparison, we also built up a standard 3-
stage phrase-structure based SRL system, where
exactly the same data set5 is used from 2004
February release of the Propbank. SVM-light with
linear kernel is used to train on a standard fea-
ture set (Xue and Palmer, 2004). The Charniak
and Johnson parser (2006) is used to produce the
automatic parses. Note that this phrase-structure
based SRL system is state-of-the-art and we have
included all the features proposed in the litera-
ture that use phrase-structure trees. This system
obtains a higher SRL accuracy which can be im-
proved only by using global inference and other
ways (such as using multiple parsers) to improve
the accuracy on automatic parses.
4.2 Results
We compared our LTAG-spinal based SRL system
with phrase-structure based one (see the descrip-
tion in earlier sections), for argument identifica-
tion and classification. In order to analyze the im-
pact of errors in syntactic parsers, results are pre-
sented on both gold-standard trees and automatic
parses. Based on the fact that nearly 97% e-trees
that correspond to the core arguments6 belong to
pattern 1 and 2, which accounts for the largest por-
tion of argument loss in automatic parses, the clas-
sification results are also given for these core argu-
ments. We also compare with the CCG-based SRL
presented in (Gildea and Hockenmaier, 2003)7,
which has a similar motivation as this paper, ex-
cept they use the Combinatory Categorial Gram-
mar formalism and the CCGBank syntactic Tree-
bank which was converted from the Penn Tree-
bank.
Scoring strategy To have a fair evaluation of argu-
ments between the LTAG-spinal dependency parse
and the Penn Treebank phrase structure, we report
the root/head-word based scoring strategy for per-
formance comparison, where a case is counted as
positive as long as the root of the argument e-tree
is correctly identified in LTAG-spinal and the head
word of the argument constituent is correctly iden-
tified in phrase structure. In contrast, boundary-
5The same 454 sentences are ignored.
6A0, A1, A2, A3, A4, A5
7Their data includes the 454 sentences. However, the
missing 115 predicate-argument pairs account for less than
1% of the total number of predicate-argument pairs in the test
data, so even if we award these cases to the CCGBank system
the system performance gap still remains.
6
based scoring is more strict in that the string span
of the argument must be correctly identified in
identification and classification.
Results from using gold standard trees Ta-
ble 3 shows the results when gold standard trees
are used. We can see that with gold-standard
derivations, LTAG-spinal obtains the highest pre-
cision on identification and classification; it also
achieves a competitive f-score (highest f-score for
identification) with the recall upper-bound lower
by 2-3% than phrase-structure based SRL. How-
ever, the recall gap between the two SRL systems
gets larger for classification compared to identifi-
cation8, which is due to the low recall that is ob-
served with our LTAG-spinal based SRL based on
our current set of features. If compare the differ-
ence between the root/head-word based score and
the boundary based score in the 3 scenarios, we
notice that the difference reflects the discrepancy
between the argument boundaries. It is not sur-
prising to see that phrase-structure based one has
the best match. However, CCGBank appears to
have a large degree of mismatch. In this sense,
root/head word based scoring provides fair com-
parison between LTAG-spinal SRL system and the
CCGBank SRL system.
Recent work (Boxwell and White, 2008)
changes some structures in the CCGBank to cor-
respond more closely with the Probbank annota-
tions. They also resolve split arguments that occur
in Propbank and add these annotations into a re-
vised version of the CCGBank. As a result they
show that the oracle f-score improves by over 2
points over the (Gildea and Hockenmaier, 2003)
oracle results for the numbered arguments only
(A0, . . ., A5). It remains an open question whether
a full SRL system based on a CCG parser trained
on this new version of the CCGBank will be com-
petitive against the LTAG-spinal based and phrase-
structure based SRL systems.
Results from using automatic parses Table 4
shows the results when automatic parses are used.
With automatic parses, the advantage of LTAG-
spinal in the precision scores still exists: giving
a higher score in both identification and core argu-
ment classification; only 0.5% lower for full argu-
ment classification. However, with over 6% dif-
ference in upper-bound of recall (?85.6% from
LTAG-spinal; ?91.7% from Charniak?s parser),
8no NULL examples are involved when training for argu-
ment classification.
the gap in recall becomes larger: increased to
?10% in automatic parses from ?6% in gold-
standard trees.
The identification result is not available for
CCG-based SRL. In terms of argument classifica-
tion, it is significantly outperformed by the LTAG-
spinal based SRL. In particular, it can be seen that
the LTAG-spinal parser performs much better on
argument boundaries than CCG-based one.
One thing worth mentioning is that since neither
the LTAG-spinal parser nor Charniak?s parser pro-
vides trace (empty category) information in their
output, no trace information is used for LTAG-
spinal based SRL or the phrase-structure based
SRL even though it is available in their gold-
standard trees.
5 Conclusion and Future Work
With a small feature set, the LTAG-spinal based
SRL system described in this paper provides the
highest precision in almost all the scenarios, which
indicates that the shallow semantic relations, e.g.,
the predicate-argument relations that are encoded
in the LTAG-spinal Treebank are useful for SRL,
especially when compared to the phrase structure
Penn Treebank. (Shen et al, 2008) achieves an f-
score of 91.6% for non-trace SRL identification on
the entire Treebank by employing a simple rule-
based system, which also suggested this conclu-
sion. In other words, there is a tighter connection
between the syntax and semantic role labels in the
LTAG-spinal representation.
However, in contrast to the high precision, the
recall performance of LTAG-spinal based SRL
needs a further improvement, especially for the ar-
gument classification task. From SRL perspective,
on one hand, this may be due to the pattern-based
candidate selection, which upper-bounds the num-
ber of predicate-argument pairs that can be re-
covered for SRL; on the other hand, it suggests
that the features for argument classification need
to be looked at more carefully, compared to the
feature selection for argument identification, es-
pecially for A2 and A3 (as indicated by our error
analysis on the results on the development set). A
possible solution is to customize a different fea-
ture set for each argument type during classifica-
tion, especially for contextual information.
Experiments show that when following the
pipelined architecture, the performance of LTAG-
based SRL is more severely degraded by the syn-
tactic parser, compared to the SRL using phrase
7
Identification gold-standard trees (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 96.0/92.1/94.0 93.0/94.0/93.5 n/a
classification (core) gold-standard trees (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 90.6/83.4/86.9 87.2/88.4/87.8 82.4/78.6/80.4
classification (full) gold-standard trees (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 88.2/81.7/84.8 86.1/87.1/86.6 76.3/67.8/71.8
Boundary 87.4/81.0/84.1 86.0/87.0/86.5 67.5/60.0/63.5
Table 3: Using gold standard trees: comparison of the three SRL systems for argument identification,
core and full argument classification
Identification automatic parses (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 85.8/80.0/82.8 85.8/87.7/86.7 n/a
classification (core) automatic parses (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 81.0/71.5/76.0 80.1/82.8/81.4 76.1/73.5/74.8
classification (full) automatic parses (p/r/f%)
Scoring LTAG phrase CCG
Root/head-word 78.0/70.0/73.7 78.5/80.3/79.4 71.0/63.1/66.8
Boundary 72.3/65.0/68.5 73.8/75.5/74.7 55.7/49.5/52.4
Table 4: Using automatic parses: comparison of the three SRL systems for argument identification, core
and full argument classification
structure and CCG formalism. Even though the
left-to-right statistical parser that was trained and
evaluated on the LTAG-spinal Treebank achieves
an f-score of 89.3% for dependencies on Section
23 of this treebank (Shen and Joshi, 2005), the
SRL that used this output is worse than expected.
An oracle test shows that via the same 7 patterns,
only 81.6% predicate-argument pairs can be re-
covered from the automatic parses, which is a big
drop from 96.1% when we use the LTAG-spinal
Treebank trees. Parser accuracy is high overall,
but needs to be more accurate in recovering the
dependencies between predicate and argument.
Based on the observation that the low recall
occurs not only to the SRL when the automatic
parses are used but also when the gold trees are
used, we would expect that a thorough error analy-
sis and feature calibrating can give us a better idea
in terms of how to increase the recall in both cases.
In on-going work, we also plan to improve
the dependency accuracy for predicate and argu-
ment dependencies by using the SRL predictions
as feedback for the syntactic parser. Our hypoth-
esis is that this approach combined with features
that would improve the recall numbers would lead
to a highly accurate SRL system.
As a final note, we believe that our effort on us-
ing LTAG-spinal for SRL is a valuable exploration
of the LTAG-spinal formalism and its Treebank re-
source. We hope our work will provide useful in-
formation on how to better utilize this formalism
and the Treebank resource for semantic role label-
ing.
Acknowledgements
We would like to thank Aravind Joshi and Lu-
cas Champollion for their useful comments and
for providing us access to the LTAG-spinal Tree-
bank. We would especially like to thank Libin
Shen for providing us with the LTAG-spinal sta-
tistical parser for our experiments and for many
helpful comments.
8
References
A. Abeille? and O. Rambow, editors. 2001. Tree Ad-
joining Grammars: Formalisms, Linguistic Analysis
and Processing. Center for the Study of Language
and Information.
Stephen A. Boxwell and Michael White. 2008. Pro-
jecting propbank roles onto the ccgbank. In LREC-
2008.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
CoNLL-2004 Shared Task. In CoNLL-2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task. In CoNLL-2005.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In EMNLP-2003.
C.J. Fillmore, C. Wooters, and C.F. Baker. 2001.
Building a large lexical databank which provides
deep semantics. In PACLIC15-2001.
D. Gildea and J. Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar.
In EMNLP-2003.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
58(3):245?288.
D. Gildea and M. Palmer. 2002. The necessity of
parsing for predicate argument recognition. In ACL-
2002.
K. Hacioglu. 2004. Semantic role labeling using de-
pendency trees. In COLING-2004.
T. Joachims. 1999. Making large-scale svm learning
practical. Advances in Kernel Methods - Support
Vector Machines.
Y. Liu and A. Sarkar. 2007. Experimental evaluation
of LTAG-based features for semantic role labeling.
In EMNLP-2007.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2004. Shallow Semantic Parsing Using
Support Vector Machines. In HLT-NAACL-2004.
S. Pradhan, W. Ward, K. Hacioglu, , J. H. Martin, and
D. Jurafsky. 2005. Semantic role labeling using dif-
ferent syntactic views. In ACL-2005.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining derivation.
Computational Linguistics, 20(1):91?124.
L. Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP-2005.
L. Shen and A. Joshi. 2008. Ltag dependency pars-
ing with bidirectional incremental construction. In
EMNLP-2008.
L. Shen, L. Champollion, and A. Joshi. 2008. Ltag-
spinal and the treebank: A new resource for in-
cremental, dependency and semantic parsing. Lan-
guage Resources and Evaluation, 42(1):1?19.
L. Shen. 2006. Statistical LTAG Parsing. Ph.D. thesis,
University of Pennsylvania.
M. Surdeanu, S. Harabagiu, J. Williams, and
P. Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In ACL-2003.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The conll 2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. In CoNLL-2008.
N. Xue and M. Palmer. 2004. Calibrating features for
semantic role labeling. In EMNLP-2004.
9
