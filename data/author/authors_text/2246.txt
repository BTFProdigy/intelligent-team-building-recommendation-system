NAACL HLT Demonstration Program, pages 5?6,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Adaptive Tutorial Dialogue Systems Using Deep NLP Techniques
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow,
Manuel Marques-Pita, Colin Matheson and Johanna D. Moore
ICCS-HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, United Kingdom
(mdzikovs,ccallawa,efarrow,mmpita,colin,jmoore)@inf.ed.ac.uk ?
Abstract
We present tutorial dialogue systems in
two different domains that demonstrate
the use of dialogue management and deep
natural language processing techniques.
Generation techniques are used to produce
natural sounding feedback adapted to stu-
dent performance and the dialogue his-
tory, and context is used to interpret ten-
tative answers phrased as questions.
1 Introduction
Intelligent tutoring systems help students improve
learning compared to reading textbooks, though not
quite as much as human tutors (Anderson et al,
1995). The specific properties of human-human di-
alogue that help students learn are still being stud-
ied, but the proposed features important for learn-
ing include allowing students to explain their actions
(Chi et al, 1994), adapting tutorial feedback to the
learner?s level, and engagement/affect. Some tuto-
rial dialogue systems use NLP techniques to analyze
student responses to ?why? questions. (Aleven et al,
2001; Jordan et al, 2006). However, for remediation
they revert to scripted dialogue, relying on short-
answer questions and canned feedback. The result-
ing dialogue may be redundant in ways detrimental
to student understanding (Jordan et al, 2005) and
allows for only limited adaptivity (Jordan, 2004).
?This work was supported under the 6th Framework Pro-
gramme of the European Commission, Ref. IST-507826, and
by a grant from The Office of Naval Research N000149910165.
We demonstrate two tutorial dialogue systems
that use techniques from task-oriented dialogue sys-
tems to improve the interaction. The systems are
built using the Information State Update approach
(Larsson and Traum, 2000) for dialogue manage-
ment and generic components for deep natural lan-
guage understanding and generation. Tutorial feed-
back is generated adaptively based on the student
model, and the interpretation is used to process
explanations and to differentiate between student
queries and hedged answers phrased as questions.
The systems are intended for testing hypotheses
about tutoring. By comparing student learning gains
between versions of the same system using different
tutoring strategies, as well as between the systems
and human tutors, we can test hypotheses about the
role of factors such as free natural language input,
adaptivity and student affect.
2 The BEEDIFF Tutor
The BEEDIFF tutor helps students solve symbolic
differentiation problems, a procedural task. Solu-
tion graphs generated by a domain reasoner are used
to interpret student actions and to generate feed-
back.1 Student input is relatively limited and con-
sists mostly of mathematical formulas, but the sys-
tem generates adaptive feedback based on the notion
of student performance and on the dialogue history.
For example, if an average student asks for a hint
on differentiating sin(x2), the first level of feedback
may be ?Think about which rule to apply?, which
1Solution graphs are generated automatically for arbitrary
expressions, with no limit on the complexity of expressions ex-
cept for possible efficiency considerations.
5
can then be specialized to ?Use the chain rule? and
then to giving away the complete answer. For stu-
dents with low performance, more specific feed-
back can be given from the start. The same strat-
egy (based on an initial corpus analysis) is used in
producing feedback after incorrect answers, and we
intend to use the system to evaluate its effectiveness.
The feedback is generated automatically from a
single diagnosis and generation techniques are used
to produce appropriate discourse cues. For example,
when a student repeats the same mistake, the feed-
back may be ?You?ve differentiated the inner layer
correctly, but you?re still missing the minus sign?.
The two clauses are joined by a contrast relationship,
and the second indicates that an error was repeated
by using the adverbial ?still?.
3 The BEETLE Tutor
The BEETLE tutor is designed to teach students ba-
sic electricity and electronics concepts. Unlike the
BEEDIFF tutor, the BEETLE tutor is built around
a pre-planned course where the students alternate
reading with exercises involving answering ?why?
questions and interacting with a circuit simulator.
Since this is a conceptual domain, for most exer-
cises there is no structured sequence of steps that the
students should follow, but students need to name a
correct set of objects and relationships in their re-
sponse. We model the process of building an answer
to an exercise as co-constructing a solution, where
the student and tutor may contribute parts of the an-
swer. For example, consider the question ?For each
circuit, which components are in a closed path?.
The solution can be built up gradually, with the stu-
dent naming different components, and the system
providing feedback until the list is complete. This
generic process of gradually building up a solution is
also applied to giving explanations. For example, in
answer to the question ?What is required for a light
bulb to light? the student may say ?The bulb must be
in a closed path?, which is correct but not complete.
The system may then say ?Correct, but is that every-
thing?? to prompt the student towards mentioning
the battery as well. The diagnosis of the student an-
swer is represented as a set of correctly given objects
or relationships, incorrect parts, and objects and re-
lationships that have yet to be mentioned, and the
system uses the same dialogue strategy of eliciting
the missing parts for all types of questions.
Students often phrase their answers tentatively,
for example ?Is the bulb in a closed path??. In the
context of a tutor question the interpretation process
treats yes-no questions from the student as poten-
tially hedged answers. The dialogue manager at-
tempts to match the objects and relationships in the
student input with those in the question. If a close
match can be found, then the student utterance is
interpreted as giving an answer rather than a true
query. In contrast, if the student said ?Is the bulb
connected to the battery??, this would be interpreted
as a proper query and the system would attempt to
answer it.
Conclusion We demonstrate two tutorial dialogue
systems in different domains built by adapting di-
alogue techniques from task-oriented dialogue sys-
tems. Improved interpretation and generation help
support adaptivity and a wider range of inputs than
possible in scripted dialogue.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In Proc. AI-ED 2001.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons learned.
The Journal of the Learning Sciences, 4(2):167?207.
M. T. H. Chi, N. de Leeuw, M.-H. Chiu, and C. La-
Vancher. 1994. Eliciting self-explanations improves
understanding. Cognitive Science, 18(3):439?477.
P. Jordan, P. Albacete, and K. VanLehn. 2005. Taking
control of redundancy in scripted tutorial dialogue. In
Proc. of AIED2005, pages 314?321.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tutorial
dialogue system for physics. In Proc. of FLAIRS-06.
P. W. Jordan. 2004. Using student explanations as mod-
els for adapting tutorial dialogue. In V. Barr and
Z. Markov, editors, FLAIRS Conference. AAAI Press.
S. Larsson and D. Traum. 2000. Information state and
dialogue management in the TRINDI Dialogue Move
Engine Toolkit. Natural Language Engineering, 6(3-
4):323?340.
6
Interpretation and Generation in a Knowledge-Based Tutorial System
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW,
United Kingdom,
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
Abstract
We discuss how deep interpretation and
generation can be integrated with a know-
ledge representation designed for question
answering to build a tutorial dialogue sys-
tem. We use a knowledge representa-
tion known to perform well in answering
exam-type questions and show that to sup-
port tutorial dialogue it needs additional
features, in particular, compositional rep-
resentations for interpretation and struc-
tured explanation representations.
1 Introduction
Human tutoring is known to help students learn
compared with reading textbooks, producing up to
two standard deviations in learning gain (Bloom,
1984). Tutorial systems, in particular cognitive
tutors which model the inner state of a student?s
knowledge, help learning but result in only up to 1
standard deviation learning gain (Anderson et al,
1995). One current research hypothesis is that this
difference is accounted for by interactive dialogue,
which allows students to ask questions freely, and
tutors to adapt their direct feedback and presenta-
tion style to the individual student?s needs.
Adding natural language dialogue to a tutorial
system is a complex task. Many existing tuto-
rial dialogue systems rely on pre-authored curricu-
lum scripts (Person et al, 2000) or finite-state ma-
chines (Rose? et al, 2001) without detailed knowl-
edge representations. These systems are easy to
design for curriculum providers, but offer limited
flexibility because the writer has to predict all pos-
sible student questions and answers.
We argue that the ability to interpret novel,
context-dependent student questions and answers,
and offer tailored feedback and explanations is
important in tutorial dialogue, and that a domain
knowledge representation and reasoning engine is
necessary to support these applications. We dis-
cuss our knowledge representation, and the issues
of integrating it with state-of-the-art interpretation
and generation components to build a knowledge-
based tutorial dialogue system.
Our application domain is in basic electricity
and electronics, specifically teaching a student
how to predict the behavior and interpret measure-
ments in series and parallel circuits. This is a con-
ceptual domain - that is, students are primarily fo-
cused on learning concepts such as voltage and
current, and their relationships with the real world.
The students use a circuit simulator to build cir-
cuits, and their questions and answers depend on
the current context.
There are various sources of context-
dependency in our domain. Students and
tutors refer to specific items in the simulation
(e.g., ?Which lightbulbs will be lit in these
circuits??), and may phrase their answers in an
unexpected way, for example, by saying ?the
lightbulbs in 2 and 4 will be out? instead of
naming the lit lightbulbs. Moreover, students
may build arbitrary circuits not included in the
question, either because they make mistakes, or
because a tutor instructs them to do so as part of
remediation. Thus it would be difficult to produce
and maintain a finite-state machine to predict all
possible situations, both for interpreting the input
and for generating feedback based on the state
of the environment and the previous dialogue
context: a domain reasoner is necessary to handle
such unanticipated situations correctly.
We describe a tutorial system which uses a de-
scription logic-based knowledge representation to
4 KRAQ06
generate intelligent explanations and answers to
a student?s questions, as well as to interpret the
student?s language at all stages of the dialogue.
Our approach relies on using an existing wide-
coverage parser for domain-independent syntactic
parsing and semantic interpretation, as well as a
wide-coverage deep generation system. We dis-
cuss the issues which arise in connecting such re-
sources to a domain knowledge representation in a
practical system.
2 Motivation
A good teaching method for basic electricity
and electronics is eliciting cognitive dissonance
(Schaffer and McDermott, 1992; Arnold and
Millar, 1987) which we are implementing as a
?predict-verify-evaluate? (PVE) cycle. The stu-
dents are asked to make predictions about the be-
havior of a schematic circuit and then build it in a
simulation environment. If the observed results do
not match their predictions, a discussion ensues,
where the computer tutor helps a student learn the
relevant concepts. The PVE exercises are comple-
mented with exercises asking the students to iden-
tify properties of circuits in diagrams and to inter-
pret a circuit?s behavior.
Thus, the system has to answer questions about
circuits which students build and manipulate dy-
namically in a simulation environment, and pro-
duce explanations and feedback tailored to that in-
dividual context. This relies on the following sys-
tem capabilities:
? Understanding and giving explanations.
Since the system relies on inducing cognitive
dissonance, it should be able to explain to the
student why their prediction for a specific cir-
cuit was incorrect, and also verify explana-
tions given by a student.
? Unrestricted language input with reference
resolution. Similar to other conceptual do-
mains (VanLehn et al, 2002) the language
observed in corpus studies is varied and syn-
tactically complex. Additionally, in our do-
main students refer to items on screen, e.g.
?the lightbulb in 5?, which requires the sys-
tem to make the connection between the lan-
guage descriptions and the actual objects in
the environment.
? Tailored generation. The level of detail in
the explanations offered should be sensitive
to student knowledge of the domain. Tutorial
utterances should be natural and use correct
terminology even if a student doesn?t.
To support answering questions and giving ex-
planations, we chose the KM knowledge represen-
tation environment (Clark and Porter, 1999) as a
basis for our implementation. KM is a description-
logic based language which has been used to rep-
resent facts and rules in a HALO system for AP
chemistry tests (Barker et al, 2004). It supports
the generation of explanations and obtained the
highest explanation scores in an independent eval-
uation based on an AP chemistry exam (Friedland
et al, 2004). Thus it is a good choice to provide
reasoning support for explanations and answering
novel questions in a tutorial system. However, KM
has not been used previously in connection with
natural language input for question answering, and
we discuss how the limitations of KM representa-
tions affect the interpretation process in Section 4.
We use a deep domain-independent parser and
grammar to support language interpretation, and
a deep generator to provide natural sounding and
context-dependent text. Both deep parsing and
generation provide the context adaptivity we need,
but they are time-consuming to build for a spe-
cific domain. Now that a number of deep domain-
independent parsing and generation systems are
available in the community, our research goal is to
investigate the issues in integrating them with the
knowledge representation for question answering
to support the requirements of a tutorial dialogue
system. We focus on context-dependent explana-
tion understanding and generation as a primary tu-
toring task in our domain. Section 3 discusses
our representations, Section 4 presents the issues
arising in knowledge representation to support in-
terpretation, and Section 5 discusses the require-
ments for appropriate explanation generation and
how it can be integrated into the system.
3 Representations
From the point of view of tutoring, the most im-
portant requirement on the knowledge representa-
tion is that system reasoning should closely match
human reasoning, so that it can be explained to
students in meaningful terms. Thus, for exam-
ple, a numerical circuit simulator is well suited for
dynamically displaying circuit behaviors, but not
for conceptually tutoring basic circuits, because it
5 KRAQ06
hides physics principles behind complex mathe-
matical equations that are not suitable for learners.
To design our knowledge representation we
started with a set of lessons for our domain de-
signed by psychologists experienced in designing
training courses for physics and simulated envi-
ronments. The lessons were used in a data col-
lection environment with experienced tutors con-
ducting tutoring sessions over a text chat interface.
Each student and tutor were required to go through
the materials presented as a set of slides and solve
pre-defined exercises, but the students asked ques-
tions to get help with problem-solving. The tutor
had complete freedom to choose how to answer
student questions and how to remediate when stu-
dents made mistakes. We are using this data set
to study the types of errors that students make as
well as the language used by both students and tu-
tors. The latter serves as a guide to developing our
interpretation and generation components.
In addition to the set of materials, the course
designers provided a ?glossary? of concepts and
facts that students need to learn and use in expla-
nations, containing approximately 200 concepts
and rules in a form which should be used in model
explanations. We then developed our knowledge
representation so that concepts listed in the glos-
sary were represented as KM concepts, and facts
are represented as rules for computing slots.
An example KM representation for our domain
is shown in Figure 1. It represents the fact that a
lightbulb will be on if it is in a complete path (i.e. a
closed path containing a battery). The explanation
is generated using the comment structure [light-
bulbstate] shown in Figure 2 (slightly simplified
for readability). Explanations are generated sepa-
rately from reasoning in the KM system because
reasoning in general contains too many low-level
details. For example, our rule for computing a
lightbulb state includes two facts: that the light-
bulb has to be in a complete path with a battery,
and that a lightbulb is always in one state only (i.e.
it cannot be broken and on at the same time). The
latter is required for proof completeness, but is too
trivial to be mentioned to students. KM therefore
requires knowledge engineers to explicitly desig-
nate the facts to be used in explanations.
This representation allows KM to generate de-
tailed explanations by using a template string and
then explaining the supporting facts. An example
of a full explanation, together with the adjustments
needed to use it in dialogue rather than as a com-
plete answer, is given in Section 5.
Currently, KM only supports generating expla-
nations, but not verifying them. The explanation
mechanism produces explanations as text directly
from the knowledge representation, as shown in
Figure 2(a). This generation method is not well
suited for a tutorial dialogue system, because it
does not take context into account, as discussed
in Section 5. Therefore, we are designing a struc-
tured representation for explanations to be pro-
duced by the KM explanation mechanism instead
of using English sentences, shown in Figure 2(b).
This will allow us to generate more flexible ex-
planations (Section 5) and also to interpret student
explanations (Section 4).
4 Interpretation
The interpretation process consists of parsing,
reference resolution, dialogue act recognition
and diagnosing student answers. We discuss
reference resolution and diagnosis here as these
are the two steps impacted by the knowledge
representation issues. As a basic example we will
use the student answer from the following pair:1
Problem: For each circuit, which lightbulbs will
be lit? Explain.
Student: the bulbs in 1 and 3 are lit because they
are in a closed path with a battery
To respond to this answer properly, the system
must complete at least the following tasks. First, it
must resolve ?the bulbs in 1 and 3? to correspond-
ing object IDs in the knowledge base, for exam-
ple, LB-13-1-1 and LB-13-3-1. Then, it must ver-
ify that the student statement is factually correct.
This includes verifying that the lightbulbs in 1 and
3 will be lit, and that each of them is in a closed
path with a battery. Finally, it must verify that
the student explanation is correct. This is sepa-
rate from verifying factual correctness. For exam-
ple, a statement ?because they are in a closed path?
is true for both of those lightbulbs, but it is not a
complete explanation, because a lightbulb may be
in a closed path which does not contain a battery,
where it won?t be lit.
1These utterances come from our corpus, though most of
the student answers are not as easy to parse. We are working
on robust parsing methods to address the issues in parsing
less coherent utterances.
6 KRAQ06
(every LightBulb has
(state ((must-be-a Electrical-Usage-State) (exactly 1 Electrical-Usage-State)
(if (the is-damaged of Self) then *Broken-Usage-State
else (if (has-value (oneof ?batt in (the powered-by of Self)))
where ((the state of ?batt) = *Charged-Power-State)))
then *On-Usage-State else *Off-Usage-State) [lightbulbstate])))
Figure 1: The representation of a lightbulb in our KM database
(a)(comment [lightbulbstate]
(:sentence (?a working lightbulb is on if it is in a complete path with a charged battery?))
(:supporting-facts (:triple Self powered-by *) (forall (the powered-by of Self) (:triple It state *)))
(b)(comment [lightbulbstate]
(:rule :object LightBulb :fact (?lb state *On-Usage-State)
:requires ((?lb powered-by ?v1) (?v1 instance-of Battery) (?v1 state *Charged-Power-State))
:bindings ((?lb ? Self ?) (?v1 ? the powered-by of Self ?)) )
Figure 2: A sample comment structure to generate an explanation for a lit lightbulb (a) The KM text
template (b) a new structured representation. Items in angle brackets are computed dynamically
4.1 Interpreting Factual Statements
We use the TRIPS dialogue parser (Dzikovska,
2004) for interpretation. The TRIPS parser pro-
vides a two-layer architecture where the utter-
ance meaning is represented using a domain-
independent semantic ontology and syntax. The
domain-independent representation is used for dis-
course processing tasks such as reference reso-
lution, but it is connected to the domain-specific
knowledge representation by mapping between
the domain-independent and domain-specific on-
tologies (Dzikovska et al, 2003; Dzikovska,
2004). This architecture allows us to separate lin-
guistic and domain-specific knowledge and easily
specialize to new domains.
When applied in our domain, the TRIPS inter-
pretation architecture was helpful in getting the
interpretation started quickly, because we only
needed to extend the lexicon with specific terms
related to basic electricity and electronics (e.g.,
?multimeter?), while other lexical items and syn-
tactic constructions were provided in the domain-
independent part.
The reference resolution module operates on
TRIPS domain-independent representations send-
ing queries to KM as necessary, because the
TRIPS representations offer linguistic features to
guide reference resolution not available in the rep-
resentations used for reasoning. We use a recur-
sive reference resolution algorithm similar to By-
ron (2002) which first resolves ?1 and 3? are re-
solved as names for Circuit-13-1 and Circuit-13-
3,2 and then queries KM to find all lightbulbs in
those circuits. Dialogue context is used to inter-
pret the reference resolution results. In this case,
the context does not matter because the question
sets up all lightbulbs on screen as contextually rel-
evant. But if the student had said ?the ones in 1
and 3?, the query would be for all components in
circuits 1 and 3, and then our algorithm will filter
the query results based on the question context to
retain only lightbulbs.
Once the references are resolved, the whole sen-
tence is converted to a KM statement which repre-
sents the student utterance, in our case (the state
of LB13-1-1) = *On-Usage-State, where LB13-1-
1 is the lightbulb obtained by reference resolution.
This statement is sent to the KM system, which
verifies that it is correct. This procedure allows us
to use dialogue context in understanding, and also
to check correctness of answers easily, even if they
are phrased in an unanticipated way.
However, even with the layer of separation of
linguistic and domain knowledge provided by the
TRIPS architecture, we found that the need to sup-
port interpretation in a compositional way influ-
ences the interaction with knowledge representa-
tion. There are many ways to express the same
query to KM, which differ in efficiency. Two ex-
2This step is not trivial, because on other slides the label
?1? refers to terminals or other components rather than whole
circuits, and therefore there is no 1-to-1 correspondence be-
tween names and objects in the environment.
7 KRAQ06
(a) (allof ?x in (the all-instances of LightBulb) where ((the components of Circuit-13-1) include ?x))
(allof ?x (LightBulb ?x) and (components Circuit-13-1 ?x))
(b) (allof ?comp in (the components of Circuit-13-1) where (?comp isa LightBulb))
(allof ?x (components Circuit-13-1 ?x) and (LightBulb ?x) )
Figure 3: KM Queries to to retrieve all lightbulbs in a circuit with corresponding first-order logic glosses.
ample queries to ask the same question are given in
Figure 3. While their first order logic semantics is
equivalent except for the order of conjuncts, they
are expressed in a very different way in the KM
syntax. Version (b) is more efficient to ask, be-
cause it retrieves the components of circuit 1 first,
a smaller set than the set of all lightbulbs.
This asymmetry presents a challenge to both
language interpretation and knowledge engineer-
ing. Existing reference resolution algorithms (By-
ron, 2002; Bos, 2004) expect the queries for ?the
lightbulb? and ?the lightbulb in 1? to be strictly
compositional in the sense that the phrase ?the
lightbulb? will be represented identically in both
cases, and ?in 1? is represented as an additional
constraint on the lightbulbs. This corresponds to
the query variant (a) in the system. Otherwise
a large amount of query-specific transformations
may be required to produce queries for complex
noun phrase descriptions, diminishing the scala-
bility of the approach.
We had to spend a significant portion of time
in the project developing an efficient and com-
positional knowledge representation. Our cur-
rent solution is to prefer compositionality over ef-
ficiency, even though it impacts performance in
some cases, but we are working on a more gen-
eral solution. Instead of converting directly to
KM from domain-independent language represen-
tations, we will convert all queries in a FOL-like
syntax shown in Figure 3 which uses concepts
from the KM representation, but where all con-
juncts are treated identically in the syntax. The
problem of converting this representation to the
optimal KM form can then be seen as an instance
of query optimization. For example, we can re-
order the conjuncts putting the relations which in-
clude an instance constant (e.g., (the components
of Circuit-13-1)) first in the query, because they
are more likely to limit the search to small sets
of objects. This representation can be easily con-
verted in the KM syntax, and is also useful for
explanation understanding and generation as dis-
cussed below.
4.2 Explanation Understanding
While KM has facilities for generating explana-
tions, it does not have support for reading in a stu-
dent explanation and verifying it. We devised a
method to support this functionality with the aid of
KM explanation generation mechanism. Any time
a student offers an explanation, the KM reasoner
will be called to generate its own explanation for
the same fact, in the structured format shown in
Figure 2(b). Then the student explanation (con-
verted into the same intermediate syntax) can be
matched against the KM-generated explanation to
verify that it is complete, or else that certain parts
are missing.
In our example, the student explanation ?be-
cause they are in a closed path with a battery? will
be represented as (?pa instance-of Path) (?pa is-
closed t) (?b instance-of Battery) (?pa contains
?b) (?pa contains LB-13-1-1).3 This explanation
does not directly match into the explanation struc-
ture from Figure 2(b), because it uses the more
specific term ?in closed path with a battery? rather
than the more general term ?in complete path?
(represented by the powered-by slot). However,
as part of generating the explanation, an explana-
tion structure for the powered-by will be gener-
ated, and it will include the facts (?pa is-closed
t) (?pa contains ?b). This will match the student
explanation. It will be up to the tutorial module to
decide whether to accept the explanation ?as is?,
or lead the student to use the more precise termi-
nology, as discussed in Section 5.
This method can address student explanations
as long as they correspond to parts of typical ex-
planations, and identify missing parts. The biggest
open problem we face is equivalent inferences.
For example, a student may say ?A lightbulb is
not on? instead of ?a lightbulb is off?. KM rea-
soning handles those differences when verifying
factual correctness, but KM does not support sim-
ilar reasoning for matching explanations (which
3Here ?they? would be resolved first to a set of lightbulbs,
and each instance will be treated separately to verify that the
explanation applies.
8 KRAQ06
would correspond to verifying full proofs rather
than individual facts). We are considering bring-
ing a theorem prover to reason over intermediate
representations together with KM axioms to help
interpret explanations, as done in (Makatchev et
al., 2004; Bos, 2005).
5 Generation
The task of the utterance generation component is
to produce tutorial dialogue, such as asking new
questions of the student, conveying the correctness
of their answers, and giving explanations. Expla-
nations may be given in response to a student?s di-
rect ?why? question or when a student has erred
and the pedagogical reasoner has decided that an
explanation is the best remediation strategy. In
each case, the utterance generator must not only
provide a correct, thorough and coherent explana-
tion, but must tailor it so that the student doesn?t
receive too much or too little information. To
be tailorable, explanations must be derived from
the represented domain knowledge and from what
the tutoring system knows about the student (e.g.,
their recent performance).
Directly producing explanations by appending
together pieces of hand-written strings as in Fig-
ure 2(a) usually results in long explanations that
contain little detail of interest to the student. Fig-
ure 4 contains one such example explanation gen-
erated by the KM system in our domain and de-
rived from a query based on the production rule in
Figure 1. This explanation makes sense in answer-
ing an exam question, as intended in the KM sys-
tem, but it is not necessarily helpful in dialogue.
As an example, suppose the student had incor-
rectly answered the question in Section 4, and the
tutoring system decides to correctly explain why
the lightbulbs are lit. Usually, a full explanation
is not necessary in these cases. In the case where
a student gave an incomplete explanation, namely
leaving out the necessary mention of the battery,
a simple response of the form ?Yes, but don?t for-
get the battery? will be infinitely more helpful than
the full explanation. If the student?s explanation is
completely correct, but they have failed to notice
a change in the environment, the more appropriate
explanation is ?The lightbulb is in a closed path,
as well as the battery, but the battery is not oper-
ational?. Furthermore, if a student has shown that
they are knowledgeable about certain fundamen-
tal facts, such as what states a lightbulb may be
in, statements like ?A lightbulb can be on, off or
broken? should be removed.
Adding this reasoning directly to the knowledge
base would make it unwieldy and unmodifiable,
and the string-based generation in KM comments
does not allow for adapting explanations based on
external knowledge such as a student model. To
adapt the KM explanation mechanism to support
such context-dependent generation, instead of cre-
ating explanations via template strings, we have
devised the representation presented in Figure 2(b)
that is based on semantics and allows us to mod-
ify an explanation after it has been produced by
the KM reasoning process but before it has been
converted into a string representation.
Based on this semantic representation, explana-
tion content can be selected more appropriately. If
the interpreter discussed in Section 4.2 determines
that parts of the explanation from the :requires
field are missing, the generation can focus only on
that part of the explanation. The requirements list
would also be used to determine if the student is
not aware of environment properties, such as that a
battery is damaged. Finally, the facts known to the
student can be removed if the corresponding se-
mantic forms were used in previous explanations.
In addition to selecting the explanation content
properly, it is important that the responses given to
the student sound fluid and are easy to understand.
In dialogue, in particular, it is important that pro-
nouns can be generated based on references im-
portant for the student, and avoid repetitiveness in
syntax. Knowledge of linguistic features such as
number and gender, and also knowledge of what
was previously mentioned in the discourse, is nec-
essary to support such natural text generation.
Deep generation utilizes this represented
knowledge along with grammatical and lexical
knowledge of a language, rather than hand-written
strings, to produce utterances. Our current im-
plementation uses a custom utterance generation
component and the STORYBOOK (Callaway and
Lester, 2002) deep text generator modified to
work in a dialogue context. Once the explanation
content is selected, it is passed to the STORYBOOK
system to produce the actual utterance text.
6 Discussion and Related Work
Existing tutorial dialogue systems most often rely
on one of two approaches for interpretation: they
either use wide coverage but shallow language
9 KRAQ06
A lightbulb can be on, off or broken.
A working lightbulb is on if it is in a complete path with a charged battery.
The complete paths of a component are those which are valid, closed, and complete.
A path is complete if it is a closed path with at least one battery and at least...
A path is closed if it is a valid path, a sequence of more than two terminals, ...
A path is valid if it is a single sequence with more than one terminal, all ...
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is valid.
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is closed.
... 6 lines showing that the path contains both L1-13-1-1 and B1-13-1-1 ...
The path (:seq t1-13-1-3 t1-13-1-2 t1-13-1-1 t1-13-1-4) is complete.
L1-13-1-1 is in a complete path with B1-13-1-1.
A battery is charged unless it is damaged.
B1-13-1-1 is charged.
L1-13-1-1 is on.
Figure 4: Untailored text produced by appending strings from production rules.
interpretation techniques (e.g. LSA (Person et
al., 2000), finite-state parsing (Glass, 2001)) in
combination with shallow knowledge represen-
tations (tutorial scripts or FSA-based knowledge
construction dialogues), or they use deep KR&R
systems but with highly domain-specific parsing
and semantic interpretation (e.g. ATLAS-ANDES
(Rose? et al, 2001), PACT (Aleven et al, 2002)).
The Why2-Atlas system (VanLehn et al, 2002)
makes progress on combining wide coverage in-
terpretation with deep knowledge representation
by utilizing a wide-coverage syntactic grammar
(Rose?, 2000) and a theorem prover to interpret stu-
dent essays (Makatchev et al, 2004). However,
once the misconceptions are diagnosed, the reme-
diation is done via KCDs, with very limited lan-
guage input and pre-authored responses, and with-
out allowing students to ask questions. Our ap-
proach attempts to address issues which arise in
making remediation more flexible and dependent
on context, while still relying on wide-coverage
language interpretation and generation.
The issues we encountered in integrating com-
positional interpretation and reference resolution
with efficient knowledge representation is simi-
lar to a known problem in natural language inter-
faces to databases which may contain slots with
complex meanings. (Stallard, 1986) solves this
problem by providing inference schemas linking
complex-valued slots with compositional repre-
sentations. Our solution in mapping domain-
independent to domain-specific representation is
similar, but stricter compositionality is needed for
reference resolution support, placing additional
constraints on knowledge engineering as we dis-
cussed in Section 4.
We glossed over the interpretation issues related
to metonymy and other imprecise formulations in
questions (Aleven et al, 2002). A taxonomy of
imprecise manual question encodings by domain
experts is presented in (Fan and Porter, 2004).
They also propose an algorithm to address loosely
encoded questions using ontological knowledge.
This algorithm in effect performs question inter-
pretation, and we are planning to incorporate it
into our interpretation mechanism to help inter-
pret question representations obtained automati-
cally during language interpretation.
Text generation of the type that can handle the
necessary linguistic phenomena needed have not
been implemented in tutoring systems that use di-
alogue. The DIAG-NLP tutorial dialogue sys-
tem (Eugenio et al, 2005) shows that structured
explanations from deep generation supported by
knowledge representation and reasoning improve
learning. However, it does not engage in a dia-
logue with the user, and in this paper we showed
that explanations need to be further adjusted in di-
alogue based on previous student responses and
knowledge. Deep generation using context has
been used in some other types of dialogue sys-
tems such as collaborative problem solving (Stent,
2001), and we expect that the approaches used in
content selection and planning in those systems
will also transfer to our deep generation system.
7 Conclusions
We discussed the implementation of a tutorial di-
alogue system which relies on a domain knowl-
edge representation to verify student answers and
offer appropriate explanations. Integration with
domain-independent interpretation and generation
components places additional requirements on
knowledge representation, and we showed how
an existing knowledge representation mechanisms
used in answering exam questions can be adapted
to the more complex task of tutoring, including in-
terpreting student explanations and generating ap-
10 KRAQ06
propriate feedback.
Acknowledgments
This material is based upon work supported by a
grant from The Office of Naval Research number
N000149910165.
References
V. Aleven, O. Popescu, and K. Koedinger. 2002. Pilot-
testing a tutorial dialogue system that supports self-
explanation. Lecture Notes in Computer Science,
2363.
J. R. Anderson, A. T. Corbett, K. R. Koedinger, and
R. Pelletier. 1995. Cognitive tutors: Lessons
learned. The Journal of the Learning Sciences,
4(2):167?207.
M. Arnold and R. Millar. 1987. Being constructive:
An alternative approach to the teaching of introduc-
tory ideas in electricity. International Journal of
Science Education, 9:553?563.
K. Barker, V. K. Chaudhri, S. Y. Chaw, P. Clark, J. Fan,
D. Israel, S. Mishra, B. W. Porter, P. Romero, D.
Tecuci, and P. Z. Yeh. 2004. A question-answering
system for AP chemistry: Assessing KR&R tech-
nologies. In KR, pages 488?497.
B. S. Bloom. 1984. The two sigma problem: The
search for methods of group instruction as effec-
tive as one-to-one tutoring. Educational Researcher,
13:3?16.
Johan Bos. 2004. Computational semantics in dis-
course: Underspecification, resolution, and infer-
ence. Journal of Logic, Language and Information,
13(2):139?157.
Johan Bos. 2005. Towards wide-coverage semantic
interpretation. In Proceedings of Sixth International
Workshop on Computational Semantics (IWCS-6).
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway and James C. Lester. 2002.
Narrative prose generation. Artificial Intelligence,
139(2):213?252, August.
P. Clark and B. Porter, 1999. KM (1.4): Users Manual.
http://www.cs.utexas.edu/users/mfkb/km.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2003.
Integrating linguistic and domain knowledge for
spoken dialogue systems in multiple domains. In
Proceedings of IJCAI-03 Workshop on Knowledge
and Reasoning in Practical Dialogue Systems.
M. O. Dzikovska. 2004. A Practical Semantic Rep-
resentation For Natural Language Parsing. Ph.D.
thesis, University of Rochester.
B. Di Eugenio, D. Fossati, D. Yu, S. Haller, and
M. Glass. 2005. Natural language generation for in-
telligent tutoring systems: A case study. In 12th In-
ternational Conference on Artificial Intelligence in
Education, pages 217?224.
J. Fan and B. W. Porter. 2004. Interpreting loosely en-
coded questions. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence, Six-
teenth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 399?405.
N. S. Friedland, P. G. Allen, M. J. Witbrock, G.
Matthews, N. Salay, P. Miraglia, J. Angele, S.
Staab, D. Israel, V. K. Chaudhri, B. W. Porter, K.
Barker, and P. Clark. 2004. Towards a quanti-
tative, platform-independent analysis of knowledge
systems. In KR, pages 507?515.
M. Glass. 2001. Processing language input in the
CIRCSIM-tutor intelligent tutoring system. In J.
Moore, C. L. Redfield, and W. L. Johnson, editors,
Artificial Intelligence in Education. IOS press.
M. Makatchev, P. W. Jordan, and K. VanLehn. 2004.
Abductive theorem proving for analyzing student
explanations to guide feedback in intelligent tutor-
ing systems. J. Autom. Reasoning, 32(3):187?226.
N. Person, A.C. Graesser, D. Harter, and E. Math-
ews. 2000. Dialog move generation and conversa-
tion management in autotutor. In Workshop Notes of
the AAAI ?00 Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. Rose?, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive concep-
tual tutoring in atlas-andes. In Proceedings of AI in
Education 2001 Conference.
C. Rose?. 2000. A framework for robust semantic inter-
pretation. In Proceedings 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics.
P. S. Schaffer and L. C. McDermott. 1992. Research
as a guide for curriculum development: An example
from introductory electricity. part ii: Design of in-
structional strategies. American Journal of Physics,
60(11):1003?1013.
D. G. Stallard. 1986. A terminological simplifica-
tion transformation for natural language question-
answering systems. In ACL Proceedings, 24th An-
nual Meeting, pages 241?246.
A. Stent. 2001. Dialogue Systems as Conversational
Partners: Applying conversation acts theory to nat-
ural language generation for task-oriented mixed-
initiative spoken dialogue. Ph.D. thesis, University
of Rochester, Rochester, NY, August.
K. VanLehn, P. Jordan, C. P. Rose?, and The Natural
Language Tutoring Group. 2002. The architecture
of why2-atlas: a coach for qualitative physics essay
writing. In Proceedings of Intelligent Tutoring Sys-
tems Conference.
11 KRAQ06
Tools for hierarchical annotation of typed dialogue
Myroslava O. Dzikovska, Charles Callaway, Elaine Farrow
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, United Kingdom,
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
1 Introduction
We discuss a set of tools for annotating a complex
hierarchical and linguistic structure of tutorial di-
alogue based on the NITE XML Toolkit (NXT)
(Carletta et al, 2003). The NXT API supports
multi-layered stand-off data annotation and syn-
chronisation with timed and speech data. Using
NXT, we built a set of extensible tools for de-
tailed structure annotation of typed tutorial dia-
logue, collected from a tutor and student typing
via a chat interface. There are several corpora of
tutoring done with such chat-style communication
techniques (Shah et al, 2002; Jordan and Siler,
2002), however, our annotation presents a special
problem because of its detailed hierarchical struc-
ture. We applied our annotation methodology to
annotating corpora in two different tutoring do-
mains: basic electricity and electronics, and sym-
bolic differentiation.
2 Data Structures
Our corpus has two sources of overlapping anno-
tations: the turn structure of the corpus and situ-
ational factors annotation. The data are naturally
split into turns whenever a participant presses their
?submit? button. Timing information is associated
with individual turns, representing the time when
the entire message was sent to the other partici-
pant, rather than with individual words and sounds
as it would be in spoken corpora.
However, turns are too large to be used as units
in the annotation for dialogue phenomena. For
example, the single turn ?Well done. Let?s try a
harder one.? consists of two utterances making
different dialogue contributions: positive tutorial
feedback for the previous student utterance and a
statement of a new tutorial goal. Thus, turns must
be segmented into smaller units which can serve
as a basis for dialogue annotation. We call these
utterances by analogy with spoken language, be-
cause they are often fragments such as ?well done?
rather than complete sentences.
Thus, the corpus has two inherently overlap-
ping layers: the turn segmentation layer, grouping
utterances into turns, and the dialogue structure
layer built up over individual utterances. The NXT
toolkit supports such overlapping annotations, and
we built two individual tools to support corpus an-
notation: an utterance segmentation tool and a tu-
torial annotation tool.
Additionally, the corpus contains annotation
done by the tutor herself at collection time which
we call ?situational factors?. The tutors were
asked to submit a set of these factors after each
turn describing the progress and state of the stu-
dent, such as answer correctness, confidence and
engagement. The factors were submitted sepa-
rately from dialogue contributions and provide an-
other layer of dialogue annotation which has to
be coordinated with other annotations. The fac-
tors are typically related to the preceding student?s
utterance, but the link is implicit in the submis-
sion time.1 Currently we include the factors in the
tool?s transcript display based on the submission
time, so they are displayed after the appropriate
turn in the transcript allowing the annotators to vi-
sually synchronise them with the dialogue. We
also provide an option to annotators for making
them visible or not. In the future we plan to make
factors a separate layer of the annotation linked by
pointers with the preceding student and tutor turns.
1The factor interface was designed to be quick to use and
minimally impact the dialogue flow, so the submission tim-
ings are generally reliable.
57
3 Utterance Segmentation
We process the raw data with an automatic seg-
menter/tokenizer which subdivides turns into indi-
vidual utterances, and utterances into tokens, pro-
viding an initial segmentation for the annotation.
However, perfect automatic segmentation is not
possible, because punctuation is often either in-
consistent or missing in typed dialogue and this
task therefore requires human judgement. The
output of our automatic segmentation algorithm
was verified and corrected by a human annotator.
A screen-shot of the interface we developed for
segmentation verification is displayed in Figure 1.
With the aid of this tool, it took 6 person-hours
to check and correct the automatically segmented
utterances for the 18 dialogues in our corpus.
4 Tutorial Annotation
To provide a detailed analysis of tutorial dialogue
and remediation strategies, we employ a hierarchi-
cal annotation scheme which encodes the recur-
sive dialogue structure. Each tutorial session con-
sists of a sequence of tasks, which may be either
teaching specific domain concepts or doing indi-
vidual exercises. Each task?s structure includes
one or more of the following: giving definitions,
formulating a question, obtaining the student an-
swer and remediation by the tutor.
Generally speaking, the structure of tutorial di-
alogue is governed by the task structure just as in
task-oriented dialogue (Grosz and Sidner, 1986).
However, the specific annotation structure differs
depending on the tutoring method. In our basic
electricity and electronics domain, a tutorial ses-
sion consists of a set of ?teach? segments, and
within each segment a number of ?task? segments.
Task segments usually contain exercises in which
the student is asked a question requiring a simple
(one- or two-line) answer, which may be followed
by a long remediation segment to address the con-
ceptual problems revealed by the answer.
In contrast, in our calculus domain the students
have to do multi-step procedures to differentiate
complex math expressions, but most of the reme-
diations are very short, fixing the immediate prob-
lem and letting the student continue on with the
procedure. Thus even though the dialogue is hier-
archically structured in both cases, the annotation
schemes differ depending on the domain. We de-
veloped a generic tool for annotating hierarchical
dialogue structure which can be configured with
the specific annotation scheme.
The tool interface (Figure 2) consists of a tran-
script of a session and a linked tree representation.
Individual utterances displayed in the transcript
are leaves of the tree. It is not possible to display
them as tree leaves directly as would be done in
syntactic trees, because they are too large to fit in
graphical tree display. Instead, a segment is high-
lighted in a transcript whenever it is selected in the
tutorial structure, and a hotkey is provided to ex-
pand the tree to see all annotations of a particular
utterance in the transcript.
The hierarchical tree structure is supported by a
schema which describes the annotations possible
on each hierarchical tree level. Since the multi-
layered annotation scheme is quite complex, the
tool uses the annotation schema to limit the num-
ber of codes presented to the annotator to be only
those consistent with the tree level. For exam-
ple, in our basic electricity domain annotation de-
scribed above, there are about 20 codes at different
level, but an annotator will only have ?teach? as an
option for assigning a code to a top tree level, and
only ?task? and ?test? (with appropriate subtypes)
for assigning codes immediately below the teach
level, based on the schema defined for the domain.
5 Transcript Segmentation
We had to conduct several simpler data analy-
ses where the utterances in the transcript are seg-
mented according to their purpose. For exam-
ple, in tutorial differentiation the dialogue con-
centrates on 4 main purposes: general discussion,
introducing problems, performing differentiation
proper, or doing algebraic transformations to sim-
plify the resulting expressions. In another analysis
we needed to mark the segments where the student
was making errors and the nature of those errors.
We developed a generic annotation tool to sup-
port such segmentation annotation over the utter-
ance layer. The tool is configured with the name
of the segment tag and colours indicating different
segment types. The annotator can enter a segment
type, and use a freetext field for other information.
A screenshot of the annotation tool with utterance
purposes marked is given in Figure 3.
6 Data Analysis
The NITE query language (NQL) enables us to ac-
cess the data as a directed acyclic graph to cor-
relate simple annotations, such as finding out the
58
Figure 1: Utterance Segmentation Tool.
Figure 2: Tutorial Strategy Annotation Tool.
59
Figure 3: Segmentation tool. The segment labels are shown on the left.
number of turns which contain only mathematical
expressions but no words. We use the NITE query
interface for simpler analysis tasks such as finding
all instances of specific tags and tag combinations.
However, we found the query language less use-
ful for coordinating the situational factors anno-
tated by tutors with other annotation. Each set of
factors submitted is normally associated with the
first student turn which precedes it, but the factors
were not linked to student utterances explicitly.
NQL does not have a ?direct precedence? opera-
tor.2 Thus it is easier derive this information using
the JAVA API. To make the data analysis simpler,
we are planning to add a pointer layer, generated
automatically based on timing information, which
will use explicit pointers between the factor sub-
missions and preceding tutor and student turns.
7 Conclusions
We presented a set of tools for hierarchically an-
notating dialogue structure, suitable for annotating
typed dialogue. The turns in these dialogues are
complex and overlap with dialogue structure, and
our toolset supports segmenting turns into smaller
2It?s possible to express the query in NQL us-
ing its precedence operator ?? as ?($f factor)
($u utterance) (forall $u1 utterance) :
(($f  $u) && ($f  u1)) ? (u  u1)?.
However, this is very inefficient since it must check all
utterance pairs in the corpus to determine direct precedence,
especially if it needs to be included as part of a bigger query.
utterance units and annotating hierarchical dia-
logue structure over the utterances, as well as pro-
viding simpler segmentation annotation.
Acknowledgements
This material is based upon work supported by a
grant from The Office of Naval Research num-
ber N000149910165 and European Union 6th
framework programme grant EC-FP6-2002-IST-
1-507826 (LeActiveMath).
References
Jean Carletta, J. Kilgour, T. O?Donnell, S. Evert, and
H. Voormann. 2003. The NITE object model li-
brary for handling structured linguistic annotation
on multimodal data sets. In Proceedings of the
EACL Workshop on Language Technology and the
Semantic Web.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175?204.
Pamela Jordan and Stephanie Siler. 2002. Student
initiative and questioning strategies in computer-
mediated human tutoring dialogues. In Proceedings
of ITS 2002 Workshop on Empirical Methods for Tu-
torial Dialogue Systems.
Farhana Shah, Martha W. Evens, Joel Michael, and
Allen Rovick. 2002. Classifying student initiatives
and tutor responses in human keyboard-to-keyboard
tutoring sessions. Discourse Processes, 33(1).
60
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 5?13,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Context-Dependent Regression Testing for Natural Language Processing
Elaine Farrow
Human Communication Research Centre
School of Informatics
University of Edinburgh
Edinburgh, UK
Elaine.Farrow@ed.ac.uk
Myroslava O. Dzikovska
Human Communication Research Centre
School of Informatics
University of Edinburgh
Edinburgh, UK
M.Dzikovska@ed.ac.uk
Abstract
Regression testing of natural language sys-
tems is problematic for two main reasons:
component input and output is complex, and
system behaviour is context-dependent. We
have developed a generic approach which
solves both of these issues. We describe our
regression tool, CONTEST, which supports
context-dependent testing of dialogue system
components, and discuss the regression test
sets we developed, designed to effectively iso-
late components from changes and problems
earlier in the pipeline. We believe that the
same approach can be used in regression test-
ing for other dialogue systems, as well as in
testing any complex NLP system containing
multiple components.
1 Introduction
Natural language processing systems, and dialogue
systems in particular, often consist of large sets of
components operating as a pipeline, including pars-
ing, semantic interpretation, dialogue management,
planning, and generation. Testing such a system can
be a difficult task for several reasons. First, the com-
ponent output may be context-dependent. This is
particularly true for a dialogue system ? reference
resolution, ellipsis, and sometimes generation typi-
cally have to query the system state to produce their
output, which depends both on the state of the world
(propositions defined in a knowledge base) and on
the dialogue history (object salience). Under these
conditions, unit testing using the input and output of
a single component in isolation is of limited value
? the entire system state needs to be preserved to
check that context-dependent components are func-
tioning as expected.
Second, the inputs and outputs of most system
components are usually very complex and often
change over time as the system develops. When
two complex representations are compared it may
be difficult to determine what impact any change is
likely to have on system performance (far-reaching
or relatively trivial). Further, if we test components
in isolation by saving their inputs, and these inputs
are reasonably complex, then it will become difficult
to maintain the test sets for the components further
along the pipeline (such as diagnosis and generation)
as the output of the earlier components changes dur-
ing development.
The simplest way to deal with both of these is-
sues would be to save a set of test dialogues as a
gold standard, checking that the final system out-
put is correct given the system input. However, this
presents another problem. If a single component
(generation, for example) malfunctions, it becomes
impossible to verify that a component earlier in the
pipeline (for example, reference resolution) is work-
ing properly. In principle we could also save the
messages passing between components and compare
their content, but then we are faced again with the
problems arising from the complexity of component
input and output which we described above.
To solve these problems, we developed a regres-
sion tool called CONTEST (for CONtext-dependent
TESTing). CONTEST allows the authors of individ-
ual system components to control what information
to record for regression testing. Test dialogues are
5
saved and replayed through the system, and individ-
ual components are tested by comparing only their
specific regression output, ignoring the outputs gen-
erated by other components. The components are
isolated by maintaining a minimal set of inputs that
are guaranteed to be processed correctly.
To deal with issues of output complexity we ex-
tend the approach of de Paiva and King (2008) for
testing a deep parser. They created test sets at dif-
ferent levels of granularity, some including detailed
representations, but some just saving very simple
output of a textual entailment component. They
showed that, given a carefully selected test set, test-
ing on the final system output can be a fast and effec-
tive way to discover problems in the interpretation
pipeline.
We show how the same idea can be used to test
other dialogue system components as well. We de-
scribe the design of three different test sets that
effectively isolate the interpretation, tutorial plan-
ning and generation components of our system. Us-
ing CONTEST allows us to detect system errors and
maintain consistent test sets even as the underlying
representations change, and gives us much greater
confidence that the results of our testing are relevant
to the performance of the system with real users.
The rest of this paper is organised as follows. In
Section 2 we describe our system and its compo-
nents in more detail. The design of the CONTEST
tool and the test sets are described in Sections 3 and
4. Finally, in Section 5 we discuss how the inter-
active nature of the dialogue influences the design
of the test sets and the process of verifying the an-
swers; and we discuss features that we would like to
implement in the future.
2 Background
This work has been carried out to support the devel-
opment of BEETLE (Callaway et al, 2007), a tuto-
rial dialogue system for basic electricity and elec-
tronics. The goal of the BEETLE system is to teach
conceptual knowledge using natural language dia-
logue. Students interact with the system through a
graphical user interface (GUI) which includes a chat
interface,1 a window to browse through slides con-
1The student input is currently typed to avoid issues with
automated speech recognition of complex utterances.
taining reading material and diagrams, and an inter-
face to a circuit simulator where students can build
and manipulate circuits.
The system consists of twelve components alto-
gether, including a knowledge base representing the
state of the world, a curriculum planner responsible
for the lesson structure, and dialogue management
and NLP components. We developed CONTEST so
that it could be used to test any system component,
though our testing focuses on the natural language
understanding and generation components.2
BEETLE uses a standard natural language pro-
cessing pipeline, starting with a parser, lexical in-
terpreter, and dialogue manager. The dialogue man-
ager handles all input from the GUI (text, button
presses and circuits) and also supports generic di-
alogue processing, such as dealing with interpreta-
tion failures and moving the lesson along. Student
answers are processed by the diagnosis and tuto-
rial planning components (discussed below), which
function similarly to planning and execution com-
ponents in task oriented dialogue systems. Finally,
a generation subsystem converts the semantic repre-
sentations output by the tutorial planner into the final
text to be presented to the student.
The components communicate with each other
using the Open Agent Architecture (Martin et al,
1998). CONTEST is implemented as an OAA agent,
accepting requests to record messages. However,
OAA is not essential for the system design ? any
communication architecture which supports adding
extra agents into a system would work equally well.
BEETLE aims to get students to support their rea-
soning using natural language, since explanations
and contentful talk are associated with learning gain
(Purandare and Litman, 2008). This requires de-
tailed analyses of student answers in terms of cor-
rect, incorrect and missing parts (Dzikovska et al,
2008; Nielsen et al, 2008). Thus, we use the TRIPS
parser (Allen et al, 2007), a deep parser which pro-
duces detailed analyses of student input. The lexical
interpreter extracts a list of objects and relationships
mentioned, which are checked against the expected
answer. These lists are fairly long ? many expected
answers have ten or more relations in them. The
2All our components are rule-based, but we expect the same
approach would work for components of a statistical nature.
6
diagnoser categorises each of the objects and rela-
tionships as correct, contradictory or irrelevant. The
tutorial planner makes decisions about the remedi-
ation strategy, choosing one strategy from a set of
about thirteen depending on the question type and
tutorial context. Finally, the generation system uses
the FUF/SURGE (Elhadad, 1991) deep generator to
generate feedback automatically.
Obviously, the output from the deep parser and
the input to the tutorial planner and generator are
quite complex, giving rise to the types of problems
that we discussed in the introduction. We already
had a tool for unit-testing the parser output (Swift et
al., 2004), plus some separate tools to test the diag-
noser and the generation component, but the com-
plexity of the representations made it impractical to
maintain large test sets that depended on such com-
plex inputs and outputs. We also wanted a unified
way to test all the components in the context of the
entire system. This led to the creation of CONTEST,
which we describe in the rest of the paper.
3 The CONTEST Tool
Figure 1: The regression testing process.
In this section we describe the process for creat-
ing and using test cases, illustrated in Figure 1. The
first step in building a useful regression tool is to
make it possible to run the same dialogue through
the system many times without retyping the student
answers. We added a wrapper around the GUI to in-
tercept and record the user actions and system calls
for later playback, thus creating a complete record
of the session. Every time our system runs, a new
saved session file is automatically created and saved
in a standard location. This file forms the basis for
our test cases. It uses an XML format, which is
human-readable and hand-editable, easily extensible
and amenable to automatic processing. A (slightly
simplified) example of a saved session file is shown
in Figure 2. Here we can see that a slide was dis-
played, the tutor asked the question ?Which compo-
nents (if any) are in a closed path in circuit 1?? and
the student answered ?the battery and the lightbulb?.
Creating a new test case is then a simple matter of
starting the system and performing the desired ac-
tions, such as entering text and building circuits in
the circuit simulator. If the system is behaving as it
should, the saved session file can be used directly as
a test case. If the system output is not as desired, the
file can be edited in any text editor.
Of course, this only allows the final output of the
system to be tested, and we have already discussed
the shortcomings of such an approach: if a com-
ponent late in the pipeline has problems, there is
no way to tell if earlier components behaved as ex-
pected. To remedy this, we added a mechanism for
components other than the GUI to record their own
information in the saved session file.
Components can be tested in effective isolation by
combining two mechanisms: carefully designed test
sets which focus on a single component and (impor-
tantly) are expected to succeed even if some other
component is having problems; and a regression tool
which allows us to test the output of an individual
component. Our test sets are discussed in detail in
Section 4. The remainder of this section describes
the design of the tool.
CONTEST reads in a saved session file and re-
produces the user actions (such as typing answers
or building circuits), producing a new saved ses-
sion file as its output. If there have been changes to
the system since the test was created, replaying the
same actions may lead to new slides and tutor mes-
sages being displayed, and different recorded output
from intermediate components. For example, given
the same student answers, the diagnosis may have
changed, leading to different tutor feedback. We
compare the newly generated output file against the
input file. If there are no differences, the test is con-
sidered to have passed. As the input and output files
7
<test>
<action agent="tutor" method="showSlide">
lesson1-oe/exercise/img1.html
</action>
<action agent="tutor" method="showOutput">
Which components (if any) are in a closed path in circuit 1?
</action>
<action agent="student" method="submitText">
the battery and the lightbulb
</action>
</test>
Figure 2: A saved session file showing a single interaction between tutor and student.
are identical in format, the comparison can be done
using a ?diff? command.
With each component recording its own output, it
can be the case that there are many differences be-
tween old and new files. It is therefore important to
be able to choose the level of detail we want when
comparing saved session files, so that the output of
a single component can be checked independently
of other system behaviour. We solved this problem
by creating a set of standard XSLT filters. One fil-
ter picks out just the dialogue between student and
tutor to produce a transcript of the session. Other
filters select the output from one particular compo-
nent, for example the tutorial planner, with the tutor
questions included to provide context. In general,
we wrote one filter for each component.
CONTEST creates a test report by comparing the
expected and actual outputs of the system on each
test run. We specify which filter to use (based on
which component we are testing). If the test fails,
we can examine the relevant differences using the
?ediff? mode in the emacs text editor. More sophis-
ticated approaches are possible, such as using a fur-
ther XSL transform to count all the errors of a partic-
ular type, but we have found ediff to be good enough
for our purposes. With the filters in place we only
see the differences for the component we are testing.
Since component regression output is designed to be
small and human-readable, checking the differences
is a very quick process.
Test cases can be run individually or in groups.3
3Test cases are usually grouped by directory, but symbolic
links allow us to use the same case in several groups.
Using CONTEST, we can create a single report for a
group of test cases: the individual outputs are com-
bined to create a new output file for the group and
this is compared to the (combined) input file, with
filters applied in the usual way. This is a very use-
ful feature, allowing us to create a report for all the
?good answer? cases (for example) in one step.
Differences do not always indicate errors; some-
times they are simply changes or additions to the
recorded information. After satisfying ourselves that
the reported differences are intentional changes, we
can update the test cases to reflect the output of the
latest run. Subsequent runs will test against the new
behaviour. CONTEST includes an update tool which
can update a group of cases with a single command.
This is simpler and less error-prone than editing po-
tentially hundreds of files by hand.
4 Test Cases
We have built several test sets for each component,
amounting to more than 400 individual test cases.
We describe examples of the test sets for three of our
components in more detail below, to demonstrate
how we use CONTEST.
4.1 Interpretation Test Cases
We have a test set consisting of ?good answers? for
each of the questions in our system which we use to
test the interpretation component. The regression in-
formation recorded by the interpretation component
includes the internal ID code of the matched answer
and a code indicating whether it is a ?best?, ?good? or
?minimal? answer. This is enough to allow us to de-
8
<test name="closed_path_discussion">
<action agent="tutor" method="showOutput">
What are the conditions that are required to make a bulb light up?
</action>
<action agent="student" method="submitText">
a bulb must be in a closed path with the battery
</action>
<action agent="simpleDiagnosis" method="logForRegression">
student-act: answer atype: diagnosis consistency: []
code: complete subcode: best
answer_id: conditions_for_bulb_to_light_ans1
</action>
</test>
Figure 3: A sample test case from our ?good answers? set showing the diagnosis produced for the student?s answer.
tect many possible errors in interpretation. We can
run this test set after every change to the parsing or
interpretation components.
A (slightly simplified) example of our XML test
case format is shown in Figure 3, with the tutor ques-
tion ?What are the conditions that are required to
make a bulb light up?? and the student answer ?a
bulb must be in a closed path with the battery?. The
answer diagnosis shows that the system recognised
that the student was attempting to answer the ques-
tion (rather than asking for help), that the answer
match was complete, with no missing or incorrect
parts, and the answer was consistent with the state of
the world as perceived by the system.4 The matched
answer is marked as the best one for that question.
While the recorded information does not supply
the full interpretation, it can suggest the source of
various possible errors. If interpretation fails, the
student act will be set to uninterpretable,
and the code will correspond to the reason
for failed interpretation: unknown input
if the parse failed, unknown mapping or
restriction failure if lexical interpretation
failed, and unresolvable if reference resolution
failed. If interpretation worked, but took incorrect
scoping or attachment decisions, the resulting
proposition is likely to be inconsistent with the
4Sometimes students are unable to interpret diagrams, or
are lacking essential background knowledge, and therefore say
things that contradict the information in the domain model. The
system detects and remediates such cases differently from gen-
eral errors in explanations (Dzikovska et al, 2006).
current knowledge base, and an inconsistency code
will be reported. In addition, verifying the matched
answer ID provides some information in case only
a partial interpretation was produced. Sometimes
different answer IDs correspond to answers that are
very complete versus answers that are acceptable
because they address the key point of the question,
but miss some small details. Thus if a different
answer ID has matched, it indicates that some
information was probably lost in interpretation.
The codes we report were not devised specifically
for the regression tests. They are used internally to
allow the system to produce accurate feedback about
misunderstandings. However, because they indicate
where the error is likely to originate (parsing, lexi-
cal interpretation, scoping and disambiguation), they
can help us to track it down.
We have another test set for ?special cases?, such
as the student requesting a hint or giving up. An ex-
ample is shown in Figure 4. Here the student gives
up completely on the first question, then asks for
help with the second. We use this test case to check
that the set phrases ?I give up? and ?help? are un-
derstood by the system. The ?special cases? test set
includes a variety of help request phrasings observed
in the corpora we collected. Note that this example
was recorded while using a tutorial policy that re-
sponds to help requests by simply providing the an-
swer. This does not matter for testing interpretation,
since the information recorded in the test case will
distinguish help requests from give ups, regardless
9
T: Which components (if any) are in a closed path
in circuit 1?
S: I give up
T: The answer is the battery and the bulb in 1.
T: Which components (if any) are in a closed path
in circuit 2?
S: help
T: Here?s the answer. The bulb in 2.
Figure 4: The transcript of a test case for ?I give up? and
?help?. T: is the tutor, S: is the student.
of the specific tutorial policy used by the system.
Finally, we have a test set for detection of un-
interpretable utterances. In a tutoring system, stu-
dents often use incorrect terminology or unex-
pected phrasings which the system cannot under-
stand. While we expect coverage and robustness of
parsing and interpretation components to improve
as the system develops, some percentage of errors
is to be expected simply because even state-of-the-
art parsers are not 100% accurate. Thus the sys-
tem needs to detect problems whenever possible,
and produce useful help messages. We have col-
lected a set of student utterances that correspond
to typical interpretation problems ? failed parses,
unknown words, sentences that are parsed but the
parser makes an incorrect attachment decision, sen-
tences where the student used terminology incor-
rectly (violating sortal constraints encoded in the
knowledge base), a pronominal referent that could
not be resolved. Identifying the problem type cor-
rectly is important because it affects later processing
in the dialogue manager and tutorial planner. Thus
we have a set of test cases using the collected sen-
tences, which we use to ensure that the problems are
flagged correctly.
4.2 Tutoring Test Cases
The BEETLE tutorial planner makes decisions on
system behaviour and corresponds to planning and
execution components in other dialogue systems.
Here, we are attempting to test that the tutorial plan-
ner selected the right system action. Therefore, since
we are not testing the interpretation component, we
don?t vary the form of the language we use. Instead,
T: Predict the state (on, off) of each bulb in dia-
gram 1.
S: all bulbs are off
T: OK, we?ll check your prediction in a moment.
T: Predict the state (on, off) of each bulb in dia-
gram 2.
S: all bulbs are off
T: OK, we?ll check your prediction in a moment.
T: Test your prediction for diagram 1 by opening
both switches and observing the bulbs.
T: Test your prediction for diagram 2 by closing
both switches and observing the bulbs.
T: Did the bulbs behave the way you expected?
S: no
T: Right. In your second prediction, you said that
Bulb A, Bulb B and Bulb C would be off. But
Bulb A, Bulb B and Bulb C are lit.
Figure 5: A tutorial planner test case transcript. T: is the
tutor, S: is the student.
we vary the combination of good and bad student an-
swers, and record the action chosen by the system.
The tutorial planner chooses feedback in 2 stages.
First, a general algorithm is chosen depending on the
exercise type and student input type: there are sepa-
rate algorithms for addressing, for example, what to
do if the student input was not interpreted, and for
correct and incorrect answers. Choosing the algo-
rithm requires some computation depending on the
question context. Once the main algorithm is cho-
sen, different tutorial strategies can be selected, and
this is reflected in the regression output: the system
records a keyword corresponding to the chosen algo-
rithm, and then the name of the strategy along with
key strategy parameters.
For example, Figure 5 shows the transcript from
a test case for a common exercise type from our
lessons: a so called predict-verify-evaluate se-
quence. In this example, the student is asked to
predict the behaviour of three light bulbs in a cir-
cuit, test it by manipulating the circuit in the simu-
lation environment, and then evaluate whether their
predictions matched the circuit behaviour. The sys-
tem reinforces the point of the exercise by producing
a summary of discrepancies between the student?s
10
<action agent="tutor" method="showOutput">
Did the bulbs behave the way you expected?
</action>
<action agent="student" method="submitText">
no
</action>
<action agent="tc-bee" method="logForRegression">
EVALUATE (INCORRECT-PREDICTION NO_NO)
</action>
Figure 6: An excerpt from a tutorial planner test case showing the recorded summary output.
predictions and the observed outcomes.
An excerpt from the corresponding test case is
shown in Figure 6. Here we can see the tutor ask
the evaluation question ?Did the bulbs behave the
way you expected?? and the student answer ?no?.
The EVALUATE algorithm was chosen to handle the
student answer, and from the set of available strate-
gies the INCORRECT-PREDICTION strategy was
chosen. That strategy takes a parameter indicating
if there was a discrepancy when the student evalu-
ated the results (here NO NO, corresponding to the
expected and actual evaluation result inputs).
In contrast, in the first example in Figure 4, where
the student gives up and doesn?t provide an an-
swer, the tutorial planner output is REMEDIATE
(BOTTOM-OUT Q IDENTIFY). This shows that
the system has chosen to use a REMEDIATE algo-
rithm, and a ?bottom-out? (giving away the answer)
strategy for remediation. The strategy parameter
Q IDENTIFY (which depends on the question type)
determines the phrasing to be used in the generator
to verbalise the tutor?s feedback.
The saved output allows us to see that the cor-
rect algorithm was chosen to handle the student in-
put (for example, that the REMEDIATE algorithm
is correctly chosen after an incorrect student answer
to an explanation question), and that the algorithm
chooses a strategy appropriate for the tutorial con-
text. Certain errors can still go undetected here, for
example, if the algorithm for verbalising the chosen
strategy in the generator is broken. Developing sum-
mary inputs to detect such errors is part of planned
future work.
In order to isolate the tutorial planner from inter-
pretation, we use standard fixed phrasings for stu-
dent answers. The answer phrasings in the ?good
answers? test set for interpretation (described in Sec-
tion 4.1) are guaranteed to be understood correctly,
so we use only these phrasings in our tutorial planner
test cases. Thus, we are able to construct tests which
will not be affected by problems in the interpretation
pipeline.
4.3 Generation Test Cases
To test generation, we have a set of test cases where
the student immediately says ?I give up? in response
to each question. This phrase is used in our system
to prevent the students getting stuck ? the tutorial
policy is to immediately stop and give the answer to
the question. The answers given are generated by
a deep generator from internal semantic representa-
tions, so this test set gives us the assurance that all
relevant domain content is being generated properly.
This is not a complete test for the generation capabil-
ities of our system, since each explanation question
can have several possible answers of varying degrees
of quality (suggested by experienced human tutors
(Dzikovska et al, 2008)), and we always choose
the best possible answer when the student gives up.
However, it gives us confidence that the student can
give up at any point and receive an answer which can
be used as a template for future answers.
5 Discussion and Future Work
We have created more than 400 individual test cases
so far. There are more than 50 for the interpretation
component, more than 150 for the tutorial planner
and more than 200 for the generation component.
We are developing new test sets based on other sce-
narios, such as responding to each question with a
11
help request. We are also refining the summary in-
formation recorded by each component.
An important feature of our testing approach is
the use of short summaries rather than the inter-
nal representations of component inputs and outputs.
Well-designed summaries provide key information
in an easy-to-read format that can remain constant as
internal formats change and develop over time. We
believe that this approach would be useful for other
language processing systems, since at present there
are few standardised formats in the community and
representations are typically developed and refined
together with the algorithms that use them.
The decision about what information to include in
the summary is vital to the success and overall use-
fulness of the regression tool. If too much detail is
recorded, there will be many spurious changes and
it will be burdensome to keep a large regression set
updated. If too little detail is recorded, unwanted
changes in the system may go undetected. The con-
tent of the test cases we discussed in Section 4 rep-
resents our approach to such decisions.
Interpretation was perhaps the most difficult, be-
cause it has a particularly complex output. In deter-
mining the information to record, we were following
the solution of de Paiva and King (2008) who use the
decision result of the textual entailment system as a
way to efficiently test parser output. For our sys-
tem, the information output by the diagnoser about
answer correctness proved to have a similar function
? it effectively provides information about whether
the output of the interpretation component was us-
able, without the need to check details carefully.
The main challenge for our tutorial planner and
generation components (corresponding to planning
and execution components in a task-oriented dia-
logue system) was to ensure that they were suffi-
ciently isolated so as to be unaffected by errors in in-
terpretation. We achieve this by maintaining a small
set of known phrasings which are guaranteed to be
interpreted correctly; this ensures that in practice,
the downstream components are isolated from un-
wanted changes in interpretation.
Our overall methodology of recording and test-
ing summary information for individual components
can be used with any complex NLP system. The spe-
cific details of what information to record obviously
depends on the domain, but our experience suggests
some general principles. For testing the interpreta-
tion pipeline, it is useful to record pre-existing error
codes and a qualitative summary of the information
used to decide on the next system action. Where we
record the code output by the diagnoser, an informa-
tion seeking system could record, for example, the
number of slots filled and the number of items re-
trieved from a database. It is also useful to record
decisions taken by the system, or actions performed
in response to user input; so, just as we record infor-
mation about the chosen tutorial policy, other sys-
tems can record the action taken ? whether it is to
search the database, query a new slot, or confirm a
slot value.
One major improvement that we have planned for
the future is adding another layer of test case man-
agement to CONTEST, to enable us to produce sum-
maries and statistics about the total number of test
cases that have passed and failed, instead of check-
ing reports individually. Such statistics can be im-
plemented easily using another XSL transform on
top of the existing filters to count the number of
test cases with no differences and produce summary
counts of each type of error detected.
6 Conclusion
The regression tool we developed, CONTEST, solves
two of the major issues faced when testing dia-
logue systems: context-dependence of component
behaviour and complexity of component output. We
developed a generic approach based on running
saved dialogues through the system, and checking
summary information recorded by different compo-
nents against separate gold standards. We demon-
strated that test sets can be designed in such a way as
to effectively isolate downstream components from
changes and problems earlier in the pipeline. We be-
lieve that the same approach can be used in regres-
sion testing for other dialogue systems, as well as in
testing any complex NLP system containing multi-
ple components.
Acknowledgements
This work has been supported in part by Office of
Naval Research grant N000140810043. We thank
Charles Callaway for help with generation and tu-
toring tests.
12
References
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
Charles B. Callaway, Myroslava Dzikovska, Elaine Far-
row, Manuel Marques-Pita, Colin Matheson, and Jo-
hanna D. Moore. 2007. The Beetle and BeeDiff tutor-
ing systems. In Proceedings of the SLaTE-2007 Work-
shop, Farmington, Pennsylvania, USA, September.
Valeria de Paiva and Tracy Holloway King. 2008. De-
signing testsuites for grammar-based systems in appli-
cations. In Coling 2008: Proceedings of the workshop
on Grammar Engineering Across Frameworks, pages
49?56, Manchester, England, August. Coling 2008 Or-
ganizing Committee.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation in
a knowledge-based tutorial system. In Proceedings of
EACL-06 workshop on knowledge and reasoning for
language processing, Trento, Italy, April.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008. Diagnosing natural language
answers to support adaptive tutoring. In Proceed-
ings 21st International FLAIRS Conference, Coconut
Grove, Florida, May.
Michael Elhadad. 1991. FUF: The universal unifier user
manual version 5.0. Technical Report CUCS-038-91,
Dept. of Computer Science, Columbia University.
D. Martin, A. Cheyer, and D. Moran. 1998. Building
distributed software systems with the open agent ar-
chitecture. In Proceedings of the Third International
Conference on the Practical Application of Intelligent
Agents and Multi-Agent Technology, Blackpool, Lan-
cashire, UK.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Learning to assess low-level conceptual under-
standing. In Proceedings 21st International FLAIRS
Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
Mary D. Swift, Joel Tetreault, and Myroslava O.
Dzikovska. 2004. Semi-automatic syntactic and se-
mantic corpus annotation with a deep parser. In Pro-
ceedings of LREC-2004.
13
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 38?45,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Dealing with Interpretation Errors in Tutorial Dialogue
Myroslava O. Dzikovska, Charles B. Callaway, Elaine Farrow, Johanna D. Moore
School of Informatics
University of Edinburgh, Edinburgh, United Kingdom
mdzikovs,ccallawa,efarrow,jmoore@inf.ed.ac.uk
Natalie Steinhauser, Gwendolyn Campbell
Naval Air Warfare Training Systems Division
Orlando, Florida, USA
Abstract
We describe an approach to dealing with
interpretation errors in a tutorial dialogue
system. Allowing students to provide ex-
planations and generate contentful talk can
be helpful for learning, but the language
that can be understood by a computer sys-
tem is limited by the current technology.
Techniques for dealing with understanding
problems have been developed primarily for
spoken dialogue systems in information-
seeking domains, and are not always appro-
priate for tutorial dialogue. We present a
classification of interpretation errors and our
approach for dealing with them within an
implemented tutorial dialogue system.
1 Introduction
Error detection and recovery is a known problem in
the spoken dialogue community, with much research
devoted to determining the best strategies, and learn-
ing how to choose an appropriate strategy from data.
Most existing research is focused on dealing with
problems in an interaction resulting from speech
recognition errors. This focus is justified, since the
majority of understanding problems observed in cur-
rent spoken dialogue systems (SDS) are indeed due
to speech recognition errors.
Recovery strategies, therefore, are sometimes de-
vised specifically to target speech recognition prob-
lems - for example, asking the user to repeat the ut-
terance, or to speak more softly, which only makes
sense if speech recognition is the source of trouble.
However, errors can occur at all levels of process-
ing, including parsing, semantic interpretation, in-
tention recognition, etc. As speech recognition im-
proves and more sophisticated systems are devel-
oped, strategies for dealing with errors coming from
higher (and potentially more complex) levels of pro-
cessing will have to be developed.
This paper presents a classification of non-
understandings, defined as the errors where the sys-
tem fails to arrive at an interpretation of the user?s
utterance (Bohus and Rudnicky, 2005), and a set of
strategies for dealing with them in an implemented
tutorial dialogue system. Our system differs from
many existing systems in two ways. First, all di-
alogue is typed. This was done in part to avoid
speech recognition issues and allow for more com-
plex language input than would otherwise be pos-
sible. But it is also a valid modality for tutoring -
there are now many GUI-based tutoring systems in
existence, and as distance and online learning have
become more popular, students are increasingly fa-
miliar with typed dialogue in chat rooms and discus-
sion boards. Second, different genres impose dif-
ferent constraints on the set of applicable recovery
strategies - as we discuss in Section 2, certain help
strategies developed for task-oriented dialogue sys-
tems are not suitable for tutorial dialogue, because
tutoring systems should not give away the answer.
We propose a targeted help approach for dealing
with interpretation problems in tutorial dialogue by
providing help messages that target errors at differ-
ent points in the pipeline. In our system they are
combined with hints as a way to lead the student
to an answer that can be understood. While some
38
parts of the system response are specific to tutorial
dialogue, the targeted help messages themselves can
serve as a starting point for developing appropriate
recovery strategies in other systems where errors at
higher levels of interpretation are a problem.
The rest of this paper is organized as follows. In
Section 2, we motivate the need for error handling
strategies in tutorial dialogue. In Section 3 we de-
scribe the design of our system. Section 4 discusses
a classification of interpretation problems and our
targeted help strategy. Section 5 provides a prelim-
inary evaluation based on a set of system tests con-
ducted to date. Finally, we discuss how the approach
taken by our system compares to other systems.
2 Background and Motivation
Tutorial dialogue systems aim to improve learning
by engaging students in contentful dialogue. There
is a mounting body of evidence that dialogue which
encourages students to explain their actions (Aleven
and Koedinger, 2000), or to generate contentful talk
(Purandare and Litman, 2008), results in improved
learning. However, the systems? ability to under-
stand student language, and therefore to encourage
contentful talk, is limited by the state of current lan-
guage technology. Moreover, student language may
be particularly difficult to interpret since students
are often unaware of proper terminology, and may
phrase their answers in unexpected ways. For exam-
ple, a recent error analysis for a domain-independent
diagnoser trained on a large corpus showed that a
high proportion of errors were due to unexpected
paraphrases (Nielsen et al, 2008).
In small domains, domain-specific grammars and
lexicons can cover most common phrasings used
by students to ensure robust interpretation (Aleven,
2003; Glass, 2000). However, as the size of the
domain and the range of possible questions and an-
swers grows, achieving complete coverage becomes
more difficult. For essays in large domains, sta-
tistical methods can be used to identify problems
with the answer (Jordan et al, 2006; Graesser et
al., 1999), but these approaches do not perform well
on relatively short single-sentence explanations, and
such systems often revert to short-answer questions
during remediation to ensure robustness.
To the best of our knowledge, none of these tu-
torial systems use sophisticated error handling tech-
niques. They rely on the small size of the domain
or simplicity of expected answers to limit the range
of student input. They reject utterances they cannot
interpret, asking the user to repeat or rephrase, or
tolerate the possibility that interpretation problems
will lead to repetitive or confusing feedback.
We are developing a tutorial dialogue system that
behaves more like human tutors by supporting open-
ended questions, as well as remediations that allow
for open-ended answers, and gives students detailed
feedback on their answers, similar to what we ob-
served with human tutors. This paper takes the first
step towards addressing the problem of handling er-
rors in tutorial dialogue by developing a set of non-
understanding recovery strategies - i.e. strategies
used where the system cannot find an interpretation
for an utterance.
In early pilot experiments we observed that if the
system simply rejects a problematic student utter-
ance, saying that it was not understood, then stu-
dents are unable to determine the reason for this
rejection. They either resubmit their answer mak-
ing only minimal changes, or else they rephrase the
sentence in a progressively more complicated fash-
ion, causing even more interpretation errors. Even
after interacting with the system for over an hour,
our students did not have an accurate picture as to
which phrasings are well understood by the system
and which should be avoided. Previous research also
shows that users are rarely able to perceive the true
causes of ASR errors, and tend to form incorrect the-
ories about the types of input a system is able to ac-
cept (Karsenty, 2001).
A common approach for dealing with these is-
sues in spoken dialogue systems is to either change
to system initiative with short-answer questions (?Is
your destination London??), or provide targeted help
(?You can say plane, car or hotel?). Neither of these
is suitable for our system. The expected utterances
in our system are often more complex (e.g., ?The
bulb must be in a closed path with the battery?), and
therefore suggesting an utterance may be equivalent
to giving away the entire answer. Giving students
short-answer questions such as ?Are the terminals
connected or not connected?? is a valid tutoring
strategy sometimes used by the tutors. However,
it changes the nature of the question from a recall
39
task to a recognition task, which may affect the stu-
dent?s ability to remember the correct solution in-
dependently. Therefore, we decided to implement
strategies that give the student information about the
nature of the mistake without directly giving infor-
mation about the expected answer, and encourage
them to rephrase their answers in ways that can be
understood by the system.
We currently focus on strategies for dealing
with non-understanding rather than misunderstand-
ing strategies (i.e. cases where the system finds an
interpretation, but an incorrect one). It is less clear
in tutorial dialogue what it means for a misunder-
standing to be corrected. In task-oriented dialogue,
if the system gets a slot value different from what
the user intended, it should make immediate correc-
tions at the user?s request. In tutoring, however, it
is the system which knows the expected correct an-
swer. So if the student gives an answer that does not
match the expected answer, when they try to correct
it later, it may not always be obvious whether the
correction is due to a true misunderstanding, or due
to the student arriving at a better understanding of
the question. Obviously, true misunderstandings can
and will still occur - for example, when the system
resolves a pronoun incorrectly. Dealing with such
situations is planned as part of future work.
3 System Architecture
Our target application is a system for tutoring ba-
sic electricity and electronics. The students read
some introductory material, and interact with a sim-
ulator where they can build circuits using batteries,
bulbs and switches, and measure voltage and cur-
rent. They are then asked two types of questions:
factual questions, like ?If the switch is open, will
bulb A be on or off??, and explanation questions.
The explanation questions ask the student to explain
what they observed in a circuit simulation, for exam-
ple, ?Explain why you got the voltage of 1.5 here?,
or define generic concepts, such as ?What is volt-
age??. The expected answers are fairly short, one or
two sentences, but they involve complex linguistic
phenomena, including conjunction, negation, rela-
tive clauses, anaphora and ellipsis.
The system is connected to a knowledge base
which serves as a model for the domain and a rea-
soning engine. It represents the objects and rela-
tionships the system can reason about, and is used
to compute answers to factual questions.1 The stu-
dent answers are processed using a standard NLP
pipeline. All utterances are parsed to obtain syntac-
tic analyses.2 The lexical-semantic interpreter takes
analyses from the parser and maps them to seman-
tic representations using concepts from the domain
model. A reference resolution algorithm similar to
(Byron, 2002) is used to find referents for named ob-
jects such as ?bulb A? and for pronouns.
Once an interpretation of a student utterance has
been obtained, it is checked in two ways. First, its
internal consistency is verified. For example, if the
student says ?Bulb A will be on because it is in a
closed path?, we first must ensure that their answer
is consistent with what is on the screen - that bulb A
is indeed in a closed path. Otherwise the student
probably has a problem either with understanding
the diagrams or with understanding concepts such as
?closed path?. These problems indicate lack of basic
background knowledge, and need to be remediated
using a separate tutorial strategy.
Assuming that the utterance is consistent with the
state of the world, the explanation is then checked
for correctness. Even though the student utterance
may be factually correct (Bulb A is indeed in a
closed path), it may still be incomplete or irrelevant.
In the example above, the full answer is ?Bulb A
is in a closed path with the battery?, hence the stu-
dent explanation is factually correct but incomplete,
missing the mention of the battery.
In the current version of our system, we are partic-
ularly concerned about avoiding misunderstandings,
since they can result in misleading tutorial feedback.
Consider an example of what can happen if there is
a misunderstanding due to a lexical coverage gap.
The student sentence ?the path is broken? should be
interpreted as ?the path is no longer closed?, corre-
sponding to the is-open relation. However, the
1Answers to explanation questions are hand-coded by tutors
because they are not always required to be logically complete
(Dzikovska et al, 2008). However, they are checked for consis-
tency as described later, so they have to be expressed in terms
that the knowledge base can reason about.
2We are using a deep parser that produces semantic analyses
of student?s input (Allen et al, 2007). However, these have to
undergo further lexical interpretation, so we are treating them
as syntactic analyses for purposes of this paper.
40
most frequent sense of ?broken? is is-damaged,
as in ?the bulb is broken?. Ideally, the system lex-
icon would define ?broken? as ambiguous between
those two senses. If only the ?damaged? sense is
defined, the system will arrive at an incorrect inter-
pretation (misunderstanding), which is false by defi-
nition, as the is-damaged relation applies only to
bulbs in our domain. Thus the system will say ?you
said that the path is damaged, but that?s not true?.
Since the students who used this phrasing were un-
aware of the proper terminology in the first instance,
they dismissed such feedback as a system error. A
more helpful feedback message is to say that the sys-
tem does not know about damaged paths, and the
sentence needs to be rephrased.3
Obviously, frequent non-understanding messages
can also lead to communication breakdowns and im-
pair tutoring. Thus we aim to balance the need to
avoid misunderstandings with the need to avoid stu-
dent frustration due to a large number of sentences
which are not understood. We approach this by us-
ing robust parsing and interpretation tools, but bal-
ancing them with a set of checks that indicate poten-
tial problems. These include checking that the stu-
dent answer fits with the sortal constraints encoded
in the domain model, that it can be interpreted un-
ambiguously, and that pronouns can be resolved.
4 Error Handling Policies
All interpretation problems in our system are han-
dled with a unified tutorial policy. Each message to
the user consists of three parts: a social response,
the explanation of the problem, and the tutorial re-
sponse. The social response is currently a simple
apology, as in ?I?m sorry, I?m having trouble under-
standing.? Research on spoken dialogue shows that
users are less frustrated if systems apologize for er-
rors (Bulyko et al, 2005).
The explanation of the problem depends on the
problem itself, and is discussed in more detail below.
The tutorial response depends on the general tu-
torial situation. If this is the first misunderstanding,
the student will be asked to rephrase/try again. If
3This was a real coverage problem we encountered early on.
While we extended the coverage of the lexical interpreter based
on corpus data, other gaps in coverage may remain. We discuss
the issues related to the treatment of vague or incorrect termi-
nology in Section 4.
they continue to phrase things in a way that is mis-
understood, they will be given up to two different
hints (a less specific hint followed by a more spe-
cific hint); and finally the system will bottom out
with a correct answer. Correct answers produced by
the generator are guaranteed to be parsed and under-
stood by the interpretation module, so they can serve
as templates for future student answers.
The tutorial policy is also adjusted depending
on the interaction history. For example, if a non-
understanding comes after a few incorrect answers,
the system may decide to bottom out immediately in
order to avoid student frustration due to multiple er-
rors. At present we are using a heuristic policy based
on the total number of incorrect or uninterpretable
answers. In the future, such policy could be learned
from data, using, for example, reinforcement learn-
ing (Williams and Young, 2007).
In the rest of this section we discuss the explana-
tions used for different problems. For brevity, we
omit the tutorial response from our examples.
4.1 Parse Failures
An utterance that cannot be parsed represents the
worst possible outcome for the system, since detect-
ing the reason for a syntactic parse failure isn?t pos-
sible for complex parsers and grammars. Thus, in
this instance the system does not give any descrip-
tion of the problem at all, saying simply ?I?m sorry,
I didn?t understand.?
Since we are unable to explain the source of the
problem, we try hard to avoid such failures. We use
a spelling corrector and a robust parser that outputs
a set of fragments covering the student?s input when
a full parse cannot be found. The downstream com-
ponents are designed to merge interpretations of the
fragments into a single representation that is sent to
the reasoning components.
Our policy is to allow the system to use such frag-
mentary parses when handling explanation ques-
tions, where students tend to use complex language.
However, we require full parses for factual ques-
tions, such as ?Which bulbs will be off?? We found
that for those simpler questions students are able to
easily phrase an acceptable answer, and the lack of
a full parse signals some unusually complex lan-
guage that downstream components are likely to
have problems with as well.
41
One risk associated with using fragmentary parses
is that relationships between objects from different
fragments would be missed by the parser. Our cur-
rent policy is to confirm the correct part of the stu-
dent?s answer, and prompt for the missing parts, e.g.,
? Right. The battery is contained in a closed path.
And then?? We can do this because we use a diag-
noser that explicitly identifies the correct objects and
relationships in the answer (Dzikovska et al, 2008),
and we are using a deep generation system that can
take those relationships and automatically generate
a rephrasing of the correct portion of the content.
4.2 Lexical Interpretation Errors
Errors in lexical interpretation typically come from
three main sources: unknown words which the lex-
ical interpreter cannot map into domain concepts,
unexpected word combinations, and incorrect uses
of terminology that violate the sortal constraints en-
coded in the domain model.
Unknown words are the simplest to deal with in
the context of our lexical interpretation policy. We
do not require that every single word of an utter-
ance should be interpreted, because we want the
system to be able to skip over irrelevant asides.
However, we require that if a predicate is inter-
preted, all its arguments should be interpreted as
well. To illustrate, in our system the interpretation of
?the bulb is still lit? is (LightBulb Bulb-1-1)
(is-lit Bulb-1-1 true). The adverbial
?still? is not interpreted because the system is un-
able to reason about time.4 But since all arguments
of the is-lit predicate are defined, we consider
the interpretation complete.
In contrast, in the sentence ?voltage is the mea-
surement of the power available in a battery?, ?mea-
surement? is known to the system. Thus, its argu-
ment ?power? should also be interpreted. However,
the reading material in the lessons never talks about
power (the expected answer is ?Voltage is a mea-
surement of the difference in electrical states be-
tween two terminals?). Therefore the unknown word
detector marks ?power? as an unknown word, and
tells the student ?I?m sorry, I?m having a problem
understanding. I don?t know the word power.?
4The lexical interpretation algorithm makes sure that fre-
quency and negation adverbs are accounted for.
The system can still have trouble interpreting sen-
tences with words which are known to the lexical
interpreter, but which appear in unexpected combi-
nations. This involves two possible scenarios. First,
unambiguous words could be used in a way that
contradicts the system?s domain model. For exam-
ple, the students often mention ?closed circuit? in-
stead of the correct term ?closed path?. The former
is valid in colloquial usage, but is not well defined
for parallel circuits which can contain many differ-
ent paths, and therefore cannot be represented in a
consistent knowledge base. Thus, the system con-
sults its knowledge base to tell the student about the
appropriate arguments for a relation with which the
failure occurred. In this instance, the feedback will
be ?I?m sorry, I?m having a problem understanding.
I don?t understand it when you say that circuits can
be closed. Only paths and switches can be closed.?5
The second case arises when a highly ambiguous
word is used in an unexpected combination. The
knowledge base uses a number of fine-grained rela-
tions, and therefore some words can map to a large
number of relations. For example, the word ?has?
means circuit-component in ?The circuit has
2 bulbs?, terminals-of in ?The bulb has ter-
minals? and voltage-property in ?The bat-
tery has voltage?. The last relation only applies to
batteries, but not to other components. These dis-
tinctions are common for knowledge representation
and reasoning systems, since they improve reason-
ing efficiency, but this adds to the difficulty of lex-
ical interpretation. If a student says ?Bulb A has a
voltage of 0.5?, we cannot determine the concept to
which the word ?has? corresponds. It could be either
terminals-of or voltage-property, since
each of those relations uses one possible argument
from the student?s utterance. Thus, we cannot sug-
gest appropriate argument types and instead we in-
dicate the problematic word combination, for exam-
ple, ?I?m sorry, I?m having trouble understanding. I
didn?t understand bulb has voltage.?
Finally, certain syntactic constructions involving
comparatives or ellipsis are known to be difficult
5Note that these error messages are based strictly on the fact
that sortal constraints from the knowledge base for the relation
that the student used were violated. In the future, we may also
want to adjust the recovery strategy depending on whether the
problematic relation is relevant to the expected answer.
42
open problems for interpretation. While we are
working on interpretation algorithms to be included
in future system versions, the system currently de-
tects these special relations, and produces a mes-
sage telling the student to rephrase without the prob-
lematic construction, e.g., ?I?m sorry. I?m having a
problem understanding. I do not understand same
as. Please try rephrasing without the word as.?
4.3 Reference Errors
Reference errors arise when a student uses an am-
biguous pronoun, and the system cannot find a suit-
able object in the knowledge base to match, or on
certain occasions when an attachment error in a
parse causes an incorrect interpretation. We use a
generic message that indicates the type of the ob-
ject the system perceived, and the actual word used,
for example, ?I?m sorry. I don?t know which switch
you?re referring to with it.?
To some extent, reference errors are instances of
misunderstandings rather than non-understandings.
There are actually 2 underlying cases for reference
failure: either the system cannot find any referent at
all, or it is finding too many referents. In the future
a better policy would be to ask the student which of
the ambiguous referents was intended. We expect to
pilot this policy in one of our future system tests.
5 Evaluation
So far, we have run 13 pilot sessions with our sys-
tem. Each pilot consisted of a student going through
1 or 2 lessons with the system. Each lesson lasts
about 2 hours and has 100-150 student utterances
(additional time is taken with building circuits and
reading material). Both the coverage of the interpre-
tation component and the specificity of error mes-
sages were improved between each set of pilots, thus
it does not make sense to aggregate the data from
them. However, over time we observed the trend
that students are more likely to change their behav-
ior when the system issues more specific messages.
Examples of successful and unsuccessful interac-
tions are shown in Figure 1. In (a), the student used
incorrect terminology, and a reminder about how the
word ?complete? is interpreted was enough to get
the conversation back on track.
The dialogue fragment in (b) shows how mes-
sages which are not specific enough can cause a
breakdown in conversation. The system used an in-
sufficiently specific message at the beginning (omit-
ting the part that says that only switches and paths
can be closed). This led the student away from an
answer which was nearly correct with slightly im-
perfect terminology to an answer which was insuffi-
cient (it?s not enough for the components to be con-
nected, they have to be in a closed path), and then
to rephrase it in a more complicated way that was
impossible for the system to understand (consistent
with findings of Bulyko et al (2005)).
The next step would be to conduct a formal evalu-
ation of our policy. We are planning to do this using
recovery rate after different strategies as our evalu-
ation measure (Bohus and Rudnicky, 2005), which
is the percentage of times when the utterance imme-
diately following a non-understanding was success-
fully interpreted by the system.
6 Discussion
Our error handling policy attempts to balance ro-
bustness in interpretation with the need for pre-
cise tutorial feedback. Human tutors accept a wide
range of variation in terminology and phrasing.
However, there is a known trade-off between non-
understandings and misunderstandings. We decided
that misunderstandings are likely to cause more seri-
ous problems with respect to tutoring. Therefore we
set strict understanding requirements and focused on
developing a policy that would encourage students
to use constructions that the system can interpret re-
liably. This is also supported by our observations in
the human-human data ? tutors believe that teaching
correct terminology is important, and we observed
them saying things like ?It?s good to put it into your
own words like that. Just for the sake of the lesson,
could you also use the term closed path??.
Our approach is similar to the TargetedHelp ap-
proach of (Hockey et al, 2003), where in the case of
an interpretation failure the task assistant produces
a report of what the recognizer heard, a description
of the problem, and a possible in-domain utterance
as an alternative. In a user evaluation such targeted
help messages resulted in significantly lower task
completion times compared to a system that pro-
duced a generic ?I didn?t understand? message, and
43
(a)
S: the open switch makes the path incomplete
T: I?m sorry, I?m having a problem understanding. Paths can?t be complete. Only circuits can be
complete. Please try again.
S: the open switch makes the path open
T: Right. An open switch creates a gap.
(b)
T: What are the conditions that are required to make a bulb light up?
S: The lightbulb has to be in a closed circuit with a battery
T: I?m sorry, I?m having a problem understanding. I don?t understand when you say that circuits
can be closed. Please try again.
S: The lightbulb has to be connected to a battery
T: Yes. You correctly mentioned a battery and a bulb. Keep trying.
S: Both the positive and negative connectors have to be connected to
T: I?m sorry, I?m having a problem understanding. Try rephrasing your answer.
S: The lightbulb has to be connected to the positive and negative connectors of the battery
T: I?m sorry, I didn?t understand. Here?s the answer. The battery and the bulb must be contained
in the same closed path.
Figure 1: Sample student-system interactions. S: is student, T: is tutor (system). (a) A successful interaction
where the student changes their language; (b) a failed interaction where system feedback was unhelpful.
subjects gradually learned how to talk to the sys-
tem, reducing the number of misunderstandings over
time. This gives us reason to believe that our sys-
tem can achieve similar effects in tutorial dialogue.
While we don?t suggest alternative domain utter-
ances due to the tutoring reasons described earlier,
the progressively more specific hints serve a simi-
lar function. To what extent this impacts learning
and interaction with the system will have to be de-
termined in future evaluations.
The error handling in our system is significantly
different from systems that analyze user essays be-
cause it needs to focus on a single sentence at a time.
In a system that does essay analysis, such as AUTO-
TUTOR (Graesser et al, 1999) or Why2-Atlas (Jor-
dan et al, 2006) a single essay can have many flaws.
So it doesn?t matter if some sentences are not fully
understood as long as the essay is understood well
enough to identify at least one flaw. Then that par-
ticular flaw can be remediated, and the student can
resubmit the essay. However, this can also cause stu-
dent frustration and potentially affect learning if the
student is asked to re-write an essay many times due
to interpretation errors.
Previous systems in the circuit domain focused on
troubleshooting rather than conceptual knowledge.
The SHERLOCK tutor (Katz et al, 1998) used only
menu-based input, limiting possible dialogue. Cir-
cuit Fix-It Shop (Smith and Gordon, 1997) was a
task-oriented system which allowed for speech in-
put, but with very limited vocabulary. Our system?s
larger vocabulary and complex input result in differ-
ent types of non-understandings that cannot be re-
solved with simple confirmation messages.
A number of researchers have developed er-
ror taxonomies for spoken dialogue systems (Paek,
2003; Mo?ller et al, 2007). Our classification does
not have speech recognition errors (since we are us-
ing typed dialogue), and we have a more complex
interpretation stack than the domain-specific pars-
ing utilized by many SDSs. However, some types
of errors are shared, in particular, our ?no parse?,
?unknown word? and ?unknown attachment? errors
correspond to command-level errors, and our sor-
tal constraint and reference errors correspond to
concept-level errors in the taxonomy of Mo?ller et al
(2007). This correspondence is not perfect because
of the nature of the task - there are no commands in
a tutoring system. However, the underlying causes
are very similar, and so research on the best way
44
to communicate about system failures would benefit
both tutoring and task-oriented dialogue systems. In
the long run, we would like to reconcile these differ-
ent taxonomies, leading to a unified classification of
system errors and recovery strategies.
7 Conclusion
In this paper we described our approach to handling
non-understanding errors in a tutorial dialogue sys-
tem. Explaining the source of errors, without giving
away the full answer, is crucial to establishing ef-
fective communication between the system and the
student. We described a classification of common
problems and our approach to dealing with different
classes of errors. Our experience with pilot studies,
as well as evidence from spoken dialogue systems,
indicates that our approach can help improve dia-
logue efficiency. We will be evaluating its impact on
both student learning and on dialogue efficiency in
the future.
8 Acknowledgments
This work has been supported in part by Office of
Naval Research grant N000140810043.
References
V. A. Aleven and K. R. Koedinger. 2000. The need for
tutorial dialog to support self-explanation. In Proc. of
AAAI Fall Symposion on Building Dialogue Systems
for Tutorial Applications.
O. P. V. Aleven. 2003. A knowledge-based approach
to understanding students? explanations. In School of
Information Technologies, University of Sydney.
J. Allen, M. Dzikovska, M. Manshadi, and M. Swift.
2007. Deep linguistic processing for spoken dialogue
systems. In Proceedings of the ACL-07 Workshop on
Deep Linguistic Processing.
D. Bohus and A. Rudnicky. 2005. Sorry, i didn?t catch
that! - an investigation of non-understanding errors
and recovery strategies. In Proceedings of SIGdial-
2005, Lisbon, Portugal.
I. Bulyko, K. Kirchhoff, M. Ostendorf, and J. Goldberg.
2005. Error-correction detection and response gener-
ation in a spoken dialogue system. Speech Communi-
cation, 45(3):271?288.
D. K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
M. O. Dzikovska, G. E. Campbell, C. B. Callaway, N. B.
Steinhauser, E. Farrow, J. D. Moore, L. A. Butler, and
C. Matheson. 2008. Diagnosing natural language an-
swers to support adaptive tutoring. In Proceedings
21st International FLAIRS Conference.
M. Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Proc.
of the AAAI Fall Symposium on Building Dialogue Sys-
tems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-Hastings,
and R. Kreuz. 1999. Autotutor: A simulation of a
human tutor. Cognitive Systems Research, 1:35?51.
B. A. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist,
J. Hieronymus, A. Gruenstein, and J. Dowding. 2003.
Targeted help for spoken dialogue systems: intelligent
feedback improves naive users? performance. In Pro-
ceedings of EACL.
P. Jordan, M. Makatchev, U. Pappuswamy, K. VanLehn,
and P. Albacete. 2006. A natural language tuto-
rial dialogue system for physics. In Proceedings of
FLAIRS?06.
L. Karsenty. 2001. Adapting verbal protocol methods to
investigate speech systems use. Applied Ergonomics,
32:15?22.
S. Katz, A. Lesgold, E. Hughes, D. Peters, G. Eggan,
M. Gordin, and L. Greenberg. 1998. Sherlock 2: An
intelligent tutoring system built on the lrdc framework.
In C. Bloom and R. Loftin, editors, Facilitating the
development and use of interactive learning environ-
ments. ERLBAUM.
S. Mo?ller, K.-P. Engelbrecht, and A. Oulasvirta. 2007.
Analysis of communication failures for spoken dia-
logue systems. In Proceedings of Interspeech.
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innovative
Use of NLP for Building Educational Applications.
T. Paek. 2003. Toward a taxonomy of communication
errors. In Proceedings of ISCA Workshop on Error
Handling in Spoken Dialogue Systems.
A. Purandare and D. Litman. 2008. Content-learning
correlations in spoken tutoring dialogs at word, turn
and discourse levels. In Proc.of FLAIRS.
R. W. Smith and S. A. Gordon. 1997. Effects of variable
initiative on linguistic behavior in human-computer
spoken natural language dialogue. Computational
Linguistics.
J. D. Williams and S. Young. 2007. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116?2129.
45
Proceedings of the ACL 2010 System Demonstrations, pages 13?18,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
BEETLE II: a system for tutoring and computational linguistics
experimentation
Myroslava O. Dzikovska and Johanna D. Moore
School of Informatics, University of Edinburgh, Edinburgh, United Kingdom
{m.dzikovska,j.moore}@ed.ac.uk
Natalie Steinhauser and Gwendolyn Campbell
Naval Air Warfare Center Training Systems Division, Orlando, FL, USA
{gwendolyn.campbell,natalie.steihauser}@navy.mil
Elaine Farrow
Heriot-Watt University
Edinburgh, United Kingdom
e.farrow@hw.ac.uk
Charles B. Callaway
University of Haifa
Mount Carmel, Haifa, Israel
ccallawa@gmail.com
Abstract
We present BEETLE II, a tutorial dia-
logue system designed to accept unre-
stricted language input and support exper-
imentation with different tutorial planning
and dialogue strategies. Our first system
evaluation used two different tutorial poli-
cies and demonstrated that the system can
be successfully used to study the impact
of different approaches to tutoring. In the
future, the system can also be used to ex-
periment with a variety of natural language
interpretation and generation techniques.
1 Introduction
Over the last decade there has been a lot of inter-
est in developing tutorial dialogue systems that un-
derstand student explanations (Jordan et al, 2006;
Graesser et al, 1999; Aleven et al, 2001; Buckley
and Wolska, 2007; Nielsen et al, 2008; VanLehn
et al, 2007), because high percentages of self-
explanation and student contentful talk are known
to be correlated with better learning in human-
human tutoring (Chi et al, 1994; Litman et al,
2009; Purandare and Litman, 2008; Steinhauser et
al., 2007). However, most existing systems use
pre-authored tutor responses for addressing stu-
dent errors. The advantage of this approach is that
tutors can devise remediation dialogues that are
highly tailored to specific misconceptions many
students share, providing step-by-step scaffolding
and potentially suggesting additional problems.
The disadvantage is a lack of adaptivity and gen-
erality: students often get the same remediation
for the same error regardless of their past perfor-
mance or dialogue context, as it is infeasible to
author a different remediation dialogue for every
possible dialogue state. It also becomes more dif-
ficult to experiment with different tutorial policies
within the system due to the inherent completixites
in applying tutoring strategies consistently across
a large number of individual hand-authored reme-
diations.
The BEETLE II system architecture is designed
to overcome these limitations (Callaway et al,
2007). It uses a deep parser and generator, to-
gether with a domain reasoner and a diagnoser,
to produce detailed analyses of student utterances
and generate feedback automatically. This allows
the system to consistently apply the same tutorial
policy across a range of questions. To some extent,
this comes at the expense of being able to address
individual student misconceptions. However, the
system?s modular setup and extensibility make it
a suitable testbed for both computational linguis-
tics algorithms and more general questions about
theories of learning.
A distinguishing feature of the system is that it
is based on an introductory electricity and elec-
tronics course developed by experienced instruc-
tional designers. The course was first created for
use in a human-human tutoring study, without tak-
ing into account possible limitations of computer
tutoring. The exercises were then transferred into
a computer system with only minor adjustments
(e.g., breaking down compound questions into in-
dividual questions). This resulted in a realistic tu-
toring setup, which presents interesting challenges
to language processing components, involving a
wide variety of language phenomena.
We demonstrate a version of the system that
has undergone a successful user evaluation in
13
2009. The evaluation results indicate that addi-
tional improvements to remediation strategies, and
especially to strategies dealing with interpretation
problems, are necessary for effective tutoring. At
the same time, the successful large-scale evalua-
tion shows that BEETLE II can be used as a plat-
form for future experimentation.
The rest of this paper discusses the BEETLE II
system architecture (Section 2), system evaluation
(Section 3), and the range of computational lin-
guistics problems that can be investigated using
BEETLE II (Section 4).
2 System Architecture
The BEETLE II system delivers basic electricity
and electronics tutoring to students with no prior
knowledge of the subject. A screenshot of the sys-
tem is shown in Figure 1. The student interface in-
cludes an area to display reading material, a circuit
simulator, and a dialogue history window. All in-
teractions with the system are typed. Students read
pre-authored curriculum slides and carry out exer-
cises which involve experimenting with the circuit
simulator and explaining the observed behavior.
The system also asks some high-level questions,
such as ?What is voltage??.
The system architecture is shown in Figure 2.
The system uses a standard interpretation pipeline,
with domain-independent parsing and generation
components supported by domain specific reason-
ers for decision making. The architecture is dis-
cussed in detail in the rest of this section.
2.1 Interpretation Components
We use the TRIPS dialogue parser (Allen et al,
2007) to parse the utterances. The parser provides
a domain-independent semantic representation in-
cluding high-level word senses and semantic role
labels. The contextual interpreter then uses a refer-
ence resolution approach similar to Byron (2002),
and an ontology mapping mechanism (Dzikovska
et al, 2008a) to produce a domain-specific seman-
tic representation of the student?s output. Utter-
ance content is represented as a set of extracted
objects and relations between them. Negation is
supported, together with a heuristic scoping algo-
rithm. The interpreter also performs basic ellipsis
resolution. For example, it can determine that in
the answer to the question ?Which bulbs will be
on and which bulbs will be off in this diagram??,
?off? can be taken to mean ?all bulbs in the di-
agram will be off.? The resulting output is then
passed on to the domain reasoning and diagnosis
components.
2.2 Domain Reasoning and Diagnosis
The system uses a knowledge base implemented in
the KM representation language (Clark and Porter,
1999; Dzikovska et al, 2006) to represent the state
of the world. At present, the knowledge base rep-
resents 14 object types and supports the curricu-
lum containing over 200 questions and 40 differ-
ent circuits.
Student explanations are checked on two levels,
verifying factual and explanation correctness. For
example, for a question ?Why is bulb A lit??, if
the student says ?it is in a closed path?, the system
checks two things: a) is the bulb indeed in a closed
path? and b) is being in a closed path a reason-
able explanation for the bulb being lit? Different
remediation strategies need to be used depending
on whether the student made a factual error (i.e.,
they misread the diagram and the bulb is not in a
closed path) or produced an incorrect explanation
(i.e., the bulb is indeed in a closed path, but they
failed to mention that a battery needs to be in the
same closed path for the bulb to light).
The knowledge base is used to check the fac-
tual correctness of the answers first, and then a di-
agnoser checks the explanation correctness. The
diagnoser, based on Dzikovska et al (2008b), out-
puts a diagnosis which consists of lists of correct,
contradictory and non-mentioned objects and re-
lations from the student?s answer. At present, the
system uses a heuristic matching algorithm to clas-
sify relations into the appropriate category, though
in the future we may consider a classifier similar
to Nielsen et al (2008).
2.3 Tutorial Planner
The tutorial planner implements a set of generic
tutoring strategies, as well as a policy to choose
an appropriate strategy at each point of the inter-
action. It is designed so that different policies can
be defined for the system. The currently imple-
mented strategies are: acknowledging the correct
part of the answer; suggesting a slide to read with
background material; prompting for missing parts
of the answer; hinting (low- and high- specificity);
and giving away the answer. Two or more strate-
gies can be used together if necessary.
The hint selection mechanism generates hints
automatically. For a low specificity hint it selects
14
Figure 1: Screenshot of the BEETLE II system
Dialogue ManagerParserContextualInterpreter
Interpretation
CurriculumPlanner
KnowledgeBase
Content Planner & Generator
TutorialPlanner
Tutoring
GUI
Diagnoser
Figure 2: System architecture diagram
15
an as-yet unmentioned object and hints at it, for
example, ?Here?s a hint: Your answer should men-
tion a battery.? For high-specificity, it attempts to
hint at a two-place relation, for example, ?Here?s
a hint: the battery is connected to something.?
The tutorial policy makes a high-level decision
as to which strategy to use (for example, ?ac-
knowledge the correct part and give a high speci-
ficity hint?) based on the answer analysis and di-
alogue context. At present, the system takes into
consideration the number of incorrect answers re-
ceived in response to the current question and the
number of uninterpretable answers.1
In addition to a remediation policy, the tuto-
rial planner implements an error recovery policy
(Dzikovska et al, 2009). Since the system ac-
cepts unrestricted input, interpretation errors are
unavoidable. Our recovery policy is modeled on
the TargetedHelp (Hockey et al, 2003) policy used
in task-oriented dialogue. If the system cannot
find an interpretation for an utterance, it attempts
to produce a message that describes the problem
but without giving away the answer, for example,
?I?m sorry, I?m having a problem understanding. I
don?t know the word power.? The help message is
accompanied with a hint at the appropriate level,
also depending on the number of previous incor-
rect and non-interpretable answers.
2.4 Generation
The strategy decision made by the tutorial plan-
ner, together with relevant semantic content from
the student?s answer (e.g., part of the answer to
confirm), is passed to content planning and gen-
eration. The system uses a domain-specific con-
tent planner to produce input to the surface realizer
based on the strategy decision, and a FUF/SURGE
(Elhadad and Robin, 1992) generation system to
produce the appropriate text. Templates are used
to generate some stock phrases such as ?When you
are ready, go on to the next slide.?
2.5 Dialogue Management
Interaction between components is coordinated by
the dialogue manager which uses the information-
state approach (Larsson and Traum, 2000). The
dialogue state is represented by a cumulative an-
swer analysis which tracks, over multiple turns,
the correct, incorrect, and not-yet-mentioned parts
1Other factors such as student confidence could be con-
sidered as well (Callaway et al, 2007).
of the answer. Once the complete answer has been
accumulated, the system accepts it and moves on.
Tutor hints can contribute parts of the answer to
the cumulative state as well, allowing the system
to jointly construct the solution with the student.
3 Evaluation
The first experimental evaluation involving 81 par-
ticipants (undergraduates recruited from a South-
eastern University in the USA) was completed in
2009. Participants had little or no prior knowledge
of the domain. Each participant took a pre-test,
worked through a lesson with the system, took a
post-test, and completed a user satisfaction survey.
Each session lasted approximately 4 hours.
We implemented two different tutoring policies
in the system for this evaluation. The baseline
policy used an ?accept and bottom out? strategy
for all student answers, regardless of their con-
tent. The students were always given the correct
answer, but the system made no attempt at reme-
diation, and never indicated whether the student
was understood (or correct). In comparison, the
full adaptive policy attempted to select an appro-
priate strategy based on student answer analysis
and dialogue context, as described above. Stu-
dents were randomly assigned to a condition. On
average, each student session contained 232 stu-
dent language turns in the fully adaptive condition
(SD = 25.6) and 156 turns in the baseline condi-
tion (SD = 2.02). Additional time was taken by
reading and interacting with the simulation envi-
ronment.
Out of 81 students, 76 successfully completed
the evaluation (data from 5 participants had to be
discarded due to system crashes). There was no
significant difference in learning gain between the
two conditions (baseline: M = 0.63, SD = 0.26,
full: M = 0.57, SD = 0.23, t-test p = 0.32),
and students gave higher satisfaction scores to the
baseline condition (baseline: M = 3.32, SD =
0.65, full: M = 2.56, SD = 0.65, t-test, p <
0.01). We are currently analyzing the data to in-
vestigate the reasons for this outcome. However,
the evaluation demonstrated that the system can be
successfully deployed in experiments with naive
users and therefore can be used to study the im-
pact of different natural language processing algo-
rithms on learning gain and user satisfaction.
An example dialogue with the system, taken
from the evaluation corpus, is shown in Figure 3.
16
Tutor: Why was bulb A on when switch Y
was open and switch Z was closed?
Student: because it had a closed path
Tutor: Right. There is a closed path. Keep
going.
Student: there was a closed path not effected by
the switch
Tutor: I?m sorry, I?m having a problem un-
derstanding. I don?t understand the
word ?effect? here. Here?s a hint. Your
answer should mention a battery.
Student: it has a closed path with the battery
Tutor: You?re on the right track. Here?s the
answer. Bulb A is still contained in a
closed path with the battery.
Figure 3: Example interaction with the system
from our corpus
It shows three key system properties: after the stu-
dent?s first turn, the system rephrases its under-
standing of the correct part of the student answer
and prompts the student to supply the missing in-
formation. In the second turn, the student utter-
ance could not be interpreted and the system re-
sponds with a targeted help message and a hint
about the object that needs to be mentioned. Fi-
nally, in the last turn the system combines the in-
formation from the tutor?s hint and the student?s
answers and restates the complete answer since the
current answer was completed over multiple turns.
4 Conclusions and Future Work
The BEETLE II system we present was built to
serve as a platform for research in computational
linguistics and tutoring, and can be used for task-
based evaluation of algorithms developed for other
domains. We are currently developing an annota-
tion scheme for the data we collected to identify
student paraphrases of correct answers. The an-
notated data will be used to evaluate the accuracy
of existing paraphrasing and textual entailment ap-
proaches and to investigate how to combine such
algorithms with the current deep linguistic analy-
sis to improve system robustness. We also plan
to annotate the data we collected for evidence of
misunderstandings, i.e., situations where the sys-
tem arrived at an incorrect interpretation of a stu-
dent utterance and took action on it. Such annota-
tion can provide useful input for statistical learn-
ing algorithms to detect and recover from misun-
derstandings.
In dialogue management and generation, the
key issue we are planning to investigate is that of
linguistic alignment. The analysis of the data we
have collected indicates that student satisfaction
may be affected if the system rephrases student
answers using different words (for example, using
better terminology) but doesn?t explicitly explain
the reason why different terminology is needed
(Dzikovska et al, 2010). Results from other sys-
tems show that measures of semantic coherence
between a student and a system were positively as-
sociated with higher learning gain (Ward and Lit-
man, 2006). Using a deep generator to automati-
cally generate system feedback gives us a level of
control over the output and will allow us to devise
experiments to study those issues in more detail.
From the point of view of tutoring research,
we are planning to use the system to answer
questions about the effectiveness of different ap-
proaches to tutoring, and the differences between
human-human and human-computer tutoring. Pre-
vious comparisons of human-human and human-
computer dialogue were limited to systems that
asked short-answer questions (Litman et al, 2006;
Rose? and Torrey, 2005). Having a system that al-
lows more unrestricted language input will pro-
vide a more balanced comparison. We are also
planning experiments that will allow us to eval-
uate the effectiveness of individual strategies im-
plemented in the system by comparing system ver-
sions using different tutoring policies.
Acknowledgments
This work has been supported in part by US Office
of Naval Research grants N000140810043 and
N0001410WX20278. We thank Katherine Harri-
son and Leanne Taylor for their help running the
evaluation.
References
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 10th International
Conference on Artificial Intelligence in Education
(AIED ?01)?.
James Allen, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL-07 Workshop on Deep Linguistic Processing.
17
Mark Buckley and Magdalena Wolska. 2007. To-
wards modelling and using common ground in tu-
torial dialogue. In Proceedings of DECALOG, the
2007 Workshop on the Semantics and Pragmatics of
Dialogue, pages 41?48.
Donna K. Byron. 2002. Resolving Pronominal Refer-
ence to Abstract Entities. Ph.D. thesis, University of
Rochester.
Charles B. Callaway, Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeD-
iff tutoring systems. In Proceedings of SLaTE?07
(Speech and Language Technology in Education).
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting
self-explanations improves understanding. Cogni-
tive Science, 18(3):439?477.
Peter Clark and Bruce Porter, 1999. KM (1.4): Users
Manual. http://www.cs.utexas.edu/users/mfkb/km.
Myroslava O. Dzikovska, Charles B. Callaway, and
Elaine Farrow. 2006. Interpretation and generation
in a knowledge-based tutorial system. In Proceed-
ings of EACL-06 workshop on knowledge and rea-
soning for language processing, Trento, Italy, April.
Myroslava O. Dzikovska, James F. Allen, and Mary D.
Swift. 2008a. Linking semantic and knowledge
representations in a multi-domain dialogue system.
Journal of Logic and Computation, 18(3):405?430.
Myroslava O. Dzikovska, Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008b. Diagnosing natural lan-
guage answers to support adaptive tutoring. In
Proceedings 21st International FLAIRS Conference,
Coconut Grove, Florida, May.
Myroslava O. Dzikovska, Charles B. Callaway, Elaine
Farrow, Johanna D. Moore, Natalie B. Steinhauser,
and Gwendolyn C. Campbell. 2009. Dealing with
interpretation errors in tutorial dialogue. In Pro-
ceedings of SIGDIAL-09, London, UK, Sep.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010. The
impact of interpretation problems on tutorial dia-
logue. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics(ACL-
2010).
Michael Elhadad and Jacques Robin. 1992. Control-
ling content realization with functional unification
grammars. In R. Dale, E. Hovy, D. Ro?sner, and
O. Stock, editors, Proceedings of the Sixth Interna-
tional Workshop on Natural Language Generation,
pages 89?104, Berlin, April. Springer-Verlag.
A. C. Graesser, P. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simula-
tion of a human tutor. Cognitive Systems Research,
1:35?51.
Beth Ann Hockey, Oliver Lemon, Ellen Campana,
Laura Hiatt, Gregory Aist, James Hieronymus,
Alexander Gruenstein, and John Dowding. 2003.
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance.
In Proceedings of the tenth conference on European
chapter of the Association for Computational Lin-
guistics, pages 147?154, Morristown, NJ, USA.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Diane Litman, Carolyn P. Rose?, Kate Forbes-Riley,
Kurt VanLehn, Dumisizwe Bhembe, and Scott Sil-
liman. 2006. Spoken versus typed human and com-
puter dialogue tutoring. International Journal of Ar-
tificial Intelligence in Education, 16:145?170.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Generalizing tutorial dia-
logue results. In Proceedings of 14th International
Conference on Artificial Intelligence in Education
(AIED), Brighton, UK, July.
Rodney D. Nielsen, Wayne Ward, and James H. Mar-
tin. 2008. Learning to assess low-level conceptual
understanding. In Proceedings 21st International
FLAIRS Conference, Coconut Grove, Florida, May.
Amruta Purandare and Diane Litman. 2008. Content-
learning correlations in spoken tutoring dialogs at
word, turn and discourse levels. In Proceedings 21st
International FLAIRS Conference, Coconut Grove,
Florida, May.
C.P. Rose? and C. Torrey. 2005. Interactivity versus ex-
pectation: Eliciting learning oriented behavior with
tutorial dialogue systems. In Proceedings of Inter-
act?05.
N. B. Steinhauser, L. A. Butler, and G. E. Campbell.
2007. Simulated tutors in immersive learning envi-
ronments: Empirically-derived design principles. In
Proceedings of the 2007 Interservice/Industry Train-
ing, Simulation and Education Conference, Orlando,
FL.
Kurt VanLehn, Pamela Jordan, and Diane Litman.
2007. Developing pedagogically effective tutorial
dialogue tactics: Experiments and a testbed. In Pro-
ceedings of SLaTE Workshop on Speech and Lan-
guage Technology in Education, Farmington, PA,
October.
Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of 19th International FLAIRS (Florida Ar-
tificial Intelligence Research Society) Conference,
Melbourne Beach, FL.
18
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 293?299,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Improving interpretation robustness in a tutorial dialogue system
Myroslava O. Dzikovska and Elaine Farrow and Johanna D. Moore
School of Informatics, University of Edinburgh
Edinburgh, EH8 9AB, United Kingdom
{m.dzikovska,elaine.farrow,j.moore}@ed.ac.uk
Abstract
We present an experiment aimed at improv-
ing interpretation robustness of a tutorial dia-
logue system that relies on detailed semantic
interpretation and dynamic natural language
feedback generation. We show that we can
improve overall interpretation quality by com-
bining the output of a semantic interpreter
with that of a statistical classifier trained on
the subset of student utterances where seman-
tic interpretation fails. This improves on a pre-
vious result which used a similar approach but
trained the classifier on a substantially larger
data set containing all student utterances. Fi-
nally, we discuss how the labels from the sta-
tistical classifier can be integrated effectively
with the dialogue system?s existing error re-
covery policies.
1 Introduction
Giving students formative feedback as they inter-
act with educational applications, such as simu-
lated training environments, problem-solving tutors,
serious games, and exploratory learning environ-
ments, is known to be important for effective learn-
ing (Shute, 2008). Suitable feedback can include
context-appropriate confirmations, hints, and sug-
gestions to help students refine their answers and
increase their understanding of the subject. Pro-
viding this type of feedback automatically, in nat-
ural language, is the goal of tutorial dialogue sys-
tems (Aleven et al, 2002; Dzikovska et al, 2010b;
Graesser et al, 1999; Jordan et al, 2006; Litman and
Silliman, 2004; Khuwaja et al, 1994; Pon-Barry et
al., 2004; VanLehn et al, 2007).
Much work in NLP for educational applications
has focused on automated answer grading (Leacock
and Chodorow, 2003; Pulman and Sukkarieh, 2005;
Mohler et al, 2011). Automated answer assess-
ment systems are commonly trained on large text
corpora. They compare the text of a student answer
with the text of one or more reference answers sup-
plied by human instructors and calculate a score re-
flecting the quality of the match. Automated grad-
ing methods are integrated into intelligent tutoring
systems (ITS) by having system developers antic-
ipate both correct and incorrect responses to each
question, with the system choosing the best match
(Graesser et al, 1999; Jordan et al, 2006; Litman
and Silliman, 2004; VanLehn et al, 2007). Such
systems have wide domain coverage and are robust
to ill-formed input. However, as matching relies on
shallow features and does not provide semantic rep-
resentations of student answers, this approach is less
suitable for dynamically generating adaptive natural
language feedback (Dzikovska et al, 2013).
Real-time simulations and serious games are
commonly used in STEM learning environments
to increase student engagement and support ex-
ploratory learning (Rutten et al, 2012; Mayo, 2007).
Natural language dialogue can help improve learn-
ing in such systems by asking students to explain
their reasoning, either directly during interaction, or
during post-problem reflection (Aleven et al, 2002;
Pon-Barry et al, 2004; Dzikovska et al, 2010b).
Interpretation of student answers in such systems
needs to be grounded in the current state of a dynam-
ically changing environment, and feedback may also
be generated dynamically to reflect the changing
system state. This is typically achieved by employ-
ing hand-crafted parsers and semantic interpreters to
produce structured semantic representations of stu-
dent input, which are then used to instantiate ab-
293
stract tutorial strategies with the help of a natural
language generation system (Freedman, 2000; Clark
et al, 2005; Dzikovska et al, 2010b).
Rule-based semantic interpreters are known to
suffer from robustness and coverage problems, fail-
ing to interpret out-of-grammar student utterances.
In the event of an interpretation failure, most sys-
tems have little information on which to base a feed-
back decision and typically respond by asking the
student to rephrase, or simply give away the answer
(though more sophisticated strategies are sometimes
possible, see Section 4). While statistical scoring ap-
proaches are more robust, they may still suffer from
coverage issues when system designers fail to antic-
ipate the full range of expected student answers. In
one study of a statistical system, a human judge la-
beled 33% of student utterances as not matching any
of the anticipated responses, meaning that the sys-
tem had no information to use as a basis for choos-
ing the next action and fell back on a single strategy,
giving away the answer (Jordan et al, 2009).
Recently, Dzikovska et al (2012b) developed an
annotated corpus of student responses (henceforth,
the SRA corpus) with the goal of facilitating dy-
namic generation of tutorial feedback.1 Student re-
sponses are assigned to one of 5 domain- and task-
independent classes that correspond to typical flaws
found in student answers. These classes can be used
to help a system choose a feedback strategy based
only on the student answer and a single reference
answer. Dzikovska et al (2013) showed that a sta-
tistical classifier trained on this data set can be used
in combination with a semantic interpreter to sig-
nificantly improve the overall quality of natural lan-
guage interpretation in a dialogue-based ITS. The
best results were obtained by using the classifier
to label the utterances that the semantic interpreter
failed to process.
In this paper we further extend this result by
showing that we can obtain similar results by train-
ing the classifier directly on the subset of utterances
that cannot be processed by the interpreter. The
distribution of labels across the classes is differ-
ent in this subset compared to the rest of the cor-
pus. Therefore we can train a subset-specific classi-
1http://www.cs.york.ac.uk/semeval-2013/
task7/index.php?id=data
fier, reducing the amount of annotated training data
needed without compromising performance of the
combined system.
The rest of the paper is organized as follows. In
Section 2 we describe an architecture for combining
semantic interpretation and classification in a sys-
tem with dynamic natural language feedback gener-
ation. In Section 3 we describe an experiment to im-
prove combined system performance using a classi-
fier trained only on non-interpretable utterances. We
discuss future improvements in Section 4.
2 Background
The SRA corpus is made up of two subsets: (1)
the SciEntsBank subset, consisting of written re-
sponses to assessment questions (Nielsen et al,
2008b), and (2) the Beetle subset consisting of ut-
terances collected from student interactions with the
BEETLE II tutorial dialogue system (Dzikovska et
al., 2010b). The SRA corpus annotation scheme
defines 5 classes of student answers (?correct?,
?partially-correct-incomplete?, ?contradictory?, ?ir-
relevant? and ?non-domain?). Each utterance is as-
signed to one of the 5 classes based on pre-existing
manual annotations (Dzikovska et al, 2012b).
We focus on the Beetle subset because the Beetle
data comes from an implemented system, meaning
that we also have access to the semantic interpreta-
tions of student utterances produced by the BEETLE
II interpretation component. The system uses fine-
grained semantic analysis to produce detailed diag-
noses of student answers in terms of correct, incor-
rect, missing and irrelevant parts. We developed a
set of rules to map these diagnoses onto the SRA
corpus 5-class annotation scheme to support system
evaluation (Dzikovska et al, 2012a).
In our previous work (Dzikovska et al, 2013), we
used this mapping as the basis for combining the
output of the BEETLE II semantic interpreter with
the output of a statistical classifier, using a rule-
based policy to determine which label to use for
each instance. If the label from the semantic in-
terpreter is chosen, then the full range of detailed
feedback strategies can be used, based on the corre-
sponding semantic representation. If the classifier?s
label is chosen, then the system can fall back to us-
ing content-free prompts, choosing an appropriate
294
prompt based on the SRA corpus label.
We evaluated 3 rule-based combination policies,
chosen to reduce the effects of the errors that the
semantic interpreter makes, and taking into account
tutoring goals such as reducing student frustration.
The best performing policy takes the classifier?s out-
put if and only if the semantic interpreter is unable
to process the utterance.2 This allows the system to
choose from a wider set of content-free prompts in-
stead of always telling the student that the utterance
was not understood.
As discussed earlier, non-interpretable utterances
present a problem for both rule-based and statistical
approaches. Therefore, we carried out an additional
set of experiments, focusing on the performance of
system combinations that use policies designed to
address non-interpretable utterances. We discuss our
results and future directions in the rest of the paper.
3 Improving Interpretation Robustness
3.1 Experimental Setup
The Beetle portion of the SRA corpus contains 3941
unique student answers to 47 different explanation
questions. Each question is associated with one or
more reference answers provided by expert tutors,
and each student answer is manually annotated with
the label assigned by the BEETLE II interpreter and
a gold-standard correctness label.
In our experiments, we follow the procedure de-
scribed in (Dzikovska et al, 2013), using 10-fold
cross-validation to evaluate the performance of the
various stand-alone and combined systems. We re-
port the per-class F1 scores as evaluation metrics,
using the macro-averaged F1 score as the primary
evaluation metric.
Dzikovska et al (2013) used a statistical classi-
fier based on lexical overlap, taken from (Dzikovska
et al, 2012a), and evaluated 3 different rule-based
policies for combining its output with that of the se-
mantic interpreter. In two of those policies the inter-
preter?s output is always used if it is available, and
the classifier?s label is used for a (subset of) non-
interpretable utterances:
1. NoReject: the classifier?s label is used in all
cases where semantic interpretation fails, thus
2We will refer to such utterances as ?non-interpretable? fol-
lowing (Bohus and Rudnicky, 2005).
creating a system that never rejects student in-
put as non-interpretable
2. NoRejectCorrect: the classifier?s label is
used for non-interpretable utterances which are
labeled as ?correct? by the classifier. This more
conservative policy aims to ensure that correct
student answers are always accepted, but incor-
rect answers may still be rejected with a request
to rephrase.
We conducted a new experiment to evaluate these
two policies together with an enhanced classifier,
discussed in the next section.
3.2 Classifier
For this paper, we extended the classifier from the
previous study (Dzikovska et al, 2013), which we
will call Sim8, with additional features to improve
handling of lexical variability and negation.
Sim8 uses the Weka 3.6.2 implementation of
C4.5 pruned decision trees, with default parameters.
It uses 8 features based on lexical overlap similarity
metrics provided by Perl?s Text::Similarity
package v.0.09: 4 metrics measuring overlap be-
tween the student answer and the expected answer,
and the same 4 metrics applied to the student?s an-
swer and the question text.
In our enhanced classifier, Sim20, we extended
the baseline feature set with 12 additional features.
8 of these are direct analogs of the baseline features,
this time computed on the stemmed text to reduce
the impact of syntactic variation, using the Porter
stemmer from the Lingua::Stem package.3 In
addition, 4 features were added to improve negation
handling and thus detection of contradictions. These
are:
? QuestionNeg, AnswerNeg: features in-
dicating the presence of a negation marker
in the question and the student?s answer re-
spectively, detected using a regular expression.
We distinguish three cases: a negation marker
3We also experimented with features that involve removing
stop words before computing similarity scores, and with using
SVMs for classification, but failed to obtain better performance.
We continue to investigate different SVM kernels and alterna-
tive classification algorithms such as random forests for our fu-
ture work.
295
Standalone Sem. Interp. + Sim20 Sem. Interp. + Sim20NI
Sem. Interp. Sim8 Sim20 no rej no rej corr no rej no rej corr
correct 0.66 0.71 0.71 0.70 0.70 0.70 0.70
pc inc 0.48 0.38 0.40 0.51 0.48 0.50 0.48
contra 0.27 0.40 0.45 0.47 0.27 0.51 0.27
irrlvnt 0.21 0.05 0.08 0.22 0.21 0.22 0.21
nondom 0.65 0.73 0.78 0.83 0.65 0.83 0.65
macro avg 0.45 0.45 0.48 0.55 0.46 0.55 0.46
Table 1: F1 scores for three stand-alone systems, and for combination systems using the Sim20 and Sim20NI
classifiers together with the semantic interpreter. Stand-alone performance for Sim20NI is not shown since it was
trained only on the non-interpretable data subset and is therefore not applicable for the complete data set.
likely to be associated with domain content
(e.g., ?not connected?); a negation marker more
likely to be associated with general expressions
of confusion (such as ?don?t know?); and no
negation marker present.
? BestOverlapNeg: true if the reference an-
swer that has the highest F1 overlap with the
student answer includes a negation marker.
? BestOverlapPolarityMatch: a flag
computed from the values of AnswerNeg and
BestOverlapNeg. Again, we distinguish
three cases: they have the same polarity (both
the student answer and the reference answer
contain negation markers, or both have no
negation markers); they have opposite polar-
ity; or the student answer contains a negation
marker associated with an expression of confu-
sion, as described above.
3.3 Evaluation
Evaluation results are shown in Table 1. Unless
otherwise specified, all performance differences dis-
cussed in the text are significant on an approximate
randomization significance test with 10,000 itera-
tions (Yeh, 2000).
Adding the new features to create the Sim20
classifier resulted in a performance improvement
compared to the Sim8 classifier, raising macro-
averaged F1 from 0.45 to 0.48, with an improvement
in contradiction detection as intended. But these im-
provements did not translate into improvements in
the combined systems. Combinations using Sim20
performed exactly the same as the combinations us-
ing Sim8 (not shown due to space limitations, see
(Dzikovska et al, 2013)). Clearly, more sophisti-
cated features are needed to obtain further perfor-
mance gains in the combined systems.
However, we noted that the subset of non-
interpretable utterances in the corpus has a differ-
ent distribution of labels compared to the full data
set. In the complete data set, 1665 utterances (42%)
are labeled as correct and 1049 (27%) as contradic-
tory. Among the 1416 utterances considered non-
interpretable by the semantic interpreter, 371 (26%)
belong to the ?correct? class, and 598 (42%) to ?con-
tradictory? (other classes have similar distributions
in both subsets). We therefore hypothesized that a
combination system that uses the classifier output
only if an utterance is non-interpretable, may ben-
efit from employing a classifier trained specifically
on this subset rather than on the whole data set.
If our hypothesis is true, it offers an interesting
possibility for combining rule-based and statistical
classifiers in similar setups: if the classifier can be
trained using only the examples that are problematic
for the rule-based system, it can provide improved
robustness at a significantly lower annotation cost.
We therefore trained another classifier,
Sim20NI, using the same feature set as Sim20,
but this time using only the instances rejected
as non-interpretable by the semantic interpreter
in each cross-validation fold (1416 utterances,
36% of all data instances). We again used the
NoReject and NoRejectCorrect policies to
combine the output of Sim20NI with that of the
semantic interpreter. Evaluation results confirmed
our hypothesis. The system combinations that
use Sim20 and Sim20NI perform identically on
296
macro-averaged F1, with NoReject being the best
combination policy in both cases and significantly
outperforming the semantic interpreter alone. How-
ever, the Sim20NI classifier has the advantage of
needing significantly less annotated data to achieve
this performance.
4 Discussion and Future Work
Our research focuses on combining deep and shal-
low processing by supplementing fine-grained se-
mantic interpretations from a rule-based system
with more coarse-grained classification labels. Al-
ternatively, we could try to learn structured se-
mantic representations from annotated text (Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al, 2010), or to learn more fine-
grained assessment labels (Nielsen et al, 2008a).
However, such approaches require substantially
larger annotation effort. Therefore, we believe it is
worth exploring the use of the simpler 5-label anno-
tation scheme from the SRA corpus. We previously
showed that it is possible to improve system perfor-
mance by combining the output of a symbolic inter-
preter with that of a statistical classifier (Dzikovska
et al, 2013). The best combination policy used the
statistical classifier to label utterances rejected as
non-interpretable by the rule-based interpreter.
In this paper, we showed that similar results can
be achieved by training the classifier only on non-
interpretable utterances, rather than on the whole la-
beled corpus. The student answers that the inter-
preter has difficulty with have a distinct distribution,
which is effectively utilized by training a classifier
only on this subset. This reduces the amount of an-
notated training data needed, reducing the amount of
manual labor required.
In future, we will further investigate the best com-
bination of parsing and statistical classification in
systems that offer sophisticated error recovery poli-
cies for non-understandings. Our top-performing
policy, NoReject, uses deep parsing and semantic
interpretation to produce a detailed semantic analy-
sis for the majority of utterances, and falls back on a
shallower statistical classifier for utterances that are
difficult for the interpreter. This policy assumes that
it is always better to use a content-free prompt than
to reject a non-interpretable student utterance. How-
ever, interpretation problems can arise from incor-
rect uses of terminology, and learning to speak in
the language of the domain has been positively cor-
related with learning outcomes (Steinhauser et al,
2011). Therefore, rejecting some non-interpretable
answers as incorrect could be a valid tutoring strat-
egy (Sagae et al, 2010; Dzikovska et al, 2010a).
The BEETLE II system offers several error re-
covery strategies intended to help students phrase
their answers in more acceptable ways by giving a
targeted help message, e.g., ?I am sorry, I?m hav-
ing trouble understanding. Paths cannot be broken,
only components can be broken? (Dzikovska et al,
2010a). Therefore, it may be worthwhile to con-
sider other combination policies. We evaluated the
NoRejectCorrect policy, which uses the statis-
tical classifier to identify correct answers rejected
by the semantic interpreter and asks for rephrasings
in other cases. Using this policy resulted in only a
small improvement in system performance. A dif-
ferent classifier geared towards more accurate iden-
tification of correct answers may help, and we are
planning to investigate this option in the future.
Alternatively, we could consider a combination
policy which looks for rejected answers that the
classifier identifies as contradictory and changes the
wording of the targeted help message to indicate that
the student may have made a mistake, instead of
apologizing for the misunderstanding. This has the
potential to help students learn correct terminology
rather than presenting the issue as strictly an inter-
pretation failure.
Ultimately, all combination policies must be
tested with users to ensure that improved robust-
ness translates into improved system effectiveness.
We have previously studied the effectiveness of our
targeted help strategies with respect to improving
learning outcomes (Dzikovska et al, 2010a). A sim-
ilar study is required to evaluate our combination
strategies.
Acknowledgments
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine and Sarah Denhe
for help with data collection and preparation. The
research reported here was supported by the US
ONR award N000141010085.
297
References
Vincent Aleven, Octav Popescu, and Kenneth R.
Koedinger. 2002. Pilot-testing a tutorial dialogue sys-
tem that supports self-explanation. In Proc. of ITS-02
conference, pages 344?354.
Dan Bohus and Alexander Rudnicky. 2005. Sorry,
I didn?t catch that! - An investigation of non-
understanding errors and recovery strategies. In Pro-
ceedings of SIGdial-2005, Lisbon, Portugal.
Brady Clark, Oliver Lemon, Alexander Gruenstein, Eliz-
abethOwen Bratt, John Fry, Stanley Peters, Heather
Pon-Barry, Karl Schultz, Zack Thomsen-Gray, and
Pucktada Treeratpituk. 2005. A general purpose ar-
chitecture for intelligent tutoring systems. In JanC.J.
Kuppevelt, Laila Dybkjr, and NielsOle Bernsen, edi-
tors, Advances in Natural Multimodal Dialogue Sys-
tems, volume 30 of Text, Speech and Language Tech-
nology, pages 287?305. Springer Netherlands.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, and Gwendolyn Campbell. 2010a. The
impact of interpretation problems on tutorial dialogue.
In Proc. of ACL 2010 Conference Short Papers, pages
43?48.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012a. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proc. of EACL-12 Confer-
ence, pages 471?481.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012b. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Myroslava O. Dzikovska, Elaine Farrow, and Johanna D.
Moore. 2013. Combining semantic interpretation and
statistical classification for improved explanation pro-
cessing in a tutorial dialogue system. In In Proceed-
ings of the The 16th International Conference on Ar-
tificial Intelligence in Education (AIED 2013), Mem-
phis, TN, USA, July.
Reva Freedman. 2000. Using a reactive planner as the
basis for a dialogue agent. In Proceedings of the Thir-
teenth Florida Artificial Intelligence Research Sympo-
sium (FLAIRS 2000), pages 203?208.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Pamela Jordan, Diane Litman, Michael Lipschultz, and
Joanna Drummond. 2009. Evidence of misunder-
standings in tutorial dialogue and their impact on
learning. In Proc. of 14th International Conference on
Artificial Intelligence in Education, pages 125?132.
Ramzan A. Khuwaja, Martha W. Evens, Joel A. Michael,
and Allen A. Rovick. 1994. Architecture of
CIRCSIM-tutor (v.3): A smart cardiovascular physi-
ology tutor. In Proc. of 7th Annual IEEE Computer-
Based Medical Systems Symposium.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proc. of EMNLP-2010 Conference,
pages 1223?1233.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Diane J. Litman and Scott Silliman. 2004. ITSPOKE:
an intelligent tutoring spoken dialogue system. In
Demonstration Papers at HLT-NAACL 2004, pages 5?
8, Boston, Massachusetts.
Merrilea J. Mayo. 2007. Games for science and engi-
neering education. Commun. ACM, 50(7):30?35, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
298
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Nico Rutten, Wouter R. van Joolingen, and Jan T. van der
Veen. 2012. The learning effects of computer simula-
tions in science education. Computers and Education,
58(1):136 ? 153.
Alicia Sagae, W. Lewis Johnson, and Stephen Bodnar.
2010. Validation of a dialog system for language
learners. In Proceedings of the 11th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 241?244, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Valerie J Shute. 2008. Focus on formative feedback.
Review of educational research, 78(1):153?189.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proc. of
15th international conference on Artificial Intelligence
in Education, pages 361?368.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing with
lambda calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2007), Prague, Czech Republic, June.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In Proceedings of the 21th Annual Conference on
Uncertainty in Artificial Intelligence (UAI-05), pages
658?666, Arlington, Virginia. AUAI Press.
299
