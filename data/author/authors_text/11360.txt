Proceedings of NAACL HLT 2009: Short Papers, pages 217?220,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Statistical Post-Editing of a
Rule-Based Machine Translation System?
A.-L. Lagarda, V. Alabau, F. Casacuberta
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia, Spain
alagarda@iti.upv.es
R. Silva, and E. D??az-de-Lian?o
Celer Soluciones, S.L.
Madrid, Spain
Abstract
Automatic post-editing (APE) systems aim at
correcting the output of machine translation
systems to produce better quality translations,
i.e. produce translations can be manually post-
edited with an increase in productivity. In this
work, we present an APE system that uses sta-
tistical models to enhance a commercial rule-
based machine translation (RBMT) system. In
addition, a procedure for effortless human eva-
luation has been established. We have tested
the APE system with two corpora of differ-
ent complexity. For the Parliament corpus, we
show that the APE system significantly com-
plements and improves the RBMT system. Re-
sults for the Protocols corpus, although less
conclusive, are promising as well. Finally,
several possible sources of errors have been
identified which will help develop future sys-
tem enhancements.
1 Introduction
Current machine translation systems are far from
perfect. To achieve high-quality output, the raw
translations they generate often need to be corrected,
or post-edited by human translators. One way of in-
creasing the productivity of the whole process is the
development of automatic post-editing (APE) sys-
tems (Dugast et al, 2007; Simard et al, 2007).
? Work supported by the EC (FEDER) and the Spanish
MEC under grant TIN2006-15694-CO2-01, by the Spanish
research programme Consolider Ingenio 2010:MIPRCV
(CSD2007-00018), and by the i3media Cenit project (CDTI
2007-1012).
Many of these works propose a combination of
rule-based machine translation (RBMT) and statisti-
cal machine translation (SMT) systems, in order to
take advantage of the particular capabilities of each
system (Chen and Chen, 1997).
A possible combination is to automatically post-
edit the output of a RBMT system employing a SMT
system. In this work, we will apply this technique
into two different corpora: Parliament and Proto-
cols. In addition, we will propose a new human eva-
luation measure that will deal with the impact of the
automatic post-editing.
This paper is structured as follows: after a brief
introduction of the RBMT, SMT, and APE systems
in Section 2, Section 3 details the carried out experi-
mentation, discussing its results. Finally, some con-
clusions and future work are presented in Section 4.
2 Systems description
Three different systems are compared in this work,
namely the RBMT, SMT, and APE systems.
Rule-based machine translation. RBMT was the
first approach to machine translation, and thus, a
relatively mature area in this field. RBMT sys-
tems are basically constituted by two components:
the rules, that account for the syntactic knowledge,
and the lexicon, which deals with the morphologi-
cal, syntactic, and semantic information. Both rules
and lexicons are grounded on linguistic knowledge
and generated by expert linguists. As a result, the
build process is expensive and the system is difficult
to maintain (Bennett and Slocum, 1985). Further-
more, RBMT systems fail to adapt to new domains.
217
Although they usually provide a mechanism to cre-
ate new rules and extend and adapt the lexicon,
changes are usually very costly and the results, fre-
quently, do not pay off (Isabelle et al, 2007).
Statistical machine translation. In SMT, transla-
tions are generated on the basis of statistical models,
which are derived from the analysis of bilingual text
corpora. The translation problem can be statistically
formulated as in (Brown et al, 1993). In practice,
several models are often combined into a log-linear
fashion. Each model can represent an important fea-
ture for the translation, such as phrase-based, lan-
guage, or lexical models (Koehn et al, 2003).
Automatic post-editing. An APE system can be
viewed as a translation process between the output
from a previous MT system, and the target language.
In our case, an APE system based on statistical mod-
els will be trained to correct the translation errors
made by a RBMT system. As a result, both RBMT
and SMT technologies will be combined in order to
increase the overall translation quality.
3 Experiments
We present some experiments carried out using the
introduced APE system, and comparing its perfor-
mance with that of the RBMT and SMT systems.
In the experimentation, two different English-to-
Spanish corpora have been chosen, Parliament and
Protocols, both of them provided by a professional
translation agency.
Corpora. The Parliament corpus consists of a se-
ries of documents from proceedings of parliamen-
tary sessions, provided by a client of the transla-
tion agency involved in this work. Most of the sen-
tences are transcriptions of parliamentary speeches,
and thus, with the peculiarities of the oral language.
Despite of the multi-topic nature of the speeches,
differences in training and test perplexities indicate
that the topics in test are well represented in the
training set (corpus statistics in Table 1).
On the other hand, the Protocols corpus is a
collection of medical protocols. This is a more
difficult task, as its statistics reflect in Table 1. There
are many factors that explain this complexity, such
as the different companies involved in training and
test sets, out-of-domain test data (see perplexity and
Table 1: Corpus statistics for Parliament and Protocols.
OOV stands for out-of-vocabulary words.
Parliament Protocols
En Sp En Sp
Tra
ini
ng Sentences 90K 90K 154K 154KRun. words 2.3M 2.5M 3.2M 3.6M
Vocabulary 29K 45K 41K 47K
Perplexity 42 37 21 19
Te
st
Sentences 1K 1K 3K 3K
Run. words 33K 33K 54K 71K
OOVs 157 219 2K 1.7K
Perplexity 44 43 131 173
out-of-vocabulary words), non-native authors, etc.
Evaluation. In order to assess the proposed sys-
tems, a series of measures have been considered. In
first place, some state-of-the-art automatic metrics
have been chosen to give a first idea of the quality of
the translations. These translations have been also
evaluated by professional translators to assess the in-
crease of productivity when using each system.
Automatic evaluation. The automatic assessment
of the translation quality has been carried out us-
ing the BiLingual Evaluation Understudy (BLEU)
(Papineni et al, 2002), and the Translation Error
Rate (TER) (Snover et al, 2006). The latter takes
into account the number of edits required to con-
vert the system output into the reference. Hence, this
measure roughly estimates the post-edition process.
Human evaluation. A new human evaluation
measure has been proposed to roughly estimate
the productivity increase when using each of the
systems in a real scenario, grounded on previous
works for human evaluation of qualitative fac-
tors (Callison-Burch et al, 2007). One of the de-
sired qualities for this measure was that it should
pose little effort to the human evaluator. Thus, a
binary measure was chosen, the suitability, where
the translations are identified as suitable or not sui-
table. A given translation is considered to be suitable
if it can be manually post-edited with effort savings,
i.e., the evaluator thinks that a manual post-editing
will increase his productivity. On the contrary, if the
evaluator prefers to ignore the proposed translation
and start it over, the sentence is deemed not suitable.
218
Significance tests. Significance of the results has
been assessed by the paired bootstrap resampling
method, described in (Koehn, 2004). It estimates
how confidently the conclusion that a system outper-
forms another one can be drawn from a test result.
Experimental setup. Rule-based translation was
performed by means of a commercial RBMT system.
On the other hand, statistical training and translation
in both SMT and APE systems were carried out using
the Moses toolkit (Koehn et al, 2007). It should be
noted that APE system was trained taking the RBMT
output as source, instead of the original text. In this
way, it is able to post-edit the RBMT translations.
Finally, the texts employed for the human eva-
luation were composed by 350 sentences randomly
drawn from each one of the two test corpora des-
cribed in this paper. Two professional translators
carried out the human evaluation.
3.1 Results and discussion
Experimentation results in terms of automatic and
human evaluation are shown in this section.
Automatic evaluation. Table 2 presents Parlia-
ment and Protocols corpora translation results in
terms of automatic metrics. Note that, as there is
a single reference, this results are somehow pes-
simistic.
In the case of the Parliament corpus, SMT system
outperforms the rest of the systems. APE results are
slightly worse than SMT, but far better than RBMT.
However, when moving to the Protocols corpus, a
more difficult task (as seen in perplexity in Table 1),
the results show quite the contrary. SMT and APE
systems show how they are more sensitive to out-
of-domain documents. Nevertheless, the RBMT sys-
tem seems to be more robust under such conditions.
Despite of the degradation of the statistical models,
APE manages to achieve much better results than the
other two systems. It is able to conserve the robust-
ness of RBMT, while its statistical counterpart deals
with the particularities of the corpus.
Human evaluation. Table 3 shows the percentage
of translations deemed suitable by the human eva-
luators. Two professional evaluators analysed the
suitability of the output of each system
In the Parliament case, APE performance is found
much more suitable than the rest of the systems. In
Table 2: Automatic evaluation for Parliament and Proto-
cols tests.
Parliament Protocols
BLEU TER BLEU TER
RBMT 29.1 46.7 29.5 48.0
SMT 49.9 34.9 22.4 59.6
APE 48.4 35.9 33.6 46.2
fact, this difference between APE and the rest is sta-
tistically significant at a 99% level of confidence.
In addition, significance tests show that, on average,
APE improves RBMT on 59.5% of translations.
Regarding to the Protocols corpus, it must be
noted that a first review of the translations pointed
out that the SMT system performed quite poorly.
Hence, SMT was not considered for the human eva-
luation on this corpus.
Figures show that APE complements and im-
proves RBMT, although differences between them
are tighter than in the Parliament corpus. However,
significance tests still prove that these improvements
are statistically significant (68% of confidence), and
that the average improvement is 6.5%.
Table 3: Human evaluation for Parliament and Protocols
corpora. Percentage of suitable translated sentences for
each system.
Parliament Protocols
RBMT 58 60
SMT 60 ?
APE 94 67
It is interesting to note how automatic measures
and human evaluation seem not to be quite corre-
lated. In terms of automatic measures, the best sys-
tem to translate the Parliament test is the SMT. This
improvement has been checked by carrying out sig-
nificance tests, resulting statistically significant with
a 99% of confidence. However, in the human eva-
luation, SMT is worse than APE (this difference is
also significant at 99%). On the other hand, when
working with the Protocols corpus, automatic me-
trics indicate that APE improves the rest (significant
improvement at 99%). Nevertheless, human evalua-
tors seem to think that the difference between APE
and RBMT is not so significant, only with a confi-
dence of 68%. Previous works confirm this apparent
219
discrepancy between automatic and human evalua-
tions (Callison-Burch et al, 2007).
Translator?s commentaries. As a subproduct of
the human evaluation, the evaluators gave some
personal impressions regarding each system perfor-
mance. They concluded that, when working with the
Parliament corpus, there was a net improvement in
the overall performance when using APE. Changes
between RBMT and APE were minor but useful.
Thus, APE did not pose a system degradation with
respect to the RBMT. Furthermore, a rough estima-
tion indicated that over 10% of the sentences were
perfectly translated, i.e. the translation was human-
like. In addition, some frequent collocations were
found to be correctly post-edited by the APE system,
which was felt very effort saving.
With respect to the Protocols corpus, as expected,
results were found not so satisfactory. However,
human translators find themselves these documents
complex.
Finally, in both cases, APE is able to make the
translation more similar to the reference by fix-
ing some words without altering the grammatical
structure of the sentence. Finally, translators would
find very useful a system that automatically decided
when to automatically post-edit the RBMT outputs.
4 Conclusions
We have presented an automatic post-editing sys-
tem that can be added at the core of the professional
translation workflow. Furthermore, we have tested it
with two corpora of different complexity.
For the Parliament corpus, we have shown that
the APE system complements and improves the
RBMT system in terms of suitability in a real transla-
tion scenario (average improvement 59.5%). Results
for the Protocols corpus, although less conclusive,
are promising as well (average improvement 6.5%).
Moreover, 67% of Protocols translations, and 94%
of Parliament translations were considered to be sui-
table.
Finally, a procedure for effortless human eva-
luation has been established. A future improve-
ment for this would be to integrate the process in the
core of the translator?s workflow, so that on-the-fly
evaluation can be made. In addition, several pos-
sible sources of errors have been identified which
will help develop future system enhancements. For
example, as stated in the translator?s commentaries,
the automatic selection of the most suitable transla-
tion among the systems is a desirable feature.
References
W. S. Bennett and J. Slocum. 1985. The lrc machine
translation system. Comp. Linguist., 11(2-3):111?121.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Comp. Linguist.,
19(2):263?312.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (meta-) evaluation of machine
translation. In Proc. of the 2nd Workshop on SMT,
pages 136?158, Prague, Czech Republic. ACL.
K. Chen and H. Chen. 1997. A hybrid approach to ma-
chine translation system design. In Comp. Linguist.
and Chinese Language Processing 23, pages 241?265.
L. Dugast, J. Senellart, and P. Koehn. 2007. Statisti-
cal post-editing on SYSTRAN?s rule-based translation
system. In Proc. of the 2nd Workshop on SMT, pages
220?223, Prague, Czech Republic. ACL.
P. Isabelle, C. Goutte, and M. Simard. 2007. Do-
main adaptation of mt systems through automatic post-
editing. In Proc. of MTSummit XI, pages 255?261,
Copenhagen, Denmark.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL-HLT,
pages 48?54, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL, pages
177?180, Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of EMNLP 2004,
Barcelona, Spain.
K. Papineni, S. Roukos, T. Ward, and W.-Jing Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318, Philadel-
phia, PA, USA.
M. Simard, C. Goutte, and P. Isabelle. 2007. Statis-
tical phrase-based post-editing. In Proc. of NAACL-
HLT2007, pages 508?515, Rochester, NY. ACL.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. of AMTA, pages
223?231.
220
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 25?28,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
CASMACAT: A Computer-assisted Translation Workbench
V. Alabau
?
, C. Buck
?
, M. Carl
?
, F. Casacuberta
?
, M. Garc??a-Mart??nez
?
U. Germann
?
, J. Gonz
?
alez-Rubio
?
, R. Hill
?
, P. Koehn
?
, L. A. Leiva
?
B. Mesa-Lao
?
, D. Ortiz
?
, H. Saint-Amand
?
, G. Sanchis
?
, C. Tsoukala
?
?
PRHLT Research Center, Universitat Polit`ecnica de Val`encia
{valabau,fcn,jegonzalez,luileito,dortiz,gsanchis}@dsic.upv.es
?
Copenhagen Business School, Department of International Business Communication
{ragnar.bonk,mc.isv,mgarcia,bm.ibc}@cbs.dk
?
School of Informatics, University of Edinburgh
{cbuck,ugermann,rhill2,pkoehn,hsamand,ctsoukal}@inf.ed.ac.uk
Abstract
CASMACAT is a modular, web-based
translation workbench that offers ad-
vanced functionalities for computer-aided
translation and the scientific study of hu-
man translation: automatic interaction
with machine translation (MT) engines
and translation memories (TM) to ob-
tain raw translations or close TM matches
for conventional post-editing; interactive
translation prediction based on an MT en-
gine?s search graph, detailed recording and
replay of edit actions and translator?s gaze
(the latter via eye-tracking), and the sup-
port of e-pen as an alternative input device.
The system is open source sofware and in-
terfaces with multiple MT systems.
1 Introduction
CASMACAT
1
(Cognitive Analysis and Statistical
Methods for Advanced Computer Aided Trans-
lation) is a three-year project to develop an
advanced, interactive workbench for computer-
assisted translation (CAT). Currently, at the end of
the second year, the tool includes an array of inno-
vative features that combine to offer a rich, user-
focused working environment not available in any
other CAT tool.
CASMACAT works in close collaboration with
the MATECAT project
2
, another open-source web-
based CAT tool. However, while MATECAT is
concerned with conventional CAT, CASMACAT is
focused on enhancing user interaction and facili-
tating the real-time involvement of human trans-
lators. In particular, CASMACAT provides highly
interactive editing and logging features.
1
http://www.casmacat.eu
2
http://www.matecat.com
Through this combined effort, we hope to foster
further research in the area of CAT tools that im-
prove the translation workflow while appealing to
both professional and amateur translators without
advanced technical skills.
GUI
web
server
CAT
server
MT
server
Javascript      PHP
    Python
  Python
web socket
HTTP
HTTP
Figure 1: Modular design of the workbench: Web-
based components (GUI and web server), CAT
server and MT server can be swapped out.
2 Design and components
The overall design of the CASMACAT workbench
is modular. The system consists of four com-
ponents. (1) a front-end GUI implemented in
HTML5 and JavaScript; (2) a back-end imple-
mented in PHP; (3) a CAT server that manages the
editing process and communicates with the GUI
through web sockets; (4) a machine translation
(MT) server that provides raw translation of source
text as well as additional information, such as a
search graph that efficiently encodes alternative
translation options. Figure 1 illustrates how these
components interact with each other. The CAT
and MT servers are written in Python and inter-
act with a number of software components imple-
mented in C++. All recorded information (source,
translations, edit logs) is permanently stored in a
MySQL database.
These components communicate through a
well-defined API, so that alternative implementa-
tions can be used. This modular architecture al-
25
Figure 2: Translation view for an interactive post-editing task.
lows the system to be used partially. For instance,
the CAT and MT servers can be used separately as
part of a larger translation workflow, or only as a
front-end when an existing MT solution is already
in place.
2.1 CAT server
Some of the interactive features of CASMACAT
require real-time interaction, such as interactive
text-prediction (ITP), so establishing an HTTP
connection every time would cause a significant
network overhead. Instead, the CAT server relies
on web sockets, by means of Python?s Tornadio.
When interactive translation prediction is en-
abled, the CAT server first requests a translation
together with the search graph of the current seg-
ment from the MT server. It keeps a copy of the
search graph and constantly updates and visualizes
the translation prediction based on the edit actions
of the human translator.
2.2 MT server
Many of the functions of the CAT server require
information from an MT server. This information
includes not only the translation of the input sen-
tence, but also n-best lists, search graphs, word
alignments, and so on. Currently, the CASMACAT
workbench supports two different MT servers:
Moses (Koehn et al., 2007) and Thot (Ortiz-
Mart??nez et al., 2005).
The main call to the MT server is a request for
a translation. The request includes the source sen-
tence, source and target language, and optionally
a user ID. The MT server returns an JSON object,
following an API based on Google Translate.
3 Graphical User Interface
Different views, based on the MATECAT GUI,
perform different tasks. The translation view is
the primary one, used when translating or post-
editing, including logging functions about the
translation/post-editing process. Other views im-
plement interfaces to upload new documents or to
manage the documents that are already in the sys-
tem. Additionally, a replay view can visualize all
edit actions for a particular user session, including
eye tracking information, if available.
3.1 Post-Editing
In the translation view (Figure 2), the document
is presented in segments and the assistance fea-
tures provided by CASMACAT work at the segment
level. If working in a post-editing task without
ITP, up to three MT or TM suggestions are pro-
vided for the user to choose. Keyboard shortcuts
are available for performing routine tasks, for in-
stance, loading the next segment or copying source
text into the edit box. The user can assign different
status to each segment, for instance, ?translated?
for finished ones or ?draft? for segments that still
need to be reviewed. Once finished, the translated
document can be downloaded in XLIFF format.
3
In the translation view, all user actions re-
lated to the translation task (e.g. typing activity,
mouse moves, selection of TM proposals, etc.) are
recorded by the logging module, collecting valu-
able information for off-line analyses.
3.2 Interactive Translation Prediction
Here we briefly describe the main advanced CAT
features implemented in the workbench so far.
Intelligent Autocompletion: ITP takes place
every time a keystroke is detected by the sys-
tem (Barrachina et al., 2009). In such event, the
system produces a prediction for the rest of the
sentence according to the text that the user has al-
ready entered. This prediction is placed at the right
of the text cursor.
Confidence Measures: Confidence mea-
sures (CMs) have two main applications in
3
XLIFF is a popular format in the translation industry.
26
MT (Gonz?alez-Rubio et al., 2010). Firstly, CMs
allow the user to clearly spot wrong translations
(e.g., by rendering in red those translations
with very low confidence according to the MT
module). Secondly, CMs can also inform the user
about the translated words that are dubious, but
still have a chance of being correct (e.g., rendered
in orange). Figure 3 illustrates this.
Figure 3: Visualisation of Confidence Measures
Prediction Length Control: Providing the user
with a new prediction whenever a key is pressed
has been proved to be cognitively demanding (Al-
abau et al., 2012). Therefore, the GUI just displays
the prediction up to the first wrong word according
to the CMs provided by the system (Figure 4).
Figure 4: Prediction Length Control
Search and Replace: Most of CAT tools pro-
vide the user with intelligent search and replace
functions for fast text revision. CASMACAT fea-
tures a straightforward function to run search and
replacement rules on the fly.
Word Alignment Information: Alignment of
source and target words is an important part of
the translation process (Brown et al., 1993). To
display their correspondence, they are hihglighted
every time the user places the mouse or the text
cursor on a word; see Figure 5.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 5: Visualisation of Word Alignment
Prediction Rejection: With the purpose of eas-
ing user interaction, CASMACAT also supports a
one-click rejection feature (Sanchis-Trilles et al.,
2008). This feature invalidates the current predic-
tion made for the sentence that is being translated,
and provides the user with an alternate one.
3.3 Replay mode and logging functions
The CASMACAT workbench implements detailed
logging of user activity data, which enables both
automatic analysis of translator behaviour and
retrospective replay of a user session. Replay
takes place in the translation view of the GUI
and it displays the screen status of the recorded
translation/post-editing process. The workbench
also features a plugin to enrich the replay mode
with gaze data coming from an eye-tracker. This
eye-tracking integration is possible through a
project-developed web browser extension which,
at the moment, has only been fully tested with SR-
Research EyeLinks
4
.
4 E-pen Interaction
E-pen interaction is intended to be a complemen-
tary input rather than a substitution of the key-
board. The GUI features the minimum compo-
nents necessary for e-pen interaction; see Figure 6.
When the e-pen is enabled, the display of the cur-
rent segment is changed so that the source seg-
ment is shown above the target segment. Then the
drawing area is maximised horizontally, facilitat-
ing handwriting, particularly in tablet devices. An
HTML canvas is also added over the target seg-
ment, where the user?s drawings are handled. This
is achieved by means of MINGESTURES (Leiva
et al., 2013), a highly accurate, high-performance
gesture set for interactive text editing that can dis-
tinguish between gestures and handwriting. Ges-
tures are recognised on the client side so the re-
sponse is almost immediate. Conversely, when
handwritten text is detected, the pen strokes are
sent to the server. The hand-written text recog-
nition (HTR) server is based on iAtros, an open
source HMM decoder.
if any feature not
is available on your network
substitution
Figure 6: Word substitution with e-pen interaction
5 Evaluation
The CASMACAT workbench was recently evalu-
ated in a field trial at Celer Soluciones SL, a
language service provider based in Spain. The
trial involved nine professional translators work-
ing with the workbench to complete different post-
editing tasks from English into Spanish. The pur-
4
http://www.sr-research.com
27
pose of this evaluation was to establish which of
the workbench features are most useful to profes-
sional translators. Three different configurations
were tested:
? PE: The CASMACAT workbench was used
only for conventional post-editing, without
any additional features.
? IA: Only the Intelligent Autocompletion fea-
ture was enabled. This feature was tested sep-
arately because it was observed that human
translators substantially change the way they
interact with the system.
? ITP: All features described in Section 3.2
were included in this configuration, except-
ing CMs, which were deemed to be not accu-
rate enough for use in a human evaluation.
For each configuration, we measured the aver-
age time taken by the translator to produce the fi-
nal translation (on a segment basis), and the aver-
age number of edits required to produce the final
translation. The results are shown in Table 1.
Setup Avg. time (s) Avg. # edits
PE 92.2 ? 4.82 141.39 ? 7.66
IA 86.07 ? 4.92 124.29 ? 7.28
ITP 123.3 ? 29.72 137.22 ? 13.67
Table 1: Evaluation of the different configurations
of the CASMACAT workbench. Edits are measured
in keystrokes, i.e., insertions and deletions.
While differences between these numbers are
not statistically significant, the apparent slowdown
in translation with ITP is due to the fact that all
translators had experience in post-editing but none
of them had ever used a workbench featuring in-
telligent autocompletion before. Therefore, these
were somewhat unsurprising results.
In a post-trial survey, translators indicated that,
on average, they liked the ITP system the best.
They were not fully satisfied with the freedom of
interactivity provided by the IA system. The lack
of any visual aid to control the intelligent auto-
completions provided by the system made transla-
tors think that they had to double-check any of the
proposals made by the system when making only
a few edits.
6 Conclusions
We have introduced the current CASMACAT work-
bench, a next-generation tool for computer as-
sisted translation. Each of the features available
in the most recent prototype of the workbench has
been explained. Additionally, we have presented
an executive report of a field trial that evaluated
genuine users? performance while using the work-
bench. Although E-pen interaction has not yet
been evaluated outside of the laboratory, it will the
subject of future field trials, and a working demon-
stration is available.
Acknowledgements
Work supported by the European Union 7
th
Framework Program (FP7/2007-2013) under the
CASMACAT project (grant agreement n
o
287576).
References
Vicent Alabau, Luis A. Leiva, Daniel Ortiz-Mart??nez,
and Francisco Casacuberta. 2012. User evaluation
of interactive machine translation systems. In Proc.
EAMT, pages 20?23.
Sergio Barrachina et al. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
Peter Brown et al. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2010. On the use of confi-
dence measures within an interactive-predictive ma-
chine translation system. In Proc. of EAMT.
Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL,
pages 177?180.
Luis A. Leiva, Vicent Alabau, and Enrique Vidal.
2013. Error-proof, high-performance, and context-
aware gestures for interactive text edition. In Proc.
of CHI, pages 1227?1232.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: a toolkit to train
phrase-based statistical translation models. In Proc.
of MT Summit X, pages 141?148.
G. Sanchis-Trilles et al. 2008. Improving interactive
machine translation via mouse actions. In Proc. of
EMNLP, pages 485?494.
28
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 389?394,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving On-line Handwritten Recognition using Translation Models
in Multimodal Interactive Machine Translation
Vicent Alabau, Alberto Sanchis, Francisco Casacuberta
Institut Tecnolo`gic d?Informa`tica
Universitat Polite`cnica de Vale`ncia
Cam?? de Vera, s/n, Valencia, Spain
{valabau,asanchis,fcn}@iti.upv.es
Abstract
In interactive machine translation (IMT), a hu-
man expert is integrated into the core of a ma-
chine translation (MT) system. The human ex-
pert interacts with the IMT system by partially
correcting the errors of the system?s output.
Then, the system proposes a new solution.
This process is repeated until the output meets
the desired quality. In this scenario, the in-
teraction is typically performed using the key-
board and the mouse. In this work, we present
an alternative modality to interact within IMT
systems by writing on a tactile display or us-
ing an electronic pen. An on-line handwrit-
ten text recognition (HTR) system has been
specifically designed to operate with IMT sys-
tems. Our HTR system improves previous ap-
proaches in two main aspects. First, HTR de-
coding is tightly coupled with the IMT sys-
tem. Second, the language models proposed
are context aware, in the sense that they take
into account the partial corrections and the
source sentence by using a combination of n-
grams and word-based IBM models. The pro-
posed system achieves an important boost in
performance with respect to previous work.
1 Introduction
Although current state-of-the-art machine transla-
tion (MT) systems have improved greatly in the last
ten years, they are not able to provide the high qual-
ity results that are needed for industrial and busi-
ness purposes. For that reason, a new interactive
paradigm has emerged recently. In interactive ma-
chine translation (IMT) (Foster et al, 1998; Bar-
rachina et al, 2009; Koehn and Haddow, 2009) the
system goal is not to produce ?perfect? translations
in a completely automatic way, but to help the user
build the translation with the least effort possible.
A typical approach to IMT is shown in Fig. 1. A
source sentence f is given to the IMT system. First,
the system outputs a translation hypothesis e?s in the
target language, which would correspond to the out-
put of fully automated MT system. Next, the user
analyses the source sentence and the decoded hy-
pothesis, and validates the longest error-free prefix
ep finding the first error. The user, then, corrects the
erroneous word by typing some keystrokes ?, and
sends them along with ep to the system, as a new val-
idated prefix ep, ?. With that information, the sys-
tem is able to produce a new, hopefully improved,
suffix e?s that continues the previous validated pre-
fix. This process is repeated until the user agrees
with the quality of the resulting translation.
system
user
f ?es
ep ,?
Figure 1: Diagram of a typical approach to IMT
The usual way in which the user introduces the
corrections ? is by means of the keyboard. How-
ever, other interaction modalities are also possible.
For example, the use of speech interaction was stud-
ied in (Vidal et al, 2006). In that work, several sce-
389
narios were proposed, where the user was expected
to speak aloud parts of the current hypothesis and
possibly one or more corrections. On-line HTR for
interactive systems was first explored for interactive
transcription of text images (Toselli et al, 2010).
Later, we proposed an adaptation to IMT in (Alabau
et al, 2010). For both cases, the decoding of the
on-line handwritten text is performed independently
as a previous step of the suffix es decoding. To our
knowledge, (Alabau et al, 2010) has been the first
and sole approach to the use of on-line handwriting
in IMT so far. However, that work did not exploit
the specific particularities of the MT scenario.
The novelties of this paper with respect to previ-
ous work are summarised in the following items:
? in previous formalisations of the problem, the
HTR decoding and the IMT decoding were per-
formed in two steps. Here, a sound statistical
formalisation is presented where both systems
are tightly coupled.
? the use of specific language modelling for on-
line HTR decoding that take into account the
previous validated prefix ep, ?, and the source
sentence f . A decreasing in error of 2% abso-
lute has been achieved with respect to previous
work.
? additionally, a thorough study of the errors
committed by the HTR subsystem is presented.
The remainder of this paper is organised as fol-
lows: The statistical framework for multimodal IMT
and their alternatives will be studied in Sec. 2. Sec-
tion 3 is devoted to the evaluation of the proposed
models. Here, the results will be analysed and com-
pared to previous approaches. Finally, conclusions
and future work will be discussed in Sec. 4.
2 Multimodal IMT
In the traditional IMT scenario, the user interacts
with the system through a series of corrections intro-
duced with the keyboard. This iterative nature of the
process is emphasised by the loop in Fig. 1, which
indicates that, for a source sentence to be translated,
several interactions between the user and the system
should be performed. In each interaction, the system
produces the most probable suffix e?s that completes
the prefix formed by concatenating the longest cor-
rect prefix from the previous hypothesis ep and the
keyboard correction ?. In addition, the concatena-
tion of them, (ep, ?, e?s), must be a translation of f .
Statistically, this problem can be formulated as
e?s = argmax
es
Pr(es|ep, ?,f) (1)
The multimodal IMT approach differs from Eq. 1
in that the user introduces the correction using a
touch-screen or an electronic pen, t. Then, Eq. 1
can be rewritten as
e?s = argmax
es
Pr(es|ep, t,f) (2)
As t is a non-deterministic input (contrarily to ?),
t needs to be decoded in a word d of the vocabu-
lary. Thus, we must marginalise for every possible
decoding:
e?s = argmax
es
?
d
Pr(es, d|ep, t,f) (3)
Furthermore, by applying simple Bayes transfor-
mations and making reasonable assumptions,
e?s ? argmax
es
max
d
Pr(t|d) Pr(d|ep,f)
Pr(es|ep, d,f) (4)
The first term in Eq. 4 is a morphological model
and it can be approximated with hidden Markov
models (HMM). The last term is an IMT model
as described in (Barrachina et al, 2009). Finally,
Pr(d|ep,f) is a constrained language model. Note
that the language model is conditioned to the longest
correct prefix, just as a regular language model. Be-
sides, it is also conditioned to the source sentence,
since d should result of the translation of it.
A typical session of the multimodal IMT is ex-
emplified in Fig. 2. First, the system starts with
an empty prefix, so it proposes a full hypothesis.
The output would be the same of a fully automated
system. Then, the user corrects the first error, not,
by writing on a touch-screen. The HTR subsys-
tem mistakenly recognises in. Consequently, the
user falls back to the keyboard and types is. Next,
the system proposes a new suffix, in which the first
word, not, has been automatically corrected. The
user amends at by writing the word , which is cor-
rectly recognised by the HTR subsystem. Finally, as
the new proposed suffix is correct, the process ends.
390
SOURCE (f ): si alguna funcio?n no se encuentra disponible en su red
TARGET (e): if any feature is not available in your network
ITER-0 (ep)
ITER-1
(e?s) if any feature not is available on your network
(ep) if any feature
(t) if any feature
(d?) if any feature in
(?) if any feature is
ITER-2
(e?s) if any feature is not available at your network
(ep) if any feature is not available
(t) if any feature is not available
(d?) if any feature is not available in
FINAL
(e?s) if any feature is not available in your network
(ep ? e) if any feature is not available in your network
Figure 2: Example of a multimodal IMT session for translating a Spanish sentence f from the Xerox corpus to an
English sentence e. If the decoding of the pen strokes d? is correct, it is displayed in boldface. On the contrary, if d? is
incorrect, it is shown crossed out. In this case, the user amends the error with the keyboard ? (in typewriter).
2.1 Decoupled Approach
In (Alabau et al, 2010) we proposed a decoupled
approach to Eq. 4, where the on-line HTR decod-
ing was a separate problem from the IMT problem.
From Eq. 4 a two step process can be performed.
First, d? is obtained,
d? ? argmax
d
Pr(t|d) Pr(d|ep,f) (5)
Then, the most likely suffix is obtained as in Eq 1,
but taking d? as the corrected word instead of ?,
e?s = argmax
es
Pr(es|ep, d?,f) (6)
Finally, in that work, the terms of Eq. 5 were in-
terpolated with a unigram in a log-linear model.
2.2 Coupled Approach
The formulation presented in Eq. 4 can be tackled
directly to perform a coupled decoding. The prob-
lem resides in how to model the constrained lan-
guage model. A first approach is to drop either the
ep or f terms from the probability. If f is dropped,
then Pr(d|ep) can be modelled as a regular n-gram
model. On the other hand, if ep is dropped, but the
position of d in the target sentence i = |ep| + 1 is
kept, Pr(d|f , i) can be modelled as a word-based
translation model. Let us introduce a hidden vari-
able j that accounts for a position of a word in f
which is a candidate translation of d. Then,
Pr(d|f , i) =
|f |?
j=1
Pr(d, j|f , i) (7)
?
|f |?
j=1
Pr(j|f , i)Pr(d|fj) (8)
Both probabilities, Pr(j|f , i) and Pr(d|fj), can
be estimated using IBM models (Brown et al,
1993). The first term is an alignment probability
while the second is a word dictionary. Word dic-
tionary probabilities can be directly estimated by
IBM1 models. However, word dictionaries are not
symmetric. Alternatively, this probability can be
estimated using the inverse dictionary to provide a
smoothed dictionary,
Pr(d|fj) =
Pr(d) Pr(fj |d)
?
d? Pr(d
?) Pr(fj |d?)
(9)
Thus, four word-based translation models have
been considered: direct IBM1 and IBM2 models,
and inverse IBM1-inv and IBM2-inv models with
the inverse dictionary from Eq. 9.
However, a more interesting set up than using lan-
guage models or translation models alone is to com-
bine both models. Two schemes have been studied.
391
The most formal under a probabilistic point of view
is a linear interpolation of the models,
Pr(d|ep,f) = ?Pr(d|ep) + (1? ?)Pr(d|f , i)
(10)
However, a common approach to combine models
nowadays is log-linear interpolation (Berger et al,
1996; Papineni et al, 1998; Och and Ney, 2002),
Pr(d|ep,f) =
exp (
?
m ?mhm(d,f , ep))
Z
(11)
?m being a scaling factor for model m, hm the log-
probability of each model considered in the log-
lineal interpolation and Z a normalisation factor.
Finally, to balance the absolute values of the mor-
phological model, the constrained language model
and the IMT model, these probabilities are com-
bined in a log-linear manner regardless of the lan-
guage modelling approach.
3 Experiments
The Xerox corpus, created on the TT2
project (SchulmbergerSema S.A. et al, 2001),
was used for these experiments, since it has been
extensively used in the literature to obtain IMT
results. The simplified English and Spanish versions
were used to estimate the IMT, IBM and language
models. The corpus consists of 56k sentences of
training and a development and test sets of 1.1k
sentences. Test perplexities for Spanish and English
are 33 and 48, respectively.
For on-line HTR, the on-line handwritten
UNIPEN corpus (Guyon et al, 1994) was used.
The morphological models were represented by con-
tinuous density left-to-right character HMMs with
Gaussian mixtures, as in speech recognition (Ra-
biner, 1989), but with variable number of states per
character. Feature extraction consisted on speed
and size normalisation of pen positions and veloc-
ities, resulting in a sequence of vectors of six fea-
tures (Toselli et al, 2007).
The simulation of user interaction was performed
in the following way. First, the publicly available
IMT decoder Thot (Ortiz-Mart??nez et al, 2005) 1
was used to run an off-line simulation for keyboard-
based IMT. As a result, a list of words the system
1http://sourceforge.net/projects/thot/
System Spanish English
dev test dev test
independent HTR (?) 9.6 10.9 7.7 9.6
decoupled (?) 9.5 10.8 7.2 9.6
best coupled 6.7 8.9 5.5 7.2
Table 1: Comparison of the CER with previous systems.
In boldface the best system. (?) is an independent, con-
text unaware system used as baseline. (?) is a model
equivalent to (Alabau et al, 2010).
failed to predict was obtained. Supposedly, this is
the list of words that the user would like to cor-
rect with handwriting. Then, from UNIPEN cor-
pus, three users (separated from the training) were
selected to simulate user interaction. For each user,
the handwritten words were generated by concate-
nating random character instances from the user?s
data to form a single stroke. Finally, the generated
handwritten words of the three users were decoded
using the corresponding constrained language model
with a state-of-the-art HMM decoder, iAtros (Luja?n-
Mares et al, 2008).
3.1 Results
Results are presented in classification error rate
(CER), i.e. the ratio between the errors committed
by the on-line HTR decoder and the number of hand-
written words introduced by the user. All the results
have been calculated as the average CER of the three
users.
Table 1 shows a comparison between the best
results in this work and the approaches in previ-
ous work. The log-linear and linear weights were
obtained with the simplex algorithm (Nelder and
Mead, 1965) to optimise the development set. Then,
those weights were used for the test set.
Two baseline models have been established for
comparison purposes. On the one hand, (?) is a
completely independent and context unaware sys-
tem. That would be the equivalent to decode the
handwritten text in a separate on-line HTR decoder.
This system obtains the worst results of all. On
the other hand, (?) is the most similar model to the
best system in (Alabau et al, 2010). This system
is clearly outperformed by the proposed coupled ap-
proach.
A summary of the alternatives to language mod-
392
System Spanish English
dev test dev test
4gr 7.8 10.0 6.3 8.9
IBM1 7.9 9.6 7.0 8.2
IBM2 7.1 8.6 6.1 7.9
IBM1-inv 8.4 9.5 7.5 9.2
IBM2-inv 7.9 9.1 7.1 9.1
4gr+IBM2 (L-Linear) 7.0 9.1 6.0 7.9
4gr+IBM2 (Linear) 6.7 8.9 5.5 7.2
Table 2: Summary of the CER results for various lan-
guage modelling approaches. In boldface the best sys-
tem.
elling is shown in Tab. 2. Up to 5-grams were used
in the experiments. However, the results did not
show significant differences between them, except
for the 1-gram. Thus, context does not seem to im-
prove much the performance. This may be due to
the fact that the IMT and the on-line HTR systems
use the same language models (5-gram in the case
of the IMT system). Hence, if the IMT has failed to
predict the correct word because of poor language
modelling that will affect on-line HTR decoding as
well. In fact, although language perplexities for the
test sets are quite low (33 for Spanish and 48 for En-
glish), perplexities accounting only erroneous words
increase until 305 and 420, respectively.
On the contrary, using IBM models provides a
significant boost in performance. Although in-
verse dictionaries have a better vocabulary coverage
(4.7% vs 8.9% in English, 7.4% vs 10.4% in Span-
ish), they tend to perform worse than their direct
dictionary counterparts. Still, inverse IBM models
perform better than the n-grams alone. Log-linear
models show a bit of improvement with respect to
IBM models. However, linear interpolated models
perform the best. In the Spanish test set the result is
not better that the IBM2 since the linear parameters
are clearly over-fitted. Other model combinations
(including a combination of all models) were tested.
Nevertheless, none of them outperformed the best
system in Table 2.
3.2 Error Analysis
An analysis of the results showed that 52.2% to
61.7% of the recognition errors were produced by
punctuation and other symbols. To circumvent this
problem, we proposed a contextual menu in (Al-
abau et al, 2010). With such menu, errors would
have been reduced (best test result) to 4.1% in Span-
ish and 2.8% in English. Out-of-vocabulary (OOV)
words also summed up a big percentage of the error
(29.1% and 20.4%, respectively). This difference
is due to the fact that Spanish is a more inflected
language. To solve this problem on-line learning al-
gorithms or methods for dealing with OOV words
should be used. Errors in gender, number and verb
tenses, which rose up to 7.7% and 5.3% of the er-
rors, could be tackled using linguistic information
from both source and target sentences. Finally, the
rest of the errors were mostly due to one-to-three
letter words, which is basically a problem of hand-
writing morphological modelling.
4 Conclusions
In this paper we have described a specific on-line
HTR system that can serve as an alternative interac-
tion modality to IMT. We have shown that a tight in-
tegration of the HTR and IMT decoding process and
the use of the available information can produce sig-
nificant HTR error reductions. Finally, a study of the
system?s errors has revealed the system weaknesses,
and how they could be addressed in the future.
5 Acknowledgments
Work supported by the EC (FEDER/FSE) and the
Spanish MEC/MICINN under the MIPRCV ?Con-
solider Ingenio 2010? program (CSD2007-00018),
iTrans2 (TIN2009-14511). Also supported by
the Spanish MITyC under the erudito.com (TSI-
020110-2009-439) project and by the Generali-
tat Valenciana under grant Prometeo/2009/014 and
GV/2010/067, and by the ?Vicerrectorado de Inves-
tigacio?n de la UPV? under grant UPV/2009/2851.
References
[Alabau et al2010] V. Alabau, D. Ortiz-Mart??nez, A. San-
chis, and F. Casacuberta. 2010. Multimodal in-
teractive machine translation. In Proceedings of the
2010 International Conference on Multimodal Inter-
faces (ICMI-MLMI?10), pages 46:1?4, Beijing, China,
Nov.
[Barrachina et al2009] S. Barrachina, O. Bender,
F. Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. L.
393
Lagarda, H. Ney, J. Toma?s, E. Vidal, and J. M. Vilar.
2009. Statistical approaches to computer-assisted
translation. Computational Linguistics, 35(1):3?28.
[Berger et al1996] A. L. Berger, S. A. Della Pietra, and
V. J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computational
Linguistics, 22:39?71.
[Brown et al1993] P. F. Brown, S. A. Della Pietra,
V. J. Della Pietra, and R. L. Mercer. 1993. The math-
ematics of machine translation. 19(2):263?311.
[Foster et al1998] G. Foster, P. Isabelle, and P. Plamon-
don. 1998. Target-text mediated interactive machine
translation. Machine Translation, 12:175?194.
[Guyon et al1994] Isabelle Guyon, Lambert Schomaker,
Re?jean Plamondon, Mark Liberman, and Stan Janet.
1994. Unipen project of on-line data exchange and
recognizer benchmarks. In Proceedings of Interna-
tional Conference on Pattern Recognition, pages 29?
33.
[Koehn and Haddow2009] P. Koehn and B. Haddow.
2009. Interactive assistance to human translators using
statistical machine translation methods. In Proceed-
ings of MT Summit XII, pages 73?80, Ottawa, Canada.
[Luja?n-Mares et al2008] M??riam Luja?n-Mares, Vicent
Tamarit, Vicent Alabau, Carlos D. Mart??nez-
Hinarejos, Moise?s Pastor i Gadea, Alberto Sanchis,
and Alejandro H. Toselli. 2008. iATROS: A speech
and handwritting recognition system. In V Jornadas
en Tecnolog??as del Habla (VJTH?2008), pages 75?78,
Bilbao (Spain), Nov.
[Nelder and Mead1965] J. A. Nelder and R. Mead. 1965.
A simplex method for function minimization. Com-
puter Journal, 7:308?313.
[Och and Ney2002] F. J. Och and H. Ney. 2002. Dis-
criminative training and maximum entropy models for
statistical machine translation. In Proceedings of the
40th ACL, pages 295?302, Philadelphia, PA, July.
[Ortiz-Mart??nez et al2005] D. Ortiz-Mart??nez, I. Garc??a-
Varea, and F. Casacuberta. 2005. Thot: a toolkit to
train phrase-based statistical translation models. In
Proceedings of the MT Summit X, pages 141?148.
[Papineni et al1998] K. A. Papineni, S. Roukos, and R. T.
Ward. 1998. Maximum likelihood and discriminative
training of direct translation models. In International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP?98), pages 189?192, Seattle, Washing-
ton, USA, May.
[Rabiner1989] L. Rabiner. 1989. A Tutorial of Hidden
Markov Models and Selected Application in Speech
Recognition. Proceedings IEEE, 77:257?286.
[SchulmbergerSema S.A. et al2001] SchulmbergerSema
S.A., Celer Soluciones, Instituto Te?cnico de In-
forma?tica, R.W.T.H. Aachen - Lehrstuhl fu?r In-
formatik VI, R.A.L.I. Laboratory - University of
Montreal, Socie?te? Gamma, and Xerox Research
Centre Europe. 2001. X.R.C.: TT2. TransType2
- Computer assisted translation. Project technical
annex.
[Toselli et al2007] Alejandro H. Toselli, Moise?s Pastor
i Gadea, and Enrique Vidal. 2007. On-line handwrit-
ing recognition system for tamil handwritten charac-
ters. In 3rd Iberian Conference on Pattern Recognition
and Image Analysis, pages 370?377. Girona (Spain),
June.
[Toselli et al2010] A. H. Toselli, V. Romero, M. Pastor,
and E. Vidal. 2010. Multimodal interactive transcrip-
tion of text images. Pattern Recognition, 43(5):1814?
1825.
[Vidal et al2006] E. Vidal, F. Casacuberta, L. Rodr??guez,
J. Civera, and C. Mart??nez. 2006. Computer-assisted
translation using speech recognition. IEEE Trans-
action on Audio, Speech and Language Processing,
14(3):941?951.
394
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 68?73,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
An Interactive Machine Translation System with Online Learning
Daniel Ortiz-Mart??nez, Luis A. Leiva, Vicent Alabau,
Ismael Garc??a-Varea?, Francisco Casacuberta
ITI - Institut Tecnolo`gic d?Informa`tica, Universitat Polite`cnica de Vale`ncia
? Departamento de Sistemas Informa?ticos, Universidad de Castilla-La Mancha
{dortiz,luileito,valabau,fcn}@iti.upv.es, ?ismael.garcia@uclm.es
Abstract
State-of-the-art Machine Translation (MT)
systems are still far from being perfect. An
alternative is the so-called Interactive Ma-
chine Translation (IMT) framework, where
the knowledge of a human translator is com-
bined with the MT system. We present a sta-
tistical IMT system able to learn from user
feedback by means of the application of on-
line learning techniques. These techniques al-
low the MT system to update the parameters of
the underlying models in real time. According
to empirical results, our system outperforms
the results of conventional IMT systems. To
the best of our knowledge, this online learning
capability has never been provided by previ-
ous IMT systems. Our IMT system is imple-
mented in C++, JavaScript, and ActionScript;
and is publicly available on the Web.
1 Introduction
The research in the field of machine translation
(MT) aims to develop computer systems which are
able to translate text or speech without human in-
tervention. However, current translation technology
has not been able to deliver full automated high-
quality translations. Typical solutions to improve the
quality of the translations supplied by an MT system
require manual post-editing. This serial process pre-
vents the MT system from integrating the knowledge
of the human translator.
An alternative way to take advantage of the exist-
ing MT technologies is to use them in collaboration
with human translators within a computer-assisted
translation (CAT) or interactive framework (Isabelle
and Church, 1997). Interactivity in CAT has been
explored for a long time. Systems have been de-
signed to interact with linguists to solve ambiguities
or update user dictionaries.
An important contribution to CAT technology was
pioneered by the TransType project (Foster et al,
1997; Langlais et al, 2002). The idea proposed in
that work was to embed data driven MT techniques
within the interactive translation environment. Fol-
lowing the TransType ideas, Barrachina et al (2009)
proposed the so-called IMT framework, in which
fully-fledged statistical MT (SMT) systems are used
to produce full target sentences hypotheses, or por-
tions thereof, which can be accepted or amended
by a human translator. Each corrected text segment
is then used by the MT system as additional infor-
mation to achieve improved suggestions. Figure 1
shows an example of a typical IMT session.
The vast majority of the existing work on
IMT makes use of the well-known batch learning
paradigm. In the batch learning paradigm, the train-
ing of the IMT system and the interactive transla-
tion process are carried out in separate stages. This
paradigm is not able to take advantage of the new
knowledge produced by the user of the IMT system.
In this paper, we present an application of the online
learning paradigm to the IMT framework. In the on-
line learning paradigm, the training and prediction
stages are no longer separated. This feature is par-
ticularly useful in IMT since it allows to take into ac-
count the user feedback. Specifically, our proposed
IMT system can be extended with the new training
samples that are generated each time the user vali-
dates the translation of a given source sentence. The
online learning techniques implemented in our IMT
system incrementally update the statistical models
involved in the translation process.
2 Related work
There are some works on IMT in the literature that
try to take advantage of user feedback. One exam-
ple is the work by Nepveu et al (2004), where dy-
namic adaptation of an IMT system via cache-based
model extensions to language and translation models
is proposed. One major drawback of such proposal
is its inability to learn new words.
68
source(f ): Para ver la lista de recursos
reference(e?): To view a listing of resources
interaction-0
ep
es To view the resources list
interaction-1
ep To view
k a
es list of resources
interaction-2
ep To view a list
k list i
es list i ng resources
interaction-3
ep To view a listing
k o
es o f resources
accept ep To view a listing of resources
Figure 1: IMT session to translate a Spanish sentence into English. In interaction-0, the system suggests a translation
(es). In interaction-1, the user moves the mouse to accept the first eight characters ?To view ? and presses the a key
(k), then the system suggests completing the sentence with ?list of resources? (a new es). Interactions 2 and 3 are
similar. In the final interaction, the user accepts the current suggestion.
Recent research on IMT has proposed the use of
online learning as one possible way to successfully
incorporate user feedback in IMT systems (Ortiz-
Mart??nez et al, 2010). In the online learning setting,
models are trained sample by sample. For this rea-
son, such learning paradigm is appropriate for its use
in the IMT framework. The work by Ortiz-Mart??nez
et al (2010) implements online learning as incre-
mental learning. Specifically, an IMT system able
to incrementally update the parameters of all of the
different models involved in the interactive transla-
tion process is proposed. One previous attempt to
implement online learning in IMT is the work by
Cesa-Bianchi et al (2008). In that work, the authors
present a very constrained version of online learn-
ing, which is not able to extend the translation mod-
els due to the high time cost of the learning process.
We have adopted the online learning techniques
proposed in (Ortiz-Mart??nez et al, 2010) to imple-
ment our IMT system. We are not aware of other
IMT tools that include such functionality. For in-
stance, a prototype system for text prediction to help
translators is shown in (Foster et al, 2002). Addi-
tionally, Koehn (2009) presents the Caitra transla-
tion tool. Caitra aids linguists suggesting sentence
completions, alternative words or allowing users to
post-edit machine translation output. However, nei-
ther of these systems are able to take advantage of
the user validated translations.
3 Interactive Machine Translation
IMT can be seen as an evolution of the statistical ma-
chine translation (SMT) framework. In SMT, given
source string f , we seek for the target string e which
maximizes the posterior probability:
e? = argmax
e
Pr(e|f) (1)
Within the IMT framework, a state-of-the-art
SMT system is employed in the following way. For
a given source sentence, the SMT system automati-
cally generates an initial translation. A human trans-
lator checks this translation from left to right, cor-
recting the first error. The SMT system then pro-
poses a new extension, taking the correct prefix ep
into account. These steps are repeated until the
whole input sentence has been correctly translated.
In the resulting decision rule, we maximize over all
possible extensions es of ep:
e?s = argmax
es
Pr(es|ep, f) (2)
It is worth to note that the user interactions are at
character level, that is, for each submitted keystroke
the system provides a new extension (or suffix) to
the current hypothesis. A typical IMT session for a
given source sentence is depicted in Figure 1.
State-of-the-art SMT systems follow a log-linear
approach (Och and Ney, 2002), where the posterior
69
probability Pr(e | f) of Eq. (1) is used. Such log-
linear approach can be easily adapted for its use in
the IMT framework as follows:
e?s = argmax
es
{
M?
m=1
?mhm(ep, es, f)
}
(3)
where each hm(ep, es, f) is a feature function rep-
resenting a statistical model and ?m its correspond-
ing weight. Typically, a set of statistical generative
models are used as feature functions. Among this
feature functions, the most relevant are the language
and translation models. The language model is im-
plemented using statistical n-gram language mod-
els and the translation model is implemented using
phrase-based models.
The IMT system proposed here is based on a log-
linear SMT system which includes a total of seven
feature functions: an n-gram language model, a tar-
get sentence length model, inverse and direct phrase-
based models, source and target phrase length mod-
els and a reordering model.
4 Online Learning
In the online learning paradigm, learning proceeds
as a sequence of trials. In each trial, a sample is
presented to the learning algorithm to be classified.
Once the sample is classified, its correct label is told
to the learning algorithm.
The online learning paradigm fits nicely in the
IMT framework, since the interactive translation of
the source sentences generates new user-validated
training samples that can be used to extend the sta-
tistical models involved in the translation process.
One key aspect in online learning is the time re-
quired by the learning algorithm to process the new
training samples. One way to satisfy this constraint
is to obtain incrementally updateable versions of the
algorithms that are executed to train the statistical
models involved in the translation process. We have
adopted this approach to implement our IMT sys-
tem. Specifically, our proposed IMT system imple-
ments the set of training algorithms that are required
to incrementally update each component of the log-
linear model. Such log-linear model is composed of
seven components (see section 3). One key aspect of
the required training algorithms is the necessity to
replace the conventional expectation-maximization
(EM) algorithm by its incremental version (Neal and
Hinton, 1998). The complete details can be found in
(Ortiz-Mart??nez et al, 2010).
5 System Overview
In this section the main features of our prototype are
shown, including prototype design, interaction pro-
tocol, prototype functionalities and demo usage.
5.1 Prototype Design
Prototype architecture has been built on two main
aspects, namely, accessibility and flexibility. The
former is necessary to reach a larger number of po-
tential users. The latter allows researchers to test
different techniques and interaction protocols.
For that reason, we developed an CAT Appli-
cation Programming Interface (API) between the
client and the actual translation engine, by using
a network communication protocol and exposing a
well-defined set of functions.
Figure 2: IMT system architecture.
A diagram of the architecture is shown in Fig-
ure 2. On the one hand, the IMT client provides a
User Interface (UI) which uses the API to commu-
nicate with the IMT server through the Web. The
hardware requirements in the client are very low,
as the translation process is carried out remotely on
the server, so virtually any computer (including net-
books, tablets or 3G mobile phones) should be fairly
enough. On the other hand, the server, which is
unaware of the implementation details of the IMT
client, uses and adapts the statistical models that are
used to perform the translation.
5.2 User Interaction Protocol
The protocol that rules the IMT process has the fol-
lowing steps:
1. The system proposes a full translation of the
selected text segment.
70
Figure 3: Demo interface. The source text segments are automatically extracted from source document. Such segments
are marked as pending (light blue), validated (dark green), partially translated (light green), and locked (light red). The
translation engine can work either at full-word or character level.
2. The user validates the longest prefix of the
translation which is error-free and/or corrects
the first error in the suffix. Corrections are
entered by amendment keystrokes or mouse
clicks/wheel operations.
3. In this way, a new extended consolidated pre-
fix is produced based on the previous validated
prefix and the interaction amendments. Using
this new prefix, the system suggests a suitable
continuation of it.
4. Steps 2 and 3 are iterated until the user-desired
translation is produced.
5. The system adapts the models to the new vali-
dated pair of sentences.
5.3 Prototype Functionality
The following is a list of the main features that the
prototype supports:
? When the user corrects the solution proposed
by the system, a new improved suffix is pre-
sented to the user.
? The system is able to learn from user-validated
translations.
? The user is able to perform actions by means
of keyboard shortcuts or mouse gestures. The
supported actions on the proposed suffix are:
Substitution Substitute the first word or char-
acter of the suffix.
Deletion Delete the first word of the suffix.
Insertion Insert a word before the suffix.
Rejection The rejected word will not appear in
the following proposals.
Acceptance Assume that the current transla-
tion is correct and adapt the models.
? At any time, the user is able to visualize the
original document (Figure 4(a)), as well as a
properly formated draft of the current transla-
tion (Figure 4(b)).
? Users can select the document to be translated
from a list or upload their own documents.
5.4 Demo Description and Usage
This demo exploits the WWW to enable the connec-
tion of simultaneous accesses across the globe, coor-
dinating client-side scripting with server-side tech-
nologies. The interface uses web technologies such
as XHTML, JavaScript, and ActionScript; while the
IMT engine is written in C++.
The prototype is publicly available on the Web
(http://cat.iti.upv.es/imt/). To begin
with, the UI loads an index of all available transla-
tion corpora. Currently, the prototype can be tested
with the well-known Europarl corpora (Koehn,
2005). The user chooses a corpus and navigates to
the main interface page (Figure 3), where she in-
teractively translates the text segments one by one.
User?s feedback is then processed by the IMT server.
71
(a) Source document example, created from EuroParl corpus.
(b) Translated example document, preserving original format and highlighting non-translated sentences.
Figure 4: Translating documents with the proposed system.
All corrections are stored in plain text logs on the
server, so the user can retake them in any mo-
ment, also allowing collaborative translations be-
tween users. On the other hand, this prototype al-
lows uploading custom documents in text format.
Since the users operate within a web browser,
the system also provides crossplatform compatibil-
ity and requires neither computational power nor
disk space on the client?s machine. The communi-
cation between application and web server is based
on asynchronous HTTP connections, providing thus
a richer interactive experience (no page refreshes are
required.) Moreover, the Web server communicates
with the IMT engine through binary TCP sockets,
ensuring really fast response times.
6 Experimental Results
Experimental results were carried out using the Xe-
rox corpus (Barrachina et al, 2009), which con-
sists of translation of Xerox printer manual involv-
ing three different language pairs: French-English,
Spanish-English, and German-English. This corpus
has been extensively used in the literature to report
IMT results. The corpus consists of approximately
50,000 sentences pairs for training, 1,000 for devel-
opment, and 1,000 for test.
The evaluation criteria used in the experiments are
the key-stroke and mouse-action ratio (KSMR) met-
ric (Barrachina et al, 2009), which measures the
user effort required to generate error-free transla-
tions, and the well-known BLEU score, which con-
stitutes a measure of the translation quality.
The test corpora were interactively translated
from English to the other three languages, compar-
ing the performance of a batch IMT (baseline) and
the online IMT systems. The batch IMT system
is a conventional IMT system which is not able to
take advantage of user feedback after each trans-
lation is performed. The online IMT system uses
the translations validated by the user to adapt the
translation models at runtime. Both systems were
initialized with a log-linear model trained in batch
mode using the training corpus. Table 1 shows the
BLEU score and the KSMR for the batch and the
online IMT systems (95% confidence intervals are
shown). The BLEU score was calculated from the
first translation hypothesis produced by the IMT sys-
tem for each source sentence. All the obtained im-
provements with the online IMT system were statis-
tically significant. The average online training time
for each new sample presented to the system, and
the average response time for each user interaction
72
(that is, time that the system uses to propose new
extensions for corrected prefixes) are also shown in
Table 1, which are less than a tenth of a second and
around two tenths of a second respectively1. Ac-
cording to the reported response and online training
times, we can argue that the system proposed here is
able to be used on real time scenarios.
System BLEU KSMR LT/RT (s)
En-Sp
batch 55.1? 2.3 18.2? 1.1 ? /0.09
online 60.6? 2.3 15.8? 1.0 0.04 /0.09
En-Fr
batch 33.7? 2.0 33.9? 1.3 ? /0.14
online 42.2? 2.2 27.9? 1.3 0.09 /0.14
En-Ge
batch 20.4? 1.8 40.3? 1.2 ? /0.15
online 28.0? 2.0 35.0? 1.3 0.07 /0.15
Table 1: BLEU and KSMR results for the XEROX test
corpora using the batch and the online IMT systems, re-
porting the average online learning (LT) and the interac-
tion response times (RP) in seconds.
It is worth mentioning that the results presented
here significantly improve those presented in (Bar-
rachina et al, 2009) for other state-of-the-art IMT
systems using the same corpora.
7 Conclusions
We have described an IMT system with online learn-
ing which is able to learn from user feedback in real
time. As far as we know, to our knowledge, this
feature have never been provided by previously pre-
sented IMT prototypes.
The proposed IMT tool is publicly available
through the Web (http://cat.iti.upv.es/
imt/). Currently, the system can be used to inter-
actively translate the well-known Europarl corpus.
We have also carried out experiments with simulated
users. According to such experiments, our IMT
system is able to outperform the results obtained
by conventional IMT systems implementing batch
learning. Future work includes researching further
on the benefits provided by our online learning tech-
niques with experiments involving real users.
Acknowledgments
Work supported by the EC (FEDER/FSE), the Span-
ish Government (MEC, MICINN, MITyC, MAEC,
1All the experiments were executed in a PC with 2.40 GHz
Intel Xeon processor and 1GB of memory.
?Plan E?, under grants MIPRCV ?Consolider In-
genio 2010? CSD2007-00018, iTrans2 TIN2009-
14511, erudito.com TSI-020110-2009-439), the
Generalitat Valenciana (grant Prometeo/2009/014,
grant GV/2010/067), the Universitat Polite`cnica de
Vale`ncia (grant 20091027), and the Spanish JCCM
(grant PBI08-0210-7127).
References
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,
and E. Vidal. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Deliverable D4.2, SMART: Stat. Multi-
lingual Analysis for Retrieval and Translation.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Target-
text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
G. Foster, P. Langlais, and G. Lapalme. 2002. Transtype:
text prediction for translators. In Proc. HLT, pages
372?374.
P. Isabelle and K. Church. 1997. Special issue on
new tools for human translators. Machine Translation,
12(1?2).
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. of the MT Summit X,
pages 79?86, September.
P. Koehn. 2009. A web-based interactive computer aided
translation tool. In Proc. ACL-IJCNLP, ACLDemos,
pages 17?20.
P. Langlais, G. Lapalme, and M. Loranger. 2002.
Transtype: Development-evaluation cycles to boost
translator?s productivity. Machine Translation,
15(4):77?98.
R.M. Neal and G.E. Hinton. 1998. A view of the
EM algorithm that justifies incremental, sparse, and
other variants. In Proc. of the NATO-ASI on Learning
in graphical models, pages 355?368, Norwell, MA,
USA.
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004.
Adaptive language and translation models for interac-
tive machine translation. In Proc. EMNLP, pages 190?
197.
F. J. Och and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Machine
Translation. In Proc. ACL, pages 295?302.
D. Ortiz-Mart??nez, I. Garc??a-Varea, and F. Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In Proc. NAACL/HLT, pages 546?
554.
73
Workshop on Humans and Computer-assisted Translation, pages 10?15,
Gothenburg, Sweden, 26 April 2014. c?2014 Association for Computational Linguistics
Proofreading Human Translations with an E-pen
Vicent Alabau and Luis A. Leiva
PRHLT Research Center
Universitat Polite`cnica de Vale`ncia
{valabau,luileito}@prhlt.upv.es
Abstract
Proofreading translated text is a task
aimed at checking for correctness, con-
sistency, and appropriate writing style.
While this has been typically done with
a keyboard and a mouse, pen-based
devices set an opportunity for making
such corrections in a comfortable way,
as if proofreading on physical paper.
Arguably, this way of interacting with
a computer is very appropriate when
a small number of modifications are
required to achieve high-quality stan-
dards. In this paper, we propose a tax-
onomy of pen gestures that is tailored
to machine translation review tasks, af-
ter human translator intervention. In
addition, we evaluate the recognition
accuracy of these gestures using a cou-
ple of popular gesture recognizers. Fi-
nally, we comment on open challenges
and limitations, and discuss possible
avenues for future work.
1 Introduction
Currently, the workflow of many translation
agencies include a final reviewing or proof-
reading process
1
where the translators? work
is checked for correctness, consistency and
appropriate writing style. If the translation
quality is good enough, only a small amount
of changes would be necessary to reach a
high-quality result. However, the required
corrections are often spread sparingly and
unequally among the screen, which renders
1
The reviewing process can be seen as a detailed
proofreading process where the target sentence is also
compared against the source sentence for errors such as
mistranslations, etc. However, for the purpose of this
paper, we can use the terms reviewing and proofread-
ing indistinguishably.
mouse/keyboard interaction both inefficient
and unappealing.
As a result of the popularization of touch-
screen and pen-based devices, text-editing ap-
plications can be operated today in a simi-
lar way people interact with pen and paper.
This way of reviewing is arguably more natu-
ral and efficient than a keyboard or a mouse,
since the e-pen can be used both to locate and
correct an erroneous word, all at once. Ad-
ditionally, the expressiveness of e-pen interac-
tion provides an opportunity to integrate use-
ful gestures that are able correct other com-
mon mistakes, such as word reordering or cap-
italization.
2 Related Work
The first attempt that we are aware of to post-
edit text with an e-pen interface dates back to
the early seventies of the past century (Cole-
man, 1969). In that work, Coleman proposed
a set of unistroke gestures for post-editing.
Later on, the same corpus was used by (Ru-
bine, 1991) in his seminal work about gesture
recognition with excellent recognition results.
However, the gesture set is too simplistic to be
used in a real translation task today.
Most of the modern applications to generate
and edit textual content using ?digital ink? are
based on ad-hoc interaction protocols
2
and of-
ten do not ship handwriting recognition soft-
ware. To our knowledge, MyScript Notes Mo-
bile
3
is the closest system to provide a natural
onscreen paper-like interaction style, includ-
ing some text-editing gestures and a powerful
handwriting recognition software. However,
this application relies on spatial relations of
the ink strokes to perform handwriting recog-
2
http://appadvice.com/appguides/show/
handwriting-apps-for-ipad
3
http://www.visionobjects.com
10
nition. For instance, to insert a new word
in the middle of a sentence the user needs to
make room for space explicitly (i.e., if the word
has N characters, the user needs to perform an
Insert Space gesture N times). Moreover, the
produced text does not flow on the UI, i.e., it is
fixed to the position of the ink, which makes
it difficult to modify. As a result, this sys-
tem does not seem suitable for reviewing trans-
lations. Other comparable work is MinGes-
tures (Leiva et al., 2013), which proposes a
simplified set of gestures for interactive text
post-editing. Although MinGestures is very
efficient and accurate, it is also very limited in
expressiveness. Only basic edition capabilities
are allowed (insertion, deletion, and substitu-
tion). Thus, advanced e-pen gestures cannot
be used to improve the efficiency of the re-
viewer.
On the other hand, there are applications
for post-editing text where user interactions
are leveraged to propagate text corrections to
the rest of the sentence. CueTIP (Shilman et
al., 2006), CATTI (Romero et al., 2009) and
IMT (Alabau et al., 2014) are the most ad-
vanced representatives of this kind of applica-
tions. These systems allow the user to cor-
rect text either in the form of unconstrained
cursive handwriting or (limited) pen gestures.
Then, the corrections are leveraged by the sys-
tem to provide smart auto-completion capa-
bilities. This way, user interaction is not only
taken into account to amend the proposed cor-
rection but other mistakes in the surrounding
text are automatically amended as well. How-
ever, user interaction is limited in these cases.
In CueTIP, only one handwritten character
can be submitted at a time and only 4 ges-
tures can be performed (join, split, delete, and
substitution). In CATTI, the user can hand-
write text freely but is still limited to perform
4 gestures as well (substitute, insert, delete,
and reject). Finally, IMT does not support
gestures other than substitution. Although
the auto-completion capability is a very inter-
esting and promising topic, it should not be
considered for reviewing: given the locality of
the small amount of changes that are probably
needed, auto-completion can make more harm
than good.
Thus, in light of the current limitations of
state-of-the-art approaches, in this work we
present an exploratory research of how paper-
like interaction should be approached to allow
proofreading translated texts.
3 A Taxonomy of Proofreading
Gestures
Indicating text modifications on a sheet of
paper can be made in many different ways.
However, the lack of a consensus may lead
to misinterpretations. Fortunately, a series
of authoritative proofreading and copy-editing
symbols have been proposed (AMA, 2007;
CMO, 2010), even leading to an eventual stan-
dardization (BS, 2005; ISO, 1983).
We have studied the aforementioned author-
itative sources and have found that there is a
huge overlap in the proposed symbols, with
only minor variations. Moreover, such sym-
bols are meant to ease human-human com-
munication and therefore we need to adapt
them to ease human-computer communica-
tion. This way, we will focus on those sym-
bols that could be used to review using stroke-
based gestures. As such, we will study gestures
that allow to change the content and not the
formatting of the text. We can define the fol-
lowing high-level operations; see Figure 1:
Word change: change text?s written form.
Letter case: change word/character casing.
Punctuation: insert punctuation symbols.
Word combination: separate or join words.
Selection: select words or characters.
Text displacement: move text around.
It is worth noting that punctuation sym-
bols are represented explicitly in the litera-
ture, probably because of their importance in
copy-editing tasks. In addition, dot and hy-
phen symbols are represented differently from
other insertion symbols. The purpose of this
convention is to reduce visual ambiguity in hu-
man recognition. Finally, the selection opera-
tion is often devoted to spell out numbers or
abbreviations.
4 Preliminary Evaluation
The initial taxonomy (Figure 1) aims to be a
complete set of symbols for proofreading and
copy-editing onscreen. Nonetheless, the suc-
cess of these gestures will depend on the accu-
11
APOS QUOT DOTCOMMA SEMI COLON COLON
Punctuation
DELETE INSERT TEXT
Word change
LOWERCASE UPPERCASE CAMELCASE
Letter case
ENCIRCLE
Selection
REMOVE SPACE INSERT SPACE HYPHEN
Word combination
MOVE SELECTION FORWARD SWAP BLOCKS TRANSPOSE TEXT BLOCKSMOVE SELECTION BACKWARD
Text displacement
Figure 1: Initial taxonomy, based on de facto proofreading symbols.
racy of gesture recognizers, to correctly trans-
late gestures into commands.
As a first approach, we wanted to evalu-
ate these symbols with state-of-the-art gesture
recognizers. The initial taxonomy differs sig-
nificantly from other gesture sets in the liter-
ature (Anthony and Wobbrock, 2012; Vatavu
et al., 2012), in the sense that the symbols we
are researching are not expected to be drawn
in isolation. Instead, reviewers will issue a ges-
ture in a very specific context, and so a proof-
reading symbol may change its meaning. This
is specially true for symbols involving multiple
spans of text or block displacements: depend-
ing of the size of the span or the length of the
displacement, the aspect ratio and proportions
among the different parts of the gesture strokes
may vary. Thus, the final shape of the gesture
can be significantly different. An example is
given in Figure 2.
Lorem ipsum dolor sit amet
(a) Move forward with 1 selected word and 2 word
displacement.
Lorem ipsum dolor sit amet
(b) Move forward with 4 selected words and 1 word
displacement.
Figure 2: Examples of the same gesture ex-
ecuted with different proportions. As a re-
sult, the shapes of both gestures significantly
diverge from each other.
4.1 Gesture Samples Acquisition
We carried out a controlled study in a real-
world setup. We developed an application
that requested a set of random challenges to
the users (Figure 3). Then, we asked the
users if they would prefer to do the acquisi-
tion on a digitizer tablet or on a tablet com-
puter. On a 1 to 5 point scale, with 1 mean-
ing ?I prefer writing with a digitizer pen? and
5 ?I prefer writing with a pen-capable tablet?,
users indicated that they would prefer a tablet
computer (M=4.6, SD=0.8). Consequently,
we deployed the application into a Lenovo
ThinkPad tablet, which had to be operated
with an e-pen. To make the paper-like ex-
perience more realistic, the touchscreen func-
tionality was disabled, so that users could rest
their hands on the screen. Eventually, 12 users
aged 24?36 submitted 5 times each gesture fol-
lowing the aforementioned random challenges.
Figure 3: Acquisition application.
4.2 The Family of $ Recognizers
In HCI, there is a popular ?dollar series?
of template-matching gesture recognizers, us-
ing a nearest-neighbor classifier with scoring
functions based on Euclidean distance. The
$ recognizers present several advantages over
other classifiers based on more complex pat-
tern recognition algorithms. First, $ recogniz-
ers are easily understandable and fast to in-
tegrate or re-implement in different program-
ming languages. Second, they do not depend
on large amounts of training data to achieve
12
high accuracy, just on a small number of pre-
defined templates.
In particular, $N (Anthony and Wobbrock,
2012) and $P (Vatavu et al., 2012) can be
used to recognize multi-stroke gestures, so
they were the only suitable candidates to rec-
ognize our initial gesture taxonomy. On the
one hand, $N deals with multiple strokes by
recombining in every possible way the strokes
of the templates in order to generate new in-
stances of unistroke templates, and then ap-
ply either the $1 recognizer (Wobbrock et al.,
2007) or Protractor (Li, 2010). On the other
hand, $P considers gesture strokes as a cloud
of points, removing thus information about
stroke sequentiality. Then, the best match
is found using an approximation of the Hun-
garian algorithm, which pairs points from the
template with points of the query gesture.
4.3 Results
We evaluated three fundamental aspects of
the recognition process: accuracy, recognition
time and memory requirements to store the
whole set of templates. Aiming for a portable
recognizer that could work on most everyday
devices, we decided to use a JavaScript (ro-
tation invariant) version of the $ family rec-
ognizers. Experiments were executed as a
nodejs program on a Ubuntu Linux computer
with a 2.83 GHz Intel QuadCore
TM
and 4 GB
of RAM. We followed a leaving-one-out (LOO)
setup, i.e., each user?s set of gestures was used
as templates and tested against the rest of the
user?s gestures. All the values show the aver-
age of the different LOO runs.
Table 1 summarizes the experimental re-
sults. For the $N recognizer we found that,
by resampling to 32 points and 5 templates,
we can achieve very good recognition times
(0.7ms in average) but high recognition error
rate (23.6%). On the other hand, the $P rec-
ognizer behaves even worse, with 27.1% error
rate. Memory requirements are marginal but
recognition times increase more than one order
of magnitude.
It must be noted that the space needed by
$N to store just one template of n strokes is
n! ? 2
n
times the space for the original tem-
plate (Vatavu et al., 2012). This is actually
a huge waste of resources. For instance, one
template of the insert space gesture requires
Recognizer Error Time Mem. usage
$N 23.6% 0.7 ms 102 MB
$P 27.1% 45 ms 1.8 MB
Table 1: Results for $N and $P recognizers,
with gestures resampled to 32 points and using
5 templates per gesture.
3840 times the original size, assuming that the
user has introduced the minimum strokes re-
quired. With a resampling 8 points, $N needs
almost 33MB of RAM to store 5 templates per
gesture.
4.4 Error analysis
Surprised by the high error rates we decided
to delve into the results of the most accurate
setup so we could find the source of errors.
We observed that the most difficult gesture
to recognize was remove space, which rep-
resented 12% of the total number of errors;
being confused with comma and semi colon
more than 50% of the time, probably because
they are formed by two arcs. It was also con-
fused, though less frequently, with move se-
lection forward/backward. These ges-
tures, excepting the circle part, are also com-
posed by two arcs.
On the other hand, punctuation symbols ac-
counted for 37% of the errors, being mostly
confused with each other, as they have very
similar shapes. Finally, some errors are harder
to dissect. For instance, uppercase was
confused mainly with both move selection
(4.4% of the errors), and punctuation and dis-
placement operations were also confused with
each other at some time, despite their very dif-
ferent visual shapes and sizes. We suspect it
is because of the internal normalization proce-
dures of the $ recognizers.
5 Discussion
Our results suggest that the $ family of gesture
recognizers, although popular, are not appro-
priate for proofreading translated texts. Our
assumption is that the normalization proce-
dures of these recognizers?mainly scaling and
resampling? are not appropriate to gestures
for which the proportions of its constituent
parts may vary according to the context. For
13
Figure 4: One proposal for gesture set sim-
plification. A Pop-up menu could assist the
user to disambiguate among perceptually sim-
ilar gestures.
example, after resizing a move selection
forward that selects a small word and has
a long arrow, the final shape would be primar-
ily that of the arrow (Figure 2).
In the light of this analysis, several actions
can be taken for future work. Firstly, other
gesture recognizers should be explored that
can deal with stroke sequences without resam-
pling (Myers and Rabiner, 1981; Sezgin and
Davis, 2005;
?
Alvaro et al., 2013). However, it
must be remarked that response time is crucial
to ensure an adequate user experience. There-
fore, the underlying algorithms should be im-
plementable on thin clients, such as mobile de-
vices, with reasonable recognition times.
Secondly, it would be also necessary to re-
duce the set of gestures, but not at the expense
of reducing also expressiveness as Leiva et al.
(2013) did. For instance, taking advantage of
the interaction that computers can provide, we
can group punctuation operations, space, and
insert hyphen all into insert above and
below gestures. Both gestures would pop-
up a menu where the user could select deter-
ministically the symbol to insert; see Figure 4.
In the same manner, letter casing operations
could be grouped into a single selection cat-
egory, which would also provide a contextual
menu to trigger the right command. The re-
sulting set of gestures should be, in principle,
much easier to recognize.
Additionally, the current set of proofread-
ing gestures present further challenges. For
instance, we would need to identify the seman-
tics of the gestures, i.e., which elements in the
text are affected by the gesture and how the
system should proceed to accomplish the task.
6 Conclusions
In this work we have defined a set of gestures
that is suitable for the reviewing process of
human-translated text. We have performed
an evaluation on gestures generated by real
users that show that popular recognizers are
not able to achieve a satisfactory accuracy. In
consequence, we have identified a series of ar-
eas for improvement that could make e-pen
devices realizable in the near future.
7 Acknowledgments
This work is supported by the 7th Frame-
work Program of European Commission un-
der grant agreements 287576 (CasMaCat) and
600707 (tranScriptorium).
References
V. Alabau, A. Sanchis, and F. Casacuberta. 2014.
Improving on-line handwritten recognition in in-
teractive machine translation. Pattern Recogni-
tion, 47(3):1217?1228.
2007. AMA manual of style: A guide for authors
and editors. 10th ed. Oxford University Press.
L. Anthony and J. O. Wobbrock. 2012. $N-
protractor: a fast and accurate multistroke rec-
ognizer. In Proc. GI, pages 117?120.
2005. BS 5261-2:2005. Copy preparation and proof
correction.
2010. The Chicago manual of style. 16th ed. Uni-
versity Of Chicago Press.
M. L. Coleman. 1969. Text editing on a graphic
display device using hand-drawn proofreader?s
symbols. In Pertinent Concepts in Computer
Graphics, Proc. 2nd Univ. Illinois Conf. on
Computer Graphics, pages 283?290.
1983. ISO 5776:1983. Symbols for text correction.
L. A. Leiva, V. Alabau, and E. Vidal. 2013. Error-
proof, high-performance, and context-aware ges-
tures for interactive text edition. In Proc. CHI
EA, pages 1227?1232.
Y. Li. 2010. Protractor: a fast and accurate ges-
ture recognizer. In Proc. CHI, pages 2169?2172.
C. S. Myers and L. R. Rabiner. 1981. A compar-
ative study of several dynamic time-warping al-
gorithms for connected-word. Bell System Tech-
nical Journal.
14
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of
text images using a web-based demo system. In
Proc. IUI, pages 477?478.
D. Rubine. 1991. Specifying gestures by example.
In Proc. SIGGRAPH, pages 329?337.
T. M. Sezgin and R. Davis. 2005. HMM-based
efficient sketch recognition. In Proc. IUI, pages
281?283.
M. Shilman, D. S. Tan, and P. Simard. 2006.
CueTIP: a mixed-initiative interface for correct-
ing handwriting errors. In Proc. UIST, pages
323?332.
R. D. Vatavu, L. Anthony, and J. O. Wobbrock.
2012. Gestures as point clouds: A $P recognizer
for user interface prototypes. In Proc. ICMI,
pages 273?280.
J. O. Wobbrock, A. D. Wilson, and Y. Li. 2007.
Gestures without libraries, toolkits or training:
A $1 recognizer for user interface prototypes. In
Proc. UIST, pages 159?168.
F.
?
Alvaro, J.-A. Sa?nchez, and J.-M. Bened??. 2013.
Classification of on-line mathematical symbols
with hybrid features and recurrent neural net-
works. In Proc. ICDAR, pages 1012?1016.
15
