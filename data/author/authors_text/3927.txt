Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 395?402, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Bootstrapping Without the Boot?
Jason Eisner and Damianos Karakos
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
{eisner,damianos}@jhu.edu
Abstract
?Bootstrapping? methods for learning require a small amount
of supervision to seed the learning process. We show that it
is sometimes possible to eliminate this last bit of supervision,
by trying many candidate seeds and selecting the one with the
most plausible outcome. We discuss such ?strapping? methods
in general, and exhibit a particular method for strapping word-
sense classifiers for ambiguous words. Our experiments on the
Canadian Hansards show that our unsupervised technique is sig-
nificantly more effective than picking seeds by hand (Yarowsky,
1995), which in turn is known to rival supervised methods.
1 Introduction
Some of NLP?s most interesting problems have to do with
unsupervised learning. Human language learners are able
to discover word senses, grammatical genders, morpho-
logical systems, grammars, discourse registers, and so
forth. One would like to build systems that discover the
same linguistic patterns in raw text. For that matter, one
would also like to discover patterns in bilingual text (for
translation), in document collections (for categorization
and retrieval), and in other data that fall outside the scope
of humans? language learning.
There are relatively few successful methods for fully
unsupervised learning from raw text. For example,
the EM algorithm (Dempster et al, 1977) extracts the
?wrong? patterns or gets stuck in local maxima.
One of the most promising avenues in recent years has
been the use of ?minimally supervised? methods. Such
methods are initialized with some sort of ?seed? that
grows into a full classifier (or generative model). We
say that a seed is ?fertile? if it grows into a classifier (or
model) that performs well on some desired criterion.
Ordinarily, it is up to a human to choose a seed that
he or she intuitively expects to be fertile. While this may
be easy when building a single classifier, it is prohibitive
when building many classifiers. For example, we may
wish to build
? word-sense classifiers for all words of a language (e.g.,
to get sharper lexical translation probabilities in a ma-
chine translation system)
? named-entity extractors for many languages
? new clusters or classifiers every day (for an evolving
document collection)
?We thank David Yarowsky for advice on the choice of data
and for the plant/tank dataset.
? new clusters or classifiers every minute (for the docu-
ment sets retrieved by ad hoc queries)
? many distinct classifiers that correspond to different
views of the data1
Even when building a single classifier, a human may not
know how to pick a good seed when working with an
unfamiliar language or sublanguage, or when trying to
induce less intuitive hidden variables, such as grammar
rules or fine-grained senses. And there is no reason to
expect humans to have good intuitions about seeds for
mining non-linguistic data such as consumer purchasing
records.
This paper considers how to remove this last element
of supervision. Our idea is to guess a number of plausi-
ble seeds, build a classifier for each one, and then try to
determine which of the seeds have grown successfully.
For example, to discover the two senses of the En-
glish word drug, we grow 200 classifiers (from different
seeds) that attempt to partition instances of drug into two
classes. We have no direct supervision about which of
the resulting partitions corresponds to the true sense dis-
tinction. Instead, we rely on clues that tend to signal that
a seed was fertile and led to a good partition. The clues
are not specific to the word drug, but they may have been
demonstrated to be good clues in general for successfully
grown word sense disambiguators.
Demonstrated how? If we consider more than one clue,
we may need some data to learn which clues to trust, and
their relative weights. Our method is unsupervised in the
conventional sense, as it obtains a classifier for drug with
no supervision about drug. However, to learn what good
classifiers generally look like2 for this task, we first use
1A word token or document can be characterized by a 20-bit
vector, corresponding to its classifications by 20 different binary
classifiers. These vectors are detailed abstract representations of
the words or documents. They can be clustered, or all their bits
can be included as potentially relevant features in another task.
2Ando and Zhang (2005) independently used this phrase, for
a semi-supervised, cross-task learner that differs from our unsu-
pervised, cross-instance learner. Both their work and ours try
to transfer knowledge to a target problem from many artificial
supervised ?auxiliary problems,? which are generated from un-
labeled data (e.g., our pseudoword disambiguation problems).
However, in their ?structural learning,? the target problem is
supervised (if inadequately), and the auxiliary problems (super-
vised instances of a different task) are a source of useful hidden
features for the classifier. In our ?strapping,? the target task is
unsupervised, and the auxiliary problems (supervised instances
395
supervised data for a few other ambiguous words?or
ambiguous pseudowords, a kind of artificial data where
supervision comes for free. This supervision?s effect on
drug might be called cross-instance learning.
To take another metaphor, minimally supervised learn-
ing is often called ?bootstrapping.? Our goal is to allow a
method to pull itself up by its own bootstraps3 even when
it has none. It places its stocking feet in anything handy,
pulls on what it hopes to be sturdy straps, and checks to
see how high it got.
We dub this family of methods ?bootstrapping without
the boot,? or ?strapping? for short. The name is meant
to evoke ?bagging? and ?boosting??other methods that
train and combine multiple classifiers of the same form.
However, we are careful to point out that strapping, un-
like those theoretically motivated methods, is an unsuper-
vised learning technique (in the sense explained above).
The clusters or other hidden variables extracted by the
winning classifier may or may not be the ones that one
had hoped to find. Designing a strapping algorithm for a
particular task requires more art than designing a super-
vised learner: one must invent not only appropriate fea-
tures for classifying the data, but also appropriate clues
for identifying ?successful? classifiers.
2 Bootstrapping
To show where strapping might be useful, we briefly re-
view a range of successful bootstrapping work. We con-
sider different tasks. Given an instance of the task and a
seed s for that instance, one bootstraps a classifier Cs that
can classify examples of the task instance.
2.1 The Yarowsky algorithm
Yarowsky (1995) sparked considerable interest in boot-
strapping with his successful method for word sense dis-
ambiguation. An instance of this task involves a homony-
mous word such as drug. A seed for the instance is a pair
of words that are strongly associated, respectively, with
the two senses of drug, such as (trafficking, therapy). An
example is a token of drug.
For our purposes, a bootstrapping method can be re-
garded almost as a black box. However, we review
the details of the Yarowsky algorithm to illustrate how
bootstrapping is possible, and why some seeds are bet-
ter than others. We will use these intuitions later in de-
signing a method to strap the Yarowsky algorithm on a
of the same task) are a source of clues for a meta-classifier that
chooses among classifiers grown from different seeds. In short,
their auxiliary problems help train the target classifier directly,
while ours help train only a simple meta-classifier that chooses
among many unsupervised target classifiers. We use far fewer
auxiliary problems but ours must be instances of the target task.
3The reference is to Baron Munchausen, a fictional 18th-
century adventurer who rescued himself from a pit in this way.
It is distinct from the ?bootstrap? in non-parametric statistics.
new instance?i.e., a method for automatically choosing
seeds that discover a true sense distinction.
A learned classifier for the instance drug is an ordered
decision list of contextual features (such as the presence
of dealer nearby) that strongly indicate one or the other
sense of drug. Given a sample token of drug, the classi-
fier picks a sense according to the single highest-ranked
feature that is present in the token?s context.
To bootstrap a decision-list classifier from a seed,
Yarowsky starts with all examples of drug that can be
classified by using the seed words as the only features.
These few examples are used as supervised data to train
a longer decision list, which includes the seed words and
any other features that suffice to distinguish these exam-
ples with high confidence. This longer decision list can
now classify further examples, which are used to train a
new and even longer decision list, and so on.
Yarowsky?s method works if it can maintain high ac-
curacy as it gradually increases its coverage. A precise
classifier at iteration t tends to accurately classify new
examples. This tends to produce a still-accurate classifier
with greater coverage at iteration t + 1.
The method fails if the initial classifier is inaccurate
(i.e., if the two seed words do not accurately pick out ex-
amples of the two senses). It may also fail if at some
point, by bad luck on sparse data, the process learns some
inappropriate features. If the classifier at iteration t is
sufficiently polluted by bad features, the classifier at iter-
ation t + 1 will start trying to distinguish examples that
do not correspond to different senses, which may lead to
even worse classifiers on subsequent iterations. However,
some alternative seed may have escaped this bad luck by
sprouting a different set of examples.
2.2 A Few Other Applications of Bootstrapping
Inspired by Yarowsky, Blum and Mitchell (1998) built a
classifier for the task of web page classification.4 They
considered only one instance of this task, namely distin-
guishing course home pages from other web pages at a
computer science department. Their seed consisted of 3
positive and 9 negative examples. Strapping a web page
classifier would mean identifying seeds that lead to other
?natural classes? of web pages. Strapping may be useful
for unsupervised text categorization in general.
Riloff et al (2003) learned lists of subjective nouns
in English, seeding their method with 20 high-frequency,
strongly subjective words. This seed set was chosen man-
ually from an automatically generated list of 850 can-
4More precisely, they bootstrapped two Naive Bayes
classifiers?one that looked at page content and the other that
looked at links to the page. This ?co-training? approach has be-
come popular. It was also used by the Cucerzan and Yarowsky
papers below, which looked at ?internal? and ?external? features
of a phrase.
396
didate words. Strapping their method would identify
subjective nouns in other languages, or other ?natural
classes? of English words.
Query expansion in IR searches for more documents
?similar to? a designated relevant document. This prob-
lem too might be regarded as searching for a natural
class?a small subset of documents that share some prop-
erty of the original document?and approached using it-
erative bootstrapping. The seed would specify the origi-
nal document plus one or two additional words or docu-
ments initially associated with the ?relevant? and/or ?ir-
relevant? classes. Strapping would guess various differ-
ent seeds that extended the original document, then try to
determine which seeds found a cohesive ?relevant set.?
Collins and Singer (1999) bootstrapped a system for
classifying phrases in context. Again, they considered
only one instance of this task: classifying English proper
names as persons, organizations, or locations. Their seed
consisted of 7 simple rules (?that New York, California,
and U.S. are locations; that any name containing Incor-
porated is an organization; and that I.B.M. and Microsoft
are organizations?). Strapping such a classifier would au-
tomatically discover named-entity classes in a different
language, or other phrase classes in English.
Cucerzan and Yarowsky (1999) built a similar system
that identified proper names as well as classifying them.
Their seed consisted of a list of 40 to 300 names. Large
seeds were not necessary for precision but did help recall.
Cucerzan and Yarowsky (2003) classified masculine
vs. feminine nouns. They experimented with several task
instances, namely different Indo-European languages. In
each instance, their seed consisted of up to 30 feminine
and 30 masculine words (e.g., girl, princess, father).
Many more papers along these lines could be listed. A
rather different task is grammar induction, where a task
instance is a corpus of text in some language, and the
learned classifier is a parser. Following Chomsky (1981),
we suggest that it may be possible to seed a grammar
induction method with a small number of facts about the
word order of the language: the basic clause order (SVO,
SOV, etc.), whether pronominal subjects may be omitted
(Chomsky?s ?pro-drop? parameter), etc. These facts can
for example be used to construct a starting point for the
inside-outside algorithm (Baker, 1979), which like other
EM algorithms is highly sensitive to starting point. In a
strapping method, one would guess a number of different
seeds and evaluate the learned grammars on likelihood,
entropy (Wang et al, 2002), correlation with semantics,
or plausibility on other linguistic grounds that were not
considered by the likelihood or the prior.
3 Strapping
Given a seed s for some task instance, let Cs denote the
classifier grown from s. Let f(s) denote the true fertility
of a seed s, i.e., the performance of Cs measured against
some set of correct answers for this instance. In gen-
eral, we do not know the correct answers and hence do
not know f(s). That is why we are doing unsupervised
learning.
Strapping relies on two estimates of f(s). Let g(s) be
a quick estimate that considers only superficial features
of the seed s. h(s) is a more careful estimate that can be
computed once Cs has been grown.
The basic method for strapping a classifier for a new
task instance is very simple:
1. Quickly select a set S of candidate seeds such that
g(s) is high.
2. For each seed s ? S, learn a classifier Cs and mea-
sure h(s).
3. Choose the seed s? ? S that maximizes h(s?).
4. Return Cs?.
Variants on this method are obviously possible. For
example, instead of returning a single classifier Cs?, one
might use classifier combination to combine several clas-
sifiers Cs that have high h(s).
It is clearly important that g and h be good estimates
of f . Can data help us design g and h? Unfortunately,
f is not known in an unsupervised setting. However, if
one can get a few supervised instances of the same task,
then one can select g and h so g(s) and h(s) approximate
f(s) for various seeds s for those instances, where f(s)
can be measured directly. The same g and h can then be
used for unsupervised learning on all new task instances.
3.1 Selecting Candidate Seeds
The first step in strapping a classifier is to select a set S
of seeds to try. For strapping to work, it is crucial that
this set contain a fertile seed. How can this be arranged?
Different strategies are appropriate for different problems
and bootstrapping methods.
? Sometimes a simple heuristic g(s) can help identify
plausibly fertile seeds, as in the pseudocode above. In
strapping the Yarowsky algorithm, we hope to find seeds
s = (x, y) such that x and y are strongly associated
with different senses of the ambiguous target word. We
choose s = (x, y) such that x and y were never ob-
served in the same sentence, but each of x and y has
high pointwise mutual information with the ambiguous
target word and appeared with it at least 5 times.
? If the space of possible seeds is small, it may be pos-
sible to try many or all of them. In grammar induction,
for example, perhaps seeding with a few basic word or-
der facts is enough. There are not so many basic word
orders to try.
397
? Some methods have many fertile seeds?so many that
a small random sample (perhaps filtered by g(s)) is
likely to include at least one. We rely on this for
the Yarowsky algorithm. If the target word is a true
homonym, there exist many words x associated strongly
with the first sense, and many words y associated
strongly with the second sense. It is not difficult to stum-
ble into a fertile seed s = (x, y), just as it is not difficult
for a human to think of one.5
? If fertile seeds are few and far between, one could
abandon the use of a candidate set S selected by g(s),
and directly use general-purpose search methods to look
for a seed whose predicted fertility h(s) is high.
For example, one could use genetic algorithms to
breed a population of seeds with high h(s). Or
after evaluating several candidate seeds to obtain
h(s1), h(s2), . . . h(sk), one could perform a regression
analysis that predicts h(s) from superficial features of
s, and use this regression function (a kind of g(s) that is
specific to the task instance) to pick sk+1.
Strapping may be harder in cases like gender induc-
tion: it is hard to stumble into the kind of detailed seed
used by Cucerzan and Yarowsky (2003). However, we
suspect that fertile seeds exist that are much smaller than
their lists of 50?60 words. While their large hand-crafted
seed is sure to work, a handful of small seeds (each
consisting of a few supposedly masculine and feminine
words) might be likely to contain at least one that is fer-
tile.6 That would be sufficient, assuming we have a way
to guess which seed in the handful is most fertile. That
issue is at the core of strapping, and we now turn to it.
3.2 Clues for Evaluating Bootstrapped Classifiers
Once we have identified a candidate seed s and built the
classifier Cs, we must evaluate whether Cs ?looks like?
the kind of classifier that tends to do well on our task.
This evaluation function h(s) is task-specific. It may
consider features of Cs, the growth trajectory of Cs, or
the relation between Cs and other classifiers.
For concretness, we consider the Yarowsky method for
word-sense disambiguation (WSD). How can we tell if a
seed s = (x, y) was fertile, without using even a small
validation set to judge Cs? There are several types of
5Alignment methods in machine translation rely even more
heavily on this property. While they begin with a small trans-
lation lexicon, they are sufficiently robust to the choice of this
initial seed (lexicon) that it suffices to construct a single seed by
crude automatic means (Brown et al, 1990; Melamed, 1997).
Human supervision (or strapping) is unnecessary.
6This is particularly likely if one favors function words (in
particular determiners and pronouns), which are strong indica-
tors of gender. Cucerzan and Yarowsky used only content words
because they could be extracted from bilingual dictionaries.
clues to fertility, which may be combined into a meta-
classifier that identifies fertile seeds.
Judge the result of classification with Cs: Even with-
out a validation set, the result of running Cs on the train-
ing corpus can be validated in various ways, using inde-
pendent plausibility criteria that were not considered by
the bootstrapping learner.
? Is the classification reasonably balanced? (If virtu-
ally all examples of the target word are labeled with
the same sense, then Cs has not found a sense dis-
tinction.)
? When a document contains multiple tokens of the
target word, are all examples labeled with the same
sense? This property tends to hold for correct clas-
sifiers (Gale et al, 1992a), at least for homonyms.
? True word senses usually correlate with document
or passage topic. Thus, choose a measure of simi-
larity between documents (e.g., the cosine measure
in TF/IDF space). Does the target word tend to
have the same sense in a document and in its nearby
neighbors?
? True word senses may also improve performance on
some task. Is the perplexity of a language model
much reduced by knowing whether sense x or sense
y (according to Cs) appeared in the current con-
text? (This relates to the previous point.) Likewise,
given a small bilingual text that has been automati-
cally (and perhaps poorly) word-aligned, is it easier
to predict how the target word will translate when
we know its sense (according to Cs)?
Judge the internal structure of Cs: Does Cs look
like a typical supervised decision list for word-sense dis-
ambiguation? For instance, does it contain many features
with high log-likelihood ratios? (If a true sense distinc-
tion was discovered, we would expect many contextual
features to correlate strongly with the predicted sense.)
Look at the process whereby Cs was learned: Does
the bootstrapping run that starts from s look like a typical
bootstrapping run from a fertile seed? For example, did
it rapidly add many new examples with high confidence?
Once new examples were classified, did their classifica-
tions remain stable rather than switching back and forth?
Judge the robustness of learning with seed s: Train
several versions of Cs, as in ensemble methods (but un-
supervised), by restricting each to a random subset of the
data, or a subset of the available features. Do these ver-
sions tend to agree on how to classify the data? If not,
seed s does not reliably find true (or even false) classes.
Judge the agreement of Cs with other classifiers:
Are there several other classifiers Cs? that agree strongly
with Cs on examples that they both classify? If the sense
398
distinction is real, then many different seeds should be
able to find it.
3.3 Training the Evaluation Function h(s)
Many of the above clues are necessary but not sufficient.
For example, a learned classification may be robust with-
out being a sense distinction. We therefore define h(s)
from a combination of several clues.
In general, h(s) is a classifier or regression function
that attempts to distinguish fertile from infertile seeds,
given the clues. As mentioned earlier, we train its free
parameters (e.g., coefficients for linear regression) on a
few supervised instances of the task. These supervised
instances allow us to measure the fertility f(s) of various
seeds, and thus to model the behavior of fertile versus
infertile seeds. The presumption is that these behavior
patterns will generalize to new seeds.
3.4 Training h(s) on Artificial Data
Optionally, to avoid the need for any human annotation at
all, the supervised task instances used to train h(s) may
be artificial instances, whose correct classifications are
known without annotation.
In the case of word-sense disambiguation, one can au-
tomatically construct ambiguous pseudowords (Gale et
al., 1992c; Schu?tze, 1998) by replacing all occurences of
two words or phrases with their conflation. For example,
banana and wine are replaced everywhere by banana-
wine. The original, unconflated text serves as a super-
vised answer key for the artificial task of disambiguating
banana-wine.
Traditionally, pseudowords are used as cheap test data
to evaluate a disambiguation system. Our idea is to use
them as cheap development data to tune a system. In
our case, they tune a few free parameters of h(s), which
says what a good classifier for this task looks like. Pseu-
dowords should be plausible instances of the task (Gaus-
tad, 2001; Nakov and Hearst, 2003): so it is deliberate
that banana and wine share syntactic and semantic fea-
tures, as senses of real ambiguous words often do.
Cheap ?pseudo-supervised? data are also available in
some other strapping settings. For grammar induction,
one could construct an artificial probabilistic grammar at
random, and generate text from it. The task of recovering
the grammar from the text then has a known answer.
4 Experiments
4.1 Unsupervised Training/Test Data
Our experiments focused on the original Yarowsky algo-
rithm. We attempted to strap word-sense classifiers, us-
ing English data only, for English words whose French
translations are ambiguous. This has obvious benefits for
training an English-to-French MT system: separate pa-
rameters can be learned for the two senses of drug.7
Gale et al (1992b) identified six such words in the
Canadian Hansards, a parallel sentence-aligned corpus of
parliamentary debate in English and French: drug, duty,
land, language, position, sentence. We extracted all ex-
amples of each word from the 14-million-word English
portion of the Hansards.8 Note that this is considerably
smaller than Yarowsky?s (1995) corpus of 460 million
words, so bootstrapping will not perform as well, and
may be more sensitive to the choice of seed.
Because we are doing unsupervised learning, we both
trained and tested these 6 words on the English Hansards.
We used the French portion of the Hansards only to create
a gold standard for evaluating our results.9 If an English
sentence containing drug is paired with a French sentence
that contains exactly one of me?dicament or drogue, we
take that as an infallible indicator of its sense.
4.2 Comparing Classifiers
Suppose binary classifier 1 assigns class ?+? to a of n
examples; binary classifier 2 assigns class ?+? to b of the
same n examples. Let e be the number of examples where
the classifiers agree (both ?+? or both ???).
An unsupervised classifier?s polarity is arbitrary: clas-
sifier 1?s ?+? may correspond to classifier 2?s ???. So we
define the overlap as E = max(e, n ? e), to reflect the
best polarity.
To evaluate a learned classifier, we measure its over-
lap with the true classification. The statistical signifi-
cance is the probability that this level of overlap would
be reached by chance under independent classifications
given the values a, b, n:
p =
?
max(a+b?n,0) ? c ? ?(a+b?E)/2?
or
?(a+b?(n?E))/2? ? c ? min(a,b)
?
a
c
? ?
n ? a
b ? c
?
/
?
n
b
?
Also, we can measure the agreement between any two
learned classifiers as ?(log p)/n. Note that a classifier
that strongly favors one sense will have low agreement
with other classifiers.
7To hedge against the possibility of misclassification, one
could interpolate with non-sense-specific parameters.
8We are not certain that our version of the Hansards is iden-
tical to that in (Gale et al, 1992b).
9By contrast, Gale et al (1992b) used the French portion as
a source of training supervision. By contrast, we will assume
that we do not have a large bilingual text such as the Hansards.
We train only on the English portion of the Hansards, ignoring
the French. This mimics the situation where we must construct
an MT system with very little bilingual text. By first discov-
ering word senses in unsupervised monolingual data (for either
language), we can avoid incorrectly mixing up two senses of
drug in our translation model.
399
4.3 Generating Candidate Seeds (via g(s))
For each target word t, we chose candidate seeds s =
(x, y) with a high score g(s), where g(s) = MI(t, x) +
MI(t, y), provided that c(x, y) = 0 and c(t, x) ? 5 and
c(t, y) ? 5 and 1/9 < c(t, x)/c(t, y) < 9.10
The set S of 200 seeds for t was constructed by repeat-
edly adding the top-scoring unused seed to S, except that
to increase the variety of words, we disallowed a seed
s = (x, y) if x or y already appeared 60 times in S.
4.4 Hand-Picked Seeds
To compare, we chose two seeds by hand for each t.
The casually hand-picked seed was chosen by intuition
from the list of 200 automatically generated seeds. This
took about 2 minutes (per seed).
The carefully hand-picked seed was not limited to this
list, and took up to 10 minutes to choose, in a data-guided
fashion. We first looked at some supervised example sen-
tences to understand the desired translational sense dis-
tinction, and then for each sense chose the highest-MI
word that both met some stringent subjective criteria and
appeared to retrieve an appropriate initial set of examples.
4.5 The Bootstrapping Classifier
Our approximate replication of Yarowsky?s algorithm
used only a small set of features:
? Original and lemmatized form of the word immedi-
ately preceding the target word t.
? Original and lemmatized form of the word immedi-
ately following t.
? Original and lemmatized form of the content words
that appear in the same sentence as t.
We used the seed to provisionally classify any token of
the target word that appeared in a sentence with exactly
one of the two seed words. This formed our initial ?train-
ing set? of disambiguated tokens. At each iteration of the
algorithm, we trained a decision list on the current train-
ing set. We then used the decision list to reclassify all k
tokens in the current training set, and also to augment the
training set by classifying the additional max(50, k/10)
tokens on which the decision list was most confident.11
10c(x, y) counts the sentences containing both x and y. MI(t,
x) = log c(t, x)c()/c(t)c(x) is pointwise mutual information.
11Such a token has some feature with high log-likelihood ra-
tio, i.e., it strongly indicates one of the senses in the current
training set. We smoothed using the method of (Yarowsky,
1996): when a feature has been observed with only one sense,
its log-likelihood ratio is estimated as a linear function of the
number of occurrences of the seen sense. Function words are
smoothed with a different linear coefficient than content words,
in order to discount their importance. We borrowed the ac-
tual coefficients from (Yarowsky, 1996), though we could have
learned them.
4.6 Development Data (for tuning h(s))
Before turning to the unsupervised Hansards, we tuned
our fertility estimator h(s) to identify good seeds on de-
velopment data?i.e., on other, supervised task instances.
In the supervised condition, we used just 2 additional
task instances, plant and tank, each with 4000 hand-
annotated instances drawn from a large balanced corpus
(Yarowsky, 1995).
In the pseudo-supervised condition, we used no hand-
annotated data, instead constructing 10 artificial super-
vised task instances (section 3.4) from the English por-
tion of the Hansards. To facilitate cross-instance learn-
ing, we tried to construct these pseudowords to behave
something like our ambiguous test words.12 Given a test
word t, we randomly selected a seed (x, y) from its candi-
date list (section 4.3), excluding any that contained func-
tion words.13 Our basic idea was to conflate x and y
into a pseudoword x-y. However, to get a pseudoword
with only two senses, we tried to focus on the particular
senses of x and y that were selected by t. We constructed
about 500 pseudoword tokens by using only x and y to-
kens that appeared in sentences that contained t, or in
sentences resembling those under a TF-IDF measure. We
repeated this process twice per test word to obtain 12
pseudowords. We then discarded the 2 pseudowords for
which no seed beat baseline performance, reasoning that
they were ill-chosen and unlike real ambiguous words.14
4.7 Clues to Fertility
For each seed s for each development or test target word,
we measured a few clues h1(s), h2(s) . . . h6(s) that we
hoped might correlate with fertility. (In future work, we
plan to investigate more clues inspired by section 3.2.)
? The agreeability of Cs with (some of) the other 199
classifiers:
?
?
1
199
?
s? 6=s
agr(Cs, Cs?)?
?
?
1/?
The agreement agr(Cs, Cs? ) was defined in section 4.2.
We tried 4 values for ? (namely 1, 2, 5, 10), each result-
ing in a different feature.
12We used collocates of t. Perhaps better yet would be words
that are distributionally similar to t (appear in same contexts).
Such words tend to be syntactically and semantically like t.
13For an unknown language or domain, a lexicon of function
words could be constructed automatically (Katz, 1996).
14Thus we discarded alcohol-trafficking and addicts-alcohol;
note that these were indeed ill-chosen (difficult) since both
words unluckily corresponded to the same sense of drug.
This left us with bound-constituents, customs-pray, claims-
value, claims-veterans, culture-unparliamentary, english-learn,
competitive-party, financial-party, death-quote, death-page.
400
? The robustness of the seed, defined by the agreement
of Cs with 10 variant classifiers C(k)s that were trained
with the same seed but under different conditions:
1
10
10
?
k=1
agr(Cs, C(k)s )
We simply trained each classifier C(k)s on a random sub-
set of the n test examples, chosen by sampling n times
with replacement.15
? The confidence of Cs on its own training data: its av-
erage confidence over the n training tokens, minus the
classifier skew.
The decision list?s confidence on a token is the log-
likelihood ratio of the single feature used to classify that
token. It has the form | log(c/d)| (perhaps smoothed)
and was previously used to select data while bootstrap-
ping Cs. Subtracting the skew, | log(a/(n?a))|,16 gives
a measurement? 0. It corrects for confidence that arises
from the classifier?s overall bias, leaving only the added
value of the relevant contextual feature.
4.8 Tuning h(s) and Strapping New Classifiers
For each of the 2 words or 10 pseudowords t in our de-
velopment set (see section 4.6), we ranked its 200 seeds
s by their true fertility f(s). We then ran support vec-
tor regression17 to learn a single linear function, h(s) =
~w ? (clue vector for Cs), that predicts the fertilities of all
2 ? 200 or 10 ? 200 seeds.18
Then, for each of our 6 Hansards test instances (sec-
tion 4.1), we used h(s) to pick the top-ranked of 200
seeds.19 It took about 3 hours total to strap classifiers for
all 6 instances, using about 40 machines and unoptimized
Perl code on the 14-million-word Hansards. For each
of the 6 instances, this involved selecting 200 candidate
15We eliminated duplicates, perhaps unfortunately.
16As before, a and n ? a are the numbers of tokens that Cs
classifies as ?+? and ??? respectively. Thus the skew is the log-
likelihood ratio of the decision list?s ?baseline? feature.
17We used cross-validation among the 10 development pseu-
dowords to choose the options to SVMlight (Joachims, 1999): a
linear kernel, a regularization parameter of 0.3, and a dependent
variable of 10f(s) ? [1, 10] rather than f(s) ? [0, 1], which
placed somewhat more emphasis on modeling the better seeds.
Our development objective function was the average over the 10
pseudowords of the Spearman rank-order correlation between
h(s) and f(s).
18We augmented the clue vector with binary clues of the form
t = plant, t = tank, etc. The regression weight of such a clue
is a learned bias term that models the inherent difficulty of the
task instance t (which varies greatly by t). This allows the other
regression features to focus on the quality of the seed given t.
19We do not have a clue t = . . . for this test instance. The re-
sulting lack of a bias term may subtract a constant from the pre-
dicted fertilities?but that does not affect the ranking of seeds.
seeds, bootstrapping 11 classifiers Cs, C(1)s , . . . C(10)s
from each seed, and choosing a particular Cs to return.
4.9 Results
Our results are in Table 1. On both development and test
instances of the task, g(s) proposed seeds with a good
range of fertilities. The correlation of predicted with ac-
tual fertility on test data averaged an outstanding 85%.
Despite having no knowledge of the desired senses,
strapping significantly beat human selection in all 24 of
the possible comparisons between a hand-picked seed
(casual or careful) and a strapped seed (chosen by an h(s)
tuned on supervised or pseudo-supervised instances).
The h(s) tuned on annotated plant/tank actually chose
the very best of the 200 seeds in 4 of the 6 instances. The
h(s) tuned on artificial pseudowords did nearly as well,
in 2 of 6 instances identifying the very best seed, and in
5 of 6 instances ranking it among its top 3 choices.
We conclude that our unsupervised clues to fertility ac-
tually work. Furthermore, combining clues via regres-
sion was wise, as it tended to work better than any single
clue. Somewhat better regression weights for the WSD
task were learned from 2 out-of-domain hand-annotated
words than from 10 in-domain artificial pseudowords.
5 Open Questions
The work reported here raises many interesting questions
for future research.
In the WSD task, we have only considered word types
with two unrelated senses (homonyms). A more general
problem is to determine when a word type is ambiguous
at all, and if so, how many coarse-grained or fine-grained
senses it has. Strapping seems naturally suited to this
problem, since it aims to discover when a sense distinc-
tion grown from some seed is a true sense distinction.
Then we would like to know how well strapping gen-
eralizes to additional bootstrapping scenarios. Our WSD
strapping experiments were successful using only a sub-
set of the techniques proposed in section 3. Generalizing
to other tasks may require other techniques for selecting
and evaluating candidate seeds, and perhaps combining
the resulting classifiers.
An interesting question is whether strapping can be
used in an active learning context. Active learning is a
kind of bootstrapping method that periodically requires
new seeds: it turns to the user whenever it gets confused.
Perhaps some of these seeds can be guessed nondetermin-
istically and the guesses evaluated automatically, with or
without user confirmation.
Finally, there may be theoretical guarantees about
strapping when something is known about the data.
When h(s) is trained to estimate f(s) well on some su-
pervised instances, there may be guarantees about how
strapping will perform on unsupervised instances drawn
401
st
ra
pp
in
g
(un
su
pe
rv
ise
d)
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
drug duty land language position sentence
baseline / # examples 51.2 / 371 70.1 / 633 76.6 / 1379 87.5 / 1012 81.7 / 2949 50.8 / 501
worst seed (of 200) 50.1 (200) traffickers trafficking 50.0 (200) 50.1 (200) claims farming 50.3 (200) 56.1 (200) 50.1 (200) length life
casually selected (from 200) 56.5 (87) food trafficking 73.4? (40) 76.2 (24) farm veterans 86.4 (76) 81.7 (41) 80.6? (40) page prison
carefully constructed 62.1? (75) alcohol costs 82.1? (8.5) 76.6 (20) farm strong 87.9 (25.5) 81.4 (56.5) 86.8? (27) death quote
best/oracle seed (of 200) 76.1?? (1) alcohol medical 86.2?? (1) 81.3?? (1) acres courts 90.9?? (1) 88.3?? (1) 89.9?? (1) reads served
most agreeable seed (?=1) 72.6?? (5) abuse information 64.7 (47) 67.5 (36) claims production 86.4 (79) 82.4 (36) 88.7?? (10) life quote
most robust seed 76.1?? (1) alcohol medical 86.2?? (1) 71.7 (29) claims price 85.6 (93) 82.7 (21) 88.8?? (9) commuted next
most confident seed 66.9? (32) trafficking used 72.1? (42) 77.9?? (3) claims courts 89.8?? (10) 84.4?? (8) 89.9?? (1) reads served
h(s)-picked (plant/tank) 76.1?? (1) alcohol medical 86.2?? (1) 81.3?? (1) acres courts 90.3?? (7) 84.5?? (7) 89.9?? (1) reads served
h(s)-picked (10 pseudowd) 70.4?? (10) alcohol found 86.2?? (1) 78.9?? (2) children farm 89.7?? (17) 83.7?? (16) 89.9?? (1) reads served
h(s)-picked, 2nd place 69.1? (13) alcohol related 85.7?? (2) 77.8?? (4) aboriginal acres 90.9?? (1) 82.8 (19) 89.0?? (7) prison quote
h(s)-picked, 3rd place 76.1?? (1) alcohol medical 84.2? (4) 77.1?? (5) acres cities 87.5 (28) 88.3?? (1) 88.6?? (15) life reads
h(s) rank of oracle seed 3 1 14 2 3 1
Spearman rank-order corr. 0.863 0.905 0.718 0.825 0.842 0.937
Table 1: [See section 4.9 for highlights.] Accuracy (as percentage) and rank (in parentheses) of bootstrapped classifiers for variously
chosen seeds, some of which are shown. * denotes statistically significant agreement with the truth (section 4.2, p < 0.01).
? denotes a seed having significantly better agreement with the truth than does the better of the hand-picked seeds (McNemar?s test,
p < 0.03). In each column, the best performance for an automatic or manual seed appears in boldface. The ?most . . . ? lines use no
tuning, the ?plant/tank? line tunes h(s) on 2 supervised instances, and the subsequent lines tune h(s) on 10 pseudoword instances.
The last line gives the Spearman rank-order correlation between seeds? predicted fertilities h(s) and their actual fertilities f(s).
from the same source (cross-instance learning). Even in
the fully unsupervised case, it may be possible to prove
that if the data were generated from a particular kind of
process (e.g., a Gaussian mixture), then a certain strap-
ping algorithm can recover the hidden variables.
6 Conclusions
In this paper, we showed that it is sometimes possible?
indeed, preferable?to eliminate the initial bit of supervi-
sion in ?bootstrapping? algorithms such as the Yarowsky
(1995) algorithm for word sense disambiguation. Our
?strapping? approach tries many candidate seeds as start-
ing points and evaluates them automatically. The eval-
uation function can be tuned if desired on other task in-
stances, perhaps artificially constructed ones. It can then
be used wherever human guidance is impractical.
We applied the method to unsupervised disambigua-
tion of English words in the Canadian Hansards, as if for
English-French translation. Our results (see section 4.9
for several highlights) show that our automatic ?strapped?
classifiers consistently outperform the classifiers boot-
strapped from manually, knowledgeably chosen seeds.
References
R. K. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In ACL.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Jared J. Wolf and Dennis H. Klatt, editors, Speech
Communication Papers Presented at the 97th meeting of the
Acoustical Society of America, MIT, Cambridge, MA, June.
A. Blum and Tom Mitchell. 1998. Combining labeled and un-
labeled data with co-training. In Proc. of COLT, July.
P. F. Brown, J. Cook, S.A. Della Pietra, V.G. Della Pietra, F. Je-
linek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin. 1990. A
statistical approach to machine translation. CL, 16(2).
N. Chomsky. 1981. Lectures on Government and Binding.
Foris, Dordrecht.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP/VLC.
S. Cucerzan and D. Yarowsky. 1999. Language independent
named entity recognition combining morphological and con-
textual evidence. In Proc. of EMNLP/VLC.
S. Cucerzan and D. Yarowsky. 2003. Minimally supervised
induction of grammatical gender. In Proc. of HLT/NAACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum
likelihood from incomplete data via the EM algorithm. J.
Royal Statist. Soc. Ser. B, 39(1):1?38.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992a. One sense
per discourse. In Proc. of the 4th DARPA Speech and Natural
Language Workshop, pages 233?237.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992b. Us-
ing bilingual materials to develop word sense disambiguation
methods. In Proc. of the 4th International Conf. on Theoret-
ical and Methodological Issues in Machine Translation.
W. A. Gale, K. W. Church, and D. Yarowsky. 1992c. Work on
statistical methods for word sense disambiguation. In Work-
ing Notes of the AAAI Fall Symposium on Probabilistic Ap-
proaches to Natural Language, pages 54?60.
T. Gaustad. 2001. Statistical corpus-based word sense disam-
biguation: Pseudowords vs. real ambiguous words. In Proc.
of ACL-EACL.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods?Support Vector Learning. MIT Press.
S. M. Katz. 1996. Distribution of context words and phrases in
text and language modelling. NLE, 2(1):15?59.
I. Dan Melamed. 1997. A word-to-word model of translational
equivalence. In Proc. of ACL/EACL, page 490.
P. Nakov and M. Hearst. 2003. Category-based pseudowords.
In HLT-NAACL?03, pages 67?69, Edmonton, Canada.
E. Riloff, J. Wiebe, and T. Wilson. 2003. Learning subjec-
tive nouns using extraction pattern bootstrapping. In Proc.
of CoNLL, pages 25?32, May?June.
H. Schu?tze. 1998. Automatic word sense discrimination. Com-
putational Linguistics, 23.
S. Wang, R. Rosenfeld, Y. Zhao, and D. Schuurmans. 2002.
The latent maximum entropy principle. In Proc. of ISIT.
D. Yarowsky. 1995. Unsupervised word sense disambiguation
rivaling supervised methods. In Proc. of ACL.
D. Yarowsky. 1996. Three Machine Learning Algorithms for
Lexical Ambiguity Resolution. Ph.D. thesis, U. of Penn.
402
Proceedings of NAACL HLT 2007, pages 252?259,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Cross-Instance Tuning of Unsupervised Document Clustering Algorithms?
Damianos Karakos, Jason Eisner
and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{damianos,eisner,khudanpur}@jhu.edu
Carey E. Priebe
Dept. of Applied Mathematics
and Statistics
Johns Hopkins University
Baltimore, MD 21218
cep@jhu.edu
Abstract
In unsupervised learning, where no train-
ing takes place, one simply hopes that
the unsupervised learner will work well
on any unlabeled test collection. How-
ever, when the variability in the data is
large, such hope may be unrealistic; a
tuning of the unsupervised algorithm may
then be necessary in order to perform well
on new test collections. In this paper,
we show how to perform such a tuning
in the context of unsupervised document
clustering, by (i) introducing a degree of
freedom, ?, into two leading information-
theoretic clustering algorithms, through
the use of generalized mutual informa-
tion quantities; and (ii) selecting the value
of ? based on clusterings of similar, but
supervised document collections (cross-
instance tuning). One option is to perform
a tuning that directly minimizes the error
on the supervised data sets; another option
is to use ?strapping? (Eisner and Karakos,
2005), which builds a classifier that learns
to distinguish good from bad clusterings,
and then selects the ? with the best pre-
dicted clustering on the test set. Experi-
ments from the ?20 Newsgroups? corpus
show that, although both techniques im-
prove the performance of the baseline al-
gorithms, ?strapping? is clearly a better
choice for cross-instance tuning.
?This work was partially supported by the DARPA GALE
program (Contract No
?
HR0011-06-2-0001) and by the JHU
WSE/APL Partnership Fund.
1 Introduction
The problem of combining labeled and unlabeled
examples in a learning task (semi-supervised learn-
ing) has been studied in the literature under various
guises. A variety of algorithms (e.g., bootstrapping
(Yarowsky, 1995), co-training (Blum and Mitchell,
1998), alternating structure optimization (Ando and
Zhang, 2005), etc.) have been developed in order to
improve the performance of supervised algorithms,
by automatically extracting knowledge from lots of
unlabeled examples. Of special interest is the work
of Ando and Zhang (2005), where the goal is to build
many supervised auxiliary tasks from the unsuper-
vised data, by creating artificial labels; this proce-
dure helps learn a transformation of the input space
that captures the relatedness of the auxiliary prob-
lems to the task at hand. In essence, Ando and Zhang
(2005) transform the semi-supervised learning prob-
lem to a multi-task learning problem; in multi-task
learning, a (usually large) set of supervised tasks is
available for training, and the goal is to build mod-
els which can simultaneously do well on all of them
(Caruana, 1997; Ben-David and Schuller, 2003; Ev-
geniou and Pontil, 2004).
Little work, however, has been devoted to study
the situation where lots of labeled examples, of one
kind, are used to build a model which is tested on
unlabeled data of a ?different? kind. This problem,
which is the topic of this paper, cannot be cast as a
multi-task learning problem (since there are labeled
examples of only one kind), neither can be cast as a
semi-supervised problem (since there are no training
labels for the test task). Note that we are interested
in the case where the hidden test labels may have
no semantic relationship with the training labels; in
252
some cases, there may not even be any informa-
tion about the test labels?what they represent, how
many they are, or at what granularity they describe
the data. This situation can arise in the case of un-
supervised clustering of documents from a large and
diverse corpus: it may not be known in what way the
resulting clusters split the corpus (is it in terms of
topic? genre? style? authorship? a combination of
the above?), unless one inspects each resulting clus-
ter to determine its ?meaning.?
At this point, we would like to differentiate be-
tween two concepts: a target task refers to a class
of problems that have a common, high-level de-
scription (e.g., the text document clustering task, the
speech recognition task, etc.). On the other hand,
a task instance refers to a particular example from
the class. For instance, if the task is ?document
clustering,? a task instance could be ?clustering of
a set of scientific documents into particular fields?;
or, if the task is ?parsing,? a task instance could be
?parsing of English sentences from the Wall Street
Journal corpus?. For the purposes of this paper, we
further assume that there are task instances which
are unrelated, in the sense that that there are no
common labels between them. For example, if the
task is ?clustering from the 20 Newsgroups corpus,?
then ?clustering of the computer-related documents
into PC-related and Mac-related? and ?clustering
of the politics-related documents into Middle-East-
related and non-Middle-East-related? are two dis-
tinct, unrelated instances. In more mathematical
terms, if task instances T1, T2 take sets of observa-
tionsX1,X2 as input, and try to predict labels from
sets S1, S2, respectively, then they are called unre-
lated if X1 ?X2 = ? and S1 ? S2 = ?.
The focus of this paper is to study the problem
of cross-instance tuning of unsupervised algorithms:
how one can tune an algorithm, which is used to
solve a particular task instance, using knowledge
from an unrelated task instance. To the best of our
knowledge, this cross-instance learning problem has
only been tackled in (Eisner and Karakos, 2005),
whose ?strapping? procedure learns a meta-classifier
for distinguishing good from bad clusterings.
In this paper, we introduce a scalar parameter ?
(a new degree of freedom) into two basic unsuper-
vised clustering algorithms. We can tune ? to max-
imize unsupervised clustering performance on dif-
ferent task instances where the correct clustering is
known. The hope is that tuning the parameter learns
something about the task in general, which trans-
fers from the supervised task instances to the un-
supervised one. Alternatively, we can tune a meta-
classifier so as to select good values of ? on the su-
pervised task instances, and then use the same meta-
classifier to select a good (possibly different) value
of ? in the unsupervised case.
The paper is organized as follows: Section 2 gives
a background on text categorization, and briefly de-
scribes the algorithms that we use in our experi-
ments. Section 3 describes our parameterization of
the clustering algorithms using Jensen-Re?nyi diver-
gence and Csisza?r?s mutual information. Experi-
mental results from the ?20 Newsgroups? data set
are shown in Section 4, along with two techniques
for cross-instance learning: (i) ?strapping,? which, at
test time, picks a parameter based on various ?good-
ness? cues that were learned from the labeled data
set, and (ii) learning the parameter from a supervised
data set which is chosen to statistically match the test
set. Finally, concluding remarks appear in Section 5.
2 Document Categorization
Document categorization is the task of deciding
whether a piece of text belongs to any of a set of
prespecified categories. It is a generic text process-
ing task useful in indexing documents for later re-
trieval, as a stage in natural language processing
systems, for content analysis, and in many other
roles (Lewis and Hayes, 1994). Here, we deal
with the unsupervised version of document cate-
gorization, in which we are interested in cluster-
ing together documents which (hopefully) belong to
the same topic, without having any training exam-
ples.1 Supervised information-theoretic clustering
approaches (Torkkola, 2002; Dhillon et al, 2003)
have been shown to be very effective, even with a
small amount of labeled data, while unsupervised
methods (which are of particular interest to us) have
been shown to be competitive, matching the classifi-
cation accuracy of supervised methods.
Our focus in this paper is on document catego-
rization algorithms which use information-theoretic
1By this, we mean that training examples having the same
category labels as the test examples are not available.
253
criteria, since there are natural ways of generalizing
these criteria through the introduction of tunable pa-
rameters. We use two such algorithms in our exper-
iments, the sequential Information Bottleneck (sIB)
and Iterative Denoising Trees (IDTs); details about
these algorithms appear below.
A note on mathematical notation: We assume
that we have a collection A = {X(1), . . . , X(N)}
of N documents. Each document X(i) is essentially
a ?bag of words?, and induces an empirical distri-
bution P?X(i) on the vocabulary X . Given a sub-
set (cluster) C of documents, the conditional dis-
tribution on X , given the cluster, is just the cen-
troid: P?X|C = 1|C|
?
X(i)?C P?X(i). If a subcollec-
tion S ? A of documents is partitioned into clusters
C1, . . . , Cm, and each document X(i) ? S is as-
signed to a cluster CZ(i), where Z(i) ? {1, . . . ,m}
is the cluster index, then the mutual information be-
tween words and corresponding clusters is given by
I(X;Z|S) =
?
z?{1,...,m}
P (z|S)D(P?X|Cz?P?X|S),
where P (z|S) , |Cz|/|S| is the ?prior? distribution
on the clusters and D(???) is the Kullback-Leibler
divergence (Cover and Thomas, 1991).
2.1 The Information Bottleneck Method
The Information Bottleneck (IB) method (Tishby et
al., 1999; Slonim and Tishby, 2000; Slonim et al,
2002) is one popular approach to unsupervised cat-
egorization. The goal of the IB (with ?hard? clus-
tering) is to find clusters such that the mutual in-
formation I(X;Z) between words and clusters is as
large as possible, under a constraint on the number
of clusters. The procedure for finding the maximiz-
ing clustering in (Slonim and Tishby, 2000) is ag-
glomerative clustering, while in (Slonim et al, 2002)
it is based on many random clusterings, combined
with a sequential update algorithm, similar to K-
means. The update algorithm re-assigns each data
point (document) d to its most ?similar? cluster C,
in order to minimize I(X;Z|C ? {d}), i.e.,
?D(P?X|{d}?P?X|{d}?C)+(1??)D(P?X|C?P?X|{d}?C),
where ? = 1|C|+1 . This latter procedure is called
the sequential Information Bottleneck (sIB) method,
and is considered the state-of-the-art in unsuper-
vised document categorization.
2.2 Iterative Denoising Trees
Decision trees are a powerful technique for equiva-
lence classification, accomplished through a recur-
sive successive refinement (Jelinek, 1997). In the
context of unsupervised classification, the goal of
decision trees is to cluster empirical distributions
(bags of words) into a given number of classes, with
each class corresponding to a leaf in the tree. They
are built top-down (as opposed to the bottom-up
construction in IB) using maximization of mutual
information between words and clusters I(X;Z|t)
to drive the splitting of each node t; the hope is that
each leaf will contain data points which belong to
only one latent category.
Iterative Denoising Trees (also called Integrated
Sensing and Processing Decision Trees) were intro-
duced in (Priebe et al, 2004a), as an extension of
regular decision trees. Their main feature is that
they transform the data at each node, before split-
ting, by projecting into a low-dimensional space.
This transformation corresponds to feature extrac-
tion; different features are suppressed (or ampli-
fied) by each transformation, depending on what
data points fall into each node (corpus-dependent-
feature-extraction property (Priebe et al, 2004b)).
Thus, dimensionality reduction and clustering are
chosen so that they jointly optimize the local objec-
tive.
In (Karakos et al, 2005), IDTs were used for an
unsupervised hyperspectral image segmentation ap-
plication. The objective at each node t was to maxi-
mize the mutual information between spectral com-
ponents and clusters given the pixels at node t, com-
puted from the projected empirical distributions. At
each step of the tree-growing procedure, the node
which yielded the highest increase in the average,
per-node mutual information, was selected for split-
ting (until a desired number of leaves was reached).
In (Karakos et al, 2007b), the mutual information
objective was replaced with a parameterized form of
mutual information, namely the Jensen-Re?nyi diver-
gence (Hero et al, 2001; Hamza and Krim, 2003), of
which more details are provided in the next section.
3 Parameterizing Unsupervised Clustering
As mentioned above, the algorithms considered in
this paper (sIB and IDTs) are unsupervised, in the
254
sense that they can be applied to test data with-
out any need for tuning. Our procedure of adapt-
ing them, based on some supervision on a different
task instance, is by introducing a parameter into the
unsupervised algorithm. At least for simple cross-
instance tuning, this parameter represents the infor-
mation which is passed between the supervised and
the unsupervised instances.
The parameterizations that we focused on have
to do with the information-theoretic objectives in
the two unsupervised algorithms. Specifically, fol-
lowing (Karakos et al, 2007b), we replace the mu-
tual information quantities in IDTs as well as sIB
with the parameterized mutual information mea-
sures mentioned above. These two quantities pro-
vide estimates of the dependence between the ran-
dom quantities in their arguments, just as the usual
mutual information does, but also have a scalar pa-
rameter ? ? (0, 1] that controls the sensitivity of the
computed dependence on the details of the joint dis-
tribution of X and Z. As a result, the effect of data
sparseness on estimation of the joint distribution can
be mitigated when computing these measures.
3.1 Jensen-Re?nyi Divergence
The Jensen-Re?nyi divergence was used in (Hero et
al., 2001; Hamza and Krim, 2003) as a measure of
similarity for image classification and retrieval. For
two discrete random variables X,Z with distribu-
tions PX , PZ and conditional PX|Z , it is defined as
I?(X;Z) = H?(PX)?
?
z
PZ(z)H?(PX|Z(?|z)),
(1)
where H?(?) is the Re?nyi entropy, given by
H?(P ) =
1
1? ?
log
(
?
x?X
P (x)?
)
, ? 6= 1. (2)
If ? ? (0, 1), H? is a concave function, and hence
I?(X;Z) is non-negative (and it is equal to zero if
and only if X and Z are independent). In the limit
as ? ? 1, H?(?) approaches the Shannon entropy
(not an obvious fact), so I?(?) reduces to the regular
mutual information. Similarly, we define
I?(X;Z|W ) =
?
w
PW (w)I?(X;Z|W = w),
where I?(X;Z|W = w) is computed via (1) using
the conditional distribution of X and Z given W .
Except in trivial cases, H?(?) is strictly larger
than H(?) when 0 < ? < 1; this means that the ef-
fects of extreme sparsity (few words per document,
or too few occurrences of non-frequent words) on
the estimation of entropy and mutual information
can be dampened with an appropriate choice of ?.
This happens because extreme sparsity in the data
yields empirical distributions which lie at, or close
to, the boundary of the probability simplex. The
entropy of such distributions is usually underesti-
mated, compared to the smooth distributions which
generate the data. Re?nyi?s entropy is larger than
Shannon?s entropy, especially in those regions close
to the boundary, and can thus provide an estimate
which is closer to the true entropy.
3.2 Csisza?r?s Mutual Information
Csisza?r defined the mutual information of order ? as
IC? (X;Z) = min
Q
?
z
PZ(z)D?(PX|Z(?|z)?Q(?)),
(3)
where D?(???) is the Re?nyi divergence (Csisza?r,
1995). It was shown that IC? (X;Z) retains most
of the properties of I(X;Z)?it is a non-negative,
continuous, and concave function of PX , it is con-
vex in PX|Z for ? < 1, and converges to I(X;Z) as
? ? 1.
Notably, IC? (X;Z) ? I(X;Z) for 0 < ? < 1;
this means, as above, that ? regulates the overesti-
mation of mutual information that may result from
data sparseness.
There is no analytic form for the minimizer of the
right-hand-side of (3) (Csisza?r, 1995), but it may be
computed via an alternating minimization algorithm
(Karakos et al, 2007a).
4 Experimental Methods and Results
We demonstrate the feasibility of cross-instance tun-
ing with experiments on unsupervised document cat-
egorization from the 20 Newsgroups corpus (Lang,
1995); this corpus consists of roughly 20,000 news
articles, evenly divided among 20 Usenet groups.
Random samples of 500 articles each were chosen
by (Slonim et al, 2002) to create multiple test col-
lections: 250 each from 2 arbitrarily chosen Usenet
255
groups for the Binary test collection, 100 articles
each from 5 groups for the Multi5 test collection,
and 50 each from 10 groups for the Multi10 test col-
lection. Three independent test collections of each
kind (Binary, Multi5 and Multi10) were created, for
a total of 9 collections. The sIB method was used to
separately cluster each collection, given the correct
number of clusters.
A comparison of sIB and IDTs on the same 9 test
collections was reported in (Karakos et al, 2007b;
Karakos et al, 2007a). Matlab code from (Slonim,
2003) was used for the sIB experiments, while the
parameterized mutual information measures of Sec-
tion 3 were used for the IDTs. A comparison was
also made with the EM-based Gaussian mixtures
clustering tool mclust (Fraley and Raftery, 1999),
and with a simple K-means algorithm. Since the
two latter techniques gave uniformly worse cluster-
ings than those of sIB and IDTs, we omit them from
the following discussion.
To show that our methods work beyond the 9 par-
ticular 500-document collections described above,
in this paper we instead use five different randomly
sampled test collections for each of the Binary,
Multi5 and Multi10 cases, making for a total of 15
new test collections in this paper. For diversity, we
ensure that none of the five test collections (in each
case) contain any documents used in the three col-
lections of (Slonim et al, 2002) (for the same case).
We pre-process the documents of each test col-
lection using the procedure2 mentioned in (Karakos
et al, 2007b). The 15 test collections are then
converted to feature matrices?term-document fre-
quency matrices for sIB, and discounted tf/idf ma-
trices (according to the Okapi formula (Gatford et
al., 1995)) for IDTs?with each row of a matrix rep-
resenting one document in that test collection.
2Excluding the subject line, the header of each abstract is
removed. Stop-words such as a, the, is, etc. are removed, and
stemming is performed (e.g., common suffixes such as -ing, -
er, -ed, etc., are removed). Also, all numbers are collapsed
to one symbol, and non-alphanumeric sequences are converted
to whitespace. Moreover, as suggested in (Yang and Pedersen,
1997) as an effective method for reducing the dimensionality of
the feature space (number of distinct words), all words which
occur fewer than t times in the corpus are removed. For the
sIB experiments, we use t = 2 (as was done in (Slonim et al,
2002)), while for the IDT experiments we use t = 3; these
choices result in the best performance for each method, respec-
tively, on another dataset.
4.1 Selecting ? with ?Strapping?
In order to pick the value of the parameter ? for
each of the sIB and IDT test experiments, we use
?strapping? (Eisner and Karakos, 2005), which, as
we mentioned earlier, is a technique for training a
meta-classifier that chooses among possible cluster-
ings. The training is based on unrelated instances of
the same clustering task. The final choice of cluster-
ing is still unsupervised, since no labels (or ground
truth, in general) for the instance of interest are used.
Here, our collection of possible clusterings for
each test collection is generated by varying the ? pa-
rameter. Strapping does not care, however, how the
collection was generated. (In the original strapping
paper, for example, Eisner and Karakos (2005) gen-
erated their collection by bootstrapping word-sense
classifiers from 200 different seeds.)
Here is how we choose a particular unsupervised
?-clustering to output for a given test collection:
? We cluster the test collection (e.g., the first Multi5
collection) with various values of ?, namely ? =
0.1, 0.2, . . . , 1.0.
? We compute a feature vector from each of the
clusterings. Note that the features are computed
from only the clusterings and the data points,
since no labels are available.
? Based on the feature vectors, we predict the
?goodness? of each clustering, and return the
?best? one.
How do we predict the ?goodness? of a cluster-
ing? By first learning to distinguish good cluster-
ings from bad ones, by using unrelated instances of
the task on which we know the true labels:
? We cluster some unrelated datasets with various
values of ?, just as we will do in the test condi-
tion.
? We evaluate each of the resulting clusterings us-
ing the true labels on its dataset.3
? We train a ?meta-classifier? that predicts the true
rank (or accuracy) of each clustering based on the
feature vector of the clustering.
3To evaluate a clustering, one only really needs the true la-
bels on a sample of the dataset, although in our experiments we
did have true labels on the entire dataset.
256
Specifically, for each task (Binary, Multi5, and
Multi10) and each clustering method (sIB and IDT),
a meta-classifier is learned thus:
? We obtain 10 clusterings (? = 0.1, 0.2, . . . , 1.0)
for each of 5 unrelated task instances (datasets
whose construction is described below).
? For each of these 50 clusterings, we compute the
following 14 features: (i) One minus the aver-
age cosine of the angle (in tf/idf space) between
each example and the centroid of the cluster to
which it belongs. (ii) The average Re?nyi diver-
gence, computed for parameters 1.0, 0.5, 0.1, be-
tween the empirical distribution of each example
and the centroid of the cluster to which it belongs.
(iii) We create 10 more features, one per ?. For
the ? used in this clustering, the feature value is
equal to e?0.1r?, where r? is the average rank of the
clustering (i.e., the average of the 4 ranks result-
ing from sorting all 10 clusterings (per training
example) according to one of the 4 features in (i)
and (ii)). For all other ??s, the feature is set to
zero. Thus, only ??s which yield relatively good
rankings can have non-zero features in the model.
? We normalize each group of 10 feature vectors,
translating and scaling each of the 14 dimensions
to make it range from 0 to 1. (We will do the same
at test time.)
? We train ranking SVMs (Joachims, 2002), with
a Gaussian kernel, to learn how to rank these 50
clusterings given their respective normalized fea-
ture vectors. The values of c, ? (which control
regularization and the Gaussian kernel) were op-
timized through leave-one-out cross validation in
order to maximize the average accuracy of the
top-ranked clustering, over the 5 training sets.
Once a local maximum of the average accuracy
was obtained, further tuning of c, ? to maximize
the Spearman rank correlation between the pre-
dicted and true ranks was performed.
A model trained in this way knows something
about the task, and may work well for many new,
unseen instances of the task. However, we pre-
sume that it will work best on a given test instance
if trained on similar instances. The ideal would be
to match the test collection in every aspect: (i) the
number of training labels should be equal to the
number of desired clusters of the test collection; (ii)
the training clusters should be topically similar to
the desired test clusters.
In our scenario, we enjoy the luxury of plenty
of labeled data that can be used to create similar
instances. Thus, given a test collection A to be
clustered into L clusters, we create similar train-
ing sets by identifying the L training newsgroups
whose centroids in tf/idf space (using the Okapi for-
mula mentioned earlier) have the smallest angle to
the centroid of A.4 (Of course, we exclude news-
groups that appear in A.) We then form a supervised
500-document training set A? by randomly choosing
500/L documents from each of these L newsgroups;
we do this 5 times to obtain 5 supervised training
sets.
Table 1 shows averaged classification errors re-
sulting from strapping (?str? rows) for the Jensen-
Re?nyi divergence and Csisza?r?s mutual information,
used within IDTs and sIB, respectively. (We also
tried the reverse, using Jensen-Re?nyi in sIB and
Csisza?r?s in IDTs, but the results were uniformly
worse in the former case and no better in the latter
case.) The ?MI? rows show the classification errors
of the untuned algorithms (? = 1), which, in almost
all cases, are worse than the tuned ones.
4.2 Tuning ? on Statistically Similar Examples
We now show that strapping outperforms a simpler
and more obvious method for cross-instance tun-
ing. To cluster a test collection A, we could simply
tune the clustering algorithm by choosing the ? that
works best on a related task instance.
We again take care to construct a training instance
A? that is closely related to the test instance A. In
fact, we take even greater care this time. Given A,
4For each of the Binary collections, the closest training
newsgroups in our experiments were talk.politics.guns,
talk.religion.misc; for each of the Multi5 collections
the closest newsgroups were sci.electronics, rec.autos,
sci.med, talk.politics.misc, talk.religion.misc, and for
the Multi10 collections they were talk.politics.misc,
rec.motorcycles, talk.religion.misc, comp.graphics,
comp.sys.ibm.pc.hardware, rec.sport.baseball, comp.os.ms-
windows.misc, comp.windows.x, soc.religion.christian,
talk.politics.mideast. Note that each of the Binary test
collections happens to be closest to the same two training
newsgroups; a similar behavior was observed for the Multi5
and Multi10 newsgroups.
257
PPPPPPPPMethod
Set Binary Multi5 Multi10
ID
Ts
MI 11.3% 9.9% 42.2%
I? (str) 10.4% 9.2% 39.0%
I? (rls) 10.1% 10.4% 42.7%
sI
B
MI 12.0% 6.8% 38.5%
IC? (str) 11.2% 6.9% 35.8%
IC? (rls) 11.1% 7.4% 37.4%
Table 1: Average classification errors for IDTs and
sIB, using strapping (?str? rows) and regularized
least squares (?rls? rows) to pick ? in Jensen-Re?nyi
divergence and Csisza?r?s mutual information. Rows
?MI? show the errors resulting from the untuned al-
gorithms, which use the regular mutual information
objective (? = 1). Results which are better than the
corresponding ?MI? results are shown in bold.
we identify the same set of L closest newsgroups as
described above. This time, however, we carefully
select |A|/L documents from each newsgroup rather
than randomly choosing 500/L of them. Specifi-
cally, for each test example (document) X ? A, we
add a similar training example X ? into A?, chosen as
follows:
We associate each test example X to the most
similar of the L training newsgroups, under a con-
straint that only |A|/L training examples may be as-
sociated to each newsgroup. To do this, we iterate
through all pairs (X,G) where X is a test example
and G is a training newsgroup, in increasing order
by the angle between X and G. If X is not yet asso-
ciated and G is not yet ?full,? then we associate X
with G, and choose X ? to be the document in G with
the smallest angle to X .
We cluster A? 10 times, for ? = 0.1, . . . , 1.0,
and we collect supervised error results E(?), ? ?
{0.1, . . . , 1.0}. Now, instead of using the single best
?? = argmin? E(?) to cluster A (which may re-
sult in overfitting) we use regularized least-squares
(RLS) (Hastie et al, 2001), where we try to approx-
imate the probability that an ? is the best. The esti-
mated probabilities are given by
p? = K(?I+K)?1p,
where I is the unit matrix, p is the training prob-
ability of the best ? (i.e., it is 1 at the position of
?? and zero elsewhere), and K is the kernel matrix,
where K(i, j) = exp(?(E(?i) ? E(?j))2/?2) is
the value of the kernel which expresses the ?sim-
ilarity? between two clusterings of the same train-
ing dataset, in terms of their errors. The parame-
ters ?, ? are set to 0.5, 0.1, respectively, after per-
forming a (local) maximization of the Spearman cor-
relation between training accuracies and predicted
probabilities p?, for all 15 training instances. Af-
ter performing a linear normalization of p? to make
it a probability vector, the average predicted value
of ?, i.e., ?? =
?10
i=1 p?i ?i, (rounded-off to one of
{0.1, . . . , 1.0}) is used to cluster A.
Table 1 shows the average classification error re-
sults using RLS (?rls? rows). We can see that, on
average over the 15 test instances, the error rate of
the tuned IDTs and sIB algorithms is lower than that
of the untuned algorithms, so cross-instance tuning
was effective. On the other hand, the errors are
generally higher than that of the strapping method,
which examines the results of using different ? val-
ues on A.
5 Concluding Remarks
We have considered the problem of cross-instance
tuning of two unsupervised document clustering al-
gorithms, through the introduction of a degree of
freedom into their mutual information objective.
This degree of freedom is tuned using labeled doc-
ument collections (which are unrelated to the test
collections); we explored two approaches for per-
forming the tuning: (i) through a judicious sampling
of training data, to match the marginal statistics of
the test data, and (ii) via ?strapping?, which trains a
meta-classifier to distinguish between good and bad
clusterings. Our unsupervised categorization exper-
iments from the ?20 Newsgroups? corpus indicate
that, although both approaches improve the base-
line algorithms, ?strapping? is clearly a better choice
for knowledge transfer between unrelated task in-
stances.
References
R. K. Ando and T. Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6:1817?1853, Nov.
258
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. In Proc. of
the Sixteenth Annual Conference on Learning Theory
(COLT-03).
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of the Workshop on Computational Learning Theory
(COLT-98), pages 92?100.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
T. Cover and J. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons.
I. Csisza?r. 1995. Generalized cutoff rates and Re?nyi?s
information measures. IEEE Trans. on Information
Theory, 41(1):26?34, January.
I. Dhillon, S. Mallela, and R. Kumar. 2003. A divisive
information-theoretic feature clustering algorithm
for text classification. Journal of Machine Learning
Research (JMLR), Special Issue on Variable and
Feature Selection, pages 1265?1287, March.
J. Eisner and D. Karakos. 2005. Bootstrapping without
the boot. In Proc. 2005 Conference on Human
Language Technology / Empirical Methods in Natural
Language Processing (HLT/EMNLP 2005), October.
T. Evgeniou and M. Pontil. 2004. Regularized multi-task
learning. In Proc. Knowledge Discovery and Data
Mining.
C. Fraley and A. E. Raftery. 1999. Mclust: Software for
model-based cluster analysis. Journal on Classifica-
tion, 16:297?306.
M. Gatford, M. M. Hancock-Beaulieu, S. Jones,
S. Walker, and S. E. Robertson. 1995. Okapi at
TREC-3. In The Third Text Retrieval Conference
(TREC-3), pages 109?126.
A. Ben Hamza and H. Krim. 2003. Jensen-Re?nyi
divergence measure: Theoretical and computational
perspectives. In Proc. IEEE Int. Symp. on Information
Theory, Yokohama, Japan, June.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer-Verlag.
A. O. Hero, B. Ma, O. Michel, and J. Gorman. 2001.
Alpha-divergence for classification, indexing and
retrieval. Technical Report CSPL-328, University of
Michigan Ann Arbor, Communications and Signal
Processing Laboratory, May.
F. Jelinek. 1997. Statistical Methods for Speech Recog-
nition. MIT Press.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM Conf. on Knowledge
Discovery and Data Mining (KDD).
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2005. Unsupervised classification via decision trees:
An information-theoretic perspective. In Proc. 2005
International Conference on Acoustics, Speech and
Signal Processing (ICASSP 2005), March.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007a. Information-theoretic aspects of iterative
denoising. Submitted to the Journal of Machine
Learning Research, February.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007b. Iterative denoising using Jensen-Re?nyi diver-
gences with an application to unsupervised document
categorization. In Proc. 2007 International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP 2007), April.
K. Lang. 1995. Learning to filter netnews. In Proc. 13th
Int. Conf. on Machine Learning, pages 331?339.
David D. Lewis and Philip J. Hayes. 1994. Guest
editorial. ACM Transactions on Information Systems,
12(3):231, July.
C. E. Priebe, D. J. Marchette, and D. M. Healy.
2004a. Integrated sensing and processing decision
trees. IEEE Trans. on Pat. Anal. and Mach. Intel.,
26(6):699?708, June.
C. E. Priebe, D. J. Marchette, Y. Park, E. Wegman,
J. Solka, D. Socolinsky, D. Karakos, K. Church,
R. Guglielmi, R. Coifman, D. Lin, D. Healy, M. Ja-
cobs, and A. Tsao. 2004b. Iterative denoising for
cross-corpus discovery. In Proc. 2004 International
Symposium on Computational Statistics (COMPSTAT
2004), August.
N. Slonim and N. Tishby. 2000. Document clustering
using word clusters via the information bottleneck
method. In Research and Development in Information
Retrieval, pages 208?215.
N. Slonim, N. Friedman, and N. Tishby. 2002. Un-
supervised document classification using sequential
information maximization. In Proc. SIGIR?02, 25th
ACM Int. Conf. on Research and Development of
Inform. Retrieval.
N. Slonim. 2003. IBA 1.0: Matlab code for information
bottleneck clustering algorithms. Available from
http://www.princeton.edu/?nslonim/IB Release1.0/
IB Release1 0.tar.
N. Tishby, F. Pereira, and W. Bialek. 1999. The informa-
tion bottleneck method. In 37th Allerton Conference
on Communication and Computation.
K. Torkkola. 2002. On feature extraction by mutual in-
formation maximization. In Proc. IEEE Int. Conf. on
Acoustics, Speech and Signal Proc. (ICASSP-2002),
May.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Intl. Conf.
on Machine Learning (ICML-97), pages 412?420.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. 33rd
Annual Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge, MA.
259
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 81?84,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Machine Translation System Combination
using ITG-based Alignments?
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, Markus Dreyer
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218
{damianos,eisner,khudanpur,dreyer}@jhu.edu
Abstract
Given several systems? automatic translations
of the same sentence, we show how to com-
bine them into a confusion network, whose
various paths represent composite translations
that could be considered in a subsequent
rescoring step. We build our confusion net-
works using the method of Rosti et al (2007),
but, instead of forming alignments using the
tercom script (Snover et al, 2006), we create
alignments that minimize invWER (Leusch
et al, 2003), a form of edit distance that
permits properly nested block movements of
substrings. Oracle experiments with Chinese
newswire and weblog translations show that
our confusion networks contain paths which
are significantly better (in terms of BLEU and
TER) than those in tercom-based confusion
networks.
1 Introduction
Large improvements in machine translation (MT)
may result from combining different approaches
to MT with mutually complementary strengths.
System-level combination of translation outputs is
a promising path towards such improvements. Yet
there are some significant hurdles in this path. One
must somehow align the multiple outputs?to iden-
tify where different hypotheses reinforce each other
and where they offer alternatives. One must then
?This work was partially supported by the DARPA GALE
program (Contract No HR0011-06-2-0001). Also, we would
like to thank the IBM Rosetta team for the availability of several
MT system outputs.
use this alignment to hypothesize a set of new, com-
posite translations, and select the best composite hy-
pothesis from this set. The alignment step is difficult
because different MT approaches usually reorder the
translated words differently. Training the selection
step is difficult because identifying the best hypothe-
sis (relative to a known reference translation) means
scoring all the composite hypotheses, of which there
may be exponentially many.
Most MT combination methods do create an ex-
ponentially large hypothesis set, representing it as a
confusion network of strings in the target language
(e.g., English). (A confusion network is a lattice
where every node is on every path; i.e., each time
step presents an independent choice among several
phrases. Note that our contributions in this paper
could be applied to arbitrary lattice topologies.) For
example, Bangalore et al (2001) show how to build
a confusion network following a multistring align-
ment procedure of several MT outputs. The proce-
dure (used primarily in biology, (Thompson et al,
1994)) yields monotone alignments that minimize
the number of insertions, deletions, and substitu-
tions. Unfortunately, monotone alignments are often
poor, since machine translations (particularly from
different models) can vary significantly in their word
order. Thus, when Matusov et al (2006) use this
procedure, they deterministically reorder each trans-
lation prior to the monotone alignment.
The procedure described by Rosti et al (2007)
has been shown to yield significant improvements in
translation quality, and uses an estimate of Trans-
lation Error Rate (TER) to guide the alignment.
(TER is defined as the minimum number of inser-
81
tions, deletions, substitutions and block shifts be-
tween two strings.) A remarkable feature of that
procedure is that it performs the alignment of the
output translations (i) without any knowledge of the
translation model used to generate the translations,
and (ii) without any knowledge of how the target
words in each translation align back to the source
words. In fact, it only requires a procedure for cre-
ating pairwise alignments of translations that allow
appropriate re-orderings. For this, Rosti et al (2007)
use the tercom script (Snover et al, 2006), which
uses a number of heuristics (as well as dynamic pro-
gramming) for finding a sequence of edits (inser-
tions, deletions, substitutions and block shifts) that
convert an input string to another. In this paper, we
show that one can build better confusion networks
(in terms of the best translation possible from the
confusion network) when the pairwise alignments
are computed not by tercom, which approximately
minimizes TER, but instead by an exact minimiza-
tion of invWER (Leusch et al, 2003), which is a re-
stricted version of TER that permits only properly
nested sets of block shifts, and can be computed in
polynomial time.
The paper is organized as follows: a summary of
TER, tercom, and invWER, is presented in Section
2. The system combination procedure is summa-
rized in Section 3, while experimental (oracle) re-
sults are presented in Section 4. Conclusions are
given in Section 5.
2 Comparing tercom and invWER
The tercom script was created mainly in order to
measure translation quality based on TER. As is
proved by Shapira and Storer (2002), computation
of TER is an NP-complete problem. For this reason,
tercom uses some heuristics in order to compute an
approximation to TER in polynomial time. In the
rest of the paper, we will denote this approximation
as tercomTER, to distinguish it from (the intractable)
TER. The block shifts which are allowed in tercom
have to adhere to the following constraints: (i) A
block that has an exact match cannot be moved, and
(ii) for a block to be moved, it should have an exact
match in its new position. However, this sometimes
leads to counter-intuitive sequences of edits; for in-
stance, for the sentence pair
?thomas jefferson says eat your vegetables?
?eat your cereal thomas edison says?,
tercom finds an edit sequence of cost 5, instead of
the optimum 3. Furthermore, the block selection is
done in a greedy manner, and the final outcome is
dependent on the shift order, even when the above
constraints are imposed.
An alternative to tercom, considered in this pa-
per, is to use the Inversion Transduction Grammar
(ITG) formalism (Wu, 1997) which allows one to
view the problem of alignment as a problem of bilin-
gual parsing. Specifically, ITGs can be used to find
the optimal edit sequence under the restriction that
block moves must be properly nested, like paren-
theses. That is, if an edit sequence swaps adjacent
substrings A and B of the original string, then any
other block move that affects A (or B) must stay
completely within A (or B). An edit sequence with
this restriction corresponds to a synchronous parse
tree under a simple ITG that has one nonterminal
and whose terminal symbols allow insertion, dele-
tion, and substitution.
The minimum-cost ITG tree can be found by dy-
namic programming. This leads to invWER (Leusch
et al, 2003), which is defined as the minimum num-
ber of edits (insertions, deletions, substitutions and
block shifts allowed by the ITG) needed to convert
one string to another. In this paper, the minimum-
invWER alignments are used for generating confu-
sion networks. The alignments are found with a 11-
rule Dyna program (Dyna is an environment that fa-
cilitates the development of dynamic programs?see
(Eisner et al, 2005) for more details). This pro-
gram was further sped up (by about a factor of 2)
with an A? search heuristic computed by additional
code. Specifically, our admissible outside heuris-
tic for aligning two substrings estimated the cost of
aligning the words outside those substrings as if re-
ordering those words were free. This was compli-
cated somewhat by type/token issues and by the fact
that we were aligning (possibly weighted) lattices.
Moreover, the same Dyna program was used for the
computation of the minimum invWER path in these
confusion networks (oracle path), without having to
invoke tercom numerous times to compute the best
sentence in an N -best list.
The two competing alignment procedures were
82
Lang. / Genre tercomTER invWER
Arabic NW 15.1% 14.9%
Arabic WB 26.0% 25.8%
Chinese NW 26.1% 25.6%
Chinese WB 30.9% 30.4%
Table 1: Comparison of average per-document ter-
comTER with invWER on the EVAL07 GALE Newswire
(?NW?) and Weblogs (?WB?) data sets.
used to estimate the TER between machine transla-
tion system outputs and reference translations. Ta-
ble 1 shows the TER estimates using tercom and
invWER. These were computed on the translations
submitted by a system to NIST for the GALE eval-
uation in June 2007. The references used are the
post-edited translations for that system (i.e., these
are ?HTER? approximations). As can be seen from
the table, in all language and genre conditions, in-
vWER gives a better approximation to TER than
tercomTER. In fact, out of the roughly 2000 total
segments in all languages/genres, tercomTER gives
a lower number of edits in only 8 cases! This is a
clear indication that ITGs can explore the space of
string permutations more effectively than tercom.
3 The System Combination Approach
ITG-based alignments and tercom-based alignments
were also compared in oracle experiments involving
confusion networks created through the algorithm of
Rosti et al (2007). The algorithm entails the follow-
ing steps:
? Computation of all pairwise alignments be-
tween system hypotheses (either using ITGs or
tercom); for each pair, one of the hypotheses
plays the role of the ?reference?.
? Selection of a system output as the ?skele-
ton? of the confusion network, whose words
are used as anchors for aligning all other ma-
chine translation outputs together. Each arc has
a translation output word as its label, with the
special token ?NULL? used to denote an inser-
tion/deletion between the skeleton and another
system output.
? Multiple consecutive words which are inserted
relative to the skeleton form a phrase that gets
Genre CNs with tercom CNs with ITG
NW 50.1% (27.7%) 48.8% (28.3%)
WB 51.0% (25.5%) 50.5% (26.0%)
Table 2: TercomTERs of invWER-oracles and (in paren-
theses) oracle BLEU scores of confusion networks gen-
erated with tercom and ITG alignments. The best results
per row are shown in bold.
aligned with an epsilon arc of the confusion
network.
? Setting the weight of each arc equal to the
negative log (posterior) probability of its la-
bel; this probability is proportional to the num-
ber of systems which output the word that gets
aligned in that location. Note that the algo-
rithm of Rosti et al (2007) used N -best lists in
the combination. Instead, we used the single-
best output of each system; this was done be-
cause not all systems were providing N -best
lists, and an unbalanced inclusion would favor
some systems much more than others. Further-
more, for each genre, one of our MT systems
was significantly better than the others in terms
of word order, and it was chosen as the skele-
ton.
4 Experimental Results
Table 2 shows tercomTERs of invWER-oracles (as
computed by the aforementioned Dyna program)
and oracle BLEU scores of the confusion networks.
The confusion networks were generated using 9
MT systems applied to the Chinese GALE 2007
Dev set, which consists of roughly 550 Newswire
segments, and 650 Weblog segments. The confu-
sion networks which were generated with the ITG-
based alignments gave significantly better oracle ter-
comTERs (significance tested with a Fisher sign
test, p ? 0.02) and better oracle BLEU scores.
The BLEU oracle sentences were found using the
dynamic-programming algorithm given in Dreyer et
al. (2007) and measured using Philipp Koehn?s eval-
uation script. On the other hand, a comparison be-
tween the 1-best paths did not reveal significant dif-
ferences that would favor one approach or the other
(either in terms of tercomTER or BLEU).
83
We also tried to understand which alignment
method gives higher probability to paths ?close?
to the corresponding oracle. To do that, we com-
puted the probability that a random path from a
confusion network is within x edits from its ora-
cle. This computation was done efficiently using
finite-state-machine operations, and did not involve
any randomization. Preliminary experiments with
the invWER-oracles show that the probability of all
paths which are within x = 3 edits from the oracle
is roughly the same for ITG-based and tercom-based
confusion networks. We plan to report our findings
for a whole range of x-values in future work. Fi-
nally, a runtime comparison of the two techniques
shows that ITGs are much more computationally
intensive: on average, ITG-based alignments took
1.5 hours/sentence (owing to their O(n6) complex-
ity), while tercom-based alignments only took 0.4
sec/sentence.
5 Concluding Remarks
We compared alignments obtained using the widely
used program tercom with alignments obtained with
ITGs and we established that the ITG alignments are
superior in two ways. Specifically: (a) we showed
that invWER (computed using the ITG alignments)
gives a better approximation to TER between ma-
chine translation outputs and human references than
tercom; and (b) in an oracle system combination ex-
periment, we found that confusion networks gen-
erated with ITG alignments contain better oracles,
both in terms of tercomTER and in terms of BLEU.
Future work will include rescoring results with a
language model, as well as exploration of heuristics
(e.g., allowing only ?short? block moves) that can
reduce the ITG alignment complexity to O(n4).
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proceedings of ASRU, pages
351?354.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing reordering constraints for smt using efficient bleu
oracle computation. In Proceedings of SSST, NAACL-
HLT 2007 / AMTA Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Rochester,
New York, April. Association for Computational Lin-
guistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of HLT-
EMNLP, pages 281?290. Association for Computa-
tional Linguistics, October.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proceedings of the
Machine Translation Summit 2003, pages 240?247,
September.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proceedings of EACL, pages 33?40.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proceedings of the ACL, pages 312?
319, June.
D. Shapira and J. A. Storer. 2002. Edit distance with
move operations. In Proceedings of the 13th Annual
Symposium on Combinatorial Pattern Matching, vol-
ume 2373/2002, pages 85?98, Fukuoka, Japan, July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, Cam-
bridge, MA, August.
J. D. Thompson, D. G. Higgins, and T. J. Gibson.
1994. Clustalw: Improving the sensitivity of progres-
sive multiple sequence alignment through sequence
weighting, position-specific gap penalties and weight
matrix choice. Nucleic Acids Research, 22(22):4673?
4680.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
84
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 65?72,
New York City, June 2006. c?2006 Association for Computational Linguistics
Generative Content Models for Structural Analysis of Medical Abstracts
Jimmy Lin1,2, Damianos Karakos3, Dina Demner-Fushman2, and Sanjeev Khudanpur3
1College of Information Studies 3Center for Language and
2Institute for Advanced Computer Studies Speech Processing
University of Maryland Johns Hopkins University
College Park, MD 20742, USA Baltimore, MD 21218, USA
jimmylin@umd.edu, demner@cs.umd.edu (damianos, khudanpur)@jhu.edu
Abstract
The ability to accurately model the con-
tent structure of text is important for
many natural language processing appli-
cations. This paper describes experi-
ments with generative models for analyz-
ing the discourse structure of medical ab-
stracts, which generally follow the pattern
of ?introduction?, ?methods?, ?results?,
and ?conclusions?. We demonstrate that
Hidden Markov Models are capable of ac-
curately capturing the structure of such
texts, and can achieve classification ac-
curacy comparable to that of discrimina-
tive techniques. In addition, generative
approaches provide advantages that may
make them preferable to discriminative
techniques such as Support Vector Ma-
chines under certain conditions. Our work
makes two contributions: at the applica-
tion level, we report good performance
on an interesting task in an important do-
main; more generally, our results con-
tribute to an ongoing discussion regarding
the tradeoffs between generative and dis-
criminative techniques.
1 Introduction
Certain types of text follow a predictable structure,
the knowledge of which would be useful in many
natural language processing applications. As an
example, scientific abstracts across many different
fields generally follow the pattern of ?introduction?,
?methods?, ?results?, and ?conclusions? (Salanger-
Meyer, 1990; Swales, 1990; Ora?san, 2001). The
ability to explicitly identify these sections in un-
structured text could play an important role in ap-
plications such as document summarization (Teufel
and Moens, 2000), information retrieval (Tbahriti
et al, 2005), information extraction (Mizuta et al,
2005), and question answering. Although there is
a trend towards analysis of full article texts, we
believe that abstracts still provide a tremendous
amount of information, and much value can still be
extracted from them. For example, Gay et al (2005)
experimented with abstracts and full article texts in
the task of automatically generating index term rec-
ommendations and discovered that using full article
texts yields at most a 7.4% improvement in F-score.
Demner-Fushman et al (2005) found a correlation
between the quality and strength of clinical conclu-
sions in the full article texts and abstracts.
This paper presents experiments with generative
content models for analyzing the discourse struc-
ture of medical abstracts, which has been con-
firmed to follow the four-section pattern discussed
above (Salanger-Meyer, 1990). For a variety of rea-
sons, medicine is an interesting domain of research.
The need for information systems to support physi-
cians at the point of care has been well studied (Cov-
ell et al, 1985; Gorman et al, 1994; Ely et al,
2005). Retrieval techniques can have a large im-
pact on how physicians access and leverage clini-
cal evidence. Information that satisfies physicians?
needs can be found in theMEDLINE database main-
tained by the U.S. National Library of Medicine
65
(NLM), which also serves as a readily available
corpus of abstracts for our experiments. Further-
more, the availability of rich ontological resources,
in the form of the Unified Medical Language Sys-
tem (UMLS) (Lindberg et al, 1993), and the avail-
ability of software that leverages this knowledge?
MetaMap (Aronson, 2001) for concept identification
and SemRep (Rindflesch and Fiszman, 2003) for re-
lation extraction?provide a foundation for studying
the role of semantics in various tasks.
McKnight and Srinivasan (2003) have previously
examined the task of categorizing sentences in med-
ical abstracts using supervised discriminative ma-
chine learning techniques. Building on the work of
Ruch et al (2003) in the same domain, we present a
generative approach that attempts to directly model
the discourse structure of MEDLINE abstracts us-
ing Hidden Markov Models (HMMs); cf. (Barzilay
and Lee, 2004). Although our results were not ob-
tained from the same exact collection as those used
by authors of these two previous studies, comparable
experiments suggest that our techniques are compet-
itive in terms of performance, and may offer addi-
tional advantages as well.
Discriminative approaches (especially SVMs)
have been shown to be very effective for many
supervised classification tasks; see, for exam-
ple, (Joachims, 1998; Ng and Jordan, 2001). How-
ever, their high computational complexity (quadratic
in the number of training samples) renders them pro-
hibitive for massive data processing. Under certain
conditions, generative approaches with linear com-
plexity are preferable, even if their performance is
lower than that which can be achieved through dis-
criminative training. Since HMMs are very well-
suited to modeling sequences, our discourse model-
ing task lends itself naturally to this particular gener-
ative approach. In fact, we demonstrate that HMMs
are competitive with SVMs, with the added advan-
tage of lower computational complexity. In addition,
generative models can be directly applied to tackle
certain classes of problems, such as sentence order-
ing, in ways that discriminative approaches cannot
readily. In the context of machine learning, we see
our work as contributing to the ongoing debate be-
tween generative and discriminative approaches?
we provide a case study in an interesting domain that
begins to explore some of these tradeoffs.
2 Methods
2.1 Corpus and Data Preparation
Our experiments involved MEDLINE, the biblio-
graphical database of biomedical articles maintained
by the U.S. National Library of Medicine (NLM).
We used the subset of MEDLINE that was extracted
for the TREC 2004 Genomics Track, consisting of
citations from 1994 to 2003. In total, 4,591,008
records (abstract text and associated metadata) were
extracted using the Date Completed (DCOM) field
for all references in the range of 19940101 to
20031231.
Viewing structural modeling of medical abstracts
as a sentence classification task, we leveraged the
existence of so-called structured abstracts (see Fig-
ure 1 for an example) in order to obtain the appro-
priate section label for each sentence. The use of
section headings is a device recommended by the
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature (1987) to help humans assess the
reliability and content of a publication and to facil-
itate the indexing and retrieval processes. Although
structured abstracts loosely adhere to the introduc-
tion, methods, results, and conclusions format, the
exact choice of section headings varies from ab-
stract to abstract and from journal to journal. In our
test collection, we observed a total of 2688 unique
section headings in structured abstracts?these were
manually mapped to the four broad classes of ?intro-
duction?, ?methods?, ?results?, and ?conclusions?.
All sentences falling under a section heading were
assigned the label of its appropriately-mapped head-
ing (naturally, the actual section headings were re-
moved in our test collection). As a concrete exam-
ple, in the abstract shown in Figure 1, the ?OBJEC-
TIVE? section would be mapped to ?introduction?,
the ?RESEARCH DESIGN AND METHODS? sec-
tion to ?methods?. The ?RESULTS? and ?CON-
CLUSIONS? sections map directly to our own la-
bels. In total, 308,055 structured abstracts were ex-
tracted and prepared in this manner, serving as the
complete dataset. In addition, we created a reduced
collection of 27,075 abstracts consisting of only
Randomized Controlled Trials (RCTs), which rep-
resent definitive sources of evidence highly-valued
in the clinical decision-making process.
Separately, we manually annotated 49 unstruc-
66
Integrating medical management with diabetes self-management training: a randomized control trial of the Diabetes
Outpatient Intensive Treatment program.
OBJECTIVE? This study evaluated the Diabetes Outpatient Intensive Treatment (DOIT) program, a multiday group educa-
tion and skills training experience combined with daily medical management, followed by case management over 6 months.
Using a randomized control design, the study explored how DOIT affected glycemic control and self-care behaviors over a
short term. The impact of two additional factors on clinical outcomes were also examined (frequency of case management
contacts and whether or not insulin was started during the program). RESEARCH DESIGN AND METHODS? Patients
with type 1 and type 2 diabetes in poor glycemic control (A1c ?8.5%) were randomly assigned to DOIT or a second con-
dition, entitled EDUPOST, which was standard diabetes care with the addition of quarterly educational mailings. A total
of 167 patients (78 EDUPOST, 89 DOIT) completed all baseline measures, including A1c and a questionnaire assessing
diabetes-related self-care behaviors. At 6 months, 117 patients (52 EDUPOST, 65 DOIT) returned to complete a follow-up
A1c and the identical self-care questionnaire. RESULTS? At follow-up, DOIT evidenced a significantly greater drop in A1c
than EDUPOST. DOIT patients also reported significantly more frequent blood glucose monitoring and greater attention to
carbohydrate and fat contents (ACFC) of food compared with EDUPOST patients. An increase in ACFC over the 6-month
period was associated with improved glycemic control among DOIT patients. Also, the frequency of nurse case manager
follow-up contacts was positively linked to better A1c outcomes. The addition of insulin did not appear to be a significant
contributor to glycemic change. CONCLUSIONS? DOIT appears to be effective in promoting better diabetes care and posi-
tively influencing glycemia and diabetes-related self-care behaviors. However, it demands significant time, commitment, and
careful coordination with many health care professionals. The role of the nurse case manager in providing ongoing follow-up
contact seems important.
Figure 1: Sample structured abstract from MEDLINE.
tured abstracts of randomized controlled trials re-
trieved to answer a question about the manage-
ment of elevated low-density lipoprotein cholesterol
(LDL-C). We submitted a PubMed query (?elevated
LDL-C?) and restricted results to English abstracts
of RCTs, gathering 49 unstructured abstracts from
26 journals. Each sentence was annotated with its
section label by the third author, who is a medical
doctor?this collection served as our blind held-out
testset. Note that the annotation process preceded
our experiments, which helped to guard against
annotator-introduced bias. Of 49 abstracts, 35 con-
tained all four sections (which we refer to as ?com-
plete?), while 14 abstracts were missing one or more
sections (which we refer to as ?partial?).
Two different types of experiments were con-
ducted: the first consisted of cross-validation on the
structured abstracts; the second consisted of train-
ing on the structured abstracts and testing on the
unstructured abstracts. We hypothesized that struc-
tured and unstructured abstracts share the same un-
derlying discourse patterns, and that content models
trained with one can be applied to the other.
2.2 Generative Models of Content
Following Ruch et al (2003) and Barzilay and
Lee (2004), we employed Hidden Markov Models
to model the discourse structure of MEDLINE ab-
stracts. The four states in our HMMs correspond
to the information that characterizes each section
(?introduction?, ?methods?, ?results?, and ?conclu-
sions?) and state transitions capture the discourse
flow from section to section.
Using the SRI language modeling toolkit, we
first computed bigram language models for each
of the four sections using Kneser-Ney discounting
and Katz backoff. All words in the training set
were downcased, all numbers were converted into
a generic symbol, and all singleton unigrams and bi-
grams were removed. Using these results, each sen-
tence was converted into a four dimensional vector,
where each component represents the log probabil-
ity, divided by the number of words, of the sentence
under each of the four language models.
We then built a four-state Hidden Markov Model
that outputs these four-dimensional vectors. The
transition probability matrix of the HMM was ini-
tialized with uniform probabilities over a fully
connected graph. The output probabilities were
modeled as four-dimensional Gaussians mixtures
with diagonal covariance matrices. Using the sec-
tion labels, the HMM was trained using the HTK
toolkit (Young et al, 2002), which efficiently per-
forms the forward-backward algorithm and Baum-
Welch estimation. For testing, we performed a
Viterbi (maximum likelihood) estimation of the la-
bel of each test sentence/vector (also using the HTK
toolkit).
67
In an attempt to further boost performance, we
employed Linear Discriminant Analysis (LDA) to
find a linear projection of the four-dimensional vec-
tors that maximizes the separation of the Gaussians
(corresponding to the HMM states). Venables and
Ripley (1994) describe an efficient algorithm (of lin-
ear complexity in the number of training sentences)
for computing the LDA transform matrix, which en-
tails computing the within- and between-covariance
matrices of the classes, and using Singular Value De-
composition (SVD) to compute the eigenvectors of
the new space. Each sentence/vector is then mul-
tiplied by this matrix, and new HMM models are
re-computed from the projected data.
An important aspect of our work is modeling con-
tent structure using generative techniques. To as-
sess the impact of taking discourse transitions into
account, we compare our fully trained model to
one that does not take advantage of the Markov
assumption?i.e., it assumes that the labels are in-
dependently and identically distributed.
To facilitate comparison with previous work, we
also experimented with binary classifiers specifi-
cally tuned to each section. This was done by creat-
ing a two-state HMM: one state corresponds to the
label we want to detect, and the other state corre-
sponds to all the other labels. We built four such
classifiers, one for each section, and trained them in
the same manner as above.
3 Results
We report results on three distinct sets of experi-
ments: (1) ten-fold cross-validation (90/10 split) on
all structured abstracts from the TREC 2004 MED-
LINE corpus, (2) ten-fold cross-validation (90/10
split) on the RCT subset of structured abstracts from
the TREC 2004 MEDLINE corpus, (3) training on
the RCT subset of the TREC 2004 MEDLINE cor-
pus and testing on the 49 hand-annotated held-out
testset.
The results of our first set of experiments are
shown in Tables 1(a) and 1(b). Table 1(a) reports
the classification error in assigning a unique label to
every sentence, drawn from the set {?introduction?,
?methods?, ?results?, ?conclusions?}. For this task,
we compare the performance of three separate mod-
els: one that does not make the Markov assumption,
Model Error
non-HMM .220
HMM .148
HMM + LDA .118
(a)
Section Acc Prec Rec F
Introduction .957 .930 .840 .885
Methods .921 .810 .875 .843
Results .921 .898 .898 .898
Conclusions .963 .898 .896 .897
(b)
Table 1: Ten-fold cross-validation results on all
structured abstracts from the TREC 2004 MED-
LINE corpus: multi-way classification on complete
abstract structure (a) and by-section binary classifi-
cation (b).
the basic four-state HMM, and the improved four-
state HMM with LDA. As expected, explicitly mod-
eling the discourse transitions significantly reduces
the error rate. Applying LDA further enhances clas-
sification performance. Table 1(b) reports accuracy,
precision, recall, and F-measure for four separate bi-
nary classifiers specifically trained for each of the
sections (one per row in the table). We only dis-
play results with our best model, namely HMM with
LDA.
The results of our second set of experiments (with
RCTs only) are shown in Tables 2(a) and 2(b).
Table 2(a) reports the multi-way classification er-
ror rate; once again, applying the Markov assump-
tion to model discourse transitions improves perfor-
mance, and using LDA further reduces error rate.
Table 2(b) reports accuracy, precision, recall, and F-
measure for four separate binary classifiers (HMM
with LDA) specifically trained for each of the sec-
tions (one per row in the table). The table also
presents the closest comparable experimental re-
sults reported by McKnight and Srinivasan (2003).1
McKnight and Srinivasan (henceforth, M&S) cre-
ated a test collection consisting of 37,151 RCTs
from approximately 12 million MEDLINE abstracts
dated between 1976 and 2001. This collection has
1After contacting the authors, we were unable to obtain the
same exact dataset that they used for their experiments.
68
Model Error
non-HMM .238
HMM .212
HMM + LDA .209
(a)
Present study McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F
Introduction .931 .898 .715 .807 .967 .920 .970 .945
Methods .904 .812 .847 .830 .895 .810 .830 .820
Results .902 .902 .831 .867 .860 .810 .830 .820
Conclusions .929 .772 .790 .781 .970 .880 .910 .820
(b)
Table 2: Ten-fold cross-validation results on the structured RCT subset of the TREC 2004 MEDLINE
corpus: multi-way classification (a) and binary classification (b). Table (b) also reproduces the results from
McKnight and Srinivasan (2003) for a comparable task on a different RCT-subset of structured abstracts.
Model Complete Partial
non-HMM .247 .371
HMM .226 .314
HMM + LDA .217 .279
(a)
Complete Partial McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F Acc Prec Rec F
Introduction .923 .739 .723 .731 .867 .368 .636 .502 .896 .630 .450 .524
Methods .905 .841 .793 .817 .859 .958 .589 .774 .897 .880 .730 .799
Results .899 .913 .857 .885 .892 .942 .830 .886 .872 .840 .880 .861
Conclusions .911 .639 .847 .743 .884 .361 .995 .678 .941 .830 .750 .785
(b)
Table 3: Training on the structured RCT subset of the TREC 2004 MEDLINE corpus, testing on corpus of
hand-annotated abstracts: multi-way classification (a) and binary classification (b). Unstructured abstracts
with all four sections (complete), and with missing sections (partial) are shown. Table (b) again repro-
duces the results from McKnight and Srinivasan (2003) for a comparable task on a different subset of 206
unstructured abstracts.
69
significantly more training examples than our corpus
of 27,075 abstracts, which could be a source of per-
formance differences. Furthermore, details regard-
ing their procedure for mapping structured abstract
headings to one of the four general labels was not
discussed in their paper. Nevertheless, our HMM-
based approach is at least competitive with SVMs,
perhaps better in some cases.
The results of our third set of experiments (train-
ing on RCTs and testing on a held-out testset of
hand-annotated abstracts) is shown in Tables 3(a)
and 3(b). Mirroring the presentation format above,
Table 3(a) shows the classification error for the four-
way label assignment problem. We noticed that
some unstructured abstracts are qualitatively differ-
ent from structured abstracts in that some sections
are missing. For example, some unstructured ab-
stracts lack an introduction, and instead dive straight
into methods; other unstructured abstracts lack a
conclusion. As a result, classification error is higher
in this experiment than in the cross-validation ex-
periments. We report performance figures for 35 ab-
stracts that contained all four sections (?complete?)
and for 14 abstracts that had one or more miss-
ing sections (?partial?). Table 3(b) reports accu-
racy, precision, recall, and F-measure for four sep-
arate binary classifiers (HMM with LDA) specifi-
cally trained for each section (one per row in the
table). The table also presents the closest compa-
rable experimental results reported by M&S?over
206 hand-annotated unstructured abstracts. Interest-
ingly, M&S did not specifically note missing sec-
tions in their testset.
4 Discussion
An interesting aspect of our generative approach
is that we model HMM outputs as Gaussian vec-
tors (log probabilities of observing entire sentences
based on our language models), as opposed to se-
quences of terms, as done in (Barzilay and Lee,
2004). This technique provides two important ad-
vantages. First, Gaussian modeling adds an ex-
tra degree of freedom during training, by capturing
second-order statistics. This is not possible when
modeling word sequences, where only the probabil-
ity of a sentence is actually used in the HMM train-
ing. Second, using continuous distributions allows
us to leverage a variety of tools (e.g., LDA) that have
been shown to be successful in other fields, such as
speech recognition (Evermann et al, 2004).
Table 2(b) represents the closest head-to-head
comparison between our generative approach
(HMM with LDA) and state-of-the-art results
reported by M&S using SVMs. In some ways, the
results reported by M&S have an advantage because
they use significantly more training examples. Yet,
we can see that generative techniques for the model-
ing of content structure are at least competitive?we
even outperform SVMs on detecting ?methods?
and ?results?. Moreover, the fact that the training
and testing of HMMs have linear complexity (as
opposed to the quadratic complexity of SVMs)
makes our approach a very attractive alternative,
given the amount of training data that is available
for such experiments.
Although exploration of the tradeoffs between
generative and discriminative machine learning
techniques is one of the aims of this work, our ul-
timate goal, however, is to build clinical systems
that provide timely access to information essential
to the patient treatment process. In truth, our cross-
validation experiments do not correspond to any
meaningful naturally-occurring task?structured ab-
stracts are, after all, already appropriately labeled.
The true utility of content models is to struc-
ture abstracts that have no structure to begin with.
Thus, our exploratory experiments in applying con-
tent models trained with structured RCTs on un-
structured RCTs is a closer approximation of an
extrinsically-valid measure of performance. Such a
component would serve as the first stage of a clin-
ical question answering system (Demner-Fushman
and Lin, 2005) or summarization system (McKe-
own et al, 2003). We chose to focus on randomized
controlled trials because they represent the standard
benchmark by which all other clinical studies are
measured.
Table 3(b) shows the effectiveness of our trained
content models on abstracts that had no explicit
structure to begin with. We can see that although
classification accuracy is lower than that from our
cross-validation experiments, performance is quite
respectable. Thus, our hypothesis that unstructured
abstracts are not qualitatively different from struc-
tured abstracts appears to be mostly valid.
70
5 Related Work
Although not the first to employ a generative ap-
proach to directly model content, the seminal work
of Barzilay and Lee (2004) is a noteworthy point
of reference and comparison. However, our study
differs in several important respects. Barzilay and
Lee employed an unsupervised approach to building
topic sequence models for the newswire text genre
using clustering techniques. In contrast, because
the discourse structure of medical abstracts is well-
defined and training data is relatively easy to ob-
tain, we were able to apply a supervised approach.
Whereas Barzilay and Lee evaluated their work in
the context of document summarization, the four-
part structure of medical abstracts allows us to con-
duct meaningful intrinsic evaluations and focus on
the sentence classification task. Nevertheless, their
work bolsters our claims regarding the usefulness of
generative models in extrinsic tasks, which we do
not describe here.
Although this study falls under the general topic
of discourse modeling, our work differs from previ-
ous attempts to characterize text in terms of domain-
independent rhetorical elements (McKeown, 1985;
Marcu and Echihabi, 2002). Our task is closer to the
work of Teufel and Moens (2000), who looked at the
problem of intellectual attribution in scientific texts.
6 Conclusion
We believe that there are two contributions as a re-
sult of our work. From the perspective of machine
learning, the assignment of sequentially-occurring
labels represents an underexplored problem with re-
spect to the generative vs. discriminative debate?
previous work has mostly focused on stateless clas-
sification tasks. This paper demonstrates that Hid-
den Markov Models are capable of capturing dis-
course transitions from section to section, and are
at least competitive with Support Vector Machines
from a purely performance point of view.
The other contribution of our work is that it con-
tributes to building advanced clinical information
systems. From an application point of view, the abil-
ity to assign structure to otherwise unstructured text
represents a key capability that may assist in ques-
tion answering, document summarization, and other
natural language processing applications.
Much research in computational linguistics has
focused on corpora comprised of newswire articles.
We would like to point out that clinical texts provide
another attractive genre in which to conduct experi-
ments. Such texts are easy to acquire, and the avail-
ability of domain ontologies provides new opportu-
nities for knowledge-rich approaches to shine. Al-
though we have only experimented with lexical fea-
tures in this study, the door is wide open for follow-
on studies based on semantic features.
7 Acknowledgments
The first author would like to thank Esther and Kiri
for their loving support.
References
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature. 1987. A proposal for more infor-
mative abstracts of clinical articles. Annals of Internal
Medicine, 106:595?604.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: The MetaMap
program. In Proceeding of the 2001 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2001), pages 17?21.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the 2004 Human Language Technology Confer-
ence and the North American Chapter of the Associ-
ation for Computational Linguistics Annual Meeting
(HLT/NAACL 2004).
David G. Covell, Gwen C. Uman, and Phil R. Manning.
1985. Information needs in office practice: Are they
being met? Annals of Internal Medicine, 103(4):596?
599, October.
Dina Demner-Fushman and Jimmy Lin. 2005. Knowl-
edge extraction for clinical question answering: Pre-
liminary results. In Proceedings of the AAAI-05 Work-
shop on Question Answering in Restricted Domains.
Dina Demner-Fushman, Susan E. Hauser, and George R.
Thoma. 2005. The role of title, metadata and ab-
stract in identifying clinically relevant journal arti-
cles. In Proceeding of the 2005 Annual Symposium of
the American Medical Informatics Association (AMIA
2005), pages 191?195.
John W. Ely, Jerome A. Osheroff, M. Lee Chambliss,
Mark H. Ebell, and Marcy E. Rosenbaum. 2005. An-
swering physicians? clinical questions: Obstacles and
71
potential solutions. Journal of the American Medical
Informatics Association, 12(2):217?224, March-April.
Gunnar Evermann, H. Y. Chan, Mark J. F. Gales, Thomas
Hain, Xunying Liu, David Mrva, Lan Wang, and Phil
Woodland. 2004. Development of the 2003 CU-HTK
Conversational Telephone Speech Transcription Sys-
tem. In Proceedings of the 2004 International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP04).
Clifford W. Gay, Mehmet Kayaalp, and Alan R. Aronson.
2005. Semi-automatic indexing of full text biomedi-
cal articles. In Proceeding of the 2005 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2005), pages 271?275.
Paul N. Gorman, Joan S. Ash, and Leslie W. Wykoff.
1994. Can primary care physicians? questions be an-
swered using the medical journal literature? Bulletin
of the Medical Library Association, 82(2):140?146,
April.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning (ECML 1998).
Donald A. Lindberg, Betsy L. Humphreys, and Alexa T.
McCray. 1993. The Unified Medical Language Sys-
tem. Methods of Information in Medicine, 32(4):281?
291, August.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Kathleen McKeown, Noemie Elhadad, and Vasileios
Hatzivassiloglou. 2003. Leveraging a common rep-
resentation for personalized search and summarization
in a medical digital library. In Proceedings of the
3rd ACM/IEEE Joint Conference on Digital Libraries
(JCDL 2003).
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press, Cambridge, England.
Larry McKnight and Padmini Srinivasan. 2003. Catego-
rization of sentence types in medical abstracts. In Pro-
ceeding of the 2003 Annual Symposium of the Ameri-
can Medical Informatics Association (AMIA 2003).
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2005. Zone analysis in biology articles as a
basis for information extraction. International Journal
of Medical Informatics, in press.
Andrew Y. Ng and Michael Jordan. 2001. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive Bayes. In Advances in Neu-
ral Information Processing Systems 14.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of the 2001 Corpus Linguistics Confer-
ence.
Thomas C. Rindflesch and Marcelo Fiszman. 2003. The
interaction of domain knowledge and linguistic struc-
ture in natural language processing: Interpreting hy-
pernymic propositions in biomedical text. Journal of
Biomedical Informatics, 36(6):462?477, December.
Patrick Ruch, Christine Chichester, Gilles Cohen, Gio-
vanni Coray, Fre?de?ric Ehrler, Hatem Ghorbel, Hen-
ning Mu?ller, and Vincenzo Pallotta. 2003. Report
on the TREC 2003 experiment: Genomic track. In
Proceedings of the Twelfth Text REtrieval Conference
(TREC 2003).
Franc?oise Salanger-Meyer. 1990. Discoursal movements
in medical English abstracts and their linguistic expo-
nents: A genre analysis study. INTERFACE: Journal
of Applied Linguistics, 4(2):107?124.
John M. Swales. 1990. Genre Analysis: English in Aca-
demic and Research Settings. Cambridge University
Press, Cambridge, England.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2005. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the MEDLINE
digital library. International Journal of Medical In-
formatics, in press.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
William N. Venables and Brian D. Ripley. 1994. Modern
Applied Statistics with S-Plus. Springer-Verlag.
Steve Young, Gunnar Evermann, Thomas Hain, Dan Ker-
shaw, Gareth Moore, Julian Odell, Dave Ollason, Dan
Povey, Valtcho Valtchev, and Phil Woodland. 2002.
The HTK Book. Cambridge University Press.
72
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 33?36,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Unigram Language Models using Diffusion Smoothing over Graphs
Bruno Jedynak
Dept. of Appl. Mathematics and Statistics
Center for Imaging Sciences
Johns Hopkins University
Baltimore, MD 21218-2686
bruno.jedynak@jhu.edu
Damianos Karakos
Dept. of Electrical and Computer Engineering
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218-2686
damianos@jhu.edu
Abstract
We propose to use graph-based diffusion
techniques with data-dependent kernels
to build unigram language models. Our
approach entails building graphs, where
each vertex corresponds uniquely to a
word from a closed vocabulary, and the
existence of an edge (with an appropri-
ate weight) between two words indicates
some form of similarity between them. In
one of our constructions, we place an edge
between two words if the number of times
these words were seen in a training set
differs by at most one count. This graph
construction results in a similarity ma-
trix with small intrinsic dimension, since
words with the same counts have the same
neighbors. Experimental results from a
benchmark task from language modeling
show that our method is competitive with
the Good-Turing estimator.
1 Diffusion over Graphs
1.1 Notation
Let G = (V,E) be an undirected graph, where V
is a finite set of vertices, and E ? V ? V is the
set of edges. Also, let V be a vocabulary of words,
whose probabilities we want to estimate. Each ver-
tex corresponds uniquely to a word, i.e., there is a
one-to-one mapping between V and V . Without loss
of generality, we will use V to denote both the set
of words and the set of vertices. Moreover, to sim-
plify notation, we assume that the letters x, y, z will
always denote vertices of G.
The existence of an edge between x, y will be
denoted by x ? y. We assume that the graph
is strongly connected (i.e., there is a path between
any two vertices). Furthermore, we define a non-
negative real valued function w over V ? V , which
plays the role of the similarity between two words
(the higher the value of w(x, y), the more similar
words x, y are). In the experimental results section,
we will compare different measures of similarity be-
tween words which will result in different smooth-
ing algorithms. The degree of a vertex is defined as
d(x) =
?
y?V :x?y
w(x, y). (1)
We assume that for any vertex x, d(x) > 0; that is,
every word is similar to at least some other word.
1.2 Smoothing by Normalized Diffusion
The setting described here was introduced in (Szlam
et al, 2006). First, we define a Markov chain {Xt},
which corresponds to a random walk over the graph
G. Its initial value is equal to X0, which has dis-
tribution pi0. (Although pi0 can be chosen arbitrar-
ily, we assume in this paper that it is equal to the
empirical, unsmoothed, distribution of words over a
training set.) We then define the transition matrix as
follows:
T (x, y) = P (X1 = y|X0 = x) = d
?1(x)w(x, y).
(2)
This transition matrix, together with pi0, induces a
distribution over V , which is equal to the distribu-
33
tion pi1 of X1:
pi1(y) =
?
x?V
T (x, y)pi0(x). (3)
This distribution can be construed as a smoothed
version of pi0, since the pi1 probability of an un-
seen word will always be non-zero, if it has a non-
zero similarity to a seen word. In the same way, a
whole sequence of distributions pi2, pi3, . . . can be
computed; we only consider pi1 as our smoothed es-
timate in this paper. (One may wonder whether the
stationary distribution of this Markov chain, i.e., the
limiting distribution of Xt, as t ? ?, has any sig-
nificance; we do not address this question here, as
this limiting distribution may have very little depen-
dence on pi0 in the Markov chain cases under con-
sideration.)
1.3 Smoothing by Kernel Diffusion
We assume here that for any vertex x, w(x, x) = 0
and that w is symmetric. Following (Kondor and
Lafferty, 2002), we define the following matrix over
V ? V
H(x, y) = w(x, y)?(x ? y)? d(x)?(x = y), (4)
where ?(u) is the delta function which takes the
value 1 if property u is true, and 0 otherwise. The
negative of the matrix H is called the Laplacian of
the graph and plays a central role in spectral graph
theory (Chung, 1997). We further define the heat
equation over the graph G as
?
?t
Kt = HKt, t > 0, (5)
with initial condition K0 = I , where Kt is a time-
dependent square matrix of same dimension as H ,
and I is the identity matrix. Kt(x, y) can be inter-
preted as the amount of heat that reaches vertex x
at time t, when starting with a unit amount of heat
concentrated at y. Using (1) and (4), the right hand
side of (5) expands to
HKt(x, y) =
?
z:z?x
w(x, z) (Kt(z, y)?Kt(x, y)) .
(6)
From this equation, we see that the amount of heat
at x will increase (resp. decrease) if the current
amount of heat at x (namely Kt(x, y)) is smaller
(resp. larger) than the weighted average amount of
heat at the neighbors of x, thus causing the system
to reach a steady state.
The heat equation (5) has a unique solution which
is the matrix exponential Kt = exp(tH), (see (Kon-
dor and Lafferty, 2002)) and which can be defined
equivalently as
etH = lim
n?+?
(
I +
tH
n
)n
(7)
or as
etH = I + tH +
t2
2!
H2 +
t3
3!
H3 + ? ? ? (8)
Moreover, if the initial condition is replaced by
K0(x, y) = pi0(x)?(x = y) then the solution of
the heat equation is given by the matrix product
pi1 = Ktpi0. In the following, pi0 will be the em-
pirical distribution over the training set and t will be
chosen by trial and error. As before, pi1 will provide
a smoothed version of pi0.
2 Unigram Language Models
Let Tr be a training set of n tokens, and T a sepa-
rate test set of m tokens. We denote by n(x),m(x)
the number of times the word x has been seen in
the training and test set, respectively. We assume a
closed vocabulary V containing K words. A uni-
gram model is a probability distribution pi over the
vocabulary V . We measure its performace using
the average code length (Cover and Thomas, 1991)
measured on the test set:
l(pi) = ?
1
|T |
?
x?V
m(x) log2 pi(x). (9)
The empirical distribution over the training set is
pi0(x) =
n(x)
n
. (10)
This estimate assigns a probability 0 to all unseen
words, which is undesirable, as it leads to zero prob-
ability of word sequences which can actually be ob-
served in practice. A simple way to smooth such
estimates is to add a small, not necessarily integer,
count to each word leading to the so-called add-?
estimate pi? , defined as
pi?(x) =
n(x) + ?
n + ?K
. (11)
34
One may observe that
pi?(x) = (1??)pi0(x)+?
1
K
, with ? = ?K
n + ?K
.
(12)
Hence add-? estimators perform a linear interpola-
tion between pi0 and the uniform distribution over
the entire vocabulary.
In practice, a much more efficient smoothing
method is the so-called Good-Turing (Orlitsky et al,
2003; McAllester and Schapire, 2000). The Good-
Turing estimate is defined as
piGT (x) =
rn(x)+1(n(x) + 1)
nrn(x)
, if n(x) < M
= ?pi0(x), otherwise,
where rj is the number of distinct words seen j times
in the training set, and ? is such that piGT sums up
to 1 over the vocabulary. The threshold M is em-
pirically chosen, and usually lies between 5 and 10.
(Choosing a much larger M decreases the perfor-
mance considerably.)
The Good-Turing estimator is used frequently in
practice, and we will compare our results against it.
The add-? will provide a baseline, as well as an idea
of the variation between different smoothers.
3 Graphs over sets of words
Our objective, in this section, is to show how to de-
sign various graphs on words; different choices for
the edges and for the weight function w lead to dif-
ferent smoothings.
3.1 Full Graph and add-? Smoothers
The simplest possible choice is the complete graph,
where all vertices are pair-wise connected. In the
case of normalized diffusion, choosing
w(x, y) = ??(x = y) + 1, (13)
with ? 6= 0 leads to the add-? smoother with param-
eter ? = ??1n.
In the case of kernel smoothing with the complete
graph and w ? 1, one can show, see (Kondor and
Lafferty, 2002) that
Kt(x, y) = K
?1
(
1 + (K ? 1)e?Kt
)
if x = y
= K?1
(
1? e?Kt
)
if x 6= y.
This leads to another add-? smoother.
3.2 Graphs based on counts
A more interesting way of designing the word graph
is through a similarity function which is based on
the training set. For the normalized diffusion case,
we propose the following
w(x, y) = ?(|n(x)? n(y)| ? 1). (14)
That is, 2 words are ?similar? if they have been seen
a number of times which differs by at most one. The
obtained estimator is denoted by piND. After some
algebraic manipulations, we obtain
piND(y) =
1
n
n(y)+1?
j=n(y)?1
jrj
rj?1 + rj + rj+1
. (15)
This estimator has a Good-Turing ?flavor?. For ex-
ample, the total mass associated with the unseen
words is
?
y;n(y)=0
pi1(y) =
1
n
r1
1 + r1r0 +
r2
r0
. (16)
Note that the estimate of the unseen mass, in the case
of the Good-Turing estimator, is equal to n?1r1,
which is very close to the above when the vocabu-
lary is large compared to the size of the training set
(as is usually the case in practice).
Similarly, in the case of kernel diffusion, we
choose w ? 1 and
x ? y ?? |n(x)? n(y)| ? 1 (17)
The time t is chosen to be |V |?1. The smoother can-
not be computed in closed form. We used the for-
mula (7) with n = 3 in the experiments. Larger
values of n did not improve the results.
4 Experimental Results
In our experiments, we used Sections 00-22 (con-
sisting of ? 106 words) of the UPenn Treebank cor-
pus for training, and Sections 23-24 (consisting of
? 105 words) for testing. We split the training set
into 10 subsets, leading to 10 datasets of size ? 105
tokens each. The first of these sets was further split
in subsets of size ? 104 tokens each. Averaged re-
sults are presented in the tables below for various
choices of the training set size. We show the mean
code-length, as well as the standard deviation (when
35
mean code length std
pi?, ? = 1 12.94 0.05
piGT 11.40 0.08
piND 11.42 0.08
piKD 11.51 0.08
Table 1: Results with training set of size ? 104.
mean code length std
pi?, ? = 1 11.10 0.03
piGT 10.68 0.06
piND 10.69 0.06
piKD 10.74 0.08
Table 2: Results with training set of size ? 105.
available). In all cases, we chose K = 105 as the
fixed size of our vocabulary.
The results show that piND, the estimate ob-
tained with the Normalized Diffusion, is competi-
tive with the Good-Turing piGT . We performed a
Kolmogorov-Smirnov test in order to determine if
the code-lengths obtained with piND and piGT in Ta-
ble 1 differ significantly. The result is negative (P-
value = .65), and the same holds for the larger train-
ing set in Table 2 (P-value=.95). On the other hand,
piKD (obtained with Kernel Diffusion) is not as effi-
cient, but still better than add-? with ? = 1.
5 Concluding Remarks
We showed that diffusions on graphs can be useful
for language modeling. They yield naturally smooth
estimates, and, under a particular choice of the ?sim-
ilarity? function between words, they are competi-
tive with the Good-Turing estimator, which is con-
sidered to be the state-of-the-art in unigram lan-
guage modeling. We plan to perform more exper-
mean code length
pi?, ? = 1 10.34
piGT 10.30
piND 10.30
piKD 10.31
Table 3: Results with training set of size ? 106.
iments with other definitions of similarity between
words. For example, we expect similarities based
on co-occurence in documents, or based on notions
of semantic closeness (computed, for instance, using
the WordNet hierarchy) to yield significant improve-
ments over estimators which are only based on word
counts.
References
F. Chung. 1997. Spectral Graph Theory. Number 92
in CBMS Regional Conference Series in Mathematics.
American Mathematical Society.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley & Sons, Inc.
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
ICML ?02: Proceedings of the Nineteenth Interna-
tional Conference on Machine Learning, pages 315?
322.
David McAllester and Robert E. Schapire. 2000. On the
convergence rate of Good-Turing estimators. In Proc.
13th Annu. Conference on Comput. Learning Theory.
Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang.
2003. Always Good Turing: Asymptotically optimal
probability estimation. In FOCS ?03: Proceedings of
the 44th Annual IEEE Symposium on Foundations of
Computer Science.
Arthur D. Szlam, Mauro Maggioni, and Ronald R. Coif-
man. 2006. A general framework for adaptive regular-
ization based on diffusion processes on graphs. Tech-
nical report, YALE/DCS/TR1365.
36
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880?885,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Morphological Segmentation for Keyword Spotting
Karthik Narasimhan
1
, Damianos Karakos
2
, Richard Schwartz
2
, Stavros Tsakalidis
2
,
Regina Barzilay
1
1
Computer Science and Artificial Intelligence Laboratory,
Massachusetts Institute of Technology
2
Raytheon BBN Technologies
{karthikn, regina}@csail.mit.edu
{dkarakos, schwartz, stavros}@bbn.com
Abstract
We explore the impact of morpholog-
ical segmentation on keyword spotting
(KWS). Despite potential benefits, state-
of-the-art KWS systems do not use mor-
phological information. In this paper,
we augment a state-of-the-art KWS sys-
tem with sub-word units derived from su-
pervised and unsupervised morphological
segmentations, and compare with phonetic
and syllabic segmentations. Our exper-
iments demonstrate that morphemes im-
prove overall performance of KWS sys-
tems. Syllabic units, however, rival the
performance of morphological units when
used in KWS. By combining morphologi-
cal, phonetic and syllabic segmentations,
we demonstrate substantial performance
gains.
1 Introduction
Morphological analysis plays an increasingly im-
portant role in many language processing appli-
cations. Recent research has demonstrated that
adding information about word structure increases
the quality of translation systems and alleviates
sparsity in language modeling (Chahuneau et al.,
2013b; Habash, 2008; Kirchhoff et al., 2006; Stal-
lard et al., 2012).
In this paper, we study the impact of morpho-
logical analysis on the keyword spotting (KWS)
task. The aim of KWS is to find instances of a
given keyword in a corpus of speech data. The
task is particularly challenging for morphologi-
cally rich languages as many target keywords are
unseen in the training data. For instance, in the
Turkish dataset (Babel, 2013) we use, from the
2013 IARPA Babel evaluations, 36.06% of the test
words are unseen in the training data. However,
81.44% of these unseen words have a morpholog-
ical variant in the training data. Similar patterns
are observed in other languages used in the Babel
evaluations. This observation strongly supports
the use of morphological analysis to handle out-
of-vocabulary (OOV) words in KWS systems.
Despite this potential promise, state-of-the-art
KWS systems do not commonly use morphologi-
cal information. This surprising fact can be due to
multiple reasons, ranging from the accuracy of ex-
isting morphological analyzers to the challenge of
integrating morphological information into exist-
ing KWS architectures. While using morphemes
is likely to increase coverage, it makes recogni-
tion harder due to the inherent ambiguity in the
recognition of smaller units. Moreover, it is not
clear a priori that morphemes, which are based on
the semantics of written language, are appropriate
segmentation units for a speech-based application.
We investigate the above hypotheses in the
context of a state-of-the-art KWS architec-
ture (Karakos et al., 2013). We augment word
lattices with smaller units obtained via segmenta-
tion of words, and use these modified lattices for
keyword spotting. We consider multiple segmen-
tation algorithms, ranging from near-perfect su-
pervised segmentations to random segmentations,
along with unsupervised segmentations and purely
phonetic and syllabic segmentations. Our exper-
iments show how sub-word units can be used ef-
fectively to improve the performance of KWS sys-
tems. Further, we study the extent of impact of the
subwords, and the manner in which they can be
used in KWS systems.
2 Related Work
Prior research on applications of morphological
analyzers has focused on machine translation, lan-
guage modeling and speech recognition (Habash,
2008; Chahuneau et al., 2013a; Kirchhoff et al.,
2006). Morphological analysis enables us to link
together multiple inflections of the same root,
thereby alleviating word sparsity common in mor-
880
phologically rich languages. This results in im-
proved language model perplexity, better word
alignments and higher BLEU scores.
Recent work has demonstrated that even mor-
phological analyzers that use little or no supervi-
sion can help improve performance in language
modeling and machine translation (Chahuneau et
al., 2013b; Stallard et al., 2012). It has also been
shown that segmentation lattices improve the qual-
ity of machine translation systems (Dyer, 2009).
In this work, we leverage morphological seg-
mentation to reduce OOV rates in KWS. We in-
vestigate segmentations produced by a range of
models, including acoustic sub-word units. We in-
corporate these subword units into a lattice frame-
work within the KWS system. We also demon-
strate the value of using alternative segmentations
instead of or in combination with morphemes. In
addition to improving the performance of KWS
systems, this finding may also benefit other appli-
cations that currently use morphological segmen-
tation for OOV reduction.
3 Segmentation Methods
Supervised Morphological Segmentation Due
to the unavailability of gold morphological seg-
mentations for our corpus (Babel, 2013), we use
a resource-rich supervised system as a proxy. As
training data for this system, we use the Mor-
phoChallenge 2010 corpus
1
which consists of
1760 gold segmentations for Turkish.
We consider two supervised frameworks, both
made up of two stages. In the first stage, com-
mon to both systems, we use a FST-based mor-
phological parser (C??oltekin, 2010) that generates a
set of candidate segmentations, leveraging a large
database of Turkish roots and affixes. This stage
tends to overgenerate, segmenting each word in
eight different ways on average. In the next stage,
we filter the resulting segmentations using one of
two supervised filters (described below) trained on
the MorphoChallenge corpus.
In the first approach, we use a binary log-linear
classifier to accept/reject each segmentation hy-
pothesis. For each word, this classifier may ac-
cept multiple segmentations, or rule out all the al-
ternatives. In the second approach, to control the
number of segmentations per word, we train a log-
linear ranker that orders the segmentations for a
word in decreasing order of likelihood. In our
1
http://research.ics.aalto.fi/events/morphochallenge2010/
Feature Example
morpheme unigrams tak, acak
morpheme bigram ?tak, acak?
phonetic seq. unigrams t.a.k., 1v.dZ.a.k.
phonetic seq. bigram ?t.a.k., 1v.dZ.a.k.?
number of morphemes 2
morpheme lengths 3, 4
Table 1: Example of features used in the super-
vised filters for the segmentation tak-acak. Each
phone is followed by a dot for clarity.
training corpus, each word has on average 2.5 gold
segmentations. Hence, we choose the top two seg-
mentations per word from the output of the ranker
to use in our KWS system. In both filters, we
use several features like morpheme unigrams, bi-
grams, lengths, number of morphemes, and phone
sequences corresponding to the morphemes.
In our supervised systems, we can encode fea-
tures that go beyond individual boundaries, like
the total number of morphemes in the segmenta-
tion. This global view distinguishes our classi-
fier/ranker from traditional approaches that model
segmentation as a sequence tagging task (Ruoko-
lainen et al., 2013; Kudo et al., 2004; Kru-
engkrai et al., 2006). Another departure of our
approach is the use of phonetic information, in
the form of phonetic sequences corresponding to
the morpheme unigrams and bigrams. The hy-
pothesis is that syllabic boundaries are correlated
with morpheme boundaries to some extent. The
phonetic sequences for words are obtained using
a publicly available Text-to-Phone (T2P) system
(Lenzo, 1998).
Unsupervised Morphological Segmentation
We employ a widely-used unsupervised sys-
tem Morfessor (Creutz and Lagus, 2005) which
achieves state-of-the-art unsupervised perfor-
mance in the MorphoChallenge evaluation. Mor-
fessor uses probabilistic generative models with
sparse priors which are motivated by the Minimum
Description Length (MDL) principle. The system
derives segmentations from raw data, without re-
liance on extra linguistic sources. It outputs a sin-
gle segmentation per word.
Random Segmentation As a baseline, we in-
clude sub-word units from random segmentations,
where we mark a segmentation boundary at each
character position in a word with a fixed probabil-
ity p. For comparison purposes, we consider two
881
Sub-word units Example
Morphemes tak - acak
Random t - aka - c - a - k
Phones t - a - k - 1v - dZ - a - k
Syllables ta - k1v - dZak
Table 2: Segmentations of the word takacak into
different types of sub-word units.
types of random segmentations that match the su-
pervised morphological segmentations in terms of
the number of uniques morphemes and the average
morpheme length, respectively. These segmenta-
tions are obtained by adjusting the segmentation
probability p appropriately.
Phones and Syllables In addition to letter-
based segmentation, we also consider other sub-
word units that stem from word acoustics. In par-
ticular, we consider segmentation using phones
and syllables, which are available for the Babel
data we work with.
Table 2 shows examples of different segmenta-
tions for the Turkish word takacak.
4 Keyword Spotting
The keyword spotting system used in this work
follows, to a large extent, the pipeline of (Bulyko
et al., 2012). Using standard speech recognition
machinery, the system produces a detailed lattice
of word hypotheses. The resulting lattice is used to
extract keyword hits with nominal posterior prob-
ability scores.
We modify this basic architecture in two ways.
First, we use subwords instead of whole-words in
the decoding lexicon. Second, we represent key-
words using all possible paths in a lattice of sub-
words. For each sequence of matching arcs in the
lattice, the posteriors of these arcs are multiplied
together to form the score of detection (hit). A
post-processing step adds up (or takes the max of)
the scores of all hits of each keyword which have
significant overlap in time. Finally, the hit lists are
processed by the score normalization and combi-
nation method described in (Karakos et al., 2013).
We use whole-word extraction for words in vo-
cabulary, but rely on subword models for OOV
words. Since we combine the hits separately for
IV and OOV keywords, using subwords can only
improve the performance of the overall system.
Language Dev Set Eval Set
Turkish 403 226
Assamese 158 563
Bengali 176 629
Haitian 107 319
Lao 110 194
Tamil 238 700
Zulu 323 1251
Table 3: Number of OOV keywords in the differ-
ent Dev and Eval sets.
5 Experimental Setup
Data The segmentation algorithms described in
Section 3 are tested using the setup of the KWS
system described in Section 4. Our experiments
are conducted using the IARPA Babel Program
language collections for Turkish, Assamese, Ben-
gali, Haitian, Lao, Tamil and Zulu (Babel, 2013)
2
.
The dataset contains audio corpora and a set of
keywords. The training corpus for KWS consists
of 10 hours of speech, while the development and
test sets have durations of 10 and 5 hours, respec-
tively. We evaluate KWS performance over the
OOV keywords in the data, which are unseen in
the training set, but appear in the development/test
set. Table 3 contains statistics on the number of
OOV keywords in the data for each language.
In our experiments, we consider the pre-indexed
condition, where the keywords are known only af-
ter the decoding of the speech has taken place.
Evaluation Measures We consider two differ-
ent evaluation metrics. To evaluate the accuracy
of the different segmentations, we compare them
against gold segmentations from the MorphoChal-
lenge data for Turkish. This set consists of 1760
words, which are manually segmented. We use
a measure of word accuracy (WordAcc), which
captures the accuracy of all segmentation deci-
sions within the word. If one of the segmenta-
tion boundaries is wrong in a proposed segmen-
tation, then that segmentation does not contribute
towards the WordAcc score. We use 10-fold cross-
validation for the supervised segmentations, while
we use the entire set for unsupervised and acoustic
cases.
We evaluate the performance of our KWS sys-
tem using a widely used metric in KWS, the Ac-
2
We perform the experiments with supervised segmenta-
tion only on Turkish, due to the lack of gold morphological
data for the other languages.
882
tual Term Weighted Value (ATWV) measure, as
described in (Fiscus et al., 2007). This measure
uses a combination of penalties for misses and
false positives to score the system. The maximum
score achievable is 1.0, if there are no misses and
false positives, while the score can be lower than
0.0 if there are a lot of misses or false positives.
6 Results
Table 4 summarizes the performance of all con-
sidered segmentation systems in the KWS task on
Turkish. The quality of the segmentations com-
pared to the gold standard is also shown. Table 5
shows the OOV ATWV performance on the six
other languages, used in the second year of the
IARPA Babel project. We summarize below our
conclusions based on these results.
Using sub-word units improves overall KWS
performance If we use a word-based KWS sys-
tem, the ATWV score will be 0.0 since the OOV
keywords are not present in the lexicon. En-
riching our KWS system with sub-word segments
yields performance gains for all the segmentation
methods, including random segmentations. How-
ever, the observed gain exhibits significant vari-
ance across the segmentation methods. For in-
stance, the gap between the performance of the
KWS system using the best supervised classifier-
based segmenter (CP) and that using the unsuper-
vised segmenter (U) is 0.059, which corresponds
to a 43.7% in relative gain. Table 4 also shows that
while methods with shorter sub-units (U, P) yield
lower OOV rate, they do not necessarily fare better
in the KWS evaluation.
Syllabic units rival the performance of mor-
phological units A surprising discovery from our
experiments is the good performance of the syl-
labic segmentation-based KWS system (S). It out-
performs all the alternative segmentations on the
test set, and ranks second on the development set
behind the CP system. These units are particularly
attractive as they can easily be computed from
acoustic input and do not require any prior linguis-
tic knowledge. We hypothesize that the granular-
ity of this segmentation is crucial to its success.
For instance, a finer-grained phone-based segmen-
tation (P) performs substantially worse than other
segmentation algorithms as the derived sub-units
are shorter and hence, harder to recognize.
Improving morphological accuracy beyond a
certain level does not translate into improved
KWS performance We observe that the segmen-
tation accuracy and KWS performance are not
positively correlated. Clearly, bad segmentations
translate into poor ATWV scores, as in the case
of random and unsupervised segmentations. How-
ever, gains on segmentation accuracy do not al-
ways result in better KWS performance. For in-
stance, the ranker systems (RP, RNP) have better
accuracies on MC2010, while the classifier sys-
tems (CP, CNP) perform better on the KWS task.
This discrepancy in performance suggests that fur-
ther gains can be obtained by optimizing segmen-
tations directly with respect to KWS metrics.
Adding phonetic information improves mor-
phological segmentation For all the morpholog-
ical systems, adding phonetic information results
in consistent performance gains. For instance,
it increases segmentation accuracy by 4% when
added to the classifier (CNP and CP in table 4).
The phonetic information used in our experiments
is computed automatically using a T2P system
(Lenzo, 1998), and can be easily obtained for a
range of languages. This finding sheds new light
on the relation between phonetic and morphologi-
cal systems, and can be beneficial for morpholog-
ical analyzers developed for other applications.
Combining morphological, phonetic and syl-
labic segmentations gives better results than ei-
ther in isolation As table 4 shows, the best KWS
results are achieved when syllabic and morphemic
systems are combined. The best combination sys-
tem (CP+P+S) outperforms the best individual
system (S) by 5.5%. This result suggests that mor-
phemic, phonemic and syllabic segmentations en-
code complementary information which benefits
KWS systems in handling OOV keywords.
Morphological segmentation helps KWS
across different languages Table 5 demonstrates
that we can obtain gains in KWS performance
across different languages using unsupervised seg-
mentation. The improvement is significant in 3 of
the 6 languages - as high as 3.2% for Assamese
and Bengali, and 2.7% for Tamil (absolute per-
centages). As such, the results of Table 2 can-
not be directly compared to those of Table 1 since
the system architecture is slightly different
3
. How-
3
The keyword spotting pipeline is based on the one used
by the Babelon team in the 2014 NIST evaluation (Tsakalidis,
2014). The pipeline was much more involved than the one de-
scribed for Turkish; multiple search methods (with/without
fuzzy search) and data structures (lattices, confusion net-
works and generalized versions of these) were all used in
combination (Karakos and Schwartz, 2014). The recognition
883
Method
Unique
units
Avg. unit
length
Reduction
in OOV (abs)
WordAcc
Dev
ATWV
Test
ATWV
Phone-based (P) 51 1 36.06% 0.06% 0.099 0.164
Syllable-based (S) 2.1k 3.62 23.91% 10.29% 0.127 0.201
Classifier w/ phone info (CP) 18.5k 6.39 18.20% 80.41% 0.146 0.194
Classifier w/o phone info (CNP) 19k 6.42 21.50% 75.66% 0.133 0.181
Ranker w/ phone info (RP) 10k 5.62 16.86% 86.03% 0.104 0.153
Ranker w/o phone info (RNP) 10k 5.71 16.44% 84.19% 0.109 0.159
Unsupervised (U) 2.4k 5.44 22.45% 39.57% 0.080 0.135
RANDLen-Classifier 11.7k 6.39 0.73% 5.11% 0.061 0.086
RANDNum-Classifier 18.2k 3.03 8.56% 3.69% 0.111 0.154
RANDLen-Ranker 11.6k 5.62 1.94% 5.79% 0.072 0.136
RANDNum-Ranker 11.7k 6.13 1.15% 5.34% 0.081 0.116
CP + P - - - - 0.190 0.246
RP + P - - - - 0.150 0.210
CP + P + S - - - - 0.208 0.257
RP + P + S - - - - 0.186 0.249
Word-based for IV words - - - - 0.385 0.400
Table 4: Segmentation Statistics and ATWV scores on Babel Turkish data along with WordAcc on
MorphoChallenge 2010 data. All rows except the last are for OOV words. Absolute reduction is from an
initial OOV of 36.06%. Higher ATWV scores are better. Best system scores are shown in bold.
Assamese Bengali Haitian Lao Tamil Zulu
Dev Test Dev Test Dev Test Dev Test Dev Test Dev Test
P + S 0.213 0.230 0.277 0.296 0.371 0.342 0.228 0.139 0.349 0.267 0.279 0.215
P + S + U 0.214 0.263 0.294 0.328 0.393 0.342 0.237 0.146 0.395 0.284 0.275 0.218
Table 5: ATWV scores for languages used in the second year of the IARPA Babel project, using two
KWS systems: Phone + Syllable (P+S) and Phone + Syllable + Unsupervised Morphemes (P+S+U).
Bold numbers show significant performance gains obtained by adding morphemes to the system.
ever, they are indicative of the large gains (1.5%,
on average, over the six languages) that can be ob-
tained through unsupervised morphology, on top
of a very good combined phonetic/syllabic system.
7 Conclusion
We explore the extent of impact of morphological
segmentation on keyword spotting (KWS). To in-
vestigate this issue, we augmented a KWS system
with sub-word units derived by multiple segmen-
tation algorithms. Our experiments demonstrate
that morphemes improve the overall performance
of KWS systems. Syllabic units, however, rival the
performance of morphemes in the KWS task. Fur-
thermore, we demonstrate that substantial perfor-
mance gains in KWS performance are obtained by
combining morphological, phonetic and syllabic
was done with audio features supplied by BUT (Karafi?at et
al., 2014), which were improved versions of those used for
Turkish.
segmentations. Finally, we also show that adding
phonetic information improves the quality of mor-
phological segmentation.
Acknowledgements
This work was supported by the Intelligence
Advanced Research Projects Activity (IARPA)
via Department of Defense US Army Research
Laboratory contract number W911NF-12-C-0013.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment. We thank the MIT NLP group and the
EMNLP reviewers for their comments and sugges-
tions.
884
References
IARPA Babel. 2013. Language collection re-
leases; Turkish: IARPA-babel105b-v0.4, As-
samese: IARPA-babel102b-v0.5a, Bengali: IARPA-
babel103b-0.4b, Haitian Creole: IARPA-babel201b-
v0.2b, Lao: IARPA-babel203b-v3.1a, Tamil:
IARPA-babel204b-v1.1b, Zulu: IARPA-babel206b-
v0.1e.
Ivan Bulyko, Owen Kimball, Man-Hung Siu, Jos?e Her-
rero, and Dan Blum. 2012. Detection of un-
seen words in conversational Mandarin. In Proc. of
ICASSP, Kyoto, Japan, Mar.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013a. Translating into morpholog-
ically rich languages with synthetic phrases. In
EMNLP, pages 1677?1687. ACL.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2013b. Knowledge-rich morphological priors for
bayesian language models. In HLT-NAACL, pages
1206?1215. The Association for Computational Lin-
guistics.
C?a?gr? C??oltekin. 2010. A freely available morpho-
logical analyzer for Turkish. In Proceedings of
the 7th International conference on Language Re-
sources and Evaluation (LREC2010), pages 820?
827.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the Interna-
tional and Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning (AKRR),
pages 106?113.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, NAACL ?09, pages 406?414, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jonathan G. Fiscus, Jerome Ajot, John S. Garofolo, and
George Doddington. 2007. Results of the 2006
spoken term detection evaluation. In Workshop on
Searching Spontaneous Conversational Speech.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ?08, pages 57?60,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Martin Karafi?at, Franti?sek Gr?ezl, Mirko Hanne-
mann, Karel Vesel?y, Igor Szoke, and Jan ?Honza?
?
Cernock?y. 2014. BUT 2014 Babel system: Anal-
ysis of adaptation in NN based systems. In Pro-
ceedings of Interspeech 2014, Singapore, Septem-
ber. IEEE.
Damianos Karakos and Richard Schwartz. 2014. Sub-
word modeling. In IARPA Babel PI Meeting, July.
Damianos Karakos, Richard Schwartz, Stavros Tsaka-
lidis, Le Zhang, Shivesh Ranjan, Tim Ng, Roger
Hsiao, Guruprasad Saikumar, Ivan Bulyko, Long
Nguyen, John Makhoul, Frantisek Grezl, Mirko
Hannemann, Martin Karafiat, Igor Szoke, Karel
Vesely, Lori Lamel, and Viet-Bac Le. 2013. Score
normalization and system combination for improved
keyword spotting. In Proc. ASRU 2013, Olomouc,
Czech Republic.
Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, Kevin
Duh, and Andreas Stolcke. 2006. Morphology-
based language modeling for conversational arabic
speech recognition. Computer Speech and Lan-
guage, 20(4):589?608.
Canasai Kruengkrai, Virach Sornlertlamvanich, and
Hitoshi Isahara. 2006. A conditional random field
framework for Thai morphological analysis. In
LREC.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In In Proc. of
EMNLP, pages 230?237.
Kevin Lenzo. 1998. Text-to-phoneme converter
builder. http://www.cs.cmu.edu/afs/cs.
cmu.edu/user/lenzo/html/areas/t2p/.
Accessed: 2014-03-11.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning, pages 29?37, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012. Unsu-
pervised morphology rivals supervised morphology
for Arabic MT. In ACL (2), pages 322?327. The
Association for Computer Linguistics.
Stavros Tsakalidis. 2014. The Babelon OpenKWS14
systems. In IARPA Babel PI Meeting, July.
885
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 171?176,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Description of the JHU System Combination Scheme for WMT 2011
Daguang Xu
Johns Hopkins University
Baltimore, USA
dxu5@jhu.edu
Yuan Cao
Johns Hopkins University
Baltimore, USA
yuan.cao@jhu.edu
Damianos Karakos
Johns Hopkins University
Baltimore, USA
damianos@jhu.edu
Abstract
This paper describes the JHU system combi-
nation scheme used in WMT-11. The JHU
system combination is based on confusion
network alignment, and inherited the frame-
work developed by (Karakos et al, 2008).
We improved our core system combination al-
gorithm by making use of TER-plus, which
was originally designed for string alignment,
for alignment of confusion networks. Exper-
imental results on French-English, German-
English, Czech-English and Spanish-English
combination tasks show significant improve-
ments on BLEU and TER by up to 2 points on
average, compared to the best individual sys-
tem output, and improvements compared with
the results produced by ITG which we used in
WMT-10.
1 Introduction
System combination aims to improve the translation
quality by combining the outputs from multiple in-
dividual MT systems. The state-of-the-art system
combination methodologies can be roughly catego-
rized as follows (Karakos et al, 2010):
1. Confusion network based: confusion network
is a form of lattice with the constraint that all
paths need to pass through all nodes. An exam-
ple of a confusion network is shown in Figure
1.
Here, the set of arcs between two consecutive
nodes represents a bin, the number following a
word is the count of this word in its bin, and
0 1this/10 2is/7was/3 3
a/8
one/2 4
dog/9
cat/1 5/0
./10
Figure 1: Example confusion network. The total count in
each bin is 10.
each bin has the same size. The basic method-
ology of system combination based on confu-
sion network includes the following steps: (a)
Choose one system output as the ?skeleton?,
which roughly decides the word order. (b)
Align further system outputs to the skeleton,
thus forming a confusion network. (c) Rescore
the final confusion network using a language
model, then pick the best path as the output of
combination.
A textual representation (where each line con-
tains the words and counts of each bin) is usu-
ally the most convenient for machine process-
ing.
2. Joint optimization based: unlike building con-
fusion network, this method considers all sys-
tem outputs at once instead of incrementally.
Then a log-linear model is used to derive costs,
followed by a search algorithm to explore the
combination space (Jayaraman et al, 2005;
Heafield et al, 2009; He et al, 2009).
3. Hypothesis selection based: this method only
includes algorithms that output one of the input
translations, and no word selection from mul-
tiple systems is performed. Typical algorithms
can be found in (Rosti et al, 2007).
171
This paper describes the JHU system com-
bination submitted to the Sixth Workshop
on Statistical Machine Translation (WMT-11)
(http://statmt.org/wmt11/index.html ). The JHU
system combination is confusion network based
as described above, following the basic system
combination framework described in (Karakos et
al., 2008). However, instead of ITG alignments
that were used in (Karakos et al, 2008), alignments
based on TER-plus (Snover et al, 2009) were used
now as the core system alignment algorithm.
The rest of the paper is organized as follows:
Section 2 introduces the application of TER-plus in
system combination. Section 3 introduces the JHU
system combination pipeline. Section 4 presents the
combination results and concluding remarks appear
in Section 5.
2 Word Reordering for Hypothesis
Alignment
Given the outputs of multiple MT systems, we
would like to reorder and align the words of different
hypothesis in a way such that an objective function is
optimized, thus reaching better translations by mak-
ing use of more information. In our system combi-
nation scheme, the objective function was based on
Translation-Edit-Rate Plus (TER-plus).
2.1 Introduction to TER-plus
TER-plus is an extension of Translation Error Rate
(TER) (Snover et al, 2006). TER is an evaluation
metric for machine translation; it generalizes Word
Error Rate (WER) by allowing block shifts in addi-
tion to the edit distance operations. However, one
problem with TER is that only exact match of word
blocks are allowed for shifting; this constraint might
be too strict as it sometimes prevents reasonable
shifts if two blocks have similar meanings.
TER-plus remedies this problem by introducing
new flexible matches between words, thus allowing
word substitutions and block shifts with costs much
lower than that of TER. Specifically, substitution
costs are now dependent on whether the words have
the same stem (stem matches) or are synonyms (syn-
onym matches). These operations relax the shift-
ing constraints of TER; shifts are now allowed if the
words of one string are synonyms or share the same
stem as the words of the string they are compared to
(Snover et al, 2009).
TER-plus identifies words with the same stem us-
ing the Porter stemming algorithm (Porter et al,
1980), and identifies synonyms using the WordNet
database (Miller et al, 1995).
2.2 TER-plus for system combination
Originally, TER-plus was designed for aligning to-
gether word strings. However, similar to the work
of (Karakos et al, 2010), who extended ITG to al-
low bilingual parsing of two confusion networks (by
treating each confusion network bin as a multi-word
entity), we converted the basic TER-plus code to
take into account multiple words present in confu-
sion network bins. Specifically, we define the cost
of aligning two confusion network bins as (Karakos
et al, 2010)
cost(b1, b2) =
1
|b1||b2|
?
w1?b1
?
w2?b2
C(w1, w2)
in which b1,b2 are the confusion network bins which
are candidates for alignment, | ? | is the size of a
bin, w1, w2 are words in b1 and b2 respectively, and
C(w1, w2) is defined as follows:
C(w1, w2) =
?
???????
???????
0 w1 matches w2
0.5 w2 is deleted
0.6 w2 is inserted
0.2 w1 and w2 are synonyms
0.2 w1 and w2 share stems
1 none of the above
Furthermore, the bin shift cost is set to 1.5. These
numbers are empirically determined based on exper-
imental results.
Similar to (Karakos et al, 2010), when a bin gets
?deleted?, it gets replaced with a NULL arc, which
simply encodes the empty string, and is otherwise
treated as a regular token in the alignments.
3 The JHU System Combination Pipeline
We now describe the JHU system combination
pipeline in which TER-plus is used as the core con-
fusion network alignment algorithm as introduced in
the previous section.
172
3.1 Combination procedure overview
The JHU system combination scheme is based on
confusion network as introduced in section 1. The
confusion networks are built in two stages:
1. Within-system combination: (optional, only
applicable in the case where per-system n-best
lists are available.) the within-system combi-
nation generates system-specific confusion net-
works based on the alignment of the n-best
translations.
2. Between-system combination: incremental
alignment of the confusion networks of differ-
ent systems generated in step 1, starting from
2-system combination up to the combination of
all systems. The order with which the systems
are selected is based on the individual BLEU
scores (i.e., the best two systems are first com-
bined, then the 3rd best is aligned to the result-
ing confusion network, etc.)
For the between-system combination we made
use of TER-plus as described in section 2.2.
3.2 Language model Rescoring with
Finite-State Transducer Operations
Once the between-system confusion networks are
ready (one confusion network per sentence), a path
through each of them has to be selected as the com-
bination output. In order to pick out the the most flu-
ent word sequence as the final translation, we need
to rescore the confusion networks using a language
model. This task can be performed efficiently via fi-
nite state transducer (FST) operations (Allauzen et
al., 2002). First, we build an FST for each confu-
sion network, called CN-FST. Since the confusion
network is just a sequence of bins and each bin is a
superposition of single words, the CN-FST can be
built as a linear FST in a straightforward way (see
Figure 1).
A 5-gram language model FST (LM-FST) is then
built for each sentence. To build the LM-FST, we
refer to the methodology described in (Allauzen et
al., 2003). In brief, the LM-FST is constructed in
the following way:
1. Extract the vocabulary of each segment.
2. Each state of the FST encodes an n-gram his-
tory (n ? 1 words). Each (non-null) arc that
originates from that state corresponds uniquely
to a word type (i.e., word that follows that his-
tory in the training data).
3. The cost of each word arc is the corre-
sponding language model score (negative log-
probability, based on the modified Kneser-Ney
formula (Kneser, 1995) for that n-gram).
4. Extra arcs are added for backing-off to lower-
order histories, thus allowing all possible word
strings to receive a non-zero probability.
In order to deal with the situation where a word
in the confusion network is not in the vocabulary of
the language model, we need to build another sim-
ple transducer, namely, the ?unknown word? FST
(UNK-FST), to map this word to the symbol <unk>
that encodes the out-of-vocabulary (OOV) words.
Note that this is useful only if one builds open-
vocabulary language models which always give a
non-zero probability to OOV words; e.g., check
out the option -unk of the SRILM toolkit (Stolcke,
2002). (Obviously, the UNK-FST leaves all other
words unmodified.)
After all these three transducers have been built,
they are composed in the following manner (for each
sentence):
CN-FST .o. UNK-FST .o. LM-FST
Note that a possible re-weighting of the arc costs
of the CN-FST can be done in order to better account
for the different dynamic ranges between the CN
costs and the LM-FST costs. Furthermore, to avoid
too many word deletions (especially in regions of the
confusion network where the words disagree most)
an additive word deletion penalty can be added to all
NULL arcs. The best (minimum-cost) path from this
resulting FST is selected as the output translation of
the system combination for that sentence.
3.3 System combination pipeline summary
We now summarize the JHU system combination
end-to-end pipeline as follows(since BLEU score is
a key metric in the WMT11 translation evaluation,
we use BLEU score as the system ranking criteria.
The BLEU score we computed for the experiments
below are all case-insensitive):
173
1. Process and re-format (lowercase, tokenize,
romanize, etc.) all individual system out-
puts. Note that we compute the case-insensitive
BLEU score in our experiments.
2. Build LM-FST and UNK-FST for each sen-
tence.
3. Decide the between-system combination order
according to the 1-best output BLEU score of
individual systems.
4. Do between-system combination based on the
order decided in step 3 using TER-plus.
5. Rescore the confusion network and start tuning
on the parameters: convert the between-system
confusion network into FST, compose it with
the UNK-FST and with the LM-FST. When
composing with LM-FST, try different CN arc
coefficients (we tried the range {5, . . . , 21}),
and unknown word insertion penalties (we tried
the values {0.3, 0.5, 0.7, 1}).
6. Compute the BLEU score for all m-syst x y
outputs, where m is the number of systems for
combination, x is the weight and y is the inser-
tion penalty.
7. Among all the scores computed in step 6, find
the best BLEU score, and keep the correspond-
ing parameter setting(m, x, y).
8. Apply the best parameter setting to the test
dataset for evaluation.
Obviously, if n-best outputs from systems are avail-
able, an extra step of producing within-system com-
binations (and searching for the best n-best size) will
also be executed.
4 Results
In WMT11, we participated in French-English,
German-English, Czech-English and Spanish-
English system combination tasks. Although we
followed the general system combination pipeline
introduced in 3.3, we did not do the within-system
combination since we received only 1-best outputs
from all systems.
We built both primary and contrastive systems,
and they differ in the way the 5-gram language mod-
els were trained. The language model for the pri-
mary system was trained with the monolingual Eu-
roparl, news commentary and news crawl corpus
provided by WMT11. The language model for the
contrastive system was trained using only the 1-
best outputs from all individual systems (sentence-
specific language model).
The number of systems used for combination
tuning in each language pair was: 24 for French-
English, 26 for German-English, 12 for Czech-
English, and 16 for Spanish-English. The best re-
sults for the combination in the primary system
made use of 23 systems for French-English, 5 sys-
tems for German-English, 10 systems for Czech-
English, 10 systems for Spanish-English. In the con-
trastive system, the number of systems were 20, 5,
6, 10 respectively.
The TER and BLEU scores on the development
set for the best individual system, the primary and
contrastive combinations are given in Table 1, and
the scores for test set are given in Table 2. From the
results we see that, compared with the best individ-
ual system outputs, system combination results in
significantly improved BLEU scores and remarkable
reductions on TER, for all language pairs. More-
over, we observe that the primary system performs
slightly better than the contrastive system in most
cases.
We also did the experiment of xx-English which
made combinations of all English outputs available
across different source languages. We used 35 sys-
tems in this experiment for both primary and con-
trastive combination, and best result made use of 15
and 16 systems respectively. The development and
test set results are shown in the ?xx-en? column in
table 1 and 2 respectively. From the results we see
the improvements on TER and BLEU scores of both
development and test sets almost doubled compared
with the best results of single language pairs.
To make a comparison with the old technique
we used in WMT10 system combination task, we
ran the WMT11 system combination task using ITG
with surface matching. The detailed implementation
is described in (Narsale, 2010). Table 3 and 4 show
the WMT11 results using ITG for alignment respec-
tively. It can be seen that TER-plus outperforms ITG
174
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 56.2 28.1 60.1 23.6 54.9 27.9 51.8 30.2 51.8 30.2
Primary combination 49.2 32.6 58.1 25.7 55.1 28.7 48.3 33.7 44.9 35.5
Contrastive combination 49.8 32.3 58.2 25.6 54.9 28.9 49.1 33.3 45.0 37.2
Table 1: Results for all language pairs on development set. The best number in each column is shown in bold.
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 58.2 30.5 65.1 23.5 59.7 29.1 60.0 28.9 58.2 30.5
Primary combination 55.9 31.9 64.4 25.0 60.1 29.6 55.4 33.5 51.7 36.3
Contrastive combination 56.5 31.6 65.7 24.4 59.9 29.8 56.5 33.4 52.5 36.5
Table 2: Results for all language pairs on test set. The best number in each column is shown in bold.
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 56.2 28.1 60.1 23.6 54.9 27.9 51.8 30.2 51.8 30.2
Primary combination 49.0 32.5 57.6 25.0 54.6 28.1 48.8 33.1 45.3 35.7
Contrastive combination 56.1 31.7 58.0 24.9 55.0 28.0 49.4 33.0 45.6 35.9
Table 3: Results for all language pairs on development set using ITG. The best number in each column is shown in
bold.
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 58.2 30.5 65.1 23.5 59.7 29.1 60.0 28.9 58.2 30.5
Primary combination 55.9 31.9 64.5 24.7 60.1 29.4 55.8 33.0 52.2 35.0
Contrastive combination 56.6 31.4 64.7 24.4 60.7 29.6 56.6 33.0 52.9 35.3
Table 4: Results for all language pairs on test set using ITG. The best number in each column is shown in bold.
175
almost in all results. We will experiment with ITG
and flexible match costs and will report results in a
subsequent publication.
5 Conclusion
We described the JHU system combination scheme
that was used in WMT-11. The JHU system com-
bination system is confusion network based, and
we demonstrated the successful application of TER-
plus (which was originally designed for string align-
ment) to confusion network alignment. The WMT-
11 submission results show that significant improve-
ments on the TER and BLEU scores (over the best
individual system) were achieved.
Acknowledgments
This work was supported by the DARPA GALE pro-
gram Grant No HR0022-06-2-0001. We would also
like to thank the IBM Rosetta team for their strong
support in the system combination evaluation tasks.
References
D. Karakos, J. Smith, and S. Khudanpur. 2010. Hypoth-
esis ranking and two-pass approaches for machine
translation system combination. Acoustics Speech
and Signal Processing (ICASSP), IEEE International
Conference on.
S. Jayaraman and A. Lavie. 2005. Multi-engine machine
translation guided by explicit word matching. Proc.
EAMT:143?152.
K. Heafield, G. Hanneman, and A. Lavie. 2009.
Machinetranslation system combination with flexible
word ordering. Proc. EACL 2009, WSMT.
X. He and K. Toutanova. 2009. Joint optimization
for machine translation system combination. Proc.
EMNLP.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. Proceedings of Association for Computa-
tional Linguistics(ACL)
D. Karakos, J. Eisner, S. Khudanpur, M. Dreyer. 2008.
Machine translation system combination using ITG-
based alignments. Proceedings of Association for
Computational Linguistics(ACL) HLT, Short Papers
(Companion Volume):81-84.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, J.
Makhoul. 2006 A Study of Translation Edit Rate with
Targeted Human Annotation. Proceedings of Associa-
tion for Machine Translation in the Americas.
G.Miller. 1995 WordNet: A Lexical Database for En-
glish. . Communications of the ACM Vol. 38, No. 11.
M. Snover, N. Madnani, B. Dorr, R. Schwartz. 2009
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation at the 12th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2009), Athens, Greece.
M.F.Porter. 1980 An algorithm for suffix stripping. Pro-
gram 14(3):130-137
C. Allauzen, M. Mohri, B. Roark 1980 Generalized Al-
gorithms for Constructing Statistical Language Mod-
els. Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, July 2003,
pp. 40-47.
Sushant Narsale. 2010 JHU system combination scheme
for WMT 2010. Proceedings of Fifth Workshop on
Machine Translation, ACL.
R. Kneser, Ney. H. 2010 Improved backing-off for m-
gram language modeling. Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP.
A. Stolcke 2002 SRILM - An Extensible Language Mod-
eling Toolkit. Proceedings of International Conference
on Spoken Language Processing.
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, Mehryar
Mohri. 2002 OpenFst: A General and Efficient
Weighted Finite-State Transducer Library Proceed-
ings of the Ninth International Conference on Im-
plementation and Application of Automata, (CIAA
2007), vol. 4783, Lecture Notes in Computer Science,
pages 11-23, 2007
WMT11 official webpage. http://statmt.org/wmt11 /in-
dex.html
176
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
