Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1128?1136,
Beijing, August 2010
Efficient Statement Identification for Automatic Market Forecasting
Henning Wachsmuth
Universita?t Paderborn
Software Quality Lab
hwachsmuth@slab.upb.de
Peter Prettenhofer and Benno Stein
Bauhaus-Universita?t Weimar
Web Technology & Information Systems
benno.stein@uni-weimar.de
Abstract
Strategic business decision making in-
volves the analysis of market forecasts.
Today, the identification and aggregation
of relevant market statements is done by
human experts, often by analyzing doc-
uments from the World Wide Web. We
present an efficient information extrac-
tion chain to automate this complex nat-
ural language processing task and show
results for the identification part. Based
on time and money extraction, we iden-
tify sentences that represent statements on
revenue using support vector classifica-
tion. We provide a corpus with German
online news articles, in which more than
2,000 such sentences are annotated by do-
main experts from the industry. On the
test data, our statement identification al-
gorithm achieves an overall precision and
recall of 0.86 and 0.87 respectively.
1 Introduction
Touch screen market to hit $9B by 2015. 50 sup-
pliers provide multi-touch screens, and that num-
ber is likely to rise.1
Strategic business decision making is a highly
complex process that requires experience as well
as an overall view of economics, politics, and
technological developments. Clearly, for the time
being this process cannot be done by a computer at
the level of a human expert. However, important
tasks may be automated such as market forecast-
ing, which relies on identifying and aggregating
relevant information from the World Wide Web
(Berekoven et. al., 2001). An analyst who inter-
prets the respective data can get a reasonable idea
about the future market volume, for example. The
1Adapted from http://industry.bnet.com.
problem is that a manually conducted Web search
is time-consuming and usually far from being ex-
haustive. With our research we seek to develop
an efficient system that finds and analyzes market
forecast information with retrieval, extraction and
natural language processing (NLP) techniques.
We contribute to the following situation. For a
given product, technology, or industry sector we
identify and aggregate statements on its market
development found on relevant websites. In par-
ticular, we extract time information (?by 2015?)
and money information (?$9B?) and use support
vector classification to identify sentences that rep-
resent market statements. The statements? sub-
jects (?touch screen?) are found by relating recog-
nized named entities to the time and money infor-
mation, which we then normalize and aggregate.
In this paper we report on results for the statement
identification. To the best of our knowledge no
data for the investigation of such market analysis
tasks has been made publicly available until now.
We provide such a corpus with statements on rev-
enue annotated in news articles from the Web; the
corpus was created in close collaboration with our
industry partner Resolto Informatik GmbH.
We pursue two objectives, namely, to support
human experts with respect to the effectiveness
and completeness of their analysis, and to estab-
lish a technological basis upon which more intri-
cate analysis tasks can be automated. To summa-
rize, the main contributions of this paper are:
1. We show how to decompose the identifi-
cation and aggregation of forecasts into re-
trieval, extraction, and normalization tasks.
2. We introduce a manually annotated German
corpus for computational linguistics research
on market information.
3. We offer empirical evidence that classifica-
tion and extraction techniques can be com-
1128
bined to precisely identify statements on rev-
enue.
1.1 Related Work
Stein et. al. (2005) were among the first to con-
sider information extraction for automatic mar-
ket forecasting. Unlike us, the authors put much
emphasis on retrieval aspects and applied depen-
dency grammar parsing to identify market state-
ments. As a consequence their approach suffers
from the limitation to a small number of prede-
fined sentence structures.
While we obtain market forecasts by extract-
ing expert statements from the Web, related ap-
proaches derive them from past market behavior
and quantitative news data. Koppel and Shtrim-
berg (2004) studied the effect of news on finan-
cial markets. Lavrenko et al (2000) used time-
series analysis and language models to predict
stock market prices and, similarly, Lerman et al
(2008) proposed a system for forecasting public
opinion based on concurrent modeling of news ar-
ticles and market history. Another related field is
opinion mining in the sense that it relies on the ag-
gregation of individual statements. Glance et al
(2005) inferred marketing intelligence from opin-
ions in online discussions. Liu et al (2007) exam-
ined the effect of Weblogs on box office revenues
and combined time-series with sentiment analysis
to predict the sales performance of movies.
The mentioned approaches are intended to re-
flect or to predict present developments and,
therefore, primarily help for operative decision
making. In contrast, we aim at predicting long-
term market developments, which are essential for
strategic decision making.
2 The Problem
Market forecasts depend on two parameters, the
topic of interest and the criterion to look at. A
topic is either an organization or a market. Under
a market we unite branches, products, and tech-
nologies, because the distinction between these is
not clear in general (e.g., for semiconductors). In
contrast, we define a criterion to be a metric at-
tribute that can be measured over time. Here we
are interested in financial criteria such as revenue,
profit, and the like. The ambitious overall task that
we want to solve is as follows:
Task description: Given a topic ? and a finan-
cial criterion ?, find information for ? on the de-
velopment of ?. Aggregate the found values on ?
with respect to time.
We omit the limitation to forecasts because we
could miss useful information otherwise:
(1) In 2008, the Egyptian automobile industry
achieved US$ 9.96bn in sales.
(2) Egypt?s automotive sales will rise by 97%
from 2008 to 2013.
Both sentences have the same topic. In Particu-
lar, the 2008 amount of money from example (1)
can be aggregated with the forecast in (2) to infer
the predicted amount in 2013.
As in these examples, market information can
often only be found in running text; the major
source for this is the Web. Thus, we seek to
find web pages with sentences that represent state-
ments on a financial criterion ? and to make
these statements processable. Conceptually, such
a statement is a 5-tuple S? = (S, g, T,M, td),
where S is the topical subject, which may have a
geographic scope g, T is a period of time, M con-
sists of a growth rate and/or an amount of money
to be achieved during T with respect to ?, and td
is the statement time, i.e., the point in time when
the statement was made.
3 Approach
Our goal is to find and aggregate statements on
a criterion ? for a topic ? . In close collaboration
with two companies from the semantic technology
field, we identified eight high-level subtasks in the
overall process as explained in the following. An
overview is given in Table 1.
3.1 Find Candidate Documents
To find web pages that are likely to contain state-
ments on ? and ? , we propose to perform a meta-
search by starting from a set of characteristic
terms of the domain and then using query expan-
sion techniques such as local context analysis (Xu
and Croft, 2000). As Stein et. al. (2005) describe,
1129
Subtask Applied technologies
1 Find candidate documents meta-search, query expansion, genre analysis
2 Preprocess content content extraction, sentence splitting, tokenization, POS tagging and chunking
3 Extract entities time and money extraction, named entity recognition of organizations and markets
4 Identify statements statistical classification based on lexical and distance features
5 Determine statement type relation extraction based on dependency parse trees, matching of word lists
6 Fill statement templates template filling, anaphora resolution, matching of word lists
7 Normalize values time and money normalization, coreference resolution
8 Aggregate information chronological merging and averaging, inference from subtopic to topic
Table 1: Subtasks of the identification and aggregation of market statements for a specified topic.
Experiments in this paper cover the subtasks written in black.
a genre analysis, which classifies a document with
respect to its form, style, and targeted audience,
may be deployed afterwards to further improve
the quality of the result list efficiently. In this way,
we only maintain candidate documents that look
promising on the surface.
3.2 Preprocess Content
Preprocessing is needed for accurate access to the
document text. Our overall task incorporates re-
lating information from different document areas,
so mixing up a web page?s main frame and side-
bars should be avoided. We choose Document
Slope Curve (DSC) for content detection, which
looks for plateaus in the HTML tag distribution.
Gottron (2007) has offered evidence that DSC
is currently the best algorithm in terms of pre-
cision. Afterwards, the sentences are split with
rules that consider the specific characteristics of
reports, press releases and the like, such as head-
lines between short paragraphs. In succeeding
subtasks, tokens as well as their Part-of-Speech
and chunk tags are also used, but we see no point
in not relying on standard algorithms here.
3.3 Extract Entities
The key to identify a statement S? on a finan-
cial criterion ? is the extraction of temporal and
monetary entities. Recent works report that sta-
tistical approaches to this task can compete with
hand-crafted rules (Ahn et. al., 2005; Cramer et.
al., 2007). In the financial domain, however, the
focus is only on dates and periods as time infor-
mation, along with currency numbers, currency
terms, or fractions as money information. We
found that with regular expressions, which rep-
resent the complex but finite structures of such
phrases, we can achieve nearly perfect recall in
recognition (see Section 5).
We apply named entity recognition (NER) of
organizations and markets in this stage, too, so we
can relate statements to the appropriate subjects,
later on. Note that market names do not follow a
unique naming scheme, but we observed that they
often involve similar phrase patterns that can be
exploited as features. NER is usually done by se-
quence labeling, and we use heuristic beam search
due to our effort to design a highly efficient overall
system. Ratinov and Roth (2009) have shown for
the CoNLL-2003 shared task that Greedy decod-
ing (i.e., beam search of width 1) is competitive
to the widely used Viterbi algorithm while being
over 100 times faster at the same time.
3.4 Identify Statements
Based on time and money information, sentences
that represent a statement S? can be identified.
Such a sentence gives us valuable hints on which
temporal and monetary entity stick together and
how to interpret them in relation. Additionally,
it serves as evidence for the statement?s correct-
ness (or incorrectness). Every sentence with at
least one temporal and one monetary entity is a
candidate. Criteria such as revenue usually imply
small core vocabularies Lpos, which indicate that
a sentence is on that criterion or which often ap-
pear close to it. On the contrary, there are sets of
words Lneg that suggest a different criterion. For
a given text collection with known statements on
?, both Lpos and Lneg can be found by computing
the most discriminant terms with respect to ?. A
reasonable first approach is then to filter sentences
1130
that contain terms from Lpos and lack terms from
Lneg, but problems arise when terms from differ-
ent vocabularies co-occur or statements on differ-
ent criteria are attached to one another.
Instead, we propose a statistical learning ap-
proach. Support Vector Machines (SVMs) have
been proven to yield very good performance in
both general classification and sentence extraction
while being immune to overfitting (Steinwart and
Christmann, 2008; Hirao et. al., 2001). For our
candidates, we compute lexical and distance fea-
tures based on Lpos, Lneg, and the time and money
information. Then we let an SVM use these fea-
tures to distinguish between sentences with state-
ments on ? and others. At least for online news
articles, this works reasonably well as we demon-
strate in Section 5. Note that classification is not
used to match the right entities, but to filter the
small set of sentences on ?.
3.5 Determine Statement Type
The statement type implies what information we
can process. If a sentence contains more than one
temporal or monetary entity, we need to relate the
correct T and M to each S?, now. The type of S?
then depends on the available money information,
its trend and the time direction.
We consider four types of money information.
? refers to a period of time that results in a new
amount A of money in contrast to its preceding
amount Ap. The difference between A and Ap
may be specified as an incremental amount ?A
or as a relative growth rate r. M can span any
combination of A, Ap, ?A and r, and at least A
and r constitute a reasonable entity on their own.
Sometimes the trend of r (i.e. decreasing or in-
creasing) cannot be derived from the given val-
ues. However, this information can mostly be ob-
tained from a nearby indicator word (e.g. ?plus? or
?decreased?) and, therefore, we address this prob-
lem with appropriate word lists. Once the trend is
known, any two types imply the others.
Though we are predominantly interested in
forecasts, statements also often represent a decla-
ration on achieved results. This distinction is es-
sential and can be based on time-directional indi-
cators (e.g. ?next?) and the tense of leading verbs.
For this, we test both feature and kernel methods
on dependency parse trees, thereby determining T
and M at the same time. We only parse the iden-
tified sentences, though. Hence, we avoid running
into efficiency problems.
3.6 Fill Statement Templates
The remaining subtasks are ongoing work, so we
only present basic concepts here.
Besides T and M , the subject S and the state-
ment time td have to be determined. S may be
found within the previously extracted named enti-
ties using the dependency parse tree from Section
3.5 or by anaphora resolution. Possible limitations
to a geographic scope g can be recognized with
word lists. In market analysis, the approximate
td suffices, and for most news articles td is simi-
lar to their release date. Thus, if no date is in the
parse tree, we search the extracted temporal enti-
ties for the release date, which is often mentioned
at the beginning or end of the document?s content.
We fill one template (S, g, T,M, td) for each S?
where we have at least S, T , and M .
3.7 Normalize Values
Since we base the extraction on regular expres-
sions, we can normalize most monetary entities
with a predefined set of rules. Section 3.5 implies
that M? = (A?, r?) is a reasonable normalized
form where A? is A specified in million US-$ and
r? is r as percentage with a fixed number of deci-
mals.2 Time normalization is more complex. Any
period should be transformed to T ? = (t?s, t?e)
consisting of the start date t?s and end date t?e .
Following Ahn et. al. (2005), we consider fully
qualified, deictic and anaphoric periods. While
normalization of fully qualified periods like ?from
Apr to Jun 1999? is straightforward, deictic (e.g.
?since 2005?, ?next year?) and anaphoric men-
tions (e.g. ?in the reported time?) require a refer-
ence time. Approaches to resolve such references
rely on dates or fully qualified periods in the pre-
ceding text (Saquete et. al., 2003; Mani and Wil-
son, 2000).3
2Translating the currency requires exchange rates at state-
ment time. We need access to such information or omit the
translation if only one currency is relevant.
3References to fiscal years even involve a whole search
problem if no look-up table on such data is available.
1131
without interference
with interference
0
2
4
A
A?p
A?
Ap
t 0
2
4
t
0
2
4
A
A?Ap=A?p
t 0
2
4
t
mill.US-$ mill.US-$
mill.US-$ mill.US-$
Figure 1: Example for merging monetary values.
9
10
11
A?
A??
A
t
mill.US-$
-10%
0%
10%
t
r
Figure 2: Example for the inference of relative in-
formation from absolute values.
If we cannot normalize M or T , we discard the
corresponding statement templates. For the oth-
ers, we have to resolve synonymous co-references
(e.g. ?Loewe AG? and ?Loewe?) before we can
proceed to the last step.
3.8 Aggregate Information
We can aggregate the normalized values in either
two or three dimensions depending on whether
to separate statements with respect to td. Aggre-
gation then incorporates two challenges, namely,
how to merge values and how to infer information
on a topic from values of a subtopic.
We say that two statements on the same topic
? and criterion ? interfere if the contained peri-
ods of time intersect and the according monetary
values do not coincide. In case of declarations,
this means that we extracted incorrect values or
extracted values incorrectly. For forecasts, on the
contrary, we are exactly onto such information.
In both cases, an intuitive solution is to compute
the average (or median) and deviations. Figure 1
graphically illustrates such merging. The subtopic
challenge is based on the assumption that a mean-
ingful number of statements on a certain subtopic
of ? implies relative information on ? , as shown in
Figure 2. One of the most interesting relations are
organizations as subtopics of markets they pro-
duce for, because it is quite usual to search for
Statements Total Forecasts Declarations
Complete corpus 2075 523 (25.2%) 1552 (74.8%)
Training set 1366 306 (22.4%) 1060 (77.6%)
Validation set 362 113 (31.2%) 249 (68.8%)
Test set 347 104 (30.0%) 243 (70.0%)
Table 2: Statements on revenue in the corpus.
information on a market, but only receive state-
ments on companies. Approaches to this relation
may rely e.g. on the web page co-occurrence and
term frequencies of the markets and companies.
Altogether, we return the aggregated values
linked to the sentences in which we found them.
In this way, we make the results verifiable and,
thereby, compensate for possible inaccuracies.
4 Corpus
To evaluate the given and related tasks, we built
a manually annotated corpus with online news ar-
ticles on the revenues of organizations and mar-
kets. The compilation aims at being representa-
tive for target documents, a search engine returns
to queries on revenue. The purpose of the corpus
is to investigate both the structure of sentences on
financial criteria and the distribution of associated
information over the text.
The corpus consists of 1,128 German news ar-
ticles from the years 2003 to 2009, which were
taken from 29 news websites like www.spiegel.de
or www.capital.de. The content of each document
comes as unicode plain text with appended URL
for access to the HTML source code. Annotations
are given in a standard XMI file preformatted for
the Unstructured Information Management Archi-
tecture (Ferrucci and Lally, 2004). We created a
split, in which 2/3 of the documents constitute the
training set and each 1/6 refers to the validation
and test set. To simulate real conditions, the train-
ing documents were randomly chosen from only
the seven most represented websites, while the
validation and test data both cover all 29 sources.
Table 2 shows some corpus statistics, which give
a hint that the validation and test set differ sig-
nificantly from the training set. The corpus is
free for scientific use and can be downloaded at
http://infexba.upb.de.
1132
Loewe AG: Vorla?ufige Neun-Monats-Zahlen
Kronach, [6. November 2007]REF ? Das Ergebnis vor
Zinsen und Steuern (EBIT) des Loewe Konzerns konnte
in den ersten 9 Monaten 2007 um 41% gesteigert wer-
den. Vor diesem Hintergrund hebt die [Loewe AG]ORG
ihre EBIT-Prognose fu?r das laufende Gescha?ftsjahr auf
20 Mio. Euro an. Beim Umsatz strebt Konzernchef
[Rainer Hecker]AUTH [fu?r das Gesamtjahr]TIME ein
ho?her als urspru?nglich geplantes [Wachstum]TREND
[von 10% auf ca. 380 Mio. Euro]MONEY an. (...)
Figure 3: An annotated document in the corpus.
The text is taken from www.boerse-online.de, but
has been modified for clarification.
4.1 Annotations
In each document, every sentence that includes a
temporal entity T and a monetary entity M and
that represents a forecast or declaration on the
revenue of an organization or market is marked
as such. T and M are annotated themselves and
linked to the sentence. Accordingly, the subject
is tagged (and linked) within the sentence bound-
aries if available, otherwise its last mention in the
preceding text. The same holds for optional en-
tities, namely a reference time, a trend indicator
and the author of a statement. Altogether, 2,075
statements are tagged in this way. As in Figure
3, only information that refers to a statement on
revenue (typed in bold face) is annotated. These
annotations may be spread across the text.
The source documents were manually selected
and prepared by our industrial partners, and two
of their employees annotated the plain document
text. With respect to the statement annotations,
a preceding pilot study yielded substantial inter-
annotator agreement, as indicated by the value
? = 0.79 of the conservative measure Cohen?s
Kappa (Carletta, 1996). Additionally, we per-
formed a manual correction process for each an-
notated document to improve consistency.
5 Experiments
We now present experiments for the statement
identification, which were conducted on our cor-
pus. The goal was to evaluate whether our com-
bined extraction and classification approach suc-
ceeds in the precise identification of sentences that
comprise a statement on revenue, while keeping
recall high. Only exact matches of the annotated
text spans were considered to be correct identifi-
cations. Unlike in Section 3, we only worked on
plain text, though.
5.1 Experimental Setup
To find candidate sentences, we implemented a
sentence splitter that can handle article elements
such as subheadings, URLs, or bracketed sen-
tences. We then constructed sophisticated, but
efficient regular expressions for time and money.
They do not represent correct language, in gen-
eral, but model the structure of temporal and mon-
etary entities, and use word lists provided by do-
main experts on the lowest level.4 For feature
computation, we assumed that the closest pair of
temporal and monetary entity refers to the enclos-
ing candidate sentence.5 Since only positive in-
stances IP of statements on revenue are annotated
in our corpus, we declared all candidates, which
have no counterpart in the annotated data, to con-
stitute the negative class IN , and balanced IP and
IN by ?randomly? (seed 42) removing instances
from IN .6
For the vocabularies Lpos = {P1, P2} we first
counted the frequencies of all words in the unbal-
anced sets IP and IN . From these, we deleted
named entities, numbers and adjectives. If the pre-
fix (e.g. ?Umsatz?) of a word (?Umsatzplus?) oc-
curred, we only kept the prefix. We then filtered
all terms that appeared in at least 1.25% of the in-
stances in IP and more than 3.5 times as much in
IP as in IN . The remaining words were manually
partitioned into two lists:
P1 = {umgesetzt, Umsatz, Umsa?tze, setzte} (all
of these are terms for revenue)
P2 = {Billionen, meldet, Mitarbeiter, Verband}
(trillions, announce, employee, association)
Lneg = {N1, N2} was built accordingly. In ad-
dition, we set up a list G1 with genitive pronouns
4More details are given at http://infexba.upb.de.
555% of the candidate sentences in the training set con-
tain more than one temporal and/or monetary entity, so this
assumption may lead to errors.
6We both tested undersampling and oversampling tech-
niques but saw no effective differences in the results.
1133
and determiners. Based on Lpos, Lneg and G1,
we computed the following 43 features for every
candidate sentence s:
? 1-8: Number of terms from P1 (N1) in s as
well as in the two preceding sentences and in
the following sentence.
? 9-10: Number of terms from P2 (N2) in s.
? 11: Occurrence of term from G1 next to the
monetary entity.
? 12-19: Forward (backward) distance in to-
kens between the monetary (temporal) entity
in s and a term from P1 (N1).
? 20-27: Forward (backward) distance in num-
ber of symbols from O1 = {?.?,???,?!?} be-
tween the monetary (temporal) entity in s
and a term from P1 (N1).
? 28-43: Same as 20-27 for O2 = {?:?,?;?} and
O3 = {?,?}, respectively.
We trained a linear SVM with cost parameter
C = 0.3 (selected during validation) on these fea-
tures using the Weka integration of LibSVM (Hall
et. al., 2009; Fan et. al., 2001). Further features
were evaluated, e.g. occurrences of contraposi-
tions or comparisons, but they did not improve the
classifier. Instead, we noticed that we can avoid
some complex cases when we apply two rules af-
ter entity extraction:
R1: Delete temporal and monetary entities that
are directly surrounded by brackets.
R2: Delete temporal entities that contain the
word ?Vorjahr? (?preceding year?).
Now, we evaluated the following five statement
identification algorithms:
? Na??ve: Simply return all candidate sentences
(to estimate the relative frequency of state-
ments on revenue in the corpus).
? Baseline: Return all candidate sentences that
contain a term from the list P1.
? NEG: Use the results from Baseline. Return
all sentences that lack terms from N1.
Recall Training Validation Test
Sentences 0.98 0.98 0.96
Temporal entities 0.97 (0.95) 0.97 (0.94) 0.98 (0.96)
Monetary entities 0.96 (0.96) 0.96 (0.96) 0.95 (0.94)
Table 3: Recall of sentence and entity extraction.
In brackets: Recall after applying R1 and R2.
? RB: Filter candidates using R1 and R2. Then
apply NEG.
? SVM: Filter candidates using R1 and R2.
Then classify sentences with the SVM.
5.2 Results
Table 3 shows that we found at least 95% of the
sentences, time and money information, which re-
fer to a statement on revenue, in all datasets.7 We
could not measure precision for these since not all
sentences and entities are annotated in the corpus,
as mentioned in Section 4.
Results for the statement identification are
given in Figure 4. Generally, the test values are
somewhat lower than the validation values, but
analog in distribution. Nearly all statements were
recognized by the Na??ve algorithm, but only with
a precision of 0.35. In contrast, both for Baseline
and NEG already around 80% of the found state-
ments were correct. The latter paid a small gain in
precision with a significant loss in recall. While
RB and SVM both achieved 86% precision on the
test set, SVM tends to be a little more precise as
suggested by the validation results. In terms of re-
call, SVM clearly outperformed RB with values
of 89% and 87% and was only a little worse than
the Baseline. Altogether, the F1-Measure values
show that SVM was the best performing algorithm
in our evaluation.
5.3 Error Analysis
To assess the influence of the sentence, time and
money extraction, we compared precision and re-
call of the classifier on the manually annotated and
the extracted data, respectively. Table 4 shows
7We intentionally did not search for unusual entities like
?am 1. Handelstag nach dem Erntedankfest? (?the 1st trading
day after Thanksgiving?) in order not to develop techniques
that are tailored to individual cases. Also, money amounts
that lack a currency term were not recognized.
1134
0,75
0,80
0,85
0,90
0,95
1,00
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
Recall
.89
.87
.83.83
0,3
0,4
0,5
0,6
0,7
0,8
0,9
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
Precision
.89 .90
.86 .86
0,5
0,6
0,7
0,8
0,9
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
F1-Measure
.89
.86
.84
.86
.79 .77
.92
.89
.85
.83
Figure 4: Precision, recall and F1-Measure of the five evaluated statement identification algorithms.
SVM is best in precision both on validation and test data and outperforms RB in recall significantly.
that only recall differs significantly. We found that
false statement identifications referred to the fol-
lowing noteworthy error cases.
False match: Most false positives result from
matchings of temporal and monetary entities that
actually do not refer to the same statement.
Missing criterion: Some texts describe the de-
velopment of revenue without ever mentioning
revenue. Surrogate words like ?market? may be
used, but they are not discriminative enough.
Multiple criteria: Though we aimed at dis-
carding sentences, in which revenue is mentioned
without comprising a statement on it, in some
cases our features did not work out, mainly due
to intricate sentence structure.
Traps: Some sentences contain numeric values
on revenue, but not the ones looked for, as in ?10%
of the revenue?. We tackled these cases, but had
still some false classifications left.
Hidden boundaries: Finally, we did not find
all correct sentence boundaries, which can lead to
both false positives and false negatives. The pre-
dominant problem was to separate headlines from
paragraph beginnings and is partly caused by the
missing access to markup tags.
5.4 Efficiency
We ran the identification algorithm on the whole
corpus using a 2 GHz Intel Core 2 Duo MacBook
with 4 GB RAM. The 1,128 corpus documents
contain 33,370 sentences as counted by our algo-
rithm itself. Tokenization, sentence splitting, time
and money extraction took only 55.2 seconds, i.e.,
more than 20 documents or 600 sentences each
second. Since our feature computation is not op-
timized yet, the complete identification process is
a little less efficient with 7.35 documents or 218
Candidates Data Precision Recall
Annotated validation data 0.91 0.94
test data 0.87 0.93
Extracted validation data 0.90 0.89
test data 0.86 0.87
Table 4: Precision and recall of the statement
identification on manually annotated data and on
automatically extracted data, respectively.
sentences per second. However, it is fast enough
to be used in online applications, which was our
goal in the end.
6 Conclusion
We presented a multi-stage approach for the au-
tomatic identification and aggregation of market
statements and introduced a manually annotated
German corpus for related tasks. The approach
has been influenced by industry and is oriented
towards practical applications, but is, in general,
not specific to the German language. It relies on
efficient retrieval, extraction and NLP techniques.
By now, we can precisely identify most sentences
that represent statements on revenue. This already
allows for the support of strategists, e.g. by high-
lighting such sentences in web pages, which we
currently implement as a Firefox extension. The
overall problem is complex, though, and we are
aware that human experts can do better at present.
Nevertheless, time-consuming tasks can be auto-
mated and, in this respect, the results on our cor-
pus are very promising.
Acknowledgement: This work was funded by
the project ?InfexBA? of the German Federal Min-
istry of Education and Research (BMBF) under
contract number 01IS08007A.
1135
References
Ahn, David, Sisay F. Adafre, and Maarten de Rijke.
2005. Extracting Temporal Information from Open
Domain Text: A Comparative Exploration. Journal
of Digital Information Management, 3(1): 14?20.
Berekoven, Ludwig, Werner Eckert, and Peter El-
lenrieder. 2001. Marktforschung: Methodische
Grundlagen und praktische Anwendung, 9th Edi-
tion, Gabler, Wiesbaden, Germany.
Carletta, Jean. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22: 249?254.
Cramer, Irene M., Stefan Schacht, and Andreas
Merkel. 2007. Classifying Number Expressions in
German Corpora. In Proceedings of the 31st An-
nual Conference of the German Classification Soci-
ety on Data Analysis, Machine Learning, and Appli-
cations, pages 553?560.
Fan, Rong-En, Pai-Hsuen Chen, and Chih-Jen Lin.
2001. Working Set Selection Using Second Order
Information for Training Support Vector Machines.
Journal of Machine Learning Research, 6: 1889?
1918.
Ferrucci, David and Adam Lally. 2004. UIMA:
An Architectural Approach to Unstructured Infor-
mation Processing in the Corporate Research Envi-
ronment. Natural Language Engineering, 10(3?4):
pages 327?348.
Glance, Natalie, Matthew Hurst, Kamal Nigam,
Matthew Siegler, Robert Stockton, and Takashi
Tomokiyo. 2005. Deriving Marketing Intelligence
from Online Discussion. In Proceedings of the
Eleventh International Conference on Knowledge
Discovery in Data Mining, pages 419?428.
Gottron, Thomas. 2007. Evaluating Content Extrac-
tion on HTML Documents. In Proceedings of the
2nd International Conference on Internet Technolo-
gies and Applications, pages 123?132.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and
Yuji Matsumoto. 2002. Extracting Important Sen-
tences with Support Vector Machines. In Proceed-
ings of the 19th International Conference on Com-
putational linguistics, pages 342?348.
Koppel, Moshe and Itai Shtrimberg. 2004. Good
News or Bad News? Let the Market Decide. In Pro-
ceedings of the AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Appli-
cations, pages 86?88.
Lavrenko, Victor, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of Concurrent Text and Time Series. In
Proceedings of the 6th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining Workshop on Text Mining, pages 37?
44.
Lerman, Kevin, Ari Gilder, Mark Dredze, and Fer-
nando Pereira. 2008. Reading the Markets: Fore-
casting Public Opinion of Political Candidates by
News Analysis. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 473?480.
Liu, Yang, Xiangji Huang, Aijun An, and Xiaohui Yu.
2007. Arsa: A Sentiment-Aware Model for Predict-
ing Sales Performance Using Blogs. In Proceedings
of the 30th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 607?614.
Mani, Inderjeet and George Wilson. 2000. Ro-
bust Temporal Processing of News. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 69?76.
Ratinov, Lev and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 147?155.
Saquete, Estela, Rafael Mun?oz, and Patricio Mart??nez-
Barco. 2003. TERSEO: Temporal Expression Res-
olution System Applied to Event Ordering. Text,
Speech and Dialogue, Springer, Berlin / Heidelberg,
Germany, pages 220?228.
Stein, Benno, Sven Meyer zu Eissen, Gernot Gra?fe,
and Frank Wissbrock. 2005. Automating Market
Forecast Summarization from Internet Data. Fourth
International Conference on WWW/Internet, pages
395?402.
Steinwart, Ingo and Andreas Christmann. 2008. Sup-
port Vector Machines, Springer, New York, NY.
Xu, Jinxi and Bruce W. Croft 2000. Improving the ef-
fectiveness of information retrieval with local con-
text analysis. ACM Transactions on Information
Systems, 18(1): 79-112.
1136
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 553?564, Dublin, Ireland, August 23-29 2014.
Modeling Review Argumentation for Robust Sentiment Analysis
Henning Wachsmuth
Universit?at Paderborn
s-lab ? Software Quality Lab
Paderborn, Germany
henningw@upb.de
Martin Trenkmann, Benno Stein
Bauhaus-Universit?at Weimar
Webis Group
Weimar, Germany
<1st>.<last>@uni-weimar.de
Gregor Engels
Universit?at Paderborn
s-lab ? Software Quality Lab
Paderborn, Germany
engels@upb.de
Abstract
Most text classification approaches model text at the lexical and syntactic level only, lacking do-
main robustness and explainability. In tasks like sentiment analysis, such approaches can result in
limited effectiveness if the texts to be classified consist of a series of arguments. In this paper, we
claim that even a shallow model of the argumentation of a text allows for an effective and more
robust classification, while providing intuitive explanations of the classification results. Here, we
apply this idea to the supervised prediction of sentiment scores for reviews. We combine existing
approaches from sentiment analysis with novel features that compare the overall argumentation
structure of the given review text to a learned set of common sentiment flow patterns. Our evalu-
ation in two domains demonstrates the benefit of modeling argumentation for text classification
in terms of effectiveness and robustness.
1 Introduction
Text classification is a key technique in natural language processing and information retrieval that is ap-
plied for several tasks. Standard classification approaches map a text to a vector of lexical and shallow
syntactic surface-level features, from which class information is inferred using supervised learning (Man-
ning et al., 2008). Even though the results of such approaches can hardly be explained, they have proven
effective for narrow-domain texts with explicit class information (Joachims, 2001; Pang et al., 2002).
However, surface-level features often do not help to classify out-of-domain texts correctly, because
they tend to model the domain of the texts and not the classes to be inferred, as we observe in (Wachsmuth
and Bujna, 2011) among others. Moreover, they are likely to fail on texts where the class information
is implicitly represented by the argumentation of the writer. Such texts are in the focus of popular tasks
like authorship attribution, automatic essay grading, and, above all, sentiment analysis. As an example,
consider the short hotel review at the top and bottom of Figure 1. It contains more positive than negative
statements. Hence, a surface-level analysis would probably classify the review to have a positive overall
sentiment polarity. In fact, the argumentation of the review text reveals a clear negative sentiment.
The analysis of argumentation is recently getting more attention (cf. Section 2 for details). With respect
to sentiment, related approaches analyze discourse relations (Mukherjee and Bhattacharyya, 2012), iden-
tify the different aspects mentioned in a text (Lazaridou et al., 2013), or the like. While these approaches
can infer implicit class information from argumentative texts like reviews, they do not address the domain
dependency problem of sentiment analysis (Wu et al., 2010). In addition, they still lack explainability,
which limits end user acceptance in case of wrong results (Lim and Dey, 2009).
In this paper, we consider the question of how to capture the argumentation of reviews for a domain-
robust and explainable text classification. As Figure 1 illustrates, we rely on a shallow model of review
argumentation, which represents a text as a sequence of statements that express local sentiment on do-
main concepts and that are connected by discourse relations. We claim that, by focusing on features that
model the abstract argumentation structure of a text, a more robust sentiment analysis can be achieved.
At the same time, such an analysis can explain its results based on the underlying model.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
553
4statement
positive
negative
objective
We spent one night at that hotel.  Staff at the front desk was very nice,  the room was clean and cozy,
and the hotel lies in the city center...  but all this never justifies the price, which is outrageous!
background elaboration elaboration contrast
5
321
Figure 1: Illustration of our shallow model of review argumentation for a sample review text from the ho-
tel domain. Domain concepts, such as ?front desk?, are marked in bold. Each circle denotes a statement
with local sentiment. The statements are connected by directed discourse relations like ?elaboration?.
Concretely, here we address the supervised prediction of sentiment scores. To this end, we combine
a number of existing argumentation-related features with a novel approach that learns common patterns
in sequences of local sentiment through a cluster analysis in order to capture a review?s overall argu-
mentation structure. Inspired by explicit semantic analysis (Gabrilovich and Markovitch, 2007), we then
compute the similarity of a given review text to each of these sentiment flow patterns and we use these
similarities as features for sentiment scoring. To explain a predicted score and, hence, to increase user
acceptance, both the underlying model and the sentiment flow patterns can be visualized.
We evaluate our approach on reviews of the hotel domain and the movie domain. In comparison to
standard baselines, we demonstrate the effectiveness and robustness of modeling argumentation. Our
results suggest that especially the sentiment flow patterns learned in one domain generalize well to other
domains. Altogether, the contributions of this paper are:
1. A shallow model of review argumentation for text classification that enables a more domain-robust
and explainable sentiment analysis (Section 3).
2. A novel feature type named sentiment flow patterns that, for the first time, captures the abstract
overall argumentation structure of review texts, irrespective of their domain (Section 4).
3. Experimental evidence for the existence of common patterns in the argumentation structure of re-
view texts across domains (Section 5).
2 Related Work
Argumentation plays a key role in human communication and cognition. Its purpose is to provide persua-
sive information for or against a decision or claim. This involves the identification of facts and warrants
justified by a backing or countered by a rebuttal (Toulmin, 1958). Argumentation is studied in various
disciplines, such as logic, philosophy, and artificial intelligence. We consider the linguistics perspective,
where it is pragmatically viewed as a regulated sequence of speech or text (Walton and Godden, 2006).
In particular, we analyze monological argumentations in written text as opposed to dialogical argu-
mentations where participants persuade each other with arguments (Cabrio and Villata, 2012). In terms
of text, one of the most obvious forms of monological argumentation can be found in reviews. A review
comprises a positional argumentation, where an author collates and structures a choice of facts, pros,
and cons in order to inform intended recipients about his or her beliefs (Besnard and Hunter, 2008).
According to Mochales and Moens (2011), an argumentation analysis targets at ?the content of serial
arguments, their linguistic structure, the relationship between the preceding and following arguments,
recognizing the underlying conceptual beliefs, and understanding within the comprehensive coherence of
the specific topic.? The authors work on argumentation mining, i.e., the detection of different arguments
for justifying a conclusion as well as their interactions. Our model of argumentation matches the quoted
definition. Similar to the distinction between shallow and deep parsing (Jurafsky and Martin, 2009),
our approach can be seen as a shallow argumentation analysis in that we consider only the sequence of
arguments. This abstraction appears very promising to address text classification.
Unlike argumentative zoning (Teufel et al., 2009), which classifies segments of scientific articles ac-
cording to argumentative functions, we predict the sentiment scores of reviews from a sequence of clas-
sified segments. Sentiment scoring is tackled in both computational linguistics (Pang and Lee, 2005) and
554
information retrieval (Wang et al., 2010). Such kind of sentiment analysis benefits from modeling argu-
mentative discourse (Villalba and Saint-Dizier, 2012). Related works already employ discourse features
to detect sentiment polarity. Some rely on complex discourse parsing (Heerschop et al., 2011), whereas
others argue that a lightweight approach is more robust for noisy texts (Mukherjee and Bhattacharyya,
2012). We rather follow the latter, but we see discourse only as one part of review argumentation.
In accordance with Lazaridou et al. (2013) who address aspect-based sentiment analysis, we addition-
ally analyze the connection of local sentiment to domain concepts and discourse relations. Even more
important for us is the local sentiment flow in a text. This term was introduced by Mao and Lebanon
(2007), who infer a text?s global sentiment from its sequence of local (sentence) sentiments, classified
with conditional random fields. Their approach converts each sentiment in the sequence to a single fea-
ture and learns a mapping from the features to global sentiment. By that, it actually disregards the order-
ing of local sentiment. In contrast, our sentiment flow patterns measure the similarity between complete
sequences of local sentiment. This resembles explicit semantic analysis (Gabrilovich and Markovitch,
2007), which classifies texts based on their relatedness to concepts modeled by complete texts.
In (Wachsmuth et al., 2014), we reveal correlations between a review?s sentiment score and its local
sentiment flow. Similar to Socher et al. (2013), we therefore argue that global sentiment emanates from
the composition of local sentiment. The authors model the semantic compositionality of words in given
sentences, thus capturing the language of a given domain. Conversely, our sentiment flow patterns focus
on the structure of complete texts in order to reduce domain dependency, which is a general problem
in text classification (Wu et al., 2010). Among others, existing strategies to tackle this problem align
features of the source and the target domain, as we do in (Prettenhofer and Stein, 2010).
Given a vector of features, text classification approaches typically output only a class label (Manning
et al., 2008). This renders the understanding and debugging of classification results hard (Kulesza et al.,
2011). Instead, our approach explains results by making the argumentation of texts visible. Thereby, we
increase intelligibility and, thus, support user acceptance (Lim and Dey, 2009).
3 A Shallow Model of Review Argumentation
This section first sketches our general hypothesis. Then, we present our model of review argumentation.
3.1 Hypothesis behind Modeling Argumentation for Text Classification
Several text classification tasks relate to the argumentation of a text. As an obvious example, automated
essay scoring explicitly rates argumentative texts, mostly targeting at structural aspects (Dikli, 2006). In
genre identification, a central concept is the form of texts. Some genre-related tasks address argumenta-
tion, e.g. by classifying texts according to their function (Wachsmuth and Bujna, 2011). Criteria in text
quality assessment often measure structure (Anderka et al., 2012), while readability is connected to dis-
course (Pitler and Nenkova, 2008). Authorship attribution profits from argumentation clues like uncon-
sciously used function words (Stamatatos, 2009), and plagiarism detection, in the end, aims to check if
the argumentation in a fragment of a text refers to the author of the text (Potthast et al., 2013).
We hypothesize that in these and further tasks the class of a text is often decided by the structure of
its argumentation rather than by its content, while the content adapts the argumentation to the domain at
hand. Following Besnard and Hunter (2008), an argumentation consists of a composition of arguments
used to justify a decision or claim. Each argument can be seen as a statement with some evidence. Under
our hypothesis, an explicit model of statements and their composition hence supports the identification of
domain-independent patterns. Together with the content, the statements enable a fine-grained analysis,
while serving as the basis for an explanation. Since the relevant types of statements vary among tasks, we
argue that such a model should be task-specific. Below, we investigate reviews on products and services
from a sentiment analysis perspective. Because of its positional nature (cf. Section 2), review argumen-
tation makes its arguments explicit, i.e., facts and opinions on different product features and aspects.
3.2 Modeling Review Argumentation for Sentiment Analysis
We consider reviews that comprise a text about some product or service as well as a numerical overall
rating. Any other metadata that might be given for reviews is ignored in the following. Our assumption
555
x1 x i xn
C(x i) C'(x i)
R(x i, x j)
local sentiment
domain concepts
discourse relations
...
x j
......
...... ... ...
... ...
S(x1) S(x i) S(xn)S(x j)... ......
statements
Figure 2: Our shallow model of review argumen-
tation defined by a segmentation into statements
and by three functions based on the statements.
local 
sentiment
sentiment flow
patterns
discourse
relations
domain 
concepts
baseline distributions
word & POS n-grams, character trigrams, lengths, SentiWordNet scores
(
)
, , , ,
a1 a2 a3 a4
b1
baseline features
argumentation features
Figure 3: A vector with all five considered feature
types including the novel sentiment flow patterns.
Each found pattern becomes a single feature in a4.
is that the overall rating denotes a sentiment score y from a metric sentiment scale that quantifies the
possibly implicit conclusion of the review text in terms of its global sentiment.
Statements To capture a review?s argumentation, we model the review?s text as a sequence of n > 0
statements x
1
, . . . , x
n
. Here, we define a statement x syntactically to be a main clause together with
all its subordinate clauses. The notion behind is that, in our experience, such a text segment is usually
meaningful on its own while bearing at most one sentiment. Many sentences in reviews comprise series
of statements. For instance, the following excerpt from Figure 1 consists of two statements, x
4
and x
5
:
x
4
: and the hotel lies in the city center... x
5
: but all this never justifies the price, which is outrageous!
Based on the set of all statements X, we capture the structure and content of review texts as follows:
Local Sentiment We assume each statement to represent either an objective fact obj, a positive opin-
ion pos, or a negative opinion neg (for a wide applicability, we ignore sentiment intensity). So, there is
an unknown function that maps each statement to a local sentiment, e.g. x
4
to obj and x
5
to neg:
local sentiment : X ?
{
S(x) | S ? {pos, neg, obj}
}
Discourse Relations As for x
4
and x
5
, the composition of statements in a text is, in general, not co-
incidental. Rather, it implies a structure made up of an ordered choice of statements as well as of a
number of directed discourse relations. We define a discourse relation to have some type R of a set of
relation typesR and to relate two (typically neighboring) statements, e.g. contrast(x
5
, x
4
) in the example
above. The following function hence can be understood as a shallow version of the rhetorical structure
theory (Mann and Thompson, 1988):
discourse relations : X ? {R(x
i
, x
j
) | 1 ? i, j ? n ; R ? R}
Domain Concepts The argumentation structure of a text is bound to the domain at hand through the
text?s content. In particular, a review text discusses a subset of the domain conceptsC that are associated
to a product or service, each being referred to in one or more statements. For instance, x
5
discusses the
price of the hotel, i.e., price(x
5
). We capture the domain concepts in statements as follows:
domain concepts : X ? {C(x
i
) | 1 ? i ? n ; C ? C}
Altogether, our model represents a review text as a sequence of interrelated statements of certain types
and content. Figure 2 illustrates the defined functions. An instance of the model is visualized in Figure 1.
The model is an abstraction of argumentation, covering some information only implicitly if at all (e.g.
lexical or syntactic clues). However, it can be extended by further information, as we do below.
4 Features for Robust Sentiment Analysis and Explanation
We now present different types of features for supervised learning that capture both distributional and
structural aspects of review argumentation based on our shallow model. Here, we assume that all infor-
mation represented in the model is given, but Section 5 analyzes the effects of inferring the information
from a text. Figure 3 gives an overview of the vector with all feature types that we consider, including a
common set of baseline features (b1). The goal of all argumentation features (a1?a4) is twofold: (1) To
enable an effective and robust sentiment analysis. (2) To provide means to explain analysis results.
556
1 161 5
positive (1.0)
negative (0.0)
objective (0.5)
Figure 4: Illustration of a length-normalized ver-
sion (small circles) of the sample local sentiment
flow from Figure 1 (big circles) for length 16.
positive (1.0)
negative (0.0)
objective (0.5)
Figure 5: Sketch of the construction of a sentiment
flow pattern (dashed curve), here from two sample
local sentiment flows (circles and squares).
4.1 Quantification of Distributional Argumentation Aspects
In terms of the distributional aspects of the three functions introduced in Section 3, we combine a selec-
tion of ideas from existing sentiment analysis approaches that are related to review argumentation. Some
features of the types described in the following are selected only if they occur frequently in a given set of
training texts. Thus, the concrete numbers of features vary, as we see in the evaluation in Section 5.
Local Sentiment (a1) In (Wachsmuth et al., 2014), we stress the impact of the distribution of local sen-
timent. Accordingly, here we determine the frequencies of all types of local sentiment in the given text as
well as of series of statements with the same type and of changes from one type to another. Also, we have
features that denote the local sentiment at specific positions like the first and last two statements, and we
compute the average local sentiment. For the latter, we map pos to 1.0, obj to 0.5, and neg to 0.0.
In addition, we follow Mao and Lebanon (2007) in that we capture the local sentiment flow based on
the defined mapping. To preserve the original flows as far as possible, we length-normalize the sequence
of values using non-linear interpolation with subsequent sampling. Figure 4 shows an example.
Discourse Relations (a2) We count the occurrences of different discourse relation types from (Mann
and Thompson, 1988), e.g. contrast or elaboration (in Section 5, we distinguish a subset of ten types). To
model connections between sentiment and discourse, we do the same for all frequently occurring combi-
nations of discourse relation types and local sentiment of the related statements, e.g. contrast(pos, neg)
or contrast(neg, pos). By that, we imitate Lazaridou et al. (2013) to some extent.
Domain Concepts (a3) With the same intention, we determine the most frequent domain concepts in the
given training set and we compute how often each concept cooccurs with each type of local sentiment.
Examples from the sample text in Figure 1 are hotel(obj) or price(neg). Moreover, we count the num-
ber of different domain concepts as well as the instances of all possibly distinguished types of domain
concepts, which would be product (like ?hotel?) and product feature (like ?price?) in the given case.
Types a1?a3 refer to important characteristics of review argumentation. However, none of them cap-
tures a review?s overall argumentation structure. Even the local sentiment flow in a1 rather measures the
impact of local sentiment at different positions. The reason behind is that the flow positions are repre-
sented by individual features. Hence, common learning approaches like regression will naturally tend to
assign positive weights to all positions, not considering the sentiment flow as a whole.
4.2 Learning of Structural Argumentation Aspects
To capture the impact of the structure of an argumentation, we introduce a novel feature type based on the
local sentiment flows of texts only. The idea behind resembles explicit semantic analysis (Gabrilovich
and Markovitch, 2007) in that every single feature represents the similarity to a complete flow:
Sentiment Flow Patterns (a4) We first construct a set of common sentiment flow patterns from a set
of known training review texts, where each pattern denotes the average of a set of similar local senti-
ment flows of normalized length. Given an unknown review text, we then measure the similarity of its
normalized local sentiment flow to each constructed pattern. The set of these similarities forms a4.
Figure 5 exemplifies the pattern construction. Our hypothesis behind sentiment flow patterns is that
similar local sentiment flows entail similar sentiment scores. Accordingly, flows that construct a pattern
should be as similar as possible and flows of different patterns as dissimilar as possible. Therefore, we ap-
ply clustering (Manning et al., 2008) to partition the flows of all texts from the given training set based
on some flow similarity function (in Section 5, we use the manhattan distance). The centroid of each ob-
557
1 1 1 3 1 3 3 3 2 3 4 4 3 3 5 54 5 5 4
highest cuts 
with relaxed
purity ? 0.8
relaxed purity
values at cuts
0.8 0.889 1.0 1.0
flow clusters
at cuts
(a)
1.0
0.5
0.00.0 0.5 1.0 x1
x2
1
1
1
1
3
55
554
43
3
3
3 4
4
3
3
2
(b) sentiment flow patterns,i.e., the centroids of the 
flow clusters
local sentiment flow 
to be classified
manhattan distances to
sentiment flow patterns
?
Figure 6: (a) A purity threshold of 0.8 derives four
clusters from a hierarchical clustering of 20 local
sentiment flows represented by their scores from 1
to 5. (b) 2D plot of computing distances to the sen-
timent flow patterns, i.e., the clusters? centroids.
(b) positive
negative
objective
common pattern
of score 3-4
score 2.69
con-
trast
elabo-
ration
elabo-
ration
back-
ground
We spent one night at that hotel ... never justifies the price, which is outrageous!
We spent 
one night at 
that hotel.
Staff at the 
front desk was 
very nice,
the room 
looked clean 
and cozy,
and the hotel 
lies in the city 
center...
but all this never
justifies the price,
which is outrageous!
objective positive objectivepositive negative
score 2.69
product feature feature feature featureproduct
hotel hotelstaff front desk priceroom
(a)
common pattern
of score 2-3
Figure 7: Two possible explanations of scoring the
sample text from Figure 1: (a) Graph visualization
of our model of review argumentation. (b) Com-
parison of the local sentiment flow of the text with
the two most similar sentiment flow patterns.
tained cluster ? then becomes a sentiment flow pattern. Since we know the sentiment scores associated to
the flows in the training set, we can measure the purity of a cluster ?, which here denotes the fraction of
those flows ?
y
?
in ? whose score equals the majority score y
?
in ? (Manning et al., 2008). The original
purity definition, however, assumes exactly one correct score for each flow. Here, this would mean that
a flow alone decides a score. Instead, for larger sentiment scales, we propose to relax the purity measure
by assuming also the dominant neighbor of the majority score as correct:
relaxed purity(?) =
(
|?
y
?
|+ max(|?
y
?
?1
|,|?
y
?
+1
|)
) /
|?|
We seek for clusters with a high purity, because such clusters support that similarities between flows
and patterns indicate specific sentiment scores. At the same time, the number of clusters should be small
in order to achieve a high average cluster size and, thus, a high commonness of the patterns. For this
purpose, we rely on hierarchical clustering, where we can easily find a flat clustering with a certain
number of clusters through cuts at appropriate nodes in the binary tree of the associated hierarchy. Pattern
construction profits from compact clusters, suggesting to compute distances between clusters from their
group-average links (Manning et al., 2008). To minimize the number of clusters, we search for all nodes
closest to the tree?s root that represent clusters with a purity above some threshold, e.g. 0.8 in the example
in Figure 6(a). The centroids of these clusters become sentiment flow patterns, if they are made up of
some minimum number of flows.
At the end of the clustering process, we remain with one feature for each constructed sentiment flow
pattern. Given a review text to be classified, we then compute its normalized local sentiment flow and we
measure the flow?s distance to all patterns. Each distance represents one similarity in the feature type a4.
Figure 6(b) sketches the computation of the distances, mapped into two dimensions.
4.3 Comparison with Baseline Approaches
In the evaluation below, we compare the feature types a1 to a4 with the well-known sentiment scoring
approach of Pang and Lee (2005) in terms of effectiveness. Our focus, however, is the robustness of
modeling argumentation structure in contrast to standard text classification features employed in many
other approaches, such as n-grams or variations of them (Qu et al., 2010). To this end, we also integrate
the following baseline features, most of which model a text at the lexical and syntactic level:
Baseline Distributions (b1) We compute the distributions of all word and part-of-speech unigrams,
bigrams, and trigrams as well as of all character trigrams that frequently occur in a given training set.
In addition, we determine the length of the given text in different units and some average SentiWordNet
scores with respect to both the first and the average senses of its words (Baccianella et al., 2010).
558
4.4 Explanation of Sentiment Scores
We propose a shallow statistical argumentation analysis that learns to predict sentiment scores on a train-
ing set of review texts. After prediction, we can directly exploit the information captured in our model
as well as the values of the feature types a1?a4 in order to explain the predicted score. Two possible
explanations are visualized in Figure 7, while a combination of them is exemplified in Figure 1. We be-
lieve that such explanations can increase a user?s confidence in statistical analysis results and, hence, the
acceptance of corresponding applications. To demonstrate the analysis and explanation of review argu-
mentation, we provide a free-to-use tool and webservice at http://www.arguana.com.
5 Evaluation of Modeling Argumentation for Sentiment Scoring
In this section, we evaluate the effectiveness and domain robustness of modeling argumentation for
sentiment scoring with a focus on the sentiment flow patterns. The source code of the evaluation can be
found at http://www.arguana.com. Our experiments are based on two English text corpora with reviews
from the hotel domain and the movie domain, respectively. In both cases, we leave out the review titles
for generality, because our approach targets at arbitrary reviews including those without a title.
Text Corpora On the one hand, we process the ArguAna TripAdvisor corpus that we have introduced
in (Wachsmuth et al., 2014). This corpus compiles a collection of 2,100 reviews of hotels from seven
locations, balanced with respect to their sentiment scores between 1 and 5. All of the reviews? texts are
segmented into statements with an average of 14.8 statements per text. Each statement is annotated as an
objective fact, a positive, or a negative opinion. Moreover, all mentions of domain concepts are marked as
such. The corpus is also available at http://www.arguana.com, free for scientific use. In the experiments,
we rely on the provided corpus split with 900 reviews from three hotel locations in the training set, and
600 reviews from two locations in the validation set and test set each.
On the other hand, we use the Sentiment Scale dataset (Pang and Lee, 2005) consisting of 5,006 movie
reviews that are split into four text corpora according to their authors (Author a, b, c, and d). From these,
we have discarded eight reviews due to encoding problems. We choose the provided sentiment scale
from 0 to 2, so we can logically map the scale of the hotel reviews (1?5) to it for a domain transfer. In
particular, scores 1?2 are mapped to 0, 3 to 1, and 4?5 to 2. On average, the movie reviews are much
longer with 36.1 statements per text. Since no local sentiment annotations are given, we also process
the subjectivity dataset (Pang and Lee, 2004) and the sentence polarity dataset (Pang and Lee, 2005) in
order to develop classifiers for sentence sentiment. Accordingly, we assume each movie review sentence
to denote one statement. To directly compare our results to those of Pang and Lee (2005), we perform
10-fold cross-validation separately on the dataset of each single author, averaged over five runs.
Preprocessing For feature computations, we preprocess all texts with a tokenizer, a sentence splitter,
and the part-of-speech tagger from (Schmid, 1995). We employ lexicon-based extractors for discourse
relations and domain concepts, which aim at a high precision while not being able to recognize unseen
instances. The former resembles the lightweight approach of Mukherjee and Bhattacharyya (2012). Pri-
marily, it looks for conjunctions that indicate certain discourse relations, such as ?but? or ?because?. The
latter detects exactly those domain concepts that are annotated largely consistently in the training set of
the ArguAna TripAdvisor corpus. Thus, it helps only on the hotel reviews. These reviews are segmented
into statements with a respective algorithm that comes with the corpus.
For both domains, we have trained linear support vector machines (SVMs) from Chang and Lin (2011)
that classify the subjectivity of each statement (opinion or fact) and the polarity of each opinion (positive
or negative). They use 1k to 2k features of different types: word and part-of-speech unigrams, character
trigrams, SentiWordNet scores (Baccianella et al., 2010), and some special features like the first word of a
statement or its position in the text. On the test set of the hotel domain, the classifiers have an accuracy of
78.1% for subjectivity and of 80.4% for polarity. In the movie domain, we achieve a subjectivity accuracy
of 91.1%, but a polarity accuracy of only 73.8% (measured through 10-fold cross-validation).
Feature Computation We determine one distinct feature set for each evaluated text corpus made up
of the feature types presented in Section 4. Where necessary, we divide the computed feature values by
559
the length of the text (in tokens or statements, as appropriate), in order to ensure that all feature values
always lie between 0 and 1.
Local sentiment flows are normalized to length 30 in case of the hotel reviews and to length 60 in case
of the movie reviews, which allows us to represent most of the original flows without loss. Altogether,
feature type a1 sums up to 50 and 80 features, respectively. For a2, a3, and b1, we consider only those
features whose frequency in the training texts exceeds some specified threshold. For instance, a word
unigram is taken into account within b1 only if it occurs in at least 5% of the hotel reviews or 10% of
the movie reviews, respectively. As a result, the number of evaluated features varies depending on the
processed text corpus. Concretely, we obtain 64 to 78 features for discourse relations (a2), 78 to 114 for
domain concepts (a3), and 1026 to 2071 baseline features (b1). More details are given in the instruction
and configuration files that come with the provided source code.
To construct sentiment flow patterns (a4), we have developed an agglomerative hierarchical clusterer
that implements the approach from Section 4.2. After some tests with different settings, we decided to
measure flow and cluster similarity using group-average link clustering based on the manhattan distance
between the length-normalized local sentiment flows. For the hierarchy tree cuts, we use a purity thresh-
old of 0.8, where we take the relaxed purity for the sentiment scale 1?5 of the hotel reviews, but the
original purity for the movie reviews (because of the limited scale from 0 to 2). All centroids of clusters
with at least three flows become a sentiment flow pattern, resulting in 16 to 86 features in a4.
Sentiment Scoring On the hotel reviews, we compute the root mean squared error of linear sentiment
score regression trained using stochastic gradient descent (SGD) from Weka 3.7.5 (Hall et al., 2009).
Both the regularization parameter and the learning rate of SGD are set to 10
?5
, whereas we determine
the epochs parameter of SGD on the validation set. Then, we measure the error on the test set.
For the comparison to (Pang and Lee, 2005), we predict the scores of the movie reviews using classifi-
cation, which additionally stresses the domain change. In particular, we measure the accuracy of a linear
1-vs.-1 multi-class SVM with probability estimates and normalization. While we optimize the cost para-
meter of the SVMs in the in-domain task, we rely on the default value (1.0) for the domain transfer.
5.1 Effectiveness of Modeling Argumentation
First, we measure the theoretically possible scoring effectiveness of all feature types within one domain.
To this end, we compare the feature types based on the ground-truth annotations of the ArguAna Trip-
Advisor corpus. The column Corpus of Table 1 lists the resulting root mean squared errors. As can
be seen, all argumentation feature types clearly outperform the baseline distributions (b1) and improve
strongly over random guessing. The distributional local sentiment (a1) does best with an error of 0.77,
whereas the domain concepts perform worst among a1 to a4. Still, they result in an 0.12 lower root mean
squared error than the baseline distributions (b1). Overall, the lowest observed error is 0.75, achieved by
the SVM with all features as well as by two subsets of the argumentation features alone.
In practice, no ground-truth annotations are given, so we need to create annotations in the review texts
ourselves using the preprocessing described above. This in turn changes the feature set and the respective
values of the argumentation features. The third column of Table 1 (Self) shows that such a resort to self-
created annotations leads to a root mean squared error increase of 0.14 to 0.22 for the types a1 to a4.
Nevertheless, the argumentation features succeed over the baseline distributions with 0.94 as opposed
to 1.11, which demonstrates the effectiveness of modeling the argumentation of hotel reviews.
5.2 Robustness of Modeling Argumentation Structure
We hypothesize that the developed structure-based argumentation features are robust against domain
transfer to a wide extent. To investigate this, we classify sentiment scores using SVMs based either on
all or on one single feature type (except for a3, for lack of movie domain concept extractors) in two tasks
on the four movie datasets: (1) with training in the movie domain (through 10-fold cross-validation), and
(2) with training out-of-domain on the hotel review training set.
Figure 8 contrasts the accuracy results for the two tasks and compares them to the best SVM approach
of Pang and Lee (2005), i.e., ova (open squares). In the in-domain task, our SVM based on all feature
types (black squares) is significantly better than ova on one dataset (Author a) and a little worse on
560
Feature type Corpus Self
none Random guessing 1.41 1.41
a1 Local sentiment 0.77 0.99
a2 Discourse relations 0.84 1.01
a3 Domain concepts 0.99 1.13
a4 Sentiment flow patterns 0.86 1.07
b1 Baseline distributions 1.11 1.11
a1?a4 Argumentation features 0.76 0.94
a2, a3, a4 w/o local sentiment 0.79 0.99
a1, a3, a4 w/o discourse relations 0.76 0.97
a1, a2, a4 w/o domain concepts 0.75 0.95
a1, a2, a3 w/o sentiment flow patterns 0.75 0.95
all All features 0.75 0.93
Table 1: Root mean squared error of sentiment
score regression on the hotel review test set
for all evaluated features types and for different
combinations of these types. Features are com-
puted based on ground-truth annotations (Cor-
pus) or based on self-created annotations (Self).
70%
60%
50%
40%
30%
Author a Author cAuthor b Author d
 a
cc
ur
ac
y 
in
 m
ov
ie
 d
om
ai
n
68.2
57.4
72.0
58.3
45.5
42.7
50.7
31.0
a4
a2
a1
b1
a4
a2
a1
b1
a4
a2
a1
b1
a4
a2
a1
b1
in hotel domain
training
in movie domain
Figure 8: Sentiment scoring accuracy of the SVM
based on feature type a1, a2, a4, and b1 (black icons)
and of each SVM based on one of these types (or-
ange icons) on the four movie datasets, when trained
on movie reviews (squares) or on hotel reviews (cir-
cles). Open squares: ova from (Pang and Lee, 2005).
301
positive (1.0)
negative (0.0)
objective (0.5)
score 5-4 (based on 226 flows)score 2-3 (based on 24 flows)
score 1-2 (based on 155 flows)
1 60
1.0
0.0
0.5
(c) Sentiment Scale dataset, Author d (scale 0-2)(b) Sentiment Scale dataset, Author c (scale 0-2)
1.0
0.0
0.5
(a) ArguAna TripAdvisor corpus (scale 1-5)
1 60
statement
score 2
(155 flows)
score 0
(11 flows)
score 1
(7 flows)
score 2
(5 flows)
score 1
(15 flows)
score 0
(16 flows)
Figure 9: (a) The three most common sentiment flow patterns in the training set of the ArguAna TripAd-
visor corpus, labeled with their associated sentiment scores. (b?c) The according sentiment flow pattern
for each possible score of the texts of Author c and Author d in the Sentiment Scale dataset, respectively.
two other datasets (Author c and Author d). On all four datasets, a4 classifies sentiment scores more
accurately than both a1 and a2, but none of the argumentation feature types can compete with the baseline
distributions (b1). We suppose that the reason behind mainly lies in the limited effectiveness of our
opinion polarity classifier, which reduces the impact of all features that rely on statement sentiment.
Conversely, b1 fails completely in the out-of-domain task (from squares to circles) with accuracy drops
of up to 41% (on Author c). This indicates a large covariate shift (Shimodaira, 2000) in the distribution
of the baseline features. In contrast, a1, a2, and a4 suffer much less from the domain transfer. Especially
the accuracy of the sentiment flow patterns (a4) remains stable on three of the four datasets and, hence,
provides strong support for our hypothesis. In case of Author c, the SVM based on a4 alone even achieves
a significantly higher accuracy than the SVM based on all features (60.5% as opposed to 50.7%), thus
offering evidence for the decisiveness of the structure of an argumentation. Only on Author d, all four
evaluated feature types similarly fail when trained on hotel reviews with a4 being the worst. Apparently,
the argumentation structure in the texts of Author d differs from the others, which is reflected by the
found sentiment flow patterns and which we therefore finally analyze.
5.3 Insights into Sentiment Flow Patterns
In Figure 9, we plot the three most common sentiment flow patterns in the training set of the ArguAna
TripAdvisor corpus (with self-created annotations) as well as the respective patterns in the movie reviews
561
of Author c and Author d for each possible sentiment score. In total, we found 38 sentiment flow patterns
in the hotel reviews, meaning that a4 consists of 38 features in this case. As depicted in Figure 9(a), they
are constructed from the local sentiment flows of up to 226 texts. One of the 75 patterns of Author c
results from 155 flows, whereas each of the 41 patterns of Author d represents at most 16 flows.
With respect to the depicted sentiment flow patterns, the movie reviews show less clear sentiment but
more changes of local sentiment than the hotel reviews. While there appears to be a certain similarity
in the overall argumentation structure between the hotel reviews and the movie reviews of Author c, two
of the three patterns of Author d contain only little clear sentiment at all, especially in the middle parts.
The disparity of the Author d dataset is additionally emphasized by the different proportions of opinions
in the evaluated text corpora. In particular, 79.7% of all statements in the ArguAna TripAdvisor corpus
are opinions, but only 36.5% of the sentences of Author d are classified as subjective. The proportions
of the three other movie datasets at least range between 58.4% and 66.5%. These numbers also serve as
a general explanation for the limited accuracy of a1, a2, and a4 in the movie domain.
A solution to achieve higher accuracy and to further improve the domain robustness of the structure-
based argumentation features might be to construct flow patterns from the subjective statements or from
the changes of local sentiments only, which we leave for future work. Here, we conclude that our novel
feature type a4 does not yet solve the domain dependency problem, but it still defines a promising step
towards a more domain-robust sentiment analysis.
6 Conclusion
Text classification tasks like sentiment analysis are domain-dependent and tend to be hard on texts that
comprise an involved argumentation, such as reviews. To classify the sentiment scores of reviews, we
model a review?s text as a composition of local sentiment, discourse relations, and domain concepts.
Based on this shallow model of argumentation, we combine existing sentiment analysis approaches with
novel features that capture the abstract overall argumentation structure of reviews irrespective of their
domain and their linguistic style. In particular, we learn common sequences of local sentiment in re-
views through clustering in order to then compare a given review to each of these learned sentiment flow
patterns. Our evaluation on hotel and movie reviews suggests that the sentiment flow patterns generalize
well across domains and it indicates the effectiveness of modeling argumentation. In addition, both the
patterns and our model help to explain sentiment scoring results, as exemplified.
Due to errors in the preprocessing of texts, some obtained effectiveness gains are rather small, though.
In the future, we seek to develop features that are less affected from preprocessing. A promising variation
in this respect is e.g. to learn patterns based on the changes of local sentiment only. Also, we plan to
analyze common sequences of discourse relations in order to capture the argumentation structure of a
text in an even more domain- and language-independent manner. By that, we contribute to the general
research on robust and explainable text classification. As outlined in Section 3, many text classification
tasks can profit from modeling argumentation. For this purpose, other types of statements, relations, and
domain concepts will be needed as well as, in some cases, a deeper argumentation analysis.
Acknowledgments
This work was funded by the German Federal Ministry of Education and Research (BMBF) under con-
tract number 01IS11016A as part of the project ?ArguAna?, http://www.arguana.com.
References
Maik Anderka, Benno Stein, and Nedim Lipka. 2012. Predicting Quality Flaws in User-generated Content: The
Case of Wikipedia. In Proceedings of the 35th International ACM Conference on Research and Development
in Information Retrieval, pages 981?990.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical
Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh International Conference
on Language Resources and Evaluation, pages 2200?2204.
562
Philippe Besnard and Anthony Hunter. 2008. Elements of Argumentation. The MIT Press.
Elena Cabrio and Serena Villata. 2012. Combining Textual Entailment and Argumentation Theory for Supporting
Online Debates Interactions. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics: Short Papers, pages 208?212.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?27:27.
Semire Dikli. 2006. An Overview of Automated Scoring of Essays. Journal of Technology, Learning, and
Assessment, 5(1).
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence,
pages 1606?1611.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The
WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10?18.
Bas Heerschop, Frank Goossen, Alexander Hogenboom, Flavius Frasincar, Uzay Kaymak, and Franciska de Jong.
2011. Polarity Analysis of Texts Using Discourse Structure. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Management, pages 1061?1070.
Thorsten Joachims. 2001. A Statistical Learning Model of Text Classification for Support Vector Machines.
In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 128?136.
Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, 2nd edition.
Todd Kulesza, Simone Stumpf, Weng-Keen Wong, Margaret M. Burnett, Stephen Perona, Andrew Ko, and Ian
Oberst. 2011. Why-oriented End-user Debugging of Naive Bayes Text Classification. ACM Transactions on
Interactive Intelligent Systems, 1(1):2:1?2:31.
Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 2013. A Bayesian Model for Joint Unsupervised Induc-
tion of Sentiment, Aspect and Discourse Representations. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 1630?1639.
Brian Y. Lim and Anind K. Dey. 2009. Assessing Demand for Intelligibility in Context-aware Applications. In
Proceedings of the 11th International Conference on Ubiquitous Computing, pages 195?204.
William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of
Text Organization. Text, 8(3):243?281.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Yi Mao and Guy Lebanon. 2007. Isotonic Conditional Random Fields and Local Sentiment Flow. Advances in
Neural Information Processing Systems, 19:961?968.
Raquel Mochales and Marie-Francine Moens. 2011. Argumentation Mining. Artificial Intelligence and Law,
19(1):1?22.
Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Sentiment Analysis in Twitter with Lightweight Dis-
course Analysis. In Proceedings of the 24th International Conference on Computational Linguistics, pages
1847?1864.
Bo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity. In Proceedings
of 42th Annual Meeting of the Association for Computational Linguistics, pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with
Respect to Rating Scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics, pages 115?124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs Up?: Sentiment Classification Using Ma-
chine Learning Techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, pages 79?86.
563
Emily Pitler and Ani Nenkova. 2008. Revisiting Readability: A Unified Framework for Predicting Text Quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186?195.
Martin Potthast, Matthias Hagen, Michael V?olske, and Benno Stein. 2013. Crowdsourcing Interaction Logs
to Understand Text Reuse from the Web. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1212?1221.
Peter Prettenhofer and Benno Stein. 2010. Cross-Language Text Classification using Structural Correspondence
Learning. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics, pages
1118?1127.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010. The Bag-of-Opinions Method for Review Rating
Prediction from Sparse Text Patterns. In Proceedings of the 23rd International Conference on Computational
Linguistics, pages 913?921.
Helmut Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Hidetoshi Shimodaira. 2000. Improving Predictive Inference under Covariate Shift by Weighting the Log-
Likelihood Function. Journal of Statistical Planning and Inference, 90(2):227?244.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christo-
pher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642.
Efstathios Stamatatos. 2009. A Survey of Modern Authorship Attribution Methods. Journal of American Society
for Information Science and Technology, 60(3):538?556.
Simone Teufel, Advaith Siddharthan, and Colin Batchelor. 2009. Towards Discipline-independent Argumentative
Zoning: Evidence from Chemistry and Computational Linguistics. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing, pages 1493?1502.
Stephen E. Toulmin. 1958. The Uses of Argument. Cambridge University Press.
Maria Paz Garcia Villalba and Patrick Saint-Dizier. 2012. Some Facets of Argument Mining for Opinion Analysis.
In Proceedings of the 2012 Conference on Computational Models of Argument, pages 23?34.
Henning Wachsmuth and Kathrin Bujna. 2011. Back to the Roots of Genres: Text Classification by Language
Function. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages
632?640.
Henning Wachsmuth, Martin Trenkmann, Benno Stein, Gregor Engels, and Tsvetomira Palakarska. 2014. A
Review Corpus for Argumentation Analysis. In Proceedings of the 15th International Conference on Intelligent
Text Processing and Computational Linguistics, pages 115?127.
Douglas Walton and David M. Godden, 2006. Considering Pragma-Dialectics, chapter The Impact of Argumen-
tation on Artificial Intelligence, pages 287?299. Erlbaum.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent Aspect Rating Analysis on Review Text Data: A
Rating Regression Approach. In Proceedings of the 16th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 783?792.
Qiong Wu, Songbo Tan, Miyi Duan, and Xueqi Cheng. 2010. A Two-Stage Algorithm for Domain Adaptation
with Application to Sentiment Transfer Problems. In Information Retrieval Technology, volume 6458 of Lecture
Notes in Computer Science, pages 443?453.
564
