First Joint Conference on Lexical and Computational Semantics (*SEM), pages 608?616,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UMCC_DLSI: Multidimensional Lexical-Semantic Textual Similarity 
 
Antonio Fern?ndez, Yoan Guti?rrez, 
Alexander Ch?vez, H?ctor D?vila, Andy 
Gonz?lez, Rainel Estrada , Yenier Casta?eda 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
 
Sonia V?zquez 
Andr?s Montoyo, Rafael Mu?oz,  
 
DLSI, University of Alicante 
Carretera de San Vicente S/N 
Alicante, Spain 
 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI system, which 
participated in the first Semantic Textual 
Similarity task (STS) of SemEval-2012. Our 
supervised system uses different kinds of 
semantic and lexical features to train classifiers 
and it uses a voting process to select the correct 
option. Related to the different features we can 
highlight the resource ISR-WN1 used to extract 
semantic relations among words and the use of 
different algorithms to establish semantic and 
lexical similarities. In order to establish which 
features are the most appropriate to improve 
STS results we participated with three runs 
using different set of features. Our best 
approach reached the position 18 of 89 runs, 
obtaining a general correlation coefficient up to 
0.72. 
1. Introduction 
SemEval 2012 competition for evaluating Natural 
Language Processing (NLP) systems presents a 
new task called Semantic Textual Similarity (STS) 
(Agirre et al, 2012). In STS the participating 
systems must examine the degree of semantic 
equivalence between two sentences. The goal of 
this task is to create a unified framework for the 
evaluation of semantic textual similarity modules 
and to characterize their impact on NLP 
applications. 
STS is related to Textual Entailment (TE) and 
Paraphrase tasks. The main difference is that STS 
                                                   
1 Integration of Semantic Resource based on WordNet. 
assumes bidirectional graded equivalence between 
the pair of textual snippets. In the case of TE the 
equivalence is directional (e.g. a student is a 
person, but a person is not necessarily a student). 
In addition, STS differs from TE and Paraphrase in 
that, rather than being a binary yes/no decision, 
STS is a similarity-graded notion (e.g. a student 
and a person are more similar than a dog and a 
person). This bidirectional gradation is useful for 
NLP tasks such as Machine Translation, 
Information Extraction, Question Answering, and 
Summarization. Several semantic tasks could be 
added as modules in the STS framework, ?such as 
Word Sense Disambiguation and Induction, 
Lexical Substitution, Semantic Role Labeling, 
Multiword Expression detection and handling, 
Anaphora and Co-reference resolution, Time and 
Date resolution and Named Entity Recognition, 
among others?2  
1.1. Description of 2012 pilot task 
In STS, all systems were provided with a set of 
sentence pairs obtained from a segmented corpus. 
For each sentence pair, s1 and s2, all participants 
had to quantify how similar s1 and s2 were, 
providing a similarity score. The output of 
different systems was compared to the manual 
scores provided by SemEval-2012 gold standard 
file, which range from 5 to 0 according to the next 
criterions3:  
? (5) ?The two sentences are equivalent, as they 
mean the same thing?. 
                                                   
2
 http://www.cs.york.ac.uk/semeval-2012/task6/ 
3
 http://www.cs.york.ac.uk/semeval-
2012/task6/data/uploads/datasets/train-readme.txt 
608
? (4) ?The two sentences are mostly equivalent, 
but some unimportant details differ?. 
? (3) ?The two sentences are roughly equivalent, 
but some important information 
differs/missing?. 
? (2) ?The two sentences are not equivalent, but 
share some details?. 
? (1) ?The two sentences are not equivalent, but 
are on the same topic?. 
? (0) ?The two sentences are on different topics?. 
After this introduction, the rest of the paper is 
organized as follows. Section 2 shows the 
architecture of our system and a description of the 
different runs. In section 3 we describe the 
algorithms and methods used to obtain the features 
for our system, and Section 4 describe the training 
phase. The obtained results and a discussion are 
provided in Section 5, and finally the conclusions 
and future works in Section 6. 
2. System architecture and description of 
the runs 
As we can see in Figure 1 our three runs begin 
with the pre-processing of SemEval 2012?s 
training set. Every sentence pair is tokenized, 
lemmatized and POS tagged using Freeling tool 
(Atserias et al, 2006). Afterwards, several 
methods and algorithms are applied in order to 
extract all features for our Machine Learning 
System (MLS). Each run uses a particular group of 
features. 
The Run 1 (MultiSemLex) is our main run. 
This takes into account all extracted features and 
trains a model with a Voting classifier composed 
by the following techniques: Bagging (using M5P), 
Bagging (using REPTree), Random SubSpace 
(using REPTree) and MP5. The training corpus has 
been provided by SemEval-2012 competition, in 
concrete by the Semantic Textual Similarity task.  
The Runs 2 and 3 use the same classifier, but 
including different features. Run 2 (MultiLex) uses 
(see Figure 1) features extracted from Lexical-
Semantic Metrics (LS-M) described in section 3.1, 
Lexical-Semantic Alignment (LS-A) described in 
section 3.2 and Sentiment Polarity (SP) described 
in section 3.3.  
On the other hand, the Run 3 (MultiSem) uses 
features extracted only from Semantic Alignment 
(SA) described in section 3.4 and the textual edit 
distances named QGram-Distances. 
 
Figure 1. System Architecture. 
As a result, we obtain three trained models 
capable to estimate the similarity value between 
two sentences. 
Finally, we test our system with the SemEval 
2012 test set (see Table 7 with the results of our 
three runs). The following section describes the 
features extraction process. 
      Run 1 
      Voting  
      Classifier 
Training set from 
SemEval 2012 
Pre-Processing (using Freeling) 
 
Run 3 
Voting classifier 
Run 2 
Voting classifier 
Similarity Scores 
Feature extraction 
Lexical-Semantic Metrics 
 
Lexical-semantic 
alignment 
Semantic 
alignment 
Sentiment 
Polarity 
Jaro QGram Rel. Concept . . . 
Tokenizing Lemmatizing POS tagging 
SemEval 2012 
Test set 
     Training Process (using Weka) 
609
3. Description of the features used in the 
Machine Learning System 
Sometimes, when two sentences are very similar, 
one sentence is in a high degree lexically 
overlapped by the other. Inspired by this fact we 
developed various algorithms, which measure the 
level of overlapping by computing a quantity of 
matching words (the quantity of lemmas that 
correspond exactly by its morphology) in a pair of 
sentences. In our system, we used lexical and 
semantic similarity measures as features for a 
MLS. Other features were extracted from a lexical-
semantic sentences alignment and a variant using 
only a semantic alignment.  
3.1. Similarity measures 
We have used well-known string based similarity 
measures like: Needleman-Wunch (NW) (sequence 
alignment), Smith-Waterman (SW) (sequence 
alignment), Jaro, Jaro-Winkler (JaroW), Chapman-
Mean-Length (CMLength), QGram-Distance 
(QGramD), Block-Distance (BD), Jaccard 
Similarity (JaccardS), Monge-Elkan (ME) and 
Overlap-Coefficient (OC). These algorithms have 
been obtained from an API (Application Program 
Interface) SimMetrics library v1.54 for .NET 2.0. 
Copyright (c) 2006 by Chris Parkinson. We 
obtained 10 features for our MLS from these 
similarity measures. 
Using Levenshtein?s edit distance (LED), we 
computed also two different algorithms in order to 
obtain the alignment of the phrases. In the first 
one, we considered a value of the alignment as the 
LED between two sentences and the normalized 
variant named NomLED. Contrary to (Tatu et al, 
2006), we do not remove the punctuation or stop 
words from the sentences, neither consider 
different cost for transformation operation, and we 
used all the operations (deletion, insertion and 
substitution). The second one is a variant that we 
named Double Levenshtein?s Edit Distance 
(DLED). For this algorithm, we used LED to 
measure the distance between the sentences, but to 
compare the similarity between the words, we used 
LED again. Another feature is the normalized 
variant of DLED named NomDLED. 
The unique difference between classic LED 
algorithm and DLED is the comparison of 
                                                   
4
 http://sourceforge.net/projects/simmetrics/ 
similitude between two words. With LED should 
be: ?[?] = ?[?], whereas for our DLED we 
calculate words similarity also with LED (e.g. ????(?[?], ?[?]) <= 2). Values above a decision 
threshold (experimentally 2) mean unequal words. 
We obtain as result two new different features 
from these algorithms. 
Another distance we used is an extension of 
LED named Extended Distance (EDx) (see 
(Fern?ndez Orqu?n et al, 2009) for details). This 
algorithm is an extension of the Levenshtein?s 
algorithm, with which penalties are applied by 
considering what kind of operation or 
transformation is carried out (insertion, deletion, 
substitution, or non-operation) in what position, 
along with the character involved in the operation. 
In addition to the cost matrixes used by 
Levenshtein?s algorithm, EDx also obtains the 
Longest Common Subsequence (LCS) 
(Hirschberg, 1977) and other helpful attributes for 
determining similarity between strings in a single 
iteration. It is worth noting that the inclusion of all 
these penalizations makes the EDx algorithm a 
good candidate for our approach. In our previous 
work (Fern?ndez Orqu?n et al, 2009), EDx 
demonstrated excellent results when it was 
compared with other distances as (Levenshtein, 
1965), (Needleman and Wunsch, 1970), (Winkler, 
1999). How to calculate EDx is briefly described 
as follows (we refer reader to (Fern?ndez Orqu?n et 
al., 2009) for a further description): 
EDx = ??  ?????????????,???????(???????)????????? ?? ; (1) 
 
 
Where: ? - Transformations accomplished on the words (?, ?, ?, ?). ? - Not operations at all, ? - Insertion, ? - Deletion, ? - Substitution.  
We formalize ? as a vector: 
? =
???
??(0,0)(1,0) :: ??(0,1)(1,1) :: ?????
??
 
?1 and ?2 - The examined words ?1j - The j-th character of the word ?1 
610
?2k - The k-th character of the word ?2 ? - The weight of each character 
We can vary all this weights in order to make a 
flexible penalization to the interchangeable 
characters.  ??1j - The weight of characters at ?1j ??2k - The weight of characters at ?2k ? = ?? + 1 ?? ?i ? ?? ?? ?i = ? ? ; ? = ?? + 1 ?? ?i ? ?? ?? ?i = ? ? ? - The biggest word length of the language ? - Edit operations length ?i - Operation at (?) position ???? - Greatest value of ? ranking 
? = ? 2????(2???? + 1)???????  (2) 
As we can see in the equation (1), the term ?(??) ? ???????, ?(???)? is the Cartesian product that 
analyzes the importance of doing i-th operation 
between the characters at j-th and k-th position 
The term (2R??? + 1)??? in equation (1) penalizes 
the position of the operations. The most to the left 
hand the operation is the highest the penalization 
is. The term ? (see equation (2) normalizes the 
EDx into [0,1] interval. This measure is also used 
as a feature for the system. 
We also used as a feature the Minimal 
Semantic Distances (Breadth First Search (BFS)) 
obtained between the most relevant concepts of 
both sentences. The relevant concepts pertain to 
semantic resources ISR-WN (Guti?rrez et al, 
2011a; 2010b), as WordNet (Miller et al, 1990), 
WordNet Affect (Strapparava and Valitutti, 2004), 
SUMO (Niles and Pease, 2001) and Semantic 
Classes (Izquierdo et al, 2007). Those concepts 
were obtained after having applied the Association 
Ratio (AR) measure between concepts and words 
over each sentence. The obtained distances for 
each resource SUMO, WordNet Affect, WordNet 
and Semantic Classes are named SDist, AffDist, 
WNDist and SCDist respectively. 
ISR-WN, takes into account different kind of 
labels linked to WN: Level Upper Concepts 
(SUMO), Domains and Emotion labels. In this 
work, our purpose is to use a semantic network, 
which links different semantic resources aligned to 
WN. After several tests, we decided to apply ISR-
WN. Although others resources provide different 
semantic relations, ISR-WN has the highest 
quantity of semantic dimensions aligned, so it is a 
suitable resource to run our algorithm.  
Using ISR-WN we are able to extract 
important information from the interrelations of 
four ontological resources: WN, WND, WNA and 
SUMO. ISR-WN resource is based on WN1.6 or 
WN2.0 versions. In the last updated version, 
Semantic Classes and SentiWordNet were also 
included. Furthermore, ISR-WN provides a tool 
that allows the navigation across internal links. At 
this point, we can discover the multidimensionality 
of concepts that exists in each sentence. In order to 
establish the concepts associated to each sentence 
we apply Relevant Semantic Trees (Guti?rrez et 
al., 2010a; Guti?rrez et al, 2011b) approach using 
the provided links of ISR-WN. We refer reader to 
(Guti?rrez et al, 2010a) for a further description. 
3.2. Lexical-Semantic alignment 
Another algorithm that we created is the Lexical-
Semantic Alignment. In this algorithm, we tried to 
align the sentences by its lemmas. If the lemmas 
coincide we look for coincidences among parts of 
speech, and then the phrase is realigned using both. 
If the words do not share the same part of speech, 
they will not be aligned. Until here, we only have 
taken into account a lexical alignment. From now 
on, we are going to apply a semantic variant. After 
all the process, the non-aligned words will be 
analyzed taking into account its WorldNet?s 
relations (synonymy, hyponymy, hyperonymy, 
derivationally ? related ? form, similar-to, verbal 
group, entailment and cause-to relation); and a set 
of equivalencies like abbreviations of months, 
countries, capitals, days and coins. In the case of 
the relation of hyperonymy and hyponymy, the 
words will be aligned if there is a word in the first 
sentence that is in the same relation (hyperonymy 
or hyponymy) of another one in the second 
sentence. For the relations of ?cause-to? and 
?implication? the words will be aligned if there is a 
word in the first sentence that causes or implicates 
another one of the second sentence. All the other 
types of relations will be carried out in 
bidirectional way, that is, there is an alignment if a 
word of the first sentence is a synonymous of 
another one belonging to the second one or vice 
versa. Finally, we obtain a value we called 
alignment relation. This value is calculated as ??? =  ??? / ????. Where ??? is the final 
611
alignment value, ??? is the number of aligned 
word and ???? is the number of words of the 
shorter phrase. This value is also another feature 
for our system. 
3.3. Sentiment Polarity Feature 
Another feature is obtained calculating 
SentiWordNet Polarities matches of the analyzed 
sentences (see (Guti?rrez et al, 2011c) for detail). 
This analysis has been applied from several 
dimensions (WordNet, WordNet Domains, 
WordNet Affect, SUMO, and Semantic Classes) 
where the words with sentimental polarity offer to 
the relevant concepts (for each conceptual resource 
from ISR-WN (e.g. WordNet, WordNet Domains, 
WordNet Affect, SUMO, and Semantic Classes)) 
its polarity values. Other analysis were the 
integration of all results of polarity in a measure 
and further a voting process where all polarities 
output are involved (for more details see 
(Fern?ndez et al, 2012)). 
The final measure corresponds to ?? =????? + ?????, where ????1 is a polarity value of 
the sentence ?1 and ????? is a polarity value of the 
sentence ?2. The negative, neutral, and positive 
values of polarities are represented as -1, 0 and 1 
respectively. 
3.4. Semantic Alignment 
This alignment method depends on calculating the 
semantic similarity between sentences based on an 
analysis of the relations, in ISR-WN, of the words 
that fix them. 
First, the two sentences are pre-processed with 
Freeling and the words are classified according to 
their parts of speech (noun, verb, adjective, and 
adverbs.).  
We take 30% of the most probable senses of 
every word and we treat them as a group. The 
distance between two groups will be the minimal 
distance between senses of any pair of words 
belonging to the group. For example: 
 
Figure 2. Minimal Distance between ?Run? and 
?Chase?. 
In the example of Figure 2 the ???? = 2 is 
selected for the pair ?Run-Chase?, because this 
pair has the minimal cost=2.  
For nouns and the words that are not found in 
WordNet like common nouns or Christian names, 
the distance is calculated in a different way. In this 
case, we used LED. 
Let's see the following example: 
We could take the pair 99 of corpus MSRvid 
(from training set) with a litter of transformation in 
order to a better explanation of our method. 
Original pair 
A: A polar bear is running towards a group of 
walruses. 
B: A polar bear is chasing a group of walruses. 
Transformed pair: 
A1: A polar bear runs towards a group of cats. 
B1: A wale chases a group of dogs. 
Later on, using the algorithm showed in the 
example of Figure 2, a matrix with the distances 
between all groups of both sentences is created 
(see Table 1). 
GROUPS polar bear runs towards group cats 
wale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2 
chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3 
group     Dist:=0  
dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1 
Table 1. Distances between the groups. 
Using the Hungarian Algorithm (Kuhn, 1955) 
for Minimum Cost Assignment, each group of the 
smaller sentence is checked with an element of the 
biggest sentence and the rest is marked as words 
that were not aligned. 
In the previous example the words ?toward? 
and ?polar? are the words that were not aligned, so 
the number of non-aligned words is 2. There is 
only one perfect match: ?group-group? (match 
with ???? = 0). The length of the shortest sentence 
is 4. The Table 2 shows the results of this analysis. 
Number of 
exact 
coincidences 
(Same) 
Total Distances 
of optimal 
Matching 
(Cost) 
Number of 
non-
aligned 
Words 
(Dif) 
Number of 
lemmas of 
shorter 
sentence 
(Min) 
1 5 2 4 
Table 2. Features extracted from the analyzed sentences. 
This process has to be repeated for the verbs, 
nouns (see Table 3), adjectives, and adverbs. On 
the contrary, the tables have to be created only 
with the similar groups of the sentences. Table 3 
Lemma: Chase 
 
 
 
 
Lemma: Run 
 
 
 
 
Dist=2 
2 
3 
5 
Sense 1 
Sense 2 
Sense 1 
Sense 2 
4 
612
shows features extracted from the analysis of 
nouns. 
GROUPS bear group cats 
wale Dist := 2  Dist := 2 
group  Dist := 0  
dogs Dist := 1  Dist := 1 
Table 3. Distances between the groups of nouns. 
Number of 
exact 
coincidences 
(SameN) 
Total 
Distances of 
optimal 
Matching 
(CostN) 
Number of 
non-aligned 
Words 
(DifN) 
Number of 
lemmas of 
shorter 
sentence 
(MinN) 
1 3 0 3 
Table 4. Feature extracted the analysis of nouns. 
Several attributes are extracted from the pair of 
sentences. Four attributes from the entire 
sentences, four attributes considering only verbs, 
only nouns, only adjectives, and only adverbs. 
These attributes are:  
? Number of exact coincidences (Same) 
? Total distance of optimal matching (Cost). 
? Number of words that do not match (Dif). 
? Number of lemmas of the shortest sentence 
(Min). 
As a result, we finally obtain 20 attributes from 
this alignment method. For each part-of-speech, 
the attributes are represented adding to its names 
the characters N, V, A and R to represent features 
for nouns, verbs, adjectives, and adverbs 
respectively. 
It is important to remark that this alignment 
process searches to solve, for each word from the 
rows (see Table 3) its respectively word from the 
columns. 
4. Description of the training phase 
For the training process, we used a supervised 
learning framework, including all the training set 
(MSRpar, MSRvid and SMTeuroparl) as a training 
corpus. Using 10 fold cross validation with the 
classifier mentioned in section 2 (experimentally 
selected). 
As we can see in Table 5, the features: FAV, 
EDx, CMLength, QGramD, BD, Same, SameN, 
obtain values over 0.50 of correlation. The more 
relevant are EDx and QGramD, which were 
selected as a lexical base for the experiment in Run 
3. It is important to remark that feature SameN and 
Same only using number of exact coincidences 
obtain an encourage value of correlation. 
 
Feature Correlation Feature Correlation Feature Correlation 
Correlation using all 
features 
(correspond to Run 1) 
FAV 0.5064 ME 0.4971 CostV 0.1517 
0.8519 
LED 0.4572 OC 0.4983 SameN 0.5307 
DLED 0.4782 SDist 0.4037 MinN 0.4149 
NormLED 0.4349 AffDist 0.4043 DifN 0.1132 
NormDLED 0.4457 WNDist 0.2098 CostN 0.1984 
EDx 0.596 SCDist  0.1532 SameA 0.4182 
NW 0.2431 PV 0.0342 MinA 0.4261 
SW 0.2803 Same 0.5753 DifA 0.3818 
Jaro 0.3611 Min 0.5398 CostA 0.3794 
JaroW 0.2366 Dif 0.2588 SameR 0.3586 
CMLength 0.5588 Cost 0.2568 MinR 0.362 
QGramD 0.5749 SameV 0.3004 DifR 0.3678 
BD 0.5259 MinV 0.4227 CostR 0.3461 
JaccardS 0.4849 DifV 0.2634 
  
 
Table 5. Correlation of individual features over all training sets. 
 
We decide to include the Sentiment Polarity as 
a feature, because our previous results on Textual 
Entailment task in (Fern?ndez et al, 2012). But, 
contrary to what we obtain in this paper, the 
influence of the polarity (PV) for this task is very 
low, its contribution working together with other 
features is not remarkable, but neither negative 
(Table 6), So we decide remaining in our system. 
In oder to select the lexical base for Run 3 
(MultiSem, features marked in bold) we compared 
the individual influences of the best lexical 
features (EDx, QGramD, CMLength), obtaining 
613
the 0.82, 0.83, 0.81 respectively (Table 6). Finally, 
we decided to use QGramD. 
The conceptual features SDist, AffDist, 
WNDist, SCDist do not increase the similarity 
score, this is due to the generality of the obtained 
concept, losing the essential characteristic between 
both sentences. Just like with PV we decide to 
keep them in our system. 
As we can see in Table 5, when all features are 
taking into account the system obtain the best 
score. 
Feature Pearson (MSRpar, MSRvid and SMTeuroparl) 
SDist 
 
       
0.8509 
AffDist 
 
       
WNDist 
 
       
SCDist 
 
       
EDx 
 
      
0.8507 
PV 
 
   
 
  QGramD 
 
    
0.8491 
CMLength
 
 
0.8075 
   
Same 
0.7043 
0.795 0.829 0.8302 0.8228 
Min 
Dif 
Cost 
SameV 
0.576 MinV DifV 
CostV 
SameN 
0.5975 MinN DifN 
CostN 
SameA 
0.4285 MinA DifA 
CostA 
SameR 
0.3778 MinR DifR 
CostR 
Table 6. Features influence.  
Note: Gray cells mean features that are not taking into 
account. 
5. Result and discussion 
Semantic Textual Similarity task of SemEval-2012 
offered three official measures to rank the 
systems5: 
1. ALL: Pearson correlation with the gold 
standard for the five datasets, and 
corresponding rank. 
2. ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
                                                   
5
 http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=results-update 
standard using least squares, and 
corresponding rank. 
3. Mean: Weighted mean across the five datasets, 
where the weight depends on the number of 
pairs in the dataset. 
4. Pearson for individual datasets. 
Using these measures, our main run (Run 1) 
obtained the best results (see Table 7). This 
demonstrates the importance of tackling this 
problem from a multidimensional lexical-semantic 
point of view. 
Run MSRpar MSRvid SMT-eur On-WN SMT-
news 
1 0.6205 0.8104 0.4325 0.6256 0.4340 
2 0.6022 0.7709 0.4435 0.4327 0.4264 
3 0.5269 0.7756 0.4688 0.6539 0.5470 
Table 7. Official SemEval 2012 results. 
Run ALL Rank ALLnrm RankNrm Mean RankMean 
1 0.7213 18 0.8239 14 0.6158 15 
2 0.6630 26 0.7922 46 0.5560 49 
3 0.6529 29 0.8115 23 0.6116 16 
Table 8. Ranking position of our runs in SemEval 2012. 
The Run 2 uses a lot of lexical analysis and not 
much of semantic analysis. For this reason, the 
results for Run 2 is poorer (in comparison to the 
Run 3) (see Table 7) for the test sets: SMT-eur, 
On-WN and SMT-news. Of course, these tests 
have more complex semantic structures than the 
others. However, for test MSRpar it function better 
and for test MSRvid it functions very similar to 
Run 3. 
Otherwise, the Run 3 uses more semantic 
analysis that Run 2 (it uses all features mentioned 
except feature marked in bold on Table 6) and only 
one lexical similarity measure (QGram-Distance). 
This makes it to work better for test sets SMT-eur, 
On-WN and SMT-news (see Table 7). It is 
important to remark that this run obtains important 
results for the test SMT-news, positioning this 
variant in the fifth place of 89 runs. Moreover, it is 
interesting to notice (Table 7) that when mixing the 
semantic features with the lexical one (creating 
Run 1) it makes the system to improve its general 
results, except for the test: SMT-eur, On-WN and 
SMT-news in comparison with Run 3. For these 
test sets seem to be necessary more semantic 
analysis than lexical in order to improve similarity 
estimation. We assume that Run 1 is non-balance 
according to the quantity of lexical and semantic 
features, because this run has a high quantity of 
614
lexical and a few of semantic analysis. For that 
reason, Run 3 has a better performance than Run 1 
for these test sets. 
Even when the semantic measures demonstrate 
significant results, we do not discard the lexical 
help on Run 3. After doing experimental 
evaluations on the training phase, when lexical 
feature from QGram-Distance is not taken into 
account, the Run 3 scores decrease. This 
demonstrates that at least a lexical base is 
necessary for the Semantic Textual Similarity 
systems. 
6. Conclusion and future works 
This paper introduced a new framework for 
recognizing Semantic Textual Similarity, which 
depends on the extraction of several features that 
can be inferred from a conventional interpretation 
of a text. 
As mentioned in section 2 we have conducted 
three different runs, these runs only differ in the 
type of attributes used. We can see in Table 7 that 
all runs obtained encouraging results. Our best run 
was placed between the first 18th positions of the 
ranking of Semeval 2012 (from 89 Runs) in all 
cases. Table 8 shows the reached positions for the 
three different runs and the ranking according to 
the rest of the teams.  
In our participation, we used a MLS that works 
with features extracted from five different 
strategies: String Based Similarity Measures, 
Semantic Similarity Measures, Lexical-Semantic 
Alignment, Semantic Alignment, and Sentiment 
Polarity Cross-checking. 
We have conducted the semantic features 
extraction in a multidimensional context using the 
resource ISR-WN, the one that allowed us to 
navigate across several semantic resources 
(WordNet, WordNet Domains, WordNet Affect, 
SUMO, SentiWorNet and Semantic Classes). 
Finally, we can conclude that our system 
performs quite well. In our current work, we show 
that this approach can be used to correctly classify 
several examples from the STS task of SemEval-
2012. Comparing with the best run (UKP_Run2 
(see Table 9)) of the ranking our main run has very 
closed results. In two times we increased the best 
UKP?s run (UKP_Run 2), for MSRvid test set in 
0.2824 points and for On-WN test set in 0.1319 
points (see Table 10).  
Run ALL Rank ALLnrm RankNrm Mean RankMean 
(UKP) 
Run 2 0.8239 1 0.8579 2 0.6773 1 
Table 9. The best run of SemEval 2012. 
It is important to remark that we do not expand 
any corpus to train the classifier of our system. 
This fact locates us at disadvantage according to 
other teams that do it. 
Run ALL MSRpar MSRvid SMT-
eur 
On-
WN 
SMT-
news 
(UKP) 
Run 2 0.8239 0.8739 0.528 0.6641 0.4937 0.4937 
(Our) 
Run 1 0.721 0.6205 0.8104 0.4325 0.6256 0.434 
Table 10. Comparison of our distance with the best. 
As future work we are planning to enrich our 
semantic alignment method with Extended 
WordNet (Moldovan and Rus, 2001), we think that 
with this improvement we can increase the results 
obtained with texts like those in On-WN test set. 
Acknowledgments 
This paper has been supported partially by 
Ministerio de Ciencia e Innovaci?n - Spanish 
Government (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat 
Valenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/288). 
Reference 
Antonio Fern?ndez, Yoan Guti?rrez, Rafael Mu?oz and 
Andr?s Montoyo. 2012. Approaching Textual 
Entailment with Sentiment Polarity. In  ICAI'12 - The 
2012 International Conference on Artificial 
Intelligence, Las Vegas, Nevada, USA.  
Antonio Celso Fern?ndez Orqu?n, D?az Blanco Josval, 
Alfredo Fundora Rolo and Rafael Mu?oz Guillena. 
2009. Un algoritmo para la extracci?n de 
caracter?sticas lexicogr?ficas en la comparaci?n de 
palabras. In  IV Convenci?n Cient?fica Internacional 
CIUM, Matanzas, Cuba.  
Carlo Strapparava and Alessandro Valitutti. 2004. 
WordNet-Affect: an affective extension of WordNet. 
In Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC 2004), 
Lisbon,  1083-1086.  
Daniel S. Hirschberg. 1977. Algorithms for the longest 
common subsequence problem Journal of the ACM, 
24: 664?675. 
615
Dan I. Moldovan and Vasile Rus. 2001. Explaining 
Answers with Extended WordNet ACL. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A 
Pilot on Semantic Textual Similarity. In Proceedings 
of the 6th International Workshop on Semantic 
Evaluation (SemEval 2012), in conjunction with the 
First Joint Conference on Lexical and Computational 
Semantics (*SEM 2012), Montreal, Canada, ACL.  
George A. Miller, Richard Beckwith, Christiane 
Fellbaum, Derek Gross and Katherine Miller. 1990. 
Introduction to WordNet: An On-line Lexical 
Database International Journal of Lexicography, 
3(4):235-244. 
Harold W. Kuhn. 1955. The Hungarian Method for the 
assignment problem Naval Research Logistics 
Quarterly,  2: 83?97. 
Ian Niles and Adam Pease. 2001. Origins of the IEEE 
Standard Upper Ontology. In  Working Notes of the 
IJCAI-2001 Workshop on the IEEE Standard Upper 
Ontology, Seattle, Washington, USA.  
Jordi Atserias, Bernardino Casas, Elisabet Comelles, 
Meritxell Gonz?lez, Llu?s Padr? and Muntsa Padr?. 
2006. FreeLing 1.3: Syntactic and semantic services 
in an open source NLP library. In  Proceedings of the 
fifth international conference on Language Resources 
and Evaluation (LREC 2006), Genoa, Italy.  
Marta Tatu, Brandon Iles, John Slavick, Novischi 
Adrian and Dan Moldovan. 2006. COGEX at the 
Second Recognizing Textual Entailment Challenge. 
In Proceedings of the Second PASCAL Recognising 
Textual Entailment Challenge Workshop, Venice, 
Italy,  104-109. 
Rub?n Izquierdo, Armando Su?rez and German Rigau. 
2007. A Proposal of Automatic Selection of Coarse-
grained Semantic Classes for WSD Procesamiento 
del Lenguaje Natural,  39: 189-196. 
Saul B. Needleman and Christian D. Wunsch. 1970. A 
general method applicable to the search for 
similarities in the amino acid sequence of two 
proteins Journal of Molecular Biology,  48(3): 443-
453. 
Vladimir Losifovich Levenshtein. 1965. Binary codes 
capable of correcting spurious insertions and 
deletions of ones. Problems of information 
Transmission.  pp. 8-17.  
William E. Winkler. 1999. The state of record linkage 
and current research problems. Technical Report. 
U.S. Census Bureau, Statistical Research Division. 
Yoan Guti?rrez, Antonio Fern?ndez, And?s Montoyo 
and Sonia V?zquez. 2010a. UMCC-DLSI: Integrative 
resource for disambiguation task. In  Proceedings of 
the 5th International Workshop on Semantic 
Evaluation, Uppsala, Sweden, Association for 
Computational Linguistics,  427-432.  
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010b. Integration of semantic 
resources based on WordNet XXVI Congreso de la 
Sociedad Espa?ola para el Procesamiento del 
Lenguaje Natural,  45: 161-168. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2011a. Enriching the Integration 
of Semantic Resources based on WordNet 
Procesamiento del Lenguaje Natural,  47: 249-257. 
Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo. 
2011b. Improving WSD using ISR-WN with Relevant 
Semantic Trees and SemCor Senses Frequency. In  
Proceedings of the International Conference Recent 
Advances in Natural Language Processing 2011, 
Hissar, Bulgaria, RANLP 2011 Organising 
Committee,  233--239.  
Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo. 
2011c. Sentiment Classification Using Semantic 
Features Extracted from WordNet-based Resources. 
In  Proceedings of the 2nd Workshop on 
Computational Approaches to Subjectivity and 
Sentiment Analysis (WASSA 2.011), Portland, 
Oregon., Association for Computational Linguistics,  
139--145.  
616
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 241?249, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Reinforcing a Ranking Algorithm with Sense 
Frequencies and Multidimensional Semantic Resources to solve 
Multilingual Word Sense Disambiguation 
 
Yoan Guti?rrez, Yenier 
Casta?eda, Andy Gonz?lez, 
Rainel Estrada, Dennys D. Piug, 
Jose I. Abreu, Roger P?rez 
Antonio Fern?ndez Orqu?n, 
Andr?s Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas DLSI, University of Alicante Independent Consultant 
Matanzas, Cuba Alicante, Spain USA 
{yoan.gutierrez, 
yenier.castaneda, 
rainel.estrada, 
dennys.puig, jose.abreu, 
roger.perez}@umcc.cu, 
andy.gonzalez@infonet.umcc
.cu 
antonybr@yahoo.com, 
{montoyo,rafael}@dlsi.ua.
es 
info@franccamara.c
om 
 
Abstract 
This work introduces a new unsupervised 
approach to multilingual word sense 
disambiguation. Its main purpose is to 
automatically choose the intended sense 
(meaning) of a word in a particular context for 
different languages. It does so by selecting the 
correct Babel synset for the word and the 
various Wiki Page titles that mention the 
word. BabelNet contains all the output 
information that our system needs, in its Babel 
synset. Through Babel synset, we find all the 
possible Synsets for the word in WordNet. 
Using these Synsets, we apply the 
disambiguation method Ppr+Freq to find what 
we need. To facilitate the work with WordNet, 
we use the ISR-WN which offers the 
integration of different resources to WordNet. 
Our system, recognized as the best in the 
competition, obtains results around 69% of 
Recall. 
1 Introduction 
Word Sense Disambiguation (WSD) focuses on 
resolving the semantic ambiguity of a given word.  
This is an important task in Natural Language 
Processing (NLP) because in many applications, 
such as Automatic Translation, it is essential to 
know the exact meaning of a word in a given 
context. In order to solve semantic ambiguity, 
different systems have been developed. However, 
we can categorize them in two main groups: 
supervised and unsupervised systems. The 
supervised ones need large quantity of hand-tagged 
data in order to gather enough information to build 
rules, train systems, and so on. Unsupervised 
systems, on the other hand, do not need such a 
large amount of hand-tagged datasets. This means 
that, when there aren?t enough corpora to train the 
systems, an unsupervised system is a good option. 
A sub-task of WSD is Multilingual Word Sense 
Disambiguation (MWSD) (Navigli et al, 2013) 
that aims at resolving ambiguities in different 
languages. 
In a language, there are words that have only one 
sense (or meaning), but in other languages, the 
same words can have different senses. For 
example, ?patient? is a word that in English can be 
either a noun or an adjective, but in German, it 
only has one sense - ?viz? (a person that needs 
treatment). This shows that the information 
obtained by combining two languages can be more 
useful for WSD because the word senses in each 
language can complement each other. For it to be 
useful, MWSD needs a multilingual resource that 
contains different languages, such as BabelNet 
(Navigli and Ponzetto, 2010; 2012) and 
EuroWordNet (Vossen, 1998). 
241
As the preferred disambiguation method, we 
decided to use the Ppr+Freq (Personalized Page 
Rank combined with Frequencies of senses)  
(Guti?rrez, 2012) method because, among 
unsupervised systems, graph-based methods have 
obtained more promising results.  
It is worth mentioning the relevant approaches 
used by the scientific community to achieve 
promising results. One approach used is structural 
interconnections, such as Structural Semantic 
Interconnections (SSI), which create structural 
specifications of the possible senses for each word 
in a context (Navigli and Velardi, 2005). The other 
approaches used are ?Exploring the integration of 
WordNet? (Miller et al, 1990), FrameNet (Laparra 
et al, 2010) and those using Page-Rank such as 
(Sinha and Mihalcea, 2007) and (Agirre and Soroa, 
2009). 
The aforementioned types of graph based 
approaches have achieved relevant results in both 
the SensEval-2 and SensEval-3 competitions (see 
Table 1). 
Algorithm Recall 
TexRank (Mihalcea, 2005)  54.2% 
(Sinha and Mihalcea, 2007) 56.4% 
(Tsatsaronis et al, 2007) 49.2% 
Ppr (Agirre and Soroa, 2009) 58.6% 
Table 1. Relevant WSD approaches. Recall measure is 
calculated recalls using SensEval-2 (English All Word 
task) guidelines over. 
Experiments using SensEval-2 and SensEval-3 
corpora suggest that Ppr+Freq (Guti?rrez, 2012) 
can lead to better results by obtaining over 64% of 
Recall. Therefore we selected Ppr+Freq as the 
WSD method for our system. 
The key proposal for this work is an 
unsupervised algorithm for MWSD, which uses an 
unsupervised method, Ppr+Freq, for semantic 
disambiguation with resources like BabelNet (as 
sense inventory only) (Navigli and Ponzetto, 2010) 
and ISR-WN (as knowledge base) (Guti?rrez et al, 
2011a; 2010a). 
ISR-WN was selected as the default knowledge 
base because of previous NLP research, which 
included: (Fern?ndez et al, 2012; Guti?rrez et al, 
2010b; Guti?rrez et al, 2012; 2011b; 2011c; 
2011d), which achieved relevant results using ISR-
WN as their knowledge base. 
2 System architecture  
By using one of BabelNet (BN) features, our 
technique begins by looking for all the Babel 
synsets (Bs) linked to the lemma of each word in 
the sentence that we need to disambiguate.  
Through the Bs offsets, we can get its 
corresponding WordNet Synset (WNS), which 
would be retrieved from WordNet (WN) using the 
ISR-WN resource. As a result, for each lemma, we 
have a WordNet Synset List (WNSL) from which 
our Word Sense Disambiguation method obtains 
one WNS as the correct meaning. 
Our WSD method consists of applying a 
modification of the Personalizing PageRank (Ppr) 
algorithm (Agirre and Soroa, 2009), which 
involves the senses frequency. More specifically, 
the key proposal is known as Ppr+Freq (see 
Section 2.3).  
Given a set of WNSLs of WNSL, as words 
window, we applied the Synsets ranking method, 
Ppr+Freq, which ranks in a descending order, the 
Synsets of each lemma according to a calculated 
factor of relevance. The first Synset (WNS) of 
each WNSL (the most relevant) is established as 
the correct one and its associated Babel synset (Bs) 
is also tagged as correct. To determine the Wiki 
Page Titles (WK), we examine the WIKI 
(Wikipedia pages) and WIKIRED (Wikipedia 
pages redirections) in the correct Babel synset 
obtained. 
Figure 1 shows a general description of our 
system that is made up of the following steps: 
I. Obtaining lemmas  
II. Obtaing WN Synset of selected lemmas  
III. Applying Ppr+Freq method  
IV. Assigning Synset, Babel synset and Wiki 
page title 
Note that ISR-WN contains WN as its nucleus. 
This allows linking both resources, BabelNet and 
ISR-WN.
242
 
Figure 1. General process description taking as instance a sentence provided by the trial dataset. 
 
2.1 Obtaining lemmas  
For each input sentence, we extract the labeled 
lemmas. As an example, for the sentence, ?The 
struggle against the drug lords in Colombia will be 
a near thing,? the selected lemmas are: ?struggle,? 
?drug_lord,? ?Colombia?, and ?near_thing.? 
 
Figure 2. Obtaining synset of lemmas. 
 
2.2 Obtaing WN Synset of selected lemmas  
For each lemma obtained in the previous section, 
we look through BabelNet to recover the Bs that 
contains the lemma among its labels. When BSs 
are mapped to WN, we use the ISR-WN resource 
to find the corresponding Synset. Since a lemma 
can appear in a different Bs, it can be mapped with 
several WNS. Thus, we get a Synset list for each 
lemma in the sentence. In case the lemma does not 
have an associated Bs, its list would be empty. An 
example of this step is shown on Figure 2. 
2.3 Applying Ppr+Freq method 
In the above case, Ppr+Freq modifies the ?classic? 
Page Rank approach instead of assigning the same 
weight for each sense of WN in the disambiguation 
graph (??). 
The PageRank (Brin and Page, 1998) 
adaptation, Ppr , which was popularized by (Agirre 
IV . Assigning Synset, Babel Synset and Wiki page title
? The struggle against the drug lords in Colombia will be a near thing .?
struggle drug_lord Colombia near_thing
Wikipedia WordNet BabelNet
ISR-WN
WordNet
(WN)
SUMO
WN-Domain
WN-Affect
SemanticClass eXtended WN3.0
eXtended WN1.7
struggle%1:04:01:: drug_lord%1:18:00:: colombia%1:15:00:: near_thing%1:04:00::
bn:00009079n bn:00028876n bn:00020697n bn:00057109n
-- Drug_Lord Colombia --
I. Obtaing lemmas
II. Obtaining Synset of selected lemmas
III. Applying Ppr+Freq method
WN key
BS
WK
struggle
drug_lord Colombia
near_thing
struggle
bn:00074762n wn:00587514n
bn:00009079n wn:00739796n
bn:00009080n wn:00901980n
drug_lord bn:00028876n wn:09394468n
colombia
bn:00020697n wn:08196765n
bn:02051949n
bn:02530766n
near_thing bn:00057109n wn:00193543n
Sentence lemmas 
Babel synset 
WordNet synset 
243
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main idea 
behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ? to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the more 
strength the voter would have. Thus, PageRank is 
generated by applying a random walkthrough from 
the internal interconnection of ?, where the final 
relevance of ??  represents the random walkthrough 
probability over ?, and ending on ??. 
Ppr+Freq includes the existent semantic and 
frequency patterns of each sense of the word to 
disambiguate while finding a way to connect each 
one of these words in a knowledge base. 
The new graph-based approach of WSD 
generates a graph of disambiguated words for each 
input sentence. For that reason, it is necessary to 
classify the word senses according to the other 
words that compose the context. The general 
method is shown in Figure 3. This method is 
divided into three steps: 
I. Creation of a disambiguation graph 
II. Application of Ppr+Freq in the generated 
graph 
III. Selection of the correct answer 
Creation of a disambiguation graph: In the first 
step, a disambiguation graph is built by means of a 
Breath First Search (BFS) over the ?super? graph 
composed by all the resources integrated into ISR-
WN. The components involved in this process are: 
WordNet, SUMO (Zouaq et al, 2009) WordNet 
Domains (Magnini and Cavaglia, 2000) WordNet 
Affects (Strapparava and Valitutti, 2004) Semantic 
Classes (Izquierdo et al, 2007) and eXtended 
WordNet (XWN) relations (Moldovan and Rus, 
2001). This search aims to recover all senses 
(nodes), domain labels (from WordNet Domain 
and WordNet Affects), SUMO categories, and 
Semantic Classes labels through the shortest path 
between every pair of senses in the WNSL set 
associated with the input sentence. Using ISR-WN 
as the KB, through experimentation, we obtained 
the shortest paths with a length of five edges. For a 
better understanding of this process, see (Guti?rrez, 
2012). 
Application of Ppr+Freq in the generated 
graph: In the second step, we use the weighted 
Personalized PageRank. Here, all the vertices from 
vector ? in ?? are initialized with the value  
1
?
 ; 
where ? is the number of nodes in ??. On the 
other hand, the vertices that represent word senses 
in the analyzed sentence are not initialized with 
this value. Instead, they are initialized with values 
in the range [0?1], which are associated to their 
occurrence frequency in SemCor1 (Corpus and 
sense frequencies knowledge). In the last step, 
after applying the Ppr+Freq algorithm over ??, we 
get a representative vector which contains ISR-WN 
nodes in ?? sorted in a descending order by a 
ranking score computed by this algorithm. For a 
better description, see (Guti?rrez, 2012). 
Selection of the correct answer: As the correct 
sense, we take the highest ranked sense of each 
target word involved in this vector. Note that 
domain labels, SUMO categories, semantic class 
labels, and affect labels are ranked too. They could 
be used in the future to determine relevant 
conceptualizations that would be useful for text 
classification and more. 
In our system, we assume the following 
configuration: dumping factor ? = 0.85 and like in 
(Agirre and Soroa, 2009) we used 30 iterations. A 
detailed explanation about PageRank algorithm 
can be found in (Agirre and Soroa, 2009). 
Table 2 shows an example that analyzes the 
Synset for each word in the sentence and also 
shows how the higher ranked Synsets of the target 
words are selected as the correct ones. For a 
detailed explanation of Ppr+Freq, see (Guti?rrez, 
2012). 
2.4 Assigning Synset, Babel synset and Wiki 
Pages 
In this step, English is handled differently from 
other languages because WordNet Synsets are 
available only for English. The following sections 
explain how we proceed in each case. Once the 
Synsets list is obtained for each lemma in section 
2.3, selecting the correct answer for the lemma is 
all that?s left to do. 
                                                     
1 http://www.cse.unt.edu/~rada/downloads.html 
244
 
Figure 3. General process of WSD with Ppr+Freq. 
2.4.1 English 
Given a lemma, we go through its Synset list from 
beginning to end looking for the first Synset that 
contains a key2 for the lemma. If such Synset 
exists, it is designated as the Synset for the lemma. 
Otherwise, no Synset is assigned. 
As already explained, each Synset in the list is 
connected to a Bs. Therefore, the lemma linked 
with the correct WNS selected in the previous step, 
is chosen as the correct lemma. In case no Synsets 
were designated as the correct ones, we take the 
first Bs in BN, which contains the lemma among 
its labels.  
To determine the Wiki pages titles (WK) we 
examine the WIKIRED and WIKI labels in the 
correct Bs selected in the preceding step. This 
search is restricted only to labels corresponding to 
the analyzed language and discriminating upper 
and lower case letters. Table 2 shows some sample 
results of the WSD process. 
Lemma struggle drug_lord 
WNS 00739796n 09394468n 
WN key struggle%1:04:01:: drug_lord%1:18:00:: 
Bs bn:00009079n bn:00028876n 
WK - Drug_Lord 
Lemma colombia near_thing 
WNS 08196765n 00193543n 
WN key colombia%1:15:00:: near_thing%1:04:00:: 
Bs bn:00020697n bn:00057109n 
WK Colombia - 
Table 2 : Example of English Language. 
                                                     
2A sense_key is the best way to represent a sense in 
semantic tagging or other systems that refer to WordNet 
senses. sense_key?s are independent of WordNet sense 
numbers and synset_offset?s, which vary between versions of 
the database. 
2.4.2 Other languages  
For this scenario, we introduce a change in the first 
step discussed in the previous section. The reason 
is that the Synsets do not contain any keys in any 
other language than English. Thus, the correct 
Synset for the lemma is the first in the Synset list 
for the lemma obtained, as described, in section 
2.3. 
3 Results 
We tested three versions (runs) of the proposed 
approach and evaluated them through a trial 
dataset provided by Task123 of Semeval-2013 
using babelnet-1.0.1. Table 3 shows the result for 
each run. Note that the table results were 
calculated with the traditional WSD recall 
measure, being this measure which has ranked 
WSD systems on mostly Semeval competitions. 
On the other hand, note that our precision and 
recall results are different because the coverage is 
not 100%. See Table 5. 
 English French 
Runs WNS Bs WK Bs WK 
Run1 0.70 0.71 0.77 0.59 0.85 
Run2 0.70 0.71 0.78 0.60 0.85 
Run3 0.69 0.70 0.77 - - 
Table 3 : Results of runs with trial recall values. 
As can be noticed on Table 3, results of different 
versions do not have big differences, but in 
general, Run2 achieves the best results; it?s better 
                                                     
3 http://www.cs.york.ac.uk/semeval-2013/task12 
ISR-WN
footballer#1 | cried#9 | winning#3
footballer | cry | winning
Lemmas
?The footballer cried when winning?
Disambiguation
Graph
(0,9)
Footballer#1
(0,3)
cry#7
(0,4)
cry#9
(0,2)
cry#10
(0,2)
cry#11
(0,2)
cry#12
(0,2)
winning#1
(0,3)
winning#3
Creating GD
Ppr+Freq
Selecting senses
245
than Run1 in the WK with a 78% in English and 
Bs with 60% in French. The best results are in the 
WK in French with a value of 85%. 
Since we can choose to include different 
resources into ISR-WN, it is important to analyze 
how doing so would affect the results. Table 4 
shows comparative results for Run 2 of a trial 
dataset with BabelNet version 1.1.1. 
As can be observed in Table 4, the result does not 
have a significant change even though we used the 
ISR-WN with all resources.  
A better analysis of Ppr+Freq in, as it relates to 
the influence of each resource involved in ISR-WN 
(similar to Table 4 description) assessing 
SensEval-2 and SensEval-3 dataset, is shown in 
(Guti?rrez, 2012). There are different resource 
combinations showing that only XWN1.7 and all 
ISR-WN resources obtain the highest performance. 
Other analysis found in (Guti?rrez, 2012) evaluates 
the influence of adding the sense frequency for 
Ppr+Freq.  
By excluding the Factotum Domain, we obtain 
the best result in Bs 54% for French (only 1% 
more than the version used in the competition). 
The other results are equal, with a 69% in WNS, 
66% in Bs, 64% in WK for English, and 69% in 
WK for French. 
        English French 
WN Domains Sumo Affect Factotum 
Domain 
SemanticClass XWN3.0 XWN1.7 WNS Bs WK Bs WK 
X X X X X X X X 0.69 0.66 0.64 0.53 0.69 
X X  X X X X X 0.69 0.66 0.64 0.53 0.69 
X    X X X X 0.68 0.65 0.64 0.52 0.69 
X X X X  X X X 0.69 0.66 0.64 0.54 0.69 
X X X X  X  X 0.68 0.65 0.65 0.53 0.69 
Table 4. Influence of different resources that integrate ISR-WN in our technique. 
    Wikipedia BabelNet WordNet 
System Language Precision Recall F-score Precision Recall F-score Precision Recall F-score 
MFS DE 0.836 0.827 0.831 0.676 0.673 0.686 - - - 
  EN 0.86 0.753 0.803 0.665 0.665 0.656 0.63 0.63 0.63 
  ES 0.83 0.819 0.824 0.645 0.645 0.644 - - - 
  FR 0.698 0.691 0.694 0.455 0.452 0.501 - - - 
  IT 0.833 0.813 0.823 0.576 0.574 0.572 - - - 
Run1 DE 0.758 0.46 0.572 0.619 0.617 0.618 - - - 
  EN 0.619 0.484 0.543 0.677 0.677 0.677 0.639 0.635 0.637 
  ES 0.773 0.493 0.602 0.708 0.703 0.705 - - - 
  FR 0.817 0.48 0.605 0.608 0.603 0.605 - - - 
  IT 0.785 0.458 0.578 0.659 0.656 0.657 - - - 
Run2 DE 0.769 0.467 0.581 0.622 0.62 0.621 - - - 
  EN 0.62 0.487 0.546 0.685 0.685 0.685 0.649 0.645 0.647 
  ES 0.778 0.502 0.61 0.713 0.708 0.71 - - - 
  FR 0.815 0.478 0.603 0.608 0.603 0.605 - - - 
  IT 0.787 0.463 0.583 0.659 0.657 0.658 - - - 
Run3 EN 0.622 0.489 0.548 0.68 0.68 0.68 0.642 0.639 0.64 
Table 5. Results of Runs for Task12 of semeval-2013 using the test dataset. 
 
246
3.1 Run1 
In this Run, WNSLs consist of all the target words 
involved in each sentence. This run is applied at 
the sentence level. The results for the competition 
are shown in Table 5. For this Run, the best result 
was obtained for Spanish with a 70.3% in Bs and 
49.3% in WK of Recall. As we can see, for Run1 
the precision is high for Wikipedia disambiguation, 
obtaining for French the best result of the ranking. The 
low Recall in Wikipedia is due to the exact mismatching 
of labels between our system output and the gold 
standard. This fact, affects the rest of our runs. 
3.2 Run2 
In this Run, WNSLs consist of all the target words 
involved in each domain. We can obtain the target 
words because the training and test dataset contain 
the sentences grouped by topics.  For instance, for 
English, 13 WNSLs are established. This Run is 
applied at the corpora level. The results for the 
competition are shown in Table 5. It is important to 
emphasize that our best results ranked our 
algorithm as first place among all proposed 
approaches for the MWSD task. 
For this run, the best Recall was obtained for 
Spanish with a 70.8% in Bs and 50.2% in WK. 
This Run also has the best result of the three runs. 
For the English competition, it ended up with a 
64.5% in WNS, 68.5% in Bs, and 48.7% in WK. 
This Run obtained promising results, which took 
first place in the competition. It also had better 
results than that of the First Sense (Most Frequent 
Sense) baseline in Bs results for all languages, 
except for German. In Bs, it only obtained lower 
results in German with a 62% of Recall for our 
system and 67.3% for the First Sense baseline. 
3.3 Run3 
In this run, WNSLs consist of all the words 
included in each sentence. This run uses target 
words and non-target words of each sentence, as 
they are applied to the sentence level. The results 
for the competition are shown in Table 5.  
As we can see, the behavior of this run is similar 
to the previous runs. 
4 Conclusions and Future work  
The above results suggest that our proposal is a 
promising approach. It is also important to notice 
that a richer knowledgebase can be built by 
combining different resources such as BabelNet 
and ISR-WN, which can lead to an improvement 
of the results. Notwithstanding, our system has 
been recognized as the best in the competition, 
obtaining results around 70% of Recall. 
According to the Task12 results4, only the 
baseline Most Frequent Sense (MFS) could 
improve our scores in order to achieve better WK 
and German (DE) disambiguation. Therefore, we 
plan to review this point to figure out why we 
obtained better results in other categories, but not 
for this one. At the same time, further work will 
use the internal Babel network to run the Ppr+Freq 
method in an attempt to find a way to enrich the 
semantic network obtained for each target sentence 
to disambiguate. On top of that, we plan to 
compare Ppr (Agirre and Soroa, 2009) with 
Ppr+Freq using the Task12 dataset. 
Availability of our Resource 
In case researchers would like to use our resource, 
it is available at the GPLSI5 home page or by 
contacting us via email. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 12th 
conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. 
                                                     
4 http://www.cs.york.ac.uk/semeval-
2013/task12/index.php?id=results 
5 http://gplsi.dlsi.ua.es/ 
247
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; A. 
Gonz?lez; R. Estrada; Y. Casta?eda; S. V?zquez; A. 
Montoyo and R. Mu?oz. UMCC_DLSI: 
Multidimensional Lexical-Semantic Textual 
Similarity. {*SEM 2012}: The First Joint Conference 
on Lexical and Computational Semantics -- Volume 
1: Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Guti?rrez, Y. An?lisis Sem?ntico Multidimensional 
aplicado a la Desambiguaci?n del Lenguaje Natural. 
Departamento de Lenguajes y Sistemas Inform?ticos. 
Alicante, Alicante, 2012. 189. p. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, RANLP 
2011 Organising Committee, 2011b. 233--239 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Sentiment 
Classification Using Semantic Features Extracted 
from WordNet-based Resources. Proceedings of the 
2nd Workshop on Computational Approaches to 
Subjectivity and Sentiment Analysis (WASSA 
2.011), Portland, Oregon., Association for 
Computational Linguistics, 2011c. 139--145 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Word Sense 
Disambiguation: A Graph-Based Approach Using N-
Cliques Partitioning Technique. en:  Natural 
Language Processing and Information Systems. 
MU?OZ, R.;MONTOYO, A.et al Springer Berlin / 
Heidelberg, 2011d. 6716: 112-124.p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. A graph-
Based Approach to WSD Using Relevant Semantic 
Trees and N-Cliques Model. CICLing 2012, New 
Delhi, India, 2012. 225-237 p.  
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Laparra, E.; G. Rigau and M. Cuadros. Exploring the 
integration of WordNet and FrameNet. Proceedings 
of the 5th Global WordNet Conference (GWC'10), 
Mumbai, India, 2010.  
Magnini, B. and G. Cavaglia. Integrating Subject Field 
Codes into WordNet. Proceedings of Third 
International Conference on Language Resources and 
Evaluation (LREC-2000), 2000. 1413--1418 p.  
Mihalcea, R. Unsupervised large-vocabulary word sense 
disambiguation with graph-based algorithms for 
sequence data labeling. Proceedings of HLT05, 
Morristown, NJ, USA., 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Navigli, R.; D. Jurgens and D. Vannella. SemEval-2013 
Task 12: Multilingual Word Sense Disambiguation. . 
Proceedings of the 7th International Workshop on 
Semantic Evaluation (SemEval 2013), in conjunction 
with the Second Joint Conference on Lexical and 
Computational Semantics (*SEM 2013), Atlanta, 
Georgia, 2013.  
Navigli, R. and S. P. Ponzetto. BabelNet: Building a 
Very Large Multilingual Semantic Network. 
Proceedings of the 48th Annual Meeting of the 
Association for Computational Linguistics, Uppsala, 
Sweden, Association for Computational Linguistics, 
2010. 216--225 p.  
Navigli, R. and S. P. Ponzetto BabelNet: The automatic 
construction, evaluation and application of a wide-
coverage multilingual semantic network Artif. Intell., 
2012, 193: 217-250. 
Navigli, R. and P. Velardi Structural Semantic 
Interconnections: A Knowledge-Based Approach to 
Word Sense Disambiguation IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 2005, 
27(7): 1075-1086. 
Sinha, R. and R. Mihalcea. Unsupervised Graph-based 
Word Sense Disambiguation Using Measures of 
Word Semantic Similarity. Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC 2007), Irvine, CA, 2007. 
248
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Tsatsaronis, G.; M. Vazirgiannis and I. 
Androutsopoulos. Word sense disambiguation with 
spreading activation networks generated from 
thesauri. IJCAI, 2007.  
Vossen, P. EuroWordNet: A Multilingual Database with 
Lexical Semantic Networks.  Dordrecht, Kluwer 
Academic Publishers, 1998.  
Zouaq, A.; M. Gagnon and B. Ozell. A SUMO-based 
Semantic Analysis for Knowledge Extraction. 
Proceedings of the 4th Language & Technology 
Conference, Pozna?, Poland, 2009.  
 
 
249
