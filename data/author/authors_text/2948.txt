Polarization and abstraction of grammatical formalisms as
methods for lexical disambiguation
Guillaume Bonfante and Bruno Guillaume and Guy Perrier
LORIA - UMR 7503,
Campus Scientifique, B.P. 239,
F-54506 Vand?uvre le`s Nancy CEDEX
{Guillaume.Bonfante, Bruno.Guillaume, Guy.Perrier}@loria.fr
Abstract
In the context of lexicalized grammars, we
propose general methods for lexical disam-
biguation based on polarization and ab-
straction of grammatical formalisms. Polar-
ization makes their resource sensitivity ex-
plicit and abstraction aims at keeping essen-
tially the mechanism of neutralization be-
tween polarities. Parsing with the simpli-
fied grammar in the abstract formalism can
be used efficiently for filtering lexical selec-
tions.
Introduction
There is a complexity issue if one consider ex-
act parsing with large scale lexicalized gram-
mars. Indeed, the number of way of associating
to each word of a sentence a corresponding ele-
mentary structure?a tagging of the sentence?
is the product of the number of lexical entries for
each word. The procedure may have an expo-
nential complexity in the length of the sentence.
In order to filter taggings, we can use proba-
bilistic methods (Joshi and Srinivas, 1994) and
keep only the most probable ones; but if we
want to keep all successful taggings, we must
use exact methods. Among these, one consists
in abstracting information that is relevant for
the filtering process, from the formalism F used
for representing the concerned grammar G. In
this way, we obtain a new formalism Fabs which
is a simplification of F and the grammar G is
translated into a grammar abs(G) in the ab-
stract framework Fabs. From this, disambiguat-
ing with G consists in parsing with abs(G). The
abstraction is relevant if parsing eliminates a
maximum of bad taggings at a minimal cost.
(Boullier, 2003) uses such a method for Lexical-
ized Tree Adjoining Grammars (LTAG) by ab-
stracting a tree adjoining grammar into a con-
text free grammar and further abstracting that
one into a regular grammar. We also propose
to apply abstraction but after a preprocessing
polarization step.
The notion of polarity comes from Categorial
Grammars (Moortgat, 1996) which ground syn-
tactic composition on the resource sensitivity of
natural languages and it is highlighted in Inter-
action Grammars (Perrier, 2003), which result
from refining and making Categorial Grammars
more flexible.
Polarization of a grammatical formalism F
consists in adding polarities to its syntactic
structures to obtain a polarized formalism Fpol
in which neutralization of polarities is used for
controlling syntactic composition. In this way,
the resource sensitivity of syntactic composition
is made explicit. (Kahane, 2004) shows that
many grammatical formalisms can be polarized
by generalizing the system of polarities used in
Interaction Grammars.
To abstract a grammatical formalism, it is in-
teresting to polarize it before because polarities
allow original methods of abstraction.
The validity of our method is based on a con-
cept of morphism (two instances of which being
polarization and abstraction) which character-
izes how one should transport a formalism into
another.
In sections 1 and 2, we present the conceptual
tools of grammatical formalism and morphism
which are used in the following.
In section 3, we define the operation of polar-
izing grammatical formalisms and in section 4,
we describe how polarization is used then for
abstracting these formalisms.
In section 5, we show how abstraction of
grammatical formalisms grounds methods of
lexical disambiguation, which reduce to pars-
ing in simplified formalisms. We illustrate our
purpose with an incremental and a bottom-up
method.
In section 6, we present some experimental
results which illustrate the flexibility of the ap-
proach.
1 Characterization of a grammatical
formalism
Taking a slightly modified characterization of
polarized unification grammars introduced by
(Kahane, 2004) we define a grammatical formal-
ism F (not necessarily polarized) as a quadruple
?StructF ,SatF ,PhonF ,RulesF ?:
1. StructF is a set of syntactic structures
which are graphs1 in which each edge
and vertex may be associated with a la-
bel representing morpho-syntactic informa-
tion; we assume that the set of labels asso-
ciated with F is equipped with subsump-
tion, a partial order denoted v, and with
unification, an operation denoted unionsq, such
that, for any labels l and l?, either l unionsq l? is
not defined, which is denoted l unionsq l? = ?, or
l unionsq l? is the least upper bound of l and l?2;
2. SatF is a subset of StructF , which repre-
sents the saturated syntactic structures of
grammatical sentences;
3. PhonF is a function that projects every
element of SatF in the sentence that has
this element as its syntactic structure.
4. RulesF is a set of composition rules be-
tween syntactic structures. Every element
of RulesF is a specific method for super-
posing parts of syntactic structures; this
method defines the characteristics of the
parts to be superposed and the unification
operation between their labels. Notice that
we do not ask rules to be deterministic.
The composition rules of syntactic structures,
viewed as superposition rules, have the funda-
mental property of monotonicity: they add in-
formation without removing it. Hence, the defi-
nition above applies only to formalisms that can
be expressed as constraint systems in opposition
to transformational systems.
Let us give some examples of grammatical for-
malisms that comply with the definition above
by examining how they do it.
? In LTAG, StructLTAG represents the set
of derived trees, SatLTAG the set of de-
rived trees with a root in the category
sentence and without non terminal leaves.
1Usually trees or directed acyclic graphs.
2The least upper bound of l and l? can exist and, at
the same time, l unionsq l? be not defined; if the operation of
unification is defined everywhere, the set of labels is a
semi-lattice.
The projection PhonLTAG is the canoni-
cal projection of a locally ordered tree on
its leaves. Finally, RulesLTAG is made
up of two rules: substitution and adjunc-
tion. To view adjunction as a superposition
rule, we resort to the monotone presenta-
tion of LTAG with quasi-trees introduced
by (Vijay-Shanker, 1992).
? In Lambek Grammars (LG), StructLG
is the set of partial proofs and these
proofs can be represented in the form
of incomplete Lambek proof nets labelled
with phonological terms (de Groote, 1999).
SatLG represents the set of complete proof
nets with the category sentence as their
conclusion and with syntactic categories
labelled with words as their hypotheses.
The projection PhonLG returns the label
of the conclusion of complete proof nets.
RulesLG is made up of two rules: a binary
rule that consists in identifying two dual
atomic formulas of two partial proof nets
by means of an axiom link and a unary rule
that consists in the same operation but in-
side the same partial proof net.
Now, inside a formalism defined as above, we
can consider particular grammars:
A grammar G of a formalism F is a
subset G ? StructF of its elementary
syntactic structures.
A grammar is lexicalized if every element of G
is anchored by a word in a lexicon. In LTAG, G
is constituted of its initial and auxiliary trees.
In LG, G is constituted of the syntactic trees of
the formulas representing syntactic categories of
words as hypotheses plus a partial proof net an-
chored by the period and including a conclusion
in the category sentence.
From a grammar G defined in a formalism
F , we build the set D(G) of its derived syntac-
tic structures by applying the rules of RulesF
recursively from the elements of G. The lan-
guage generated by the grammar is the projec-
tion L(G) = PhonF (SatF ? D(G)).
2 Morphisms between grammatical
formalisms
Polarization and abstraction can be defined
from a more general notion of morphism be-
tween grammatical formalisms. A morphism
from a grammatical formalism C to a grammat-
ical formalism A is a function f from StructC
to StructA with the following properties3:
(i) f(SatC) ? SatA;
(ii) ?S ? SatC ,PhonA(f(S)) = PhonC(S);
(iii) if S1, . . . , Sn are composed into a struc-
ture S in C by means of rules of RulesC ,
then f(S1), . . . , f(Sn) can be composed
into the structure f(S) by means of rules
of RulesA.
Given such a morphism f and a grammar G
in C, the image of G by f denoted f(G) is
the grammar?in A?induced by the morphism.
The three properties of morphism guarantee
that the language generated by any grammar
G of C is a subset of the language generated by
f(G). In other words, L(G) ? L(f(G)).
We propose to use the notion of morphism in
two ways:
? for polarizing grammatical formalisms and
in this case, morphisms are isomorphisms;
grammars are transposed from a formalism
to another formalism with the same gener-
ative power; in other words, with the pre-
vious notations: L(G) = L(f(G));
? for abstracting grammatical formalisms
and this case, the transposition of gram-
mars by morphisms entails simplification of
grammars and extension of the generated
languages; we have only: L(G) ? L(f(G)).
An example of the use of abstraction for lex-
ical disambiguation may be found in (Boul-
lier, 2003)4. We propose to link polarization
with abstraction because polarities allow origi-
nal methods of abstraction. Polarization is used
as a preprocessing step before the application of
these methods.
3 Polarization of grammatical
formalisms
The goal of polarizing a grammatical formal-
ism is to make explicit the resource sensitiv-
ity that is hidden in syntactic composition, by
adding polarities to the labels of its structures.
When morpho-syntactic labels become polar-
ized in syntactic structures, they get the status
3An elegant definition of morphism could be given
in a category-theoretical framework but we have chosen
here a more elementary definition.
4Our definition of morphism must be slightly ex-
tended for embedding the proposal of (Boullier, 2003).
of consumable resources: a label that is asso-
ciated with the polarity + becomes an avail-
able resource whereas a label that is associated
with the polarity ? becomes an expected re-
source; both combine for producing a saturated
resource associated with the polarity $; labels
associated with the polarity = are neutral in
this process. In a polarized formalism, the sat-
urated structures are those that have all labels
associated with the polarity = or $. We call
them neutral structures. The composition of
structures is guided by a principle of neutraliza-
tion: every positive (negative) label must unify
with a negative (positive) label.
The polarization of a formalism must pre-
serve its generative power: the language that
is generated by a polarized grammar must be
the same as that generated by the initial non-
polarized grammar. This property of (weak and
even strong) equivalence is guaranteed if the
polarized formalism is isomorphic to the non-
polarized formalism from which it stems. For-
mally, given a grammatical formalism F , any
formalism Fpol with a morphism pol : F ? Fpol
is a polarization of F if:
(i) For any structure S ? StructF , pol(S)
results from associating each label of S
with one of the polarities: +, ?, =, $;
in others words, labels of Fpol are pairs
(p, l) with p a polarity and l a label of
F . The set of polarities {+, ?, =, $} is
equipped with the operation of unification
and the subsumption order defined by
Figure 1. The operations of subsumption
and unification on pairs are the pointwise
operations. That is, for any pairs (p, l)
and (p?, l?),
(p, l)v(p?, l?) iff pvp? and lvl?
(p, l)unionsq(p?, l?) = (punionsqp?, lunionsql?)
(ii) SatFpol is constituted of the neutral struc-
tures of StructFpol .
(iii) pol is an isomorphism whose inverse mor-
phism is the function that ignores polar-
ities and keeps invariant the rest of the
structure.
Let us illustrate our purpose by taking again
our two examples of formalisms.
? For LTAG (see figure 2), pol consists in
labelling the root of elementary syntactic
trees with the polarity + and their non ter-
minal leaves (substitution and foot nodes)
? + = $
? $ ?
+ $ +
= ? + = $
$ $
=
? ?
+ ?
? ?
$
Figure 1: unification and subsumption between
polarities
pol destr
N
N*Adj
red
N?
N+
N+
Adj N?
red
 N+, N+, N?, N? 
 red , Adj
Figure 2: Syntactic structures associated
with the adjective red in LTAG, LTAGpol,
(LTAGpol)destr
with the polarity ?. In every pair of quasi-
nodes, the top quasi-node is labelled with
the polarity ? and the bottom quasi-node
is labelled with the polarity +. With re-
spect to the classical presentation of LTAG,
initial trees must be completed by an axiom
with two nodes of the type sentence: a root
with the polarity = and its unique daugh-
ter with the polarity ?. In this way, pol
establishes a perfect bijection between the
saturated structures of LTAG and the neu-
tral structures of LTAGpol. The rules of ad-
junction and substitution of RulesLTAGpol
mimic the corresponding rules in LTAG,
taking into account polarities. We add a
third composition rule, a unary rule which
identifies the two quasi-nodes of a same
pair. It is routine to check that pol is a
polarisation.
? In LG(see figure 3), polarization is already
present explicitly in the formalism: nega-
tive formulas and sub-formulas are input
formulas, hypotheses whereas positive for-
mulas and sub-formulas are output formu-
las, conclusions.
4 Abstraction of polarized
grammatical formalisms
The originality of abstracting polarized for-
malisms is to keep a mechanism of neutraliza-
tion between opposite polarities at the heart of
the abstract formalism. Furthermore, we can
choose different levels of abstraction by keeping
more or less information from the initial formal-
S+ NP?
eats
NP?(NP \ S ) / NP
eats
pol S+, NP?, NP?
eatsdestr
Figure 3: Syntactic structures associated
with the transitive verb eats in LG, LGpol,
(LGpol)destr
ism.
As an example, we propose a high degree ab-
straction, destructuring. Destructuring a polar-
ized formalism consists in ignoring the struc-
ture from the initial syntactic objects to keep
merely the multisets of polarized labels. For-
mally, given a polarized formalism P , we define
the formalism Pdestr as follows:
? Any element M of StructPdestr is a multi-
set of labels. All elements of M are labels
of P , except one exactly, the anchor, which
is a neutral string.
? SatPdestr is made up of multisets containing
only neutral and saturated labels;
? The projection PhonPdestr returns the la-
bel of the anchor.
? RulesPdestr has two neutralization rules. A
binary rule takes two multisets M1 and M2
from StructPdestr as inputs; two unifiable
labels +l1 ? M1(M2) and ?l2 ? M2(M1)
are selected. The rule returns the union of
M1 and M2 in which +l1 and ?l2 are uni-
fied and the two anchors are concatenated.
The only change with the unary rule is that
this operates inside the same multiset.
A morphism destr is associated to Pdestr (see
figure 2 and 3): it takes any structure S from
StructP as input and returns the multiset of its
labels with an additionnal anchor. This anchor
is the neutral string PhonP (S) if this one is
defined.
An important property of Pdestr is that it is
not sensitive to word order: if a sentence is gen-
erated by a particular grammar of Pdestr, by
permuting the words of the sentence, we ob-
tain another sentence generated by the gram-
mar. Destructuring is an abstraction that ap-
plies to any polarized formalism but we can de-
sign abstractions with lower degree which are
specific to particular formalisms (see Section 6).
5 Application to lexical
disambiguation
Abstraction is the basis for a general method
of lexical disambiguation. Given a lexicalized
grammar G in a concrete formalism C, we con-
sider a sentence w1 . . . wn. For each 1 ? i ? n,
let the word wi have the following entries in the
lexicon of G: Si,1, Si,2 . . . Si,mi . A tagging of
the sentence is a sequence S1,k1 , S2,k2 . . . Sn,kn .
We suppose now that we have given an abstrac-
tion morphism abs : C ? Cabs. As L(G) ?
L(abs(G)), any tagging in abs(G) which has no
solutions comes from a bad tagging in G. As
a consequence, the methods we develop try to
eliminate such bad taggings by parsing the sen-
tence w1w2 . . . wn within the grammar abs(G).
We propose two procedures for parsing in the
abstract formalism:
? an incremental procedure which is specific
to the destructuring abstraction,
? a bottom-up procedure which can apply to
various formalisms and abstractions.
5.1 Incremental procedure
We choose polarization followed by destructur-
ing as abstraction. In other words: abs =
destr ?pol. Let us start with the particular case
where unification of labels in C reduces to iden-
tity. In this case, parsing inside the formalism
Cabs is greatly simplified because composition
rules reduce to the neutralization of two labels
+l and ?l. As a consequence, parsing reduces
to a counting of positive and negative polarities
present in the selected tagging for every label
l: every positive label counts for +1 and ev-
ery negative label for ?1, the sum must be 0;
since this counting must be done for every pos-
sible tagging and for every possible label, it is
crucial to factorize counting. For this, we use
automata, which drastically decrease the space
(and also the time) complexity.
For every label l of C that appears with a
polarity + or ? in the possible taggings of the
sentence w1w2 . . . wn, we build the automaton
Al as follows. The set of states of Al is [0..n]?Z.
For any state (i, c), i represents the position at
the beginning of the word wi+1 in the sentence
and c represents a positive or negative count of
labels l. The initial state is (0, 0), and the final
state is (n, 0). Transitions are labeled by lexicon
entries Si,j . Given any Si,j , there is a transition
(i? 1, x)
Si,j
?? (i, y) if y is the sum of x and the
count of labels l in the multi-set destr(Si,j).
Reaching state (i, c) from the initial state
(0, 0) means that
(a) the path taken is of the form
S1,j1 , S2,j2 , . . . , Si,ji , that is a tagging
of the first i words,
(b) c is the count of labels l present
in the union of the multi-sets
abs(S1,j1), abs(S2,j2), . . . , abs(Si,ji).
As a consequence, any path that leads to the fi-
nal state corresponds to a neutral choice of tag-
ging for this label l.
The algorithm is now simply to construct for
each label l the automaton Al and to make the
intersection A =
?
l?LabelsAl of all these au-
tomata. The result of the disambiguation is
the set of paths from the initial state to the fi-
nal state described by this intersection automa-
ton. Notice that at each step of the construction
of the intersection, one should prune automata
from their blind states to ensure the efficiency
of the procedure.
Now, in the general case, unification of labels
in F does not reduce to identification, which in-
troduces nondeterminism in the application of
the neutralization rule. Parsing continues to re-
duce to counting polarities but now the counting
of different labels is nondeterministic and inter-
dependent. For instance, consider the multiset
{+a, +b, ?aunionsq+b} of three different elements.
If we count the number of a, we find 0 if we
consider that +a is neutralized by ?aunionsqb and
+1 otherwise; in the first case, we find +1 for
the count of b and in the second case, we find 0.
Interdependency between the counts of different
labels is very costly to be taken into account and
in the following we ignore this property; there-
fore, in the previous exemple, we consider that
the count of a is 0 or +1 and the count of b is
also 0 or +1 independently from the first one.
For expressing this, given a label l of F and a
positive or negative label l? of Fpol, we define
Pl(l?) as a segment of integers, which represents
the possible counts of l found in l?, as follows:
? if l? is positive, then Pl(l?) =?
?
?
J1, 1K if lvl?
J0, 0K if lunionsql? = ?
J0, 1K otherwise
? if l? is negative, then Pl(l?) =?
?
?
J?1,?1K if lvl?
J0, 0K if lunionsql? = ?
J?1, 0K otherwise
We generalize the function Pl to count the num-
ber ol labels l present in a multi-set abs(S):
Pl(S) = Jinf, supKwith:
inf =
?
l??abs(S) min(Pl(l
?))
sup =
?
l??abs(S) max(Pl(l
?))
The method of disambiguation using au-
tomata presented above is still valid in the gen-
eral case with the following change in the defini-
tion of a transition in the automaton Al: given
any Si,j , there is a transition (i?1, x)
Si,j
?? (i, y)
if y is the sum of x and some element of Pl(Si,j).
With this change, the automaton Al becomes
nondeterministic.
The interest of the incremental procedure is
that it is global to the sentence and that it ig-
nores word order. This feature is interesting for
generation where the question of disambigua-
tion is crucial. This advantage is at the same
time its drawback when we need to take word
order and locality into account. Under this an-
gle, the bottom-up procedure, which will be pre-
sented below, is a good complement to the in-
cremental procedure.
5.2 Bottom-up procedure
We propose here another procedure adapted to
a formalism C with the property of projectiv-
ity. Because of this property, it is possible to
use a CKY-like algorithm in the abstract for-
malism Cabs. To parse a sentence w1w2 ? ? ?wn,
we construct items of the form (i, j, S) with S
an element of StructCabs and i and j such that
wi+1 . . . wj represents the phonological form of
S. We assume that Rules(Cabs) has only unary
and binary rules. Then, three rules are used for
filling the chart:
initialization: the chart is initialized with
items in the form (i, i+ 1, abs(Si+1,k));
reduction: if the chart contains an item
(i, j, S), we add the item (i, j, S?) such that
S? is obtained by application of a unary
composition rule to S;
concatenation: if the chart contains two item
(i, j, S) and (j, k, S?), we add the item
(i, k, S??) such that S?? is obtained by ap-
plication of a binary composition rule to S
and S?.
Parsing succeeds if the chart contains an item
in the form (0, n, S0) such that S0 is an element
of SatCabs . From such an item, we can recover
all taggings that are at its source if, for every
application of a rule, we keep a pointer from the
conclusion to the corresponding premisses. The
other taggings are eliminated.
6 Experiments
In order to validate our methodology, we have
written two toy English grammars for the LG
and the LTAG formalisms. The point of the
tests we have done is to observe the performance
of the lexical disambiguation on highly ambigu-
ous sentences. Hence, we have chosen the three
following sentences which have exactly one cor-
rect reading:
(a) the saw cut the butter.
(b) the butter that the present saw cut
cooked well.
(c) the present saw that the man thinks that
the butter was cut with cut well.
For each test below, we give the execution
time in ms (obtained with a PC Pentium III,
600Mhz) and the performance (number of se-
lected taggings / number of possible taggings).
6.1 Incremental procedure
The incremental procedure (IP) results are
given in Figure 4:
LG LTAG
ms perf. ms perf.
(a) 1 3/36 3 3/96
(b) 42 126/12 960 40 126/48 384
(c) 318 761/248 832 133 104/1 548 288
Figure 4: IP with destr ? pol
One may notice that the number of selected
taggings/total taggings decrease with the length
of the sentence. This is a general phenomenon
explained in (Bonfante et al, 2003).
6.2 Bottom-up procedure
The execution time for the bottom-up proce-
dure (BUP) grows quickly with the ambiguity
of the sentence. So this procedure is not very
relevant if it is used alone. But, if it is used as
a second step after the incremental procedure,
it gives interesting results. In Figure 5, we give
the results obtained with the destr abstraction.
Some other experiments show that we can im-
LG LTAG
ms perf. ms perf.
(a) 2 3/36 9 3/96
(b) 154 104/12 960 339 82/48 384
(c) 2 260 266/248 832 1 821 58/1 548 288
Figure 5: IP + BUP with destr ? pol
prove performance or execution time with spe-
cific methods for each formalism which are less
abstract than destr.
6.2.1 Tailor-made abstraction for LG
For the formalism LG, instead of complete de-
structuring, we keep some partial structural in-
formation to the polarized label. As the for-
malism is projective, we record some constraints
about the continuous segment associated with a
polarity. In this way, some neutralizations pos-
sible in the destr abstraction are not possible
anymore if the two polarities have incompatible
constraints (i.e. lie in different segments). This
new morphism is called proj. The execution
time is problematic but it might be controlled
with a bound on the number of polarities in ev-
ery multiset5 (see Figure 6)
LG
sentence Time(ms) Perf.
(a) 2 1/36
(b) 168 5/12 960
(c) with bound 6 2 364 3/248 832
Figure 6: IP + BUP with proj ? pol
Without bound for sentence (c), the running
time is over 1 min.
6.2.2 Tailor-made abstraction for LTAG
For LTAG: a possible weaker abstraction (called
ltag) consists in keeping, with each polarity,
some information of the LTAG tree it comes
from. Rather than bags where all polarized la-
bels are brought together, we have four kind
of polarized pieces: (1) a positive label coming
from the root of an initial tree, (2) a negative
label coming from a substitution node, (3) a
couple of dual label coming from the root and
the foot of an auxiliary tree or (4) a couple of
dual label coming from the two parts of a quasi-
node. Rules in this formalism reflect the two
operations of LTAG; they do not mix polarities
relative to adjunction with polarities relative to
substitution. Figure 7 shows that the execution
time is improved (wrt. Figure 5).
Conclusion
The examples we have presented above should
not be used for a definitive evaluation of partic-
ular methods, but more as a presentation of the
flexibility of our program: polarizing grammati-
cal formalisms for abstracting them and parsing
5This bound expresses the maximum number of syn-
tactic dependencies between a constituent and the others
in a sentence.
LTAG
ms perf.
(a) 6 3/96
(b) 89 58/48 384
(c) 272 54/1 548 288
Figure 7: IP + BUP with ltag ? pol
in the resulting abstract frameworks for disam-
biguating lexical selections. We have presented
one general tool (the destructuring abstraction)
that may apply to various grammatical frame-
work. But we think that abstractions should be
considered for specific frameworks to be really
efficient. One of our purpose is now to try the
various tools we have developped to some large
covering lexicons.
So far, we have not taken into account the tra-
ditional techniques based on probabilities. Our
point is that these should be seen as an other
way of abstracting grammars. Our hope is that
our program is a good way to mix different
methods, probabilistic or exact.
References
G. Bonfante, B. Guillaume, and G Perrier.
2003. Analyse syntaxique e?lectrostatique.
Traitement Automatique des Langues. To ap-
pear.
P. Boullier. 2003. Supertagging: a Non-
Statistical Parsing-Based Approach. In 8th
International Workshop on Parsing Tech-
nologies (IWPT?03), Nancy, France, 2003,
pages 55?66.
P. de Groote. 1999. An algebraic correct-
ness criterion for intuitionistic multiplica-
tive proofnets. Theoretical Computer Sci-
ence, 224:115?134.
A. Joshi and B. Srinivas. 1994. Disambiguation
of super parts of speech (or supertags) : Al-
most parsing. In COLING?94, Kyoto.
S. Kahane. 2004. Grammaires d?unification po-
larise?es. In TALN?2004, Fe`s, Maroc.
M. Moortgat. 1996. Categorial Type Logics. In
J. van Benthem and A. ter Meulen, editors,
Handbook of Logic and Language, chapter 2.
Elsevier.
G. Perrier. 2003. Les grammaires d?interaction.
Habilitation a` diriger des recherches, Univer-
site? Nancy2.
K. Vijay-Shanker. 1992. Using description of
trees in a tree adjoining grammar. Computa-
tional Linguistics, 18(4):481?517.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 153?156
Manchester, August 2008
A Toolchain for Grammarians
Bruno Guillaume
LORIA
INRIA Nancy Grand-Est
Bruno.Guillaume@loria.fr
Joseph Le Roux
LORIA
Nancy Universit?e
Joseph.Leroux@loria.fr
Jonathan Marchand
LORIA
Nancy Universit?e
Jonathan.Marchand@loria.fr
Guy Perrier
LORIA
Nancy Universit?e
Guy.Perrier@loria.fr
Kar
?
en Fort
LORIA
INRIA Nancy Grand-Est
Karen.Fort@loria.fr
Jennifer Planul
LORIA
Nancy Universit?e
Jennifer.Planul@loria.fr
Abstract
We present a chain of tools used by gram-
marians and computer scientists to develop
grammatical and lexical resources from
linguistic knowledge, for various natural
languages. The developed resources are
intended to be used in Natural Language
Processing (NLP) systems.
1 Introduction
We put ourselves from the point of view of re-
searchers who aim at developing formal grammars
and lexicons for NLP systems, starting from lin-
guistic knowledge. Grammars have to represent all
common linguistic phenomena and lexicons have
to include the most frequent words with their most
frequent uses. As everyone knows, building such
resources is a very complex and time consuming
task.
When one wants to formalize linguistic knowl-
edge, a crucial question arises: which mathemat-
ical framework to choose? Currently, there is no
agreement on the choice of a formalism in the sci-
entific community. Each of the most popular for-
malisms has its own advantages and drawbacks. A
good formalism must have three properties, hard to
conciliate: it must be sufficiently expressive to rep-
resent linguistic generalizations, easily readable by
linguists and computationally tractable. Guided
by those principles, we advocate a recent formal-
ism, Interaction Grammars (IGs) (Perrier, 2003),
the goal of which is to synthesize two key ideas,
expressed in two kinds of formalisms up to now:
using the resource sensitivity of natural languages
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as a principle of syntactic composition, which is
a characteristic feature of Categorial Grammars
(CG) (Retor?e, 2000), and viewing grammars as
constraint systems, which is a feature of unifica-
tion grammars such as LFG (Bresnan, 2001) or
HPSG (Pollard and Sag, 1994).
Researchers who develop large lexicons and
grammars from linguistic knowledge are con-
fronted to the contradiction between the necessity
to choose a specific grammatical framework and
the cost of developing resources for this frame-
work. One of the most advanced systems de-
voted to such a task is LKB (Copestake, 2001).
LKB allows grammars and lexicons to be devel-
oped for different languages, but only inside the
HPSG framework, or at most a typed feature struc-
ture framework. Therefore, all produced resources
are hardly re-usable for other frameworks. Our
goal is to design a toolchain that is as much as pos-
sible re-usable for other frameworks than IG.
Our toolchain follows the following architecture
(see Figure 1):
? First, for building grammars, we use XMG
(Section 3.1) which translates the source
grammar into an object grammar.
? IGs that we have developed with XMG are
all lexicalized. Therefore, the object grammar
has to be anchored in a lexicon (Section 3.2)
in order to produce the anchored grammar.
? Then, when analyzing a sentence, we start
with a lexical disambiguation module (Sec-
tion 3.3).
? The resulting lexical selections, presented in
the compact form of an automaton, are finally
sent to the LEOPAR parser (Section 3.4).
153
LEOPAR
source grammar
XMG
object grammar lexicons
anchoring
input sentence
lexical disambiguation
output parse trees
anchored grammar
automaton
parsing
Figure 1: Toolchain architecture
2 Interaction Grammars
IGs (Perrier, 2003) are a grammatical formalism
based on the notion of polarity. Polarities express
the resource sensitivity of natural languages by
modeling the distinction between saturated and un-
saturated syntactic structures. Syntactic composi-
tion is represented as a chemical reaction guided
by the saturation of polarities. In a more precise
way, syntactic structures are underspecified trees
equipped with polarities expressing their satura-
tion state. They are superposed under the con-
trol of polarities in order to saturate them. In
CG, Tree Adjoining Grammars (TAGs) and De-
pendency Grammars, syntactic composition can
also be viewed as a mechanism for saturating po-
larities, but this mechanism is less expressive be-
cause node merging is localized at specific places
(root nodes, substitution nodes, foot nodes, ad-
junction nodes . . .). In IGs, tree superposition is
a more flexible way of realizing syntactic compo-
sition. Therefore, it can express sophisticated con-
straints on the environment in which a polarity has
to be saturated. From this angle, IGs are related
to Unification Grammars, such as HPSG, because
tree superposition is a kind of unification, but with
an important difference: polarities play an essen-
tial role in the control of unification.
3 Description of the Toolchain
3.1 The XMG Grammar Compiler
The first piece of software in our toolchain is
XMG
1
(Duchier et al, 2004), a tool used to de-
velop grammars. XMG addresses the issue of de-
signing wide-coverage grammars: it is based on a
distinction between source grammar, written by a
human, and object grammar, used in NLP systems.
XMG provides a high level language for writing
source grammars and a compiler which translates
those grammars into operational object grammars.
XMG is particularly adapted to develop lexical-
ized grammars. In those grammars, parsing a sen-
tence amounts to combining syntactical items at-
tached to words. In order to have an accurate lan-
guage model, it may be necessary to attach a huge
number of syntactical items to some words (verbs
and coordination words, in particular) that describe
the various usages of those words. In this context,
a grammar is a collection of items representing
syntactical behaviors. Those items, although dif-
ferent from each other, often share substructures
(for instance, almost all verbs have a substruc-
ture for subject verb agreement). That is to say,
if a linguist wants to change the way subject-verb
agreement is modeled, (s)he would have to mod-
ify all the items containing that substructure. This
is why designing and maintaining strongly lexical-
ized grammars is a difficult task.
The idea behind the so-called metagrammati-
cal approach is to write only substructures (called
fragments) and then add rules that describe the
combinations (expressed with conjunctions, dis-
junctions and unifications) of those fragments to
obtain complete items.
Fragments may contain syntactic, morpho-
syntactic and semantic pieces of information. An
object grammar is a set of structures containing
syntactic and semantic information, that can be an-
chored using morpho-syntactic information stored
in the interface of the structure (see Section 3.2).
During development and debugging stages, por-
tions of the grammar can be evaluated indepen-
dently. The grammar can be split into various mod-
ules that can be shared amongst grammars. Fi-
nally, graphical tools let the users explore the in-
heritance hierarchy and the partial structures be-
fore complete evaluation.
1
XMG is freely available under the CeCILL license at
http://sourcesup.cru.fr/xmg
154
XMG is also used to develop TAGs (Crabb?e,
2005) and it can be easily extended to other gram-
matical frameworks based on tree representations.
3.2 Anchoring the Object Grammar with a
Lexicon
The tool described in the previous section builds
the set of elementary trees of the grammar. The
toolchain includes a generic anchoring mechanism
which allows to use formalism independent lin-
guistic data for the lexicon part.
Each structure produced by XMG comes with
an interface (a two-level feature structure) which
describes morphological and syntactical con-
straints used to select words from the lexicon. Du-
ally, in the lexicon, each inflected form of the natu-
ral language is described by a set of two-level fea-
ture structures that contain morphological and syn-
tactical information.
If the interface of an unanchored tree unifies
with some feature structure associated with w in
the lexicon, then an anchored tree is produced for
the word w.
The toolchain also contains a modularized lexi-
con manager which aims at easing the integration
of external and formalism independent resources.
The lexicon manager provides several levels of lin-
guistic description to factorize redundant data. It
also contains a flexible compilation mechanism to
improve anchoring efficiency and to ease lexicon
debugging.
3.3 Lexical Disambiguation
Neutralization of polarities is the key mechanism
in the parsing process as it is used to control syn-
tactic composition. This principle can also be used
to filter lexical selections. For a input sentence, a
lexical selection is a choice of an elementary tree
from the anchored grammar for each word of the
sentence.
Indeed, the number of possible lexical selec-
tions may present an exponential complexity in
the length of the sentence. A way of filter-
ing them consists in abstracting some information
from the initial formalism F to a new formalism
F
abs
. Then, parsing in F
abs
allows to eliminate
wrong lexical selections at a minimal cost (Boul-
lier, 2003). (Bonfante et al, 2004) shows that po-
larities allow original methods of abstraction.
Following this idea, the lexical disambiguation
module checks the global neutrality of every lex-
ical selection for each polarized feature: a set of
trees bearing negative and positive polarities can
only be reduced to a neutral tree if the sum of the
negative polarities for each feature equals the sum
of its positive polarities.
Counting the sum of positive and negative fea-
tures can be done in a compact way by using an au-
tomaton. This automaton structure allows to share
all paths that have the same global polarity bal-
ance (Bonfante et al, 2004).
3.4 The LEOPAR Parser
The next piece of software in our toolchain is a
parser based on the IGs formalism
2
. In addition
to a command line interface, the parser provides
an intuitive graphical user interface. Parsing can
be highly customized in both modes. Besides, the
processed data can be viewed at each stage of the
analysis via the interface so one can easily check
the behavior of the grammar and the lexicons in
the parsing process.
The parsing can also be done manually: one first
chooses a lexical selection of the sentence given
by the lexer and then proceeds to the analysis by
neutralizing nodes from the selection. This way,
the syntactic composition can be controlled by the
user.
4 Results
Our toolchain has been used first to produce a large
coverage French IG. Most of the usual syntactical
constructions of French are covered. Some non
trivial constructions covered by the grammar are,
for instance: coordination, negation (in French,
negation is expressed with two words with com-
plex placement rules), long distance dependencies
(with island constraints). The object grammar con-
tains 2,074 syntactic structures which are produced
by 455 classes in the source grammar.
The French grammar has been tested on the
French TSNLP (Test Suite for the Natural Lan-
guage Processing) (Lehmann et al, 1996); this test
suite contains around 1,300 grammatical sentences
and 1,600 ungrammatical ones. The fact that our
grammar is based on linguistic knowledge ensures
a good coverage and greatly limits overgeneration:
88% of the grammatical sentences are correctly
parsed and 85% of the ungrammatical sentences
are rejected by our grammar.
2
LEOPAR is freely available under the CeCILL license at
http://www.loria.fr/equipes/calligramme/
leopar
155
A few months ago, we started to build an En-
glish IG. The modularity of the toolchain was an
advantage to build this grammar by abstracting the
initial grammar and then specifying the abstract
kernel for English. The English TSNLP has been
used to test the new grammar: 85% of the gram-
matical sentences are correctly parsed and 84%
of the ungrammatical sentences are rejected. It is
worth noting that those scores are obtained with a
grammar that is still being developed .
5 Future work
The toolchain we have presented here aims at pro-
ducing grammars and lexicons with large cover-
age from linguistic knowledge. This justifies the
choice of discarding statistical methods in the first
stage of the toolchain development: in the two
steps of lexical disambiguation and parsing, we
want to keep all possible solutions without dis-
carding even the less probable ones. Now, in a
next future, we have the ambition of using the
toolchain for parsing large raw corpora in differ-
ent languages.
For French, we have a large grammar and a large
lexicon, which are essential for such a task. The in-
troduction of statistics in the two modules of lexi-
cal disambiguation and parsing will contribute to
computational efficiency. Moreover, we have to
enrich our parsing strategies with robustness. We
also ambition to integrate semantics into grammars
and lexicons.
Our experience with English is a first step to take
multi-linguality into account. The crucial point
is to make our grammars evolve towards an even
more multi-lingual architecture with an abstract
kernel, common to different languages, and differ-
ent specifications of this kernel for different lan-
guages, thus following the approach of the Gram-
matical Framework (Ranta, 2004).
Finally, to make the toolchain evolve towards
multi-formalism, it is first necessary to extend
XMG for more genericity; there is no fundamental
obstacle to this task. Many widespread formalisms
can then benefit from our original methods of lex-
ical disambiguation and parsing, based on polari-
ties. (Kahane, 2006) presents the polarization of
several formalisms and (Kow, 2007) shows that
this way is promising.
References
Bonfante, G., B. Guillaume, and G. Perrier. 2004. Po-
larization and abstraction of grammatical formalisms
as methods for lexical disambiguation. In CoL-
ing?2004, 2004, pages 303?309, Geneva, Switzer-
land.
Boullier, P. 2003. Supertagging: A non-statistical
parsing-based approach. In IWPT 03, pages 55?65,
Nancy, France.
Bresnan, J. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
Copestake, A. 2001. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Crabb?e, B. 2005. Repr?esentation informatique de
grammaires fortement lexicalis?ees : application `a la
grammaire d?arbres adjoints. Phd thesis, Universit?e
Nancy 2.
Duchier, D., J. Le Roux, and Y. Parmentier. 2004.
The metagrammar compiler : A NLP Application
with a Multi-paradigm Architecture. In Second
International Mozart/Oz Conference - MOZ 2004,
Charleroi, Belgium.
Kahane, S. 2006. Polarized unification grammars.
In 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 137?144,
Sydney, Australia.
Kow, E. 2007. Surface realisation: ambiguity and de-
terminism. Phd thesis, Universit?e Nancy 2.
Lehmann, S., S. Oepen, S. Regnier-Pros, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,
E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and
D. Arnold. 1996. TSNLP ? Test Suites for Natu-
ral Language Processing. In CoLing 1996, Kopen-
hagen.
Perrier, G. 2003. Les grammaires d?interaction. Ha-
bilitation thesis, Universit?e Nancy 2.
Pollard, C.J. and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ranta, A. 2004. Grammatical Framework: A Type-
Theoretical Grammar Formalism. Journal of Func-
tional Programming, 14(2):145?189.
Retor?e, C. 2000. The Logic of Categorial Grammars.
ESSLI?2000, Birmingham.
156
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 17?24,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
PrepLex: a lexicon of French prepositions for parsing
Kare?n Fort
Calligramme and TALARIS projects
LORIA/INRIA Lorraine / Nancy, France
Karen.Fort@loria.fr
Bruno Guillaume
Calligramme project
LORIA/INRIA Lorraine / Nancy, France
Bruno.Guillaume@loria.fr
Abstract
PrepLex is a lexicon of French prepositions
which provides all the syntactic information
needed for parsing. It was built by compar-
ing and merging several authoritative lexical
sources. This lexicon also includes infor-
mation about the prepositions or classes of
prepositions that appear in French verb sub-
categorization frames. This resource has
been developed as a first step in making cur-
rent French preposition lexicons available
for effective natural language processing.
1 Introduction
When defining lexical entry classes according to cat-
egories, an obvious distinction appears between two
types of classes. First, the closed classes, compris-
ing elements which can be exhaustively enumerated,
for example pronouns or determiners. Second, open
classes for which it is impossible to list all the el-
ements (for example, they may vary according to
the domain). The four main open classes are nouns,
verbs, adjectives and adverbs. The lexicon construc-
tion methodology has to be adapted according to the
type of class that is being dealt with.
The status of the class of prepositions is difficult
to determine. A priori, prepositions may seem to be
a closed class, with elements which can be enumer-
ated. In practice, however, a comparison of the dif-
ferent available resources shows that it is not an easy
task to exhaustively list prepositions. Besides, they
represent more than 14% of French lemma tokens.1
1see for example, on a newspaper corpus:
A complete lexicon for parsing applications
should contain subcategorization information for
predicative words (Briscoe and Carroll, 1993; Car-
roll and Fang, 2004). This subcategorization infor-
mation often refers to prepositions in the description
of their arguments. Arguments are commonly used
with a particular preposition (for example compter
sur [count on]) or a set of semantically linked prepo-
sitions (such as aller [go] LOC, where LOC can be
any locative preposition).
For deep parsing, we need to distinguish between
indirect complements, required by the verb, and
adjuncts which do not appear in the verb valence.
The following two examples (1a) and (1b) have
the same surface structure, in which the two
preposition uses for avec can only be distinguished
semantically: in the first case, it introduces an
oblique complement, whereas in the second case,
it introduces an adjunct. This issue can be solved
using finer-grained semantic information.
1a. Jean se bat avec Paul
[Jean fights against Paul]
1b. Jean se bat avec courage
[Jean fights with courage]
This distinction leads us to allow two different
preposition uses and therefore causes lexical ambi-
guity. In order to limit this ambiguity, it is important
for a lexicon to identify the prepositions which can
have both functions (we will call these ?argument?
prepositions).
https://www.kuleuven.be/ilt/blf/
rechbaselex kul.php\#freq (Selva et al, 2002)
17
Our work aims at providing the community with
a lexicon that can be directly used by a parser. We
focused on syntactic aspects and extended the work
to some semantic elements, like semantically linked
sets of prepositions (as LOC). The generated lexicon
is freely available and is expected to be integrated
into larger resources for French, whether existing or
under development.
Section 2 describes the sources and the compar-
ative methodology we used. Section 3 details the
results of the comparison. Section 4 explains how
the lexicon was created from the above-mentioned
results. Finally, Section 5 shows an example of use
of the lexicon in a parsing application.
2 Methodology
In order to use prepositions for parsing, we need
a large list, containing both garden-variety preposi-
tions and prepositions that appear in verb subcatego-
rization frames.
2.1 Using syntactic lexicons
Obviously, some lexicons already exist which pro-
vide interesting lists of prepositions. This is the
case of Lefff (Sagot et al, 2006), which contains
a long list of prepositions. However, the syntactic
part of the lexicon is still under development and
it provides only few prepositions in verb subcate-
gorization frames. Besides, some prepositions in
Lefff are obsolete or rare. The French-UNL dic-
tionary (Se?rasset and Boitet, 2000) also contains
prepositions, but its coverage is quite limited and
the quality of its entries is not homogeneous. Other
sources present prepositions in verb subcategoriza-
tion frames, but the lists are not quite consistent.
We thus collected, as a first step, prepositions
from a certain number of resources, lexicons and
dictionaries for the garden-variety list, and syntactic
lexicons for the argument prepositions list. Two re-
sources belong to both categories, Lefff and French-
UNL dictionary:
? Lefff (Lexique des Formes Fle?chies du
Franc?ais/French inflected form lexicon (Sagot
et al, 2006)) is a large coverage (more than
110,000 lemmas) French morphological and
syntactic lexicon (see table 1 for an example of
a Lefff syntactic entry).
In its latest public version, 2.2.1, Lefff con-
tains 48 simple prepositions and 164 multiword
prepositions. It also provides information on
verb subcategorization frames, which contain
14 argument prepositions.
? UNL (Universal Networking Lan-
guage (Se?rasset and Boitet, 2000)), is a
French to disambiguated English dictionary for
machine translation, which contains syntactic
information in its French part (see table 1 for a
UNL example entry).
UNL has limited coverage (less than 27,000
lemmas), but it provides, in the English part,
semantic information that we will consider us-
ing in the near future. UNL contains 48 simple
prepositions, among which 12 appear in verb
subcategorization frames.
2.2 Using reference sources
We then completed the list of prepositions using
manually built resources, including lexicons, dictio-
naries and grammars:
? The Grevisse (Grevisse, 1997) grammar, in its
paper version, allowed us to check some intu-
itions concerning the obsolescence or usage of
some prepositions.
? The TLFi (Tre?sor de la langue franc?aise in-
formatise?), that we consulted through the CN-
RTL2, and that offers a slightly different list of
prepositions. In particular, it contains the forms
voici and voila`, that are seldom quoted in the
other available resources.
? Finally, the PrepNet (Saint-Dizier, 2006)
prepositions database was used to check the
completeness of our list as well as the semantic
information provided by other sources.
2.3 Using verb valence dictionaries
We then looked for a way to enrich the list of prepo-
sitions appearing in verb subcategorization frames
in Lefff and UNL, using resources that focus more
particularly on verbs:
2see: http://www.cnrtl.fr
18
Lefff entry for dialoguer avec [to talk to]
dialoguer: suj:sn|sinf|scompl,obja:(a`-sn|avec-sn),objde:(de-sn|de-scompl|de-sinf)
UNL entry for dialoguer avec [to talk to]
[dialoguer] {AUX(AVOIR),CAT(CATV),GP1(AVEC),VAL1(GN)} "have_talks";
DICOVALENCE entry for dialoguer avec [to talk to]
VAL$ dialoguer: P0 PP<avec>
VTYPE$ predicator simple
VERB$ DIALOGUER/dialoguer
NUM$ 29730
EG$ le de?le?gue? des e?tudiants a dialogue? avec le directeur de l?e?cole
TR$ spreken, zich onderhouden, een gesprek hebben, onderhandelen
P0$ qui, je, nous, elle, il, ils, on, celui-ci, ceux-ci
PP_PR$ avec
PP$ qui, lui_ton, eux, celui-ci, ceux-ci, l?un l?autre
LCCOMP$ nous dialoguons, je dialogue avec toi
SynLex entry for adapter avec [to adapt to]
adapter ?<suj:sn,obj:sn,obl:avec-sn>?
Table 1: Description of some entries with the preposition avec [with] in valence dictionaries
? DICOVALENCE, a valence dictionary of
French, formerly known as PROTON (van den
Eynde and Mertens, 2002), which has been
based on the pronominal approach. In version
1.1, this dictionary details the subcategoriza-
tion frames of more than 3,700 verbs (table 1
gives an example of a DICOVALENCE entry).
We extracted the simple and multiword prepo-
sitions it contains (i.e. more than 40), as well
as their associated semantic classes.
? We completed this argument prepositions list
with information gathered from SynLex (Gar-
dent et al, 2006), a syntactic lexicon cre-
ated from the LADL lexicon-grammar ta-
bles (Gross, 1975) (see table 1 for a SynLex
entry).
Using these sources, we conducted a systematic
study of each preposition, checking its presence
in each source, whether in verb subcategorization
frames or not, as well as its associated semantic
class(es). We then grouped the prepositions that ap-
pear both as lexical entries and in verb subcatego-
rization frames.
As multiword prepositions show specific charac-
teristics (in particular, their number) and raise partic-
ular issues (segmentation), we processed them sepa-
rately, using the same methodology.
3 Source comparison results
3.1 Simple prepositions
We thus listed 85 simple prepositions, among which
24 appear in verb subcategorization frames (see ta-
ble 2).
It is noticeable that the different sources use quite
different representations of syntactic information as
shown in table 1. Lefff offers a condensed vision
of verbs, in which valence patterns are grouped into
one single entry, whereas SynLex uses a flatter rep-
resentation without disjunction on syntactic cate-
gories for argument realization or for optional argu-
ments. To summarize, we could say that DICOVA-
LENCE lies somewhere between Lefff and SynLex,
since it uses disjunctive representation but has a finer
description of syntactic information and hence splits
many entries which are collapsed in Lefff.
3.2 Multiword prepositions
We obtained a list of 222 multiword prepositions,
among which 18 appear in verb subcategorization
frames (see table 3). It is to be noticed that only
DICOVALENCE and SynLex contain multiword
prepositions in verb subcategorization frames. As
for Lefff, it provides an impressive list of multiword
19
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL Lefff DVa SynLex UNL
a` X X X loc 319 895 (18 loc) 887 (70 loc) 246
apre`s X X X loc X 2 12 1
aussi X
avec X X X X X 35 193 (1 loc) 611 (1 loc) 49
chez X X X loc X 9 (5 loc) 1
comme X X 14 11 10 3
de X X X deloc X 310 888 1980 282
(117 deloc) (69 deloc)
depuis X X X deloc X 2 1
derrie`re X X X loc X 3
devers X X X
dixit X
emmi X
entre X X X loc X 19 (3 loc) 4
hormis X X X X X
jusque X X X X 7 (7 loc)
le`s X X X
moyennant X X X X X
par X X X loc X 3 38 (4 loc) 73 8
parmi X X X loc X 7 (3 loc) 7
passe? X X
selon X X X X X 1 1
voici X X
Table 2: Some simple prepositions in different sources
aDICOVALENCE
prepositions (more than 150) which represents an
excellent basis for our work.
4 Lexicon construction
The first selection criterion we applied to build the
lexicon is that a preposition should appear in at least
one source among the above-mentioned ones. Also,
we consider a preposition to be an argument prepo-
sition if it appears in at least one verb subcategoriza-
tion frame.
4.1 Manual filtering
We then filtered the prepositions according to very
simple criteria. In particular, we identified some
prepositions to be removed as they were:
? erroneous, this is the case, for example, of
aussi (adverb rather than preposition), which is
present in the UNL dictionary as a preposition,
? obsolete or very rare, like emmi (from TLFi),
devers (from Lefff, TLFi, Grevisse) or comme
de (from DICOVALENCE).
We also checked the semantic features given in
the sources and removed erroneous ones, like avec
as locative in SynLex and DICOVALENCE.
4.2 Some remarks
Some sources include as prepositions forms that are
not universally considered to be prepositions in lin-
guistics. This is the case, in particular, for:
? comme, which is not present in the three refer-
ence sources (Grevisse, TLFi and PrepNet) as
it is ambiguous and can also be used as a con-
junction,
20
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL Lefff DVa SynLex UNL
a` cause de X X X
a` la faveur de X X
a` partir de X X deloc 1
afin de X X X X
au nord de loc
au vu de X
aupre`s de X X X loc 27 (1 loc) 35
comme de 1
conforme?ment a` X X
d?avec X 1 6
d?entre X
en faveur de X X X 13
face a` X X 2
il y a X
jusqu?a` X loc X 10 (2 loc)
jusqu?en X
jusqu?ou` X
loin de X X loc
par suite de X
pour comble de X
pre`s de X X loc
quant a` X X X
tout au long de X X
vis-a`-vis de X X X 1
Table 3: Some multiword prepositions in different sources
aDICOVALENCE
? il y a or y compris, which only appear in Lefff,
? d?avec, which only appears in Grevisse and
verb subcategorization frames in DICOVA-
LENCE and SynLex.
We decided to keep those forms in the lexicon for
practical reasons, keeping the parsing application in
mind.
Moreover, even if its coverage is quite large, the
created lexicon is obviously not exhaustive. In
this respect, some missing entries should be added,
namely:
? prepositions from the DAFLES (Selva et al,
2002), like, for example, au de?triment de,
? prepositions appearing in reference grammars,
like question, in Grammaire me?thodique du
franc?ais (Riegel et al, 1997),
? some locative prepositions (and, through
metonymy, time prepositions) that are pre-
fixed by jusqu?, for example jusqu?aupre`s de.
This elided form of jusque should probably
be treated separately, as a preposition modi-
fier. The same goes for de`s, followed by a
time preposition (or a locative one, through
metonymy).
However, it is to be noticed that none of these
missing prepositions appear in verb subcategoriza-
tion frames.
This filtering process also allowed us to iden-
tify some issues, in particular elisions in multiword
21
forms, like afin de, afin d?, or contractions like face
a`, face au or a` partir de, a` partir du, which will be
processed in the segmentation step.
Others, like le`s, which is only used in toponyms
in dashed forms (e.g. Bathele?mont-le`s-Bauzemont),
will be processed during named entity segmentation.
4.3 Results
We obtained a list of 49 simple prepositions, of
which 23 appear in verb subcategorization frames
in at least one source and are therefore considered to
be argument prepositions (see table 4).
We also obtain a list of more than 200 multi-
word prepositions, among which 15 appear in verb
subcategorization frames in at least one source and
are therefore considered to be argument prepositions
(see table 5).
For the time being, we limited the semantic in-
formation in the lexicon to loc (locative) and deloc
(source), but we intend to extend those categories to
those used in DICOVALENCE (time, quantity, man-
ner). We have already added those to the preposi-
tions database that is being populated.
We also referred to the sources to add the cat-
egories of the arguments introduced by argument
prepositions.
PrepLex is currently distributed in a text format
suitable both for hand-editing and for integration in
a parser or other natural language processing tools.
In the format we propose, syntactic information is
described via feature structures. These feature struc-
tures are always recursive structures of depth 2. The
external level describes the structure in terms of ?ar-
guments? whereas the internal level gives a finer
syntactic description of either the head or of each
argument. This format aims at being modular and at
defining some ?classes? that share redundant infor-
mation. In the case of prepositions, the skeleton of
the feature structure used by all entries is:
Prep : [
head [cat=prep, prep=#, funct=#]
comp [cat=#, cpl=@]
]
When instantiated for a particular preposition, 3
feature values are to be provided (written with ?#?
in the above description) and the last parametrized
feature (written with @) is optional. When they are
in the head sub-structure, features are referred to by
their names whereas, in other cases, a prefix notation
is used.
a` [prep=a|LOC; funct=aobj|loc|adj;
comp.cat=np|sinf; comp.cpl=void|ceque]
apre`s [prep=apres|LOC; funct=obl|loc|adj;
comp.cat=np]
avec [prep=avec; funct=obl|adj;
comp.cat=np]
a`_travers [prep=a_travers; funct=obl|adj;
comp.cat=np]
Technically, the only difficult part is to decide
how to represent semantic classes of prepositions
like LOC. Here, we chose to define the whole set
of argument prepositions as well as all the semantic
classes (noted in uppercase) as possible atomic val-
ues for the prep feature. We then used the disjunc-
tion a|LOC to indicate that the preposition a` can be
used, either as a specific preposition or as a locative
preposition.
Additionally, we decided to add to the lexicon in-
formation about the sources in which the preposition
appears, in order to allow filtering for some specific
applications. In the case of argument prepositions,
we also added information about the preposition?s
frequency in the source, as well as a relevant exam-
ple.
We also decided to add corpus-based frequencies
to the lexicon. Thus, for each preposition, we pro-
vide its frequency per 1000 words, either as found in
the DAFLES (Selva et al, 2002), from a newspaper
corpus composed of Le Monde and Le Soir (1998),
or as extracted directly from Le Monde (1998) with
a simple grep command, without tagging.
5 Using the lexicon in a NLP system
We briefly expose some parsing problems related to
prepositions.
5.1 Segmentation issues
The first issue that appears when integrating preposi-
tions in a parsing system is that of segmentation. In
particular, contractions have to be processed specif-
ically so that au is identified as the equivalent of
a` le. The same goes for de, which can appear in
some multiword prepositions and can be elided as
d?. However, these phenomena are not specific to
prepositions. They can be addressed either in the
lexicon (for example Lefff explicitly contains both
22
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL PrepLex Lefff DV SynLex UNL PrepLex
44 69 55 36 46 49 14 24 18 11 23
Table 4: Total number of simple prepositions by source
Lexicons Subcategorization frames
Lefff TLFi Grevisse PrepNet UNL PrepLex Lefff DV SynLex UNL PrepLex
166 11 77 89 2 206 0 16 4 0 15
Table 5: Total number of multiword prepositions by source
au cours de and au cours d?), or during the segmen-
tation step.
We decided on the second solution as it improves
lexicon maintainability.
An issue that is more directly linked to multiword
prepositions is that of segmentation ambiguities. For
example, in the following two sentences (2a) and
(2b) the group of words au cours de is a multiword
preposition in the first case, but it has to be decom-
posed in the second one. Other multiword preposi-
tions can never be decomposed, for example y com-
pris.
This highlights the fact that segmentation is am-
biguous and that it is necessary to be able to keep
the segmentation ambiguity through the whole pars-
ing process.
2a. Il a beaucoup travaille? au cours de cette anne?e
[He worked hard during the year]
2b. Il a beaucoup travaille? au cours de M. Durand
[He worked hard in Mr Durand?s course]
5.2 Adjunct prepositions vs argument
prepositions
In deep parsing we have to distinguish between
prepositions introducing a verb argument and prepo-
sitions introducing adjuncts. However, we have
seen that this distinction often relies on semantics
and that parsing should leave the two possibilities
open. Precise information about argument preposi-
tions and verb subcategorizations eliminates many
of these ambiguities.
6 Conclusion
We created a list of French prepositions for parsing
applications by comparing various lexicons and dic-
tionaries. We hence focused on syntactic aspects.
Manual filtering was used to eliminate obsolete or
rare prepositions, as well as a number of errors.
The resulting lexicon contains more than 250 French
prepositions, among which 49 are simple preposi-
tions.
In syntactic lexicons, subcategorization frames
describe prepositions introducing arguments. Prepo-
sitions appearing in verbal valence frames are called
?argument prepositions?. We identified 40 of them.
The produced lexicon is freely available. 3 It will
be developed further. In particular, some other in-
formation sources will be incorporated. This is the
case for the verbs constructions fields from the TFLi
which contain prepositions, that can be considered
as argument prepositions. We plan to use this infor-
mation to improve the lexicon.
We are also populating a database with this lexical
information. 3 This will help us ensure a better main-
tenance of the lexicon and will allow enrichment of
the entries, in particular with examples and associ-
ated verbs. We are adding corpus-based frequencies
to this database.
A more ambitious task would be to enrich the lex-
icon with fine-grained semantic information (more
detailed than the general classes loc, deloc, . . . ).
Many interesting linguistic studies have been con-
ducted on prepositions, including cross-lingual ap-
proaches. However, most of them are limited to de-
tailing the semantics of a small number of preposi-
tions; with the exceptions of PrepNet (Saint-Dizier,
2006) for French prepositions and TPP (Litkowski
and Hargraves, 2005) (The Preposition Project) for
English. It is now necessary to transform those re-
sources in order to make them directly usable by nat-
ural language processing systems.
3http://loriatal.loria.fr/Resources.html
23
References
Ted Briscoe and John A. Carroll. 1993. Generalized
probabilistic LR parsing of natural language (corpora)
with unification-based grammars. Computational Lin-
guistics, 19(1):25?59.
John A. Carroll and Alex C. Fang. 2004. The automatic
acquisition of verb subcategorisations and their impact
on the performance of an HPSG parser. In Proceed-
ings of the 1st International Joint Conference on Nat-
ural Language Processing (IJCNLP), pages 107?114,
Sanya City, China.
Claire Gardent, Bruno Guillaume, Guy Perrier, and In-
grid Falk. 2006. Extraction d?information de sous-
cate?gorisation a` partir des tables du LADL. In Pro-
ceedings of TALN 06, pages 139?148, Leuven.
Maurice Grevisse. 1997. Le Bon Usage ? Gram-
maire franc?aise, e?dition refondue par Andre? Goosse.
DeBoeck-Duculot, Paris ? Leuven, 13th edition.
Maurice Gross. 1975. Me?thodes en syntaxe. Hermann.
Ken Litkowski and Orin Hargraves. 2005. The preposi-
tion project. In Proc. of the ACL Workshop on Prepo-
sitions.
Martin Riegel, Jean-Christophe Pellat, and Rene? Rioul.
1997. Grammaire me?thodique du franc?ais. PUF, 3rd
edition.
Benoit Sagot, Lionel Cle?ment, ?Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for French: architecture, acquisition,
use. In Proc. of LREC 06, Genoa, Italy.
Patrick Saint-Dizier. 2006. PrepNet: a Multilingual Lex-
ical Description of Prepositions. In Proc. of LREC 06,
Genoa, Italy, pages 877?885. European Language Re-
sources Association (ELRA).
Thierry Selva, Serge Verlinde, and Jean Binon. 2002.
Le DAFLES, un nouveau dictionnaire pour apprenants
du franc?ais. In Proc. of EURALEX?2002 (European
Association for Lexicography), Copenhagen.
Gilles Se?rasset and Christian Boitet. 2000. On UNL as
the future ?html of the linguistic content? and the reuse
of existing NLP components in UNL-related applica-
tions with the example of a UNL-French deconverter.
In Proceedings of COLING 2000, Saarbru?cken.
Karel van den Eynde and Piet Mertens, 2002. La valence:
l?approche pronominale et son application au lexique
verbal, pages 63?104. Cambridge University Press,
13th edition.
24
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 107?114,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Interaction Grammar for the Persian Language:  
Noun and Adjectival Phrases 
 
Masood Ghayoomi Bruno Guillaume 
Nancy2 University LORIA - INRIA, BP 239 
54506 Vandoeuvre, Nancy cedex, France 54506 Vandoeuvre, Nancy cedex, France 
masood29@gmail.com Bruno.Guillaume@loria.fr 
 
  
 
 
Abstract 
In this paper we propose a modelization of 
the construction of Persian noun and adjec-
tival phrases in a phrase structure grammar. 
This modelization uses the Interaction 
Grammar (IG) formalism by taking advan-
tage of the polarities on features and tree 
descriptions for the various constructions 
that we studied. The proposed grammar was 
implemented with a Metagrammar compiler 
named XMG. A small test suite was built 
and tested with a parser based on IG, called 
LEOPAR. The experimental results show 
that we could parse the phrases successfully, 
even the most complex ones which have 
various constructions in them. 
1 Introduction 
Interaction Grammar (IG) is a grammatical for-
malism which is based on the notions of polar-
ized features and tree descriptions. 
Polarities express the resource sensitivity of 
natural language by modeling the distinction be-
tween saturated and unsaturated syntactic con-
struction (Guillaume and Perrier, 2008). 
IG focuses on the syntactic level of a natural lan-
guage. This formalism is designed in such a way 
that it can be linked with a lexicon, independent 
of any formalism. The notion of polarity that is at 
the heart of IG will be discussed in section 2.2. 
In IG, the parsing output of a sentence is an or-
dered tree where nodes represent syntactic con-
stituents described by feature structures.  
What we are interested in is studying the con-
struction of constituencies of the Persian lan-
guage according to IG. Among various 
constituencies in the language, we have focused 
on the construction of Persian noun phrases and 
adjectival phrases as the first step to build a 
grammar for this language. 
The current work covers only noun and adjecti-
val phrases; it is only a first step toward a full 
coverage of Persian grammar. The grammar pre-
sented here could have been expressed in Tree 
Adjoining Grammar (TAG) or even in Context 
Free Grammar with features, but we strongly 
believe that the modelization of the verbal con-
struction of Persian, which is much more com-
plex, can benefit from advanced specificities of 
IG, like polarities, underspecifications and trees. 
2 Previous Studies  
2.1 IG for French and English 
The first natural language considered within IG 
was French. A large coverage grammar which 
covers most of the frequent constructions of 
French, including coordination, has been built 
(Perrier, 2007; Le Roux and Perrier, 2007).  
Recently, using the fact that the French and Eng-
lish languages have many syntactic similarities, 
Planul (2008) proposed an English IG built by 
modifying the French one. These two grammars 
were tested on the Test Suite for Natural Lan-
guage Processing (TSNLP; Oepen et al 1996). 
Both cover 85% of the sentences in the TSNLP.  
2.2 Polarity 
The notion of polarity is based on the old idea of 
Tesni?re (1934), Jespersen (1935), and Adjuk-
iewicz (1935) that a sentence is considered as a 
molecule with its words as the atoms; every word 
is equipped with a valence which expresses its 
capacity of interaction with other words, so that 
syntactic composition appears as a chemical re-
action (Gaiffe and Perrier, 2004). Apparently, it 
seems Nasr (1995) was the first to propose a 
107
formalism that explicitly uses the polarized struc-
ture in computational linguistics. Then re-
searches such as Muskens and Krahmer (1998), 
Duchier and Thater (1999), and Perrier (2000) 
proposed grammatical formalisms in which po-
larity is also explicitly used. However, Categorial 
Grammar was the first grammatical formalism 
that exploited implicitly the idea of polarity 
(Lambek, 1958). Recently, Kahane (2006) 
showed that well-known formalisms such as 
CFG, TAG, HPSG, and LFG could be viewed as 
polarized formalisms.  
IG has highlighted the fundamental mechanism 
of neutralization between polarities underlying 
CG in such a way that polarities are attached to 
the features used for describing constituents and 
not to the constituents themselves. Polarization 
of a grammatical formalism consists of adding 
polarities to its syntactic structure to obtain a po-
larized formalism in which neutralization of po-
larities is used to control syntactic composition. 
In this way, the resource sensitivity of syntactic 
composition is made explicit (Kahane, 2004). 
In trees expressing syntactic structures, nodes 
that represent constituents are labeled with po-
larities with the following meanings: A constitu-
ent labeled with a negative polarity (<-) 
represents an expected constituent, whereas a 
constituent labeled with the positive polarity (->) 
represents an available resource. Both of these 
polarities can unify to build a constituent which 
is labeled with a saturated neutral polarity (<=>) 
that cannot interact with any other constituents. 
The composition of structures is guided by the 
principle of neutralization that every positive 
label must unify with a negative label, and vice 
versa. Nodes that are labeled with the simple 
neutral polarity (=) do not behave as consumable 
resources and can be superposed with any other 
nodes any number of times; they represent con-
stituents or features indifferently.  
The notion of saturation in terms of polarity is 
defined as a saturated structure that has all its 
polarities neutral, whereas an unsaturated struc-
ture keeps positive or negative polarities which 
express its ability to interact with other struc-
tures. A complete syntactic tree must be satu-
rated; that means it is without positive or 
negative nodes and it can not be composed with 
other structures: so all labels are associated with 
the polarity of = or <=>. 
The set of polarities {-> , <- , = , <=>} is 
equipped with the operation of compositional 
unification as defined in the table below (Bon-
fante et al 2004): 
 <- -> = <=> 
<-  <=> <-  
-> <=>  ->  
= <- -> = <=> 
<=>   <=>  
 
Table 1. Polarity compositions on the nodes 
2.3 Tree Description Logic in IG 
Another specification of IG is that syntactic 
structures can be underspecified: these structures 
are trees descriptions. It is possible, for instance, 
to impose that a node dominates another node 
without giving the length of the domination path. 
Guillaume and Perrier (2008) have defined four 
kinds of relations: 
- Immediate dominance relations: N > M means 
that M is an immediate sub-constituent of N.  
- Underspecified dominance relations: N >* M 
means that the constituent N includes another 
constituent M at a more or less deep level. (With 
this kind of node relations, long distance depend-
encies and possibilities of applying modifiers 
could be expressed.)  
- Immediate precedence relations: N << M means 
that the constituent M precedes the constituent N 
immediately in the linear order of the sentence. 
- Underspecified precedence relations: N <<+ M 
means that the constituent M precedes the con-
stituent N in the linear order of the sentence but  
the relation between them cannot be identified. 
3 The Persian Language Properties 
Persian is a member of the Indo-European lan-
guage family and has many features in common 
with the other languages in this family in terms 
of morphology, syntax, phonology, and lexicon. 
Although Persian uses a modified version of the 
Arabic alphabet, the two languages differ from 
one another in many respects.  
Persian is a null-subject language with SOV 
word order in unmarked structures. However, the 
word order is relatively free. The subject mood is 
widely used. Verbs are inflected in the language 
and they indicate tense and aspect, and agree 
with subject in person and number. The language 
does not make use of gender (M?hooti?n, 1997). 
In noun phrases, the sequence of words is around 
at least one noun, namely the head word. So, the 
noun phrase could be either a single unit noun, or 
a sequence of other elements with a noun. The 
syntax of Persian allows for having elements be-
fore a noun head _prenominal, and after the noun 
head _postnominal. 
108
To make a phrase, there are some restrictions for 
the elements surrounding a head to make a con-
stituent; otherwise the sequence of elements will 
be ill-formed, that is, ungrammatical.  
Nouns belong to an open class of words. The 
noun could be a common noun, a proper noun, or 
a pronoun. If this noun is not a proper noun or a 
pronoun, some elements can come before it and 
some after it (M?hooti?n, 1997). Some of the 
prenominal elements coming before a noun head 
are cardinal numbers, ordinal numbers, superla-
tive adjectives, and indefinite determiners; post-
nominal elements are nouns and noun phrases, 
adjectives and adjectival phrases, adjectival 
clauses with conjunctions, indefinite post-
determiners, prepositional phrases, adverbs of 
place and time, ordinal numbers, possessive ad-
jectives, and Ezafeh.  
The syntactical structure of an adjectival phrase 
is simple. It is made up of a head adjective and 
elements that come before and after the head. An 
adjectival phrase is a modifier of a noun. The 
elements coming before a simple adjective are 
adverbs of quantity and prepositional phrases. 
4 Required Tools 
4.1 Test Suite 
The test suite is a set of controlled data that is 
systematically organized and documented. In this 
case, the test suite is a kind of reference data dif-
ferent from data in large collections of text cor-
pora. A test suite should have the following 
advantages: it should have a broad coverage on 
the structural level, so you can find many struc-
tures of a language with a minimal lexicon; it 
could be multilingual, so the structure of the lan-
guages could be compared; it should be a consis-
tent and highly structured linguistic annotation. 
The differences between a test suite and a corpus 
are: that in test suite there is a control on the 
data, that the data has a systematic coverage, that 
the data has a non-redundant representation, that 
the data is annotated coherently, and that relevant 
ungrammatical constructions are included inten-
tionally in a test suite (Oepen et al 1996).  
Since our end goal is to develop a fragment of 
Persian grammar, to the best of our knowledge 
no already developed test suite for our target 
constructions was available; so we built a very 
small test suite with only 50 examples based on a 
small lexicon _only 41 entries.  
 
4.2 XMG 
The XMG system is usually called a "meta-
grammar compiler" is a tool for designing large-
scale grammars for natural language. This system 
has been designed and implemented in the 
framework of Benoit Crabb? (2005). 
XMG has provided a compact representation of 
grammatical information which combines ele-
mentary fragments of information to produce a 
fully redundant, strongly lexicalized grammar. 
The role of such a language is to allow us to 
solve two problems that arise while developing 
grammars: to reach a good factorization in the 
shared structures, and to control the way the 
fragments are combined.  
It is possible to use XMG as a tool for both tree 
descriptions in IG and TAG. Since there isnot 
any built-in graphical representation for IG in 
XMG, LEOPAR is used to display the grammar.  
LEOPAR is a parser for processing natural lan-
guages based on the IG formalism. 
4.3 LEOPAR 
LEOPAR is a tool chain constructed based on IG 
(Guillaume et al 2008). It is a parser for IG that 
can be used as a standalone parser in which in-
puts are sentences and outputs are constitu-
ent trees. But it also provides a graphical user 
interface which is mostly useful for testing and 
debugging during the stages of developing the 
grammar. The interface can be used for interac-
tive or automated parsing.  LEOPAR also pro-
vides several visualization modes for the 
different steps in the parsing process. Further-
more, it offers some tools to deal with lexicons:  
they can be expressed in a factorized way and 
they can be compiled to improve parsing effi-
ciency. 
LEOPAR is based on UTF8 encoding, so it sup-
ports Persian characters. It is also modified to 
take into account the right-to-left languages. For 
our designed grammar we have taken the advan-
tage of this parser for IG. 
5 Designing the Grammar 
In this section we explicitly describe the tree 
construction of the Persian noun and adjectival 
phrase structures which are polarized. We have 
provided the elementary syntactic structures de-
rived from the existing rules in the language and 
then polarized the features in the trees which are 
named initial polarized tree descriptions. 
109
To be more comprehensible and clear, nodes are 
indexed for addressing. More importantly, the 
trees should be read from right-to-left to match 
the writing system in the right-to-left language.  
For clarity in the tree representations in this pa-
per, no features are given to the nodes. But while 
developing the grammar with XMG, polarized 
features are given to the nodes to put a control on 
constructing the trees and avoid over-generating 
some constructions.  
There are some constructions whose tree repre-
sentations are the same but represent two differ-
ent constructions, so they could be described 
from two different points of views. Such trees are 
described in the sections corresponding to the 
relevant constructions. Some morphophonemic 
phenomena were considered at the syntactic 
level, while developing our grammar. Such a 
phenomenon is defined at the feature level for 
the lexicon which will be described in their rele-
vant sections. 
5.1 Noun Construction 
A noun phrase could consist of several elements 
or only one head noun element. If the element of 
a noun phrase (N1) is a noun, it is anchored to a 
lexicon item (N2) which could be a common 
noun, or a proper noun. The symbol ? has been 
used for the nodes that are anchored to a lexical 
item.            -> N1 
| 
= N2? 
The tree of a common noun and a proper noun 
are the same, but features should be given to the 
tree to make a distinction between the anchored 
nouns. With the help of features, we can make 
some restrictions to avoid some constructions. 
Features and their values are not fully discussed 
here. 
5.2 Pronoun Construction 
A pronoun can appear both in subject and object 
positions to make a noun. In this construction, 
node N3 is anchored to a pronoun: 
-> N3 
| 
= PRON? 
A pronoun cannot be used in all constructions. 
For example, N3 cannot be plugged into N5 in a 
determiner construction because a determiner 
could not come before a pronoun. To avoid this 
construction, some features have been used for 
the node N5 to stop the unification with some N 
nodes like N3.  
5.3 Determiner Construction 
In Persian a determiner comes before a common 
noun or a noun phrase, and not a proper noun or 
a pronoun.  
Persian does not benefit from the definite deter-
miner, but there are two kinds of indefinite de-
terminers: one comes before a noun as a separate 
lexical item and the other one comes after a noun 
(post-determiner) which is joined to the end of 
the noun as described below: 
If the determiner comes before a noun, there 
must be a tree in which a Det node is anchored to 
a lexicon item that is a determiner and which 
comes immediately before a noun. In other 
words, some lexical items which are determiners 
could attach to this node:  
 -> N4  
   
<- N5  = Det? 
If the determiner comes after a noun (i.e. if it is a 
post-determiner), then it can be joined to the end 
of a noun. The post-determiner (P-Det) and the 
preceding noun (N7), make a noun (N6):  
 -> N6  
   
= P-Det?  <- N7 
The post-determiner has three different written 
forms: ??? /i/, ???? /yi/, and ???? /?i/. The reason 
to have them is phonological. In our formalism 
we have considered this phonological phenome-
non at a syntactic level.  
If the post-determiner construction is used after 
an adjective in the linguistic data, it does not be-
long to the adjective (since the adjective is only 
the modifier of the noun), but it belongs to the 
noun. According to the phonological context and 
the final sound of the adjective, the post-
determiner that belongs to the noun changes and 
takes one of the written forms. 
5.4 Ezafeh Construction 
One of the properties of Persian is that usually 
short vowels are not written. In this language, the 
Ezafeh construction is represented by the short 
vowel ?-?? /e/ after consonants or ???? /ye/ after 
vowels at the end of a noun or an adjective.  
Here we try to give a formal representation of 
such construction that is described from a purely 
syntactical point of view. Ezafeh (Ez) appears on 
(Kahnemuyipour, 2002): a noun before another 
noun (attributive); a noun before an adjective; a 
noun before a possessor (noun or pronoun); an 
adjective before another adjective; a pronoun 
110
before an adjective; first names before last 
names; a combination of the above. 
Note that Ezafeh only appears on a noun when it 
is modified. In other words, it does not appear on 
a bare noun (e.g. ?????? /ket?b/ 'book'). In Ezafeh 
construction, the node Ez is anchored to the 
Ezafeh lexeme. The below tree could make a 
noun phrase (N8) with Ezafeh construction, in 
which a common noun or a proper noun on N9 is 
followed by an Ezafeh (Ez) and another common 
noun, proper noun, pronoun or another noun 
phrase plugs to the node N10: 
      -> N8  
   
<- N10   = N 
    
          = Ez?                                <- N9 
The below tree could make a noun phrase (N11) 
with Ezafeh construction in which a common 
noun or a proper noun on N12 is modified by an 
adjectival phrase on node ADJ1. Ezafeh has to be 
used after the noun to link it to the adjective: 
    -> N11  
   
<- ADJ1   = N 
    
     = Ez?                                <- N12 
Based on the final sound of the word which is 
just before Ezafeh, there are two written forms 
for Ezafeh, depending on whether the noun ends 
with a consonant or a vowel. 
As we have already said, Ezafeh contraction 
could be used for an adjective (ADJ1). After this 
construction, another adjectival phrase (ADJ3 
and ADJ4) with Ezafeh could appear too. It 
should be mentioned that ADJ4 is plugged into 
an adjective without Ezafeh construction:  
 ->ADJ2  
   
<-ADJ4  =ADJ 
    
                        = Ez?   <-ADJ3 
5.5 Possessive Construction 
In Persian there are two different constructions 
for possessive. One is a separate lexical item as a 
common noun, a proper noun, or a pronoun. The 
second is a possessive pronoun that is a kind of 
suffix which attaches to the end of the noun. In 
the first construction, a noun with an Ezafeh con-
struction is used and then a common noun, a 
proper noun, or a pronoun as a separate lexical 
item follows. In the latter construction, there is a 
common noun and the joined possessive pro-
noun. The two constructions are discussed here: 
In section 5.4 we described Ezafeh construction 
(N8). This tree could be used for possessive con-
struction, too. In this tree an Ezafeh is used after 
a common noun and Ezafeh is followed by either 
a common noun or a proper noun. A pronoun 
could not be used in N9 with Ezafeh. Such a kind 
of construction is avoided by defining features. 
The possessive construction as a suffix could 
come after both a noun and an adjective. The 
general property of the joined possessive pro-
nouns is that there is an agreement between the 
subject and the possessive pronoun in terms of 
number and person, no matter whether it is used 
after a noun or an adjective. 
If the joined possessive pronoun (S-P) is used 
after a noun (N14), we would have the tree N13 
in which the possessive pronoun is anchored to 
the suffix (S-P): 
 -> N13 
   
= S-P?  <- N14 
Based on the phonological reasons and consider-
ing Persian syllables, as was discussed previ-
ously in section 5.3, this suffix would have 
different written forms based on the phonological 
context it appears in: after a consonant, the vowel 
/?/, or any other vowels except /?/. For adjec-
tives, there is no suffix possessive pronoun. In 
the linguistic data, this pronoun could appear 
after the adjective. But the point is that the adjec-
tive is only the modifier of the noun. This pos-
sessive pronoun, in fact, belongs to the noun and 
not the adjective, but based on the phonological 
rules (i.e. the final sound of the adjective) only 
one of the written forms would appear after that. 
5.6 Count noun Construction 
There are some nouns in Persian referred to as 
count nouns which have collocational relations 
with the head noun that is counted. So, in such a 
construction, the node C-N is anchored to a lexi-
cal item that is a count noun: 
  -> N15  
   
<- N16  = C-N? 
5.7 Object Construction 
In Persian, a noun phrase can appear both in sub-
ject and object positions. If the noun phrase ap-
pears in a subject position, it does not require any 
indicator. But if the noun phrase appears in the 
direct object position (N18), the marker ???? /r?/ 
is used to indicate that this noun phrase (N17) is 
a direct object. We call this marker ?Object Indi-
111
cator? (O-I) so the node is anchored to the object 
maker. The representation of the tree for the ob-
ject construction (N17) is the followings:  
 -> N17  
   
= O-I?  <- N18 
5.7 Conjunction Construction 
In Persian, there is a construction to modify the 
preceding noun phrase with an adjective clause 
which we have named the Conjunction construc-
tion. In such a construction, there are a noun 
phrase (N20), a conjunctor (Conj), and a clause 
to modify the noun phrase (S1). In the tree, the 
conjunction node is anchored to a conjunctor: 
 -> N19  
   
    = S  <- N20 
   
       <- S1         = Conj? 
5.8 Adjective Constructions 
There are two classes of adjectives: the first class 
comes before a noun head, the second one after.  
There are three kinds of adjectives in the first 
class which can be differentiated from each other 
with the help of features. The first class of adjec-
tives contains superlative adjectives, cardinal 
numbers, and ordinal numbers that modify a 
noun, a count noun, or a noun phrase. Usually, 
the adjectives coming before a noun phrase are in 
complementary distribution; i.e. the presence of 
one means the absence of the two others.  
The following tree represents the adjective con-
struction coming before a noun (N22). The ad-
jective ADJ5 is anchored to a lexical item: 
 ->  N21  
   
<- N22  =ADJ5? 
The second class of adjectives (which comes af-
ter a noun) contains mostly simple adjectives, 
ordinal numbers and comparative adjectives.  
As we have already described tree N11 in section 
5.4, to have an adjective after a noun the noun 
must have an Ezafeh construction. So, this tree 
represents a construction where an adjective 
(ADJ1) comes after a noun (N12). 
To saturate ADJ1, the tree ADJ6 is required 
which is anchored to an adjective lexical item: 
->ADJ6 
| 
=ADJ7? 
In some adjective constructions, a prepositional 
phrase could be used which comes before or after 
some adjective constituents. With the help of 
some features, we have made restrictions on the 
kind of adjective and the preposition lexical item 
that could plug into this node. 
If a preposition is used before the adjective 
(ADJ9), it is a comparative adjective: 
 ->ADJ8  
   
=ADJ9?  <- P1 
If the preposition is used after the adjective 
(ADJ11), it is either a comparative or a simple 
adjective: 
 ->ADJ10  
   
    <- P2                      =ADJ11? 
5.9 Preposition Construction 
In Persian a common noun, a proper noun, a pro-
noun, or a noun phrase could come after a 
preposition (P4) to make a prepositional phrase 
(P3):  -> P3  
   
    <- N23                        = P4? 
If the preposition construction is used in an ad-
jective construction, only some specific preposi-
tions can be used. Once again, the restrictions are 
encoded with features. 
6 Implementation and Results 
So far we have explicitly described the noun and 
adjectival phrase constructions in Persian accord-
ing to the constituency rules that are extracted 
from the linguistic data. These rules are repre-
sented by polarized trees. Since we wanted to 
study the noun and adjectival phrase structures, 
they required data. We have gathered this data 
for our purpose as a test suite. 
To design IG for the constructions that were de-
scribed, we have used XMG as the basic tool to 
have the initial tree descriptions. While describ-
ing the trees in XMG, several operators will be 
used to polarizing features. The categories of the 
nodes are considered as features, so the nodes are 
polarized. Using XMG, we have done factoriza-
tions and defined classes for general trees. Three 
factorized general trees are defined in our XMG 
coding. We have also defined 17 classes for cod-
ing of trees to represent the constructions as de-
scribed. 
The output of XMG is given to LEOPAR to dis-
play the graphical representations of the tree 
structures and also parse the data. The test suite 
is given to LEOPAR for parsing.  
Having the developed trees and the test suite, we 
successfully parsed all available phrases, from 
112
the simplest to the most complex ones that had a 
variety of constructions in them. Example 1 has a 
simple construction, example 2 is of medium 
complexity, and example 3 is the most complex: 
 
1.                                                              ??????????   
/ket?b/ (/e/) /d?niy?l/                           
 book    (Ez)   Daniel 
?the book of Daniel / Daniel?s book? 
 
2.                                  ????? ???? ???????? ?? ??????  
/hamzam?n/   /b?/ /ente??r/  (/e/) /avvalin/ 
in coincidence  with publishing  (Ez)  the first     
/ket?b/ (/e/)   /?u/ 
 book    (Ez)  his/her  
?in coincidence with the publishing of his/her 
first book? 
 
3.                           ???? ?? ?? ?????  ??? ???? ????????  
/?n/ /do/ /jeld/ /ket?b/ (/e/) /jadid/ (/e/)  
that   two    volume    book     (Ez)     new      (Ez)  
/mohem/   (/e/)  /d?niyal/  /r?/    /ke/ 
important   (Ez)    Daniel   POBJ that 
?the two new important book volumes of Daniel 
that? 
 
We know from section 5.4 that Ezafeh is pro-
nounced but not written. Since the anchored 
nodes require a lexical item, we put the word 
??????? /ez?fe/ ?Ezafeh? in the lexicon to have a 
real representation of Ezafeh. Also, wherever 
Ezafeh is used in the test suite, this word is re-
placed. 
As a sample, we give a brief description of pars-
ing the phrases 1 and 2 with LEOPAR and dis-
play the outputs. 
In our test suite, phrase 1 is found as 
????? ????? ???????. In this phrase, the common 
noun /ket?b/ is followed by a proper noun 
/d?niy?l/ with Ezafeh. The possessive construc-
tion (N8) would be used to parse this phrase.  
In parsing this phrase, firstly LEOPAR reads the 
words and matches them with the lexical items 
available in the lexicon to identify their catego-
ries. Then it plugs these words into the nodes in 
the trees that have the same syntactic category 
and have an anchored node. Finally, it gives the 
parsed graphical representation of the phrase.  
For this phrase, the Ezafeh construction tree is 
used in such a way that N2 is anchored to the 
word /ket?b/ and N1 plugs into N9 to saturate it. 
Then, N2 is again anchored to the word /d?niy?l/ 
and N1 plugs in to saturate N10. The final parsed 
phrase is such that all internal nodes are saturated 
and have neutral polarity, as shown in Figure 1. 
As another example, consider phrase 2, which is 
 
 
Figure 1: Parsing the phrase ????? ???????  
with LEOPAR 
 
 
 
 
Figure 2: Parsing the phrase? ???? ?? ?????? ?? ?????? ????? ? 
with LEOPAR 
 
found as ? ?? ????? ????? ???? ??????????? ?? ?????? ? in 
our test-suite. Since some various constructions 
are used to build this phrase, we could say that it 
113
is a complex phrase. Firstly it takes the adjective 
phrase construction (ADJ10). P3, the preposi-
tional phrase, plugs into P2. Since a noun or a 
noun phrase could be used after a preposition 
(N23), the Ezafeh construction (N8) that takes 
the noun plugs to this node. Another Ezafeh con-
struction (N8) will be plugged into N10. The ad-
jective construction (ADJ5) for ordinal numbers 
as the modifier of a noun (N22) could be used 
while a noun (N1) would plug into N22. Finally, 
the pronoun (N3) plugs into the unsaturated noun 
position in the second Ezafeh construction. Pars-
ing the phrase with LEOPAR, the result has all 
internal nodes saturated and neutralized, and no 
polarities on the nodes are left unsaturated, as 
shown in Figure 2. 
7 Conclusion and Future Work 
In our research we have used IG to represent the 
construction of Persian noun and adjectival 
phrases in trees. XMG was used to represent the 
constructions using factorization and inherited 
hierarchy relations. Then, with the help of XMG, 
we defined IG by taking advantage of polarities 
on the features and tree descriptions for the vari-
ous constructions that are introduced. Then, we 
used LEOPAR for the graphical representations 
of the trees and parsing the phrases. Finally, we 
applied our test suite to the parser to check 
whether we had the correct parsing and represen-
tation of the phrases. The experimental results 
showed that we could parse the phrases success-
fully, including the most complex ones, which 
have various constructions in them. 
In the next step of our research, we would like to 
study the construction of prepositions and, more 
importantly, verbs in depth to make it possible to 
parse at the sentence level. 
References  
Adjukiewcz K., 1935. ?Die syntaktiche konnexit?t? 
Studia Philadelphica 1, pp. 1-27. 
Bonfante G. and B. Guillaume and G. Perrier, 2004. 
?Polarization and abstraction of grammatical for-
malism as methods for lexical disambiguation? In 
Proc.s of 20th Int. Conf. on CL, Gen?ve. 
Candito,  M. H., 1996. ?A principle-based hierarchical 
representa-tion of LTAGs?. COLING-96. 
Crabb?, B., 2005. ?Grammatical development with 
XMG?. LACL 05. 
Duchier and Thater, 1999. ?Parsing with tree descrip-
tions: A constraint based approach? In Proc.s of 
NLU and Logic Programming, New Mexico. 
Gaiffe, B. and G. Perrier, 2004. ?Tools for parsing 
natural language? ESSLLI 2004. 
Guillaume B. and G. Perrier, 2008. ?Interaction 
Grammars? INRIA Research Report 6621:  
http://hal.inria.fr/inria-00288376/ 
Guillaume B. and J. Le Roux and J. Marchand and G. 
Perrier and K. Fort and J. Planul, 2008, ?A Tool-
chain for Grammarians? CoLING 08, Manchester. 
Jesperson , O., 1935. Analytic Syntax. Allen and 
Uwin, London. 
Kahane, S., 2004. ?Grammaries d?unification polari-
s?es? In 11i?me Conf. sur le TAL, F?s, Maroc. 
Kahane, S., 2006. ?Polarized unification grammars?. 
In Proce.s of 21st Int. Conf. on CL and 44th An-
nual Meeting of the ACL. Sydney, Australia. 
Kahnemuyipour, A., 2000. "Persian Ezafe construc-
tion revisited: Evidence for modifier phrase," An-
nual Conf. of the Canadian Linguistic Association. 
Lambek, J., 1958. "The mathematics of sentence 
structure", The American Mathematical Monthly 
65: 154?170. 
Leopar: a parser for Interaction Grammar 
http://leopar.loria.fr/ 
Le Roux, J. and G. Perrier, 2007. ?Mod?lisation de la 
coordination dans les Grammaires d?Interaction?, 
Traitement Automatique des Langues (TAL 47-3) 
M?hooti?n, Sh, 1997. Persian. Routledge. 
Muskens and Krahmer, 1998. ?Talking about trees 
and truth conditions?. In Logical Aspects of CL, 
Grenoble, France, Dec 1998.  
Nasr A., 1995. ?A formalism and a parser for lexical-
ized dependency grammars? In Proce.s of 4th Int. 
Workshop on Parsing Technologies, Prague. 
Oepen, S. and K. Netter and J. Klein, 1996. ?TSNLP- 
Test suites for natural language processing?. In 
Linguistic Database, CSLI Lecture Notes. Center 
for the Study of Language and information. 
Perrier, G., 2000. ?Interaction grammar? Coling 2000. 
Perrier, G., 2007. "A French Interaction Grammar", 
RANLP  2007, Borovets Bulgarie.  
Planul, J., 2008. Construction d'une Grammaire d'In-
teraction  pour l'anglais, Master thesis, Universit? 
Nancy 2, France. 
Tesni?re L., 1934. ?Comment construire une syntaxe? 
Bulletin de la Facult? des Lettres de Strasbourg 7-
12i?me. pp. 219-229. 
XMG Documentation 
http/wiki.loria.fr/wiki/XMG/Documentation 
114
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 242?253,
Paris, October 2009. c?2009 Association for Computational Linguistics
Dependency Constraints for Lexical Disambiguation
Guillaume Bonfante
LORIA INPL
guillaume.bonfante@loria.fr
Bruno Guillaume
LORIA INRIA
bruno.guillaume@loria.fr
Mathieu Morey
LORIA Nancy-Universite?
mathieu.morey@loria.fr
Abstract
We propose a generic method to per-
form lexical disambiguation in lexicalized
grammatical formalisms. It relies on de-
pendency constraints between words. The
soundness of the method is due to invariant
properties of the parsing in a given gram-
mar that can be computed statically from
the grammar.
1 Introduction
In this work, we propose a method of lexical dis-
ambiguation based on the notion of dependencies.
In modern linguistics, Lucien Tesnie`re devel-
oped a formal and sophisticated theory with de-
pendencies (Tesnie`re, 1959). Nowadays, many
current grammatical formalisms rely more or less
explicitly on the notion of dependencies between
words. The most straightforward examples are
formalisms in the Dependency Grammars family
but it is also true of the phrase structure based for-
malisms which consider that words introduce in-
complete syntactic structures which must be com-
pleted by other words. This idea is at the core of
Categorial Grammars (CG) (Lambek, 1958) and
all its trends such as Abstract Categorial Gram-
mars (ACG) (de Groote, 2001) or Combinatory
Categorial Grammars (CCG) (Steedman, 2000),
being mostly encoded in their type system. De-
pendencies in CG were studied in (Moortgat and
Morrill, 1991) and for CCG in (Clark et al,
2002; Koller and Kuhlmann, 2009). Other for-
malisms can be viewed as modeling and using
dependencies, such as Tree Adjoining Grammars
(TAG) (Joshi, 1987) with their substitution and ad-
junction operations. Dependencies for TAG were
studied in (Joshi and Rambow, 2003). More re-
cently, Marchand et al (2009) showed that it is
also possible to extract a dependency structure
from a syntactic analysis in Interaction Grammars
(IG) (Guillaume and Perrier, 2008).
Another much more recent concept of polarity
can be used in grammatical formalisms to express
that words introduce incomplete syntactic struc-
tures. IG directly use polarities to describe these
structures but it is also possible to use polarities
in other formalisms in order to make explicit the
more or less implicit notion of incomplete struc-
tures: for instance, in CG (Lamarche, 2008) or in
TAG (Kahane, 2006; Bonfante et al, 2004; Gar-
dent and Kow, 2005). On this regard, Marchand
et al (2009) exhibited a direct link between polar-
ities and dependencies. This encourages us to say
that in many respects dependencies and polarities
are two sides of the same coin.
The aim of this paper is to show that dependen-
cies can be used to express constraints on the tag-
gings of a sentence and hence these dependency
constraints can be used to partially disambiguate
the words of a sentence. We will see that, in prac-
tice, using the link between dependencies and po-
larities, these dependency constraints can be com-
puted directly from polarized structures.
Exploiting the dependencies encoded in lexical
entries to perform disambiguation is the intuition
behind supertagging (Bangalore and Joshi, 1999),
a method introduced for LTAG and successfully
applied since then to CCG (Clark and Curran,
2004) and HPSG (Ninomiya et al, 2006). These
approaches select the most likely lexical entry (en-
tries) for each word, based on Hidden Markov
Models or Maximum Entropy Models. Like the
work done by Boullier (2003), our method is not
based on statistics nor heuristics, but on a neces-
sary condition of the deep parsing. Consequently,
we accept to have more than one lexical tagging
for a sentence, as long as we can ensure to have
the good ones (when they exist!). This property is
particulary useful to ensure that the deep parsing
will not fail because of an error at the disambigua-
tion step.
In wide-coverage lexicalized grammars, a word
242
typically has about 10 corresponding lexical de-
scriptions, which implies that for a short sentence
of 10 words, we get 1010 possible taggings. It is
not reasonable to treat them individually. To avoid
this, it is convenient to use an automaton to rep-
resent the set of all paths. This automaton has lin-
ear size with regard to the initial lexical ambiguity.
The idea of using automata is not new. In partic-
ular, methods based on Hidden Markov Models
(HMM) use such a technique for part-of-speech
tagging (Kupiec, 1992; Merialdo, 1994). Using
automata, we benefit from dynamic programming
procedures, and consequently from an exponential
temporal and space speed up.
2 Abstract Grammatical Framework
Our filtering method is applicable to any lexical-
ized grammatical formalism which exhibits some
basic properties. In this section we establish these
properties and define from them the notion of Ab-
stract Grammatical Framework (AGF).
Formally, an Abstract Grammatical Frame-
work is an n-tuple (V, S,G,anc,F,p,dep)
where:
? V is the vocabulary: a finite set of words of
the modeled natural language;
? S is the set of syntactic structures used by
the formalism;
? G ? S is the grammar: the finite set of initial
syntactic structures; a finite list [t1, . . . , tn] of
elements of G is called a lexical tagging;
? anc : G ? V maps initial syntactic struc-
tures to their anchors;
? F ? S is the set of final syntactic structures
that the parsing process builds (for instance
trees);
? p is the parsing function from lexical tag-
gings to finite subsets of F;
? dep is the dependency function which maps
a couple composed of a lexical tagging and a
final syntactic structure to dependency struc-
tures.
Note that the anc function implies that the
grammar is lexicalized: each initial structure in G
is associated to an element of V. Note also that no
particular property is required on the dependency
structures that are obtained with the dep function,
they can be non-projective for instance.
We call lexicon the function (written `) from V
to subsets of G defined by:
`(w) = {t ? G | anc(t) = w}.
We will say that a lexical tagging L =
[t1, . . . , tn] is a lexical tagging of the sentence
[anc(t1), . . . ,anc(tn)].
The final structures in p (L) ? F are called the
parsing solutions of L.
Henceforth, in our examples, we will consider
the ambiguous French sentence (1).
(1) ?La belle ferme la porte?
Example 1 We consider the following toy AGF,
suited for parsing our sentence:
? V = { ?la?, ?belle?, ?ferme?, ?porte? };
? the grammar G is given in the table be-
low: each ? corresponds to an element in
G, written with the category and the French
word as subscript. For instance, the French
word ?porte? can be either a common noun
(?door?) or a transitive verb (?hangs?);
hence G contains the 2 elements CNporte and
TrVporte .
la belle ferme porte
Det ?
LAdj ? ?
RAdj ? ?
CN ? ? ? ?
Clit ?
TrV ? ?
IntrV ?
In our example, categories stand for, respec-
tively: determiner, left adjective, right adjec-
tive, common noun, clitic pronoun, transitive
verb and intransitive verb.
With respect to our lexicon, for sentence (1),
there are 3 ? 3 ? 5 ? 3 ? 2 = 270 lexical tag-
gings.
The parsing function p is such that 3 lexical
taggings have one solution and the 267 remaining
ones have no solution; we do not need to precise
the final structures, so we only give the English
translation as the result of the parsing function:
243
? p([Detla , CNbelle , TrVferme , Detla , CNporte ]) =
{?The nice girl closes the door?}
? p([Detla , LAdjbelle , CNferme , Clitla , TrVporte ]) =
{?The nice farm hangs it?}
? p([Detla , CNbelle , RAdjferme , Clitla , TrVporte ]) =
{?The firm nice girl hangs it?}
3 The Companionship Principle
We have stated in the previous section the frame-
work and the definitions required to describe our
principle.
3.1 Potential Companion
We say that u ? G is a companion of t ? G if
anc(t) and anc(u) are linked by a dependency
in dep(L,m) for some lexical tagging L which
contains t and u and some m ? p(L). The subset
of elements of G that are companions of t is called
the potential companion set of t.
The Companionship Principle says that if a lex-
ical tagging contains some t but no potential com-
panion of t, then it can be removed.
In what follows, we will generalize a bit this
idea in two ways. First, the same t can be implied
in more than one kind of dependency and hence it
can have several different companion sets with re-
spect to the different kinds of dependencies. Sec-
ondly, it can be the case that some companion t
has to be on the right (resp. on the left) to fulfill its
duty. These generalizations are done through the
notion of atomic constraints defined below.
3.2 Atomic constraints
We say that a pair (L,R) of subsets of G is an
atomic constraint for an initial structure t ? G
if for each lexical tagging L = [t1, . . . , tn] such
that p(L) 6= ? and t = ti for some i then:
? either there is some j < i such that tj ? L,
? or there is some j > i such that tj ? R.
In other words, (L,R) lists the potential com-
panions of t, respectively on the left and on the
right.
A system of constraints for a grammar G is a
function C which associates a finite set of atomic
constraints to each element of G.
The Companionship Principle is an immedi-
ate consequence of the definition of atomic con-
straints. It can be stated as the necessary condi-
tion:
The Companionship Principle
If a lexical tagging [t1, . . . , tn] has a solution
then for all i and for all atomic constraints
(L,R) ? C(ti)
? {t1, . . . , ti?1} ? L 6= ?
? or {ti+1, . . . , tn} ? R 6= ?.
Example 2 Often, constraints can be expressed
independently of the anchors. In our example, we
use the category to refer to the subset of G of struc-
tures defined with this category: LAdj for instance
refers to the subset {LAdjbelle , LAdjferme}.
We complete the example of the previous section
with the following constraints1:
? t ? CN? (Det, ?) ? C(t)
? t ? LAdj? (?, CN) ? C(t)
? t ? RAdj? (CN, ?) ? C(t)
? t ? Det? (?, CN) ? C(t)
? t ? Det? (TrV, TrV ? IntrV) ? C(t)
? t ? TrV? (Clit, Det) ? C(t)
? t ? TrV? (Det, ?) ? C(t)
? t ? IntrV? (Det, ?) ? C(t)
? t ? Clit? (?, TrV) ? C(t)
The two constraints ? and ? for instance ex-
press that every determiner is implied in two de-
pendencies. First, it must find a common noun on
its right to build a noun phrase. Second, the noun
phrase has to be used in a verbal construction.
Now, let us consider the lexical tagging:
[Detla , LAdjbelle , TrVferme , Clitla , CNporte ] and
the constraint ? (a clitic is waiting for a transitive
verb on its right). This constraint is not fulfilled
by the tagging so this tagging has no solution.
3.3 The ?Companionship Principle?
language
Actually, a lexical tagging is an element of the
formal language G? and we can consider the fol-
lowing three languages. First, G? itself. Second,
the set C ? G? corresponds to the lexical tag-
gings which can be parsed. The aim of lexical
disambiguation is then to exhibit for each sen-
tence [w1, . . . , wn] all the lexical taggings that are
within C. Third, the Companionship Principle de-
fines the language P of lexical taggings which ver-
ify this Principle. P squeezes between the two lat-
1These constraints are relative to our toy grammar and are
not linguistically valid in a larger context.
244
ter sets C ? P ? G?. Remarkably, the language
P can be described as a regular language. Since C
is presumably not a regular language (at least for
natural languages!), P is a better regular approxi-
mation than the trivial G?.
Let us consider one lexical entry t and an atomic
constraint (L,R) ? C(t). Then, the set of lexical
taggings verifying this constraint can be described
as
Lt:(L,R) = {(({L)?t({R)?)
where { denotes the complement of a set.
Since P is defined as the lexical taggings verify-
ing all constraints, P is a regular language defined
by :
P = ?
(L,R)?C(t)
Lt:(L,R)
From the Companionship Principle, we derive
a lexical disambiguation Principle which simply
tests tagging candidates with P . Notice that P can
be statically computed (at least, in theory) from
the grammar itself.
Example 3 For instance, for our example gram-
mar, this automaton is given in the figure 1 where
c=Clit, n=CN, d=Det, i=IntrV, l=LAdj, r=RAdj
and t=TrV.
A rough approximation of the size of the au-
tomaton corresponding to P can be easily com-
puted. Since each automaton Lt:(L,R) has 4 states,
P has at most 4m states where m is the num-
ber of atomic constraints. For instance, the gram-
mar used in the experiments contains more than
one atomic constraint for each lexical entry, and
m > |G| > 106. Computing P by brute-force is
then intractable.
4 Implementation of the Companionship
Principle with automata
In this section we show how to use the Compan-
ionship Principle for disambiguation. Actually, we
propose two implementations based on the princi-
ple, an exact one and an approximate one. The
latter is really fast and can be used as a first step
before applying the first one.
4.1 Automaton to represent sets of lexical
taggings
The number of lexical taggings grows exponen-
tially with the length of sentences. To avoid that,
we represent sets of lexical taggings as the sets of
paths of some acyclic automata where transitions
are labeled by elements of G . We call such an
automaton a lexical taggings automaton (LTA).
Generally speaking, such automata save a lot of
space. For instance, given a sentence [w1, . . . , wn]
the number of lexical taggings to consider at the
beginning of the parsing process is ?1?i?n|`(wi)|.
This set of taggings can be efficiently represented
as the set of paths of the automaton with n + 1
states s0, . . . , sn and with a transition from si?1
to si with the label t for each t ? `(wi). This
automaton has?1?i?n |`(wi)| transitions.
Example 4 With the data of the previous exam-
ples, we have the initial automaton:
0 1DetCN
Clit
2LAdjRAdj
CN
3
TrV
IntrVLAdjRAdj
CN
4DetCN
Clit
5CNTrV
To improve readability, only the categories are
given on the edges, while the French words can be
inferred from the position in the automaton.
4.2 Exact Companionship Principle (ECP)
Suppose we have a LTA A for a sentence
[w1, . . . , wn]. For each transition t and for each
atomic constraint in (L,R) ? C(t), we construct
an automaton At,L,R in the following way.
Each state s of At,L,R is labeled with a triple
composed of a state of the automaton A and
two booleans. The intended meaning of the first
boolean is to say that each path reaching this
state passes through the transition t. The second
boolean means that the atomic constraint (L,R) is
necessarily fulfilled.
The initial state is labeled (s0, F, F) where s0 is
the initial state of A and other states are labeled as
follows: if s u?? s? in A then, in At,L,R, we have:
1. (s, F, b) u?? (s?, T, b) if u = t
2. (s, F, b) u?? (s?, F, T) if u ? L
3. (s, F, b) u?? (s?, F, b) if u /? L
4. (s, T, b) u?? (s?, T, T) if u ? R
5. (s, T, b) u?? (s?, T, b) if u /? R
where b ? {T, F}.
245
Det
CN
li
L
AdjRjTr
V
t
I
N
n
i
LD

AdjRjTjr
t LL
ANjir

AdjRjTr
t

L
ANjir
Atjir
Le
N
ANjir
Ld
t
LC

Ll
R
t
N
i
AtjdjRjTr
LV
ANjir
ANjijTr
d

R
t
R N AdjijTr

t
AdjijRjTjr
N
D
t
R
ANjdjijTjr
t
R
ANjdjijTr
t
RN
Adjir
t

R
AtjNjdjir
LI
R
Adjijr
Ln
t
N
d
t
ANjir ARjTr
R

AtjNjdjijTr
R
t
ANjdjijr
N
AtjdjirR
L

N
AtjdjijRjTr


N
tAdjijr
R
R
t

ANjdjir
N
tAdjijRjTjr
Figure 1: The P language for G
It is then routine to show that, for each state la-
beled (s, b1, b2):
? b1 is T iff all paths from the initial state to s
contain the transition t;
? b2 is T iff for all paths p reaching this state,
either there is some u ? L or p goes through
t and there is some u ? R. In other words,
the constraint is fulfilled.
In conclusion, a path ending with (sf , T, F) with
sf a final state of A is built with transitions 1, 3
and 5 only and hence contains t but no transition
able to fulfill the constraint. The final states are:
? (sf , F, b): each path ending here does not
contain the edge t and thus the constraint
does not apply here,
? (sf , T, T) each path ending here contains the
edge t but it contains also either a transition
2 or 4, so the constraint is fulfilled by these
paths.
The size of these automata is easily bounded by
4n where n is the size of A. Using a slightly more
intricated presentation, we built automata of size
2n.
Example 5 We give below the automaton A for
the atomic constraint ? (an intransitive verb is
waiting for a determiner on its left):
Detet
CeteNliL
Cetet
AdAjRL
TeteNrVInVIn
Ad
Tetet
rVInVInAd
eteN
rVIn
VIn
Ad
N
eNeNL
etet
rVIn
VInAdN
eNet
L
eteNliLAdAjRL
etet
liLAdAjRL
eNeNliLAdAjRL
eNet
liLAdAjRL
eteNAdN
etetAdN
eNeNAdN
eNetAdN
The dotted part of the graph corresponds to the
part of the automaton that can be safely removed.
After minimization, we finally obtain:
D
etCN
el
iLiAdN
jRTrVITrV
iL
jl
RTrVITrViL
n
RTrV
ITrV
iL
 N
RTrV
ITrViL
tCNiLiAdN iL
This automaton contains 234 paths (36 lexical
taggings are removed by this constraint).
For each transition t of the lexical taggings au-
tomaton and for each constraint (L,R) ? C(t),
we construct the atomic constraint automaton
At,L,R. The intersection of these automata rep-
resents all the possible lexical taggings of the sen-
tence which respect the Companionship Principle.
246
That is, we output :
ACP =
?
1?i?n, t?A;(L,R)?C(t)
At,L,R
It can be shown that the automaton is the same
as the one obtained by intersection with the au-
tomaton of the language defined in 3.3:
ACP = A ? P
Example 6 In our example, the intersection of
the 9 automata built for the atomic constraints is
given below:
D etCN
liLAd
l
jR
Tr
jR TVIn
TNn
jR
LAd
In
TANn jN
tCN
jR
jN
jR In
jR
This automaton has 8 paths: there are 8 lexical
taggings which fulfill every constraint.
4.3 Approximation: the Quick
Companionship Principle (QCP)
The issue with the previous algorithm is that it in-
volves a large number of automata (actuallyO(n))
where n is the size of the input sentence. Each
of these automata has size O(n). The theoreti-
cal complexity of the intersection is then O(nn).
Sometimes, we face the exponential. So, let
us provide an algorithm which approximates the
Principle. The idea is to consider at the same time
all the paths that contain some transition.
We consider a LTA A. We write ?A the prece-
dence relation on transitions in an automaton A.
We define lA(t) = {u ? G, u ?A t} and rA(t) =
{u ? G, t ?A u}.
For each transition s t?? s? and each constraint
(L,R) ? C(t), if lA(t) ? L = ? and rA(t) ? R =
?, then none of the lexical taggings which use the
transition t has a solution and the transition t can
be safely removed from the automaton.
This can be computed by a double-for loop: for
each atomic constraint of each transition, verify
that either the left context or the right context of
the transition contains some structure to solve the
constraint. Observe that the cost of this algorithm
is O(n2), where n is the size of the input automa-
ton.
Note that one must iterate this algorithm until a
fixpoint is reached. Indeed, removing a transition
which serves as a potential companion breaks the
verification. Nevertheless, since for each step be-
fore the fixpoint is reached, we remove at least one
transition, we iterate the double-for at most O(n)
times. The complexity of the whole algorithm is
thenO(n3). In practice, we have observed that the
complexity is closer to O(n2): only 2 or 3 loops
are enough to reach the fixpoint.
Example 7 If we apply the QCP to the automaton
of Example 4, in the first step, only the transition
0 CN?? 1 is removed by applying the atomic con-
straint ?. In the next step, the transition 1 RAdj???? 2
is removed by applying the atomic constraint ?.
The fixpoint is reached and the output automaton
(with 120 paths) is:
0 1DetCNlt 2iLAdCj 3
iLAd
RLAd
CjTrV
IntrV
4DetCjCNlt 5CjTrV
5 The Generalized Companionship
Principle
In practice, of course, we have to face the problem
of the computation of the constraints. In a large
coverage grammar, the size of G is too big to com-
pute all the constraints in advance. However, as
we have seen in example 2 we can identify sub-
sets of G that have the same constraints; the same
way, we can use these subsets to give a more con-
cise presentation of the L and R sets of the atomic
constraints. This motivates us to define a General-
ized Principle which is stated on a quotient set of
G.
5.1 Generalized atomic constraints
Let U be a set of subsets of G that are a partition
of G. For t ? G, we write t the subset of U which
contains t.
We say that a pair (L,R) of subsets of U is a
generalized atomic constraint for u ? U if for
each lexical tagging L = [t1, . . . , tn] such that
p(L) 6= ? and u = ti for some i then:
? either there is some j < i such that tj ? L,
? or there is some j > i such that tj ? R.
A system of generalized constraints for a par-
titionU of a grammar G is a function Cwhich asso-
247
ciates a finite set of generalized atomic constraints
to each element of U.
5.2 The Generalized Principle
The Generalized Companionship Principle is then
an immediate consequence of the previous defini-
tion and can be stated as the necessary condition:
The Generalized Companionship Principle
If a lexical tagging [t1, . . . , tn] has a solution
then for all i and for all generalized atomic con-
straints (L,R) ? C(ti)
? {t1, . . . , ti?1} ? L 6= ?
? or {ti+1, . . . , tn} ? R 6= ?.
Example 8 The constraints given in example 2
are in fact generalized atomic constraints on the
set (recall that we write LAdj for the 2 elements
set {LAdjbelle , LAdjferme}):
U = {Det, LAdj, RAdj, CN, Clit, TrV, IntrV}.
Then the constraints are expressed on |U| = 7 el-
ements and not on |G| = 13.
A generalized atomic constraint on U can, of
course, be expressed as a set of atomic constraints
on G: let u ? U and t ? G such that t = u
(L,R) ? C(u) =?
(?
L?L
L,
?
R?R
R
)
? C(t)
5.3 Implementation of lexicalized grammars
In implementations of large coverage linguistic re-
sources, it is very common to have, first, the de-
scription of the set of ?different? structures needed
to describe the modeled natural language and then
an anchoring mechanism that explains how words
of the lexicon are linked to these structures. We
call unanchored grammar the set U of differ-
ent structures (not yet related to words) that are
needed to describe the grammar. In this context,
the lexicon is split in two parts:
? a function ` from V to subsets of U,
? an anchoring function ? which builds the
grammar elements from a word w ? V and
an unanchored structure u ? `(w); we sup-
pose that ? verifies that anc(?(w, u)) = w.
In applications, we suppose that U, ` and ? are
given. In this context, we define the grammar as
the codomain of the anchoring function:
G = ?
w?V,u?`(w)
?(w, u)
Now, we can define generalized constraints on
the unanchored grammar, which are independent
of the lexicon and can be computed statically for a
given unanchored grammar.
6 Application to Interaction Grammars
In this section, we apply the Companionship Prin-
ciple to the Interaction Grammars formalism. We
first give a short and simplified description of IG
and an example to illustrate them at work; we re-
fer the reader to (Guillaume and Perrier, 2008) for
a complete and detailed presentation.
6.1 Interaction Grammars
We illustrate some of the important features on
the French sentence (2). In this sentence, ?la?
is an object clitic pronoun which is placed before
the verb whereas the canonical place for the (non-
clitic) object is on the right of the verb.
(2) ?Jean la demande.? [John asks for it]
The set F of final structures, used as output of
the parsing process, contains ordered trees called
parse trees (PT). An example of a PT for the sen-
tence (2) is given in Figure 2. A PT for a sentence
contains the words of the sentence or the empty
word  in its leaves (the left-right order of the tree
leaves follows the left-right order of words in the
input sentence). The internal nodes of a PT repre-
sent the constituents of the sentence. The morpho-
syntactic properties of these constituents are de-
scribed with feature structures (only the category
is shown in the figure).
As IG use the Model-Theoretic Syntax (MTS)
framework, a PT is defined as the model of a set
of constraints. Constraints are defined at the word
level: words are associated to a set of constraints
formally described as a polarized tree descrip-
tion (PTD). A PTD is a set of nodes provided with
relations between these nodes. The three PTDs
used to build the model above are given in Fig-
ure 3. The relations used in the PTDs are: imme-
diate dominance (lines) and immediate sisterhood
(arrows). Nodes represent syntactic constituents
248
A2-A3=S
B1-B3=NP C2-C3=V D2-D3=NP
Jean E2=Cl F2-F3=V ?
la demande
Figure 2: The PT of sentence (2)
and relations express structural dependencies be-
tween these constituents.
Moreover, nodes carry a polarity: the set of po-
larities is {+,?,=,?}. A + (resp.?) polarity
represents an available (resp. needed) resource, a
? polarity describes a node which is unsaturated.
Each + must be associated to exactly one ? (and
vice versa) and each ? must be associated to at
least another polarity.
A2-3=
SB1N
PCVD
JCVe aC-3=
nCEJl FCVe ?
l1
PdED
Adm3= JdEe adm3=
FdEe
B1NB
Figure 3: PTDs for the sentence (2)
Now, we define a PT to be a model of a set of
PTDs if there is a surjective function I from nodes
of the PTDs to nodes of the PT such that:
? relations in the PTDs are realized in the PT:
if M is a daughter (resp. immediate sister)
of N in some PTD then I(M) is a daughter
(resp. immediate sister) of I(N);
? each node N in the PT is saturated: the
composition of the polarities of the nodes in
I?1(N) with the associative and commuta-
tive rule given in Table 4 is =;
? the feature structure of a node N in the PT is
the unification of the feature structures of the
nodes in I?1(N).
One of the strong points of IG is the flexibility
given by the MTS approach: PTDs can be partially
superposed to produce the final tree (whereas su-
perposition is limited in usual CG or in TAG for
instance). In our example, the four grey nodes
in the PTD which contains ?la? are superposed
to the four grey nodes in the PTD which contains
?demande? to produce the four grey nodes in the
model.
? ? + =
? ? ? + =
? ? =
+ + =
= =
Figure 4: Polarity composition
In order to give an idea of the full IG system,
we briefly give here the main differences between
our presentation and the full system.
? Dominance relations can be underspecified:
for instance a PTD can impose a node to be an
ancestor of another one without constraining
the length of the path in the model. This is
mainly used to model unbounded extraction.
? Sisterhood relations can also be underspeci-
fied: when the order on subconstituents is not
total, it can be modeled without using several
PTDs.
? Polarities are attached to features rather than
nodes: it sometimes gives more freedom
to the grammar writer when the same con-
stituent plays a role in different constructions.
? Feature values can be shared between several
nodes: once again, this is a way to factorize
the unanchored grammar.
The application of the Companionship Princi-
ple is described on the reduced IG but it can
be straightforwardly extended to full IG with
unessential technical details.
Following the notation given in 5.3, an IG is
made of:
? A finite set V of words;
? A finite set U of unanchored PTDs (without
any word attached to them);
? A lexicon function ` from V to subsets of U.
249
When t ? `(w), we can construct the anchored
PTD ?(w, u). Technically, in each unanchored
PTD u, a place is marked to be the anchor, i.e.
to be replaced by the word during the anchoring
process. Moreover, the anchoring process can also
be used to refine some features. The fact that
the feature can be refined gives more flexibility
and more compactness to the unanchored gram-
mar construction. In the French IG grammar, the
same unanchored PTD can be used for masculine
or feminine common nouns and the gender is spec-
ified during the anchoring to produce distinct an-
chored PTDs for masculine and feminine nouns. G
is defined by:
G = ?
w?V,u?`(w)
?(w, u)
The parsing solutions of a lexical tagging is the
set of PTs that are models of the list of PTDs de-
scribed by the lexical tagging:
p(L) = {m ? F | m is a model of L}
With the definitions of this section, an IG is a
special case of AGF as defined in section 2.
6.2 Companionship Principle for IG
In order to apply the Companionship Principle, we
have to explain how the generalized atomic con-
straints are built for a given grammar. One way
is to look at dependency structures but in IG po-
larities are built in and then we can read the de-
pendency information we need directly on polari-
ties. A requirement to build a model is the satura-
tion of all the polarities. This requirement can be
expressed using atomic constraints. Each time a
PTD contains an unsaturated polarity +, ? or ?,
we have to find some other compatible dual po-
larity somewhere else in the grammar to saturate
it.
From the general MTS definition of IG above,
we can define a step by step process to build mod-
els of a lexical tagging. The idea is to build in-
crementally the interpretation function I with the
atomic operation of node merging. In this atomic
operation, we choose two nodes and make the hy-
pothesis that they have the same image through I
and hence that they can be identified.
Now, suppose that the unanchored PTD u con-
tains some unsaturated polarity p. We can use the
atomic operation of node merging to test if the
unanchored PTD u? can be used to saturate the po-
larity p. Let L (resp R) be the set of PTDs that
can be used on the left (resp. on the right) of u
to saturate p, then (L,R) is a generalized atomic
constraint in C(u).
7 Companionship Principle for other
formalisms
As we said in the introduction, many current gram-
matical formalisms can more or less directly be
used to generate dependency structures and hence
are candidate for disambiguation with the Com-
panionship Principle. With IG, we have seen that
dependencies are strongly related to polarities: de-
pendency constraints in IG are built with the polar-
ity system.
We give below two short examples of polarity
use to define atomic constraints on TAG and on
CG. We use, as for IG, the polarity view of depen-
dencies to describe how the constraints are built.
7.1 Tree Adjoining Grammars
Feature-based Tree Adjoining Grammars (here-
after FTAG) (Joshi, 1987) are a unification based
version of Tree Adjoining Grammars. An FTAG
consists of a set of elementary trees and of two
tree composition operations: substitution and ad-
junction. There are two kinds of trees: auxiliary
and initial. Substitution inserts a tree t with root
r onto a leaf node l of another tree t? under the
condition that l is marked as a place for substitu-
tion and l and r have compatible feature structures.
Adjunction inserts an auxiliary tree t into a tree t?
by splitting a node n of t? under the condition that
the feature structures of the root and foot nodes of
t are compatible with the top and bottom ones of
n.
Getting the generalized atomic constraints and
the model building procedure for lexical tagging
is extremely similar to what was previously de-
scribed for IG if we extend the polarization pro-
cedure which was described in (Gardent and Kow,
2005) to do polarity based filtering in FTAG. The
idea is that for each initial tree t, its root of cate-
gory C is marked as having the polarity +C, and
its substitution nodes of category S are marked as
having the polarity ?S. A first constraint set con-
tains trees t? whose root is polarized +S and such
that feature structures are unifiable. A second con-
straint set contains trees t?? which have a leaf that
is polarized ?C. We can extend this procedure to
250
auxiliary trees: each auxiliary tree t of category A
needs to be inserted in a node of category A of an-
other tree t?. This gives us a constraint in the spirit
of the ? polarity in IG: C(t) contains all the trees
t? in which t could be inserted2.
7.2 Categorial Grammars
In their type system, Categorial Grammars en-
code linearity constraints and dependencies be-
tween constituents. For example, a transitive verb
is typed NP\S/NP , meaning that it waits for a
subject NP on its left and an object NP on its
right. This type can be straightforwardly decom-
posed as two ?NP and one +S polarities. Then
again, getting the generalized atomic constraints
is immediate and in the same spirit as what was
described for IG.
7.3 Non-lexicalized formalisms
The lexicalization condition stated in section 2
excludes non-lexicalized formalisms like LFG or
HPSG. Nothing actually prevents our method
from being applied to these, but adding non-
lexicalized combinators requires to complexify the
formal account of the method. Adapting our
method to HPSG would result in a generaliza-
tion and unification of some of the techniques de-
scribed in (Kiefer et al, 1999).
8 Experimental results
8.1 Setup
The experiments are performed using a French IG
grammar on a set of 31 000 sentences taken from
the newspaper Le Monde.
The French grammar we consider (Perrier,
2007) contains |U| = 2 088 unanchored trees.
It covers 88% of the grammatical sentences and
rejects 85% of the ungrammatical ones on the
TSNLP (Lehmann et al, 1996) corpus.
The constraints have been computed on the
unanchored grammar as explained in section 5:
each tree contains several polarities and therefore
several atomic constraints. Overall, the grammar
contains 20 627 atomic constraints. It takes 2 days
to compute the set of constraints and the results
can be stored in a constraints file of 10MB. Of
course, an atomic constraint is more interesting
when the sizes of L and R are small. In our gram-
mar, 50% of the constraints set (either R or L)
2Note that in the adjunction case, the constraint is not ori-
ented and then L= R.
contain at most 40 elements and 80% of these sets
contain at most 200 elements over 2 088.
We give in figure 5 the number of sentences of
each length in the corpus we consider.
0?
500?
1000?
1500?
2000?
2500?
3000?
3500?
4000?
4500?
5000?
6? 7? 8? 9? 10? 11? 12? 13? 14? 15? 16? 17? 18? 19?
num
ber?o
f?sen
tenc
es?
sentence?length?(number?of?words)?
Figure 5: number of sentences of each length
8.2 Results
Two preliminary comments need to be made on
the treatment of the results.
First, as we observed above, the number n
of lexical taggings is a priori exponential in the
length of the sentence. We thus consider its log.
Moreover, because we use a raw corpus, some
sentences are considered as ungrammatical by the
grammar; in this case it may happen that the dis-
ambiguation method removes all taggings. In or-
der to avoid undefined values when n = 0, we in
fact consider log10(1 + n).
Second, as expected, the ECP method is more
time consuming and for some sentences the time
and/or memory required is problematic. To be able
to apply the ECP to a large number of sentences,
we have used it after another filtering method
based on polarities and described in (Bonfante et
al., 2004).
Thus, for each sentence we have computed 3
different filters, each one finer than the previous:
? QCP the Quick Companionship Principle;
? QCP+POL QCP followed by a filtering tech-
nique based on polarity counting;
? QCP+POL+ECP the Exact Companionship
Principle applied to the previous filter.
Figure 6 displays the mean computation time
for each length: it confirms that the ECP is more
time consuming and goes up to 5s for our long sen-
tences.
251
Finally, we report the number of lexical tag-
gings that each method returns. Figure 7 displays
the mean value of log10(1 + n) where n is either
the initial number of lexical taggings or the num-
ber of lexical taggings left by the filter.
0.01?
0.1?
1?
10?
6? 7? 8? 9? 10? 11? 12? 13? 14? 15? 16? 17? 18? 19?
?es?(in
?s)?
sentence?length?(number?of?words)?
QCP?
QCP+POL?
QCP+POL+ECP?
Figure 6: mean execution time (in s)
0?
2?
4?
6?
8?
10?
12?
14?
16?
18?
6? 7? 8? 9? 10? 11? 12? 13? 14? 15? 16? 17? 18? 19?
Log?(
1+n)
?
sentence?length?(number?of?words)?
QCP?
QCP+POL?
QCP+POL+ECP?
Ini?l?
Figure 7: number of taggings (initial and after the
3 disambiguation methods)
We can observe that the slope of the lines cor-
responds to the mean word ambiguity: if the
mean ambiguity is a then the number of taggings
for a sentence of length n is about an and then
log(an) = n ? log(a). As a consequence, the mean
ambiguity can be read as 10s where s is the slope
in the last figure.
An illustration is given in figure 8 which ex-
hibits the mean word ambiguity for sentences of
length 16.
init QCP QCP+POL QCP+POL+ECP
6.13 3.41 1.93 1.41
Figure 8: mean word ambiguity for sentences of
length 16
9 Conclusion
We have presented a disambiguation method
based on dependency constraints which allows to
filter out many wrong lexical taggings before en-
tering the deep parsing. As this method relies on
the computation of static constraints on the lin-
guistic data and not on a statistical model, we can
be sure that we will never remove any correct lex-
ical tagging. Moreover, we manage to apply our
methods to an interesting set of data and prove that
it is efficient for a large coverage grammar and not
only for a toy grammar.
These results are also an encouragement to de-
velop further this kind of disambiguation methods.
In the near future, we would like to explore some
improvements.
First, we have seen that our principle cannot be
computed on the whole grammar and that in its im-
plementation we consider unanchored structures.
We would like to explore the possibility of com-
puting finer constraints (relative to the full gram-
mar) on the fly for each sentence. We believe that
this can eliminate some more taggings before en-
tering the deep parsing.
Concerning the ECP, as we have seen, there is a
kind of interplay between the efficiency of the fil-
tering and the time of the computation. We would
like to explore the possibility to define some in-
termediate way between QCP and ECP either by
using approximate automata or using the ECP but
only on a subset of elements where it is known to
be efficient.
Another challenging method we would like to
investigate is to use the Companionship Principle
not only as a disambiguation method but as a guide
for the deep parsing. Actually, we have observed
for at least 20% of the words that dependencies are
completely determined by the filtering methods. If
deep parsing can be adapted to use this observation
(this is the case for IG), this can be of great help.
Finally, we can improve the filtering using both
worlds: the Companionship Principle and the po-
larity counting method. Two different constraints
cannot be fulfilled by the same potential compan-
ion: this may allow to discover some more lexical
taggings that can be safely removed.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful comments and suggestions.
252
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Comput.
Linguist., 25(2):237?265.
G. Bonfante, B. Guillaume, and G. Perrier. 2004.
Polarization and abstraction of grammatical for-
malisms as methods for lexical disambiguation. In
CoLing 2004, pages 303?309, Gene`ve, Switzerland.
P. Boullier. 2003. Supertagging : A non-statistical
parsing-based approach. In Pro- ceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 55?65, Nancy, France.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In COLING ?04: Proceedings of the 20th in-
ternational conference on Computational Linguis-
tics, page 282, Morristown, NJ, USA. Association
for Computational Linguistics.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building Deep Dependency Structures with a Wide-
Coverage CCG Parser. In Proceedings of ACL?02,
pages 327?334, Philadephia, PA.
Ph. de Groote. 2001. Towards abstract categorial
grammars. In Association for Computational Lin-
guistics, 39th Annual Meeting and 10th Conference
of the European Chapter, Proceedings of the Confer-
ence, pages 148?155.
C. Gardent and E. Kow. 2005. Generating and se-
lecting grammatical paraphrases. Proceedings of the
ENLG, Aug.
B. Guillaume and G. Perrier. 2008. Interaction Gram-
mars. Research Report RR-6621, INRIA.
A. Joshi and O. Rambow. 2003. A Formalism for De-
pendency Grammar Based on Tree Adjoining Gram-
mar. In Proceedings of the Conference on Meaning-
Text Theory.
A. Joshi. 1987. An Introduction to Tree Adjoining
Grammars. Mathematics of Language.
S. Kahane. 2006. Polarized unification grammar. In
Proceedings of Coling-ACL?02, Sydney.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A bag of useful techniques for
efficient and robust parsing. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 473?480, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Koller and M. Kuhlmann. 2009. Dependency
trees and the strong generative capacity of CCG. In
EACL? 2009, Athens, Greece.
J. Kupiec. 1992. Robust Part-of-Speech Tagging Us-
ing a Hidden Markov Model. Computer Speech and
Language, 6(3):225?242.
F. Lamarche. 2008. Proof Nets for Intuitionistic Linear
Logic: Essential Nets. Technical report, INRIA.
J. Lambek. 1958. The mathematics of sentence struc-
ture. American mathematical monthly, pages 154?
170.
S. Lehmann, S. Oepen, S. Regnier-Prost, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Esti-
val, E. Dauphin, H. Compagnion, J. Baur, L. Balkan,
and D. Arnold. 1996. TSNLP: Test Suites for Nat-
ural Language Processing. In Proceedings of the
16th conference on Computational linguistics, pages
711?716.
J. Marchand, B. Guillaume, and G. Perrier. 2009.
Analyse en de?pendances a` l?aide des grammaires
d?interaction. In Actes de TALN 09, Senlis, France.
B. Merialdo. 1994. Tagging English Text with a Prob-
abilistic Model. Computational linguistics, 20:155?
157.
M. Moortgat and G. Morrill. 1991. Heads and phrases.
Type calculus for dependency and constituent struc-
ture. In Journal of Language, Logic and Informa-
tion.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 155?163, Sydney, Australia, July.
Association for Computational Linguistics.
G. Perrier. 2007. A French Interaction Grammar. In
RANLP 2007, pages 463?467, Borovets Bulgarie.
M. Steedman. 2000. The Syntactic Process. MIT
Press.
L. Tesnie`re. 1959. E?le?ments de syntaxe structurale.
Klinksieck.
253
Modular Graph Rewriting to Compute Semantics
Guillaume Bonfante
Nancy-Universite? - LORIA
bonfante@loria.fr
Bruno Guillaume
INRIA - LORIA
guillaum@loria.fr
Mathieu Morey
Nancy-Universite? - LORIA
moreymat@loria.fr
Guy Perrier
Nancy-Universite? - LORIA
perrier@loria.fr
Abstract
Taking an asynchronous perspective on the syntax-semantics interface, we propose to use modu-
lar graph rewriting systems as the model of computation. We formally define them and demonstrate
their use with a set of modules which produce underspecified semantic representations from a syn-
tactic dependency graph. We experimentally validate this approach on a set of sentences. The results
open the way for the production of underspecified semantic dependency structures from corpora an-
notated with syntactic dependencies and, more generally, for a broader use of modular rewriting
systems for computational linguistics.
Introduction
The aim of our work is to produce a semantic representation of sentences on a large scale using a formal
and exact approach based on linguistic knowledge. In this perspective, the design of the syntax-semantics
interface is crucial.
Based on the compositionality principle, most models of the syntax-semantics interface use a syn-
chronous approach: the semantic representation of a sentence is built step by step in parallel with its
syntactic structure. According to the choice of the syntactic formalism, this approach is implemented in
different ways: in a Context-Free Grammars (CFG) style framework, every syntactic rule of a grammar
is associated with a semantic composition rule, as in the classical textbook by Heim and Kratzer (1998);
following the principles introduced by Montague, Categorial Grammars use an homomorphism from the
syntax to the semantics (Carpenter (1992)). HPSG integrates the semantic and syntactic representations
in feature structures which combine by unification (Copestake et al (2005)). LFG follows a similar prin-
ciple (Dalrymple (2001)). In a synchronous approach, the syntax-semantics interface closely depends on
the grammatical formalism. Building such an interface can be very costly, especially if we aim at a large
coverage for the grammar.
In our work, we have chosen an asynchronous approach in the sense that we start from a given
syntactic analysis of a sentence to produce a semantic representation. With respect to the synchronous
approach, a drawback is that the reaction of the semantics on the syntax is delayed. On the other hand,
the computation of the semantics is made relatively independent from the syntactic formalism. The only
constraint is the shape of the output of the syntactic analysis.
In the formalisms mentioned above, the syntactic structure most often takes the form of a phrase
structure, but the choice of constituency for the syntax makes the relationship with the semantics more
complicated. We have chosen dependency graphs, because syntactic dependencies are closely related
to predicate-argument relations. Moreover, they can be enriched with relations derived from the syntax,
which are usually ignored, such as the arguments of infinitives or the anaphora determined by the syntax.
One may observe that our syntactic representation of sentences involves plain graphs and not trees.
Indeed, these relations can give rise to multiple governors and dependency cycles. On the semantic side,
65
we have also chosen graphs, which are widely used in different formalisms and theories, such as DMRS
(Copestake (2009)) or MTT (Mel?c?uk (1988)) .
The principles being fixed, our problem was then to choose a model of computation well suited
to transforming syntactic graphs into semantic graphs. The ?-calculus, which is widely used in formal
semantics, is not a good candidate because it is appropriate for computing on trees but not on graphs. Our
choice naturally went to graph rewriting. Graph rewriting is barely used in computational linguistics;
it could be due to the difficulty to manage large sets of rules. Among the pioneers in the use of graph
rewriting, we mention Hyvo?nen (1984); Bohnet and Wanner (2001); Crouch (2005); Jijkoun and de Rijke
(2007); Be?daride and Gardent (2009); Chaumartin and Kahane (2010).
A graph rewriting system is defined as a set of graph rewrite rules and a computation is a sequence
of rewrite rule applications to a given graph. The application of a rule is triggered via a mechanism of
pattern matching, hence a sub-graph is isolated from its context and the result is a local modification of
the input. This allows a linguistic phenomenon to be easily isolated for applying a transformation.
Since each step of computation is fired by some local conditions in the whole graph, it is well known
that one has no grip on the sequence of rewriting steps. The more rules, the more interaction between
rules, and the consistency of the whole rule system becomes difficult to maintain. This bothers our
ambition of a large coverage for the grammar. To solve this problem, we propose to organize rules in
modules. A module is a set of rules that is linguistically consistent and represents a particular step of
the transformation. For instance, in our proposal, there is a module transforming the syntactic arguments
of verbs, predicative nouns and adjectives into their semantic arguments. Another module resolves the
anaphoric links which are internal to the sentence and determined by the syntax.
From a computational point of view, the grouping of a small number of rules inside a module allows
some optimizations in their application, thus leading to efficiency. For instance, the confluence of rewrit-
ing is a critical feature ? one computes only one normal form, not all of them ? for the performance
of the program. Since the underlying relation from syntax to semantics is not functional but relational,
the system cannot be globally confluent. Then, it is particularly interesting to isolate subsets of conflu-
ent rules. Second point, with a small number of rules, one gets much more control on their output. In
particular, it is possible to automatically infer some invariant properties of graphs along the computation
within a particular module. Thus, it simplifies the writing of the rules for the next modules. It is also
possible to plan a strategy in the global evaluation process.
It is well known that syntactic parsers produce outputs in various formats. As a by-product of our
approach, we show that the choice of the input format (that is the syntax) seems to be of low importance
overall. Indeed, as far as two formats contain the same linguistic information with different representa-
tions, a system of rewrite rules can be designed to transform any graph from one format to another as a
preliminary step. The same remark holds for the output formats.
To illustrate our proposal, we have chosen the Paris7 TreeBank (hereafter P7TB) dependency format
defined by Candito et al (2010) as the syntactic input format and the Dependency MRS format (hereafter
DMRS) defined by Copestake (2009) as the semantic output format. We chose those two formats because
the information they represent, if it is not complete, is relatively consensual and because both draw on
large scale experiments: statistical dependency parsing for French1 on the one hand and the DELPH-IN
project2 on the other hand.
Actually, in our experiments, since we do not have an appropriate corpus annotated according to the
P7TB standard, we used our syntactic parser LEOPAR3 whose outputs differ from this standard and we
designed a rewriting system to go from one format to the other.
The paper is organized as follows. In section 1, we define our graph rewriting calculus, the ?-calculus.
In Section 2, we describe the particular rewriting system that is used to transform graphs from the syn-
tactic P7TB format into the DMRS semantic format. In Section 3, we present experimental results on a
test suite of sentences.
1http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html
2http://www.delph-in.net/
3http://leopar.loria.fr
66
1 The ?-calculus, a graph rewriting calculus
Term rewriting and tree rewriting can be defined in a straightforward and canonical way. Graph rewriting
is much more problematic and there is unfortunately no canonical definition of a graph rewriting system.
Graph rewriting can be defined through a categorical approach like SPO or DPO (Rozenberg (1997)).
But, in practice, it is much easier to use a more operational view of rewriting where modification of
the graph (the ?right-hand side? of a rule) is defined by means of a set of commands; the control of the
way rules are applied (the ?left hand-side?) still uses pattern matching as this is done in traditional graph
rewriting.
In this context, a rule is a pair of a pattern and a sequence of commands. We give below the formal
materials about graphs, patterns, matchings and commands. We illustrate the section with examples of
rules and of rewriting.
1.1 Graph definition
In the following, we suppose given a finite set L of edge labels corresponding to the kind of dependencies
used to describe sentences. They may correspond to syntax or to semantics. For instance, we use
L = {SUJ, OBJ, ARG1, ANT, . . .}.
To decorate vertices, we use the standard notion of feature structures. Let N be a finite set of
feature names and A be a finite set of atomic feature values. In our example, N = {cat,mood, . . .} and
A = {passive, v, n, . . .}. A feature is a pair made of a feature name and a set of atomic values. The
feature (cat, {v, aux}) means that the feature name cat is associated to either the value v or aux. In the
sequel, we use the notation cat = v|aux for this feature. Two features f = v and f ? = v? are compatible
whenever f = f ? and v ? v? 6= ?.
A feature structure is a finite set of features such that each feature name occurs at most once. F de-
notes the set of feature structures. Two feature structures are compatible if their respective features with
the same name are pairwise compatible.
A graph G is then defined by a 6-tuple (V, fs, E , lab, ?, ?) with:
? a finite set V of vertices;
? a labelling function fs from V to F ;
? a finite set E of edges;
? a labelling function lab from E to L;
? two functions ? and ? from E to V which give the source and the target of each edge.
Moreover, we require that two edges between the same couple of nodes cannot have the same label.
1.2 Patterns and matchings
Formally, a pattern is a graph and a matching ? of a pattern P = (V ?, fs?, E ?, lab?, ??, ? ?) into a graph
G = (V, fs, E , lab, ?, ?) is an injective graph morphism from P to G. More precisely, ? is a couple of
injective functions: ?V from V ? to V and ?E from E ? to E which:
? respects vertex labelling: fs(?V(v)) and fs?(v) are compatible;
? respects edge labelling: lab(?E(e)) = lab?(e);
? respects edge sources: ?(?E(e)) = ?V(??(e));
? respects edge targets: ?(?E(e)) = ?V(? ?(e)).
67
1.3 Commands
Commands are low-level operations on graphs that are used to describe the rewriting of the graph within
a rule application. In the description below, we suppose to be given a pattern matching ? : P ? G. We
describe here the set of commands which we used in our experiment so far. Naturally, this set could be
extended.
? del edge(?, ?, `) removes the edge labelled ` between ? and ?. More formally, we suppose that
? ? VP , ? ? VP andP contains an edge e from? to ? with label ` ? L. Then, del edge(?, ?, `)(G)
is the graph G without the edge ?(e). In the following, we give only the intuitive definition of the
command: thanks to injectivity of the matching ?, we implicitly forget the distinction between x
and ?(x).
? add edge(?, ?, `) adds an edge labelled ` between ? and ?. Such an edge is supposed not to exist
in G.
? shift edge(?, ?) modifies all edges that are incident to ?: each edge starting from ? is moved to
start from ?; similarly each edge ending on ? is moved to end on ?;
? del node(?) removes the ? node in G. If G contains edges starting from ? or ending on ?, they
are silently removed.
? add node(?) adds a new node with identifier ? (a fresh name).
? add feat(?, f = v) adds the feature f = v to the node ?. If ? already contains a feature name f ,
it is replaced by the new one.
? copy feat(?, ?, f) copies the value of the feature named f from the node ? to the node ?. If ?
does not contain a feature named f , nothing is done. If ? already contains a feature named f , it is
replaced by the new value.
Note that commands define a partial function on graphs: the action add edge(?, ?, `) is undefined
on a graph which already contains an edge labelled ` from ? to ?.
The action of a sequence of commands is the composition of actions of each command. Sequences
of commands are supposed to be consistent with the pattern:
? del edge always refers to an edge described in the pattern and not previously modified by a
del edge or a shift edge command;
? each command refers only to identifiers defined either in the pattern or in a previous add node;
? no command refers to a node previously deleted by a del node command.
Finally, we define a rewrite rule to be a pair of a pattern and a consistent sequence of commands.
A first example of a rule is given below with the pattern on the left and the sequence of commands
on the right. This rule called INIT PASSIVE is used to remove the node corresponding to the auxiliary
of the passive construction and to modify the features accordingly.
INIT PASSIVE
?
cat = v
voice = active
?
cat = v
voice = unk
AUX PASS
c1 = copy feat(?, ?,mood)
c2 = copy feat(?, ?, tense)
c3 = add feat(?, voice = passive)
c4 = del edge(?, ?, AUX PASS)
c5 = shift edge(?, ?)
c6 = del node(?)
Our second example (PASSIVE ATS) illustrates the add node command. It is used in a passive
construction where the semantic subject of the verb is not realized syntactically.
68
PASSIVE ATS
?
cat = v
voice = passive
? ?
SUJ ATS c1 = del edge(?, ?, SUJ)
c2 = add edge(?, ?, OBJ)
c3 = del edge(?, ?, ATS)
c4 = add edge(?, ?, ATO)
c5 = add feat(?, voice = active)
c6 = add node(?)
c7 = add edge(?, SUJ, ?)
1.4 Rewriting
We consider a graph G and a rewrite rule r = (P, [c1, . . . , ck]). We say that G? is obtained from G by a
rewrite step with the r rule (written G ??r G?) if there is a matching morphism ? : P ? G and G? is
obtained from G by applying the composition of commands ck ? . . . ? c1.
Let us now illustrate two rewrite steps with the rules above. Consider the first graph below which is
a syntactic dependency structure for the French sentence ?Marie est conside?re?e comme brillante? [Mary
is considered as bright]. The second graph is obtained by application of the INIT PASSIVE rewrite rule
and the last one with the PASSIVE ATS rewrite rule.
Marie
cat = np
lemma = MARIE
est
cat = v
lemma = E?TRE
voice = active
tense = present
conside?re?e
cat = v
lemma = CONSIDE?RER
voice = unk
comme
cat = prep
lemma = COMME
brillante
cat = adj
lemma = BRILLANT
SUJ
AUX PASS ATS OBJ
Marie
cat = np
lemma = MARIE
est conside?re?e
cat = v
lemma = CONSIDE?RER
voice = passive
tense = present
comme
cat = prep
lemma = COMME
brillante
cat = adj
lemma = BRILLANT
SUJ ATS OBJ
 Marie
cat = np
lemma = MARIE
est conside?re?e
cat = v
lemma = CONSIDE?RER
voice = active
tense = present
comme
cat = prep
lemma = COMME
brillante
cat = adj
lemma = BRILLANT
SUJ
OBJ ATO OBJ
1.5 Modules and normal forms
A module contains a set of rewrite rules but, in order to have a finer control on the output of these
modules, it is useful to declare some forbidden patterns. Hence a module is defined by a set R of rules
and a set P of forbidden patterns.
For a given module M = (R,P), we say that G? is an M-normal form of the graph G if there is a
sequence of rewriting steps with rules of R from G to G?: G ??r1 G1 ??r2 G2 . . . ??rk G?, if no rule
of R can be applied to G? and no pattern of P matches in G?.
In our experiment, forbidden patterns are often used to control the subset of edges allowed in normal
forms. For instance, the NORMAL module contains the forbidden pattern: AUX PASS . Hence, we
can then safely suppose that no graph contains any AUX PASS edge afterward.
2 From syntactic dependency graphs to semantic graphs
Linguistic theories diverge on many issues including the exact definition of the linguistic levels and
the relationships between them. Our aim here is not to commit to any linguistic theory but rather to
69
demonstrate that graph rewriting is an adequate and realistic computational framework for the syntax-
semantics interface. Consequently, our approach is bound to neither the (syntactic and semantic) formats
we have chosen nor the transformation modules we have designed; both are mainly meant to exemplify
our proposal.
2.1 Representational formats
Our syntactic and semantic formats both rely on the notion of linguistic dependency. The syntactic
format is an enrichment of the one which was designed to annotate the French Treebank (Abeille? and
Barrier (2004)) with surface syntactic dependencies (Candito et al (2010)). The enrichment is twofold:
? if they are present in the sentence, the deep arguments of infinitives and participles (from participial
subordinate clauses) are marked with the usual labels of syntactic functions,
? the anaphora relations that are predictable from the syntax (i.e. the antecedents of relative, reflexive
and repeated pronouns) are marked with a special label ANT.
This additional information can already be provided by many syntactic parsers and is particularly inter-
esting to compute semantics.
The semantic format is DependencyMinimal Recursion Semantics (DMRS) which was introduced by
Copestake (2009) as a compact and easily readable equivalent to Robust Minimal Recursion Semantics
(RMRS), which was defined by Copestake (2007). This underspecified semantic formalism was designed
for large scale experiments without committing to fine-grained semantic choices. DMRS graphs contain
the predicate-argument relations, the restriction of generalized quantifiers and the mode of combination
between predicates. Predicate-argument relations are labelled ARGi, where i is an integer following a
fixed order of obliqueness SUJ, OBJ, ATS, ATO, A-OBJ, DE-OBJ. . . . Naturally, the lexicon must be consistent
with this ordering. The restrictions of generalized quantifiers are labelled RSTR ; their bodies are not
overtly expressed but can be retrieved from the graph. There are three ways of combining predicates:
? EQ when two predicates are elements of a same conjunction;
? H when a predicate is in the scope of another predicate; it is not necessarily one of its arguments
because quantifiers may occur between them;
? NEQ for all other cases.
2.2 Modular rewriting system
Graph rewriting allows to proceed step by step to the transformation of a syntactic graph into a semantic
one, by associating a rewrite rule to each linguistic rule. While the effect of every rule is local, grouping
rules in modules allows a better control on the global effect of all rules.
We do not have the space here to propose a system of rules that covers the whole French grammar.
We however propose six modules which cover a significative part of this grammar (cleft clauses, coor-
dination, enumeration, comparatives and ellipses are left aside but they can be handled by other rewrite
modules):
? NORMAL handles the regular syntactic transformations involving predicates: it computes tense
and transforms all redistributions of arguments (passive and middle voices, impersonal construc-
tions and the combination of them) to the active canonical form. This reduces the number of rules
required to produce the predicate-argument relations in the ARG module below.
? PREP removes affixes, prepositions and complementizers.
? ARG transforms the verbal, nominal and adjectival predicative phrases into predicate-argument
relations.
70
? DET translates the determiner dependencies (denoted DET) to generalized quantifiers.
? MOD interprets the various modifier dependencies (denoted MOD), according to their specificity:
adjectives, adverbs, adjunct prepositional phrases, participial clauses, relative clauses, adjunct
clauses.
? ANA interprets all anaphoric relations that are determined by the syntax (denoted ANT).
Modules provide an easy way to control the order in which rules are fired. In order to properly set up the
rules in modules, we first have to fix the global ordering of the modules. Some ordering constraints are
evident: for instance, NORMAL must precede PREP, which must precede ARG. The rules we present in
the following are based on the order NORMAL, PREP, ARG, DET, MOD, ANA.
2.2.1 Normalization of syntactic dependencies
The NORMAL module has two effects: it merges tense and voice auxiliaries with their past participle
and brings all the argument redistributions back to the canonical active form. This module accounts
for the passive and middle voices and the impersonal construction for verbs that are not essentially
impersonal. The combination of the two voices with the impersonal construction is naturally expressed
by the composition of the corresponding rewrite rules. The two rules given in section 1.4 are part of this
module. The first rule (INIT PASSIVE) merges the past participle of the verb with its passive auxiliary.
The auxiliary brings its mood and tense to the verb, which is marked as being passive. The second rule
(PASSIVE ATS) transforms a passive verb with a subject and an attribute of the subject into its active
equivalent with a semantically undetermined subject, an object (which corresponds to the subject of the
passive form) and an attribute of the object (which corresponds to the attribute of the subject of the
passive form).
2.2.2 Erasure of affixes, prepositions and complementizers
The PREP module removes affixes, prepositions and complementizers. For example, the rule given here
merges prepositions with the attribute of the object that they introduce. The value of the preposition is
kept to compute the semantics.
PREP ATO
?
voice = active
?
cat = prep
prep = ?
?
ATO OBJ c1 = copy feat(?, ?, prep)
c2 = del edge(?, ?, OBJ)
c3 = shift edge(?, ?)
c4 = del node(?)
2.2.3 From lexical predicative phrases to semantic predicates
The ARG module transforms the syntactic arguments of a predicative word (a verb, a common noun or
an adjective) into its semantic arguments. Following DMRS, the predicate-argument relations are not
labelled with thematic roles but only numbered. The numbering reflects the syntactic obliqueness.
ARG OBJ
? ?
cat = n|np|pro
OBJ
c1 = del edge(?, ?, OBJ)
c2 = add edge(?, ?, ARG2)
c3 = add edge(?, ?, NEQ)
2.2.4 From determiners to generalized quantifiers
DET reverts the determiner dependencies (labelled DET) from common nouns to determiners into depen-
dencies of type RSTR from the corresponding generalized quantifier to the nominal predicate which is
the core of their restriction.
71
DET
?
cat = det
?
cat = n
DET
c1 = del edge(?, ?, DET)
c2 = add edge(?, ?, RSTR)
c3 = add edge(?, ?, H)
2.2.5 Interpretation of different kinds of modification
MOD deals with the modifier dependencies (labelled MOD, MOD REL and MOD LOC), providing rules
for the different kinds of modifiers. Adjectives and adverbs are translated as predicates whose first
argument is the modified entity. The modifier and modified entities are in a conjunction (EQ), except
for scopal adverbs which take scope (H) over the modified predicate. Because only lexical information
enables to differentiate scopal from non-scopal adverbs, we consider all adverbs to be systematically
ambiguous at the moment. Adjunct prepositional phrases (resp. clauses) have a similar rule except that
their corresponding predicate is the translation of the preposition (resp. complementizer), which has
two arguments: the modified entity and the noun (resp. verb) which heads the phrase (resp. clause).
Participial and relative clauses exhibit a relation labelled EQ or NEQ between the head of the clause and
the antecedent, depending on the restrictive or appositive type of the clause.
2.2.6 Resolution of syntactic anaphora
ANA deals with dependencies of type ANT and merges their source and their target. We apply them to
reflexive, relative and repeated pronouns.
3 Experiments
For the experimentation, we are interested in a test suite which is at the same time small enough to be
manually validated and large enough to cover a rich variety of linguistic phenomena. As said earlier, we
use the P7 surface dependency format as input, so the first attempt at building a test suite is to consider
examples in the guide which describes the format. By nature, an annotation guide tries to cover a large
range of phenomena with a small set of examples.
The latest version4 of this guide (Candito et al (2010)) contains 186 linguistic examples. In our cur-
rent implementation of the semantic constructions, we leave out clefts, coordinations and comparatives.
We also leave out a small set of exotic sentences for which we are not able to give a sensible syntactic
structure. Finally, our experiment runs on 116 French sentences. Syntactic structures following P7 spec-
ifications are obtained with some graph rewriting on the output of our parser. Each syntactic structure
was manually checked and corrected when needed. Then, graph rewriting with the modules described in
the previous section is performed.
For all of these sentences, we produce at least one normal form. Even if DMRS is underspecified, our
system can output several semantic representations for one syntactic structure (for instance, for appositive
and restrictive relative clauses). We sometimes overgenerate because we do not use lexical information
like the difference between scopal and non-scopal adverbs.
The result for three sentences is given below and the full set is available on a web page 5.
4version 1.1, january 2010
5http://leopar.loria.fr/doku.php?id=iwcs2011
72
[012] ?Le franc?ais se parle de moins en moins dans les confe?rences.? [The French language is less and
less spoken in conferences.]
le
cat=det
fran?ais
cat=n
se
cat=pro
parle
cat=v
mood=ind
tense=pres
voice=unk
de moins en moins
cat=adv
dans
cat=prep
prep=loc
les
cat=det
conf?rences
cat=n
DET AFF_MOYEN MOD DET
SUJ MOD_LOC OBJ
/la/ct=dea=
/s?tritnR/ct=dr
S THPT
/pt?la/ct=do?vvednre=arRadp?aRovncadtc=noa
mTEQ NAG
//
mTE1 NAG
/eaf?vnrRfarf?vnrR/ct=dteo
mTE1 AG
/etrR/ct=dp?app?apdlvc
AG mTE1
/cvrs2?arcaR/ct=dr
NAG mTEQ
/laR/ct=dea=
S THPT
[057] ?J?encourage Marie a` venir.? [I invite Mary to come.]
je
cat=pro
encourage
cat=v
mood=ind
tense=pres
voice=unk
Marie
cat=np
?
cat=prep
prep=?
venir
cat=v
mood=inf
voice=unk
SUJ OBJ OBJ
A-OBJ
SUJ
/je/cat=pro
/encourage/cat=vmood=indtense=presvoice=active
ARG1 NEQ
/Marie/cat=np
ARG2 NEQ
/venir/cat=vmood=infprep=?voice=active
ARG3 EQ
ARG1 NEQ
[106] ?La se?rie dont Pierre conna??t la fin? [The story Peter knows the end of]
la
cat=det
s?rie
cat=n
dont
cat=pro
Pierre
cat=np
conna?t
cat=v
mood=ind
tense=pres
voice=unk
la
cat=det
fin
cat=n
DET ANT SUJ DET
OBJMOD_REL
DE-OBJ
/la/cat=det
/s?rie/cat=n
RSTR H
/Pierre/cat=np
/conna?t/cat=vmood=indtense=presvoice=active
EQ
NEQ ARG1
/fin/cat=n
NEQ ARG2
/la/cat=det
RSTR H
ARG1 NEQ
73
Conclusion
In this paper, we have shown the relevance of modular graph rewriting to compute semantic representa-
tions from graph-shaped syntactic structures. The positive results of our experiments on a test suite of
varied sentences make us confident that the method can apply to large corpora.
The particular modular graph rewriting system presented in the paper was merely here to illustrate
the method, which can be used for other input and output formats. There is another aspect to the flexi-
bility of the method: we may start from the same system of rules and enrich it with new rules to get a
finer semantic analysis ? if DMRS is considered as providing a minimal analysis ? or integrate lexi-
cal information. The method allows the semantic ambiguity to remain unsolved within underspecified
representations or to be solved with a rule system aiming at computing models of underspecified rep-
resentations. Moreover, we believe that its flexibility makes graph rewriting a convenient framework to
deal with idiomatic expressions.
References
Abeille?, A. and N. Barrier (2004). Enriching a french treebank. In Proceedings of LREC.
Be?daride, P. and C. Gardent (2009). Semantic Normalisation : a Framework and an Experiment. In
Proceedings of IWCS, Tilburg Netherlands.
Bohnet, B. and L. Wanner (2001). On using a parallel graph rewriting formalism in generation. In
Proceedings of EWNLG ?01, pp. 1?11. Association for Computational Linguistics.
Candito, M., B. Crabbe?, and P. Denis (2010). Statistical french dependency parsing: Treebank conversion
and first results. Proceedings of LREC2010.
Candito, M., B. Crabbe?, and M. Falco (2010). De?pendances syntaxiques de surface pour le fran?cais.
Carpenter, B. (1992). The logic of typed feature structures. Cambridge: Cambridge University Press.
Chaumartin, F.-R. and S. Kahane (2010). Une approche paresseuse de l?analyse se?mantique ou comment
construire une interface syntaxe-se?mantique a` partir d?exemples. In TALN 2010, Montreal, Canada.
Copestake, A. (2007). Semantic composition with (robust) minimal recursion semantics. In Proceedings
of the Workshop on Deep Linguistic Processing, pp. 73?80. Association for Computational Linguistics.
Copestake, A. (2009). Invited Talk: Slacker semantics: Why superficiality, dependency and avoidance
of commitment can be the right way to go. In Proceedings of EACL 2009, Athens, Greece, pp. 1?9.
Copestake, A., D. Flickinger, C. Pollard, and I. Sag (2005). Minimal Recursion Semantics - an Introduc-
tion. Research on Language and Computation 3, 281?332.
Crouch, D. (2005). Packed Rewriting for Mapping Semantics to KR. In Proceedings of IWCS.
Dalrymple, M. (2001). Lexical Functional Grammar. New York: Academic Press.
Heim, I. and A. Kratzer (1998). Semantics in generative grammar. Wiley-Blackwell.
Hyvo?nen, E. (1984). Semantic Parsing as Graph Language Transformation - a Multidimensional Ap-
proach to Parsing Highly Inflectional Languages. In COLING, pp. 517?520.
Jijkoun, V. and M. de Rijke (2007). Learning to transform linguistic graphs. In Second Workshop on
TextGraphs: Graph-Based Algorithms for Natural Language Processing, Rochester, NY, USA.
Mel?c?uk, I. (1988). Dependency Syntax: Theory and Practice. Albany: State Univ. of New York Press.
Rozenberg, G. (Ed.) (1997). Handbook of Graph Grammars and Computing by Graph Transformations,
Volume 1: Foundations. World Scientific.
74
