 Non-Verbal Cues for Discourse Structure 
Justine Cassell?, Yukiko I. Nakano?, Timothy W. Bickmore?,  
Candace L. Sidner?, and Charles Rich? 
 
?MIT Media Laboratory 
20 Ames Street 
Cambridge, MA 02139 
{justine, yukiko, bickmore}@media.mit.edu 
 
?Mitsubishi Electric Research Laboratories  
201 Broadway 
Cambridge, MA 02139 
{sidner, rich}@merl.com
Abstract 
This paper addresses the issue of 
designing embodied conversational 
agents that exhibit appropriate posture 
shifts during dialogues with human 
users.  Previous research has noted the 
importance of hand gestures, eye gaze 
and head nods in conversations 
between embodied agents and humans. 
We present an analysis of human 
monologues and dialogues that 
suggests that postural shifts can be 
predicted as a function of discourse 
state in monologues, and discourse and 
conversation state in dialogues. On the 
basis of these findings, we have 
implemented an embodied 
conversational agent that uses 
Collagen in such a way as to generate 
postural shifts.  
1. Introduction 
This paper provides empirical support for the 
relationship between posture shifts and 
discourse structure, and then derives an 
algorithm for generating posture shifts in an 
animated embodied conversational agent from 
discourse states produced by the middleware 
architecture known as Collagen [18].  Other 
nonverbal behaviors have been shown to be 
correlated with the underlying conversational 
structure and information structure of discourse.  
For example, gaze shifts towards the listener 
correlate with a shift in conversational turn 
(from the conversational participants? 
perspective, they can be seen as a signal that the 
floor is available).  Gestures correlate with 
rhematic content in accompanying language 
(from the conversational participants? 
perspective, these behaviors can be seen as a 
signal that accompanying speech is of high 
interest).  A better understanding of the role of 
nonverbal behaviors in conveying discourse 
structures enables improvements in the 
naturalness of embodied dialogue systems, such 
as embodied conversational agents, as well as 
contributing to algorithms for recognizing 
discourse structure in speech-understanding 
systems.   Previous work, however, has not 
addressed major body shifts during discourse, 
nor has it addressed the nonverbal correlates of 
topic shifts. 
2. Background 
Only recently have computational linguists 
begun to examine the association of nonverbal 
behaviors and language.  In this section we 
review research by non-computational linguists 
and discuss how this research has been 
employed to formulate algorithms for natural 
language generation or understanding. 
About three-quarters of all clauses in descriptive 
discourse are accompanied by gestures [17], and 
within those clauses, the most effortful part of 
gestures tends to co-occur with or just before the 
phonologically most prominent syllable of the 
accompanying speech [13]. It has been shown 
that when speech is ambiguous or in a speech 
situation with some noise, listeners rely on 
gestural cues [22] (and, the higher the noise-to-
signal ratio, the more facilitation by gesture). 
Even when gestural content overlaps with 
speech (reported to be the case in roughly 50% 
of utterances, for descriptive discourse), gesture 
often emphasizes information that is also 
focused pragmatically by mechanisms like 
prosody in speech.  In fact, the semantic and 
pragmatic compatibility in the gesture-speech 
relationship recalls the interaction of words and 
graphics in multimodal presentations [11]. 
On the basis of results such as these, several 
researchers have built animated embodied 
conversational agents that ally synthesized 
speech with animated hand gestures.  For 
example, Lester et al [15] generate deictic 
gestures and choose referring expressions as a 
function of the potential ambiguity and 
proximity of objects referred to.  Rickel and 
Johnson [19]'s pedagogical agent produces a 
deictic gesture at the beginning of explanations 
about objects. Andr? et al [1] generate pointing 
gestures as a sub-action of the rhetorical action 
of labeling, in turn a sub-action of elaborating.   
Cassell and Stone [3] generate either speech, 
gesture, or a combination of the two, as a 
function of the information structure status and 
surprise value of the discourse entity. 
Head and eye movement has also been examined 
in the context of discourse and conversation.   
Looking away from one?s interlocutor has been 
correlated with the beginning of turns.  From the 
speaker?s point of view, this look away may 
prevent an overload of visual and linguistic 
information. On the other hand, during the 
execution phase of an utterance, speakers look 
more often at listeners. Head nods and eyebrow 
raises are correlated with emphasized linguistic 
items ? such as words accompanied by pitch 
accents [7].  Some eye movements occur 
primarily at the ends of utterances and at 
grammatical boundaries, and appear to function 
as synchronization signals. That is, one may 
request a response from a listener by looking at 
the listener, and suppress the listener?s response 
by looking away.  Likewise, in order to offer the 
floor, a speaker may gaze at the listener at the 
end of the utterance. When the listener wants the 
floor, s/he may look at and slightly up at the 
speaker [10].  It should be noted that turn taking 
only partially accounts for eye gaze behavior in 
discourse. A better explanation for gaze 
behavior integrates turn taking with the 
information structure of the propositional 
content of an utterance [5]. Specifically, the 
beginning of themes are frequently accompanied 
by a look-away from the hearer, and the 
beginning of rhemes are frequently accompanied 
by a look-toward the hearer. When these 
categories are co-temporaneous with turn 
construction, then they are strongly predictive of 
gaze behavior.  
Results such as these have led researchers to 
generate eye gaze and head movements in 
animated embodied conversational agents.  
Takeuchi and Nagao, for example, [21] generate 
gaze and head nod behaviors in a ?talking head.?  
Cassell et al [2] generate eye gaze and head 
nods as a function of turn taking behavior, head 
turns just before an utterance, and eyebrow 
raises as a function of emphasis.   
To our knowledge, research on posture shifts 
and other gross body movements, has not been 
used in the design or implementation of 
computational systems.  In fact, although a 
number of conversational analysts and 
ethnomethodologists have described posture 
shifts in conversation, their studies have been 
qualitative in nature, and difficult to reformulate 
as the basis of algorithms for the generation of 
language and posture.  Nevertheless, researchers 
in the non-computational fields have discussed 
posture shifts extensively.  Kendon [13] reports 
a hierarchy in the organization of movement 
such that the smaller limbs such as the fingers 
and hands engage in more frequent movements, 
while the trunk and lower limbs change 
relatively rarely.   
A number of researchers have noted that 
changes in physical distance during interaction 
seem to accompany changes in the topic or in 
the social relationship between speakers.  For 
example Condon and Osgton [9] have suggested 
that in a speaking individual the changes in 
these more slowly changing body parts occur at 
the boundaries of the larger units in the flow of 
speech.  Scheflen (1973) also reports that 
posture shifts and other general body 
movements appear to mark the points of change 
between one major unit of communicative 
activity and another.   Blom & Gumperz (1972) 
identify posture changes and changes in the 
spatial relationship between two speakers as 
indicators of what they term "situational  shifts" 
-- momentary changes in the mutual rights and 
obligations between  speakers accompanied by 
shifts in language style. Erickson (1975) 
concludes that proxemic shifts seem to be 
markers of 'important' segments. In his analysis 
of college counseling interviews, they occurred 
more frequently than any other coded indicator 
of segment changes, and were therefore the best 
predictor of new segments in the data.  
Unfortunately, in none of these studies are 
statistics provided, and their analyses rely on 
intuitive definitions of discourse segment or 
?major shift?.  For this reason, we carried out 
our own empirical study. 
3. Empirical Study 
Videotaped ?pseudo-monologues? and dialogues 
were used as the basis for the current study.  In 
?pseudo-monologues,? subjects were asked to 
describe each of the rooms in their home, then 
give directions between four pairs of locations 
they knew well (e.g., home and the grocery 
store). The experimenter acted as a listener, only 
providing backchannel feedback (head nods, 
smiles and paraverbals such as "uh-huh").  For 
dialogues, two subjects were asked to generate 
an idea for a class project that they would both 
like to work on, including: 1) what they would 
work on; 2) where they would work on it 
(including facilities, etc.), and 3) when they 
would work on it. Subjects stood in both 
conditions and were told to perform their tasks 
in 5-10 minutes.  The pseudo-monologue 
condition (pseudo- because there was in fact an 
interlocutor, although he gave backchannel 
feedback only and never took the turn) allowed 
us to investigate the relationship between 
discourse structure and posture shift 
independent of turn structure.  The two tasks 
were constructed to allow us to identify exactly 
where discourse segment boundaries would be 
placed.  
The video data was transcribed and coded for 
three features: discourse segment boundaries, 
turn boundaries, and posture shifts. A discourse 
segment is taken to be an aggregation of 
utterances and sub-segments that convey the 
discourse segment purpose, which is an 
intention that leads to the segment initiation 
[12].   In this study we chose initially to look at 
high-level discourse segmentation phenomena 
rather than those discourse segments embedded 
deeper in the discourse.  Thus, the time points at 
which the assigned task topics were started 
served as segmentation points.  Turn boundaries 
were coded (for dialogues only) as the point in 
time in which the start or end of an utterance co-
occurred with a change in speaker, but excluding 
backchannel feedback. Turn overlaps were 
coded as open-floor time. We defined a posture 
shift as a motion or a position shift for a part of 
the human body, excluding hands and eyes 
(which we have dealt with in other work).  
Posture shifts were coded with start and end 
time of occurrence (duration), body part in play 
(for this paper we divided the body at the 
waistline and compared upper body vs. lower 
body shifts), and an estimated energy level of 
the posture shift. Energy level was normalized 
for each subject by taking the largest posture 
shift observed for each subject as 100% and 
coding all other posture shift energies relative to 
the 100% case.  Posture shifts that occurred as 
part of gesture or were clearly intentionally 
generated (e.g., turning one's body while giving 
directions) were not coded.  
4. Results 
Data from seven monologues and five dialogues 
were transcribed, and then coded and analyzed 
independently by two raters. A total of 70.5 
minutes of data was analyzed (42.5 minutes of 
dialogue and 29.2 minutes of monologue). A 
total of 67 discourse segments were identified 
(25 in the dialogues and 42 in the monologues), 
which constituted 407 turns in the dialogue data.  
We used the instructions given to subjects 
concerning the topics to discuss as segmentation 
boundaries.  In future research, we will address 
the smaller discourse segmentation.  For posture 
shift coding, raters coded all posture shifts 
independently, and then calculated reliability on 
the transcripts of one monologue (5.2 minutes) 
and both speakers from one dialogue (8.5 
minutes).   Agreement on the presence of an 
upper body or lower body posture shift in a 
particular location (taking location to be a 1-
second window that contains all of or a part of a 
posture shift) for these three speakers was 89% 
(kappa = .64).  For interrater reliability of the 
coding of energy level, a Spearman?s rho 
revealed a correlation coefficient of .48 (p<.01).  
4.1 Analysis 
Posture shifts occurred regularly throughout the 
data (an average of 15 per speaker in both 
pseudo-monologues and dialogues). This, 
together with the fact that the majority of time 
was spent within discourse segments and within 
turns (rather than between segments), led us to 
normalize our posture shift data for comparison 
purposes. For relatively brief intervals (inter-
discourse-segment and inter-turn) normalization 
by number of inter-segment occurrences was 
sufficient (ps/int), however, for long intervals 
(intra-discourse segment and intra-turn) we 
needed to normalize by time to obtain 
meaningful comparisons. For this normalization 
metric we looked at posture-shifts-per-second 
(ps/s).  This gave us a mean average of .06 
posture shifts/second (ps/s) in the monologues 
(SD=.07), and .07 posture shifts/second in the 
dialogues (SD=.08). 
 
Table 4.1.1. Posture WRT Discourse Segments 
Our initial analysis compared posture shifts 
made by the current speaker within discourse 
segments (intra-dseg) to those produced at the 
boundaries of discourse segments (inter-dseg). It 
can be seen (in Table 4.1.1) that posture shifts 
occur an order of magnitude more frequently at 
discourse segment boundaries than within 
discourse segments in both monologues and 
dialogues. Posture shifts also tend to be more 
energetic at discourse segment boundaries 
(F(1,251)=10.4; p<0.001). 
Table 4.1.2 Posture Shifts WRT Turns 
 ps/s ps/int energy 
inter-turn 0.140 0.268 0.742 
intra-turn 0.022  0.738 
Initially, we classified data as being inter- or 
intra-turn. Table 4.1.2 shows that turn structure 
does have an influence on posture shifts; 
subjects were five times more likely to exhibit a 
shift at a boundary than within a turn. 
 
Table 4.1.3 Posture by Discourse and Turn Breakdown 
 ps/s ps/int 
inter-dseg/start-turn 0.562 0.542 
inter-dseg/mid-turn 0.000 0.000 
inter-dseg/end-turn 0.130 0.125 
intra-dseg/start-turn 0.067 0.135 
intra-dseg/mid-turn 0.041  
intra-dseg/end-turn 0.053 0.107 
An interaction exists between turns and 
discourse segments such that discourse segment 
boundaries are ten times more likely to co-occur 
with turn changes than within turns. Both turn 
and discourse structure exhibit an influence on 
posture shifts, with discourse having the most 
predictive value. Starting a turn while starting a 
new discourse segment is marked with a posture 
shift roughly 10 times more often than when 
starting a turn while staying within discourse 
segment.  We noticed, however, that posture 
shifts appeared to congregate at the beginnings 
or ends of turn boundaries, and so our 
subsequent analyses examined start-turns, mid-
turns and end-turns. It is clear from these results 
that posture is indeed correlated with discourse 
state, such that speakers generate a posture shift 
when initiating a new discourse segment, which 
is often at the boundary between turns. 
In addition to looking at the occurrence and 
energy of posture shifts we also analyzed the 
distributions of upper vs. lower body shifts and 
the duration of posture shifts.  Speaker upper 
body shifts were found to be used more 
frequently at the start of turns (48%) than at the 
middle of turns (36%) or end of turns (18%) 
(F(2,147)=5.39; p<0.005), with no significant 
 Monologues Dialogues 
 ps/s ps/int energy ps/s ps/int energy 
inter-
dseg 
0.340 0.837 0.832 0.332 0.533 0.844 
intra-
dseg 
0.039 0.701 0.053  0.723 
dependence on discourse structure. Finally, 
speaker posture shift duration was found to 
change significantly as a function of both turn 
and discourse structure (see Figure 4.1.3). At the 
start of turns, posture shift duration is 
approximately the same whether a new topic is 
introduced or not (2.5 seconds). However, when 
ending a turn, speakers move significantly 
longer (7.0 seconds) when finishing a topic than 
when the topic is continued by the other 
interlocutor (2.7 seconds) (F(1,148)=17.9; 
p<0.001). 
Figure 4.1.1 Posture Shift Duration by DSeg and Turn 
 
5. System  
In the following sections we discuss how the 
results of the empirical study were integrated 
along with Collagen into our existent embodied 
conversational agent, Rea. 
5.1 System Architecture 
Rea is an embodied conversational agent that 
interacts with a user in the real estate agent 
domain [2]. The system architecture of Rea is 
shown in Figure 5.1. Rea takes input from a 
microphone and two cameras in order to sense 
the user?s speech and gesture. The UM 
interprets and integrates this multimodal input 
and outputs a unified semantic representation. 
The Understanding Module then sends the 
output to Collagen as the Dialogue Manager. 
Collagen, as further discussed below, maintains 
the state of the dialogue as shared between Rea 
and a user. The Reaction Module decides Rea?s 
next action based on the discourse state 
maintained by Collagen. It also assigns 
information structure to output utterances so that 
gestures can be appropriately generated.  The 
semantic representation of the action, including 
verbal and non-verbal behaviors, is sent to the 
Generation Module which generates surface 
linguistic expressions and gestures, including a 
set of instructions to achieve synchronization 
between animation and speech. These 
instructions are executed by a 3D animation 
renderer and a text-to-speech system. Table 5.1 
shows the associations between discourse and 
conversational state that Rea is currently able to 
handle. In other work we have discussed how 
Rea deals with the association between 
information structure and gesture [6]. In the 
following sections, we focus on Rea?s 
generation of posture shifts. 
 
Table 5.1:
 Discourse functions & non-verbal 
behavior cues 
Discourse 
level info. 
Functions non-verbal 
behavior cues 
Discourse 
structure 
new segment Posture_shift 
turn giving eye_gaze & 
(stop_gesturing  
hand_gesture) 
turn keeping (look_away  
keep_gesture) 
Conversation 
structure 
turn taking eye_gaze & 
posture_shift 
Information 
structure 
emphasize 
information 
eye_gaze & 
beat_and 
other_hand_gsts 
 
 
DSEG 
mid 
end 
intra inter 
8
7
6
5
4
3
2
1
start 
Understanding 
Module
Dialogue 
Manager
(Collagen)
Reaction 
Module (RM)
Animation 
Renderer 
Text to 
Speech
Speech 
Recognition
Vision 
Processing
Microphone Camera
Animation Speech 
Generation Module
Sentence 
Realizer
Gesture  
Component
Figure5.1: System architecture 
5.2 The Collagen dialogue manager 
CollagenTM is JAVA middleware for building 
COLLAborative interface AGENts to work with 
users on interface applications.  Collagen is 
designed with the capability to participate in 
collaboration and conversation, based on [12], 
[16].  Collagen updates the focus stack and 
recipe tree using a combination of the discourse 
interpretation algorithm of [16] and plan 
recognition algorithms of [14].  It takes as input 
user and system utterances and interface actions, 
and accesses a library of recipes describing 
actions in the domain.  After updating the 
discourse state, Collagen makes three resources 
available to the interface agent: focus of 
attention (using the focus stack), segmented 
interaction history (of completed segments) and 
an agenda of next possible actions created from 
the focus stack and recipe tree.  
 
5.3 Output Generation 
The Reaction Module works as a content 
planner in the Rea architecture, and also plays 
the role of an interface agent in Collagen. It has 
access to the discourse state and the agenda 
using APIs provided by Collagen. Based on the 
results reported above, we describe here how 
Rea plans her next nonverbal actions using the 
resources that Collagen maintains.  
The empirical study revealed that posture shifts 
are distributed with respect to discourse segment 
and turn boundaries, and that the form of a 
posture shift differs according to these co-
determinants. Therefore, generation of posture 
shifts in Rea is determined according to these 
two factors, with Collagen contributing 
information about current discourse state.  
5.3.1 Discourse structure information 
Any posture shift that occurs between the end of 
one discourse segment and the beginning of the 
next is defined as an inter-discourse segment 
posture shift. In order to elaborate different 
generation rules for inter- vs. intra-discourse 
segments, Rea judges (D1) whether the next 
utterance starts a new topic, or contributes to the 
current discourse purpose, (D2) whether the 
next utterance is expected to finish a segment. 
First, (D1) is calculated by referring to the focus 
stack and agenda. In planning a next action, Rea 
accesses the goal agenda in Collagen and gets 
the content of her next utterance. She also 
accesses the focus stack and gets the current 
discourse purpose that is shared between her and 
the user. By comparing the current purpose and 
the purpose of her next utterance, Rea can judge 
whether the her next utterance contributes to the 
current discourse purpose or not. For example, if 
the current discourse purpose is to find a house 
to show the user (FindHouse), and the next 
utterance that Rea plans to say is as follows, 
(1) (Ask.What (agent Propose.What (user FindHouse 
<city ?>)))  
Rea says: "What kind of transportation access do you 
need?" 
then Rea uses Collagen APIs to compare the 
current discourse purpose (FindHouse) to the 
purpose of utterance (1). The purpose of this 
utterance is to ask the value of the transportation 
parameter of FindHouse. Thus, Rea judges that 
this utterance contributes to the current 
discourse purpose, and continues the same 
discourse segment (D1 = continue).  On the 
other hand, if Rea?s next utterance is about 
showing a house,  
(2) (Propose.Should (agent ShowHouse (joint 
123ElmStreet))   
Rea says: "Let's look at 123 Elm Street." 
then this utterance does not directly contribute 
to the current discourse purpose because it does 
not ask a parameter of FindHouse, and it 
introduces a new discourse purpose ShowHouse. 
In this case, Rea judges that there is a discourse 
segment boundary between the previous 
utterance and the next one (D1 = topic change).  
In order to calculate (D2), Rea looks at the plan 
tree in Collagen, and judges whether the next 
utterance addresses the last goal in the current 
discourse purpose. If it is the case, Rea expects 
to finish the current discourse segment by the 
next utterance (D1 = finish topic).  As for 
conversational structure, Rea needs to know; 
(T1) whether Rea is taking a new turn with the 
next utterance, or keeping her current turn for 
the next utterance, (T2) whether Rea?s next 
utterance requires that the user respond.  
First, (T1) is judged by referring to the dialogue 
history1. The dialogue history stores both system 
utterances and user utterances that occurred in 
the dialogue. In the history, each utterance is 
stored as a logical form based on an artificial 
discourse language [20]. As shown above in 
utterance (1), the first argument of the action 
indicates the speaker of the utterance; in this 
example, it is ?agent?. The turn boundary can be 
estimated by comparing the speaker of the 
previous utterance with the speaker of the next 
utterance. If the speaker of the previous 
utterance is not Rea, there is a turn boundary 
before the next utterance (T1 = take turn). If the 
speaker of the previous utterance is Rea, that 
means that Rea will keep the same turn for the 
next utterance (T1 = keep turn).  
Second, (T2) is judged by looking at the type of 
Rea?s next utterance. For example, when Rea 
asks a question, as in utterance (1), Rea expects 
the user to answer the question. In this case, Rea 
must convey to the user that the system gives up 
the turn (T2 = give up turn).  
5.3.2 Deciding and selecting a posture shift 
Combining information about discourse 
structure (D1, D2) and conversation structure 
(T1, T2), the system decides on posture shifts 
                                                                 
1
 We currently maintain a dialogue history in Rea even 
though Collagen has one as well. This is in order to store 
and manipulate the information to generate hand gestures 
and assign intonational accents. This information will be 
integrated into Collagen in the near future. 
for the beginning of the utterance and the end of 
the utterance. Rea decides to do or not to do a 
posture shift by calling a probabilistic function 
that looks up the probabilities in Table 5.3.1.  
A posture shift for the beginning of the utterance 
is decided based on the combination of (D1) and 
(T1). For example, if the combined factors 
match Case (a), the system decides to generate a 
posture shift with 54% probability for the 
beginning of the utterance.  Note that in Case 
(d), that is, Rea keeps the turn without changing 
a topic, we cannot calculate a per interval 
posture shift rate. Instead, we use a posture shift 
rate normalized for time. This rate is used in the 
GenerationModule, which calculates the 
utterance duration and generates a posture shift 
during the utterance based on this posture shift 
rate.   On the other hand, ending posture shifts 
are decided based on the combination of (D2) 
and (T2).  
For example, if the combined factors match 
Case (e), the system decides to generate a 
posture shift with 0.04% probability for the 
ending of the utterance. When Rea does decide 
to activate a posture shift, she then needs to 
choose which posture shift to perform. Our 
empirical data indicates that the energy level of 
the posture shift differs depending on whether 
there is a discourse segment boundary or not. 
Moreover the duration of a posture shift differs 
depending on the place in a turn: start-, mid-, or 
end-turn. 
Posture shift selection Place of a 
posture shift Case 
Discourse 
structure 
information 
Conversation 
structure 
information 
Posture shift 
decision 
probability energy duration body part 
a 
topic 
change  take turn 0.54/int high default 
upper & 
lower 
b topic 
change keep turn 0 - - - 
c continue take turn 0.13/int low default upper or lower 
beginning of 
the utterance 
d 
D1  
continue 
T1 
keep turn 0.14/sec low short lower 
e 
finish 
topic give turn 0.04/int high long lower End of the 
utterance 
f 
D2 
continue 
T2 
give turn 0.11/int low default lower 
Table 5.3.1:Posture Decision Probabilities for Dialogue 
Based on these results, we define posture shift 
selection rules for energy, duration, and body 
part. The correspondence with discourse 
information is shown in Table 5.3.1.  For 
example, in Case (a), the system selects a 
posture shift with high energy, using both upper 
and lower body. After deciding whether or not 
Rea should shift posture and (if so) choosing a 
kind of posture shift, Rea sends a command to 
the Generation Module to generate a specific 
kind of posture shift within a specific time 
duration. 
Posture shift 
selection 
 
Ca
se 
Discourse 
structure 
information 
Posture 
shift 
decision 
probability energy 
g change topic 0.84/int high 
h 
D1 
continue 0.04/sec low 
 
Posture shifts for pseudo-monologues can be 
decided using the same mechanism as that for 
dialogue, but omitting conversation structure 
information.   The probabilities are given in 
table Table 5.3.2. For example, if Rea changes 
the topic with her next utterance, a posture shift 
is generated 84% of the time with high-energy 
motion. In other cases, the system randomly 
generates low-energy posture shifts 0.04 times 
per second.  
 
6. Example 
Figure 6.1 shows a dialogue between Rea and 
the user, and shows how Rea decides to generate 
posture shifts. This dialogue consists of two 
major segments: finding a house (dialogue), and 
showing a house (pseudo-monologue). Based on 
this task structure, we defined plan recipes for 
Collagen. The first shared discourse purpose 
[goal: HaveConversation] is introduced by the 
user before the example. Then, in utterance (1), 
the user introduces the main part of the 
conversation [goal: FindHouse].  
The next goal in the agenda, [goal: 
IdentifyPreferredCity], should be 
accomplished to identify a parameter value for 
[goal: FindHouse]. This goal directly 
contributes to the current purpose, [goal: 
FindHouse].  This case is judged to be a turn 
boundary within a discourse segment (Case (c)), 
and Rea decides to generate a posture shift at the 
beginning of the utterance with 13% probability. 
If Rea decides to shift posture she selects a low 
energy posture shift using either upper or lower 
body. In addition to a posture shift at the 
beginning of the utterance, Rea may also choose 
to generate a posture shift to end the turn. As 
utterance (2) expects the user to take the turn, 
and continue to work on the same discourse 
purpose, this is Case (f). Thus, the system 
generates an end utterance posture shift 11% of 
the time. If generated, a low energy  posture 
shift is chosen. If a beginning and/or ending 
posture shifts are generated, they are sent to the 
GM, which calculates the schedule of these 
multimodal events and generates them.  
In utterance (25), Rea introduces a new 
discourse purpose [goal : ShowHouse]. Rea, 
using a default rule, decides to take the initiative 
on this goal.  At this point, Rea accesses the 
discourse state and confirms that a new goal is 
about to start.  Rea judges this case as a 
discourse segment boundary and also a turn 
boundary (Case (a)). Based on this information, 
Rea selects a high energy posture shift.  An 
example of Rea?s high energy posture shift is 
shown on the right in Figure 5.2. 
As a subdialogue of showing a house, in a 
discourse purpose [goal : DiscussFeature], Rea 
keeps the turn and continues to describe the 
house. We handle this type of interaction as a 
pseudo-monologue. Therefore, we can use table 
Table 5.3.2 for deciding on posture shifts here. 
In utterance (27), Rea starts the discussion about 
the house, and takes the initiative. This is judged 
as Case (g), and a high energy body motion is 
generated 84% of the time. 
Table 5.3.2: Posture Decision Probabilities: Monologue 
  
7. Conclusion and Further work 
We have demonstrated a clear relationship 
between nonverbal behavior and discourse state, 
and shown how this finding can be incorporated 
into the generation of language and nonverbal 
behaviors for an embodied conversational agent. 
Speakers produce posture shifts at 53% of 
discourse segment boundaries, more frequently 
than they produce those shifts discourse 
segment-internally, and with more motion 
energy.  Furthermore, there is a relationship 
between discourse structure and conversational 
structure such that when speakers initiate a new 
segment at the same time as starting a turn (the 
most frequent case by far), they are more likely 
to produce a posture shift; while when they end 
a discourse segment and a turn at the same time, 
their posture shifts last longer than when these 
categories do not co-occur. 
Although this paper reports results from a 
limited number of monologues and dialogues, 
the findings are promising.  In addition, they 
point the way to a number of future directions, 
both within the study of posture and discourse, 
and more generally within the study of non-
verbal behaviors in computational linguistics. 
 
Figure 6.2: Rea demonstrating a low and high energy 
posture shift 
First, given the relationship between 
conversational and information structure in [5], 
a natural next step is to examine the three-way 
relationship between discourse state, 
conversational structure (turns), and information 
structure (theme/rheme).  For the moment, we 
have demonstrated that posture shifts may signal 
boundaries of units; do they also signal the 
information content of units? Next, we need to 
look at finer segmentations of the discourse, to 
see whether larger and smaller discourse 
segments are distinguished through non-verbal 
means.  Third, the question of listener posture is 
an important one.  We found that a number of 
posture shifts were produced by the participant 
who was not speaking.  More than half of these 
shifts were produced at the same time as a 
speaker shift, suggesting a kind of mirroring.  In 
order to interpret these data, however, a more 
sensitive notion of turn structure is required, as 
one must be ready to define when exactly 
speakers and listeners shift roles. Also, of 
course, evaluation of the importance of such 
nonverbal behaviors to user interaction is 
essential.  In a user study of our earlier Gandalf 
system [4], users rated the agent's language 
skills significantly higher under test conditions 
in which Gandalf deployed conversational 
behaviors (gaze, head movement and limited 
gesture) than when these behaviors were 
disabled.  Such an evaluation is also necessary 
for the Rea-posture system.  But, more 
generally, we need to test whether generating 
posture shifts of this sort actually serves as a 
signal to listeners, for example to initiative 
 
[Finding a house] < dialogue> 
  
(1) 
  
U: I?m looking for a house. 
  
(2) 
  
R:  (c)   Where do you want to live? (f)  (3) 
  
U: I like Boston. 
  
(4) 
  
R:  (c) (d)  What kind of transportation  
access do you need? (f)   
(5) 
  
U: I need T access. 
  
 ?.  
(23) 
  
R:  (c) (d)  How much storage space do  
you need?  (f)  
(24) 
  
U: I need to have a storage place in the  
basement. 
  
(25) 
  
R:  (a) (d) 
  
Let?s look at 123 Elm Street. (f) 
  
(26) 
  
U: OK. 
  
[Discuss a feature of the house] 
  
(27) 
  
R:  (g)  Let's discuss a feature of this place. 
  
(28) 
  
R:  (h)  Notice the hardw ood flooring in the  
living room. 
  
(29) 
  
R:  (h)  Notice the jacuzzi. 
  
(30) 
  
R:  (h) Notice the remodeled kitchen 
  
[Showing a house] <Pseudo-monologue> 
  Figure 6.1: Example dialogue 
structure in task and dialogue [8]. These 
evaluations form part of our future research 
plans. 
8. Acknowledgements 
This research was supported by MERL, France 
Telecom, AT&T, and the other generous sponsors of 
the MIT Media Lab.  Thanks to the other members of 
the Gesture and Narrative Language Group, in 
particular Ian Gouldstone and Hannes Vilhj?lmsson. 
9. REFERENCES  
[1] Andre, E., Rist, T., & Muller, J., Employing AI 
methods to control the behavior of animated 
interface agents, Applied Artificial Intelligence, 
vol. 13, pp. 415-448, 1999. 
[2] Cassell, J., Bickmore, T., Billinghurst, M., 
Campbell, L., Chang, K., Vilhjalmsson, H., & 
Yan, H., Embodiment in Conversational 
Interfaces: Rea, Proc. of CHI 99, Pittsburgh, PA, 
ACM, 1999. 
[3] Cassell, J., Stone, M., & Yan, H., Coordination 
and context-dependence in the generation of 
embodied conversation, Proc. INLG 2000, 
Mitzpe Ramon, Israel, 2000. 
[4] Cassell, J. and Thorisson, K. R., The Power of a 
Nod and a Glance: Envelope vs. Emotional 
Feedback in Animated Conversational Agents, 
Applied Art. Intell., vol. 13, pp. 519-538, 1999. 
[5] Cassell, J., Torres, O., & Prevost, S., Turn 
Taking vs. Discourse Structure: How Best to 
Model Multimodal Conversation., in Machine 
Conversations, Y. Wilks, Ed. The Hague: 
Kluwer, 1999, pp. 143-154. 
[6] Cassell, J., Vilhj?lmsson, H., & Bickmore, T., 
BEAT: The Behavior Expression Animation 
Toolkit, Proc. of SIGGRAPH, ACM Press, 
2001. 
[7] Chovil, N., Discourse-Oriented Facial Displays 
in Conversation, Research on Language and 
Social Interaction, vol. 25, pp. 163-194, 1992. 
[8] Chu-Carroll, J. & Brown, M., Initiative in 
Collaborative Interactions - Its Cues and Effects, 
Proc. of AAAI Spring 1997 Symp. on 
Computational Models of Mixed Initiative, 
1997. 
[9] Condon, W. S. & Osgton, W. D., Speech and 
body motion synchrony of the speaker-hearer, in 
The perception of language, D. Horton & J. 
Jenkins, Eds. NY: Academic Press, 1971, pp. 
150-184. 
[10] Duncan, S., On the structure of speaker-auditor 
interaction during speaking turns, Language in 
Society, vol. 3, pp. 161-180, 1974. 
[11] Green, N., Carenini, G., Kerpedjiev, S., & Roth, 
S, A Media-Independent Content Language for 
Integrated Text and Graphics Generation, Proc. 
of Workshop on Content Visualization and 
Intermedia Representations at COLING and 
ACL '98, 1998. 
[12] Grosz, B. & Sidner, C., Attention, Intentions, 
and the Structure of Discourse, Computational 
Linguistics, vol. 12, pp. 175-204, 1986. 
[13] Kendon, A., Some Relationships between Body 
Motion and Speech, in Studies in Dyadic 
Communication, A. W. Siegman and B. Pope, 
Eds. Elmsford, NY: Pergamon Press, 1972, pp. 
177-210. 
[14] Lesh, N., Rich, C., & Sidner, C., Using Plan 
Recognition in Human-Computer Collaboration, 
Proc. of the Conference on User Modelling, 
Banff, Canada, NY: Springer Wien, 1999. 
[15] Lester, J., Towns, S., Callaway, C., Voerman, J., 
& FitzGerald, P., Deictic and Emotive 
Communication in Animated Pedagogical 
Agents, in Embodied Conversational Agents, J. 
Cassell, J. Sullivan, et. al, Eds. Cambridge: MIT 
Press, 2000. 
[16] Lochbaum, K., A Collaborative Planning Model 
of Intentional Structure, Computational 
Linguistics, vol. 24, pp. 525-572, 1998. 
[17] McNeill, D., Hand and Mind: What Gestures 
Reveal about Thought. Chicago, IL/London, 
UK: The University of Chicago Press, 1992. 
[18] Rich, C. & Sidner, C. L., COLLAGEN: A 
Collaboration Manager for Software Interface 
Agents, User Modeling and User-Adapted 
Interaction, vol. 8, pp. 315-350, 1998. 
 [19] Rickel, J. & Johnson, W. L., Task-Oriented 
Collaboration with Embodied Agents in Virtual 
Worlds, in Embodied Conversational Agents, J. 
Cassell, Ed. Cambridge, MA: MIT Press, 2000. 
[20] Sidner, C., An Artificial Discourse Language for 
Collaborative Negotiation, Proc. of 12th Intnl. 
Conf. on Artificial Intelligence (AAAI), Seattle, 
WA, MIT Press, 1994. 
[21] Takeuchi, A. & Nagao, K., Communicative 
facial displays as a new conversational modality, 
Proc. of InterCHI '93, Amsterdam, NL, ACM, 
1993. 
[22] Thompson, L. and Massaro, D., Evaluation and 
Integration of Speech and Pointing Gestures 
during Referential Understanding, Journal of 
Experimental Child Psychology, vol. 42, pp. 
144-168, 1986. 
Lessons Learned in Building 
Spoken Language Collaborative Interface Agents 
Candace  L. S idner  
Caro lyn  Boet tner  
Lotus Development Corporat ion 
Cambridge, MA 02142 USA 
csidnerlcarolyn_boettner@lotus.com 
Char les  R ich  
Mitsubishi Electric Research Laboratory 
Cambridge, MA 02139 USA 
rich@merl.com 
Abst rac t  
This paper reports on the development of two 
spoken language collaborative interface agents 
built with the Collagen system. It presents 
sample dialogues with the agents working with 
email applications and meeting planning appli- 
cations, and discusses how these applications 
were created. It also discusses limitations and 
benefits of this approach. 
1 Co l laborat ive  Agents  
The underlying premise of the Collageff M (for 
Collaborative agent) project is that software 
agents, when they interact with people, should 
be governed by the same principles that govern 
human-to-human collaboration. To determine 
the principles governing human collaboration, 
we have relied on research in computational lin- 
guistics on collaborative discourse, specifically 
within the SharedPlan framework of Grosz and 
Sidner (1986, 1990) (Grosz and Kraus, 1996, 
Lochbaum, 1998). This work has provided us 
with a computationally-specified theory that 
has been empirically validated across a range of 
User Agent 
communicate 
l Application 
Figure 1: Collaborative interface agent paradigm. 
human tasks. We have implemented the algo- 
rithms and information structures of this theory 
in the form of a Java middleware component, 
a collaboration manager called Collagen, which 
software developers can use to implement a col- 
laborative interface agent for any Java applica- 
tion. 
In the collaborative interface agent paradigm, 
illustrated abstractly in Figure 1, a software 
agent is able to both communicate with and 
observe the actions of a user on a shared ap- 
plication interface, and vice versa. The soft- 
ware agent in this paradigm takes an active r01e 
in joint problem solving, including advising the 
user when he gets stuck, suggesting what to do 
next when he gets lost, and taking care of low- 
level details after a high-level decision is made. 
The screenshot in Figure 2 shows how the 
collaborative interface agent paradigm is con- 
cretely realized on a user's display. The large 
window in the background is the shared appli- 
cation, in this case, the Lotus eSuite TM email 
program. The two smaller overlapping windows 
Figure 2: Interface for Collagen email agent. 
in the corners of the screen are the agent's and 
user's home windows, through which they com- 
municate with each other. 
A key benefit of using Collagen to build an in- 
terface agent is that the collaboration manager 
automatically constructs a structured history of 
the user's and agent's activities. This segmented 
interaction history is hierarchically organized 
according to the goal structure of the applica- 
tion tasks. Among other things, this history can 
help re-orient he user when he gets confused 
or after an extended absence. It also supports 
high-level, task-oriented transformations, uch 
as returning to an earlier goal. Figure 3 shows 
a sample segmented interaction history for the 
an email interaction. 
To apply Collagen to a particular application, 
the application developer must provide an ab- 
stract model of the tasks for which the appli- 
cation software will be used. This knowledge 
is formalized in a recipe library, which is then 
automatically compiled for use by the interface 
agent. This approach also allows us to easily 
vary an agent's level of initiative from very pas- 
sive to very active, using the same task model. 
For more details on the internal architecture of 
Collagen, see (Rich and Sidner, 1998). 
We have developed prototype interface agents 
using Collagen for several applications, includ- 
ing air travel planning (Rich and Sidner, 1998), 
resource allocation, industrial control, and com- 
mon PC desktop activities. 
2 A Collaborative Email Agent 
The email agent (Gruen et al, 1999) is the first 
Collagen-based agent we have built that sup- 
ports spoken-language interaction. Our other 
agents avoided the need for natural language 
understanding by presenting the user with a 
dynamically-changing menu of expected utter- 
ances, which was generated from the current 
discourse state according to the predictions of 
the SharedPlan theory. Sample menus are dis- 
played in Figure 2. The email agent, how- 
ever, incorporates a speech and natural lan- 
guage understanding system developed by IBM 
Research, allowing users to collaborate ither 
entirely in speech or with a mixture of speech 
and interface actions, such as selecting a mes- 
sage. More recently we have developed the 
Lotus Notes TM meeting planning agent, which 
incorporates peech and sentence level under- 
standing using the Java Speech API, as imple- 
mented by IBM. The JSAPI toolkit provides 
a parser, which we use with a vocabulary and 
grammar we developed for the domain of meet- 
ing planning. The tags produced by the Java 
Speech parser are interpreted with a set of se- 
mantic rules that produce internal structures 
used by the Collagen agent. 
With the email application, the user can read, 
compose and send messages as one typically 
does with email. The Collagen email agent, 
called Daffy, performs actions requested by the 
user with speech and watches user interface ac- 
tions. It can perform a few email actions on its 
own (such as opening and closing windows, and 
filling in the user's signature on email) and can 
also undertake actions that the user requests in 
spoken utterances. In the sample session shown 
in Figure 4, the agent keeps a todo list for the 
user, explains how to accomplish email tasks for 
a user who is new to email, answers user ques- 
tions about what actions were taken in the in- 
teraction and offers suggestions about what to 
do next in the interaction as well as forming {lser 
requests. 
To create the email agent, we built a recipe 
library about email, as required for the Colla- 
gen architecture, of about 55 actions and 32 
recipes for doing those actions; the actions in- 
cluded GUI primitives uch as sending a mes- 
sage, and high level actions such as reacting to 
a message. 
3 A Collaborative Agent for  
Planning Meetings 
Our experience with Daffy convinced us that 
collaborative agents would be more useful if 
they not only understood what the user was do- 
ing in the interface but could undertake more 
of the user's sub-goals and thus off-load some 
of the burden from the user. To explore this 
notion, we built Dotty, a Collagen agent, that 
works with a user who is planning a meeting 
with a customer, using Lotus Notes. As the di- 
alogue in Figure 5 demonstrates, Dotty is able 
to take over many of the details of planning the 
meeting. Dotty uses a library that is far smaller 
than Daffy's: 19 actions and 5 recipes. 
This dialogue begins with an overall goals of 
managing sales contacts and several sub-goals, 
: 2 
USER: "Your turn," 
Done checking our to-do list. 
AGENT: "Let's check our to-do list." 
USER: "Ok . "  
AGENT: "The following items are on our to-do list." 
AGENT: "Work on email." 
AGENT: "Prepare a presentation about Java Beans." 
AGENT: "That's all," 
USER: "Ok . "  
Done work ing  on emai l .  
AGENT: "Let ' s  work  on emai l . "  
Done explaining how to work on email. 
USER: "What should I do?" 
AGENT: "First, you reed a message." 
USER: "Ok." 
AGENT: "Then, you might store, print or delete the message." 
Done reading message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH GROUP .... 
Done explaining how to read a message. 
Done agent selecting message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH GROUP .... 
USER: "Select message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH GROUP .... " 
AGENT: Select message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH GROUP .... 
AGENT: 0pen ~essage from Brian Gorwin <brg@dewey.watson.ibm.eom> about WEEKLY SPEECH GROUP SEMINAR. 
Done using the contents of message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH .... 
AGENT: "Shall we use the contents of message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY ...?" 
Done scheduling meeting with Brian at three P.M. Thursday by schedul ing. 
Done agent setting up meeting with Brian. 
USER: "Set up meeting with Brian." 
AGENT: Set up meeting with Brian. 
Done agent scheduling event tentatively at three P.M. Thursday. 
AGENT: "Let's schedule event tentatively at a time." 
USER: "Schedule event tentatively at three P.M. Thursday." 
AGENT: Schedule event tentatively at three P.M. Thursday. 
AGENT: "There is a conflict of meeting with Brian at three P.M. Thursday with 
meeting with Irene Landoz at three P.M. Thursday." 
AGENT: "Let's remember to reschedule meeting with Irene Landoz at three P.M, Thursday." 
USER: "Ok . "  
AGENT: Finalize meeting with Brian at three P.M. Thursday. 
AGENT: Close message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH GROUP SEMINAR. 
Done reacting to message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY SPEECH .... 
AGENT: "Shall we react to message from Brian Gorwin <brg@dewey.watson.ibm.com> about WEEKLY ...?" 
Done agent forwarding message from Brian Gorwin <brg@dewey.watson.ibm,com> about WEEKLY ... to Steve Krantz. 
USER: "Forward message from Brian Gorwin <brg@dewey.watson.ibm.com> about ... to Steve Krantz." 
Done user saying identify the recipient as Steve Krantz. 
AGENT: "Who is the recipient?" 
USER: "The recipient is Steve Krantz." 
AGENT: Forward message from Brian Gorwin <brg@dewey.watson.ibm.com> about ... to Steve Krantz. 
Done filling in message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY SPEECH GROUP SEMINAR. 
AGENT: "Shall we fill in message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY ,..?" 
USER: "Ok." 
AGENT: Add text to message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY .... 
Stopped ccing message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY SPEECH .... 
AGENT: "Shall we cc message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY SPEECH GROUP SEMINAR?" 
USER: "No." 
AGENT: "0k, your turn." 
Done sending, canceling or saving this message as a draft. 
Done agent sending message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY SPEECH .... 
USER: "Send message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY SPEECH GROUP .... " 
AGENT: Send message from Candy Sidner <csidner@lotus.com> about Fwd:WEEKLY SPEECH .... 
Done storing, printing or deleting this message. 
AGENT: "Shall we store, print or delete this message?" 
USER: "Did I forward message to Steve Krantz." 
AGENT: "Yes, you did forward message from Brian Gorwin <brg@dewey.watson.ibm,com> about ... to Steve Krantz." 
USER: "Done." 
Done interruption agent checking history. 
USER: "Please check history," 
USER: "Done." 
Figure 3: Sample s?~gnmnted interaction history for ~m email interactiou. 
3 

AGENT: 
USER: 
AGENT: 
USER: 
AGENT: 
USER: 
AGENT: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
AGENT: 
AGENT: 
USER: 
AGENT: 
AGENT: 
"Let's manage sales contacts." 
"Let's create profile for Miles White from Abbott Laboratories." 
"What is the eemail address?" 
"The emailAddress is mileswhite@abl.com." 
"What is the workPhone?" 
"The workPhone is 8173329280." 
Goto profile for Abbott Laboratories. 
Create profile for Miles White. 
"Do you want to schedule a presentation for Miles White?" 
"Schedule the meeting for December 9 from 2 to 4 pm." 
Make meeting entry for Miles White on December 9. 
"Is the meeting here or at the customer site?" 
"The site is here." 
"Which building?" 
"The building is Rogers Street." 
Locate room 5003 Center in Rogers Street. 
"I'm going to reserve room 5003 Center in Rogers Street." 
"Ok." 
Reserve room 5003 Center in Rogers Street. 
"Do you need any special equipment?" 
"No." 
"Okay." 
"Who will attend the meeting?" 
"Invite Fred Smith and Jerry Johnson." 
Invite Fred Smith and Jerry Johnson. 
"Will the customer need consulting services?" 
"Ok." 
"Identifying the consulting staff." 
"Invite Pam Bridges from Lotus Consulting and Jimmy Barnes from IGS." 
Invite Pam Bridges and Jimmy Barnes. 
"The following people: Pam Bridges have a conflict with the meeting time.." 
"Instead invite Julie Knox." 
Invite Julie Knox instead of Pam Bridges. 
"I will get back to you to confirm this meeting." 
"Ok." 
Send invitations to Fred Smith, Jerry Johnson, Julie Knox, and Jimmy Barnes. 
"Do you want to see company information?" 
"Show me information about Dover Hospital." 
Goto profile for Dover Hospital. 
"You need to plan a presentation for Miles White." 
"Shall we find presentations about a subject?" 
"No." 
"Okay." 
"I'm going to get back to you about meeting." 
"The following people: Jimmy Barnes have a conflict with the meeting time.." 
"This is message from Jimmy Barnes: I have to be in Cleveland on December 9. 
"Invite Jonathan Parks instead." 
Invite Jonathan Parks instead of Jimmy Barnes. 
"I will get back to you to confirm this meeting." 
Jonathan Parks is available." 
Figure 5: Sample session of Collagen agent using Lotus Notes for meeting planning (unsegmented). 
5 
including creating a profile for Miles White 
(which is displayed to the user as a Notes doc- 
ument), scheduling a meeting with Miles White 
(which the agent undertakes by itself using facil- 
ities in Lotus Notes), finding information about 
Dover Hospital (which is displayed as a Notes 
document), and a brief discussion about plan- 
ning a presentation. 
4 Cur rent  L imi ta t ions  
The spoken interaction of our two Collagen 
agents is limited by the range of utterances 
that the utterance understanding components 
can interpret More significantly, we feel these 
agents are limited in dealing with spoken con- 
versational errors, i.e. errors that arise either 
because the recognition system produces an er- 
ror, or the semantic interpretation is faulty 
(even given the correct choice of words). Er- 
rors resulting from semantic mis-interpretation 
are especially important because often the con- 
tent of the faulty interpretation is something 
that the agent can respond to and does, which 
results in the conversation going awry. In 
such cases we have in mind using the history 
based transformations possible in Collagen (c.f. 
(Rich and Sidner, 1998)) to allow the user to 
turn the conversation back to before where the 
error occurred. 
Whether communicating byspeech or menus, 
our agents are limited by their inability to ne- 
gotiate with their human partner. For example, 
whenever one of our agents propose an action 
to perform that the user rejects (as in the email 
conversation i Figure 4, where the agent pro- 
poses filling in the cclist and the user says no), 
the agent currently does not have any strategies 
for responding in the conversation other than to 
accept he rejection and turn the conversation 
back to the user. We are in present exploring 
how to use a set of strategies for negotiation 
of activities and beliefs that we have identified 
from corpora of human-human collaborations. 
Using these strategies in the Collagen system 
will give interface agents a richer set of negoti- 
ation capabilities critical for collaboration. 
Finally, our agents need a better model 
of conversational initiative. We have experi- 
mented in the Collagen system with three initia- 
tive modes, one dominated by the user, one by 
the agent and one that gives each some control 
of the conversation. The dialogues presented in
this paper are all from agent initiative. None of 
these modes is quite right. The user dominated 
mode is characterized byan agent hat only acts 
when specifically directed to or when explicitly 
told to take a turn in the conversation, while the 
agent dominated mode has a very chatty agent 
that constantly offers next possible actions rel- 
evant to the collaboration. We are currently 
investigating additional modes of initiative. 
The collaborative agent paradigm that we 
have implemented has several original features. 
The conversation and collaboration model is 
general and does not require tuning or the im- 
plementation of special dialogue steps for the 
agent to participate. The model tracks the in- 
teraction and treats both the utterances ofboth 
participants and the GUI level actions as com- 
munications for the discourse; it relates these to 
the actions and recipes for actions. The model 
has facilities for richer interpretation of dis- 
course level phenomena, such as reference and 
anaphora, through the use of the focus stack. 
Finally, when we began this research, we were 
not certain that the Collagen system could be 
used to create agents that would interact with 
users for many different applications. Our expe- 
rience with five different applications indicates 
that the model has the flexibility and richness to 
make human and computer collaboration possi- 
ble in many circumstances. 
Re ferences  
B. J. Grosz and S. Kraus. 1996. Collaborative plans 
for complex group action. Artificial Intelligence, 
86(2):269-357, October. 
B. J. Grosz and C. L. Sidner. 1986. Attention, in- 
tentions, and the structure of discourse. Compu- 
tational Linguistics, 12(3):175-204. 
B. J. Grosz and C. L. Sidner. 1990. Plans for dis- 
course. In P. R. Cohen, J. L. Morgan, and M. E. 
Pollack, editors, Intentions and Communication, 
pages 417-444. MIT Press, Cambridge, MA. 
D. Cruen, C. Sidner, C. Boettner, and C. Rich. 
1999. A collaborative assistant for email. In Proc. 
ACM SIGCHI Conference on Human Factors in 
Computing Systems, Austin, TX, May. 
K. E. Lochbaum. 1998. A collaborative planning 
model of intentional structure. Computational 
Linguistics, 24(4), December. 
C. Rich and C. Sidner. 1998. COLLAGEN: A col- 
laboration manager for software interface agents. 
User Modeling and User-Adapted Interaction, 
8(3/4):315-350. 
6 
Proceedings of the SIGDIAL 2013 Conference, pages 148?150,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of an Always-On Companion for
Isolated Older Adults
Candace Sidner
Worcester Polytechnic Institute
Worcester, MA, USA
sidner@wpi.edu
Timothy Bickmore
Northeastern University
Boston, MA, USA
bickmore@ccs.neu.edu
Charles Rich
Worcester Polytechnic Institute
Worcester, MA, USA
Barbara Barry, Lazlo Ring
Northeastern University
Boston, MA, USA
Morteza Behrooz, Mohammad Shayganfar
Worcester Polytechnic Institute
Worcester, MA, USA
Abstract
We summarize the status of an ongoing
project to develop and evaluate a compan-
ion for isolated older adults. Four key
scientific issues in the project are: em-
bodiment, interaction paradigm, engage-
ment and relationship. The system ar-
chitecture is extensible and handles real-
time behaviors. The system supports mul-
tiple activities, including discussing the
weather, playing cards, telling stories, ex-
ercise coaching and video conferencing. A
live, working demo system will be pre-
sented at the meeting.
1 Introduction
The Always-On project1 is a four-year effort, cur-
rently in its third year, supported by the U.S. Na-
tional Science Foundation at Worcester Polytech-
nic Institute and Northeastern University. The goal
of the project is to create a relational agent that
will provide social support to reduce the isolation
of healthy, but isolated older adults. The agent is
?always on,? which is to say that it is continuously
available and aware (using a camera and infrared
motion sensor) when the user is in its presence and
can initiate interaction with the user, rather than,
for example requiring the user login to begin in-
teraction. Our goal is for the agent to be a natural,
human-like presence that ?resides? in the user?s
dwelling for an extended period of time. Begin-
ning in the fall of 2013, we will be placing our
agents with about a number of users for a month-
long, 4 arm, evaluation/comparison study.
1http://www.cs.wpi.edu/?rich/always
Some%reply!
Another%reply%
Something%else%
Me%too!!
I?ve%got%great%%cards%
Some%reply!
Another%reply%
Something%else%
Some%reply!
Another%reply%
Something%else%
Just%%play!%
I?ve%got%terrible%cards!%
Figure 1: Virtual agent interface ? ?Karen?
Our project focuses on four key scientific is-
sues:
? the embodiment of the agent,
? the interaction paradigm,
? the engagement between the user and the
agent, and
? the nature of the social relationship between
the user and the agent.
1.1 Embodiment
We are experimenting with two forms of agent em-
bodiment. Our main study will employ the vir-
tual agent Karen, shown in Figure 1, that comes
from the work of Bickmore et al (Bickmore et
al., 2005). Karen is a human-like agent animated
from a cartoon-shaded 3D model. She is shown
in Figure 1 playing a social game of cards with
user. Notice that user input is via a touch-screen
menu. Also, the speech bubble does not appear
148
in the actual interface, which uses text-to-speech
generation.
We are also planning an exploratory study sub-
stituting the Reeti2 robot, shown in Figure 2,
for Karen, but otherwise keeping the rest of the
system (i.e., the menus, text-to-speech and other
screen graphics) as much the same as possible.
One big difference we expect is that the effect of
face tracking with the robotic agent will be much
stronger than with Karen. On the other hand, be-
cause Reeti is not as human-like as Karen, it is
possible that it will not be as well accepted overall
as Karen.
1.2 Interaction Paradigm
The main interaction paradigm in our system is
conversation, and in particular, dialog. The agent
makes its contributions to the dialog using speech,
and the user chooses his/her contribution from a
menu of utterances provided on the touch screen.
Dialogs evolve around various activities and can
extend for quite a long time (up to five or ten min-
utes) if the user chooses to continue the conversa-
tion. Dialog models can be created using whatever
system that the system designer chooses. In our
work, we use models that are scripting formats,
a Java state machine model based on adjacency
pairs or created with the dialog tool Disco (Rich
and Sidner, 2012). This variety of models makes
our system more flexible for system designers.
The agent is not designed to accept speech input
for several reasons:
? lack of voice models for older adults;
? no reliable means to circumscribe the collec-
tion of utterances that the system could un-
derstand;
? the wide range of activities to talk about with
the agent results in a huge number of utter-
ance structures, semantic structures and pos-
sible intentions. We doubt there are existing
speech-to-utterance semantics systems avail-
able to support such a plethora of choices
with high reliability. As our project is not
about spoken language understanding, we
opted not to take on this burden.
Some of the activities between user and agent
involve additional on-screen graphics, such as the
2http://www.reeti.fr
card game shown in Figure 1, or a Week-At-A-
GlanceTM style planning calendar. When playing
cards together, the user is allowed to directly ma-
nipulate the cards on-screen. For the calendar,
the user may only do deictic gestures. All other
information is handled through dialog. We have
thus eschewed other traditional GUI methods us-
ing icons, pull-down lists, etc., in favor of using
speech and menu dialog interaction whenever pos-
sible. The other exception, like direct manipula-
tion of cards on-screen, is a virtual keyboard to
allow typing in of proper names of people and
places. Our motivation for this design choice is
to reinforce the relationship between the user and
the agent, and to simplify the interaction in com-
parison to standard GUIs.
1.3 Engagement
Our system continu-
Figure 2: Robotic
interface ? ?Reeti?
ously maintains a model
of the state of engage-
ment (Sidner et al, 2005)
between the user and the
agent. For example, when
the agent senses nearby
motion (via infrared) fol-
lowed by the appearance
of a face in its vision
system, it decides that the
user is initiating engagement. Disengagement
can come about at the natural conclusion of
the conversation or when the user leaves for an
unexpected reason, e.g., to answer a ringing door
bell. Because our agent cannot understand sounds
in the environment, it may not know why the user
has disengaged, but it does have simple strategies
for dealing with unexpected interruptions. Gen-
erally, the agent does not initiate disengagement,
although it may attempt to hurry the conclusion
of a session if some event in the user?s calendar is
about to start.
Since the user and agent have conversations
over an extended period of time, it is natural to
consider that they have some kind of social re-
lationship (Bickmore and Schulman, 2012; Kidd
and Breazeal, 2007). To reason about this rela-
tionship, we have implemented a planning system
(Coon et al, 2013) that decides which activities
are appropriate to suggest to the user each time
they interact (in what we call a session). This plan-
ning system uses a relationship model based on
149
the closeness between the agent and user. Their
closeness increases as they do activities together.
Closeness decreases when the user and agent do
not interact for a period of time, such as a few
days.
Each available activity has a required closeness
in order to be undertaken. Only those activities
whose required closeness is less than or equal to
the current closeness between the user and agent
will be suggested for the current session. Activi-
ties that, although suggested, do not actually occur
(due to user choice or other reasons) are reported
to the relationship planning system for planning
the next session.
2 Activities for User and Agent
We will demonstrate our operational system with a
several of the activities that the user and agent can
do together. In total, we will have more than ten
different activities including: discuss the weather,
learn about the activities to do with the agent, play
a social game of cards, talk about family/friends,
tell a life story to the agent, promote exercise,
promote nutrition, hear a humorous tale from the
agent, get health tips from the agent, speak with
a friend/family member via SkypeTM (with all the
details of SKYPE managed by the agent), and
manage a personal calendar for the user.
A typical interaction with the agent might start
with some greetings (specific to the time of day)
and then some discussion of the weather. The
weather discussion can be as short as today?s
weather forecast or extend to the next day, weather
in other cities, and weather where friends or fam-
ily live. At the user?s choice, weather might be fol-
lowed by a social game of cards where the agent?s
and user?s hands in the game and the way the game
is played out are commented upon. If the user
and agent are somewhat well acquainted, there-
after might follow discussion of the user?s family
and friends. For each person that agent has learned
about, the agent might ask:
Figure 3 shows an excerpt from a menu-based
interaction in our currently running system. Fol-
lowing ths discussion of family and friends the
user might opt to end the conversation or continue
with other activities such as the calendaring sys-
tem, hearing some health tips from the agent, and
so on.
Agent: So how is Mary doing?
? I don?t want to talk about her today.
? I?m done talking about my family and friends.
? Overall pretty good.
? Not so great. ?User
Agent: Oh, what?s wrong?
? She is ill.
? She had an accident.
? She lost a family member. ?User
? She is pretty lonely.
? She is postponing a visit to me.
Agent: That?s very sad.
Figure 3: Example menu-based interaction.
Acknowledgments
This work is supported in part by the National Sci-
ence Foundation under award IIS-1012083. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
T. Bickmore and D. Schulman. 2012. Empirical val-
idation of an accomodation theory-based model of
user-agent relationship. In Proc. Int. Conf. on Intel-
ligent Virtual Agents, Santa Cruz, CA.
T. Bickmore, L. Caruso, K. Clough-Gorr, and
T. Heeren. 2005. ?It?s just like you talk to a friend??
Relational agents for older adults. Interacting with
Computers, 17(6):711?735.
W. Coon, C. Rich, and C. Sidner. 2013. Activity plan-
ning for long-term relationships. In Proc. Int. Conf.
on Intelligent Virtual Agents, Edinburgh, UK.
C.D. Kidd and C. Breazeal. 2007. A robotic weight
loss coach. In Proc. 22nd National Conference on
Artificial Intelligence, Vancouver, Canada.
C. Rich and C. L. Sidner. 2012. Using collaborative
discourse theory to partially automate dialogue tree
authoring. In Proc. Int. Conf. on Intelligent Virtual
Agents, Santa Cruz, CA, September.
C. L. Sidner, C. Lee, C. Kidd, N. Lesh, and C. Rich.
2005. Explorations in engagement for humans and
robots. Artificial Intelligence, 166(1-2):104?164.
150
