Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 45?53,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Sentence Diagram Generation Using Dependency Parsing
Elijah Mayfield
Division of Science and Mathematics
University of Minnesota, Morris
mayf0016@morris.umn.edu
Abstract
Dependency parsers show syntactic re-
lations between words using a directed
graph, but comparing dependency parsers
is difficult because of differences in the-
oretical models. We describe a system
to convert dependency models to a struc-
tural grammar used in grammar educa-
tion. Doing so highlights features that are
potentially overlooked in the dependency
graph, as well as exposing potential weak-
nesses and limitations in parsing models.
Our system performs automated analysis
of dependency relations and uses them to
populate a data structure we designed to
emulate sentence diagrams. This is done
by mapping dependency relations between
words to the relative positions of those
words in a sentence diagram. Using an
original metric for judging the accuracy of
sentence diagrams, we achieve precision
of 85%. Multiple causes for errors are pre-
sented as potential areas for improvement
in dependency parsers.
1 Dependency parsing
Dependencies are generally considered a strong
metric of accuracy in parse trees, as described in
(Lin, 1995). In a dependency parse, words are
connected to each other through relations, with a
head word (the governor) being modified by a de-
pendent word. By converting parse trees to de-
pendency representations before judging accuracy,
more detailed syntactic information can be discov-
ered. Recently, however, a number of dependency
parsers have been developed that have very differ-
ent theories of a correct model of dependencies.
Dependency parsers define syntactic relations
between words in a sentence. This can be done
either through spanning tree search as in (McDon-
ald et al, 2005), which is computationally expen-
sive, or through analysis of another modeling sys-
tem, such as a phrase structure parse tree, which
can introduce errors from the long pipeline. To
the best of our knowledge, the first use of de-
pendency relations as an evaluation tool for parse
trees was in (Lin, 1995), which described a pro-
cess for determining heads in phrase structures
and assigning modifiers to those heads appropri-
ately. Because of different ways to describe rela-
tions between negations, conjunctions, and other
grammatical structures, it was immediately clear
that comparing different models would be diffi-
cult. Research into this area of evaluation pro-
duced several new dependency parsers, each us-
ing different theories of what constitutes a cor-
rect parse. In addition, attempts to model multi-
ple parse trees in a single dependency relation sys-
tem were often stymied by problems such as dif-
ferences in tokenization systems. These problems
are discussed by (Lin, 1998) in greater detail. An
attempt to reconcile differences between parsers
was described in (Marneffe et al, 2006). In this
paper, a dependency parser (from herein referred
to as the Stanford parser) was developed and com-
pared to two other systems: MINIPAR, described
in (Lin, 1998), and the Link parser of (Sleator and
Temperley, 1993), which uses a radically differ-
ent approach but produces a similar, if much more
fine-grained, result.
Comparing dependency parsers is difficult. The
main problem is that there is no clear way to com-
pare models which mark dependencies differently.
For instance, when clauses are linked by a con-
junction, the Link parser considers the conjunction
related to the subject of a clause, while the Stan-
ford parser links the conjunction to the verb of a
clause. In (Marneffe et al, 2006), a simple com-
parison was used to alleviate this problem, which
was based only on the presence of dependencies,
without semantic information. This solution loses
45
information and is still subject to many problems
in representational differences. Another problem
with this approach is that they only used ten sen-
tences for comparison, randomly selected from the
Brown corpus. This sparse data set is not necessar-
ily congruous with the overall accuracy of these
parsers.
In this paper, we propose a novel solution to
the difficulty of converting between dependency
models. The options that have previously been
presented for comparing dependency models are
either too specific to be accurate (relying on an-
notation schemes that are not adequately parallel
for comparison) or too coarse to be useful (such
as merely checking for the existence of depen-
dencies). By using a model of language which
is not as fine-grained as the models used by de-
pendency parsers, but still contains some semantic
information beyond unlabelled relations, a com-
promise can be made. We show that using linear
diagramming models can do this with acceptable
error rates, and hope that future work can use this
to compare multiple dependency models.
Section 2 describes structural grammar, its his-
tory, and its usefulness as a representation of syn-
tax. Section 3 describes our algorithm for conver-
sion from dependency graphs to a structural rep-
resentation. Section 4 describes the process we
used for developing and testing the accuracy of
this algorithm, and Section 5 discusses our results
and a variety of features, as well as limitations and
weaknesses, that we have found in the dependency
representation of (Marneffe et al, 2006) as a result
of this conversion.
2 Introduction to structural grammar
Structural grammar is an approach to natural lan-
guage based on the understanding that the major-
ity of sentences in the English language can be
matched to one of ten patterns. Each of these pat-
terns has a set of slots. Two slots are universal
among these patterns: the subject and the predi-
cate. Three additional slots may also occur: the
direct object, the subject complement, and the ob-
ject complement. A head word fills each of these
slots. In addition, any word in a sentence may be
modified by an additional word. Finally, anywhere
that a word could be used, a substitution may be
made, allowing the position of a word to be filled
by a multiple-word phrase or an entire subclause,
with its own pattern and set of slots.
To understand these relationships better, a stan-
dardized system of sentence diagramming has
been developed. With a relatively small number of
rules, a great deal of information about the func-
tion of each word in a sentence can be represented
in a compact form, using orientation and other spa-
tial clues. This provides a simpler and intuitive
means of visualizing relationships between words,
especially when compared to the complexity of di-
rected dependency graphs. For the purposes of this
paper, we use the system of diagramming formal-
ized in (Kolln and Funk, 2002).
2.1 History
First developed in the early 20th century, structural
grammar was a response to the prescriptive gram-
mar approach of the time. Structural grammar de-
scribes how language actually is used, rather than
prescribing how grammar should be used. This
approach allows an emphasis to be placed on the
systematic and formulaic nature of language. A
key change involved the shift to general role-based
description of the usage of a word, whereas the fo-
cus before had been on declaring words to fall into
strict categories (such as the eight parts of speech
found in Latin).
Beginning with the work of Chomsky in the
1950s on transformational grammar, sentence di-
agrams, used in both structural and prescriptive
approaches, slowly lost favor in educational tech-
niques. This is due to the introduction of trans-
formational grammar, based on generative theo-
ries and intrinsic rules of natural language struc-
ture. This generative approach is almost uni-
versally used in natural language processing, as
generative rules are well-suited to computational
representation. Nevertheless, both structural and
transformational grammar are taught at secondary
and undergraduate levels.
2.2 Applications of structural grammar
Structural grammar still has a number of advan-
tages over generative transformational grammar.
Because it is designed to emulate the natural usage
of language, it is more intuitive for non-experts to
understand. It also highlights certain features of
sentences, such as dependency relationships be-
tween words and targets of actions. Many facets
of natural language are difficult to describe using
a parse tree or other generative data structure. Us-
ing structural techniques, many of these aspects
are obvious upon basic analysis.
46
Figure 1: Diagram of ?The students are scholars.?
and ?The students studied their assignment.?
By developing an algorithm to automatically
analyze a sentence using structural grammar, we
hope that the advantages of structural analysis
can improve the performance of natural language
parsers. By assigning roles to words in a sentence,
patterns or structures in natural language that can-
not be easily gleaned from a data structure are
made obvious, highlighting the limitations of that
structure. It is also important to note that while
sentence diagrams are primarily used for English,
they can be adapted to any language which uses
subjects, verbs, and objects (word order is not im-
portant in sentence diagramming). This research
can therefore be expanded into multilingual de-
pendency parser systems in the future.
To test the effectiveness of these approaches, a
system must be developed for structural analysis
of sentences and subsequent conversion to a sen-
tence diagram.
3 Sentence diagram generation
algorithm
In order to generate a sentence diagram, we make
use of typed dependency graphs from the Stanford
dependency parser. To understand this process
requires understanding both the underlying data
structure representing a sentence diagram, and the
conversion from a directed graph to this data struc-
ture.
3.1 Data structure
In order to algorithmically convert dependency
parses to a structural grammar, we developed an
original model to represent features of sentence
diagrams. A sentence is composed of four slots
(Subject, Predicate, Object, Complement). These
slots are represented
1
in two sentences shown in
1
All sentence diagram figures were generated by the al-
gorithm described in this paper. Some diagrams have been
Figure 2: Diagram of ?Running through the woods
is his favorite activity.?
Figure 1 by the words ?students,? ?are,? ?assign-
ment,? and ?scholars? respectively. Each slot con-
tains three sets (Heads, Expletives, Conjunctions).
With the exception of the Heads slot in Subject
and Predicate, all sets may be empty. These sets
are populated by words. A word is comprised of
three parts: the string it represents, a set of mod-
ifying words, and information about its orienta-
tion in a diagram. Finally, anywhere that a word
may fill a role, it can be replaced by a phrase or
subclause. These phrases are represented iden-
tically to clauses, but all sets are allowed to be
empty. Phrases and subclauses filling the role of
a word are connected to the slot they are filling by
a pedestal, as in Figure 2.
3.2 Conversion from dependency graph
A typed dependency representation of a sentence
contains a root ? that is, a dependency relation
in which neither the governor nor the dependent
word in the relation is dependent in any other re-
lation. We use this relation to determine the predi-
cate of a sentence, which is almost always the gov-
ernor of the root dependency. The dependent is
added to the diagram data structure based on its
relation to the governor.
Before analysis of dependency graphs begins,
our algorithm takes in a set of dependency rela-
tions S and a set of actions (possible objects and
methods to call) A. This paper describes an algo-
rithm that takes in the 55 relations from (Marn-
effe et al, 2006) and the actions in Table 1. The
algorithm then takes as input a directed graph G
representing a sentence, composed of a node rep-
edited for spacing and readability concerns. These changes
do not affect their accuracy.
47
resenting each word in the sentence. These nodes
are connected by edges in the form reln(gov,
dep) representing a relation from S between a
word gov and dep. Our algorithm performs the
following steps:
1. Determining root actions: For each relation
type R ? S, create an ordered list of actions
Root < R,A > from A to perform if that re-
lation is the root relation in the graph.
2. Determining regular actions: For each re-
lation type R ? S, create an ordered list of
actions Reln < R,A > from A to perform if
R is found anywhere other than the root in G.
3. Determining the root: Using the root-
finding process described in (Marneffe et al,
2006), find the root relation
?
R(
?
G,
?
D) ? G.
4. Initialize a sentence diagram: Find the set
of actions
?
A from Root <
?
R,A >and perform
those actions.
5. Finding children: Create a set Open and add
to it each relation ? G in which
?
G or
?
D from
step 3 is a governor.
6. Processing children: For each relation
?
R(
?
G,
?
D) in Open,
(a) Populate the sentence diagram: Find
the set of actions
?
A from Reln <
?
R,A >
and perform those actions.
(b) Finding children: Add to Open each
relation R ?G in which
?
G or
?
D is a gov-
ernor.
This step continues until all relations have
been found in a breadth-first order.
Our system of conversion makes the assumption
that the governor of a typed dependency will al-
ready have been assigned a position in a diagram.
This is due to the largely tree-like structure of
dependency graphs generated by the dependency
parser. Dependencies in most cases ?flow? down-
wards to the root, and in exceptions, such as cy-
cles, the governor will have been discovered by the
time it is reached again. As we are searching for
words breadth-first, we know that the dependent
of any relation will have been discovered already
so long as this tree-like structure holds. The num-
ber of cases where it does not is small compared
to the overall error rate of the dependency parser,
and does not have a large impact on the accuracy
of the resulting diagram.
3.3 Single-relation analysis
A strength of this system for conversion is that in-
formation about the overall structure of a sentence
is not necessary for determining the role of each
individual word as it is added to the diagram. As
each word is traversed, it is assigned a role relative
to its parent only. This means that overall structure
will be discovered naturally by tracing dependen-
cies throughout a graph.
There is one exception to this rule: when com-
paring relationships of type cop (copula, a link-
ing verb, usually a variant of ?to be?), three words
are involved: the linking verb, the subject, and the
subject complement. However, instead of a tran-
sitive relationship from one word to the next, the
parser assigns the subject and subject complement
as dependent words of the linking verb. An exam-
ple is the sentence ?The students are scholars? as
in Figure 1. This sentence contains three relations:
det(students, The)
nsubj(scholars, students)
cop(scholars, are)
A special case exists in our algorithm to check
the governor of a cop relation for another rela-
tion (usually nsubj). This was a necessary ex-
ception to make given the frequency of linking
verbs in the English language. Dependency graphs
from (Marneffe et al, 2006) are defined as a
singly rooted directed acyclic graph with no re-
entrancies; however, they sometimes share nodes
in the tree, with one word being a dependent of
multiple relations. An example of this exists in
the sentence ?I saw the man who loves you.? The
word ?who? in this sentence is dependent in two
relations:
ref(man, who)
rel(loves, who)
We here refer to this phenomenon as breaking
the tree structure. This is notable because it causes
a significant problem for our approach. While the
correct relation is identified and assigned in most
cases, a duplicated copy of the dependent word
will appear in the resulting diagram. This is be-
cause the dependent word in each relation is added
to the diagram, even if it has already been added.
Modifiers of these words are then assigned to each
copy, which can result in large areas of duplica-
tion. We decided this duplication was acceptable
48
Term Definition Example
Input Output
GOV, DEP, RELN Elements of a relation det(??woods",
??the").GOV
??woods"
SBJ, PRD, OBJ,
CMP
Slots in a clause CLAUSE.PRD HEADS(??is"),
EXPL(),
CONJ()
HEADS, EXPL,
CONJ
Sets of words in a slot CLAUSE.PRD.HEADS() ??is"
MODS Set of modifiers of a
word
??activity".MODS (??his",
??favorite")
SEGMENT, CLAUSE Set or clause of word ??is".SEGMENT() CLAUSE.PRD
NEW[WORD, Slot] New clause constructor NEW(??is", PRD) CLAUSE(SBJ(),
PRD(??is"),
OBJ(),
CMP())
ADD(WORD[,ORIENT]) Word added to modi-
fiers
??activity".ADD(??his")
APP(WORD[,RIGHT?]) Word appended to
phrasal head
??down".APP(??shut",
false)
SET(ORIENT) Word orientation set ??his".SET(DIAGONAL)
Periods represent ownership, parentheses represent parameters passed to a method, separated by commas, and brackets repre-
sent optional parameters.
Orientations include HORIZONTAL, DIAGONAL, VERTICAL, GERUND, BENT, DASHED, and CLAUSE as defined in (Kolln
and Funk, 2002) .
Table 1: Terms and methods defined in our algorithm.
Figure 3: The sentence ?A big crowd turned out
for the parade.? shown as a dependency graph
(top) and a sentence diagram.
to maintain the simplicity of single-relation con-
version rules, though remedying this problem is an
avenue for further research. For testing purposes,
if duplicate copies of a word exist, the correct one
is given preference over the incorrect copy, and the
diagram is scored as correct if either copy is cor-
rectly located.
3.4 An example diagram conversion
To illustrate the conversion process, consider the
sentence ?A big crowd turned out for the parade.?
The dependency graph for this, as generated by the
Stanford dependency parser, is shown in Figure 3.
The following relations are found, with the actions
taken by the conversion algorithm described:
Root: nsubj(turned, crowd)
NEW(GOV, PRD);
GOV.CLAUSE.SBJ.ADD(DEP);
Finding Children: det(crowd, A),
amod(crowd, big), prt(turned, out),
prep(turned, for) added to Open.
Relation: det(crowd, A)
GOV.ADD(DEP,DIAGONAL);
Relation: amod(crowd, big)
GOV.ADD(DEP,DIAGONAL);
Relation: prt(turned, out)
GOV.APP(DEP,TRUE);
Relation: prep(turned, for)
Finding Children: pobj(for, parade)
added to Open.
GOV.ADD(DEP,DIAGONAL);
Relation: pobj(for, parade)
Finding Children: det(parade, the)
added to Open.
GOV.ADD(DEP,HORIZONTAL);
Relation: det(parade, the)
GOV.ADD(DEP,DIAGONAL);
4 Experimental setup
In order to test our conversion algorithm, a large
number of sentence diagrams were needed in order
49
to ensure a wide range of structures. We decided to
use an undergraduate-level English grammar text-
book that uses diagramming as a teaching tool
for two reasons. The first is a pragmatic matter:
the sentences have already been diagrammed ac-
curately for comparison to algorithm output. Sec-
ond, the breadth of examples necessary to allow
students a thorough understanding of the process
is beneficial in assuring the completeness of the
conversion system. Cases that are especially diffi-
cult for students are also likely to be stressed with
multiple examples, giving more opportunities to
determine the problem if parsers have similar dif-
ficulty.
Therefore, (Kolln and Funk, 2002) was selected
to be used as the source of this testing data. This
textbook contained 292 sentences, 152 from ex-
amples and 140 from solutions to problem sets.
50% of the example sentences (76 in total, chosen
by selecting every other example) were set aside
to use for development. The remaining 216 sen-
tences were used to gauge the accuracy of the con-
version algorithm.
Our implementation of this algorithm was de-
veloped as an extension of the Stanford depen-
dency parser. We developed two metrics of pre-
cision to evaluate the accuracy of a diagram. The
first approach, known as the inheritance metric,
scored the results of the algorithm based on the
parent of each word in the output sentence dia-
gram. Head words were judged on their placement
in the correct slot, while modifiers were judged
on whether they modified the correct parent word.
The second approach, known as the orientation
metric, judged each word based solely on its ori-
entation. This distinction judges whether a word
was correctly identified as a primary or modifying
element of a sentence.
These scoring systems have various advantages.
By only scoring a word based on its immediate
parent, a single mistake in the diagram does not
severely impact the result of the score, even if it is
at a high level in the diagram. Certain mistakes are
affected by one scoring system but not the other;
for instance, incorrect prepositional phrase attach-
ment will not have an effect on the orientation
score, but will reduce the value of the inheritance
score. Alternatively, a mistake such as failing to
label a modifying word as a participial modifier
will reduce the orientation score, but will not re-
duce the value of the inheritance score. Generally,
orientation scoring is more forgiving than inheri-
tance scoring.
5 Results and discussion
The results of testing these accuracy metrics are
given in Figure 4 and Table 2. Overall inheritance
precision was 85% and overall orientation preci-
sion was 92%. Due to the multiple levels of analy-
sis (parsing from tree to phrase structure to depen-
dency graph to diagram), it is sometimes difficult
to assign fault to a specific step of the algorithm.
There is clearly some loss of information when
converting from a dependency graph to a sentence
diagram. For example, fifteen dependency rela-
tions are represented as diagonal modifiers in a
sentence diagram and have identical conversion
rules. Interestingly, these relations are not nec-
essarily grouped together in the hierarchy given
in (Marneffe et al, 2006). This suggests that the
syntactic information represented by these words
may not be as critical as previously thought, given
enough semantic information about the words. In
total, six sets of multiple dependency relations
mapping to the same conversion rule were found,
as shown in Table 3.
The vast majority of mistakes that were made
came from one of two sources: an incorrect con-
version from a correct dependency parse, or a fail-
ure of the dependency parser to correctly identify
a relation between words in a sentence. Both are
examined below.
5.1 Incorrect conversion rules
On occasion, a flaw in a diagram was the result of
an incorrect conversion from a correct interpreta-
tion in a dependency parse. In some cases, these
were because of simple changes due to inaccura-
cies not exposed from development data. In some
cases, this was a result of an overly general rela-
tionship, in which one relation correctly describes
two or more possible structural patterns in sen-
tences. This can be improved upon by specializ-
ing dependency relation descriptions in future ver-
sions of the dependency parser.
One frequent failure of the conversion rules is
due to the overly generalized handling of the root
of sentences. It is assumed that the governing
word in the root relation of a dependency graph
is the main verb of a sentence. Our algorithm has
very general rules for root handling. Exceptions
to these general cases are possible, especially in
50
Sentence Length Ori Mean Ori Std.Dev. Inh Mean Inh Std.Dev. Count
3-6 96.61 7.42 90.34 15.20 56
7-8 92.37 15.77 86.00 19.34 57
9-10 92.80 8.18 82.73 17.15 45
11-20 89.97 12.54 82.52 15.51 58
3-20 92.91 11.84 85.51 17.05 216
Table 2: Precision of diagramming algorithm on testing data.
Relations Rule
abbrev, advmod, amod, dep, det, measure, neg, nn,
num, number, poss, predet, prep, quantmod, ref
GOV.ADD(DEP,DIAGONAL)
iobj, parataxis, pobj GOV.ADD(DEP,HORIZONTAL)
appos, possessive, prt GOV.APP(DEP,TRUE)
aux, tmod GOV.APP(DEP,FALSE)
advcl, csubj, pcomp, rcmod GOV.ADD(NEW(DEP,PRD))
complm, expl, mark GOV.SEGMENT.EXPL.ADD(DEP)
Table 3: Sets of multiple dependency relations which are converted identically.
0.00.20.40.60.81.0
Orient
ation b
y Quar
tiles
0.00.20.40.60.81.0
Inherit
ance b
y Quar
tiles
Figure 4: Inheritance (top) and Orientation preci-
sion results of diagramming algorithm on testing
data. Results are separated by sentence length into
quartiles.
interrogative sentences, e.g. the root relation of
the sentence ?What have you been reading?? is
dobj(reading, What). This should be han-
dled by treating ?What? as the object of the clause.
This problem can be remedied in the future by cre-
ating specialized conversion rules for any given re-
lation as a root of a dependency graph.
A final issue is the effect of a non-tree struc-
ture on the conversion algorithm. Because rela-
tionships are evaluated individually, multiple in-
heritance for words can sometimes create dupli-
cate copies of a word which are then modified in
parallel. An example of this is shown in Figure 5,
which is caused due to the dependency graph for
this sentence containing the following relations:
nsubj(is-4, hope-3)
xsubj(beg-6, hope-3)
xcomp(is-4, beg-6)
Because the tree structure is broken, a word
(hope) is dependent on two different governing
words. While the xsubj relation places the phrase
?to beg for mercy? correctly in the diagram, a sec-
ond copy is created because of the xcomp depen-
dency. A more thorough analysis approach that
checks for breaking of the tree structure may be
useful in avoiding this problem in the future.
5.2 Exposed weaknesses of dependency
parsers
A number of consistent patterns are poorly dia-
grammed by this system. This is usually due to
51
Figure 5: Duplication in the sentence diagram for
?Our only hope is to beg for mercy.?
limitations in the theoretical model of the depen-
dency parser. These differences between the ac-
tual structure of the sentence and the structure the
parser assigns can lead to a significant difference
in semantic value of phrases. Improving the accu-
racy of this model to account for these situations
(either through more fine-grained separation of re-
lationships or a change in the model) may improve
the quality of meaning extraction from sentences.
One major shortcoming of the dependency
parser is how it handles prepositional phrases.
As described in (Atterer and Schutze, 2007), this
problem has traditionally been framed as involv-
ing four words (v, n1, p, n2) where v is the head of
a verb phrase, n1 is the head of a noun phrase dom-
inated by v, p is the head of a prepositional phrase,
and n2 the head of a noun phrase dominated by
p. Two options have generally been given for at-
tachment, either to the verb v or the noun n1. This
parser struggles to accurately determine which of
these two possibilities should be used. However,
in the structural model of grammar, there is a third
option, treating the prepositional phrase as an ob-
ject complement of n1. This possibility occurs fre-
quently in English, such as in the sentence ?We
elected him as our secretary.? or with idiomatic ex-
pressions such as ?out of tune.? The current depen-
dency parser cannot represent this at all.
5.3 Ambiguity
A final case is when multiple correct structural
analyses exist for a single sentences. In some
cases, this causes the parser to produce a gramati-
cally and semantically correct parse which, due to
ambiguity, does not match the diagram for com-
parison. An example of this can be seen in Fig-
ure 6, in which the dependency parser assigns the
Figure 6: Diagram of ?On Saturday night the li-
brary was almost deserted.?
predicate role to ?was deserted? when in fact de-
serted is acting as a subject complement. How-
ever, the phrase ?was deserted? can accurately act
as a predicate in that sentence, and produces a se-
mantically valid interpretation of the phrase.
6 Conclusion
We have demonstrated a promising method for
conversion from a dependency graph to a sentence
diagram. However, this approach still has the op-
portunity for a great deal of improvement. There
are two main courses of action for future work to
reap the benefits of this approach: analyzing cur-
rent results, and extending this approach to other
parsers for comparison. First, a more detailed
analysis of current errors should be undertaken to
determine areas for improvement. There are two
broadly defined categories of error (errors made
before a dependency graph is given to the algo-
rithm for conversion, and errors made during con-
version to a diagram). However, we do not know
what percent of mistakes falls into those two cat-
egories. We also do not know what exact gram-
matical idiosyncracy caused each of those errors.
With further examination of current data, this in-
formation can be determined.
Second, it must be determined what level of
conversion error is acceptable to begin making
quantitative comparisons of dependency parsers.
Once the level of noise introduced by the conver-
sion process is lowered to the point that the major-
ity of diagram errors are due to mistakes or short-
falls in the dependency graph itself, this tool will
be much more useful for evaluation. Finally, this
system should be extended to other dependency
parsers so that a comparison can be made between
multiple systems.
References
Michaela Atterer and Hinrich Schutze. 2007. Preposi-
tional Phrase Attachment without Oracles. In Com-
52
putational Linguistics.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Pro-
ceedings of the EACL workshop on Linguistically
Interpreted Corpora.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics.
Martha Kolln and Robert Funk. 2002. Understanding
English Grammar, Sixth Edition. Longman Publish-
ers.
Dekang Lin. 1995. A Dependency-based Method
for Evaluating Broad-Coverage Parsers. In Natural
Language Engineering.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Workshop on the Evaluation of Pars-
ing Systems
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In International Conference on Language Resources
and Evaluation.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing.
Daniel D. Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Conference on Parsing Technologies.
53
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 25?28,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Interactive Tool for Supporting Error Analysis for Text Mining
Elijah Mayfield
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15216, USA
elijah@cmu.edu
Carolyn Penstein-Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15216, USA
cprose@cs.cmu.edu
Abstract
This demo abstract presents an interactive tool
for supporting error analysis for text mining,
which is situated within the Summarization
Integrated Development Environment (SIDE).
This freely downloadable tool was designed
based on repeated experience teaching text
mining over a number of years, and has been
successfully tested in that context as a tool for
students to use in conjunction with machine
learning projects.
1 Introduction
In the past decade, more and more work in the
language technologies community has shifted from
work on formal, rule-based methods to work involv-
ing some form of text categorization or text mining
technology. At the same time, use of this technology
has expanded; where it was once accessible only to
those within studying core language technologies,
it is now almost ubiquitous. Papers involving text
mining can currently be found even in core social
science and humanities conferences.
The authors of this demonstration are involved
in regular teaching of an applied machine learning
course, which attracts students from virtually every
field, including a variety of computer science related
fields, the humanities and social sciences, and the
arts. In five years of teaching this course, what has
emerged is the finding that the hardest skill to impart
to students is the ability to do a good error analysis.
In response to this issue, the interactive error analy-
sis tool presented here was designed, developed, and
successfully tested with students.
In the remainder of this demo abstract, we offer an
overview of the development environment that pro-
vides the context for this work. We then describe
on a conceptual level the error analysis process that
the tool seeks to support. Next, we step through the
process of conducting an error analysis with the in-
terface. We conclude with some directions for our
continued work, based on observation of students?
use of this interface.
2 Overview of SIDE
The interactive error analysis interface is situated
within an integrated development environment for
building summarization systems. Note that the
SIDE (Kang et al, 2008) software and comprehen-
sive user?s manual are freely available for down-
load1. We will first discuss the design of SIDE from
a theoretical standpoint, and then explore the details
of practical implementation.
2.1 Design Goals
SIDE was designed with the idea that documents,
whether they are logs of chat discussions, sets of
posts to a discussion board, or notes taken in a
course, can be considered relatively unstructured.
Nevertheless, when one thinks about their interpre-
tation of a document, or how they would use the in-
formation found within a document, then a structure
emerges. For example, an argument written in a pa-
per often begins with a thesis statement, followed by
supporting points, and finally a conclusion. A reader
1SIDE and its documentation are downloadable from
http://www.cs.cmu.edu/?cprose/SIDE.html
25
can identify with this structure even if there is noth-
ing in the layout of the text that indicates that certain
sentences within the argument have a different sta-
tus from the others. Subtle cues in the language can
be used to identify those distinct roles that sentences
might play.
Conceptually, then, the use of SIDE proceeds in
two main parts. The first part is to construct filters
that can impose that structure on the texts to be sum-
marized, to identify the role a sentence is playing
in a document; and the second part is constructing
specifications of summaries that refer to that struc-
ture, such as subsets of extracted text or data visu-
alizations. This demo is primarily concerned with
supporting error analysis for text mining. Thus, the
first of these two stages will be the primary focus.
This approach to summarization was inspired by
the process described in (Teufel and Moens, 2002).
That work focused on the summarization of scien-
tific articles to describe a new work in a way which
rhetorically situates that work?s contribution within
the context of related prior work. This is done by
first overlaying structure onto the documents to be
summarized, categorizing the sentences they contain
into one of a number of rhetorical functions. Once
this structure is imposed, using the information it
provides was shown to increase the quality of gener-
ated summaries.
2.2 Building Text Mining Models with SIDE
This demo assumes the user has already interacted
with the SIDE text mining interface for model build-
ing, including feature extraction and machine learn-
ing, to set up a model. Defining this in SIDE terms,
to train the system and create a model, the user first
has to define a filter. Filters are trained using ma-
chine learning technology. Two customization op-
tions are available to analysts in this process.
The first and possibly most important is the set of
customization options that affect the design of the
attribute space. The standard attribute space is set
up with one attribute per unique feature - the value
corresponds to the number of times that feature oc-
curs in a text. Options include unigrams, bigrams,
part-of-speech bigrams, stemming, and stopword re-
moval.
The next step is the selection of the machine
learning algorithm that will be used. Dozens of op-
tions are made available through the Weka toolkit
(Witten and Frank, 2005), although some are more
commonly used than others. The three options that
are most recommended to analysts beginning work
with machine learning are Na??ve Bayes (a prob-
abilistic model), SMO (Weka?s implementation of
Support Vector Machines), and J48, which is one
of many Weka implementations of a Decision Tree
learner. SMO is considered state-of-the-art for text
classification, so we expect that analysts will fre-
quently find that to be the best choice.
As this error analysis tool is built within SIDE, we
focus on applications to text mining. However, this
tool can also be used on non-text data sets, so long as
they are first preprocessed through SIDE. The details
of our error analysis approach are not specific to any
individual task or machine learning algorithm.
3 High Level View of Error Analysis
In an insightful usage of applied machine learning, a
practitioner will design an approach that takes into
account what is known about the structure of the
data that is being modeled. However, typically, that
knowledge is incomplete, and there is thus a good
chance that the decisions that are made along the
way are suboptimal. When the approach is evalu-
ated, it is possible to determine based on the pro-
portion and types of errors whether the performance
is acceptable for the application or not. If it is not,
then the practitioner should engage in an error analy-
sis process to determine what is malfunctioning and
what could be done to better model the structure in
the data.
In well-known machine learning toolkits such as
Weka, some information is available about what er-
rors are being made. Predictions can be printed out,
to allow a researcher to identify how a document is
being classified. One common format for summariz-
ing these predictions is a confusion matrix, usually
printed in a format like:
a b <-- classified as
67 19 | a = PT
42 70 | b = DR
This lists, for example, that 19 text segments were
classified as type DR but were actually type PT.
While this gives a rough view of what errors are
26
Figure 1: The error analysis interface with key function-
ality locations highlighted.
appearing, it gives no indication of why the errors
are being made. This is where a more extensive er-
ror analysis is necessary. Two common ways to ap-
proach this question are top down, which starts with
a learned model, and bottom up, which starts with
the confusion matrix from that model?s performance
estimate. In the first case, the model is examined
to find the attributes that are treated as most impor-
tant. These are the attributes that have the great-
est influence on the predictions made by the learned
model, and thus these attributes provide a good start-
ing point. In the second case, the bottom-up case,
one first examines the confusion matrix to identify
large off-diagonal cells, which represent common
confusions. The error analysis for any error cell is
then the process of determining relations between
three sets of text segments2 related to that cell.
Within the ?classified as DR but actually PT? cell,
for instance, error analysis would require finding
what makes these examples most different from ex-
amples correctly classified as PT, and what makes
these examples most similar to those correctly clas-
sified as DR. This can be done by identifying at-
tributes that mostly strongly differentiate the first
two sets, and attributes most similar between the first
and third sets. An ideal approach would combine
these two directions.
4 Error Analysis Process
Visitors to this demo will have the opportunity to ex-
periment with the error analysis interface. It will be
set up with multiple data sets and previously trained
text mining models. These models can first be exam-
ined from the model building window, which con-
tains information such as:
? Global feature collection, listing all features
that were included in the trained model.
? Cross-validation statistics, including variance
and kappa statistics, the confusion matrix and
other general information.
? Weights or other appropriate information for
the text mining model that was trained.
By moving to the error analysis interface, the user
can explore a model more deeply. The first step is
to select a model to examine. By default, all text
segments that were evaluated in cross-validation dis-
play in a scrolling list in the bottom right corner of
the window. Each row contains the text within a seg-
ment, and the associated feature vector. Users will
first be asked to examine this data to understand the
magnitude of the error analysis challenge.
Clicking on a cell in the confusion matrix (at the
top of the screen) will fill the scrolling list at the bot-
tom left corner of the screen with the classified seg-
ments that fall in that cell. A comparison chooser
dropdown menu gives three analysis options - full,
horizontal, and vertical. By default, full comparison
2Our interface assumes that the input text has been seg-
mented already; depending on the task involved, these segments
may correspond to a sentence, a paragraph, or even an entire
document.
27
is selected, and shows all text segments used in train-
ing. The two additional modes of comparison allow
some insight into what features are most representa-
tive of the subset of segments in that cell, compared
to the correct predictions aligned with that cell (ei-
ther vertically or horizontally within the confusion
matrix). By switching to horizontal comparison, the
scrolling list on the right changes to display only text
segments that fall in the cell which is along the con-
fusion matrix diagonal and horizontal to the selected
cell. Switching to vertical comparison changes this
list to display segments categorized in the cell which
is along the diagonal and vertically aligned with the
selected error cell.
Once a comparison method is selected, there is
a feature highlighting dropdown menu which is of
use. The contents in this menu are sorted by degree
of difference between the segments in the two lists
at the bottom of the screen. This means, for a hor-
izontal comparison, that features at the top of this
list are the most different between the two cells (this
difference is displayed in the menu). We compute
this difference by the difference in expected (aver-
age) value for that feature between the two sets. In a
vertical comparison, features are ranked by similar-
ity, instead of difference. Once a feature is selected
from this menu, two significant changes are made.
The first is that a second confusion matrix appears,
giving the confusion matrix values (mean and stan-
dard deviation) for the highlighted feature. The sec-
ond is that the two segment lists are sorted according
to the feature being highlighted.
User interface design elements were important in
this design process. One option available to users is
the ability to ?hide empty features,? which removes
features which did not occur at all in one or both of
the sets being studied. This allows the user to fo-
cus on features which are most likely to be causing
a significant change in a classifier?s performance. It
is also clear that the number of different subsets of
classified segments can become very confusing, es-
pecially when comparing various types of error in
one session. To combat this, the labels on the lists
and menus will change to reflect some of this infor-
mation. For instance, the left-hand panel gives the
predicted and actual labels of the segments you have
highlighted, while the right-hand panel is labelled
with the name of the category of correct prediction
you are comparing against. The feature highlighting
dropdown menu also changes to reflect similar in-
formation about the type of comparison being made.
5 Future Directions
This error analysis tool has been used in the text
mining unit for an Applied Machine Learning course
with approximately 30 students. In contrast to pre-
vious semesters where the tool was not available
to support error analysis, the instructor noticed that
many more students were able to begin surpassing
shallow observations, instead forming hypotheses
about where the weaknesses in a model are, and
what might be done to improve performance.
Based on our observations, however, the error
analysis support could still be improved by directing
users towards features that not only point to differ-
ences and similarities between different subsets of
instances, but also to more information about how
features are being used in the trained model. This
can be implemented either in algorithm-specific
ways (such as displaying the weight of features in
an SVM model) or in more generalizable formats,
for instance, through information gain. Investigating
how to score these general aspects, and presenting
this information in an intuitive way, are directions
for our continued development of this tool.
Acknowledgements
This research was supported by NSF Grant DRL-
0835426.
References
Moonyoung Kang, Sourish Chaudhuri, Mahesh Joshi,
and Carolyn Penstein-Rose? 2008. SIDE: The Summa-
rization Integrated Development Environment. Pro-
ceedings of the Association for Computational Lin-
guistics, Demo Abstracts.
Simone Teufel and Marc Moens 2002. Summarizing
Scientific Articles: Experiments with Relevance and
Rhetorical Status. Computational Linguistics, Vol. 28,
No. 1.
Ian Witten and Eibe Frank 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, second
edition. Elsevier: San Fransisco.
28
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1018?1026,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Recognizing Authority in Dialogue with an Integer Linear Programming
Constrained Model
Elijah Mayfield
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
elijah@cmu.edu
Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
cprose@cs.cmu.edu
Abstract
We present a novel computational formula-
tion of speaker authority in discourse. This
notion, which focuses on how speakers posi-
tion themselves relative to each other in dis-
course, is first developed into a reliable cod-
ing scheme (0.71 agreement between human
annotators). We also provide a computational
model for automatically annotating text using
this coding scheme, using supervised learning
enhanced by constraints implemented with In-
teger Linear Programming. We show that this
constrained model?s analyses of speaker au-
thority correlates very strongly with expert hu-
man judgments (r2 coefficient of 0.947).
1 Introduction
In this work, we seek to formalize the ways speak-
ers position themselves in discourse. We do this in
a way that maintains a notion of discourse structure,
and which can be aggregated to evaluate a speaker?s
overall stance in a dialogue. We define the body of
work in positioning to include any attempt to formal-
ize the processes by which speakers attempt to influ-
ence or give evidence of their relations to each other.
Constructs such as Initiative and Control (Whittaker
and Stenton, 1988), which attempt to operationalize
the authority over a discourse?s structure, fall under
the umbrella of positioning. As we construe posi-
tioning, it also includes work on detecting certainty
and confusion in speech (Liscombe et al, 2005),
which models a speaker?s understanding of the in-
formation in their statements. Work in dialogue act
tagging is also relevant, as it seeks to describe the ac-
tions and moves with which speakers display these
types of positioning (Stolcke et al, 2000).
To complement these bodies of work, we choose
to focus on the question of how speakers position
themselves as authoritative in a discourse. This
means that we must describe the way speakers intro-
duce new topics or discussions into the discourse;
the way they position themselves relative to that
topic; and how these functions interact with each
other. While all of the tasks mentioned above focus
on specific problems in the larger rhetorical question
of speaker positioning, none explicitly address this
framing of authority. Each does have valuable ties
to the work that we would like to do, and in section
2, we describe prior work in each of those areas, and
elaborate on how each relates to our questions.
We measure this as an authoritativeness ratio. Of
the contentful dialogue moves made by a speaker,
in what fraction of those moves is the speaker po-
sitioned as the primary authority on that topic? To
measure this quantitatively, we introduce the Nego-
tiation framework, a construct from the field of sys-
temic functional linguistics (SFL), which addresses
specifically the concepts that we are interested in.
We present a reproducible formulation of this so-
ciolinguistics research in section 3, along with our
preliminary findings on reliability between human
coders, where we observe inter-rater agreement of
0.71. Applying this coding scheme to data, we see
strong correlations with important motivational con-
structs such as Self-Efficacy (Bandura, 1997) as well
as learning gains.
Next, we address automatic coding of the Ne-
gotiation framework, which we treat as a two-
1018
dimensional classification task. One dimension is
a set of codes describing the authoritative status of
a contribution1. The other dimension is a segmen-
tation task. We impose constraints on both of these
models based on the structure observed in the work
of SFL. These constraints are formulated as boolean
statements describing what a correct label sequence
looks like, and are imposed on our model using an
Integer Linear Programming formulation (Roth and
Yih, 2004). In section 5, this model is evaluated
on a subset of the MapTask corpus (Anderson et
al., 1991) and shows a high correlation with human
judgements of authoritativeness (r2 = 0.947). After
a detailed error analysis, we will conclude the paper
in section 6 with a discussion of our future work.
2 Background
The Negotiation framework, as formulated by the
SFL community, places a special emphasis on how
speakers function in a discourse as sources or recip-
ients of information or action. We break down this
concept into a set of codes, one code per contribu-
tion. Before we break down the coding scheme more
concretely in section 3, it is important to understand
why we have chosen to introduce a new framework,
rather than reusing existing computational work.
Much work has examined the emergence of dis-
course structure from the choices speakers make at
the linguistic and intentional level (Grosz and Sid-
ner, 1986). For instance, when a speaker asks a
question, it is expected to be followed with an an-
swer. In discourse analysis, this notion is described
through dialogue games (Carlson, 1983), while con-
versation analysis frames the structure in terms of
adjacency pairs (Schegloff, 2007). These expec-
tations can be viewed under the umbrella of con-
ditional relevance (Levinson, 2000), and the ex-
changes can be labelled discourse segments.
In prior work, the way that people influence dis-
course structure is described through the two tightly-
related concepts of initiative and control. A speaker
who begins a discourse segment is said to have ini-
tiative, while control accounts for which speaker is
being addressed in a dialogue (Whittaker and Sten-
ton, 1988). As initiative passes back and forth be-
tween discourse participants, control over the con-
1We treat each line in our corpus as a single contribution.
versation similarly transfers from one speaker to an-
other (Walker and Whittaker, 1990). This relation is
often considered synchronous, though evidence sug-
gests that the reality is not straightforward (Jordan
and Di Eugenio, 1997).
Research in initiative and control has been ap-
plied in the form of mixed-initiative dialogue sys-
tems (Smith, 1992). This is a large and ac-
tive field, with applications in tutorial dialogues
(Core, 2003), human-robot interactions (Peltason
and Wrede, 2010), and more general approaches to
effective turn-taking (Selfridge and Heeman, 2010).
However, that body of work focuses on influenc-
ing discourse structure through positioning. The
question that we are asking instead focuses on how
speakers view their authority as a source of informa-
tion about the topic of the discourse.
In particular, consider questioning in discourse.
In mixed-initiative analysis of discourse, asking a
question always gives you control of a discourse.
There is an expectation that your question will be
followed by an answer. A speaker might already
know the answer to a question they asked - for
instance, when a teacher is verifying a student?s
knowledge. However, in most cases asking a ques-
tion represents a lack of authority, treating the other
speakers as a source for that knowledge. While there
have been preliminary attempts to separate out these
specific types of positioning in initiative, such as
Chu-Carroll and Brown (1998), it has not been stud-
ied extensively in a computational setting.
Another similar thread of research is to identify
a speaker?s certainty, that is, the confidence of a
speaker and how that self-evaluation affects their
language (Pon-Barry and Shieber, 2010). Substan-
tial work has gone into automatically identifying
levels of speaker certainty, for example in Liscombe
et al (2005) and Litman et al (2009). The major
difference between our work and this body of liter-
ature is that work on certainty has rarely focused on
how state translates into interaction between speak-
ers (with some exceptions, such as the application
of certainty to tutoring dialogues (Forbes-Riley and
Litman, 2009)). Instead, the focus is on the person?s
self-evaluation, independent of the influence on the
speaker?s positioning within a discourse.
Dialogue act tagging seeks to describe the moves
people make to express themselves in a discourse.
1019
This task involves defining the role of each contri-
bution based on its function (Stolcke et al, 2000).
We know that there are interesting correlations be-
tween these acts and other factors, such as learning
gains (Litman and Forbes-Riley, 2006) and the rel-
evance of a contribution for summarization (Wrede
and Shriberg, 2003). However, adapting dialogue
act tags to the question of how speakers position
themselves is not straightforward. In particular,
the granularity of these tagsets, which is already a
highly debated topic (Popescu-Belis, 2008), is not
ideal for the task we have set for ourselves. Many
dialogue acts can be used in authoritative or non-
authoritative ways, based on context, and can posi-
tion a speaker as either giver or receiver of informa-
tion. Thus these more general tagsets are not specific
enough to the role of authority in discourse.
Each of these fields of prior work is highly valu-
able. However, none were designed to specifically
describe how people present themselves as a source
or recipient of knowledge in a discourse. Thus, we
have chosen to draw on a different field of soci-
olinguistics. Our formalization of that theory is de-
scribed in the next section.
3 The Negotiation Framework
We now present the Negotiation framework2, which
we use to answer the questions left unanswered in
the previous section. Within the field of SFL, this
framework has been continually refined over the last
three decades (Berry, 1981; Martin, 1992; Martin,
2003). It attempts to describe how speakers use their
role as a source of knowledge or action to position
themselves relative to others in a discourse.
Applications of the framework include distin-
guishing between focus on teacher knowledge and
student reasoning (Veel, 1999) and distribution of
authority in juvenile trials (Martin et al, 2008). The
framework can also be applied to problems similar
to those studied through the lens of initiative, such
as the distinction between authority over discourse
structure and authority over content (Martin, 2000).
A challenge of applying this work to language
technologies is that it has historically been highly
2All examples are drawn from the MapTask corpus and in-
volve an instruction giver (g) and follower (f). Within examples,
discourse segment boundaries are shown by horizontal lines.
qualitative, with little emphasis placed on repro-
ducibility. We have formulated a pared-down, repro-
ducible version of the framework, presented in Sec-
tion 3.1. Evidence of the usefulness of that formu-
lation for identifying authority, and of correlations
that we can study based on these codes, is presented
briefly in Section 3.2.
3.1 Our Formulation of Negotiation
The codes that we can apply to a contribution us-
ing the Negotiation framework are comprised of four
main codes, K1, K2, A1, and A2, and two additional
codes, ch and o. This is a reduction over the many
task-specific or highly contextual codes used in the
original work. This was done to ensure that a ma-
chine learning classification task would not be over-
whelmed with many infrequent classes.
The main codes are divided by two questions.
First, is the contribution related to exchanging infor-
mation, or to exchanging services and actions? If the
former, then it is a K move (knowledge); if the latter,
then an A move (action). Second, is the contribution
acting as a primary actor, or secondary? In the case
of knowledge, this often correlates to the difference
between assertions (K1) and queries (K2). For in-
stance, a statement of fact or opinion is a K1:
g K1 well i?ve got a great viewpoint
here just below the east lake
By contrast, asking for someone else?s knowledge
or opinion is a K2:
g K2 what have you got underneath the
east lake
f K1 rocket launch
In the case of action, the codes usually corre-
spond to narrating action (A1) and giving instruc-
tions (A2), as below:
g A2 go almost to the edge of the lake
f A1 yeah
A challenge move (ch) is one which directly con-
tradicts the content or assertion of the previous line,
or makes that previous contribution irrelevant. For
instance, consider the exchange below, where an in-
struction is rejected because its presuppositions are
broken by the challenging statement.
g A2 then head diagonally down to-
wards the bottom of the dead tree
f ch i have don?t have a dead tree i
have a dutch elm
1020
All moves that do not fit into one of these cate-
gories are classified as other (o). This includes back-
channel moves, floor-grabbing moves, false starts,
and any other non-contentful contributions.
This theory makes use of discourse segmenta-
tion. Research in the SFL community has focused
on intra-segment structure, and empirical evidence
from this research has shown that exchanges be-
tween speakers follow a very specific pattern:
o* X2? o* X1+ o*
That is to say, each segment contains a primary
move (a K1 or an A1) and an optional preceding
secondary move, with other non-contentful moves
interspersed throughout. A single statement of fact
would be a K1 move comprising an entire segment,
while a single question/answer pair would be a K2
move followed by a K1. Longer exchanges of many
lines obviously also occur.
We iteratively developed a coding manual which
describes, in a reproducible way, how to apply the
codes listed above. The six codes we use, along with
their frequency in our corpus, are given in Table 1.
In the next section, we evaluate the reliability and
utility of hand-coded data, before moving on to au-
tomation in section 4.
3.2 Preliminary Evaluation
This coding scheme was evaluated for reliability on
two corpora using Cohen?s kappa (Cohen, 1960).
Within the social sciences community, a kappa
above 0.7 is considered acceptable. Two conversa-
tions were each coded by hand by two trained anno-
tators. The first conversation was between three stu-
dents in a collaborative learning task; inter-rater re-
liability kappa for Negotiation labels was 0.78. The
second conversation was from the MapTask corpus,
and kappa was 0.71. Further data was labelled by
hand by one trained annotator.
In our work, we label conversations using the cod-
ing scheme above. To determine how well these
codes correlate with other interesting factors, we
choose to assign a quantitative measure of authori-
tativeness to each speaker. This measure can then
be compared to other features of a speaker. To do
this, we use the coded labels to assign an Authori-
tativeness Ratio to each speaker. First, we define a
Code Meaning Count Percent
K1 Primary Knower 984 22.5
K2 Secondary Knower 613 14.0
A1 Primary Actor 471 10.8
A2 Secondary Actor 708 16.2
ch Challenge 129 2.9
o Other 1469 33.6
Total 4374 100.0
Table 1: The six codes in our coding scheme, along with
their frequency in our corpus of twenty conversations.
functionA(S, c, L) for a speaker, a contribution, and
a set of labels L ? {K1,K2, A1, A2, o, ch} as:
A(S, c, L) =
{
1 c spoken by S with label l ? L
0 otherwise.
We then define the Authoritativeness ratio
Auth(S) for a speaker S in a dialogue consisting
of contributions c1...cn as:
Auth(S) =
n?
i=1
A(S, ci, {K1, A2})
n?
i=1
A(S, ci, {K1,K2, A1, A2})
The intuition behind this ratio is that we are only
interested in the four main label types in our analy-
sis - at least for an initial description of authority, we
do not consider the non-contentful o moves. Within
these four main labels, there are clearly two that ap-
pear ?dominant? - statements of fact or opinion, and
commands or instructions - and two that appear less
dominant - questions or requests for information,
and narration of an action. We sum these together
to reach a single numeric value for each speaker?s
projection of authority in the dialogue.
The full details of our external validations of this
approach are available in Howley et al (2011). To
summarize, we considered two data sets involving
student collaborative learning. The first data set con-
sisted of pairs of students interacting over two days,
and was annotated for aggressive behavior, to assess
warning factors in social interactions. Our analysis
1021
showed that aggressive behavior correlated with au-
thoritativeness ratio (p < .05), and that less aggres-
sive students became less authoritative in the second
day (p < .05, effect size .15?). The second data
set was analyzed for Self-Efficacy - the confidence
of each student in their own ability (Bandura, 1997)
- as well as actual learning gains based on pre- and
post-test scores. We found that the Authoritativeness
ratio was a significant predictor of learning gains
(r2 = .41, p < .04). Furthermore, in a multiple re-
gression, we determined that the Authoritativeness
ratio of both students in a group predict the average
Self-Efficacy of the pair (r2 = .12, p < .01).
4 Computational Model
We know that our coding scheme is useful for mak-
ing predictions about speakers. We now judge
whether it can be reproduced fully automatically.
Our model must select, for each contribution ci in a
dialogue, the most likely classification label li from
{K1,K2, A1, A2, o, ch}. We also build in paral-
lel a segmentation model to select si from the set
{new, same}. Our baseline approach to both prob-
lems is to use a bag-of-words model of the contribu-
tion, and use machine learning for classification.
Certain types of interactions, explored in section
4.1, are difficult or impossible to classify without
context. We build a contextual feature space, de-
scribed in section 4.2, to enhance our baseline bag-
of-words model. We can also describe patterns that
appear in discourse segments, as detailed in section
3.1. In our coding manual, these instructions are
given as rules for how segments should be coded by
humans. Our hypothesis is that by enforcing these
rules in the output of our automatic classifier, per-
formance will increase. In section 4.3 we formalize
these constraints using Integer Linear Programming.
4.1 Challenging cases
We want to distinguish between phenomena such as
in the following two examples.
f K2 so I?m like on the bank on the
bank of the east lake
g K1 yeah
In this case, a one-token contribution is indis-
putably a K1 move, answering a yes/no question.
However, in the dialogue below, it is equally inar-
guable that the same move is an A1:
g A2 go almost to the edge of the lake
f A1 yeah
Without this context, these moves would be indis-
tinguishable to a model. With it, they are both easily
classified correctly.
We also observed that markers for segmentation
of a segment vary between contentful initiations and
non-contentful ones. For instance, filler noises can
often initiate segments:
g o hmm...
g K2 do you have a farmer?s gate?
f K1 no
Situations such as this are common. This is also a
challenge for segmentation, as demonstrated below:
g K1 oh oh it?s on the right-hand side
of my great viewpoint
f o okay yeah
g o right eh
g A2 go almost to the edge of the lake
f A1 yeah
A long statement or instruction from one speaker
is followed up with a terse response (in the same
segment) from the listener. However, after that back-
channel move, a short floor-grabbing move is often
made to start the next segment. This is a distinc-
tion that a bag-of-words model would have difficulty
with. This is markedly different from contentful seg-
ment initiations:
g A2 come directly down below the
stone circle and we come up
f ch I don?t have a stone circle
g o you don?t have a stone circle
All three of these lines look like statements, which
often initiate new segments. However, only the first
should be marked as starting a new segment. The
other two are topically related, in the second line by
contradicting the instruction, and in the third by re-
peating the previous person?s statement.
4.2 Contextual Feature Space Additions
To incorporate the insights above into our model, we
append features to our bag-of-words model. First,
in our classification model we include both lexical
bigrams and part-of-speech bigrams to encode fur-
ther lexical knowledge and some notion of syntac-
tic structure. To account for restatements and topic
shifts, we add a feature based on cosine similarity
(using term vectors weighted by TF-IDF calculated
1022
over training data). We then add a feature for the
predicted label of the previous contribution - after
each contribution is classified, the next contribution
adds a feature for the automatic label. This requires
our model to function as an on-line classifier.
We build two segmentation models, one trained
on contributions of less than four tokens, and an-
other trained on contributions of four or more to-
kens, to distinguish between characteristics of con-
tentful and non-contentful contributions. To the
short-contribution model, we add two additional fea-
tures. The first represents the ratio between the
length of the current contribution and the length of
the previous contribution. The second represents
whether a change in speaker has occurred between
the current and previous contribution.
4.3 Constraints using Integer Linear
Programming
We formulate our constraints using Integer Linear
Programming (ILP). This formulation has an ad-
vantage over other sequence labelling formulations,
such as Viterbi decoding, in its ability to enforce
structure through constraints. We then enhance this
classifier by adding constraints, which allow expert
knowledge of discourse structure to be enforced in
classification. We can use these constraints to elim-
inate label options which would violate the rules for
a segment outlined in our coding manual.
Each classification decision is made at the contri-
bution level, jointly optimizing the Negotiation la-
bel and segmentation label for a single contribution,
then treating those labels as given for the next con-
tribution classification.
To define our objective function for optimization,
for each possible label, we train a one vs. all SVM,
and use the resulting regression for each label as
a score, giving us six values ~li for our Negotiation
label and two values ~si for our segmentation label.
Then, subject to the constraints below, we optimize:
arg max
l?~li,s?~si
l + s
Thus, at each contribution, if the highest-scoring
Negotiation label breaks a constraint, the model can
optimize whether to drop to the next-most-likely la-
bel, or start a new segment.
Recall from section 3.1 that our discourse seg-
ments follow strict rules related to ordering and rep-
etition of contributions. Below, we list the con-
straints that we used in our model to enforce that
pattern, along with a brief explanation of the intu-
ition behind each.
?ci ? s, (li = K2)?
?j < i, cj ? t? (lj 6= K1)
(1)
?ci ? s, (li = A2)?
?j < i, cj ? t? (lj 6= A1)
(2)
The first constraints enforce the rule that a pri-
mary move cannot occur before a secondary move
in the same segment. For instance, a question must
initiate a new segment if it follows a statement.
?ci ? s, (li ? {A1, A2})?
?j < i, cj ? s? (lj /? {K1,K2})
(3)
?ci ? s, (li ? {K1,K2})?
?j < i, cj ? s? (lj /? {A1, A2})
(4)
These constraints specify that A moves and K
moves cannot cooccur in a segment. An instruc-
tion for action and a question requesting information
must be considered separate segments.
?ci ? s, (li = A1)? ((li?1 = A1) ?
?j < i, cj ? s? (lj 6= A1))
(5)
?ci ? s, (li = K1)? ((li?1 = K1) ?
?j < i, cj ? s? (lj 6= K1))
(6)
This pair states that two primary moves cannot oc-
cur in the same segment unless they are contiguous,
in rapid succession.
?ci ? s, (li = A1)?
?j < i, cj ? s, (lj = A2)? (Si 6= Sj)
(7)
?ci ? s, (li = K1)?
?j < i, cj ? s, (lj = K2)? (Si 6= Sj)
(8)
The last set of constraints enforce the intuitive
notion that a speaker cannot follow their own sec-
ondary move with a primary move in that segment
(such as answering their own question).
1023
Computationally, an advantage of these con-
straints is that they do not extend past the current
segment in history. This means that they usually
are only enforced over the past few moves, and do
not enforce any global constraint over the structure
of the whole dialogue. This allows the constraints
to be flexible to various conversational styles, and
tractable for fast computation independent of the
length of the dialogue.
5 Evaluation
We test our models on a twenty conversation sub-
set of the MapTask corpus detailed in Table 1. We
compare the use of four models in our results.
? Baseline: This model uses a bag-of-words fea-
ture space as input to an SVM classifier. No
segmentation model is used and no ILP con-
straints are enforced.
? Baseline+ILP: This model uses the baseline
feature space as input to both classification and
segmentation models. ILP constraints are en-
forced between these models.
? Contextual: This model uses our enhanced
feature space from section 4.2, with no segmen-
tation model and no ILP constraints enforced.
? Contextual+ILP: This model uses the en-
hanced feature spaces for both Negotiation la-
bels and segment boundaries from section 4.2
to enforce ILP constraints.
For segmentation, we evaluate our models using
exact-match accuracy. We use multiple evaluation
metrics to judge classification. The first and most
basic is accuracy - the percentage of accurately cho-
sen Negotiation labels. Secondly, we use Cohen?s
Kappa (Cohen, 1960) to judge improvement in ac-
curacy over chance. The final evaluation is the r2
coefficient computed between predicted and actual
Authoritativeness ratios per speaker. This represents
how much variance in authoritativeness is accounted
for in the predicted ratios. This final metric is the
most important for measuring reproducibility of hu-
man analyses of speaker authority in conversation.
We use SIDE for feature extraction (Mayfield
and Rose?, 2010), SVM-Light for machine learning
Model Accuracy Kappa r2
Baseline 59.7% 0.465 0.354
Baseline+ILP 61.6% 0.488 0.663
Segmentation 72.3%
Contextual 66.7% 0.565 0.908
Contextual+ILP 68.4% 0.584 0.947
Segmentation 74.9%
Table 2: Performance evaluation for our models. Each
line is significantly improved in both accuracy and r2 er-
ror from the previous line (p < .01).
(Joachims, 1999), and Learning-Based Java for ILP
inference (Rizzolo and Roth, 2010). Performance
is evaluated by 20-fold cross-validation, where each
fold is trained on 19 conversations and tested on the
remaining one. Statistical significance was calcu-
lated using a student?s paired t-test. For accuracy
and kappa, n = 20 (one data point per conversation)
and for r2, n = 40 (one data point per speaker).
5.1 Results
All classification results are given in Table 2 and
charts showing correlation between predicted and
actual speaker Authoritativeness ratios are shown in
Figure 1. We observe that the baseline bag-of-words
model performs well above random chance (kappa
of 0.465); however, its accuracy is still very low
and its ability to predict Authoritativeness ratio of
a speaker is not particularly high (r2 of 0.354 with
ratios from manually labelled data). We observe a
significant improvement when ILP constraints are
applied to this model.
The contextual model described in section 4.2
performs better than our baseline constrained model.
However, the gains found in the contextual model
are somewhat orthogonal to the gains from using
ILP constraints, as applying those constraints to
the contextual model results in further performance
gains (and a high r2 coefficient of 0.947).
Our segmentation model was evaluated based on
exact matches in boundaries. Switching from base-
line to contextual features, we observe an improve-
ment in accuracy of 2.6%.
5.2 Error Analysis
An error analysis of model predictions explains the
large effect on correlation despite relatively smaller
1024
Figure 1: Plots of predicted (x axis) and actual (y axis) Authoritativeness ratios for speakers across 20 conversations,
for the Baseline (left), Baseline+Constraints (center), and Contextual+Constraints (right) models.
changes in accuracy. Our Authoritativeness ratio
does not take into account moves labelled o or
ch. What we find is that the most advanced model
still makes many mistakes at determining whether a
move should be labelled as o or a core move. This er-
ror rate is, however, fairly consistent across the four
core move codes. When a move is determined (cor-
rectly) to not be an o move, the system is highly ac-
curate in distinguishing between the four core labels.
The one systematic confusion that continues to
appear most frequently in our results is the inabil-
ity to distinguish between a segment containing an
A2 move followed by an A1 move, and a segment
containing a K1 move followed by an o move. The
surface structure of these types of exchanges is very
similar. Consider the following two exchanges:
g A2 if you come down almost to the
bottom of the map that I?ve got
f A1 uh-huh
f K1 but the meadow?s below my bro-
ken gate
g o right yes
These two exchanges on a surface level are highly
similar. Out of context, making this distinction is
very hard even for human coders, so it is not surpris-
ing then that this pattern is the most difficult one to
recognize in this corpus. It contributes most of the
remaining confusion between the four core codes.
6 Conclusions
In this work we have presented one formulation of
authority in dialogue. This formulation allows us
to describe positioning in discourse in a way that
is complementary to prior work in mixed-initiative
dialogue systems and analysis of speaker certainty.
Our model includes a simple understanding of dis-
course structure while also encoding information
about the types of moves used, and the certainty of a
speaker as a source of information. This formulation
is reproducible by human coders, with an inter-rater
reliability of 0.71.
We have then presented a computational model
for automatically applying these codes per contribu-
tion. In our best model, we see a good 68.4% accu-
racy on a six-way individual contribution labelling
task. More importantly, this model replicates human
analyses of authoritativeness very well, with an r2
coefficient of 0.947.
There is room for improvement in our model in
future work. Further use of contextual features will
more thoroughly represent the information we want
our model to take into account. Our segmentation
accuracy is also fairly low, and further examination
of segmentation accuracy using a more sophisticated
evaluation metric, such as WindowDiff (Pevzner and
Hearst, 2002), would be helpful.
In general, however, we now have an automated
model that is reliable in reproducing human judg-
ments of authoritativeness. We are now interested in
how we can apply this to the larger questions of po-
sitioning we began this paper by asking, especially
in describing speaker positioning at various instants
throughout a single discourse. This will be the main
thrust of our future work.
Acknowledgements
This research was supported by NSF grants SBE-
0836012 and HCC-0803482.
1025
References
Anne Anderson, Miles Bader, Ellen Bard, Elizabeth
Boyle, Gwyneth Doherty, Simon Garrod, et al 1991.
The HCRC Map Task Corpus. In Language and
Speech.
Albert Bandura. 1997. Self-efficacy: The Exercise of
Control
Margaret Berry. 1981. Towards Layers of Exchange
Structure for Directive Exchanges. In Network 2.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis.
Jennifer Chu-Carroll and Michael Brown. 1998. An Ev-
idential Model for Tracking Initiative in Collaborative
Dialogue Interactions. In User Modeling and User-
Adapted Interaction.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. In Educational and Psychological
Measurement.
Mark Core and Johanna Moore and Claus Zinn. 2003.
The Role of Initiative in Tutorial Dialogue. In Pro-
ceedings of EACL.
Kate Forbes-Riley and Diane Litman. 2009. Adapting to
Student Uncertainty Improves Tutoring Dialogues. In
Proceedings of Artificial Intelligence in Education.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. In Compu-
tational Linguistics.
Iris Howley and Elijah Mayfield and Carolyn Penstein
Rose?. 2011. Missing Something? Authority in Col-
laborative Learning. In Proceedings of Computer-
Supported Collaborative Learning.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In Advances in Kernel Methods
- Support Vector Learning.
Pamela Jordan and Barbara Di Eugenio. 1997. Control
and Initiative in Collaborative Problem Solving Dia-
logues. In Proceedings of AAAI Spring Symposium
on Computational Models for Mixed Initiative Inter-
actions.
Stephen Levinson. 2000. Pragmatics.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting Certainness in Spoken Tutorial
Dialogues. In Proceedings of Interspeech.
Diane Litman and Kate Forbes-Riley. 2006. Correlations
betweeen Dialogue Acts and Learning in Spoken Tu-
toring Dialogue. In Natural Language Engineering.
Diane Litman, Mihai Rotaru, and Greg Nicholas. 2009.
Classifying Turn-Level Uncertainty Using Word-Level
Prosody. In Proceedings of Interspeech.
James Martin. 1992. English Text: System and Structure.
James Martin. 2000. Factoring out Exchange: Types of
Structure. In Working with Dialogue.
James Martin and David Rose. 2003. Working with Dis-
course: Meaning Beyond the Clause.
James Martin, Michele Zappavigna, and Paul Dwyer.
2008. Negotiating Shame: Exchange and Genre Struc-
ture in Youth Justice Conferencing. In Proceedings of
European Systemic Functional Linguistics.
Elijah Mayfield and Carolyn Penstein Rose?. 2010. An
Interactive Tool for Supporting Error Analysis for Text
Mining. In Proceedings of Demo Session at NAACL.
Julia Peltason and Britta Wrede. 2010. Modeling
Human-Robot Interaction Based on Generic Interac-
tion Patterns. In AAAI Report on Dialog with Robots.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. In Computational Linguistics.
Heather Pon-Barry and Stuart Shieber. 2010. Assessing
Self-awareness and Transparency when Classifying a
Speakers Level of Certainty. In Speech Prosody.
Andrei Popescu-Belis. 2008. Dimensionality of Dia-
logue Act Tagsets: An Empirical Analysis of Large
Corpora. In Language Resources and Evaluation.
Nick Rizzolo and Dan Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Language
Resources and Evaluation.
Dan Roth and Wen-Tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural Lan-
guage Tasks. In Proceedings of CoNLL.
Emanuel Schegloff. 2007. Sequence Organization in In-
teraction: A Primer in Conversation Analysis.
Ethan Selfridge and Peter Heeman. 2010. Importance-
Driven Turn-Bidding for Spoken Dialogue Systems.
In Proceedings of ACL.
Ronnie Smith. 1992. A computational model of
expectation-driven mixed-initiative dialog processing.
Ph.D. Dissertation.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, et al 2000.
Dialogue Act Modeling for Automatic Tagging and
Recognition of Conversational Speech. In Computa-
tional Linguistics.
Robert Veel. 1999. Language, Knowledge, and Author-
ity in School Mathematics. In Pedagogy and the Shap-
ing of Consciousness: Linguistics and Social Pro-
cesses
Marilyn Walker and Steve Whittaker. 1990. Mixed Ini-
tiative in Dialogue: An Investigation into Discourse
Structure. In Proceedings of ACL.
Steve Whittaker and Phil Stenton. 1988. Cues and Con-
trol in Expert-Client Dialogues. In Proceedings of
ACL.
Britta Wrede and Elizabeth Shriberg. 2003. The Re-
lationship between Dialogue Acts and Hot Spots in
Meetings. In IEEE Workshop on Automatic Speech
Recognition and Understanding.
1026
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 740?749,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Historical Analysis of Legal Opinions
with a Sparse Mixed-Effects Latent Variable Model
William Yang Wang1 and Elijah Mayfield1 and Suresh Naidu2 and Jeremiah Dittmar3
1School of Computer Science, Carnegie Mellon University
2Department of Economics and SIPA, Columbia University
3American University and School of Social Science, Institute for Advanced Study
{ww,elijah}@cmu.edu sn2430@columbia.edu dittmar@american.edu
Abstract
We propose a latent variable model to enhance
historical analysis of large corpora. This work
extends prior work in topic modelling by in-
corporating metadata, and the interactions be-
tween the components in metadata, in a gen-
eral way. To test this, we collect a corpus
of slavery-related United States property law
judgements sampled from the years 1730 to
1866. We study the language use in these
legal cases, with a special focus on shifts in
opinions on controversial topics across differ-
ent regions. Because this is a longitudinal
data set, we are also interested in understand-
ing how these opinions change over the course
of decades. We show that the joint learning
scheme of our sparse mixed-effects model im-
proves on other state-of-the-art generative and
discriminative models on the region and time
period identification tasks. Experiments show
that our sparse mixed-effects model is more
accurate quantitatively and qualitatively inter-
esting, and that these improvements are robust
across different parameter settings.
1 Introduction
Many scientific subjects, such as psychology, learn-
ing sciences, and biology, have adopted computa-
tional approaches to discover latent patterns in large
scale datasets (Chen and Lombardi, 2010; Baker and
Yacef, 2009). In contrast, the primary methods for
historical research still rely on individual judgement
and reading primary and secondary sources, which
are time consuming and expensive. Furthermore,
traditional human-based methods might have good
precision when searching for relevant information,
but suffer from low recall. Even when language
technologies have been applied to historical prob-
lems, their focus has often been on information re-
trieval (Gotscharek et al, 2009), to improve acces-
sibility of texts. Empirical methods for analysis and
interpretation of these texts is therefore a burgeoning
new field.
Court opinions form one of the most important
parts of the legal domain, and can serve as an excel-
lent resource to understand both legal and political
history (Popkin, 2007). Historians often use court
opinions as a primary source for constructing in-
terpretations of the past. They not only report the
proceedings of a court, but also express a judges?
views toward the issues at hand in a case, and reflect
the legal and political environment of the region and
period. Since there exists many thousands of early
court opinions, however, it is difficult for legal his-
torians to manually analyze the documents case by
case. Instead, historians often restrict themselves to
discussing a relatively small subset of legal opinions
that are considered decisive. While this approach
has merit, new technologies should allow extraction
of patterns from large samples of opinions.
Latent variable models, such as latent Dirichlet al
location (LDA) (Blei et al, 2003) and probabilistic
latent semantic analysis (PLSA) (Hofmann, 1999),
have been used in the past to facilitate social science
research. However, they have numerous drawbacks,
as many topics are uninterpretable, overwhelmed by
uninformative words, or represent background lan-
guage use that is unrelated to the dimensions of anal-
ysis that qualitative researchers are interested in.
SAGE (Eisenstein et al, 2011a), a recently pro-
posed sparse additive generative model of language,
addresses many of the drawbacks of LDA. SAGE
assumes a background distribution of language use,
and enforces sparsity in individual topics. Another
advantage, from a social science perspective, is that
SAGE can be derived from a standard logit random-
utility model of judicial opinion writing, in contrast
to LDA. In this work we extend SAGE to the su-
pervised case of joint region and time period pre-
diction. We formulate the resulting sparse mixed-
effects (SME) model as being made up of mixed
effects that not only contain random effects from
sparse topics, but also mixed effects from available
metadata. To do this we augment SAGE with two
sparse latent variables that model the region and
time of a document, as well as a third sparse latent
740
variable that captures the interactions among the re-
gion, time and topic latent variables. We also intro-
duce a multiclass perceptron-style weight estimation
method to model the contributions from different
sparse latent variables to the word posterior prob-
abilities in this predictive task. Importantly, the re-
sulting distributions are still sparse and can therefore
be qualitatively analyzed by experts with relatively
little noise.
In the next two sections, we overview work re-
lated to qualitative social science analysis using la-
tent variable models, and introduce our slavery-
related early United States court opinion data. We
describe our sparse mixed-effects model for joint
modeling of region, time, and topic in section 4.
Experiments are presented in section 5, with a ro-
bust analysis from qualitative and quantitative stand-
points in section 5.2, and we discuss the conclusions
of this work in section 6.
2 Related Work
Natural Language Processing (NLP) methods for
automatically understanding and identifying key
information in historical data have not yet been
explored until recently. Related research efforts
include using the LDA model for topic model-
ing in historical newspapers (Yang et al, 2011),
a rule-based approach to extract verbs in histor-
ical Swedish texts (Pettersson and Nivre, 2011),
a system for semantic tagging of historical Dutch
archives (Cybulska and Vossen, 2011).
Despite our historical data domain, our approach
is more relevant to text classification and topic mod-
elling. Traditional discriminative methods, such as
support vector machine (SVM) and logistic regres-
sion, have been very popular in various text cate-
gorization tasks (Joachims, 1998; Wang and McKe-
own, 2010) in the past decades. However, the main
problem with these methods is that although they are
accurate in classifying documents, they do not aim
at helping us to understand the documents.
Another problem is lack of expressiveness. For
example, SVM does not have latent variables to
model the subtle differences and interactions of fea-
tures from different domains (e.g. text, links, and
date), but rather treats them as a ?bag-of-features?.
Generative methods, by contrast, can show the
causes to effects, have attracted attentions in re-
cent years due to the rich expressiveness of the
models and competitive performances in predictive
tasks (Wang et al, 2011). For example, Nguyen et
al. (2010) study the effect of the context of inter-
action in blogs using a standard LDA model. Guo
and Diab (2011) show the effectiveness of using se-
mantic information in multifaceted topic models for
text categorization. Eisenstein et al (2010) use a
latent variable model to predict geolocation infor-
mation of Twitter users, and investigate geographic
variations of language use. Temporally, topic mod-
els have been used to show the shift in language use
over time in online communities (Nguyen and Rose?,
2011) and the evolution of topics over time (Shub-
hankar et al, 2011).
When evaluating understandability, however,
dense word distributions are a serious issue in many
topic models as well as other predictive tasks. Such
topic models are often dominated by function words
and do not always effectively separate topics. Re-
cent work have shown significant gains in both pre-
dictiveness and interpretatibility by enforcing spar-
sity, such as in the task of discovering sociolinguistic
patterns of language use (Eisenstein et al, 2011b).
Our proposed sparse mixed-effects model bal-
ances the pros and cons the above methods, aim-
ing at higher classification accuracies using the SME
model for joint geographic and temporal aspects pre-
diction, as well as richer interaction of components
from metadata to enhance historical analysis in legal
opinions. To the best of our knowledge, this study is
the first of its kind to discover region and time spe-
cific topical patterns jointly in historical texts.
3 Data
We have collected a corpus of slavery-related United
States supreme court legal opinions from Lexis
Nexis. The dataset includes 5,240 slavery-related
state supreme court cases from 24 states, during the
period of 1730 - 1866. Optical character recognition
(OCR) software was used by Lexis Nexis to digitize
the original documents. In our region identification
task, we wish to identify whether an opinion was
written in a free state1 (R1) or a slave state (R2)2.
In our time identification experiment, we approx-
imately divide the legal documents into four time
quartiles (Q1, Q2, Q3, and Q4), and predict which
quartile the testing document belongs to. Q1 con-
tains cases from 1837 or earlier, where as Q2 is for
1838-1848, Q3 is for 1849-1855, and Q4 is for 1856
and later.
4 The Sparse Mixed-Effects Model
To address the over-parameterization, lack of ex-
pressiveness and robustness issues in LDA, the
SAGE (Eisenstein et al, 2011a) framework draws a
1Including border states, this set includes CT, DE, IL, KY,
MA, MD, ME, MI, NH, NJ, NY, OH, PA, and RI.
2These states include AR, AL, FL, GA, MS, NC, TN, TX,
and VA.
741
Figure 1: Plate diagram representation of the proposed
Sparse Mixed-Effects model with K topics, Q time peri-
ods, and R regions.
constant background distribution m, and additively
models the sparse deviation ? from the background
in log-frequency space. It also incorporates latent
variables ? to model the variance for each sparse de-
viation ?. By enforcing sparsity, the model might be
less likely to overfit the training data, and requires
estimation of fewer parameters.
This paper further extends SAGE to analyze mul-
tiple facets of a document collection, such as the
regional and temporal differences. Figure 1 shows
the graphical model of our proposed sparse mixed-
effects (SME) model. In this SME model, we still
have the same Dirichlet ?, the latent topic proportion
?, and the latent topic variable z as the original LDA
model. For each document d, we are able to ob-
serve two labels: the region label y(R)d and the time
quartile label y(Q)d . We also have a background dis-
tributionm that is drawn from a uninformative prior.
The three major sparse deviation latent variables are
?(T )k for topics, ?
(R)
j for regions, and ?
(Q)
q for time
periods. All of the three latent variables are condi-
tioned on another three latent variables, which are
their corresponding variances ? (T )k , ?
(R)
j and ?
(Q)
q .
In the intersection of the plates for topics, regions,
and time quartiles, we include another sparse latent
variable ?(I)qjk, which is conditioned on a variance
? (I)qjk, to model the interactions among topic, region
and time. ?(I)qjk is the linear combination of time pe-
riod, region and topic sparse latent variables, which
absorbs the residual variation that is not captured in
the individual effects.
In contrast to traditional multinomial distribution
of words in LDA models, we approximate the con-
ditional word distribution in the document d as the
exponentiated sum ? of all latent sparse deviations
?(T )k , ?
(R)
j , ?
(Q)
q , and ?
(I)
qjk, as well as the background
m:
P (w(d)n |z
(d)
n , ?,m, y
(R)
d , y
(Q)
d ) ? ?
=exp
(
m+ ?(T )
z(d)n
+ ?(R)?(R)y(r)
+ ?(Q)?(Q)y(q) + ?
(I)
y(r),y(q),z(d)n
)
Despite SME learns in a Bayesian framework, the
above ?(R) and ?(Q) are dynamic parameters that
weight the contributions of ?(R)
y(r)
and ?(Q)
y(q)
to the
approximated word posterior probability. A zero-
mean Laplace prior ? , which is conditioned on pa-
rameter ?, is introduced to induce sparsity, where
its distribution is equivalent to the joint distribution,?
N (?;m, ?)?(? ;?)d? , and ?(? ;?)d? is the Expo-
nential distribution (Lange and Sinsheimer, 1993).
We first describe a generative story for this SME
model:
? Draw a background m from corpus mean and ini-
tialize ?(T ), ?(R), ?(Q) and ?(I) sparse deviations
from corpus
? For each topic k
? For each word i
? Draw ? (T )k,i ? ?(?)
? Draw ?(T )k,i ? N (0, ?
(T )
k,i )
? Set ?k ? exp(m+?k+?(R)?(R)+?(Q)?(Q)+
?(I))
? For each region j
? For each word i
? Draw ? (R)j,i ? ?(?)
? Draw ?(R)j,i ? N (0, ?
(R)
j,i )
? Update ?j ? exp(m + ?(R)?j + ?(T ) +
?(Q)?(Q) + ?(I))
? For each time quartile q
? For each word i
? Draw ? (Q)q,i ? ?(?)
? Draw ?(Q)q,i ? N (0, ?
(Q)
q,i )
? Update ?q ? exp(m + ?(Q)?q + ?(T ) +
?(R)?(R) + ?(I))
? For each time quartile q, for each region j, for each
topic k
? For each word i
? Draw ? (I)q,j,k,i ? ?(?)
? Draw ?(I)q,j,k,i ? N (0, ?
(I)
q,j,k,i)
? Update ?q,j,k ? exp(m + ?q,j,k + ?(T ) +
?(R)?(R) + ?(Q)?(Q))
742
? For each document d
? Draw the region label y(R)d
? Draw the time quartile label y(Q)d
? For each word n, draw w(d)n ? ?yd
4.1 Parameter Estimation
We follow the MAP estimation method that Eisen-
stein et al (2011a) used to train all sparse latent vari-
ables ?, and perform Bayesian inference on other la-
tent variables. The estimation of all variance vari-
ables ? remains as plugging the compound distri-
bution of Normal-Jeffrey?s prior, where the latter is
a replacement of the Exponential prior. When per-
forming Expectation-Maximization (EM) algorithm
to infer the latent variables in SME, we derive the
following likelihood function:
L =
?
d
?logP (?d|?)?+
?
logP (Z(d)n |?d)
?
+
Nd?
n
?
logP (w(d)n |z
(d)
n , ?,m, y
(R)
d , y
(Q)
d )
?
+
?
k
?logP (?(T )k |0, ?
(T )
k )?+
?
k
?logP (? (T )k |?)?
+
?
j
?logP (?(R)j |0, ?
(R)
j )?+
?
j
?logP (? (R)j |?)?
+
?
q
?logP (?(Q)q |0, ?
Q)
q )?+
?
q
?logP (? (Q)q |?)?
+
?
q
?
j
?
k
?logP (?(I)q,j,k|0, ?
(I)
q,j,k)?
+
?
q
?
j
?
k
?logP (? (I)q,j,k|?)?
?
?
logQ(?, z, ?)
?
The above E step likelihood score can be intuitively
interpreted as the sum of topic proportion scores, la-
tent topic scores, the word scores, the ? scores with
their priors, and minus the joint variance. In the M
step, when we use Newton?s method to optimize the
sparse deviation ?k parameter, we need to modify
the original likelihood function in SAGE and its cor-
responding first and second order derivatives when
deriving the gradient and Hessian matrix. The like-
lihood function for sparse topic deviation ?k is:
L(?k) = ?c
(T )
k ?T?k
? Cd log
?
q
?
j
?
i
exp(?(Q)?qi + ?
(R)?ji
+ ?ki + ?qjki +mi)? ?kTdiag(?(?
(T )
k )
?1?)?(T )k /2
and we can derive the gradient when taking the first
order partial derivative:
?L
??(T )k
=?c(T )k ? ?
?
q
?
j
?Cqjk??qjk
? diag(?(? (T )k )
?1?)?(T )k
where c(T )k is the true count, and ?qjk is the log
word likelihood in the original likelihood function.
Cqjk is the expected count from combinations of
time, region and topic.
?
q
?
j?Cqjk??qjk will then
be taken the second order derivative to form the Hes-
sian matrix, instead of ?Ck??k in the previous SAGE
setting.
To learn the weight parameters ?(R) and ?(Q),
we can approximate the weights using a multiclass
perceptron-style (Collins, 2002) learning method. If
we say that the notation of
?
V (R?) is to marginalize
out all other variables in ? except ?(R), and P (y(R)d )
is the prior for the region prediction task, we can pre-
dict the expected region value y?(R)d of a document d:
y?(R)d ? argmax
y?(R)d
exp
(?
V (R?) log ? + logP (y(R)d )
)
=argmax
y?(R)d
(
exp
(?
V (R?)
(
m+ ?(T )
z(d)n
+ ?(R)?(R)
y(R)d
+ ?(Q)?(Q)
y(Q)d
+ ?(I)
y(R)d ,y
(Q)
d ,z
(d)
n
))
P (y(R)d )
)
If the symbol ? is the hyperprior for the learning
rate and y?(R)d is the true label, the update procedure
for the weights becomes:
?(R
?)
d = ?
(R)
d + ?(y?
(R)
d ? y?
(R)
d )
Similarly, we derive the ?(Q) parameter using the
above formula. It is necessary to normalize the
weights in each EM loop to preserve the sparsity
property of latent variables. The weight update of
?(R) and ?(Q) is bound by the averaged accuracy
of the two classification tasks in the training data,
which is similar to the notion of minimizing empiri-
cal risk (Bahl et al, 1988). Our goal is to choose the
two weight parameters that minimize the empirical
classification error rate on training data when learn-
ing the word posterior probability.
5 Prediction Experiments
We perform three quantitative experiments to evalu-
ate the predictive power of the sparse mixed-effects
model. In these experiments, to predict the region
and time period labels of a given document, we
743
jointly learn the two labels in the SME model, and
choose the pair which maximizes the probability of
the document.
In the first experiment, we compare the prediction
accuracy of our SME model to a widely used dis-
criminative learner in NLP ? the linear kernel sup-
port vector machine (SVM)3. In the second experi-
ment, in addition to the linear kernel SVM, we also
compare our SME model to a state-of-the-art sparse
generative model of text (Eisenstein et al, 2011a),
and vary the size of input vocabulary W exponen-
tially from 29 to the full size of our training vocab-
ulary4. In the third experiment, we examine the ro-
bustness of our model by examining how the number
of topics influences the prediction accuracy when
varying the K from 10 to 50.
Our data consists of 4615 training documents and
625 held-out documents for testing. While individ-
ual judges wrote multiple opinions in our corpus,
no judges overlapped between training and test sets.
When measuring by the majority class in the testing
condition, the chance baseline for the region iden-
tification task is 57.1% and the time identification
task is 32.3%. We use three-fold cross-validation to
infer the learning rate ? and cost C hyperpriors in
the SME and SVM model respectively. We use the
paired student t-test to measure the statistical signif-
icance.
5.1 Quantitative Results
5.1.1 Comparing SME to SVM
We show in this section the predictive power of
our sparse mixed-effects model, comparing to a lin-
ear kernel SVM learner. To compare the two mod-
els in different settings, we first empirically set the
number of topics K in our SME model to be 25, as
this setting was shown to yield a promising result in
a previous study (Eisenstein et al, 2011a) on sparse
topic models. In terms of the size of vocabulary W
for both the SME and SVM learner, we select three
values to represent dense, medium or sparse feature
spaces: W1 = 29, W2 = 212, and the full vocabu-
lary size of W3 = 213.8. Table 1 shows the accuracy
of both models, as well as the relative improvement
(gain) of SME over SVM.
When looking at the experiment results under dif-
ferent settings, we see that the SME model always
outperforms the SVM learner. In the time quar-
tile prediction task, the advantage of SME model
3In our implementation, we use LibSVM (Chang and Lin,
2011).
4To select the vocabulary size W , we rank the vocabulary
by word frequencies in a descending order, and pick the top-W
words.
Method Time Gain Region Gain
SVM (W1) 33.2% ? 69.7% ?
SME (W1) 36.4% 9.6% 71.4% 2.4%
SVM (W2) 35.8% ? 72.3% ?
SME (W2) 40.9% 14.2% 74.0% 2.4%
SVM (W3) 36.1% ? 73.5% ?
SME (W3) 41.9% 16.1% 74.8% 1.8%
Table 1: Compare the accuracy of the linear kernel sup-
port vector machine to our sparse mixed-effects model in
the region and time identification tasks (K = 25). Gain:
the relative improvement of SME over SVM.
is more salient. For example, with a medium den-
sity feature space of 212, SVM obtained an accuracy
of 35.8%, but SME achieved an accuracy of 40.9%,
which is a 14.2% relative improvement (p < 0.001)
over SVM. When the feature space becomes sparser,
the SME obtains an increased relative improvement
(p < 0.001) of 16.1%, using full size of vocabu-
lary. The performance of SVM in the binary region
classification is stronger than in the previous task,
but SME is able to outperform SVM in all three set-
tings, with tightened advantages (p < 0.05 in W2
and p < 0.001 in W3). We hypothesize that it might
because that SVM, as a strong large margin learner,
is a more natural approach in a binary classification
setting, but might not be the best choice in a four-
way or multiclass classification task.
5.1.2 Comparing SME to SAGE
In this experiment, we compare SME with a state-
of-the-art sparse generative model: SAGE (Eisen-
stein et al, 2011a).
Most studies on topic modelling have not been
able to report results when using different sizes of
vocabulary for training. Because of the importance
of interpretability for social science research, the
choice of vocabulary size is critical to ensure un-
derstandable topics. Thus we report our results at
various vocabulary sizes W on SME and SAGE. To
better validate the performance of SME, we also in-
clude the performance of SVM in this experiment,
and fix the number of topics K = 10 for the SME
and SAGE models, which is a different value for the
number of topicsK than the empiricalK we used in
the experiment of Section 5.1.1. Figure 2 and Fig-
ure 3 show the experiment results in both time and
region classification task.
In Figure 2, we evaluate the impacts of W on our
time quartile prediction task. The advantage of the
SME model is very obvious throughout the experi-
ments. Interestingly, when we continue to increase
744
Figure 2: Accuracy on predicting the time quartile vary-
ing the vocabulary size W , while K is fixed to 10.
Figure 3: Accuracy on predicting the region varying the
vocabulary size W , while K is fixed to 10.
the vocabulary size W exponentially and make the
feature space more sparse, SME obtains its best re-
sult at W = 213, where the relative improvement
over SAGE and SVM is 16.8% and 22.9% respec-
tively (p < 0.001 under all comparisons).
Figure 3 shows the impacts of W on the accu-
racy of SAGE and SME in the region identification
task. In this experiment, the results of SME model
are in line with SAGE and SVM when the feature
space is dense. However, when W reaches the full
vocabulary size, we have observed significantly bet-
ter results (p < 0.001 in the comparison to SAGE
and p < 0.05 with SVM). We hypothesize that there
might be two reasons: first, the K parameter is set
to 10 in this experiment, which is much denser than
the experiment setting in Section 5.1.1. Under this
condition, the sparse topic advantage of SME might
be less salient. Secondly, in the two tasks, it is ob-
served that the accuracy of the binary region classi-
fication task is much higher than the four-way task,
thus while the latter benefits significantly from this
joint learning scheme of the SME model, but the for-
mer might not have the equivalent gain5.
5We hypothesize that this problem might be eliminated if
5.1.3 Influence of the number of topics K
Figure 4: Accuracy on predicting the time quartile vary-
ing the number of topics K, while W is fixed to 29.
Figure 5: Accuracy on predicting the region varying the
number of topics K, while W is fixed to 29.
Unlike hierarchical Dirichlet processes (Teh et al,
2006), in parametric Bayesian generative models,
the number of topics K is often set manually, and
can influence the model?s accuracy significantly. In
this experiment, we fix the input vocabulary W to
29, and compare the mixed-effect model with SAGE
in both region and time identification tasks.
Figure 4 shows how the variations of K can in-
fluence the system performance in the time quartile
prediction task. We can see that the sparse mixed-
effects model (SME) reaches its best performance
when the K is 40. After increasing the number of
topics K, we can see SAGE consistently increase
its accuracy, obtaining its best result when K = 30.
When comparing these two models, SME?s best per-
formance outperforms SAGE?s with an absolute im-
provement of 3%, which equals to a relative im-
provement (p < 0.001) of 8.4%. Figure 5 demon-
strates the impacts of K on the predictive power of
SME and SAGE in the region identification task.
the two tasks in SME have similar difficulties and accuracies,
but this needs to be verified in future work.
745
Keywords discovered by the SME model
Prior to 1837 (Q1) pauperis, footprints, American Colonization Society, manumissions, 1797
1838 - 1848 (Q2) indentured, borrowers, orphan?s, 1841, vendee?s, drawer?s, copartners
1849 - 1855 (Q3) Frankfort, negrotrader, 1851, Kentucky Assembly, marshaled, classed
After 1856 (Q4) railroadco, statute, Alabama, steamboats, Waterman?s, mulattoes, man-trap
Free Region (R1) apprenticed, overseer?s, Federal Army, manumitting, Illinois constitution
Slave Region (R2) Alabama, Clay?s Digest, oldest, cotton, reinstatement, sanction, plantation?s
Topic 1 in Q1 R1 imported, comaker, runs, writ?s, remainderman?s, converters, runaway
Topic 1 in Q1 R2 comaker, imported, deceitful, huston, send, bright, remainderman?s
Topic 2 in Q1 R1 descendent, younger, administrator?s, documentary, agreeable, emancipated
Topic 2 in Q1 R2 younger, administrator?s, grandmother?s, plaintiffs, emancipated, learnedly
Topic 3 in Q2 R1 heir-at-law, reconsidered, manumissions, birthplace, mon, mother-in-law
Topic 3 in Q2 R2 heir-at-law, reconsideration, mon, confessions, birthplace, father-in-law?s
Topic 4 in Q2 R1 indentured, apprenticed, deputy collector, stepfather?s, traded, seizes
Topic 4 in Q2 R2 deputy collector, seizes, traded, hiring, stepfather?s, indentured, teaching
Topic 5 in Q4 R1 constitutionality, constitutional, unconstitutionally, Federal Army, violated
Topic 5 in Q4 R2 petition, convictions, criminal court, murdered, constitutionality, man-trap
Table 2: A partial listing of an example for early United States state supreme court opinion keywords generated from
the time quartile ?(Q) , region ?(R) and topic-region-time ?(I) interactive variables in the sparse mixed-effects model.
Except that the two models tie up when K = 10,
SME outperforms SAGE for all subsequent varia-
tions ofK. Similar to the region task, SME achieves
the best result when K is sparser (p < 0.01 when
K = 40 and K = 50).
5.2 Qualitative Analysis
In this section, we qualitatively evaluate the topics
generated vis-a-vis the secondary literature on the
legal and political history of slavery in the United
States. The effectiveness of SME could depend not
just on its predictive power, but also in its ability
to generate topics that will be useful to historians
of the period. Supreme court opinions on slavery
are of significant interest for American political his-
tory. The conflict over slave property rights was at
the heart of the ?cold war? (Wright, 2006) between
North and South leading up to the U.S. Civil War.
The historical importance of this conflict between
Northern and Southern legal institutions is one of the
motivations for choosing our data domain.
We conduct qualitative analyses on the top-ranked
keywords6 that are associated with different geo-
graphical locations and different temporal frames,
generated by our SME model. In our analysis, for
6Keywords were ranked by word posterior probabilities.
each interaction of topic, region, and time period, a
list of the most salient vocabulary words was gener-
ated. These words were then analyzed in the context
of existing historical literature on the shift in atti-
tudes and views over time and across regions. Table
2 shows an example of relevant keywords and topics.
This difference between Northern and Southern
opinion can be seen in some of the topics generated
by the SME. Topic 1 deals with transfers of human
beings as slave property. The keyword ?remainder-
man? designates a person who inherits or is entitled
to inherit property upon the termination of an es-
tate, typically after the death of a property owner,
and appears in Northern and Southern cases. How-
ever, in Topic 1 ?runaway? appears as a keyword in
decisions from free states but not in decisions from
slave states. The fact that ?runaway? is not a top
word in the same topic in the Southern legal opin-
ions is consistent with a spatial (geolocational) di-
vision in which the property claims of slave owners
over runaways were not heavily contested in South-
ern courts.
Topic 3 concerns bequests, as indicated by the
term ?heir-at-law?, but again the term ?manumis-
sions?, ceases to show up in the slave states after the
first time quartile, perhaps reflecting the hostility to
746
manumissions that southern courts exhibited as the
conflict over slavery deepened.
Topic 4 concerns indentures and apprentices. In-
terestingly, the terms indentures and apprenticeships
are more prominent in the non-slave states, reflect-
ing the fact that apprenticeships and indentures were
used in many border states as a substitute for slavery,
and these were often governed by continued usage of
Master and Servant law (Orren, 1992).
Topic 5 shows the constitutional crisis in the
states. In particular, the anti-slavery state courts are
prone to use the term ?unconstitutional? much more
often than the slave states. The word ?man-trap?, a
term used to refer to states where free blacks could
be kidnapped purpose of enslaving them. The fugi-
tive slave conflicts of the mid-19th century that led
to the civil war were precisely about this aversion
of the northern states to having to return runaway
slaves to the Southern states.
Besides these subjective observations about the
historical significance of the SME topics, we also
conduct a more formal analysis comparing the SME
classification to that conducted by a legal histo-
rian. Wahl (2002) analyses and classifies by hand
10989 slave cases in the US South into 6 categories:
?Hires?, ?Sales?, ?Transfers?, ?Common Carrier?,
?Black Rights? and ?Other?. An example of ?Hires?
is Topic 4. Topics 1, 2, and 3 concern ?Transfers? of
slave property between inheritors, descendants and
heirs-at-law. Topic 5 would be classified as ?Other?.
We take each of our 25 modelled topics and clas-
sify them along Wahl?s categories, using ?Other?
when a classification could not be obtained. The
classifications are quite transparent in virtually all
cases, as certain words (such as ?employer? or ?be-
quest?) clearly designate certain categories (respec-
tively, such as ?Hires? or ?Transfers?). We then cal-
culate the probability of each of Wahl?s categories in
Region 2. We then compare these to the relative fre-
quencies of Wahl?s categorization in the states that
overlap with our Region 2 in Figure 6 and do a ?2
test for goodness of fit, which allows us to reject dif-
ference at 0.1% confidence.
The SME model thus delivers topics that, at a first
pass, are consistent with the history of the period
as well as previous work by historians, showing the
qualitative benefits of the model. We plan to conduct
more vertical and temporal analyses using SME in
the future.
6 Conclusion and Future Work
In this work, we propose a sparse mixed-effects
model for historical analysis of text. This model is
built on the state-of-the-art in latent variable mod-
Figure 6: Comparison with Wahl (2002) classification.
elling and extends that model to a setting where
metadata is available for analysis. We jointly model
those observed labels as well as unsupervised topic
modelling. In our experiments, we have shown that
the resulting model jointly predicts the region and
the time of a given court document. Across vocab-
ulary sizes and number of topics, we have achieved
better system accuracy than state-of-the-art genera-
tive and discriminative models of text. Our quantita-
tive analysis shows that early US state supreme court
opinions are predictable, and contains distinct views
towards slave-related topics, and the shifts among
opinions depending on different periods of time. In
addition, our model has been shown to be effective
for qualitative analysis of historical data, revealing
patterns that are consistent with the history of the
period.
This approach to modelling text is not limited
to the legal domain. A key aspect of future work
will be to extend the Sparse Mixed-Effects paradigm
to other problems within the social sciences where
metadata is available but qualitative analysis at a
large scale is difficult or impossible. In addition
to historical documents, this can include humani-
ties texts, which are often sorely lacking in empir-
ical justifications, and analysis of online communi-
ties, which are often rife with available metadata but
produce content far faster than it can be analyzed by
experts.
Acknowledgments
We thank Jacob Eisenstein, Noah Smith, and anony-
mous reviewers for valuable suggestions. William
Yang Wang is supported by the R. K. Mellon Presi-
dential Fellowship.
747
References
Lalit R. Bahl, Peter F. Brown., Peter V. de Souza, and
Robert L. Mercer. 1988. A new algorithm for the
estimation of hidden Markov model parameters. In
IEEE Inernational Conference on Acoustics, Speech
and Signal Processing, ICASSP, pages 493?496.
Ryan S.J.D. Baker and Kalina Yacef. 2009. The state of
educational data mining in 2009: a review and future
visions. In Journal of Educational Data Mining, pages
3?17.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learn-
ing Research (JMLR), pages 993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Transac-
tions on Intelligent System Technologies, pages 1?27.
Jake Chen and Stefano Lombardi. 2010. Biological data
mining. Chapman and Hall/CRC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), pages 1?8.
Agata Katarzyna Cybulska and Piek Vossen. 2011. His-
torical event extraction from text. In Proceedings of
the 5th ACL-HLT Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Humani-
ties, pages 39?43.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2010), pages 1277?
1287.
Jacob Eisenstein, Amr Ahmed, and Eric. Xing. 2011a.
Sparse additive generative models of text. Proceed-
ings of the 28th International Conference on Machine
Learning (ICML 2011), pages 1041?1048.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011b. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL HLT 2011),
pages 1365?1374.
Annette Gotscharek, Andreas Neumann, Ulrich Reffle,
Christoph Ringlstetter, and Klaus U. Schulz. 2009.
Enabling information retrieval on historical document
collections: the role of matching procedures and spe-
cial lexica. In Proceedings of The Third Workshop
on Analytics for Noisy Unstructured Text Data (AND
2009), pages 69?76.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011), pages 552?561.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence (UAI 1999), pages 289?296.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures.
Kenneth Lange and Janet S. Sinsheimer. 1993. Nor-
mal/independent distributions and their applications in
robust regression.
Dong Nguyen and Carolyn Penstein Rose?. 2011. Lan-
guage use as a reflection of socialization in online
communities. In Workshop on Language in Social Me-
dia at ACL.
Dong Nguyen, Elijah Mayfield, and Carolyn P. Rose?.
2010. An analysis of perspectives in interactive set-
tings. In Proceedings of the First Workshop on Social
Media Analytics (SOMA 2010), pages 44?52.
Karen Orren. 1992. Belated feudalism: labor, the law,
and liberal development in the united states.
Eva Pettersson and Joakim Nivre. 2011. Automatic verb
extraction from historical swedish texts. In Proceed-
ings of the 5th ACL-HLT Workshop on Language Tech-
nology for Cultural Heritage, Social Sciences, and Hu-
manities, pages 87?95.
William D. Popkin. 2007. Evolution of the judicial opin-
ion: institutional and individual styles. NYU Press.
Kumar Shubhankar, Aditya Pratap Singh, and Vikram
Pudi. 2011. An efficient algorithm for topic ranking
and modeling topic evolution. In Proceedings of Inter-
national Conference on Database and Expert Systems
Applications.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, pages 1566?1581.
Jenny Bourne Wahl. 2002. The Bondsman?s Burden: An
Economic Analysis of the Common Law of Southern
Slavery. Cambridge University Press.
William Yang Wang and Kathleen McKeown. 2010. ?got
you!?: automatic vandalism detection in wikipedia
with web-based shallow syntactic-semantic modeling.
In Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
1146?1154.
William Yang Wang, Kapil Thadani, and Kathleen McK-
eown. 2011. Identifyinge event descriptions using co-
training with online news summaries. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 281?291.
748
Gavin Wright. 2006. Slavery and american economic
development. Walter Lynwood Fleming Lectures in
Southern History.
Tze-I Yang, Andrew Torget, and Rada Mihalcea. 2011.
Topic modeling on historical newspapers. In Proceed-
ings of the 5th ACL-HLT Workshop on Language Tech-
nology for Cultural Heritage, Social Sciences, and Hu-
manities, pages 96?104.
749
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104?113,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Rare Social Phenomena in Conversation:
Empowerment Detection in Support Group Chatrooms
Elijah Mayfield, David Adamson, and Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, PA 15213
{emayfiel, dadamson, cprose}@cs.cmu.edu
Abstract
Automated annotation of social behavior
in conversation is necessary for large-scale
analysis of real-world conversational data.
Important behavioral categories, though,
are often sparse and often appear only
in specific subsections of a conversation.
This makes supervised machine learning
difficult, through a combination of noisy
features and unbalanced class distribu-
tions. We propose within-instance con-
tent selection, using cue features to selec-
tively suppress sections of text and bias-
ing the remaining representation towards
minority classes. We show the effective-
ness of this technique in automated anno-
tation of empowerment language in online
support group chatrooms. Our technique
is significantly more accurate than multi-
ple baselines, especially when prioritizing
high precision.
1 Introduction
Quantitative social science research has experi-
enced a recent expansion, out of controlled set-
tings and into natural environments. With this
influx of interest comes new methodology, and
the inevitable question arises of how to move
towards testable hypotheses, using these uncon-
trolled sources of data as scientific lenses into the
real world.
The study of conversational transcripts is a key
domain in this new frontier. There are certain
social and behavioral phenomena in conversation
that cannot be easily identified through question-
naire data, self-reported surveys, or easily ex-
tracted user metadata. Examples of these social
phenomena in conversation include overt displays
of power (Prabhakaran et al, 2012) or indicators
of rapport and relationship building (Wang et al,
2012). Manually annotating these social phenom-
ena cannot scale to large data, so researchers turn
to automated annotation of transcripts (Rose? et al,
2008). While machine learning is highly effec-
tive for annotation tasks with relatively balanced
labels, such as sentiment analysis (Pang and Lee,
2004), more complex social functions are often
rarer. This leads to unbalanced class label distri-
butions and a much more difficult machine learn-
ing task. Moreover, features indicative of rare so-
cial annotations tend to be drowned out in favor of
features biased towards the majority class. The net
effect is that classification algorithms tend to bias
towards the majority class, giving low accuracy for
rare class detection.
Automated annotation of social phenomena also
brings opportunities for real-world applications.
For example, real-time annotation of conversation
can power adaptive intervention in collaborative
learning settings (Rummel et al, 2008; Adamson
and Rose?, 2012). However, with the considerable
power of automation comes great responsibility. It
is critical to avoid intervening in the case of er-
roneous annotations, as providing unnecessary or
inappropriate support in such a setting has been
shown to be harmful to group performance and so-
cial cohesion (Dillenbourg, 2002; Stahl, 2012).
We propose adaptations to existing machine
learning algorithms which improve recognition of
rare annotations in conversational text data. Our
primary contribution comes in the form of within-
instance content selection. We develop a novel al-
gorithm based on textual cues, suppressing infor-
mation which is likely to be irrelevant to an in-
stance?s class label. This allows features which
predict minority classes to gain prominence, help-
ing to sidestep the frequency of common features
pointing to a majority class label.
Additionally, we propose modifications to ex-
isting algorithms. First, we identify a new appli-
cation of logistic model trees to text data. Next,
104
we define a modification of confidence-based en-
semble voting which encourages minority class la-
beling. Using these techniques, we demonstrate a
significant improvement in classifier performance
when recognizing the language of empowerment
in support group chatrooms, a critical application
area for researchers studying conversational inter-
actions in healthcare (Uden-Kraan et al, 2009).
The remainder of this paper is structured as fol-
lows. We introduce the domain of empowerment
in support contexts, along with previous studies on
the challenges that these annotations (and similar
others) bring to machine learning. We introduce
our new technique for improving the ability to au-
tomate this annotation, along with other optimiza-
tions to the machine learning workflow which are
tailored to this skewed class balance. We present
experimental results showing that our method is
effective, and provide a detailed analysis of the be-
havior of our model and the features it uses most.
We conclude with a discussion of particularly use-
ful applications of this work.
2 Background
We ground this paper?s discussion of machine
learning with a real problem, turning to the an-
notation of empowerment language in chat1. The
concept of empowerment, while a prolific area
of research, lacks a broad definition across pro-
fessionals, but broadly relates to ?the power to
act efficaciously to bring about desired results?
(Boehm and Staples, 2002) and ?experiencing per-
sonal growth as a result of developing skills and
abilities along with a more positive self-definition?
(Staples, 1990). Participants in online support
groups feel increased empowerment (Uden-Kraan
et al, 2009; Barak et al, 2008). Quantita-
tive studies have shown the effect of empower-
ment through statistical methods such as structural
equation modeling (Vauth et al, 2007), as have
qualitative methods such as deductive transcript
analysis (Owen et al, 2008) and interview studies
(Wahlin et al, 2006).
The transition between these styles of research
has been gradual. Pioneering work has demon-
strated the ability to distinguish empowerment lan-
guage in written texts, including prompted writ-
ing samples (Pennebaker and Seagal, 1999), nar-
1Definitions of empowerment are closely related to the
notion of self-efficacy (Bandura, 1997). For simplicity, we
use the former term exclusively in this paper.
Table 1: Empowerment label distribution in our
corpus.
Annotation Label # %
Self-Empowerment NA 1522 79.3
POS 202 10.5
NEG 196 10.2
Other-Empowerment NA 1560 81.3
POS 217 11.3
NEG 143 7.4
ratives in online forums (Hoybye et al, 2005), and
some preliminary analysis of synchronous discus-
sion (Ogura et al, 2008; Mayfield et al, 2012b).
These transitional works have used limited analy-
sis methodology; in the absence of sophisticated
natural language processing, their conclusions of-
ten rely on coarse measures, such as word counts
and proportions of annotations in a text.
Users, of course, do not express empowerment
in every thread in which they participate, which
leads to a challenge for machine learning. Threads
often focus on a single user?s experiences, in
which most participants in a chat are merely com-
mentators, if they participate at all, matching pre-
vious research on shifts in speaker salience over
time (Hassan et al, 2008). This leads to many
user threads which are annotated as not applicable
(N/A). We move to our proposed approach with
these skewed distributions in mind.
3 Data
Our data consists of a set of chatroom conversa-
tion transcripts from the Cancer Support Commu-
nity2. Each 90-minute conversation took place in
the context of a weekly meeting in a real-time chat,
with up to 6 participants in addition to a profes-
sional therapist facilitating the discussion. In to-
tal, 2,206 conversations were collected from 2007-
2011. This data offers potentially rich insight into
coping and social support; however, annotating
such a dataset by hand would be prohibitively ex-
pensive, even when it is already transcribed.
Twenty-one of these conversations have been
annotated, as originally described and analyzed
in (Mayfield et al, 2012b)3. This data was dis-
entangled into threads based on common themes
or topics, as in prior work (Elsner and Charniak,
2www.cancersupportcommunity.org
3All annotations were found to be adequately reliable be-
tween humans, with thread disentanglement f = 0.75 and
empowerment annotation ? > 0.7.
105
Figure 1: An example mapping from a single thread?s chat lines (left) to the per-user, per-thread instances
used for classification in this paper (right), with example annotations for self-empowerment indicated.
2010; Adams and Martel, 2010). A novel per-
user, per-thread annotation was then employed
for empowerment annotation, following a coding
manual based on definitions like those in Section
2. Each user was assigned a label of positive
or negative empowerment if they exhibited such
emotions, or was left blank if they did not do so
within the context of that thread. This annotation
was performed both for their self-empowerment
as well as their attitude towards others? situations
(other-empowerment). An example of this annota-
tion for self-empowerment is presented in Figure
1 and the distribution of labels is given in Table 1.
Most previous annotation tasks attempt to an-
notate on a per-utterance basis, such as dialogue
act tagging (Popescu-Belis, 2008), or on arbitrary
spans of text, such as in the MPQA subjectivity
corpus (Wiebe et al, 2005). However, for our task,
a per-user, per-thread annotation is more appropri-
ate, because empowerment is often indicated best
through narrative (Hoybye et al, 2005). Human
annotators are instructed to take this context into
account when annotating (Mayfield et al, 2012b).
It would therefore be nonsensical to annotate indi-
vidual lines as ?embodying? empowerment. Simi-
lar arguments have been made for sentiment, espe-
cially as the field moves towards aspect-oriented
sentiment (Breck et al, 2007). Assigning labels
based on thread boundaries allows for context to
be meaningfully taken into account, without cross-
ing topic boundaries.
However, this granularity comes with a price:
the distribution of class values in these instances
is highly skewed. In our data, the vast majority of
users? threads are marked as not applicable to em-
powerment. Perhaps more inconveniently, while
taking context into account is important for reli-
able annotation, it leads to extraneous information
in many cases. Many threads can have multiple
lines of contributions that are topically related to
an expression of empowerment (and thus belong
in the same thread), but which do not indicate any
empowerment themselves. This exacerbates the
likelihood of instances being classified as N/A.
We choose to take advantage of these attributes
of threads. We know from research in discourse
analysis that many sections of conversations are
formulaic and rote, like introductions and greet-
ings (Schegloff, 1968). We additionally know that
polarity often shifts in dialogue through the use
of discourse connectives such as conjunctions and
transitional phrases. These issues have been ad-
dressed in work in the language technologies com-
munity, most notably through the Penn Discourse
Treebank (Prasad et al, 2008); however, their ap-
plications to noisier synchronous conversation has
beenrare in computational linguistics.
With these linguistic insights in mind, we ex-
amine how we can make best use of them for
machine learning performance. While techniques
for predicting rare events (Weiss and Hirsh, 1998)
and compensating for class imbalance (Frank and
106
Bouckaert, 2006), these approaches generally fo-
cus on statistical properties of large class sets with-
out taking the nature of their datasets into account.
In the next section, we propose a new algorithm
which takes advantage specifically of the linguis-
tic phenomena in the conversation-based data that
we study for empowerment detection. As such,
our algorithm is highly suited to this data and task,
with the necessary tradeoff in uncertain generality
to new domains with unrelated data.
4 Cue Discovery for Content Selection
Our algorithm performs content selection by
learning a set of cue features. Each of these fea-
tures indicates some linguistic function within the
discourse which should downplay the importance
of features either before or after that discourse
marker. Our algorithm allows us to evaluate the
impact of rules against a baseline, and to itera-
tively judge each rule atop the changes made by
previous rules.
This algorithm fits into existing language tech-
nologies research which has attempted to partition
documents into sections which are more or less
relevant for classification. Many researchers have
attempted to make use of cue phrases (Hirschberg
and Litman, 1993), especially for segmentation
both in prose (Hearst, 1997) and conversation
(Galley et al, 2003). The approach of content se-
lection, meanwhile, has been explored for senti-
ment analysis (Pang and Lee, 2004), where indi-
vidual sentences may be less subjective and there-
fore less relevant to the sentiment classification
task. It is also similar conceptually to content
selection algorithms that have been used for text
summarization (Teufel and Moens, 2002) and text
generation (Sauper and Barzilay, 2009), both of
which rely on finding highly-relevant passages
within source texts.
Our work is distinct from these approaches.
While we have coarse-grained annotations of em-
powerment, there is no direct annotation of what
makes a good cue for content selection. With
our cues, we hope to take advantage of shallow
discourse structure in conversation, such as con-
trastive markers, making use of implicit structure
in the conversational domain.
4.1 Notation
Before describing extensions to the baseline lo-
gistic regression model, we define notation. Our
data is arranged hierarchically. We assume that
we have a collection of d training documents Tr =
{D1 . . . Dd}, each of which contains many train-
ing instances (in our task, an instance consists of
all lines of chat from one user in one thread). Our
total set of n instances I thus consists of instances
{I1, I2, . . . In}. Each document contains lines of
chat L and each instance Ii is comprised of some
subset of those lines, Li ? L.
Our feature space X = {x1, x2, . . . xm} con-
sists of m unigram features representing the ob-
served vocabulary used in our corpus. Each in-
stance is associated with a feature vector x? con-
taining values for each x ? X, and each feature
x that is present in the i-th instance maintains a
?memory? of the lines in which it appeared in that
instance, Lix, where Lix ? Li. Our potential out-
put labels consist of Y = {NA,NEG,POS},
though this generalizes to any nominal classifica-
tion task. Each instance I is associated with ex-
actly one y ? Y for self-empowerment and one
for other-empowerment; these two labels do not
interact and our tasks are treated as independent
in this paper4. We define classifiers as functions
f(x?? y ? Y); in practice, we use logistic regres-
sion via LibLINEAR (Fan et al, 2008).
We define a content selection rule as a pairing
r = ?c, t? between a cue feature c ? X and a se-
lection function t ? T . We created a list of possi-
ble selection functions, given a cue c, maximizing
for generality while being expressive. These are
illustrated in Figure 2 and described below:
? Ignore Local Future (A): Ignore all features
from the two lines after each occurrence of c.
? Ignore All Future (B): Ignore all features
occurring after the first occurrence of c.
? Ignore Local History (C): Ignore all features
in the two lines preceding each occurrence of
c.
? Ignore All History (D): Ignore all features
occurring only before the last occurrence of
c.
We define an ensemble member E = ?R, fR? -
the ordered list of learned content selection rules
R = [r1, r2, . . . ] and a classifier fC trained on in-
stances transformed by those rules. Our final out-
4Future work may examine the interaction of jointly an-
notating multiple sparse social phenomena.
107
Figure 2: Effects of content selection rules, based
on a cue feature (ovals) observed at lines m and n.
put of a trained model is a set of ensemble mem-
bers {E1, . . . , Ek}.
4.2 Algorithm
Our ensemble learning follows the paradigm
of cross-validated committees (Parmanto et al,
1996), where k ensemble members are trained by
subdividing our training data into k subfolds. For
each ensemble classifier, cue rulesR are generated
on k ? 1 subfolds (Trk) and evaluated on the re-
maining subfold (Tek). In practice, with 21 train-
ing documents, 7-fold cross-validation, and k = 3
ensemble members, each generation set consists
of 12 documents? instances, while each evaluation
set contains instances from 6 documents.
Our full algorithm is presented in Algorithm
1, and is broken into component parts for clar-
ity. Algorithm 2 begins by measuring the base-
line classifier?s ability to recognize minority-class
labels. After training on Trk, we measure the
average probability assigned to the correct label
of instances in Tek, but only for instances whose
correct labels are minority classes (remember, be-
cause both Trk and Tek are drawn from the over-
all Tr, we have access to true class labels). We
choose this subset of only minority instances, as
we are not interested in optimizing to the majority
class.
We next enumerate all rules that we wish to
judge. To keep this problem tractable, we ignore
features which do not occur in at least 5% of train-
ing instances. For the remaining features, we cre-
ate a candidate rule for each possible pairing of
features and selection functions. For each of these
candidates, we test its utility by selecting content
as if it were an actual rule, then building a new
classifier (trained on the generation set) using in-
stances that have been altered in that way. In the
evaluation set, we measure the difference in prob-
ability of minority class labels being assigned cor-
rectly between the baseline and this altered space.
This measure of an individual rule?s impact is de-
scribed in Algorithm 3.
Once we have evaluated every possible rule
once, we select the top-ranked rule and ap-
ply it to the feature set. We then iteratively
progress through our now-ranked list of candi-
dates, each time treating the newly filtered dataset
as our new baseline. We search only top can-
didates for efficiency, following the fixed-width
search methodology for feature selection in very
high-dimensionality feature spaces (Gu?tlein et al,
2009). Each ensemble classifier is finally retrained
on all training data, after applying the correspond-
ing content selection rules to that data.
5 Prediction
Our prediction algorithm begins with a stan-
dard implementation of cross-validated commit-
tees (Parmanto et al, 1996), whose results are
aggregated with a confidence voting method in-
tended to favor rare labels (Erp et al, 2002).
Cross-validated committees are an ensemble tech-
nique used to subsample training data to produce
multiple hypotheses for classification. Each clas-
sifier produced by our cue-based transformation
is trained on a subset of our training data. Each
makes predictions on all test set instances, pro-
ducing a distribution of confidence across possi-
ble labels. These values serve as inputs to a voting
method to produce a final label for each instance.
Compared to other ensemble methods, cross-
validated committees as described above are a
good fit for our task, because of its unique unit of
analysis. As thread-level analysis is the set of in-
dividual participants? turns in a conversation, we
risk overfitting if we sample from the same con-
versations for the training and testing sets. In con-
trast to standard bagging, hard sampling bound-
aries never train and test on instances drawn from
the same conversation.
To aggregate the votes from members of this en-
semble into a final prediction, we employ a variant
on Selfridge?s Pandemonium (Selfridge, 1958).
If a minority label is selected as the highest-
confidence value in any classifier in our ensem-
ble, it is selected. The majority label, by contrast,
is only selected if it is the most likely prediction
by all classifiers in our ensemble. Thus consen-
sus is required to elect the majority class, and the
strongest minority candidate is elected otherwise.
108
In : generation set Trk, evaluation set Tek
Out: ensemble committee {E1 . . . Ek}
for i = 1 to k do
Rfinal ? [ ];
Xfreq ? {x ? X | freq(x) ? Trk >
5%};
R? Xfreq ? T ;
R? ? R;
repeat
Pbase ? EvaluateClassifier(Trk,Tek);
EvaluateRules(Pbase,Trk,Tek, R?);
Trk,Tek ? ApplyRule(R?[0]);
R? R?R?[0];
?? score(R?[0]);
Rfinal ? Rfinal +R?[0];
R? ? R[0 . . . 50];
until ? < threshold;
Trfinal ? Trk ? Tek;
foreach r ? Rfinal do
Trfinal ? ApplyRule(Trfinal, r);
end
Train f(x?? y) on Trfinal;
end
Algorithm 1: LearnSelectionCues()
This approach is designed to bias the prediction
of our machine learning algorithms in favor of mi-
nority classes in a coherent manner. If there is a
plausible model that has been trained which rec-
ognizes the possibility of a rare label, it is used;
the prediction only reverts to the majority class
when no plausible minority label could be chosen.
As validation of this technique, we compare our
?minority pandemonium? approach against both
typical pandemonium and standard sum-rule con-
fidence voting (Erp et al, 2002).
5.1 Logistic Model Stumps
One characteristic of highly skewed data is that,
while minority labels may be expressed in a num-
ber of different surface forms, there are many ob-
vious cases in which they do not apply. These
cases can actually be harmful to classification of
borderline cases. Features that could be given high
weight in marginal cases may be undervalued in
?low-hanging fruit? easy cases. To remove those
obvious instances, a very simple screening heuris-
tic is often enough to eliminate frequent pheno-
types of instances where the rare annotation is
not present. Prior work has sometimes screened
training data through obvious heuristic rules, espe-
In : generation set Trk, evaluation set Tek
Out: minority class probability average Pbase
Train f(x?? y) on Trk;
Temink ? {Instance I ? Tek | yI 6= ?NA?}
;
Pbase ? 0 ;
foreach Instance I ? Temink do
Pbase ? Pbase + P (f(x?I) = yI)
end
Pbase = Pbase/size(Temink )Algorithm 2: EvaluateClassifier()
In : Trk, Tek, rules R, base probability Pbase
Out: R sorted on each rule?s improvement
score
foreach Rule r ? R do
Tr?k,Te?k ? ApplyRule(Trk,Tek, r);
Palter ? EvaluateClassifier(Tr?k,Te?k);
score(r)? Palter ? Pbase;
end
Sort R on score(r) from high to low;
Algorithm 3: EvaluateRules()
cially in speech recognition; for instance, training
speech recognition for words followed by a pause
separately from words followed by another word
(Franco et al, 2010), or training separate models
based on gender (Jiang et al, 1999).
We achieve this instance screening by learn-
ing logistic model tree stumps (Landwehr et al,
2005), which allow us to quickly partition data if
there is a particularly easy heuristic that can be
learned to eliminate a large number of majority-
class labels. One challenge of this approach is
our underlying unigram feature space - tree-based
algorithms are generally poor classifiers for the
high-dimensionality, low-information features in a
lexical feature space (Han et al, 2001). To com-
pensate, we employ a smaller, denser set of binary
features for tree stump screening: instance length
thresholds and LIWC category membership.
First, we define a set of features that split based
on the number of lines an instance contains, from
1 to 10 (only a tiny fraction of instances are more
than 10 lines long). For example, a feature split-
ting on instances with lines ? 2 would be true
for one- and two-line instances, and false for all
others. Second, we define a feature for each cate-
gory in the Linguistic Inquiry and Word Count dic-
tionary (Tausczik and Pennebaker, 2010) - these
broad classes of words allow for more balanced
109
Figure 3: Precision/recall curves for algorithms.
After 50% recall all models converge and there are
no significant differences in performance.
splits than would unigrams alone. Each category?s
feature is true if any word in that category was
used at least once in that instance.
We exhaustively sweep this feature space, and
report the most successful stump rules for each an-
notation task. In our other experiments, we report
results with and without the best rule for this pre-
processing step; we also measure its impact alone.
6 Experimental Results
All experiments were performed using LightSIDE
(Mayfield and Rose?, 2013). We use a binary uni-
gram feature space, and we perform 7-fold cross-
validation. Instances from the same chat transcript
never occur in both train and testing folds. Fur-
thermore, we assume that threads have been dis-
entangled already, and our experiments use gold
standard thread structure. While this is not a triv-
ial assumption, prior work has shown thread dis-
entanglement to be manageable (Mayfield et al,
2012a); we consider it an acceptable simplify-
ing assumption for our experiments. We compare
our methods against baselines including a majority
baseline, a baseline logistic regression classifier
with L2 regularized features, and two common en-
semble methods, AdaBoost (Freund and Schapire,
1996) and bagging (Breiman, 1996) with logistic
regression base classifiers5.
Table 2 presents the best-performing result
from each classification method. For self-
empowerment recognition, all methods that we
introduce are significant improvements in ?, the
5These methods usually use weak, unstable base classi-
fiers; however, in our experiments, those performed poorly.
Table 2: Performance for baselines, common en-
semble algorithms, and proposed methods. Statis-
tically significant improvements over baseline are
marked (p < .01, ?; p < .05, *; p < 0.1, +).
Self Other
Method % ? % ?
Majority 79.3 .000 81.3 .000
LR Baseline 81.0 .367 81.0 .270
LR + Boosting 78.1 .325 78.5 .275
LR + Bagging 81.2 .352 81.9 .265
LR + Committee 81.0 .367 81.0 .270
Learned Stumps 81.8* .385? 81.7 .293+
Content Selection 80.9 .389? 80.7 .282
Stumps+Selection 81.3 .406? 79.4 .254
Table 3: Performance of content-selection
wrapped learners, for minority voting and two
baseline voting methods.
Self Other
Method % ? % ?
Pandemonium 80.3 .283 81.4 .239
Averaged 80.6 .304 81.6 .251
Minority Voting 80.9? .389? 80.7 .282
measurement of agreement over chance, compared
to all baselines. While accuracy remains stable,
this is due to predictions shifting away from the
majority class and towards minority classes. Our
combined model using both logistic model tree
stumps and content selection is significantly better
than either alone (p < .01). To compare the mi-
nority pandemonium voting method against base-
lines of simple pandemonium and summed confi-
dence voting, Table 3 presents the results of con-
tent selection wrappers with each voting method.
Minority voting is more effective compared to
standard confidence voting, improving ? while
modestly reducing accuracy; this is typical of a
shift towards minority class predictions.
7 Discussion
These results show promise for our techniques,
which are able to distinguish features of rare la-
bels, previously awash in a sea of irrelevance. Fig-
ure 3 shows the impact of our rules as we tune
to different levels of recall, with a large boost in
precision when recall is not important; our model
converges with the baseline for high-recall, low-
precision tuning. This suggests that our method is
particularly suitable for tasks where confident la-
110
Table 4: Cue rules commonly selected by the algo-
rithm. Average improvement over the LR baseline
is also shown.
Self-Empowerment
Cue Transformation ?%
and,but Ignore Local Future +5.0
have Ignore All History +4.3
! Ignore All History +4.2
me,my Ignore All History +3.4
Other-Empowerment
Cue Transformation ?%
and,but Ignore Local Future +5.5
you Ignore Local History +5.2
?s Ignore Local History +4.1
that Ignore Local History +3.9
beling of a few instances is more important than
labeling as many instances as possible. This is
common when tasks have a high cost or carry high
risk (for instance, providing real-time conversa-
tional supports with an agent, where inappropriate
intervention could be disruptive). Other low-recall
applications include exploration large corpora for
exemplar instances, where the most confident pre-
dictions for a given label should be presented first
for analyst use. In the rest of this section, we
examine notable within-instance and per-instance
rules selected by our methods. These rules are
summarized in Tables 4 and 5.
For both self- and other-empowerment, we find
pronoun rules that match the task (first-person and
second-person pronouns for self-Empowerment
and other-Empowerment respectively). In both
tasks, we find cue rules that suppress the context
preceding personal pronouns. These, as well as
the possessive suffix ?s, echo the per-instance ef-
fect of the Self and You splits, anticipating that
what follows such a personal reference is likely to
bear an evaluation of empowerment. Exclamation
marks may indicate strong emotion - we find many
instances where what precedes a line with an ex-
clamation is more objective, and what follows in-
cludes an assessment. Conjunctions but and and
are selected as cue rules suppressing the two lines
that follow the occurrence - suggesting, as sus-
pected, that connective discourse markers play a
role in indicating empowerment (Fraser, 1999).
The best-performing stump splits for the Self-
Empowerment annotation are Line Length ? 1
and the LIWC word-categories Article, Swear, and
Table 5: Best decision rules for logistic model
stumps. Significant improvement (p < 0.05) in-
dicated with *.
Self-Empowerment
Split Rule ? ?? % ?%
Split ? 1 * 0.385 +.018 81.8 +0.8
LIWC-Article 0.379 +.012 81.6 +0.6
LIWC-Swear * 0.376 +.009 81.4 +0.4
LIWC-Self * 0.376 +.009 81.5 +0.5
Other-Empowerment
Split Rule ? ?? % ?%
LIWC-You 0.293 +.023 81.7 +0.7
LIWC-Eating * 0.283 +.013 81.6 +0.6
LIWC-Negate * 0.282 +.012 82.3 +1.3
LIWC-Present 0.281 +.011 81.6 +0.6
Self. The split on line length corresponds to the
observation that longer instances provide greater
opportunity for personal narrative self-assessment
to occur (95% of single-line instances are labeled
NA). The Article category may serve as a proxy for
content length - article-less instances in our corpus
include one-line social greetings and exchanges
of contact information. Swear words may be a
cue for awareness of self-empowerment - a recent
study of women coping with illness reported that
swearing in the presence of others, but not alone,
was related to potentially harmful outcomes (Rob-
bins et al, 2011). Among other- oriented split
rules, Eating stands out as non-obvious, although
medical literature has suggested a link between
dietary behavior and empowerment attitudes in a
study of women with cancer (Pinto et al, 2002).
8 Conclusion
We have demonstrated an algorithm for improv-
ing automated classification accuracy on highly
skewed tasks for conversational data. This algo-
rithm, particularly its focus on content selection, is
rooted in the structural format of our data, which
can generalize to many tasks involving conversa-
tional data. Our experiments show that this model
significantly improves machine learning perfor-
mance. Our algorithm is taking advantage of
structural facets of discourse markers, lending ba-
sic sociolinguistic validity to its behavior. Though
we have treated each of these rarely-occurring la-
bels as independent thus far, in practice we know
that this is not the case. Joint prediction of labels
through structured modeling is an obvious next
111
step for improving classification accuracy.
This is an important step towards large-scale
analysis of the impact of support groups on pa-
tients and caregivers. Our method can be used to
confidently highlight occurrences of rare labels in
large data sets. This has real-world implications
for professional intervention in social conversa-
tional domains, especially in scenarios where such
an intervention is likely to be associated with a
high cost or high risk. With the construction of
more accurate classifiers, we open the possibility
of automating annotation on large conversational
datasets, enabling new directions for researchers
with domain expertise.
Acknowledgments
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485.
References
Paige Adams and Craig Martel. 2010. Conversational
thread extraction and topic detection in text-based
chat. In Semantic Computing.
David Adamson and Carolyn Penstein Rose?. 2012.
Coordinating multi-dimensional support in collabo-
rative conversational agents. In Proceedings of In-
telligent Tutoring Systems.
Albert Bandura. 1997. Self-Efficacy: The Exercise of
Control.
Azy Barak, Meyran Boniel-Nissim, and John Suler.
2008. Fostering empowerment in online support
groups. Computers in Human Behavior.
A Boehm and L H Staples. 2002. The functions of the
social worker in empowering: The voices of con-
sumers and professionals. Social Work.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Pierre Dillenbourg. 2002. Over-scripting cscl: The
risks of blending collaborative learning with instruc-
tional design. Three worlds of CSCL. Can we sup-
port CSCL?
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Merijn Van Erp, Louis Vuurpijl, and Lambert
Schomaker. 2002. An overview and comparison of
voting methods for pattern recognition. In Frontiers
in Handwriting Recognition. IEEE.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Horacio Franco, Harry Bratt, Romain Rossier,
Venkata Rao Gadde, Elizabeth Shriberg, Victor
Abrash, and Kristin Precoda. 2010. Eduspeak: A
speech recognition and pronunciation scoring toolkit
for computer-aided language learning applications.
Language Testing.
Eibe Frank and Remco R Bouckaert. 2006. Naive
bayes for text classification with unbalanced classes.
Knowledge Discovery in Databases.
Bruce Fraser. 1999. What are discourse markers?
Journal of pragmatics, 31(7):931?952.
Yoav Freund and Robert E Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proceed-
ings of ICML.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL.
Martin Gu?tlein, Eibe Frank, Mark Hall, and Andreas
Karwath. 2009. Large-scale attribute selection us-
ing wrappers. In Proceedings of IEEE CIDM.
Eui-Hong Han, George Karypis, and Vipin Kumar.
2001. Text categorization using weight adjusted
k-nearest neighbor classification. Lecture Notes in
Computer Science: Advances in Knowledge Discov-
ery and Data Mining.
Ahmed Hassan, Anthony Fader, Michael H Crespin,
Kevin M Quinn, Burt L Monroe, Michael Colaresi,
and Dragomir R Radev. 2008. Tracking the dy-
namic evolution of participant salience in a discus-
sion. In Proceedings of Coling.
Marti A Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics.
Mette Terp Hoybye, Christoffer Johansen, and Tine
Tjornhoj-Thomsen. 2005. Online interaction ef-
fects of storytelling in an internet breast cancer sup-
port group. Psycho-oncology.
Hui Jiang, Keikichi Hirose, and Qiang Huo. 1999. Ro-
bust speech recognition based on a bayesian predic-
tion approach. In IEEE Transactions on Speech and
Audio Processing.
Niels Landwehr, Mark Hall, and Eibe Frank. 2005.
Logistic model trees. Machine Learning.
Elijah Mayfield and Carolyn Penstein Rose?. 2013.
Lightside: Open source machine learning for text.
In Handbook of Automated Essay Evaluation: Cur-
rent Applications and New Directions.
112
Elijah Mayfield, David Adamson, and Carolyn Pen-
stein Rose?. 2012a. Hierarchical conversation struc-
ture prediction in multi-party chat. In Proceedings
of SIGDIAL Meeting on Discourse and Dialogue.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and
Carolyn Penstein Rose?. 2012b. Discovering habits
of effective online support group chatrooms. In
ACM Conference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jason E. Owen, Erin O?Carroll Bantum, and Mitch
Golant. 2008. Benefits and challenges experienced
by professional facilitators of online support groups
for cancer survivors. In Psycho-Oncology.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the Association for Computational Linguistics.
Bambang Parmanto, Paul Munro, and Howard R
Doyle. 1996. Improving committee diagnosis with
resampling techniques. In Proceedings of NIPS.
James W Pennebaker and J D Seagal. 1999. Forming
a story: The health benefits of narrative. Journal of
Clinical Psychology.
Bernardine M Pinto, Nancy C Maruyama, Matthew M
Clark, Dean G Cruess, Elyse Park, and Mary
Roberts. 2002. Motivation to modify lifestyle risk
behaviors in women treated for breast cancer. In
Mayo Clinic Proceedings.
Andrei Popescu-Belis. 2008. Dimensionality of di-
alogue act tagsets: An empirical analysis of large
corpora. In Language Resources and Evaluation.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting overt display of power in
written dialogs. In Proceedings of NAACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of LREC.
Megan L Robbins, Elizabeth S Focella, Shelley Kasle,
Ana Mar??a Lo?pez, Karen L Weihs, and Matthias R
Mehl. 2011. Naturalistically observed swear-
ing, emotional support, and depressive symptoms
in women coping with illness. Health Psychology,
30:789.
Carolyn Penstein Rose?, Yi-Chia Wang, Yue Cui, Jaime
Arguello, Karsten Stegmann, Armin Weinberger,
and Frank Fischer. 2008. Analyzing collabo-
rative learning processes automatically: Exploit-
ing the advances of computational linguistics in
computer-supported collaborative learning. In Inter-
national Journal of Computer Supported Collabora-
tive Learning.
Nikol Rummel, Armin Weinberger, Christof Wecker,
Frank Fischer, Anne Meier, Eleni Voyiatzaki,
George Kahrimanis, Hans Spada, Nikolaos Avouris,
and Erin Walker. 2008. New challenges in cscl:
Towards adaptive script support. In Proceedings of
ICLS.
Christina Sauper and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of ACL.
Emanuel A Schegloff. 1968. Sequencing in conversa-
tional openings. American Anthropologist.
Oliver G Selfridge. 1958. Pandemonium: a
paradigm for learning. In Proceedings of Sympo-
sium on Mechanisation of Thought Processes, Na-
tional Physical Laboratory.
Gerry Stahl. 2012. Interaction analysis of a biology
chat. Productive multivocality.
Lee H Staples. 1990. Powerful ideas about empower-
ment. Administration in Social Work.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientic articles: Experiments with relevance and
rhetorical status. Computational Linguistics.
C F Van Uden-Kraan, C H C Drossaert, E Taal, E R
Seydel, and M A F J Van de Laar. 2009. Partici-
pation in online patient support groups endorses pa-
tients empowerment. Patient Education and Coun-
seling.
R Vauth, B Kleim, M Wirtz, and P W Corrigan. 2007.
Self-efficacy and empowerment as outcomes of self-
stigmatizing and coping in schizophrenia. Psychia-
try Research.
Ingrid Wahlin, Anna-Christina Ek, and Ewa Idvali.
2006. Patient empowerment in intensive carean in-
terview study. Intensive and Critical Care Nursing.
William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan Black, and Justine Cassell. 2012. ?love
ya, jerkface:? using sparse log-linear models to build
positive (and impolite) relationships with teens. In
Proceedings of SIGDIAL.
Gary M Weiss and Haym Hirsh. 1998. Learning to
predict rare events in event sequences. In Proceed-
ings of KDD.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation.
113
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 131?139,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sentiment Classification using Automatically Extracted Subgraph Features
Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rose? and Eric Nyberg
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{shilpaa, emayfiel, cprose, ehn}@cs.cmu.edu
Abstract
In this work, we propose a novel representa-
tion of text based on patterns derived from lin-
guistic annotation graphs. We use a subgraph
mining algorithm to automatically derive fea-
tures as frequent subgraphs from the annota-
tion graph. This process generates a very large
number of features, many of which are highly
correlated. We propose a genetic program-
ming based approach to feature construction
which creates a fixed number of strong classi-
fication predictors from these subgraphs. We
evaluate the benefit gained from evolved struc-
tured features, when used in addition to the
bag-of-words features, for a sentiment classi-
fication task.
1 Introduction
In recent years, the topic of sentiment analysis has
been one of the more popular directions in the field
of language technologies. Recent work in super-
vised sentiment analysis has focused on innovative
approaches to feature creation, with the greatest im-
provements in performance with features that in-
sightfully capture the essence of the linguistic con-
structions used to express sentiment, e.g. (Wilson et
al., 2004), (Joshi and Rose?, 2009)
In this spirit, we present a novel approach that
leverages subgraphs automatically extracted from
linguistic annotation graphs using efficient subgraph
mining algorithms (Yan and Han, 2002). The diffi-
culty with automatically deriving complex features
comes with the increased feature space size. Many
of these features are highly correlated and do not
provide any new information to the model. For ex-
ample, a feature of type unigram POS (e.g. ?cam-
era NN?) doesn?t provide any additional informa-
tion beyond the unigram feature (e.g. ?camera?),
for words that are often used with the same part of
speech. However, alongside several redundant fea-
tures, there are also features that provide new infor-
mation. It is these features that we aim to capture.
In this work, we propose an evolutionary ap-
proach that constructs complex features from sub-
graphs extracted from an annotation graph. A con-
stant number of these features are added to the un-
igram feature space, adding much of the represen-
tational benefits without the computational cost of a
drastic increase in feature space size.
In the remainder of the paper, we review prior
work on features commonly used for sentiment anal-
ysis. We then describe the annotation graph rep-
resentation proposed by Arora and Nyberg (2009).
Following this, we describe the frequent subgraph
mining algorithm proposed in Yan and Han (2002),
and used in this work to extract frequent subgraphs
from the annotation graphs. We then introduce our
novel feature evolution approach, and discuss our
experimental setup and results. Subgraph features
combined with the feature evolution approach gives
promising results, with an improvement in perfor-
mance over the baseline.
2 Related Work
Some of the recent work in sentiment analysis has
shown that structured features (features that capture
syntactic patterns in text), such as n-grams, depen-
dency relations, etc., improve performance beyond
131
the bag of words approach. Arora et al (2009) show
that deep syntactic scope features constructed from
transitive closure of dependency relations give sig-
nificant improvement for identifying types of claims
in product reviews. Gamon (2004) found that using
deep linguistic features derived from phrase struc-
ture trees and part of speech annotations yields sig-
nificant improvements on the task of predicting sat-
isfaction ratings in customer feedback data. Wilson
et al (2004) use syntactic clues derived from depen-
dency parse tree as features for predicting the inten-
sity of opinion phrases1.
Structured features that capture linguistic patterns
are often hand crafted by domain experts (Wilson
et al, 2005) after careful examination of the data.
Thus, they do not always generalize well across
datasets and domains. This also requires a signif-
icant amount of time and resources. By automati-
cally deriving structured features, we might be able
to learn new annotations faster.
Matsumoto et al (2005) propose an approach that
uses frequent sub-sequence and sub-tree mining ap-
proaches (Asai et al, 2002; Pei et al, 2004) to derive
structured features such as word sub-sequences and
dependency sub-trees. They show that these features
outperform bag-of-words features for a sentiment
classification task and achieve the best performance
to date on a commonly-used movie review dataset.
Their approach presents an automatic procedure for
deriving features that capture long distance depen-
dencies without much expert intervention.
However, their approach is limited to sequences
or tree annotations. Often, features that combine
several annotations capture interesting characteris-
tics of text. For example, Wilson et al (2004), Ga-
mon (2004) and Joshi and Rose? (2009) show that
a combination of dependency relations and part of
speech annotations boosts performance. The anno-
tation graph representation proposed by Arora and
Nyberg (2009) is a formalism for representing sev-
eral linguistic annotations together on text. With an
annotation graph representation, instances are rep-
resented as graphs from which frequent subgraph
patterns may be extracted and used as features for
learning new annotations.
1Although, in this work we are classifying sentences and not
phrases, similar clues may be used for sentiment classification
in sentences as well
In this work, we use an efficient frequent sub-
graph mining algorithm (gSpan) (Yan and Han,
2002) to extract frequent subgraphs from a linguis-
tic annotation graph (Arora and Nyberg, 2009). An
annotation graph is a general representation for ar-
bitrary linguistic annotations. The annotation graph
and subgraph mining algorithm provide us a quick
way to test several alternative linguistic representa-
tions of text. In the next section, we present a formal
definition of the annotation graph and a motivating
example for subgraph features.
3 Annotation Graph Representation and
Feature Subgraphs
Arora and Nyberg (2009) define the annotation
graph as a quadruple: G = (N,E,?, ?), where
N is the set of nodes, E is the set of edges, s.t.
E ? N ? N , and ? = ?N ? ?E is the set of la-
bels for nodes and edges. ? : N ? E ? ? is the
labeling function for nodes and edges. Examples of
node labels (?N ) are tokens (unigrams) and annota-
tions such as part of speech, polarity etc. Examples
of edge labels (?E) are leftOf, dependency type etc.
The leftOf relation is defined between two adjacent
nodes. The dependency type relation is defined be-
tween a head word and its modifier.
Annotations may be represented in an annotation
graph in several ways. For example, a dependency
triple annotation ?good amod movie?, may be repre-
sented as a d amod relation between the head word
?movie? and its modifier ?good?, or as a node d amod
with edges ParentOfGov and ParentOfDep to the
head and the modifier words. An example of an an-
notation graph is shown in Figure 1.
The instance in Figure 1 describes a movie review
comment, ?interesting, but not compelling.?. The
words ?interesting? and ?compelling? both have pos-
itive prior polarity, however, the phrase expresses
negative sentiment towards the movie. Heuristics for
special handling of negation have been proposed in
the literature. For example, Pang et al (2002) ap-
pend every word following a negation, until a punc-
tuation, with a ?NOT? . Applying a similar technique
to our example gives us two sentiment bearing fea-
tures, one positive (?interesting?) and one negative
(?NOT-compelling?), and the model may not be as
sure about the predicted label, since there is both
132
positive and negative sentiment present.
In Figure 2, we show three discriminating sub-
graph features derived from the annotation graph in
Figure 1. These subgraph features capture the nega-
tive sentiment in our example phrase. The first fea-
ture in 2(a) captures the pattern using dependency
relations between words. A different review com-
ment may use the same linguistic construction but
with a different pair of words, for example ?a pretty
good, but not excellent story.? This is the same lin-
guistic pattern but with different words the model
may not have seen before, and hence may not clas-
sify this instance correctly. This suggests that the
feature in 2(a) may be too specific.
In order to mine general features that capture the
rhetorical structure of language, we may add prior
polarity annotations to the annotation graph, us-
ing a lexicon such as Wilson et al (2005). Fig-
ure 2(b) shows the subgraph in 2(a) with polar-
ity annotations. If we want to generalize the pat-
tern in 2(a) to any positive words, we may use the
feature subgraph in Figure 2(c) with X wild cards
on words that are polar or negating. This feature
subgraph captures the negative sentiment in both
phrases ?interesting, but not compelling.? and ?a
pretty good, but not excellent story.?. Similar gener-
alization using wild cards on words may be applied
with other annotations such as part of speech anno-
tations as well. By choosing where to put the wild
card, we can get features similar to, but more pow-
erful than, the dependency back-off features in Joshi
and Rose? (2009).
 
U_interesting U_, U_but U_not U_compelling U_. 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
posQ 
P_VBN 
posQ 
P_, 
posQ 
P_CC 
posQ 
P_RB 
posQ 
P_JJ 
posQ 
P_. 
Figure 1: Annotation graph for sentence ?interesting, but not
compelling.? . Prefixes: ?U? for unigrams (tokens), ?L? for po-
larity, ?D? for dependency relation and ?P? for part of speech.
Edges with no label encode the ?leftOf? relation between words.
4 Subgraph Mining Algorithms
In the previous section, we demonstrated that sub-
graphs from an annotation graph can be used to iden-
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
(a)
 
U_interesting U_not U_compelling 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(b)
 
X X X 
D_conj-but 
D_neg 
L_POSITIVE L_POSITIVE 
polQ polQ 
(c)
Figure 2: Subgraph features from the annotation graph in
Figure 1
tify the rhetorical structure used to express senti-
ment. The subgraph patterns that represent general
linguistic structure will be more frequent than sur-
face level patterns. Hence, we use a frequent sub-
graph mining algorithm to find frequent subgraph
patterns, from which we construct features to use in
the supervised learning algorithm.
The goal in frequent subgraph mining is to find
frequent subgraphs in a collection of graphs. A
graph G? is a subgraph of another graph G if there
exists a subgraph isomorphism2 from G? to G, de-
noted by G? ? G.
Earlier approaches in frequent subgraph mining
(Inokuchi et al, 2000; Kuramochi and Karypis,
2001) used a two-step approach of first generating
the candidate subgraphs and then testing their fre-
quency in the graph database. The second step in-
volves a subgraph isomorphism test, which is NP-
complete. Although efficient isomorphism testing
algorithms have been developed making it practical
to use, with lots of candidate subgraphs to test, it can
2http://en.wikipedia.org/wiki/Subgraph_
isomorphism_problem
133
still be very expensive for real applications.
gSpan (Yan and Han, 2002) uses an alternative
pattern growth based approach to frequent subgraph
mining, which extends graphs from a single sub-
graph directly, without candidate generation. For
each discovered subgraph G, new edges are added
recursively until all frequent supergraphs of G have
been discovered. gSpan uses a depth first search tree
(DFS) and restricts edge extension to only vertices
on the rightmost path. However, there can be multi-
ple DFS trees for a graph. gSpan introduces a set of
rules to select one of them as representative. Each
graph is represented by its unique canonical DFS
code, and the codes for two graphs are equivalent if
the graphs are isomorphic. This reduces the compu-
tational cost of the subgraph mining algorithm sub-
stantially, making gSpan orders of magnitude faster
than other subgraph mining algorithms. With sev-
eral implementations available 3, gSpan has been
commonly used for mining frequent subgraph pat-
terns (Kudo et al, 2004; Deshpande et al, 2005). In
this work, we use gSpan to mine frequent subgraphs
from the annotation graph.
5 Feature Construction using Genetic
Programming
A challenge to overcome when adding expressive-
ness to the feature space for any text classification
problem is the rapid increase in the feature space
size. Among this large set of new features, most
are not predictive or are very weak predictors, and
only a few carry novel information that improves
classification performance. Because of this, adding
more complex features often gives no improvement
or even worsens performance as the feature space?s
signal is drowned out by noise.
Riloff et al (2006) propose a feature subsump-
tion approach to address this issue. They define a
hierarchy for features based on the information they
represent. A complex feature is only added if its
discriminative power is a delta above the discrimi-
native power of all its simpler forms. In this work,
we use a Genetic Programming (Koza, 1992) based
approach which evaluates interactions between fea-
3http://www.cs.ucsb.edu/?xyan/software/
gSpan.htm, http://www.kyb.mpg.de/bs/people/
nowozin/gboost/
tures and evolves complex features from them. The
advantage of the genetic programing based approach
over feature subsumption is that it allows us to eval-
uate a feature using multiple criteria. We show that
this approach performs better than feature subsump-
tion.
A lot of work has considered this genetic pro-
gramming problem (Smith and Bull, 2005). The
most similar approaches to ours are taken by Kraw-
iec (2002) and Otero et al (2002), both of which use
genetic programming to build tree feature represen-
tations. None of this work was applied to a language
processing task, though there has been some sim-
ilar work to ours in that community, most notably
(Hirsch et al, 2007), which built search queries for
topic classification of documents. Our prior work
(Mayfield and Rose?, 2010) introduced a new feature
construction method and was effective when using
unigram features; here we extend our approach to
feature spaces which are even larger and thus more
problematic.
The Genetic Programming (GP) paradigm is most
advantageous when applied to problems where there
is not a correct answer to a problem, but instead
there is a gradient of partial solutions which incre-
mentally improve in quality. Potential solutions are
represented as trees consisting of functions (non-leaf
nodes in the tree, which perform an action given
their child nodes as input) and terminals (leaf nodes
in the tree, often variables or constants in an equa-
tion). The tree (an individual) can then be inter-
preted as a program to be executed, and the output
of that program can be measured for fitness (a mea-
surement of the program?s quality). High-fitness in-
dividuals are selected for reproduction into a new
generation of candidate individuals through a breed-
ing process, where parts of each parent are combined
to form a new individual.
We apply this design to a language processing
task at the stage of feature construction - given many
weakly predictive features, we would like to com-
bine them in a way which produces a better feature.
For our functions we use boolean statements AND
and XOR, while our terminals are selected randomly
from the set of all unigrams and our new, extracted
subgraph features. Each leaf?s value, when applied
to a single sentence, is equal to 1 if that subgraph is
present in the sentence, and 0 if the subgraph is not
134
present.
The tree in Figure 3 is a simplified example of our
evolved features. It combines three features, a uni-
gram feature ?too? (centre node) and two subgraph
features: 1) the subgraph in the leftmost node oc-
curs in collocations containing ?more than? (e.g.,
?nothing more than? or ?little more than?), 2) the
subgraph in the rightmost node occurs in negative
phrases such as ?opportunism at its most glaring?
(JJS is a superlative adjective and PRP$ is a pos-
sessive pronoun). A single feature combining these
weak indicators can be more predictive than any part
alone.
!"#$
!"#$
%&'(($
%&)(*+$
,&-*+-&'./0$
%&1'2$
3"4&3#35$
3"4&664$
,&-(22$
Figure 3: A tree constructed using subgraph features and GP
(Simplified for illustrative purposes)
In the rest of this section, we first describe the
feature construction process using genetic program-
ming. We then discuss how fitness of an individual
is measured for our classification task.
5.1 Feature Construction Process
We divide our data into two sets, training and test.
We again divide our training data in half, and train
our GP features on only one half of this data4 This is
to avoid overfitting the final SVM model to the GP
features. In a single GP run, we produce one feature
to match each class value. For a sentiment classifica-
tion task, a feature is evolved to be predictive of the
positive instances, and another feature is evolved to
be predictive of the negative documents. We repeat
this procedure a total of 15 times (using different
seeds for random selection of features), producing
a total of 30 new features to be added to the feature
space.
4For genetic programming we used the ECJ toolkit
(http://cs.gmu.edu/?eclab/projects/ecj/).
5.2 Defining Fitness
Our definition of fitness is based on the concepts
of precision and recall, borrowed from informa-
tion retrieval. We define our set of documents
as being comprised of a set of positive documents
P0, P1, P2, ...Pu and a set of negative documents
N0, N1, N2, ...Nv. For a given individual I and doc-
ument D, we define hit(I,D) to equal 1 if the state-
ment I is true of that document and 0 otherwise. Pre-
cision and recall of an individual feature for predict-
ing positive documents5 is then defined as follows:
Prec(I) =
u
?
i=0
hit(I, Pi)
u
?
i=0
hit(I, Pi) +
v
?
i=0
hit(I,Ni)
(1)
Rec(I) =
u
?
i=0
hit(I, Pi)
u
(2)
We then weight these values to give significantly
more importance to precision, using the F? measure,
which gives the harmonic mean between precision
and recall:
F?(I) =
(1 + ?2)? (Prec(I)?Rec(I))
(?2 ? Prec(I)) +Rec(I) (3)
In addition to this fitness function, we add two
penalties to the equation. The first penalty applies to
prevent trees from becoming overly complex. One
option to ensure that features remain moderately
simple is to simply have a maximum depth beyond
which trees cannot grow. Following the work of
Otero et al (2002), we penalize trees based on the
number of nodes they contain. This discourages
bloat, i.e. sections of trees which do not contribute to
overall accuracy. This penalty, known as parsimony
pressure, is labeled PP in our fitness function.
The second penalty is based on the correlation be-
tween the feature being constructed, and the sub-
graphs and unigrams which appear as nodes within
that individual. Without this penalty, a feature may
5Negative precision and recall are defined identically, with
obvious adjustments to test for negative documents instead of
positive.
135
often be redundant, taking much more complexity
to represent the same information that is captured
with a simple unigram. We measure correlation us-
ing Pearson?s product moment, defined for two vec-
tors X , Y as:
?x,y =
E[(X ? ?X)(Y ? ?Y )]
?X?Y
(4)
This results in a value from 1 (for perfect align-
ment) to -1 (for inverse alignment). We assign a
penalty for any correlation past a cutoff. This func-
tion is labeled CC (correlation constraint) in our fit-
ness function.
Our fitness function therefore is:
Fitness = F 1
8
+ PP + CC (5)
6 Experiments and Results
We evaluate our approach on a sentiment classifi-
cation task, where the goal is to classify a movie
review sentence as expressing positive or negative
sentiment towards the movie.
6.1 Data and Experimental Setup
Data: The dataset consists of snippets from Rot-
ten Tomatoes (Pang and Lee, 2005) 6. It consists
of 10662 snippets/sentences total with equal num-
ber positive and negative sentences (5331 each).
This dataset was created and used by Pang and Lee
(2005) to train a classifier for identifying positive
sentences in a full length review. We use the first
8000 (4000 positive, 4000 negative) sentences as
training data and evaluate on remaining 2662 (1331
positive, 1331 negative) sentences. We added part
of speech and dependency triple annotations to this
data using the Stanford parser (Klein and Manning,
2003).
Annotation Graph: For the annotation graph rep-
resentation, we used Unigrams (U), Part of Speech
(P) and Dependency Relation Type (D) as labels for
the nodes, and ParentOfGov and ParentOfDep as la-
bels for the edges. For a dependency triple such as
?amod good movie?, five nodes are added to the an-
notation graph as shown in Figure 4(a). ParentOf-
Gov and ParentOfDep edges are added from the
6http://www.cs.cornell.edu/people/pabo/
movie-review-data/rt-polaritydata.tar.gz
D_amod
U_good
P_JJ
P_NN
U_movie
ParentofGov
ParentofGovParentofDep
ParentofDep
(a)
D_amod
U_good
P_NN
ParentofGov
ParentofDep
(b)
D_amod
X
P_JJ
P_NN
X
posQ
ParentofGov
ParentofDep
posQ
(c)
Figure 4: Annotation graph and a feature subgraph for
dependency triple annotation ?amod good camera?. (c)
shows an alternative representation with wild cards
dependency relation node D amod to the unigram
nodes U good and U movie. These edges are also
added for the part of speech nodes that correspond
to the two unigrams in the dependency relation, as
shown in Figure 4(a). This allows the algorithm to
find general patterns, based on a dependency rela-
tion between two part of speech nodes, two unigram
nodes or a combination of the two. For example,
a subgraph in Figure 4(b) captures a general pat-
tern where good modifies a noun. This feature ex-
ists in ?amod good movie?, ?amod good camera?
and other similar dependency triples. This feature is
similar to the the dependency back-off features pro-
posed in Joshi and Rose? (2009).
The extra edges are an alternative to putting wild
cards on words, as proposed in section 3. On the
other hand, putting a wild card on every word in
the annotation graph for our example (Figure 4(c)),
will only give features based on dependency rela-
tions between part of speech annotations. Thus, the
wild card based approach is more restrictive than
136
adding more edges. However, with lots of edges, the
complexity of the subgraph mining algorithm and
the number of subgraph features increases tremen-
dously.
Classifier: For our experiments we use Support
Vector Machines (SVM) with a linear kernel. We
use the SVM-light7 implementation of SVM with
default settings.
Parameters: The gSpan algorithm requires setting
the minimum support threshold (minsup) for the
subgraph patterns to extract. Support for a subgraph
is the number of graphs in the dataset that contain
the subgraph. We experimented with several values
for minimum support and minsup = 2 gave us the
best performance.
For Genetic Programming, we used the same pa-
rameter settings as described in Mayfield and Rose?
(2010), which were tuned on a different dataset8
than one used in this work, but it is from the same
movie review domain. We also consider one alter-
ation to these settings. As we are introducing many
new and highly correlated features to our feature
space through subgraphs, we believe that a stricter
constraint must be placed on correlation between
features. To accomplish this, we can set our correla-
tion penalty cutoff to 0.3, lower than the 0.5 cutoff
used in prior work. Results for both settings are re-
ported.
Baselines: To the best of our knowledge, there is
no supervised machine learning result published on
this dataset. We compare our results with the fol-
lowing baselines:
? Unigram-only Baseline: In sentiment analysis,
unigram-only features have been a strong base-
line (Pang et al, 2002; Pang and Lee, 2004).
We only use unigrams that occur in at least
two sentences of the training data same as Mat-
sumoto et al (2005). We also filter out stop
words using a small stop word list9.
? ?2 Baseline: For our training data, after filter-
ing infrequent unigrams and stop words, we get
7http://svmlight.joachims.org/
8Full movie review data by Pang et al (2002)
9http://nlp.stanford.edu/
IR-book/html/htmledition/
dropping-common-terms-stop-words-1.html
(with one modification: removed ?will?, added ?this?)
8424 features. Adding subgraph features in-
creases the total number of features to 44, 161,
a factor of 5 increase in size. Feature selec-
tion can be used to reduce this size by select-
ing the most discriminative features. ?2 feature
selection (Manning et al, 2008) is commonly
used in the literature. We compare two methods
of feature selection with ?2, one which rejects
features if their ?2 score is not significant at the
0.05 level, and one that reduces the number of
features to match the size of our feature space
with GP.
? Feature Subsumption (FS): Following the idea
in Riloff et al (2006), a complex feature
C is discarded if IG(S) ? IG(C) ? ?,
where IG is Information Gain and S is
a simple feature that representationally sub-
sumes C, i.e. the text spans that match S
are a superset of the text spans that match
C. In our work, complex features are sub-
graph features and simple features are uni-
gram features contained in them. For example,
(D amod) Edge ParentOfDep (U bad) is
a complex feature for which U bad is a sim-
ple feature. We tried same values for ? ?
{0.002, 0.001, 0.0005}, as suggested in Riloff
et al (2006). Since all values gave us same
number of features, we only report a single re-
sult for feature subsumption.
? Correlation (Corr): As mentioned earlier,
some of the subgraph features are highly corre-
lated with unigram features and do not provide
new knowledge. A correlation based filter for
subgraph features can be used to discard a com-
plex feature C if its absolute correlation with its
simpler feature (unigram feature) is more than
a certain threshold. We use the same threshold
as used in the GP criterion, but as a hard filter
instead of a penalty.
6.2 Results and Discussion
In Table 1, we present our results. As can be
seen, subgraph features when added to the unigrams,
without any feature selection, decrease the perfor-
mance. ?2 feature selection with fixed feature space
size provides a very small gain over unigrams. All
other feature selection approaches perform worse
137
Settings #Features Acc. ?
Uni 8424 75.66 -
Uni + Sub 44161 75.28 -0.38
Uni + Sub, ?2 sig. 3407 74.68 -0.98
Uni + Sub, ?2 size 8454 75.77 +0.11
Uni + Sub, (FS) 18234 75.47 -0.19
Uni + Sub, (Corr) 18980 75.24 -0.42
Uni + GP (U) ? 8454 76.18 +0.52
Uni + GP (U+S) ? 8454 76.48 +0.82
Uni + GP (U+S) ? 8454 76.93 +1.27
Table 1: Experimental results for feature spaces with un-
igrams, with and without subgraph features. Feature se-
lection with 1) fixed significance level (?2 sig.), 2) fixed
feature space size (?2 size), 3) Feature Subsumption (FS)
and 4) Correlation based feature filtering (Corr)). GP fea-
tures for unigrams only {GP(U)}, or both unigrams and
subgraph features {GP(U+S)}. Both the settings from
Mayfield and Rose? (2010) (?) and more stringent correla-
tion constraint (?) are reported. #Features is the num-
ber of features in the training data. Acc is the accuracy
and ? is the difference from unigram only baseline. Best
performing feature configuration is highlighted in bold.
than the unigram-only approach. With GP, we ob-
serve a marginally significant gain (p < 0.1) in per-
formance over unigrams, calculated using one-way
ANOVA. Benefit from GP is more when subgraph
features are used in addition to the unigram features,
for constructing more complex pattern features. Ad-
ditionally, our performance is improved when we
constrain the correlation more severely than in previ-
ously published research, supporting our hypothesis
that this is a helpful way to respond to the problem
of redundancy in subgraph features.
A problem that we see with ?2 feature selection is
that several top ranked features may be highly cor-
related. For example, the top 5 features based on ?2
score are shown in Table 2; it is immediately obvi-
ous that the features are highly redundant.
With GP based feature construction, we can con-
sider this relationship between features, and con-
struct new features as a combination of selected un-
igram and subgraph features. With the correlation
criterion in the evolution process, we are able to
build combined features that provide new informa-
tion compared to unigrams.
The results we present are for the best perform-
(D advmod) Edge ParentOfDep (U too)
U too
U bad
U movie
(D amod) Edge ParentOfDep (U bad)
Table 2: Top features based on ?2 score
ing parameter configuration that we tested, after a
series of experiments. We realize that this places us
in danger of overfitting to the particulars of this data
set, however, the data set is large enough to partially
mitigate this concern.
7 Conclusion and Future Work
We have shown that there is additional information
to be gained from text beyond words, and demon-
strated two methods for increasing this information -
a subgraph mining approach that finds common syn-
tactic patterns that capture sentiment-bearing rhetor-
ical structure in text, and a feature construction
technique that uses genetic programming to com-
bine these more complex features without the redun-
dancy, increasing the size of the feature space only
by a fixed amount. The increase in performance that
we see is small but consistent.
In the future, we would like to extend this work to
other datasets and other problems within the field of
sentiment analysis. With the availability of several
off-the-shelf linguistic annotators, we may add more
linguistic annotations to the annotation graph and
richer subgraph features may be discovered. There
is also additional refinement that can be performed
on our genetic programming fitness function, which
is expected to improve the quality of our features.
Acknowledgments
This work was funded in part by the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and in part by NSF grant DRL-0835426.
We would like to thank Dr. Xifeng Yan and Marisa
Thoma for the gSpan code.
References
Shilpa Arora, Mahesh Joshi and Carolyn P. Rose?. 2009.
Identifying Types of Claims in Online Customer Re-
138
views. Proceedings of the HLT/NAACL.
Shilpa Arora and Eric Nyberg. 2009. Interactive Anno-
tation Learning with Indirect Feature Voting. Proceed-
ings of the HLT/NAACL (Student Research Work-
shop).
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroshi
Sakamoto and Setsuo Arikawa. 2002. Efficient sub-
structure discovery from large semi-structured data.
Proceedings of SIAM Int. Conf. on Data Mining
(SDM).
Mukund Deshpande , Michihiro Kuramochi , Nikil Wale
and George Karypis. 2005. Frequent Substructure-
Based Approaches for Classifying Chemical Com-
pounds. IEEE Transactions on Knowledge and Data
Engineering.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vec-
tors, and the role of linguistic analysis, Proceedings
of COLING.
Laurence Hirsch, Robin Hirsch and Masoud Saeedi.
2007. Evolving Lucene Search Queries for Text Clas-
sification. Proceedings of the Genetic and Evolution-
ary Computation Conference.
Mahesh Joshi and Carolyn P. Rose?. 2009. Generalizing
Dependency Features for Opinion Mining. Proceed-
ings of the ACL-IJCNLP Conference (Short Papers).
Akihiro Inokuchi, Takashi Washio and Hiroshi Motoda.
2000. An Apriori-based Algorithm for Mining Fre-
quent Substructures from Graph Data. Proceedings
of PKDD.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the main con-
ference of the ACL.
John Koza. 1992. Genetic Programming: On the Pro-
gramming of Computers by Means of Natural Selec-
tion. MIT Press.
Krzysztof Krawiec. 2002. Genetic programming-based
construction of features for machine learning and
knowledge discovery tasks. Genetic Programming and
Evolvable Machines.
Taku Kudo, Eisaku Maeda and Yuji Matsumoto. 2004.
An Application of Boosting to Graph Classification.
Proceedings of NIPS.
Michihiro Kuramochi and George Karypis. 2002. Fre-
quent Subgraph Discovery. Proceedings of ICDM.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Proceedings of PAKDD.
Shotaro Matsumoto, Hiroya Takamura and Manabu Oku-
mura. 2005. Sentiment Classification Using Word
Sub-sequences and Dependency Sub-trees. Proceed-
ings of PAKDD.
Elijah Mayfield and Carolyn Penstein-Rose?. 2010.
Using Feature Construction to Avoid Large Feature
Spaces in Text Classification. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Fernando Otero, Monique Silva, Alex Freitas and Julio
Nievola. 2002. Genetic Programming for Attribute
Construction in Data Mining. Proceedings of the Ge-
netic and Evolutionary Computation Conference.
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classication using Ma-
chine Learning Techniques. Proceedings of EMNLP.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. Proceedings of the
main conference of ACL.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. Proceedings of the main con-
ference of ACL.
Jian Pei, Jiawei Han, Behzad Mortazavi-asl, Jianyong
Wang, Helen Pinto, Qiming Chen, Umeshwar Dayal
and Mei-chun Hsu. 2004. Mining Sequential Pat-
terns by Pattern-Growth: The PrefixSpan Approach.
Proceedings of IEEE Transactions on Knowledge and
Data Engineering.
Ellen Riloff, Siddharth Patwardhan and Janyce Wiebe.
2006. Feature Subsumption for Opinion Analysis.
Proceedings of the EMNLP.
Matthew Smith and Larry Bull. 2005. Genetic Program-
ming with a Genetic Algorithm for Feature Construc-
tion and Selection. Genetic Programming and Evolv-
able Machines.
Theresa Wilson, Janyce Wiebe and Rebecca Hwa. 2004.
Just How Mad Are You? Finding Strong and Weak
Opinion Clauses. Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity
in Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP.
Xifeng Yan and Jiawei Han. 2002. gSpan: Graph-
Based Substructure Pattern Mining. UIUC Techni-
cal Report, UIUCDCS-R-2002-2296 (shorter version
in ICDM?02).
139
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 60?69,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Hierarchical Conversation Structure Prediction in Multi-Party Chat
Elijah Mayfield, David Adamson, and Carolyn Penstein Rose?
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213
{emayfiel, dadamson, cprose}@cs.cmu.edu
Abstract
Conversational practices do not occur at a sin-
gle unit of analysis. To understand the inter-
play between social positioning, information
sharing, and rhetorical strategy in language,
various granularities are necessary. In this
work we present a machine learning model
for multi-party chat which predicts conversa-
tion structure across differing units of analy-
sis. First, we mark sentence-level behavior us-
ing an information sharing annotation scheme.
By taking advantage of Integer Linear Pro-
gramming and a sociolinguistic framework,
we enforce structural relationships between
sentence-level annotations and sequences of
interaction. Then, we show that clustering
these sequences can effectively disentangle
the threads of conversation. This model is
highly accurate, performing near human accu-
racy, and performs analysis on-line, opening
the door to real-time analysis of the discourse
of conversation.
1 Introduction
When defining a unit of analysis for studying lan-
guage, one size does not fit all. Part-of-speech tag-
ging is performed on individual words in sequences,
while parse trees represent language at the sentence
level. Individual tasks can be performed at the lex-
ical, sentence, or document level, or even to arbi-
trary length spans of text (Wiebe et al, 2005), while
rhetorical patterns are annotated in a tree-like struc-
ture across sentences or paragraphs.
In dialogue, the most common unit of analysis is
the utterance, usually through dialogue acts. Here,
too, the issue of granularity and specificity of tags
has been a persistent issue, along with the inte-
gration of larger discourse structure. Both theory-
driven and empirical work has argued for a col-
lapsing of annotations into fewer categories, based
on either marking the dominant function of a given
turn (Popescu-Belis, 2008) or identifying a single
construct of interest and annotating only as nec-
essary to distinguish that construct. We take the
latter approach in this work, predicting conversa-
tion structure particularly as it relates to informa-
tion sharing and authority in dialogue. We use sys-
temic functional linguistics? Negotiation annotation
scheme (Mayfield and Rose?, 2011) to identify utter-
ances as either giving or receiving information. This
annotation scheme is of particular interest because in
addition to sentence-level annotation, well-defined
sequences of interaction are incorporated into the
annotation process. This sequential structure has
been shown to be useful in secondary analysis of
annotated data (Mayfield et al, 2012a), as well as
providing structure which improves the accuracy of
automated annotations.
This research introduces a model to predict infor-
mation sharing tags and Negotiation sequence struc-
ture jointly with thread disentanglement. We show
that performance can be improved using integer lin-
ear programming to enforce constraints on sequence
structure. Structuring and annotation of conversa-
tion is available quickly and with comparatively lit-
tle effort compared to manual annotation. More-
over, all of our results in this paper were obtained
using data a real-world, chat-based internet commu-
nity, with a mix of long-time expert and first-time
60
novice users, showing that the model is robust to the
challenges of messy data in natural environments.
The remainder of this paper is structured as fol-
lows. First, we review relevant work in annota-
tion at the levels of utterance, sequence, and thread,
and applications of each. We then introduce the
domain of our data and the framework we use for
annotation of conversation structure. In Section 4
we define a supervised, on-line machine learning
model which performs this annotation and structur-
ing across granularities. In Section 5, we evaluate
this model and show that it approaches or matches
human reliability on all tasks. We conclude with dis-
cussion of the utility of this conversation structuring
algorithm for new analyses of conversation.
2 Related Work
Research on multi-party conversation structure is
widely varied, due to the multifunctional nature of
language. These structures have been used in di-
verse fields such as computer-supported collabora-
tive work (O?Neill and Martin, 2003), dialogue sys-
tems (Bohus and Horvitz, 2011), and research on
meetings (Renals et al, 2012). Much work in an-
notation has been inspired by speech act theory and
dialogue acts (Traum, 1994; Shriberg et al, 2004),
which operate primarily on the granularity of indi-
vidual utterances. A challenge of tagging is the issue
of specificity of tags, as previous work has shown
that most utterances have multiple functions (Bunt,
2011). General tagsets have attempted to capture
multi-functionality through independent dimensions
which produce potentially millions of possible an-
notations, though in practice the number of varia-
tions remains in the hundreds (Jurafsky et al, 1998).
Situated work has jointly modelled speech act and
domain-specific topics (Laws et al, 2012).
Additional structure inspired by linguistics, such
as adjacency pairs (Schegloff, 2007) or dialogue
games (Carlson, 1983), has been used to build dis-
course relations between turns. This additional
structure has been shown to improve performance
of automated analysis (Poesio and Mikheev, 1998).
Identification of this fine-grained structure of an in-
teraction has been studied in prior work, with appli-
cations in agreement detection (Galley et al, 2004),
addressee detection (op den Akker and Traum,
2009), and real-world applications, such as cus-
tomer service conversations (Kim et al, 2010).
Higher-order structure has also been explored in dia-
logue, from complex graph-like relations (Wolf and
Gibson, 2005) to simpler segmentation-based ap-
proaches (Malioutov and Barzilay, 2006). Utterance
level-tagging can take into account nearby structure,
e.g. forward-looking and backward-looking func-
tions in DAMSL (Core and Allen, 1997), while dia-
logue management systems in intelligent agents of-
ten have a plan unfolding over a whole dialogue
(Ferguson and Allen, 1998).
In recent years, threading and maintaining of mul-
tiple ?floors? has grown in popularity (Elsner and
Charniak, 2010), especially in text-based media.
This level of analysis is designed with the goal of
separating out sub-conversations which are indepen-
dently coherent. There is a common ground emerg-
ing in the thread detection literature on best prac-
tices for automated prediction. Early work viewed
the problem as a time series analysis task (Bingham
et al, 2003). Treating thread detection as a cluster-
ing problem, with lines representing instances, was
given great attention in Shen et al (2006). Subse-
quent researchers have treated the thread detection
task as based in discourse coherence, and have pur-
sued topic modelling (Adams, 2008) or entity refer-
ence grids (Elsner and Charniak, 2011) to define that
concept of coherence.
Other work integrates local discourse structure
with the topic-based threads of discourse. Ai et al
(2007) utilizes information state, a dialogue man-
agement component which loosely parallels thread
structure, to improve dialogue act tagging. In the
context of Twitter conversations, Ritter et al (2010)
suggests using dialogue act tags as a middle layer to-
wards conversation reconstruction. Low-level struc-
ture between utterances has also been used as a
foundation for modelling larger-level sociological
phenomena between speakers in a dialogue, for in-
stance, identifying leadership (Strzalkowski et al,
2011) and rapport between providers and patients
in support groups (Ogura et al, 2008). These
works have all pointed to the utility of incorporat-
ing sentence-level annotations, low-level interaction
structure, and overarching themes into a unified sys-
tem. To our knowledge, however, this work is the
first to present a single system for simultaneous an-
61
Negotiation/Threads Seq User Text
K2 1 C [M], fast question, did your son have a biopsy?
K2 1 C or does that happen when he comes home
K1 2 V i have 3 dogs.
K1 2 V man?s best friend
f 2 S :-D
o 2 C and women
K2 3 J what kind of dogs????
K1 4 C [D], I keep seeing that you are typing and then it stops
K2 5 C how are you doing this week
K1 3 V the puppies are a maltese/yorkie mix and the full grown is a pomara-
nian/yorkie.
K1 1 M No, he did not have a biopsy.
K1 1 M The surgeon examined him and said that by feel, he did not think the
lump was cancerous, and he should just wait until he got home.
f 1 C that has to be very hard
o 7 M A question, however? [J], you would probably know.
K2 7 M He was told that they could not just do a needle biopsy, that he would
have to remove the whole lump in order to tell if it was malignant.
o 8 D Yes.
K1 8 D I was waiting for [M] to answer.
K1 7 J That sounds odd to me
Table 1: An example excerpt with Negotiation labels, sequences, and threads structure (columns) annotated.
notation and structuring at all three levels.
3 Data and Annotation
Our data comes from the Cancer Support Commu-
nity, which provides chatrooms, forums, and other
resources for support groups for cancer patients.
Each conversation took place in the context of a
weekly meeting, with several patient participants as
well as a professional therapist facilitating the dis-
cussion. In total, our annotated corpus consists of
45 conversations. This data was sampled from three
group sizes - 15 conversations from small groups (2
patients, in addition to the trained facilitator), 15
from medium-sized groups (3-4 patients), and 15
from large groups (5 or more patients).
3.1 Annotation
Our data is annotated at the three levels of granu-
larity described previously in this paper: sentences,
sequences, and threads. In this section we define
those annotations in greater detail. Sentence-level
and sequence-level annotations were performed us-
ing the Negotiation framework from systemic func-
tional linguistics (Martin and Rose, 2003). Once
sequences were identified, those sequences were
grouped together into threads based on shared topic.
We annotate our data using an adaptation of the
Negotiation framework. This framework has been
proven reliable and reproducible in previous work
(Mayfield and Rose?, 2011). By assigning aggregate
scores over a conversation, the framework also gives
us a notion of Authoritativeness. This metric, de-
fined later in Section 5, allows us to test whether
automated codes faithfully reproduce human judg-
ments of information sharing behavior at a per-user
level. This metric has proven to be a statistically
significant indicator of outcome variables in direc-
tion giving (Mayfield et al, 2011) and collaborative
learning domains (Howley et al, 2011).
In particular, Negotiation labels define whether
each speaker is a source or recipient of information.
Our annotation scheme has four turn-level codes
and a rigidly defined information sharing structure,
rooted in sociolinguistic observation. We describe
62
each in detail below.
Sentences containing new information are marked
as K1, as the speaker is the ?primary knower,? the
source of information. These sentences can be gen-
eral facts and world knowledge, but can also con-
tain opinions, retelling of narrative, or other contex-
tualized information, so long as the writer acts as
the source of that information. Sentences requesting
information, on the other hand, are marked K2, or
?secondary knower,? when the writer is signalling
that they want information from other participants
in the chat. This can be direct question asking, but
can also include requests for elaboration or indirect
illocutionary acts (e.g. ?I?d like to hear more.?).
In addition to these primary moves, we also use a
social feedback code, f, for sentences consisting of
affective feedback or sentiment, but which do not
contain new information. These moves can include
emoticons, fixed expressions such as ?good luck,? or
purely social banter. All other moves, such as typo
correction or floor grabbing, are labelled o.
This annotation scheme is highly flexible and
adaptive to new domains, and is not specific to med-
ical topics or chatroom-based media. It also gives us
a well-defined structure of an interaction: each se-
quence consists of exactly one primary knower (K1)
move, which can consist of any number of primary
knower sentences from a single speaker. If a K2
move occurs in the sequence, it occurs before any
K1 moves. Feedback moves (f) may come at any
time so long as the speaker is responding to another
speaker in the same sequence. Sentences labeled
o are idiosyncratic and may appear anywhere in a
sequence. In section 4.3, we represent these con-
straints formally.
In addition to grouping sentences together into se-
quences structurally, we also group those sequences
into threads. These threads are based on annotator
judgement, but generally map to the idea that a sin-
gle thread should be on a single theme, e.g. ?han-
dling visiting relatives at holidays.? These threads
are both intrinsically interesting for identifying the
topics of a conversation, as well as being a useful
preprocessing step for any additional, topic-based
annotation that may be desired for later analysis.
We iteratively developed a coding manual for
these layers of annotation; to test reliability at each
iteration of instructions, two annotators each inde-
Figure 1: Structured output at each phase of the two-
pass machine learning model. In pass one, utterances are
grouped into sequences with organizational structure; the
second pass groups sequences based on shared themes.
pendently annotated one full conversation. Inter-
annotator reliability is high for sentence-level an-
notation (? = 0.75). Following Elsner and Char-
niak (2010), we use micro-averaged f-score to eval-
uate inter-rater agreement on higher-level structure.
We find that inter-annotator agreement is high for
both sequence-level structure (f = 0.82) and thread-
level structure (f = 0.80). A detailed description
of the annotation process is available in Mayfield et
al. (2012b). After establishing reliability, our entire
corpus was annotated by one human coder.
4 Conversation Structure Prediction
In previous work, the Negotiation framework has
been automatically coded with high accuracy (May-
field and Rose?, 2011). However, that work restricted
the domain to a task-based, two-person dialogue,
and structure was viewed as a segmentation, rather
than threading, formulation. At each turn, a se-
quence could continue or a new sequence could be-
gin.
Here, we extend this automated coding to larger
groups speaking in unstructured, social chat, and we
extend the structured element of this coding scheme
to structure by sequence and thread. To our knowl-
edge, this is also the first attempt to utilize functional
sequences of interaction as a preprocessing step for
thread disentanglement in chat. We now present a
comprehensive machine learning model which an-
notates a conversation by utterance, groups utter-
ances topics by local structure into sequences, and
assigns sequences to threads.
63
4.1 On-Line Instance Creation
This is a two-pass algorithm. The first pass la-
bels sentences and detects sequences, and the second
pass groups these sequences into threads. We follow
Shen et al (2006) in treating the sequence detection
problem as a single-pass clustering algorithm. Their
model is equivalent to the Previous Cluster model
described below, albeit with more complex features.
In that work a threshold was defined in order for a
new message to be added to an existing cluster. If
that threshold is not passed, a new cluster is formed.
Modelling the probability that a new cluster should
be formed is similar to a context-sensitive threshold,
and because we do not impose a hard threshold, we
can pass the set of probabilities for cluster assign-
ments to a structured prediction system.
4.2 Model Definitions
At its core, our model relies on three probabilistic
classifiers. One of these models is a classification
model, and the other two treat sequence and thread
structure as clusters. All models use the LightSIDE
(Mayfield and Rose?, 2010) with the LibLinear algo-
rithm (Fan et al, 2008) for machine learning..
Negotiation Classifier (Neg)
The Negotiation model takes a single sentence as
input. The output of this model is a distribution over
the four possible sentence-level labels described in
section 3.1. The set of features for this model con-
sists of unigrams, bigrams, and part-of-speech bi-
grams. Part-of-speech tagging was performed using
the Stanford tagger (Toutanova et al, 2003) within
LightSIDE.
Cluster Classifiers (PC, NC)
We use two models of cluster assignment prob-
ability. The Previous Cluster (PC) classifier
takes as input a previous set of sentences C =
{c1, c2, . . . , cn} and set of new sentences N =
{N1, N2, . . . , Nm}. To evaluate whether c? should
be added to this cluster, we train a binary proba-
bilistic classifier that predicts the probability that the
sentences inN belong to the same cluster as the sen-
tences already inC. In the first pass, each inputN to
the PC classifier is a set containing a single sentence,
and each C is the set of sentences in a previously-
identified sequence. In the second pass, each N is a
sequence as predicted by the first pass.
The PC model uses two features. The first is a
time-based feature, measuring the amount of time
that has elapsed between the last sentence in C and
the first sentence in N . The time feature is repre-
sented differently between sequence prediction and
thread prediction. Elsner and Charniak (2010) rec-
ommends using bucketed nominal values based on
the log time, to group together very recent and very
distant posts. We follow this for sequence predic-
tion. Due to the more complex structure of the se-
quence grouping task in the second pass, we use a
raw numeric time feature. The second feature is a
coherence metric, the cosine similarity between the
centroid of C and the centroid of N . We define the
centroid based on TF-IDF weighted unigram vec-
tors.
We impose a threshold after which previous clus-
ters are no longer considered as options for the
PC classifier. Because sequences are shorter than
threads, we set these thresholds separately, at 90 sec-
onds for sequences and 120 seconds for threads. Ap-
proximately 1% of correct assignments are impossi-
ble due to these thresholds.
The New Cluster (NC) classifier takes as input
a set of sentences n = {n1, n2, . . . , nm}, and pre-
dicts the probability that a given sentence is initiat-
ing a new sequence (or, in the second pass, whether
a given sequence is initiating a new thread). This
model contains only unigram features.
At each sentence s we consider the set of possible
previous cluster assignments C = {c1, c2, . . . , cn},
and define psc(s, c) to be the probability that s
will be assigned to cluster c. We define pnc(s) =
?sNC(s). The addition of a weight parameter to
the output of the NC classifier allows us to tune the
likelihood of transitioning to a new cluster. This pre-
diction structure is illustrated in Figure 2. In the
first pass, these cluster probabilities are used in con-
junction with the output of the Negotiation classifier
to form a structured output; in the second pass, the
maximum cluster probability is chosen.
4.3 Constraining Sequence Structure with ILP
In past work the Negotiation framework has bene-
fited from enforced constraints of linguistically sup-
ported rules on sequence structure (Mayfield and
64
Figure 2: The output of the cluster classifier in either pass
is a set of probabilities corresponding to possible clus-
ter assignments, including that of creating a new cluster.
In the second pass, the input is a set of sentences (a se-
quence) rather than a single sentence, and output assign-
ments are to threads rather than sequences.
Rose?, 2011). Constraints on the structure of anno-
tations are easily defined using Integer Linear Pro-
gramming. Recent work has used boolean logic
(Chang et al, 2008) to allow intuitive rules about
a domain to be enforced at classification time. ILP
inference was performed using Learning-Based Java
(Rizzolo and Roth, 2010).
First, we define the classification task. Opti-
mization is performed given the set of probabilities
N (s) as the distribution output of the Neg classifier
given sentence s as input, and the set of probabilities
C(s) = pnc(s) ? psc(s, c), ?c ? C. Instance classi-
fication requires maximizing the objective function:
arg max
n?N (s),c?C(s)
n+ c
We impose constraints on sequence prediction. If
the most likely output from this function assigns
a label that is incompatible with the assigned se-
quence, either the label is changed or a new se-
quence is assigned so that constraints are met. For
each constraint, we give the intuition from sec-
tion 3.1, followed by our formulation of that con-
straint. us is shorthand for the user who wrote
sentence s; ns is shorthand for a proposed Ne-
gotiation label of sentence s; while cs is a pro-
posed sequence assignment for s, c? is shorthand
for assignment to a new sequence, and Sc =
{(nc,1, uc,1), (nc,2, uc,2), . . . , (nc,k, uc,k)} is the set
of Negotiation labels n and users u associated with
sentences (sc,1 . . . sc,k) already in sequence c.
1. K2 moves, if any, occur before K1 moves.
((cs = c) ? (ns = K2))
? (@i ? Sc s.t. nc,i = K1)
2. f moves may occur at any time but must be re-
sponding to a different speaker in the same se-
quence.
((cs = c) ? (ns = f))
? (?i ? Sc s.t. uc,i 6= us)
3. Functionally, therefore, f moves may not initi-
ate a sequence).
(cs = c?) ? (ns 6= f)
4. Speakers do not respond to their own requests
for information (the speakers of K2 and K1
moves in the same sequence must be different).
((cs = c) ? (ns = K1))
? (?i ? Sc, ((nc,i = K2) ? (uc,i 6= us)))
5. Each sequence consists of at most one continu-
ous series of K1 moves from the same speaker.
(cs = c) ? ((?i ? Sc s.t. (nc,i = K1))
? ( (uc,i = us) ? (?j > i,
(uc,j = us) ? (nc,i = K1)) )
Human annotators treated these rules as hard con-
straints, as the classifier does. In circumstances
where these rules would be broken (for instance, due
to barge-in or trailing off), a new sequence begins.
5 Evaluation
5.1 Methods
To evaluate the performance of this model, we wish
to know how it replicates human annotation at each
granularity. For Negotiation labels, agreement is
measured by terms of absolute accuracy and kappa
agreement above chance. We also include a measure
of aggregate information sharing behavior per user.
This score, which we term Information Authorita-
tiveness (Auth), is defined per user as the percentage
65
of their contentful sentences (K1 or K2) which were
giving information (K1). To measure performance
on this measure, we measure the r2 coefficient be-
tween user authoritativeness scores calculated from
the predicted labels compared to actual labels. This
is equivalent to measuring the variance explained by
our model, where each data point represents a single
user?s predicted and actual authoritativeness scores
over the course of a whole conversation (n = 215).
Sequence and thread agreement is evaluated by
micro-averaged f-score (MAF), defined in prior
work for a gold sequence i with size ni, and a pro-
posed sequence j with size nj , based on precision
and recall metrics:
P = nijnj R =
nij
ni F (i, j) =
2?P?R
P+R
MAF across an entire conversation is then a
weighted sum of f-scores across all sequences1:
MAF =
?
i
ni
n maxj F (i, j)
We implemented multiple baselines to test
whether our methods improve upon simpler ap-
proaches. For sequence and thread prediction, we
implement the following baselines. Speaker Shift
predicts a new thread every time a new writer adds a
line to the chat. Turn Windows predicts a new se-
quence or thread after every n turns. Pause Length
predicts a new sequence or thread every time that a
gap of n seconds has occurred between lines of chat.
For both of the previous two baselines, we vary the
parameter n to optimize performance and provide
a challenging baseline. None of these models use
any features or constraints, and are based on heuris-
tics. To compare to our model, we present both an
Unconstrained model, which uses machine learn-
ing and does not impose sequence constraints from
Section 4.3, as well as our full Constrained model.
Evaluation is performed using 15-fold cross-
validation. In each fold, one small, one medium,
and one large conversation are held out as a test set,
and classifiers are trained on the remaining 42 con-
versations. Significance is evaluated using a paired
student?s t-test per conversation (n = 45).
Sentence-Level (Human ? = 0.75)
Model Accuracy ? Auth r2
Unconstrained .7736 .5870 .7498
Constrained .7777 .5961 .7355
Sequence-Level (Human MAF = 0.82)
Model Precision Recall MAF
Speaker Shift .7178 .5140 .5991
Turn Windows .7207 .6233 .6685
Pause Length .8479 .6582 .7411
Unconstrained .7909 .7068 .7465
Constrained .8557 .7116 .7770
Thread-Level (Human MAF = 0.80)
Model Precision Recall MAF
Turn Windows .5994 .7173 .6531
Pause Length .6145 .6316 .6229
Unconstrained .7132 .5781 .6386
Constrained .6805 .6024 .6391
Table 2: Tuned optimal annotation performances of base-
line heuristics compared to our machine learning model.
5.2 Results
Results of experimentation show that all models
are highly accurate in their respective tasks. With
sentence-level annotation approaching 0.6 ?, the
output of the model is reliable enough to allow
automatically annotated data to be included reli-
ably alongside human annotations. Performance for
sequence-based modelling is even stronger, with no
statistically significant difference in f-score between
the machine learning model and human agreement.
Table 2 reports our best results after tuning to
maximize performance of baseline models, our orig-
inal machine learning model, and the model with
ILP constraints enforced between Negotiation labels
and sequence. In all three cases, we see machine
performance approaching, but not matching, human
agreement. Incorporating ILP constraints improves
per-sentence Negotiation label classification by a
small but significant amount (p < .001).
Clustering performance is highly robust, as
demonstrated in Figure 3, which shows the effect of
changing window sizes and pause lengths and values
of ?s for machine learned models. Our thread disen-
tanglement performance matches our baselines, and
1This metric extends identically to a gold thread i and pro-
posed thread j.
66
Figure 3: Parameter sensitivity on sequence-level (top)
and thread-level (bottom) annotation models.
is in line with heuristic-based assignments from El-
sner and Charniak (2010). In sequence clustering,
we observe improvement across all metrics. The
Constrained model achieves a higher f-score than all
other models (p < 0.0001). We determine through
a two-tailed confidence interval that sequence clus-
tering performance is statistically indistinguishable
from human annotation (p < 0.05).
Error analysis suggests that the constraints are too
punishing on the most constrained labels, K2 and f.
The differences in performance between constrained
and unconstrained models is largely due to higher
recall for both K1 and o move prediction, while
recall for K2 and f moves lowered slightly. One
possibility for future work may include compensat-
ing for this by artificially inflating the likelihood of
highly-constrained Negotiation labels. Additionally,
we see that the most common mistakes involve dis-
tinguishing between K1 and f moves. While many
f moves are obviously non-content-bearing (?Wow,
what fun!?), others, especially those based in humor,
may look grammatical and contentful (?We?ve got to
stop meeting this way.?). Better detection of humor
and a more well-defined definition of what informa-
tion is being shared will improve this aspect of the
model. Overall, these errors do not limit the efficacy
of the model for enabling future analysis.
6 Conclusion and Future Work
This work has presented a unified machine learn-
ing model for annotating information sharing acts
on a sentence-by-sentence granularity; grouping se-
quences of sentences based on functional structure;
and then grouping those sequences into topic-based
threads. The model performs at a high accuracy,
approaching human agreement at the sentence and
thread level. Thread-level accuracy matched but did
not exceed simpler baselines, suggesting that this
model could benefit from a more elaborate repre-
sentation of coherence and topic. At the level of se-
quences, the model performs statistically the same
as human annotation.
The automatic annotation and structuring of di-
alogue that this model performs is a vital prepro-
cessing task to organize and structure conversational
data in numerous domains. Our model allows re-
searchers to abstract away from vocabulary-based
approaches, instead working with interaction-level
units of analysis. This is especially important in
the context of interdisciplinary research, where other
representations may be overly specialized towards
one task, and vocabulary may differ for spurious rea-
sons across populations and cultures.
Our evaluation was performed on a noisy, real-
world chatroom corpus, and still performed very ac-
curately. Coherent interfacing between granularities
of analysis is always a challenge. Segmentation,
tokenization, and overlapping or inconsistent struc-
tured output are nontrivial problems. By incorpo-
rating sentence-level annotation, discourse-level se-
quence structure, and topical thread disentanglement
into a single model, we have shown one way to re-
duce or eliminate this interfacing burden and allow
greater structural awareness in real-world systems.
Future work will improve this model?s accuracy fur-
ther, test its generality in new domains such as spo-
ken multi-party interactions, and evaluate its useful-
ness in imposing structure for secondary analysis.
67
Acknowledgments
The research reported here was supported by Na-
tional Science Foundation grant IIS-0968485, Of-
fice of Naval Research grant N000141110221, and
in part by the Pittsburgh Science of Learning Center,
which is funded by the National Science Foundation
grant SBE-0836012.
References
Paige H. Adams. 2008. Conversation Thread Extraction
and Topic Detection in Text-based Chat. Ph.D. thesis.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007. Using information state to improve dia-
logue move identification in a spoken dialogue system.
In Proceedings of Interspeech.
Ella Bingham, Ata Kaban, and Mark Girolami. 2003.
Topic identification in dynamical text by complexity
pursuit. In Neural Processing Letters.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn tak-
ing in situated dialog. In Procedings of SIGDIAL.
Harry Bunt. 2011. Multifunctionality in dialogue. In
Computer Speech and Language.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. Massachussetts Institute of Tech-
nology.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of the Association for the Ad-
vancement of Artificial Intelligence.
Mark G Core and James F Allen. 1997. Coding dialogs
with the damsl annotation scheme. In AAAI Fall Sym-
posium on Communicative Action in Humans and Ma-
chines.
Micha Elsner and Eugene Charniak. 2010. Disentan-
gling chat. Computational Linguistics.
Micha Elsner and Eugene Charniak. 2011. Disentan-
gling chat with local coherence models. In Proceed-
ings of the Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification.
George Ferguson and James Allen. 1998. Trips: An in-
tegrated intelligent problem-solving assistant. In Pro-
ceedings of AAAI.
Michael Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL.
Iris Howley, Elijah Mayfield, and Carolyn Penstein Rose?.
2011. Missing something? authority in collaborative
learning. In Proceedings of Computer Supported Col-
laborative Learning.
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Switchboard discourse language
modelling final report. Technical report.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
M Barton Laws, Mary Catherine Beach, Yoojin Lee,
William H. Rogers, Somnath Saha, P Todd Korthuis,
Victoria Sharp, and Ira B Wilson. 2012. Provider-
patient adherence dialogue in hiv care: Results of a
multisite study. AIDS Behavior.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL/COLING.
J.R. Martin and David Rose. 2003. Working with Dis-
course: Meaning Beyond the Clause. Continuum.
Elijah Mayfield and Carolyn Penstein Rose?. 2010. An
interactive tool for supporting error analysis for text
mining. In NAACL Demonstration Session.
Elijah Mayfield and Carolyn Penstein Rose?. 2011. Rec-
ognizing authority in dialogue with an integer linear
programming constrained model. In Proceedings of
Association for Computational Linguistics.
Elijah Mayfield, Michael Garbus, David Adamson, and
Carolyn Penstein Rose?. 2011. Data-driven interac-
tion patterns: Authority and information sharing in di-
alogue. In Proceedings of AAAI Fall Symposium on
Building Common Ground with Intelligent Agents.
Elijah Mayfield, David Adamson, Alexander I Rudnicky,
and Carolyn Penstein Rose?. 2012a. Computational
representations of discourse practices across popula-
tions in task-based dialogue. In Proceedings of the
International Conference on Intercultural Collabora-
tion.
Elijah Mayfield, Miaomiao Wen, Mitch Golant, and Car-
olyn Penstein Rose?. 2012b. Discovering habits of ef-
fective online support group chatrooms. In ACM Con-
ference on Supporting Group Work.
Kanayo Ogura, Takashi Kusumi, and Asako Miura.
2008. Analysis of community development using
chat logs: A virtual support group of cancer patients.
In Proceedings of the IEEE Symposium on Universal
Communication.
Jacki O?Neill and David Martin. 2003. Text chat in ac-
tion. In Proceedings of the International Conference
on Supporting Group Work.
68
Rieks op den Akker and David Traum. 2009. A compari-
son of addressee detection methods for multiparty con-
versations. In Workshop on the Semantics and Prag-
matics of Dialogue.
Massimo Poesio and Andrei Mikheev. 1998. The pre-
dictive power of game structure in dialogue act recog-
nition: Experimental results using maximum entropy
estimation. In Proceedings of the International Con-
ference on Spoken Language Processing.
Andrei Popescu-Belis. 2008. Dimensionality of dialogue
act tagsets: An empirical analysis of large corpora. In
Language Resources and Evaluation.
Steve Renals, Herve? Bourlard, Jean Carletta, and Andrei
Popescu-Belis. 2012. Multimodal Signal Processing:
Human Interactions in Meetings.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of NAACL.
Nicholas Rizzolo and Dan Roth. 2010. Learning based
java for rapid development of nlp systems. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
E. Schegloff. 2007. Sequence organization in interac-
tion: A primer in conversation analysis. Cambridge
University Press.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In Proceedings of SIGIR.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The icsi meeting
recorder dialog act (mrda) corpus. In Proceedings of
SIGDIAL.
Tomek Strzalkowski, George Aaron Broadwell, Jennifer
Stromer-Galley, Samira Shaikh, Ting Liu, and Sarah
Taylor. 2011. Modeling socio-cultural phenomena in
online multi-party discourse. In AAAI Workshop on
Analyzing Microtext.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
David Traum. 1994. A computational theory of ground-
ing in natural language conversation. Ph.D. thesis.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics.
69
