Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 273?280,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Measuring Language Divergence by Intra-Lexical Comparison
T. Mark Ellison
Informatics
University of Edinburgh
mark@markellison.net
Simon Kirby
Language Evolution and Computation Research Unit
Philosophy, Psychology and Language Sciences,
University of Edinburgh
simon@ling.ed.ac.uk
Abstract
This paper presents a method for build-
ing genetic language taxonomies based
on a new approach to comparing lexi-
cal forms. Instead of comparing forms
cross-linguistically, a matrix of language-
internal similarities between forms is cal-
culated. These matrices are then com-
pared to give distances between languages.
We argue that this coheres better with
current thinking in linguistics and psy-
cholinguistics. An implementation of this
approach, called PHILOLOGICON, is de-
scribed, along with its application to Dyen
et al?s (1992) ninety-five wordlists from
Indo-European languages.
1 Introduction
Recently, there has been burgeoning interest in
the computational construction of genetic lan-
guage taxonomies (Dyen et al, 1992; Nerbonne
and Heeringa, 1997; Kondrak, 2002; Ringe et
al., 2002; Benedetto et al, 2002; McMahon
and McMahon, 2003; Gray and Atkinson, 2003;
Nakleh et al, 2005).
One common approach to building language
taxonomies is to ascribe language-language dis-
tances, and then use a generic algorithm to con-
struct a tree which explains these distances as
much as possible. Two questions arise with this
approach. The first asks what aspects of lan-
guages are important in measuring inter-language
distance. The second asks how to measure dis-
tance given these aspects.
A more traditional approach to building lan-
guage taxonomies (Dyen et al, 1992) answers
these questions in terms of cognates. A word in
language A is said to be cognate with word in lan-
guage B if the forms shared a common ancestor
in the parent language of A and B. In the cognate-
counting method, inter-language distance depends
on the lexical forms of the languages. The dis-
tance between two languages is a function of the
number or fraction of these forms which are cog-
nate between the two languages1 . This approach
to building language taxonomies is hard to imple-
ment in toto because constructing ancestor forms
is not easily automatable.
More recent approaches, such as Kondrak?s
(2002) and Heggarty et als (2005) work on di-
alect comparison, take the synchronic word forms
themselves as the language aspect to be compared.
Variations on edit distance (see Kessler (2005) for
a survey) are then used to evaluate differences be-
tween languages for each word, and these differ-
ences are aggregated to give a distance between
languages or dialects as a whole. This approach
is largely automatable, although some methods do
require human intervention.
In this paper, we present novel answers to the
two questions. The features of language we will
compare are not sets of words or phonological
forms. Instead we compare the similarities be-
tween forms, expressed as confusion probabilities.
The distribution of confusion probabilities in one
language is called a lexical metric. Section 2
presents the definition of lexical metrics and some
arguments for their being good language represen-
tatives for the purposes of comparison.
The distance between two languages is the di-
vergence their lexical metrics. In section 3, we
detail two methods for measuring this divergence:
1McMahon and McMahon (2003) for an account of tree-
inference from the cognate percentages in the Dyen et al
(1992) data.
273
Kullback-Liebler (herafter KL) divergence and
Rao distance. The subsequent section (4) de-
scribes the application of our approach to automat-
ically constructing a taxonomy of Indo-European
languages from Dyen et al (1992) data.
Section 5 suggests how lexical metrics can help
identify cognates. The final section (6) presents
our conclusions, and discusses possible future di-
rections for this work.
Versions of the software and data files described
in the paper will be made available to coincide
with its publication.
2 Lexical Metric
The first question posed by the distance-based ap-
proach to genetic language taxonomy is: what
should we compare?
In some approaches (Kondrak, 2002; McMahon
et al, 2005; Heggarty et al, 2005; Nerbonne and
Heeringa, 1997), the answer to this question is that
we should compare the phonetic or phonological
realisations of a particular set of meanings across
the range of languages being studied. There are
a number of problems with using lexical forms in
this way.
Firstly, in order to compare forms from differ-
ent languages, we need to embed them in com-
mon phonetic space. This phonetic space provides
granularity, marking two phones as identical or
distinct, and where there is a graded measure of
phonetic distinction it measures this.
There is growing doubt in the field of phonol-
ogy and phonetics about the meaningfulness of as-
suming of a common phonetic space. Port and
Leary (2005) argue convincingly that this assump-
tion, while having played a fundamental role in
much recent linguistic theorising, is nevertheless
unfounded. The degree of difference between
sounds, and consequently, the degree of phonetic
difference between words can only be ascertained
within the context of a single language.
It may be argued that a common phonetic space
can be found in either acoustics or degrees of free-
dom in the speech articulators. Language-specific
categorisation of sound, however, often restruc-
tures this space, sometimes with distinct sounds
being treated as homophones. One example of
this is the realisation of orthographic rr in Euro-
pean Portuguese: it is indifferently realised with
an apical or a uvular trill, different sounds made at
distinct points of articulation.
If there is no language-independent, common
phonetic space with an equally common similar-
ity measure, there can be no principled approach
to comparing forms in one language with those of
another.
In contrast, language-specific word-similarity is
well-founded. A number of psycholinguistic mod-
els of spoken word recognition (Luce et al, 1990)
are based on the idea of lexical neighbourhoods.
When a word is accessed during processing, the
other words that are phonemically or orthograph-
ically similar are also activated. This effect can
be detected using experimental paradigms such as
priming.
Our approach, therefore, is to abandon the
cross-linguistic comparison of phonetic realisa-
tions, in favour of language-internal comparison
of forms. (See also work by Shillcock et al (2001)
and Tamariz (2005)).
2.1 Confusion probabilities
One psychologically well-grounded way of de-
scribing the similarity of words is in terms of their
confusion probabilities. Two words have high
confusion probability if it is likely that one word
could be produced or understood when the other
was intended. This type of confusion can be mea-
sured experimentally by giving subjects words in
noisy environments and measuring what they ap-
prehend.
A less pathological way in which confusion
probability is realised is in coactivation. If a per-
son hears a word, then they more easily and more
quickly recognise similar words. This coactiva-
tion occurs because the phonological realisation
of words is not completely separate in the mind.
Instead, realisations are interdependent with reali-
sations of similar words.
We propose that confusion probabilities are
ideal information to constitute the lexical met-
ric. They are language-specific, psychologically
grounded, can be determined by experiment, and
integrate with existing psycholinguistic models of
word recognition.
2.2 NAM and beyond
Unfortunately, experimentally determined confu-
sion probabilities for a large number of languages
are not available. Fortunately, models of spoken
word recognition allow us to predict these proba-
bilities from easily-computable measures of word
similarity.
274
For example, the neighbourhood activation
model (NAM) (Luce et al, 1990; Luce and Pisoni,
1998) predicts confusion probabilities from the
relative frequency of words in the neighbourhood
of the target. Words are in the neighbourhood of
the target if their Levenstein (1965) edit distance
from the target is one. The more frequent the word
is, the greater its likelihood of replacing the target.
Bailey and Hahn (2001) argue, however, that the
all-or-nothing nature of the lexical neighbourhood
is insufficient. Instead word similarity is the com-
plex function of frequency and phonetic similarity
shown in equation (1). Here A,B,C and D are
constants of the model, u and v are words, and d
is a phonetic similarity model.
s = (AF (u)2 + BF (u) + C)e?D.d(u,v) (1)
We have adapted this model slightly, in line with
NAM, taking the similarity s to be the probabil-
ity of confusing stimulus v with form u. Also, as
our data usually offers no frequency information,
we have adopted the maximum entropy assump-
tion, namely, that all relative frequencies are equal.
Consequently, the probability of confusion of two
words depends solely on their similarity distance.
While this assumption degrades the psychological
reality of the model, it does not render it useless, as
the similarity measure continues to provide impor-
tant distinctions in neighbourhood confusability.
We also assume for simplicity, that the constant
D has the value 1.
With these simplifications, equation (2) shows
the probability of apprehending word w, out of
a set W of possible alternatives, given a stimulus
word ws.
P (w|ws) = e?d(w,ws)/N(ws) (2)
The normalising constant N(s) is the sum of the
non-normalised values for e?d(w,ws) for all words
w.
N(ws) =
?
w?W
e?d(u,v)
2.3 Scaled edit distances
Kidd and Watson (1992) have shown that discrim-
inability of frequency and of duration of tones in
a tone sequence depends on its length as a pro-
portion of the length of the sequence. Kapatsinski
(2006) uses this, with other evidence, to argue that
word recognition edit distances must be scaled by
word-length.
There are other reasons for coming to the same
conclusion. The simple Levenstein distance exag-
gerates the disparity between long words in com-
parison with short words. A word of consisting of
10 symbols, purely by virtue of its length, will on
average be marked as more different from other
words than a word of length two. For example,
Levenstein distance between interested and rest is
six, the same as the distance between rest and by,
even though the latter two have nothing in com-
mon. As a consequence, close phonetic transcrip-
tions, which by their very nature are likely to in-
volve more symbols per word, will result in larger
edit distances than broad phonemic transcriptions
of the same data.
To alleviate this problem, we define a new edit
distance function d2 which scales Levenstein dis-
tances by the average length of the words being
compared (see equation 3). Now the distance be-
tween interested and rest is 0.86, while that be-
tween rest and by is 2.0, reflecting the greater rel-
ative difference in the second pair.
d2(w2, w1) =
2d(w2, w1)
|w1|+ |w2|
(3)
Note that by scaling the raw edit distance with
the average lengths of the words, we are preserv-
ing the symmetric property of the distance mea-
sure.
There are other methods of comparing strings,
for example string kernels (Shawe-Taylor and
Cristianini, 2004), but using Levenstein distance
keeps us coherent with the psycholinguistic ac-
counts of word similarity.
2.4 Lexical Metric
Bringing this all together, we can define the lexical
metric.
A lexicon L is a mapping from a set of mean-
ings M , such as ?DOG?, ?TO RUN?, ?GREEN?,
etc., onto a set F of forms such as /pies/, /biec/,
/zielony/.
The confusion probability P of m1 for m2 in
lexical L is the normalised negative exponential
of the scaled edit-distance of the corresponding
forms. It is worth noting that when frequencies
are assumed to follow the maximum entropy dis-
tribution, this connection between confusion prob-
abilities and distances (see equation 4) is the same
as that proposed by Shepard (1987).
275
P (m1|m2;L) =
e?d2(L(m1),L(m2))
N(m2;L)
(4)
A lexical metric of L is the mapping LM(L) :
M2 ? [0, 1] which assigns to each pair of mean-
ings m1,m2 the probability of confusing m1 for
m2, scaled by the frequency of m2.
LM(L)(m1,m2)
= P (L(m1)|L(m2))P (m2)
= e
?d2(L(m1),L(m2))
N(m2;L)|M |
where N(m2;L) is the normalising function de-
fined in equation (5).
N(m2;L) =
?
m?M
e?d2(L(m),L(m2)) (5)
Table 1 shows a minimal lexicon consisting only
of the numbers one to five, and a corresponding
lexical metric. The values in the lexical metric are
one two three four five
one 0.102 0.027 0.023 0.024 0.024
two 0.028 0.107 0.024 0.026 0.015
three 0.024 0.024 0.107 0.023 0.023
four 0.025 0.025 0.022 0.104 0.023
five 0.026 0.015 0.023 0.025 0.111
Table 1: A lexical metric on a mini-lexicon con-
sisting of the numbers one to five.
inferred word confusion probabilities. The matrix
is normalised so that the sum of each row is 0.2,
ie. one-fifth for each of the five words, so the total
of the matrix is one. Note that the diagonal values
vary because the off-diagonal values in each row
vary, and consequently, so does the normalisation
for the row.
3 Language-Language Distance
In the previous section, we introduced the lexi-
cal metric as the key measurable for comparing
languages. Since lexical metrics are probability
distributions, comparison of metrics means mea-
suring the difference between probability distri-
butions. To do this, we use two measures: the
symmetric Kullback-Liebler divergence (Jeffreys,
1946) and the Rao distance (Rao, 1949; Atkinson
and Mitchell, 1981; Micchelli and Noakes, 2005)
based on Fisher Information (Fisher, 1959). These
can be defined in terms the geometric path from
one distribution to another.
3.1 Geometric paths
The geometric path between two distributions P
and Q is a conditional distribution R with a con-
tinuous parameter ? such that at ? = 0, the distri-
bution is P , and at ? = 1 it is Q. This conditional
distribution is called the geometric because it con-
sists of normalised weighted geometric means of
the two defining distributions (equation 6).
R(w?|?) = P (w?)?Q(w?)1??/k(?;P,Q) (6)
The function k(?;P,Q) is a normaliser for the
conditional distribution, being the sum of the
weighted geometric means of values from P and
Q (equation 7). This value is known as the
Chernoff coefficient or Helliger path (Basseville,
1989). For brevity, the P,Q arguments to k will
be treated as implicit and not expressed in equa-
tions.
k(?) =
?
w??W 2
P (w?)1??Q(w?)? (7)
3.2 Kullback-Liebler distance
The first-order (equation 8) differential of the nor-
maliser with regard to ? is of particular interest.
k?(?) =
?
w??W 2
log Q(w?)P (w?)P (w?)
1??Q(w?)? (8)
At ? = 0, this value is the negative of the
Kullback-Liebler distance KL(P |Q) of Q with re-
gard to P (Basseville, 1989). At ? = 1, it is the
Kullback-Liebler distance KL(Q|P ) of P with re-
gard to Q. Jeffreys? (1946) measure is a symmetri-
sation of KL distance, by averaging the commuta-
tions (equations 9,10).
KL(P,Q) = KL(Q|P ) + KL(P |Q)2 (9)
= k
?(1)? k?(0)
2 (10)
3.3 Rao distance
Rao distance depends on the second-order (equa-
tion 11) differential of the normaliser with regard
to ?.
k??(?) =
?
w??W 2
log2 Q(w?)P (w?)P (w?)
1??Q(w?)?
(11)
Fisher information is defined as in equation (12).
FI(P, x) = ?
? ?2 log P (y|x)
?x2 P (y|x)dy (12)
276
Equation (13) expresses Fisher information along
the path R from P to Q at point ? using k and its
first two derivatives.
FI(R,?) = k(?)k
??(?)? k?(?)2
k(?)2 (13)
The Rao distance r(P,Q) along R can be approxi-
mated by the square root of the Fisher information
at the path?s midpoint ? = 0.5.
r(P,Q) =
?
k(0.5)k??(0.5) ? k?(0.5)2
k(0.5)2 (14)
3.4 The PHILOLOGICON algorithm
Bringing these pieces together, the PHILOLOGI-
CON algorithm for measuring the divergence be-
tween two languages has the following steps:
1. determine their joint confusion probability
matrices, P and Q,
2. substitute these into equation (7), equation
(8) and equation (11) to calculate k(0),
k(0.5), k(1), k?(0.5), and k??(0.5),
3. and put these into equation (10) and equation
(14) to calculate the KL and Rao distances
between between the languages.
4 Indo-European
The ideal data for reconstructing Indo-European
would be an accurate phonemic transcription of
words used to express specifically defined mean-
ings. Sadly, this kind of data is not readily avail-
able. However, as a stop-gap measure, we can
adopt the data that Dyen et al collected to con-
struct a Indo-European taxonomy using the cog-
nate method.
4.1 Dyen et als data
Dyen et al (1992) collected 95 data sets, each pair-
ing a meaning from a Swadesh (1952)-like 200-
word list with its expression in the corresponding
language. The compilers annotated with data with
cognacy relations, as part of their own taxonomic
analysis of Indo-European.
There are problems with using Dyen?s data for
the purposes of the current paper. Firstly, the word
forms collected are not phonetic, phonological or
even full orthographic representations. As the au-
thors state, the forms are expressed in sufficient
detail to allow an interested reader acquainted with
the language in question to identify which word is
being expressed.
Secondly, many meanings offer alternative
forms, presumably corresponding to synonyms.
For a human analyst using the cognate approach,
this means that a language can participate in two
(or more) word-derivation systems. In preparing
this data for processing, we have consistently cho-
sen the first of any alternatives.
A further difficulty lies in the fact that many lan-
guages are not represented by the full 200 mean-
ings. Consequently, in comparing lexical metrics
from two data sets, we frequently need to restrict
the metrics to only those meanings expressed in
both the sets. This means that the KL divergence
or the Rao distance between two languages were
measured on lexical metrics cropped and rescaled
to the meanings common to both data-sets. In
most cases, this was still more than 190 words.
Despite these mismatches between Dyen et al?s
data and our needs, it provides an testbed for the
PHILOLOGICON algorithm. Our reasoning being,
that if successful with this data, the method is rea-
sonably reliable. Data was extracted to language-
specific files, and preprocessed to clean up prob-
lems such as those described above. An additional
data-set was added with random data to act as an
outlier to root the tree.
4.2 Processing the data
PHILOLOGICON software was then used to calcu-
late the lexical metrics corresponding to the indi-
vidual data files and to measure KL divergences
and Rao distances between them. The program
NEIGHBOR from the PHYLIP2 package was used
to construct trees from the results.
4.3 The results
The tree based on Rao distances is shown in figure
1. The discussion follows this tree except in those
few cases mentioning differences in the KL tree.
The standard against which we measure the suc-
cess of our trees is the conservative traditional tax-
onomy to be found in the Ethnologue (Grimes
and Grimes, 2000). The fit with this taxonomy
was so good that we have labelled the major
branches with their traditional names: Celtic, Ger-
manic, etc. In fact, in most cases, the branch-
internal divisions ? eg. Brythonic/Goidelic in
Celtic, Western/Eastern/Southern in Slavic, or
2See http://evolution.genetics.washington.edu/phylip.html.
277
Western/Northern in Germanic ? also accord.
Note that PHILOLOGICON even groups Baltic and
Slavic together into a super-branch Balto-Slavic.
Where languages are clearly out of place in
comparison to the traditional taxonomy, these are
highlighted: visually in the tree, and verbally in
the following text. In almost every case, there are
obvious contact phenomena which explain the de-
viation from the standard taxonomy.
Armenian was grouped with the Indo-Iranian
languages. Interestingly, Armenian was at first
thought to be an Iranian language, as it shares
much vocabulary with these languages. The com-
mon vocabulary is now thought to be the result
of borrowing, rather than common genetic origin.
In the KL tree, Armenian is placed outside of the
Indo-Iranian languages, except for Gypsy. On the
other hand, in this tree, Ossetic is placed as an
outlier of the Indian group, while its traditional
classification (and the Rao distance tree) puts it
among the Iranian languages. Gypsy is an Indian
language, related to Hindi. It has, however, been
surrounded by European languages for some cen-
turies. The effects of this influence is the likely
cause for it being classified as an outlier in the
Indo-Iranian family. A similar situation exists for
Slavic: one of the two lists that Dyen et al of-
fer for Slovenian is classed as an outlier in Slavic,
rather than classifying it with the Southern Slavic
languages. The other Slovenian list is classified
correctly with Serbocroatian. It is possible that
the significant impact of Italian on Slovenian has
made it an outlier. In Germanic, it is English that
is the outlier. This may be due to the impact of the
English creole, Takitaki, on the hierarchy. This
language is closest to English, but is very distinct
from the rest of the Germanic languages. Another
misclassification also is the result of contact phe-
nomena. According to the Ethnologue, Sardinian
is Southern Romance, a separate branch from Ital-
ian or from Spanish. However, its constant contact
with Italian has influenced the language such that
it is classified here with Italian. We can offer no
explanation for why Wakhi ends up an outlier to
all the groups.
In conclusion, despite the noisy state of Dyen et
al.?s data (for our purposes), the PHILOLOGICON
generates a taxonomy close to that constructed us-
ing the traditional methods of historical linguis-
tics. Where it deviates, the deviation usually
points to identifiable contact between languages.
G
r
e
e
k
I
n
d
o
?
I
r
a
n
i
a
n
A
l
b
a
n
i
a
n
B
a
l
t
o
?
S
l
a
v
i
c
G
e
r
m
a
n
i
c
R
o
m
a
n
c
e
C
e
l
t
i
c
Wakhi
Greek D
Greek MD
Greek ML
Greek Mod
Greek K
Afghan
Waziri
Armenian List
Baluchi
Persian List
Tadzik
Ossetic
Bengali
Hindi
Lahnda
Panjabi ST
Gujarati
Marathi
Khaskura
Nepali List
Kashmiri
Singhalese
Gypsy Gk
ALBANIAN
Albanian G
Albanian C
Albanian K
Albanian Top
Albanian T
Bulgarian
BULGARIAN P
MACEDONIAN P
Macedonian
Serbocroatian
SERBOCROATIAN P
SLOVENIAN P
Byelorussian
BYELORUSSIAN P
Russian
RUSSIAN P
Ukrainian
UKRAINIAN P
Czech
CZECH P
Slovak
SLOVAK P
Czech E
Lusatian L
Lusatian U
Polish
POLISH P
Slovenian
Latvian
Lithuanian O
Lithuanian ST
Afrikaans
Dutch List
Flemish
Frisian
German ST
Penn Dutch
Danish
Riksmal
Swedish List
Swedish Up
Swedish VL
Faroese
Icelandic ST
English ST
Takitaki
Brazilian
Portuguese ST
Spanish
Catalan
Italian
Sardinian L
Sardinian N
Ladin
French
Walloon
Provencal
French Creole C
French Creole D
Rumanian List
Vlach
Breton List
Breton ST
Breton SE
Welsh C
Welsh N
Irish A
Irish B
Random
Armenian Mod
Sardinian C
Figure 1: Taxonomy of 95 Indo-European data
sets and artificial outlier using PHILOLOGICON
and PHYLIP
278
5 Reconstruction and Cognacy
Subsection 3.1 described the construction of geo-
metric paths from one lexical metric to another.
This section describes how the synthetic lexical
metric at the midpoint of the path can indicate
which words are cognate between the two lan-
guages.
The synthetic lexical metric (equation 15) ap-
plies the formula for the geometric path equation
(6) to the lexical metrics equation (5) of the lan-
guages being compared, at the midpoint ? = 0.5.
R 1
2
(m1,m2) =
?
P (m1|m2)Q(m1|m2)
|M |k(12 )
(15)
If the words for m1 and m2 in both languages have
common origins in a parent language, then it is
reasonable to expect that their confusion probabil-
ities in both languages will be similar. Of course
different cognate pairs m1,m2 will have differing
values for R, but the confusion probabilities in P
and Q will be similar, and consequently, the rein-
force the variance.
If either m1 or m2, or both, is non-cognate, that
is, has been replaced by another arbitrary form
at some point in the history of either language,
then the P and Q for this pair will take indepen-
dently varying values. Consequently, the geomet-
ric mean of these values is likely to take a value
more closely bound to the average, than in the
purely cognate case.
Thus rows in the lexical metric with wider dy-
namic ranges are likely to correspond to cognate
words. Rows corresponding to non-cognates are
likely to have smaller dynamic ranges. The dy-
namic range can be measured by taking the Shan-
non information of the probabilities in the row.
Table 2 shows the most low- and high-
information rows from English and Swedish
(Dyen et als (1992) data). At the extremes of
low and high information, the words are invari-
ably cognate and non-cognate. Between these ex-
tremes, the division is not so clear cut, due to
chance effects in the data.
6 Conclusions and Future Directions
In this paper, we have presented a distance-
based method, called PHILOLOGICON, that con-
structs genetic trees on the basis of lexica
from each language. The method only com-
pares words language-internally, where compari-
son seems both psychologically real and reliable,
English Swedish 104(h? h?)
Low Information
we vi -1.30
here her -1.19
to sit sitta -1.14
to flow flyta -1.04
wide vid -0.97
:
scratch klosa 0.78
dirty smutsig 0.79
left (hand) vanster 0.84
because emedan 0.89
High Information
Table 2: Shannon information of confusion dis-
tributions in the reconstruction of English and
Swedish. Information levels are shown translated
so that the average is zero.
and never cross-linguistically, where comparison
is less well-founded. It uses measures founded
in information theory to compare the intra-lexical
differences.
The method successfully, if not perfectly, recre-
ated the phylogenetic tree of Indo-European lan-
guages on the basis of noisy data. In further work,
we plan to improve both the quantity and the qual-
ity of the data. Since most of the mis-placements
on the tree could be accounted for by contact phe-
nomena, it is possible that a network-drawing,
rather than tree-drawing, analysis would produce
better results.
Likewise, we plan to develop the method
for identifying cognates. The key improvement
needed is a way to distinguish indeterminate dis-
tances in reconstructed lexical metrics from deter-
minate but uniform ones. This may be achieved by
retaining information about the distribution of the
original values which were combined to form the
reconstructed metric.
References
C. Atkinson and A.F.S. Mitchell. 1981. Rao?s distance
measure. Sankhya?, 4:345?365.
Todd M. Bailey and Ulrike Hahn. 2001. Determinants
of wordlikeness: Phonotactics or lexical neighbor-
hoods? Journal of Memory and Language, 44:568?
591.
Michle Basseville. 1989. Distance measures for signal
processing and pattern recognition. Signal Process-
ing, 18(4):349?369, December.
279
D. Benedetto, E. Caglioti, and V. Loreto. 2002. Lan-
guage trees and zipping. Physical Review Letters,
88.
Isidore Dyen, Joseph B. Kruskal, and Paul Black.
1992. An indo-european classification: a lexicosta-
tistical experiment. Transactions of the American
Philosophical Society, 82(5).
R.A. Fisher. 1959. Statistical Methods and Scientific
Inference. Oliver and Boyd, London.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the ana-
tolian theory of indo-european origin. Nature,
426:435?439.
B.F. Grimes and J.E. Grimes, editors. 2000. Ethno-
logue: Languages of the World. SIL International,
14th edition.
Paul Heggarty, April McMahon, and Robert McMa-
hon, 2005. Perspectives on Variation, chapter From
phonetic similarity to dialect classification. Mouton
de Gruyter.
H. Jeffreys. 1946. An invariant form for the prior prob-
ability in estimation problems. Proc. Roy. Soc. A,
186:453?461.
Vsevolod Kapatsinski. 2006. Sound similarity rela-
tions in the mental lexicon: Modeling the lexicon as
a complex network. Technical Report 27, Indiana
University Speech Research Lab.
Brett Kessler. 2005. Phonetic comparison algo-
rithms. Transactions of the Philological Society,
103(2):243?260.
Gary R. Kidd and C.S. Watson. 1992. The
?proportion-of-the-total-duration rule for the dis-
crimination of auditory patterns. Journal of the
Acoustic Society of America, 92:3109?3118.
Grzegorz Kondrak. 2002. Algorithms for Language
Reconstruction. Ph.D. thesis, University of Toronto.
V.I. Levenstein. 1965. Binary codes capable of cor-
recting deletions, insertions and reversals. Doklady
Akademii Nauk SSSR, 163(4):845?848.
Paul Luce and D. Pisoni. 1998. Recognizing spoken
words: The neighborhood activation model. Ear
and Hearing, 19:1?36.
Paul Luce, D. Pisoni, and S. Goldinger, 1990. Cog-
nitive Models of Speech Perception: Psycholinguis-
tic and Computational Perspectives, chapter Simi-
larity neighborhoods of spoken words, pages 122?
147. MIT Press, Cambridge, MA.
April McMahon and Robert McMahon. 2003. Find-
ing families: quantitative methods in language clas-
sification. Transactions of the Philological Society,
101:7?55.
April McMahon, Paul Heggarty, Robert McMahon,
and Natalia Slaska. 2005. Swadesh sublists and the
benefits of borrowing: an andean case study. Trans-
actions of the Philological Society, 103(2):147?170.
Charles A. Micchelli and Lyle Noakes. 2005. Rao dis-
tances. Journal of Multivariate Analysis, 92(1):97?
115.
Luay Nakleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of
phylogenetic reconstruction methods on an ie
dataset. Transactions of the Philological Society,
103(2):171?192.
J. Nerbonne and W. Heeringa. 1997. Measuring
dialect distance phonetically. In Proceedings of
SIGPHON-97: 3rd Meeting of the ACL Special In-
terest Group in Computational Phonology.
B. Port and A. Leary. 2005. Against formal phonology.
Language, 81(4):927?964.
C.R. Rao. 1949. On the distance between two popula-
tions. Sankhya?, 9:246?248.
D. Ringe, Tandy Warnow, and A. Taylor. 2002. Indo-
european and computational cladistics. Transac-
tions of the Philological Society, 100(1):59?129.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
R.N. Shepard. 1987. Toward a universal law of gen-
eralization for physical science. Science, 237:1317?
1323.
Richard C. Shillcock, Simon Kirby, Scott McDonald,
and Chris Brew. 2001. Filled pauses and their status
in the mental lexicon. In Proceedings of the 2001
Conference of Disfluency in Spontaneous Speech,
pages 53?56.
M. Swadesh. 1952. Lexico-statistic dating of prehis-
toric ethnic contacts. Proceedings of the American
philosophical society, 96(4).
Monica Tamariz. 2005. Exploring the Adaptive Struc-
ture of the Mental Lexicon. Ph.D. thesis, University
of Edinburgh.
280
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 1?5,
Prague, June 2007. c?2007 Association for Computational Linguistics
Computing and Historical Phonology
John Nerbonne
Alfa-Informatica
University of Groningen
j.nerbonne@rug.nl
T. Mark Ellison
Informatics
University of Western Australia
mark@markellison.net
Grzegorz Kondrak
Computing Science
University of Alberta
kondrak@cs.ualberta.ca
Abstract
We introduce the proceedings from the
workshop ?Computing and Historical Pho-
nology: 9th Meeting of the ACL Special In-
terest Group for Computational Morphology
and Phonology?.
1 Background
Historical phonology is the study of how the sounds
and sound systems of a language evolve, and in-
cludes research issues concerning the triggering of
sound changes; their temporal and geographic prop-
agation (including lexical diffusion); the regular-
ity/irregularity of sound change, and its interaction
with morphological change; the role of borrowing
and analogy in sound change; the interaction of
sound change with the phonemic system (poten-
tially promoting certain changes, but also neutral-
izing phonemic distinctions); and the detection of
these phenomena in historical documents.
There is a substantial and growing body of work
applying computational techniques of various sorts
to problems in historical phonology. We mention a
few here to give the flavor of the sort of work we
hoped to attract for presentation in a coherent SIG-
MORPHON workshop. Kessler (2001) estimates
the likelihood of chance phonemic correspondences
using permutation statistics; Kondrak (2002) devel-
ops algorithms to detect cognates and sound corre-
spondences; McMahon and McMahon (2005) and
also Nakhleh, Ringe and Warnow (2005) apply phy-
logenetic techniques to comparative reconstruction;
and Ellison and Kirby (2006) suggest means of de-
tecting relationships which do not depend on word
by word comparisons. But we likewise wished to
draw on the creativity of the computational linguis-
tics (CL) community to see which other important
problems in historical phonology might also be ad-
dressed computationally (see below).
There has recently been a good deal of computa-
tional work in historical linguistics involving phylo-
genetic inference, i.e., the inference to the genealog-
ical tree which best explains the historical develop-
ments (Gray and Atkinson, 2003; Dunn et al, 2005).
While the application of phylogenetic analysis has
not universally been welcomed with open philolog-
ical arms (Holm, 2007), it has attracted a good deal
of attention, some of which we hoped to engage. We
take no stand on these controversies here, but note
that computing may be employed in historical lin-
guistics, and in particular in historical phonology in
a more versatile way, its uses extending well beyond
phylogenetic inference.
2 Introduction
The workshop thus brings together researchers inter-
ested in applying computational techniques to prob-
lems in historical phonology. We deliberately de-
fined the scope of the workshop broadly to include
problems such as identifying spelling variants in
older manuscripts, searching for cognates, hypothe-
sizing and confirming sound changes and/or sound
correspondences, modeling likely sound changes,
the relation of synchronic social and geographic
variation to historical change, the detection of pho-
netic signals of relatedness among potentially re-
lated languages, phylogenetic reconstruction based
on sound correspondences among languages, dating
1
historical changes, or others.
We were emphatically open to proposals to ap-
ply techniques from other areas to problems in his-
torical phonology such as applying work on confus-
able product names to the modeling of likely sound
correspondences or the application of phylogenetic
analysis from evolutionary biology to the problem
of phonological reconstruction.
3 Papers
We provide a preview to some of the issues in the
papers in this bundle.
Brett Kessler?s invited contribution sketches the
opportunities for multiple string alignment, which
would be extremely useful in historical phono-
logy, but which is also technically so challenging
that Gusfield (1999, Ch. 14) refers to it as ?the
holy grail? (of algorithms on strings, trees, and se-
quences).
3.1 Identification of Cognates
T. Mark Ellison combines Bayes?s theorem with gra-
dient descent in a method for finding cognates and
correspondences. A formal model of language is ex-
tended to include the notion of parent languages, and
a mechanism whereby parent languages project onto
their descendents. This model allows the quantifica-
tion of the probability of word lists in two languages
given a common ancestor which was the source for
some of the words. Bayes?s theorem reverses this
expression into the evaluation of possible parent lan-
guages. Gradient descent finds the best, or at least a
good one, of these. The method is shown to find
cognates in data from Russian and Polish.
Grzegorz Kondrak, David Beck and Philip Dilts
apply algorithms for the identification of cognates
and recurrent sound correspondences proposed by
Kondrak (2002) to the Totonac-Tepehua family of
indigenous languages in Mexico. Their long-term
objective is providing tools for rapid construction
of comparative dictionaries for relatively unfamiliar
language families. They show that by combining ex-
pert linguistic knowledge with computational analy-
sis, it is possible to quickly identify a large number
of cognate sets across related languages. The ex-
periments led to the creation of the initial version of
an etymological dictionary. The authors hope that
the dictionary will facilitate the reconstruction of
a more accurate Totonac-Tepehua family tree, and
shed light on the problem of the family origins and
migratory patterns.
Michael Cysouw and Hagen Jung use an itera-
tive process of alignment between words in differ-
ent languages in an attempt to identify cognates. In-
stead of using consistently coded phonemic (or pho-
netic) transciption, they use practical orthographies,
which has the advantage of being applicable without
expensive and error-prone manual processing. Pro-
ceeding from semantically equivalent words in the
Intercontinental Dictionary Series (IDS) database,
the program aligns letters using a variant of edit
distance that includes correspondences of one let-
ter with two or more, (?multi-n-gram?). Once ini-
tial alignments are obtained, segment replacement
costs are inferred. This process of alignment and
inferring segment replacement costs may then be
iterated. They succeed in distinguishing noise on
the one hand from borrowings and cognates on the
other, and the authors speculate about being able to
distinguish inherited cognates from borrowings.
3.2 A View from Dialectology
Several papers examined language change from the
point of view of dialectology. While the latter stud-
ies variation in space, the former studies variation
over time.
Hans Goebl, the author of hundreds of papers ap-
plying quantitative analysis to the analysis of lin-
guistic varieties in dialects, applies his dialectomet-
ric techniques both to modern material (1900) from
the Atlas Linguistique de France and to material dat-
ing from approximate 1300 provided by Dutch Ro-
manists. Dialectometry aims primarily at establish-
ing the aggregate distances (or conversely, similari-
ties), and Goebl?s analysis shows that these have re-
main relatively constant even while the French lan-
guage has changed a good deal. The suggestion is
that geography is extremely influential.
Wilbert Heeringa and Brian Joseph first recon-
struct a protolanguage based on Dutch dialect data,
which they compare to the proto-Germanic found in
a recent dictionary, demonstrating that their recon-
struction is quite similar to the proto-Germanic, even
though it is only based on a single branch of a large
family. They then apply a variant of edit distance to
2
the pronunciation of the protolanguage, comparing
it to the pronunciation in modern Dutch dialects, al-
lowing on the one hand a quantitative evaluation of
the degree to which ?proto-Dutch? correlates with
proto-Germanic (r = 0.87), and a sketch of conser-
vative vs. innovative dialect areas in the Netherlands
on the other.
Anil Singh and Harshit Surana ask whether
corpus-based measures can be used to compare lan-
guages. Most research has proceeded from the as-
sumption that lists of word pairs be available, as in-
deed they normally are in the case of dialect atlas
data or as they often may be obtained by construct-
ing lexicalizations of the concepts in the so-called
?Swadesh? list. But such data is not always avail-
able, nor is it straightforward to construct. Singh and
Surana construct n-gram models of order five (5),
and compare Indo-Iranian and Dravidian languages
based on symmetric cross-entropy.
Martijn Wieling, Therese Leinonen and John Ner-
bonne apply PAIR HIDDEN MARKOV MODELS
(PHMM), introduced to CL by Mackay and Kon-
drak (2005), to a large collection of Dutch dialect
pronunciations in an effort to learn the degree of
segment differentiation. Essentially the PHMM re-
gards frequently aligned segments as more similar,
and Wieling et al show that the induced similar-
ity indeed corresponds to phonetic similarity in the
case of vowels, whose acoustic properties facilitate
the assessment of similarity.
3.3 Views from other Perspectives
Several papers examined diachronic change from
well-developed perspectives outside of historical
linguistics, including evolution and genetic algo-
rithms, language learning, biological cladistics, and
the structure of vowel systems.
Monojit Choudhury, Vaibhav Jalan, Sudeshna
Sarkar and Anupam Basu distinguish two compo-
nents in language developments, on the one hand
functional forces or constraints including ease of
articulation, perceptual contrast, and learnability,
which are modeled by the fitness function of a ge-
netic algorithm (GA). On the other hand, these func-
tional forces operate against the background of lin-
guistic structure, which the authors dub ?genotype?
phenotype mapping?, and which is realized by the
set of forms in a given paradigm, and a small set
of possible atomic changes which map from form
set to form set. They apply these ideas to morpho-
logical changes in dialects of Bengali, an agglutina-
tive Indic language, and they are able to show that
some modern dialects are optimal solutions to the
functional constraints in the sense that any further
changes would be worse with respect to at least one
of the constraints.
Eric Smith applies the gradual learning algorithm
(GLA) developed in Optimality Theory by Paul
Boersma to the problem of reconstructing a dead
language. In particular the GLA is deployed to de-
duce the phonological representations of a dead lan-
guage, Elamite, from the orthography, where the
orthography is treated as the surface representation
and the phonological representation as the underly-
ing representation. Elamite was spoken in south-
western and central Iran, and survives in texts dating
from 2400? 360 BCE, written in a cuneiform script
borrowed from Sumerians and Akkadians. Special
attention is paid to the difficult mapping between or-
thography and phonology, and to OT?s Lexicon Op-
timization module.
Antonella Gaillard-Corvaglia, Jean-Le?o Le?onard
and Pierre Darlu apply cladistic analysis to dialect
networks and language phyla, using the detailed in-
formation in phonetic changes to increase the re-
solution beyond what is possible with simple word
lists. They examine Gallo-Romance vowels, south-
ern Italo-Romance dialects and Mayan languages,
foregoing analyses of relatedness based on global
resemblance between languages, and aiming instead
to view recurrent phonological changes as first-class
entities in the analysis of historical phonology with
the ambition of including the probability of specific
linguistic changes in analyses.
Animesh Mukherjee, Monojit Choudhury, Anu-
pam Basu and Niloy Ganguly examine the struc-
ture of vowel systems by defining a weighted net-
work where vowels are represented by the nodes
and the likelihood of vowels? co-occurring in the
languages of the world by weighted edges be-
tween nodes. Using data from the 451 lan-
guages in the UCLA Phonological Segment Inven-
tory Database (UPSID), Mukherjee and colleagues
seek high-frequency symmetric triplets (with sim-
ilar co-occurrence weights). The vowel networks
which emerged tend to organize themselves to max-
3
imize contrast between the vowels when inventories
are small, but they tend to grow by systematically
applying the same contrasts (short vs long, oral vs
nasal) across the board when they grow larger.
3.4 Methodology
Finally, there were three papers focusing on more
general methodological issues, one on non-linearity,
one on a direct manipulation interface to cross-
tabulation, and one on visualizing distance mea-
sures.
Hermann Moisl has worked a great deal with the
Newcastle Electronic Corpus of Tyneside English
(NECTE). NECTE is a corpus of dialect speech
from Tyneside in North-East England which was
collected in an effort to represent not only geograph-
ical, but also social variation in speech. In the con-
tribution to this volume, Moisl addresses the prob-
lem of nonlinearity in data, using the distribution of
variance in the frequency of phonemes in NECTE
as an example. He suggests techniques for spotting
nonlinearity as well as techniques for analyzing data
which contains it.
Tyler Peterson and Gessiane Picanco experiment
with cross tabulation as an aid to phonemic re-
construction. In particular they use PIVOT TA-
BLES, which are cross tabulations supported by new
database packages, and which allow direct manipu-
lation, e.g., drag and drop methods of adding and re-
moving new sets of data, including columns or rows.
This makes it easier for the linguist to track e.g.
phoneme correspondences and develop hypotheses
about them. Tup?? stock is a South American lan-
guage family with about 60 members, mostly in
Brazil, but also in Bolivia and Paraguay. Pivot tables
were employed to examine this data, which resulted
in a reconstruction a great deal like the only pub-
lished reconstruction, but which nevertheless sug-
gested new possibilities.
Thomas Pilz, Axel Philipsenburg and Wolfram
Luther describe the development and use of an in-
terface for visually evaluating distance measures.
Using the problem of identifying intended modern
spellings from manuscript spellings using various
techniques, including edit distance, they note ex-
amples where the same distance measure performs
well on one set of manuscripts but poorly on another.
This motivates the need for easy evaluation of such
measures. The authors use multidimensional scal-
ing plots, histograms and tables to expose different
levels of overview and detail.
3.5 Other
Although this meeting of SIGMORPHON focused
on contributions to historical phonology, there was
also one paper on synchronic morphology.
Christian Monson, Alon Lavie, Jaime Carbonell
and Lori Levin describe ParaMor, a system aimed
at minimally supervised morphological analysis that
uses inflectional paradigms as its key concept.
ParaMor gathers sets of suffixes and stems that co-
occur, collecting each set of suffixes into a potential
inflectional paradigm. These candidate paradigms
then need to be compared and filtered to obtain a
minimal set of paradigms. Since there are many
hundreds of languages for which paradigm discov-
ery would be a very useful tool, ParaMor may be
interesting to researchers involved in language doc-
umentation. This paper sketches the authors? ap-
proach to the problem and presents evidence for
good performance in Spanish and German.
4 Prospects
As pleasing as it to hear of the progress reported
on in this volume, it is clear that there is a great
deal of interesting work ahead for those interested
in computing and historical phonology. This is im-
mediately clear if one compares the list of potential
topics noted in Sections 1-2 with the paper topics
actually covered, e.g. by skimming Section 3 or the
table of contents. For example we did not receive
submissions on the treatment of older documents, on
recognizing spelling variants, or on dating historical
changes.
In addition interesting topics may just now be ris-
ing above the implementation horizon, e.g. com-
putational techniques which strive to mimic inter-
nal reconstruction (Hock and Joseph, 1996), or those
which aim at characterizing general sound changes,
or perspectives which attempt to tease apart histori-
cal, areal and typological effects (Nerbonne, 2007).
In short, we are optimistic about interest in follow-
up workshops!
4
5 Acknowledgments
We are indebted to our program committee, to the in-
cidental reviewers named in the organizational sec-
tion of the book, and to some reviewers who re-
main anonymous. We also thank the SIGMOR-
PHON chair Jason Eisner and secretary Richard Wi-
centowski for facilitating our organization of this
workshop under the aegis of SIGMORPHON, the
special interest group in morphology and phonology
of the Association for Computational Linguistics.1
We thank Peter Kleiweg for managing the produc-
tion of the book. We are indebted to the Netherlands
Organization for Scientific Research (NWO), grant
235-89-001, for cooperation between the Center for
Language and Cognition, Groningen, and the De-
partment of Linguistics The Ohio State University,
for support of the work which is reported on here.
References
A. Michael Dunn, A. Terrill, Geert Reesink, and Stephen
Levinson. 2005. Structural phylogenetics and the
reconstruction of ancient language history. Science,
309(5743):2072?2075.
Mark Ellison and Simon Kirby. 2006. Measuring lan-
guage divergence by intra-lexical divergence. In Proc.
of ACL/COLING 2006, pages 273?280, Shroudsburg,
PA. ACL.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426(6965):435?439.
Dan Gusfield. 1999. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Bi-
ology. Cambridge Univesity Press, Cambridge.
Hans Henrich Hock and Brian D. Joseph. 1996. Lan-
guage History, Language Change, and Language Re-
lationship : An Introduction to Historical and Com-
parative Linguistics. Mouton de Gruyter, Berlin.
Hans J. Holm. 2007. The new arboretum of Indo-
European ?trees?: Can new algorithms reveal the phy-
logeny and even prehistory of IE? Journal of Quanti-
tative Linguistics, 14(2).
Brett Kessler. 2001. The Significance of Word Lists.
CSLI Press, Stanford.
Grzegorz Kondrak. 2002. Algorithms for Language Re-
construction. Ph.D. thesis, University of Toronto.
1http://nlp.cs.swarthmore.edu/sigphon/
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
paring word similarity and identifying cognates with
pair hidden markov models. In Proceedings of the 9th
Conference on Natural Language Learning (CoNLL),
pages 40?47, Shroudsburg, PA. ACL.
April McMahon and Robert McMahon. 2005. Language
Classification by Numbers. Oxford University Press,
Oxford.
Luay Nakleh, Don Ringe, and Tandy Warnow. 2005.
Perfect phylogentic networks: A new metholodology
for reconstructing the evolutionary history of natural
languages. Language, 81(2):382?420.
John Nerbonne. 2007. Review of April McMahon &
Robert McMahon Language Classification by Num-
bers. Oxford: OUP, 2005. Linguistic Typology, 11.
5
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 15?22,
Prague, June 2007. c?2007 Association for Computational Linguistics
Bayesian Identiation of Cognates and Correspondenes
T. Mark Ellison
Linguistis, University of Western Australia,
and Analith Ltd
markmarkellison.net
Abstrat
This paper presents a Bayesian approah
to omparing languages: identifying og-
nates and the regular orrespondenes
that ompose them. A simple model of
language is extended to inlude these no-
tions in an aount of parent languages.
An expression is developed for the pos-
terior probability of hild language forms
given a parent language. Bayes' Theo-
rem oers a shema for evaluating hoies
of ognates and orrespondenes to ex-
plain semantially mathed data. An im-
plementation optimising this value with
gradient desent is shown to distinguish
ognates from non-ognates in data from
Polish and Russian.
Modern historial linguistis addresses ques-
tions like the following. How did language
originate? What were historially-reorded lan-
guages like? How related are languages? What
were the anestors of modern languages like?
Reently, omputation has beome a key tool in
addressing suh questions.
Kirby (2002) gives an overview of urrent ur-
rent work on how language evolved, muh of it
based on omputational models and simulations.
Ellison (1992) presents a linguistially motivated
method for lassifying onsonants as onsonants
or vowels. An unexpeted result for the dead
language Gothi provides added weight to one
of two ompeting phonologial interpretations of
the orthography of this dead language.
Other reent work has applied omputational
methods for phylogenetis to measuring linguis-
ti distanes, and/or onstruting taxonomi
trees from distanes between languages and di-
alets (Dyen et al, 1992; Ringe et al, 2002; Gray
and Atkinson, 2003; MMahon and MMahon,
2003; Nakleh et al, 2005; Ellison and Kirby,
2006).
A entral fous of historial linguistis is the
reonstrution of parent languages from the ev-
idene of their desendents. In historial lin-
guistis proper, this is done by the ompara-
tive method (Jeers and Lehiste, 1989; Hok,
1991) in whih shared arbitrary struture is as-
sumed to reet ommon origin. At the phono-
logial level, reonstrution identies ognates
and orrespondenes, and then onstruts sound
hanges whih explain them.
This paper presents a Bayesian approah to
assessing ognates and orrespondenes. Best
sets of ognates and orrespondenes an then
be identied by gradient asent on this evalua-
tion measure. While the work is motivated by
the eventual goal of oering software solutions
to historial linguistis, it also hopes to show
that Bayes' theorem applied to an expliit, sim-
ple model of language an lead to a prinipled
and tratable method for identifying ognates.
The struture of the paper is as follows. The
next setion details the notions of historial lin-
guistis needed for this paper. Setion 2 for-
mally denes a model of language and parent
language. The subsequent setion situates the
work amongst similar work in the literature,
15
making use of onepts desribed in the earlier
setions. Setion 4 desribes the alulation of
the probability of wordlist data given a hypoth-
esised parent language. This is ombined with
Bayes' theorem and gradient searh in an algo-
rithm to nd the best parent language for the
data. Setion 5 desribes the results of apply-
ing an implementation of the algorithm to data
from Polish and Russian. The nal setion sum-
marises the paper and suggests further work.
1 Cognates, Correspondenes and
Reonstrution
In the neo-Grammarian model of language
hange, a population speaking a uniform lan-
guage divides, and then the two populations un-
dergo separate language hanges.
Word forms with ontinuous histories in re-
spetive daughter languages desending whih
from a ommon word-form anestor are alled
ognate, no matter what has happened to their
semantis. Cognate word forms may have un-
dergone deformations to make them less simi-
lar to eah other, these deformations resulting
from regular, phonologial hanges. Note that
in the elds of applied linguistis, seond lan-
guage aquisition, and mahine translation, the
term ognate is used to mean any words that are
phonologially similar to eah other. This is not
the sense meant here.
Phonologial hange produes modiations
to the segmental inventory, replaing one seg-
ment by another in all or only some ontexts.
This sometimes has the eet of ollapsing seg-
ment types together. Other hanges may di-
vide one segment type into two, depending on
a ontextual ondition. The relation of parent-
language segments to daughter-language seg-
ments is, usually, a many-to-many relation.
Parent-hild segmental relations are reeted
in the orrespondenes between segment in-
ventories in the daughter languages. Cor-
respondenes are pairings of segments from
daughter languages whih have derived from
a ommon parent segment. For example, p
in Latin frequently orresponds to f in En-
glish, as in words like pater and father. Both
segments have developed from a (postulated)
Proto-IndoEuropean *p. Beause orrespon-
denes only our between ognates, identify-
ing the two is often a bootstrap proess: or-
raling ognates helps nd more orrespondenes,
and forms sharing a number orrespondenes are
probably ognate.
2 Formal Strutures
The method presented in this paper is based on
a formal model of language. This is desribed in
setion 2.1. The subsequent setion extends the
model to dene a parent language, whose seg-
mental inventory is orrespondenes and whose
lexion is ognates linking two desendent lan-
guages.
2.1 Language model
The language model is based on three assump-
tions.
Assumption 1 There is a universal, disrete
set M of meanings.
Assumption 2 A language L has its own set of
segments ?(L).
Assumption 3 The lexion ? of a language L is
a partial map of meanings to strings of segments
? : M ? ?(L)?.
On the basis of these assumptions, we an de-
ne a language L to be a triple (M,?(L), ?(L))
of meanings, segments and mappings from mean-
ings onto strings of segments.
For example, onsider written Polish. The
set of meanings ontains onepts as to take-
perfet-innitive, tree-nominative-singular,
and so on. The segmental inventory ontains
the 32 segments a a b   d e e f g h i j k l
 l m n n o o p r s s t u w y z 
z z, ignoring
apitalisation. The lexion mathes meanings to
strings of segments, to take-perfet-innitive
to wzia, tree-nominative-singular to drzewo.
2.2 Parent language model
Denition 1 A degree-(u, v) orrespondene
between L1 and L2 is a pair of strings (s, t) ?
?(L1) ? ?(L2) over the segments of L1 and L2
16
respetively, with lengths at least u and no more
than v.
As an example of a orrespondene, onsider
the pair of small strings from Polish and Russian,
(,??). This is a degree-(1, 2) orrespondene
beause its members have lengths as low as one
and as high as two. It is also a degree-(u, v)
orrespondene for any u ? 1 and v ? 2.
Any orrespondene an be mapped onto its
omponents by projetion funtions.
Denition 2 The projetions pi1 and pi2 map
a orrespondene (s, t) onto its rst pi1(s, t) = s
or seond pi2(s, t) = t omponent string respe-
tively.
The rst projetion funtion will map (,??)
onto , while the seond maps (,??) onto ??.
Correspondenes an be formed into strings.
These strings also have projetions.
Denition 3 The projetions pi1 and pi2 map
a string of orrespondenes c1..ck onto the on-
atenation of the projetions of eah orrespon-
dene.
pi1(c1..ck) = pi1(c1)pi1(c2)..pi1(ck),
pi2(c1..ck) = pi2(c1)pi2(c2)..pi2(ck)
Suppose we sequene four orrespondenes
into the string (w,?)(z,?)(ia,?)(,??). This
string has rst and seond projetions, wzia
and ?????, formed by onatenating the respe-
tive projetions of eah orrespondene.
We an now dene a parent language.
Denition 4 A degree-(u, v) parent L0 of two
languages L1, L2 is a triple (M,?(L0), ?(L0))
where ?(L0) is a set of degree-(u, v) orrespon-
denes between L1 and L2, exluding the pair of
null strings, and ?(L0) is a partial mapping from
M onto ?(L0) whih obeys
pi1 ? ?(L0) ? ?(L1), pi2 ? ?(L0) ? ?(L2)
The irle stands for funtion omposition.
Continuing our past example, we will fous
on the two meanings to take-perfet-innitive
and tree-nominative-singular. The segment in-
ventory for the parent language ontains degree-
(0, 2) orrespondenes: (,?), (,??), (d,?),
(e,?), (ia,?), (o,?), (rz,?), (w,?), (z,?). The
lexial funtion maps to take-perfet-innitive
onto the string of orrespondenes (w,?) (z,?)
(ia,?) (,??) while tree-nominative-singular
maps to (d,?) (,?) (rz,?) (e,?) (w,?) (o,?).
The parent language ondition is veried by
heking the projetions of the two orrespon-
dene strings. The rst string has proje-
tions wzia and ?????, whih are forms for
the meaning to take-perfet-innitive in Pol-
ish and Russian respetively. The seond string
has projetions drzewo and ??????, whih are
forms for the meaning tree-nominative-singular
in Polish and Russian respetively. So the pro-
jetion ondition is satised. If the lexial fun-
tion is only dened on these two meanings, then
this is a valid parent language.
It is worth emphasising that the projetion
ondition for qualifying as a parent language ap-
plies only for those meanings for whih the par-
ent lexial mapping is dened. The orrespond-
ing forms in the hild languages are said to be
ognate in this model. Where no parent form
is reonstruted, the forms are not ognate, and
are to be aounted for in some way other than
the parent language.
3 Related Work
The urrent work is, of ourse, far from the rst
to seek to identify ognates and/or orrespon-
denes. Here is an abbreviated overview of pre-
vious work in the eld
1
. More detailed surveys
an be found in hapter 3 of Kondrak's (2002)
PhD thesis or Lowe's online survey
2
of prior art
in this eld.
In perhaps the rst omputational work on
historial linguistis, Kay (1964) desribed an al-
gorithm for determining orrespondenes given
a list of ognate pairs aross two daughter lan-
guages. His method seeks to nd the smallest set
1
An anonymous reviewer suggests that the urrent
work shares features with that of Kessler (2001). I have
been unable to aess this book in time to inlude dis-
ussion of it in this paper.
2
linguistis.berkeley.edu/? jblowe/REWWW/PriorArt.html
17
of orrespondenes whih allows a degree-(1,?)
alignment for eah ognate pair. Unfortunately,
the omplexity of the problem has preluded its
appliation to signiant daa sets.
Frantz (1970) developed a PL/1 programming
whih returned numerial evaluations of orre-
spondenes and ognay, given a list of possi-
ble ognate word-pairs. Eah word pair must be
supplied as a degree-(0, 1) reonstrution, that
is, aligning single segments with eah other or
with gaps.
Guy (1984; 1994) presented a program alled
COGNATE whih nds regular orrespondenes
and identies ognates using statistial teh-
niques.
For his Master's, Broza (1998) developed
MDL-based software alled andid whih identi-
es orrespondenes from ognates and expresses
these as ontextual phonologial transformation
rules.
Kondrak's (2002) dotoral dissertation om-
bines phonologial and semanti similarity meth-
ods with orrespondane-learning. The algo-
rithms for learning orrespondenes are taken
from Melamed's (2000) probabilisti methods
for identifying word-word translation equiva-
lene. These methods, like the urrent work,
are Bayesian. Beause Melamed's problem seeks
partial rather than omplete explanation of the
inputs in terms of orrespondenes, the math-
ing problem is somewhat more diult theoret-
ially. As a result, he does not arrive at the de-
omposition of the sum of the probability of two
inputs given the set of possible orrespondenes,
approximating this with a high probability align-
ment.
4 Conditional Probability of the
Data
The ore of any Bayesian model is the ondi-
tional probability of the data given the hypoth-
esis. This setion details how probabilities as-
signed to data, and the assumptions on whih
this assignment is based.
The data is the mapping of meanings onto
forms in two daughter languages. If those two
languages are L1 and L2, we want to determine
P (?(L1), ?(L2)|h). The nature of h will be dis-
ussed in setion 4.6.
For brevity, we will write ?i for ?(Li).
4.1 Meaning independene
The rst step in dening the onditional prob-
ability of the data is to deompose it into
meaning-by-meaning probabilities. This an be
ahieved by adopting the following two assump-
tions.
Assumption 4 In a given language, the forms
for dierent meanings are seleted indepen-
dently.
This assumption states that within a single
language hoosing, for example, a form wzia
for meaning to take-perfet-innitive is no help
in prediting the form whih expresses tree-
nominative-singular.
Assumption 5 Aross dierent languages, the
forms orresponding to dierent meanings are
independent.
Aording to this assumption, the Polish word
wzia and the Russian word ????? an be
struturally dependent beause they express the
same meaning. In ontrast, we an only ex-
pet a hane relationship between the Rus-
sian word ????? meaning to take-perfet-
innitive, and the Polish word drzewo express-
ing tree-nominative-singular.
Together, these two assumptions imply that
the only dependenies possible between any four
forms expressing the two meanings m1 and m2 in
two languages L1 and L2 are between ?(m1) and
?(m1) on the one hand and ?(m2) and ?(m2) on
the other.
Consequently the probability of generating the
word forms in two languages an be deomposed
into the produt of generating the two language-
partiular forms for eah meaning.
P (?1, ?2|h) =
?
m?M
P (?1(m), ?2(m)|h)
18
4.2 Cognay and independene
The next assumption holds that strutural or-
relation between orresponding forms should be
explained as resulting from ognay.
Assumption 6 Aross dierent languages,
forms orresponding to the same meaning are
dependent only if the forms are ognate.
If the words for a partiular meaning do not
derive from a ommon anestral form, then they
are unorrelated. To return to our Polish and
Russian examples, we an expet dependenies
in struture between the ognate words drzewo
and ??????. But we should expet no suh or-
relation in the non-ognate pair pomaranza
and ???????? meaning orange-nominative-
singular.
Let us write Mi for the domain of the lexial
funtion in language Li. This is the set of mean-
ings for whih this language has dened a word
form. The set of ognates is the domain of the
lexial funtion of the parent language, M0. We
an deompose the evidential words into three
sets: M0 of ognates, M1 \M0 of meanings only
expressed in language L1, and M2 \M0 of mean-
ings only expressed in language L2. Words in the
seond and third ategories are non-ognate, and
so probabilistially independent of eah other.
The onditional probability of the data an
thus be expressed as follows.
P (?1, ?2|h) =
?
m?M0
P (?1(m), ?2(m)|h)
?
m?M1\M0
P (?1(m)|h)
?
m?M2\M0
P (?2(m)|h)
4.3 Probability of a word
We now turn to the probability of generating a
string in a language. The rst assumption de-
nes the distribution over word-length.
Assumption 7 The probability of a word hav-
ing a partiular length is negative exponential in
that length.
The seond assumption allows segment prob-
ability to depend only on the segment identity,
and not on its neighbourhood.
Assumption 8 Segment hoie is ontext-
independent.
These two assumptions together imply that
the probability of strings is determined by a xed
distribution over ?(Li) ? {#}, where # is an
end-of-word marker. For the desendent lan-
guages, this distribution an be taken as the rela-
tive frequenies of the segments and end-of-word
marker. Denote this distribution for language Li
by fi.
The probability of generating a word in a lan-
guage, given relative frequenies fi, is the prod-
ut of the relative frequenies for eah lettern in
the word, multiplied by the relative frequeny of
the end-of-word marker.
P (?i(m)|h) = fi(#)
?
a??i(m)
fi(a)
Note that this expression only holds for words
that are independent of all others, suh as om-
ponents of non-ognate pairs.
4.4 Probability of generating a ognate
pair
The probability of generating a ognate pair of
words is similar to the above, beause desen-
dent forms are deterministially derivable from
the parent forms. If (?1(m), ?2(m)) are a pair of
ognates derived from an anestral form ?0(m),
then there is unit probability that the desen-
dent forms are what they are given the parent:
P (?1(m), ?2(m)|?0(m)) = 1.
Sine a ognate pair is derivable from a par-
ent form, the probability of a ognate pair is
the sum of the probabilities of all parent forms
whih will generate the two desendents. Write
W (m) = W (?1(m), ?2(m)) for the set of pos-
sible orrespondene strings in the parent whih
projet onto wordforms ?1(m) and ?2(m). Then
the probability of the word pair is given by:
P (?1(m), ?2(m)|h) =
?
s?W (m)
P (?0(m) = s|h)
The summation poses a slight problem, however.
How do we sum over all possible strings with
given projetions? Fortunately, we an deom-
pose the summation. Start by reognising that
19
the parent language is also a language, and so
the probability of forms in the language is de-
termined by a distribution over segments  in
this ase orrespondenes  and the end-of-word
marker. For onsisteny, we all this distribution
f0.
The only parent form whih projets onto two
empty strings is the empty string, onsisting
only of the end-of-word marker. For brevity,
we will drop the lambdas, writing P (x, y|h) for
P (?1(m) = x, ?2(m) = y|h)
P (0, 0|h) = f0(#)
We assume, without loss of generality, that
the segmental inventory of the parent language
onsists of all degree-(u, v) orrespondenes be-
tween L1 and L2. Parent segments whih are
never used an be exluded by giving them zero
relative frequeny in f0.
The funtion Pre(s;u, v) returns the set of bi-
nary divisions (a, b) of the string s, suh that the
length of the rst part a is at least u and at most
v.
Pre(s;u, v) = {(a, b)|ab = s,m ? |a| ? n}
With this funtion, we an reursively dene a
funtion W (s, t;u, v) on pairs of strings (s, t)
whih returns the set of all degree-(u, v) parent
language strings whih projet onto s and t. For
brevity, we will treat all u, v arguments as im-
pliit.
W (0, 0) = {0}
By denition, the only parent language string
whih an map onto the empty string in both
desendents is the empty string.
The reursive step breaks the strings s and
t into all possible prexes a and c respetively.
The orrespondene (a, c) is then preposed on all
strings returned by W when it is applied to the
remainders of s and t.
W (s, t) =
?
(a,b)?Pre(s)
?
(c,d)?Pre(t)
(a, c)W (b, d)
Note that this is the set W (m) we dened earlier.
W (m) = W (?1(m), ?2(m);u, v)
The reursive denition of W in terms of dis-
joint unions and onatenation an be trans-
formed into a reursive denition for the proba-
bility P0(s, t|h) of onstruting a member of the
set. Disjoint union is replaed by summation,
onatenation by produt. The probability of
an individual orrespondene (a, c) is its (un-
known) relative frequeny f0(a, c) in the parent
language. One again, we hide the impliit u, v
parameters.
P0(0, 0|h) = f0(#)
P0(s, t|h) =
?
(a,b)?Pre(s)
?
(c,d)?Pre(t)
f0(a, c)P (b, d|h)
4.5 Probability of a form-pair
We now have the piees to speify the probabil-
ity of nding any partiular form as the form-
pair for the desendent languages. The prob-
ability of the pair in the ase of ognay is
P0(?1(m), ?2(m)|h). If the pair are not ognate,
then they are independent, and their probabil-
ity is P1(?1(m))P2(?2(m)|h). If we write c(m|h)
for the likelihood that the pair is ognate, we
an ombine these two values to given a total
probability of the two forms.
P0(?1(m), ?2(m)|h)c(m|h)
+P1(?1(m))P2(?2(m)|h)(1.0 ? c(m|h))
Beause the word-pairs are independent (as-
sumption 4), the produt of the above probabil-
ity for eah meaning m gives the probability of
the data given the hypothesis.
4.6 Hypothesis
One burning question remains, however. What
is the hypothesis? The simple answer is that it
is exatly those free variables in the speiation
of the probability of the data
There were two groups of unknowns in the
probability of the data. The rst is the rela-
tive frequeny f0 assigned to orrespondenes in
parent-language forms. The seond is the like-
lihood of ognay c, a vetor of values between
zero and one indexed by meanings.
A hypothesis is therefore any setting of values
for the pair of vetors (f, c).
20
Note that while the degree variables u, v were
not xed in the above derivation, they will be
held onstant for any partiular searh, and thus
do not dene a dimension in the hypothesis
spae.
4.7 Searh
In this setion, we have derived P (D|h), the like-
lihood of our data given a hypothesis.
For simpliity, we hoose a at prior over hy-
potheses, rendering the MAP Bayesian approah
an instane of maximum likelihood determina-
tion. The value for the likelihood is dierentiable
in eah of the parameters. Consequently, gradi-
ent desent an be used to nd the hypothesis
whih maximises the probability of the data.
5 Results
In onstruting the method, we made a number
of assumptions about independene of forms. It
is sensible that for testing, the method is applied
to data that onforms reasonably well to these
assumptions. The alternative is to apply it to
data whih ontradits its fundamental assump-
tions, onsequently hampering its eetiveness.
5.1 The data
Polish and Russian were hosen to provide the
data beause they approximately obey assump-
tion 6: words have dependent strutures if and
only if they are ognate. For our two lan-
guages, this means that borrowings from om-
mon soures are unommon (numbering 45 in
our data set), at least in omparison with the
number of ognates (numbering 156).
The data was harvested from two online
ditionaries (Wordgumbo, 2007a; Wordgumbo,
2007b), one English-Polish, the other English-
Russian. Multiple translations were simplied,
with the shortest translation retained. The En-
glish glosses were used as the meanings for the
words. Where the gloss ontained a apital let-
ter, indiating a proper noun, this was elimi-
nated from the data.
The data should also onform to assumption
4, that words for dierent meanings with a lan-
guage are independent. So where two meanings
in the data sets were realised with the same form,
these meanings were deemed to be struturally
dependent, and so only the rst was retained in
the wordlist.
The remaining data ontains 407 aligned
Polish-Russian word pairs.
Polish and Russian both use a great deal of
derivational and inetional morphology. The
simple language model used here does not take
this into aount, so this will be a disturbing
inuene on the results.
5.2 Evaluation
The aligned wordlists were hand-tagged as og-
nate, ommon borrowing or non-ognate. A per-
missive rule of ognay was used: if the roots
of words in the two languages were ognate,
they were ognate, even if represented with non-
ognate derivational and/or inetional mor-
phology.
Figure 1 shows the evaluation of the program's
performane on the data.
Borrowings as: ognates non-ognates
Found f 162 119
Missed m 41 37
Errant e 6 49
Auray f/(f + e) 96% 71%
Reall f/(f + m) 81% 76%
Figure 1: Evaluation of program performane
on 407 meaning-mathed pairs of Polish-Russian
words. Common borrowings are sored as og-
nates in the rst olumn, non-ognates in the
seond.
The sores show that the method works well
in identifying ognates, partiularly if ommon
borrowings are aepted as ognates, or exluded
manually. If ommon borrowings are sored as
non-ognates, then the auray falls.
Of the orrespondenes found between Polish
and Russian, 67 have a phonologial basis. The
remaining 27 result from mismath morphology
in ognates or dierenes in ommon borrowings.
6 Conlusion
This paper has presented a model of language
whih allows the alulation of the posterior
probability of forms arising in the ases where
21
they are ognate, and where they are not. Bayes'
theorem relates these probabilities to the poste-
rior likelihood of partiular orrespondenes and
ognay relationships. Gradient desent an be
used to searh this spae for the best distribution
over orrespondenes, and best ognay evalua-
tions for meaning-paired words. The appliation
to data from Polish and Russian shows remark-
able suess identifying both ognates and non-
ognates.
Future work will proeed by relaxing on-
straints on the parent language. The parent in-
ventory will be widened to inlude multisegment
orrespondenes. Multiple parent languages will
be permitted, to the end of separating borrow-
ings from ognates. Finally, riher models of
language, inorporating syllable struture, will
allow more information to identify ognates.
Referenes
Gil Broza. 1998. Inter-language regularity: the
transformation learning problem. Master's thesis,
Institute of Computer Siene, Hebrew University
of Jerusalem, Otober.
Isidore Dyen, Joseph B. Kruskal, and Paul Blak.
1992. An Indo-European lassiation: a lexio-
statistial experiment. Transations of the Amer-
ian Philosophial Soiety, 82(5).
T. Mark Ellison and Simon Kirby. 2006. Measuring
language divergene by intra-lexial omparison.
In ACL, pages 273280, Sydney.
T. Mark Ellison. 1992. The Mahine Learning of
Phonologial Struture. Ph.D. thesis, University
of Western Australia.
Donald G. Frantz. 1970. A PL/1 program to assist
the omparative linguist. Communiations of the
ACM, 13(6):353356.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergene times support the ana-
tolian theory of indo-european origin. Nature,
426:435439.
Jaques B. M. Guy. 1984. An algorithm for identi-
fying ognates between related languages. In 10th
International Conferene on Computational Lin-
guistis and 22nd Annual Meeting of the Asso-
iation for Computational Linguistis. Available
online as http://al.ld.upenn.edu/P/P84/P84-
1091.pdf.
Jaques B. M. Guy. 1994. An algorithm for identi-
fying ognates in bilingual wordlists and its appli-
ability to mahine translation. Journal of Quan-
titative Linguistis, 1(1):3542.
Hans Heinrih Hok. 1991. Priniples of Historial
Linguistis. Mouton de Gruyter, Berlin.
Robert J. Jeers and Ilse Lehiste. 1989. Prini-
ples and Methods for Historial Linguistis. MIT
Press, Cambridge, MA.
Martin Kay. 1964. The logi of ognate reognition
in historial linguistis. Tehnial Report RM-
4224-PR, The RAND Corporation, Santa Monia,
CA, September.
Brett Kessler. 2001. The Signiane of Word Lists.
CSLI Publiations, Stanford, CA.
Simon Kirby. 2002. Natural language from artiial
life. Artiial Life, 8(2):185215.
Grzegorz Kondrak. 2002. Algorithms for Lan-
guage Reonstrution. Ph.D. thesis, University of
Toronto.
April MMahon and Robert MMahon. 2003. Find-
ing families: quantitative methods in language
lassiation. Transations of the Philologial So-
iety, 101:755.
I. Dan Melamed. 2000. Models of translational
equivalene among words. Computational Lin-
guistis, 26(2):221249.
Luay Nakleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A omparison of
phylogeneti reonstrution methods on an ie
dataset. Transations of the Philologial Soiety,
103(2):171192.
D. Ringe, Tandy Warnow, and A. Taylor.
2002. Indo-European and omputational ladis-
tis. Transations of the Philologial Soiety,
100(1):59129.
Wordgumbo. 2007a. /ie/sla/pol/erengpol.htm.
Website http://www.wordgumbo.om/.
Wordgumbo. 2007b. /ie/sla/rus/erengrus.htm.
Website http://www.wordgumbo.om/.
22
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 1?9,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Distinguishing Contact-Induced Change from Language Drift in
Genetically Related Languages
T. Mark Ellison
Psychology
University of Western Australia
Mark.Ellison@uwa.edu.au
Luisa Miceli
Linguistics
University of Western Australia
lmiceli@cyllene.uwa.edu.au
Abstract
Languages evolve, undergoing repeated
small changes, some with permanent ef-
fect and some not. Changes affecting a
language may be independent or contact-
induced. Independent changes arise inter-
nally or, if externally, from non-linguistic
causes. En masse, such changes cause
isolated languages to drift apart in lexical
form and grammatical structure. Contact-
induced changes can happen when lan-
guages share speakers, or when their speak-
ers are in contact.
Frequently, languages in contact are re-
lated, having a common ancestor from
which they still retain visible structure.
This relatedness makes it difficult to distin-
guish contact-induced change from inher-
ited similarities.
In this paper, we present a simulation of
contact-induced change. We show that it
is possible to distinguish contact-induced
change from independent change given (a)
enough data, and (b) that the contact-
induced change is strong enough. For a par-
ticular model, we determine how much data
is enough to distinguish these two cases at
p < 0.05.
1 Introduction
Evolutionary change happens when structures are
copied, the copying is inexact, and the survival of
copies is uncertain. Many structures undergo this
kind of reproduction, change and death: biologi-
cal organisms, fashions, languages. Often evolu-
tionary change leaves little or no trace, except for
those copies which are present at the moment. In
these cases, determining the evolutionary history
of a family of structures involves comparing sur-
viving copies and making inferences from where
they correspond and where they differ.
Language is, for the most part, one of those
cases. Most languages have not had a writing
system until recently, and so their history has
left no direct trace. Since the 18th century, lin-
guists have been comparing languages to recon-
struct both common parents and individual histo-
ries for these languages (Jones, 1786; Schleicher,
1861; Brugmann, 1884, for example).
In this paper, we hope to contribute to this effort
by presenting a formal model of a particular kind
of evolutionary change, namely contact-induced
change, and placing limits on when its past pres-
ence can be inferred from synchronic evidence.
Contact-induced change can happen when
speakers of different languages come in contact,
or where there is a sizeable group of bi- or multi-
linguals. We distinguish two different types.
One type, contact-induced assimilation (CIA)
changes languages so that they become more sim-
ilar to each other. This is the type of contact-
induced change that is most obvious and that has
been best studied. The consensus is that it can
affect all sub-systems of a language depending
on the intensity of contact (see eg. Thomason &
Kaufman 1988). The other type, less frequently
noticed and only recently receiving attention (see
eg. Franc?ois 2011, Arnal 2011), is contact-
induced differentiation (CID) where the change
acts specifically to make the languages less sim-
ilar. This type of contact-induced change pre-
dominantly affects the parts of a language which
speakers are most conscious of being distinct: the
phonological forms of morphemes and words.
It is hard to isolate contact-induced change in
1
related languages from the effects of common in-
heritance or normal independent drift. In lan-
guages in contact over a long period of time, it
is impossible to tell whether the dropping of any
single cognate is the result of chance variation or
the action of a differentiation process. Likewise,
if languages are compared using a single-valued
measure of similarity (such as fraction of cog-
nates in a Swadesh list), the effects of more or less
contact-induced changes cannot be distinguished
from a greater or lesser time-depth since the com-
mon ancestor. This is shown in figure 1.
Less
contact?induced differentiation
contact?induced assimilation
separated earlier
separated later
Similarity More
Figure 1: shows the problem of identifying contact-
induced change between related languages. Contact-
induced assimilation and having a more recent com-
mon ancestor can both account for language similari-
ties. Contact-induced differentiation accounts for less
similarity, but so does positing a remoter common an-
cestor that allows time for more independent drift re-
sulting in greater differentiation without contact. A
single similarity measure is insufficient to separate
time-depth from contact-induced change.
Contact-induced change is, however, different
from independent drift. If it is detectable at all, it
will be because it creates different counts of syn-
onyms and different proportions of cognates, than
drift alone. Thus, with enough data, it should pos-
sible to distinguish the effects of time-depth and
contact-induced change. This paper presents the
results of a simulation to determine just how much
data would be enough.
1.1 Overview
Section 2 discusses contact-induced change, and
CID in particular. While it is easy to find instances
of CIA, eg. borrowing a word from one language
to another, it is harder to find unarguable cases of
CID. They can be found, however, and some of
these are discussed in section 2.2.
Section 3 describes language as a bundle of re-
lations. Language changes can then be modelled
as changes in these relations. A formal account of
independent and contact-induced changes in rela-
tions is given, as the underpinnings for the next
section.
This next section (section 4) investigates how
much data is needed to develop 95% certainty that
contact-induced change has occurred as opposed
to independent change alone. As might be ex-
pected, the weaker the CIA or CID pressure, the
more evidence needed to distinguish the types of
change.
The final section considers the implications
of the research, and situates it within a larger
programme of investigation into contact-induced
change.
1.2 Terminology
This paper uses terms from mathematics and lin-
guistics. The term relation will only be used in
its mathematical sense of a potentially many-to-
many association from elements in one set, the
domain, to elements in another, the range. An as-
sociation between a domain element and a range
element will be called a link. We introduce the
term doppels to describe words from different
languages which have had a common origin, or
are so similar that they might be presumed to have
a common origin. These differ from cognates in
two ways. Although cognates must have had a
common origin, doppels need not ? they may just
look like they do. Also, where there is a common
origin, cognates must have evolved with the lan-
guage as a whole, while doppels may be the result
of borrowing. Etymologically, doppel is a doppel
of the German Doppel, duplicate, copy, double.
2 Contact-Induced Change in Natural
Languages
It is impossible to study language history without
being aware of the impact of contact on languages
all around the world, not least in the current
age of globalisation. However, while the most
transparent and best known process of contact-
induced assimilation, word borrowing, has been
a focus in historical linguistics, some other assim-
ilatory phenomena and almost all differentiating
processes are only recently receiving attention.
2.1 Contact-Induced Assimilation
Contact-induced assimilation (CIA) describes any
process which causes two languages to become
more similar. The increased similarity could be
2
the result of: more doppels between the lan-
guages, due to one language borrowing from an-
other; convergent phonology, as a large commu-
nity of bilinguals use a single phonemic inventory
for both languages; or convergent syntax and mor-
phology. This last may occur as the speech of
weak bilinguals, dropping rich morphology and
using a lot of word-for-word translations in their
non-native tongues, impacts the entire commu-
nity.
English itself exemplifies the extent to which
borrowing can make languages similar. Finken-
staedt and Wolff (1973) found that Latin and
French (including Old Norman) have each con-
tributed more words to Modern English than its
Germanic parent language has. English speakers
consequently often find it easier to learn a Ro-
mance language than a Germanic one.
Metatypy (Ross, 2006) is one type of contact-
induced change at the grammatical level. Lan-
guages engaged in metatypy, such as Kannada and
Marathi in the Indian village of Kupwar, can come
to have (nearly) identical grammatical and mor-
phological organisation; the languages only differ
in their lexical forms. One result is that it is easy
to translate from one language to the other, sim-
ply by replacing a morpheme in one language by
its form in the other.
CIA seems to be much more common than
CID. This may, however, be due to the fact that
it is much easier to detect, because similarity is
inherently less likely to occur by chance than dis-
similarity.
2.2 Contact-Induced Differentiation
Because dissimilatory change is sometimes, but
not always, hard to detect, many of the known
cases of it arise because it is done deliberately and
speakers report that they are doing it. Thoma-
son (2007) gives two principal motivations for
this kind of deliberate change: (a) a desire or
need to increase the difference between one?s own
speech and someone else?s, and (b) a desire or
need to keep outsiders at a distance. However, the
two recent studies already mentioned ? Franc?ois
(2011) and Arnal (2011) ? describe how this type
of change may arise without ?differentiation? per
se being the primary motivation (see Franc?ois
2011:229-30 in particular).
A situation that fits the first description is
that found in one of the dialects of Lambayeque
Quechua where speakers systematically distort
their words in order to make their speech dif-
ferent from that of neighbouring dialects. One
of the processes used involves the distortion of
words by metathesis giving, for example: /yaw.ra/
from /yawar/, /-tqa/ from /taq/, /-psi/ from /pis/
and /kablata /from /kabalta/ (Thomason 2007:51).
This kind of process clearly gives rise to a system
with different phonotactics.
There is also anecdotal evidence that non-
Castilian languages of the Iberian Peninsula have
undergone deliberate differentiation. Wright
(1998) reports that some late-medieval Por-
tuguese avoided using words similar or identi-
cal to the corresponding Castilian words when
a less similar synonym was available, while Vi-
dal (1998) reports the same behaviour among the
Catalan. More recently Arnal (2011) has de-
scribed further differentiating change to Catalan
lexical forms due to increased levels of Span-
ish/Catalan bilingualism among native Spanish
speakers, following the establishment of Catalan
as a co-official language in 1983. There have also
been processes of differentiation at play in Gali-
cian, where purists have promoted alternatives to
items shared with Castilian (Posner and Green,
1993; Beswick, 2007). These in turn are bal-
anced by movements to assimilate Galician with
Portuguese.
Franc?ois (2011) describes the strong tendency
for languages spoken in the Torres and Banks is-
lands of northern Vanuatu to diverge in the forms
of their words, resulting in a pattern where closely
related languages that would be expected to have
high levels of cognacy, instead exhibit highly dis-
tinctive vocabularies.
Perhaps the most extreme example of change
aimed at increasing the difference in one?s own
speech is that of the Uisai dialect of Buin,
a language spoken in Papua New Guinea on
Bougainville island. Laycock (1982:34) reports
that Uisai shows diametrically opposed noun cat-
egories to other dialects. The markers for cate-
gory 1 in Uisai occur only with category 2 else-
where, and vice-versa. In this particular parame-
ter these dialects are significantly more different
than would be expected by chance.
The desire to differentiate languages in this way
doesn?t necessarily imply hostility or antagonism.
Laycock also reports an opinion from the Sepik
region of Papua New Guinea: it wouldn?t be any
3
good if we all talked the same, we like to know
where people come from.
One of the reasons for the current work is to
create the tools which might let us see whether
these efforts to change languages, for social or po-
litical reasons, actually have a lasting effect on the
vocabulary, or whether they are at best ephemeral
(see eg. Thomason & Kaufman 1988, Ross 2007,
Aikhenvald 2002; and Franc?ois 2011, Arnal 2011
on differentiation).
3 Evolutionary Change in Relations
In this section, we explore the formal model that
we will use to distinguish normal, independent
change from contact-induced change. The first
step is to model languages as a bundle of relations.
Modelling language in this way is not new, but is
rarely made explicit.
3.1 Language as a Bundle of Relations
Much language structure can be expressed as rela-
tions between different spaces. For example, the
lexicon can be regarded as a relation between the
space of meanings available in a language and the
phonological forms of morphemes expressing that
meaning. There can be meanings represented by
multiple forms, such as ready and prepared, or
forms with multiple meanings such as fire in the
sense of burning or terminating employment.
Another language relation maps phonemes-in-
contexts to phones that can realise them. Phone-
mic distinctions may collapse in some contexts,
such as with the final devoicing of obstruents in
Polish, so that distinct phonemes are realised with
the same phone. Likewise, the same phoneme,
even in the one context, may be realised by mul-
tiple phones; the Portuguese phoneme /K/ is re-
alised as [K], [?], [G] or even [r], with multiple
possible realisations even for the one speaker.
So both the lexicon and phonetic realisation can
be modelled with relations.
3.2 Primitive Changes on Relations
If some important language structures are rela-
tional, an interesting question is what sort of evo-
lutionary changes can effect these relations. This
subsection explores a number of minimal changes
which can effect relations. To the best of our
knowledge, this is the first time that language
changes have been characterised this way. The
starting point is a simple relation between a do-
main and a range, as shown in figure 2.
A
B
C
1
2
3
Domain Range
Figure 2: shows a relation from a small domain to a
similarly-sized range.
The first kind of change is a global substitution,
see figure 3. This is where a change of permuta-
tion or merger applies to elements of either the
domain or the range. All of the pairs which con-
tain the affected elements are modified, hence the
name.
A
B
C
1
2
3
Domain Range
A
B
C 3
Domain Range
12
Figure 3: shows a global substitution: range elements
1 and 2 are merged, preserving all links. It is called
a global substitution as every link with 1 or 2 in the
range now has 12 as its range element.
Modifications of the phonetic relation can be of
this kind. For example, when Gaelic ? both Irish
and Scottish ? merged [D] into [G], the change af-
fected both lexical /D/ in closed class words, such
as the preposition <dha>, /Da/, to, as well as lex-
ical /D/ in open class words such as <duine>,
/du?@/, person. This was a global substitution.
More frequently met are small changes, we will
call local mutations. These involve either the in-
sertion of a single link, or the deletion of a single
link.
A
B
C
1
2
3
Domain Range
X
Figure 4: shows two separate local mutations in a re-
lation: a deletion marked by an X on the link, and an
insertion shown as a dotted arrow.
Gloabl changes can be expressed as local
changes combined with relation composition.
4
The lexical relation associates meanings with the
phonological forms, which may take the form
phonemes in contexts. The phonemic map then
projects these onto their phonetic realisations.
If a single link in the phoneme realisation map
is dropped, then all lexical meanings expressed
using that phoneme-in-context can no longer re-
alise it with that phone. If a single link is added
to the phonetic relation, then all lexical meanings
expressed using that phoneme-in-context can now
realise it with the new phone. This multiplier
effect on changes means single sound changes
can have a disproportionate effect on the simi-
larity of cognate forms in two languages. Elli-
son and Kirby (2006) presented a similarity mea-
sure which bypasses this superficial difference:
pairs of domain elements are compared for the
similarity of the corresponding sets of range el-
ements, and these similarity values are then com-
pared cross-linguistically. This measure mitigates
the effect of global substitutions.
The iterated application of local mutational
changes to language structures is called drift.
In traditional models of language history, it is
the primary mechanism for explaining difference,
while the shared parent language is the primary
explanation of similarity.
3.3 Contact-induced change
So far, we have only looked at change arising
in independent relations. Change, in language at
least, is often the result of contact with the corre-
sponding relational structure in another language.
Figure 5 shows two relations between the same
domain and range, superimposed. Later diagrams
will use this same superimposed representation in
describing contact-induced changes.
A
B
C
1
2
3
Domain Range
Figure 5: shows two relations simultaneously: the
links from one are shown with thick arrows, those from
the other with thin. Links common to both relations are
doppels.
In considering contact-induced change, it is
worth noting that the change need not be sym-
metrical between the languages involved. If one
language is spoken by a dominant, larger popu-
lation, it may see no reason to differentiate itself
from the language of a smaller community. The
smaller community may feel that language differ-
entiation is a way to protect its identity. Whatever
the reason, we shall call the relation undergoing
differentiation the assimilating or differentiating
relation, and the relation it is pushing away from,
or pull towards, the reference relation.
Contact-induced assimilation or CIA can con-
sist of the insertion of a new link into the relation,
or the deletion of a link in the relation. As assim-
ilation is about making the relations more similar,
so insertion applies to create doppels where the
reference relation has a link and the assimilating
relation does not. Likewise assimilation applies
to delete links where the reference relation does
not have a link but the assimilating relation does.
Examples of this kind of assimilation are shown
in figure 6.
A
B
C
1
2
3
Domain Range
X
Figure 6: shows contact-induced assimilation (CIA)
as an insertion shown as a dotted line and a deletion
marked with an X. Existing links of the assimilating
relation are shown thin, while those of the reference
relation are shown thick. In CIA, links are more likely
to be inserted to make a doppel, and deleted where no
doppel exists.
The reverse is true in cases of contact-induced
differentiation ? see figure 7. The differentiating
A
B
C
1
2
3
Domain Range
X
Figure 7: shows contact-induced differentiation (CID)
in the form of an insertion shown as a dotted line and
a deletion marked with an X. Existing links of the dif-
ferentiating relation are shown thin, while those of the
reference relation are shown thick. In CID, links are
more likely to be deleted if they have a doppel, and
inserted where they do not.
relation is more likely to delete a link which is half
of a doppel than delete other links. Likewise, it is
5
more likely to create a link where there is none
in the reference relation, rather than borrow a link
from it.
4 When can CIA/CID be Inferred?
This paper addresses the question: how much data
is required to distinguish cases of contact-induced
change from similarity due to a common ancestor
and differences due to drift? The question will
be addressed in terms of relations and the types
of changes covered in section 3.2 and section 3.3.
To render the problem tractable, we need an addi-
tional assumption about the lexical relations: they
have the form described in section 4.1.
4.1 RPOFs
We restrict lexical relations to RPOFs. An RPOF
is a reverse of a partial onto function, in other
words, a relation such that each element of the
domain participates in at least one link, while each
element in the range participates in at most one
link. An example of such a relation appears in
figure 8. If the lexical relation in a language is
A
B
1
2
3
Domain Range
Figure 8: shows an RPOF relation. In RPOFs, each
element of the domain has at least one link, while each
element of the range has at most one link.
an RPOF, then each meaning is expressible with
at least one morphemic form, and each potential
form expresses exactly one meaning, or else is not
used in the language. In other words, the language
has no homophones.
This assumption is usually only mildly inaccu-
rate. For some languages, however, such as Chi-
nese, mono-syllabic morphemes are frequently
homophonous. The analysis presented here may
fail for languages of this kind.
The advantage of using RPOFs is that their
structure can be summarised by a cardinality
function ? a partial function from natural num-
bers to natural numbers. This function associates
with any cardinality of range subset the number
of elements of the domain which associate with
a range set of exactly that size. For example, the
relation shown in figure 8 maps one input onto
two outputs, while it maps the second input to
a single output. Thus its cardinality function is
{2 : 1, 1 : 1}. Such specifications completely
characterise an RPOF relation upto permutation
of either the domain or range.
One of the effects of assuming RPOF structure
for the lexical relation is that we do not allow the
sole link from any domain element to undergo
deletion. This is because all domain elements
must retain at least a single link. For the lexi-
cal relation, this has the fairly likely consequence
that the sole morpheme representing a meaning is
unlikely to be lost, while if there are multiple syn-
onyms, one might fall out of use.
4.2 Pairs of RPOFs
When we are comparing RPOFs evolved from
a common parent, we can characterise their re-
lationship, upto permutation of the domain and
range, by frequency counts over triples. The
triples are numbers describing how many ele-
ments of the range a domain element links to:
solely in relation 1, in both relations (ie, the num-
ber of doppels), and solely in relation 2. For each
triple, we count the number of domain elements
which have the correspondingly sized projections
on the range. This kind of summarisation allows
us to describe the similarity of two lexical rela-
tions with a few hundred numbers if we limit our-
selves to, say, domain elements linking to at most
10 range elements in either relation.
4.3 Significance Testing
It easy to evaluate the posterior likelihood of a
set of data associating a counting number with
each triple, D ? NTriples, given a model M ?
Dist(Triples) in the form of a distribution over
triples. The triple associated with each domain el-
ement is assumed to be the result of independent
processes ? in other words, we assume that the
number of doppel and non-doppel forms associ-
ated with a meaning is independent of the num-
bers associated with other meanings.
P (D|M) =
?
t?Triples
M(t)D(t)
We can evaluate the likelihood of one model
M1 generating data at the frequencies produced
by a second model M2. The posterior probability
of the data relative to the second model is shown
6
in equation (1), while the probability of generat-
ing that data from the model which did indeed
generate it is shown in equation (2).
P (M2|M1) =
?
t?Triples
M1(t)M2(t) (1)
P (M2|M2) =
?
t?Triples
M2(t)M2(t) (2)
The likelihood ratio, i.e. the ratio of posterior
likelihoods of M2 and M1, is shown in equation
(3).
P (M2|M1)
P (M2|M2)
=
?
t?Triples
M1(t)
M2(t)
M2(t)
(3)
This ratio expresses the amount of information
we are likely to gain about which distribution is
correct as a result of looking at a single data item.
In terms of RPOF relations, this single data item
is the triple of counts for relation-1-only, doppels,
and relation-2-only associated with a meaning. If,
as assumed above, the counts associated with each
domain element are independent, then the likeli-
hood ratio is raised to the power of the number N
of items seen.
P (M2|M1)
P (M2|M2)
N
= [
?
t?Triples
M1(t)
M2(t)
M2(t)
]N (4)
To establish a chance prediction at p < 0.05,
we merely need to know that P (M2|M1) <
P (M2|M2), and then determine the minimum
level of N for which the ratio in equation (4) is
less than 1/19. This number of items generated
from the target distribution would allow it to be
distinguished from chance at a ratio of 19 : 1.
Determining the correct value for N here is
a general problem known as power analysis.
For standard experimental designs and corre-
sponding statistics, the power analysis can be
found in many texts, such as that by (Bausell
and Li, 2006), and many computing libraries
such as the pwr library for power analysis
in R (see http://cran.r-project.org/
web/packages/pwr/). Where the model de-
sign is as complex as that described here, the
power analysis must be constructed from first
principles.
It is often easier to work with this quantity in in-
formational rather than probabilistic form, where
it takes the form shown in equation (5).
? log P (M2|M1)P (M2|M2)
= ?
?
t?Triples
M2(t) log
M1(t)
M2(t)
(5)
The quantity in equation (5) is the well-known
Kullback-Liebler divergence DKL(M2||M1) of
the two distributions, also known as the discrim-
ination information. Significance is achieved
when this value multiplied by the number of data
items is greater than log2(19) = 4.2479.
4.4 Models with and without
Context-Induced Change
The construction of the no-CIA/CID and the with-
CIA/CID distributions makes use of four parame-
ters.
In the non-context model:
insertion of a link combines the probability
? of making a change at all for any given
domain element, with the probability ?/(1+
?) that the change will be the addition rather
than deletion of a link, into a likelihood of
adding a link per domain element of ??/(1+
?).
deletion of a link combines the probability ?
of making a change at all for any given do-
main element, with the probability 1.0/(1 +
?) that the change will be to a deletion, with
the number m of links to select from for that
domain element, so the probability of delet-
ing any of those links is ?/(m+m?).
In the case of CIA/CID, we only consider the
impact of contact on deletion. The per-link prob-
ability of deletion ?/(m + m?) is modified by a
parameter ? indicating how strong the effects of
contact are. Positive ? brings about CIA ? with
shared links less likely to be dropped than others,
while negative ? develops CID ? shared links are
more likely to be dropped than others. The prob-
ability of dropping any given doppel link from a
given range node is (1??)z, and of any unshared
link is z where nd is the number of doppel links
from the domain element, and nu the number of
7
unshared links in the differentiating relation, and
z is given in equation (6).
z = ?
((1? ?)nd + nu)(1 + ?)
(6)
4.5 Simulation Results
The above model was used to generate distribu-
tions over triples for non-CIA/CID relation pairs,
and relation pairs with additional CIA/CID pro-
cesses. The number of iterations of the mutation
process with or without CIA/CID was fixed at 100
in creating the generating distribution M2. The
parameter ? was fixed at 0.1 and ? at 0.5. The
value for ? was chosen to approximately repro-
duce the single-language distribution of range-set
sizes for Castillian as computed from the Spanish
wordnet. The bias parameter ? was varied from
?0.5 to 0.5 in steps of 0.1. For each level of
bias, a search was made over non-CIA/CID distri-
butions at different depths from the common an-
cestor ? this is the parameter N ? until the dis-
tribution with the least K-L divergence from the
generated distribution was found. This found dis-
tribution M1 represents the null hypothesis, that
the data arose without CIA/CID bias.
The number of data items needed to achieve
significant recognition of the presence of
CIA/CID bias is 4.2479/DKL(M1||M2). The
results for various levels of ? are shown in figure
9.
? N S D
-0.5 118 3128 0.091
-0.4 115 4364 0.096
-0.3 111 6839 0.101
-0.2 108 13800 0.107
-0.1 104 47378 0.114
0.1 95 30913 0.133
0.2 90 7331 0.145
0.3 85 2793 0.160
0.4 79 1278 0.178
0.5 72 654 0.203
Figure 9: Tabulation of number S of data items needed
to achieve significance and the number of iterations N
of the best non-CIA/CID model, and fraction of dop-
pels remaining D, against CIA/CID bias parameter ?.
Note that fewer data items are needed to recognise sig-
nificant assimilatory bias (positive values for ?) than
differentiating bias (negative values of ?) at the same
strength.
5 Conclusion
This paper has looked at different ways that re-
lations may evolve from a common parent struc-
ture. They may undergo local mutational changes,
global substitutions, independent changes, or
those triggered by contact with other relations. In
one class of relations, with reasonable assump-
tions, it is clear that a large, but possible, amount
of data needs to be adduced to ascertain that CIA
and/or CID have occured, rather than just shared
origin and independent drift.
In historical linguistics, this opens the door, for
testing whether the impressionistic accounts of
CID are reflected in the distributional properties
of the languages concerned. It may also be possi-
ble to circumvent the onerous data requirements,
by bringing in data from multiple independent re-
lations within the language, such as those defining
morphological structure and phonology, as well as
the lexicon.
As mentioned in the introduction, this work
is part of a larger programme by the authors to
develop statistical tools able to show that CID
has taken place, if it has. This work is partly
driven by the need to account historically for the
low cognacy but high structural similarity be-
tween nearby Australian languages. In the Daly
River area, adjacent languages with very similar
phonology, syntax and morphology show remark-
ably low cognacy counts, often around 8% (Har-
vey, 2008). One possible explanation for this is a
powerful CID imperative acting over a short time
depth to differentiate the vocabularies of the lan-
guages. The result presented in this paper sug-
gests that with sufficient lexical data, direct statis-
tical evidence could be found if this is indeed the
correct explanation.
There are potential uses for this work beyond
historical linguistics as well. The model might as-
sist in some cases of plagiarism detection, for ex-
ample, where two students worked together on an
assignment, and then set out to deliberately differ-
entiate them by altering vocabulary. Similar anal-
ysis of documents might reflect other reasons for
reworking a text, such as to give it a new identity
for a new setting.
8
References
A. Aikhenvald. 2002. Language contact in Amazonia.
Oxford University Press, Oxford.
Antoni Arnal. 2011. Linguistic changes in the catalan
spoken in catalonia under new contact conditions.
Journal of Language Contact, 4:5?25.
R. Barker Bausell and Yu-Fang Li. 2006. Power
Analysis for Experimental Research: A Practical
Guide for the Biological, Medical and Social Sci-
ences. Cambridge University Press, March.
Jaine E. Beswick. 2007. Regional nationalism in
Spain: language use and ethnic identity in Galicia.
Multilingual Matters.
Karl Brugmann. 1884. Zur frage nach den
verwandtschaftverhltnissen der indogermanischen
sprachen. Internationale Zeitschrift fr allgemeine
Sprachwissenschaft, 1:226?56.
T. Mark Ellison and Simon Kirby. 2006. Measur-
ing language divergence by intra-lexical compari-
son. In ACL, pages 273?80, Sydney.
Thomas Finkenstaedt and Dieter Wolff. 1973. Or-
dered profusion: studies in dictionaries and the En-
glish lexicon. C Winter.
Alexandre Franc?ois. 2011. Social ecology and lan-
guage history in the northern vanuatu linkage: a tale
of divergence and convergence. Journal of Histori-
cal Linguistics, 1:175?246.
Mark Harvey. 2008. Proto-Mirndi: a discontinuous
language family in northern Australia. Pacific Lin-
guistics, Canberra.
Sir William Jones. 1786. The third anniversary dis-
course, delivered 2nd february, 1786: on the hindus.
Asiatick Researches, 1:415?31.
Donald C. Laycock. 1982. Melanesian linguistic di-
versity: a melanesian choice? In R.J. May and
H. Nelson, editors, Melanesia: beyond diversity,
pages 33?38. Australian National University Press,
Canberra.
Rebecca Posner and John N. Green. 1993. Bilingual-
ism and Linguistic Conflict in Romance. Walter de
Gruyter.
Malcolm D. Ross. 2006. Metatypy. In K. Brown, edi-
tor, Encylcopedia of language and linguistics. Else-
vier, Oxford, 2nd ed edition.
Malcolm Ross. 2007. Calquing and metatypy. Jour-
nal of Language Contact, Thema, 1:116?43.
August Schleicher. 1861. Compendium der
vergleichenden Grammatik der indogermanischen
Sprachen. Hermann Bhlau, Weimar.
Sarah Grey Thomason and Terrence Kaufman. 1988.
Language contact, creolization, and genetic linguis-
tics. University of California Press, Berkeley & Los
Angeles.
Sarah Grey Thomason. 2007. Language contact and
deliberate change. Journal of Language Contact,
Thema, 1:41?62.
Carrasquer Vidal. 1998. Untitled post in ?Cladistic
language concepts? thread, HISTLING mailing list,
Oct.
Roger Wright. 1998. Untitled post in ?Cladistic lan-
guage concepts? thread, HISTLING mailing list,
Oct.
9
