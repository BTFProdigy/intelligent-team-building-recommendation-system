Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 17?25,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Clause Identification and Classification in Bengali  
Aniruddha Ghosh1 Amitava Das2 Sivaji Bandyopadhyay3 
Department of Computer Science and Engineering  
Jadavpur University 
arghyaonline@gmail.com1 amitava.santu@gmail.com2 si-
vaji_cse_ju@yahoo.com3  
 
Abstract 
This paper reports about the develop-
ment of clause identification and classi-
fication techniques for Bengali language. 
A syntactic rule based model has been 
used to identify the clause boundary. For 
clause type identification a Conditional 
random Field (CRF) based statistical 
model has been used. The clause identi-
fication system and clause classification 
system demonstrated 73% and 78% pre-
cision values respectively.  
1 Introduction 
The clause identification is one of the shallow 
semantic parsing tasks, which is important in 
various NLP applications such as Machine 
Translation, parallel corpora alignment, Informa-
tion Extraction and speech applications. Gram-
matically a clause is a group of words having a 
subject and a predicate of its own, but forming 
part of a sentence. Clause boundary identifica-
tion of natural language sentences poses consi-
derable difficulties due to the ambiguous nature 
of natural languages. Clause classification is a 
convoluted task as natural language is generally 
syntactically rich in formation of sentences or 
clauses. 
By the classical theory of Panini (Paul and 
Staal, 1969) a clause is the surface level basic 
syntactic element which holds the basic depen-
dent semantics (i.e. lexical semantic have no 
dependency) to represent the overall meaning of 
any sentence. This syntactic to semantic deriva-
tion proceeds through two intermediate stages: 
the level of karaka relations, which are compa-
rable to the thematic role types and the level of 
inflectional or derivational morphosyntax. 
Fillmore?s Case Grammar (Fillmore et. al, 
2003), and much subsequent work, revived the 
Panini?s proposals in a modern setting. A main 
objective of Case Grammar was to identify syn-
tactic positions of semantic arguments that may 
have different realizations in syntax.  
In the year of 1996 Bharati et al (1996) de-
fines the idea of Chunk or local word group for 
Indian languages. After the successful imple-
mentation of Shakti1 , the first publicly available 
English-Hindi machine translation system the 
idea of chunk became the most acceptable syn-
tactic/semantic representation format for Indian 
languages, known as Shakti Standard Format 
(SSF).   
In 2009 Bali et al (2009) redefines the idea of 
chunk and establishes that the idea of chunking 
varies with prosodic structure of a language. 
Boundary of chunk level is very ambiguous it-
self and can differ by writer or speaker accord-
ing to their thrust on semantic. 
Therefore it is evident that automatic clause 
identification for Indian languages needs more 
research efforts. In the present task, clause 
boundary identification is attempted using the 
classical theory of Panini and the Case Grammar 
approach of Fillmore on the shallow parsed out-
put in SSF structure. It may be worth mentioning 
that several basic linguistic tools in Indian lan-
guages such as part of speech tagger, chunker, 
and shallow parser follow SSF2  as a standard.  
Previous research on clause identification was 
done mostly on the English language (Sang and 
Dejean, 2001). There have been limited efforts 
on clause identification for Indian languages. 
One such effort is proposed in Ram and Devi, 
                                                 
1
 http://shakti.iiit.ac.in/ 
2
 http://ltrc.iiit.ac.in/MachineTrans/research/tb/shakti-
analy-ssf.pdf 
17
(2008) with statistical method. The idea of ge-
nerative grammar based on rule-based descrip-
tions of syntactic structures introduced by 
Chomsky (Chomsky, 1956) points out that every 
language has its own peculiarities that cannot be 
described by standard grammar. Therefore a new 
concept of generative grammar has been pro-
posed by Chomsky. Generative grammar can be 
identified by statistical methods. In the present 
task, conditional random field (CRF) 3  -based 
machine learning method has been used in 
clause type classification. According to the best 
of our knowledge this is the first effort to identi-
fy and classify clauses in Bengali. 
The present system is divided into two parts. 
First, the clause identification task aims to iden-
tify the start and the end boundaries of the claus-
es in a sentence. Second, Clause classification 
system identifies the clause types. 
Analysis of corpus and standard grammar of 
Bengali revealed that clause boundary identifica-
tion depends mostly on syntactic dependency. 
For this reason, the present clause boundary 
identification system is rule based in nature. 
Classification of clause is a semantic task and 
depends on semantic properties of Bengali lan-
guage. Hence we follow the theory of 
Chomsky?s generative grammar to disambiguate 
among possible clause types. The present classi-
fication system of clause is a statistics-based 
approach. A conditional random field (CRF) 
based machine learning method has been used in 
the clause classification task. The output of the 
rule based identification system is forwarded to 
the machine learning model as input. 
The rest of the paper is organized as follows. 
In section 2 we elaborate the rule based clause 
boundary identification. The next section 3 de-
scribes the implementation detail with all identi-
fied features for the clause classification prob-
lem. Result section 4 reports about the accuracy 
of the hybrid system. In error analysis section 
we reported the limitations of the present sys-
tem. The conclusion is drawn in section 5 along 
with the future task direction. 
2 Resource Acquisition 
Bengali belongs to Indo-Aryan language family. 
A characteristic of Bengali is that it is under-
                                                 
3
 http://crf.sourceforge.net/ 
resourced. Language research for Bengali got 
attention recently. Resources like annotated cor-
pus and linguistics tools for Bengali are very 
rarely available in the public domain. 
2.1 Corpus 
We used the NLP TOOLS CONTEST: ICON 
20094 dependency relation marked training data-
set of 980 sentences for training of the present 
system. The data has been further annotated at 
the clause level. According to the standard 
grammar there are two basic clause types such as 
Principal clause and Subordinate clause. Subor-
dinate clauses have three variations as Noun 
clause, Adjective clause and Adverbial clause. 
The tagset defined for the present task consists 
of four tags as Principal clause (PC), Noun 
clause (NC), Adjective clause (AC) and Adver-
bial clause (RC). The annotation tool used for 
the present task is Sanchay5. The detailed statis-
tics of the corpus are reported in Table 1. 
 
 Train Dev Test 
No of Sentences 980 150 100 
Table 1: Statistics of Bengali Corpus 
2.1.1 Annotation Agreement 
Two annotators (Mr. X and Mr. Y) participated 
in the present task. Annotators were asked to 
identify the clause boundaries as well as the type 
of the identified clause. The agreement of anno-
tations among two annotators has been eva-
luated. The agreements of tag values at clause 
boundary level and clause type levels are listed 
in Table 2. 
 
 
Boundary Type 
Percentage 76.54% 89.65% 
Table 2: Agreement of annotators at clause 
boundary and type level 
It is observed from the Table 2 that clause 
boundary identification task has lower agree-
ment value. A further analysis reveals that there 
are almost 9% of cases where clause boundary 
has nested syntactic structure. These types of 
clause boundaries are difficult to identify. One 
of such cases is Inquisitive semantic (Groenen-
dijk, 2009) cases, ambiguous for human annota-
                                                 
4
 http://ltrc.iiit.ac.in/nlptools2009/ 
5
 http://ltrc.iiit.ac.in/nlpai_contest07/Sanchay/ 
18
tors too. It is better to illustrate with some spe-
cific example. 
If John goes to the party, 
will Mary go as well? 
In an inquisitive semantics for a language of 
propositional logic the interpretation of disjunc-
tion is the source of inquisitiveness. Indicative 
conditionals and conditional questions are 
treated both syntactically and semantically. The 
semantics comes with a new logical-
pragmatically notion that judges and compares 
the compliance of responses to an initiative in 
inquisitive dialogue (Groenendijk, 2009). Hence 
it is evident that these types of special cases 
need special research attention. 
2.2 Shallow Parser 
Shallow parser6 for Indian languages, developed 
under a Government of India funded consortium 
project named Indian Language to Indian Lan-
guage Machine Translation System (IL-ILMT), 
are now publicly available. It is a well developed 
linguistic tool and produce good credible analy-
sis. For the present task the linguistic analysis is 
done by the tool and it gives output as pruned 
morphological analysis at each word level, part 
of speech at each word level, chunk boundary 
with type-casted chunk label, vibhakti computa-
tion and chunk head identification. 
2.3 Dependency parser 
A dependency parser for Bengali has been used 
as described in Ghosh et al (2009). The depen-
dency parser follows the tagset7  identified for 
Indian languages as a part of NLP TOOLS 
CONTEST 2009 as a part of ICON 2009. 
3 Rule-based Clause Boundary Identi-
fication 
Analysis of a Bengali corpus and standard 
grammar reveals that clause boundaries are di-
rectly related to syntactic relations at sentence 
level. The present system first identifies the 
number of verbs present in a sentence and sub-
sequently finds out dependant chunks to each 
verb. The set of identified chunks that have rela-
tion with a particular verb is considered as a 
clause. But some clauses have nested syntactic 
                                                 
6
 http://ltrc.iiit.ac.in/analyzer/bengali/ 
7
 http://ltrc.iiit.ac.in/nlptools2009/CR/intro-husain.pdf 
formation, known as inquisitive semantic. These 
clauses are difficult to identify by using only 
syntactic relations. The present system has limi-
tations on those inquisitive types of clauses. 
Bengali is a verb final language. Most of the 
Bengali sentences follow a Subject-Object-Verb 
(SOV) pattern. In Bengali, subject can be miss-
ing in a clause formation. Missing subjects and 
missing keywords lead to ambiguities in clause 
boundary identification. In sentences which do 
not follow the SOV pattern, chunks that appear 
after the finite verb are not considered with that 
clause. For example:  
 
wAra AyZawana o parimANa 
xeKe buJawe asubiXA hayZa ei  
paWa hAwi geCe. 
 
After seeing the size and 
effect, it is hard to under-
stand that an elephant went 
through this way. 
 
In the above example, there is hardly any clue 
to find beginning of subordinate clause. To solve 
this type of problem, capturing only the tree 
structure of a particular sentence has been 
treated as the key factor to the goal of disambig-
uation. One way to capture the regularity of 
chunks over different sentences is to learn a ge-
nerative grammar that explains the structure of 
the chunks one finds. These types of language 
properties make the clause identification prob-
lem difficult.  
3.1 Karaka relation 
Dependency parsing generates the inter chunk 
relation and generates the tree structure. The de-
pendency parser as described in Section 2.3 used 
as a supportive tool for the present problem.   
In the output of the dependency parsing sys-
tems, most of the chunks have a dependency 
relation with the verb chunk. These relations are 
called as karaka relation. Using dependency re-
lations, the chunks having dependency relation 
i.e. karaka relation with same verb chunk are 
grouped. The set of chunks are the members of a 
clause. Using this technique, identification of 
chunk members of a certain clause becomes in-
dependent of SOV patterns of sentences. An ex-
ample is shown in Figure 1. 
19
 Figure 1: Karaka Relations 
3.2 Compound verbs  
In Bengali language a noun chunk with an infi-
nite verb chunk or a finite verb chunk can form a 
compound verb. An example is shown in Figure 
2. 
 
Figure 2: Compound Verb 
In the above example, the noun chunk and the 
VGF chunk form a compound verb. These two 
consecutive noun and verb chunks appearing in 
a sentence are merged to form a compound verb. 
These chunks are connected with a part-of rela-
tion in Dependency Parsing. The set of related 
chunks with these noun and verb chunks are 
merged.  
3.3 Shasthi Relation (r6) 
In dependency parsing the genitive relation are 
marked with shasthi (r6) relation. The chunk 
with shasthi (r6) (see the tagset of NLP Tool 
Contest: ICON 2009) relation always has a rela-
tion with the succeeding chunk. An example is 
shown in Figure 3. 
In the example as mentioned in Figure 3, the 
word ?wadera?(their) has a genitive relation 
with the word in the next chunk ?manera?(of 
mind). These chunks are placed in a set. It forms 
a set of two chunks members. The system gene-
rates two different types of set. In one forms a 
set of members having relation with verb 
chunks. Another set contains two noun chunks 
with genitive relation. Now the sets containing 
only noun chunks with genitive relation does not 
form a clause. Those sets are merged with the set 
containing verb chunk and having dependency 
relation with the noun chunks. An example is 
shown in Figure 3. 
 
Figure 3: Shasthi Relation 
 
Consider ? is set of all sets containing two 
chunk members connected with genitive marker. 
Consider ? is a set of all sets consisting of re-
lated chunks with a verb chunk. ? is a element of 
?. ? is a element of ?. Now, If a set ? which can 
have common chunks from a ? set then ? set is 
associated with the proper ? set. So, ? ? ? ? 
Null then ? = ? ? ?. If a set ? which can have 
common chunks from two ? sets which leads to 
ambiguity of associability of the ? set with the 
proper ? set. If ? ? ? = verb chunk, then ? set 
will be associated with ? set containing the verb 
chunk. From the related set of chunk of verb 
chunks, system has identified the clauses in the 
sentence. Afterwards, the clauses are marked 
with the B-I-E (Beginning-Intermediate-End) 
notation.  
4 Case Grammar-Identification of Ka-
raka relations 
The classical Sanskrit grammar Astadhyayi 8  
(?Eight Books?), written by the Indian gramma-
                                                 
8
 
http://en.wikipedia.org/wiki/P%C4%81%E1%B9%87
ini 
20
rian Panini sometime during 600 or 300 B.C. 
(Robins, 1979), includes a sophisticated theory 
of thematic structure that remains influential till 
today. Panini?s Sanskrit grammar is a system of 
rules for converting semantic representations of 
sentences into phonetic representations (Ki-
parsky, 1969). This derivation proceeds through 
two intermediate stages: the level of karaka rela-
tions, which are comparable to the thematic role 
types described above; and the level of morpho-
syntax. 
Fillmore?s Case Grammar (Fillmore, 1968), 
and much subsequent work, revived the Panini?s 
proposals in a modern setting. A main objective 
of Case Grammar was to identify semantic ar-
gument positions that may have different realiza-
tions in syntax. Fillmore hypothesized ?a set of 
universal, presumably innate, concepts which 
identify certain types of judgments human be-
ings are capable of making about the events that 
are going on around them?. He posited the fol-
lowing preliminary list of cases, noting however 
that ?additional cases will surely be needed?.  
? Agent: The typically animate perceived 
instigator of the action. 
? Instrument: Inanimate force or object 
causally involved in the action or state. 
? Dative: The animate being affected by 
the state or action. 
? Factitive: The object or being resulting 
from the action or state. 
? Locative: The location or time-spatial 
orientation of the state or action. 
? Objective: The semantically most neu-
tral case, the concept should be limited to 
things which are affected by the action or 
state. 
The SSF specification handles this syntactic 
dependency by a coarse-grain tagset of Nomini-
tive, Accusative, Genitive and Locative case 
markers. Bengali shallow parser identifies the 
chunk heads as part of the chunk level analysis. 
Dependency parsing followed by a rule based 
module has been developed to analyze the inter-
chunk relationships depending upon each verb 
present in a sentence. Described theoretical as-
pect can well define the problem definition of 
clause boundary identification but during prac-
tical implementation of the solution we found 
some difficulties. Bengali has explicit case 
markers and thus long distant chunk relations are 
possible as valid grammatical formation. As an 
example: 
bAjAre yAoyZAra samayZa xeKA 
kare gela rAma. 
 
bAjAre yAoyZAra samayZa rAma 
xeKA kare gela. 
 
rAma bAjAre yAoyZAra samayZa 
xeKA kare gela. 
 
Rama came to meet when he 
was going to market. 
 
In the above example rAma could be placed 
anywhere and still all the three syntactic forma-
tion are correct. For these feature of Bengali 
many dependency relation could be missed out 
located at far distance from the verb chunk in a 
sentence. Searching for uncountable numbers of 
chunks have dependency relation with a particu-
lar verb may have good idea theoretically but we 
prefer a checklist strategy to resolve the problem 
in practice. At this level we decided to check all 
semantic probable constituents by the definition 
of universal, presumably innate, concepts list. 
We found this is a nice fall back strategy to iden-
tify the clause boundary. Separately rules are 
written as described below. 
4.1 Agent 
Bengali is a verb final language. Most of the 
Bengali sentences follow a Subject-Object-Verb 
(SOV) pattern. In Bengali, subject can be miss-
ing in a clause formation. Missing subjects and 
missing keywords lead to ambiguities in clause 
boundary identification. 
 
  	? 
Close the door. 
 
In the previous case system marks 
?/door? as an ?Agent? whereas the 
?Agent? is ?you? (2nd person singular number), 
silent here.  
We developed rules using case marker, Gend-
er-Number-Person (GNP), morphological fea-
ture and modality features to disambiguate these 
21
types of phenomena. These rules help to stop 
false hits by identifying no 2nd person phrase 
was there in the example type sentences and em-
power to identify proper phrases by locating 
proper verb modality matching with the right 
chunk.  
4.2 Instrument 
Instrument identification is ambiguous for the 
same type of case marker (nominative) taken by 
agent and instrument. There is no ani-
mate/inanimate information is available at syn-
tactic level. 

	 
  ? 
The music of Shyam?s messme-
rized me. 
 ? 
The umbrella of Sumi. 
 
Bengali sentences follow a Subject-Object-
Verb (SOV) pattern. Positional information is 
helpful to disambiguate between agent and in-
strument roles. 
4.3 Dative 
G
en
er
a
l 
Bengali English Gloss 
/	//	
... 
Morn-
ing/evening/night/da
wn? 
_ 
//The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 201?207,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detection and Correction of Preposition and  Determiner Errors in English: HOO 2012   Pinaki Bhaskar Aniruddha Ghosh Santanu Pal Sivaji Bandyopadhyay Department of Computer Science and Engineering, Jadavpur University 188, Raja S. C. Mallick Road Kolkata ? 700032, India pinaki.bhaskar @gmail.com arghyaonline @gnail.com santanu.pal.ju @gmail.com sivaji_cse_ju @yahoo.com       Abstract 
This paper reports on our work in the HOO 2012 shared task. The task is to automatically detect, recognize and correct the errors in the use of prepositions and determiners in a set of given test documents in English. For that, we have developed a hybrid system of an n-gram statistical model along with some rule-based techniques. The system has been trained on the HOO shared task?s training datasets and run on the test set given. We have submitted one run, which has demonstrated an F-score of 7.1, 6.46 and 2.58 for detection, recognition and correction respectively before revision and F-score of 8.22, 7.59 and 3.16 for detec-tion, recognition and correction respectively after revision. 
1 Introduction Writing research papers or theses in English is a very challenging task for those researchers and scientists whose first language or mother tongue is not English. Depicting their research works proper-ly in English is a hard job for them. Generally their papers, which are submitted to conferences, may be rejected not because of their research works but because of the English writing, which makes the papers harder for the reviewer to understand the intentions of author. This kind of problem will be faced in any field where someone has to provide 
material in a language other than his/her first lan-guage. The mentoring service of Association for Com-putational Linguistics (ACL) is one part of a re-sponse. This service can address a wider range of problems than those related purely to writing. The aim of this service is that a research paper should be judged only on its research content. The organizer of ?Help Our Own? (HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge. In 2012, the task is more specific and only deals with de-terminers and prepositions as described in (Dale and Kilgarriff, 2011). For this shared task, HOO, we have developed two models, one is rule-based model and the other is the statistical model for both determiners and prepositions. Then we have combined both these models and developed our system for HOO 2012. 2 Related Work The English language belongs to the Germanic languages branch of the Indo-European language family, widely spoken on six continents. The HOO 
201
shared task is organized to help authors with writ-ing tasks. Identifying grammatical and linguistic errors in text is an open challenge to researchers. In recent times, researchers (Heidorn, 2000) have provided quite a benchmark for spell checker and grammar checkers, which is commonly available. In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-of-speech (POS) level, syntax level and semantic lev-el. Earlier Heidorn (1975) developed augmented phrase structure grammar. (Tetreault et. al., 2008) has dealt with error pattern with preposition by non-native speakers. Meurers and Wunsch (2010) showed a surface based state-of-the-art machine learning technique, which deals with some fre-quently used prepositions. (Elghafari et al, 2010) worked on Data-Driven Prediction of Prepositions in English. Boyd et al (2011) used an n-gram based machine-learning approach. Last year we have also participated in this shared task; our sys-tem report was reported in (Bhaskar et. al., 2011).  3 Corpus Statistics There are two sets of data, training set and test set provided by the organizer. The training set has 1000 documents, which are collected from the FCE dataset. The publicly available dataset was in the native FCE format. So, the organizer first convert-ed it to the HOO data format. Then CUP annota-tors found the errors and marked them up in the dataset. This year the task is only about the errors related to prepositions and determiners. So the or-ganizer set only six types of errors, listed in table 1, which were dealt with this year. Hence, the other errors were discarded and replace with its corre-sponding standoff annotation in the training set. The training set consists of 1000 documents of to-tal 374680 words, which means 375 words per document. All the standoff annotations of training set were provided and an example of the standoff annotation is shown in the figure 1. Table 2 gives the error statistics of training set as reported in (Dale et. al., 2012). The test dataset has another 100 documents, which contain total of 18013 words at an average of 180 words per document. The test data was pro-cessed as the training data was done, but the stand-off annotation of the test documents was not provided before the task completion. The docu-
ments were provided in XML format as shown in the figure 2.   Error Type Tag Original Correction Replacement Preposition  RT He was born on January He was born in January Missing Preposition  MT Because it reminds me my child-hood. 
Because it reminds me of my child-hood. Unwanted Preposition  UT Regarding to the accom-modation Regarding the accom-modation Replacement Determiner  RD I used to go-ing with my friends to the camp. 
I used to go-ing with my friends to a camp. Missing Determiner  MD That will be nice to go on 1st of July That will be nice to go on the 1st of July Unwanted Determiner  UD The most suitable time for shopping is weekend when parents don't work and children haven't got a school. 
The most suitable time for shopping is weekend when parents don't work and children haven't got school.  Table 1. Examples of the six types of error.   Error Type # Training # Test # before Revised # after Revised UT  822  43 39 MT  1104 57 56 RT  2618 136 148 Prep  4545 236 243 UD  1048 53 62 MD  2230 125 131 RD  609 39 37 Det  3887 217 230 Total  8432 453 473 Words/  Error  44.18 39.77 38.08  Table 2. Error Statistics in the Training set. 
202
 <edit end="779" file="0004" in-dex="0008" part="1" start="775" type="UD">   <original>the </original>     <corrections>       <correction>         <empty/>       </correction>   </corrections> </edit>  <edit end="1041" file="0004" in-dex="0010" part="1" start="1039" type="RT">   <original>in</original>     <corrections>       <correction>at</correction>   </corrections> </edit>   Figure 1: An example of a standoff error annotation  4 System Description  The task is consisted of two coarse parts ? Preposi-tion and Determiner detection, recognition and cor-rection. In our previous year?s hybrid model, to resolve preposition errors, a rule-based model was developed and for determiner errors, a linear statis-tical method was used. There was no linear statisti-cal model for prepositions. So this year we have induced a statistical model to incorporate larger coverage of preposition error detection, which is not detected by the appropriate preposition list de-scribed in section 4.1.2. To resolve preposition errors and determiner er-rors we have built a hybrid model for both of them and used a voting technique among the rule based and statistical model for determiners and rule based post processing for prepositions. The system architecture is shown in the figure 3.  
 <?xml version="1.0" encod-ing="utf-8"?> <HOO version="2.1">   <HEAD sortkey="" source-type="FCE">     <CANDIDATE>       <AGE>20-30</AGE>     </CANDIDATE>   </HEAD>   <BODY>     <PART id="1">       <P>Dear Chris</P>       <P>I was great ?</P>       .       .       .     </PART>   </BODY> </HOO>   Figure 2: An example of the XML format of documents  4.1 Preposition Error Detection 
4.1.1 Statistical Model for Preposition An n-gram based linear statistical model is used. From the training corpus, it was trained with 3, 5 and 7-gram models. After testing, the 5-gram mod-el is performing best as from 3-gram, the statistical model fails to classify since probability distance is too small among the probable set to distinguish proper one while in 7-gram it fails to score high as training data set is relatively small and there are no similar occurrences. For the statistical model, dif-ferent linguistic information is taken as features. Initially, surface words are only considered which actually is similar to fingerprinting technique. Due to different inflected forms, the system fails to identify possible cases for a similar type of error with different inflected forms. Hence the root form of the word is included as a feature. Chunk infor-mation is included as a feature. The preposition with same word varies with if following word is animate or inanimate. As example, collaborate with SB collaborate in/on ST
203
  Figure 3. System Architecture  The text is parsed using the Stanford Dependen-cy parser1 to retrieve animate and inanimate infor-mation. After including animate and inanimate information the system didn?t improve much as training data set is quite small and animate infor-mation is not correct for names. Hence, this feature is discarded from the statistical model. 4.1.2 Appropriate Preposition List An appropriate preposition list consists of list of words along with preposition. The list is prepared in different corpus and training data. In the list, all possible formation with a word and preposition is stored. Let us take an example: admit ST to SB admit to From corpus, two patterns for admit are found. Between admit and preposition something (ST) 
                                                            1 http://nlp.stanford.edu/software/lex-parser.shtml 
may come. Hence both of the entries are combined and formed in a regular expression format. 
admit (ST)* to SB 4.1.3 Rule Based model for Preposition Rule based post processing was applied on output of statistical model. For the rule based post pro-cessing, an appropriate preposition list was pre-pared manually. The list contains 1567 entries. The list is associated with animate and inanimate in-formation. Hence, we aim to use dependency par-ser to identify subject object relation. Since the test data was in XML format, raw text was extracted from the XML document and the extracted sen-tences were parsed using Stanford dependency par-ser. After parsing the document with the dependency parser, subject and object information was extract-ed. From all the sentences, proposition are detected and cross-validated with the appropriate preposi-tion list. The preposition is dependent of the local association of the word around it. For the baseline
204
model, we have found that due to the list being small, few errors are being detected. Hence from the training corpus, the appropriate proposition list is enriched. The list is prepared in regular expres-sion format. Here is an example: ask * out + invite on a date  In the above example, + means the two phrases have a similar meaning and * means one or more words can appear between the two words. Hence, when a match is found from the appropriate propo-sition list with the first word or the preposition, the words local to it are validated. Since the task is about correcting preposition errors, only words are matched with the list.  grateful to SB for ST  In the above example, ST means something or an object and SB means somebody or a subject, this information being retrieved from the depend-ency parser.  4.2 Determiner Error Detection At the beginning of the determiner error detection task, we found that generation of list of rules to detect and correct the probable linguistic errors is a non-exhaustive set. Hence, we have decided to use a statistical model. After the statistical model, a rule based system is implemented with a few rules for the determiner devised from grammar books as for certain patterns statistical model fails to identi-fy. 4.2.1 Statistical Model for Determiner Similarly to preposition error detection, here a 5-gram linear statistical model is used. As same au-thors are prone to repeat same types of mistakes, we have decided to list out the errors from the training corpus documents. We have listed the er-rors document wise. In the training corpus, age information of author is mentioned. Hence docu-ments are grouped according to age. After a close inspection of the document wise error list, the age group is prone to make similar type of errors, which depicts the attributes of the age group. Our statistical model is trained with every set of train-ing data grouped by age separately. Hence differ-ent statistical models are prepared for different age 
groups. Now statistical model are applied accord-ing to the age group. It is found that age wise train-ing incurred better result than single statistical model over whole data.  4.2.2 Rule Based Model for Determiner It is found that statistical models works best for detecting the a and an determiner whereas perfor-mance drops for the determiner. Hence, rules for the are crafted manually from grammar books. A few rules for a and an are defined based on the first letter of the following word.  Among the determiners, usage of the is the most complicated one. For the rule based system differ-ent lists like nation, nationalities, unique objects, etc are produced. A few of the rules, which have been developed for the the determiner are men-tioned below. 1. In most cases, if a sentence starts with a proper noun or common noun the is dropped. 2. Before a country name, the is dropped except if starts with kingdom or republic. 3. They system checks whether a common noun is appeared in a previous line of the docu-ment, i.e. it has already been referred to, in which case the is added. 4. If subject and object belong to same class i.e. they share the same hyponym class, the is added to the subject. 5. In case of superlatives like best, worst etc. the is added. 6. Before numerals, the is added. 7. Before unique things, the is added. Unique-ness is defined if a thing has single embod-iment like moon etc. 8.  It is found that if some geographical location is mentioned at a position other than start of sentence, the is added. For different rules word lists are prepared such as a unique things list, superlatives, common nouns, country names, citizenships etc.  For a and an determiner correction, a list of dif-ferent phonemes is prepared. Rule based system 
205
trims the first two characters and maps them into a phoneme to decide between a and an. 4.2.3 Voting Technique The voting technique is used on the output of the rule based model and the statistical model. For a and an determiners, statistical model works best, especially in missing determiner and unnecessary determiner but for wrong determiner the rule based model performs better. For the determiner, the sta-tistical model identified missing determiner and unnecessary determiner cases to some extent whereas list based rule-based system elevates the accuracy.  5 Evaluation The system was evaluated for its performance in detecting, recognizing and correcting preposition and determiner errors in English documents. Sepa-rate scores were calculated for detection, recogni-tion and correction for both the errors of preposition and determiner separately and then combined scores were also calculated. For all re-sults, the organizer has provided three measures: Precision, Recall and F-Score. The precise defini-tions of these measures as implemented in the evaluation tool, and further details on the evalua-tion process are provided in (Dale and Narroway, 2012) and elaborated on at the HOO website.4 Each team was allowed to submit up to 10 sepa-rate runs over the test data, thus allowing them to have different configurations of their systems eval-
                                                            4 See www.correcttext.org/hoo2012. 
uated. Teams were asked to indicate whether they had used only publicly available data to train their systems, or whether they had made use of privately held data. We have submitted only one run (JU_run1) which has demonstrated F-scores of 7.1, 6.46 and 2.58 for detection, recognition and correc-tion respectively before revision. And after revi-sion it has demonstrated F-scores of 8.22, 7.59 and 3.16 for detection, recognition and correction re-spectively. Table 3 shows all the results of our run. We had used only publicly available data to train our systems, which are provided by the organizer as training set; we didn?t use any privately held data. 6 Conclusion and Future Works Our system has achieved F-scores of 8.22, 7.59 and 3.16 in detection, recognition and correction respectively. Our system failed to detect and cor-rect many syntactic and semantic errors like wrong a determiner. Since the data consists of mostly mail conversation, it retains huge number of spelling mistakes, which misdirected the statistical, and rule based model to detect probable errors. For the determiner, if the size of the produced lists in-creases, better accuracy can be achieved with the rule-based system. Co-reference is another issue to identify, as the determiner is used mostly subse-quent references. Anaphora resolution might there-fore be of some help.    
 
    Element Task Before Revision After Revision Precision Recall F-score Precision Recall F-score 
Preposition Detection 6.10 7.63 6.78 7.12 8.61 7.79 Recognition 5.42 6.78 6.03 6.44 7.79 7.05 Correction 3.05 3.81 3.39 3.73 4.51 4.08 
Determiner Detection 7.73 6.45 7.04 9.39 7.42 8.29 Recognition 7.73 6.45 7.04 9.39 7.42 8.29 Correction 1.66 1.38 1.51 2.21 1.75 1.95 
Combined Detection 6.93 7.28 7.10 8.19 8.25 8.22 Recognition 6.30 6.62 6.46 7.56 7.61 7.59 Correction 2.52 2.65 2.58 3.15 3.17 3.16  Table 3. Results for Preposition, Determiner and Combined (preposition and determiner) errors.  
206
Acknowledgments We acknowledge the support of the IFCPAR fund-ed Indo-French project ?An Advanced Platform for Question Answering Systems? and the DIT, Gov-ernment of India funded project ?Development of English to Indian Language Machine Translation (EILMT) System Phase II?. References  Adriane Boyd and Detmar Meurers. Data-Driven Cor-rection of FunctionWords in Non-Native English. In 2011 Generation Challenges, HOO: Helping Our Own in the Proceedings of the 13th European Work-shop on Natural Language Generation (ENLG), 28th ? 30th September, 2011, Nancy, France. Anas Elghafari, Detmar Meurers and Holger Wunsch, 2010. Exploring the Data-Driven Prediction of Prep-ositions in English. In the Proceedings of the 23rd In-ternational Conference on Computational Linguistics, Beijing, China, 2010. George Heidorn. 2000. Intelligent writing assistance. In R Dale, H Moisl, and H Somers, editors, Handbook of Natural Language Processing, pages 181?207. Marcel Dekker Inc.  GE Heidorn. 1975. Augmented phrase structure gram-mars. In: BL Webber, RC Schank, eds. Theoretical Issues in Natural Language Processing. Assoc. for Computational Linguistics, pp.1-5.  J R Tetreault and M S Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of the 22nd International Conference on Computational Linguistics, pp-865-872, Manches-ter,2008. Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal and Sivaji Bandyopadhyay. May I correct the English of your paper!!!. In 2011 Generation Challenges, HOO: Helping Our Own in the Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation (ENLG), pp 250-253, 28th ? 30th September, 2011, Nancy, France. Robert Dale and A Kilgarriff. 2010. Helping Our Own: Text massaging for computational linguistics as a new shared task. In Proceedings of the 6th Interna-tional Natural Language Generation Conference, Dublin, Ireland, pages 261?266, 7th-9th July 2010. Robert Dale and A Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), 28th ? 30th September, 2011, Nancy, France. 
Robert Dale and George Narroway. 2012. A framework for evaluating text correction. In Proceedings of the Eighth International Conference on Language Re-sources and Evaluation (LREC2012), 21?27 May 2012. Robert Dale, Ilya Anisimoff and George Narroway (2012) HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task.  In Pro-ceedings of the Seventh Workshop on Innovative Use of NLP for Building Educational Applications, Mon-treal, Canada, 7th June 2012.             
207
