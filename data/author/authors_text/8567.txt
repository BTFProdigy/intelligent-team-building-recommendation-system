Corpus-based Development and Evaluation of a System for Processing 
Definite Descriptions 
Renata Vieira 
Universidade do Vale do Rio dos Sines 
Av. Unisinos 950 - Cx. Postal 275 
93022-000 S~o Leopoldo RS Brazil. 
renata@exatas, unisinos, br 
Massimo Poesio 
University of Edinburgh 
HCRC and Informatics 
Edinburgh, Scotland 
Massimo. Poesio@ed. ac.uk 
Abstract 
we present an implelnented system for processing 
definite descriptions. The system is based on the re- 
sults of a corpus analysis previously reported, which 
showed how common discourse-new descriptions 
are in newspaper corpora, and identified several 
problems to be dealt with when developing compu- 
tational methods for interpreting bridging descrip- 
tions. The annotated corpus produced in this ear- 
licr work was used to extensively evaluate the pro- 
posed techniques for matchiug delinite descriptions 
with their antecedents, discourse segmentation, rec- 
ognizing discourse-new descriptions, and suggest- 
ing anchors for bridging descriptions. 
1 Motivation 
In previous work (Poesio and Vieira, 1998) we re- 
ported the results of corpus annotation experiments 
in which the subjects were asked to classify lhe uses 
of delinite descriptions in Wall Stree Journal arti- 
cles according to a scheme derived from work by 
Hawkins (1978) and Prince (1981 ) and including 
three classes: I)II{I'~CT ANAPltORA, I)ISCOURSE- 
NEW, and I~RIDGING DESCI{II'TION (Clark, 1977). 
This study showed that about half of the time, deli- 
nite descriptions are used to introduce a new entity 
in the discourse, rather than to refer to an object al- 
ready mentioned. We also observed that our sub- 
jects didn't always agree on the classification of a 
given delinite; the problem was especially acute for 
bridging descriptions. 
In this paper, we present an implemented system 
for processing delinite descriptions based on the re- 
suits of that earlier study. In our system, techniques 
for recognising discourse-new descriptions play a 
role as ilnportant as techniques for identifying the 
antecedent of anaphoric ones. The system also in- 
corporates robust techniques for processing bridg- 
ing descriptions. 
A fundamental characteristic ofour system is that 
it was developed so that its perfomlance could be 
evaluated using the annotated corpus. In the papm; 
we discuss how we arrived at the optimal version of 
the system by measuring the performance of each 
method in this way. Because of the problems ob- 
served in our previous tudy concerning agreement 
between annotators, we evaluated the system both 
by measnring precision/recall against a 'gold stan- 
dard' and by meastu'ing the agreement between the 
annotation it produces and the mmotators. 
2 General Overview 
At the moment, the only systems engaged in se- 
mantic interpretation whose performance can be 
evaluated on l'aMy unrestricted text such as the 
Wall Street Journal articles are based on a shallow- 
processing approach, i.e., that do not rely on exten- 
sive amounts of hand-coded commonsense knowl- 
edge (Carter, 1987; Appelt, 1995; Humphreys et al, 
1998). I Our system is of this type: it only relies 
on structural information, on the infommtion pro- 
vided by pre-existing lexical sources uch as Word- 
Net (Fellbaum, 1998), on minimal amouuts of gen- 
eral hand-coded iuformation, and on information 
that can be acquired automatically from a corpus. 
Although we believe that quantitative ewduations of 
the performance of a systeln on a large number of 
examples are the only true assessment of its perfor- 
mance, and therefore a shallow processing approach 
is virtually unavoidable for implemented systems 
until better sources of commonsense knowledge be- 
come available, we do know that this approach liln- 
its the performance of a system on those instances 
of definite descriptions which do require common- 
sense knowledge for their resolution. (We grouped 
these in what we call the 'bridging' chtss.) We 
I Most systems participating in the Message Understanding 
Conference (MUC) evaluations are customized tospecific do- 
nmins by adding hand-coded commonsense knowledge. 
899 
nevertheless developed heuristic techniques for pro- 
cessing these types of delinilcs its well, which n/ay 
provide a baseline against which the hains in perfor-- 
nlanco (\]lit to tile llSO oi: COlllnlOllSellse knowlodoe  
can be assessed more clearly. 
Our system attempts to classify each deli- 
nitc description as either I)IRIX:T ANAI' I tORA, 
I)IS('.()UI~,SIT,-NI';W, all(t IgRII)GING I)lv.S(21{II'TION. 
The lirsl chlss includes deihfite descriptions whose 
head is identical to thai o1' their antecedent, as in 
a Iiouse ... lhe house? The second includes del L 
inile descriptions that refer to objects not alma@ 
mentioned in the text and ,lot rclated to any such 
object. (Some of these definite descriptions refer 
to objects whose existence is widely known, such 
as discourse-initial references to lhe i)ot;e; other in- 
stances of discourse-new descriptions refer to o1:> 
jects thai can be assumed to bc unique, even if unfa~ 
miliar, such as lhe.filwl woman lo climb all Scollish 
Mum'os.) lqnally, we classify as bridging descrip- 
tions all dclinitc descriptions whose resolution de-. 
pen(Is on knowledge of relalions between objects, 
such as delinite descriptions thai refer to an object 
rehlted 1o an entity ah'eady introduced in the dis-- 
com'se by a relation other than identity (Prince's 
'inlerrables'), as in the f la t . . ,  the living, room; and 
de/tulle descriptions that refer an object aheady m 
troduced, but using a different predicate, as in Ihe 
car . . . lhe  vehicle, hi addition to this chlssitica- 
lion, the system iries to identify the antecedents 
of anaphoric descriptions and the anchors (Fraurud, 
1990) of bridging ones. Accordingly, we developed 
three types of heuristics: 
, for resolving directly anaphoric descriptions. 
These iuclude heuristics for dealing with see- 
mentation and to handle modificatiou. 
? for identifying discourse-new descriptions. 
Some of these heuristics attempt to recognize 
semantically lunctional definite descriptions 
(Hawkins, 1978; Loebner, 1987), whereas oth- 
ers try to recognize definite descriptions that 
are anchored via their modification (Clark and 
Marshall, 1981; Prince, 1981). 
,, for identifying the anchor of a bridging de- 
scription and the semantic relation between the 
bridging description and its anchor. WordNet 
is accessed, and heuristics for named entity 
recognition were also developed. 
The final configuration of the system was arrived 
at on the basis of an extensive wfluation of the 
hemislics using Ihe corpus annotated in our previ- 
ous work (Poesio and Vieira, 1998). The evaluation 
was used both io detemfine whMl version o1' each 
heuristic worked belier, and to identify the best o1", 
der in which to try them. 
The corl)us we used consists of 3d. texts frolu/he 
Peun Treebank I included in the ACIJl)CI (\]\])-rolu. 
20 of lhese texts were treated its 'training,, corptls'; 
this corpus contains 1040 (lclinite descril)tions, ()l' 
which 312 arc anaphoric, /192 discernso-.new, and 
204 bridging, id more texts were used as 'test 
corpus'; fllesc include d64 delinile description,% of 
which 154 haw: been classified its anaphoric, 218 as 
discourse-new, and 81 as bridging. 
3 Tlhe Hem~istics And  The i r  Per fbrmance  
3.1t /Resolving Anaplhorie Detinites 
We discuss heuristics for two sut)problcms of lhe 
lask o\[ resolving anaphoric dclinites: limitin?; ihe 
accessibility of discourse entities (seomcnlation), 
and |aking into accotln/ 1t4e information given by 
pre- and post-modiliers. See (Vieira, 1998) for a 
discussion of tile other heuristics used by the sy> 
tonl. 
Segmentalion In 
l i IE-spans limited 
MI",NTS t1131 l l lay 
general, discourse entities have 
to pra?matical ly delermined ,Slit;.. 
be nested (see, e.g., (Rcichman, 
1985; Grosz and Sidner, 1986; Fox, 1987)). E.g., 
in our corpus we found that about 10% of direct 
anal~horic detinite descriptions have more than one 
possible antecedent if segmentalion is nol taken into 
account (Vieira and Poesio, 1999). Recognizing 
the hierarchical structure of segments in a text is, 
howevm; still pretty umch an open problem, kS it 
involves reasoning about intentions; 2 better results 
have been achieved on the simpler task of 'chtlnk- 
ing' the text into approximate segments, generally 
by means of lexical density measures (Hearst, 1997) 
In fact, the lnethods Io limit the lifespan of dis- 
course entity we considered for our system were 
even simplel: One type of heuristics we looked 
at are window-based techniques, i.e., considering 
as potential antecedents only the discourse ntities 
within fixed-size windows of previous entences, al- 
lowing however for some discourse ntities to take 
a longer life span: we call this method LOOSE SEG- 
MENTATION. More specifically, a discourse entity 
is considered as potential antecedent for a definite 
2See, howeve,, (Marcu, 1999). 
900 
doscril)iiol~ w\] l ( : l i  lhc mi{oc~:dclii'~; head i~ idcniic~ll 
lo lhc doscril)lioli~s licad, nnd 
o lho poicnlial anlcc:cdoniL <', (li,,4ailcc lroul llic do-. 
,smrip/ioii b; \vidiin Ihc es/ab\]ishod window, or 
c;i sc 
o ihe ix)lcnliai aulcccdcnl is il~;cll-~t stil)~;t:ClllCli{ 
i l lC i l l iO l l ,  o r  c ls( :  
o ill~; (lciilfi ic dcscrif)iitni alid lhc 4iiicc:cxJoiii ;llC 
ktcnllcal Ni's (i i iohidhig ilic ariiclc). 
Wo a lso  coiisido'rcd mi c'vcii shill)lor I',i,;(:i,.NCY 
hc:uristic: fllis involw:s kt:cl)iu ?, a l~ibl,: iiidc:xcd l)y 
ihc" hc, ad.'; o\[  i)()ic-ntial nuli'x:cd(;lli,<;, ';llch dial the cn- 
i r j  for i iO l l i l  \[\I c(;iliahl<~ ihc iilc!cx oi il ic I:isi t)cciil (. 
rciio(', t)\[:ilii 4ui(tcc'.dciii wilh licad N. i:iiulliy~ wc (',oii- 
,<;i<icri'd moiul)hiaii{)ny, <fl~;('l,, c. lllllitiii ;iiid rc~(:cuc3,. 
Tli(~ be',<;1 rl'!;ull,<; wi;rc obi;ihicd wilh a couit)i- 
iia{i()n ()1 ilic: rccmicy mid ,<;c;?,iii('.ul~liion hcuri<~lic,<,: 
.iu,'a o,c  polciiii41 anicccdclli \[or CilCii dJl\]orc, iH lioad 
liOtlil iF, awlilal:)lc Jor r~soluiion> l lw la!d oC('llrfOllCO 
()t  ihai hc41d ilOlili. Tilt; rc,solutic)u ,~lill re.six:oh; lhc 
sc:Oill{:ill;lliOii h?llrislic (I()()SO vcrxioli). ' I ' l l ( ;  rc:(:;ll\] 
(J:,), i)ro(;i,';ioit (P) all(l IL-illt:ilStlrc (l;) ft:stlllt; Io\] l\[lo 
lwo hcurislics ;irc i)rc:sc;ntt:d hi 'l\]iblc I. 
( kmfl>iucd lic/u i:-ilics I l> I i' I I: 
"I ?CIIICIICCS -I rt}ct~llcx/ 1 "/~7-()()(A' KI.77<A ' S i. '!'7!(/< 
{~ SCIIICIICCS I rt'CCilCy I "li.~I4'J S,I\]I)()<,/~ ~gi.'~7'/<, 
'lhl)lc: I: (:OlilI)iilii/~t Ioo~;r ,'-;t;l'JllC:ili~lliOl! ;llltl i'(; 
CCiiCy lict;ri',dics 
The vcr<<doli with liighc:r I,' value: iu 'lhhlo I (4- 
.~;ciitt;liOC window i)his rt:ct:llcy) \vat.; dlO.'4t;ii aud 
u,.4od hi ill(" icsl,<; di,~mus.sod hi /he icst <ff lliis ,<;cc/iou. 
/Vom~ Mod~fie#;~' lll 9,onoial, wlioii niaichhi~,~ a (Ioli- 
liito doscriplion wilh a i)olt;utial aillocodOlit tho in- 
\[orillatiOli provided hy lho pfCllOlllhlaI ;Jild lhc l)OSl - 
noinifial pni( of lJic ll<)tili i)hraso also ll;i<<; lo I)o lakon 
iltto aoco/inl: SO, lor  O?alnplo, ~t i)\[ue cdv Cailll()t 
SClVO ~lS the anlcccdonl for l/re red cas, of lhe hottxe 
on the 1(',./7 for fl#e house on lhe righ#. ' lt iking l)ropor 
care of  the scniantic contril)uiion of these prcnlod- 
iliers would, in ~oiloral, roquiro OOliimOllSt'nso roa -  
lhc  <'-;laiidard dchnihOliS of precision aild recall ffoill ilif()i- 
n la l ion  r t : l r i cva i  \VOfC used: R :: i l l ln lbc,  r of ottIocIS ('f type A 
corrcclly idcnlilicd I)y the system / tolal lltlmbcr of objccls o1" 
lyi)e A, I ) = llunfl~c:r (fl" corrccl ideniilicali(ms of oh.iccls of lypc 
A / l\[)I~11 nund}cf of objects of lyl)c A identilicd by file system, 
1;:~ RI)/1~,+1 ). 
,<;olihiF, ~ Jk)l- ilic; lilonit:lil, WC unly dcvclolxxl hc.uiiv 
iic sohilioii.~> to ihc prol)lcni> hichidhil,.: 
al lowiug all anlcccdcnl to n-ialch with a dcli 
nik; dtJscripl\]oli i I  lho l)lcn\]odificrs ol: tile d('- 
scrip/ion illt: ~1 ,'4ttl),'4t;l t)l lhc l)rc',niodiiiurs (>1 
ihu, aiilcccdciii. 'rhi,<~ heuristic deals wiih &' l  t- 
hiiiot-; whicl i  mmlain io~,s inlornmlion lhan lilt; 
;tillt;t;c;dc;li/~ Silch ~i,<; f i l l  0/(\[  VicloFidn hotlmd... 
/hu /IOlt,~d~ ~lli(t t)rt:vt;il/S Illli/c;ho,~; ,~;uch zis ih? 
/)!/A'iH(tXX COIIHH\[Illi\[Y... lhc fOlllL~(t#; IIIf)F(f (l(;-. 
i ivixl I)l(/(:k l)oliii(:al (:ommus~i/y. 
~, al lowing a n(m-i)rolnodilicd auteccdoilt lo 
lllaich wilh lilly SalllO head definite. Tll is t ;cc  
oud ht;ulixlic (!c'~il~; with dc, iinitc'.~ lhai (:onlilhl 
nddilioii;il i i l \[ornlaiion, such ;l!', (~ check?. ,  llm 
Io:>7 check. 
The ll'MIIi,~; o\[  ~)lu i)roniodiJit;r in,lithium, ~ll~to-. 
ri lhni art: l);cs(:iilod hi Table 2. hi lhai Tal)lc; we: al,~;o 
.<;how lhc: fO.~lilb; oblahlod wilh a niodiliod lnatchhig 
algoril lnn hicludhil,> ~i ihird rulo> thai allows ;i lm; ? 
modified nnlLc;Ccdoni io nialch with a dclhfiio wl~o,~;o 
xcl o l  i)rc-liiodilic;rs i,s a SUl)Oisol of lhc s('i of iuodi- 
lit'r.~ o1: lho aiilccc, donl (Jill claboralion (ff rule 2). \Vc 
Ic~dcd ciicli of lllcso lhrt;o hourislics alollt: mid ihcir 
coiiil)iiialit)l/,<;. (Tlic. Iourih liliC .~hnl)ly lc;i)~',;il,~ lli~: 
lc, sldis showu in ' lhl4c 1.) 
Alllcccdci/\[s clcclion 14 1 1' 
I. Aiil -,';till )csc'- subscl ()9.197</< , 91.21% 
2. Ani-cinply 3_'i. 12% <$8.2()</~ 
3. Ant suhsct/I)csc, sci ().I.'t4{A 
i,i,a 7 0.i~<: ,,.> 7s-'~U 15/.H% 
1 and 3 "15.96% ~1.13% 
None 7g.. ~2% g .03% 
Tal)h" ?,: l:,valualioll of ihc houristics 
cqlIoII (Vt'l'Sit)ll 1 ) 
I I: 
7g. I "%4 
I 0',I.~59; 
'iq.,'11(/< 
~J..,!:,1% I 
,*~ I. I (t/< 
~(. o% 
Ik)r laCJm)dili- 
The I)csl inccisiol\] is achieved by lhc niatchin~,, al. 
gori l lnn Ihm does not  allow for new information in 
tilt; anaphoric expression, but i l ic best results ovc> 
all arc again obtained by combining rule I and rule 
2, alfllough oilhcr 2 or 3 works equally wcll when 
combinod with 1. 
Ovc#wll results .for amqUioric Ue.finite Uescriplions 
'lk) sumnm|'izc, lhc version of the system that 
achiow, s the best rcsulls as far as anal)hOl'iC dcl ini lc 
descriptions arc concerned includes : 
I. combined scgmmHation and roconcy~ 
901 
2. 4-sentence window, 
3. considering indefinites, definites and posses- 
sives as potential antecedents (Vieira, 1998), 
4. the pmmodification of the description must be 
contained in the premodificatiou of the an- 
tecedent when the antecedeut has no premodi- 
tiers. 
3.2 Heuristics for Recognizing Discourse-New 
Descriptions 
As mentioned above, a central characteristic of our 
system is that it also includes heuristics for recog- 
nising discourse-new descriptions (i.e., definite de- 
scriptions that introduce new discourse ntities) on 
the basis of syntactic and lexical features of the 
noun phrase. Our heuristics are based on the dis- 
cussion by Hawkins (1978), who identified a num- 
ber of correlations between certain types of syntac- 
tic structure and discourse-new descriptions, par- 
ticularly those that he called 'unfamiliar' definites 
(i.e., those whose existence cannot be expected to 
be known ou the basis of generally shared knowl- 
edge), including: 
? the presence of 'special predicates':4 
- the occurrence of pre-modifiers uch as 
first or best when accompanied with full 
relatives, e.g., the .\[irs't peJwon to sail 
to America (Hawkius calls these 'un- 
explanatory modifiers'; Loebner (1987) 
showed how these predicates may license 
the use of definite descriptions in an ac- 
count of definite descriptions based on 
functionality); 
- a head noun taking a complement such 
as the fact that there is li\['e on Earth 
(Hawkins calls this subclass 'Nt' comple- 
ments'); 
? the presence of restrictive modification, as in 
the inequities of the current land-ownership 
system. 
Our system attempts to recognize these syntactic 
patterus; in addition, it considers as unfamiliar some 
definites occurring in 
4This list was developed by hand; more recently, Bean and 
Riloff (1999) proposed methods for autolnatically extracting 
fl'om a corpus uch special predicates, i.e., heads that correlate 
well with discourse novelty. 
? appositive coustructions (e.g., Glenn Cox, the 
president of Phillips Petroleum Co.); 
* copular constructions (e.g.,the man most likely 
to gain custody of all this is a career politician 
named David Dinkins). 
In our corpus study (Poesio and Vieira, 1998) 
we found that our subjects did better at ideutify- 
ing discourse-new descriptions all together (K=.68) 
than they did at distinguish 'unfamiliar' from 'larger 
situation' (Hawkins, 1978) cases (K = .63). This 
finding was confirmed by our implementation: al- 
though each of the heuristics is designed, in princi- 
ple, to identify only one of the uses (larger situation 
or unfamiliar), they work better when used all to- 
gether to the class of discourse new descriptions. 
The overall recall and precision results for the 
heuristics for identifying discourse new descriptions 
are shown in Table 3. In this Table we do not distin- 
guish between the two types of discourse-new de- 
scriptions, 'unfamiliar' and 'larger-situation'. The 
column headed by (#) represents the number of 
cases of descriptions classified as discourse new in 
the standard annotation; + indicates the total num- 
ber of discourse-new descriptions correctly identi- 
fied; - the nmnber of errors. These results are for the 
version of the system (version 1) that uses the best 
version of the heuristics for dealing with anaphoric 
descriptions discussed above, and that doesn't at- 
tempt o resolve bridging descriptions. 
I Discourse new l# \] + l -  I P" I P I F I 
Training data 492 368 60 75% 86% 80% 
Test data 218 151 58 69% 72% 70% 
Table 3: Evaluation of the heuristics for identifying 
discourse new descriptions 
3.3 Bridging Descriptions 
Bridging descriptions are tile class of definite de- 
scriptions which a shallow processing system is 
least equipped to handle, and therefore the most cru- 
cial indicator of where commonsense knowledge is 
actually needed. We knew from the start that in 
general, a system can only resolve certain types of 
bridging descriptions when supplied with an ade- 
quate kuowledge base; in fact, the typical way of 
implementing a system for resolving bridging ref- 
erences has been to restrict tim domain and feed 
the system with hand-coded world knowledge (see, 
e.g., (Siduei; 1979) and especially (Carter, 1987)). 
902 
Furthermore, the relation between bridging descrip- 
tions and their anchors may be arbitrarily complex 
(Clark, 1977; Sidnm; 1979; Prince, 1981; Strand, 
1996) and our own results indicate that the stone de- 
scription may relate to different anchors in a text, 
which makes it difficult to decide what the intended 
anchor and the intended link are (Poesio and Vieira, 
1998). Nevertheless, we feel that trying to process 
these definite descriptions is the only way to dis- 
cover which types of commonsense knowledge are 
actually needed.. 
We began by developing a classilication of bridg- 
ing descriptions according to the kind of informa- 
tion needed to resolve them, rather than on the ba- 
sis of the possible relations between descriptions 
and their anchors as usually done in the literature 
(Vieira, 1998). This allowed us to get an idea 
of what types of bridging descriptions our system 
might be able to resolve. We classified definite de- 
scriptions as follows: 
? cases based on well-delined lexical relations, 
such as synonymy, hypernymy and meronymy, 
that can be found in a lexical database such as 
WordNet (Fellbaum, 1998)-as in thef lat. . ,  lhe 
living room; 
? bridging descriptions in which the antecedent 
is a proper name and the description a common 
noun, whose resolution requires some way o1' 
recognizing the type of object denoted by the 
proper name (as in Bach ... the composer); 
? cases in which the anchor is not the head noun 
but a noun modifying an antecedent, as in the 
compaw has been selling discount packages 
... the discounts 
? cases in which the antecedent (anchor) is not 
introduced by an NP but by a vl', as in 
Kadane oil is currently drilling two oil wells. 
The activity...  
? descriptions whose the antecedent is not ex- 
plicitly mentioned in the text, but is implicitly 
available because it is a discourse topic-e.g., 
the industo, in a text referring to oil coml)a- 
nies; 
? cases in which the relation with the anchor is 
based on more general commonsense knowl- 
edge, e.g., about cause-consequence relations. 
We developed heuristics for handling the first 
three of these classes: lexical bridges, bridges based 
on names, and bridges to entities introduced by non- 
head nouns in a compound nominal. We refer the 
reader to (Vieira, 1998) for discussion of the heuris- 
tics for this last class. 
Our system attempts to resolve lexical bridges by 
consulting WordNet to determine if there is a se- 
mantic relation between the head noun of the de- 
scription and the head noun of one of the NI'S in the 
previous five sentences. The results of this search 
for our training corpus, in which 204 descriptions 
are classified as bridging, are shown in Table 4. It is 
interesting to note that the semantic relations found 
in this automatic semch were not always those ob- 
served in our manual analysis. 
Bridging 
Class 
Synonimy 
Hyponimy 
Meronimy 
Sister 
Relations 
Found 
11 
59 
30 
Total 106 30 
Right % 
Anchors Right 
4 36% 
18 30% 
2 33% 
6 20% 
28% 
Table 4: Ewduation of the search for anchors using 
WordNet 
We developed a simple heuristic method for as- 
signing types to named entities. Our method iden- 
titied entity types for 66% (535/814) of all names 
in the corpus (organizations, persons and locations). 
The precision was 95%. We could have had a better 
recall if we had adopted more comprehensive lists 
of cue words, or consulted dictionaries of names 
as done for the systems participating in MUC-6. 
There, recall in the named entity task varies t'rom 
82% to 96%, and precision l'rom 89% to 97%. 5 
4 Overall Evaluation of the System 
The order of application of heuristics is as impof  
tant as the heuristics themselves. The final order of 
application was also arrived at on the basis of an ex- 
tensive evaluation (Vieira, 1998), and is based on 
the l'ollowing strategy: 6 
5A more recent version o1' the system using the named en- 
tity recognition software developed by ItCRC for the MUC-7 
competition (Mikheev el al., 1999) is discussed in (Isbikawa, 
1998). 
aWe also attempted tolearn the best order of application of 
the heuristics automatically b  means of decision tree learn- 
ing algorithms (Quinlan, 1993), without however observing a
signflicant difference in pcrfommnce. See (Vieira, 1998) for 
details. 
903 
An Empirically Based System for 
Processing Definite Descriptions 
Renata Vieira* 
Universidade do Vale do Rio dos Sinos 
Mass imo Poesio t
University of Edinburgh 
We present an implemented system for processing definite descriptions in arbitrary domains. 
The design of the system is based on the results of a corpus analysis previously reported, which 
highlighted the prevalence ofdiscourse-new descriptions in newspaper corpora. The annotated 
corpus was used to extensively evaluate the proposed techniques for matching definite descriptions 
with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and 
suggesting anchors for bridging descriptions. 
1. Introduction 
Most models of definite description processing proposed in the literature tend to 
emphasise the anaphoric role of these elements. 1 (Heim \[1982\] is perhaps the best for- 
malization of this type of theory). This approach is challenged by the results of exper- 
iments we reported previously (Poesio and Vieira 1998), in which subjects were asked 
to classify the uses of definite descriptions in Wall Street Journal articles according 
to schemes derived from proposals by Hawkins (1978) and Prince (1981). The results 
of these experiments indicated that definite descriptions are not primarily anaphoric; 
about half of the time they are used to introduce a new entity in the discourse. In 
this paper, we present an implemented system for processing definite descriptions 
based on the results of that earlier study. In our system, techniques for recognizing 
discourse-new descriptions play a role as important as techniques for identifying the 
antecedent of anaphoric ones. 
A central characteristic of the work described here is that we intended from the 
start to develop a system whose performance could be evaluated using the texts an- 
notated in the experiments mentioned above. Assessing the performance of an NLP 
system on a large number of examples is increasingly seen as a much more thorough 
evaluation of its performance than trying to come up with counterexamples; it is con- 
sidered essential for language ngineering applications. These advantages are thought 
by many to offset some of the obvious disadvantages of this way of developing NLP 
theories-- in particular, the fact that, given the current state of language processing 
technology, many hypotheses of interest cannot be tested yet (see below). As a result, 
quantitative valuation is now commonplace in areas of language engineering such 
as parsing, and quantitative valuation techniques are being proposed for semantic 
* Universidade do Vale do Rio dos Sinos - UNISINOS, Av. Unisinos 950 - Cx. Postal 275, 93022-000 Sao 
Leopoldo RS Brazil. E-mail: renata@exatas.unisinos.br 
t University of Edinburgh, ICCS and Informatics, 2 Buccleuch Place, EH8 9LW Edinburgh UK. E-maih 
Massimo.Poesio@ed.ac.uk 
1 We use the term definite description (Russell 1905) to indicate definite noun phrases with the definite 
article the, such as the car. We are not concerned with other types of definite noun phrases uch as 
pronouns, demonstratives, or possessive descriptions. Anaphoric expressions are those linguistic 
expressions u ed to signal, evoke, or refer to previously mentioned entities. 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 4 
interpretation as well, for example, at the Sixth and Seventh Message Understand- 
ing Conferences (MUC-6 and MUC-7) (Sundheim 1995; Chinchor 1997), which also 
included evaluations of systems on the so-called coreference task, a subtask of which 
is the resolution of definite descriptions. The system we present was developed to be 
evaluated in a quantitative fashion, as well, but because of the problems concerning 
agreement between annotators observed in our previous tudy, we evaluated the sys- 
tem both by measuring precision/recall against a "gold standard," as done in MUC, 
and by measuring agreement between the annotations produced by the system and 
those proposed by the annotators. 
The decision to develop a system that could be quantitatively evaluated on a large 
number of examples resulted in an important constraint: we could not make use of 
inference mechanisms such as those assumed by traditional computational theories 
of definite description resolution (e.g., Sidner 1979; Carter 1987; Alshawi 1990; Poesio 
1993). Too many facts and axioms would have to be encoded by hand for theories 
of this type to be tested even on a medium-sized corpus. Our system, therefore, is 
based on a shallow-processing approach more radical even than that attempted by 
the first advocate of this approach, Carter (1987), or by the systems that participated 
in the MUC evaluations (Appelt et al 1995; Gaizaukas et al 1995; Humphreys et al 
1988), since we made no attempt o fine-tune the system to maximize performance 
on a particular domain. The system relies only on structural information, on the in- 
formation provided by preexisting lexical sources uch as WordNet (Fellbaum 1998), 
on minimal amounts of general hand-coded information, or on information that could 
be acquired automatically from a corpus. As a result, the system does not really have 
the resources to correctly resolve those definite descriptions whose interpretation does 
require complex reasoning (we grouped these in what we call the "bridging" class). 
We nevertheless developed heuristic techniques for processing these types of definites 
as well, the idea being that these heuristics may provide a baseline against which the 
gains in performance due to the use of commonsense knowledge can be assessed more 
clearly. 2
The paper is organized as follows: We first summarize the results of our previous 
corpus study (Poesio and Vieira 1998) (Section 2) and then discuss the model of deft- 
nite description processing that we adopted as a result of that work and the general 
architecture of the system (Section 3). In Section 4 we discuss the heuristics that we 
developed for resolving anaphoric definite descriptions, recognizing discourse-new 
descriptions, and processing bridging descriptions, and, in Section 5, how the per- 
formance of these heuristics was evaluated using the annotated corpus. Finally, we 
present he final configuration of the two versions of the system that we developed 
(Section 6), review other systems that perform similar tasks (Section 7), and present 
our conclusions and indicate future work (Section 8). 
2. Preliminary Empirical Work 
As mentioned above, the architecture of our system is motivated by the results con- 
cerning definite description use in our corpus, discussed in Poesio and Vieira (1998). 
In this section we briefly review the results presented in that paper. 
2 In fact, it is precisely because we are interested in identifying the types of commonsense r asoning 
actually used in language processing that we focused on definite descriptions rather than on other 
types of anaphoric expressions ( uch as pronouns and ellipsis) that can be processed much more 
effectively on the basis of syntactic information alone (Lappin and Leass 1994; Hardt 1997). 
540 
Vieira and Poesio Processing Definite Descriptions 
2.1 The Corpus 
We used a subset of the Penn Treebank I corpus (Marcus, Santorini, and Marcinkiewicz 
1993) from the ACL/DCI  CD-ROM, containing newspaper articles from the Wall Street 
Journal. We divided the corpus into two parts: one, containing about 1,000 definite 
descriptions, was used as a source during the development of the system; we will refer 
to these texts as Corpus 1. 3 The other part, containing about 400 definite descriptions, 
was kept aside during development and used for testing; we will refer to this subset 
as Corpus 2. 4 
2.2 Classifications of Anaphoric Expressions 
The best-known studies of definite description use (Hawkins 1978; Prince 1992; Frau- 
rud 1990; L6bner 1987; Clark 1977; Sidner 1979; Strand 1996) classify definite descrip- 
tions on the basis of their relation with their antecedent. A fundamental distinction 
made in these studies is between descriptions that denote the same discourse ntity 
as their antecedent (which we will call anaphoric or, following Fraurud, subsequent 
mention), descriptions that denote an object hat is in some way "associated" with the 
antecedent-- for example, it is part of it, as in a car. . ,  the wheel (these definite expres- 
sions are called "associative descriptions" by Hawkins and "inferrables" by Prince), 
and descriptions that introduce a new entity into the discourse. 
In the case of semantic identity between definite description and antecedent, a 
further distinction can be made depending on the semantic relation between the pred- 
icate used in the description and that used for the antecedent. The predicate used in 
an anaphoric definite description may be a synonym of the predicate used for the an- 
tecedent (a house ... the home), a general izat ion/hypernym (an oak.., the tree), and even, 
sometimes, a special izat ion/hyponym (a tree.., the oak). In fact, the NP introducing the 
antecedent may not have a head noun at all, e.g., when a proper name is used, as in 
Bill Clinton... the president. We will use the term direct anaphora when both description 
and antecedent have the same head noun, as in a house.., the house. Direct anaphors are 
the easiest definite descriptions for a shallow system to resolve; in all other cases, as 
well as when the antecedent and the definite description are related in a more indirect 
way, lexical knowledge, or more generally encyclopedic knowledge, is needed. 
All of the classifications mentioned above also acknowledge the fact that not all 
definite descriptions depend on the previous discourse for their interpretation. Some 
refer to an entity in the physical environment, others to objects which are assumed to 
be known on the basis of common knowledge (Prince's "discourse-new/hearer-old" 
expressions, uch as the pope), and still others are licensed by virtue of the semantics 
of their head noun and complement (as in the fact that Milan won the Italian football 
championship). 
2.3 A Study of Definite Description Use 
In the experiments discussed in Poesio and Vieira (1998) we asked our subjects to 
classify all definite description uses in our two corpora. These experiments had the 
dual objective of verifying how easy it was for human subjects to agree on the distinc- 
tions between definite descriptions just discussed, and producing data that we could 
use to evaluate the performance of a system, The classification schemes we used were 
simpler than those proposed in the literature just mentioned and were motivated, on 
3 The texts in question are w0203, w0207, w0209, w0301, w0305, w0725, w0760, w0761, w0765, w0766, 
w0767, w0800, w0803, w0804, w0808, w0820, w1108, w1122, w1124, and w1137. 
4 The articles in this second subset are w0766, wsj_0003, wsj_0013, wsj_0015, wsj_0018, wsj_0020, wsj_0021, 
wsj_0022, wsj_0024, wsj_0026, wsj_0029, wsj_0034, wsj_0037, and wsj_0039. 
541 
Computational Linguistics Volume 26, Number 4 
the one hand, by the desire to make the annotation uncomplicated for the subjects 
employed in the empirical analysis and, on the other hand, by our intention to use the 
annotation to get an estimate of how well a system using only limited lexical and en- 
cyclopedic knowledge could do. s We ran two experiments, using two slightly different 
classification schemes. In the first experiment we used the following three classes: 6 
? direct anaphora: subsequent-mention definite descriptions that refer to 
an antecedent with the same head noun as the description; 
? br idging descriptions: definite descriptions that either (i) have an 
antecedent denoting the same discourse ntity, but using a different head 
noun (as in house  . . .  bu i ld ing) ,  or (ii) are related by a relation other than 
identity to an entity already introduced in the discourse; 7 
? discourse-new: first-mention definite descriptions that denote objects not 
related by shared associative knowledge to entities already introduced in 
the discourse. 
In the second experiment we treated all anaphoric definite descriptions as part of 
one class (direct anaphora + bridging (i)), and all inferrables as part of a different class 
(bridging (ii)), without significant changes in the agreement results. 
Agreement among annotators was measured using the K statistic (Siegel and 
Castellan 1988; Carletta 1996). K measures agreement among k annotators over and 
above chance agreement (Siegel and Castellan 1988). The K coefficient of agreement is
defined as: 
K - -  P (a )  - P (E )  
1 - 
where P(A) is the proportion of times the annotators agree, and P(E) the proport ion of 
times that we would expect hem to agree by chance. The interpretation of K figures is 
an open question, but in the field of content analysis, where reliability has long been 
an issue (Krippendorff 1980), K > 0.8 is generally taken to indicate good reliability, 
whereas 0.68 < K < 0.8 allows tentative conclusions to be drawn. Carletta et al (1997) 
observe, however, that in other areas, such as medical research, much lower levels of 
K are considered acceptable (Landis and Koch 1977). 
An interesting overall result of our study was that the most reliable distinction that 
our annotators could make was that between first-mention and subsequent-mention 
(K = 0.76); the measure of agreement for the three-way distinction just discussed was 
K = 0.73. The second interesting result concerned the distribution of definite descrip- 
tions in the three classes above: we found that about half of the definite descriptions 
were discourse-new. The distribution of the definite descriptions in classes in our first 
experiment according to annotators A and B are shown in Tables 1 and 2, respec- 
tively. (Class IV includes cases of idiomatic expressions or doubts expressed by the 
annotators). 
The third main result was that we found very little agreement between our sub- 
jects on identifying briding descriptions: in our second experiment, he agreement on 
5 Previous attempts o annotate anaphoric relations had resulted in very low agreement levels; for 
example, in the coreference annotation experiments for MUC-6 (Sundheim 1995), relations other than 
identity were dropped ue to difficulties in annotating them. 
6 In this experiment, our subjects could also classify adefinite description as "idiomatic" or 
"doubt'--see tables below. 
7 In Poesio and Vieira (1998), Hawkins's term "associative" was used for this class; but in fact, the 
definition we used for the class is closest to the sense of "bridging" used by Clark (1977). 
542 

Computational Linguistics Volume 26, Number 4 
ined, Fraurud (1990, 421) claims that: 
a model where the processing of first-mention definites always in- 
volves a failing search for an already established iscourse referent as 
a first step seems less attractive. A reverse ordering of the procedures 
is, quite obviously, no solution to this problem, but a simultaneous 
processing as proposed by Bosch and Geurts (1989) might be. 
Fraurud proposes, contra Heim (1982), that processing a definite NP may in- 
volve establishing a new discourse entity. 8 This new discourse entity may then be 
linked to one or more anchors in the text or to a background referent. 9 Fraurud dis- 
cusses the example of the description the king, interpreted relationally, encountered in
a text in which no king has been previously mentioned. Lexicoencyclopedic knowl- 
edge would provide the information that a king is related to a period and a country; 
these would constitute the anchors. The selection of the anchors would identify the 
pertinent period and country, and this would make possible the identification of a 
referent: say, for the anchors 1989 and Sweden, the referent identified would be Carl 
Gustav XVI. 1? 
The most interesting aspect of Fraurud's proposal is the hypothesis that first- 
mention definites are not necessarily recognized simply because no suitable antecedent 
has been found; independent strategies for recognizing them may be involved. This 
hypothesis is consistent with L6bner's proposal (L6bner 1987) that the fundamental 
property of a definite description is that it denotes a function (in a logical sense); this 
function can be part of the meaning assigned to the definite description by the gram- 
mar (as in the beginning of X), or can be specified by context (as in the case of anaphoric 
definites). Fraurud's and L6bner's ideas can be translated into a requirement that a 
system have separate methods or rules for recognizing discourse-new descriptions 
(and in particular, L6bner's "semantically functional" definites) in addition to rules 
for resolving anaphoric definite descriptions; these rules may run in parallel with the 
rules for resolving anaphoric definites, rather than after them. 
Rather than deciding a priori on the question of whether the heuristic rules (in 
our case) for identifying discourse-new descriptions hould be run in parallel with 
resolution or after it, we treated this as an empirical question. We made the archi- 
tecture of the system fairly modular, so that we could both try different heuristics 
and try applying them in a different order, using the corpus for evaluation. We dis- 
cuss all the heuristics that we tried in Section 4, and our evaluation of them in Sec- 
tion 5. 
3.2 Architecture of Our System 
The overall architecture of our system is shown in Figure 1. The system attempts to 
classify each definite description as either direct anaphora, discourse-new, or bridging 
description. In addition to this classification, the system tries to identify the antecedents 
of anaphoric descriptions and the anchors (Fraurud 1990) of bridging descriptions. The 
8 Discourse entities are representations in the discourse model of entities explicitly mentioned (Webber 
1979; Heim 1982). 
9 Background referents are entities that have not been mentioned in the discourse--those entities that 
Grosz (1977) would call "elements ofthe implicit focus." 
10 Fraurud oes not explain what it is that justifies the use of definite descriptions, if not familiarity. In
Poesio and Vieira (1998) we suggest that L/3bner's proposal (LSbner 1987) seems to account for the 
most data. 
544 
Vieira and Poesio Processing Definite Descriptions 
Treebank 1 
~xt ract ion~ 
I 
\[NP, a, house\] 
\[NP, the, house\] 
\[S,...\[...\]\] 
-1 List of NPs and Sentences 
potential_antecedent( l,np(...),...). 
potential_antecedent(2,np(...),...). 
definite_description(3,np(...),...). 
definite_description(5,np(...),...). 
/ 
Text p rocess ing~ 
\ 
L i~ngu ist ic ~ 
CZ?  
Discourse r presentation 
dd_class(3,anaphora). 
dd_class(5,bridging). 
dd_class(6,discourse_new). 
corer(3,1 ). 
bridging(5,2). 
coref_chain(\[ 13\]). 
total(anaphora,22). 
total(bridging,9). 
total(discourse_new,28). 
System's results 
Figure 1 
System architecture. 
system processes parsed newswire texts from the Penn Treebank I, constructing a fairly 
simple discourse model that consists of a list of discourse ntities that may serve as 
potential antecedents (which we call simply potential antecedents), according to the 
chosen segmentation algorithm (see below). The system uses the discourse model, 
syntactic information, and a small amount of lexical information to classify definite 
descriptions as discourse-new or to link them to anchors in the text; WordNet is also 
consulted by the version of the system that attempts to resolve bridging descriptions. 
The system is implemented in Sicstus Prolog. 
545 
Computational Linguistics Volume 26, Number 4 
Input. The texts in the Penn Treebank corpus consist of parsed sentences represented 
as Lisp lists. During a preprocessing phase, a representation i Prolog list format 
is produced for each sentence, and the noun phrases it contains are extracted. The 
output of this preprocessing phase is passed to the system proper. For example, the 
sentence in (1) is represented in the Treebank as (2) and the input to the system after the 
preprocessing phase is (3). 11 Note that all nested NPs are extracted, and that embedded 
NPs such as the Organization of Petroleum Exporting Countries are processed before the 
NPs that embed them (in this case, the squabbling within the Organization of Petroleum 
Exporting Countries). 
(1) 
(2) 
(3) 
Mideast politics have calmed down and the squabbling within the 
Organization of Petroleum Exporting Countries eems under control for now. 
( (s (s 
(NP Mideast politics) 
have 
(VP calmed 
down)) 
and 
(S (NP the squabbling 
(PP within 
(NP the Organization 
(PP of 
(NP Petroleum Exporting Countries))))) 
(VP seems 
(PP under 
(NP control))) 
(PP for 
(NP now)))) 
.) 
\[NP,Mideast,politics\]. 
\[NP,Petroleum,Exporting,Countries\]. 
\[NP,the,Organization, 
\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\]. 
\[NP,the,squabbling,\[PP,within,\[NP,the,0rganization, 
\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\]\]\]. 
\[NP,control\]. 
\[\[S,\[S,\[NP,Mideast,politics\],have,\[VP,calmed, 
\[PP,down\]\]\],and,\[S,\[NP,the,squabbling,\[PP,within, 
\[NP,the,Drganization,\[PP,of,\[NP,Petroleum,Exporting, 
Countries\]\]\]\]\],\[VP,seems,\[PP,under,\[NP,control\]\], 
\[PP,for,now\]\]\]\],.\]. 
Output. The system outputs the classification it has assigned to each definite descrip- 
tion in the text, together with the coreferential nd bridging links it has identified. 
11 Prolog variables will be indicated in the rest of the paper by the use of .... at the beginning and end of 
the variables; e.g., _X_ for variable X. 
546 

Computational Linguistics Volume 26, Number 4 
is coordination: for example, our algorithm does not recognize that a noun such as 
reporters below is a head noun: 
\[NP, reporters,and, editors,\[PP, of \[NP, The,WSJ\]\]\]. 
4.1.2 Potential Antecedents. The second problem is to determine which NPs should 
be used to resolve definite descriptions, among all those in the text. The system keeps 
track of NP index, NP structure, head noun, and NP type (definite, indefinite, bare 
plural, possessive n) of each potential antecedent, as illustrated by (5) below. 
(5) potential_antecedent(l,np(NP),head(H),type(T)). 
Examples of potential antecedents extracted from (6) are shown in (7): 
(6) 
(7) 
In an interview with reporters of the Wall Street Journal, the candidate 
appears quite confident of victory and of his ability to handle the 
mayoralty. 
a. 
potential_antecedent(l,np(_NPstructure), 
head(reporters), 
type(indef)). 
potential antecedent(2,np( NPstructure ), 
head(interview), 
type(indef)). 
b? 
potential_antecedent(3,np(_NPstructure_), 
head(3ournal), 
type(def)). 
potential antecedent(4,np( NPstructure ), 
head(candidate), 
type(def)). 
potential antecedent(5,np(NPstructure_), 
head(mayoralty), 
type(def)). 
C. 
potential_antecedent(6,np(_NPstructure_), 
head(ability), 
type(possessive)). 
d. 
potential_antecedent(7,np(_NPstructure_), 
head(victory), 
type(other)). 
We found that different recall/precision trade-offs can be achieved epending on the 
choice of potential antecedents--i.e., depending on whether all NPs are considered 
as possible antecedents, or only indefinite NPs, or various other subsets--so we ran 
experiments to identify the best group of potential antecedents. Four different NP 
12 Other NPs not included in any of these categories are identified as type(other). 
548 , 
Vieira and Poesio Processing Definite Descriptions 
subsets were considered: 
. 
. 
. 
. 
indefinite NPs, defined as NPs containing the indefinite articles a, an, 
some and bare/cardinal plurals, as in (7a); 13 
indefinite NPs and definite descriptions (NPs beginning with the definite 
article) ((7a) and (7b)); 
indefinite NPs, definite descriptions, and possessive NPs (NPs with a 
possessive pronoun or possessive mark) ((7a), (7b) and (7c)); 
all NPs ((7a), (7b), (7c)) and (7d)). 
The results obtained by considering each subset of the total number of NPs as potential 
antecedents are discussed in Section 5.2. 
4.1.3 Segmentation. The set of potential antecedents of anaphoric expressions i also 
restricted by the fact that antecedents end to have a limited "life span"--i.e., they 
only serve as antecedents for anaphoric expressions within pragmatically determined 
segments of the whole text (see, for example, Reichman \[1985\], Grosz and Sidner 
\[1986\] and Fox \[1987\]). In our corpus we found that about 10% of direct anaphoric 
definite descriptions have more than one possible antecedent if segmentation is not 
taken into account (Vieira and Poesio 1996). In (8), for example, the antecedent of 
the housej mentioned in sentence 50 is not the house mentioned earlier in sentences 2 
and 19, but another (nonmobile) house implicitly introduced in sentence 49 by the 
reference to the yard. 
(8) 2. A deep trench now runs along its north wall, exposed when the housei 
lurched two feet off its foundation during last week's earthquake? 
? ? ? 
19. Others grab books, records, photo albums, sofas and chairs, working 
frantically in the fear that an aftershock will jolt the housei again. 
20. The owners, William and Margie Hammack, are luckier than many 
others? 
49. When Aetna adjuster Bill Schaeffer visited a retired couple in 
Oakland last Thursday, he found them living in a mobile homek parked in 
front of their yard. 
50. The housej itself, located about 50 yards from the collapsed section of 
double-decker highway Interstate 880, was pushed about four feet off its 
foundation and then collapsed into its basement. 
? . ?  
65. As Ms. Johnson stands outside the Hammack housei after winding up 
her chores there, the house/begins to creak and sway. 
13 Only plural nouns ending in s are handled by the system. 
549 
Computational Linguistics Volume 26, Number 4 
In general, it is not sufficient o look at the most recent antecedents only: this 
is because segments are organized hierarchically, and the antecedents introduced in 
a segment at a lower level are typically not accessible from a segment at a higher 
level (Fox 1987; Grosz 1977; Grosz and Sidner 1986; Reichman 1985), whereas the 
antecedents introduced in a prior segment at the same level may be. Later in (8), 
for example, the housej in sentence 50 becomes inaccessible again, and in sentence 65, 
the text starts referring again to the house introduced in sentence 2. Automatically 
recognizing the hierarchical structure of texts is an unresolved problem, as it involves 
reasoning about intentions; 14 better esults have been achieved on the simpler task of 
"chunking" the text into sequences of segments, generally by means of lexical density 
measures (Hearst 1997; Richmond, Smith, and Amitay 1997). 
The methods for limiting the life span of discourse ntities that we considered 
for our system are even simpler. One type of heuristic we looked at are window- 
based techniques, i.e., considering only the antecedents within fixed-size windows of 
previous entences, although we allow some discourse ntities to have a longer life 
span: we call this method loose segmentation. More specifically, a discourse ntity 
is considered a potential antecedent for a definite description when the antecedent's 
head is identical to the description's head, and 
? the potential antecedent is within the established window, or else 
? the potential antecedent is itself a subsequent mention, or else 
? the definite description and the antecedent are identical NPs (including 
the article). 
We also considered an even simpler ecency heuristic: this involves keeping atable 
indexed by the heads of potential antecedents, such that the entry for noun N contains 
the index of the last occurrence of an antecedent with head N. Finally, we considered 
combinations of segmentation a d recency. (The results of these two heuristics are 
compared in Section 5.2. 
4.1.4 Noun Modifiers. Once the head nouns of the antecedent and of the description 
have been identified, the system attempts to match them. This head-matching strategy 
works correctly in simple cases like (9): 
(9) Grace Energy hauled a rig here ... The rig was built around 1980. 
In general, however, when matching a definite description with a potential an- 
tecedent the information provided by the prenominal nd the postnominal part of the 
noun phrases also has to be taken into account. For example, a blue car cannot serve 
as the antecedent for the red car, or the house on the left for the house on the right. In our 
corpus, cases of antecedents that would incorrectly match by simply matching heads 
without regarding premodification include: 
(10) a. the business community .. .  the younger, more activist black political 
community; 
b. the population.., the voting population. 
14 See, however, Marcu (1999). 
550 
Vieira and Poesio Processing Definite Descriptions 
Again, taking proper account of the semantic ontribution of these premodif iers would, 
in general, require commonsense reasoning. For the moment,  we only developed 
heuristic solutions to the problem, including: 
? al lowing an antecedent to match with a definite description if the 
premodif iers of the description are a subset of the premodif iers of the 
antecedent. This heuristic deals with definites that contain less 
information than the antecedent, such as an old Victorian house .. .  the 
house, and prevents matches uch as the business community .. .  the 
younger, more activist black political community. 
? al lowing a nonpremodif ied antecedent to match with any same head 
definite. This second heuristic deals with definites that contain additional 
information, such as a check.. ,  the lost check. 
The information that two discourse entities are disjoint may come from postmod- 
ification, as well, although same head antecedents with different postmodification are 
not as common as those with differences in premodification. An example from our 
corpus is shown in (11). 
(11) a chance to accomplish several objectives ... the chance 
to demonstrate an entrepreneur 
like himself could run Pinkerton's better than an unfocused conglomerate or
investment banker. 
The heuristic method we developed to deal with postmodification is to compare the 
description and antecedent, preventing resolution in those cases where both are post- 
modif ied and the modifications are not the same. (These results are also discussed in 
Section 5.2.) 
4.2 Discourse-New Descriptions 
As mentioned above, a fundamental  characteristic of our system is that it also includes 
heuristics for recognizing discourse-new descriptions (i.e., definite descriptions that 
introduce new discourse entities) on the basis of syntactic and lexical features of the 
noun phrase. Our heuristics are based on the discussion in Hawkins (1978), who 
identified a number  of correlations between certain types of syntactic structure and 
discourse-new descriptions, particularly those he called "unfamil iar" definites (i.e., 
those whose existence cannot be expected to be known on the basis of generally 
shared knowledge), including: is 
the presence of "special predicates": 
the occurrence of premodif iers uch as first or best when 
accompanied with full relatives, e.g., the first person to sail to 
America (Hawkins calls these "unexplanatory modifiers"; LObner 
15 Hawkins himself proposes a transformation-based account of unfamiliar definites, but the correlations 
he identified proved to be a useful source of heuristics for identifying these uses of definite 
descriptions even though the existence of counterexamples to these heuristics uggests that a 
syntactic-based account cannot be entirely correct. Most of these examples can be accounted for in 
terms of L6bner's theory of definiteness. 
551 
Computational Linguistics Volume 26, Number 4 
\[1987\] showed how these predicates may license the use of 
definite descriptions in an account of definite descriptions based 
on functionality); 
a head noun taking a complement such as the fact that there is life 
on Earth (Hawkins calls this subclass "NP complements"); 
the presence of restrictive modification, as in the inequities of the current 
land-ownership system. 
Our system attempts to recognize these syntactic patterns. We also added heuristics 
classifying as unfamiliar some definites occurring in 
? appositive constructions (e.g., Glenn Cox, the president of Phillips 
Petroleum Co.); 
? copular constructions (e.g., the man most likely to gain custody of all this is a 
career politician named David Dinkins). 
(The reason definite descriptions in appositive and copular constructions tend to be 
discourse-new, in fact unfamiliar, is that the information eeded for the identification 
is given by the NP to which the apposition is attached and the predicative part of the 
copular construction, respectively. 16) 
Finally, we found that three classes of what Hawkins called "larger situation" 
definites (those whose existence can be assumed to be known on the basis of encyclo- 
pedic knowledge, such as the pope) can also be recognized on the basis of heuristics 
exploiting syntactic and lexical features: 
? definites that behave like proper nouns, like the United States; 
? definites that have proper nouns in their premodification, such as the 
Iran-Iraq war; 
? definites referring to time, such as the time or the morning. 
In our corpus study we found that our subjects did much better at identifying 
discourse-new descriptions all together (K = 0.68) than they did at distinguishing 
unfamiliar from larger situation cases (K = 0.63). This finding was confirmed by our 
implementation: although each of the heuristics is designed, in principle, to identify 
only one of the uses (larger situation or unfamiliar), they work better at identifying 
together the whole class of discourse-new descriptions. 
4.2.1 Special Predicates. Some cases of discourse-new definite descriptions can be 
identified by comparing the head noun or modifiers of the definite NP with a list of 
predicates that are either functional or likely to take a complement (L6bner 1987). Our 
list of predicates that, when taking NP complements, are generally used to introduce 
discourse-new entities, was compiled by hand and currently includes the nouns fact, 
result, conclusion, idea, belief, saying, and remark. In these cases, what licenses the use of 
a definite is not anaphoricity, but the fact that the head noun can be interpreted as 
16 In the systems participating inMUC, definite descriptions occurring in appositions are treated as 
anaphoric on the preceding NP; our system considers the NP and the apposition as a unit that 
introduces a new referent to the discourse. 
552 
Vieira and Poesio Processing Definite Descriptions 
semantically functional; the noun complement specifies the argument of the function. 
Functionality is enough to license the use of the definite description (LObner 1987). An 
example of definite description classified as discourse-new on these grounds is given 
in (12). 
(12) Mr. Dinkins also has failed to allay Jewish voters' fears about his 
association with the Rev. Jesse Jackson, despite the fact that few local 
non-Jewish politicians have been as vocal for Jewish causes in the past 20 years as 
Mr. Dinkins has. 
When encountering a definite whose head noun occurs in this list, the system 
checks if a complement is present or if the definite appears in a copular construction 
(e.g., the fact is that...). 
A second list of special predicates consulted by the system includes what Hawkins 
called unexplanatory modifiers: these include adjectives uch as -first, last, best, most, 
maximum, minimum, and only and superlatives in general. 17. All of these adjectives are 
predicate modifiers that turn a head noun into a function, therefore again--according 
to LObner--licensing the use of a definite even when no antecedent is present (see 
examples below). When applying this heuristic, the system verifies the presence of a 
complement for some of the modifiers (first, last), but not for superlatives. 
(13) a. Mr. Ramirez just got the first raise he can remember in eight years, to $8.50 
an hour from $8. 
b. She jumps at the slightest noise. 
Finally, our system uses a list of special predicates that we found to correlate well 
with larger situation uses (i.e., definite descriptions referring to objects whose existence 
is generally known). This list consists mainly of terms indicating time reference, and 
includes the nouns hour, time, morning, afternoon, night, day, week, month, period, quarter, 
year, and their respective plurals. An example from the corpus is: 
(14) Only 14,505 wells were drilled for oil and natural gas in the U.S. in the 
first nine months of the year. 
Other definites typically used with a larger situation interpretation are the moon, 
the sky, the pope, and the weather. 
It should be noted that although these constructions may indicate a discourse- 
new interpretation, these expressions may also be used anaphorically; this is one of 
the cases in which a decision has to be made concerning the relative priority of differ- 
ent heuristics. We discuss this issue further in connection with the evaluation of the 
system's performance in Section 5.18 
4.2.2 Restrictive and Nonrestrictive Modification. A second set of heuristics for iden- 
tifying discourse-new descriptions that we derived from Hawkins's suggestions and 
17 The list should be made more comprehensive; so far it includes the cases observed in the corpus 
analysis and a few other similar modifiers. 
18 More recently, Bean and Riloff (1999) have proposed methods for automatically extracting from a 
corpus heads that correlate well with discourse novelty. 
553 
Computational Linguistics Volume 26, Number 4 
Table 3 
Distribution of prepositional phrases and relative clauses. 
Restrictive Postmodification # % 
Prepositional phrases 152 77% 
Relative clauses 45 23% 
Total 197 100 % 
from our corpus analysis look for restrictive modification. 19We developed patterns 
to recognize restrictive postmodification and nonrestrictive postmodification; we also 
tested the correlation between discourse novelty and premodification. We discuss each 
of these heuristics in turn. 
Restrictive Postmodification. Hawkins (1978) pointed out that unfamiliar definites often 
include referent-establishing relative clauses and associative clauses, while warning 
that not all relative clauses are referent-establishing. Some statistics about this corre- 
lation were reported by Fraurud (1990): she found that in her corpus 75% of complex 
definite NPs (i.e., modified by genitives, postposed PPs, restrictive adjectival modifiers) 
were first-mention. A great number of definite descriptions with restrictive postmod- 
ifiers are unfamiliar in our corpus as well (Poesio and Vieira 1998); in fact, restrictive 
postmodification was found to be the single most frequent feature of first-mention de- 
scriptions. Constructions of this type are good indicators of discourse novelty because 
a restrictive postmodifier may license the use of a definite description either by pro- 
viding a link to the rest of the discourse (as in Prince's "containing inferrables') or by 
making the description into a functional concept. Looking for restrictive postmodifiers 
might therefore be a good way of identifying discourse-new descriptions. 
The distribution of restrictive postmodifiers in our corpus is shown in Table 3; 
examples of each type of postmodifier are given below. 
(15) 
(16) 
Relative clauses: these are finite clauses sometimes (but not always) 
introduced by relative pronouns uch as who, whom, which, where, when, 
why, and that: 
a. The place where he lives .. .  
b. The guy we met .. .  
Nonfinite postmodifiers: these include ing, ed (participle), and infinitival 
clauses. 
a. The man writing the letter is my friend. 
b. The man to consult is Wilson. 
Prepositional phrases and of-clauses: Quirk et al (1985) found that 
prepositional phrases are the most common type of postmodification i  
English---three or four times more frequent han either finite or nonfinite 
clausal postmodification. This was confirmed by our corpus study (see 
19 The term restrictive modification is used when the modifier provides information that is essential to 
identify the discourse ntity referred to by the NP (Quirk et al 1985). The modification is nonrestrictive 
when the head provides ufficient information to identify the discourse ntity, so that the information 
provided by the modification is not essential for identification. 
554 
Vieira and Poesio Processing Definite Descriptions 
Table 4 
Distribution of prepositions (1). 
Prepositional Phrases # % 
Of-phrases 120 79% 
Other prepositions 32 21% 
Total 152 100% 
Table 3). The types of prepositions observed for 188 postmodified 
descriptions are shown in Table 4; of-clauses are the most common. 
Our program uses the following patterns to identify restrictive postmodifiers: 2? 
(17) a. 
\[HP,the,_Premodifiers_,_Head_, \[SBAKQI_\] i_\] ; 
b. 
\[NP, the, _Premodif iers_, _Head_, \[SBAR i _\] \[ _\] ; 
C. 
\[HP ,the, _Premodifiers_, _Head_, \[S i_\] I_\] ; 
d. 
\[NP, the, _Premodif iers_, _Head_, \[VP J _\] l _\] ; 
e. 
\[NP, the, _Premodifiers_, _Head_, \[PP,_ I_\] i_\] ; 
f. 
\[NP, the, _Premodifiers_, _Head_, \[WHPP, _ I _\] I _\] ? 
In the Treebank, sometimes the modified NP is embedded in another NP, so structures 
like (18) are also considered (again for all types of clauses just shown above): 
(18) \[NP,\[NP,the,_Premodifiers_,_Head_\],\[Clause\]\]. 
Nonrestrictive postmodiJi'cation. We found it important to distinguish restrictive from 
nonrestrictive postmodification, since in our corpus, definite descriptions with nonre- 
strictive postmodifiers were generally not discourse-new. Our system recognizes non- 
restrictive postmodifiers by the simple yet effective heuristic of looking for commas. 
This heuristic orrectly recognizes nonrestrictive postmodification in cases like: 
(19) The substance, discovered almost by accident, is very important. 
which are annotated in the Penn Treebank I as follows: 
(20) \[NP,the,proposal,',',\[SBAR,\[WHHP,which\],also,\[S,\[HP,T\],would, 
\[VP,create,\[NP,a,new,type,\[PP,of,\[HP,individual, 
retirement,account\]Ill\]I, ',' \]... 
20 Note that an NP may have zero, one, or more premodifiers. 
555 
Computational Linguistics Volume 26, Number 4 
Restrictive Premodification. Restrictive modification is not as common in prenominal 
position as in posthead position, but it is often used, and was also found to correlate 
well with larger situation and unfamiliar uses of definite descriptions (Poesio and 
Vieira 1998). A restrictive premodifier may be a noun (as in (21)), a proper noun, or 
an adjective, m Sometimes numerical figures (usually referring to dates) are used as 
restrictive premodifiers, as in (22)). 
(21) 
(22) 
A native of the area, he is back now after riding the oil-field boom to the 
top, then surviving the bust running an Oklahoma City convenience 
store. 
the 1987 stock market crash; 
The heuristic we tested was to classify definite descriptions premodified by a proper 
noun as larger situation. 
4.2.3 Appositions. During our corpus analysis we found additional syntactic patterns 
that appeared to correlate well with discourse novelty yet had not been discussed 
by Hawkins, such as definite descriptions occurring in appositive constructions: they 
usually refer to the NP modified by the apposition, therefore there is no need for the 
system to look for an antecedent. Appositive constructions are treated in the Treebank 
as NP modifiers; therefore the system recognizes an apposition by checking whether 
the definite occurs in a complex noun phrase with a structure consisting of a sequence 
of noun phrases (which might be separated by commas, or not) one of which is a 
name or is premodified by a name, as in the examples in (23). 
(23) a. Glenn Cox, the president of Phillips Petroleum 
b. \[NP, \[NP,Glenn,Cox\] , ', ', \[NP,the,president, 
\[PP, of, \[NP, Phil l ips, Petroleum\] \] \] \] ; 
c. the oboist, Heinz Holliger 
d. \[NP, \[NP, the, oboist\] , \[NP, Heinz, Holl iger\] \] . 
In fact a definite description may itself be modified by an apposition, e.g., an indefinite 
NP, as shown by (24). Such cases of appositive constructions are also recognized by 
the system. 
(24) the Sandhills Luncheon Care, a tin building in midtown. 
Other examples of apposition recognized by the system are: 
(25) a. the very countercultural chamber group Tashi; 
b. the new chancellor, John Major; 
c. the Sharpshooter, a freshly drilled oil well two miles deep; 
21 Our system cannot distinguish adjectives orverbs from nouns in premodification because it works 
directly off the parsed version of the Treebank, without looking at part-of-speech tags. 
556 
Vieira and Poesio Processing Definite Descriptions 
4.2.4 Copular Phrases. Copular phrases such as the Prime Minister is Tony Blair also 
often involve discourse-new descriptions. We developed the following heuristic for 
handling copula constructions. If a description occurs in subject position, the system 
looks at the VP. If the head of the VP is the verb to be, to seem, or to become, and the 
complement of the verb is not an adjectival phrase, the system classifies the description 
as discourse-new. Two examples correctly handled by this heuristic are shown in (26); 
the syntactic representation f these cases in the Penn Treebank I is shown in (27). 
(26) a. The bottom line is that he is a very genuine and decent guy. 
b. When the dust and dirt settle in an extra-nasty mayoral race, the man 
most likely to gain custody of all this is a career politician named David 
Dinkins. 
(27) \[S,\[NP,The,bottom,line\],\[VP,is,\[NP,\[SBAR,that... l \ ] l \ ] .  
If the complement of the verb is an adjective, the subject is typically interpreted 
referentially and should not be considered iscourse-new on the basis of its comple- 
ment (e.g., The president of the US is tall). Adjectival complements are represented as 
follows in the Treebank: 
(28) \[S,\[NP,The,missing,watch\],\[VP,is,\[AD3P,emblematic...\]\]\]. 
Definite descriptions in object position of the verb to be, such as the one shown in 
(29), are also considered iscourse-new. 
(29) What the investors object o most is the effect hey say the proposal would 
have on their ability to spot telltale "clusters" of trading activity. 
4.2.5 Proper Names. Proper names preceded by the definite article, such as (30), are 
common in the genre we are dealing with, newspaper articles. 
(30) the Securities and Exchange Commission. 
The first appearance of these definite descriptions in the text is usually a discourse- 
new description; subsequent mentions of proper names are regarded as cases of 
anaphora. To recognize proper names, the system simply checks whether the head 
is capitalized. If the test succeeds, the definite is classified as a larger situation use .  22 
4.3 Bridging Descriptions 
Bridging descriptions are the definite descriptions that a shallow processing system 
is least equipped to handle. Linguistic and computational theories of bridging de- 
scriptions identify two main subtasks involved in their resolution: finding the element 
in the text to which the bridging description is related (anchor) and identifying the 
relation (link) holding between the bridging description and its anchor (Clark 1977; 
Sidner 1979; Heim 1982; Carter 1987; Fraurud 1990; Strand 1996). The speaker is h- 
censed to use a bridging description when he or she can assume that the commonsense 
22 Note that this test is performed just after trying to find an antecedent, so that the second instance of 
the same proper (head) noun will be classified as an anaphoric use. 
557 
Computational Linguistics Volume 26, Number 4 
knowledge required to identify the relation is shared by the listener (Hawkins 1978; 
Clark and Marshall 1981; Prince 1981). This dependence on commonsense knowledge 
means that, in general, a system can only resolve bridging descriptions when supplied 
with an adequate knowledge base; for this reason, the typical way of implementing 
a system for resolving bridging descriptions has been to restrict he domain and feed 
the system with hand-coded world knowledge (see, for example, Sidner \[1979\] and 
especially Carter \[1987\]). A broader view of bridging phenomena (not only bridging 
descriptions) is presented in Hahn, Strube, and Markert (1996). They make use of a 
knowledge base from which they extract conceptual links to feed an adaptation of the 
centering model (Grosz, Joshi, and Weinstein 1995). 
The relation between bridging descriptions and their anchors may be arbitrarily 
complex (Clark 1977; Sidner 1979; Prince 1981; Strand 1996), and the same description 
may relate to different anchors in a text: this makes it difficult to decide what the 
intended anchor and the intended link are (Poesio and Vieira 1998). For all these 
reasons, this class has been the most challenging problem we have dealt with in the 
development of our system, and the results we have obtained so far can only be 
considered very preliminary. Nevertheless, we feel that trying to process these definite 
descriptions i the only way to discover which types of commonsense knowledge are 
actually needed. 
4.4 Types of Bridging Descriptions 
Our work on bridging descriptions began with the development of a classification of
bridging descriptions (Vieira and Teufel 1997) according to the kind of information 
needed to resolve them, rather than on the basis of the possible relations between 
descriptions and their anchors as is typical in the literature. This allowed us to get 
an estimate of what types of bridging descriptions we might expect our system to 
resolve. The classification is as follows: 
? cases based on well-defined lexical relations, such as synonymy, 
hypernymy, and meronymy, that can be found in a lexical database such 
as WordNet (Fellbaum 1998), as in theyqat . . .  the living room; 
? bridging descriptions in which the antecedent is a proper name and the 
description a common oun, whose resolution requires ome way of 
recognizing the type of object denoted by the proper name, as in Bach . . .  
the composer; 
? cases in which the anchor is not the head noun but a noun modifying an 
antecedent, asin the company has been selling discount packages . . .  the 
discounts 
? cases in which the antecedent (anchor) is not introduced by an NP but 
by a VP, as in Kadane oil is currently drilling two oil wells. The activity . . .  
? descriptions whose antecedent is not explicitly mentioned in the text, but 
is implicitly available because it is a discourse topic, e.g., the industry in a 
text referring to oil companies; 
? cases in which the relation with the anchor is based on more general 
commonsense knowledge, e.g., about cause-consequence relations. 
In the rest of this section, we describe the heuristics we developed for handling 
the first three of these classes: lexical bridges, bridges based on names, and bridges 
558 
Vieira and Poesio Processing Definite Descriptions 
to entities introduced by nonhead nouns in a compound nominal (Poesio, Vieira, and 
Teufel 1997). 
4.4.1 Bridging Descriptions and WordNet. In order to get a system that could be eval- 
uated on a corpus containing texts in different domains, we used WordNet (Fellbaum 
1998) as an approximation of a lexical knowledge source. We developed a WordNet 
interface (Vieira and Teufel 1997) that reports a possible semantic link between two 
nouns when one of the following is true: 
? the nouns are in the same synset (i.e., they are synonyms of each other), 
as in suit~lawsuit; 
? the nouns are in a hyponymy/hypernymy relation with each other, as in 
dollar~currency; 
? there is a direct or indirect meronymy/holonymy (part of/has parts) 
relation between them, as in door~house; 
? the nouns are coordinate sisters, i.e. hyponyms of the same hypernym, 
such as home~house, which are hyponyms of housing, lodging. 
Sometimes, finding a relation between two predicates involves complex searches 
through WordNet's hierarchy. For example, there may be no relation between two 
head nouns, but there is a relation between compound nouns in which these nouns 
appear: thus, there is no semantic relation between record~album, but only a synonymy 
relation between record_album~album. We found that extended searches of this type, or 
searches for indirect meronymy relations, yielded extremely ow recall and precision at 
a very high computational cost; both types of search were dropped at the beginning of 
the tests we ran to process the corpus consulting WordNet (Poesio, Vieira, and Teufel 
1997). The results of our tests with WordNet are presented in Section 5.4. 
4.4.2 Bridging Descriptions and Named Entity Recognition. Definite descriptions 
that refer back to entities introduced by proper names (such as Pinkerton Inc ... the 
company) are very common in newspaper articles. Processing such descriptions requires 
determining an entity type for each name in the text, that is, if we recognize Pinkerton 
Inc. as an entity of type company, we can then resolve the subsequent description 
the company, or even a description such as the firm by finding a synonymy relation 
between company and firm using WordNet. 
This so-called named entity recognition task has received considerable attention 
recently (Mani and MacMillan 1996; McDonald 1996; Paik et al 1996; Bikel et al 
1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 
1999) and was one of the tasks evaluated in the Sixth and Seventh Message Under- 
standing Conferences. In MUC-6, 15 different systems participated in the competition 
(Sundheim 1995). For the version of the system discussed and evaluated here, we im- 
plemented a preliminary algorithm for named entity recognition that we developed 
ourselves; a more recent version of the system (Ishikawa 1998) uses the named en- 
tity recognition software developed by HCRC for the MUC-7 competition (Mikheev, 
Moens, and Grover 1999). 
WordNet contains the types of a few names--typically, of famous people, coun- 
tries, states, cities, and languages. Other entity types can be identified using appositive 
constructions and abbreviations ( uch as Mr., Co., and Inc.) as cues. Our algorithm for 
assigning a type to proper names is based on a mixture of the heuristics just described. 
559 
Computational Linguistics Volume 26, Number 4 
The system first looks for the above-mentioned cues to try to identify the name type. If 
no cue is found, pairs consisting of the proper name and each of the elements from the 
list country, city, state, continent, language, person are consulted in our WordNet interface 
to verify the existence of a semantic relation. 
The recall of this algorithm was increased by including a backtracking mechanism 
that reprocesses a text, filling in the discourse representation with missing name types. 
With this mechanism we can identify later the type for the name Morishita in a textual 
sequence in which the first occurrence of the name does not provide surface indication 
of the entity type: e.g., Morishita . . .  Mr. Morishita. The second mention includes such 
a clue (Mr.); by processing the text twice, we recover such missing types. 
After finding the types for names, the system uses the techniques previously de- 
scribed for same-head matching or WordNet lookup to match the descriptions with 
the types found for previous named entities. 
4.4.3 Compound Nouns. Sometimes, the anchor for a bridging description is a non- 
head noun in a compound noun: 
(31) stock market crash..,  the markets; 
One way to process these definite descriptions would be to update the discourse 
model with discourse referents not only for the NP as a whole, but also for the em- 
bedded nouns. For example, after processing stock market crash, we could introduce a
discourse referent for stock market, and another discourse referent for stock market crash. 23 
The description the markets would be coreferring with the first of these referents (with 
an identical head noun), and then we could simply use our anaphora resolution algo- 
rithms. This solution, however, makes available discourse referents that are generally 
inaccessible for anaphora (Postal 1969). For example, it is generally accepted that in 
(32), a deer is not accessible for anaphoric reference. 24
(32) I saw la deeri hunter\]j. ItT was dead. 
Therefore, we followed a different route. Our algorithm for identifying anchors at- 
tempts to match not only heads with heads, but also: 
. 
(33) 
2. 
(34) 
3. 
(35) 
The head of a description with the premodifiers of a previous NP: 
the stock market crash..,  the markets; 
The premodifiers of a description with the premodifiers of its 
antecedents: 
his art business .. .  the art gallery. 
And finally, the premodifiers of the description with the head of a 
previous NP: 
a 15-acre plot and main home .. .  the home site. 
23 Note that the collection of potential antecedents containing all NPs will just have the NP head crash for 
stock market crash. The system considers the whole NP structure as only one discourse referent, 
according to the structure of the Penn Treebank: \[NP, the,1987,stock,market,crash\]. 
24 These proposed constraints have been challenged by Ward, Sproat, and McKoon (1991). 
560 
Vieira and Poesio Processing Definite Descriptions 
5. Evaluation of the Heuristics 
In this section we discuss the tests we ran to arrive at a final configuration of the 
system. The performance of the heuristics discussed in Section 4 was evaluated by 
comparing the results of the system with the human annotation of the corpus pro- 
duced during the experiments discussed in Poesio and Vieira (1998). Several variants 
of our heuristics were tried using Corpus 1 as training data; after deciding upon an 
optimal version, our algorithms were evaluated using Corpus 2 as test data. Because 
our proposals concerning bridging descriptions are much less developed than those 
concerning anaphoric descriptions and discourse-new descriptions, we ran separate 
evaluations of two versions of the system: Version 1, which does not attempt o re- 
solve bridging descriptions, and Version 2, which does; we will point out below which 
version of the system is considered in each evaluation. 
5.1 Evaluation Methods 
The fact that the annotators working on our corpus did not always agree either on 
the classification of a definite description or on its anchor aises the question of how 
to evaluate the performance of our system. We tried two different approaches: eval- 
uating the performance of the system by measuring its precision and recall against a 
standardized annotation based on majority voting (as done in MUC), and measuring 
the extent of the system's agreement with the rest of the annotators by means of the 
same metric used to measure agreement among the annotators themselves (the kappa 
statistic). We used the first form of evaluation to measure both the performance of
the single heuristics and the performance of the system as a whole; the agreement 
measure was only used to measure the overall performance of the system. We discuss 
each of these in turn. as 
5.1.1 Precision and Recall. Recall and precision are measures commonly used in In- 
formation Retrieval to evaluate a system's performance. Recall is the percentage of 
correct answers reported by the system in relation to the number of cases indicated 
by the annotated corpus: 
R = number of correct responses 
number of cases 
whereas precision is the percentage of correctly reported results in relation to the total 
reported: 
p = number of correct responses 
number of responses 
These two measures may be combined to form one measure of performance, the F 
measure, which is computed as follows: 
F - (W + 1)RP 
(WR) + P 
W represents he relative weight of recall to precision and typically has the value 1. 
A single measure gives us a balance between the two results; 100% of recall may be 
due to a precision of 0% and vice versa. The F measure penalizes both very low recall 
and very low precision. 
25 For a rather thorough discussion ofthe problem of evaluating anaphora esolution algorithms, see 
Mitkov (2000). 
561 
Computational Linguistics Volume 26, Number 4 
5.1.2 Semiautomatic Evaluation against a Standardized Annotation. The precision 
and recall figures for the different variants of the system were obtained by com- 
paring the classification produced by each version with a standardized annotation, 
extracted from the annotations produced by our human annotators by majority judge- 
ment: whenever at least two of the three coders agreed on a class, that class was chosen. 
Details of how the standard annotation was obtained are given in Vieira (1998). 26 
The system's performance as a classifier was automatically evaluated against he 
standard annotation of the corpus as follows. Each NP in a text is given an index: 
(36) A house1?6... The house135... 
When a text is annotated or processed, the coder or system associates each index of a 
definite description with a type of use; both the standard annotation and the system's 
output are represented as Prolog assertions. 
(37) a .  
system: 
b. 
coder: 
dd_class(135,anaphoric). 
dd_class(135,anaphoric). 
To assess the system's performance on the identification of a coreferential an- 
tecedent, it is necessary to compare the links that indicate the antecedent of each de- 
scription classified as anaphora. These links are also represented as Prolog assertions, 
as follows: 
(38) a .  
coder : corer (135,106). 
b. 
system: corer (135,106). 
The system uses these assertions to build an equivalence class of discourse ntities, 
called a coreference chain. When comparing an antecedent indicated by the system 
for a given definite description with that in the annotated corpus, the corresponding 
coreference chain is checked--that is, the system's indexes and the annotated indexes 
do not need to be exactly the same as long as they belong to the same coreference 
chain. In this way, both (40a) and (40b) would be evaluated as correct answers if the 
corpus is annotated with the links shown in (39). 
(39) A house1?6... The house135... The house154... 
coder: corer(135,106). 
coder: corer(154,135). 
(40) a .  
system: corer (154,135). 
b. 
system: corer(154,106). 
26 An alternative method is to give fractional values to a classification depending on the number of 
agreements (Hatzivassiloglou and McKeown (1993). 
562 
Vieira and Poesio Processing Definite Descriptions 
In the end, we still need to check the results manually, because our annotated coref- 
erence chains are not complete: our annotators did not annotate all types of anaphoric 
expressions, o it may happen that the system indicates as antecedent an element out- 
side an annotated coreference chain, such as a bare noun or possessive. In (41), for 
example, suppose that all references to the house are coreferential: 
(41) A house1?6... The house135... His house14?... The house154... 
corer ( 154,140). 
If NP 135 is indicated as the antecedent for NP 154 in the corpus annotation (so that 
140 is not part of the annotated coreference hain), and the system indicates 140 as the 
antecedent for 154, an error is reported by the automatic evaluation, even though all 
of these NPs refer to the same entity. A second consequence of the fact that the coref- 
erence chains in our standard annotation are not complete is that in the evaluation of 
direct anaphora resolution, we only verify if the antecedents indicated are correct; we 
do not evaluate how complete the coreferential chains produced by the system are. By 
contrast, in the evaluation of the MUC coreference task, where all types of referring 
expressions are considered, the resulting co-reference hains are evaluated, rather than 
just the indicated antecedent (Vilain et al 1995). Even our limited notion of corefer- 
ence chain was, nevertheless, very helpful in the automatic evaluation, considerably 
reducing the number of cases to be checked manually. 
5.1.3 Measuring the Agreement of the System with the Annotators. Because the 
agreement between our annotators in Poesio and Vieira (1998) was often only partial, 
in addition to precision and recall measures, we evaluated the system's performance 
by measuring its agreement with the annotators using the K statistic we used in Poesio 
and Vieira (1998) to measure agreement among annotators. Because the proper inter- 
pretation of K figures is still open to debate, we interpret the K figures resulting from 
our tests comparatively, rather than absolutely, (by comparing better and worse levels 
of agreement). 
5.2 Anaphora Resolution 
We now come to the results of the evaluation of alternative versions of the heuristics 
dealing with the resolution of direct anaphora (segmentation, selection of potential 
antecedents, and premodification) discussed in Section 4.1. The optimal version of our 
system is based on the best results we could get for resolving direct anaphora, because 
we wanted to establish the coreferential relations among discourse NPs as precisely 
as possible. 
5.2.1 Life Span of Discourse Entities. In Section 4.1 we discussed two heuristics for 
limiting the life span of discourse ntities. The first segmentation heuristic discussed 
there, loose segmentation, is window based, but the restriction on sentence distance 
is relaxed (i.e., the resolver will consider an antecedent outside the window) when 
either: 
? the antecedent is itself a subsequent-mention; or 
? the antecedent is identical to the definite description being resolved 
(including the article). 
With loose segmentation, it is possible for the system to identify more than one 
coreference link for a definite description: all antecedents satisfying the requirements 
563 
Computational Linguistics Volume 26, Number 4 
Table 5 
Evaluation of loose segmentation a d recency heuristics. 
Heuristics R P F 
Segmentation: 1-sentence window 71.79% 86.48% 78.45% 
Segmentation: 4-sentence window 76.92% 82.75% 79.73% 
Segmentation: 8-sentence window 78.20% 80.26% 79.22% 
Recency: all sentences 80.76% 78.50% 79.62% 
Table 6 
Evaluation of the strict segmentation heuristic. 
Strict Segmentation R P F 
1-sentence window 29.48% 89.32% 44.33% 
4-sentence window 57.69% 88.23% 69.76% 
8-sentence window 67.94% 84.46% 75.31% 
within the current w indow will be indicated as a possible antecedent. Therefore, when 
evaluating the system's results, we may find that all antecedents indicated for the 
resolution of a description were right, or some were right and some wrong, or that all 
were wrong. The recall and precision figures reported here relate to those cases were 
all resolutions indicated were right according to the annotated corpus. 
In Section 4.1 we also discussed a second segmentation heuristic, which we called 
recency: the system does not collect all candidate NPs as potential antecedents, but 
only keeps the last occurrence of an NP from all those having the same head noun, 
and there are no restrictions regarding the antecedent's distance. 
The results of these two methods for different w indow sizes are shown in Ta- 
ble 5. The results in this table were obtained by considering as potential antecedents 
indefinites (i.e., NPs with determiners a,an, and some; bare NPs; and cardinal plurals), 
possessives, and definite descriptions, as in Vieira and Poesio (1996); we also used the 
premodif ication heuristics proposed there. Alternatives to these heuristics were also 
evaluated; the results are discussed later in this section. 
The resulting F measures were almost the same for all heuristics, but there was 
clearly an increase in recall with a loss of precision when enlarging the window size} 7 
The recency heuristic had the best recall, but the lowest precision, although not much 
lower than the others. The best precision was achieved with a one-sentence window, 
and recall was not dramatical ly affected, but this only happened because the window 
size constraint was relaxed. 
To show what happens when a strict version of the window-based segmentation 
approach is used, consider Table 6. (Strict segmentation means that the system only 
considers those antecedents that are inside the sentence window for resolving a de- 
scription, with no exceptions.) As the table shows, this form of segmentation results 
in higher precision, but has a strong negative ffect on recall. The overall F values are 
all worse than for the heuristics in Table 5. 
Finally, we tried a combination of the recency and segmentation heuristics: just one 
potential antecedent for each different head noun is available for resolution, the last 
27 In our experiments small differences in recall, precision, and F measures are frequent. We generally 
assume in this paper that such differences are not significant, but a more formal significance t st along 
the lines of that in Chinchor (1995) will eventually be necessary to verify this. 
564 
Vieira and Poesio Processing Definite Descriptions 
Table 7 
Combining loose segmentation a d recency heuristics. 
Combined Heuristics R P F 
4 sentences + recency 75.96% 87.77% 81.44% 
8 sentences + recency 77.88% 84.96% 81.27% 
Table 8 
Evaluation of the heuristics for choosing potential antecedents. 
Antecedents Selection R P F 
Indefinites, definite descriptions, and possessives 75.96% 87.77% 81.44% 
All NPS 77.88% 86.17% 81.81% 
Indefinites and definite descriptions 73.39% 88.41% 80.21% 
Indefinites only 12.17% 77.55% 21.05% 
occurrence of that head noun. The resolution still respects the segmentation heuristic 
(loose version). The results are presented inTable 7. This table shows that by combining 
the recency and loose segmentation approaches to segmentation we obtain a better 
trade-off between recall and precision than using each heuristic separately. The version 
with higher F value in Table 7 (four-sentence window plus recency) was chosen as 
standard and used in the tests discussed in the rest of this section. 
5.2.2 Potent ia l  Antecedents .  Next, we evaluated the various ways of restricting the set 
of potential antecedents discussed in Section 4.1, using four-sentence-window l ose 
segmentation with recency. In an earlier version of the system (Vieira and Poesio 1996), 
only those definite descriptions that were not resolved with a same-head antecedent 
were considered as potential antecedents; resolved definite descriptions would be 
linked to previous NPs, but would not be made available for subsequent resolution. 
(The idea was that the same antecedent used in one resolution could be used to resolve 
all subsequent mentions cospecifying with that definite description.) An important dif- 
ference between that implementation a d the current one is that in the new version, 
the definltes resolved by the system are also made available as potential antecedents of 
subsequent definites. This is because in our previous prototype, errors in identifying 
an indefinite antecedent were sometimes propagated through a coreference chain, so 
that the right antecedent would be missed. The results are shown in Table 8. 
If we only consider indefinites as potential antecedents, recall is extremely low 
(12%); we also get the worst precision. In other words, considering only indefinites 
for the resolution of definite descriptions i too restrictive; this is because our corpus 
contains alarge number of first-mention definite descriptions that serve as antecedents 
for subsequent references ( imilar results were also reported in Fraurud \[1990\]). The 
version with the highest precision (88%) is the one that only considers indefinites and 
definite descriptions as antecedents, but recall is lower compared to the version that 
considered other NPs. We chose, as the basis for further testing, aversion that combines 
near-optimal values for F and precision, i.e., the version that takes indefinites, definite 
descriptions, and possessives (first row in Table 8). 
5.2.3 Premodifiers. Finally, we tested our heuristics for dealing with premodifiers. We 
tested the matching algorithm from Vieira and Poesio (1996) in the present version 
of the system; the results are presented in Table 9. In that table, we also show the 
565 
Computational Linguistics Volume 26, Number 4 
Table 9 
Evaluation of the heuristics for premodification (Version 1). 
Antecedents Selection R P F 
1. Ant-set/Desc-subset 69.87% 91.21% 79.12% 
2. Ant-empty 55.12% 88.20% 67.85% 
3. Ant-subset/Desc-set 64.74% 88.59% 74.81% 
1 and 2 (basic v.) 75.96% 87.77% 81.44% 
1 and 3 75.96% 87.13% 81.16% 
None 78.52% 81.93% 80.19% 
results obtained with a modified matching algorithm including a third rule, which 
allows a premodified antecedent to match with a definite whose set of premodifiers 
is a superset of the set of modifiers of the antecedent (an elaboration of rule 2). We 
tested each of these three heuristics alone and in combination. (The fourth line simply 
repeats the results hown in Table 7.) 
The main result of this evaluation is that using a modified segmentation heuristic 
(including recency) reduces the overall impact of the heuristics for premodification 
the performance of the algorithm in comparison with the system discussed in Vieira 
and Poesio (1996)? The best precision is still achieved by the matching algorithm that 
does not allow for new information in the anaphoric expression, but the best results 
overall are again obtained by combining rule I and rule 2, although either 2 or 3 works 
equally well when combined with 1. (Note that the combination of heuristics 2 and 3 
is equivalent to heuristic 3alone, since rule 3 subsumes rule 2.) Heuristic 2 and 3 alone 
are counterintuitive and indeed give the poorest results; however, the impact is greater 
on recall than precision, which suggests that the introduction of new information in 
noun modification is not very frequent? 
One of the problems with our premodifier heuristics i that although a difference in 
premodification usually indicates noncoreference, as for the company's abrasive segment 
and the engineering materials egment, there are a few cases in our corpus in which 
coreferent descriptions have totally different premodification from their antecedents, 
as in: 
(42) the pixie-like clarinetist ... the soft-spoken clarinetist? 
These cases would be hard even for a system using real commonsense reasoning, since 
often the information i  the premodifier isnew; we consider these examples one of the 
best arguments for including in the system a focus-tracking mechanism along the lines 
of Sidner (1979). Our heuristic matching algorithm also suggests wrong antecedents 
in cases like the rules in (43), when the last mention refers to a modified concept (the 
new rules are different from the previous ones). 
(43) Currently, the rules force executives ... 
The rule changes would ... 
The rules will eliminate ... 
Finally, the matching algorithm gets the wrong result in cases uch as the population 
? . .  the voting population where the new information i dicates a subset, superset, or part 
of a previously mentioned referent. 
566 
Vieira and Poesio Processing Definite Descriptions 
Table 10 
Evaluation of the heuristics for direct anaphora (Version 1). 
Anaphora Classification # + - R P F 
Training data 312 243 27 78% 90% 83% 
Test data 154 103 12 67% 90% 77% 
Anaphora Resolution # + - R P F 
Training data 312 237 33 76% 88% 81% 
Test data 154 96 19 62% 83% 71% 
5.2.4 Overa l l  Resu l ts  for Anaphor ic  Def in i te  Descr ipt ions .  To summarize, on the 
basis of the tests just discussed, the heuristics that achieve the best results for anaphoric 
definite descriptions are: 
. 
2. 
3. 
. 
combined loose segmentation a d recency, 
four-sentence window, 
considering indefinites, definites, and possessives as potential 
antecedents, 
the premodification f the description must be contained in the 
premodification f the antecedent or when the antecedent has no 
premodifiers. 
In Table 10 we present the overall results on anaphora classification and anaphora 
resolution for the version of the system that does not attempt o resolve bridging 
descriptions, for both training data and test data. The reason there are different figures 
for anaphora resolution and classification is that the system may correctly classify a 
description as anaphoric, but then find the wrong antecedent. We used this set of 
heuristics when evaluating the heuristics for discourse-new and bridging descriptions 
in the rest of the paper. 
The column headed # represents he number of cases of descriptions classified as 
anaphora in the standard annotation; +indicates the total number of anaphora (clas- 
sification and resolution) correctly identified; - indicates the total number of errors. 
5.2.5 Errors in Anaphora  Reso lu t ion .  Before discussing the results of the other heuris- 
tics used by the system, we will discuss in more detail some of the errors in the 
resolution of anaphoric descriptions made by using the heuristics just discussed. 
Some errors are simply caused by misspellings in the Treebank, as in the example 
below, where the antecedent is misspelled as spokewoman. 
(44) A Lorillard spokewoman ... The Lorillard spokeswoman 
The most common problems are due to the heuristics limiting the search for an- 
tecedents. In (45), both sentence 7 and sentence 30 are outside the window considered 
by the system when trying to resolve the adjusters in 53. 
(45) 7. She has been on the move almost incessantly since last Thursday, 
when an army of adjusters, employed by major insurers, invaded the San 
Francisco area. 
567 
Computational Linguistics Volume 26, Number 4 
? ? ?  
30. Aetna, which has nearly 3,000 adjusters, had deployed about 750 of 
them 
53. Many of the adjusters employed by Aetna and other insurers 
Limiting the type of potential antecedents o indefinites, definite descriptions, 
and possessives, while improving precision, also leads to problems, because the an- 
tecedents introduced by other NPs, such as proper names, are missed--e.g., Toni John- 
son in (46). The following definite description is then classified by the system as larger 
situation/unfamiliar. Some of these problems are corrected in Version 2 of the system, 
which also attempts to handle bridging descriptions and therefore uses algorithms for 
assigning a type to such entities? 
(46) Toni Johnson pulls a tape measure across the front of what was once a 
stately Victorian home. 
The petite, 29-year-old Ms. Johnson ... 
The premodification heuristics prevent the system from finding the right an- 
tecedent in the (rare) cases of coreferent descriptions with different premodifiers, as 
in (47). 
(47) The Victorian house that Ms. Johnson is inspecting has been deemed 
unsafe by town officials? 
Once inside, she spends nearly four hours measuring and diagramming 
each room in the 80-year-old house? 
In the following example, it is the lack of a proper treatment of postmodification 
that causes the problem. The system classifies the description the earthquake-related 
claims as anaphoric to claims from that storm, but it is discourse-new according to the 
standardized annotation. 
(48) Most companies till are trying to sort through the wreckage caused by 
Hurricane Hugo in the Carolinas last month? 
Aetna, which has nearly 3,000 adjusters, had deployed about 750 of them 
in Charlotte, Columbia, and Charleston? 
Adjusters who had been working on the East Coast say the insurer will 
still be processing claims from that storm through December. 
It could take six to nine months to handle the earthquake-related claims? 
In (49), the system correctly classifies the definite description the law as anaphoric, 
but suggests as antecedent an income tax law, whereas a majority of our annotators 
568 
Vieira and Poesio Processing Definite Descriptions 
Table 11 
Evaluation of the heuristics for identifying discourse-new descriptions. 
Discourse -new # + - R P F 
Training data 492 368 60 75% 86% 80% 
Test data 218 151 58 69% 72% 70% 
indicated a money lending law as the antecedent. 28 
(49) Nearly 20 years ago, Mr. Morishita, founder and chairman of Aichi 
Corp., a finance company, received a 10-month suspended sentence from 
a Tokyo court for violating a money-lending law and an income tax law. 
He was convicted of charging interest rates much higher than what the 
law permitted, and attempting to evade income taxes by using a double 
accounting system. 
Finally, the system is incapable of resolving plural references to collections of ob- 
jects introduced by singular NPs, even when these collections were introduced by 
coordinated noun phrases. Although it would be relatively easy to add rules for han- 
dling the simplest cases (possibly at the expense of a decrease in precision), many of 
these references can only be resolved by means of nontrivial operations. 
(50) The owners, William and Margie Hammack, are luckier than many others. 
The Hammacks ... 
5.3 Ident i f i ca t ion  o f  D iscourse -New Descr ip t ions  
The overall recall and precision results for the heuristics for identifying discourse- 
new descriptions presented in Section 4.2 are shown in Table 11. In this table we do 
not distinguish between the two types of discourse-new descriptions, unfamiliar and 
larger-situation (Hawkins 1978). As already mentioned in Section 4.2, distinguishing 
between the two types of discourse-new descriptions identified by Hawkins, Prince, 
and others isn't easy even for humans (Fraurud 1990; Poesio and Vieira 1998); and 
indeed, our heuristics for recognizing discourse-new descriptions work better when 
evaluated together. The column headed # represents the number of cases of descrip- 
tions classified as discourse-new in the standard annotation; +indicates the total num- 
ber of discourse-new descriptions correctly identified; - the number of errors. These 
results are for the version of the system that uses the best version of the heuristics 
for dealing with anaphoric descriptions discussed above, and that doesn't attempt o 
resolve bridging descriptions (Version 1). 
The performance of the specific heuristics discussed in Section 4.2 is shown in 
Tables 12 to 15. Table 12 shows the results of the heuristics for larger situation uses 
on the training data, whereas Table 13 reports the performance on the same data of 
28 The law could also be interpreted as referring to "the law system in general," in which case none of the 
antecedents would be correct (or either could be taken as anchor for a bridging interpretation f the 
definite). 
569 
Computational Linguistics Volume 26, Number 4 
Table 12 
Evaluation of heuristics for larger situation uses (training data). 
Larger Situation Total Found Errors Precision 
Names 73 10 86% 
Time references 50 7 86% 
Premodification 41 19 54% 
Total 164 36 78% 
Table 13 
Evaluation of heuristics for unfamiliar uses (training data). 
Unfamiliar Total Found Errors Precision 
NP compl/Unexp mod 32 2 93% 
Apposition 27 2 92% 
Copula 8 2 75% 
Postmodification 197 18 91% 
Total 264 24 91% 
Table 14 
Evaluation of heuristics for larger situation uses (test data). 
Larger Situation Total Found Errors Precision 
Names 44 14 68% 
Time references 21 5 64% 
Premodification 17 9 47% 
Total 82 28 66% 
the heuristics for unfamiliar uses. We report only precision figures because our stan- 
dard annotation only gives us information about the classification of these discourse 
descriptions as discourse-new, not about the reason they were classified in a certain 
way (larger situation or unfamiliar). The most common feature of discourse-new de- 
scriptions is postmodification; the least satisfactory results are those for proper names 
in premodification. As expected, the heuristics for recognizing unfamiliar uses (many 
of which are licensed by linguistic knowledge) achieve better precision than those for 
larger situation uses, which depend more on commonsense knowledge. 
Tables 14 and 15 summarize the results of the heuristics for discourse-new de- 
scriptions on the test data (Corpus 2). Again, the best results were obtained by the 
heuristics for recognizing unfamiliar uses. The biggest difference in performance was 
shown by the heuristic hecking the presence of the definite in a copula construction, 
which performed very well on the training data, but poorly on the test data. The actual 
performance of that heuristic is difficult to evaluate, however, as a very low recall was 
reported for both training and test data. 
In the following sections, we analyze some of the problems encountered by the 
version of the system using these heuristics. 
Apposition. Coordinated NPs with more than two conjuncts are a problem for this 
heuristic, since in the Penn Treebank I, coordinated NPs have a structure that matches 
the pattern used by the system for recognizing appositions. For example, the coordi- 
nated NP in the sentence G-7 consists of the U.S., Japan, Britain, West Germany, Canada, 
570 
Vieira and Poesio Processing Definite Descriptions 
Table 15 
Evaluation of heuristics for unfamiliar uses (test data). 
Unfamiliar Total Found Errors Precision 
NP compl/Unexp mod 16 2 87% 
Apposition 10 2 80% 
Copula 6 4 33% 
Postmodification 95 22 77% 
Total 127 30 76% 
France and Italy has the structure in (51). 
(51) \[NP, \[NP,the,U.S.\] ,,, \[NP, Japan\] ,,, \[NP,Britain\] ,,, \[NP,West, 
Germany\] ,,, \[NP, Canada\],,,  \[NP, France\], and, \[NP, Italy\] \] 
Copula. This heuristic was difficult to evaluate because there few examples, and the 
precision in the two data sets is very different (see Tables 13 and 15 above). One 
problem is that the descriptions in copula constructions might also be interpreted 
as bridging descriptions. For instance, the description the result in (52a) below is the 
result of something mentioned previously, while the copula construction specifies its 
referent. Other ambiguous examples are (52b) and (52c): 
(52) a. The result is that those rich enough to own any real estate at all have 
boosted their holdings ubstantially. 
b. The chief culprits, he says, are big companies and business groups that 
buy huge amounts of land not for their corporate use, but for resale at 
huge profit. 
c. The key man seems to be the campaign manager, Mr. Lynch. 
Restrictive premod~cation. One problem with this heuristic is that although proper 
nouns in premodifier positions are often used with discourse-new definites (e.g., 
the Iran-Iraq war), they may also be used as additional information in associative or 
anaphoric uses: 
(53) Others grab books, records, photo albums, sofas and chairs, working 
frantically in the fear that an aftershock will jolt the house again. 
As Ms. Johnson stands outside the Hammack house after winding up her 
chores there, the house begins to creak and sway. 
Restrictive postmodification. If the system fails to find an antecedent or anchor and the 
description is postmodified, it may wrongly be classified as discourse-new. In (54) the 
filing on the details of the spinoff was classified as bridging on documents filed ... by the 
coders, but the system classified it as discourse-new. 
(54) Documents filed with the Securities and Exchange Commission on the pending 
spinoff disclosed that Cray Research Inc. will withdraw the almost $100 
million in financing it is providing the new firm if Mr. Cray leaves or if 
the product-design project he heads is scrapped. 
571 
Computational Linguistics Volume 26, Number 4 
The filing on the details of the spinoff caused Cray Research stock to jump 
$2.875 yesterday to close at $38 in New York Stock Exchange composite 
trading. 
Proper nouns. As we have already seen--(46), repeated below as (55)--a definite de- 
scription that looks like a proper noun (the petite, 29-year-old Ms. Johnson) may in fact 
be anaphoric. This is not always a problem, as the system does attempt to find an- 
tecedents for these definites, as well, but if the antecedent is not found (as in the 
example below) the description is incorrectly classified as discourse-new. 
(55) Toni Johnson pulls a tape measure across the front of what was once a 
stately Victorian home. 
The petite, 29-year-old Ms. Johnson ... 
Special predicates? In this example the system classified as discourse-new a time refer- 
ence (the same time), which is classified as bridging in the standard annotation. 
(56) Newsweek's circulation for the first six months of 1989 was 3,288,453, flat 
from the same period last year. 
U.S. News' circulation in the same time was 2,303,328, down 2.6%. 
5.4 Bridging Descriptions 
As mentioned in Section 2, our corpus annotation experiments showed bridging de- 
scriptions to be the most difficult class for humans to agree on. Even when our anno- 
tators agreed that a particular expression was a bridging description, different anchors 
would be available in the text for the interpretation f that bridging description? This 
makes the results of the system for this class very difficult to evaluate; furthermore, 
the results must be evaluated by hand? 
We first tested the heuristics individually on the training data (the same data 
used in a previous analysis of the performance ofour system on bridging descriptions 
\[Vieira nd Teufel 1997\]) by adding them to Version I of the system one at a time. These 
separate tests were manually evaluated? We then integrated all of these heuristics into 
a version of the system called Version 2, using both automatic and manual evaluation. 
In this section we discuss only the results of the individual heuristics; the overall 
results of Version 2 are discussed in Section 6. 
Bridging descriptions are much more sensitive than other types of definite de- 
scriptions to the local focus (Sidner 1979); for this reason, Version 2 uses a different 
search strategy for bridging descriptions than for other definite descriptions. Rather 
than considering all definite descriptions in the current window simultaneously, it goes 
back one sentence at a time and stops as soon as a relation with a potential anchor is 
found. 
5.4.1 Using WordNet to Identify Anchors. Our system consults WordNet o determine 
if a definite description may be semantically related to one of the NPs in the previous 
572 
Vieira and Poesio Processing Definite Descriptions 
Table 16 
Evaluation of the search for anchors using WordNet. 
Bridging Class Relations Found Right Anchors % Right 
Synonymy 11 4 36% 
Hyponymy 59 18 30% 
Meronymy 6 2 33% 
Sister 30 6 20% 
Total 106 30 28% 
five sentences. 29 The results of this search over our training corpus, in which 204 
descriptions were classified as bridging, are shown in Table 16. It is interesting to 
note that the semantic relations found in this automatic search were not always those 
observed in our manual analysis. 
The main reason the figures are so low is that the existence of a semantic relation 
in WordNet is not a sufficient condition (nor a strong indication) to establish a link 
between an antecedent and a bridging description. In only about a third of the cases 
was a potential antecedent for which we could find a semantic relation in WordNet an 
appropriate anchor. An example is (57): although there is a semantic relation between 
argument and information in WordNet, the description the argument is related to the 
VP contend rather than to the NP information. Some form of focusing seems to play a 
crucial role in restricting the range of antecedents ( ee also the discussion in Hitzeman 
and Poesio \[1998\]). 
(57) A SEC proposal to ease reporting requirements for some company 
executives would undermine the usefulness of information on insider 
trades as a stock-picking tool, individual investors and professional 
money managers contend. 
They make the argument in letters to the agency about rule changes 
proposed this past summer that, among other things, would exempt 
many middle-management executives from reporting trades in their own 
companies' shares. 
Sense ambiguity is responsible for some of the false positives. For instance, the 
noun company has at least two distinct senses: "visitor" (as in Ihave company) and "busi- 
ness." A relation of hypernymy was found between company and human (its "visitor" 
sense), whereas in the text the noun company was used in the "business" sense. A 
more important problem, however, is the incompleteness of the information encoded 
in WordNet. To have an idea of how complete the information in WordNet is con- 
cerning the relations that are encoded, we selected from our two corpora 70 bridging 
descriptions that we had manually identified as being linked to their anchors by one of 
the semantic relations encoded in WordNet--synonymy, hypernymy (hyponymy), and 
meronymy (holonymy). In Table 17 we show the percentages of such relations actually 
encoded in WordNet. (The fourth column in the table indicates the cases in which the 
expected relation is not encoded, but the two nouns are sisters in the hierarchy.) 
As we can see from the table, the recall figure was quite disappointing, especially 
for synonymy relations. In some cases, the problem was simply that some of the 
29 We found that for bridging descriptions, a five-sentence window worked better than a four-sentence 
one. 
573 
Computational Linguistics Volume 26, Number 4 
Table 17 
Evaluation of the encoding of semantic relations in WordNet. 
Bridging Class Anchor/DD Pairs Found in WN Found Sister % 
Syn 20 5 2 35% 
Hyp 32 17 1 56% 
Mer 18 5 2 38% 
Total 70 27 5 46% 
artifact 
I is_a 
structure 
housing 
lodging 
i s -~"x .~. -a  
house home 
building 
edifice 
part_of 
room 
part_~"x...part_of 
wall floor 
Figure 2 
An example of problematic organization i WordNet. 
words we looked for were not in WordNet: examples include newsweekly (news-weekly), 
crocidolite, countersuit (counter-suit). Other times, the word we looked for was contained 
in WordNet, but not in the same typographic format as it was presented in the text; 
for example we had spinoff in a text, whereas WordNet had only an entry for spin- 
off. A second source of problems was the use in the WSJ articles of domain-specific 
terminology with context-dependent senses, such as slump, crash, and bust, which in 
articles about the economy are all synonyms. Finally, in other cases the relations were 
missing due to the structure of WordNet: for instance, in WordNet the nouns room, 
wall, and floor are encoded as part of building but not of house (see Figure 2). 
In summary, our tests have shown that the knowledge encoded in WordNet is 
not sufficient o interpret al semantic relations between a bridging description and 
its antecedent found in the kind of text we are dealing with: only 46% of the rela- 
tions observed were encoded in WordNet. The possibility of using domain-specific, 
automatically acquired lexical information for this purpose is being explored: see, for 
example, Poesio, Schulte im Walde, and Brew (1998). In addition, we found that just 
looking for the closest semantic relative is not enough to find anchors for bridging 
descriptions; this search has to be constrained by some type of focusing mechanism. 
5.4.2 Evaluating the Results for Bridging Descriptions Based on Proper Names. 
Identifying named entity types is a prerequisite for resolving descriptions based on 
names. The simple heuristics discussed in Section 5.4 identified entity types for 66% 
574 
Vieira and Poesio Processing Definite Descriptions 
(535/814) of all names in the corpus (organizations, persons, and locations); precision 
was 95%.  30 The errors we found were sometimes due to name or sense ambiguity. In 
the same text a name may refer both to a person and a company, as in Cray Comput- 
ers Corp. and Seymour Cray. When looking in WordNet for a type for the name Steve 
Reich we found for the name Reich the type country. These problems have also been 
noted by the authors of systems participating in MUC-6 (Appelt 1995). We also found 
undesirable relations uch as hypernymy for person and company. 
5.4.3 Evaluating the Results for Bridging Descriptions Based on Compound Nouns. 
We had 25 definite descriptions manual ly  identified as based on compound nouns. 
For these 25 cases, our implemented heuristics achieved a recall of 36% (9/25) but, 
in some cases, found valid relations other than the ones we identified. The low recall 
was sometimes due to segmentation. Sometimes the spelling of the premodif ication 
was slightly different from the one of the description, as in a 15-acre plot.. ,  the 15 acres. 
6. Overall Evaluation of the System 
As mentioned above, we implemented two versions of the system. Version 1 only 
resolves direct anaphora nd identifies discourse-new descriptions; Version 2 also deals 
with bridging descriptions. Both versions of the system have at their core a decision 
tree in which the heuristics discussed in the previous sections are tried in a fixed order 
to classify a certain definite description and find its anchor. Determining the optimal 
order of application of the heuristics in the decision tree is crucial to the performance 
of the system. In both versions of the system we used a decision tree developed by 
hand on the basis of extensive valuation; we also attempted to determine the order 
of application automatically, by means of decision tree learning algorithms (Quinlan 
1993). 
In this section we first present he hand-crafted ecision tree and the results ob- 
tained using this decision tree for Version 1 and Version 2; we then present he results 
concerning the agreement between system and annotators, and we briefly discuss the 
results obtained using the decision tree acquired automatically. 
6.1 Integration of the Heuristics 
The hand-crafted order of the heuristics in both versions is the following. For each NP 
of the input, 
. 
2. 
The system assigns an index to it. 
The NPs that may serve as potential antecedents are made available for 
description resolution by means of the optimal selection criterion 
discussed in Section 4.1. 
30 By comparison, the systems participating in MUC-6 had a recall for the named entity task ranging 
from 82% to 96%, and precision from 89% to 97%, but used comprehensive lists of cue words or 
consulted ictionaries of names. The system from Sheffield (Gaizauskas et al 1995), for instance, used a 
list of 2,600 names of organizations, 94company designators (Co., Ltd, PLC, etc.), 160 titles (Dr., Mr., 
etc.), about 500 human ames from the Oxford Advanced Learner's Dictionary, 2,268 place names 
(country, province, and city names), and other trigger words for locations, government institutions and 
organizations (Golf, Mountain, Agency, Ministry, Airline, etc.). In MUC-7, the best combined precision 
score, 93.39%, was achieved by the system from LTG in Edinburgh (Mikheev, Moens, and Grover 1999), 
which doesn't use such knowledge sources. We used this system in a version of our prototype that 
only attempts to resolve bridging descriptions (Ishikawa 1998). 
575 

Vieira and Poesio Processing Definite Descriptions 
Spec-Pred 
Y 
/ 
1 
N 
Y 
/ 
3 
N 
Y 
/ 
3 
N 
Y 
/ 
3 
Spec-Pred = special predicate 
Appos = apposition 
Dir-Ana = same head antecedent 
PropN = proper noun 
RPostm = restrictive postmodification 
RPrem = restrictive premodification 
CopC = copular construction 
N 
Y 
/ 
3 
N 
1 Direct anaphora 
2 Bridging 
3 Discourse new 
~C 
\N  
N 
2 Fail 
Figure 3 
Hand-designed decision tree for Version 1 and Version 2. 
? only then try to interpret he definite description as a bridge (last test). 
The heuristics for recognizing bridging descriptions are only applied when the 
other heuristics fail. This is because the performance of these heuristics is very poor 
and also because some of the heuristics that deal with bridging descriptions are com- 
putationally expensive; the idea was to eliminate those cases less likely to be bridg- 
ing before applying these heuristics. The system does not classify all occurrences of 
definite descriptions: when none of the tests succeeds, the definite description is not 
classified. We observed in our first tests that definite descriptions not resolved as direct 
anaphora nd not identified as discourse-new by our heuristics were mostly classified 
in the standardized annotation as bridging descriptions or discourse-new. Examples 
of discourse-new descriptions not identified by our heuristics are larger situation uses 
such as the world, the nation, the government, the economy, the marketplace, the spring, the 
577 

Vieira and Poesio Processing Definite Descriptions 
TOTAL TYPES IDENTIF IED BY THE SYSTEM 
anaphoric:  270 
larger s it . /unfam: 428 
total: 698 
TOTAL NON CLASSIF IED 
anaphoric:  41 
larger s it . /unfam: 113 
associat ive:  162 
idiom: 20 
doubt: 6 
total: 342 
TOTAL TYPES CLASSIF IED BY HAND 
anaphoric:  312 
larger sit . /unfam: 492 
associat ive:  204 
idiom: 22 
doubt: i0 
total: 1040 
Figure 5 
Summary of the results of Version 1 on training data. 
Table 18 
Global results of Version 1 on training data. 
System's tasks R P F 
Anaphora classification 78% 90% 83% 
Anaphora resolution 76% 88% 81% 
Discourse-new 75% 86% 80% 
Overall 59% 88% 70% 
Table 19 
Evaluation of Version l on the test data. 
System's tasks R P F 
Anaphora classification 67% 90% 77% 
Anaphora resolution 62% 83% 71% 
Discourse-new 69% 72% 70% 
Overall 53% 76% 63% 
The recall and precision figures for the system's performance over the test data are 
presented in Table 19. This corpus consisted of 14 texts, containing 2,990 NPs. Again, 
almost half of the NPs were considered as potential antecedents. The system processed 
464 defir~te descriptions; of these, the system could classify 324:115 as direct anaphora, 
209 as discourse-new. Of the antecedents, 88 were definites themselves. The system 
incorrectly resolved 77 definite descriptions: 19 anaphoric definites and 58 discourse- 
new. As before, there were just a few more errors in anaphora resolution than in 
anaphora classification. The overall recall for the test data was 53% (247/464); precision 
was 76% (247/324). 
One difference between the results on the two data sets is the distribution into 
classes of those descriptions that the system fails to classify. In the first corpus, the 
largest number of cases not classified are bridging descriptions. By contrast, the largest 
number of cases not classified by the system in Corpus 2 are discourse-new. 
579 
Computational Linguistics Volume 26, Number 4 
NR. OF TEXTS:  14 NR. OF NOUN PHRASES:  2990 
NR. OF ANTECEDENTS CONSIDERED:  1226 
Indefinites: 657 
Possessives: 144 
Def in i tes :  425 
NR. OF DEF IN ITE  DESCRIPT IONS:  464 
D IRECT ANAPHORA:  115 ANTECEDENTS FOUND: Indef in i tes :  21 
Possessives: 6 
Def in i tes :  88 
D ISCOURSE NEW DESCRIPT IONS:  209 
LARGER S ITUAT ION USES:  82 UNFAMIL IAR USES : 127 
NAMES : 44 NP  COMP. /UN.MOD. :  16 
T IME REFERENCES : 21 APPOSIT IONS : i0 
REST.PREMOD.  : 17 REST.  POSTMOD.  : 95 
COPULA : 6 
NON- IDENTIF IED:  140 
TOTAL  EST IMATED ERRORS (for anaphora  c lass i f i ca t ion)  : 12 
TOTAL  EST IMATED ERRORS (for anaphora  reso lu t ion)  : 19 
TOTAL  EST IMATED ERRORS (for la rger  s i tuat ion /unfami l ia r ) :  58 
Figure 6 
Global results of Version 1 on test data, 
TOTAL TYPES IDENTIF IED BY  THE SYSTEM 
anaphoric: 115 
la rger  s i t . /un fam:  209 
total :  324 
TOTAL  NON CLASSIF IED 
anaphoric: 29 
la rger  s i t . /un fam:  61 
assoc ia t ive :  46 
doubt :  4 
total :  140 
TOTAL  TYPES CLASS IF IED BY  HAND 
anaphoric: 154 
la rger  s i t . /un fam:  218 
assoc ia t ive :  81 
doubt :  I i 
total :  464 
Figure 7 
Summary of the results for test data. 
6.3 Results for Bridging Descriptions 
As discussed in Section 5.4, the results of the heuristics for bridging descriptions pre- 
sented in Section 4.3 were not very good. We nevertheless included these heuristics in 
Version 2 of the system, which, as discussed above, applied them to those descriptions 
that failed to be recognized as direct anaphora or discourse-new. The heuristics were 
applied in the following order: 
1. proper names, 
580 
Vieira and Poesio Processing Definite Descriptions 
Table 20 
Evaluation of the bridging heuristics all together. 
Bridging Found by System False 
Class Positive 
Names 12 14 
Common ouns 15 10 
WordNet 34 76 
Total 61 100 
Table 21 
Comparative evaluation of the two versions (test data). 
System's versions R P F 
V.1 Overall 53% 76% 62% 
V.2 Overall 57% 70% 62% 
2. compound nouns, 
3. WordNet, 
Training Data. The manual evaluation of the results of Version 2 on the training data 
is presented in Table 20. The table lists the number of acceptable anchors and the 
number of false positives found by each heuristic. Note that the system sometimes 
finds anchors that are not those identified manually, but are nevertheless acceptable. 
We found fewer bridging relations than the number we observed in the corpus 
analysis (204); furthermore, the number of false positives produced by such heuristics 
is almost wice the number of right answers. 
Test Data. Version 2 was tested over the test data using automatic evaluation--i.e., the 
system was only evaluated as a classifier, and the anchors found were not analyzed. 
A total of 57 bridging relations were found, but only 19 of the definite descriptions 
classified as bridges by the system had been classified as bridging descriptions in the 
standard annotation. Compared to Version 1 of the system, which does not resolve 
bridging descriptions, Version 2 has higher recall but lower precision, as shown in 
Table 21. 
6.4 Agreement  among System and Annotators for Version 1 and Version 2 
As a second form of evaluation of the performance of the system, we measured its 
agreement with the annotators on the test data using the K statistic. 
Version 1 of the system finds a classification for 318 out of 464 definite descrip- 
tions in Corpus 2 (the test data). If all the definite descriptions that the system cannot 
classify are treated as discourse-new, the agreement between the system and the three 
subjects that annotated this corpus on the two classes first-mention (= discourse-old) 
and subsequent-mention (= discourse-new or bridges) is K = 0.7; this should be com- 
pared with an agreement of K = 0.77 between the three annotators themselves. If, 
instead of counting these definite descriptions as discourse-new, e simply do not 
include them in our measure of agreement, then the agreement between the system 
and the annotators i  K = 0.78, as opposed to K = 0.81 between the annotators. (Notice 
that the fact that the agreement between annotators goes up, as well, indicates that 
the definite descriptions that the system can't handle are "harder" than the rest.) 
581 
Computational Linguistics Volume 26, Number 4 
Version 2 finds a classification for 355 out of 464 definite descriptions; however, 
its agreement figures are worse. If we count the cases that the system can't classify 
as discourse-new, the agreement between the system and the three annotators for the 
three classes is K = 0.57; if we count hem as bridges, K = 0.63; if we just discard those 
cases, K = 0.63 again. (By comparison, the agreement among annotators on the three 
classes was K -~ 0.68 overall and K = 0.70 on just the cases that the system was able 
to classify.) As mentioned above, the cases that the system can't handle are mainly 
discourse-new descriptions ( ee Figure 7). 
6.5 Deriving the Order of Application of the Heuristics Automatically 
6.5.1 Inducing a Decision Tree. The decision tree discussed in Section 6.1 was derived 
manually, by trial and error. We also tried to derive the order of application of the 
heuristics automatically. To do this, we used a modified version of the system to 
assign Boolean feature values to each definite description in the training corpus (i.e., 
the system checked if the features applied to a definite description instance or not). 
The following features were used: 
. 
. 
. 
4. 
5. 
Special predicates (Spec-Pred): this feature has the value yes if a special 
predicate occurs in the definite description (as specified in Section 4.2), 
and if a complement is there when needed. 
Direct anaphora (Dir-Ana): this feature has the value yes if the system 
can find an antecedent with a same-head noun for that description 
(respecting the constraints discussed in Section 4.1). 
Apposition (Appos): yes when the description is in appositive 
construction. 
Proper noun (PropN): yes when the description has a capitalized initial. 
Restrictive postmodification (RPostm): yes if the definite description is
modified by relative or associative clauses. 
This list of features, together with the classification assigned to each description i  
the standard annotation (DDUse), was used to train an implementation f Quinlan's 
learning algorithm ID3 (Quinlan 1993). We excluded the verification of restrictive pre- 
modification and copula constructions, since these parameters had given the poorest 
results before (see Section 6.2). An example of the samples used to train ID3 is shown 
in (58). 
(58) Spec-Pred Dir-Ana Appos PropN RPostm DDUse 
no no  no  yes  no  3 
no no  no  no  yes  3 
no no no  no  no  2 
no no  no  no  no  2 
no no no  no  no 1 
no yes  no  no  no  1 
The algorithm generates a decision tree on the basis of the samples given. The resulting 
decision tree is presented in Figure 8. 
The main difference between this algorithm and the algorithm we arrived at by 
hand is that the first feature checked by the decision tree generated by ID3 is the 
presence of an antecedent with a same-head noun. The presence of special predicates, 
which we adopted as the first test in our decision tree, is only the fourth test in the 
tree in Figure 8. 
582 
Vieira and Poesio Processing Definite Descriptions 
Dir-Ana Dir-Ana = same head antecedent 
/////NNNNN RPostm = restrictive postmodification 
/ /~stm Appos = apposition 
Spec-Pred =special predicate 
1 /~// ~ &  N PropN = proper noun 
3 ~ec-Pred 
2 Bridging 
3 Discourse new 3 2 
Figure 8 
Generated ecision tree. 
6.5.2 Evaluation of the Automatically Learned Decision Tree. The performance of 
the learned decision tree was compared with that of the algorithm we arrived at by 
trial and error as follows: The first 14 texts of Corpus 1 (845 descriptions) were used 
as training data to generate the decision tree. We then tested the learned algorithm 
over the other 6 texts of that corpus (195 instances of definite descriptions). 
Two different ests were undertaken: 
first, we gave as input to the learning algorithm all cases classified as 
direct anaphora, discourse-new, or bridging, 818 in total (this test 
produces the decision tree presented in the previous section); 
in a second test, the algorithm was trained only with direct anaphora 
and discourse-new descriptions (639 descriptions); all cases classified as 
bridging, idiom, or doubt in the standard annotation were not given as 
input in the learning process. This algorithm was then only able to 
classify descriptions as one of those two classes. The resulting decision 
tree classifies descriptions with a same-head antecedent as anaphoric; all 
the rest as discourse-new. 
Here we present he results evaluated all together, considering the system as a 
classifier only, i.e., without considering the tasks of anaphora resolution and of identi- 
fication of discourse-new descriptions separately. The output produced by the learned 
algorithm is compared to the standard annotation. Since the learned algorithm classi- 
fies all cases, the number of responses i equal to the number of cases, as a consequence, 
recall is the same as precision, and so is the F measure. 
The tests over 6 texts with 195 definite descriptions gave the following results: 
? R = P = F = 69% when the algorithm was trained with three classes; 
? R = P = F = 75%, when training with two classes only. 
583 
Computational Linguistics Volume 26, Number 4 
The best results were achieved by the algorithm trained for two classes only. 
This is not surprising, especially considering how difficult it was for our subjects to 
distinguish between discourse-new and bridging descriptions. 
The hand-crafted decision tree (Version 2) achieved 62% recall and 85% precision 
(F = 71.70%) on those same texts: i.e., a higher precision, but a lower F measure, due 
to a lower recall, since---unlike the learned algorithm--it does not classify all instances 
of definite descriptions. If, however, we take the class discourse-new as a default for 
all cases of definite descriptions not resolved by the system, recall, precision, and F 
value go to 77%, slightly higher than the rates achieved by the decision tree produced 
by ID3. 
As the learned decision tree has the search for a same-head antecedent as the first 
test, we modified our algorithm to work in the same way, and tested it again with the 
two corpora. The results with this configuration were: 
? R = 0.75, P = 0.87, F = 0.80, for the training data (compared with 
R = 0.76, P = 0.88, F = 0.81) ; 
? R = 0.59, P = 0.83, F = 0.69, for the test data (compared with R = 0.62, 
P = 0.83, F = 0.71). 
In other words, the results were about the same, although a slightly better performance 
was obtained when the tests to identify discourse-new descriptions were tried first. 
7. Other Computational Models of Definite Description Processing 
A major difference between our proposal and almost all others (theoretical and im- 
plemented) is that we concentrate on definite descriptions; most of the systems we 
discuss below attempt o resolve all types of anaphoric expressions, often concentrat- 
ing on pronouns. Focusing on definite descriptions allowed us to investigate what 
types of lexical knowledge and commonsense inference are actually used in natural 
language comprehension. 
From an architectural standpoint, he main difference between our work and other 
proposals in the literature is that we paid considerably more attention to the problem 
of identifying discourse-new definite descriptions. 32 
Previous work on computational methods for definite description resolution can 
be divided in two camps: proposals that rely on commonsense r asoning (and are 
therefore ither mainly theoretical or domain dependent), and systems that can be 
quantitatively evaluated, such as those competing on the coreference task in the Sixth 
and Seventh Message Understanding Conference (Sundheim 1995). We discuss these 
two types of work in turn. 
7.1 Models Based On Commonsense Reasoning 
The crucial characteristic of these proposals is that they exploit hand-coded common- 
sense knowledge, and cannot therefore be tested on just any arbitrary text. Some of 
them are simply tested on texts that were especially built for the purpose of testing 
the system (Carter 1987; Carbonell and Brown 1988); systems like the Core Language 
Engine are more robust, but they have to be applied to a domain restricted enough 
that all relevant knowledge can be encoded by hand. 
32 This problem is also a central concern in the work by Bean and Riloff (1999). 
584 
Vieira and Poesio Processing Definite Descriptions 
Sidner's Theory of Definite Anaphora Comprehension. I  her dissertation, Sidner (1979) 
proposed a complete theory of definite NP resolution, including detailed algorithms 
for resolving pronouns, anaphoric definite descriptions, and bridging descriptions. She 
also proposed methods for resolving larger situation uses; the one class her methods 
do not handle are those definite descriptions that, following Hawkins, we have called 
unfamiliar uses. 
The main contribution of Sidner's dissertation is her theory of focus and its role 
in resolving definite NPs; to this day, her focus-tracking algorithms are arguably the 
most detailed account of the phenomenon. The main problem with Sidner's work from 
our perspective is that her algorithms rely heavily on the availability of a semantic 
network and causal reasoner; furthermore, some of the inference mechanisms are left 
relatively underspecified (this latter problem was in part corrected in subsequent work 
by Carter--see below). Lexical and con~nonsense knowledge play three important 
roles in Sidner's system: they are used to track focus, to resolve bridging descriptions 
and larger situation uses, and to evaluate interpretive hypotheses, discarding those 
that seem implausible. Only recently have robust knowledge-based methods for some 
of these tasks begun to appear, and their performance is still not very good, as seen 
above in our discussion of using WordNet as a semantic network; 33 as for checking 
the plausibility of a hypothesis on the basis of causal knowledge about the world, we 
now have a much better theoretical grasp of how such inferences could be made (see, 
for example, Hobbs et al \[1993\] and Lascarides and Asher \[1993\]), but we are still 
quite a long way from a general inference ngine. 
We also found that some of Sidner's resolution rules are too restrictive. For ex- 
ample, her Cospecification rule 1 prescribes that definite description and focus must 
have the same head, and no new information can be introduced by the definite; but 
this rule is violated fairly frequently in our corpus. This criticism is not new: In 1983, 
it was already recognized that an anaphoric full noun phrase may include some new 
and unshared information about a previously mentioned entity (Grosz, Joshi, and We- 
instein 1983), and Carter (1987) weakened some of the restrictions proposed by Sidner 
in his system. 
Carter's Shallow Processing Anaphor Resolver. Carter (1987) implemented a modified ver- 
sion of Sidner's algorithm and integrated it with an implemented version of Wilks' 
theory of commonsense r asoning. This work is interesting for two reasons: first of all, 
because Carter, unlike Sidner, attempted to evaluate the performance ofhis system; and 
because, in doing so, he addressed the commonsense r asoning problem in some detail. 
Carter's ystem, SPAR, is based on the Shallow Processing Hypothesis: that in re- 
solving anaphors, reasoning should be avoided as much as possible. This is, of course, 
the same approach taken in our own work, which could be seen as pushing Carter's ap- 
proach to the extreme. The difference is that when it becomes necessary, SPAR does use 
two commonsense knowledge sources: a semantic network based on Alshawi's theory 
of memory for text interpretation (Alshawi 1987) and a causal reasoner based on Wilks' 
work (Wilks 1975). In both cases, the necessary information was encoded by hand. 
Carter's system was tested over short stories specifically designed for the testing 
of the system: about 40 written by Carter himself, and 23 written by others. These 
latter contain about 80 definite descriptions. SPAR correctly resolved all anaphors in 
the stories written by Carter, and 66 out of 80 of the descriptions in the 23 other stories. 
33 An implementation f a (simplified) version of Sidner's focus-tracking algorithms capable of being 
used by a system like ours was presented in Azzam, Humphreys, and Gaizauskas (1998). 
585 
Computational Linguistics Volume 26, Number 4 
(Carter himself points out that these results are "of limited significance because of the 
simplicity of the texts processed compared to 'real' texts" \[p. 238\].) 
The Core Language Engine. The Core Language Engine (CLE) (Alshawi 1992) is a domain- 
independent system developed at SRI Cambridge, which translates English sentences 
into formal representations. The system was used by SRI for a variety of applications, 
including spoken language translation and airline reservations. The CLE makes use of 
a core lexicon (to which new entries can be added) and uses an abductive common- 
sense reasoner to produce an interpretation a d to verify the plausibility of choice of 
referents from an ordered list; the required world knowledge has to be added by hand 
for each domain, together with whatever lexical knowledge is needed. 
The construction of the formal representation goes through an intermediate stage 
called quasi-logical form (QLF). The QLF may contain unresolved terms correspond- 
ing to anaphoric NPs including, among others, definite descriptions. The resolution 
process that transforms QLFs into resolved logical form representations of entences i  
described in Alshawi (1990). Definite descriptions are represented asquantified terms. 
The referential readings of definite descriptions are handled by proposing referents 
from the external application context (larger situation uses) as well as the CLE context 
model (anaphoric uses). Attributive readings may also be proposed uring QLF reso- 
lution; some of these seem to correspond to our unfamiliar uses. Thus, the CLE seems 
to account for discourse-new descriptions, although they are not explicitly mentioned, 
and the methods used for choosing a referential or an attributive interpretation are 
not discussed. To our knowledge, no analysis of the performance of the system has 
been published. 
7.2 The Systems Involved in the MUC-6 Coreference Task 
The seven systems that participated in the MUC-6 competition can all be quantitatively 
evaluated; they achieved recall scores ranging from 35.69% to 62.78% and precision 
scores ranging from 44.23% to 71.88% on nominal coreference. 
It is important to note that the evaluation in MUC-6 differed from ours in three 
important aspects. First of all, these systems have to parse the texts, which often in- 
troduces errors; furthermore, these systems often cannot get complete parses for the 
sentences they are processing. Secondly, the evaluation i  MUC-6 considers the coref- 
erential chain as a whole, and not only one correct antecedent. The third difference is
that these systems process a wider range of referring expressions, including pronouns 
and bare nouns, while our system only processes definite NPs. On the other hand, not 
all definite descriptions are marked in the MUC-6 coreference task: these systems are 
only required to identify identity relations, and only if the antecedent was introduced 
by a noun phrase (not if it was a clause or a conjoined NP). This leaves out discourse- 
new descriptions and, especially, bridging descriptions, which, as we have seen, are 
by far the most difficult cases. 
Kameyama (1997) analyzes in detail the coreference module of the SRI system 
that participated in MUC-6 (Appelt et al 1995). This system achieved one of the top 
scores for the coreference task: a recall of 59% and a precision of 72%. The SRI system 
uses a sort hierarchy claimed to be sparse and incomplete. For definite descriptions, 
Kameyama reports the results of a test on five articles, containing 61 definite descrip- 
tions in total; recall was 46% (28/61), and for proper names, 69% (22/32). The precision 
figures for these two subclasses are not reported. Some of the errors in definite de- 
scriptions are said to be due to nonidentity referential relations; however, there is no 
mention of differences between discourse-new and bridging descriptions. Other errors 
were said to be related to failure in recognizing synonyms. 
586 
Vieira and Poesio Processing Definite Descriptions 
7.3 Probabilistic Methods in Anaphora Resolution 
Aone and Bennet (1995) propose an automatically trainable anaphora resolution sys- 
tem. They train a decision tree using the C 4.5 algorithm by feeding feature vectors 
for pairs of anaphor and antecedent. They use 66 features, including lexical, syntac- 
tic, semantic, and positional features. Their overall recall and precision figures are 
66.56% and 72.18%. Considering only definite NPs whose referent is an organization 
(that is the only distinction available in their report), recall is 35.19% and precision 
50% (measured on 54 instances). Their training and test texts were newspaper articles 
about joint ventures, and they claim that because ach article always talked about 
more than one organization, finding the antecedents of organizational naphora was 
not straightforward. 
In Burger and Connolly (1992) a Bayesian network is used to resolve anaphora 
by probabilistically combining linguistic evidence. Their sources of evidence are c- 
command (syntactic onstraints), semantic agreement (gender, person, and number 
plus a term subsumption hierarchy), discourse focus, discourse structure, recency, and 
centering. Their methods are described and exemplified but not evaluated. A Bayesian 
framework is also proposed by Cho and Maida (1992) for the identification of definite 
descriptions' referents. 
8. Conclusions and Future Work 
8.1 Contributions 
We have presented a domain-independent system for definite description interpreta- 
tion whose development was based on an empirical study of definite description use 
that included multiannotator experiments. Our system not only attempts to find an 
antecedent for a definite description, it also uses methods for recognizing discourse- 
new descriptions, which our previous studies revealed to be the largest class of def- 
inite descriptions in our corpus. Our algorithms for segmentation, matching, and 
identification of discourse-new descriptions only rely on syntax-based heuristics and 
on on-line lexical sources such as WordNet; the final configuration of these heuris- 
tics, as well as their order of application, was arrived at on the basis of extensive 
experiments using our training corpus. Because our system only relies on "shal- 
low" information, it encounters problems when commonsense r asoning is actually 
needed; on the other hand, it can be tested on any domain without extensive hand- 
coding. 
As far as direct anaphora is concerned, we evaluated heuristic algorithms for 
segmentation and matching. Our system achieved 62% recall and 83% precision for 
direct anaphora resolution on our test data. For identifying discourse-new descriptions, 
we exploited the correlation between certain types of syntactic onstructions and type 
of use noted by Hawkins (1978) and semantically explained by L6bner (1987). Our 
system achieved 69% recall and 72% precision for this class on the test data. Overall, 
the version of the system that only attempts to recognize first-mention and subsequent- 
mention definite descriptions achieved a recall of 53% and a precision of 76% on the 
test corpus if we count the definite descriptions the system can't handle as errors; if 
we count them as discourse-new, both recall and precision are 66%. 
The class of bridging descriptions i  the most difficult to process: this is in part 
because humans themselves do not agree much on which definites count as bridges 
and what their anchors are, in part because lexical knowledge and commonsense 
reasoning are necessary to solve them. Our results for this class are, therefore, still 
very tentative; this did not much affect the performance of the system, however, since 
in the texts we tried, bridging descriptions are a relatively small class. Noncoreferent 
587 
Computational Linguistics Volume 26, Number 4 
bridging descriptions were around 8% of the definite descriptions in the corpus, and 
the class of bridging descriptions including those with a coreferent antecedent with 
a different head noun were about 15% of the total. We tried techniques that do not 
involve heavy axiomatization f commonsense knowledge, and only used an existing 
lexical source, WordNet. 
In other text genres the distribution of definite descriptions into classes might 
change; spoken dialogue, for example, tends to have a higher number of deictic def- 
inite descriptions. However, other researchers (Fraurud 1990) found a similar distri- 
bution of first-mention and subsequent-mention definites in text corpora; we believe 
therefore that the heuristics we propose here, and their ordering, will still be ade- 
quate. Direct anaphora nd discourse-new descriptions can be processed with much 
simpler methods and it seems that the distinguishing features do not usually over- 
lap. 
8.2 What's Needed Next? 
We would like to emphasize again that we are not trying to suggest hat shallow 
methods will be sufficient for processing definite descriptions in the long run. What 
we do believe is that hypotheses about processing should be evaluated; unfortunately, 
only fairly simple techniques can be tested in this way at the moment, but this work 
can serve to motivate more clearly the use of more complex methods. 
We highlighted throughout the paper, and particularly in Section 5, some of the 
points where shallow methods break down, and better lexical sources or commonsense 
knowledge are needed. By far the worse results are obtained for bridging descriptions; 
in this area, the most urgent needs are better sources of lexical knowledge, 34 and some 
robust focusing mechanism. Finding better ways of segmenting the text is perhaps the 
area in which the most progress has been made since we started this project; robust 
methods for text segmentation are now available (Hearst 1997; Richmond, Smith, and 
Amitay 1997). A proper treatment of modification seems harder; as discussed in Sec- 
tion 4.1, it seems necessary to rely heavily on reasoning in some cases. In order to 
improve our treatment of discourse-new descriptions it will be necessary, on the one 
hand, to find ways of automatically acquiring lexical information about the function- 
ality of nouns and adjectives, and on the other hand, to have sources of encyclopedic 
knowledge available. 
8.3 Future Work 
8.3.1 Simple Extensions. In this project we were more interested in clearly identify- 
ing the subtasks of the definite description process that in achieving optimum per- 
formance; as a consequence, there are a number of fairly simple ways in which the 
final version of the system could be improved. The next step in making our system 
truly testable on any type of text would be to make it work off the output of a robust 
parser: we are currently testing Abney's CASS parser (Abney 1991) for this purpose. 
See Ishikawa (1998), for some initial results. We are also experimenting with existing 
software that performs in a more sophisticated way some of the tasks that our system 
currently implements in a fairly crude fashion, including lemmatization, proper name 
recognition, and named entity typing. 
Another aspect of the system that deserves further examination is the construction 
of coreference chains and cases of multiple resolutions. We did not get a clear picture 
34 As mentioned above, we have done some preliminary work on acquiring this information 
automatically (Poesio, Schulte im Walde, and Brew 1998; Ishikawa 1998). 
588 
Vieira and Poesio Processing Definite Descriptions 
of how complete or incomplete, or how broken, the coreferential chains resulting from 
the processing of one text are, nor did we relate them to the chains of the annotated 
texts; to do so, the system and the annotation would have to be extended to cover all 
cases of anaphoric expressions. 
8.3.2 The Role of Focus in Definite Descriptions Processing. Our tests with bridging 
descriptions resulted in a great number of false positives. Our analysis of these data, 
as well as of other corpora (Hitzeman and Poesio 1998), suggests that a local focusing 
mechanism as proposed in Grosz (1977), Sidner (1979), Grosz, Joshi, and Weinstein 
(1983, 1995), and Grosz and Sidner (1986) would improve the results obtained by our 
system. 
There are several reasons why our system does not yet include such a mechanism. 
One problem already mentioned is that Sidner's algorithms as stated, and even as 
implemented by Carter, are difficult to implement, since considerably more lexical 
information is needed than we have available (e.g., about the thematic roles of verbs), 
a rich knowledge base is needed both to resolve bridging descriptions and larger 
situation uses, and commonsense inference is needed to evaluate the plausibility of 
hypotheses. A second problem with Sidner's theory of local focus, as well as others 
such as Centering Theory (Grosz, Joshi, and Weinstein 1995), is the lack of a precise 
characterization f how to deal with complex sentences. Revisions and extensions of 
Sidner's proposal related to these problems have been proposed in Suri and McCoy 
(1994), and include algorithms for updating focus in complex sentences containing 
adjunct clauses uch as before- and after-clauses. 
We plan to incorporate simpler focus-tracking mechanisms in future versions of 
the system, possibly along the lines of Azzam, Humphreys, and Gaizauskas (1998) or 
Tetreault (1999). 
8.3.3 Theoretical Developments. We defended the importance of developing methods 
for identifying discourse-new descriptions, and we believe that there is still need for 
research into the semantics of this class; that is, what, exactly, licenses the use of a 
definite description to refer to a discourse-new entity? The role of premodification 
and postmodification should also be further examined. Postmodification is one of 
the most frequent features of discourse-new descriptions; additional empirical studies 
considering a detailed subclassification f discourse-new descriptions would give us 
a better understanding of the problem. The postmodification of a description often 
acts as an explicit anchor (what LObner \[1987\] calls "disambiguating arguments and 
attributes"); understanding how the head noun of a postmodified escription relates 
"semantically" with its complement is a problem similar to that of identifying the 
semantic relation between a bridging description and its anaphoric anchor, but to date 
there hasn't been much research on this topic (while there has been a lot of work on 
identifying the relations that hold between the premodifiers, especially in noun-noun 
compounds). An NP's head noun may also corefer with its complement, as seen in 
the examples in (59): 
(59) a. the dream of home ownership 
b. the issue of student grants 
We also observed that definite descriptions with premodification were responsible for 
considerable disagreement among the annotators, the reasons for which are still to be 
explained. 
589 
Computational Linguistics Volume 26, Number 4 
We wish to thank Ellen Bard, Rafael 
Bordini, Jean Carletta, Miriam Eckert, Kari 
Fraurud, Rob Gaizauskas, Janet Hitzeman, 
Chris Mellish, and our anonymous 
reviewers for comments, help, and 
suggestions. Renata Vieira was supported in 
part by a fellowship from CNPq, Brazil; 
Massimo Poesio is supported by an EPSRC 
Advanced Research Fellowship. 
References 
Abney, Steve. 1991. Parsing by chunks. In 
R. Berwick, S. Abney, and C. Tenny, 
editors, Principle-based Parsing. Kluwer, 
Dordrecht, pages 257-278. 
Alshawi, Hiyan. 1987. Memory and Context 
for Language Interpretation. Cambridge 
University Press, Cambridge. 
Alshawi, Hiyan. 1990. Resolving 
quasiqogical forms. Computational 
Linguistics, 16(3):133-144. 
Alshawi, Hiyan, editor. 1992. The Core 
Language Engine. MIT Press, Cambridge, 
MA. 
Aone, Chinatsu and Scott W. Bennett. 1995. 
Automated acquisition of anaphora 
resolution strategies. In Proceedings ofthe 
AAAI Spring Symposium on Empirical 
Methods in Discourse Interpretation and 
Generation, pages 1-7, Stanford. 
Appelt, Douglas, Jerry R. Hobbs, John Bear, 
David Israel, Megumi Kameyama, Andy 
Kehler, David Martin, Karen Myers, and 
Mabry Tyson. 1995. SRI International 
FASTUS system MUC-6 test results and 
analysis. In Proc. of the Sixth Message 
Understanding Conference, pages 237-248, 
Columbia, MD, November. 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Evaluating a
focus-based approach to anaphora 
resolution. In COLING-ACL "98: 36th 
Annual Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics, pages 74-78, Montreal, 
Quebec, Canada. 
Bean, David L. and Ellen Riloff. 1999. 
Corpus-based i entification of 
non-anaphoric noun phrases. In 
Proceedings ofthe 37th Annual Meeting, 
pages 373-380, University of Maryland. 
Association for Computational 
Linguistics. 
Bikel, Daniel, Scott Miller, 
Richard Schwartz, and Ralph Weischedel. 
1997. Nymble: A high-performance 
learning name finder. In Proceedings ofthe 
5th Conference on Applied Natural Language 
Processing, pages 194-201, Washington, 
DC Association for Computational 
Linguistics. 
Bosch, Peter and Bart Geurts. 1989. 
Processing definite NPs. IWBS Report 78, 
IBM Germany, July. 
Burger, John D. and Dennis Connolly. 1992. 
Probabilistic resolution of anaphoric 
reference. In Proceedings ofthe AAAI Fall 
Symposium on Probabilistic Approaches to 
Natural Language, pages 17-24, 
Cambridge, MA. 
Carbonell, Jamie and Ralf D. Brown. 1988. 
Anaphora resolution: A multi-strategy 
approach. In Proceedings ofthe 12th 
International Conference on Computational 
Linguistics (COLING-88), pages 96-101, 
Budapest, Hungary. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: The kappa statistic. 
Computational Linguistics, 22(2):249-254. 
Carletta, Jean, Amy Isard, Stephen Isard, 
Jacqueline C. Kowtko, Gwyneth 
Doherty-Sneddon, and Anne H. 
Anderson. 1997. The reliability of a 
dialogue structure coding scheme. 
Computational Linguistics, 23(1):13-32. 
Carter, David M. 1987. Interpreting Anaphors 
in Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
Chinchor, Nancy A. 1995. Statistical 
significance of MUC-6 results. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 39-44, Columbia, MD, 
November 6-8. 
Chinchor, Nancy A. 1997. Overview of 
MUC-7/MET-2. In Proceedings ofthe 
Seventh Message Understanding Conference 
(MUC-7). Available at http://www.muc. 
saic.com/proceedings/ 
muc_7_proceedings/overview, html. 
Cho, Sehyeong and Anthony S. Maida. 1992. 
Using a Bayesian framework to identify 
the referents of definite descriptions. In
Proceedings ofthe AAAI Fall Symposium on 
Probabilistic Approaches to Natural Language, 
pages 39-46, Cambridge, MA. 
Clark, Herbert H. 1977. Inferences in 
comprehension. I  D. Laberge and S. J. 
Samuels, editors, Basic Process in Reading: 
Perception and Comprehension. Lawrence 
Erlbaum, pages 243-263. 
Clark, Herbert H. and Catherine R. 
Marshall. 1981. Definite reference and 
mutual knowledge. In A. Joshi, 
B. Webber, and I. Sag, editors, Elements of 
Discourse Understanding. Cambridge 
University Press, New York. 
Fellbaum, Christiane, editor. 1998. WordNet: 
An Electronic Lexical Database. MIT Press, 
Cambridge, MA. 
590 
Vieira and Poesio Processing Definite Descriptions 
Fox, Barbara A. 1987. Discourse Structure and 
Anaphora. Cambridge University Press, 
Cambridge, UK. 
Fraurud, Keri. 1990. Definiteness and the 
processing of NPs in natural discourse. 
Journal o/Semantics, 7:395-433. 
Gaizauskas, Robert, Takahiro Wakao, 
Kevin Humphreys, Hamish Cunningham, 
and Yorick Wilks. 1995. University of 
Sheffield: Description of the LaSIE System 
as used for MUC-6. In Proceedings o/the 
Sixth Message Understanding Conference 
(MUC-6), pages 207-220. Morgan 
Kaufmann. 
Grosz, Barbara J. 1977. The Representation a d 
Use of Focus in Dialogue Understanding. 
Ph.D. thesis, Stanford University. 
Grosz, Barbara J., Aravind K. Joshi, and 
Scott Weinstein. 1983. Providing a unified 
account of definite noun phrases in 
discourse. In Proceedings o/the 21st Annual 
Meeting, pages 44-50. Association for 
Computational Linguistics. 
Grosz, Barbara. J , Aravind. K. Joshi, and 
Scott Weinstein. 1995. Centering: A
framework for modeling the local 
coherence of discourse. Computational 
Linguistics, 21(2):202-225. (The paper 
originally appeared as an unpublished 
manuscript in 1986.). 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intention, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Hahn, Udo; Michael Strube, and Katja 
Markert. 1996. Bridging textual ellipsis. In 
COLING '96: Proceedings ofthe 16th 
International Conference on Computational 
Linguistics, pages 496-501, Kopenhagen, 
Aug 5-9 1996. 
Hardt, Daniel. 1997. An empirical approach 
to VP ellipsis. Computational Linguistics, 
23(4):525-541. 
Hatzivassiloglou, Vasileios and 
Kathleen McKeown. 1993. Towards the 
automatic identification of adjectival 
scales: clustering adjectives according to 
meaning. In Proceedings o/the 31st Annual 
Meeting, pages 172-182, Ohio State 
University. Association for Computational 
Linguistics. 
Hawkins, John A. 1978. DeJiniteness and 
Indefiniteness. Croom Helm, London. 
Hearst, Marti A. 1997. TextTiling: 
Segmenting text into multi-paragraph 
subtopic passages. Computational 
Linguistics, 23(1):33-64. 
Helm, Irene. 1982. The Semantics o/Definite 
and Indefinite Noun Phrases. Ph.D. thesis, 
University of Massachusetts atAmherst. 
Hitzeman, Janet and Massimo Poesio. 1998. 
Long-distance pronominalisation a d 
global focus. In COLING/ACL "98: 36th 
Annual Meeting of the Association/or 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics. Volume 1, pages 550-556, 
Montreal, Quebec, Canada. 
Hobbs, Jerry R., Mark E. Stickel, Douglas A. 
Appelt, and Paul Martin. 1993. 
Interpretation asabduction. Arti~cial 
Intelligence Journal, 63:69-142. 
Humphreys, Kevin, Robert Gaizauskas, 
Saliha Azzam, Chris Huyck, B. Mitchell, 
and Hamish Cunningham. 1998. 
University of Sheffield: Description of the 
LaSIE-II System as used for MUC-7. In 
Proceedings ofthe Seventh Message 
Understanding Conference (MUC-7). 
Available on the Web at 
www.muc.saic.com. 
Ishikawa, Tomonori. 1998. Acquisition of 
associative information and resolution of 
bridging descriptions. Master's thesis, 
University of Edinburgh, Department of
Linguistics, Edinburgh, Scotland. 
Kameyama, Megumi. 1997. Recognizing 
referential links: An information 
extraction perspective. In Proceedings off the 
ACL Workshop on Operational Factors in 
Practical, Robust Anaphora Resolution/or 
Unrestricted Texts, pages 46-53, Madrid, 
Spain, July. Association for 
Computational Linguistics. 
Krippendorff, Klaus. 1980. Content Analysis: 
An Introduction to its Methodology. Sage 
Publications, Beverly Hills, London. 
Landis, J. R., and G. G. Koch. 1977. The 
measurement of observer agreement for 
categorial data. Biometrics, 36:159-174. 
Lappin, Shalom and H. J. Leass. 1994. An 
algorithm for pronominal anaphora 
resolution. Computational Linguistics, 
20(4):535-562. 
Lascarides, Alex and Nicholas Ashen 1993. 
Temporal interpretation, discourse 
relations and commonsense entailment. 
Linguistics and Philosophy, 16(5):437-493. 
LObner, Sebastian. 1987. Natural anguage 
and generalised quantifier theory. In 
P. G/irdenfors, editor, Generalized 
Quantifiers. D. Reidel, Dordrecht, The 
Netherlands, pages 93-108. 
Mani, Inderjeet and T. Richard MacMillan. 
1996. Identifying unknown proper names 
in newswire text. In Bran Boguraev and 
James Pustejovsky, editors, Corpus 
Processing/or Lexical Acquisition. MIT 
Press, Cambridge, MA, pages 41-59. 
Marcu, Daniel. 1999. A decision-based 
approach to rhetorical parsing. In 
Proceedings off the 37th Annual Meeting, 
591 
Computational Linguistics Volume 26, Number 4 
pages 365--372, University of Maryland, 
June. Association for Computational 
Linguistics. 
Marcus, Mitchell P., Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building 
a large annotated corpus of English: The 
Penn Treebank. Computational Linguistics, 
19(2):313-330. 
McDonald, David. 1996. Internal and 
external evidence in the identification and 
semantic ategorization f proper names. 
In Bran Boguraev and James Pustejovsky, 
editors, Corpus Processing for Lexical 
Acquisition. MIT Press, Cambridge, MA, 
pages 21-39. 
Mikheev, Andrei, Marc Moens, and 
Claire Grover. 1999. Named entity 
recognition without gazetteers. In
Proceedings ofEACL, pages 1-8, Bergen, 
Norway. EACL. 
Mitkov, Ruslan. 2000. Towards more 
comprehensive evaluation in anaphora 
resolution. In Proceedings ofthe 2nd 
International Conference on Language 
Resources and Evaluation, 
pages 1,309-1,314, Athens. 
Paik, Woojin, Elizabeth D. Liddy, 
Edmund Yu, and Mary McKenna. 1996. 
Categorizing and standardizing proper 
nouns for efficient information retrieval. 
In Bran Boguraev and James Pustejovsky, 
editors, Corpus Processing for Lexical 
Acquisition. MIT Press, Cambridge, MA, 
pages 61-73. 
Palmer, David D. and David S. Day. 1997. A 
statistical profile of the named entity task. 
In Proceedings ofthe 5th Conference on 
Applied Natural Language Processing, 
pages 190-193, Washington, DC, March. 
Association for Computational 
Linguistics. 
Poesio, Massimo. 1993. A situation-theoretic 
formalization of definite description 
interpretation i  plan elaboration 
dialogues. In Peter Aczel, David Israel, 
Yasuhiro Katagiri, and Stanley Peters, 
editors, Situation Theory and its 
Applications, Volume 3. CSLI, Stanford, 
chapter 12, pages 339-374. 
Poesio, Massimo, Sabine Schulte im Walde, 
and Chris Brew. 1998. Lexical clustering 
and definite description interpretation. In 
Proceedings ofthe AAAI Spring Symposium 
on Learning for Discourse, pages 82-89, 
Stanford, CA, March. AAAI. 
Poesio, Massimo and Renata Vieira. 1998. A 
corpus-based investigation of definite 
description use. Computational Linguistics, 
24(2):183-216. 
Poesio, Massimo, Renata Vieira, and 
Simone Teufel. 1997. Resolving bridging 
references in unrestricted text. In 
R. Mitkov, editor, Proceedings ofthe ACL 
Workshop on Operational Factors in Robust 
Anaphora Resolution, pages 1-6, Madrid. 
Also available as HCRC Research Paper 
HCRC/RP-87, University of Edinburgh. 
Postal, Paul M. 1969. Anaphoric islands. In 
R. I. Binnick et al, editor, Papers~om the 
Fifth Regional Meeting of the Chicago 
Linguistic Society, pages 205-235. 
University of Chicago. 
Prince, Ellen F. 1981. Toward a taxonomy of 
given-new information. In Peter Cole, 
editor, Radical Pragmatics. Academic Press, 
New York, pages 223-256. 
Prince, Ellen F. 1992. The ZPG letter: 
Subjects, definiteness, and information 
status. In S. Thompson and W. Mann, 
editors, Discourse Description: Diverse 
Analyses of a Fund-Raising Text. John 
Benjamins, pages 295-325. 
Quinlan, J. Ross. 1993. C4.5: Programs for 
Machine Learning. Morgan Kaufmann, San 
Mateo, CA. 
Quirk, Randolph, Sydney Greenbaum, 
Geoffrey Leech, and Jan Svartvik. 1985. A 
Comprehensive Grammar of the English 
Language. Longman, London. 
Reichman, Rachel. 1985. Getting Computers to 
Talk Like You and Me. MIT Press, 
Cambridge, MA. 
Richmond, Kevin, Andrew Smith, and 
Einat Amitay. 1997. Detecting subject 
boundaries within text: A 
language-independent sta istical 
approach. In Proceedings ofThe Second 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2), 
pages 47-54, Brown University. 
Russell, Bertrand. 1905. On denoting. Mind, 
14:479-493. Reprinted in Logic and 
Knowledge, R. C. Marsh, editor, George 
Allen and Unwin, London. 
Sidner, Candace L. 1979. Towards a
computational theory of definite anaphora 
comprehension in English discourse. Ph.D. 
thesis, MIT. 
Siegel, Sydney and N. John Castellan. 1988. 
Nonparametric statistics for the Behavioral 
Sciences. 2nd edition. McGraw-Hill. 
Strand, Kjetil. 1996. A taxonomy of linking 
relations. Manuscript. A preliminary 
version presented at the Workshop on 
Indirect Anaphora, Lancaster University, 
1996. 
Sundheim, Beth M. 1995. Overview of the 
results of the MUC-6 evaluation. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 13-31, Columbia, MD, 
November 6-8. 
592 
Vieira and Poesio Processing Definite Descriptions 
Suri, Linda Z. and Kathleen F. McCoy. 1994. 
RAFT/RAPR and centering: A
comparison and discussion of problems 
related to processing complex sentences. 
Computational Linguistics, 20(2):301-317. 
Tetreault, Joel R. 1999. Analysis of 
syntax-based pronoun resolution 
methods. In Proceedings ofthe 37th Annual 
Meeting, pages 602-605, University of 
Maryland, June. Association for 
Computational Linguistics. 
Vieira, Renata. 1998. Definite Description 
Resolution i  Unrestricted Texts. Ph.D. 
thesis, University of Edinburgh, Centre 
for Cognitive Science, February. 
Vieira, Renata and Massimo Poesio. 1996. 
Processing definite descriptions in
corpora. Presented at the Discourse 
Anaphora nd Resolution Colloquium 
(DAARC), Lancaster University, 
Lancaster, UK. Also available as Research 
Paper HCRC/RP-86, University of 
Edinburgh, Human Communication 
Research Centre. 
Vieira, Renata and Simone Teufel. 1997. 
Towards resolution of bridging 
descriptions. In Proceedings ofthe 35th Joint 
Meeting of the Association for Computational 
Linguistics, pages 522-524, Madrid. 
Vilain, Marc, John Burger, John Aberdeen, 
Dennis Connolly, and Lynette Hirschman. 
1995. A model-theoretic coreference 
scoring scheme. In Proc. of the Sixth 
Message Understanding Conference, 
pages 45-52. 
Wacholder, Nina and Yael. Ravin. 1997. 
Disambiguation ofproper names in text. 
In Proceedings ofthe 5th Conference on 
Applied Natural Language Processing, 
pages 202-208, Washington, DC, March. 
Association for Computational 
Linguistics. 
Ward, Gregory, Richard Sproat, and 
Gail McKoon. 1991. A pragmatic analysis 
of so-called anaphoric islands. Language, 
67:439-474. 
Webber, Bonnie L. 1979. A Formal Approach to 
Discourse Anaphora. Garland, New York. 
Wilks, Yorick A. 1975. A preferential 
pattern-matching semantics for natural 
language. Artificial Intelligence, 6:53-74. 
593 

From concrete to virtual annotation mark-up language: the case of
COMMOn-REFs
Renata Vieira   , Caroline Gasperin   , Rodrigo Goulart  
PIPCA - Unisinos
S?o Leopoldo, Brazil
{renata,caroline,rodrigo}@exatas.unisinos.br
Susanne Salmon-Alt
ATILF-CNRS
Nancy, France
Susanne.Alt@loria.fr
Abstract
This work presents the data model we
adopted for annotating coreference. Our
data model includes different levels of an-
notation, such as part-of-speech, syntax
and discourse. We compare our encod-
ing schemes to the abstract XML encod-
ing being proposed as standard. We also
present our tool for coreference resolution
that handles our data model.
1 Introduction
We have been dealing with corpus based studies
since 1997 (Renata Vieira and Simone Teufel, 1997;
Poesio et al, 1997). Our focus has been the study
of coreference. In the study of coreference we have
dealt with annotation experiments (manual and auto-
matic) and their respective annotation schemes. To
work on coreference we used information from syn-
tactic annotated corpus, the Penn Treebank. Our re-
sults (annotated corpus with coreference links and
classification of coreference status) were Prolog en-
coded. When we first adapted our tool for Por-
tuguese (Rossi et al, 2001) we dealt with other tools
and annotation formats. The resources built on these
previous works were difficult to share due to their
particular information encoding.
Our current work in the COMMOn-REFs project
(A computational model for processing referring ex-
pressions)1 , involves Portuguese and French. We
1http://www.inf.unisinos.br/ renata/documents/commonrefs-
proposal.pdf.
Research Grant CNPq- Brazil.
are using MMAX, a tool for multimodal annotation
in XML (M?ller and Strube, 2001), for manual an-
notation of coreference, and we are developing a
tool for automatic coreference resolution. Our tool
deals with XML encoding provided by MMAX and
syntactic information for Portuguese and French en-
coded in XML. In order to be able to share the re-
sources being built, we are relating our model with
proposed standards.
In Section 2 we present previous annotation for-
mats that we dealt with. In Section 3 we give an
overview of the work in COMMOn-REFs. Section 4
relates our current model with the standards recently
proposed (Ide and Romary, 2002; Ide and Romary,
2003; Ide and Romary, 2001). Section 5 describes
our tool for coreference resolution. A discussion on
the problems we face with our annotation model is
presented in Section 6.
2 Previous work
Our first annotation schemes were Prolog lists of
treebank sentences and their noun phrases (NPs), as
shown in Figure 1. The lists were extracted from
Lisp lists of the Penn Treebank. These lists were
manipulated in our experiments on coreference an-
notation and resolution.
The results of coreference annotation were lists of
Prolog facts dcc(Index1,Index2,Code) as shown
in Figure 2. Index1 refers to the sequen-
tial numbering of definite descriptions; Index2
refers to the sequential numbering of noun
phrases; and Code refers to their classification,
according to discourse status (Poesio and Vieira,
1998). For some of them there were also facts
[S,[NP,the,squabbling,[PP,within,[NP,the,
Organization,[PP,of,[NP,Petroleum,Exporting,
Countries]]]]],[VP,seems,[PP,under,
[NP,control]],[PP,for,now]].].
[NP,Petroleum,Exporting,Countries].
[NP,the,Organization,
[PP,of,[NP,Petroleum,Exporting,Countries]]].
[NP,the,squabbling,[PP,within,
[NP,the,Organization...
Figure 1: Prolog NP and S lists.
...
ddc(5, 16, r).
ddc(6, 18, k).
ddc(7, 28, r).
...
ddsr(5, 16, r, np(5)).
ddsr(7, 28, r, np(16)).
...
Figure 2: Prolog coreference annotation.
ddsr(Index1,Index2,Code,Antecedent) indicat-
ing their antecedent NPs. We could only link the an-
notation to the data by running the Prolog code that
loaded the lists of NPs and sentences and generated
their indexes. Although we had carried out intensive
research with these resources and tools, the re-use of
our data in other environments was very difficult.
Despite the lack of fully annotated data for Por-
tuguese, we tried to check out whether the same
heuristics we used for English would be suitable for
this new language. To test our heuristics we used
the PALAVRAS parser2 (Bick, 2000) to parse Por-
tuguese corpus. From parsed texts we extracted Pro-
log lists of NPs as illustrated in Figure 3. Experi-
ments were carried out over these resources. Heuris-
tics for correference resolution were adapted to Por-
tuguese and the results obtained were comparable to
those previously obtained for English. However, the
genericity of the Portuguese resolver and annotated
data still raised the same re-usability problems as for
English, since the encoding format had not evolved.
3 COMMOn-REFs
In the COMMOn-REFs project we face the chal-
lenge of dealing with different languages (French
2http://visl.hum.sdu.dk/visl/pt
STA:fcl
P:v-fin(?ser? PR 3P IND) S?o
SC:adj(?remoto? F P) remotas
SUBJ:np
=>N:art(?o? <artd> F P) as
=H:n(?chance? F P) chances
=N<:pp
==H:prp(?de?) de
==P<:np
===H:n(?aprova??o? F S) aprova??o
....
[NP,as,[N,chances],[PP,de,
[NP,[N,aprovacao]]]]
[NP,[N,aprovacao]]
Figure 3: PALAVRAS output.
and Portuguese). Therefore, we have to share cor-
pora and tools, initially available under different for-
mats.
We adopted MMAX3 as our manual annotation
tool. With MMAX we could annotate our corpus
according to our theoretical principles. The follow-
ing corpus studies were developed with the aid of
the tool: (Salmon-Alt and Vieira, 2002; Vieira et al,
2002b; Vieira et al, 2002a). In these studies, our
annotation targets were manually marked and coref-
erence information was added to them according to
subjects? analysis of the texts.
We are currently developing a coreference resolu-
tion tool on the basis of XML files and XSL scripts.
The tool manipulates several levels of linguistic in-
formation. Parsing information has been provided
by the PALAVRAS parser. The parser output is
transformed into two XML files: one with POS
and another with syntactic information (chunks).
Coreference information, manually annotated with
MMAX (markables), is used for evaluation. Our
tool, besides manipulating three different annotation
levels (POS, chunks, markables), creates two others:
anaphors and candidates, as detailed in Section 5.
As we are interested in having our resources made
available, we relate our annotation schemes to stan-
dard proposals presented in (Ide and Romary, 2002;
Ide and Romary, 2003; Ide and Romary, 2001).
3The MMAX tool is available for download in
http://www.eml.org/english/Research/NLP/Downloads.
<markables>
...
<markable id="markable_1"
coref="no"
classif="disc_stat_1"
span="word_1..word_2" />
...
<markable id="markable_3"
coref="yes"
pointer="markable_1"
classif="disc_stat_2"
span="word_8" />
...
</markables>
<struct type="CRAnnot">
...
<struct id="m1" type="C-level">
<feat type="coref">no</feat>
<feat type="classif">disc_stat_1</feat>
<feat type="target">#w1 #w3</feat>
</struct>
<struct id="m3" type="C-level">
<feat type="coref">yes</feat>
<feat type="pointer">#m1</feat>
<feat type="classif">disc_stat_2</feat>
<feat type="target">#w8</feat>
</struct>
...
</struct>(a) (b)
Figure 5: Markables file.
<words>
<word id="word_1">O</word>
<word id="word_2">jogador</word>
<word id="word_3">pode</word>
<word id="word_4">deixar</word>
<word id="word_5">o</word>
<word id="word_6">time</word>
<word id="word_7">.</word>
<word id="word_8">Ele</word>
<word id="word_9">recebeu</word>
<word id="word_10">uma</word>
<word id="word_11">proposta</word>
<word id="word_10">excelente</word>
<word id="word_11">.</word>
</words>
Figure 4: Words file.
4 Data model
4.1 Encoding standards
Directions for standard corpus encoding in XML
have been proposed in (Ide and Romary, 2002; Ide
and Romary, 2003; Ide and Romary, 2001). Such
efforts consist on defining abstract formats for cor-
pus annotation that could be instantiated according
to specific project requirements. An abstract XML
file can be generated for each annotation level ac-
cording to a Virtual Annotation Markup Language
(VAML). The structure of this language is defined
by a skeleton that consists on <struct> (a node/level
in the annotation) and <feat> elements (feature at-
tached to the enclosing <struct> node).
Particular project specifications are defined
through data categories (component categories to be
annotated) and dialect (encoding style). On the basis
of these specifications, a mapping between VAML
and Concrete AML (CAML) can be made. CAML
is the language used for annotation encoding in par-
ticular projects.
4.2 Our schemes
Our first experiments with MMAX were on man-
ual coreference annotation. The tool required spe-
cific input and output formats. Our corpus, that is,
the primary data, were first converted from raw texts
to XML, encoded as <word> elements, like the ex-
ample in Figure 4 for the sentences O jogador pode
deixar o time. Ele recebeu uma proposta excelente.
(The player may leave the club. He received an ex-
cellent proposal.) Each corpus token (words and
punctuation) corresponds to a <word>.
The coreference was manually annotated and en-
coded as <markable> elements. Each anaphoric ex-
pression and antecedent were represented by mark-
ables. Anaphors? markables had an extra attribute
?pointer?, that refers to its antecedent markable. An
example of a markable file is presented on Figure
5(a). Markables correspond to our final level of an-
notation. The ?span? attribute refers to our primary
data, the words. The other attributes (coref, classific)
were specified according to our application. Fig-
ure 5(b) represents the abstract XML encoding for
our markables file, according to VAML4. Pointer,
coreference and classification compose our dialect
vocabulary for the following data categories: an-
tecedent and types of discourse status, as in (Poesio
4As we are not aware of a registry of data categories for
coreference level, in our examples throughout the paper we of-
ten use the same vocabulary in abstract and concrete encodings.
<words>
<word id="word_1">
<art canon="o" gender="M" number="S"/>
</word>
<word id="word_2">
<n canon="jogador" gender="M" number="S"/>
</word>
...
</words>
<struct type="MSAnnot">
<struct id="w1" type="W-level">
<feat type="pos">ART</feat>
<feat type="lemma">o</feat>
<feat type="gender">M</feat>
<feat type="number">S</feat>
<feat type="target">#w1</feat>
</struct>
<struct id="w2" type="W-level">
<feat type="pos">N</feat>
<feat type="lemma">jogador</feat>
<feat type="gender">M</feat>
<feat type="number">S</feat>
<feat type="target">#w2</feat>
</struct>
...
</struct>(a) (b)
Figure 6: POS file.
and Vieira, 1998) inpired on (Prince, 1981; Prince,
1992). According to our dialect instantiation style,
the data categories are represented as attributes of
<markable> elements.
To develop a tool for automatic coreference reso-
lution, we needed to consider other intermediate lev-
els of annotation: source of linguistic information
used for solving anaphoras.
Our corpus was analysed by the Portuguese parser
PALAVRAS (Bick, 2000). The original format of
PALAVRAS output is not standard. As previously
presented in Figure 3, on each line of the figure:
  the first symbol represents the syntactic func-
tion (?SUBJ?= subject, ?N?=noun modifier,
?H?=head, etc.);
  following ?:? , we have the syntactic form for
groups of words (?np?=noun phrase, etc.) and
POS-tags for single words (?n?=noun, ?v?=verb,
etc.);
  in brackets are the word canonical form and
other inflectional tags;
  after the brackets comes the word as it occurs
in the corpus.
The ?=? signs in the beginning of each line repre-
sent the level of the phrase in the parsing tree5.
5A complete description of the tagset symbols is available at
http://visl.hum.sdu.dk/visl/pt/info/symbolset-manual.html.
We defined the XML encoding for PALAVRAS
output. We split PALAVRAS output into two anno-
tation levels, one for POS and another for syntactic
data. Figure 6(a) shows our scheme for POS file.
The corresponding abstract XML file is presented on
Figure 6(b). Our data categories are word canonic
form (lemma), pos, gender, number, person, tense,
mode, and case. According to our dialect instanti-
ation style, each POS data category is represented
by a new XML element, the other inflexional tags
are encoded as attributes of this element. By han-
dling a parsed corpus we could treat compounds at
word level; the multi word expression ?S?o Paulo?,
for example, is tokenised as one word and codified
as <word id="word_9">S?o_Paulo</word>.
We encode syntactic data as chunks. Each syntac-
tic structure is represented by a <chunk> element.
Figure 7(a) shows our encoding. The mapping to
abstract XML is presented on Figure 7(b). In our di-
alect, each <chunk> in the concrete XML encoding
corresponds to a <struct> in the abstract one.
5 Automatic coreference resolution
The tool we are developing for anaphora resolution
takes as input word, POS and chunk files (the archi-
tecture design is shown on Figure 8). The resolution
process is perfomed by a set of stylesheets, each one
representing a different heuristic. This set is called
Resolution Heuristics Base (RHB). A stylesheet is
connected to another through pipes and it filters the
information flowing through the system (Gamma et
<text>
<paragraph id="paragraph_1">
<sentence id="sentence_1" span="word_1..word_7">
<chunk id="chunk_1" function="subj" form="np" span="word_1..word_2">
<chunk id="chunk_2" function="n" form="art" span="word_1"/>
<chunk id="chunk_3" function="h" form="n" span="word_2"/>
</chunk>
<chunk id="chunk_4" function="p" form="vp" span="word_3..word_4">
<chunk id="chunk_5" function="aux" form="v" span="word_3"/>
<chunk id="chunk_6" function="h" form="v" span="word_4"/>
</chunk>
<chunk id="chunk_7" function="acc" form="np" span="word_5..word_6">
<chunk id="chunk_8" function="n" form="art" span="word_5"/>
<chunk id="chunk_9" function="h" form="n" span="word_6"/>
</chunk>
</sentence>
...
</text> (a)
<struct id="s0" type="T">
<struct id="s1" type="P">
<struct id="s2" type="S" span="word_1..word_7">
<struct id="s3" type="NP" rel="subj" ref="word_1..word_2">
<struct id="s4" type="art" rel="n-mod" ref="word_1"/>
<struct id="s5" type="n" rel="h" ref="word_2"/>
</struct>
<struct id="s6" type="VP" rel="p" ref="word_3..word_4">
<struct id="s7" type="v" rel="aux" ref="word_3"/>
<struct id="s8" type="v" rel="h" ref="word_4"/>
</struct>
<struct id="s9" type="NP" rel="acc" ref="word_5..word_6">
<struct id="s10" type="art" rel="n-mod" ref="word_5"/>
<struct id="s11" type="n" rel="h" ref="word_6"/>
</struct>
</struct>
...
</struct> (b)
Figure 7: Chunks file.
Chunks
 A 
Anaphor
selection
Markables
generation
 C 
Markables
 B 
Candidates
selection
POS Words
R1
Rn
RHB
R2
Figure 8: Anaphora resolution design.
al., 1995), and all heuristics can access the input files
when necessary. Our tool strategy follows four main
steps: anaphor selection, candidates selection, reso-
lution, and output generation.
Two new intermediate annotation levels are
generated: the anaphor entities (represented by
<anaphor> elements) and antecedent candidates
(represented by <candidate> elements).
The <candidate> represents possible antecedents
in the corpus, and it also has a ?span? attribute. (Fig-
ure 9(a)). Different candidate sets can be gener-
ated according to the heuristics used for its selec-
tion. demonstrates the corresponding VAML en-
coding. The <anaphor> depicts the anaphoric noun
phrases (pronouns, definite descriptions, demonstra-
tives) and it has the attribute?span? (Figure 9(b)).
Through ?span? value we can get information from
the input files (words, POS, chunks), needed for the
resolution process.
Along the resolution process other attributes
are added to anaphor elements, such as ?coref?,
?pointer? and ?classif? attributes, as seen in Fig-
ure 10(a). Figure 10(b) represents the corresponding
VAML encoding for the <anaphor> elements.
The heuristics to be applied to resolve coreference
are based on previous studies about resolution of re-
ferring expressions (Vieira and Poesio, 2000; Lap-
pin and Leass, 1994; Strube et al, 2002) and they
are not discussed here.
The output is the last step in the process and
it is also played by a stylesheet that translates the
<anaphor> nodes into <markable> ones, so the re-
sults can be visualized using the MMAX tool.
6 Discussion
We have presented the evolution of our annotation
schemes over 7 years of corpus research. We be-
lieve that a standard orientation may shed some light
to those who are defining their projects. Concerning
annotation level relations our annotation is based on
object-based anchoring, especially because our pri-
mary data is represented by XML elements (words
in our dialect, basic struct elements with id attributes
in VAML).
Considering relations like parallelism, alterna-
tives and aggregation (Ide and Romary, 2002) we
see that our model includes aggregation at the chunk
level. When studying annotation agreement we need
to represent alternative data according to the judg-
ment of each annotator (although we have adopted
duplicated annotated files previously in our project).
Previous work on encoding standards has men-
tioned mainly POS and syntactic annotation. In this
paper we extended its use for coreference annota-
tion. Our data model could be adequately mapped
to the standards.
An issue raised by coreference annotation is the
need of two references for primary data in the same
<struct>, one for anaphor (target) and another for its
antecedent. In our examples, we encoded the ref-
erence to primary data indicating the antecedent by
<feat> elements with attribute type=?pointer?.
An advantage we could expect to take from work
related to standards is knowledge about the impact
on performance in data handling according to en-
coding decisions.
Our project deals with different input and output
formats. We intend to share our results and compare
our techniques to different ones for anaphora reso-
<candidates>
<candidate span="word_1..word_2"/>
<candidate span="word_5..word_6"/>
<candidate span="word_8"/>
</candidates>
<anaphors>
<anaphor span="word_1..word_2"/>
<anaphor span="word_5..word_6" />
<anaphor span="word_8" />
</anaphors>
(a) (b)
Figure 9: Candidate and anaphors.
<anaphors>
<anaphor span="word_1..word_2"/>
<anaphor span="word_5..word_6" />
<anaphor span="word_8"
coref="yes"
classif="disc_stat_2"
pointer="word_1..word_2"/>
</anaphors>
<struct type="ANAnnot">
<struct type="A-level">
<feat type="target">#w1 #w2</feat>
</struct>
<struct type="A-level">
<feat type ="target">#w5 #w6</feat>
</struct>
<struct type="A-level">
<feat type="target">#w8</feat>
<feat type="coref">yes</feat>
<feat type="classif">disc_stat_2</feat>
<feat type="pointer">#w1 #w2</feat>
</struct>
</struct>(a) (b)
Figure 10: Resolved anaphors.
lution. Since we use XML for external and internal
encoding, and there is a mapping between them and
standard formats, such as VAML, we will be able to
import and export the corresponding VAML for our
CAML and share both our resources and tools.
7 Acknowledgments
We would like to thank CNPq, INRIA and
FAPERGS for financial support. We would like to
thank the help of Eckhard Bick, Christoph M?ller,
Michael Strube and Paulo Quaresma.
References
Eckhard Bick. 2000. The Parsing System PALAVRAS:
Automatic Grammatical Analysis of Portuguese in a
Constraint Grammar Framework. Ph.D. thesis, ?rhus
University, ?rhus.
Erich Gamma, Richard Helm, Ralph Johnson, and John
Vlissides. 1995. Design Patterns: Elements of
Reusable Object-Oriented Software. Addison-Wesley
Professional Computing Series. Addison-Wesley Pub-
lishing Company, New York.
Nancy Ide and Laurent Romary. 2001. Common frame-
work for syntactic annotation. In Proceedings of
ACL?2001, pages 298?305, Toulouse.
Nancy Ide and Laurent Romary. 2002. Standards for
language resources. In Proceedings of the LREC 2002,
pages 839?844, Las Palmas de Gran Canaria.
Nancy Ide and Laurent Romary. 2003. Encoding syn-
tactic annotation. In Anne Abeill?, editor, Building
and Using Syntactically Annotated Corpora (in press).
Kluwer, Dordrecht.
Shalom Lappin and Herbert Leass. 1994. An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20(4).
Christoph M?ller and Michael Strube. 2001. MMAX:
A tool for the annotation of multi-modal corpora. In
Proceedings of the IJCAI 2001, pages 45?50, Seattle.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging descriptions in unrestricted
texts. In Proceedings of the Practical, Robust,
Anaphora Resolution for Unrestricted Texts, Workshop
on Operational Factors, Madrid.
Ellen F. Prince. 1981. Toward a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?256. Academic Press, New York.
Ellen F. Prince. 1992. The ZPG letter: subjects, def-
initeness, and information status. In S. Thompson
and W. Mann, editors, Discourse description: diverse
analyses of a fund-raising text, pages 295?325. John
Benjamins, New York.
Renata Vieira and Simone Teufel. 1997. Towards reso-
lution of bridging descriptions. In Proceedings of the
ACL-EACL?97 Joint Conference: 35th Annual Meet-
ing of the Association for Computational Linguistics
and 8th Conference of the European Chapter of the
Association for Computational Linguistics, Madrid.
Daniela Rossi, Clarissa Pinheiro, Nara Feier, and Renata
Vieira. 2001. Resolu??o de correfer?ncia em textos
da l?ngua portuguesa. Revista Eletr?nica de Inicia??o
Cient?fica, 1(2).
Susanne Salmon-Alt and Renata Vieira. 2002. Nomi-
nal expressions in multilingual corpora: Definites and
demonstratives. In Proceedings of the LREC 2002,
Las Palmas de Gran Canaria.
Michael Strube, Stefan Rapp, and Christoph M?ller.
2002. The influence of minimum edit distance on
reference resolution. In Proceedings of the EMNLP
2002, Philadelphia.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):525?
579.
Renata Vieira, Susanne Salmon-Alt, Caroline Gasperin,
Emmanuel Schang, and Gabriel Othero. 2002a.
Coreference and anaphoric relations of demonstrative
noun phrases in multilingual corpus. In Proceedings
of the DAARC 2002, Estoril.
Renata Vieira, Susanne Salmon-Alt, and Emmanuel
Schang. 2002b. Multilingual corpora annotation for
processing definite descriptions. In Proceedings of the
PorTAL 2002, Faro.
Using word similarity lists for resolving indirect anaphora
Caroline Gasperin and Renata Vieira
PIPCA - Unisinos
S?o Leopoldo, Brazil
{caroline,renata}@exatas.unisinos.br
Abstract
In this work we test the use of word similarity lists
for anaphora resolution in Portuguese corpora. We
applied an automatic lexical acquisition technique
over parsed texts to identify semantically similar
words. After that, we made use of this lexical
knowledge to resolve coreferent definite descrip-
tions where the head-noun of the anaphor is differ-
ent from the head-noun of its antecedent, which we
call indirect anaphora.
1 Introduction
In this work we investigate the use of word similar-
ity list for treating coreference, especially the cases
where the coreferent expressions have semantically
related head nouns (instead of same head nouns),
which we call indirect anaphora.
We applied a lexical acquisition technique
(Gasperin, 2001) over Portuguese parsed corpora to
automatically identify semantically similar words.
After that, we made use of this lexical knowledge
to resolve the coreferent definite descriptions where
the head-noun of the anaphor is different from the
head-noun of its antecedent.
Previous work on anaphoric resolution of English
texts has used acquired lexical knowledge in differ-
ent ways, examples are (Poesio et al, 2002; Schulte
im Walde, 1997; Bunescu, 2003).
This paper is organised as follows. The next sec-
tion explain our notion of indirect anaphora. Section
3 details the tools and techniques used to the con-
struction of our lexical resource. Section 4 presents
our heuristic for solving the indirect anaphors on the
basis of such resource. Section 5 details the corpus
we are using for evaluating the proposed heuristics.
Section 6 reports the implementation of the heuris-
tic and in Section 7 we present our experiments
over Portuguese annotated corpora. In Section 8 we
discuss our results and compare them to previous
works. Finally, Section 9 presents our concluding
comments.
2 Indirect anaphora
Coreference has been defined by (van Deemter and
Kibble, 2000) as the relation holding between lin-
guistic expressions that refer to the same extralin-
guistic entity. A slightly different discourse rela-
tion is anaphora. In an anaphoric relation the in-
terpretation of an expression is dependent on previ-
ous expressions within the same discourse (in vari-
ous ways). Therefore, an anaphoric relation may be
coreferent or not. An expression may be anaphoric
in the strict sense that its interpretation is only possi-
ble on the basis of the antecedent, as it is in general
the case of pronouns in written discourse. On the
other hand, it might be just coreferent, in the sense
that the entity has been mentioned before in the text.
In this work, we focus on the expressions that are
anaphoric and coreferent, and restricting even more,
just the indirect cases, when the antecedent head-
noun and the anaphor head-noun are not same but
semantically related.
To clarify what we mean by indirect anaphora,
we detail the classification we adopted in our pre-
vious work (Vieira et al, 2002; Vieira et al, 2003).
Our classes of analyses were based on the analy-
ses of English texts presented in (Poesio and Vieira,
1998), with the difference that we divided the Bridg-
ing class of their analyses into two different classes,
separating coreferent (Indirect Anaphora) and non-
coreferent (Other Anaphora) cases. Each definite
description (d) is classified into one of the follow-
ing four classes:
1. Direct anaphora: d corefers with a previous
expression a; d and a have the same nominal
head:
a. A Comiss?o tem conhecimento do livro... (the
Commission knows the book)
d. a Comiss?o constata ainda que o livro n?o se
debru?a sobre a actividade das v?rias... (the Com-
mission remarks that the book ignores the activity
of various)
2. Indirect anaphora: d corefers with a previous
expression a; d and a have different nominal
heads:
a. a circula??o dos cidad?es que dirigem-se... (the
flow of the citizens heading to...)
d. do controle das pessoas nas fronteiras (the con-
trol of the people in the borders)
3. Other Anaphora: d does not corefer with a pre-
vious expression a, but depends for its interpre-
tation on a:
a. o recrutamento de pessoal cient?fico e t?cnico...
(the recruitment of scientific and technical em-
ployees)
d. as condi??es de acesso ? carreira cient?fica (the
conditions of employment for scientific jobs)
4. Discourse New: the interpretation of d does not
depend on any previous expression:
d. o livro n?o se debru?a sobre a actividade das
v?rias organiza??es internacionais... (the book ig-
nores the activity of various international organ-
isation...)
In (Schulte im Walde, 1997) acquired lexical
knowledge is used for solving bridging descrip-
tions, a broader class of anaphoric relations that
includes our class, indirect anaphora. (Poesio et
al., 2002) presents alternative techniques, based
on syntactic patterns, focusing on meronymy rela-
tions. Finally, (Bunescu, 2003) deals with another
class of anaphoric descriptions, which is also in-
cluded in the bridging class, called as associative
anaphora, following (Hawkins, 1978), where asso-
ciative anaphora is an anaphoric relation between
non-coreferent entities.
3 Lexical resource
Our lexical resource consists on lists of semantically
related words. These lists are constructed automati-
cally by a syntax-based knowledge-poor technique.
The technique used is described in (Gasperin et al,
2001; Gasperin, 2001), and it is an extension of the
technique presented in (Grefenstette, 1994).
Briefly, this technique consists on extracting spe-
cific syntactic contexts for every noun in the parsed
whole corpus and then applying a similarity mea-
sure (the weighted Jaccard measure) to compare the
nouns by the contexts they have in common (more
contexts they share, more similar they are). As syn-
tactic context, we understand any word that estab-
lishes a syntactic relation with a given noun in the
corpus. An example of one kind of syntactic context
considered is subject/verb, meaning that two nouns
that occur as subject of the same verb share this
context. Other examples of syntactic contexts are
verb/object, modifier/noun, etc. To each context it
is assigned a global and a local weight: the first re-
lated to the context frequency in the corpus, and the
second related to its frequency as a context of the
noun in focus. As output, we have a list of the most
similar nouns to each noun in the corpus, ordered
by the similarity value. We present the similarity
list for the noun acusa??o (accusation) in Table 1
as an example.
Table 1: Similarity list for the noun acusa??o
den?ncia (denunciation)
esc?ndalo (scandal)
crime (crime)
pedido (demand)
declara??o (declaration)
proposta (proposal)
not?cia (news)
acusa??o carta (letter)
(accusation) lista (list)
cargo (post)
ataque (attack)
arma (gun)
caso (case)
impress?o (impression)
reclama??o (complain)
The similarity lists can contain any kind of se-
mantic relation (e.g. synonymy, hyponymy, etc.)
between the words, but they are not classified. In
general, the similarity lists for the less frequent
words in the corpus contain some non-semantically
related words (noise), since the relations were based
on few syntactic contexts they shared along the cor-
pus.
The main advantage of this technique is the pos-
sibility of having a corpus-tunned lexical resource
built completely automatically. This resource re-
flects closely the semantic relations present in the
corpus used to create the lists. So, we believe the
similarity lists are more suitable for being used as
lexical knowledge for resolving the anaphoras than
a generic lexical base (e.g. Wordnet), since it focus
on the semantic relations between the terms that ap-
pear in the corpus, without considering extra mean-
ings that some words could have. New lists could be
generated from each corpus that one aims to resolve
the anaphoras.
To generate the similarity lists for Portuguese we
utilised a 1,400,000-words corpus from the Brazil-
ian newspaper ?Folha de S?o Paulo?, containing
news about different subjects (sports, economics,
computers, culture, etc.). This corpus includes the
set of texts that was hand-annotated with corefer-
ence information in previous work (Vieira et al,
2002; Salmon-Alt and Vieira, 2002). The corpus
was parsed by the Portuguese parser PALAVRAS
(Bick, 2000), provided by VISL project1.
We created two different sets of similarity lists:
one considering just nouns and the other consider-
ing nouns and proper names. So, the first set of lists
includes one list for each noun in the corpus and
each list is composed by other common nouns. The
second set of lists has one list for each noun and
proper name in the corpus, and each list is com-
posed by other nouns and proper names. The first
set contains 8019 lists and the second 12275, corre-
sponding to the different nouns (and proper names)
appearing in the corpus. Each similarity list con-
tains the 15 words that are more similar to the word
in focus, according to the calculated similarity val-
ues.
Having lexical information about the proper
names in the corpus is important, since we have
many coreference cases whose anaphor or an-
tecedent is a proper name. But when generating the
similarity lists, proper names bring noise (in gen-
eral they are less frequent then common nouns) and
the lists became more heterogeneous (includes more
non semantically related words).
4 Using similar words lists to solve
indirect anaphora
From the manual annotation and classification of
680 definite descriptions we selected those cases
classified as indirect anaphora (95). For each of
them there is a list of candidate antecedents. This
list is formed by all NPs that occur in the text. We
consider as candidates all the NPs that occur in the
text before the anaphor being mentioned.
Our heuristic for solving indirect anaphoras using
lists of similar words is the following. Consider:
? Hana is the head-noun of the anaphor
? Hcani is the head-noun of the antecedent can-
didate i
? Lana is the anaphor?s list of similar nouns
? Lcani is the list of similar nouns for the candi-
date i
? So,Hcani is considered the antecedent ofHana
if
(1)Hcani ? Lana
or
(2)Hana ? Lcani
1See http://visl.hum.sdu.dk/visl/pt/
or
(3)Lana 3 Hj ? Lcani
We call (1) ?right direction?, (2) ?opposite direc-
tion?, and (3) ?indirect way?.
We consider (1) > (2) > (3) when regarding the
reliability of the semantic relatedness betweenHana
and Hcani .
If the application of the heuristic resulted in more
than one possible antecedent, we adopted a weight-
ing scheme to choose only one among them. The
candidate with the lowest weight wins. For ranking
the possible antecedents, we considered two param-
eters:
? reliability: how the possible antecedent was se-
lect, according to (1), (2) or (3). A penalis-
ing value is added to its weight: 0, 40, 200,
respectively. The higher penalty for the ?indi-
rect way? is because we expected it could cause
many false positives;
? recency: we consider the distance in words be-
tween the anaphor and the possible antecedent.
The penalty values for the reliability parameter
were chosen in such a way they could be in the same
magnitude as the recency parameter values, that are
measured in words. For example, if candidate A is
250 words far from the anaphor and was selected
by (1) (getting weight=250) and a candidate B is 10
words far from the anaphor and was selected by (3)
(getting weight=210), candidate B will be selected
as the correct antecedent.
5 Our evaluation corpus
As result of previous work (Vieira et al, 2002;
Vieira et al, 2003), we have a Portuguese corpus
manually annotated with coreference information.
This corpus is considered our gold-standard to eval-
uate the performance of the heuristic presented in
the previous section. The study aimed to verify if
we could get a similar distribution of types of defi-
nite descriptions for Portuguese and English, which
would serve as an indication that the same heuristics
tested for English (Vieira et al, 2000) could apply
for Portuguese. The main annotation task in this
experiment was identifying antecedents and classi-
fying each definite description according to the four
classes presented in section 2.
For the annotation task, we adopted the MMAX
annotation tool (M?ller and Strube, 2001), that re-
quires all data to be encoded in XML format. The
corpus is encoded by <word> elements with sequen-
tial identifiers, and the output - the anaphors and
its antecedents - are enconded as <markable> ele-
ments, with the anaphor markable pointing to the
antecedent markable by a ?pointer? attribute.
The annotation process was split in 4 steps: se-
lecting coreferent terms; identifying the antecedent
of coreferent terms; classifying coreferent terms (di-
rect or indirect); classifying non-coreferent terms
(discourse new or other anaphora). About half of
the anaphoras were classified as discourse new de-
scriptions, which account for about 70% of non-
coreferent cases. Among the coreferent cases the
number of direct coreference is twice the number of
indirect coreference. This confirms previous work
done for English.
For the present work, we took then the 95 cases
classified as indirect coreference to serve as our
evaluation set. In 14 of this cases, the relation be-
tween anaphor and antecedent is synonymy, in 43
of the cases the relation is hyponymy, and in 38, the
antecedent or the anaphor are a proper name.
6 Implementing heuristics for indirect
anaphora in ART
Our heuristics were implemented as an XSL
stylesheet on the basis of the Anaphora Resolution
Tool (ART) (Vieira et al, 2003).
The tool integrates a set of heuristics correspond-
ing to one or more stylesheets to resolve different
sorts of anaphora. The heuristics may be applied in
a sequence defined by the user. As resolving direct
anaphoric descriptions (the ones where anaphor and
antecedent have the same head noun) is a much sim-
pler problem with high performance rates as shown
in previous results (Vieira et al, 2000; Bean and
Riloff, 1999), these heuristics should be applied
first in a system that resolves definite descriptions.
In this work, however, we decided to consider for
the experiments just the anaphoras that were pre-
viously annotated as indirect and check if the pro-
posed heuristic is able to find the correct antecedent.
ART allows the user to define the set of anaphors
to be resolved, in our case they are selected from
previously classified definite descriptions. The
stylesheet for indirect anaphora takes as input this
list of indirect anaphors, a list of the candidates and
the similarity lists. We consider all NPs in the text
as candidates, and for each anaphor we consider just
the candidates that appear before it in the text (we
are ignoring cataphora at moment).
All the input and output data is in XML for-
mat, based on the data format used by MMAX.
Our stylesheet for solving indirect anaphora takes
the <markable> elements with empty ?pointer? at-
tribute (coming unsolved from passing by the previ-
Table 2: Results considering just nouns
Description Numbers
Total indirect anaphors 57
Correctly
resolved
anaphors
Right direction 8
Opposite direction 5
Indirect way 6
TOTAL 19 (33.3%)
Unsolved anaphors 21
ously applied stylesheets/heuristics) and create and
intermediate file with <anaphor> elements to be re-
solved. The resolved <anaphor>s are again encoded
as <markable>s, with the ?pointer? filled. A de-
tailed description of our data encoding is presented
in (Gasperin et al, 2003).
7 Experiments
We run two experiments: one using the similarity
lists with proper names and another with the lists
containing just common nouns.
With these experiments we verify the values for
precision, recall and false positives on the task
of choosing an semantically similar antecedent for
each indirect anaphor. Our annotated corpus has 95
indirect anaphors with nominal antecedents, where
57 of them do not include proper names (as anaphor
or as antecedent). We use a non annotated version of
this corpus for the experiments. It contains around
6000 words, from 24 news texts of 6 different news-
paper sections.
Firstly, we reduced both sets of similarity lists to
contain just the list for the words present in this por-
tion of the corpus (660 lists without proper names
and 742 including proper names).
7.1 Experiment 1
Considering the 57 indirect anaphoras to be solved
(the ones that do not include any proper name), we
could solve 19 of them. It leads to a precision of
52.7% and a a recall of 33.3%. Table 2 shows the
result of our study considering the set of common
noun lists.
Most of the cases could be resolved by ?right di-
rection?, that represents the more intuitive way. 21
of the cases didn?t get any antecedent. We got 17
false positives, with different causes:
1. the right antecedent was not in the lists, there-
fore it could not be found but other wrong an-
tecedents were retrieved. For example, in meu
amigo Ives Gandra da Silva Martins escreveu
para esse jornal ... o conselheiro Ives (my
friend Ives_Gandra_da_Silva_Martins wrote
to this newspaper ... the councillor Ives), two
more candidates head-nouns are similar words
to ?conselheiro? (councillor): ?arquiteto? (ar-
chitect) and ?consultor? (consultant), but not
?amigo? (friend);
2. the right antecedent was in the lists but another
wrong antecedent was given higher weights,
because of proximity to the anaphora, as in the
example a rodovia Comandante Jo?o Ribeiro
de Barros ... pr?ximo a ponte ... ao ten-
tar atravessar a estrada (the highway Coman-
dante Joao Ribeiro de Barros ... near to the
bridge ... while trying to cross the road). Here,
the correct antecedent to ?a estrada? (the road)
is ?rodovia? (the highway) and it is present in
?estrada??s similarity list (right direction), but
also is ?ponte? (the bridge) and it is closer to
the anaphor in the text.
As expected, most of the false positives (11 cases)
were ?resolved? by ?indirect way?.
Considering all similar words found among the
candidates, not just the one with highest weight, we
could find the correct antecedent in 24 cases (42%).
The average number of similar words among the
candidates was 2.8, taking into account again the
positive and false positive cases. These numbers
report how much the similarity lists encode the se-
mantic relations present in the corpus. 64% of the
synonymy cases and 28% of the hyponymy cases
could be resolved. 35% of the hyponymy cases re-
sulted in false positives, the same happened with
just 14% of the synonymy cases.
7.2 Experiment 2
We replicated the previous experiment now using
the similarity lists that include proper names. Table
3 shows the results considering the set of lists for
nouns and proper names. Considering the 95 indi-
rect anaphoras to be solved, we could solve 21 of
them. It leads to a precision of 36.8% and a a recall
of 22.1%. There was no antecedent found for 38
anaphors, and 36 anaphors got wrong antecedents
(half of them by ?inderect way?). We observed the
same causes for false positives as the two presented
for experiment 1.
Considering all cases resolved (correct and false
ones), we could find the correct antecedent among
the similar words of the anaphor in 31 cases
(32.6%). The average number of similar words
among the candidates was 2.75. The numbers for
synonymy and hyponymy cases were the same as
in experiment 1 - 64% and 28% respectively. The
numbers for proper names were 50% of false posi-
tives and 50% of unresolved cases. It means none
Table 3: Results considering nouns and proper
names
Description Numbers
Total indirect anaphors 95
Correctly
resolved
anaphors
Right direction 13
Opposite direction 3
Indirect way 5
TOTAL 21 (22.1%)
Unsolved anaphors 38
of the cases that include proper names could be
resolved, but do not means they hadn?t any influ-
ence in other nouns similarity lists. In 26% of the
false positive cases, the correct antecedent (a proper
name) was in the anaphor similarity list (but was not
selected due to the weighting strategy).
The experiment with the similarity lists that in-
clude proper names was able to solve more cases,
but experiment 1 got better precision and recall val-
ues.
8 Related work
An evaluation of the use of WordNet for treating
bridging descriptions is presented in (Poesio et al,
1997). This evaluation considers 204 bridging de-
scriptions, distributed as follows, where NPj is the
anaphora and NPi is antecedent.
? synonymy relation between NPj and NPi: 12
cases;
? hypernymy relation between NPj and NPi: 14
cases;
? meronymy between NPj and NPi: 12;
? NPj related with NPi being a proper name: 49;
? NPj sharing a same noun in NPi other than
head (compound nouns): 25;
? NPj with antecedent being an event 40;
? NPj with antecedents being an implicit dis-
course topic: 15;
? other types of inferences holding between NPj
and antecedent: 37.
Due to the nature of the relations, only some of
them were expected to be found in WordNet. For
Synonymy, hypernymy and meronymy, 39% of the
38 cases could be solved on the basis of WordNet.
From this related work we can see the large variety
of cases one can found in a class such as bridging. In
our work we concentrated on coreference relations,
these can be related to synonymy, hypernymy, and
proper name sub-classes evaluated in (Poesio et al,
1997).
The technique presented in (Schulte im Walde,
1997) based on lexical acquisition from the British
National Corpus was evaluated against the same
cases in (Poesio et al, 1997). For synonymy, hy-
pernymy and meronymy, it was reported that 22%
of the 38 cases were resolved. In (Poesio et al,
2002) the inclusion of syntactic patterns improved
the resolution of meronymy in particular, result-
ing in 66% of the meronymy cases being resolved.
Bunescu (Bunescu, 2003) reports for his method on
resolving associative anaphora (anaphoric relation
between non-coreferent entities) a precision of 53%
when his recall is 22.7%.
9 Concluding remarks
We tested the use of word similarity lists on re-
solving indirect anaphoras on Portuguese newspa-
per texts. We presented our heuristic for searching
word similarity lists to be able to find the relation
between an anaphor and its antecedent. We con-
sidered similarity lists containing proper names and
lists containing just common nouns. Our heuris-
tic was able to resolve 33.3% of the cases, with
precision of 52.7% when considering just common
nouns, and we got 22.1%recall with precision of
36.8% when including proper names. Even though
considering proper names give us the possibility of
treating more anaphora cases, we got lower preci-
sion than using the lists with only nouns, since such
lists are more homogeneous. These results are com-
parable to previous work dealing with such complex
anaphora.
As future work, we intend to integrate our heuris-
tic for indirect anaphora with other heuristics for
anaphora resolution into ART and investigate the
best combination of application of these. Concern-
ing refining the proposed heuristic, we intend to
run more experiments aiming to tune the penalis-
ing weights when choosing an antecedent among
the candidates already selected by the search on the
similarity lists.
Acknowledgements
We would like to thank CNPq (Brazil) / INRIA
(France) for their financial support, and Susanne
Salmon-Alt, for her collaboration in this work.
References
D. Bean and E. Riloff. 1999. Corpus-based identi-
fication of non-anaphoric noun phrases. In Pro-
ceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-99).
Eckhard Bick. 2000. The Parsing System
PALAVRAS: Automatic Grammatical Analysis of
Portuguese in a Constraint Grammar Frame-
work. Ph.D. thesis, ?rhus University, ?rhus.
Razvan Bunescu. 2003. Associative anaphora res-
olution: A web-based approach. In Proceedings
of EACL 2003 - orkshop on The Computational
Treatment of Anaphora, Budapest.
Caroline Gasperin, Pablo Gamallo, Alexandre
Agustini, Gabriel Lopes, and Vera Lima. 2001.
Using syntactic contexts for measuring word sim-
ilarity. In Proceedings of the Workshop on Se-
mantic Knowledge Acquisition and Categorisa-
tion, Helsink, Finland.
Caroline Gasperin, Renata Vieira, Rodrigo Goulart,
and Paulo Quaresma. 2003. Extracting xml
syntactic chunks from portuguese corpora. In
Traitement automatique des langues minoritaires
- TALN 2003, Btaz-sur-mer, France.
Caroline Varaschin Gasperin. 2001. Extra??o au-
tom?tica de rela??es sem?nticas a partir de re-
la??es sint?ticas. Master?s thesis, PUCRS, Porto
Alegre.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic
Publishers, USA.
John A. Hawkins. 1978. Definiteness and Indef-
initeness. Humanities Press, Atlantic Highland,
NJ.
Christoph M?ller and Michael Strube. 2001.
MMAX: A tool for the annotation of multi-modal
corpora. In Proceedings of the IJCAI 2001, pages
45?50, Seattle.
Massimo Poesio and Renata Vieira. 1998. A
corpus-based investigation of definite description
use. Computational Linguistics, 24(2):183?216.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging descriptions in unre-
stricted texts. In Proceedings of the Practical,
Robust, Anaphora Resolution for Unrestricted
Texts, Workshop on Operational Factors, Madrid.
Massimo Poesio, Tomonori Ishikawa, Sabine
Schulte Im Walde, and Renata Vieira. 2002. Ac-
quiring lexical knowledge for anaphora resolu-
tion. In Proceedings of LREC 2002, Las Palmas
De Gran Canaria.
Susanne Salmon-Alt and Renata Vieira. 2002.
Nominal expressions in multilingual corpora:
Definites and demonstratives. In Proceedings of
the LREC 2002, Las Palmas de Gran Canaria.
Sabine Schulte im Walde. 1997. Resolving
Bridging Descriptions in High-Dimensional
Space. Master?s thesis, Institut f?r Maschinelle
Sprachverarbeitung, University of Stuttgart, and
Center for Cognitive Science, University of
Edinburgh.
K. van Deemter and R. Kibble. 2000. On corefer-
ring: Coreference in muc and related annotation
schemes. Computational Linguistics, 26(4).
Renata Vieira, Susanne Salmon-Alt, and Emmanuel
Schang. 2002. Multilingual corpora annotation
for processing definite descriptions. In Proceed-
ings of the PorTAL 2002, Faro.
Renata Vieira, Caroline Gasperin, and Rodrigo
Goulart. 2003. From manual to automatic anno-
tation of coreference. In Proceedings of the In-
ternational Symposium on Reference Resolution
and Its Applications to Question Answering and
Summarization, Venice.
Vieira et al 2000. Extra??o de sintagmas nominais
para o processamento de co-refer?ncia. In Anais
do V Encontro para o processamento computa-
cional da L?ngua Portuguesa escrita e falada -
PROPOR, Atibaia.
DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION
RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL
Massimo Poesio,? Olga Uryupina,? Renata Vieira,?
Mijail Alexandrov-Kabadjov? and Rodrigo Goulart?
?University of Essex, Computer Science and Cognitive Science (UK)
?Universita?t des Saarlandes, Computerlinguistik (Germany)
?Unisinos, Computac?a?o Aplicada (Brazil)
Abstract
Vieira and Poesio (2000) proposed an algorithm for
definite description (DD) resolution that incorpo-
rates a number of heuristics for detecting discourse-
new descriptions. The inclusion of such detec-
tors was motivated by the observation that more
than 50% of definite descriptions (DDs) in an av-
erage corpus are discourse new (Poesio and Vieira,
1998), but whereas the inclusion of detectors for
non-anaphoric pronouns in algorithms such as Lap-
pin and Leass? (1994) leads to clear improvements
in precision, the improvements in anaphoric DD res-
olution (as opposed to classification) brought about
by the detectors were rather small. In fact, Ng and
Cardie (2002a) challenged the motivation for the
inclusion of such detectors, reporting no improve-
ments, or even worse performance. We re-examine
the literature on the topic in detail, and propose a re-
vised algorithm, taking advantage of the improved
discourse-new detection techniques developed by
Uryupina (2003).
1 Introduction
Although many theories of definiteness and many
anaphora resolution algorithms are based on the as-
sumption that definite descriptions are anaphoric,
in fact in most corpora at least half of definite de-
scriptions are DISCOURSE-NEW (Prince, 1992), as
shown by the following examples, both of which are
the first sentences of texts from the Penn Treebank.
(1) a. Toni Johnson pulls a tape measure across
the front of what was once a stately Victorian
home.
b. The Federal Communications Commission
allowed American Telephone & Telegraph
Co. to continue offering discount phone
services for large-business customers
and said it would soon re-examine its
regulation of the long-distance market.
Vieira and Poesio (2000) proposed an algorithm for
definite description resolution that incorporates a
number of heuristics for detecting discourse-new
(henceforth: DN) descriptions. But whereas the
inclusion of detectors for non-anaphoric pronouns
(e.g., It in It?s raining) in algorithms such as Lappin
and Leass? (1994) leads to clear improvements in
precision, the improvements in anaphoric DD reso-
lution (as opposed to classification) brought about
by the detectors were rather small. In fact, Ng
and Cardie (2002a) challenged the motivation for
the inclusion of such detectors, reporting no im-
provements or even worse performance. We re-
examine the literature on the topic in detail, and
propose a revised algorithm, taking advantage of
the improved DN detection techniques developed by
Uryupina (2003).
2 Detecting Discourse-New Definite
Descriptions
2.1 Vieira and Poesio
Poesio and Vieira (1998) carried out corpus stud-
ies indicating that in corpora like the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993), around 52% of DDs are discourse-new
(Prince, 1992), and another 15% or so are bridg-
ing references, for a total of about 66-67% first-
mention. These results led Vieira and Poesio to
propose a definite description resolution algorithm
incorporating independent heuristic strategies for
recognizing DN definite descriptions (Vieira, 1998;
Vieira and Poesio, 2000).
The heuristics proposed by Vieira and Poesio
assumed a parsed input (the Penn Treebank) and
aimed at identifying five categories of DDs licensed
to occur as first mention on semantic or pragmatic
grounds on the basis of work on definiteness includ-
ing Loebner?s account (1987):
1. So-called SEMANTICALLY FUNCTIONAL de-
scriptions (Loebner, 1987). This class included
descriptions with modifiers like first or best
that turned a possibly sortal predicate into a
function (as in the first person to cross the Pa-
cific on a row boat); as well as descriptions
with predicates like fact or belief followed by a
that-clause with the function of specifying the
fact or belief under question. Both types of
definites descriptions were recognized by con-
sulting a hand-coded list of SPECIAL PREDI-
CATES.
2. Descriptions serving as disguised PROPER
NAMES, such as The Federal Communications
Commission or the Iran-Iraq war. The heuris-
tics for recognizing these definite descriptions
were primarily based on capitalization (of the
head or the modifiers).
3. PREDICATIVE descriptions, i.e., descriptions
semantically functioning as predicates rather
than as referring. These include descriptions
occurring in appositive position (as in Glenn
Cox, the president of Phillips Petroleum) and
in certain copular constructions (as in the man
most likely to gain custody of all this is a career
politician named Dinkins). The heuristics used
to recognize these cases examined the syntac-
tic structure of the NP and the clause in which
it appeared.
4. Descriptions ESTABLISHED (i.e., turned
into functions in context) by restric-
tive modification, particularly by es-
tablishing relative clauses (Loebner,
1987) and prepositional phrases, as in
The hotel where we stayed last night was
pretty good. These heuristics, as well,
examined the syntactic structure of the NP.
5. LARGER SITUATION definite descriptions
(Hawkins, 1978), i.e., definite descriptions like
the sun, the pope or the long distance mar-
ket which denote uniquely on the grounds of
shared knowledge about the situation (these are
Loebner?s ?situational functions?). Vieira and
Poesio?s system had a small list of such defi-
nites.
These heuristics were included as tests both of a de-
cision tree concerned only with the task of DN de-
tection, and of decision trees determining the classi-
fication of DDs as anaphoric, bridging or discourse
new. In both cases, the DN detection tests were in-
tertwined with attempts to identify an antecedent for
such DDs. Both hand-coded decision trees and auto-
matically acquired ones (trained using ID3, (Quin-
lan, 1986)) were used for the task of two-way clas-
sification into discourse-new and anaphoric. Vieira
and Poesio found only small differences in the order
of tests in the two decision trees, and small differ-
ences in performance. The hand-coded decision tree
executes in the following order:
1. Try the DN heuristics with the highest accu-
racy (recognition of some types of semanti-
cally functional DDs using special predicates,
and of potentially predicative DDs occurring in
appositions);
2. Otherwise, attempt to resolve the DD as direct
anaphora;
3. Otherwise, attempt the remaining DN heuris-
tics in the order: proper names, descrip-
tions established by relatives and PPs, proper
name modification, predicative DDs occurring
in copular constructions.
If none of these tests succeeds, the algorithm can ei-
ther leave the DD unclassified, or classify it as DN.
The automatically learned decision tree attempts di-
rect anaphora resolution first. The overall results on
the 195 DDs on which the automatically trained de-
cision tree was tested are shown in Table 1. The
baseline is the result achieved by classifying every
DD as discourse-new?with 99 discourse-new DDs
out of 195, this means a precision of 50.8%. Two
results are shown for the hand-coded decision tree:
in one version, the system doesn?t attempt to clas-
sify all DDs; in the other, all unclassified DDs are
classified as discourse-new.
Version of the System P R F
Baseline 50.8 100 67.4
Discourse-new detection only 69 72 70
Hand-coded DT: partial 62 85 71.7
Hand-coded DT: total 77 77 77
ID3 75 75 75
Table 1: Overall results by Vieira and Poesio
2.2 Bean and Riloff
Bean and Riloff (1999) developed a system for iden-
tifying discourse-new DDs1 that incorporates, in ad-
dition to syntax-based heuristics aimed at recogniz-
ing predicative and established DDs using postmod-
ification heuristics similar to those used by Vieira
and Poesio, additional techniques for mining from
corpora unfamiliar DDs including proper names,
larger situation, and semantically functional. Two
1Bean and Riloff use the term EXISTENTIAL for these DDs.
of the techniques proposed by Bean and Riloff are
particularly worth noticing. The SENTENCE-ONE
(S1) EXTRACTION heuristic identifies as discourse-
new every DD found in the first sentence of a text.
More general patterns can then be extracted from
the DDs initially found by S1-extraction, using the
EXISTENTIAL HEAD PATTERN method which, e.g.,
would extract the N+ Government from the
Salvadoran Government and the Guatemalan Gov-
ernment. The DEFINITE ONLY (DO) list contained
NPs like the National Guard or the FBI with a high
DEFINITE PROBABILITY, i.e., whose nominal com-
plex has been encountered at least 5 times with the
definite article, but never with the indefinite. VAC-
CINES were also developed that prevented the use
of patterns identified by S1-extraction or DO-list el-
ements when the definite probability of the definite
was too low. Overall, the algorithm proposed by
Bean and Riloff is as follows:
1. If the head noun of the DD appeared earlier in
the text, classify as anaphoric.
2. Otherwise, if the DD occurs in the S1 list, clas-
sify as discourse-new unless stopped by vac-
cine.
3. Otherwise, classify the DD as DN if one of the
following tests applies:
(a) it occurs in the DO list;
(b) it matches one of the EHP patterns, and is
not stopped by vaccine;
(c) it matches one of the syntactic heuristics
4. Otherwise, classify the DD as anaphoric.
(Note that as in the machine-learned version of the
Vieira and Poesio decision tree, a (simplified) direct
anaphora test is tried first, followed by DN detectors
in decreasing order of accuracy.)
Bean and Riloff trained their system on 1600 ar-
ticles from MUC-4, and tested it on 50 texts. The
S1 extraction methods produced 849 DDs; the DO
list contained 65 head nouns and 321 full NPs. The
overall results are shown in Table 2; the baseline
are the results obtained when classifying all DDs as
discourse-new.
Although the overall precision is not better than
what obtained with the partial hand-coded decision
tree used by Vieira and Poesio, recall is substantially
improved.
2.3 Ng and Cardie
Ng and Cardie (2002a) directly investigate the ques-
tion of whether employing a discourse-new pre-
diction component improves the performance of a
Method R P
Baseline 100 72.2
Syntactic Heuristics 43 93.1
Synt. Heuristics + S1 66.3 84.3
Synt. Heuristics + EHP 60.7 87.3
Synt. Heuristics + DO 69.2 83.9
Synt. Heuristics + S1 + EHP + DO 81.7 82.2
Synt. Heuristics + S1 + EHP + DO + V 79.1 84.5
Table 2: Discourse-new prediction results by Bean
and Riloff
coreference resolution system (specifically, the sys-
tem discussed in (Ng and Cardie, 2002b)). Ng and
Cardie?s work differs from the work discussed so far
in that their system attempts to deal with all types of
NPs, not just definite descriptions.
The discourse-new detectors proposed by Ng and
Cardie are statistical classifiers taking as input 37
features and trained using either C4.5 (Quinlan,
1993) or RIPPER (Cohen, 1995). The 37 features
of a candidate anaphoric expression specify, in ad-
dition to much of the information proposed in pre-
vious work, a few new types of information about
NPs.
? The four boolean so-called LEXICAL features
are actually string-level features: for exam-
ple, str_match is Y if a preceding NP
string-matches the anaphoric expression (ex-
cept for the determiner), and head_match =
Y if a preceding NP?s head string-matches the
anaphoric expression?s. embedded=Y if the
anaphoric expression is a prenominal modifier.
? The second group of 11 (mostly boolean) fea-
tures specifies the type of NP: e.g., pronoun
is Y if the anaphoric expression is a pronoun,
else N.
? The third group of 7 features specifies syn-
tactic properties of the anaphoric expression,
including number, whether NPj is the first of
two NPs in an appositive or predicative con-
struction, whether NPj is pre- or post-modified,
whether it contains a proper noun, and whether
it is modified by a superlative.
? The next group of 8 features are mostly novel,
and capture information not used by previ-
ous DN detectors about the exact composition
of definite descriptions: e.g., the_2n=Y if
the anaphoric expression starts with deter-
miner the followed by exactly two common
nouns, the_num_n=Y if the anaphoric ex-
pression starts with determiner the followed
by a cardinal and a common noun, and
the_sing_n=Y if the anaphoric expression
starts with determiner the followed by a singu-
lar NP not containing a proper noun.
? The next group of features consists of 4 fea-
tures capturing a variety of ?semantic? infor-
mation, including whether a previous NP is an
?alias? of NPj , or whether NPj is the title of a
person (the president).
? Finally, the last three features capture informa-
tion about the position in the text in which NPj
occurs: the header, the first sentence, or the
first paragraph.
Ng and Cardie?s discourse-new predictor was
trained and tested over the MUC-6 and MUC-7 coref-
erence data sets, achieving accuracies of 86.1% and
84%, respectively, against a baseline of 63.8% and
73.2%, respectively. Inspection of the top parts
of the decision tree produced with the MUC-6 sug-
gests that head_match is the most important fea-
ture, followed by the features specifying NP type,
the alias feature, and the features specifying the
structure of definite descriptions.
Ng and Cardie discuss two architectures for the
integration of a DN detector in a coreference sys-
tem. In the first architecture, the DN detector is
run first, and the coreference resolution algorithm
is run only if the DN detector classifies that NP as
anaphoric. In the second architecture, the system
first computes str_match and alias, and runs
the anaphoric resolver if any of them is Y; other-
wise, it proceeds as in the first architecture. The
results obtained on the MUC-6 data with the base-
line anaphoric resolver, the anaphoric resolver aug-
mented by a DN detector as in the first architecture,
and as in the second architecture (using C4.5), are
shown in Table 3. The results for all NPs, pronouns
only, proper names only, and common nouns only
are shown.2
As indicated in the Table, running the DN detector
first leads to worse results?this is because the detec-
tor misclassifies a number of anaphoric NPs as non-
anaphoric. However, looking first for a same-head
antecedent leads to a statistically significant im-
provement over the results of the baseline anaphoric
resolver. This confirms the finding both of Vieira
and Poesio and of Bean and Riloff that the direct
anaphora should be called very early.
2It?s not clear to us why the overall performance of the algo-
rithm is much better than the performance on the three individ-
ual types of anaphoric expressions considered?i.e., which other
anaphoric expressions are handled by the coreference resolver.
MUC-6 MUC-7
R P F R P F
Baseline (no DN detector) 70.3 58.3 63.8 65.5 58.2 61.6
Pronouns 17.9 66.3 28.2 10.2 62.1 17.6
Proper names 29.9 84.2 44.1 27.0 77.7 40.0
Common nouns 25.2 40.1 31.0 26.6 45.2 33.5
DN detector runs first 57.4 71.6 63.7 47.0 77.1 58.4
Pronouns 17.9 67.0 28.2 10.2 62.1 17.6
Proper names 26.6 89.2 41.0 21.5 84.8 34.3
Common nouns 15.4 56.2 24.2 13.8 77.5 23.4
Same head runs first 63.4 68.3 65.8 59.7 69.3 64.2
Pronouns 17.9 67.0 28.2 10.2 62.1 17.6
Proper names 27.4 88.5 41.9 26.1 84.7 40.0
Common nouns 20.5 53.1 29.6 21.7 59.0 31.7
Table 3: Evaluation of the three anaphoric resolvers
discussed by Ng and Cardie.
2.4 Uryupina
Uryupina (2003) trained two separate classifiers (us-
ing RIPPER, (Cohen, 1995)): a DN detector and a
UNIQUENESS DETECTOR, i.e., a classifier that de-
termines whether an NP refers to a unique object.
This is useful to identify proper names (like 1998,
or the United States of America), semantic definites
(like the chairman of Microsoft) and larger situation
definite descriptions (like the pope). Both classi-
fiers use the same set of 32 features. The features of
an NP encode, first, of all, string-level information:
e.g., whether the NP contains capitalized words, dig-
its, or special symbols. A second group of features
specifies syntactic information: whether the NP is
postmodified, and whether it contains an apposition.
Two types of appositions are distinguished, with and
without commas. CONTEXT features specify the
distance between the NP and the previous NP with
the same head, if any. Finally, Uryupina?s system
computes four features specifying the NP?s definite
probability. Unlike the definite probability used by
Bean and Riloff, these features are computed from
the Web, using Altavista. From each NP, its head H
and entire NP without determiner Y are determined,
and four ratios are then computed:
#?the Y?
#Y ,
#?the Y?
#??aY ?? ,
#?the H?
#H ,
#?the H?
#??aH?? .
The classifiers were tested on 20 texts from MUC-
7 (a subset of the second data set used by Ng and
Cardie), parsed by Charniak?s parser. 19 texts were
used for training and for tuning RIPPER?s parame-
ters, one for testing. The results for the discourse
new detection task are shown in Table 4, separat-
ing the results for all NPs and definite NPs only,
and the results without definite probabilities and in-
cluding them. The results for uniqueness detection
are shown in Table 4, in which the results obtained
by prioritizing precision and recall are shown sepa-
rately.
Features P R F
All NPs String+Syn+Context 87.9 86.0 86.9
All 88.5 84.3 86.3
Def NPs String+Syn+Context 82.5 79.3 80.8
All 84.8 82.3 83.5
Table 4: Results of Uryupina?s discourse new clas-
sifier
Features P R F
Best Prec String+Syn+Context 94.0 84.0 88.7
All 95.0 83.5 88.9
Best Rec String+Syn+Context 86.7 96.0 91.1
All 87.2 97.0 91.8
Table 5: Results of Uryupina?s uniqueness classifier
The first result to note is that both of Uryupina?s
classifiers work very well, particularly the unique-
ness classifier. These tables also show that the def-
inite probability helps somewhat the discourse new
detector, but is especially useful for the uniqueness
detector, as one would expect on the basis of Loeb-
ner?s discussion.
2.5 Summary
Quite a lot of consensus on many of the factors play-
ing a role in DN detection for DDs. Most of the al-
gorithms discussed above incorporate methods for:
? recognizing predicative DDs;
? recognizing discourse-new proper names;
? identifying functional DDs;
? recognizing DDs modified by establishing rel-
atives (which may or may not be discourse-
new).
There is also consensus on the fact that DN detection
cannot be isolated from anaphoric resolution (wit-
ness the Ng and Cardie results).
One problem with some of the machine learning
approaches to coreference is that these systems do
not achieve very good results on pronoun and defi-
nite description resolution in comparison with spe-
cialized algorithms: e.g., although Ng and Cardie?s
best version achieves F=65.8 on all anaphoric ex-
pressions, it only achieves F=29.6 for definite de-
scriptions (cfr. Vieira and Poesio?s best result of
F=77), and F=28.2 for pronouns (as opposed to re-
sults as high as F=80 obtained by the pronoun res-
olution algorithms evaluated in (Tetreault, 2001)).
Clearly these systems can only be properly com-
pared by evaluating them all on the same corpora
and the same data, and discussion such as (Mitkov,
2000) suggest caution in interpreting some of the
results discussed in the literature as pre- and post-
processing often plays a crucial role, but we feel that
evaluating DN detectors in conjunction with high-
performing systems would give a better idea of the
improvements that one may hope to achieve.
3 Do Discourse-New Detectors Help?
Preliminary Evaluations
Vieira and Poesio did not test their system with-
out DN-detection, but Ng and Cardie?s results indi-
cate that DN detection does improve results, if not
dramatically, provided that the same_head test is
run first?although their DN detector does not appear
to improve results for pronouns, the one category
for which detection of non-anaphoricity has been
shown to be essential (Lappin and Leass, 1994). In
order to evaluate how much improvement can we
expect by just improving the DN detector, we did
a few preliminary evaluations both with a reimple-
mentation of Vieira and Poesio?s algorithm which
does not include a discourse-new detector, running
over treebank text as the original algorithm, and
with a simple statistical coreference resolver at-
tempting to resolve all anaphoric expressions and
running over unparsed text, using Uryupina?s fea-
tures for discourse-new detection, and over the same
corpus used by Ng and Cardie (MUC-7).
3.1 How much does DN-detection help the
Vieira / Poesio algorithm?
GUITAR (Poesio and Alexandrov-Kabadjov, 2004)
is a general-purpose anaphoric resolver that in-
cludes an implementation of the Vieira / Poesio al-
gorithm for definite descriptions and of Mitkov?s al-
gorithm for pronoun resolution (Mitkov, 1998). It is
implemented in Java, takes its input in XML format
and returns as output its input augmented with the
anaphoric relations it has discovered. GUITAR has
been implemented in such a way as to be fully mod-
ular, making it possible, for example, to replace the
DD resolution method with alternative implementa-
tions. It includes a pre-processor incorporating a
chunker so that it can run over both hand-parsed and
raw text.
A version of GUITAR without the DN detection
aspects of the Vieira / Poesio algorithm was evalu-
ated on the GNOME corpus (Poesio, 2000; Poesio et
al., 2004), which contains 554 definite descriptions,
of which 180 anaphoric, and 305 third-person pro-
nouns, of which 217 anaphoric. The results for defi-
nite descriptions over hand-parsed text are shown in
Table 6.
Total Res Corr NM WM SM R P F
180 182 121 43 16 45 67.2 66.5 66.8
Table 6: Evaluation of the GUITAR system without
DN detection over a hand-annotated treebank
GUITAR without a DN recognizer takes 182 DDs
(Res) as anaphoric, resolving 121 of them cor-
rectly (Corr); of the 182 DDs it attempts to resolve,
only 16 are incorrectly resolved (WM); almost three
times that number (45) are Spurious Matches (SM),
i.e., discourse-new DDs incorrectly interpreted as
anaphoric. (Res=Corr+WM+SM.) The system can?t
find an antecedent for 43 of the 180 anaphoric DDs.
When endowed with a perfect DN detector, GUI-
TAR could achieve a precision P=88.3 which, as-
suming recall stays the same (R=67.2) would mean
a F=76.3.
Of course, these results are obtained assuming
perfect parsing. For a fairer comparison with the
results of Ng and Cardie, we report in Table 7 the
results for both pronouns and definite descriptions
obtained by running GUITAR off raw text.
R P F
Pronouns 65.5 63.0 64.2
DDs 56.7 56.1 56.4
Table 7: Evaluation of the GUITAR system without
DN detection off raw text
Notice that although these results are not partic-
ularly good, they are still better than the results re-
ported by Ng and Cardie for pronouns and definite
NPs.
3.2 How much might DN detection help a
simple statistical coreference resolver?
In order to have an even closer comparison with
the results of Ng and Cardie, we implemented a
simple statistical coreference system, that, like Ng
and Cardie?s system, would resolve all types of
anaphoric expressions, and would run over unparsed
text, but without DN detection. We ran the system
over the MUC-7 data used by Ng and Cardie, and
compared the results with those obtained by using
perfect knowledge about discourse novelty. The re-
sults are shown in Table 8.
R P F
Without DN detection 44.7 54.9 49.3
With DN detection 41.4 80.0 54.6
Table 8: Using an oracle
These results suggest that a DN detector could
lead to substantial improvements for coreference
resolution in general: DN detection might improve
precision by more than 30%, which more than
makes up for the slight deterioration in recall. Of
course, this test alone doesn?t tell us how much im-
provement DN detection would bring to a higher-
performance anaphoric resolver.
4 A New Set of Features for
Discourse-New Detection
Next, we developed a new set of features for dis-
course new detection that takes into account the
findings of the work on DN detection discussed in
the previous sections. This set of features will be
input to an anaphoric resolver for DDs working in
two steps. For each DD,
1. The direct anaphora resolution algorithm from
(Vieira and Poesio, 2000) is run, which at-
tempts to find an head-matching antecedent
within a given window and taking premodifica-
tion into account. The results of the algorithm
(i.e., whether an antecedent was found) is used
as one of the input features of the classifier in
the next step. In addition, a number of features
of the DD that may help recognizing the classes
of DDs discussed above are extracted from the
input. Some of these features are computed ac-
cessing the Web via the Google API.
2. A decision tree classifier is used to classify the
DD as anaphoric (in which case the antecedents
identified at the first step are also returned) or
discourse-new.
The features input to the classifier can be catego-
rized as follows:
Anaphora A single feature,
direct-anaphora, specifying the distance
of the (same-head) antecedent from the DD, if
any (values: none, zero, one, more)
Predicative NPs Two boolean features:
? apposition, if the DD occurs in appos-
itive position;
? copular, if the DD occurs in post-verbal
position in a copular construction.
Proper Names Three boolean features:
? c-head: whether the head is capitalized;
? c-premod: whether one of the premod-
ifiers is capitalized;
? S1: whether the DD occurs in the first sen-
tence of a Web page.
Functionality The four definite probabilities used
by Uryupina (computed accessing the Web),
plus a superlative feature specifying if
one of the premodifiers is a superlative, ex-
tracted from the part of speech tags.
Establishing relative A single feature, specifying
whether NP is postmodified, and by a relative
clause or a prepositional phrase;
Text Position Whether the DD occurs in the title,
the first sentence, or the first paragraph.
We are testing several classifiers in-
cluded in the Weka 3.4 library
(http://www.cs.waikato.ac.nz/?ml/)
including an implementation of C4.5 and a
multi-layer perceptron.
5 Evaluation
Data We are using three corpora for the evalua-
tion, including texts from different genres, in which
all anaphoric relations between (all types of) NPs are
marked. The GNOME corpus includes pharmaceuti-
cal leaflets and museum ?labels? (i.e., descriptions
of museum objects and of the artists that realized
them). As said above, the corpus contains 554 def-
inite descriptions. In addition, we are using the 14
texts from the Penn Treebank included in the cor-
pus used by Vieira and Poesio. We transferred these
texts to XML format, and added anaphoric informa-
tion for all types of NPs according to the GNOME
scheme. Finally, we are testing the system on the
MUC-7 data used by Ng and Cardie
Methods We will compare three versions of the
DD resolution component:
1. The baseline algorithm without DN detection
incorporated in GUITAR described above (i.e.,
only the direct anaphora resolution part of
(Vieira and Poesio, 2000));
2. A complete implementation of the Vieira and
Poesio algorithm, including also the DN detect-
ing heuristics;
3. An algorithm using the statistical classifier dis-
cussed above.
Results Regrettably, the system is still being
tested. We will report the results at the workshop.
6 Discussion and Conclusions
Discussions and conclusions will be based on the
final results.
Acknowledgments
Mijail Alexandrov-Kabadjov is supported by Cona-
cyt. Renata Vieira and Rodrigo Goulart are partially
supported by CNPq.
References
D. L. Bean and E. Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
Proc. of the 37th ACL, pages 373?380, University
of Maryland. ACL.
W. Cohen. 1995. Fast effective rule induction. In
Proc. of ICML.
J. A. Hawkins. 1978. Definiteness and Indefinite-
ness. Croom Helm, London.
S. Lappin and H. J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):535?562.
S. Loebner. 1987. Definites. Journal of Semantics,
4:279?326.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of english: the Penn Treebank.
Computational Linguistics, 19(2):313?330.
R. Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proc. of the 18th COL-
ING, pages 869?875, Montreal.
R. Mitkov. 2000. Towards more comprehensive
evaluation in anaphora resolution. In Proc. of
the 2nd International Conference on Language
Resources and Evaluation, pages 1309?1314,
Athens, May.
V. Ng and C. Cardie. 2002a. Identifying anaphoric
and non-anaphoric noun phrases to improve
coreference resolution. In Proc. of 19th COL-
ING.
V. Ng and C. Cardie. 2002b. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th Meeting of the ACL.
M. Poesio and M. Alexandrov-Kabadjov. 2004. A
general-purpose, off the shelf anaphoric resolver.
In Proc. of LREC, Lisbon, May.
M. Poesio and R. Vieira. 1998. A corpus-based in-
vestigation of definite description use. Compu-
tational Linguistics, 24(2):183?216, June. Also
available as Research Paper CCS-RP-71, Centre
for Cognitive Science, University of Edinburgh.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M.
Hitzeman. 2004. Centering: A parametric theory
and its instantiations. Computational Linguistics.
To appear.
M. Poesio. 2000. Annotating a corpus to develop
and evaluate discourse entity realization algo-
rithms: issues and preliminary results. In Proc.
of the 2nd LREC, pages 211?218, Athens, May.
E. F. Prince. 1992. The ZPG letter: subjects, defi-
niteness, and information status. In S. Thompson
and W. Mann, editors, Discourse description: di-
verse analyses of a fund-raising text, pages 295?
325. John Benjamins.
J. R. Quinlan. 1986. Induction of decision trees.
Machine Learning, 1(1):81?106.
J. R. Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann, San Mateo, CA.
J. R. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics, 27(4):507?520.
O. Uryupina. 2003. High-precision identification
of discourse-new and unique noun phrases. In
Proc. of the ACL 2003 Student Workshop, pages
80?86.
R. Vieira and M. Poesio. 2000. An empirically-
based system for processing definite descriptions.
Computational Linguistics, 26(4), December.
R. Vieira. 1998. Definite Description Resolution
in Unrestricted Texts. Ph.D. thesis, University of
Edinburgh, Centre for Cognitive Science, Febru-
ary.
Mining Linguistically Interpreted Texts  
Cassiana Fagundes da Silva, Renata Vieira, 
Fernando Santos Os?rio 
PIPCA - Unisinos  
Av. Unisinos, 950  - S?o Leopoldo, RS 
Brasil ? 93.022-000  
{cassiana, renata, osorio}@exatas.unisinos.br 
Paulo Quaresma 
Departamento de Inform?tica, 
Universidade de ?vora, 7000  
?vora - Portugal 
 {pq}@di.uevora.pt 
 
Abstract 
This paper proposes and evaluates the use of 
linguistic information in the pre-processing 
phase of text mining tasks. We present several 
experiments comparing our proposal for 
selection of terms based on linguistic 
knowledge with usual techniques applied in 
the field. The results show that part of speech 
information is useful for the pre-processing 
phase of text categorization and clustering, as 
an alternative for stop words and stemming.  
1 Introduction 
Natural language texts can be viewed as 
resources containing uniform data in such a way 
that methods similar to those used in Data Base 
Knowledge Extraction can be applied to them. The 
adaptation of these methods to texts is known as 
Text Mining (Tan, 1999). Machine learning 
techniques are applied to document collections 
aiming at extracting patterns that may be useful to 
organize or recover information from the 
collections. Tasks related to this area are text 
categorization, clustering, summarization, and 
information extraction.  One of the first steps in 
text mining tasks is the pre-processing of the 
documents, as they need to be represented in a 
more structured way.  
Our work proposes a new technique to the pre-
processing phase of documents and we compare it 
with usual pre-processing methods. We focus on 
two text mining tasks, namely text categorization 
and clustering. In the categorization task we 
associate each document to a class from a pre-
defined set, in the clustering task the challenge is 
to identify groups of similar documents without 
being aware of pre-defined classes. Usually, the 
pre-processing phase in these tasks are based on 
the approach called bag-of-words, in which just 
simple techniques are used to eliminate 
uninteresting words and to reduce various 
semantically related terms to the same root (stop-
words and stemming, respectively). As an 
alternative, we propose the use of linguistic 
information in the pre-processing phase, by 
selecting words according to their category (nouns, 
adjectives, proper names, verbs) and using its 
canonical form. We ran a series of experiments to 
evaluate this proposal over Brazilian Portuguese 
texts.  
This paper is organized as follows. Section 2 
presents an overview of text mining. Section 3 
presents the methods used for collecting the 
linguistic knowledge used in the experiments. The 
experiments themselves are described in Section 4. 
Section 5 presents an analysis of the results and the 
paper is concluded in Section 6. 
 
2 Text Mining 
Text mining processes are usually divided in five 
major phases: A) Document collection: consists of 
the definition of the set of the documents from 
which knowledge must be extracted. B) Pre-
processing: consists of a set of actions that 
transform the set of documents in natural language 
into a list of useful terms. C) Preparation and 
selection of the data: consists in the identification 
and selection of relevant terms form the pre-
processed ones. D) Knowledge Extraction: consists 
of the application of machine learning techniques 
to identify patterns that can classify or cluster the 
documents in the collection. E) Evaluation and 
interpretation of the results: consists of the 
analysis of the results.  
The pre-processing phase in text mining is 
essential and usually very expensive and time 
consuming. As texts are originally non-structured a 
series of steps are required to represent them in a 
format compatible with knowledge extraction 
methods and tools. The usual techniques employed 
in phase B are the use of a list of stop-words, 
which are discarded from the original documents 
and the use of stemming which reduces the words 
to their root.  
Having the proper tools to process Portuguese 
texts, we investigate whether linguistic information 
can have an impact on the results of the whole 
process. In the next section we describe the tools 
we used for acquiring the linguistic knowledge in 
which we base our experiments. 
3 Tools for acquiring linguistic knowledge 
The linguistic knowledge we use in the 
experiments is based on the syntactic analysis 
performed by the PALAVRAS parser (Bick, 
2000). This Portuguese parser is robust enough to 
always give an output even for incomplete or 
incorrect sentences (which might be the case for 
the type of documents used in text mining tasks). It 
has a comparatively low percentage of errors (less 
than 1% for word class and 3-4% for surface 
syntax) (Bick, 2003). We also used another tool 
that makes easier the extraction of features from 
the analyzed texts: the Palavras Xtractor (Gasperin 
et. al. 2003).  This tool converts the parser output 
into three XML files, containing: a) the list of all 
words from the text and their identifier; b) morpho-
syntactic information for each word; c) the 
sentence?s syntactic structures. Using XSL 
(eXtensible Stylesheet Language)1 we can extract 
specified terms from the texts, according to their 
linguistic value. The resulting lists of terms 
according to each combination are then passed to 
phases C, D and E. The experiments are described 
in detail in the next section. 
4 Experiments 
4.1 Corpus 
The corpus used in the experiments is composed 
by a subset of the NILC corpus (N?cleo 
Interdisciplinar de Ling??stica Computacional2) 
containing 855 documents corresponding to 
newspaper articles of Folha de S?o Paulo from 
1994. These documents are related to five 
newspaper sections: informatics, property, sports, 
politics and tourism.  
4.2 Pre-processing techniques 
We prepared three different versions of the 
corpus (V1, V2 and V3) for 3-fold cross validation. 
Each version is partitioned in different training and 
testing parts, containing 2/3 and 1/3 of the 
documents respectively. 
For the experiments with the usual methods, 
irrelevant terms (stop-words) were eliminated from 
the documents, on the basis of a list of stop-words, 
containing 476 terms (mainly articles, prepositions, 
auxiliary verbs, pronouns, etc). The remaining 
terms were stemmed according to Martin Porter?s 
algorithm (Porter, 1980). Based on these 
                                                     
1
 Available in http://www.w3.org/Style/XSL/ 
2
 Available in http://nilc.icmc.sc.usp.br/nilc/   
techniques we generated a collection of pre-
processed documents called PD1. 
To test our proposal we then pre-processed the 
855 documents in a different way: we parsed all 
texts of our corpus, generating the corresponding 
XML files and extracted terms according to their 
grammatical categories, using XSL. Based on these 
techniques we generated a collection of pre-
processed documents called PD2. 
4.2.1 Other mining phases 
All other text mining phases were equally 
applied to both PD1 and PD2. We used relative 
frequency for the selection of relevant terms. The 
representation of the documents was according to 
the vector space model. For the categorization task, 
vectors corresponding to each class were built, 
where the more frequent terms were selected. After 
that, a global vector was composed. We also tested 
with different numbers of terms in the global 
vector (30, 60, 90, 120, 150). For the clustering 
task we measured the similarity of the documents 
using cosine. After calculating similarity of the 
documents, the data was codified according to 
format required by the machine learning tool Weka 
(Witten, 2000). Weka is a collection of machine 
learning algorithms for data mining tasks that 
contains tools for data pre-processing, 
classification, regression, clustering, association 
rules, and visualization.  
In this work the adopted machine learning 
techniques are Decision Tree for the categorization 
process and K-means for text clustering.  
Decision Tree is a supervised learning algorithm 
based on the recursive division of the training 
examples in representative subsets, using the 
metric of information gain. After the induction of a 
classifying tree, it can be applied to new examples, 
described with the same attributes of the training 
examples. 
K-means divides a group of objects in k groups 
in a way that the resulting intracluster similarity is 
high, but the intercluster similarity is low. The 
similarity of groups is measured in respect to the 
medium value of the objects in a group, which can 
be seen as the center of gravity (centroid) of the 
group. The parameters used to run k-means are the 
default ones as suggested by the tool, seed 10 and 
5 groups.  
The evaluation of the results for the 
categorization task is based on the classification 
error, which was used to compare the results for 
PD1 and PD2. For the clustering task the 
evaluation of the results is given by recall and 
precision, based on the generated confusion 
matrices. 
5 Results 
5.1 Text Categorization 
Table 1 shows the results for text categorization 
of PD1, given by the average error rates 
considering the three versions the corpus (V1, V2 
and V3). We had around 20% of error for the 
categorization task. We can see minor variations in 
the results according to the size of the vectors. Best 
results were obtained for 150 terms.  
 
Terms 30 60 90 120 150 
Errors 21,64 21,99 20,47 20,35 19,77 
Table 1: Average Classification Error for PD1% 
Table 2 shows the results for different 
grammatical combinations in PD2, while Figure 1 
summarizes the lowest error rates found for PD1 
and all groups of PD2. The group nouns and 
adjectives presents the lower error rates of all 
experiments (18,01). However, due to the small 
size of the corpus, the improvement reported 
between usual methods (18,01) and nouns-
adjectives (20,47), when considering the same 
number of terms (90), are at 75-80% confidence 
level only (t-test). 
In general, the results show that the presence of 
nouns is crucial, the worst classification errors are 
based on groups that do not contain the category 
nouns, and here the confidence level for the 
differences reported reaches 95%. The groups 
containing nouns present results comparable to 
those found in the experiments based on usual 
methods of pre-processing. The use of verbs, either 
alone or with other grammatical groups is not an 
interesting option. 
 
Terms 30 60 90 120 150 
Nouns 24,91 21,75 23,98 23,51 22,69 
Nouns-adjec. 23,15 20,35 18,01 19,18 18,71 
Nouns-adjec.-
proper names 20,82 22,92 20,94 21,05 21,17 
Nouns-proper 
names 
24,09 24,56 22,80 22,45 22,80 
Adjec.-proper 
names 
47,01 46,34 32,51 33,21 32,86 
Verbs 63,73 62,33 57,75 58,45 55,64 
Nouns-verbs 40 27,72 25,61 24,21 26,32 
Nouns-verbs-
adjectives 35,09 27,02 27,72 24,21 23,51 
Table 2: Average Classification Error for PD2  
It can be observed that usually the best results 
are obtained when the documents are represented 
by a larger number of terms (90, 120 and 150), for 
the group nouns, however, the best results were 
obtained for vectors containing just 60 terms.  
Figure 1: Lower error rates for PD1 and PD2 
We looked at the terms resulting from different 
selection methods and categories to check the 
overlap among the groups. From PD1 to PD2 
based on nouns and adjectives (the one with the 
best results) we could see that we had around 50% 
of different terms. That means that 50% of terms in 
PD1 are terms included in the categories nouns and 
adjectives and 50% of the terms selected on the 
basis of stop-words and stemming are from other 
grammatical categories. As adjectives added to 
nouns improved the results, we checked adjectives 
to figure out their significance. We found terms 
such as Brazilian, electoral, multimedia, political. 
Intuitively, these terms seem to be relevant for the 
classes we had. Analysing the groups containing 
verbs, we observed that the verbs are usually very 
common or auxiliary verbs (such as to be, to have, 
to say), therefore not relevant for classification. 
5.2 Text Clustering 
We tested our hypothesis through clustering 
experiments for PD1 and variations of PD2. For 
the experiments on clustering we used vectors 
containing 150 features from V2 and we set k to 5 
groups. The resulting confusion matrix for PD1 is 
presented in Table 3.  
 
 Cl.0 Cl.1 Cl.2 Cl.3 Cl.4 
Sp. 1  31 2 0 23 
Prop. 2 0 4 0 51 
Inf. 0 0 1 0 55 
Pol. 0 0 2 39 16 
Tour. 5 0 17 0 33 
Table 3: Confusion Matrix PD1 (150 terms) 
Considering the larger group in each row and 
column (highlighted in the table) as the intended 
cluster for each class, the   corresponding precision 
is of 50,52%. 
We repeated the same set of experiments for 
PD2. We tested several grammatical groups, the 
best result was related to nouns and proper names. 
The results are shown in Tables 4. The 
corresponding precision is 63,15%. 
 
 Cl.0 Cl.1 Cl.2 Cl.3  Cl.4 
Sp. 0 38 19 0 0 
Prop. 11 0 44 1 1 
Inf. 0 0 19 0 38 
Pol. 0 1 20 36 0 
Tour. 0 0 57 0 0 
Table 4:  Confusion Matrix PD2 (nouns + proper 
names, 150 terms) 
6 Conclusions 
This paper presented a series of experiments 
aiming at comparing our proposal of pre-
processing techniques based on linguistic 
information with usual methods adopted for pre-
processing in text mining.  
We find in the literature other alternative 
proposals for the pre-processing phase of text 
mining. (Gon?alves and Quaresma, 2003) use the 
canonical form of the word instead stemming, for 
European Portuguese. (Feldman et al 1998) 
proposes the use of compound terms as opposed to 
single terms for text mining. Similarly, (Aizawa, 
2001) uses morphological analysis to aid the 
extraction of compound terms. Our approach 
differs from those since we propose single terms 
selection based on different part of speech 
information. 
The results show that a selection made solely on 
the basis of category information produces results 
at least as good as those produced by usual 
methods (when the selection considers nouns and 
adjectives or nouns and proper nouns) both in 
categorization and clustering tasks. In the 
categorization experiments we obtained the lowest 
error rate for PD2 when the pre-processing phase 
was based on the selection of nouns and adjectives, 
18,01%. However, the second best score in the 
case of categorization was achieved by the 
traditional methods, 19,77%. Due to the small 
corpus, further experiments are needed to verify 
the statistical significance of  the reported gains. 
The results of the clustering experiments show a 
difference in precision from  50,52% to 63,15%. 
As we are planning to test our techniques with a 
larger number of documents and consequently a 
larger number of terms, we are considering 
applying other machine-learning techniques such 
as Support Vector Machines that are robust enough 
to deal with a large number of terms. We are also 
planning to apply more sophisticated linguistic 
knowledge than just grammatical categories, as, for 
instance, the use of noun phrases for terms 
selection, since this information is provided by the 
parser PALAVRAS. Other front for future work is 
further tests for other languages. 
References  
Aizawa A., 2001. Linguistic Techniques to 
Improve the Performance of Automatic Text 
Categorization. Proc. of the Sixth Natural 
Language Processing Pacific Rim Symposium, 
pages 307-314. 
Bick, E. 2000. The Parsing System PALAVRAS: 
Automatic Gramatical Analysis of Porutugese in 
a Constraint Grammar Framework. ?rhus 
University. ?rhus: ?rhus University Press. 
Bick, E. 2003. A Constraint Grammar Based 
Question Answering System for Portuguese. 
Proceedings of the 11? Portuguese Conference 
on Artificial Intelligence, pages 414-418. LNAI 
Springer Verlag. 
Feldman R., et al 1998. Text Mining at the Term 
Level. Proc. of the Second European Symposium 
on Principles of Data Mining and Knowledge 
Discovery, pages 65-73. LNCS Springer. 
Gasperin, C.; Vieira, R.; Goulart, R. and 
Quaresma, P. 2003. Extracting XML Syntactic 
Chunks from Portuguese Corpora. Proc. of the 
TALN Workshop on Natural Language 
Processing of Minority Languages and Small 
Languages, pages 223-232. Batz-sur-Mer  
France. 
Gon?alves, T. and Quaresma, P. 2003. A prelimary 
approach classification problem of Portuguese 
juridical documents. Proceedings of the 11? 
Portuguese Conference on Artificial Intelligence, 
pages 435-444. LNAI Springer Verlag. 
Porter, M. F. 1980. An Algorithm for Suffix 
Stripping. Program, 14 no. 3, pages 130-137. 
Tan, Ah-Hwee. 1999. Text mining: the state of the 
art and the challenges. Proc. of the Pacific-Asia  
Workshop on Knowledge Discovery from 
Advanced Databases, pages 65-70, Beijing. 
Witten, I. H. 2000. Data mining: Pratical Machine 
Learning tools and techniques with Java 
implementations. Academic Press. 
 
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 76?79,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic tagging for resolution of indirect anaphora
R. Vieira1, E. Bick2, J. Coelho1, V. Muller1, S. Collovini1, J. Souza1, L. Rino3
UNISINOS1, University of Denmark2, UFSCAR3
renatav@unisinos.br, eckhard.bick@mail.dk, lucia@dc.ufscar.br
Abstract
This paper presents an evaluation of indi-
rect anaphor resolution which considers as
lexical resource the semantic tagging pro-
vided by the PALAVRAS parser. We de-
scribe the semantic tagging process and a
corpus experiment.
1 Introduction
Bridging anaphora represents a special part of the
general problem of anaphor resolution. As a spe-
cial case of anaphora, it has been studied and dis-
cussed by different authors and for various lan-
guages. There are many problems in develop-
ing such studies. First, bridging is not a regu-
lar class, it seldom contains cases of associative
and indirect anaphora (defined in the sequence);
lexical resources such as Wordnet are not avail-
able for every language, and even when available
such resources have proven to be insufficient for
the problem. In fact, different sources of lexi-
cal knowledge have been evaluated for anaphora
resolution (Poesio et al, 2002; Markert and Nis-
sim, 2005; Bunescu, 2003). At last, corpus stud-
ies of bridging anaphora usually report results
on a reduced number of examples, because this
kind of data is scarce. Usually bridging anaphora
considers two types: Associative anaphors are
NPs that have an antecedent that is necessary
to their interpretation (the relation between the
anaphor and its antecedent is different from iden-
tity); and Indirect anaphor are those that have
an identity relation with their antecedents but the
anaphor and its antecedent have different head-
nouns. In both associative and indirect anaphora,
the semantic relation holding between the anaphor
and its antecedent play an essential role for res-
olution. However, here we present an evalu-
ation of the semantic tagging provided by the
Portuguese parser PALAVRAS (Bick, 2000)
(http://visl.sdu.dk/visl/pt/parsing/automatic) as a
lexical resource for indirect anaphora resolution.
We focus on indirect anaphors for two reasons,
they are greater in number and they present better
agreement features concerning human annotation.
2 Semantic Annotation with Prototype
Tags
As a Constraint Grammar system, PALAVRAS
encodes all annotational information as word
based tags. A distinction is made between mor-
phological, syntactic, valency and semantic tags,
and for a given rule module (or level of analysis),
one tag type will be regarded as primary (= flagged
for disambiguation), while tags from lower lev-
els provide unambiguous context, and tags from
higher levels ambiguous lexical potentialities.
Thus, semantic tags are regarded as secondary
help tags at the syntactic level, but will have un-
dergone some disambiguation at the anaphora res-
olution level. The semantic noun classes were
conceived as distinctors rather than semantic de-
finitions, the goal being on the one hand to cap-
ture semantically motivated regularities and rela-
tions in syntax, on the other hand to allow to dis-
tinguish between different senses, or to chose dif-
ferent translation equivalents in MT applications.
A limited set of semantic prototype classes was
deamed ideal for both purposes, since it allows at
the same time similarity-based lumping of words
(useful in structural analysis, IR, anaphora reso-
lution) and context based polysemy resolution for
an individual word (useful in MT, lexicography,
alignment). Though we define class hypernyms
as prototypes in the Roschian sense (Rosch, 1978)
76
as an (idealized) best instance of a given class of
entities, we avoided low level prototypes, using
<Azo> for four-legged land-animals rather than
<dog> and <cat> for dog and cat races etc.).
Where possible, systematic sub-classes were es-
tablished. Semiotic artifacts <sem>, for instance
are sub-divided into ?readables? <sem-r> (book-
prototype: book, paper, magazine), ?watchables?
<sem-w> (film, show, spectacle), ?listenables?
etc. The final category inventory, though devel-
oped independently, resembles the ontology used
in the multilingual European SIMPLE project
(http://www.ub.es/ gilcub/SIMPLE/simple.html).
For the sake of rule based inheritance reasoning,
semantic prototype classes were bundled using a
matrix of 16 atomic semantic features. Thus,
the atomic feature +MOVE is shared by the dif-
ferent human and animal prototypes as well as
the vehicle prototype, but the vehicle prototype
lacks the +ANIM feature, and only the bun-
dle on human prototypes (<Hprof>, <Hfam>,
<Hideo>,...) shares the +HUM feature (human
professional, human family, human follower of a
theory/belief/conviction/ideology). In the parser,
a rule selecting the +MOVE feature (e.g. for sub-
jects of movement verbs) will help discard com-
peting senses from lemmas with the above proto-
types, since they will all inherit choices based on
the shared atomic feature. Furthermore, atomic
features can themselves be subjected to inheri-
tance rules, e.g. +HUM ?> +ANIM ?> +CON-
CRETE, or +MOVE?> +MOVABLE. In Table 1,
which contains examples of polysemic institution
nouns, positive features are marked with capital
letters, negative features with small letters1. The
words in the Table 1 are ambiguous with regard
to the feature H, and since it is only the <inst>
prototype that contributes the +HUM feature po-
tential, it can be singled out by a rule selecting
?H? or by discarding ?h?. The parser?s about 140
prototypes have been manually implemented for a
lexicon of about 35.000 nouns. In addition, the
?HUM category was also introduced as a selec-
tion restriction for 2.000 verb senses (subject re-
striction) and 1.300 adjective senses (head restric-
tion).
While the semantic annotation of common
nouns is carried out by disambiguating a given
lemma?s lexicon-listed prototype potential, this
strategy is not sufficient for proper nouns, due
1furn=furniture, con=container, inst=institution
Ee = entities (?CONCRETE)
Jj = ?MOVABLE
Hh = ?HUMAN ENTITY
Mm = ?MAS
Ll = ?LOCATION
polysemy spectrum
Ee j Hh m Ll faculdade
E H L <inst> univ. faculty
e h l <f-c> property
Ee j Hh m Ll fundo
e h L <Labs> bottom
E H L <inst> foundation
e h l <ac> <smP> funds
Ee j Hh Mm Ll indu?stria
E H m L <inst> industry
e h M l <am> diligence
E Jj Hh m L rede
J h <con> net
j H <inst> <+n> network
J h <furn> hammock
Table 1: Feature bundles in prototype based poly-
semy
to the productive nature of this word class. In
two recent NER projects, the parser was aug-
mented with a pattern recognition module and a
rule-based module for identifying and classify-
ing names. In the first project (Bick, 2003),
6 main classes with about 15 subclasses were
used in a lexeme-based approach, while the
second adopted the 41 largely functional cate-
gories of Linguateca?s joint HAREM evaluation
in 2005 (http://www.linguateca.com). A lexicon-
registered name like Berlin would have a stable
tag (<civ> = civitas) in the first version, while
it would be tagged as either <hum>, <top> or
<org> in the second, dependent on context. At
the time of writing, we have not yet tagged our
anaphora corpus with name type tags, and it is
unclear which approach, lexematic or functional,
will work best for the resolution of indirect and
associative anaphora.
3 Indirect Anaphora Resolution
Our work was based on a corpus formed by 31
newspaper articles, from Folha de Sa?o Paulo, writ-
ten in Brazilian Portuguese. The corpus was au-
tomatically parsed using the parser PALAVRAS,
and manually annotated for anaphoricity using
the MMAX tool(http://mmax.eml-research.de/) .
Four subjects annotated the corpus. All annota-
tors agreed on the antecedent in 73% of the cases,
in other 22% of the cases there was agreement be-
tween three annotators and in 5% of the cases only
two annotators agreed. There were 133 cases of
77
definite Indirect anaphors (NPs starting with def-
inite articles) from the total of 1454 definite de-
scriptions (near to 10%) and 2267 NPs.
The parser gives to each noun of the text (or to
most of them) a semantic tag. For instance, the
noun japone?s [japanese] has the following seman-
tic tags ling and Hnat, representing the features:
human nationality and language respectively.
<word id="word_28">
<n can="japone?s" gender="M" number="S">
<secondary_n tag="Hnat"/>
<secondary_n tag="ling"/>
</n>
</word>
The approach consists in finding relationships
with previous nouns through the semantic tags.
The chosen antecedent will be the nearest expres-
sion with the largest number of equal semantic
tags. For instance, in the example below, the
anaphor is resolved by applying this resolution
principle, to japone?s - a l??ngua.
O Eurocenter oferece cursos de japone?s em Kanazawa.
Apo?s um me?s, o aluno falara? modestamente a l??ngua.
The Eurocenter offers Japanese courses in Kanazawa. Af-
ter one month, a student can modestly speak the language.
As both expressions (japanese and language)
hold the semantic tag ?ling? the anaphor is re-
solved. For the experiments, we considered as cor-
rect the cases where the antecedent found automat-
ically was the same as in the manual annotation
(same), and also the cases in which the antecedent
of the manual annotation was found further up in
the chain identified automatically (in-chain). We
also counted those cases in which the antecedent
of the manual annotation was among the group of
candidates sharing the same tags (in-candidates),
but was not the chosen one (the chosen being the
nearest with greater number of equal tags).
Indirect anaphora
Results # % of Total
Same 25 19%
In-chain 15 11%
Total Correct 40 30%
In-candidates 9 7%
Unsolved 40 30%
Error 44 33%
Total 133 100%
Table 2: Indirect anaphor resolution
Table 2 shows the results of the indirect anaphor
resolution. In 19% of the cases, the system found
the same antecedent as marked in the manual an-
notation. Considering the chain identified by the
system the correct cases go up to 30%. The great
number of unsolved cases were related to the fact
that proper names were not tagged. Considering
mainly the tagged nouns (about 93 cases), the cor-
rect cases amount to 43%). This gives us an idea
of the quality of the tags for the task. We further
tested if increasing the weight of more specific
features in opposition to the more general ones
would help in the antecedent decision process. A
semantic tag that is more specific receives a higher
weight The semantic tag set has three levels, level
1, which is more general receives weight 1, level 2
receives 5, and level 3 receives 10. See the exam-
ple below.
<A> 1 Animal, umbrella tag
<AA> 5 Group of animals
<Adom> 10 Domestic animal
In this experiment the chosen candidate is the
nearest one whose sum of equal tag values has
higher weight. Table 3 shows just a small im-
provement in the correct cases. If we do not
consider unsolved cases, mostly related to proper
names, indirect anaphors were correctly identified
in 46% of the cases (43/96).
Indirect anaphora
Results # % of Total
Same 24 18%
In-chain 19 14%
Total Correct 43 32%
In-candidates 6 5%
Unsolved 40 30%
Error 44 33%
Total 133 100%
Table 3: Indirect anaphor - weighting schema
Since there is no semantic tagging for proper
names as yet, the relationship between pairs such
as Sa?o Carlos - a cidade [Sa?o Carlos - the city]
could not be found. Regarding wrong antecedents,
we have seen that some semantic relationships are
weaker, having no semantic tags in common, for
instance: a proposta - o aumento [the proposal -
the rise]. In some cases the antecedent is not a
previous noun phrase but a whole sentence, para-
graph or disjoint parts of the text. As we con-
sider only relations holding between noun phrases,
these cases could not be resolved. Finally, there
are cases of plain heuristic failure. For instance,
establishing a relationship between os professores
78
[the teachers], with the semantic tags H and Hprof,
and os politicos [the politicians], with the seman-
tic tags H and Hprof, when the correct antecedent
was os docentes [the docents], with the semantic
tags HH (group of humans) and Hprof.
4 Final Remarks
Previous work on nominal anaphor resolution has
used lexical knowledge in different ways. (Poe-
sio et al, 1997) presented results concerning the
resolution of bridging definitions, using the Word-
Net (Fellbaum, 1998), where bridging DDs en-
close our Indirect and Associative anaphora. Poe-
sio et al reported 35% of recall for synonymy,
56% for hypernymy and 38% for meronymy.
(Schulte im Walde, 1997) evaluated the bridg-
ing cases presented in (Poesio et al, 1997), on
the basis of lexical acquisition from the British
National Corpus. She reported a recall of 33%
for synonymy, 15% for hypernymy and 18% for
meronymy. (Poesio et al, 2002) considering syn-
tactic patterns for lexical knowledge acquisition,
obtained better results for resolving meronymy
(66% of recall). (Gasperin and Vieira, 2004)
tested the use of word similarity lists on resolv-
ing indirect anaphora, reporting 33% of recall.
(Markert and Nissim, 2005) presented two ways
(WordNet and Web) of obtaining lexical knowl-
edge for antecedent selection in coreferent DDs
(Direct and Indirect anaphora). Markert and
Nissim achieved 71% of recall using Web-based
method and 65% of recall using WordNet-based
method. We can say that our results are very sat-
isfactory, considering the related work. Note that
usually evaluation of bridging anaphora is made
on the basis of a limited number of cases, because
the data is sparse. Our study was based on 133
examples, which is not much but surpasses some
of the previous related work. Mainly, our results
indicate that the semantic tagging provided by the
parser is a good resource for dealing with the prob-
lem, if compared to other lexical resources such as
WordNet and acquired similarity lists. We believe
that the results will improve significantly once se-
mantic tags for proper names are provided by the
parser. This evaluation is planned as future work.
Acknowledgments
This work was partially funded by CNPq.
References
Eckhard Bick. 2000. The Parsing System PALAVRAS:
Automatic Grammatical Analysis of Protuguese in
a Constraint Grammar Framework. Ph.D. thesis,
Arhus University, Arhus.
Eckhard Bick. 2003. Multi-level ner for portuguese in
a cg framework. In Nuno J. et al Mamede, editor,
Computational Processing of the Portuguese Lan-
guage (Procedings of the 6th International Work-
shop, PROPOR 2003), number 2721 in Lecture
Notes in Computer Science, pages 118?125, Faro,
Portugal. Springer.
Razvan Bunescu. 2003. Associative anaphora reso-
lution: A web-based approach. In Proceedings of
the Workshop on The Computational Treatment of
Anaphora - EACL 2003, Budapest.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Caroline Gasperin and Renata Vieira. 2004. Us-
ing word similarity lists for resolving indirect
anaphora. In Proceedings of ACL Workshop on Ref-
erence Resolution and its Applications, pages 40?
46, Barcelona.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?401.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging descriptions in un-
restricted texts. In Proceedings of the Work-
shop on Operational Factors In Practical, Robust,
Anaphora Resolution for Unrestricted Texts, pages
1?6, Madrid.
Masimo Poesio, Ishikawa Tomonori, Sabine Shulte im
Walde, and Renata Vieira. 2002. Acquiring lexical
knowledge for anaphora resolution. In Proceedings
of 3rd Language resources and evaluation confer-
ence LREC 2002, Las Palmas.
Eleanor Rosch. 1978. Principles of categorization.
In E. Rosch and B. Lloyd, editors, Cognition and
Categorization, pages 27?48. Hillsdale, New Jersey:
Lawrence Erlbaum Associate.
Sabine Schulte im Walde. 1997. Resolving Bridging
Descriptions in High-Dimensional Space Resolving
Bridging Descriptions in High-Dimensional Space.
Ph.D. thesis, Institut fu?r Maschinelle Sprachverar-
beitung, Universita?t Stuttgart, and Center for Cogni-
tive Science, University of Edinburgh, Edinburgh.
79
Proceedings of the First Workshop on Multilingual Modeling, pages 25?31,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparable Corpus Based on Aligned Multilingual Ontologies
Roger Granada
PUCRS (Brazil)
roger.granada@acad.pucrs.br
Lucelene Lopes
PUCRS (Brazil)
lucelene.lopes@pucrs.br
Carlos Ramisch
University of Grenoble (France)
ceramisch@inf.ufrgs.br
Cassia Trojahn
University of Grenoble (France)
cassia.trojahn@inria.fr
Renata Vieira
PUCRS (Brazil)
renata.vieira@pucrs.br
Aline Villavicencio
UFRGS (Brazil)
alinev@gmail.com
Abstract
In this paper we present a methodology for
building comparable corpus, using multilin-
gual ontologies of a scpecific domain. This
resource can be exploited to foster research on
multilingual corpus-based ontology learning,
population and matching. The building re-
source process is exemplified by the construc-
tion of annotated comparable corpora in En-
glish, Portuguese, and French. The corpora,
from the conference organization domain, are
built using the multilingual ontology concept
labels as seeds for crawling relevant docu-
ments from the web through a search engine.
Using ontologies allows a better coverage of
the domain. The main goal of this paper is
to describe the design methodology followed
by the creation of the corpora. We present a
preliminary evaluation and discuss their char-
acteristics and potential applications.
1 Introduction
Ontological resources provide a symbolic model of
the concepts of a scientific, technical or general
domain (e.g. Chemistry, automotive industry, aca-
demic conferences), and of how these concepts are
related to one another. However, ontology creation
is labour intensive and error prone, and its mainte-
nance is crucial for ensuring the accuracy and util-
ity of a given resource. In multilingual contexts, it
is hard to keep the coherence among ontologies de-
scribed in different languages and to align them ac-
curately. These difficulties motivate the use of semi-
automatic approaches for cross-lingual ontology en-
richment and population, along with intensive reuse
and interoperability between ontologies. For that, it
is crucial to have domain-specific corpora available,
or the means of automatically gathering them.
Therefore, this paper describes an ontology-based
approach for the generation of multilingual compa-
rable corpora. We use a set of multilingual domain-
dependent ontologies, which cover different aspects
of the conference domain. These ontologies provide
the seeds for building the domain specific corpora
from the web. Using high-level background knowl-
edge expressed in concepts and relations, which are
represented as natural language descriptions in the
labels of the ontologies, allow focused web crawl-
ing with a semantic and contextual coverage of the
domain. This approach makes web crawling more
precise, which is crucial when exploiting the web as
a huge corpus.
Our motivation is the need of such resources
in tasks related to semi-automatic ontology cre-
ation and maintenance in multilingual domains.
We exemplify our methodology focusing on the
construction of three corpora, one in English,
one in Portuguese, and one in French. This
effort is done in the context of a larger re-
search project which aims at investigating meth-
ods for the construction of lexical resources, in-
tegrating multilingual lexica and ontologies, fo-
cusing on collaborative and automatic techniques
(http://cameleon.imag.fr/xwiki/bin/view/Main/).
In the next section, we present some relevant re-
lated work (?2). This is followed by a description
of the methodology used to build the corpora (?3).
Finally, the application example expressed by the
resulting corpora are evaluated (?4) and discussed
25
(?5). We conclude by outlining their future applica-
tions (? 6).
2 Related Work
Web as corpus (WAC) approaches have been suc-
cessfully adopted in many cases where data sparse-
ness plays a major limiting role, either in specific
linguistic constructions and words in a language
(e.g. compounds and multiword expressions), or for
less resourced languages in general1.
For instance, Grefenstette (1999) uses WAC for
machine translation of compounds from French into
English, Keller et al (2002) for adjective-noun,
noun-noun and verb-object bigram discovery, and
Kim and Nakov (2011) for compound interpretation.
Although a corpus derived from the web may con-
tain noise, the sheer size of data available should
compensate for that. Baroni and Ueyama (2006)
discuss in details the process of corpus construc-
tion from web pages for both generic and domain-
specific corpora. In particular, they focus on the
cleaning process applied to filter the crawled web
pages. Much of the methodology applied in our
work is similar to their proposed approach (see ?3).
Moreover, when access to parallel corpora is lim-
ited, comparable corpora can minimize data sparse-
ness, as discussed by Skadina et al (2010). They
create bilingual comparable corpora for a variety of
languages, including under-resourced ones, with 1
million words per language. This is used as ba-
sis for the definition of metrics for comparability of
texts. Forsyth and Sharoff (2011) compile compa-
rable corpora for terminological lexicon construc-
tion. An initial verification of monolingual compa-
rability is done by partitioning the crawled collec-
tion into groups. Those are further extended through
the identification of representative archetypal texts
to be used as seeds for finding documents of the
same type.
Comparable corpora is a very active research sub-
ject, being in the core of several European projects
(e.g. TTC2, Accurat3). Nonetheless, to date most of
1Kilgarriff (2007) warns about the dangers of statistics heav-
ily based on a search engine. However, since we use the down-
loaded texts of web pages instead of search engine count esti-
mators, this does not affect the results obtained in this work.
2www.ttc-project.eu
3www.accurat-project.eu
the research on comparable corpora seems to focus
on lexicographic tasks (Forsyth and Sharoff, 2011;
Sharoff, 2006), bilingual lexicon extraction (Morin
and Prochasson, 2011), and more generally on ma-
chine translation and related applications (Ion et al,
2011). Likewise, there is much to be gained from
the potential mutual benefits of comparable corpora
and ontology-related tasks.
Regarding multilingually aligned ontologies, very
few data sets have been made available for use in
the research community. Examples include the vlcr4
and the mldirectory5 datasets. The former con-
tains a reduced set of alignments between the the-
saurus of the Netherlands Institute for Sound and
Vision and two other resources, English WordNet
and DBpedia. The latter consists of a set of align-
ments between web site directories in English and
in Japanese. However, these data sets provide sub-
sets of bilingual alignments and are not fully pub-
licly available. The MultiFarm dataset6, a multilin-
gual version of the OntoFarm dataset (S?va?b et al,
2005), has been designed in order to overcome the
lack of multilingual aligned ontologies. MultiFarm
is composed of a set of seven ontologies that cover
the different aspects of the domain of organizing sci-
entific conferences. We have used this dataset as the
basis for generating our corpora.
3 Methodology
The main contribution of this paper is the proposal
of the methodology to build corpora. This sec-
tion describes the proposed methodology present-
ing our own corpus crawler, but also its application
to construct three corpora, in English, Portuguese,
and French. These corpora are constructed from the
MultiFarm dataset.
3.1 Tools and Resources
Instead of using an off-the-shelf web corpus tool
such as BootCaT (Baroni and Bernardini, 2004), we
implemented our own corpus crawler. This allowed
us to have more control on query and corpus con-
struction process. Even though our corpus construc-
4www.cs.vu.nl/?laurah/oaei/2009
5oaei.ontologymatching.org/2008/
mldirectory
6web.informatik.uni-mannheim.de/
multifarm
26
tion strategy is similar to the one implemented in
BootCaT, there are some significant practical issues
to take into account, such as:
? The predominance of multiword keywords;
? The use of the fixed keyword conference;
? The expert tuning of the cleaning process;
? The use of a long term support search AP[b].
Besides, BootCaT uses the Bing search API,
which will no longer work in 2012. As our work
is part of a long-term project, we preferred to use
Google?s search API as part of the University Re-
search Program.
The set of seed domain concepts comes from
the MultiFarm dataset. Seven ontologies from the
OntoFarm project (Table 1), together with the align-
ments between them, have been translated from En-
glish into eight languages (Chinese, Czech, Dutch,
French, German, Portuguese, Russian, and Span-
ish). As shown in Table 1, the ontologies differ
in numbers of classes, properties, and in their log-
ical expressivity. Overall, the ontologies have a high
variance with respect to structure and size and they
were based upon three types of resources:
? actual conferences and their web pages (type
?web?),
? actual software tools for conference organisa-
tion support (type ?tool?), and
? experience of people with personal participa-
tion in organisation of actual conferences (type
?insider?).
Currently, our comparable corpus generation ap-
proach focuses on a subset of languages, namely En-
glish (en), Portuguese (pt) and French (fr). The
labels of the ontology concepts, like conference and
call for papers, are used to generate queries and re-
trieve the pages in our corpus. In the current imple-
mentation, the structure and relational properties of
the ontologies were ignored. Concept labels were
our choice of seed keywords since we intended to
have comparable, heterogeneous and multilingual
domain resources. This means that we need a corpus
and an ontology referring to the same set of terms or
concepts. We want to ensure that the concept labels
Name Type C DP OP
Ekaw insider 74 0 33
Sofsem insider 60 18 46
Sigkdd web 49 11 17
Iasted web 140 3 38
ConfTool tool 38 23 13
Cmt tool 36 10 49
Edas tool 104 20 30
Table 1: Ontologies from the OntoFarm dataset in terms
of number of classes (C), datatype properties (DP) and
object properties (OP).
are present in the corresponding natural language,
textual sources. This combination of resources is es-
sential for our goals, which involve problems such as
ontology learning and enriching from corpus. Thus,
the original ontology can serve as a reference for
automatically extracted resources. Moreover, we
intend to use the corpus as an additional resource
for ontology (multilingual) matching, and again the
presence of the labels in the corpus is of great rele-
vance.
3.2 Crawling and Preprocessing
In each language, a concept label that occurs in
two or more ontologies provides a seed keyword
for query construction. This results in 49 en key-
words, 54 pt keywords and 43 fr keywords. Be-
cause many of our keywords are formed by more
than one word (average length of keywords is re-
spectively 1.42, 1.81 and 1.91 words), we combine
three keywords regardless of their sizes to form a
query. The first keyword is static, and corresponds
to the word conference in each language. The query
set is thus formed by permuting keywords two by
two and concatenating the static keyword to them
(e.g. conference reviewer program committee). This
results in 1 ? 48 ? 47 = 2, 256 en queries, 2,756
pt queries and 1,892 fr queries. Average query
length is 3.83 words for en, 4.62 words for pt and
4.91 words for fr. This methodology is in line with
the work of Sharoff (2006), who suggests to build
queries by combining 4 keywords and downloading
the top 10 URLs returned for each query.
The top 10 results returned by Google?s search
27
API7 are downloaded and cleaned. Duplicate URLs
are automatically removed. We did not filter out
URLs coming from social networks or Wikipedia
pages because they are not frequent in the corpus.
Results in formats other than html pages (like .doc
and .pdf documents) are ignored. The first clean-
ing step is the extraction of raw text from the html
pages. In some cases, the page must be discarded for
containing malformed html which our page cleaner
is not able to parse. In the future, we intend to im-
prove the robustness of the HTML parser.
3.3 Filtering and Linguistic Annotation
After being downloaded and converted to raw text,
each page undergoes a two-step processing. In the
first step, markup characters as interpunctuation,
quotation marks, etc. are removed leaving only let-
ters, numbers and punctuation. Further heuristics
are applied to remove very short sentences (less than
3 words), email addresses, URLs and dates, since
the main purpose of the corpus is related to concept,
instance and relations extraction. Finally, heuristics
to filter out page menus and footnotes are included,
leaving only the text of the body of the page. The
raw version of the text still contains those expres-
sions in case they are needed for other purposes.
In the second step, the text undergoes linguistic
annotation, where sentences are automatically lem-
matized, POS tagged and parsed. Three well-known
parsers were employed: Stanford parser (Klein and
Manning, 2003) for texts in English, PALAVRAS
(Bick, 2000) for texts in Portuguese, and Berkeley
parser (Petrov et al, 2006) for texts in French.
4 Evaluation
The characteristics of the resulting corpora are sum-
marized in tables 2 and 3. Column D of table 2
shows that the number of documents retrieved is
much higher in en than in pt and fr, and this is
not proportional to the number of queries (Q). In-
deed, if we look in table 3 at the average ratio of
documents retrieved per query (D/Q), the en queries
return much more documents than queries in other
languages. This indicates that the search engine re-
turns more distinct results in en and more duplicate
URLs in fr and in pt. The high discrepancy in
7research.google.com/university/search
Q D W token W type
en 2,256 10,127 15,852,650 459,501
pt 2,756 5,342 12,876,344 405,623
fr 1,892 5,154 9,482,156 362,548
Table 2: Raw corpus dimensions: number of queries (Q),
documents (D), and words (W).
D/Q S/D W/S TTR
en 4.49 110.59 14.15 2.90%
pt 1.94 120.08 20.07 3.15%
fr 2.72 115.63 15.91 3.82%
Table 3: Raw corpus statistics: average documents per
query (D/Q), sentences per document (S/D), words per
sentence (W/S) and type-token ration (TTR).
the number of documents has a direct impact in the
size of the corpus in each language. However, this
is counterbalanced by the average longer documents
(S/D) and longer sentences (W/S) in pt and fr with
respect to en. The raw corpus contains from 9.48
million words in fr, 12.88 million words in pt to
15.85 million words in en, constituting a large re-
source for research on ontology-related tasks.
A preliminary semi-automated analysis of the cor-
pus quality was made by extracting the top-100 most
frequent n-grams and unigrams for each language.
Using the parsed corpora, the extraction of the top-
100 most frequent n-grams for each language fo-
cused on the most frequent noun phrases composed
by at least two words. The lists with the top-100
most frequent unigrams was generated by extract-
ing the most frequent nouns contained in the parsed
corpus for each language. Four annotators manually
judged the semantic adherence of these lists to the
conference domain.
We are aware that semantic adherence is a vague
notion, and not a straightforward binary classifica-
tion problem. However, such a vague notion was
considered useful at this point of the research, which
is ongoing work, to give us an initial indication
of the quality of the resulting corpus. Examples
of what we consider adherent terms are appel a?
communication (call for papers), conference pro-
gram and texto completo (complete text), examples
28
# of adherent terms
Lower Upper
en words 46 85
en n-grams 57 94
fr words 21 69
fr n-grams 24 45
pt words 32 70
pt n-grams 11 45
Table 4: Number of words and n-grams judged as seman-
tically adherent to the domain.
of nonadherent terms extracted from the corpus were
produits chimiques (chemical products), following
case, projeto de lei (law project). In the three lan-
guages, the annotation of terms included misparsed
and mistagged words (ad hoc), places and dates typ-
ical of the genre (but not necessarily of the domain),
general-purpose terms frequent in conference web-
sites (email, website) and person names.
Table 4 shows the results of the annotation. The
lower bound considers an n-gram as semantically
adherent if all the judges agree on it. The upper
bound, on the other hand, considers as relevant n-
grams all those for which at least one of the four
judges rated it as relevant. As a result of our anal-
ysis, we found indications that the English corpus
was more adherent, followed by French and Por-
tuguese. This can be explained by the fact that
the amount of internet content is larger for English,
and that the number of international conferences
is higher than national conferences adopting Por-
tuguese and French as their official languages. We
considered the adherence of Portuguese and French
corpora rather low. There are indications that mate-
rial related to political meetings, law and public in-
stitutions was also retrieved on the basis of the seed
terms.
The next step in our evaluation is verifying its
comparable nature, by counting the proportion of
translatable words. Thus, we will use existing bilin-
gual dictionaries and measure the rank correlation of
equivalent words in each language pair.
5 Discussion
The first version of the corpus containing the to-
tality of the raw pages, the tools used to process
them, and a sample of 1,000 annotated texts for
each language are freely available for download at
the CAMELEON project website8. For the raw
files, each page is represented by an URL, a lan-
guage code, a title, a snippet and the text of the
page segmented into paragraphs, as in the original
HTML file. A companion log file contains informa-
tion about the download dates and queries used to re-
trieve each URL. The processed files contain the fil-
tered and parsed texts. The annotation format varies
for each language according to the parser used. The
final version of this resource will be available with
the totality of pages parsed.
Since the texts were extracted from web pages,
there is room for improvement concerning some im-
portant issues in effective corpus cleaning. Some of
these issues were dealt with as described in the ? 3,
but other issues are still open and are good candi-
dates for future refinements. Examples already fore-
seen are the removal of foreign words, special char-
acters, and usual web page expressions like ?site un-
der construction?, ?follow us on twitter?, and ?click
here to download?. However, the relevance of some
of these issues depends on the target application. For
some domains, foreign expressions may be genuine
part of the vocabulary (e.g. parking or weekend in
colloquial French and deadline in Portuguese), and
as such, should be kept, while for other domains
these expressions may need to be removed, since
they do not really belong to the domain. Therefore,
the decision of whether to implement these filters
or not, and to deal with truly multilingual texts, de-
pends on the target application.
Another aspect that was not taken into account in
this preliminary version is related to the use of the
relations between concepts in the ontologies to guide
the construction of the queries. Exploiting the con-
textual and semantic information expressed in these
relations may have an impact in the set of retrieved
documents and will be exploited in future versions
of the corpus.
6 Conclusions and Future Work
This paper has described an ontology-based ap-
proach for the generation of a multilingual compara-
8cameleon.imag.fr/xwiki/bin/view/Main/
Resources
29
ble corpus in English, Portuguese and French. The
corpus constructed and discussed here is an impor-
tant resource for ontology learning research, freely
available to the research community. The work on
term extraction that we are doing for the initial as-
sessment of the corpus is indeed the initial step to-
wards more ambitious research goals such as multi-
lingual ontology learning and matching in the con-
text of our long-term research project.
The initial ontologies (originally built by hand)
and resulting corpora can serve as a reference, a re-
search resource, for information extraction tasks re-
lated to ontology learning (term extraction, concept
formation, instantiation, etc). The resource also al-
lows the investigation of ontology enriching tech-
niques, due to dynamic and open-ended nature of
language, by which relevant terms found in the cor-
pus may not be part of the original ontology. We can
also assess the frequencies (relevance) of the labels
of the ontology element with respect to the corpus,
thus assessing the quality of the ontology itself. An-
other research that can be developed on the basis of
our resource is to evaluate the usefulness of a corpus
in the improvement of existing multilingual ontol-
ogy matching techniques9.
Regarding to our own crawler implementation,
we plan to work on its evaluation by using other
web crawlers, as BootCaT, and compare both ap-
proaches, specially on what concerns the use of on-
tologies.
From the point of view of NLP, several techniques
can be compared showing the impact of adopting
different tools in terms of depth of analysis, from
POS tagging to parsing. This is also an important re-
source for comparable corpora research, which can
be exploited for other tasks such as natural language
translation and ontology-based translation. So far
this corpus contains English, Portuguese and French
versions, but the ontology data set includes 8 lan-
guages, to which this corpus may be extended in the
future.
9An advantage of this resource is that the Multilingual Onto-
Farm is to be included in the OAEI (Ontology Alignment Eval-
uation Initiative) evaluation campaign.
References
Marco Baroni and Silvia Bernardini. 2004. BootcaT:
Bootstrapping corpora and terms from the web. In
Proc. of the Fourth LREC (LREC 2004), Lisbon, Por-
tugal, May. ELRA.
Marco Baroni and Motoko Ueyama. 2006. Building
general- and special-purpose corpora by web crawling.
In Proceedings of the 13th NIJL International Sympo-
sium on Language Corpora: Their Compilation and
Application, pages 31?40.
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press.
Richard Forsyth and Serge Sharoff. 2011. From crawled
collections to comparable corpora: an approach based
on automatic archetype identification. In Proc. of Cor-
pus Linguistics Conference, Birmingham, UK.
Gregory Grefenstette. 1999. The World Wide Web as a
resource for example-based machine translation tasks.
In Proc. of the Twenty-First Translating and the Com-
puter, London, UK, Nov. ASLIB.
Radu Ion, Alexandru Ceaus?u, and Elena Irimia. 2011.
An expectation maximization algorithm for textual
unit alignment. In Zweigenbaum et al (Zweigenbaum
et al, 2011), pages 128?135.
Frank Keller, Maria Lapata, and Olga Ourioupina. 2002.
Using the Web to overcome data sparseness. In Jan
Hajic? and Yuji Matsumoto, editors, Proc. of the 2002
EMNLP (EMNLP 2002), pages 230?237, Philadel-
phia, PA, USA, Jul. ACL.
Adam Kilgarriff. 2007. Googleology is bad science.
Comp. Ling., 33(1):147?151.
Su Nam Kim and Preslav Nakov. 2011. Large-scale noun
compound interpretation using bootstrapping and the
web as a corpus. In Proc. of the 2011 EMNLP
(EMNLP 2011), pages 648?658, Edinburgh, Scotland,
UK, Jul. ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of the 41st ACL (ACL
2003), pages 423?430, Sapporo, Japan, Jul. ACL.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Zweigenbaum et al
(Zweigenbaum et al, 2011), pages 27?34.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COLING
and 44th ACL (COLING/ACL 2006), pages 433?440,
Sidney, Australia, Jul. ACL.
Serge Sharoff, 2006. Creating general-purpose cor-
pora using automated search engine queries. Gedit,
Bologna, Italy.
30
Inguna Skadina, Ahmed Aker, Voula Giouli, Dan Tufis?,
Robert Gaizauskas, Madara Mieirina, and Nikos Mas-
tropavlos. 2010. A Collection of Comparable Cor-
pora for Under-resourced Languages. In Inguna Skad-
ina and Andrejs Vasiljevs, editors, Frontiers in Artifi-
cial Intelligence and Applications, volume 219, pages
161?168, Riga, Latvia, Oct. IOS Press.
Ondr?ej S?va?b, Vojte?ch Sva?tek, Petr Berka, Dus?an Rak,
and Petr Toma?s?ek. 2005. Ontofarm: Towards an ex-
perimental collection of parallel ontologies. In Poster
Track of ISWC 2005.
Pierre Zweigenbaum, Reinhard Rapp, and Serge Sharoff,
editors. 2011. Proc.of the 4th Workshop on Building
and Using Comparable Corpora: Comparable Cor-
pora and the Web (BUCC 2011), Portland, OR, USA,
Jun. ACL.
31
