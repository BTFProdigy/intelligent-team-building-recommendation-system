NAACL HLT Demonstration Program, pages 31?32,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Voice-Rate: A Dialog System for Consumer Ratings 
 
Geoffrey Zweig, Y.C. Ju, Patrick Nguyen, Dong Yu, 
Ye-Yi Wang and Alex Acero 
Speech Research Group 
Microsoft Corp. 
Redmond, WA 98052 
{gzweig,yuncj,panguyen,dongyu, yeyi-
wang,alexac}@microsoft.com 
 
  
Abstract 
Voice-Rate is an automated dialog system 
which provides access to over one million 
ratings of products and businesses. By 
calling a toll-free number, consumers can 
access ratings for products, national busi-
nesses such as airlines, and local busi-
nesses such as restaurants. Voice-Rate 
also has a facility for recording and ana-
lyzing ratings that are given over the 
phone. The service has been primed with 
ratings taken from a variety of web 
sources, and we are augmenting these 
with user ratings. Voice-Rate can be ac-
cessed by dialing 1-877-456-DATA. 
1 Overview 
 Voice-Rate is an automated dialog system de-
signed to help consumers while they are shopping. 
The target user is a consumer who is considering 
making an impulse purchase and would like to get 
more information. He or she can take out a cell-
phone, call Voice-Rate, and get rating information 
to help decide whether to buy the item. Here are 
three sample scenarios: 
 
 Sally has gone to Home Depot to buy 
some paint to touch-up scratches on the 
wall at home. She?ll use exactly the same 
color and brand as when she first painted 
the wall, so she knows what she wants. 
While at Home Depot, however, Sally sees 
some hand-held vacuum cleaners and de-
cides it might be nice to have one. But, she 
is unsure whether which of the available 
models is better: The ?Black & Decker 
CHV1400 Cyclonic DustBuster,? the 
?Shark SV736? or the ?Eureka 71A.? Sally 
calls Voice-Rate and gets the ratings and 
makes an informed purchase. 
 John is on vacation with his family in Seat-
tle. After going up in the Space Needle, 
they walk by ?Abbondanza Pizzeria? and 
are considering lunch there. While it looks 
good, there are almost no diners inside, 
and John is suspicious. He calls Voice-
Rate and discovers that in fact the restau-
rant is highly rated, and decides to go 
there. 
 Returning from his vacation, John drops 
his rental car off at the airport. The rental 
company incorrectly asserts that he has 
scratched the car, and causes a big hassle, 
until they finally realize that they already 
charged the last customer for the same 
scratch. Unhappy with the surly service, 
John calls Voice-Rate and leaves a warn-
ing for others.  
 
Currently, Voice-Rate can deliver ratings for over 
one million products, two hundred thousand res-
taurants in over sixteen hundred cities; and about 
three thousand national businesses.  
2 Technical Challenges 
To make Voice-Rate operational, it was necessary 
to solve the key challenges of name resolution and 
disambiguation. Users rarely make an exactly cor-
rect specification of a product or business, and it is 
necessary both to utilize a ?fuzzy-match? for name 
lookup, and to deploy a carefully designed disam-
biguation strategy.  
31
Voice-Rate solves the fuzzy-matching process by 
treating spoken queries as well as business and 
product names as documents, and then performing 
TF-IDF based lookup. For a review of name 
matching methods, see e.g. Cohen et al, 2003. In 
the ideal case, after a user asks for a particular 
product or business, the best-matching item as 
measured by TF-IDF would be the one intended by 
the user. In reality, of course, this is often not the 
case, and further dialog is necessary to determine 
the user?s intent. For concreteness, we will illu-
strate the disambiguation process in the context of 
product identification. 
 
When a user calls Voice-Rate and asks for a prod-
uct review, the system solicits the user for the 
product name, does TF-IDF lookup, and presents 
the highest-scoring match for user confirmation. If 
the user does not accept the retrieved item, Voice-
Rate initiates a disambiguation dialog.  
 
Aside from inadequate product coverage, which 
cannot be fixed at runtime, there are two possible 
sources for error: automatic speech recognition 
(ASR) errors, and TF-IDF lookup errors.  The dis-
ambiguation process begins by eliminating the 
first. To do this, it asks the user if his or her exact 
words were the recognized text, and if not to repeat 
the request. This loop iterates twice, and if the us-
er?s exact words still have not been identified, 
Voice-Rate apologizes and hangs up. 
 
Once the user?s exact words have been validated, 
Voice-Rate gets a positive identification on the 
product category. From the set of high-scoring TF-
IDF items, a list of possible categories is compiled. 
For example, for ?The Lord of the Rings The Two 
Towers,? there are items in Video Games, DVDs, 
Music, VHS, Software, Books, Websites, and Toys 
and Games. These categories are read to the user, 
who is asked to select one. All the close-matching 
product names in the selected category are then 
read to the user, until one is selected or the list is 
exhausted.  
3 Related Work 
To our knowledge, Voice-Rate is the first large 
scale ratings dialog system. However, the technol-
ogy behind it is closely related to previous dialog 
systems, especially directory assistance or ?411? 
systems (e.g. Kamm et al, 1994, Natarajan et al, 
2002, Levin et al, 2005, Jan et al, 2003).  A gen-
eral discussion of name-matching techniques such 
as TF-IDF can be found in (Cohen et al, 2003, 
Bilenko et al, 2003). 
 
The second area of related research has to do with 
web rating systems. Interesting work on extracting 
information from such ratings can be found in, e.g. 
(Linden et al, 2003, Hu et al, 2004, Gammon et 
al., 2005). Work has also been done using text-
based input to determine relevant products (Chai et 
al., 2002).  Our own work differs from this in that 
it focuses on spoken input, and in its breadth ? 
covering both products and businesses. 
References  
M. Bilenko, R. Mooney, W. W. Cohen, P. Ravikumar and S. 
Fienberg. 2003. Adaptive Name-Matching in Information 
Integration. IEEE Intelligent Systems 18(5): 16-23 (2003).  
J. Chai, V. Horvath, N. Nicolov, M. Stys, N. Kambhatla, W. 
Zadrozny and P. Melville.  2002. Natural Language Assis-
tant- A Dialog System for Online Product Recommenda-
tion. AI Magazine (23), 2002 
 
W. W. Cohen, P Ravikumar and S. E. Fienberg . 2003. A 
comparison of string distance metrics for name-matching 
tasks. Proceedings of the IJCAI-2003 Workshop on Infor-
mation, 2003 
M.  Gamon, A. Aue, S. Corston-Oliver and E. Ringger. 2005. 
Pulse: Mining Customer Opinions from Free Text. In Lec-
ture Notes in Computer Science. Vol. 3646. Springer Ver-
lag. (IDA 2005)., pages 121-132. 
M. Hu and B. Liu. 2004. Mining and summarizing customer 
reviews. Proceedings of the 2004 ACM SIGKDD interna-
tional conference. 
 
E. E. Jan, B. Maison, L. Mangu and G. Zweig. 2003. Auto-
matic construction of Unique Signatures and Confusable 
sets for Natural Language Directory Assistance Applica-
tion.  Eurospeech 2003 
C. A. Kamm, K. M. Yang, C. R. Shamieh and S. Singhal. 
1994. Speech recognition issues for directory assistance 
applications. Second IEEE Workshop on Interactive Voice 
Technology for Telecommunications Applications. 
 
E. Levin and A. M. Man?. 2005. Voice User Interface Design 
for Automated Directory Assistance Eurospeech 2005. 
G. Linden, B. Smith and J. York. Amazon.com recommenda-
tions: item-to-item collaborative filtering. 2003.  Internet 
Computing, IEEE , vol.7, no.1pp. 76- 80. 
 
P. Natarajan, R. Prasad, R. Schwartz and J. Makhoul. 2002. A 
Scalable Architecture for Directory Assistance Automation, 
ICASSP 2002, Orlando, Florida. 
32
Proceedings of SPEECHGRAM 2007, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Handling Out-of-Grammar Commands in Mobile Speech Interaction  
Using Backoff Filler Models 
Tim Paek1, Sudeep Gandhe2, David Maxwell Chickering1, Yun Cheng Ju1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 USA 
2
 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292, USA 
{timpaek|dmax|yuncj}@microsoft.com, gandhe@usc.edu 
 
 
Abstract 
In command and control (C&C) speech in-
teraction, users interact by speaking com-
mands or asking questions typically speci-
fied in a context-free grammar (CFG). Un-
fortunately, users often produce out-of-
grammar (OOG) commands, which can re-
sult in misunderstanding or non-
understanding.  We explore a simple ap-
proach to handling OOG commands that 
involves generating a backoff grammar 
from any CFG using filler models, and util-
izing that grammar for recognition when-
ever the CFG fails.  Working within the 
memory footprint requirements of a mobile 
C&C product, applying the approach 
yielded a 35% relative reduction in seman-
tic error rate for OOG commands.  It also 
improved partial recognitions for enabling 
clarification dialogue. 
1 Introduction 
In command and control (C&C) speech interaction, 
users interact with a system by speaking com-
mands or asking questions.  By defining a rigid 
syntax of possible phrases, C&C reduces the com-
plexity of having to recognize unconstrained natu-
ral language.  As such, it generally affords higher 
recognition accuracy, though at the cost of requir-
ing users to learn the syntax of the interaction 
(Rosenfeld et al, 2001).  To lessen the burden on 
users, C&C grammars are authored in an iterative 
fashion so as to broaden the coverage of likely ex-
pressions for commands, while remaining rela-
tively simple for faster performance.  Nevertheless, 
users can, and often still do, produce OOG com-
mands.  They may neglect to read the instructions, 
or forget the valid expressions.  They may mistak-
enly believe that recognition is more robust than it 
really is, or take too long to articulate the right 
words.  Whatever the reason, OOG commands can 
engender misunderstanding (i.e., recognition of the 
wrong command) or non-understanding (i.e., no 
recognition), and aggravate users who otherwise 
might not realize that their commands were OOG.  
In this paper, we explore a simple approach to 
handling OOG commands, designed specifically to 
meet the memory footprint requirements of a C&C 
product for mobile devices.  This paper is divided 
into three sections.  First, we provide background 
on the C&C product and discuss the different types 
of OOG commands that occur with personal mo-
bile devices. Second, we explain the details of the 
approach and how we applied it to the product do-
main. Finally, we evaluate the approach on data 
collected from real users, and discuss possible 
drawbacks. 
2 Mobile C&C 
With the introduction of voice dialing on mobile 
devices, C&C speech interaction hit the wider 
consumer market, albeit with rudimentary pattern 
recognition. Although C&C has been 
commonplace in telephony and accessibility for 
many years, only recently have mobile devices 
have the memory and processing capacity to 
support not only automatic speech recognition 
(ASR), but a whole range of multimedia 
functionalities that can be controlled with speech.  
Leveraging this newfound computational capacity 
is Voice Command, a C&C application for high-
end mobile devices that allows users to look up 
contacts, place phone calls, retrieve appointments, 
obtain device status information, control 
multimedia and launch applications.  It uses an 
embedded, speaker-independent recognizer and 
operates on 16 bit, 16 kHz, Mono audio. 
33
OOG commands pose a serious threat to the us-
ability of Voice Command. Many mobile users ex-
pect the product to ?just work? without having to 
read the manual.  So, if they should say ?Dial Bob?, 
when the proper syntax for making a phone call is 
Call {Name}, the utterance will likely be mis-
recognized or dropped as a false recognition.  If 
this happens enough, users may abandon the prod-
uct, concluding that it or ASR in general, does not 
work. 
2.1 OOG frequency 
Given that C&C speech interaction is typically 
geared towards a relatively small number of words 
per utterance, an important question is, how often 
do OOG commands really occur in C&C?  In Pro-
ject54 (Kun & Turner, 2005), a C&C application 
for retrieving police information in patrol cars, 
voice commands failed on average 15% of the time, 
roughly 63% of which were due to human error.  
Of that amount, roughly 54% were from extrane-
ous words not found in the grammar, 12% from 
segmentation errors, and the rest from speaking 
commands that were not active. 
To examine whether OOG commands might be 
as frequent on personal mobile devices, we col-
lected over 9700 commands of roughly 1 to 3 sec-
onds each from 204 real users of Voice Command, 
which were recorded as sound (.wav) files.  We 
also logged all device data such as contact entries 
and media items.  All sound files were transcribed 
by a paid professional transcription service.  We 
ignored all transcriptions that did not have an asso-
ciated command; the majority of such cases came 
from accidental pressing of the push-to-talk button.  
Furthermore, we focused on user-initiated com-
mands, during which time the active grammar had 
the highest perplexity, instead of yes-no responses 
and clarification dialogue.  This left 5061 tran-
scribed utterances. 
2.2 Emulation method 
With the data transcribed, we first needed a me-
thod to distinguish between In-Grammar (ING) 
and OOG utterances.  We developed a simulation 
environment built around a desktop version of the 
embedded recognizer which could load the same 
Voice Command grammars and update them with 
user device data, such as contact entries, for each 
sound file.  It is important to note the desktop ver-
sion was not the engine that is commercially 
shipped and optimized for particular devices, but 
rather one that serves testing and research purposes. 
The environment could not only recognize sound 
files, but also parse string input using the dynami-
cally updated grammars as if that were the recog-
nized result.  We utilized the latter to emulate rec-
ognition of all transcribed utterances for Voice 
Command.  If the parse succeeded, we labeled the 
utterance ING, otherwise it was labeled OOG. 
Overall, we found that slightly more than one 
out of every four (1361 or 26.9%) transcribed ut-
terances were OOG.  We provide a complete 
breakdown of OOG types, including extraneous 
words and segmentation errors similar to Project54, 
in the next section.  It is important to keep in mind 
that being OOG by emulation does not necessarily 
entail that the recognizer will fail on the actual 
sound file.  For example, if a user states ?Call Bob 
at mobile phone? when the word ?phone? is OOG, 
the recognizer will still perform well.  The OOG 
percentage for Voice Command may also reflect 
the high perplexity of the name-dialing task.  Users 
had anywhere from 5 to over 2000 contacts, each 
of which could be expressed in multiple ways (e.g., 
first name, first name + last name, prefix + last 
name, etc.).  In summary, our empirical analysis of 
the data suggests that OOG utterances for mobile 
C&C on personal devices can indeed occur on a 
frequent basis, and as such, are worth handling. 
2.3 OOG type 
In order to explore how we might handle different 
types of OOG commands, we classified them ac-
cording to functional anatomy and basic edit op-
erations.  With respect to the former, a C&C utter-
ance consists of three functional components: 
 
1. Slot: A dynamically adjustable list repre-
senting a semantic argument, such as {Con-
tact} or {Date}, where the value of the ar-
gument is typically one of the list members. 
2. Keyword: A word or phrase that uniquely 
identifies a semantic predicate, such as Call 
or Battery, where the predicate corresponds 
in a one-to-one mapping to a type of com-
mand.  
3. Carrier Text: A word or phrase that is de-
signed to facilitate naturalistic expression of 
commands and carries no attached semantic 
content, such as ?What is? or ?Tell me?. 
 
34
For example, in the command ?Call Bob at mo-
bile?, the word ?Call? is the keyword, ?Bob? and 
?mobile? are slots, and ?at? is a carrier word.  
If we were to convert an ING command to 
match an OOG command, we could perform a se-
ries of edit operations: substitution, deletion, and 
insertion.  For classifying OOG commands, substi-
tution implies the use of an unexpected word, dele-
tion implies the absence of an expected word, and 
insertion implies the addition of a superfluous 
word. 
Starting with both functional anatomy and edit 
operations for classification, Table 1 displays the 
different types of OOG commands we labeled 
along with their relative frequencies.  Because 
more than one label might apply to an utterance, 
we first looked to the slot for an OOG type label, 
then keyword, then everything else.  
The most frequent OOG type, at about 60%, was 
OOG Slot, which referred to slot values that did 
not exist in the grammar.  The majority of these 
cases came from two sources: 1) contact entries 
that users thought existed but did not ? sometimes 
they did exist, but not in any normalized form (e.g., 
?Rich? for ?Richard?), and 2) mislabeling of most-
ly foreign names by transcribers.  Although we 
tried to correct as many names as we could, given 
the large contact lists that many users had, this 
proved to be quite challenging.  
The second most frequent OOG type was Inser-
tion at about 14%.  The majority of these insertions 
were single words.  Note that similar to Project54, 
segmentation errors occurred quite often at about 
9%, when the different segmentation types are 
added together. 
3 Backoff Approach 
Having identified the different types of OOG 
commands, we needed to devise an approach for 
handling them that satisfied the requirements of 
Voice Command for supporting C&C on mobile 
devices.  
3.1 Mobile requirements 
For memory footprint, the Voice Command team 
specified that our approach should operate with 
less than 100KB of ROM and 1MB of RAM.  Fur-
thermore, the approach could not require changes 
OOGType % Total Description Examples 
Insertion 14.2% adding a non-keyword, non-slot word call britney porter on mobile phone [?phone? is 
superfluous] 
Deletion 3.1% deleting a non-keyword, non-slot 
word my next appointments [?what are? missing] 
Substitution 2.5% replacing a non-keyword, non-slot 
word 
where is my next appointment  
[?where? is not supported] 
Segmentation 8.2% incomplete utterance show, call, start 
Keyword 
Substitution 4.6% replacing a keyword 
call 8 8 2 8 0 8 0 [?dial? is keyword] ,  
dial john horton [?call? is keyword] 
Keyword 
Segmentation 0.1% incomplete keyword what are my appoint 
Keyword  
Deletion 2.2% deleting the keyword marsha porter at home [?call? missing] 
Slot  
Substitution 0.4% replacing slot words 
call executive 5 on desk  
[?desk? is not slot value] 
Slot  
Segmentation 0.9% incomplete slot call alexander woods on mob 
Slot Deletion 1.0% deleted slot call tracy morey at 
Disfluencies 1.8% disfluencies - mostly repetitions start expense start microsoft excel 
Order  
Rearrangement 0.6% 
changing the order of words within a 
keyword 
what meeting is next [Should be ?what is my 
next meeting?] 
Noise 0.7% non primary speaker oregon state home coming call brandon jones 
on mobile phone 
OOG Slot 59.8% The slot associated with this utterance is out of domain 
Show Rich Lowry [?Richard? is contact entry] , 
dial 0 2 1 6 [Needs > 7 digits] 
 
Table 1. Different OOG command types and their relative frequencies for the Voice Command product. The brack-
eted text in the ?Examples? column explicates the cause of the error 
35
to the existing embedded Speech API (SAPI).  
Because the team also wanted to extend the func-
tionality of Voice Command to new domains, we 
could not assume that we would have any data for 
training models.  Although statistical language 
models (SLM) offer greater robustness to varia-
tions in phrasing than fixed grammars (Rosenfeld, 
2000), the above requirements essentially prohib-
ited them.  So, we instead focused on extending the 
use of the base grammar, which for Voice Com-
mand was a context-free grammar (CFG): a formal 
specification of rules allowing for embedded recur-
sion that defines the set of possible phrases (Man-
ning & Sch?tze, 1999).  
Despite the manual effort that CFGs often re-
quire, they are widely prevalent in industry (Knight 
et al, 2001) for several reasons. First, they are easy 
for designers to understand and author.  Second, 
they are easy to modify; new phrases can be added 
and immediately recognized with little effort.  And 
third, they produce transparent semantics without 
requiring a separate natural language understand-
ing component; semantic properties can be at-
tached to CFG rules and assigned during recogni-
tion.  By focusing on CFGs, our approach allows 
industry designers who are more accustomed to 
fixed grammars to continue using their skill set, 
while hopefully improving the handling of utter-
ances that fall outside of their grammar. 
3.2 Leveraging a backoff grammar 
As long as utterances remain ING, a CFG affords 
fast and accurate recognition, especially because 
engines are often tuned to optimize C&C recogni-
tion.  For example, in comparing recognition per-
formance in a statistical and a CFG-based recog-
nizer for the same domain, Knight et al (2001) 
found that the CFG outperformed the SLM.  In 
order to exploit the optimization of the engine for 
C&C utterances that are ING, we decided to utilize 
a two-pass approach where each command is ini-
tially submitted to the base CFG.  If the confidence 
score of the top recognition C1 falls below a rejec-
tion threshold RCFG, or if the recognizer declares a 
false recognition (based on internal engine fea-
tures), then the audio stream is passed to a backoff 
grammar which then attempts to recognize the 
command.  If the backoff grammar fails to recog-
nize the command, or the top recognition falls 
again below a rejection threshold RBG, then users 
experience the same outcome as they normally 
would otherwise, except with a longer delay.  Fig-
ure 1(a) summarizes the approach. 
In order to generate the backoff grammar and 
still stay within the required memory bounds of 
Voice Command, we explored the use of the built-
in filler or garbage model, which is a context-
independent, acoustic phone loop.  Expressed in 
the syntax as ?...?, filler models capture phones in 
whatever context they are placed.  The functional 
anatomy of a C&C utterance, as explained in Sec-
tion 2.3, sheds light on where to place them: before 
and/or after keywords and/or slots.  As shown Fig-
ure 1(b), to construct a backoff grammar from a 
CFG during design time, we simply parse each 
CFG rule for keywords and slots, remove all car-
rier phrases, and insert filler models before and/or 
after the keywords and/or slots.  Although it is 
straightforward to automatically identify keywords 
(words that uniquely map to a CFG rule) and slots 
(lists with semantic properties), developers may 
want to edit the generated backoff grammar for any 
keywords and slots they wish to exclude; for ex-
ample, in cases where more than one keyword is 
found for a CFG rule. 
For both slots and keywords, we could employ 
any number of different patterns for placing the 
filler models, if any.  Table 2 displays some of the 
patterns in SAPI 5 format, which is an XML for-
mat where question marks indicate optional use.  
Although the Table is for keywords, the same pat-
terns apply for slots.  As shown in k4, even the 
functional constituent itself can be optional.  Fur-
thermore, alternate lists of patterns can be com-
posed, as in kn.  Depending on the number and type 
 
 
Figure 1. (a) A two-pass approach which leverages a 
base CFG for ING recognition and a backoff grammar 
for failed utterances. (b) Design time procedure for 
generating a backoff grammar 
36
of functional constituents for a CFG rule, backoff 
rules can be constructed by adjoining patterns for 
each constituent.  We address the situation when a 
backoff rule corresponds to multiple CFG rules in 
Section 3.4. 
3.3 Domain feasibility 
Because every C&C utterance can be characterized 
by its functional constituents, the backoff filler ap-
proach generically applies to C&C domains, re-
gardless of the actual keywords and slots.  But the 
question remains, is this generic approach feasible 
for handling the different OOG types for Voice 
Command discussed in Section 2.3? 
The filler model is clearly suited for Insertions, 
which are the second most frequent OOG type, 
because it would capture the additional phones.  
However, the most frequent OOG type, OOG Slot, 
cannot be handled by the backoff approach.  That 
requires the developer to write better code for 
proper name normalization (e.g, ?Rich? from ?Ri-
chard?) as well as breaking down the slot value 
into further components for better partial matching 
of names.  Because new C&C domains may not 
utilize name slots, we decided to treat improving 
name recognition as separate research.  Fortu-
nately, opportunity for applying the backoff filler 
approach to OOG Slot types still exists. 
3.4 Clarification of partial recognitions 
As researchers have observed, OOG words con-
tribute to increased word-error rates (Bazzi & 
Glass, 2000) and degrade the recognition perform-
ance of surrounding ING words (Gorrell, 2003).  
Hence, even if a keyword surrounding an OOG 
slot is recognized, its confidence score and the 
overall phrase confidence score will often be de-
graded.  This is in some ways an unfortunate by-
product of confidence annotation, which might be 
circumvented if SAPI exposed word lattice prob-
abilities.  Because SAPI does not, we can instead 
generate partial backoff rules that comprise only a 
subset of the functional constituents of a CFG rule.  
For example, if a CFG rule contains both a key-
word and slot, then we can generate a partial back-
off rule with just one or the other surrounded by 
filler models.  Using partial backoff rules prevents 
degradation of confidence scores for ING constitu-
ents and improves partial recognitions, as we show 
in Section 4.  Partial backoff rules not only handle 
OOG Slot commands where, for example, the 
name slot is not recognized, but also many types of 
segmentation, deletion and substitution commands 
as well. 
Following prior research (Gorrell et al, 2002; 
Hockey et al, 2003), we sought to improve partial 
recognitions so that the system could provide feed-
back to users on what was recognized, and to en-
courage them to stay within the C&C syntax.  Cla-
rification dialogue with implicit instruction of the 
syntax might proceed as follows: If a partial recog-
nition only corresponded to one CFG rule, then the 
system could assume the semantics of that rule and 
remind the user of the proper syntax.  On the other 
hand, if a partial recognition corresponded to more 
than one rule, then a disambiguation dialogue 
could relate the proper syntax for the choices.  For 
example, suppose a user says ?Telephone Bob?, 
using the OOG word ?Telephone?.  Although the 
original CFG would most likely misrecognize or 
even drop this command, our approach would ob-
tain a partial recognition with higher confidence 
score for the contact slot.  If only one CFG rule 
contained the slot, then the system could engage in 
the confirmation, ?Did you mean to say, call 
Bob?? On the other hand, if more than one CFG 
rule contained the slot, then the system could en-
gage in a disambiguation dialogue, such as ?I 
heard 'Bob'. You can either call or show Bob?.  
Either way, the user is exposed to and implicitly 
taught the proper C&C syntax. 
3.5 Related research 
In related research, several researchers have inves-
tigated using both a CFG and a domain-trained 
SLM simultaneously for recognition (Gorrell et al, 
2002; Hockey et al, 2003).  To finesse the per-
formance of a CFG, Gorrell (2003) advocated a 
two-pass approach where an SLM trained on CFG 
Scheme Keyword Pattern 
k1 <keyword/> 
k2 (?)?  <keyword> 
k3 (?)?  <keyword/>  (?)? 
k4 (?)?  <keyword/>? (?)? 
kn 
<list> 
(?)?  <keyword/>? (?)? 
(?) 
</list> 
 
Table 2. Possible patterns in SAPI 5 XML format for 
placing the filler model, which appears as 
?...?.Question marks indicate optional use. 
37
data (and slightly augmented) is utilized as a back-
off grammar.  However, only the performance of 
the SLM on a binary OOG classification task was 
evaluated and not the two-pass approach itself.  In 
designing a multimodal language acquisition sys-
tem, Dusan & Flanagan (2002) developed a two-
pass approach where they utilized a dictation n-
gram as a backoff grammar and added words rec-
ognized in the second pass into the base CFG.  Un-
fortunately, they only evaluated the general usabil-
ity of their architecture. 
Because of the requirements outlined in Section 
3.1, we have focused our efforts on generating a 
backoff grammar from the original CFG, taking 
advantage of functional anatomy and filler models.  
The approach is agnostic about what the actual fil-
ler model is, and as such, the built-in phone loop 
can easily be replaced by word-level (e.g., Yu et 
al., 2006) and sub-word level filler models (e.g., 
Liu et al, 2005).  In fact, we did explore the word-
level filler model, though so far we have not been 
able to meet the footprint requirements.  We are 
currently investigating phone-based filler models. 
Outside of recognition with a CFG, researchers 
have pursued methods that directly model OOG 
words as sub-word units in the recognition search 
space of a finite state transducer (FST) (Bazzi & 
Glass, 2000).  OOG words can also be dynamically 
incorporated into the FST (Chung et al, 2004).  
Because this line of research depends on entirely 
different engine architecture, we could not apply 
the techniques. 
4 Evaluation 
In C&C speech interaction, what matters most is 
not word-error rate, but semantic accuracy and task 
completion.  Because task completion is difficult to 
evaluate without collecting new data, we evaluated 
the semantic accuracy of the two-pass approach 
against the baseline of using just the CFG on the 
data we collected from real users, as discussed in 
Section 2.1.  Furthermore, because partial 
recognitions can ultimately result in a successful 
dialogue, we carried out separate evaluations for 
the functional constituents of a command (i.e., 
keyword and slot) as well as the complete 
command (keyword + slot).  For Voice Command, 
no command contained more than one slot, and 
because the vast majority of single slot commands 
were commands to either call or show a contact 
entry, we focused on those two commands as a 
proof of concept. 
For any utterance, the recognizer can either ac-
cept or reject it.  If it is accepted, then the seman-
tics of the utterance can either be correct (i.e., it 
matches what the user intended) or incorrect.  The 
following metrics can now be defined: 
 
precision = CA / (CA + IA)   (1) 
recall = CA / (CA + R)    (2) 
accuracy = CA / (CA + IA + R)   (3) 
 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejected 
commands.  Although R could be decomposed into 
correct and incorrect rejections, for C&C, 
recognition failure is essentially perceived the 
same way by users: that is, as a non-understanding. 
4.1 Results 
For every C&C command in Voice Command, the 
embedded recognizer returns either a false 
recognition (based on internal engine parameters) 
or a recognition event with a confidence score.  As 
described in Section 3.2, if the confidence score 
falls below a rejection threshold RCFG, then the 
audio stream is processed by the backoff grammar 
which also enforces its own threshold RBG.  The 
RCFG for Voice Command was set to 45% by a 
proprietary tuning procedure for optimizing 
acoustic word-error rate.  For utterances that 
exceeded RCFG, 84.2% of them were ING and 
15.8% OOG.  For utterances below RCFG, 48.5% 
 
 
Figure 2. The semantic accuracies comparing the 
baseline CFG against both the BG (backoff grammar 
alone) and the two-pass approach (CFG + Backoff) 
separated into functional constituent groups and fur-
ther separated by ING and OOG commands. 
38
were ING and 51.5% OOG.  Because a 
considerable number of utterances may be ING in 
the second pass, as it was in our case, RBG requires 
tuning as well.  Instead of using a development 
dataset to tune RBG, we decided to evaluate our 
approach on the entire data with RBG set to the 
same proprietary threshold as RCFG.  In post-hoc 
analyses, this policy of setting the two thresholds 
equal and reverting to the CFG recognition if the 
backoff confidence score falls below RBG achieved 
results comparable to optimizing the thresholds. 
Figure 2 displays semantic accuracies separated 
by ING and OOG commands.  Keyword evalua-
tions comprised 3700 ING and 1361 OOG com-
mands.  Slot and keyword + slot evaluations com-
prised 2111 ING and 138 OOG commands.  Over-
all, the two-pass approach was significantly higher 
in semantic accuracy than the baseline CFG, using 
McNemar's test (p<0.001).  Not surprisingly, the 
largest gains were with OOG commands.  Notice 
that for partial recognitions (i.e., keyword or slot 
only), the approach was able to improve accuracy, 
which with further clarification dialogue, could 
result in task completions.  Interestingly, the ap-
proach performed the same for keyword + slot as it 
did for slot, which suggests that getting the slot 
correct is crucial to recognizing surrounding key-
words.  Despite the high percentage of OOG Slots, 
slot accuracy still increased due to better handling 
of other OOG types such as deletions, insertions 
and substitutions.   
Finally, as a comparison, for the keyword + slot 
task, an upper bound of 74.3% ? 1.1% (10-fold 
cross-validated standard error) overall semantic 
accuracy was achieved using a small footprint sta-
tistical language modeling technique that re-ranked 
CFG results (Paek & Chickering, 2007), though 
the comparison is not completely fair given that the 
technique was focused on predictive language 
modeling and not on explicitly handling OOG ut-
terances.  Also note that in all cases, the backoff 
grammar alone performed worse than either the 
CFG or the two-pass approach.   
Table 3 provides a more detailed view of the re-
sults for the just OOG commands as well as the 
relative reductions in semantic error rate (RER).  
Notice that the approach increases recall, which 
signifies less non-understandings. However, this 
comes at the price of a small increase in misunder-
standings, as seen in the decrease in precision.  
Overall, the best reduction in semantic error rate 
achieved by the approach was about 35%. 
Decomposing RER by OOG types, we found 
that for keyword evaluations, the biggest im-
provement (52% RER), came about for Deletion 
types, or commands with missing carrier words.  
This makes sense because the backoff grammar 
only cares about the keyword.  For slot and key-
word + slot evaluations, Insertion types maintained 
the biggest improvement at 38% RER. 
Note that the results presented are those ob-
tained without tuning.  If application developers 
wanted to find an optimal operating point, they 
would need to decide what is more important for 
their application: precision or recall, and adjust the 
thresholds until they reach acceptable levels of per-
formance.  Ideally, these levels should accord with 
what real users of the application would accept. 
4.2 Efficiency 
Given that the approach was aimed at satisfying 
the mobile requirements stated in Section 3.1, 
which it did, we also compared the processing time 
it takes to arrive at a recognition or false 
recognition between the CFG alone and the two-
pass approach.  Because of the filler models, the 
backoff grammar is a more relaxed version of CFG 
with a larger search space, and as such, takes 
slightly more processing time. The average 
processing time for the CFG in our simulation 
environment was about 395 milliseconds, whereas 
the average processing time for the two passes was 
about 986 milliseconds.  Hence, when the backoff 
grammar is used, the total computation time is 
approximately 2.5 times that of a single pass alone.  
In our experiments, a total of 1570 commands (i.e. 
31%) required the two passes, while 3491 of them 
were accepted after a single CFG pass. 
 CFG 2-PASS RER 
Prec 85.0% 79.0% -39.7% 
Recall 36.8% 58.6% 34.5% Keyword 
Acc 34.5% 50.7% 24.7% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Slot 
Acc 54.4% 70.3% 34.9% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Keyword 
+ Slot 
Acc 54.4% 70.3% 34.9% 
 
Table 3. Relative reductions in semantic error rate, or 
Relative Error Reduction (RER) for OOG commands 
grouped by keyword, slot and keyword + slot evalua-
tions. ?2-PASS? denotes the two-pass approach. 
39
4.3 Drawbacks 
In exploring the backoff filler approach, we 
encountered a few drawbacks that are worth 
considering when applying this approach to other 
domains.  The first issue dealt with false positives.  
In the data collection for Voice Command, a total 
of 288 utterances contained no discernable speech.  
If these were included in the data set, they would 
amount to about 5% of all utterances.  As 
mentioned previously, these were mostly cases 
when the push-to-talk button was accidentally 
triggered.  When we evaluated the approach on 
these utterances, we found that the CFG accepted 
36 or roughly 13% of them, while the proposed 
approach accepted 115 or roughly 40% of them.  
For our domain, this problem can be avoided by 
instructing users to lock their devices when not in 
use to prevent spurious initiations.  For other C&C 
domains where unintentional command initiations 
occur frequently, this may be a serious concern, 
though we suspect that users will be more 
forgiving of accidental errors than real errors. 
Another drawback dealt with generating the 
backoff grammar.  As we discussed in Section 3.2, 
various patterns for placing filler models can be 
utilized.  Although we did explore the possibility 
that perhaps certain patterns might generalize 
across domains, we found that it was better to 
hand-craft patterns to the application.  For Voice 
Command, we used the kn pattern specified in Ta-
ble 2 for keywords, and the identical sn pattern for 
slots because they proved to be best suited to the 
product grammars in pre-trial experiments. 
5 Conclusion & Future Direction 
In this paper, we classified the different types of 
OOG commands that might occur in a mobile 
C&C application, and presented a simple two-pass 
approach for handling them that leverages the base 
CFG for ING recognition and a backoff grammar 
OOG recognition.  The backoff grammar is gener-
ated from the original CFG by surrounding key-
words and/or slots with filler models.  Operating 
within the memory footprint requirements of a 
mobile C&C product, the approach yielded a 35% 
relative reduction in semantic error rate for OOG 
commands, and improved partial recognitions, 
which can facilitate clarification dialogue. 
We are now exploring small footprint, phone-
based filler models.  Another avenue for future 
research is to further investigate optimal policies 
for deciding when to pass to the backoff grammar 
and when to use the backoff grammar recognition. 
References 
I. Bazzi & J. Glass. 2000. Modeling out-of-vocabulary 
words for robust speech recognition. In Proc. ICSLP. 
G. Chung, S. Seneff, C.Wang, & I. Hetherington. 2004. 
A dynamic vocabulary spoken dialogue interface. In 
Proc. ICSLP. 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based 
upon multimodal language acquisition. In Proc. IC-
MI. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intel-
ligent help to mixed initiative spoken dialogue sys-
tems. In Proc. ICSLP. 
G. Gorrell. 2003. Using statistical language modeling to 
identify new vocabulary in a grammar-based speech 
recognition system. In Proc. Eurospeech. 
B. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist, J. 
Hieronymus, A. Gruenstein, & J. Dowding. 2003. 
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance. In 
Proc. EACL, pp. 147?154. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, & I. Lewin. 2001. Comparing grammar-based 
and robust approaches to speech understanding: A 
case study. In Proc. Eurospeech. 
A. Kun & L. Turner. 2005. Evaluating the project54 
speech user interface. In Proc. Pervasive. 
P. Liu, Y. Tian, J. Zhou, & F. Soong. 2005. Background 
model based posterior probability for measuring 
confidence. In Proc. Interspeech. 
C.D. Manning & H. Sch?utze. 1999. Foundations of 
Statistical Natural Language Processing. MIT Press, 
Cambridge,Massachusetts. 
Paek, T. & Chickering, D. 2007. Improving command 
and control speech recognition: Using predictive us-
er models for language modeling. UMUAI, 17(1):93-
117. 
Rosenfeld, R. 2000. Two decades of statistical language 
modeling: Where do we go from here? In Proc. of the 
IEEE, 88(8): 1270?1278. 
R. Rosenfeld, D. Olsen, & A. Rudnicky. 2001. Univer-
sal speech interfaces. Interactions, 8(6):34?44. 
D. Yu, Y.C. Ju, Y. Wang, & A. Acero. 2006. N-gram 
based filler model for robust grammar authoring. In 
Proc. ICASSP. 
40
Proceedings of the ACL 2010 Conference Short Papers, pages 313?317,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Using Speech to Reply to SMS Messages While Driving: 
An In-Car Simulator User Study 
Yun-Cheng Ju, Tim Paek 
Microsoft Research 
Redmond, WA USA 
{yuncj|timpaek}@microsoft.com 
 
 
Abstract 
Speech recognition affords automobile 
drivers a hands-free, eyes-free method of 
replying to Short Message Service (SMS) 
text messages. Although a voice search 
approach based on template matching has 
been shown to be more robust to the chal-
lenging acoustic environment of automo-
biles than using dictation, users may have 
difficulties verifying whether SMS re-
sponse templates match their intended 
meaning, especially while driving. Using a 
high-fidelity driving simulator, we com-
pared dictation for SMS replies versus 
voice search in increasingly difficult driv-
ing conditions. Although the two ap-
proaches did not differ in terms of driving 
performance measures, users made about 
six times more errors on average using 
dictation than voice search. 
1 Introduction 
Users love Short Message Service (SMS) text 
messaging; so much so that 3 trillion SMS mes-
sages are expected to have been sent in 2009 
alone (Stross, 2008). Because research has 
shown that SMS messaging while driving results 
in 35% slower reaction time than being intox-
icated (Reed & Robbins, 2008), campaigns have 
been launched by states, governments and even 
cell phone carriers to discourage and ban SMS 
messaging while driving (DOT, 2009). Yet, au-
tomobile manufacturers have started to offer in-
fotainment systems, such as the Ford Sync, 
which feature the ability to listen to incoming 
SMS messages using text-to-speech (TTS). Au-
tomatic speech recognition (ASR) affords users a 
hands-free, eyes-free method of replying to SMS 
messages. However, to date, manufacturers have 
not established a safe and reliable method of le-
veraging ASR, though some researchers have 
begun to explore techniques. In previous re-
search (Ju & Paek, 2009), we examined three 
ASR approaches to replying to SMS messages: 
dictation using a language model trained on SMS 
responses, canned responses using a probabilistic 
context-free grammar (PCFG), and a ?voice 
search? approach based on template matching. 
Voice search proceeds in two steps (Natarajan et 
al., 2002): an utterance is first converted into 
text, which is then used as a search query to 
match the most similar items of an index using 
IR techniques (Yu et al, 2007). For SMS replies, 
we created an index of SMS response templates, 
with slots for semantic concepts such as time and 
place, from a large SMS corpus. After convolv-
ing recorded SMS replies so that the audio would 
exhibit the acoustic characteristics of in-car rec-
ognition, they compared how the three approach-
es handled the convolved audio with respect to 
the top n-best reply candidates. The voice search 
approach consistently outperformed dictation and 
canned responses, achieving as high as 89.7% 
task completion with respect to the top 5 reply 
candidates. 
Even if the voice search approach may be 
more robust to in-car noise, this does not guaran-
tee that it will be more usable. Indeed, because 
voice search can only match semantic concepts 
contained in the templates (which may or may 
not utilize the same wording as the reply), users 
must verify that a retrieved template matches the 
semantics of their intended reply. For example, 
suppose a user replies to the SMS message ?how 
about lunch? with ?can?t right now running er-
rands?. Voice search may find ?nope, got er-
rands to run? as the closest template match, in 
which case, users will have to decide whether 
this response has the same meaning as their re-
ply. This of course entails cognitive effort, which 
is very limited in the context of driving. On the 
other hand, a dictation approach to replying to 
SMS messages may be far worse due to misre-
cognitions. For example, dictation may interpret 
?can?t right now running errands? as ?can right 
313
now fun in errands?. We posited that voice 
search has the advantage because it always gene-
rates intelligible SMS replies (since response 
templates are manually filtered), as opposed to 
dictation, which can sometimes result in unpre-
dictable and nonsensical misrecognitions. How-
ever, this advantage has not been empirically 
demonstrated in a user study. This paper presents 
a user study investigating how the two approach-
es compare when users are actually driving ? that 
is, when usability matters most. 
2 Driving Simulator Study 
Although ASR affords users hands-free, eyes-
free interaction, the benefits of leveraging speech 
can be forfeit if users are expending cognitive 
effort judging whether the speech interface cor-
rectly interpreted their utterances. Indeed, re-
search has shown that the cognitive demands of 
dialogue seem to play a more important role in 
distracting drivers than physically handling cell 
phones (Nunes & Recarte, 2002; Strayer & 
Johnston, 2001). Furthermore, Kun et al (2007) 
have found that when in-car speech interfaces 
encounter recognition problems, users tend to 
drive more dangerously as they attempt to figure 
out why their utterances are failing. Hence, any 
approach to replying to SMS messages in auto-
mobiles must avoid distracting drivers with er-
rors and be highly usable while users are en-
gaged in their primary task, driving. 
2.1 Method 
To assess the usability and performance of both 
the voice search approach and dictation, we con-
ducted a controlled experiment using the STISIM 
Drive? simulator. Our simulation setup con-
sisted of a central console with a steering wheel 
and two turn signals, surrounded by three 47?? 
flat panels placed at a 45? angle to immerse the 
driver. Figure 1 displays the setup. 
We recruited 16 participants (9 males, 7 fe-
males) through an email sent to employees of our 
organization. The mean age was 38.8. All partic-
ipants had a driver?s license and were compen-
sated for their time.  
We examined two independent variables: SMS 
Reply Approach, consisting of voice search and 
dictation, and Driving Condition, consisting of 
no driving, easy driving and difficult driving. We 
included Driving Condition as a way of increas-
ing cognitive demand (see next section). Overall, 
we conducted a 2 (SMS Reply Approach) ? 3 
(Driving Condition) repeated measures, within-
subjects design experiment in which the order of 
SMS Reply for each Driving Condition was coun-
ter-balanced. Because our primary variable of 
interest was SMS Reply, we had users experience 
both voice search and dictation with no driving 
first, then easy driving, followed by difficult 
driving. This gave users a chance to adjust them-
selves to increasingly difficult road conditions. 
 
Driving Task: As the primary task, users were 
asked to drive two courses we developed with 
easy driving and difficult driving conditions 
while obeying all rules of the road, as they would 
in real driving and not in a videogame. With 
speed limits ranging from 25 mph to 55 mph, 
both courses contained five sequential sections 
which took about 15-20 minutes to complete: a 
residential area, a country highway, and a small 
city with a downtown area as well as a busi-
ness/industrial park. Although both courses were 
almost identical in the number of turns, curves, 
stops, and traffic lights, the easy course consisted 
mostly of simple road segments with relatively 
no traffic, whereas the difficult course had four 
times as many vehicles, cyclists, and pedestrians. 
The difficult course also included a foggy road 
section, a few busy construction sites, and many 
unexpected events, such as a car in front sudden-
ly breaking, a parked car merging into traffic, 
and a pedestrian jaywalking. In short, the diffi-
cult course was designed to fully engage the at-
tention and cognitive resources of drivers.  
 
SMS Reply Task: As the secondary task, we 
asked users to listen to an incoming SMS mes-
sage together with a formulated reply, such as: 
(1) Message Received: ?Are you lost?? Your 
Reply: ?No, never with my GPS? 
The users were asked to repeat the reply back to 
the system. For Example (1) above, users would 
have to utter ?No, never with my GPS?. Users 
 
Figure 1. Driving simulator setup. 
314
could also say ?Repeat? if they had any difficul-
ties understanding the TTS rendering or if they 
experienced lapses in attention. For each course, 
users engaged in 10 SMS reply tasks. SMS mes-
sages were cued every 3000 feet, roughly every 
90 seconds, which provided enough time to 
complete each SMS dialogue. Once users uttered 
the formulated reply, they received a list of 4 
possible reply candidates (each labeled as ?One?, 
?Two?, etc.), from which they were asked to ei-
ther pick the correct reply (by stating its number 
at any time) or reject them all (by stating ?All 
wrong?). We did not provide any feedback about 
whether the replies they picked were correct or 
incorrect in order to avoid priming users to pay 
more or less attention in subsequent messages. 
Users did not have to finish listening to the entire 
list before making their selection.  
 
Stimuli: Because we were interested in examin-
ing which was worse, verifying whether SMS 
response templates matched the meaning of an 
intended reply, or deciphering the sometimes 
nonsensical misrecognitions of dictation, we de-
cided to experimentally control both the SMS 
reply uttered by the user as well as the 4-best list 
generated by the system. However, all SMS rep-
lies and 4-best lists were derived from the logs of 
an actual SMS Reply interface which imple-
mented the dictation and the voice search ap-
proaches (see Ju & Paek, 2009). For each course, 
5 of the SMS replies were short (with 3 or fewer 
words) and 5 were long (with 4 to 7 words). The 
mean length of the replies was 3.5 words (17.3 
chars). The order of the short and long replies 
was randomized. 
We selected 4-best lists where the correct an-
swer was in each of four possible positions (1-4) 
or All Wrong; that is, there were as many 4-best 
lists with the first choice correct as there were 
with the second choice correct, and so forth. We 
then randomly ordered the presentation of differ-
ent 4-best lists. Although one might argue that 
the four positions are not equally likely and that 
the top item of a 4-best list is most often the cor-
rect answer, we decided to experimentally con-
trol the position for two reasons: first, our pre-
vious research (Ju & Paek, 2009) had already 
demonstrated the superiority of the voice search 
approach with respect to the top position (i.e., 1-
best), and second, our experimental design 
sought to identify whether the voice search ap-
proach was more usable than the dictation ap-
proach even when the ASR accuracy of the two 
approaches was the same. 
In the dictation condition, the correct answer 
was not always an exact copy of the reply in 0-2 
of the 10 SMS messages. For instance, a correct 
dictation answer for Example (1) above was ?no 
I?m never with my GPS?. On the other hand, the 
voice search condition had more cases (2-4 mes-
sages) in which the correct answer was not an 
exact copy (e.g., ?no I have GPS?) due to the 
nature of the template approach. To some degree, 
this could be seen as handicapping the voice 
search condition, though the results did not re-
flect the disadvantage, as we discuss later. 
 
Measures: Performance for both the driving task 
and the SMS reply tasks were recorded. For the 
driving task, we measured the numbers of colli-
sions, speeding (exceeding 10 mph above the 
limit), traffic light and stop sign violations, and 
missed or incorrect turns. For the SMS reply 
task, we measured duration (i.e., time elapsed 
between the beginning of the 4-best list and 
when users ultimately provided their answer) and 
the number of times users correctly identified 
which of the 4 reply candidates contained the 
correct answer. 
Originally, we had an independent rater verify 
the position of the correct answer in all 4-best 
lists, however, we considered that some partici-
pants might be choosing replies that are semanti-
cally sufficient, even if they are not exactly cor-
rect. For example, a 4-best list generated by the 
dictation approach for Example (1) had: ?One: 
no I?m never want my GPS. Two: no I?m never 
with my GPS. Three: no I?m never when my 
GPS. Or Four: no no I?m in my GPS.? Although 
the rater identified the second reply as being 
?correct?, a participant might view the first or 
third replies as sufficient. In order to avoid am-
biguity about correctness, after the study, we 
showed the same 16 participants the SMS mes-
sages and replies as well as the 4-best lists they 
received during the study and asked them to se-
lect, for each SMS reply, any 4-best list items 
they felt sufficiently conveyed the same mean-
ing, even if the items were ungrammatical. Par-
ticipants were explicitly told that they could se-
lect multiple items from the 4-best list. We did 
not indicate which item they selected during the 
experiment and because this selection task oc-
curred months after the experiment, it was un-
likely that they would remember anyway. Partic-
ipants were compensated with a cafeteria vouch-
er. 
In computing the number of ?correct? an-
swers, for each SMS reply, we counted an an-
315
swer to be correct if it was included among the 
participants? set of semantically sufficient 4-best 
list items. Hence, we calculated the number of 
correct items in a personalized fashion for every 
participant. 
2.2 Results 
We conducted a series of repeated measures 
ANOVAs on all driving task and SMS reply task 
measures. For the driving task, we did not find 
any statistically significant differences between 
the voice search and dictation conditions. In oth-
er words, we could not reject the null hypothesis 
that the two approaches were the same in terms 
of their influence on driving performance. How-
ever, for the SMS reply task, we did find a main 
effect for SMS Reply Approach (F1,47 = 81.28, p < 
.001, ?Dictation = 2.13 (.19), ?VoiceSearch = .38 (.10)). 
As shown in Figure 2, the average number of 
errors per driving course for dictation is roughly 
6 times that for voice search. We also found a 
main effect for total duration (F1,47 = 11.94, p < 
.01, ?Dictation = 113.75 sec (3.54) or 11.4 sec/reply, 
?VoiceSearch = 125.32 sec (3.37) or 12.5 sec/reply). 
We discuss our explanation for the shorter dura-
tion below. For both errors and duration, we did 
not find any interaction effects with Driving 
Conditions. 
3 Discussion 
We conducted a simulator study in order to ex-
amine which was worse while driving: verifying 
whether SMS response templates matched the 
meaning of an intended reply, or deciphering the 
sometimes nonsensical misrecognitions of dicta-
tion. Our results suggest that deciphering dicta-
tion results under the duress of driving leads to 
more errors. In conducting a post-hoc error anal-
ysis, we noticed that participants tended to err 
when the 4-best lists generated by the dictation 
approach contained phonetically similar candi-
date replies. Because it is not atypical for the dic-
tation approach to have n-best list candidates 
differing from each other in this way, we rec-
ommend not utilizing this approach in speech-
only user interfaces, unless the n-best list candi-
dates can be made as distinct from each other as 
possible, phonetically, syntactically and most 
importantly, semantically. The voice search ap-
proach circumvents this problem in two ways: 1) 
templates were real responses and manually se-
lected and cleaned up during the development 
phase so there were no grammatical mistakes, 
and 2) semantically redundant templates can be 
further discarded to only present the distinct con-
cepts at the rendering time using the paraphrase 
detection algorithms reported in (Wu et al, 
2010). 
Given that users committed more errors in the 
dictation condition, we initially expected that 
dictation would exhibit higher duration than 
voice search since users might be spending more 
time figuring out the differences between the 
similar 4-best list candidates generated by the 
dictation approach. However, in our error analy-
sis we observed that most likely users did not 
discover the misrecognitions, and prematurely 
selected a reply candidate, resulting in shorter 
durations. The slightly higher duration for the 
voice search approach does not constitute a prob-
lem if users are listening to all of their choices 
and correctly selecting their intended SMS reply. 
Note that the duration did not bring about any 
significant driving performance differences. 
Although we did not find any significant driv-
ing performance differences, users experienced 
more difficulties confirming whether the dicta-
tion approach correctly interpreted their utter-
ances than they did with the voice search ap-
proach. As such, if a user deems it absolutely 
necessary to respond to SMS messages while 
driving, our simulator study suggests that the 
most reliable (i.e., least error-prone) way to re-
spond may just well be the voice search ap-
proach. 
References  
Distracted Driving Summit. 2009. Department of 
Transportation. Retrieved Dec. 1: 
http://www.rita.dot.gov/distracted_driving_summit 
 
Figure 2. Mean number of errors for the dictation 
and voice search approaches. Error bars represent 
standard errors about the mean. 
316
Y.C. Ju & T. Paek. 2009. A Voice Search Approach 
to Replying to SMS Messages in Automobiles. In 
Proc. of Interspeech. 
A. Kun, T. Paek & Z. Medenica. 2007. The Effect of 
Speech Interface Accuracy on Driving Perfor-
mance, In Proc. of Interspeech. 
P. Natarajan, R. Prasad, R. Schwartz, & J. Makhoul. 
2002. A Scalable Architecture for Directory Assis-
tance Automation. In Proc. of ICASSP, pp. 21-24. 
L. Nunes & M. Recarte. 2002. Cognitive Deamnds of 
Hands-Free-Phone Conversation While Driving. 
Transportation Research Part F, 5: 133-144. 
N. Reed & R. Robbins. 2008. The Effect of Text 
Messaging on Driver Behaviour: A Simulator 
Study. Transport Research Lab Report, PPR 367. 
D. Strayer & W. Johnston. 2001. Driven to Distrac-
tion: Dual-task Studies of Simulated Driving and 
Conversing on a Cellular Phone. Psychological 
Science, 12: 462-466. 
R. Stross. 2008. ?What carriers aren?t eager to tell you 
about texting?, New York Times, Dec. 26, 2008: 
http://www.nytimes.com/2008/12/28/business/28di
gi.html?_r=3  
D. Yu, Y.C. Ju, Y.-Y. Wang, G. Zweig, & A. Acero. 
2007. Automated Directory Assistance System: 
From Theory to Practice. In Proc. of Interspeech. 
Wei Wu, Yun-Cheng Ju, Xiao Li, and Ye-Yi Wang, 
Paraphrase Detection on SMS Messages in Auto-
mobiles, in ICASSP, IEEE, March 2010 
317
