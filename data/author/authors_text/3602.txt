Proceedings of NAACL HLT 2007, pages 292?299,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Extracting Semantic Orientations of Phrases from Dictionary
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Integrated Research Institute
Tokyo Institute of Technology
inui@iri.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose a method for extracting se-
mantic orientations of phrases (pairs of an
adjective and a noun): positive, negative,
or neutral. Given an adjective, the seman-
tic orientation classification of phrases can
be reduced to the classification of words.
We construct a lexical network by con-
necting similar/related words. In the net-
work, each node has one of the three ori-
entation values and the neighboring nodes
tend to have the same value. We adopt
the Potts model for the probability model
of the lexical network. For each adjec-
tive, we estimate the states of the nodes,
which indicate the semantic orientations
of the adjective-noun pairs. Unlike ex-
isting methods for phrase classification,
the proposed method can classify phrases
consisting of unseen words. We also pro-
pose to use unlabeled data for a seed set of
probability computation. Empirical evalu-
ation shows the effectiveness of the pro-
posed method.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial ar-
eas. It can be applied to, for example, a survey of
new products or a questionnaire analysis. Automatic
sentiment analysis enables a fast and comprehensive
investigation.
The most fundamental step for sentiment analy-
sis is to acquire the semantic orientations of words:
positive or negative (desirable or undesirable). For
example, the word ?beautiful? is positive, while the
word ?dirty? is negative. Many researchers have de-
veloped several methods for this purpose and ob-
tained good results. One of the next problems to be
solved is to acquire semantic orientations of phrases,
or multi-term expressions, such as ?high+risk? and
?light+laptop-computer?. Indeed the semantic ori-
entations of phrases depend on context just as the se-
mantic orientations of words do, but we would like
to obtain the orientations of phrases as basic units
for sentiment analysis. We believe that we can use
the obtained basic orientations of phrases for affect
analysis of higher linguistic units such as sentences
and documents.
A computational model for the semantic orienta-
tions of phrases has been proposed by Takamura et
al. (2006). However, their method cannot deal with
the words that did not appear in the training data.
The purpose of this paper is to propose a method for
extracting semantic orientations of phrases, which is
applicable also to expressions consisting of unseen
words. In our method, we regard this task as the
noun classification problem for each adjective; the
nouns that become respectively positive (negative,
or neutral) when combined with a given adjective
are distinguished from the other nouns. We create
a lexical network with words being nodes, by con-
necting two words if one of the two appears in the
gloss of the other. In the network, each node has one
of the three orientation values and the neighboring
nodes expectedly tend to have the same value. For
292
example, the gloss of ?cost? is ?a sacrifice, loss, or
penalty? and these words (cost, sacrifice, loss, and
penalty) have the same orientation. To capture this
tendency of the network, we adopt the Potts model
for the probability distribution of the lexical net-
work. For each adjective, we estimate the states of
the nodes, which indicate the semantic orientations
of the adjective-noun pairs. Information from seed
words is diffused to unseen nouns on the network.
We also propose a method for enlarging the seed
set by using the output of an existing method for the
seed words of the probability computation.
Empirical evaluation shows that our method
works well both for seen and unseen nouns, and that
the enlarged seed set significantly improves the clas-
sification performance of the proposed model.
2 Related Work
The semantic orientation classification of words has
been pursued by several researchers. Some of
them used corpora (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003), while others used
dictionaries (Kobayashi et al, 2001; Kamps et al,
2004; Takamura et al, 2005; Esuli and Sebastiani,
2005).
Turney (2002) applied an internet-based tech-
nique to the semantic orientation classification of
phrases, which had originally been developed for
word sentiment classification. In their method, the
number of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
?phrase NEAR good?) is used to determine the ori-
entation. Baron and Hirst (2004) extracted colloca-
tions with Xtract (Smadja, 1993) and classified the
collocations using the orientations of the words in
the neighboring sentences. Their method is similar
to Turney?s in the sense that cooccurrence with seed
words is used. In addition to individual seed words,
Kanayama and Nasukawa (2006) used more compli-
cated syntactic patterns that were manually created.
The four methods above are based on context infor-
mation. In contrast, our method exploits the internal
structure of the semantic orientations of phrases.
Wilson et al (2005) worked on phrase-level se-
mantic orientations. They introduced a polarity
shifter. They manually created the list of polarity
shifters. Inui (2004) also proposed a similar idea.
Takamura et al (2006) proposed to use based on
latent variable models for sentiment classification of
noun-adjective pairs. Their model consists of vari-
ables respectively representing nouns, adjectives, se-
mantic orientations, and latent clusters, as well as
the edges between the nodes. The words that are
similar in terms of semantic orientations, such as
?risk? and ?mortality? (i.e., the positive orientation
emerges when they are ?low?), make a cluster in
their model, which can be an automated version of
Inui?s or Wilson et al?s idea above. However, their
method cannot do anything for the words that did not
appear in the labeled training data. In this paper, we
call their method the latent variable method (LVM).
3 Potts Model
If a variable can have more than two values and
there is no ordering relation between the values,
the network comprised of such variables is called
Potts model (Wu, 1982). In this section, we ex-
plain the simplified mathematical model of Potts
model, which is used for our task in Section 4.
The Potts system has been used as a mathematical
model in several applications such as image restora-
tion (Tanaka and Morita, 1996) and rumor transmis-
sion (Liu et al, 2001).
3.1 Introduction to the Potts Model
Suppose a network consisting of nodes and weighted
edges is given. States of nodes are represented by c.
The weight between i and j is represented by wij .
Let H(c) denote an energy function, which indi-
cates a state of the whole network:
H(c) = ??
?
ij
wij?(ci, cj)+?
?
i?L
??(ci, ai), (1)
where ? is a constant called the inverse-temperature,
L is the set of the indices for the observed variables,
ai is the state of each observed variable indexed by i,
and ? is a positive constant representing a weight on
labeled data. Function ? returns 1 if two arguments
are equal to each other, 0 otherwise. The state is
penalized if ci (i ? L) is different from ai. Using
H(c), the probability distribution of the network is
represented as P (c) = exp{?H(c)}/Z, where Z is
a normalization factor.
However, it is computationally difficult to exactly
estimate the state of this network. We resort to a
293
mean-field approximation method that is described
by Nishimori (2001). In the method, P (c) is re-
placed by factorized function ?(c) =
?
i ?i(ci).
Then we can obtain the function with the smallest
value of the variational free energy:
F (c) =
?
c
P (c)H(c)?
?
c
?P (c) logP (c)
= ??
?
i
?
ci
?i(ci)?(ci, ai)
??
?
ij
?
ci,cj
?i(ci)?j(cj)wij?(ci, cj)
?
?
i
?
ci
??i(ci) log ?i(ci). (2)
By minimizing F (c) under the condition that ?i,?
ci ?i(ci) = 1, we obtain the following fixed point
equation for i ? L:
?i(c) =
exp(??(c, ai) + ?
?
j wij?j(c))?
n exp(??(n, ai) + ?
?
j wij?j(n))
. (3)
The fixed point equation for i /? L can be obtained
by removing ??(c, ai) from above.
This fixed point equation is solved by an itera-
tive computation. In the actual implementation, we
represent ?i with a linear combination of the dis-
crete Tchebycheff polynomials (Tanaka and Morita,
1996). Details on the Potts model and its computa-
tion can be found in the literature (Nishimori, 2001).
After the computation, we obtain the function?
i ?i(ci). When the number of classes is 2, the Potts
model in this formulation is equivalent to the mean-
field Ising model (Nishimori, 2001).
3.2 Relation to Other Models
This Potts model with the mean-field approximation
has relation to several other models.
As is often discussed (Mackay, 2003), the min-
imization of the variational free energy (Equa-
tion (2)) is equivalent to the obtaining the factorized
model that is most similar to the maximum likeli-
hood model in terms of the Kullback-Leibler diver-
gence.
The second term of Equation (2) is the entropy
of the factorized function. Hence the optimization
problem to be solved here is a kind of the maxi-
mum entropy model with a penalty term, which cor-
responds to the first term of Equation (2).
We can find a similarity also to the PageRank al-
gorithm (Brin and Page, 1998), which has been ap-
plied also to natural language processing tasks (Mi-
halcea, 2004; Mihalcea, 2005). In the PageRank al-
gorithm, the pagerank score ri is updated as
ri = (1? d) + d
?
j
wijrj , (4)
where d is a constant (0 ? d ? 1). This update
equation consists of the first term corresponding to
random jump from an arbitrary node and the sec-
ond term corresponding to the random walk from the
neighboring node.
Let us derive the first order Taylor expansion of
Equation (3). We use the equation for i /? L and
denote the denominator by Z? , for simplicity. Since
expx ? 1 + x, we obtain
?i(c) =
exp(?
?
j wij?j(c))
Z?
?
1 + ?
?
j wij?j(c)
Z?
= 1Z?
+ ?Z?
?
j
wij?j(c). (5)
Equation (5) clearly has a quite similar form as
Equation (4). Thus, the PageRank algorithm can be
regarded as an approximation of our model. Let us
clarify the difference between the two algorithms.
The PageRank is designed for two-class classifica-
tion, while the Potts model can be used for an arbi-
trary number of classes. In this sense, the PageRank
is an approximated Ising model. The PageRank is
applicable to asymmetric graphs, while the theory
used in this paper is based on symmetric graphs.
4 Potts Model for Phrasal Semantic
Orientations
In this section, we explain our classification method,
which is applicable also to the pairs consisting of an
adjective and an unseen noun.
4.1 Construction of Lexical Networks
We construct a lexical network, which Takamura et
al. (2005) call the gloss network, by linking two
words if one word appears in the gloss of the other
word. Each link belongs to one of two groups:
294
the same-orientation links SL and the different-
orientation links DL.
If a negation word (e.g., nai, for Japanese) follows
a word in the gloss of the other word, the link is a
different-orientation link. Otherwise the links is a
same-orientation link1.
We next set weights W = (wij) to links :
wij =
?
??
??
1?
d(i)d(j)
(lij ? SL)
? 1?
d(i)d(j)
(lij ? DL)
0 otherwise
, (6)
where lij denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0.
4.2 Classification of Phrases
Takamura et al (2005) used the Ising model to ex-
tract semantic orientations of words (not phrases).
We extend their idea and use the Potts model to ex-
tract semantic orientations of phrasal expressions.
Given an adjective, the decision remaining to be
made in classification of phrasal expressions con-
cerns nouns. We therefore estimate the state of the
nodes on the lexical network for each adjective. The
nouns paring with the given adjective in the train-
ing data are regarded as seed words, which we call
seen words, while the words that did not appear in
the training data are referred to as unseen words.
We use the mean-field method to estimate the
state of the system. If the probability ?i(c) of a vari-
able being positive (negative, neutral) is the highest
of the three classes, then the word corresponding to
the variable is classified as a positive (negative, neu-
tral) word.
We explain the reason why we use the Potts model
instead of the Ising model. While only two classes
(i.e., positive and negative) can be modeled by the
Ising model, three classes (i.e., positive, negative
and neutral) can be modelled by the Potts model.
For the semantic orientations of words, all the words
are sorted in the order of the average orientation
value, equivalently the probability of the word be-
ing positive. Therefore, even if the neutral class is
1For English data, a negation should precede a word, in or-
der for the corresponding link to be a different-orientation link.
not explicitly incorporated, we can manually deter-
mine two thresholds that define respectively the pos-
itive/neutral and negative/neutral boundaries. For
the semantic orientations of phrasal expressions,
however, it is impractical to manually determine
the thresholds for each of the numerous adjectives.
Therefore, we have to incorporate the neutral class
using the Potts model.
For some adjectives, the semantic orientation is
constant regardless of the nouns. We need not use
the Potts model for those unambiguous adjectives.
We thus propose the following two-step classifica-
tion procedure for a given noun-adjective pair <
n, a >.
1. if the semantic orientation of all the instances
with a in L is c, then classify < n, a > into c.
2. otherwise, use the Potts model.
We can also construct a probability model for
each noun to deal with unseen adjectives. However,
we focus on the unseen nouns in this paper, because
our dataset has many more nouns than adjectives.
4.3 Hyper-parameter Prediction
The performance of the proposed method largely de-
pends on the value of hyper-parameter ?. In order to
make the method more practical, we propose a cri-
terion for determining its value.
Takamura et al (2005) proposed two kinds of cri-
teria. One of the two criteria is an approximated
leave-one-out error rate and can be used only when a
large labeled dataset is available. The other is a no-
tion from statistical physics, that is, magnetization:
m =
?
i
x?i/N. (7)
At a high temperature, variables are randomly ori-
ented (paramagnetic phase, m ? 0). At a low
temperature, most of the variables have the same
direction (ferromagnetic phase, m 6= 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, variables are lo-
cally polarized; strongly connected nodes have the
same polarity, but not in a global way. Intuitively,
the state of the lexical network is locally polarized.
295
Therefore, they calculate values of m with several
different values of ? and select the value just before
the phase transition.
Since we cannot expect a large labeled dataset
to be available for each adjective, we use not
the approximated leave-one-out error rate, but the
magnetization-like criterion. However, the magne-
tization above is defined for the Ising model. We
therefore consider that the phase transition has oc-
curred, if a certain class c begins to be favored all
over the system. In practice, when the maximum of
the spatial averages of the approximated probabil-
ities maxc
?
i ?i(c)/N exceeds a threshold during
increasing ?, we consider that the phase transition
has occurred. We select the value of ? slightly be-
fore the phase transition.
4.4 Enlarging Seed Word Set
We usually have only a few seed words for a given
adjective. Enlarging the set of seed words will in-
crease the classification performance. Therefore, we
automatically classify unlabeled pairs by means of
an existing method and use the classified instances
as seeds.
As an existing classifier, we use LVM. Their
model can classify instances that consist of a seen
noun and a seen adjective, but are unseen as a pair.
Although we could classify and use all the nouns
that appeared in the training data (with an adjective
which is different from the given one), we do not
adopt such an alternative, because it will incorporate
even non-collocating pairs such as ?green+idea? into
seeds, resulting in possible degradation of classifi-
cation performance. Therefore, we sample unseen
pairs consisting of a seen noun and a seen adjective
from a corpus, classify the pairs with the latent vari-
able model, and add them to the seed set. The en-
larged seed set consists of pairs used in newspaper
articles and does not include non-collocating pairs.
5 Experiments
5.1 Dataset
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper arti-
cles (1995) written in Japanese, and annotated the
pairs with semantic orientation tags : positive, neu-
tral or negative. We thus obtained the labeled dataset
consisting of 12066 pair instances (7416 different
pairs). The dataset contains 4459 negative instances,
4252 neutral instances, and 3355 positive instances.
The number of distinct nouns is 4770 and the num-
ber of distinct adjectives is 384. To check the inter-
annotator agreement between two annotators, we
calculated ? statistics, which was 0.6402. This value
is allowable, but not quite high. However, positive-
negative disagreement is observed for only 0.7% of
the data. In other words, this statistics means that
the task of extracting neutral examples, which has
hardly been explored, is intrinsically difficult.
We should note that the judgment in annotation
depends on which perspective the annotator takes;
?high+salary? is positive from employee?s perspec-
tive, but negative from employer?s perspective. The
annotators are supposed to take a perspective subjec-
tively. Our attempt is to imitate annotator?s decision.
To construct a classifier that matches the decision of
the average person, we also have to address how to
create an average corpus. We do not pursue this is-
sue because it is out of the scope of the paper.
As unlabeled data, we extracted approximately
65,000 pairs for each iteration of the 10-fold cross-
validation, from the same news source.
The average number of seed nouns for each am-
biguous adjective was respectively 104 in the la-
beled seed set and 264 in the labeled+unlabeled seed
set. Please note that these figures are counted for
only ambiguous adjectives. Usually ambiguous ad-
jectives are more frequent than unambiguous adjec-
tives.
5.2 Experimental Settings
We employ 10-fold cross-validation to obtain the
averaged classification accuracy. We split the data
such that there is no overlapping pair (i.e., any pair
in the training data does not appear in the test data).
Hyperparameter ? was set to 1000, which is very
large since we regard the labels in the seed set is
reliable. For the seed words added by the classifier,
lower ? can be better. Determining a good value for
? is regarded as future work.
Hyperparameter ? is automatically selected from
2Although Kanayama and Nasukawa (2006) that ? for their
dataset similar to ours was 0.83, this value cannot be directly
compared with our value because their dataset includes both in-
dividual words and pairs of words.
296
{0.1, 0.2, ? ? ?, 2.5} for each adjective and each fold
of the cross-validation using the prediction method
described in Section 4.3.
5.3 Results
The results of the classification experiments are
summarized in Table 1.
The proposed method succeeded in classifying,
with approximately 65% in accuracy, those phrases
consisting of an ambiguous adjective and an unseen
noun, which could not be classified with existing
computational models such as LVM.
Incorporation of unlabeled data improves accu-
racy by 15.5 points for pairs consisting of a seen
noun and an ambiguous adjective, and by 3.5 points
for pairs consisting of an unseen noun and an am-
biguous adjective, approximately. The reason why
the former obtained high increase is that pairs with
an ambiguous adjective3 are usually frequent and
likely to be found in the added unlabeled dataset.
If we regard this classification task as binary clas-
sification problems where we are to classify in-
stances into one class or not, we obtain three accu-
racies: 90.76% for positive, 81.75% for neutral, and
86.85% for negative. This results suggests the iden-
tification of neutral instances is relatively difficult.
Next we compare the proposed method with
LVM. The latent variable method is applicable only
to instance pairs consisting of an adjective and a
seen noun. Therefore, we computed the accuracy
for 6586 instances using the latent variable method
and obtained 80.76 %. The corresponding accuracy
by our method was 80.93%. This comparison shows
that our method is better than or at least comparable
to the latent variable method. However, we have to
note that this accuracy of the proposed method was
computed using the unlabeled data classified by the
latent variable method.
5.4 Discussion
There are still 3320 (=12066-8746) word pairs
which could not be classified, because there are no
entries for those words in the dictionary. However,
the main cause of this problem is word segmenta-
3Seen nouns are observed in both the training and the test
datasets because they are frequent. Ambiguous adjectives are
often-used adjectives such as ?large?, ?small?, ?high?, and
?low?.
tion, since many compound nouns and exceedingly-
subdivided morphemes are not in dictionaries. An
appropriate mapping from the words found in cor-
pus to entries of a dictionary will solve this problem.
We found a number of proper nouns, many of which
are not in the dictionary. By estimating a class of a
proper noun and finding the words that matches the
class in the dictionary, we can predict the semantic
orientations of the proper noun based on the orienta-
tions of the found words.
In order to see the overall tendency of errors, we
calculated the confusion matrices both for pairs of
an ambiguous adjective and a seen noun, and for
pairs of an ambiguous adjective and an unseen noun
(Table 2). The proposed method works quite well for
positive/negative classification, though it finds still
some difficulty in correctly classifying neutral in-
stances even after enhanced with the unlabeled data.
In order to qualitatively evaluate the method,
we list several word pairs below. These word
pairs are classified by the Potts model with the la-
beled+unlabeled seed set. All nouns are unseen;
they did not appear in the original training dataset.
Please note again that the actual data is Japanese.
positive instances
noun adjective
cost low
basic price low
loss little
intelligence high
educational background high
contagion not-happening
version new
cafe many
salary high
commission low
negative instances
noun adjective
damage heavy
chance little
terrorist many
trouble many
variation little
capacity small
salary low
disaster many
disappointment big
knowledge little
For example, although both ?salary? and ?com-
mission? are kinds of money, our method captures
297
Table 1: Classification accuracies (%) for various seed sets and test datasets. ?Labeled? seed set corresponds
to the set of manually labeled pairs. ?Labeled+unlabeled? seed set corresponds to the union of ?labeled? seed
set and the set of pairs labeled by LVM. ?Seen nouns? for test are the nouns that appeared in the training
data, while ?unseen nouns? are the nouns that did not appear in the training dataset?. Please note that seen
pairs are excluded from the test data. ?Unambiguous? adjectives corresponds to the pairs with an adjective
which has a unique orientation in the original training dataset, while ?ambiguous? adjectives corresponds to
the pairs with an adjective which has more than one orientation in the original training dataset.
seed\test seen nouns unseen nouns total
labeled 68.24 73.70 69.59
(4494/6586) (1592/2160) (6086/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 61.65 94.85 61.85
(1166/1188) (3328/5398) (736/776) (856/1384)
labeled+unlabeled 80.93 75.88 79.68
(5330/6586) (1639/2160) (6969/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 77.14 94.85 65.25
(1166/1188) (4164/5398) (736/776) (903/1384)
Table 2: Confusion matrices of classification result with labeled+unlabeled seed set
Potts model
seen nouns unseen nouns
positive neutral negative sum positive neutral negative sum
positive 964 254 60 1278 126 84 30 240
Gold standard neutral 198 1656 286 2140 60 427 104 591
negative 39 397 1544 1980 46 157 350 553
sum 1201 2307 1890 5398 232 668 484 1384
the difference between them; ?high salary? is posi-
tive, while ?low (cheap) commission? is also posi-
tive.
6 Conclusion
We proposed a method for extracting semantic ori-
entations of phrases (pairs of an adjective and a
noun). For each adjective, we constructed a Potts
system, which is actually a lexical network extracted
from glosses in a dictionary. We empirically showed
that the proposed method works well in terms of
classification accuracy.
Future work includes the following:
? We assumed that each word has a semantic ori-
entation. However, word senses and subjectiv-
ity have strong interaction (Wiebe and Mihal-
cea, 2006).
? The value of ? must be properly set, because
lower ? can be better for the seed words added
by the classifier,
? To address word-segmentation problem dis-
cussed in Section 5.3, we can utilize the fact
that the heads of compound nouns often inherit
the property determining the semantic orienta-
tion when combined with an adjective.
? The semantic orientations of pairs consisting of
a proper noun will be estimated from the named
entity classes of the proper nouns such as per-
son name and organization.
298
References
Faye Baron and Graeme Hirst. 2004. Collocations as
cues to semantic orientation. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss
analysis. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM?05), pages 617?624.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics and the
8th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 174?181.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute of
Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC?04), volume IV, pages
1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?06), pages 355?363.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelligence,
SLUD-33, pages 45?50.
Zhongzhu Liu, Jun Luo, and Chenggang Shao. 2001.
Potts model for exaggeration of a simple rumor trans-
mitted by recreant rumormongers. Physical Review E,
64:046134,1?046134,9.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Rada Mihalcea. 2004. Graph-based ranking algorithms
for sentence extraction, applied to text summarization.
In The Companion Volume to the Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, (ACL?04), pages 170?173.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Conference on Human Language Technology
/ Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 411?418.
Hidetoshi Nishimori. 2001. Statistical Physics of Spin
Glasses and Information Processing. Oxford Univer-
sity Press.
Frank Z. Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143?
177.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 133?140.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orientations
of phrases. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL?06).
Kazuyuki Tanaka and Tohru Morita. 1996. Application
of cluster variation method to image restoration prob-
lem. In Theory and Applications of the Cluster Vari-
ation and Path Probability Methods, pages 353?373.
Plenum Press, New York.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), pages 417?424.
Janyce M. Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), pages 1065?
1072.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of joint confer-
ence on Human Language Technology / Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP?05), pages 347?354.
Fa-Yueh Wu. 1982. The potts model. Reviews of Mod-
ern Physics, 54(1):235?268.
299
Proceedings of the 43rd Annual Meeting of the ACL, pages 133?140,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Extracting Semantic Orientations of Words using Spin Model
Hiroya Takamura Takashi Inui Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, 226-8503 Japan
{takamura,oku}@pi.titech.ac.jp,
tinui@lr.pi.titech.ac.jp
Abstract
We propose a method for extracting se-
mantic orientations of words: desirable
or undesirable. Regarding semantic ori-
entations as spins of electrons, we use
the mean field approximation to compute
the approximate probability function of
the system instead of the intractable ac-
tual probability function. We also pro-
pose a criterion for parameter selection on
the basis of magnetization. Given only
a small number of seed words, the pro-
posed method extracts semantic orienta-
tions with high accuracy in the exper-
iments on English lexicon. The result
is comparable to the best value ever re-
ported.
1 Introduction
Identification of emotions (including opinions and
attitudes) in text is an important task which has a va-
riety of possible applications. For example, we can
efficiently collect opinions on a new product from
the internet, if opinions in bulletin boards are auto-
matically identified. We will also be able to grasp
people?s attitudes in questionnaire, without actually
reading all the responds.
An important resource in realizing such identifi-
cation tasks is a list of words with semantic orienta-
tion: positive or negative (desirable or undesirable).
Frequent appearance of positive words in a docu-
ment implies that the writer of the document would
have a positive attitude on the topic. The goal of this
paper is to propose a method for automatically cre-
ating such a word list from glosses (i.e., definition
or explanation sentences ) in a dictionary, as well as
from a thesaurus and a corpus. For this purpose, we
use spin model, which is a model for a set of elec-
trons with spins. Just as each electron has a direc-
tion of spin (up or down), each word has a semantic
orientation (positive or negative). We therefore re-
gard words as a set of electrons and apply the mean
field approximation to compute the average orienta-
tion of each word. We also propose a criterion for
parameter selection on the basis of magnetization, a
notion in statistical physics. Magnetization indicates
the global tendency of polarization.
We empirically show that the proposed method
works well even with a small number of seed words.
2 Related Work
Turney and Littman (2003) proposed two algorithms
for extraction of semantic orientations of words. To
calculate the association strength of a word with pos-
itive (negative) seed words, they used the number
of hits returned by a search engine, with a query
consisting of the word and one of seed words (e.g.,
?word NEAR good?, ?word NEAR bad?). They re-
garded the difference of two association strengths as
a measure of semantic orientation. They also pro-
posed to use Latent Semantic Analysis to compute
the association strength with seed words. An em-
pirical evaluation was conducted on 3596 words ex-
tracted from General Inquirer (Stone et al, 1966).
Hatzivassiloglou and McKeown (1997) focused
on conjunctive expressions such as ?simple and
133
well-received? and ?simplistic but well-received?,
where the former pair of words tend to have the same
semantic orientation, and the latter tend to have the
opposite orientation. They first classify each con-
junctive expression into the same-orientation class
or the different-orientation class. They then use the
classified expressions to cluster words into the pos-
itive class and the negative class. The experiments
were conducted with the dataset that they created on
their own. Evaluation was limited to adjectives.
Kobayashi et al (2001) proposed a method for ex-
tracting semantic orientations of words with boot-
strapping. The semantic orientation of a word is
determined on the basis of its gloss, if any of their
52 hand-crafted rules is applicable to the sentence.
Rules are applied iteratively in the bootstrapping
framework. Although Kobayashi et al?s work pro-
vided an accurate investigation on this task and in-
spired our work, it has drawbacks: low recall and
language dependency. They reported that the seman-
tic orientations of only 113 words are extracted with
precision 84.1% (the low recall is due partly to their
large set of seed words (1187 words)). The hand-
crafted rules are only for Japanese.
Kamps et al (2004) constructed a network by
connecting each pair of synonymous words provided
by WordNet (Fellbaum, 1998), and then used the
shortest paths to two seed words ?good? and ?bad?
to obtain the semantic orientation of a word. Limi-
tations of their method are that a synonymy dictio-
nary is required, that antonym relations cannot be
incorporated into the model. Their evaluation is re-
stricted to adjectives. The method proposed by Hu
and Liu (2004) is quite similar to the shortest-path
method. Hu and Liu?s method iteratively determines
the semantic orientations of the words neighboring
any of the seed words and enlarges the seed word
set in a bootstrapping manner.
Subjective words are often semantically oriented.
Wiebe (2000) used a learning method to collect sub-
jective adjectives from corpora. Riloff et al (2003)
focused on the collection of subjective nouns.
We later compare our method with Turney and
Littman?s method and Kamps et al?s method.
The other pieces of research work mentioned
above are related to ours, but their objectives are dif-
ferent from ours.
3 Spin Model and Mean Field
Approximation
We give a brief introduction to the spin model
and the mean field approximation, which are well-
studied subjects both in the statistical mechanics
and the machine learning communities (Geman and
Geman, 1984; Inoue and Carlucci, 2001; Mackay,
2003).
A spin system is an array of N electrons, each of
which has a spin with one of two values ?+1 (up)? or
??1 (down)?. Two electrons next to each other en-
ergetically tend to have the same spin. This model
is called the Ising spin model, or simply the spin
model (Chandler, 1987). The energy function of a
spin system can be represented as
E(x,W ) = ?12
?
ij
wijxixj , (1)
where xi and xj (? x) are spins of electrons i and j,
matrix W = {wij} represents weights between two
electrons.
In a spin system, the variable vector x follows the
Boltzmann distribution :
P (x|W ) = exp(??E(x,W ))Z(W ) , (2)
where Z(W ) = ?x exp(??E(x,W )) is the nor-
malization factor, which is called the partition
function and ? is a constant called the inverse-
temperature. As this distribution function suggests,
a configuration with a higher energy value has a
smaller probability.
Although we have a distribution function, com-
puting various probability values is computationally
difficult. The bottleneck is the evaluation of Z(W ),
since there are 2N configurations of spins in this sys-
tem.
We therefore approximate P (x|W ) with a simple
function Q(x; ?). The set of parameters ? for Q, is
determined such that Q(x; ?) becomes as similar to
P (x|W ) as possible. As a measure for the distance
between P and Q, the variational free energy F is
often used, which is defined as the difference be-
tween the mean energy with respect to Q and the
entropy of Q :
F (?) = ?
?
x
Q(x; ?)E(x;W )
134
?
(
?
?
x
Q(x; ?) logQ(x; ?)
)
. (3)
The parameters ? that minimizes the variational free
energy will be chosen. It has been shown that mini-
mizing F is equivalent to minimizing the Kullback-
Leibler divergence between P and Q (Mackay,
2003).
We next assume that the function Q(x; ?) has the
factorial form :
Q(x; ?) =
?
i
Q(xi; ?i). (4)
Simple substitution and transformation leads us to
the following variational free energy :
F (?) = ??2
?
ij
wij x?ix?j
?
?
i
(
?
?
xi
Q(xi; ?i) logQ(xi; ?i)
)
.
(5)
With the usual method of Lagrange multipliers,
we obtain the mean field equation :
x?i =
?
xi xi exp
(
?xi
?
j wij x?j
)
?
xi exp
(
?xi
?
j wij x?j
) . (6)
This equation is solved by the iterative update rule :
x?newi =
?
xi xi exp
(
?xi
?
j wij x?oldj
)
?
xi exp
(
?xi
?
j wij x?oldj
) . (7)
4 Extraction of Semantic Orientation of
Words with Spin Model
We use the spin model to extract semantic orienta-
tions of words.
Each spin has a direction taking one of two values:
up or down. Two neighboring spins tend to have the
same direction from a energetic reason. Regarding
each word as an electron and its semantic orientation
as the spin of the electron, we construct a lexical net-
work by connecting two words if, for example, one
word appears in the gloss of the other word. Intu-
ition behind this is that if a word is semantically ori-
ented in one direction, then the words in its gloss
tend to be oriented in the same direction.
Using the mean-field method developed in statis-
tical mechanics, we determine the semantic orienta-
tions on the network in a global manner. The global
optimization enables the incorporation of possibly
noisy resources such as glosses and corpora, while
existing simple methods such as the shortest-path
method and the bootstrapping method cannot work
in the presence of such noisy evidences. Those
methods depend on less-noisy data such as a the-
saurus.
4.1 Construction of Lexical Networks
We construct a lexical network by linking two words
if one word appears in the gloss of the other word.
Each link belongs to one of two groups: the same-
orientation links SL and the different-orientation
links DL. If at least one word precedes a nega-
tion word (e.g., not) in the gloss of the other word,
the link is a different-orientation link. Otherwise the
links is a same-orientation link.
We next set weights W = (wij) to links :
wij =
?
???
???
1?
d(i)d(j) (lij ? SL)
? 1?d(i)d(j) (lij ? DL)
0 otherwise
, (8)
where lij denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0. We call this net-
work the gloss network (G).
We construct another network, the gloss-
thesaurus network (GT), by linking synonyms,
antonyms and hypernyms, in addition to the the
above linked words. Only antonym links are in DL.
We enhance the gloss-thesaurus network with
cooccurrence information extracted from corpus. As
mentioned in Section 2, Hatzivassiloglou and McK-
eown (1997) used conjunctive expressions in corpus.
Following their method, we connect two adjectives
if the adjectives appear in a conjunctive form in the
corpus. If the adjectives are connected by ?and?, the
link belongs to SL. If they are connected by ?but?,
the link belongs to DL. We call this network the
gloss-thesaurus-corpus network (GTC).
135
4.2 Extraction of Orientations
We suppose that a small number of seed words are
given. In other words, we know beforehand the se-
mantic orientations of those given words. We incor-
porate this small labeled dataset by modifying the
previous update rule.
Instead of ?E(x,W ) in Equation (2), we use the
following function H(?, x,W ) :
H(?, x,W ) = ??2
?
ij
wijxixj + ?
?
i?L
(xi ? ai)2,
(9)
where L is the set of seed words, ai is the orientation
of seed word i, and ? is a positive constant. This
expression means that if xi (i ? L) is different from
ai, the state is penalized.
Using function H , we obtain the new update rule
for xi (i ? L) :
x?newi =
?
xi xi exp
(
?xisoldi ? ?(xi ? ai)2
)
?
xi exp
(
?xisoldi ? ?(xi ? ai)2
) ,
(10)
where soldi =
?
j wij x?oldj . x?oldi and x?newi are the
averages of xi respectively before and after update.
What is discussed here was constructed with the ref-
erence to work by Inoue and Carlucci (2001), in
which they applied the spin glass model to image
restoration.
Initially, the averages of the seed words are set
according to their given orientations. The other av-
erages are set to 0.
When the difference in the value of the variational
free energy is smaller than a threshold before and
after update, we regard computation converged.
The words with high final average values are clas-
sified as positive words. The words with low final
average values are classified as negative words.
4.3 Hyper-parameter Prediction
The performance of the proposed method largely de-
pends on the value of hyper-parameter ?. In order to
make the method more practical, we propose criteria
for determining its value.
When a large labeled dataset is available, we can
obtain a reliable pseudo leave-one-out error rate :
1
|L|
?
i?L
[aix??i], (11)
where [t] is 1 if t is negative, otherwise 0, and x??i is
calculated with the right-hand-side of Equation (6),
where the penalty term ?(x?i?ai)2 in Equation (10)
is ignored. We choose ? that minimizes this value.
However, when a large amount of labeled data is
unavailable, the value of pseudo leave-one-out error
rate is not reliable. In such cases, we use magnetiza-
tion m for hyper-parameter prediction :
m = 1N
?
i
x?i. (12)
At a high temperature, spins are randomly ori-
ented (paramagnetic phase, m ? 0). At a low
temperature, most of the spins have the same di-
rection (ferromagnetic phase, m 6= 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, spins are locally
polarized; strongly connected spins have the same
polarity, but not in a global way.
Intuitively, the state of the lexical network is lo-
cally polarized. Therefore, we calculate values of
m with several different values of ? and select the
value just before the phase transition.
4.4 Discussion on the Model
In our model, the semantic orientations of words
are determined according to the averages values of
the spins. Despite the heuristic flavor of this deci-
sion rule, it has a theoretical background related to
maximizer of posterior marginal (MPM) estimation,
or ?finite-temperature decoding? (Iba, 1999; Marro-
quin, 1985). In MPM, the average is the marginal
distribution over xi obtained from the distribution
over x. We should note that the finite-temperature
decoding is quite different from annealing type algo-
rithms or ?zero-temperature decoding?, which cor-
respond to maximum a posteriori (MAP) estima-
tion and also often used in natural language process-
ing (Cowie et al, 1992).
Since the model estimation has been reduced
to simple update calculations, the proposed model
is similar to conventional spreading activation ap-
proaches, which have been applied, for example, to
word sense disambiguation (Veronis and Ide, 1990).
Actually, the proposed model can be regarded as a
spreading activation model with a specific update
136
rule, as long as we are dealing with 2-class model
(2-Ising model).
However, there are some advantages in our mod-
elling. The largest advantage is its theoretical back-
ground. We have an objective function and its ap-
proximation method. We thus have a measure of
goodness in model estimation and can use another
better approximation method, such as Bethe approx-
imation (Tanaka et al, 2003). The theory tells
us which update rule to use. We also have a no-
tion of magnetization, which can be used for hyper-
parameter estimation. We can use a plenty of knowl-
edge, methods and algorithms developed in the field
of statistical mechanics. We can also extend our
model to a multiclass model (Q-Ising model).
Another interesting point is the relation to maxi-
mum entropy model (Berger et al, 1996), which is
popular in the natural language processing commu-
nity. Our model can be obtained by maximizing the
entropy of the probability distribution Q(x) under
constraints regarding the energy function.
5 Experiments
We used glosses, synonyms, antonyms and hyper-
nyms of WordNet (Fellbaum, 1998) to construct an
English lexical network. For part-of-speech tag-
ging and lemmatization of glosses, we used Tree-
Tagger (Schmid, 1994). 35 stopwords (quite fre-
quent words such as ?be? and ?have?) are removed
from the lexical network. Negation words include
33 words. In addition to usual negation words such
as ?not? and ?never?, we include words and phrases
which mean negation in a general sense, such as
?free from? and ?lack of?. The whole network con-
sists of approximately 88,000 words. We collected
804 conjunctive expressions from Wall Street Jour-
nal and Brown corpus as described in Section 4.2.
The labeled dataset used as a gold standard is
General Inquirer lexicon (Stone et al, 1966) as in the
work by Turney and Littman (2003). We extracted
the words tagged with ?Positiv? or ?Negativ?, and
reduced multiple-entry words to single entries. As a
result, we obtained 3596 words (1616 positive words
and 1980 negative words) 1. In the computation of
1Although we preprocessed in the same way as Turney and
Littman, there is a slight difference between their dataset and
our dataset. However, we believe this difference is insignificant.
Table 1: Classification accuracy (%) with various
networks and four different sets of seed words. In
the parentheses, the predicted value of ? is written.
For cv, no value is written for ?, since 10 different
values are obtained.
seeds GTC GT G
cv 90.8 (?) 90.9 (?) 86.9 (?)
14 81.9 (1.0) 80.2 (1.0) 76.2 (1.0)
4 73.8 (0.9) 73.7 (1.0) 65.2 (0.9)
2 74.6 (1.0) 61.8 (1.0) 65.7 (1.0)
accuracy, seed words are eliminated from these 3596
words.
We conducted experiments with different values
of ? from 0.1 to 2.0, with the interval 0.1, and pre-
dicted the best value as explained in Section 4.3. The
threshold of the magnetization for hyper-parameter
estimation is set to 1.0 ? 10?5. That is, the pre-
dicted optimal value of ? is the largest ? whose
corresponding magnetization does not exceeds the
threshold value.
We performed 10-fold cross validation as well as
experiments with fixed seed words. The fixed seed
words are the ones used by Turney and Littman: 14
seed words {good, nice, excellent, positive, fortu-
nate, correct, superior, bad, nasty, poor, negative,
unfortunate, wrong, inferior}; 4 seed words {good,
superior, bad, inferior}; 2 seed words {good, bad}.
5.1 Classification Accuracy
Table 1 shows the accuracy values of semantic ori-
entation classification for four different sets of seed
words and various networks. In the table, cv corre-
sponds to the result of 10-fold cross validation, in
which case we use the pseudo leave-one-out error
for hyper-parameter estimation, while in other cases
we use magnetization.
In most cases, the synonyms and the cooccurrence
information from corpus improve accuracy. The
only exception is the case of 2 seed words, in which
G performs better than GT. One possible reason of
this inversion is that the computation is trapped in a
local optimum, since a small number of seed words
leave a relatively large degree of freedom in the so-
lution space, resulting in more local optimal points.
We compare our results with Turney and
137
Table 2: Actual best classification accuracy (%)
with various networks and four different sets of seed
words. In the parenthesis, the actual best value of ?
is written, except for cv.
seeds GTC GT G
cv 91.5 (?) 91.5 (?) 87.0 (?)
14 81.9 (1.0) 80.2 (1.0) 76.2 (1.0)
4 74.4 (0.6) 74.4 (0.6) 65.3 (0.8)
2 75.2 (0.8) 61.9 (0.8) 67.5 (0.5)
Littman?s results. With 14 seed words, they achieved
61.26% for a small corpus (approx. 1? 107 words),
76.06% for a medium-sized corpus (approx. 2?109
words), 82.84% for a large corpus (approx. 1?1011
words).
Without a corpus nor a thesaurus (but with glosses
in a dictionary), we obtained accuracy that is compa-
rable to Turney and Littman?s with a medium-sized
corpus. When we enhance the lexical network with
corpus and thesaurus, our result is comparable to
Turney and Littman?s with a large corpus.
5.2 Prediction of ?
We examine how accurately our prediction method
for ? works by comparing Table 1 above and Ta-
ble 2 below. Our method predicts good ? quite well
especially for 14 seed words. For small numbers of
seed words, our method using magnetization tends
to predict a little larger value.
We also display the figure of magnetization and
accuracy in Figure 1. We can see that the sharp
change of magnetization occurs at around ? = 1.0
(phrase transition). At almost the same point, the
classification accuracy reaches the peak.
5.3 Precision for the Words with High
Confidence
We next evaluate the proposed method in terms of
precision for the words that are classified with high
confidence. We regard the absolute value of each
average as a confidence measure and evaluate the top
words with the highest absolute values of averages.
The result of this experiment is shown in Figure 2,
for 14 seed words as an example. The top 1000
words achieved more than 92% accuracy. This re-
sult shows that the absolute value of each average
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1  2  3  4  5  6  7  8  9  10
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
M
ag
ne
tiz
ati
on
Ac
cu
ra
cy
Beta
magnetization
accuracy
Figure 1: Example of magnetization and classifica-
tion accuracy(14 seed words).
 75
 80
 85
 90
 95
 100
 0  500  1000  1500  2000  2500  3000  3500  4000
Pr
ec
isi
on
Number of selected words
GTC
GT
G
Figure 2: Precision (%) with 14 seed words.
138
Table 3: Precision (%) for selected adjectives.
Comparison between the proposed method and the
shortest-path method.
seeds proposed short. path
14 73.4 (1.0) 70.8
4 71.0 (1.0) 64.9
2 68.2 (1.0) 66.0
Table 4: Precision (%) for adjectives. Comparison
between the proposed method and the bootstrapping
method.
seeds proposed bootstrap
14 83.6 (0.8) 72.8
4 82.3 (0.9) 73.2
2 83.5 (0.7) 71.1
can work as a confidence measure of classification.
5.4 Comparison with other methods
In order to further investigate the model, we conduct
experiments in restricted settings.
We first construct a lexical network using only
synonyms. We compare the spin model with
the shortest-path method proposed by Kamps et
al. (2004) on this network, because the shortest-
path method cannot incorporate negative links of
antonyms. We also restrict the test data to 697 ad-
jectives, which is the number of examples that the
shortest-path method can assign a non-zero orien-
tation value. Since the shortest-path method is de-
signed for 2 seed words, the method is extended
to use the average shortest-path lengths for 4 seed
words and 14 seed words. Table 3 shows the re-
sult. Since the only difference is their algorithms,
we can conclude that the global optimization of the
spin model works well for the semantic orientation
extraction.
We next compare the proposed method with a
simple bootstrapping method proposed by Hu and
Liu (2004). We construct a lexical network using
synonyms and antonyms. We restrict the test data
to 1470 adjectives for comparison of methods. The
result in Table 4 also shows that the global optimiza-
tion of the spin model works well for the semantic
orientation extraction.
We also tested the shortest path method and the
bootstrapping method on GTC and GT, and obtained
low accuracies as expected in the discussion in Sec-
tion 4.
5.5 Error Analysis
We investigated a number of errors and concluded
that there were mainly three types of errors.
One is the ambiguity of word senses. For exam-
ple, one of the glosses of ?costly?is ?entailing great
loss or sacrifice?. The word ?great? here means
?large?, although it usually means ?outstanding? and
is positively oriented.
Another is lack of structural information. For ex-
ample, ?arrogance? means ?overbearing pride evi-
denced by a superior manner toward the weak?. Al-
though ?arrogance? is mistakingly predicted as posi-
tive due to the word ?superior?, what is superior here
is ?manner?.
The last one is idiomatic expressions. For exam-
ple, although ?brag? means ?show off?, neither of
?show? and ?off? has the negative orientation. Id-
iomatic expressions often does not inherit the se-
mantic orientation from or to the words in the gloss.
The current model cannot deal with these types of
errors. We leave their solutions as future work.
6 Conclusion and Future Work
We proposed a method for extracting semantic ori-
entations of words. In the proposed method, we re-
garded semantic orientations as spins of electrons,
and used the mean field approximation to compute
the approximate probability function of the system
instead of the intractable actual probability function.
We succeeded in extracting semantic orientations
with high accuracy, even when only a small number
of seed words are available.
There are a number of directions for future work.
One is the incorporation of syntactic information.
Since the importance of each word consisting a gloss
depends on its syntactic role. syntactic information
in glosses should be useful for classification.
Another is active learning. To decrease the
amount of manual tagging for seed words, an active
learning scheme is desired, in which a small number
of good seed words are automatically selected.
Although our model can easily extended to a
139
multi-state model, the effectiveness of using such a
multi-state model has not been shown yet.
Our model uses only the tendency of having the
same orientation. Therefore we can extract seman-
tic orientations of new words that are not listed in
a dictionary. The validation of such extension will
widen the possibility of application of our method.
Larger corpora such as web data will improve per-
formance. The combination of our method and the
method by Turney and Littman (2003) is promising.
Finally, we believe that the proposed model is ap-
plicable to other tasks in computational linguistics.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
David Chandler. 1987. Introduction to Modern Statisti-
cal Mechanics. Oxford University Press.
Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lexi-
cal disambiguation using simulated annealing. In Pro-
ceedings of the 14th conference on Computational lin-
guistics, volume 1, pages 359?365.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database, Language, Speech, and Communi-
cation Series. MIT Press.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the Thirty-Fifth Annual Meet-
ing of the Association for Computational Linguistics
and the Eighth Conference of the European Chapter of
the Association for Computational Linguistics, pages
174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004
ACM SIGKDD international conference on Knowl-
edge discovery and data mining (KDD-2004), pages
168?177.
Yukito Iba. 1999. The nishimori line and bayesian statis-
tics. Journal of Physics A: Mathematical and General,
pages 3875?3888.
Junichi Inoue and Domenico M. Carlucci. 2001. Image
restoration using the q-ising spin glass. Physical Re-
view E, 64:036121?1 ? 036121?18.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to mea-
sure semantic orientation of adjectives. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115?1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelligence,
SLUD-33, pages 45?50.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Jose L. Marroquin. 1985. Optimal bayesian estima-
tors for image segmentation and surface reconstruc-
tion. Technical Report A.I. Memo 839, Massachusetts
Institute of Technology.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Seventh Con-
ference on Natural Language Learning (CoNLL-03),
pages 25?32.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, pages 44?49.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. The MIT
Press.
Kazuyuki Tanaka, Junichi Inoue, and Mike Titterington.
2003. Probabilistic image processing by means of the
bethe approximation for the q-ising model. Journal
of Physics A: Mathematical and General, 36:11023?
11035.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Jean Veronis and Nancy M. Ide. 1990. Word sense dis-
ambiguation with very large neural networks extracted
from machine readable dictionaries. In Proceedings
of the 13th Conference on Computational Linguistics,
volume 2, pages 389?394.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence (AAAI-
2000), pages 735?740.
140
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 37?44,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Investigating the Characteristics of Causal Relations in Japanese Text
Takashi Inui and Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
tinui@lr.pi.titech.ac.jp, oku@pi.titech.ac.jp
Abstract
We investigated of the characteristics of
in-text causal relations. We designed
causal relation tags. With our designed
tag set, three annotators annotated 750
Japanese newspaper articles. Then, using
the annotated corpus, we investigated the
causal relation instances from some view-
points. Our quantitative study shows that
what amount of causal relation instances
are present, where these relation instances
are present, and which types of linguistic
expressions are used for expressing these
relation instances in text.
1 Introduction
For many applications of natural language tech-
niques such as question-answering systems and di-
alogue systems, acquiring knowledge about causal
relations is one central issue. In recent researches,
some automatic acquisition methods for causal
knowledge have been proposed (Girju, 2003; Sato et
al., 1999; Inui, 2004). They have used as knowledge
resources a large amount of electric text documents:
newspaper articles and Web documents.
To realize their knowledge acquisition methods
accurately and efficiently, it is important to know-
ing the characteristics of presence of in-text causal
relations. However, while the acquisition methods
have been improved by some researches, the char-
acteristics of presence of in-text causal relations are
still unclear: we have no empirical study about what
amount of causal relation instances exist in text and
where in text causal relation instances tend to ap-
pear.
In this work, aiming to resolve the above issues,
we create a corpus annotated with causal relation
information which is useful for investigating what
amount of causal relation instances are present and
where these instances are present in text. Given
some Japanese newspaper articles, we add our de-
signed causal relation tags to the text segments. Af-
ter creating the annotated corpus, we investigate the
causal relation instances from three viewpoints: (i)
cue phrase markers, (ii) part-of-speech information,
and (iii) positions in sentences.
There are some pieces of previous work on anal-
ysis of in-text causal relations. However, although
causal relation instances appear in several different
ways, just a few forms have been treated in the pre-
vious studies: the verb phrase form with cue phrase
markers such as in (1a) has been mainly treated. In
contrast, we add our causal relation tags to several
types of linguistic expressions with wide coverage to
realize further analyses from above three points. Ac-
tually, we treat not only linguistic expressions with
explicit cues such as in (1a) , but also those with-
out explicit cues, i.e. implicit, as in (1b) , those
formed by noun phrases as in (1c), and those formed
between sentences as in (1d) .
(1) a.   -   -  	 
 -   - 
heavy rain-NOM fall-PAST because river-NOM rise-PAST
(explicit)
b.   -   -  
 -   - 
heavy rain-NOM fall-PUNC river-NOM rise-PAST
(implicit)
c.   -  
 -   - 
heavy rain-because of river-NOM rise-PAST
(noun phrase)
37
d.   -   -  -  
 -   - 
heavy rain-NOM fall-PAST-PUNC river-NOM rise-PAST
(between sentences)
We apply new criteria for judging whether a lin-
guistic expression includes a causal relation or not.
Generally, it is hard to define rigorously the notion
of causal relation. Therefore, in previous studies,
there have been no standard common criteria for
judging causal relations. Researchers have resorted
to annotators? subjective judgements. Our criteria
are represented in the form of linguistic templates
which the annotators apply in making their judge-
ments (see Section 3.2).
In Section 2, we will outline several previous
research efforts on in-text causal relations. In
Section 3 to Section 6, we will describe the details
of the design of our causal relation tags and the an-
notation workflow. In Section 7, using the annotated
corpus, we will then discuss the results for the inves-
tigation of characteristics of in-text causal relations.
2 Related work
Liu (2004) analyzed the differences of usages of
some Japanese connectives marking causal rela-
tions. The results are useful for accounting for an
appropriate connective for each context within the
documents. However Liu conducted no quantitative
studies.
Marcu (1997) investigated the frequency distri-
bution of English connectives including ?because?
and ?since? for implementation of rhetorical pars-
ing. However, although Marcu?s study was quanti-
tative one, Marcu treated only explicit linguistic ex-
pressions with connectives. In the Timebank corpus
(Pustejovsky et al, 2003), the causal relation infor-
mation is included. However, the information is op-
tional for implicit linguistic expressions.
Although both explicit expressions and implicit
expressions are treated in the Penn Discourse Tree-
bank (PDTB) corpus (Miltsakaki et al, 2004), no
information on causal relations is contained in this
corpus.
Altenberg (1984) investigated the frequency dis-
tribution of causal relation instances from some
viewpoints such as document style and the syntac-
tic form in English dialog data. Nishizawa (1997)
also conducted a similar work using Japanese dialog
data. Some parts of their viewpoints are overlapping
with ours. However, while their studies focused on
dialog data, our target is text documents. In fact, Al-
tenberg treated also English text documents. How-
ever, our focus in this work is Japanese.
3 Annotated information
3.1 Causal relation tags
We use three tags head, mod, and causal rel to rep-
resent the basic causal relation information. Our an-
notation scheme for events is similar to that of the
PropBank (Palmer et al, 2005). An event is re-
garded as consisting of a head element and some
modifiers. The tags head and mod are used to repre-
sent an event which forms one part of the two events
held in a causal relation. The tag causal rel is used
to represent a causal relation between two annotated
events.
Figure 1 shows an example of attaching the causal
relation information to the sentence (2a), in which a
causal relation is held between two events indicated
(2b) and (2c) . Hereafter, we denote the former
(cause) part of event as e1 and the latter (effect) part
of event as e2.
(2) a. ffCommittee-based Decision Making 
in Probabilistic Partial Parsing 
INU I  Takashi*  and INUI  Kentaro  *t 
? Department  of Artificial Intelligence, Kyushu Inst itute of Technology 
? PRESTO,  Japan Science and Technology Corporat ion 
{ t_ inu i ,  inu i}@plut  o. a i .  kyutech ,  ac.  jp  
Abst ract  
This paper explores two directions ibr the 
next step beyond the state of the art of statis- 
tical parsing: probabilistic partial parsing and 
committee-based decision making. Probabilis- 
tic partial parsing is a probabilistic extension of 
the existing notion of partial parsing~ which en- 
ables fine-grained arbitrary choice on the trade- 
off between accuracy and coverage. Committee- 
based decision making is to combine the out- 
puts from different systems to make a better 
decision. While varions committee-based tech- 
niques for NLP have recently been investigated, 
they would need to be fln'ther extended so as 
to be applicable to probabilistic partial pars- 
ing. Aiming at this coupling, this paper gives 
a general fl'amework to committee-based deci- 
sion making, which consists of a set of weight- 
ing flmctions and a combination function, and 
discusses how it can be coupled with probabilis- 
tic partial parsing. Our ext)eriments have so far 
been producing promising results. 
1 In t roduct ion  
There have been a number of attempts to use 
statistical techniques to improve parsing perfor- 
mance. While this goal has been achieved to a 
certain degree given the increasing availability 
of large tree banks, the remaining room tbr the 
improvement appears to be getting saturated 
as long as only statistical techniques are taken 
into account. This paper explores two directions 
tbr the next step beyond the state of the art of 
statistical parsing: probabilistic partial parsing 
and committee-based decision making. 
Probabilistic partial parsing is a probabilistic 
extension of the existing notion of partial pars- 
ing ( e.g. (Jensen et al, 1993)) where a parser 
selects as its output only a part of the parse tree 
that are probabilistically highly reliable. This 
decision-making scheme enables a fine-grained 
arbitrary choice on the trade-off between ac- 
curacy and coverage. Such trade-oil is impor- 
tant since there are various applications that re- 
quire reasonably high accuracy even sacrificing 
coverage. A typical example is the t)araI)hras- 
ing task embedded in summarization, sentence 
simplification (e.g. (Carroll et al, 1998)), etc. 
Enabling such trade-off" choice will make state- 
o f  the-art parsers of wider application. Partial 
parsing has also been proven useflll ibr boot- 
strapping leanfing. 
One may suspect hat the realization of par- 
tial parsing is a trivial matter in probabilistic 
parsing just because a probabilistic parser in- 
herently has the notion of "reliability" and thus 
has the trade-off:' between accuracy and cover- 
age. However, there has so far been surpris- 
ingly little research focusing on this matter and 
ahnost no work that evaluates statistical parsers 
according to their coverage-accuracy (or recall- 
precision) curves. Taking the significance of 
partial parsing into account, therefi)re in this 
paper, we evaluate parsing perfbrmance accord- 
ing tO coverage-accuracy cnrves. 
Committee-based decision making is to con> 
bine the outputs from several difl'erent systems 
(e.g. parsers) to make a better decision. Re- 
cently, there have been various attempts to at)- 
ply committee-based techniques to NLP tasks 
such as POS tagging (Halteren et al, 1998; 
Brill et al, 1998), parsing (Henderson and 
Brill, 1999), word sense disambiguation (Peder- 
sen, 2000), machine translation (lh'ederking and 
Nirenburg, 1994), and speech recognition (Fis- 
cus, 1997). Those works empirically demon- 
strated that combining different systems often 
achieved significant improvelnents over the pre- 
vious best system. 
In order to couple those committee-based 
348 
schemes with t)robat)ilistic t)artial parsing, how- 
ever, Olle would still need to make a fllrther ex- 
tension. Ainling at this coupling, ill this t)at)er, 
we consider a general framework of (:ommil, tee- 
based decision making that consists of ~ set 
of weighting flmctions mid a combination flmc- 
tion, and (lis('uss how that Kalnework enal)les 
the coupling with t)robal)ilistic t)artial t)m:sing. 
To denionstr~te how it works, we ret)ort the re- 
sults of our t)arsing exl)eriments on a Japanese 
tree bank. 
2 Probabilistic partial parsing 
2.1 Dependency  probab i l i ty  
In this t)at)er, we consider the task of (le(:id- 
ing the det)endency structure of a Jat);mese in- 
put sentence. Note that, while we restrict ore: 
discussion to analysis of Jat)anese senl;(;nc(;s in 
this t)~l)er, what we present l)elow should also 
t)e strnightfi?rwardly ?xt)plical)h~ to more wide- 
ranged tasks such as English det)endency anal- 
ysis just  like the t)roblem setting considered t)y 
Collins (1996). 
Givell ;m inl)ut sentence ,s as a sequence, of 
B'unset,su-t)hrases (BPs) J, lq b2 . . .  lh~, our task 
is to i(tent, i\[y their inter-BP del)endency struc- 
t , ,e  n = l,j)l,: = ',,,}, where 
(tenot;es that bi (let)on(Is on (or modities) bj. 
Let us consider a dependency p'roba, bility (I)P): 
P('r(bi, bj)l.s'), a t)rol)al)ility l;lu~t 'r(bi, b:j) hohts 
in a Given senl:ence s: Vi. E j  P(','(51, t , j ) l .4 = a. 
2.2  Es t imat ion  o f  DPs  
Some of the state-of:the-art 1)rol)at)ilis(;ic bm- 
guage inodels such as the l)ottomu t) models 
P(l~,l.,.) propos,,d by Collins (1:)96) and Fujio 
et al (1998) directly est imate DPs tbr :~ given 
int)ut , whereas other models su('h as PCFO-  
t)ased tel)down generation mod(;ls P(H,,,s) do 
not, (Charnink, 1997; Collins, 1997; Shir~fi et ~rl., 
1998). If the latter type of mod(,'ls were total ly 
exchlded fronl any committee, our commit;tee- 
based framework would not work well in I)rac- 
lice. Fortm:ately, how(:ver, even tbr such a 
model, one can still est imate l)l?s in the follow- 
ing way if the rood(;1 provides the n-best del)en- 
1A bunsctsu phrase (BP) is a chunk of words (-on- 
sist;ing of a content word (noun, verl), adjective, etc.) 
accoml)mfied by sonic flmctional word(s) (i)arti(:le, mlx- 
iliary, etc.). A .lai)anes(' sentc'nce can 1)c analyzed as a 
sequence of BPs, which constitutes an inter-BP deI)en- 
dency structure 
dency structure candidates cout)led with prot)- 
abilistic scores. 
Let Ri be the i-th best del)endency st;ruct;ure 
(i = 1 , . . . ,  'n) of ;~ given input ,s' according to a 
given model, and h;t ~H l)e a set; of H,i. Then, 
,.,u, l,e csl;ima|;ed by the following 
ai)l)l"OXilnation equation: 
./)F 
? 7~H P(',(b,z, (1) 
where P'R.u is the probal)ilit;y mass of H, E 7~Lr, 
and prn. is the probabi l i ty mass of R ~ ~H 
that suppori;s 'r(bi, bj). Tile approximation er- 
ror c is given 1)y c < l;r~--1%, where l),p,, is 1;t2(; 
- -  l~p~ ' 
prol)abilil;y mass of all the dependency struc- 
ture candidates for s (see (Peele, 1993) for the 
l?roof). This means that the al)t)roximation er- 
ror is negligil)le if P'R,, is sut\[iciently close to 
1),R, which holds for a reasonably small mlmt)er 
'n in lnOSt cases in practical statistical parsing. 
2.3 Coverage-accuracy  curves  
We then conside, r the task of selecting depen- 
dency relations whose est imated probabi l i ty is 
higher I:han a (:e|:i;ain l;hreshoht o- (0 < a < 1). 
When (r is set 1;o be higher (closer to 1.0), t;he 
accuracy is cxt)ected to become higher, while 
the coverage is ext)ecl;ed to become lowe,:, and 
vi(:e versm Here, (;over~ge C* and a,(;ctlra(;y A 
are defined as follows: 
# of the. decided relations 
C 
# of nil the re, lations in I;\]le t;est so,}i2 )/~ 
# of the COl'rectly decided relati?n~3~vJ A 
# of the decided relations 
Moving the threshohl cr from 1.0 down to- 
ward 0.0, one (:an obtain a coverage-a(:cura(:y 
(:urve (C-A curve). In 1)rol)al)ilistic t)artial pars- 
ing, we ewflunte the t)erforman('e of a model 
~mcording to its C-A curve. A few examt)les 
are shown in Figure 1, which were obtained 
in our ext)erim(mt (see Section 4). Ot)viously, 
Figure 1 shows that model A outt)erformed the 
or, her two. To summarize a C-A cIlrve, we use 
the l l -t)oint average of accuracy (l l-t)oint at:- 
curacy, hereafl;er), where the eleven points m'e 
C = 0.5, 0 .55 , . . . ,  1.0. The accuracy of total 
parsing correst)onds to the accuracy of the t)oint 
in a C-A curve where C = 1.0. We call it total 
~ccuracy to distinguish it from l\]- l)oint at:el> 
racy. Not;('. that two models with equal achieve- 
349 
! 
A 
0.95 
0.9 
0.85  
0 .8  
0 0 .2  0 .4  0 .6  0 .8  1 
coverage 
Figure 1: C-A curves 
meuts in total accuracy may be different in l l -  
point accuracy. In fact, we found such cases in 
our experiments reported below. Plotting C-A 
curves enable us to make a more fine-grained 
perfbrmance valuation of a model. 
3 Committee-based probabilis- 
tic partial parsing 
We consider a general scheme of comnfittee- 
based probabilistic partial parsing as illustrated 
in Figure 2. Here we assume that each connnit- 
tee member M~ (k = 1 , . . . ,  m) provides a DP 
matrix PM~(r(bi, bj)ls ) (bi, bj E s) tbr each in- 
put 8. Those matrices are called inlmt matrices, 
and are give:: to the committee as its input. 
A committee consists of a set of weighting 
functions and a combination flmction. The role 
assigned to weighting flmctions is to standardize 
input matrices. The weighting function associ- 
ated with model Mk transforms an input ma-  
trix given by MI~ to a weight matrix WaG- The 
majority flmction then combines all the given 
weight matrices to produce an output matrix O, 
which represents the final decision of the con> 
mittee. One can consider various options for 
both flmctions. 
3.1 Weight ing  funct ions  
We have so far considered the following three 
options. 
S imple  The simplest option is to do nothing: 
~a~ = PA~ (,.(b~, bj)l~) (4) ij 
o Mk where wij is the ( i , j )  element of I/VMk. 
Normal  A bare DP may not be a precise es- 
timation of the actual accuracy. One can see 
this by plotting probability-accuracy curves (P- 
A curves) as shown in Figure 3. Figure 3 shows 
that model A tends to overestimate DPs, while 
/ 
O}lll l l l \[ttct ~ based  dec is ion  nlakil l l~ 
i 
i " i - -  =" ,, 
models  it lpl lt  i weight matrices i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  : 
matrices 
"tV l : :  "~Vt~iglt|iJI g \]"u n ?1\]Oll 
CF: Ct3mhinat lan I :mlc l \ [ .n 
Figure 2: Committee-based probabilistic partial 
parsing 
0,9 
;,~ 
0,8 
= 
0.7 
0.6 
C 
0.5 
0,5 0.6 0.7 0.8 0.9 
dependency  probabi l i ty  
Figure 3: P-A curves 
model C tends to underestimate DPs. This 
lneans that if A and C give different answers 
with the same DP, C's answer is more likely 
to be correct. Thus, it is :sot necessarily a 
good strategy to simply use give:: bare DPs in 
weighted majority. To avoid this problem, we 
consider the tbllowing weighting flmction: 
w~J k =@lkAM~(PMk(','(bi, b:i)l.s)) (5) 
where AMk (P) is the function that returns the 
expected accuracy of Mk's vote with its depen- 
Mk dency probability p, and oz i is a normalization 
factor. Such a function can be trained by plot- 
ting a P-A curve fbr training data. Note that 
training data should be shared by all the com- 
mittee members. In practice, tbr training a P-A 
curve, some smoothing technique should be ap- 
plied to avoid overfitting. 
C lass The standardization process in the 
above option Normal  can also be seen as an 
effort for reducing the averaged cross entropy 
of the model on test, data. Since P-A curves 
tend to defi~,r not only between different mod- 
els but also between different problem classes, 
if one incorporates ome problem classification 
into (5), the averaged cross entropy is expected 
350 
to be reduced fllrther: 
'w~  =/~';'~AM.%(i)M~(r(b~,bj)l,s)) (6) 
where AMkcl, i (P) is the P-A curve of model Mk 
only tbr the problems of class Cb~ in training 
data, and flMk is a normalization factor. For i 
probleln classification, syntactie/lexieal features 
of bi may be useful. 
3.2  Combin ing  funct ions  
For combination flmctions, we have so far con- 
sidered only simple weighted voting, which av- 
erages the given weight matrices: 
1;I, 
Mk 1 v-" Mk 
? = - -  2_, ~'J'U (7) ?iJ 'm, 
h=l 
where o.i.f/~:_ is the (i, j) element of O. 
Note that the committee-based partial pars- 
ing frmnework t)resented here can be see, n as 
a generalization of the 1)reviously proposed 
voting-based techniques in the following re- 
spects: 
(a) A committee a(:(:epts probabilistically para- 
meterized votes as its intmt. 
(d) A committee ac(:el)ts multil)le voting (i.e. it; 
allow a comnfittee menfl)er to vote not only 
to the 1)est-scored calMi(late trot also to all 
other potential candidates). 
((:) A. (:ommittee 1)rovides a metals tbr standard- 
izing original votes. 
(b) A committee outl)uts a 1)rot)abilisti(" distri- 
bution representing a tinal decision, which 
constitutes a C-A curve. 
For examt)le, none of simple voting techniques 
for word class tagging t)roposed 1)y van Hal- 
teren et al (1998) does not accepts multiple 
voting. Henderson and Brill (1999) examined 
constituent voting and naive Bayes classifi(:a- 
lion for parsing, ol)taining positive results ibr 
each. Simple constituent voting, however, does 
not accept parametric votes. While Naive Bayes 
seems to partly accept l)arametric multit)le vot- 
ing, it; does not consider either sl;andardization 
or coverage/accuracy trade-off. 
4 Exper iments  
4.1  Set t ings  
We conducted eXl)erinmnts using the tbllow- 
ing tive statistical parsers: 
Table 1: The total / l l-t)oint accuracy achieved 
1)y each individual model 
total 11-point 
A 0.8974 0.9607 
B 0.8551 0.9281 
C 0.8586 0.9291 
D 0.8470 0.9266 
E 0.7885 0.8567 
? KANA (Ehara, 1998): a bottom-up model 
based oll maxinmm entropy estimation, 
Since dependency score matrices given by 
KANA have no probabilistic semantics, we 
normalized them tbr each row using a cer- 
tain function manually tuned for this parser. 
? CI\]AGAKE (Fujio et al, 1998): an exten- 
sion of the bottom-up model proposed by 
Collins (Collins, 1996). 
? Kanaymna's parser (Kanayama et al, 1999): 
a l)o|,tom-up model coupled with an HPSG. 
? Shirai's parser (Shirai et al, 1998): a top- 
down model incorporating lexical collocation 
statistics. Equation (1) was used tbr estimat- 
ing DPs. 
? Peach Pie Parser (Uchilnoto et al, 1999): 
a bottom-up model based on maximum en- 
tropy estimation. 
Note that these models were developed flfily 
independently of ea('h other, and have siglfifi- 
Calltly different (:haracters (Ii)r a comparison of 
their performance, see %tble 1). In what Jbl- 
lows, these models are referred to anonymously. 
For the source of the training/test set, we 
used the Kyoto corpus (ver.2.0) (Kurohashi et 
al., 1.997), which is a collection of Japanese 
newspaper articles mmotated in terms of word 
boundaries, POS tags, BP boundaries, and 
inter-BP dependency relations. The corpus 
originally contained 19,956 sentences. To make 
the training/test sets~ we tirst removed all the 
sentences that were rejected by any of the above 
five parsers (3,146 sentences). For the remain- 
ing 16,810 sentences, we next checked the con- 
sistency of the BP boundaries given by the 
parsers since they had slightly different crite- 
ria tbr BP segmentation fl'om each other. In 
this process, we tried to recover as many in- 
consistent boundaries as possible. For example, 
we tbund there were quite a few cases where 
a parser recoglfized a certain word sequence ms 
a single BP, whereas ome other parser recog- 
nized the same sequence as two BPs. In such 
351 
0.965 
\[ASimple DNormal  \ [ \ ]C lass  
0.96 
0.955 
0.95 
A : 0.9607 \ \ 
na ?o ~ ~0 
0.975 \[ 
e eg  
Figure 4: l l -po int  accuracy: A included 
0.96 ', \[.~Normal mClass  
0.95 
0.94 
0.93 
0.92 
9291 f 
5 g a0 
Figure 5: l 1-point accuracy: B /C  included 
a case, we regarded that  sequence as a single 
BP under a certain condition. An a result;, we 
obtained 13,990 sentences that  can be accepted 
by all the parsers with all the BP boundaries 
consistent 2 We used thin set tbr training and 
evaluation. 
For cloned tests, we used 11,192 sentences 
(66,536 BPs a) for both  training and tests. 
For open tests, we conducted five-fold cross- 
val idation on the whole sentence set. 
2In the BP concatenation process described here, 
quite a few trivial dependency relations between eigl,- 
boring BPs were removed from the test set. This made 
our test set slightly more difficult tlmn what it should 
have 1)cert. 
3This is the total nmnber of BPs excluding the right- 
most two BPs for each sentence. Since, in Jal)anese, a 
BP ahvays depends on a BP following it, the right-most 
BP of a sentence does not (lei)(tnd on any other BP, and 
the second right-most BP ahvays depends on the right- 
most BP. Therefore, they were not seen as subjects of 
evahmtion. 
0.97 
DSimple  \[21Normal mClass  
0.965 
0.97 
0.96 
0.955 
Figure 6: l l -po in t  accuracy: +KNP 
For the classification of problems, we man- 
ually established the following twelve (:lasses, 
each of which is defined in terms of a certain 
nlol:phological pat tern  of depending BPs: 
1.. nonfinal BP wit, h a case marker "'wa (topic)" 
2. nominal BP with a case marker "no (POS)" 
3. nominal BP with a case marker "ga (NOM)" 
4. nominal BP with a case marker % (ACC)" 
5. nonlinal BP with a case marker "hi (DAT)" 
6. nominal BP with a case marker "de (LOC/. . . )"  
nominal BP (residue) 
adnominal verbal BP 
verbal BP (residue) 
adverb 
adjective 
residue 
4.2 Resu l ts  and d iscuss ion 
Table 1 shown the to ta l / l l -po in t  accuracy of 
each individual model. The  performance of each 
model widely ranged from 0.96 down to 0.86 
in l l -po int  accuracy. Remember  that  A is the 
opt imal  model, and there are two second-best 
models, B and C, which are closely comparable.  
In what tbllows, we use these achievements ms 
the baseline for evaluating the error reduct ion 
achieved by organizing a committee. 
The pertbrmanee of various committees is 
shown in Figure 4 and 5. Our pr imary inter- 
est here is whether  the weighting functions pre- 
sented above effectively contr ibute to error re- 
duction. According to those two figures, al- 
though the contr ibut ion of the f lmction Nor -  
ma l  were nor very visible, the flmction C lass  
consistently improved the accuracy. These re- 
sults can be a good evidence tbr the impor tant  
role of weighting f lmctions in combining parsers. 
7. 
8. 
9. 
1(). 
11. 
12. 
352 
0.96 
0.94 
0.92 t ~ t I I ,  \[iJ 
Figure 7: Single voting vs. Multiple voting 
While we manually tmill: the 1)roblem classiti('a- 
l;ion in our ext)erimen|;, autom;~I;ic (:lassitication 
te.chniques will also 1)e obviously worth consid- 
ering. 
We l;hen e.on(tucted another exl)e, rime.nI; to ex- 
amine the, et\['e(-l;s of muli;it)le voting. One (:an 
sl;raighi;forwardly sinn|late a single-voting com- 
nlil;tee by ret)lacing wij in equal;ion (7) with w~. i 
given by: 
, { wi.i (if' j = m'g m~xk 'wit~) 
=_ 0 (o|;he.wise) (S) 
The resull;s are showll in Figure 7, which 
corot)ares l;he original multi-voting committees 
and l;he sinmlai;e(t single-voi:ing (:olmnil;l;ees. 
Clearly, in our se|;tings, multil)le voting signif- 
icanl;ly oul;pertbrmed single vol;ing 1)arti(:ul~rly 
when t;he size of a ('ommii;tee is small. 
The nexl; issues are whel;her ~ (:Omlnil;te,(', al- 
ways oul;perform its indivi(tmd memt)ers, mtd if 
not;, what should be (-onsidered in organizing a 
commii;i;ee. Figure 4 and 5 show |;hal; COllllllil;- 
tees nol; ilmlu(ling t;he ot)timal model A achieved 
extensive imt)rovemenl;s, whereas the merit of 
organizing COlmnitl;ees including A is not very 
visible. This can be t)arl, ly attrilml;ed to the 
fa.ct that the corot)el;once of the, individual mem- 
l)ers widely diversed, and A signiti(:md;ly OUtl)er- 
forms the ol:her models. 
Given l,he good error reduct;ion achieved 
by commit, tees containing comt)ar~ble meml)ers 
sueh ~s BC, BD a, nd B@I), however, it should t)e 
reasonable 1;o eXl)ect thai; a (:omlnil,l,e,e includ- 
ing A would achieve a significant imt)rovement; if 
anol;her nearly ol)t;ilnal model was also incorl)o- 
0.8 
v 
0,7 
0.fi 
0.5 0.6 |1,7 {).g 0.9 
dependency probability 
Figure 8: P-A curves: +KNP 
rated. To empirically prove this assmnpl;ion, we, 
conduct;ed anot;her experiment, where we add 
another parser KNP (Kurohashi el; al., 1 !)94:) 1;o 
each commil;|;ee that apt)ears in Figure 4. KNI? 
is much closer to lnodel A in l;ol;al accuracy 
t;han t;he other models (0.8725 in tol;al accu- 
racy). However, il; does not provide. DP rea- 
l;rices since it is designed in a rule-l)ased fash- 
ion the current; version of KNP 1)rovides only 
the t)esl;-t)referrext parse t;ree for ea(:h inl)Ul; sen- 
tence without ~my scoring annotation. We l;hus 
let KNP 1;o simply vol;e its l;ol;al aeem:aey. Tim 
results art; shown in lqgure 6. This time all l;he 
commil;tees achieved significant improvemenl;s, 
wil;h |;he m~ximum e, rror re(hu:|;ion rate up l;o 
'3~%. 
As suggested 1)y |;he. re, suits of t;his exl)erimenl; 
with KNP, our scheme Mlows a rule-based 11011- 
t)~r;m,el:ric p~rse.r t;o pb~y in a eommil;l;e.e pre- 
serving it;s ~d)ilit:y t;o oui;t)ul; t)aralnel;rie I)P ma- 
(;ri(:es. To 1)ush (;he ~u'gumen(; fl,rl;her, SUl)pose 
;~ 1)lausil)le sil;ual;ion where we have ;m Ol)l;imal 
l)ut non-1)arametrie rule-based parser and sev- 
eral suboptimal si;atistical parsers. In su('h ~ 
case, our commil;teeA)ased scheme may t)e able 
l;o organize a commi|,tee that can 1)rovide l)P 
lnatri(:es while preserving the original tol;al ac- 
curacy of the rule-b~sed parser. To set this, we 
conducted another small experiment, where, we 
combined KNP with each of C and D, 1)oth of 
whi(:h are less compe.tent than KNP. The result- 
ing (:ommil;l;ees successflflly t)rovided reasonal)le 
P-A curves as shown in Figure 8, while even 
further lint)roving the original |;ol;al at:curacy of 
KNP (0.8725 to 0.8868 tbr CF and 0.8860 for 
DF). Furthermore, t;he COmlnittees also gained 
the 11-point accuracy over C and D (0.9291 to 
353 
0.9600 tbr CF and 0.9266 to 0.9561 for DF). 
These. results suggest hat our committee-based 
scheme does work even if the most competent 
member of a committee is rule-based and thus 
non-parametric. 
5 Conclusion 
This paper presented a general committee- 
based frmnework that can be coupled with prob- 
abilistic partial parsing. In this framework, a 
committee accepts parametric multiple votes, 
and then standardizes them, and finally pro- 
vides a probabilistic distribution. We presented 
a general method for producing probabilistic 
multiple votes (i.e. DP matrices), which al- 
lows most of the existing probabilistic models 
for parsing to join a committee. Our experi- 
ments revealed that (a) if more than two compa- 
rably competent models are available, it is likely 
to be worthwhile to combine them, (b) both 
multit)le voting and vote standardization effec- 
tively work in committee-based partial parsing, 
(c) our scheme also allows a non-parametric 
rule-based parser to make a good contribution. 
While our experiments have so far been produc- 
ing promising results, there seems to be much 
room left for investigation and improvement. 
Acknowledgments 
We would like to express our special thanks to 
all the creators of the parsers used here for en- 
abling ~fll of this research by providing us their 
systems. We would also like to thank the re- 
viewers tbr their suggestive comments. 
References 
Brill, E. and J. Wu. Classifier Combination for ha- 
proved Lexical Disambiguation. In Proc. of the 
17th COLING, pp.191-195, 1998. 
Carroll, J. ,G. Minnen, Y. Cmming, S. Devlin and 
J. Tait. Practical Simplification of English News- 
paper Text to Assist Aphasic Readers. In Prvc. of 
AAAI-98 Workshop on Integrating Artificial In- 
telligence and Assistive Technology,1998. 
Charniak, E. Statistical parsing with a context- 
free grammar and word statistics. In Prvc. of the 
AAAI, pp.598 603, 1997. 
Collins, M. J. A new statistical parser based on bi- 
grmn lexical dependencies. In Proc. of the 3~th 
ACL, pp.184-191, 1996. 
Collins, M. J. Three generative, lexicalised models 
for statistical parsing. In Proc. of the 35th A CL, 
pp.16-23, 1997. 
Ehara, T. Estinlating the consistency of Japanese 
dependency relations based on the maximam en-  
trot)y modeling. Ill Proc. of the/~th Annual Meet- 
ing of The Association of Natural Language Pro- 
cessing, 1)t).382-385, 1998. (In Japanese) 
Fiscus, J. G. A post-processing system to yield re- 
duced word error rates: Recognizer output voting 
error reduction (ROVER). In EuroSpccch, 1997. 
Fk'ederking, R. and S. Nirenburg. Three heads are 
better titan one. In Proc. of the dth ANLP, 1994. 
Fujio, M. and Y. Matsmnoto. Japmmse dependency 
structure analysis based on lexicalized statistics. 
In Proc. of the 3rd EMNLP, t)I).87-96, 1998. 
Henderson, J. C. and E. Brill. Exploiting Diver- 
sity in Natural Language Processing: Combining 
Parsers. In Proc. of the 1999 Joint SIGDAT Con- 
fcrcncc on EMNLP and I/LC, pt).187--194. 
Jensen, K., G. E. Heidorn, and S. D. Richardson, 
editors, natural anguage processing: The PLNLP 
AppTvach. Kluwer Academic Publishers, 1993. 
Kanayama, H., K. Torisawa, Y. Mitsuisi, and 
J. Tsujii. Statistical Dependency Analysis with 
an HPSG-based Japanese Grainmar. In Proc. of 
the NLPRS, pp.138-143, 1999. 
Kurohashi, S. and M. Nagao. Building a Jat)anese 
parsed corpus while lint)roving tile parsing system. 
In Proc. of NLPRS, pp.151-156, 1997. 
Kurohashi, S. and M. Nagao. KN Parser : Japanese 
Dependency/Case Structure Analyzer. in Proc. of 
Th.e httcrnational Worksh.op on Sharablc Natural 
Lang'aagc Rcso'arccs, pp.48-55, 1994. 
Poole, D. Average-case analysis of a search algo- 
rithm fl)r estimating prior and 1)ostcrior probabil- 
ities in Bayesian etworks with extreme 1)rot)abil- 
ities, thc i3th LICAL pp.606 612, 1993. 
Pedersen, T. A Simple AI)l)roach to Building En- 
sembles of Naive Bayesian Classifiers for Word 
Sense Dismnbiguation In Proc. of the NAACL, 
pp.63-69, 2000. 
Shirai, K., K. hmi, T. Tokunaga and H. Tanaka 
An empirical evaluation on statistical 1)arsing 
of Japanese sentences using a lexical association 
statistics, thc 3rd EMNLP, pp.80-87, 1998. 
Uchimoto, K., S. Sekine, and H. Isahara. Japanese 
dependency structure analysis based on maxi- 
mum entopy models. In Proc. of thc 13th EACL, 
pp.196-203, 1999. 
van Halteren, H., J. Zavrel, and W. Daelemans. hn- 
t)roving data driven wordclass tagging 1)y system 
combination. In Proc. of the 17th COLING, 1998. 
354 
Latent Variable Models for Semantic Orientations of Phrases
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Japan Society of the Promotion of Science
tinui@lr.pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose models for semantic orienta-
tions of phrases as well as classification
methods based on the models. Although
each phrase consists of multiple words, the
semantic orientation of the phrase is not a
mere sum of the orientations of the com-
ponent words. Some words can invert the
orientation. In order to capture the prop-
erty of such phrases, we introduce latent
variables into the models. Through exper-
iments, we show that the proposed latent
variable models work well in the classifi-
cation of semantic orientations of phrases
and achieved nearly 82% classification ac-
curacy.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial
areas. It can be applied to, for example, a survey
of new products or a questionnaire analysis. Au-
tomatic sentiment analysis enables a fast and com-
prehensive investigation.
The most fundamental step for sentiment anal-
ysis is to acquire the semantic orientations of
words: desirable or undesirable (positive or neg-
ative). For example, the word ?beautiful? is pos-
itive, while the word ?dirty? is negative. Many
researchers have developed several methods for
this purpose and obtained good results (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al, 2004; Takamura
et al, 2005; Kobayashi et al, 2001). One of
the next problems to be solved is to acquire se-
mantic orientations of phrases, or multi-term ex-
pressions. No computational model for semanti-
cally oriented phrases has been proposed so far al-
though some researchers have used techniques de-
veloped for single words. The purpose of this pa-
per is to propose computational models for phrases
with semantic orientations as well as classification
methods based on the models. Indeed the seman-
tic orientations of phrases depend on context just
as the semantic orientations of words do, but we
would like to obtain the most basic orientations of
phrases. We believe that we can use the obtained
basic orientations of phrases for affect analysis of
higher linguistic units such as sentences and doc-
uments.
The semantic orientation of a phrase is not a
mere sum of its component words. Semantic
orientations can emerge out of combinations of
non-oriented words. For example, ?light laptop-
computer? is positively oriented although neither
?light? nor ?laptop-computer? has a positive ori-
entation. Besides, some words can invert the ori-
entation of a neighboring word, such as ?low?
in ?low risk?, where the negative orientation of
?risk? is inverted to a ?positive? by the adjective
?low?. This kind of non-compositional operation
has to be incorporated into the model. We focus
on ?noun+adjective? in this paper, since this type
of phrase contains most of interesting properties
of phrases, such as emergence or inversion of se-
mantic orientations.
In order to capture the properties of semantic
orientations of phrases, we introduce latent vari-
ables into the models, where one random variable
corresponds to nouns and another random vari-
able corresponds to adjectives. The words that
are similar in terms of semantic orientations, such
as ?risk? and ?mortality? (i.e., the positive ori-
entation emerges when they are ?low?), make a
cluster in these models. Our method is language-
201
independent in the sense that it uses only cooccur-
rence data of words and semantic orientations.
2 Related Work
We briefly explain related work from two view-
points: the classification of word pairs and the
identification of semantic orientation.
2.1 Classification of Word Pairs
Torisawa (2001) used a probabilistic model to
identify the appropriate case for a pair of words
constituting a noun and a verb with the case of
the noun-verb pair unknown. Their model is the
same as Probabilistic Latent Semantic Indexing
(PLSI) (Hofmann, 2001), which is a generative
probability model of two random variables. Tori-
sawa?s method is similar to ours in that a latent
variable model is used for word pairs. How-
ever, Torisawa?s objective is different from ours.
In addition, we used not the original PLSI, but
its expanded version, which is more suitable for
this task of semantic orientation classification of
phrases.
Fujita et al (2004) addressed the task of the de-
tection of incorrect case assignment in automat-
ically paraphrased sentences. They reduced the
task to a problem of classifying pairs of a verb
and a noun with a case into correct or incorrect.
They first obtained a latent semantic space with
PLSI and adopted the nearest-neighbors method,
in which they used latent variables as features. Fu-
jita et al?s method is different from ours, and also
from Torisawa?s, in that a probabilistic model is
used for feature extraction.
2.2 Identification of Semantic Orientations
The semantic orientation classification of words
has been pursued by several researchers (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al, 2004; Takamura et
al., 2005). However, no computational model for
semantically oriented phrases has been proposed
to date although research for a similar purpose has
been proposed.
Some researchers used sequences of words as
features in document classification according to
semantic orientation. Pang et al (2002) used bi-
grams. Matsumoto et al (2005) used sequential
patterns and tree patterns. Although such patterns
were proved to be effective in document classi-
fication, the semantic orientations of the patterns
themselves are not considered.
Suzuki et al (2006) used the Expectation-
Maximization algorithm and the naive bayes clas-
sifier to incorporate the unlabeled data in the clas-
sification of 3-term evaluative expressions. They
focused on the utilization of context information
such as neighboring words and emoticons. Tur-
ney (2002) applied an internet-based technique to
the semantic orientation classification of phrases,
which had originally been developed for word sen-
timent classification. In their method, the num-
ber of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
?phrase NEAR good?) is used to determine the
orientation. Baron and Hirst (2004) extracted col-
locations with Xtract (Smadja, 1993) and classi-
fied the collocations using the orientations of the
words in the neighboring sentences. Their method
is similar to Turney?s in the sense that cooccur-
rence with seed words is used. The three methods
above are based on context information. In con-
trast, our method exploits the internal structure of
the semantic orientations of phrases.
Inui (2004) introduced an attribute plus/minus
for each word and proposed several rules that
determine the semantic orientations of phrases
on the basis of the plus/minus attribute val-
ues and the positive/negative attribute values of
the component words. For example, a rule
[negative+minus=positive] determines ?low (mi-
nus) risk (negative)? to be positive. Wilson et
al. (2005) worked on phrase-level semantic orien-
tations. They introduced a polarity shifter, which
is almost equivalent to the plus/minus attribute
above. They manually created the list of polarity
shifters. The method that we propose in this paper
is an automatic version of Inui?s or Wilson et al?s
idea, in the sense that the method automatically
creates word clusters and their polarity shifters.
3 Latent Variable Models for Semantic
Orientations of Phrases
As mentioned in the Introduction, the semantic
orientation of a phrase is not a mere sum of its
component words. If we know that ?low risk? is
positive, and that ?risk? and ?mortality?, in some
sense, belong to the same semantic cluster, we can
infer that ?low mortality? is also positive. There-
fore, we propose to use latent variable models to
extract such latent semantic clusters and to real-
ize an accurate classification of phrases (we focus
202
N Z
A
N
A C
N Z
A C
N Z
A C
N Z
A C
(a) (b) (c) (d) (e)
Figure 1: Graphical representations:(a) PLSI, (b) naive bayes, (c) 3-PLSI, (d) triangle, (e) U-shaped;
Each node indicates a random variable. Arrows indicate statistical dependency between variables. N , A,
Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations.
on two-term phrases in this paper). The models
adopted in this paper are also used for collabora-
tive filtering by Hofmann (2004).
With these models, the nouns (e.g., ?risk? and
?mortality?) that become positive by reducing
their degree or amount would make a cluster. On
the other hand, the adjectives or verbs (e.g., ?re-
duce? and ?decrease?) that are related to reduction
would also make a cluster.
Figure 1 shows graphical representations of sta-
tistical dependencies of models with a latent vari-
able. N , A, Z and C respectively correspond to
nouns, adjectives, latent clusters and semantic ori-
entations. Figure 1-(a) is the PLSI model, which
cannot be used in this task due to the absence of
a variable for semantic orientations. Figure 1-(b)
is the naive bayes model, in which nouns and ad-
jectives are statistically independent of each other
given the semantic orientation. Figure 1-(c) is,
what we call, the 3-PLSI model, which is the 3-
observable variable version of the PLSI. We call
Figure 1-(d) the triangle model, since three of its
four variables make a triangle. We call Figure 1-
(e) the U-shaped model. In the triangle model and
the U-shaped model, adjectives directly influence
semantic orientations (rating categories) through
the probability P (c|az). While nouns and adjec-
tives are associated with the same set of clusters Z
in the 3-PLSI and the triangle models, only nouns
are clustered in the U-shaped model.
In the following, we construct a probability
model for the semantic orientations of phrases us-
ing each model of (b) to (e) in Figure 1. We ex-
plain in detail the triangle model and the U-shaped
model, which we will propose to use for this task.
3.1 Triangle Model
Suppose that a set D of tuples of noun n, adjective
a (predicate, generally) and the rating c is given :
D = {(n1, a1, c1), ? ? ? , (n|D|, a|D|, c|D|)}, (1)
where c ? {?1, 0, 1}, for example. This can be
easily expanded to the case of c ? {1, ? ? ? , 5}. Our
purpose is to predict the rating c for unknown pairs
of n and a.
According to Figure 1-(d), the generative prob-
ability of n, a, c, z is the following :
P (nacz) = P (z|n)P (a|z)P (c|az)P (n). (2)
Remember that for the original PLSI model,
P (naz) = P (z|n)P (a|z)P (n).
We use the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977) to estimate the pa-
rameters of the model. According to the theory of
the EM algorithm, we can increase the likelihood
of the model with latent variables by iteratively in-
creasing the Q-function. The Q-function (i.e., the
expected log-likelihood of the joint probability of
complete data with respect to the conditional pos-
terior of the latent variable) is expressed as :
Q(?) =
?
nac
fnac
?
z
P? (z|nac) log P (nazc|?), (3)
where ? denotes the set of the new parameters.
fnac denotes the frequency of a tuple n, a, c in the
data. P? represents the posterior computed using
the current parameters.
The E-step (expectation step) corresponds to
simple posterior computation :
P? (z|nac) = P (z|n)P (a|z)P (c|az)?
z P (z|n)P (a|z)P (c|az)
. (4)
For derivation of update rules in the M-step (max-
imization step), we use a simple Lagrange method
for this optimization problem with constraints :
?z, ?n P (n|z) = 1, ?z,
?
a P (a|z) = 1, and
?a, z, ?c P (c|az) = 1. We obtain the following
update rules :
P (z|n) =
?
ac fnacP? (z|nac)
?
ac fnac
, (5)
203
P (y|z) =
?
nc fnacP? (z|nac)
?
nac fnacP? (z|nac)
, (6)
P (c|az) =
?
n fnacP? (z|nac)
?
nc fnacP? (z|nac)
. (7)
These steps are iteratively computed until conver-
gence. If the difference of the values of Q-function
before and after an iteration becomes smaller than
a threshold, we regard it as converged.
For classification of an unknown pair n, a, we
compare the values of
P (c|na) =
?
z P (z|n)P (a|z)P (c|az)
?
cz P (z|n)P (a|z)P (c|az)
. (8)
Then the rating category c that maximize P (c|na)
is selected.
3.2 U-shaped Model
We suppose that the conditional probability of c
and z given n and a is expressed as :
P (cz|na) = P (c|az)P (z|n). (9)
We compute parameters above using the EM al-
gorithm with the Q-function :
Q(?) =
?
nac
fnac
?
z
P? (z|nac) log P (cz|na, ?).(10)
We obtain the following update rules :
E step
P? (z|nac) = P (c|az)P (z|n)?
z P (c|az)P (z|n)
, (11)
M step
P (c|az) =
?
n fnacP? (z|nac)
?
nc fnacP? (z|nac)
, (12)
P (z|n) =
?
ac fnacP? (z|nac)
?
ac fnac
. (13)
For classification, we use the formula :
P (c|na) =
?
z
P (c|az)P (z|n). (14)
3.3 Other Models for Comparison
We will also test the 3-PLSI model corresponding
to Figure 1-(c).
In addition to the latent models, we test a base-
line classifier, which uses the posterior probabil-
ity :
P (c|na) ? P (n|c)P (a|c)P (c). (15)
This baseline model is equivalent to the 2-term
naive bayes classifier (Mitchell, 1997). The graph-
ical representation of the naive bayes model is (b)
in Figure 1. The parameters are estimated as :
P (n|c) = 1 + fnc|N | + fc
, (16)
P (a|c) = 1 + fac|A| + fc
, (17)
where |N | and |A| are the numbers of the words
for n and a, respectively.
Thus, we have four different models : naive
bayes (baseline), 3-PLSI, triangle, and U-shaped.
3.4 Discussions on the EM computation, the
Models and the Task
In the actual EM computation, we use the tem-
pered EM (Hofmann, 2001) instead of the stan-
dard EM explained above, because the tempered
EM can avoid an inaccurate estimation of the
model caused by ?over-confidence? in computing
the posterior probabilities. The tempered EM can
be realized by a slight modification to the E-step,
which results in a new E-step :
P? (z|nac) =
(
P (c|az)P (z|n)
)?
?
z
(
P (c|az)P (z|n)
)? , (18)
for the U-shaped model, where ? is a positive
hyper-parameter, called the inverse temperature.
The new E-steps for the other models are similarly
expressed.
Now we have two hyper-parameters : inverse
temperature ?, and the number of possible val-
ues M of latent variables. We determine the
values of these hyper-parameters by splitting the
given training dataset into two datasets (the tempo-
rary training dataset 90% and the held-out dataset
10%), and by obtaining the classification accuracy
for the held-out dataset, which is yielded by the
classifier with the temporary training dataset.
We should also note that Z (or any variable)
should not have incoming arrows simultaneously
from N and A, because the model with such ar-
rows has P (z|na), which usually requires an ex-
cessively large memory.
To work with numerical scales of the rating
variable (i.e., the difference between c = ?1 and
c = 1 should be larger than that of c = ?1
and c = 0), Hofmann (2004) used also a Gaus-
sian distribution for P (c|az) in collaborative filter-
ing. However, we do not employ a Gaussian, be-
cause in our dataset, the number of rating classes is
204
only 3, which is so small that a Gaussian distribu-
tion cannot be a good approximation of the actual
probability density function. We conducted pre-
liminary experiments with the model with Gaus-
sians, but failed to obtain good results. For other
datasets with more classes, Gaussians might be a
good model for P (c|az).
The task we address in this paper is somewhat
similar to the trigram prediction task, in the sense
that both are classification tasks given two words.
However, we should note the difference between
these two tasks. In our task, the actual answer
given two specific words are fixed as illustrated
by the fact ?high+salary? is always positive, while
the answer for the trigram prediction task is ran-
domly distributed. We are therefore interested in
the semantic orientations of unseen pairs of words,
while the main purpose of the trigram prediction
is accurately estimate the probability of (possibly
seen) word sequences.
In the proposed models, only the words that ap-
peared in the training dataset can be classified. An
attempt to deal with the unseen words is an in-
teresting task. For example, we could extend our
models to semi-supervised models by regarding C
as a partially observable variable. We could also
use distributional similarity of words (e.g., based
on window-size cooccurrence) to find an observed
word that is most similar to the given unseen word.
However, such methods would not work for the
semantic orientation classification, because those
methods are designed for simple cooccurrence and
cannot distinguish ?survival-rate? from ?infection-
rate?. In fact, the similarity-based method men-
tioned above failed to work efficiently in our pre-
liminary experiments. To solve the problem of un-
seen words, we would have to use other linguistic
resources such as a thesaurus or a dictionary.
4 Experiments
4.1 Experimental Settings
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper ar-
ticles (1995) written in Japanese, and annotated
the pairs with semantic orientation tags : positive,
neutral or negative. We thus obtained the labeled
dataset consisting of 12066 pair instances (7416
different pairs). The dataset contains 4459 neg-
ative instances, 4252 neutral instances, and 3355
positive instances. The number of distinct nouns is
4770 and the number of distinct adjectives is 384.
To check the inter-annotator agreement between
two annotators, we calculated ? statistics, which
was 0.640. This value is allowable, but not quite
high. However, positive-negative disagreement is
observed for only 0.7% of the data. In other words,
this statistics means that the task of extracting neu-
tral examples, which has hardly been explored, is
intrinsically difficult.
We employ 10-fold cross-validation to obtain
the average value of the classification accuracy.
We split the dataset such that there is no overlap-
ping pair (i.e., any pair in the training dataset does
not appear in the test dataset).
If either of the two words in a pair in the test
dataset does not appear in the training dataset, we
excluded the pair from the test dataset since the
problem of unknown words is not in the scope of
this research. Therefore, we evaluate the pairs that
are not in the training dataset, but whose compo-
nent words appear in the training dataset.
In addition to the original dataset, which we call
the standard dataset, we prepared another dataset
in order to examine the power of the latent variable
model. The new dataset, which we call the hard
dataset, consists only of examples with 17 difficult
adjectives such as ?high?, ?low?, ?large?, ?small?,
?heavy?, and ?light?. 1 The semantic orientations
of pairs including these difficult words often shift
depending on the noun they modify. Thus, the
hard dataset is a subset of the standard dataset. The
size of the hard dataset is 4787. Please note that
the hard dataset is used only as a test dataset. For
training, we always use the standard dataset in our
experiments.
We performed experiments with all the values
of ? in {0.1, 0.2, ? ? ? , 1.0} and with all the values
of M in {10, 30, 50, 70, 100, 200, 300, 500}, and
predicted the best values of the hyper-parameters
with the held-out method in Section 3.4.
4.2 Results
The classification accuracies of the four methods
with ? and M predicted by the held-out method
are shown in Table 1. Please note that the naive
bayes method is irrelevant of ? and M . The table
shows that the triangle model and the U-shaped
1The complete list of the 17 Japanese adjectives with their
English counterparts are : takai (high), hikui (low), ookii
(large), chiisai (small), omoi (heavy), karui (light), tsuyoi
(strong), yowai (weak), ooi (many), sukunai (few/little), nai
(no), sugoi (terrific), hageshii (terrific), hukai (deep), asai
(shallow), nagai (long), mizikai (short).
205
Table 1: Accuracies with predicted ? and M
standard hard
accuracy ? M accuracy ? M
Naive Bayes 73.40 ? ? 65.93 ? ?
3-PLSI 67.02 0.73 91.7 60.51 0.80 87.4
Triangle model 81.39 0.60 174.0 77.95 0.60 191.0
U-shaped model 81.94 0.64 60.0 75.86 0.65 48.3
model achieved high accuracies and outperformed
the naive bayes method. This result suggests that
we succeeded in capturing the internal structure
of semantically oriented phrases by way of latent
variables. The more complex structure of the tri-
angle model resulted in the accuracy that is higher
than that of the U-shaped model.
The performance of the 3-PLSI method is even
worse than the baseline method. This result shows
that we should use a model in which adjectives can
directly influence the rating category.
Figures 2, 3, 4 show cross-validated accuracy
values for various values of ?, respectively yielded
by the 3-PLSI model, the triangle model and the
U-shaped model with different numbers M of pos-
sible states for the latent variable. As the figures
show, the classification performance is sensitive to
the value of ?. M = 100 and M = 300 are mostly
better than M = 10. However, this is a tradeoff
between classification performance and training
time, since large values of M demand heavy com-
putation. In that sense, the U-shaped model is use-
ful in many practical situations, since it achieved a
good accuracy even with a relatively small M .
To observe the overall tendency of errors, we
show the contingency table of classification by the
U-shaped model with the predicted values of hy-
perparameters, in Table 2. As this table shows,
most of the errors are caused by the difficulty of
classifying neutral examples. Only 2.26% of the
errors are mix-ups of the positive orientation and
the negative orientation.
We next investigate the causes of errors by ob-
serving those mix-ups of the positive orientation
and the negative orientation.
One type of frequent errors is illustrated by the
pair ?food (?s price) is high?, in which the word
?price? is omitted in the actual example 2. As in
this expression, the attribute (price, in this case) of
an example is sometimes omitted or not correctly
2This kind of ellipsis often occurs in Japanese.
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 2: 3-PLSI model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 3: Triangle model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 4: U-shaped model with standard dataset
206
Table 2: Contingency table of classification result by the U-shaped model
U-shaped model
positive neutral negative sum
positive 1856 281 69 2206
Gold standard neutral 202 2021 394 2617
negative 102 321 2335 2758
sum 2160 2623 2798 7581
identified. To tackle these examples, we will need
methods for correctly identifying attributes and
objects. Some researchers are starting to work on
this problem (e.g., Popescu and Etzioni (2005)).
We succeeded in addressing the data-sparseness
problem by introducing a latent variable. How-
ever, this problem still causes some errors. Pre-
cise statistics cannot be obtained for infrequent
words. This problem will be solved by incorporat-
ing other resources such as thesaurus or a dictio-
nary, or combining our method with other methods
using external wider contexts (Suzuki et al, 2006;
Turney, 2002; Baron and Hirst, 2004).
4.3 Examples of Obtained Clusters
Next, we qualitatively evaluate the proposed meth-
ods. For several clusters z, we extract the words
that occur more than twice in the whole dataset
and are in top 50 according to P (z|n). The model
used here as an example is the U-shaped model.
The experimental settings are ? = 0.6 and M =
60. Although some elements of clusters are com-
posed of multiple words in English, the original
Japanese counterparts are single words.
Cluster 1 trouble, objection, disease, complaint, anx-
iety, anamnesis, relapse
Cluster 2 risk, mortality, infection rate, onset rate
Cluster 3 bond, opinion, love, meaning, longing, will
Cluster 4 vote, application, topic, supporter
Cluster 5 abuse, deterioration, shock, impact, burden
Cluster 6 deterioration, discrimination, load, abuse
Cluster 7 relative importance, degree of influence,
number, weight, sense of belonging, wave,
reputation
These obtained clusters match our intuition. For
example, in cluster 2 are the nouns that are neg-
ative when combined with ?high?, and positive
when combined with ?low?. In fact, the posterior
probabilities of semantic orientations for cluster 2
are as follows :
P (negative|high, cluster 2) = 0.995,
P (positive|low, cluster 2) = 0.973.
With conventional clustering methods based on
the cooccurrence of two words, cluster 2 would
include the words resulting in the opposite orien-
tation, such as ?success rate?. We succeeded in
obtaining the clusters that are suitable for our task,
by incorporating the new variable c for semantic
orientation in the EM computation.
5 Conclusion
We proposed models for phrases with semantic
orientations as well as a classification method
based on the models. We introduced a latent vari-
able into the models to capture the properties of
phrases. Through experiments, we showed that
the proposed latent variable models work well
in the classification of semantic orientations of
phrases and achieved nearly 82% classification ac-
curacy. We should also note that our method is
language-independent although evaluation was on
a Japanese dataset.
We plan next to adopt a semi-supervised learn-
ing method in order to correctly classify phrases
with infrequent words, as mentioned in Sec-
tion 4.2. We would also like to extend our method
to 3- or more term phrases. We can also use the
obtained latent variables as features for another
classifier, as Fujita et al (2004) used latent vari-
ables of PLSI for the k-nearest neighbors method.
One important and promising task would be the
use of semantic orientations of words for phrase
level classification.
References
Faye Baron and Graeme Hirst. 2004. Collocations
as cues to semantic orientation. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society Series B, 39(1):1?38.
207
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2004. Detection of incorrect case assignments in au-
tomatically generated paraphrases of Japanese sen-
tences. In Proceedings of the 1st International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 14?21.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of ad-
jectives. In Proceedings of the Thirty-Fifth Annual
Meeting of the Association for Computational Lin-
guistics and the Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 174?181.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42:177?196.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22:89?115.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute
of Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115?1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelli-
gence, SLUD-33, pages 45?50.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Proceedings of the 9th Pacific-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD-
05), pages 301?310.
Tom M. Mitchell. 1997. Machine Learning. McGraw
Hill.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?02), pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of joint conference on Hu-
man Language Technology / Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP?05), pages 339?346.
Frank Z. Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,,
19(1):143?177.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Pro-
ceedings of the 7th International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-06), pages 502?513.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 133?140.
Kentaro Torisawa. 2001. An unsuperveised method
for canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS 2001), pages
211?218.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?02), pages 417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of joint
conference on Human Language Technology / Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP?05), pages 347?354.
208
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1153?1160,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Time Period Identification of Events in Text 
 
 
Taichi Noro? Takashi Inui?? Hiroya Takamura? Manabu Okumura?
?Interdisciplinary Graduate School of Science and Engineering 
Tokyo Institute of Technology 
4259 Nagatsuta-cho, Midori-ku, Yokohama, Kanagawa, Japan 
??Japan Society for the Promotion of Science 
?Precision and Intelligence Laboratory, Tokyo Institute of Technology 
{norot, tinui}@lr.pi.titech.ac.jp,{takamura, oku}@pi.titech.ac.jp 
 
  
 
Abstract 
This study aims at identifying when an 
event written in text occurs. In particular, 
we classify a sentence for an event into 
four time-slots; morning, daytime, eve-
ning, and night. To realize our goal, we 
focus on expressions associated with 
time-slot (time-associated words). How-
ever, listing up all the time-associated 
words is impractical, because there are 
numerous time-associated expressions. 
We therefore use a semi-supervised 
learning method, the Na?ve Bayes classi-
fier backed up with the Expectation 
Maximization algorithm, in order to it-
eratively extract time-associated words 
while improving the classifier. We also 
propose to use Support Vector Machines 
to filter out noisy instances that indicates 
no specific time period. As a result of ex-
periments, the proposed method achieved 
0.864 of accuracy and outperformed 
other methods. 
1 Introduction 
In recent years, the spread of the internet has ac-
celerated. The documents on the internet have 
increased their importance as targets of business 
marketing. Such circumstances have evoked 
many studies on information extraction from text 
especially on the internet, such as sentiment 
analysis and extraction of location information. 
In this paper, we focus on the extraction of tem-
poral information. Many authors of documents 
on the web often write about events in their daily 
life. Identifying when the events occur provides 
us valuable information. For example, we can 
use temporal information as a new axis in the 
information retrieval. From time-annotated text, 
companies can figure out when customers use 
their products. We can explore activities of users 
for marketing researches, such as ?What do 
people eat in the morning??, ?What do people 
spend money for in daytime?? 
Most of previous work on temporal processing 
of events in text dealt with only newswire text. In 
those researches, it is assumed that temporal ex-
pressions indicating the time-period of events are 
often explicitly written in text. Some examples of 
explicit temporal expressions are as follows: ?on 
March 23?, ?at 7 p.m.?. 
However, other types of text including web 
diaries and blogs contain few explicit temporal 
expressions. Therefore one cannot acquire suffi-
cient temporal information using existing meth-
ods. Although dealing with such text as web dia-
ries and blogs is a hard problem, those types of 
text are excellent information sources due to 
their overwhelmingly huge amount. 
In this paper, we propose a method for estimat-
ing occurrence time of events expressed in in-
formal text. In particular, we classify sentences 
in text into one of four time-slots; morning, day-
time, evening, and night. To realize our goal, we 
focus on expressions associated with time-slot 
(hereafter, called time-associated words), such as 
?commute (morning)?, ?nap (daytime)? and 
?cocktail (night)?. Explicit temporal expressions 
have more certain information than the time-
associated words. However, these expressions 
are rare in usual text. On the other hand, al-
though the time-associated words provide us 
only indirect information for estimating occur-
rence time of events, these words frequently ap-
pear in usual text. Actually, Figure 2 (we will 
discuss the graph in Section 5.2, again) shows 
the number of sentences including explicit tem-
1153
poral expressions and time-associated words re-
spectively in text. The numbers are obtained 
from a corpus we used in this paper. We can fig-
ure out that there are much more time-associated 
words than explicit temporal expressions in blog 
text. In other words, we can deal with wide cov-
erage of sentences in informal text by our 
method with time-associated words. 
However, listing up all the time-associated 
words is impractical, because there are numerous 
time-associated expressions. Therefore, we use a 
semi-supervised method with a small amount of 
labeled data and a large amount of unlabeled data, 
because to prepare a large quantity of labeled 
data is costly, while unlabeled data is easy to ob-
tain. Specifically, we adopt the Na?ve Bayes 
classifier backed up with the Expectation Maxi-
mization (EM) algorithm (Dempster et al, 1977) 
for semi-supervised learning. In addition, we 
propose to use Support Vector Machines to filter 
out noisy sentences that degrade the performance 
of the semi-supervised method. 
In our experiments using blog data, we ob-
tained 0.864 of accuracy, and we have shown 
effectiveness of the proposed method. 
This paper is organized as follows. In Section 
2 we briefly describe related work. In Section 3 
we describe the details of our corpus. The pro-
posed method is presented in Section 4. In Sec-
tion 5, we describe experimental results and dis-
cussions. We conclude the paper in Section 6. 
 
2 Related Work 
The task of time period identification is new 
and has not been explored much to date. 
Setzer et al (2001) and Mani et al (2000) 
aimed at annotating newswire text for analyzing 
temporal information. However, these previous 
work are different from ours, because these work 
only dealt with newswire text including a lot of 
explicit temporal expressions. 
Tsuchiya et al (2005) pursued a similar goal 
as ours. They manually prepared a dictionary 
with temporal information. They use the hand-
crafted dictionary and some inference rules to 
determine the time periods of events. In contrast, 
we do not resort to such a hand-crafted material, 
which requires much labor and cost. Our method 
automatically acquires temporal information 
from actual data of people's activities (blog). 
Henceforth, we can get temporal information 
associated with your daily life that would be not 
existed in a dictionary. 
3 Corpus 
In this section, we describe a corpus made from 
blog entries. The corpus is used for training and 
test data of machine learning methods mentioned 
in Section 4. 
The blog entries we used are collected by the 
method of Nanno et al (2004). All the entries are 
written in Japanese. All the entries are split into 
sentences automatically by some heuristic rules. 
In the next section, we are going to explain 
?time-slot? tag added at every sentence. 
3.1 Time-Slot Tag 
The ?time-slot? tag represents when an event 
occurs in five classes; ?morning?, ?daytime?, 
?evening?, ?night?, and ?time-unknown?. ?Time-
unknown? means that there is no temporal in-
formation. We set the criteria of time-slot tags as 
follows. 
Morning: 04:00--10:59 
from early morning till before noon, breakfast 
Daytime: 11:00--15:59 
from noon till before dusk, lunch 
Evening: 16:00--17:59 
from dusk till before sunset 
Night: 18:00--03:59 
from sunset till dawn, dinner 
Note that above criteria are just interpreted as 
rough standards. We think time-slot recognized 
by authors is more important. For example, in a 
case of ?about 3 o'clock this morning? we judge 
the case as ?morning? (not ?night?) with the ex-
pression written by the author ?this morning?. 
To annotate sentences in text, we used two dif-
ferent clues. One is the explicit temporal expres-
sions or time-associated words included in the 
sentence to be judged. The other is contextual 
information around the sentences to be judged. 
The examples corresponding to the former case 
are as follows: 
 
Example 1 
a. I went to post office by bicycle in the morning. 
b. I had spaghetti at restaurant at noon. 
c. I cooked stew as dinner on that day. 
 
Suppose that the two sentences in Example 2 
appear successively in a document. In this case, 
we first judge the first sentence as morning. Next, 
we judge the second sentence as morning by con-
textual information (i.e., the preceding sentence 
is judged as morning), although we cannot know 
the time period just from the content of the sec-
ond sentence itself. 
1154
4.2 Na?ve Bayes Classifier Example 2 
1. I went to X by bicycle in the morning. In this section, we describe multinomial model 
that is a kind of Na?ve Bayes classifiers. 2. I went to a shop on the way back from X. 
A generative probability of example x  given a 
category  has the form: c
3.2 Corpus Statistics 
We manually annotated the corpus. The number 
of the blog entries is 7,413. The number of sen-
tences is 70,775. Of 70,775, the number of sen-
tences representing any events1 is 14,220. The 
frequency distribution of time-slot tags is shown 
in Table 1. We can figure out that the number of 
time-unknown sentences is much larger than the 
other sentences from this table. This bias would 
affect our classification process. Therefore, we 
propose a method for tackling the problem. 
 
( ) ( ) ( ) ( )( )?= w
xwN
xwN
cwP
xxPcxP
,
|
!,|
,
?  (1) 
where ( )xP  denotes the probability that a sen-
tence of length x  occurs,  denotes the 
number of occurrences of w  in text 
( xwN , )
x . The oc-
currence of a sentence is modeled as a set of tri-
als, in which a word is drawn from the whole 
vocabulary.  
In time-slot classification, the x  is correspond 
to each sentence, the c  is correspond to one of 
time-slots in {morning, daytime, evening, night}. 
Features are words in the sentence. A detailed 
description of features will be described in Sec-
tion 4.5. 
morning 711 
daytime 599 
evening 207 
night 1,035 
time-unknown 11,668 
Total 14,220 
4.3 Incorporation of Unlabeled Data with 
the EM Algorithm 
 
Table 1: The numbers of time-slot tags. 
 The EM algorithm (Dempster et al, 1977) is a 
method to estimate a model that has the maximal 
likelihood of the data when some variables can-
not be observed (these variables are called latent 
variables). Nigam et al (2000) proposed a com-
bination of the Na?ve Bayes classifiers and the 
EM algorithm. 
4 Proposed Method 
4.1 Basic Idea 
Suppose, for example, ?breakfast? is a strong 
clue for the morning class, i.e. the word is a 
time-associated word of morning. Thereby we 
can classify the sentence ?I have cereal for 
breakfast.? into the morning class. Then ?cereal? 
will be a time-associated word of morning. 
Therefore we can use ?cereal? as a clue of time-
slot classification. By iterating this process, we 
can obtain a lot of time-associated words with 
bootstrapping method, improving sentence clas-
sification performance at the same time. 
Ignoring the unrelated factors of Eq. (1), we 
obtain 
 
( ) ( ) ( )??
w
xwNcwPcxP ,|,| ,?  (2) 
( ) ( ) ( ) ( )???
w
xwN
c
cwPcPxP .|| ,?  (3) 
We express model parameters as ? . 
If we regard c  as a latent variable and intro-
duce a Dirichlet distribution as the prior distribu-
tion for the parameters, the Q-function (i.e., the 
expected log-likelihood) of this model is defined 
as: 
To realize the bootstrapping method, we use 
the EM algorithm. This algorithm has a theoreti-
cal base of likelihood maximization of incom-
plete data and can enhance supervised learning 
methods. We specifically adopted the combina-
tion of the Na?ve Bayes classifier and the EM 
algorithm. This combination has been proven to 
be effective in the text classification (Nigam et 
al., 2000). 
 ( ) ( )( ) ( )
( ) ( ) ( ) ,|log
,|log|
, ???
????
?
?+=
?
??
?
w
xwN
Dx c
cwPcP
cxPPQ ????
 (4) 
where ( ) ( ) ( )( )( )? ? ??? c w cwPcPP 11 | ??? . ?  is a 
user given parameter and D  is the set of exam-
ples used for model estimation. 
 
                                                 
1 The aim of this study is time-slot classification of 
events. Therefore we treat only sentences expressing 
an event. 
We obtain the next EM equation from this Q-
function: 
1155
 
Figure 1: The flow of 2-step classification. 
 
 
E-step: 
( ) ( ) ( )( ) ( ),,|| ,||,| ?= c cxPcP
cxPcP
xcP ??
???  (5) 
M-step: 
( ) ( ) ( )( ) ,1 ,|1 DC xcPcP Dx +?+?= ? ?? ??  (6) 
( )
( ) ( ) ( )
( ) ( ) ( ) ,,,|1 ,,|1
|
? ?
?
?
?
+?
+?=
w Dx
Dx
xwNxcPW
xwNxcP
cwP
??
??
 (7) 
where C  denotes the number of categories, W  
denotes the number of features variety. For la-
beled example x , Eq. (5) is not used. Instead, ( )?,| xcP  is set as 1.0 if c  is the category of x , 
otherwise 0. 
Instead of the usual EM algorithm, we use the 
tempered EM algorithm (Hofmann, 2001). This 
algorithm allows coordinating complexity of the 
model. We can realize this algorithm by substi-
tuting the next equation for Eq. (5) at E-step: 
 
( ) ( ) ( ){ }( ) ( ){ } ,,|| ,||,| ?= c cxPcP
cxPcP
xcP ?
?
??
???  (8) 
where ?  denotes a hyper parameter for coordi-
nating complexity of the model, and it is positive 
value. By decreasing this hyper-parameter ? , we 
can reduce the influence of intermediate classifi-
cation results if those results are unreliable. 
Too much influence by unlabeled data some-
times deteriorates the model estimation. There-
fore, we introduce a new hyper-parameter 
( 10 ?? )??  which acts as weight on unlabeled 
data. We exchange the second term in the right-
hand-side of Eq. (4) for the next equation: 
( ) ( ) ( ) ( )
( ) ( ) ( ) ( ) ,|log,|
|log,|
,
,
? ??
? ??
?
?
???
????
?+
???
????
?
u
l
Dx w
xwN
c
Dx w
xwN
c
cwPcPxcP
cwPcPxcP
??
?
 
where lD  denotes labeled data, uD  denotes 
unlabeled data. We can reduce the influence of 
unlabeled data by decreasing the value of ? . 
We derived new update rules from this new Q-
function. The EM computation stops when the 
difference in values of the Q-function is smaller 
than a threshold. 
4.4 Class Imbalance Problem 
We have two problems with respect to ?time-
unknown? tag.  
The first problem is the class imbalance prob-
lem (Japkowicz 2000). The number of time-
unknown time-slot sentences is much larger than 
that of the other sentences as shown in Table 1. 
There are more than ten times as many time-
unknown time-slot sentences as the other sen-
tences.  
Second, there are no time-associated words in 
the sentences categorized into ?time-unknown?. 
Thus the feature distribution of time-unknown 
time-slot sentences is remarkably different from 
the others. It would be expected that they ad-
versely affect proposed method. 
There have been some methodologies in order 
to solve the class imbalance problem, such as 
Zhang and Mani (2003), Fan et al (1999) and 
Abe et al (2004). However, in our case, we have 
to resolve the latter problem in addition to the 
class imbalance problem. To deal with two prob-
lems above simultaneously and precisely, we 
develop a cascaded classification procedure. 
SVM 
NB + EM 
Step 2 
Time-Slot 
Classifier 
time-slot = time-unknown 
time-slot = morning, daytime, evening, night 
time-slot = morning 
time-slot = daytime 
time-slot = morning, daytime, evening, night, time-unknown Step1 
Time-Unknown 
Filter 
time-slot = night 
time-slot = evening 
1156
4.5 Time-Slot Classification Method 
It?s desirable to treat only ?time-known? sen-
tences at NB+EM process to avoid the above-
mentioned problems. We prepare another classi-
fier for filtering time-unknown sentences before 
NB+EM process for that purpose. Thus, we pro-
pose a classification method in 2 steps (Method 
A). The flow of the 2-step classification is shown 
in Figure 1. In this figure, ovals represent classi-
fiers, and arrows represent flow of data. 
The first classifier (hereafter, ?time-unknown? 
filter) classifies sentences into two classes; 
?time-unknown? and ?time-known?. The ?time-
known? class is a coarse class consisting of four 
time-slots (morning, daytime, evening, and 
night). We use Support Vector Machines as a 
classifier. The features we used are all words 
included in the sentence to be classified.  
The second classifier (time-slot classifier) 
classifies ?time-known? sentences into four 
classes. We use Na?ve Bayes classifier backed up 
with the Expectation Maximization (EM) algo-
rithm mentioned in Section 4.3.  
The features for the time-slot classifier are 
words, whose part of speech is noun or verb. The 
set of these features are called NORMAL in the 
rest of this paper. In addition, we use information 
from the previous and the following sentences in 
the blog entry. The words included in such sen-
tences are also used as features. The set of these 
features are called CONTEXT. The features in 
CONTEXT would be effective for estimating 
time-slot of the sentences as mentioned in Ex-
ample2 in Section 3.1. 
We also use a simple classifier (Method B) for 
comparison. The Method B classifies all time-
slots (morning ~ night, time-unknown) sentences 
at just one step. We use Na?ve Bayes classifier 
backed up with the Expectation Maximization 
(EM) algorithm at this learning. The features are 
words (whose part-of-speech is noun or verb) 
included in the sentence to be classified. 
 
5 Experimental Results and Discussion 
5.1 Time-Slot Classifier with Time-
Associated Words 
5.1.1 Time-Unknown Filter 
We used 11.668 positive (time-unknown) sam-
ples and 2,552 negative (morning ~ night) sam-
ples. We conducted a classification experiment 
by Support Vector Machines with 10-fold cross 
validation. We used TinySVM2 software pack-
age for implementation. The soft margin parame-
ter is automatically estimated by 10-fold cross 
validation with training data. The result is shown 
in Table 2. 
 
Table 2 clarified that the ?time-unknown? fil-
ter achieved good performance; F-measure of 
0.899. In addition, since we obtained a high re-
call (0.969), many of the noisy sentences will be 
filtered out at this step and the classifier of the 
second step is likely to perform well. 
 
Accuracy 0.878 
Precision 0.838 
Recall 0.969 
F-measure 0.899 
 
Table 2: Classification result of  
the time-unknown filter. 
 
5.1.2 Time-Slot Classification 
In step 2, we used ?time-known? sentences clas-
sified by the unknown filter as test data. We con-
ducted a classification experiment by Na?ve 
Bayes classifier + the EM algorithm with 10-fold 
cross validation. For unlabeled data, we used 
64,782 sentences, which have no intersection 
with the labeled data. The parameters, ?  and ? , 
are automatically estimated by 10-fold cross 
validation with training data. The result is shown 
in Table 3. 
 
Accuracy Method 
NORMAL CONTEXT
Explicit 0.109 
Baseline 0.406 
NB 0.567 0.464 
NB + EM 0.673 0.670 
Table 3: The result of time-slot classifier. 
                                                 
2 http://www.chasen.org/~taku/software/TinySVM 
1157
 
 
 
 
 
 
 
 
 
 
Table 4: Confusion matrix of output. 
 
 morning daytime evening night 
rank word p(c|w) word p(c|w) word p(c|w) word p(c|w)
1 this morning 0.729 noon 0.728 evening 0.750 last night 0.702 
2 morning 0.673 early after noon 0.674 sunset 0.557 night 0.689 
3 breakfast 0.659 afternoon 0.667 academy 0.448 fireworks 0.688 
4 early morning 0.656 daytime 0.655 dusk 0.430 dinner 0.684 
5 before noon 0.617 lunch 0.653 Hills 0.429 go to bed 0.664 
6 compacted snow 0.603 lunch 0.636 run on 0.429 night 0.641 
7 commute 0.561 lunch break 0.629 directions 0.429 bow 0.634 
8 --- 0.541 lunch 0.607 pinecone 0.429 overtime 0.606 
9 parade 0.540 noon 0.567 priest 0.428 year-end party 0.603 
10 wake up 0.520 butterfly 0.558 sand beach 0.428 dinner 0.574 
11 leave harbor 0.504 Chinese food 0.554 --- 0.413 beach 0.572 
12 rise late 0.504 forenoon 0.541 Omori 0.413 cocktail 0.570 
13 cargo work 0.504 breast-feeding 0.536 fan 0.413 me 0.562 
14 alarm clock 0.497 nap 0.521 Haneda 0.412 Tomoyuki 0.560 
15 --- 0.494 diaper 0.511 preview 0.402 return home 0.557 
16 sunglow 0.490 Japanese food 0.502 cloud 0.396 close 0.555 
17 wheel 0.479 star festival 0.502 Dominus 0.392 stay up late 0.551 
18 wake up 0.477 hot noodle 0.502 slip 0.392 tonight 0.549 
19 perm 0.474 pharmacy 0.477 tasting 0.391 night 0.534 
20 morning paper 0.470 noodle 0.476 nest 0.386 every night 0.521 
Table 5: Time-associated words examples. 
 
In Table 3, ?Explicit? indicates the result by a 
simple classifier based on regular expressions 3  
including explicit temporal expressions. The 
baseline method classifies all sentences into 
night because the number of night sentences is 
the largest. The ?CONTEXT? column shows the 
results obtained by classifiers learned with the 
features in CONTEXT in addition to the features 
                                                 
3 For example, we classify sentences matching follow-
ing regular expressions into morning class: 
[(gozen)(gozen-no)(asa) (asa-no)(am)(AM)(am-
no)(AM-no)][456789(10)] ji, [(04)(05)(06)(07)(08) 
(09)]ji, [(04)(05)(06)(07) (08) (09)]:[0-9]{2,2}, 
[456789(10)][(am)(AM)]. 
??gozen?, ?gozen?no? means before noon. ?asa?, 
?asa-no? means morning. ?ji? means o?clock.? 
in NORMAL. The accuracy of the Explicit 
method is lower than the baseline. This means 
existing methods based on explicit temporal ex-
pressions cannot work well in blog text. The ac-
curacy of the method 'NB' exceeds that of the 
baseline by 16%. Furthermore, the accuracy of 
the proposed method 'NB+EM' exceeds that of 
the 'NB' by 11%. Thus, we figure out that using 
unlabeled data improves the performance of our 
time-slot classification.  
In this experiment, unfortunately, CONTEXT 
only deteriorated the accuracy. The time-slot tags 
of the sentences preceding or following the target 
sentence may still provide information to im-
prove the accuracy. Thus, we tried a sequential 
tagging method for sentences, in which tags are 
output of time-slot classifier 
 morning daytime evening night time-unknown 
sum 
morning 332 14 1 37 327 711 
daytime 30 212 1 44 312 599 
evening 4 5 70 18 110 207 
night 21 19 4 382 609 1035 
tim
e-
sl
ot
 ta
g 
time-unknown 85 66 13 203 11301 11668 
sum 472 316 89 684 12659 14220 
1158
predicted in the order of their occurrence. The 
predicted tags are used as features in the predic-
tion of the next tag. This type of sequential tag-
ging method regard as a chunking procedure 
(Kudo and Matsumoto, 2000) at sentence level. 
We conducted time-slot (five classes) classifica-
tion experiment, and tried forward tagging and 
backward tagging, with several window sizes. 
We used YamCha4, the multi-purpose text chun-
ker using Support Vector Machines, as an ex-
perimental tool. However, any tagging direction 
and window sizes did not improve the perform-
ance of classification. Although a chunking 
method has possibility of correctly classifying a 
sequence of text units, it can be adversely biased 
by the preceding or the following tag. The sen-
tences in blog used in our experiments would not 
have a very clear tendency in order of tags. This 
is why the chunking-method failed to improve 
the performance in this task. We would like to 
try other bias-free methods such as Conditional 
Random Fields (Lafferty et al, 2001) for future 
work. 
5.1.3 2-step Classification 
Finally, we show an accuracy of the 2-step clas-
sifier (Method A) and compare it with those of 
other classifiers in Table 6. The accuracies are 
calculated with the equation: 
 
. 
 
In Table 6, the baseline method classifies all 
sentences into time-unknown because the num-
ber of time-unknown sentences is the largest. 
Accuracy of Method A (proposed method) is 
higher than that of Method B (4.1% over). These 
results show that time-unknown sentences ad-
versely affect the classifier learning, and 2-step 
classification is an effective method. 
Table 4 shows the confusion matrix corre-
sponding to the Method A (NORMAL). From 
this table, we can see Method A works well for 
classification of morning, daytime, evening, and 
night, but has some difficulty in 
 
                                                 
4 http://www.chasen.org/~taku/software/YamCha 
Table 6: Comparison of the methods for five 
class classification 
 
 
Figure 2: Change of # sentences that have time-
associated words: ?Explicit? indicates the num-
ber of sentences including explicit temporal ex-
pressions, ?NE-TIME? indicates the number of 
sentences including NE-TIME tag. 
 
classification of time-unknown. The 11.7% of 
samples were wrongly classified into ?night? or 
?unknown?. 
We briefly describe an error analysis. We 
found that our classifier tends to wrongly classify 
samples in which two or more events are written 
in a sentence. The followings are examples: 
 
Example 3 
a. I attended a party last night, and I got back 
on the first train in this morning because the 
party was running over. 
b. I bought a cake this morning, and ate it after 
the dinner. 
5.2 Examples of Time-Associated Words 
Table 5 shows some time-associated words ob-
tained by the proposed method. The words are 
sorted in the descending order of the value of ( )wcP | . Although some consist of two or three 
words, their original forms in Japanese consist of 
one word. There are some expressions appearing 
more than once, such as ?dinner?. Actually these 
expressions have different forms in Japanese. 
Meaningless (non-word) strings caused by mor-
Method Conclusive accuracy
Explicit 0.833 
Baseline 0.821 
Method A (NORMAL) 0.864 
Method A (CONTEXT) 0.862 
Method B 0.823 
0
1000
2000
3000
4000
5000
1 10 20 30 40 50 60 70 80 90 100
# time-associated words (N-best)
# 
se
nt
en
ce
s 
in
cl
ud
in
g 
ti
m
e-
as
so
ci
at
ed
 w
or
ds
   Explicit 
NE-TIME 
# time-unknown sentences correctly classi-
fied by the time-unknown filter 
# known sentences correctly classi-
fied by the time-slot classifier + 
# sentences with a time-slot tag value 
1159
phological analysis error are presented as the 
symbol ?---?. We obtained a lot of interesting 
time-associated words, such as ?commute (morn-
ing)?, ?fireworks (night)?, and ?cocktail (night)?. 
Most words obtained are significantly different 
from explicit temporal expressions and NE-
TIME expressions. 
Figure 2 shows the number of sentences in-
cluding time-associated words in blog text. The 
horizontal axis represents the number of time-
associated words. We sort the words in the de-
scending order of  and selected the top N 
words. The vertical axis represents the number of 
sentences including any N-best time-associated 
words. We also show the number of sentences 
including explicit temporal expressions, and the 
number of sentences including NE-TIME tag 
(Sekine and Isahara, 1999) for comparison. The 
set of explicit temporal expressions was ex-
tracted by the method described in Section 5.1.2. 
We used a Japanese linguistic analyzer ?Cabo-
Cha
( wcP | )
                                                
5 ? to obtain NE-TIME information. From 
this graph, we can confirm that the number of 
target sentences of our proposed method is larger 
than that of existing methods. 
 
6 Conclusion 
In our study, we proposed a method for identify-
ing when an event in text occurs. We succeeded 
in using a semi-supervised method, the Na?ve 
Bayes Classifier enhanced by the EM algorithm, 
with a small amount of labeled data and a large 
amount of unlabeled data. In order to avoid the 
class imbalance problem, we used a 2-step classi-
fier, which first filters out time-unknown sen-
tences and then classifies the remaining sen-
tences into one of 4 classes. The proposed 
method outperformed the simple 1-step method. 
We obtained 86.4% of accuracy that exceeds the 
existing method and the baseline method. 
 
References 
Naoki Abe, Bianca Zadrozny, John Langford. 2004. 
An Iterative Method for Multi-class Cost-sensitive 
Learning. In Proc. of the 10th. ACM SIGKDD, 
pp.3?11. 
Arthur P. Dempster, Nan M. laird, and Donald B. 
Rubin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the 
 
5 http://chasen.org/~taku/software/cabocha/ 
Royal Statistical Society Series B, Vol. 39, No. 1, 
pp.1?38. 
Wei Fan, Salvatore J. Stolfo, Junxin Zhang, Philip K. 
Chan. 1999. AdaCost: Misclassification Cost-
sensitive Boosting. In Proc. of ICML, pp.97?105. 
Thomas Hofmann. 2001. Unsupervised learning by 
probabilistic latent semantic analysis. Machine 
Learning, 42:177?196. 
Nathalie Japkowicz. 2000. Learning from Imbalanced 
Data Sets: A Comparison of Various Strategies. In 
Proc. of the AAAI Workshop on Learning from Im-
balanced Data Sets, pp.10 ?15. 
Taku Kudo, Yuji Matsumoto. 2000. Use of Support 
Vector Learning for Chunking Identification, In 
Proc of the 4th CoNLL, pp.142?144. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Probabil-
istic models for segmenting and labeling sequence 
data, In Proc. of ICML, pp.282?289. 
Inderjeet Mani, George Wilson 2000. Robust Tempo-
ral Processing of News. In Proc. of the 38th ACL, 
pp.69?76. 
Tomoyuki Nanno, Yasuhiro Suzuki, Toshiaki Fujiki, 
Manabu Okumura. 2004. Automatically Collecting 
and Monitoring Japanese Weblogs. Journal for 
Japanese Society for Artificial Intelligence ?
Vol.19, No.6, pp.511?520. (in Japanese) 
Kamal Nigam, Andrew McCallum, Sebastian Thrun, 
and Tom Mitchell. 2000. Text classification from 
labeled and unlabeled documents using EM. Ma-
chine Learning, Vol. 39, No.2/3, pp.103?134. 
Satoshi Sekine, Hitoshi Isahara. 1999. IREX project 
overview. Proceedings of the IREX Workshop. 
Andrea Setzer, Robert Gaizauskas. 2001.  A Pilot 
Study on Annotating Temporal Relations in Text. 
In Proc. of the ACL-2001 Workshop on Temporal 
and Spatial Information Processing, Toulose, 
France, July, pp.88?95. 
Seiji Tsuchiya, Hirokazu Watabe, Tsukasa Kawaoka. 
2005. Evaluation of a Time Judgement Technique 
Based on an Association Mechanism. IPSG SIG 
Technical Reports?2005-NL-168, pp.113?118. (in 
Japanese) 
Jianping Zhang, Inderjeet Mani. 2003. kNN Approach 
to Unbalanced Data Distributions: A Case Study 
involving Information Extraction. In Proc. of 
ICML Workshop on Learning from Imbalanced 
Datasets II., pp.42?48. 
1160
