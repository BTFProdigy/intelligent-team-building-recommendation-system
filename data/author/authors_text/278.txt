XMLTrans :  a Java-based XML Trans format ion  Language for 
S t ructured  Data  
Derek Walker and Dominique Petitpierre and Susan Armstrong 
{Derek. Walker, Dominique. Pet it:pierre, Susan. Armsl;rong}@? ssco. unige, ch 
ISSCO, University of Genew 
40 blvd. du Pont d 'Arve 
CH-1211 Genev~ 4 
Switzerland 
Abst ract  
The recently completed MLIS DieoPro project 
addressed the need tbr a uniform, platform- 
independent interface for: accessing multiple dic- 
tionaries and other lexical resources via the In- 
ternet/intranets. Lexical data supplied by dic- 
tionary publishers for the project was in a vari- 
ety of SGML forn\]ats. In order to transforrn this 
data to a convenient standard format (IJTML), 
a high level transformation language was devel- 
oped. This language is simple to use, yet power- 
ful enough to perlbrm complex transformations 
not possible with similar transformation tools. 
XMLTrans provides rooted/recursive transduc- 
tions, simila.r to tr,~nsducers used for na.tura.l 
language translation. The tool is written in 
standard .lava and is available to the general 
public. 
l Introduction 
The MMS l)icoPro project 1, which ran from 
April 11998 to Sept 1999, addressed the need for 
a uniIbrm, plattbrm-indel)endent i erface for 
accessing multiple dictionaries and other lexi- 
cal resources via the lnternet/intranets. One 
project deliverable was a client-server tool en- 
abling trm~slators and other language profes- 
sionals connected to an intranet o consult dic- 
tionaries and related lexica.1 data from multiple 
sources .  
Dictionary data was supplied by participat- 
ing dictionary publishers in a variety of propri- 
etary formats 2. One important DicoPro mod- 
ule wa.s a transformation language capable of 
1DicoPro was a project funded within the MullAlin- 
gum hfformation Society programme (MLIS), an EU ini- 
t iative launched by the European Commission's DG XIlI 
and the Swiss Federal OIrtce of Education and Science. 
2Project participants were: IlarperCollins, Hachette 
Livre, Oxford Unlversit~y Press. 
standardizing tile variety of lexical data. Tile 
language needed to be straightforward enough 
tbr ~ non-programnmr to master, yet powerful 
enough to perform all tile transfbrmations ec- 
essary to achieve tile desired output. The re- 
sult of our efforts, XMLTrans, takes as input 
a well-lbrmed XML file and a file containing a 
set of transformation rules and gives as output 
the.application of the rules to the input file. 
The transducer was designed tbr the processing 
of large XML files, keeping only the minimum 
necessary part of the document in memory at 
all times. This tool should be of use for: anyone 
wishing to tr~msform large amounts of (particu- 
larly lexical) data from one XML representation 
to another. 
At; the time XM1;l?rans was being developed 
(mid 11998), XML was only an emerging stan- 
dard. As a. consequence, we first looked to more 
esta.blished SGMI~ resources to find a. suitable 
trans\[brmation tool. Initial experimentation be- 
gan with I)SSSL (Binghaln, :1996) as a possible 
solution. Some time was invested in develop- 
ing a user-friendly "front-end" to the I)SSSL 
engine .jade developed by James Clark (Clark, 
1998). This turned out to be extremely cumber- 
some to implement, and was ~ba.ndoned. There 
were a number of commercial products such 
as Omnimark Light (Ominimark Corp; :1998), 
TXL (Legasys Corp; 1.998) and PatMI, (IBM 
Corp; 1998) which looked promising but could 
not be used since we wanted our transducer to 
be ill tile 1)ublic domain. 
We subsequently began to examine avail- 
able XML transduction resources. XSL (Clark, 
Deach, 11998) was still not mature nough to rely 
on as a core tbr tile language. In addition, XSL 
dkl not (at the time) provide for rooted, recur- 
sive transductions needed to convert the com- 
plex data structures found in l)icoPro's lexica.1 
1136 
d a.ta. 
F, din1)llrgh's La.ngua.ge 'lhchnology Group 
ha,d l)roduced a. nun~l)er of usefi,1 SGM\]ffXMI, 
ma.nipulaCion tools (I;.I'G, 11999). Un\['ortunately 
none of these ma.tched our specific needs. \]~br 
instance, ~.qmltrans does not permit matching 
of com l)lex expressions invoh, ing elements, text, 
and a?tributes. A nether I/FG tool, ~.qu)g is more 
powerful, 1)ut its control files have (in our opin- 
ion) a. non-intuitive and COml)lex syntax 3. 
Since a, large number of standardized XML 
APIs had been developed tbr the Java. program- 
ruing language this appeared to be a. prondsing 
direction. Ill addition, Java's portal)fifty was a. 
strong dra.wing point. The API model which 
best suited our needs was 1;he "Document Oh: 
ject Model" (DOM) with an underlying "Simple 
A Pl for XMI2' (SA X) I>arser. 
The event-based SAX parser reads into lneln- 
ory only the elements in the input document 
releva.nt o the tra.nsfornl alien. In efti.'(;t, X MI,- 
Tra.ns is intended 1;o 1)recess lexicaJ entries 
which a.re indel)en(lent of ca.cA other and tha.t 
ha.ve a. few basic formats. Since only one entry 
is ever in memory at a.ny given point in time, 
extremely la.rge files can be I)rocessed wil;h low 
nmmory overhea.d. 
The \])OM AI)I is used in the tra.nsforma.tion 
l)rocess to access the the element which is cur- 
rently in menlory. The element is tra.nsformed 
a.ccording to rules sl)ecilied in a. rule tile. These 
rules a.re interpreted by XMl/l'rans as opera- 
lions to l>erfbrnl on the data through I;llo I)OM 
A.PI. 
We begin with a s\]ml)le examl>le to illus- 
tra.te the kinds of transformations l>erlbrmed by 
XMLTrans. Then we introduce the language 
concepts a.nd structure of XMLTrans rules and 
rule files. A comparison of XMLT,:a.ns with 
XSLT will help situate our work with respecl; 
to the state-of-the-art in XML data processing. 
2 An  example  t rans format ion  
A typical dictiona, ry entry might ha.ve a. surpris- 
ingly complex structure. The various compo- 
nents of the entry: headword, pa.rt-ofst)eech , 
pronunciation, definitions, translations, nla.y 
themselves contain complex substructures. For 
\])icoPro, these structures were interl)reted in o f  
aThe UI'G have since developed another interesting 
t, ransformation tool called XMIA)erl. 
der 1;o construct I ITML output for tyl)ographi- 
cal rendition and also to extract indexing inibr- 
marion. 
A fictitious source entry might be of tile form: 
<entry> 
<hw>my word</hw> 
<defs> 
<def num="l">first def.</def> 
<def num="2">second def.</def> 
</defs> 
</entry> 
\'Ve would like to convert this entry to HTML, 
extra.cling tile headword fbr indexing pnrl)oses. 
Apl)lying the rules which are shown in section 
d, XML\]'rans generates the following outl)uC: 
<HTNL> 
<!-- INDEX="my word .... > 
<HEAD> 
<TITLE>my word</TITLE> 
</HEAD> 
<BODY> 
<Hi>my word</Hl> 
<OL> 
<LI VhLUE="l">first def.</Ll> 
<LI VhLUE="2">second def.</LI> 
</OL> 
</BODY> 
</HTNL> 
If" this were an actual dictionary, the XMI/l'rans 
1,ransducer would itera.te over all the entries in 
the dictiona.ry, converting ea.(:h in turn to the 
OUtl)Ut format above. 
3 Aspects  of  the  XMLTrans  
l anguage 
Each XMLTrans rule file contains a number of 
rule sets as described in tile next sections. 'l.'he 
transducer attempts to match each rule in tile 
set sequentially until either a rule m~tches or 
there are no more rules. 
The document I)TD is not used to check the 
validity of the input document. Consequenl;ly, 
input documents need not be valid XMI,, but 
must still be well-formed to be accel)ted by the 
parser. 
The rule syntax borrows heavily from tha.t of 
regular expressions and in so doing it allows for 
very concise and compact rule specifica.tion. As 
will be seen shortly, many simple rules can be 
expressed in a single short line. 
1137 
3.1  Ru le  Sets  
At tile top of an XMLTrans rule file at least 
one "trigger" is required to associate an XML 
element(e.g, an element containing a dictionary 
entry) with a collection of rules, called a "rule 
set ~" 
The syntax for a "trigger" is as follows: 
element_name : ~ ru le_set_name 
Multiple triggers can be used to allow different 
kinds of rules to process different kinds of ele- 
ments. For example: 
ENTRY : 0 normalEntryRules  
COMPOUNDENTRY : @ compoundEntryRules  
The rule set itself is declared with the following 
syntax: 
? \[rule set name\] 
For examl)le4: 
normalEntryRules  
; the rules for this set fo l low 
; the declarat ion. . .  
The rule set: is terminated either by the end of 
the file oi: with the declaration of another rule 
set. 
3.2  Var iab les  
In XMLTrans rule syntax, variables (prefaced 
with "$") m:e implicitly declared with their first 
use. There are two types of variables: 
? Element varial)les: created by an assign- 
ment of a pattern of elements to a. vari- 
M)le...For example: $a = LI, where <LI> 
is an element. Element variables can con- 
tain one or more elements. If a given vari- 
able $a contains a list of elements { A, B, 
C, . . .} ,  transforming $a will apply the 
transformation i sequence to <A>,  <13>, 
<C> and so on. 
? Attr ibute variables: created by an assign- 
ment of a pattern of attributes to a vari- 
able. For Example: LI \[ $a=TYPE \], where 
TYPE is a standard XML attribute. 
While variables are not strongly typed (i.e. a 
list of elements is not distinguished from an in- 
dividual element), attribute variables cannot be 
used in the place of element variables and vice 
versa. 
4XML~l}'ans comments are preceded by a semicolon. 
3.3  Ru les  
The basic control structure of XMLTrans is the 
rule, consisting of a left-hand side (LHS) and 
a right-hand side (RHS) separated by an arrow 
( " -  >"). The LHS is a pattern of XML ele- 
ment(s) to match while the RHS is a specitica- 
tion for a transfbrmation on those elements. 
a.a.1 The  Le f t -hand Side 
The basic building block of the M tS is the ele- 
ment pattern involving a single element, its at- 
tributes and children. 
XMLTrans allows for complex regular expres- 
sions of elements on the t i t s  to match over the 
children of the element being examined. The 
following rule will match an element <Z> which 
has exactly two children, <X> and <Y> (in the 
examples that \[Bllow "..." indicates any comple- 
tion of the rule): 
z{ x Y } -> . . . ;  
XMH?rans supports the notion of a logical NOT 
over an element expression. This is represented 
by the standard "\[" symbol. Support for gen- 
eral regular expressions is built into the lan- 
guage grammar: "Y*" will match 0 or more 
occurences of the element <Y>, "Y+" one or 
more occurences, and "g?" 0 o1" l occurences. 
In order to create rules of greater generality, 
elements and attributes in the LHS of a. rule 
can be assigned to variables. Per instance, we 
might want to transform a given element <X> 
in a certain way without specifying its children. 
The following rule would be used in such a case: 
; Match X with zero or more unspeci f ied 
; children. 
X{$a*}  -> . . . ;  
In tile rule above, the variable $a will be ei- 
ther empty (if <X> has no children), a single 
element (if <X> has one child), or a list of el- 
ements (if <X> has a series of children. Sinl- 
ilarly, the pattern X{$a} matches an dement 
<X> with exactly one child. 
If an expression contains complex patterns, 
it is often useful to assign specific parts to dif- 
ferent variables. This allows child nodes to be 
processed in groul)s on the billS, perhaps being 
re-used several times or reordered. Consider the 
following rule: 
Z{ $a = (X Y)* $b = Q} -> ... ; 
1138 
in this case $a contains a (possibly e,npty) list 
o\[' {<X>, <Y>} element l)airs. The variable Sb 
will contain exactly one <Q>. If' this pal;tern 
cannot be matched the rule will fail. 
Attribul;es may a,lso 1)e assigned to variables. 
'l"he following three rules demonstrate some l>OS- 
sibilities: 
; Match any X which has an att r ibute ATT 
X\[ Satt = ATT \] -> ...; 
; Match any X which has an at t r ibute  
; ATT with the value "VALUE". 
X\[ Satt = ATT == "VALUE"\] -> ...; 
; Match any X with an attribute 
; which is NOT equal to "VALUE" 
X\[ Satt = ATT != "VALUE"\] -> ...; 
The last tyl>e of exl)ressions used <)u the IAIS 
a.re string expressions. Strings are considered 
to l)e elements in their own right, but; they ~l,re 
enclosed in (luotes and cannot have atl;ribute 
patterns like regular e,h'ments (:an. A special 
syntax , / . * / ,  is used to mean a, ny element which 
is a string. The following are some sample string 
matching rules: 
; Match any string 
/ . , /  -> . . .  ; 
; Match text "suppress" & newline. 
"suppress\n"-> . . . ;  
3 .3 .2  The  R ight -hand S ide  
The R, II,q SUl)l)lies a COllStruction pa.ttern R)r tile 
tra, nsformed 1;tee node. 
A simple rule might be used to tel)lace a,n 
demenI, and its contents wit\]l some text: 
X -> "Hello world" 
l"or the input <X>Text</X>,  this rule, yiekls 
the oul;l)ut string Hello wor ld .  A more useful 
rule might strip off the enclosing element using 
a variable refhrence on the \]J IS : 
$X{$a*} -> $a 
For the input <X>Text</X>,  this rule gener- 
ates glle oul;l)lll; Text. Elements lnay also be re- 
nnmed while dmir contents remain unmodified. 
The tbllowing rule demonstrates this facility: 
$X{$a*} -> Y{$a} 
\]ibr the input <X>Text</X>,  the rule yields 
the outl)ut <Y>Text</Y>.  Note that any chil- 
dren o\[' <X> will be reproduced, regardless of 
whether ghey are text elements or not. 
Attribute varialJes may also be ,sed in XML- 
Trans rules. The rule below shows how this is 
aecomplished: 
X \[$a=ATT\] {$b*} -> Y \[OLDATT=$a\] {$b} 
Given the input <X ATT="VAL">Text</X>, 
the r.le yields the output <Y 
OLDATT="VAL" >Text  </Y  >. 
l{ecursion is a fundamenta,\[ concept used 
ill writing XMLTrans rules. The exl>ression 
@set_name(var iab lemame)  tells the XML- 
Trans transformer to continue processing on the 
elements contained ill tile indica.l;ed variable. 
l'br instance, @setl($a) indicates that the el-- 
ements contained in the va.l'ial)le $a shoukl be 
processed by the rules in the set setl. A spe: 
cial notation ? (var iab le~ame)  is used to tell 
t;he trausi'ormer to contin,e processing with the 
current rule set. Thus, if dm current rule set 
is set2, the expression @($a) indicates that 
\[)recessing sho,l<l coudnue on tile elelnent,s in 
Sa using the rule set set2 .  the following rule 
(lemonstra,tes how 1;r~llSOFlllalJOllS ca,n \])e ap- 
plied recusively to an element: 
X{$a*} -> Y{e($a)} 
"Text" -> "txeT" 
For the input element <\>Text</ \>,  the rule 
generai;es the output <Y>txeT</Y>. \])ifl'erent 
rule sets Call 1)e accessed as ill the following rule 
file segment: 
X : setl 
@ setl 
X{$a*} -> Y{?set2($a)}  
"Text" -> "txeT" 
@ set2 
"Text" -> "Nothing" 
Initially, set1 is invoked to process the el<;= 
merit <X>,  but then the rule set set2 is in- 
yoked to 1)recess its children. Consequently, 
for the input <\>Text</ \>,  the outing; is 
<Y>Nothing</Y>. 
1139 
4 Rules for the example 
t rans for lnat ion  
The transformation of the example ill section 
2 can be achieved with a few XMLTrans rules. 
The main rule treats the <entry> element, cre- 
ating a HTML document fl'om it, and copying 
the headword to several places. The subsequent 
rules generate the HTML output from section 2: 
entry : ? entrySet 
@ entrySet 
entry{$a=hw Sb=defs*} 
-> HTML?"<!-- INDEX=" Sa .... >" 
HEAD{TITLE{$a} BODY{HI{$a} 
?($b)}} 
defs?$a=def*} -> 0L{@($a)} 
def \[$att=NUM\] ?$a*} 
->LI \[VALUE=$att\] {$a} 
5 Colnparison with XSLT 
The advent of stable versions of XSLT (Clark, 
2000) has dramatically changed the landscape 
of XML transformations, so it is interesting to 
compare XMLTrans with recent developments 
with XSLT. 
lit is evident that the set of transformations 
described by the XMLTrans transformation lan- 
guage is a subset of those described by XSLT. In 
addition, XSLT is integrated with XSL allowing 
the style sheet author to access to the rendering 
aspects of XSL such as \[brmatting objects. 
Untbrtunately, it takes some time to learn 
the syntax of XSL and the various aspects of 
XSLT, such as XPath specifications. This task 
may be particularly difficult for those with no 
prior experience with SGML/XML documents. 
In contrast, one needs only have a knowledge of 
regular expressions to begin writing rules with 
XMLTrans. 
6 Conc lus ion  
The XMLTrans transducer was used to success- 
fully convert all the lexical data for the l)icoPro 
project. There were 3 bilingual dictionairies and 
one monoligual dictionary totalling 140 Mb in 
total( average size of 20 MB), each requiring its 
own rule file (and sometimes a rule file for each 
language pair direction). Original SGML files 
were preprocessed to provide XMLTrans with 
pure, well-formed XML input. Inputs were in 
a variety of XML formats, and the output was 
HTMI,  Rule files had an average of 178 rules, 
and processing time per dictionary was aI)proxi- 
lnately I hour (including pre- and postprocesss- 
ing steps). 
This paper has presented the XMI,Trans 
tra.nsduction language. The code is portable 
and should be executable on any platform for 
which a .\]aw~ runtime environment exists. A 
free version of XMLTrans can be downloaded 
fromS: h t tp  :/ /?ssco-www. unige, ch/proj  ects  
/d i copro_publ i c/XMLTrans / 
References 
Bingham, 11.:1996 q)SSSL Syntax ,.qunlnlal'y In- 
dex', a.t http://www.tiac.net/uscrs/bingham/ 
dssslsyn/indcx, htm 
Clark, J.:1998 'Jade - James' \])SSSL Engine', 
at http://www.jclark.com/iadc/ 
Clark, J. Ed.:2000 'XSL Transformations 
(XSLT) Version 1.0: W3C Recommendation 
16 November 1999,' at 
http://www, w3. ow/TR /1999/l~l'~'C-a:slt- 
19991116 
Clark, J. and Deach, S. eds.:1998 'Extensible 
Stylesheet Language (XSL) Version 1.0 W3C 
Working Draft 16-December-1998' at 
h~p://w,~w, a.o,~j/Tl~/i OgS/WD-.,.sl- 
19981210 
Glazman, D.:\]998 'Siml)le Tree 'l'ransformation 
Sheets 3', at htlp://www, w3. org/77~,/NO I'1~'- 
,~7"/S'3 
IBM Corp.:1999 qBM/Alphawork's l)atML ', at 
h ttp :///www. alph, aWorks. ibm. com /tech /patml 
Language Technology Group:1999 q,T XML 
version 1.1' at 
http://www. Itg. cd. ac. uk/softwarc/xml/indcx, htmI 
Legasys Corp.:1998 'The TXL Source Transfor- 
mat;ion System', at 
http://www.qucis.quccnsu.ca/ Icg sys/ 
TXL_lnJ'o findcx, h t ml 
Omnimark Corp.:1998 'Omnima.rk Corporation 
Home Page', at 
http ://www. omnimark, corn~ 
5Users will also need Sun's SAX and DOM Java 
libraries (Java Project X) available from: 
http : / / j  ava. sun. com/product s/ j  avapro j ectx/index, html: 
1140 
A Comparative Study of Mixture Models for Automatic Topic Segmentation
of Multiparty Dialogues
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
susan.armstrong@issco.unige.ch
Abstract
In this article we address the task of auto-
matic text structuring into linear and non-
overlapping thematic episodes at a coarse
level of granularity. In particular, we
deal with topic segmentation on multi-party
meeting recording transcripts, which pose
specific challenges for topic segmentation
models. We present a comparative study
of two probabilistic mixture models. Based
on lexical features, we use these models in
parallel in order to generate a low dimen-
sional input representation for topic segmen-
tation. Our experiments demonstrate that in
this manner important information is cap-
tured from the data through less features.
1 Introduction
Some of the earliest research related to the prob-
lem of text segmentation into thematic episodes used
the word distribution as an intrinsic feature of texts
(Morris and Hirst, 1991). The studies of (Reynar,
1994; Hearst, 1997; Choi, 2000) continued in this
vein. While having quite different emphasis at dif-
ferent levels of detail (basically from the point of
view of the employed term weighting and/or the
adopted inter-block similarity measure), these stud-
ies analyzed the word distribution inside the texts
through the instrumentality of merely one feature,
i.e. the one-dimensional inter-block similarity.
More recent work use techniques from graph the-
ory (Malioutov and Barzilay, 2006) and machine
learning (Galley et al, 2003; Georgescul et al,
2006; Purver et al, 2006) in order to find patterns
in vocabulary use.
We investigate new approaches for topic segmen-
tation on corpora containing multi-party dialogues,
which currently represents a relatively less explored
domain. Compared to other types of audio content
(e.g. broadcast news recordings), meeting record-
ings are less structured, often exhibiting a high de-
gree of participants spontaneity and there may be
overlap in finishing one topic while introducing an-
other. Moreover while ending the discussion on a
certain topic, there can be numerous new attempts
to introduce a new topic before it becomes the fo-
cus of the dialogue. Therefore, the task of automatic
topic segmentation of meeting recordings is more
difficult and requires a more refined analysis. (Gal-
ley et al, 2003; Georgescul et al, 2007) dealt with
the problem of topic segmentation of multiparty di-
alogues by combining various features based on cue
phrases, syntactic and prosodic information. In this
article, our investigation is based on using merely
lexical features.
We study mixture models in order to group the
words co-occurring in texts into a small number
of semantic concepts in an automatic unsupervised
way. The intuition behind these models is that a
text document has an underlying structure of ?la-
tent? topics, which is hidden. In order to reveal
these latent topics, the basic assumption made is that
words related to a semantic concept tend to occur in
the proximity of each other. The notion of proxim-
ity between semantically related words can vary for
various tasks. For instance, bigrams can be consid-
ered to capture correlation between words at a very
925
short distance. At the other extreme, in the domain
of document classification, it is often assumed that
the whole document is concerned with one specific
topic and in this sense all words in a document are
considered to be semantically related. We consider
for our application that words occurring in the same
thematic episode are semantically related.
In the following, the major issues we will discuss
include the formulations of two probabilistic mix-
ture approaches, their methodology, aspects of their
implementation and the results obtained when ap-
plied in the topic segmentation context. Section 2
presents our approach on using probabilistic mix-
ture models for topic segmentation and shows com-
parisons between these techniques. In Section 3 we
discuss our empirical evaluation of these models for
topic segmentation. Finally, some conclusions are
drawn in Section 4.
2 Probabilistic Mixture Models
The probabilistic latent models described in the fol-
lowing exploit hierarchical Bayesian frameworks.
Based on prior distributions of word rate variability
acquired from a training corpus, we will compute a
density function to further analyze the text content in
order to perform topic segmentation at a coarse level
of granularity. In this model, we will be working
with ?blocks? of text which consist of a fixed num-
ber of consecutive utterances.
In the following two subsections, we use the fol-
lowing notation:
? We consider a text corpus B = {b1, b2, ..., bM}
containing M blocks of text with words from
a vocabulary W = {w1, w2, ..., wN}. M is
a constant scalar representing the number of
blocks of text. N is a constant scalar represent-
ing the number of terms in vocabulary W .
? We pre-process the data by eliminating con-
tent free words such as articles, prepositions
and auxiliary verbs. Then, we proceed by lem-
matizing the remaining words and by adopt-
ing a bag-of-words representation. Next,
we summarize the data in a matrix F =
(f(bi, wi,j))(i,j)?M?N , where f(bi, wi,j) de-
notes the log.entropy weighted frequency of
word wi,j in block bi.
? Each occurrence of a word in a block of
text is considered as representing an ob-
servation (wm,n, bm), i.e. a realization from
an underlying sequence of random variables
(Wm,n, Bm)
1?m?M
1?n?N . wm,n denotes the term
indicator for the n-th word in the m-th block
of text.
? Each pair (wm,n, bm) is associated with a dis-
crete hidden random variable Zm,n over some
finite set Z ={z1, z2, ..., zK}. K is a constant
scalar representing the number of mixture com-
ponents to generate.
? We denote by P (zm,n = zk) or simply by
P (zk) the probability that the k-th topic has
been sampled for the n-th word in the m-th
block of text.
2.1 Aspect Model for Dyadic Data (AMDD)
In this section we describe how we apply latent mod-
eling for dyadic data (Hofmann, 2001) to text repre-
sentation for topic segmentation.
2.1.1 Model Setting
 
 
 
 
 
n,mw  
n,mz  
mb  
M 
block  plate 
n,mw  
n,mz  
mb  
block  plate 
M 
word  plate 
N 
word  plate 
N 
1) Asymmetric PLSA parameterization 2) Symmetric PLSA parameterization 
Figure 1: Graphical model representation of the as-
pect model.
We express the joint or conditional probability
of words and blocks of text, by assuming that the
choice of a word during the generation of a block
of text is independent of the block itself, given some
(unobserved) hidden variable, also called latent vari-
able or aspect.
The graphical representation of the AMDD data
generation process is illustrated in Figure 1 by using
926
the plate notation. That is, the ovals (i.e. the nodes
of the graph) represent probabilistic variables. The
double ovals around the variables wm,n and bm de-
note observed variables. zm,n is the mixture indi-
cator, the hidden variable, that chooses the topic for
the n-th word in the m-th block of text. Arrows in-
dicate conditional dependencies between variables.
For instance, the wm,n variable in the word space
and the bm variable in the block space have no di-
rect dependencies, i.e. it is assumed that the choice
of words in the generation of a block of text is in-
dependent of the block given a hidden variable. The
boxes represent ?plates?, i.e. replicates of sampling
steps with the variable in the lower left corner re-
ferring to the number of samples. For instance, the
?word plate? in Figure 1 illustrates N independently
and identically distributed repeated trials of the ran-
dom variable wm,n.
According to the topology of the asymmetric
AMDD Bayesian network from Figure 1, we can
specify the joint distribution of a word wm,n, a latent
topic zk and a block of text bm: P (wm,n, zk, bm) =
P (bm) ? P (zk|bm) ? P (wm,n|zk). The joint distribu-
tion of a block of text bm and a word wm,n is thus:
P (bm, wm,n) =
K?
k=1
P (wm,n, zk, bm) = P (bm)
?
?K
k=1 P (zk|bm)? ?? ?
mixing proportions
? P (wm,n|zk)
? ?? ?
mixture components
(1)
Equation 1 describes a special case of a finite mix-
ture model, i.e. it uses a convex combination of a set
of component distributions to model the observed
data. That is, each word in a block of text is seen
as a sample from a mixture model, where mixture
components are multinomials P (wm,n|zk) and the
mixing proportions are P (zk|bm).
2.1.2 Inferring and Employing the AMDD
Model
The Expectation-Maximization (EM) algorithm is
the most popular method to estimate the parameters
for mixture models to fit a training corpus. The
EM algorithm for AMDD is based on iteratively
maximizing the log-likelihood function: LPLSA =?M
m=1
?N
n=1f(bm, wm,n) ? logP (wm,n, bm). How-
ever, the EM algorithm for AMDD is prone to over-
fitting since the number of parameters to be esti-
mated grows linearly with the number of blocks of
text. In order to avoid this problem, we employed
the tempered version of the EM algorithm that has
been proposed by Hofmann (2001).
We use the density estimation method in AMDD
to reduce the dimension of the blocks-by-words
space. Thus, instead of using the words as ba-
sic units for each block of text representation, we
employ a ?topic? basis, assuming that a few top-
ics will capture more information than the entire
huge amount of words in the vocabulary. Thus,
the m-th block of text is represented by the vector
(P (z1|bm), P (z2|bm), ..., P (zk|bm)). Then, we use
these posterior probabilities as a threshold to iden-
tify the boundaries of thematic episodes via sup-
port vector classification (Georgescul et al, 2006).
That is, we consider the topic segmentation task as a
binary-classification problem, where each utterance
should be classified as marking the presence or the
absence of a topic shift in the dialogue.
2.2 Latent Dirichlet Allocation (LDA)
Latent Dirichlet Allocation (Blei et al, 2003) can
be seen as an extension of AMDD by defining a
probabilistic mixture model that includes Dirichlet-
distributed priors over the masses of the multinomi-
als P (w|z) and P (z|b).
2.2.1 Model Setting
In order to describe the formal setting of LDA in
our context, we use the following notation in addi-
tion to those given at the beginning of Section 2:
? ~?m is a parameter notation for P (z|b = bm),
the topic mixture proportion for the m-th block
of text;
? ~? is a hyperparameter (a vector of dimension
K) on the mixing proportions ~?m;
? ? =
{
~?m
}M
m=1
is a matrix (of dimension
M ? K), composed by placing the vectors
~?1, ~?2, ..., ~?M as column components;
? ~?k is a parameter notation for P (w|zk), the
mixture component for topic k;
? ~? is a hyperparameter (a vector of dimension
N ) on the mixture components ~?k ;
927
? ? = {~?k}
K
k=1 is a matrix of dimension
K ? N composed by placing the vectors
~?1, ~?2, ..., ~?K as column components;
? Nm denotes the length of the m-th block of text
and is modeled with a Poisson distribution with
constant parameter ?;
 
 
 
 
 
word plate 
?
?  
??  
topic plate 
K 
n,mw  ?k?  
n,mz  
Nm 
m
??  
M 
block  plate 
Figure 2: Graphical model representation of latent
Dirichlet alocation.
LDA generates a stream of observable words
wm,n partitioned into blocks of text ~bm as shown
by the graphical model in Figure 2. The Bayesian
network can be interpreted as follows: the variables
?, ? and z are the three sets of latent variables that
we would like to infer. The plate surrounding ~?k il-
lustrates the repeated sampling of word distributions
for each topic zk until K topics have been generated.
The plate surrounding ~?m illustrates the sampling of
a distribution over topics for each block b for a to-
tal of M blocks of text. The inner plate over zm,n
and wm,n illustrates the repeated sampling of topics
and words until Nm words have been generated for
a block~bm.
Each block of text is first generated by drawing
a topic proportion ~?m, i.e. by picking a distribution
over topics from a Dirichlet distribution. For each
word wm,n from a block of text~bm, a topic indicator
k is sampled for zm,n according to the block-specific
mixture proportion ~?m. That is, ~?m determines
P (zm,n). The topic probabilities ~?k are also sam-
pled from a Dirichlet distribution. The words in each
block of text are then generated by using the corre-
sponding topic-specific term distribution ~?zm,n .
Given the graphical representation of LDA illus-
trated in Figure 2, we can write the joint distribution
of a word wm,n and a topic zk as:
P (wm,n, zk|~?m,?) = P (zk|~?m) ? P (wm,n|~?k).
Summing over k, we obtain the marginal distribu-
tion:
P (wm,n|~?m,?) =
?K
k=1
?
?
? P (zk|~?m)
? ?? ?
mixture proportion
? P (wm,n|~?k)
? ?? ?
mixture component
?
?
?.
Hence, similarly to AMDD (see Equation 1), the
LDA model assumes that a word wm,n is generated
from a random mixture over topics. Topic proba-
bilities are conditioned on the block of text a word
belongs to. Moreover LDA leaves flexibility to
assign a different topic to every observed word and
a different proportion of topics for every block of
text.
The joint distribution of a block of text ~bm
and the latent variables of the model ~zm, ~?m,
?, given the hyperparameters ~?, ~? is further
specified by: P (~bm, ~zm, ~?m,?|~?, ~?) =
topic plate
? ?? ?
P (?|~?) ?
P (~?m|~?) ?
Nm?
n=1
word plate
? ?? ?
P (zm,n|~?m) ? P (wm,n|~?zm,n)
? ?? ?
block plate
.
Therefore, the likelihood of a block~bm is derived
as the marginal distribution obtained by summing
over the zm,n and integrating out the distributions
~?m and ?.
2.2.2 Inferring and Employing the LDA Model
Since the integral involved in computing the like-
lihood of a block ~bm is computationally intractable,
several methods for approximating this posterior
have been proposed, including variational expecta-
tion maximization (Blei et al, 2003) and Markov
chain Monte Carlo methods (Griffiths and Steyvers,
2004).
We follow an approach based on Gibbs sampling
as proposed in (Griffiths and Steyvers, 2004). As
the convergence criteria for the Markov chain, we
928
check how well the parameters cluster semantically
related blocks of text in a training corpus and then
we use these values as estimates for comparable set-
tings.
The LDA model provides a soft clustering of the
blocks of text, by associating them to topics. We
exploit this clustering information, by using the dis-
tribution of topics over blocks of text to further
measure the inter-blocks similarity. As in Section
2.1.2, the last step of our system consists in em-
ploying binary support vector classification to iden-
tify the boundaries of thematic episodes in the text.
That is, we consider as input features for support
vector learning the component values of the vector
(?m,z1 , ?m,z2 , ..., ?m,zk).
3 Experiments
In order to evaluate the performance of AMDD and
LDA for our task of topic segmentation, in our ex-
periments we used the transcripts of ICSI-MR cor-
pus (Janin et al, 2004), which consists of 75 meet-
ing recordings. A subset of 25 meetings, which are
transcribed by humans and annotated with thematic
boundaries (Galley et al, 2003), has been kept for
testing purposes and support vector machine train-
ing. The transcripts of the remaining 50 meetings
have been used for the unsupervised inference of
our latent models. The fitting phase of the mix-
ture models rely on the same data set that have been
pre-processed by tokenization, elimination of stop-
words and lemmatization.
Once the models? parameters are learned, the in-
put data representation is projected into the lower
dimension latent semantic space. The evaluation
phase consists in checking the performance of each
model for predicting thematic boundaries. That is,
we check the performance of the models for predict-
ing thematic boundaries on the same test set. The
size of a block of text during the testing phase has
been set to one, i.e. each utterance has been consid-
ered as a block of text.
Figure 3 compares the performance obtained for
various k values, i.e. various dimensions of the latent
semantic space, or equivalently different numbers of
latent topics. We have chosen k={50, ...400} using
incremental steps of 50.
The performance of each latent model is mea-
0.000
0
0.100
0
0.200
0
0.300
0
0.400
0
0.500
0
0.600
0
0.700
0
0.800
0
0.900
0
50
100
150
200
250
300
350
400
Laten
t spa
ce di
men
sion
Accuracy
PLSA LDA
Figure 3: Results of applying the mixture models for
topic segmentation.
sured by the accuracy Acc = 1 ? Pk, where Pk
denotes the error measure proposed by (Beeferman
et al, 1999). Note that the Pk error allows for a
slight variation in where the hypothesized thematic
boundaries are placed. That is, wrong hypothesized
thematic boundaries occurring in the proximity of
a reference boundary (i.e. in a fixed-size interval of
text) are tolerated. As proposed by (Beeferman et
al., 1999), we set up the size of this interval to half
of the average number of words per segment in the
gold standard segmentation.
As we observe from Figure 3, LDA and AMDD
achieved rather comparable thematic segmenta-
tion accuracy. While LDA steadily outperformed
AMDD, the results do not show a notable advan-
tage of LDA over AMDD. In contrast, AMDD has
better performances for less dimensionality reduc-
tion. That is, the LDA performance curve goes down
when the number of latent topics exceeds over 300.
LDA LCSeg SVMs
Pk error rate 21% 32 % 22%
Table 1: Comparative performance results.
In Table 1, we provide the best results obtained
on ICSI data via LDA modeling. We also reproduce
the results reported on in the literature by (Galley
et al, 2003) and (Georgescul et al, 2006), when
the evaluation of their systems was also done on
ICSI data. The LCSeg system proposed by (Gal-
ley et al, 2003) is based on exploiting merely lex-
ical features. Improved performance results have
929
been obtained by (Galley et al, 2003) when extra
non-lexical features have been adopted in a decision
tree classifier. The system proposed by (Georges-
cul et al, 2006) is based on support vector machines
(SVMs) and is labeled in the table as SVMs. We
observe from the table that our approach based on
combining LDA modeling with SVM classification
outperforms LCSeg and performs comparably to the
system of Georgescul et al (2006). Thus, our exper-
iments show that the LDA word density estimation
approach does capture important information from
the data through 90% less features than a bag-of-
words representation.
4 Conclusions
With the goal of performing linear topic segmen-
tation by exploiting word distributions in the input
text, the focus of this article was on both comparing
theoretical aspects and experimental results of two
probabilistic mixture models. The algorithms are
applied to a meeting transcription data set and are
found to provide an appropriate method for reduc-
ing the size of the data representation, by perform-
ing comparably to previous state-of-the-art methods
for topic segmentation.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34:177?210. Special Issue on Natural
Language Learning.
David M. Blei, Andrew Y. Ng, and Michael Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, pages 993?1022.
Freddy Choi. 2000. Advances in Domain Indepen-
dent Linear Text Segmentation. In Proceedings of the
1st Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL),
Seattle, USA.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multi-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 562?569,
Sapporo, Japan.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. Word Distributions for Thematic Seg-
mentation in a Support Vector Machine Approach. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 101?108,
New York City, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2007. Exploiting Structural Meeting-Specific
Features for Topic Segmentation. In Actes de la
14e`me Confe?rence sur le Traitement Automatique des
Langues Naturelles (TALN), pages 15?24, Toulouse,
France.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the National
Academy of Sciences, volume 101, pages 5228?5235.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177?196.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004.
The ICSI Meeting Project: Resources and Research.
In Proceedings of the International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
Meeting Recognition Workshop, Montreal, Quebec,
Canada.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (COL-
ING/ACL), pages 25?32, Sydney, Australia.
Jane Morris and Graeme Hirst. 1991. Lexical Cohe-
sion Computed by Thesaural Relations as an Indicator
of the Structure of Text. Computational Linguistics,
17(1):21?48.
Matthew Purver, Konrad P. Ko?rding, Thomas L. Grif-
fiths, and Joshua B. Tenenbaum. 2006. Unsupervised
Topic Modelling for Multi-Party Spoken Discourse.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL), pages 17?24, Sydney, Australia.
Jeffrey Reynar. 1994. An Automatic Method of Finding
Topic Boundaries. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 331?333, Las Cruces, New Mexico,
USA.
930
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 49?52,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Archivus: A multimodal system for multimedia meeting browsing and
retrieval
Marita Ailomaa, Miroslav Melichar,
Martin Rajman
Artificial Intelligence Laboratory
?Ecole Polytechnique Fe?de?rale de Lausanne
CH-1015 Lausanne, Switzerland
marita.ailomaa@epfl.ch
Agnes Lisowska,
Susan Armstrong
ISSCO/TIM/ETI
University of Geneva
CH-1211 Geneva, Switzerland
agnes.lisowska@issco.unige.ch
Abstract
This paper presents Archivus, a multi-
modal language-enabled meeting brows-
ing and retrieval system. The prototype
is in an early stage of development, and
we are currently exploring the role of nat-
ural language for interacting in this rela-
tively unfamiliar and complex domain. We
briefly describe the design and implemen-
tation status of the system, and then focus
on how this system is used to elicit useful
data for supporting hypotheses about mul-
timodal interaction in the domain of meet-
ing retrieval and for developing NLP mod-
ules for this specific domain.
1 Introduction
In the past few years, there has been an increasing
interest in research on developing systems for effi-
cient recording of and access to multimedia meet-
ing data1. This work often results in videos of
meetings, transcripts, electronic copies of docu-
ments referenced, as well as annotations of various
kinds on this data. In order to exploit this work, a
user needs to have an interface that allows them to
retrieve and browse the multimedia meeting data
easily and efficiently.
In our work we have developed a multimodal
(voice, keyboard, mouse/pen) meeting browser,
Archivus, whose purpose is to allow users to ac-
cess multimedia meeting data in a way that is most
natural to them. We believe that since this is a new
domain of interaction, users can be encouraged to
1The IM2 project http://www.im2.ch, the AMI project
www.amiproject.org, The Meeting Room Project at Carnegie
Mellon University, http://www.is.cs.cmu.edu/mie, and rich
transcription of natural and impromptu meetings at ICSI,
Berkeley, http://www.icsi.berkeley.edu/Speech/EARS/rt.html
try out and consistently use novel input modalities
such as voice, including more complex natural lan-
guage, and that in particular in this domain, such
multimodal interaction can help the user find in-
formation more efficiently.
When developing a language interface for an in-
teractive system in a new domain, the Wizard of
Oz (WOz) methodology (Dahlba?ck et al, 1993;
Salber and Coutaz, 1993) is a very useful tool.
The user interacts with what they believe to be a
fully automated system, when in fact another per-
son, a ?wizard? is simulating the missing or incom-
plete NLP modules, typically the speech recogni-
tion, natural language understanding and dialogue
management modules. The recorded experiments
provide valuable information for implementing or
fine-tuning these parts of the system.
However, the methodology is usually applied
to unimodal (voice-only or keyboard-only) sys-
tems, where the elicitation of language data is not
a problem since this is effectively the only type of
data resulting from the experiment. In our case, we
are developing a complex multimodal system. We
found that when the Wizard of Oz methodology
is extended to multimodal systems, the number of
variables that have to be considered and controlled
for in the experiment increases substantially. For
instance, if it is the case that within a single inter-
face any task that can be performed using natural
language can also be performed with other modal-
ities, for example a mouse, the user may prefer
to use the other ? more familiar ? modality for
a sizeable portion of the experiment. In order to
gather a useful amount of natural language data,
greater care has to be taken to design the system
in a way that encourages language use. But, if
the goal of the experiment is also to study what
modalities users find more useful in some situa-
49
Figure 1: The Archivus Interface
tions compared to others, language use must be
encouraged without being forced, and finding this
balance can be very hard to achieve in practice.
2 Design and implementation
The Archivus system has been designed to sat-
isfy realistic user needs based on a user require-
ment analysis (Lisowska, 2003), where subjects
were asked to formulate queries that would enable
them to find out ?what happened at a meeting?.
The design of the user interface is based on the
metaphor of a person interacting in an archive or
library (Lisowska et al, 2004).
Furthermore, Archivus is flexibly multimodal,
meaning that users can interact unimodally choos-
ing one of the available modalities exclusively,
or multimodally, using any combination of the
modalities. In order to encourage natural lan-
guage interaction, the system gives textual and vo-
cal feedback to the user. The Archivus Interface
is shown in Figure 1. A detailed description of all
of the components can be found in Lisowska et al
(2004).
Archivus was implemented within a software
framework for designing multimodal applications
with mixed-initiative dialogue models (Cenek et
al., 2005). Systems designed within this frame-
work handle interaction with the user through
a multimodal dialogue manager. The dialogue
manager receives user input from all modalities
(speech, typing and pointing) and provides mul-
timodal responses in the form of graphical, textual
and vocal feedback.
The dialogue manager contains only linguistic
knowledge and interaction algorithms. Domain
knowledge is stored in an SQL database and is ac-
cessed by the dialogue manager based on the con-
straints expressed by the user during interaction.
The above software framework provides sup-
port for remote simulation or supervision of
some of the application functionalities. This fea-
ture makes any application developed under this
methodology well suited for WOz experiments. In
the case of Archivus, pilot experiments strongly
suggested the use of two wizards ? one supervising
the user?s input (Input Wizard) and the other con-
trolling the natural language output of the system
(Output Wizard). Both wizards see the user?s in-
put, but their actions are sequential, with the Out-
put Wizard being constrained by the actions of the
Input Wizard.
The role of the Input Wizard is to assure that
the user?s input (in any modality combination)
is correctly conveyed to the system in the form
of sets of semantic pairs. A semantic pair (SP)
is a qualified piece of information that the dia-
50
logue system is able to understand. For exam-
ple, a system could understand semantic pairs such
as date:Monday or list:next. A user?s
utterance ?What questions did this guy ask in
the meeting yesterday?? combined with point-
ing on the screen at a person called ?Raymond?
could translate to dialogact:Question,
speaker:Raymond, day:Monday.
In the current version of Archivus, user clicks
are translated into semantic pairs automatically by
the system. Where written queries are concerned,
the wizard sometimes needs to correct automat-
ically generated pairs due to the currently low
performance of our natural language understand-
ing module. Finally since the speech recognition
engine has not been implemented yet, the user?s
speech is fully processed by a wizard. The Input
Wizard also assures that the fusion of pairs coming
from different modalities is done correctly.
The role of the Output Wizard is to monitor, and
if necessary change the default prompts that are
generated by the system. Changes are made for
example to smooth the dialogue flow, i.e. to bet-
ter explain the dialogue situation to the user or to
make the response more conversational. The wiz-
ard can select a prompt from a predefined list, or
type a new one during interaction.
All wizards? actions are logged and afterwards
used to help automate the correct behavior of the
system and to increase the overall performance.
3 Collecting natural language data
In order to obtain a sufficient amount of language
data from the WOz experiments, several means
have been used to determine what encourages
users to speak to the system. These include giving
users different types of documentation before the
experiment ? lists of possible voice commands, a
user manual, and step-by-step tutorials. We found
that the best solution was to give users a tutorial
in which they worked through an example using
voice alone or in combination with other modali-
ties, explaining in each step the consequences of
the user?s actions on the system. The drawback of
this approach is that the user may be biased by the
examples and continue to interact according to the
interaction patterns that are provided, rather than
developing their own patterns. These influences
need to be considered both in the data analysis,
and in how the tutorials are written and structured.
The actual experiment consists of two parts in
which the user gets a mixed set of short-answer
and true-false questions to solve using the system.
First they are only allowed to use a subset of the
available modalities, e.g. voice and pen, and then
the full set of modalities. By giving the users dif-
ferent subsets in the first part, we can compare if
the enforcement of certain modalities has an im-
pact on how they choose to use language when all
modalities are available.
On the backend, the wizards can also to some
extent have an active role in encouraging language
use. The Input Wizard is rather constrained in
terms of what semantic pairs he can produce, be-
cause he is committed to selecting from a set of
pairs that are extracted from the meeting data.
For example if ?Monday? is not a meeting date
in the database, the input is interpreted as having
?no match?, which generates the system prompt
?I don?t understand?. Here, the Output Wizard
can intervene by replacing that prompt by one that
more precisely specifies the nature of the problem.
The Output Wizard can also decide to replace
default prompts in situations when they are too
general in a given context. For instance, when
the user is browsing different sections of a meeting
book (cover page, table of contents, transcript and
referenced documents) the default prompt gives
general advice on how to access the different parts
of the book, but can be changed to suggest a spe-
cific section instead.
4 Analysis of elicited language data
The data collected with Archivus through WOz
experiments provide useful information in several
ways. One aspect is to see the complexity of the
language used by users ? for instance whether they
use more keywords, multi-word expressions or
full-sentence queries. This is important for choos-
ing the appropriate level of language processing,
for instance for the syntactic analysis. Another as-
pect is to see the types of actions performed us-
ing language. On one hand, users can manipulate
elements in the graphical interface by expressing
commands that are semantically equivalent with
pointing, e.g. ?next page?. On the other hand,
they can freely formulate queries relating to the
information they are looking for, e.g. ?Did they
decide to put a sofa in the lounge??. Commands
are interface specific rather than domain specific.
From the graphical interface the user can easily
predict what they can say and how the system will
51
Part 1 condition Pointing Language
Experiment set 1
voice only 91% 9%
voice+keyboard 88% 12%
keyboard+pointing 66% 34%
voice+keyb.+pointing 79% 21%
Experiment set 2
voice only 68% 32%
voice+pointing 62% 38%
keyboard+pointing 39% 61%
pointing 76% 24%
Table 1: Use of each modality in part 2.
respond. Queries depend on the domain and the
data, and are more problematic for the user be-
cause they cannot immediately see what types of
queries they can ask and what the coverage of
the data is. But, using queries can be very use-
ful, because it allows the user to express them-
selves in their own terms. An important goal of the
data analysis is to determine if the language inter-
face enables the user to interact more successfully
than if they are limited to pointing only. In addi-
tion, the way in which the users use language in
these two dimensions has important implications
for the dialogue strategy and for the implementa-
tion of the language processing modules, for in-
stance the speech recognition engine. A speech
recognizer can be very accurate when trained on a
small, fixed set of commands whereas it may per-
form poorly when faced with a wide variety of lan-
guage queries.
Thus far, we have performed 3 sets of pilot
WOz experiments with 40 participants. The pri-
mary aim was to improve and finetune the system
and the WOz environment as a preparation for the
data-collection experiments that we plan to do in
the future. In these experiments we compared how
frequently users used voice and keyboard in rela-
tion to pointing as we progressively changed fea-
tures in the system and the experimental setup to
encourage language use. The results between the
first and the third set of experiments can be seen
in table 1, grouped by the subset of modalities that
the users had in the first part of the experiment.
From the table we can see that changes made
between the different iterations of the system
achieved their goal ? by the third experiment set
we were managing to elicit larger amounts of nat-
ural language data. Moreover, we noticed that the
modality conditions that are available to the user
in the first part play a role in the amount of use of
language modalities in the second part.
5 Conclusions and future work
We believe that the work presented here (both the
system and the WOz environment and experimen-
tal protocol) has now reached a stable stage that
allows for the elicitation of sufficient amounts of
natural language and interaction data. The next
step will be to run a large-scale data collection.
The results from this collection should provide
enough information to allow us to develop and in-
tegrate fairly robust natural language processing
into the system. Ideally, some of the components
used in the software framework will be made pub-
licly available at the end of the project.
References
Pavel Cenek, Miroslav Melichar, and Martin Rajman.
2005. A Framework for Rapid Multimodal Appli-
cation Design. In Va?clav Matous?ek, Pavel Mautner,
and Toma?s? Pavelka, editors, Proceedings of the 8th
International Conference on Text, Speech and Dia-
logue (TSD 2005), volume 3658 of Lecture Notes
in Computer Science, pages 393?403, Karlovy Vary,
Czech Republic, September 12-15. Springer.
Nils Dahlba?ck, Arne Jo?nsson, and Lars Ahrenberg.
1993. Wizard of Oz Studies ? Why and How. In
Dianne Murray Wayne D. Gray, William Hefley, ed-
itor, International Workshop on Intelligent User In-
terfaces 1993, pages 193?200. ACM Press.
Agnes Lisowska, Martin Rajman, and Trung H. Bui.
2004. ARCHIVUS: A System for Accessing the
Content of Recorded Multimodal Meetings. In
In Procedings of the JOINT AMI/PASCAL/IM2/M4
Workshop on Multimodal Interaction and Related
Machine Learning Algorithms, Bourlard H. & Ben-
gio S., eds. (2004), LNCS, Springer-Verlag, Berlin.,
Martigny, Switzerland, June.
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary in-
dications from a query analysis study. Project re-
port IM2.MDM-11, University of Geneva, Geneva,
Switzerland, November.
Daniel Salber and Joe?lle Coutaz. 1993. Applying
the wizard of oz technique to the study of multi-
modal systems. In EWHCI ?93: Selected papers
from the Third International Conference on Human-
Computer Interaction, pages 219?230, London, UK.
Springer-Verlag.
52
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144?151,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Analysis of Quantitative Aspects in the Evaluation of Thematic
Segmentation Algorithms
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX, UK
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
susan.armstrong@issco.unige.ch
Abstract
We consider here the task of linear the-
matic segmentation of text documents, by
using features based on word distributions
in the text. For this task, a typical and of-
ten implicit assumption in previous stud-
ies is that a document has just one topic
and therefore many algorithms have been
tested and have shown encouraging results
on artificial data sets, generated by putting
together parts of different documents. We
show that evaluation on synthetic data is
potentially misleading and fails to give an
accurate evaluation of the performance on
real data. Moreover, we provide a criti-
cal review of existing evaluation metrics in
the literature and we propose an improved
evaluation metric.
1 Introduction
The goal of thematic segmentation is to iden-
tify boundaries of topically coherent segments
in text documents. Giving a rigorous definition
of the notion of topic is difficult, but the task
of discourse/dialogue segmentation into thematic
episodes is usually described by invoking an ?in-
tuitive notion of topic? (Brown and Yule, 1998).
Thematic segmentation also relates to several no-
tions such as speaker?s intention, topic flow and
cohesion.
Since it is elusive what mental representations
humans use in order to distinguish a coherent
text, different surface markers (Hirschberg and
Nakatani, 1996; Passonneau and Litman, 1997)
and external knowledge sources (Kozima and Fu-
rugori, 1994) have been exploited for the purpose
of automatic thematic segmentation. Halliday and
Hasan (1976) claim that the text meaning is re-
alised through certain language resources and they
refer to these resources by the term of cohesion.
The major classes of such text-forming resources
identified in (Halliday and Hasan, 1976) are: sub-
stitution, ellipsis, conjunction, reiteration and col-
location. In this paper, we examine one form of
lexical cohesion, namely lexical reiteration.
Following some of the most prominent dis-
course theories in literature (Grosz and Sidner,
1986; Marcu, 2000), a hierarchical representation
of the thematic episodes can be proposed. The
basis for this is the idea that topics can be re-
cursively divided into subtopics. Real texts ex-
hibit a more intricate structure, including ?seman-
tic returns? by which a topic is suspended at one
point and resumed later in the discourse. However,
we focus here on a reduced segmentation prob-
lem, which involves identifying non-overlapping
and non-hierarchical segments at a coarse level of
granularity.
Thematic segmentation is a valuable initial
tool in information retrieval and natural language
processing. For instance, in information ac-
cess systems, smaller and coherent passage re-
trieval is more convenient to the user than whole-
document retrieval and thematic segmentation has
been shown to improve the passage-retrieval per-
formance (Hearst and Plaunt, 1993). In cases such
as collections of transcripts there are no headers
or paragraph markers. Therefore a clear separa-
tion of the text into thematic episodes can be used
together with highlighted keywords as a kind of
?quick read guide? to help users to quickly navi-
gate through and understand the text. Moreover
automatic thematic segmentation has been shown
to play an important role in automatic summariza-
tion (Mani, 2001), anaphora resolution and dis-
144
course/dialogue understanding.
In this paper, we concern ourselves with the task
of linear thematic segmentation and are interested
in finding out whether different segmentation sys-
tems can perform well on artificial and real data
sets without specific parameter tuning. In addi-
tion, we will refer to the implications of the choice
of a particular error metric for evaluation results.
This paper is organized as follows. Section 2
and Section 3 describe various systems and, re-
spectively, different input data selected for our
evaluation. Section 4 presents several existing
evaluation metrics and their weaknesses, as well
as a new evaluation metric that we propose. Sec-
tion 5 presents our experimental set-up and shows
comparisons between the performance of different
systems. Finally, some conclusions are drawn in
Section 6.
2 Comparison of Systems
Combinations of different features (derived for ex-
ample from linguistic, prosodic information) have
been explored in previous studies like (Galley et
al., 2003) and (Kauchak and Chen, 2005). In
this paper, we selected for comparison three sys-
tems based merely on the lexical reiteration fea-
ture: TextTiling (Hearst, 1997), C99 (Choi, 2000)
and TextSeg (Utiyama and Isahara, 2001). In the
following, we briefly review these approaches.
2.1 TextTiling Algorithm
The TextTiling algorithm was initially developed
by Hearst (1997) for segmentation of exposi-
tory texts into multi-paragraph thematic episodes
having a linear, non-overlapping structure (as re-
flected by the name of the algorithm). TextTiling
is widely used as a de-facto standard in the eval-
uation of alternative segmentation systems, e.g.
(Reynar, 1998; Ferret, 2002; Galley et al, 2003).
The algorithm can briefly be described by the fol-
lowing steps.
Step 1 includes stop-word removal, lemmatiza-
tion and division of the text into ?token-sequences?
(i.e. text blocks having a fixed number of words).
Step 2 determines a score for each gap between
two consecutive token-sequences, by computing
the cosine similarity (Manning and Schu?tze, 1999)
between the two vectors representing the frequen-
cies of the words in the two blocks.
Step 3 computes a ?depth score? for each token-
sequence gap, based on the local minima of the
score computed in step 2.
Step 4 consists in smoothing the scores.
Step 5 chooses from any potential boundaries
those that have the scores smaller than a certain
?cutoff function?, based on the average and stan-
dard deviation of score distribution.
2.2 C99 Algorithm
The C99 algorithm (Choi, 2000) makes a linear
segmentation based on a divisive clustering strat-
egy and the cosine similarity measure between any
two minimal units. More exactly, the algorithm
consists of the following steps.
Step 1: after the division of the text into min-
imal units (in our experiments, the minimal unit
is an utterance1), stop words are removed and a
stemmer is applied.
The second step consists of constructing a sim-
ilarity matrix Sm?m, where m is the number of
utterances and an element sij of the matrix corre-
sponds to the cosine similarity between the vectors
representing the frequencies of the words in the i-
th utterance and the j-th utterance.
Step 3: a ?rank matrix? Rm?m is computed, by
determining for each pair of utterances, the num-
ber of neighbors in Sm?m with a lower similarity
value.
In the final step, the location of thematic bound-
aries is determined by a divisive top-down cluster-
ing procedure. The criterion for division of the
current segment B into b1, ...bm subsegments is
based on the maximisation of a ?density? D, com-
puted for each potential repartition of boundaries
as
D =
?m
k=1 sumk?m
k=1 areak
,
where sumk and areak refers to the sum of rank
and area of the k-th segment in B, respectively.
2.3 TextSeg Algorithm
The TextSeg algorithm (Utiyama and Isahara,
2001) implements a probabilistic approach to de-
termine the most likely segmentation, as briefly
described below.
The segmentation task is modeled as a problem
of finding the minimum cost C(S) of a segmenta-
tion S. The segmentation cost is defined as:
C(S) ? ?logPr(W|S)Pr(S),
1Occasionally within this document we employ the term
utterance to denote either a sentence or an utterance in its
proper sense.
145
where W = w1w2...wn represents the text con-
sisting of n words (after applying stop-words re-
moval and stemming) and S = S1S2...Sm is a po-
tential segmentation of W in m segments. The
probability Pr(W|S) is defined using Laplace
law, while the definition of the probability Pr(S)
is chosen in a manner inspired by information the-
ory.
A directed graph G is defined such that a path
in G corresponds to a possible segmentation of
W . Therefore, the thematic segmentation pro-
posed by the system is obtained by applying a dy-
namic programming algorithm for determining the
minimum cost path in G.
3 Input Data
When evaluating a thematic segmentation system
for an application, human annotators should pro-
vide the gold standard. The problem is that the
procedure of building such a reference corpus is
expensive. That is, the typical setting involves an
experiment with several human subjects, who are
asked to mark thematic segment boundaries based
on specific guidelines and their intuition. The
inter-annotator agreement provides the reference
segmentation. This expense can be avoided by
constructing a synthetic reference corpus by con-
catenation of segments from different documents.
Therefore, the use of artificial data for evaluation
is a general trend in many studies, e.g. (Ferret,
2002; Choi, 2000; Utiyama and Isahara, 2001).
In our experiment, we used artificial and real
data, i.e. the algorithms have been tested on the
following data sets containing English texts.
3.1 Artificially Generated Data
Choi (2000) designed an artificial dataset, built by
concatenating short pieces of texts that have been
extracted from the Brown corpus. Any test sample
from this dataset consists of ten segments. Each
segment contains the first n sentences (where 3 ?
n ? 11) of a randomly selected document from
the Brown corpus. From this dataset, we randomly
chose for our evaluation 100 test samples, where
the length of a segment varied between 3 and 11
sentences.
3.2 TDT Data
One of the commonly used data sets for topic seg-
mentation emerged from the Topic Detection and
Tracking (TDT) project, which includes the task
of story segmentation, i.e. the task of segmenting
a stream of news data into topically cohesive sto-
ries. As part of the TDT initiative several datasets
of news stories have been created. In our evalua-
tion, we used a subset of 28 documents randomly
selected from the TDT Phase 2 (TDT2) collection,
where a document contains an average of 24.67
segments.
3.3 Meeting Transcripts
The third dataset used in our evaluation contains
25 meeting transcripts from the ICSI-MR corpus
(Janin et al, 2004). The entire corpus contains
high-quality close talking microphone recordings
of multi-party dialogues. Transcriptions at word
level with utterance-level segmentations are also
available. The gold standard for thematic segmen-
tations has been kindly provided by (Galley et
al., 2003) and has been chosen by considering the
agreement between at least three human annota-
tions. Each meeting is thus divided into contigu-
ous major topic segments and contains an average
of 7.32 segments.
Note that thematic segmentation of meeting
data is a more challenging task as the thematic
transitions are subtler than those in TDT data.
4 Evaluation Metrics
In this section, we will look in detail at the error
metrics that have been proposed in previous stud-
ies and examine their inadequacies. In addition,
we propose a new evaluation metric that we con-
sider more appropriate.
4.1 Pk Metric
(Passonneau and Litman, 1996; Beeferman et al,
1999) underlined that the standard evaluation met-
rics of precision and recall are inadequate for the-
matic segmentation, namely by the fact that these
metrics did not account for how far away is a hy-
pothesized boundary (i.e. a boundary found by
the automatic procedure) from a reference bound-
ary (i.e. a boundary found in the reference data).
On the other hand, it is desirable that an algorithm
that places for instance a boundary just one utter-
ance away from the reference boundary to be pe-
nalized less than an algorithm that places a bound-
ary two (or more) utterances away from the ref-
erence boundary. Hence (Beeferman et al, 1999)
proposed a new metric, called PD, that allows for
a slight vagueness in where boundaries lie. More
146
specifically, (Beeferman et al, 1999) define PD
as follows2:
PD(ref, hyp) =
?
1?i?j?N D(i, j)[?ref (i, j) ?
?hyp(i, j)].
N is the number of words in the reference data.
The function ?ref (i, j) is evaluated to one if the
two reference corpus indices specified by its pa-
rameters i and j belong in the same segment, and
zero otherwise. Similarly, the function ?hyp(i, j)
is evaluated to one, if the two indices are hypothe-
sized by the automatic procedure to belong in the
same segment, and zero otherwise. The ? opera-
tor is the XNOR function ?both or neither?. D(i, j)
is a ?distance probability distribution over the set
of possible distances between sentences chosen
randomly from the corpus?. In practice, a distri-
bution D having ?all its probability mass at a fixed
distance k? (Beeferman et al, 1999) was adopted
and the metric PD was thus renamed Pk.
In the framework of the TDT initiative, (Allan
et al, 1998) give the following formal definition
of Pk and its components:
Pk = PMiss ? Pseg + PFalseAlarm ? (1? Pseg),
where:
PMiss =
PN?k
i=1 [?hyp(i,i+k)]?[1??ref (i,i+k)]
PN?k
i=1 [1??ref (i,i+k)]
,
PFalseAlarm =
PN?k
i=1 [1??hyp(i,i+k)]?[?ref (i,i+k)]
PN?k
i=1 ?ref (i,i+k)
,
and Pseg is the a priori probability that in
the reference data a boundary occurs within an
interval of k words. Therefore Pk is calculated by
moving a window of a certain width k, where k is
usually set to half of the average number of words
per segment in the gold standard.
Pevzner and Hearst (2002) highlighted several
problems of the Pk metric. We illustrate below
what we consider the main problems of the Pk
metric, based on two examples.
Let r(i, k) be the number of boundaries be-
tween positions i and i + k in the gold standard
segmentation and h(i, k) be the number of bound-
aries between positions i and i+k in the automatic
hypothesized segmentation.
? Example 1: If r(i, k) = 2 and h(i, k) = 1
then obviously a missing boundary should
2Let ref be a correct segmentation and hyp be a segmen-
tation proposed by a text segmentation system. We will keep
this notations in equations introduced below.
be counted in Pk, i.e. PMiss should be in-
creased.
? Example 2: If r(i, k) = 1 and h(i, k) =
2 then obviously PFalseAlarm should be in-
creased.
However, considering the first example, we will
obtain ?ref (i, i + k) = 0, ?hyp(i, i + k) = 0
and consequently PMiss is not increased. By tak-
ing the case from the second example we obtain
?ref (i, i + k) = 0 and ?hyp(i, i + k) = 0, involv-
ing no increase of PFalseAlarm.
In (TDT, 1998), a slightly different defini-
tion is given for the Pk metric: the definition of
miss and false alarm probabilities is replaced with:
P ?Miss =
PN?k
i=1 [1??hyp(i,i+k)]?[1??ref (i,i+k)]
PN?k
i=1 [1??ref (i,i+k)]
,
P ?FalseAlarm =
PN?k
i=1 [1??hyp(i,i+k)]?[?ref (i,i+k)]
PN?k
i=1 ?ref (i,i+k)
,
where:
?hyp(i, i+ k) =
{
1, if r(i, k) = h(i, k),
0, otherwise.
We will refer to this new definition of Pk by
P ?k. Therefore, by taking the definition of
P ?k and the first example above, we obtain
?ref (i, i+ k) = 0 and ?hyp(i, i+ k) = 0 and thus
P ?Miss is correctly increased. However for the case
of example 2 we will obtain ?ref (i, i + k) = 0
and ?hyp(i, i + k) = 0, involving no increase of
P ?FalseAlarm and erroneous increase of P ?Miss.
4.2 WindowDiff metric
Pevzner and Hearst (2002) propose the alternative
metric called WindowDiff. By keeping our nota-
tions concerning r(i, k) and h(i, k) introduced in
the subsection 4.1, WindowDiff is defined as:
WindowDiff =
PN?k
i=1 [|r(i,k)? h(i,k)|>0]
N?k .
Similar to both Pk and P ?k, WindowDiff is
also computed by moving a window of fixed size
across the test set and penalizing the algorithm
misses or erroneous algorithm boundary detec-
tions. However, unlike Pk and P ?k, WindowDiff
takes into account how many boundaries fall
within the window and is penalizing in ?how
many discrepancies occur between the reference
and the system results? rather than ?determining
how often two units of text are incorrectly labeled
147
as being in different segments? (Pevzner and
Hearst, 2002).
Our critique concerning WindowDiff is that
misses are less penalised than false alarms and
we argue this as follows. WindowDiff can be
rewritten as:
WindowDiff = WDMiss +WDFalseAlarm,
where:
WDMiss =
PN?k
i=1 [r(i,k)>h(i,k)]
N?k ,
WDFalseAlarm =
PN?k
i=1 [r(i,k)<h(i,k)]
N?k .
Hence both misses and false alarms are weighted
by 1N?k .
Note that, on the one hand, there are indeed (N-
k) equiprobable possibilities to have a false alarm
in an interval of k units. On the other hand, how-
ever, the total number of equiprobable possibil-
ities to have a miss in an interval of k units is
smaller than (N-k) since it depends on the num-
ber of reference boundaries (i.e. we can have a
miss in the interval of k units only if in that interval
the reference corpus contains at least one bound-
ary). Therefore misses, being weighted by 1N?k ,
are less penalised than false alarms.
Let Bref be the number of thematic boundaries
in the reference data. Let?s say that the refer-
ence data contains about 20% boundaries and 80%
non-boundaries from the total number of potential
boundaries. Therefore, since there are relatively
few boundaries compared with non-boundaries, a
strategy introducing no false alarms, but introduc-
ing a maximum number of misses (i.e. k ? Bref
misses) can be judged as being around 80% cor-
rect by the WindowDiff measure. On the other
hand, a segmentation with no misses, but with a
maximum number of false alarms (i.e. (N ? k)
false alarms) is judged as being 100% erroneous
by the WindowDiff measure. That is, misses and
false alarms are not equally penalised.
Another issue regarding WindowDiff is that it is
not clear ?how does one interpret the values pro-
duced by the metric? (Pevzner and Hearst, 2002).
4.3 Proposal for a New Metric
In order to address the inadequacies of Pk and
WindowDiff, we propose a new evaluation metric,
defined as follows:
Prerror = Cmiss ? Prmiss + Cfa ? Prfa,
where:
Cmiss (0 ? Cmiss ? 1) is the cost of a miss, Cfa
(0 ? Cfa ? 1) is the cost of a false alarm,
Prmiss =
PN?k
i=1 [?ref hyp(i,k)]
PN?k
i=1 [?ref (i,k)]
,
P rfa =
PN?k
i=1 [?ref hyp(i,k)]
N?k ,
?ref hyp(i, k) =
{
1, if r(i, k) > h(i, k)
0, otherwise
?ref hyp(i, k) =
{
1, if r(i, k) < h(i, k)
0, otherwise.
?ref (i, k) =
{
1, if r(i, k) > 0
0, otherwise.
Prmiss could be interpreted as the probability
that the hypothesized segmentation contains less
boundaries than the reference segmentation in an
interval of k units3, conditioned by the fact that
the reference segmentation contains at least one
boundary in that interval. Analogously Prfa is
the probability that the hypothesized segmentation
contains more boundaries than the reference seg-
mentation in an interval of k units.
For certain applications where misses are more
important than false alarms or vice versa, the
Prerror can be adjusted to tackle this trade-off via
the Cfa and Cmiss parameters. In order to have
Prerror ? [0, 1], we suggest that Cfa and Cmiss
be chosen such that Cfa + Cmiss = 1. By choos-
ing Cfa=Cmiss=12 , the penalization of misses and
false alarms is thus balanced. In consequence, a
strategy that places no boundaries at all is penal-
ized as much as a strategy proposing boundaries
everywhere (i.e. after every unit). In other words,
both such degenerate algorithms will have an error
rate Prerror of about 50%. The worst algorithm,
penalised as having an error rate Prerror of 100%
when k = 2, is the algorithm that places bound-
aries everywhere except the places where refer-
ence boundaries exist.
5 Results
5.1 Test Procedure
For the three datasets we first performed two
common preprocessing steps: common words are
eliminated using the same stop-list and remaining
words are stemmed by using Porter?s algorithm
(1980). Next, we ran the three segmenters de-
scribed in Section 2, by employing the default val-
ues for any system parameters and by letting the
3A unit can be either a word or a sentence / an utterance.
148
systems estimate the number of thematic bound-
aries.
We also considered the fact that C99 and
TextSeg algorithms can take into account a fixed
number of thematic boundaries. Even if the num-
ber of segments per document can vary in TDT
and meeting reference data, we consider that in a
real application it is impossible to provide to the
systems the exact number of boundaries for each
document to be segmented. Therefore, we ran C99
and TextSeg algorithms (for a second time), by
providing them only the average number of seg-
ments per document in the reference data, which
gives an estimation of the expected level of seg-
mentation granularity.
Four additional naive segmentations were also
used for evaluation, namely: no boundaries,
where the whole text is a single segment; all
boundaries, i.e. a thematic boundary is placed af-
ter each utterance; random known, i.e. the same
number of boundaries as in gold standard, distrib-
uted randomly throughout text; and random un-
known: the number of boundaries is randomly
selected and boundaries are randomly distributed
throughout text. Each of the segmentations was
evaluated with Pk, P ?k and WindowDiff, as de-
scribed in Section 4.
5.2 Comparative Performance of
Segmentation Systems
The results of applying each segmentation algo-
rithm to the three distinct datasets are summa-
rized in Figures 1, 2 and 3. Percent error values
are given in the figures and we used the follow-
ing abbreviations: WD to denote WindowDiff er-
ror metric; TextSeg KA to denote the TextSeg algo-
rithm (Utiyama and Isahara, 2001) when the av-
erage number of boundaries in the reference data
was provided to the algorithm; C99 KA to denote
the C99 algorithm (Choi, 2000) when the aver-
age number of boundaries in the reference data
was provided to the algorithm; N0 to denote the al-
gorithm proposing a segmentation with no bound-
aries; All to denote the algorithm proposing the de-
generate segmentation all boundaries; RK to de-
note the algorithm that generates a random known
segmentation; and RU to denote the algorithm that
generates a random unknown segmentation.
5.2.1 Comparison of System Performance
from Artificial to Realistic Data
From the artificial data to the more realistic
data, we expect to have more noise and thus the
algorithms to constantly degrade, but as our ex-
periments show a reversal of the assessment can
appear. More exactly: as can be seen from Figure
1, both C99 and TextSeg algorithms significantly
outperformed TextTiling algorithm on the artifi-
cially created dataset, when the number of seg-
ments was determined by the systems. A com-
parison between the error rates given in Figure
1 and Figure 2 show that C99 and TextSeg have
a similar trend, by significantly decreasing their
performance on TDT data, but still giving bet-
ter results than TextTiling on TDT data. When
comparing the systems by Prerror, C99 has simi-
lar performance with TextTiling on meeting data
(see Figure 3). Moreover, when assessment is
done by using WindowDiff, Pk or P ?k, both C99
and TextSeg came out worse than TextTiling on
meeting data. This demonstrates that rankings ob-
tained when evaluating on artificial data are dif-
ferent from those obtained when evaluating on re-
alistic data. An alternative interpretation can be
given by taking into account that the degenerative
no boundaries segmentation has an error rate of
only 30% by the WindowDiff, Pk and P ?k metrics
on meeting data. That is, we could interpret that
all three systems give completely wrong segmen-
tations on meeting data (due to the fact that topic
shifts are subtler and not as abrupt as in TDT and
artificial data). Nevertheless, we tend to adopt the
first interpretation, given the weaknesses of Pk, P ?k
and WindowDiff (where misses are less penalised
than false alarms), as discussed in Section 4.
5.2.2 The Influence of the Error Metric on
Assessment
By following the quantitative assessment given
by the WindowDiff metric, we observe that the
algorithm labeled N0 is three times better than
the algorithm All on meeting data (see Figure 3),
while the same algorithm N0 is considered only
two times better than All on the artificial data (see
Figure 1). This verifies the limitation of the Win-
dowDiff metric discussed in Section 4.
The four error metrics described in detail in
Section 4 have shown that the effect of knowing
the average number of boundaries on C99 is posi-
tive when testing on meeting data. However if we
want to take into account all the four error met-
149
0
20
40
60
80
100
120
Er
ro
r r
ate
Pk 34.75 11.01 7.89 10 7.15 44.12 55.5 47.71 52.51
P'k 35.1 13.21 8.55 10.94 7.87 44.13 99.58 48.85 80.84
WD 35.73 13.58 9.21 11.34 8.59 43.1 99.59 48.89 80.63
Pr_error 33.33 9.1 7.71 9.34 6.87 49.87 49.79 41.61 45.01
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 1: Error rates of the segmentation systems on artificial data, where k = 42 and Pseg = 0.44.
0
20
40
60
80
100
120
Err
or
 
ra
te
Pk 40.7 21.36 13.97 18.83 11.33 36.02 63.93 37.03 60.04
P'k 44.92 29.5 20.37 27.69 21.4 36.04 100 45.28 89.93
WD 44.76 36.28 30.3 40.26 31.46 46.69 100 53.75 91.92
Pr_error 34.09 25.69 25.62 27.17 21.05 49.96 50 44.89 48.31
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 2: Error rates of the segmentation systems on TDT data, where k = 55 and Pseg = 0.3606.
rics, it is difficult to draw definite conclusions re-
garding the influence of knowing the average num-
ber of boundaries on TextSeg and C99 algorithms.
For example, when tested on TDT data, C99 KA
seems to work better than C99 by Pk and P ?k met-
rics, while the WindowDiff metric gives a contra-
dictory assessment.
6 Conclusions
By comparing the performance of three systems
for thematic segmentation on different kinds of
data, we address two important issues in a quan-
titative evaluation. Strong emphasis was put on
the kind of data used for evaluation and we have
demonstrated experimentally that evaluation on
synthetic data is potentially misleading. The sec-
ond major issue addressed in this paper concerns
the choice of a valuable error metric and its side
effects on the evaluation assessment.
Acknowledgments
This work is supported by the Interactive
Multimodal Information Management project
(http://www.im2.ch/). Many thanks to Andrei
Popescu-Belis and the anonymous reviewers for
their valuable comments. We are grateful to the
International Computer Science Institute (ICSI),
University of California for sharing the data with
us. We also wish to thank Michael Galley who
kindly provided us the thematic annotations of
ICSI data.
References
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
Detection and Tracking Pilot Study: Final Re-
port. In DARPA Broadcast News Transcription and
Understanding Workshop, pages 194?218, Lands-
downe, VA. Morgan Kaufmann.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation.
Machine Learning, 34(Special Issue on Natural Lan-
guage Learning):177?210.
Gillian Brown and George Yule. 1998. Discourse
Analysis. (Cambridge Textbooks in Linguistics),
Cambridge.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
150
0
20
40
60
80
100
120
Err
or
 
ra
te
P_k 38.22 54.62 40.82 35.65 35.94 30.82 69.09 45.42 68.48
P'_k 39.12 66.78 45.66 39.04 39.6 30.89 100 47.97 95.99
WD 40.82 69.41 49.27 41.98 42.48 29.31 100 49.64 95.48
Pr_error 40.17 40.27 35.45 35.83 36.61 49.8 50 50.7 53.38
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 3: Error rates of the segmentation systems on meeting data, where k = 85 and Pseg = 0.3090.
Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle,
USA.
Olivier Ferret. 2002. Using Collocations for Topic
Segmentation and Link Detection. In The 19th In-
ternational Conference on Computational Linguis-
tics, Taipei, Taiwan.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Annual
Meeting of the Association for Computational Lin-
guistics, pages 562?569.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions and the Structure of Discourse.
Computational Linguistics, 12:175?204.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Marti Hearst and Christian Plaunt. 1993. Subtopic
Structuring for Full-Length Document Access.
In Proceedings of the 16th Annual International
ACM/SIGIR Conference, pages 59?68, Pittsburgh,
Pennsylvania, United States.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Julia Hirschberg and Christine Nakatani. 1996.
A Prosodic Analysis of Discourse Segments in
Direction-Giving Monologues. In Proceedings of
the 34th Annual Meeting on Association for Com-
putational Linguistics, pages 286 ? 293, Santa Cruz,
California.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Macias-Guarasa, Nel-
son Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI Meeting Project: Resources and Re-
search. In ICASSP 2004 Meeting Recognition Work-
shop (NIST RT-04 Spring Recognition Evaluation),
Montreal.
David Kauchak and Francine Chen. 2005. Feature-
based segmentation of narrative documents. In Pro-
ceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing, pages 32?39, Ann Arbor; MI; USA.
Hideki Kozima and Teiji Furugori. 1994. Segmenting
Narrative Text into Coherent Scenes. Literary and
Linguistic Computing, 9:13?19.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Pub Co.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press Cambridge, MA, USA.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press
Cambridge, MA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1996.
Empirical Analysis of Three Dimensions of Spoken
Discourse: Segmentation, Coherence and Linguistic
Devices.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1).
Lev Pevzner and Marti Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 16(1):19?
36.
Martin Porter. 1980. An Algorithm for Suffix Strip-
ping. Program, 14:130 ? 137.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Penn-
sylvania.
TDT. 1998. The Topic Detection and Tracking - Phase
2 Evaluation Plan. Available from World Wide Web:
http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.
Masao Utiyama and Hitoshi Isahara. 2001. A Statisti-
cal Model for Domain-Independent Text Segmenta-
tion. In ACL/EACL, pages 491?498.
151
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 101?108, New York City, June 2006. c?2006 Association for Computational Linguistics
Word Distributions for Thematic Segmentation in a Support Vector
Machine Approach
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX, UK
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
susan.armstrong@issco.unige.ch
Abstract
We investigate the appropriateness of us-
ing a technique based on support vector
machines for identifying thematic struc-
ture of text streams. The thematic seg-
mentation task is modeled as a binary-
classification problem, where the different
classes correspond to the presence or the
absence of a thematic boundary. Exper-
iments are conducted with this approach
by using features based on word distri-
butions through text. We provide em-
pirical evidence that our approach is ro-
bust, by showing good performance on
three different data sets. In particu-
lar, substantial improvement is obtained
over previously published results of word-
distribution based systems when evalua-
tion is done on a corpus of recorded and
transcribed multi-party dialogs.
1 Introduction
(Todd, 2005) distinguishes between ?local-level top-
ics (of sentences, utterances and short discourse seg-
ments)? and ?discourse topics (of more extended
stretches of discourse)?.1 (Todd, 2005) points out
that ?discourse-level topics are one of the most elu-
sive and intractable notions in semantics?. Despite
this difficulty in giving a rigorous definition of dis-
course topic, the task of discourse/dialogue segmen-
tation into thematic episodes can be described by
1In this paper, we make use of the term topic or theme as
referring to the discourse/dialogue topic.
invoking an ?intuitive notion of topic? (Brown and
Yule, 1998). Thematic segmentation also relates
to several notions such as speaker?s intention, topic
flow and cohesion.
In order to find out if thematic segment identi-
fication is a feasible task, previous state-of-the-art
works appeal to experiments, in which several hu-
man subjects are asked to mark thematic segment
boundaries based on their intuition and a minimal
set of instructions. In this manner, previous studies,
e.g. (Passonneau and Litman, 1993; Galley et al,
2003), obtained a level of inter-annotator agreement
that is statistically significant.
Automatic thematic segmentation (TS), i.e. the
segmentation of a text stream into topically coher-
ent segments, is an important component in ap-
plications dealing with large document collections
such as information retrieval and document brows-
ing. Other tasks that could benefit from the thematic
textual structure include anaphora resolution, auto-
matic summarisation and discourse understanding.
The work presented here tackles the problem
of TS by adopting a supervised learning approach
for capturing linear document structure of non-
overlapping thematic episodes. A prerequisite for
the input data to our system is that texts are divided
into sentences or utterances.2 Each boundary be-
tween two consecutive utterances is a potential the-
matic segmentation point and therefore, we model
the TS task as a binary-classification problem, where
each utterance should be classified as marking the
2Occasionally within this document we employ the term ut-
terance to denote either a sentence or an utterance in its proper
sense.
101
presence or the absence of a topic shift in the dis-
course/dialogue based only on observations of pat-
terns in vocabulary use.
The remainder of the paper is organised as fol-
lows. The next section summarizes previous tech-
niques, describes how our method relates to them
and presents the motivations for a support vector ap-
proach. Sections 3 and 4 present our approach in
adopting support vector learning for thematic seg-
mentation. Section 5 outlines the empirical method-
ology and describes the data used in this study. Sec-
tion 6 presents and discusses the evaluation results.
The paper closes with Section 7, which briefly sum-
marizes this work and offers some conclusions and
future directions.
2 Related Work
As in many existing approaches to the thematic seg-
mentation task, we make the assumption that the
thematic coherence of a text segment is reflected at
lexical level and therefore we attempt to detect the
correlation between word distribution and thematic
changes throughout the text. In this manner, (Hearst,
1997; Reynar, 1998; Choi, 2000) start by using a
similarity measure between sentences or fixed-size
blocks of text, based on their word frequencies in
order to find changes in vocabulary use and there-
fore the points at which the topic changes. Sen-
tences are then grouped together by using a cluster-
ing algorithm. (Utiyama and Isahara, 2001) models
the problem of TS as a problem of finding the mini-
mum cost path in a graph and therefore adopts a dy-
namic programming algorithm. The main advantage
of such methods is that no training time and corpora
are required.
By modeling TS as binary-classification problem,
we introduce a new technique based on support vec-
tor machines (SVMs). The main advantage offered
by SVMs with respect to methods such as those de-
scribed above is related to the distance (or similarity)
function used. Thus, although (Choi, 2000; Hearst,
1997) employ a distance function (i.e. cosine dis-
tance) to detect thematic shifts, SVMs are capable
of using a larger variety of similarity functions.
Moreover, SVMs can employ distance functions
that operate in extremely high dimensional feature
spaces. This is an important property for our task,
where handling high dimensionality data represen-
tation is necessary (see section 4).
An alternative to dealing with high dimension
data may be to reduce the dimensionality of the
data representation. Therefore, linear algebra di-
mensionality reduction methods like singular value
decomposition have been adopted by (Choi et al,
2001; Popescu-Belis et al, 2004) in Latent Seman-
tic Analysis (LSA) for the task of thematic segmen-
tation. A Probabilistic Latent Semantic Analysis
(PLSA) approach has been adopted by (Brants et
al., 2002; Farahat and Chen, 2006) for the TS task.
(Blei and Moreno, 2001) proposed a TS approach,
by embedding a PLSA model in an extended Hid-
den Markov Model (HMM) approach, while (Yam-
ron et al, 1998) have previously proposed a HMM
approach for TS.
A shortcoming of the methods described above
is due to their typically generative manner of train-
ing, i.e. using the maximum likelihood estimation
for a joint sampling model of observation and la-
bel sequences. This poses the challenge of finding
more appropriate objective functions, i.e. alterna-
tives to the log-likelihood that are more closely re-
lated to application-relevant performance measures.
Secondly, efficient inference and learning for the TS
task often requires making questionable conditional
independence assumptions. In such cases, improved
performance may be obtained by using methods
with a more discriminative character, by allowing
direct dependencies between a label and past/future
observations and by efficient handling higher-order
combinations of input features. Given the discrim-
inative character of SVMs, we expect our model to
attain similar benefits.
3 Support Vector Learning Task and
Thematic Segmentation
The theory of Vapnik and Chervonenkis (Vapnik,
1995) motivated the introduction of support vector
learning. SVMs have originally been used for clas-
sification purposes and their principles have been ex-
tended to the task of regression, clustering and fea-
ture selection. (Kauchak and Chen, 2005) employed
SVMs using features (derived for instance from in-
formation given by the presence of paragraphs, pro-
nouns, numbers) that can be reliably used for topic
102
segmentation of narrative documents. Aside from
the fact that we consider the TS task on different
datasets (not only on narrative documents), our ap-
proach is different from the approach proposed by
(Kauchak and Chen, 2005) mainly by the data repre-
sentation we propose and by the fact that we put the
emphasis on deriving the thematic structure merely
from word distribution, while (Kauchak and Chen,
2005) observed that the ?block similarities provide
little information about the actual segment bound-
aries? on their data and therefore they concentrated
on exploiting other features.
An excellent general introduction to SVMs and
other kernel methods is given for instance in (Cris-
tianini and Shawe-Taylor, 2000). In the section be-
low, we give some highlights representing the main
elements in using SVMs for thematic segmentation.
The support vector learner L is given a training
set of n examples, usually denoted by Strain= ((~u1,
y1),...,(~un, yn))? (U ? Y )n drawn independently
and identically distributed according to a fixed dis-
tribution Pr(u, y) = Pr(y|u)Pr(u). Each train-
ing example consists of a high-dimensional vector ~u
describing an utterance and the class label y. The
utterance representations we chose are further de-
scribed in Section 4. The class label y has only
two possible values: ?thematic boundary? or ?non-
thematic boundary?. For notational convenience, we
replace these values by +1 and -1 respectively, and
thus we have y ? {-1, 1}. Given a hypothesis space
H, of functions h : U ? {?1,+1} having the form
h(~u) = sign(< ~w, ~u > +b), the inductive sup-
port vector learner Lind seeks a decision function
hind from H, using Strain so that the expected num-
ber of erroneous predictions is minimized. Using
the structural risk minimization principle (Vapnik,
1995), the support vector learner gets the optimal de-
cision function h by minimizing the following cost
function:
W ind(~w, b, ?1, ?2, ..., ?n) = 12 < ~w, ~w > +
+ C+
n?
i=0,yi=1
?i + C?
n?
i=0,yi=?1
?i,
subject to:
yi[< ~w ? ~ui > +b] ? 1? ?i for i = 1, 2, ..., n;
?i ? 0 for i = 1, 2, ..., n.
The parameters ~w and b follow from the optimi-
sation problem, which is solved by applying La-
grangian theory. The so-called slack variables ?i,
are introduced in order to be able to handle non-
separable data. The positive parameters C+ and C?
are called regularization parameters and determine
the amount up to which errors are tolerated. More
exactly, training data may contain noisy or outlier
data that are not representative of the underlying dis-
tribution. On the one hand, fitting exactly to the
training data may lead to overfitting. On the other
hand, dismissing true properties of the data as sam-
pling bias in the training data will result in low accu-
racy. Therefore, the regularization parameter is used
to balance the trade-off between these two compet-
ing considerations. Setting the regularization para-
meter too low can result in poor accuracy, while set-
ting it too high can lead to overfitting. In the TS task,
we used an automated procedure to select the regu-
larization parameters, as further described in section
5.3.
In cases where non-linear hypothesis functions
should be optimised, each ~ui can be mapped into
?(~ui) ? F , where F is a higher dimensional space
usually called feature space, in order to make linear
the relation between ~ui and yi. Thus the original lin-
ear learning machine can be adopted in finding the
classification solution in the feature space.
When using a mapping function ? : U ? F ,
if we have a way of computing the inner product
??(~ui), ?(~uj)? directly as a function of the origi-
nal input point, then the so-called kernel function
K(~ui, ~uj) = ??(~ui), ?(~uj)? is proved to simplify
the computational complexity implied by the direct
use of the mapping function ?. The choice of appro-
priate kernels and its specific parameters is an empir-
ical issue. In our experiments, we used the Gaussian
radial basis function (RBF) kernel:
KRBF (~ui, ~uj) = exp(??
2||~ui ? ~uj ||
2).
For the SVM calculations, we used the LIBSVM li-
brary (Chang and Lin, 2001).
4 Representation of the information used
to determine thematic boundaries
As presented in section 3, in the thematic segmen-
tation task, an input ~ui to the support vector classi-
fier is a vectorial representation of the utterance to
103
be classified and its context. Each dimension of the
input vector indicates the value of a certain feature
characterizing the utterance. All input features here
are indicator functions for a word occurring within
a fixed-size window centered on the utterance being
labeled. More exactly, the input features are com-
puted in the following steps:
1. The text has been pre-processed by tokeniza-
tion, elimination of stop-words and lemmatiza-
tion, using TreeTagger (Schmid, 1996).
2. We make use of the so-called bag of words ap-
proach, by mapping each utterance to a bag, i.e.
a set that contains word frequencies. Therefore,
word frequencies have been computed to count
the number of times that each term (i.e. word
lemma) is used in each utterance. Then a trans-
formation of the raw word frequency counts
is applied in order to take into account both
the local (i.e. for each utterance) word fre-
quencies as well as the overall frequencies of
their occurrences in the entire text collection.
More exactly, we made experiments in paral-
lel with three such transformations, which are
very commonly used in information retrieval
domain (Dumais, 1991): tf.idf, tf.normal and
log.entropy.
3. Each i-th utterance is represented by a vector
~ui, where a j-th element of ~ui is computed as:
ui,j =
?
?
i?
t=i?winSize
ft,j
?
?
?
?
i+winSize?
k=i+1
fk,j
?
? ,
where winSize ? 1 and fi,j is the weighted
frequency (determined in the previous step) of
the j-th word from the vocabulary in the i-th ut-
terance. In this manner, we will have ui,j > 0 if
and only if at least two occurrences of the j-th
term occur within (2 ? winSize) utterances on
opposite sides of a boundary candidate. That
is, each ui,j is capturing how many word co-
occurrences appear across the candidate utter-
ance in an interval (of (2?winSize) utterances)
centered in the boundary candidate utterance.
4. Each attribute value from the input data is
scaled to the interval [0, 1].
Note that the vector space representation adopted in
the previous steps will result in a sparse high dimen-
sional input data for our system. More exactly, table
1 shows the average number of non-zero features per
example corresponding to each data set (further de-
scribed in section 5.1).
Data set Non zero features
ICSI 3.67%
TDT 0.40%
Brown 0.12%
Table 1: The percentage of non-zero features per ex-
ample.
5 Experimental Setup
5.1 Data sets used
In order to evaluate how robust our SVM approach
is, we performed experiments on three English data
sets of approximately the same dimension (i.e. con-
taining about 260,000 words).
The first dataset is a subset of the ICSI-MR cor-
pus (Janin et al, 2004), where the gold standard for
thematic segmentations has been provided by tak-
ing into account the agreement of at least three hu-
man annotators (Galley et al, 2003). The corpus
consists of high-quality close talking microphone
recordings of multi-party dialogues. Transcriptions
at word level with utterance-level segmentations are
also available. A test sample from this dataset con-
sists of the transcription of an approximately one-
hour long meeting and contains an average of about
seven thematic episodes.
The second data set contains documents randomly
selected from the Topic Detection and Tracking
(TDT) 2 collection, made available by (LDC, 2006).
The TDT collection includes broadcast news and
newswire text, which are segmented into topically
cohesive stories. We use the story segmentation pro-
vided with the corpus as our gold standard labeling.
A test sample from our subset contains an average
of about 24 segments.
The third dataset we use in this study was origi-
nally proposed in (Choi, 2000) and contains artifi-
cial thematic episodes. More precisely, the dataset
is built by concatenating short pieces of texts that
104
Data set Weighting schema winSize ? C
ICSI log.entropy 57 0.0625 0.01
TDT tf.idf 17 0.0625 0.1
Brown tf.idf 5 0.0625 0.001
Table 2: The optimal settings found for the SVM model, using the RBF kernel.
have been randomly extracted from the Brown cor-
pus. Any test sample from this dataset consists of
ten segments. Each segment contains at least three
sentences and no more than eleven sentences.
While the focus of our paper is not on the method
of evaluation, it is worth pointing out that the per-
formance on the synthetic data set is a very poor
guide to the performance on naturally occurring data
(Georgescul et al, 2006). We include the synthetic
data for comparison purposes.
5.2 Handling unbalanced data
We have a small percentage of positive examples
relative to the total number of training examples.
Therefore, in order to ensure that positive points are
not considered as being noisy labels, we change the
penalty of the minority (positive) class by setting the
parameter C+ of this class to:
C+ = ? ?
(
n
n+ ? 1
? 1
)
? C?,
where n+ is the number of positive training exam-
ples, n is the total number of training examples and
? is the scaling factor. In the experiments reported
here, we set the value for the scale factor ? to ? = 1
and we have: C+ = 7 ? C? for the synthetic data
derived from Brown corpus; C+ = 18 ? C?for the
TDT data and C+ = 62 ? C? for the ICSI meeting
data.
5.3 Model selection
We used 80% of each dataset to determine the best
model settings, while the remaining 20% is used
for testing purposes. Each training set (for each
dataset employed) was divided into disjoint subsets
and five-fold cross-validation was applied for model
selection.
In order to avoid too many combinations of pa-
rameter settings, model selection is done in two
phases, by distinguishing two kinds of parameters.
First, the parameters involved in data representation
(see section 4) are addressed. We start with choosing
an appropriate term weighting scheme and a good
value for the winSize parameter. This choice is
based on a systematic grid search over 20 differ-
ent values for winSize and the three variants tf.idf,
tf.normal and log.entropy for term weighting. We
ran five-fold cross validation, by using the RBF ker-
nel with its parameter ? fixed to ? = 1. We also set
the regularization parameter C equal to C = 1.
In the second phase of model selection, we
take the optimal parameter values selected in the
previous phase as a constant factor and search
the most appropriate values for C and ? para-
meters. The range of values we select from is:
C ?
{
10?3, 10?2, 10?1, 1, 10, 102, 103
}
and ? ?
{
2?6, 2?5, 2?4, ..., 24, 26
}
and for each possible
value we perform five-fold cross validation. There-
fore, we ran the algorithm five times for the 91 =
7 ? 13 parameter settings. The most suitable model
settings found are shown in Table 2. For these set-
tings, we show the algorithm?s results in section 6.
6 Evaluation
6.1 Evaluation Measures
Beeferman et al (1999) underlined that the stan-
dard evaluation metrics of precision and recall are
inadequate for thematic segmentation, namely by
the fact that these metrics did not account for how
far away a hypothesized boundary (i.e. a boundary
found by the automatic procedure) is from the ref-
erence boundary. On the other hand, for instance,
an algorithm that places a boundary just one utter-
ance away from the reference boundary should be
penalized less than an algorithm that places a bound-
ary ten (or more) utterances away from the reference
boundary.
Hence the use of two other evaluation metrics
is favored in thematic segmentation: the Pk met-
ric (Beeferman et al, 1999) and the WindowDiff
error metric (Pevzner and Hearst, 2002). In con-
105
020406080100120 Algorith
ms
Error rates
P_k18.
5411.0
152.51
20.492
1.3660
.04
21.683
1.912
354.6
268.48
WD19.
4713.5
880.63
23.993
6.2891
.92
25.535
.8825.4
769.41
95.48
SVMC
99   Ran
d      
SVMC
99Ran
d  
SVMG
03 G03
* C99
Rand
Brown d
ata
TDT da
ta
ICSI da
ta
Figure 1: Error rates of the segmentation systems.
trast to precision and recall, these metrics allow for a
slight vagueness in where the hypothesized thematic
boundaries are placed and capture ?the notion of
nearness in a principled way, gently penalizing algo-
rithms that hypothesize boundaries that aren?t quite
right, and scaling down with the algorithm?s degra-
dation? (Beeferman et al, 1999). That is, comput-
ing both Pk and WindowDiff metrics involves the
use of a fixed-size (i.e. having a fixed number of
either words or utterances) window that is moved
step by step over the data. At each step, Pk and
WindowDiff are basically increased (each metric in
a slightly different way) if the hypothesized bound-
aries and the reference boundaries are not within the
same window.
During the model selection phase, we used pre-
cision and recall in order to measure the system?s
error rate. This was motivated by the fact that pos-
ing the TS task as a classification problem leads to a
loss of the sequential nature of the data, which is an
inconvenient in computing the Pk and WindowDiff
measures. However, during the final testing phase
of our system, as well as for the evaluation of the
previous systems, we use both the Pk and the Win-
dowDiff error metric.
The relatively small size of our datasets does not
allow for dividing our test set into multiple sub-test
sets for applying statistical significance tests. This
would be desirable in order to indicate whether the
differences in system error rates are statistically sig-
nificant over different data sets. Nevertheless, we
believe that measuring differences in error rates ob-
tained on the test set is indicative of the relative per-
formance. Thus, the experimental results shown in
this paper should be considered as illustrative rather
than exhaustive.
6.2 Results
In order to determine the adequacy of our SVM ap-
proach over different genres, we ran our system over
three datasets, namely the ICSI meeting data, the
TDT broadcast data and the Brown written genre
data.
By measuring the system error rates using the
Pk and the WindowDiff metrics, Figure 1 summa-
rizes the quantitative results obtained in our empir-
ical evaluation. In Figure 1, our SVM approach is
labeled as SVM and we abbreviate WindowDiff as
WD. The results of our SVM system correspond to
the parameter values detected during model selec-
tion (see Table 2). We compare our system against
an existing thematic segmenter in the literature: C99
(Choi, 2000). We also give for comparison the
error rates of a naive algorithm, labeled as Rand
algorithm, which randomly distributes boundaries
throughout the text.
The LCseg system (Galley et al, 2003), labeled
here as G03, is to our knowledge the only word dis-
tribution based system evaluated on ICSI meeting
data. Therefore, we replicate the results reported by
(Galley et al, 2003) when evaluation of LCseg was
done on ICSI data. The so-labeled G03* algorithm
106
indicates the error rates obtained by (Galley et al,
2003) when extra (meeting specific) features have
been adopted in a decision tree classifier. However,
note that the results reported by (Galley et al) are
not directly comparable with our results because of
a slight difference in the evaluation procedure: (Gal-
ley et al) performed 25-fold cross validation and the
average Pk and WD error rates have been computed
on the held-out sets.
Figure 1 illustrates the following interesting re-
sults. For the ICSI meeting data, our SVM approach
provides the best performance relative to the com-
peting word distribution based state-of-the-art meth-
ods. This proves that our SVM-based system is able
to build a parametric model that leads to a segmenta-
tion that highly correlates to a human thematic seg-
mentation. Furthermore, by taking into account the
relatively small size of the data set we used for train-
ing, it can be concluded that the SVM can build
qualitatively good models even with a small train-
ing data. The work of (Galley et al, 2003) shows
that the G03* algorithm is better than G03 by ap-
proximately 10%, which indicates that on meeting
data the performance of our word-distribution based
approach could possibly be increased by using other
meeting-specific features.
By examining the error rates given by Pk metric
for the three systems on the TDT data set, we ob-
serve that our system and C99 performed more or
less equally. With respect to the WindowDiff met-
ric, our system has an error rate approximately 10%
smaller than C99.
On the synthetic data set, the SVM approach
performed slightly worse than C99, avoiding how-
ever catastrophic failure, as observed with the C99
method on ICSI data.
7 Conclusions
We have introduced a new approach based on word
distributions for performing thematic segmentation.
The thematic segmentation task is modeled here as
a binary classification problem and support vector
machine learning is adopted. In our experiments, we
make a comparison of our approach versus existing
linear thematic segmentation systems reported in the
literature, by running them over three different data
sets. When evaluating on real data, our approach ei-
ther outperformed the other existing methods or per-
forms comparably to the best. We view this as a
strong evidence that our approach provides a unified
and robust framework for the thematic segmentation
task. The results also suggest that word distributions
themselves might be a good candidate for capturing
the thematic shifts of text and that SVM learning can
play an important role in building an adaptable cor-
relation.
Our experiments also show the sensitivity of a
segmentation method to the type of a corpus on
which it is tested. For instance, the C99 algorithm
which achieves superior performance on a synthetic
collection performs quite poorly on the real-life data
sets.
While we have shown empirically that our tech-
nique can provide considerable gains by using sin-
gle word distribution features, future work will in-
vestigate whether the system can be improved by ex-
ploiting other features derived for instance from syn-
tactic, lexical and, when available, prosodic infor-
mation. If further annotated meeting data becomes
available, it would be also interesting to replicate our
experiments on a bigger data set in order to verify
whether our system performance improves.
Acknowledgments This work is partially sup-
ported by the Interactive Multimodal Information
Management project (http://www.im2.ch/). Many
thanks to the reviewers for their insightful sugges-
tions. We are grateful to the International Computer
Science Institute (ICSI), University of California for
sharing the data with us. The authors also thank
Michael Galley who kindly provided us the thematic
annotations of the ICSI data.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34(1-3):177?210.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with an Aspect Hidden Markov Model. In
Proceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 343?348. ACM Press.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic Analysis. In Pro-
ceedings of the Eleventh International Conference on
107
Information and Knowledge Management, pages 211?
218, McLean, Virginia, USA. ACM Press.
Gillian Brown and George Yule. 1998. Discourse Analy-
sis. Cambridge Textbooks in Linguistics, Cambridge.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 6th Conference on
Empirical Methods in Natural Language Processing,
Seattle, WA.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33,
Seattle, USA.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and other
kernel-based learning methods. Cambridge Univer-
sity Press, Cambridge, UK.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229?236.
Ayman Farahat and Francine Chen. 2006. Improving
Probabilistic Latent Semantic Analysis with Principal
Component Analysis. In Proceedings of the 11th Con-
ference of the European Chapter of the Asociation for
Computational Linguistics, Trento, Italy.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 562?569.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An Analysis of Quantitative Aspects in
the Evaluation of Thematic Segmentation Algorithms.
To appear.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004. The
ICSI Meeting Project: Resources and Research. In
ICASSP 2004 Meeting Recognition Workshop (NIST
RT-04 Spring Recognition Evaluation), Montreal.
David Kauchak and Francine Chen. 2005. Feature-
Based Segmentation of Narrative Documents. In Pro-
ceedings of the ACL Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, pages 32?39, Ann Arbor; MI; USA.
LDC. 2006. The Linguistic Data Consortium. Available
from World Wide Web: http://www.ldc.upenn.edu.
Rebecca J. Passonneau and Diane J. Litman. 1993.
Intention-based Segmentation: Human Reliability and
Correlation with Linguistic Cues. In Proceedings of
the 31st conference on Association for Computational
Linguistics, pages 148 ? 155, Columbus, Ohio.
Lev Pevzner and Marti Hearst. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics, 16(1):19?36.
Andrei Popescu-Belis, Alexander Clark, Maria Georges-
cul, Sandrine Zufferey, and Denis Lalanne. 2004.
Shallow Dialogue Processing Using Machine Learn-
ing Algorithms (or Not). In Bourlard H. and Ben-
gio S., editors, Multimodal Interaction and Related
Machine Learning Algorithms, pages 277?290. LNCS
3361, Springer-Verlag, Berlin.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Pennsyl-
vania.
Helmut Schmid. 1996. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Technical report, Insti-
tute for Computational Linguistics of the University
of Stuttgart.
Richard Watson Todd. 2005. A fuzzy approach to dis-
course topics. Journal of the International Association
for Semiotic Studies, 155:93?123.
Masao Utiyama and Hitoshi Isahara. 2001. A Statis-
tical Model for Domain-Independent Text Segmenta-
tion. In Proceedings of the 39th Annual Meeting of
the ACL joint with the 10th Meeting of the European
Chapter of the ACL, pages 491?498, Toulouse, France.
Vladimir Naumovich Vapnik. 1995. The Nature of Sta-
tistical Learning Theory. Springer-Verlag, New York.
Jonathan P. Yamron, Ira Carp, Lawrence Gillick, Stewe
Lowe, and Paul van Mulbregt. 1998. A Hidden
Markov Model Approach to Text Segmentation and
Event Tracking. In Proceedings of the IEEE Confer-
ence on Acoustics, Speech, and Signal Processing, vol-
ume 17, pages 333?336, Seattle, WA.
108
