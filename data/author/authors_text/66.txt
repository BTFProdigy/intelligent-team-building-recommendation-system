Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 937?946,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
CoCQA: Co-Training Over Questions and Answers 
with an Application to Predicting Question Subjectivity Orientation 
Baoli Li 
Emory University 
csblli@gmail.com 
Yandong Liu 
Emory University 
yliu49@emory.edu 
Eugene Agichtein 
Emory University 
eugene@mathcs.emory.edu
 
 
Abstract 
An increasingly popular method for 
finding information online is via the 
Community Question Answering 
(CQA) portals such as Yahoo! An-
swers, Naver, and Baidu Knows. 
Searching the CQA archives, and rank-
ing, filtering, and evaluating the sub-
mitted answers requires intelligent 
processing of the questions and an-
swers posed by the users. One impor-
tant task is automatically detecting the 
question?s subjectivity orientation: 
namely, whether a user is searching for 
subjective or objective information. 
Unfortunately, real user questions are 
often vague, ill-posed, poorly stated. 
Furthermore, there has been little la-
beled training data available for real 
user questions. To address these prob-
lems, we present CoCQA, a co-training 
system that exploits the association be-
tween the questions and contributed 
answers for question analysis tasks. 
The co-training approach allows 
CoCQA to use the effectively unlim-
ited amounts of unlabeled data readily 
available in CQA archives. In this pa-
per we study the effectiveness of 
CoCQA for the question subjectivity 
classification task by experimenting 
over thousands of real users? questions.
1 Introduction 
Automatic question answering (QA) has been 
one of the long-standing goals of natural lan-
guage processing, information retrieval, and 
artificial intelligence research. For a natural 
language question we would like to respond 
with a specific, accurate, and complete an-
swer that addresses the question. Although 
much progress has been made, answering 
complex, opinion, and even many factual 
questions automatically is still beyond the 
current state-of-the-art.  At the same time, the 
rise of popularity in social media and collabo-
rative content creation services provides a 
promising alternative to web search or com-
pletely automated QA. The explicit support 
for social interactions between participants, 
such as posting comments, rating content, and 
responding to questions and comments makes 
this medium particularly amenable to Ques-
tion Answering. Some very successful exam-
ples of Community Question Answering 
(CQA) sites are Yahoo! Answers 1  and 
Naver2, and Baidu Knows3. Yahoo! Answers 
alone has already amassed hundreds of mil-
lions of answers posted by millions of par-
ticipants on thousands of topics.  
The questions posted to such CQA portals 
are typically complex, subjective, and rely on 
human interpretation to understand the corre-
sponding information need. At the same time, 
the questions are also usually ill-phrased, 
vague, and often subjective in nature. Hence, 
analysis of the questions (and of the corre-
sponding user intent) in this setting is a par-
ticularly difficult task. At the same time, 
CQA content incorporates the relationships 
between questions and the corresponding an-
swers. Because of the various incentives pro-
vided by the CQA sites, answers posted by 
users tend to be, at least to some degree, re-
sponsive to the question. This observation 
suggests investigating whether the relation-
                                                 
1 http://answers.yahoo.com 
2 http://www.naver.com 
3 http://www.baidu.com 
937
ship between questions and answers can be 
exploited to improve automated analysis of the 
CQA content and the user intent behind the 
questions posted.  
     Figure 1: Example question (Yahoo! Answers) 
To this end, we exploit the ideas of co-
training, a general semi-supervised learning 
approach naturally applicable to cases of com-
plementary views on a domain, for example, 
web page links and content (Blum and 
Mitchell, 1998). In our setting, we focus on the 
complimentary views for a question, namely 
the text of the question and the text of the as-
sociated answers.  
As a concrete case-study of our approach 
we focus on one particularly important aspect 
of intent detection: the subjectivity orientation. 
We attempt to predict whether a question 
posted in a CQA site is subjective or objective. 
Objective questions are expected to be an-
swered with reliable or authoritative informa-
tion, typically published online and possibly 
referenced as part of the answer, whereas sub-
jective questions seek answers containing pri-
vate states, e.g. personal opinions, judgment, 
experiences. If we could automatically predict 
the orientation of a question, we would be able 
to better rank or filter the answers, improve 
search over the archives, and more accurately 
identify similar questions. For example, if a 
question is objective, we could try to find a 
few highly relevant articles as references, 
whereas if a question is subjective, useful an-
swers are not expected to be found in authori-
tative sources and tend to rank low with cur-
rent question answering and CQA search tech-
niques. Finally, learning how to identify ques-
tion orientation is a crucial component of in-
ferring user intent, a long-standing problem in 
web information access settings.  
In particular, we focus on the following re-
search questions: 
? Can we utilize the inherent structure of the 
CQA interactions and use the unlimited 
amounts of unlabeled data to improve classi-
fication performance, and/or reduce the 
amount of manual labeling required?  
? Can we automatically predict question sub-
jectivity in Community Question Answering 
? and which features are useful for this task 
in the real CQA setting? 
The rest of the paper is structured as fol-
lows. We first overview the community ques-
tion answering setting, and state the question 
orientation classification problem, which we 
use as the motivating application for our sys-
tem, more precisely. We then introduce our 
CoCQA system for semi-supervised classifi-
cation of questions and answers in CQA com-
munities (Section 3). We report the results of 
our experiments over thousands of real user 
questions in Section 4, showing the effective-
ness of our approach. Finally, we review re-
lated work in Section 5, and discuss our con-
clusions and future work in Section 6.
2 Question Orientation in CQA 
We first briefly describe the essential features 
of question answering communities such as 
Yahoo! Answers or Naver. Then, we formally 
state the problem addressed in this paper, and 
the features used for this setting. 
938
2.1 Community Question Answering  
Online social media content and associated 
services comprise one of the fastest growing 
segments on the Web. The explicit support for 
social interactions between participants, such 
as posting comments, rating content, and re-
sponding to questions and comments makes 
the social media unique. Question answering 
has been particularly amenable to social media 
by directly connecting information seekers 
with the community members willing to share 
the information. Yahoo! Answers, with mil-
lions of users and hundreds of millions of an-
swers for millions of questions is a very suc-
cessful implementation of CQA. 
For example, consider two example user-
contributed questions, objective and subjective 
respectively:  
Q1: What?s the difference between 
chemotherapy and radiation treat-
ments? 
Q2: Has anyone got one of those 
home blood pressure monitors? and 
if so what make is it and do you 
think they are worth getting? 
Figure 1 shows an example of community 
interactions in Yahoo! Answers around the 
question Q2 above. A user posted the question 
in the Health category of the site, and was able 
to obtain 10 responses from other users. Even-
tually, the asker chooses the best answer. Fail-
ing that, as shown in the example, the best an-
swer can also be chosen according to the votes 
from other users. Many of the interactions de-
pend on the perceived goals of the asker: if the 
participants interpret the question as subjec-
tive, they will tend to share their experiences 
and opinions, and if they interpret the question 
as objective, they may still share their experi-
ences but may also provide more factual in-
formation. 
2.2 Problem Definition 
We now state our problem of question orienta-
tion more precisely. We consider question ori-
entation from the perspective of user goals: 
authors of objective questions request authori-
tative, objective information (e.g., published 
literature or expert opinion), whereas authors 
of subjective questions seek opinions or judg-
ments of other users in the community.  We 
state our problem as follows. 
 
Question Subjectivity Problem: Given a 
question Q in a question answering com-
munity, predict whether Q has objective 
or subjective orientation, based on ques-
tion and answer text as well as the user 
and community feedback. 
3 CoCQA: A Co-Training Frame-
work over Questions and Answers 
In the CQA setting we could easily obtain 
thousands or millions of unlabeled examples 
from the online CQA archives. On the other 
hand, it is difficult to create a labeled dataset 
with a reasonable size, which could be used 
to train a perfect classifier to analyze ques-
tions from different domains and sub-
domains. Therefore, semi-supervised learning 
(Chapelle et al, 2006) is a natural approach 
for this setting. 
Intuitively, we can consider the text of the 
question itself or answers to it. In other 
words, we have multiple (at least two) natural 
views of the data, which satisfies the condi-
tions of the co-training approach (Blum and 
Mitchell, 1998). In co-training, two separate 
classifiers are trained on two sets of features, 
respectively. By automatically labeling the 
unlabeled examples, these two classifiers it-
eratively ?teach? each other by giving their 
partners a newly labeled data that they can 
predict with high confidence. Based on the 
original co-training algorithm in (Blum and 
Mitchell, 1998) and other implementations, 
we develop our algorithm CoCQA shown in 
Figure 2. 
At Steps 1 and 2, the K examples are com-
ing from different feature spaces, and each 
category (for example, Subjective and Objec-
tive) has top Kj most confident examples cho-
sen, where Kj corresponds to the distribution 
of class in the current set of labeled examples 
L. CoCQA will terminate when the incre-
ments of both classifiers are less than a speci-
fied threshold X or the maximum number of 
iterations are exceeded. Following the co-
training approach, we include the most confi-
dently predicted examples as additional ?la-
beled? data. The SVM output margin value 
was used to estimate confidence; alternative 
939
methods (including reliability of this confi-
dence prediction) could further improve per-
formance, and we will explore these issues in 
future work. Finally, the next question is how 
to estimate classification performance with 
training data. For each pass, we randomly split 
the original training data into N folds (N=10 in 
our experiments), and keep one part for valida-
tion and the rest, augmented with the newly 
added examples, as the expanded training set. 
After CoCQA terminates, we obtain two 
classifiers. When a new example arrives, we 
will classify it with these two classifiers based 
on both of the feature sets, and combine the 
predictions of these two classifiers. We ex-
plored two strategies to make the final deci-
sion based on the confidence values given by 
two classifiers: 
? Choose the class with higher confidence 
? Multiply the confidence values, and 
choose the class that has the highest 
product. 
We found the second heuristic to be more 
effective than the first in our experiments. As 
the base classifier we use SVM in the current 
implementation, but other classifiers could be 
incorporated as well. 
4 Experimental Evaluation  
We experiment with supervised and semi-
supervised methods on a relatively large data 
set from Yahoo! Answers. 
4.1 Datasets 
To our knowledge, there is no standard data-
set of real questions and answers posted by 
online users, labeled for subjectivity orienta-
tion. Hence, we had to create a dataset our-
selves. To create our dataset, we downloaded 
more than 30,000 resolved questions from 
each of the following top-level categories of 
Yahoo! Answers: Arts, Education, Health, 
Science, and Sports. We randomly chose 200 
questions from each category to create a raw 
dataset with 1,000 questions total. Then, we 
labeled the dataset with annotators from the 
Amazon?s Mechanical Turk service4.  
For annotation, each question was judged 
by 5 Mechanical Turk workers who passed a 
qualification test of 10 questions (labeled by 
ourselves) with at least 9 of them correctly 
marked. The qualification test was required to 
ensure that the raters were sufficiently com-
petent to make reasonable judgments. We 
grouped the tasks into 25 question batches, 
where the whole batch was submitted as the 
Mechanical Turk?s Human Intelligence Task 
(HIT). The batching of questions was done to 
easily detect the ?random? ratings produced 
by irresponsible workers. That is, each 
worker rated a batch of 25 questions.  
While precise definition of subjectivity is 
elusive, we decided to take the practical per-
spective, namely the "majority" interpreta-
tion. The annotators were instructed to guess 
orientation according to how the question 
would be answered by most people. We did 
not deal with multi-part questions: if any part 
of question was subjective, the whole ques-
tion was labeled as subjective. The gold stan-
dard was thus derived with the majority strat-
egy, followed by manual inspection as a ?san-
ity check?. At this stage we removed 22 ques-
tions with undeterminable meaning, including 
gems such as ?Upward Soccer 
                                                 
4 http://www.mturk.com 
Figure 2: Algorithm CoCQA: A co-training algo-
rithm for exploiting redundant feature sets in 
community question answering. 
Input: 
? FQ and FA are Question and Answer feature views 
? CQ and CA are classifiers trained on FQ and FA  respec-
tively 
? L is a set of labeled training examples 
? U is a set of unlabeled examples 
? K: Number of unlabeled examples to choose on  
each iteration 
? X:  the threshold for  increment 
? R:  the maximal number of iterations 
Algorithm CoCQA 
1. Train CQ ,0 on L: FQ , and record resulting   ACCQ,0 
2. Train CA ,0 on L: FA , and record resulting  ACCA ,0 
3. for j=1 to R do: 
        Use CQ,j-1 to predict labels for U and choose 
               top K items with highest confidence ? EQ, , j-1 
        Use CA,j-1 to predict labels for U and  choose  
                top K items with highest confidence ? EA, , j-1 
        Move examples EQ, , j-1 U EA, , j-1 ? L 
        Train CQ,j on L: FQ and record training  ACCQ,j 
        Train CA,j on L: FA and record training  ACCA,j 
             if Max(?ACCQ,j, ? ACCA,j) < X break 
  
4.     return final classifiers CQ,j ? CQ and CA,j ? CA
940
Shorts??5 and ?1+1=?fdgdgdfg??6. Fi-
nally, we create a labeled dataset consisting of 
978 resolved questions, available online7.  
 
 
Num. of 
SUB. Q 
Num. of 
OBJ. Q 
Total 
Num. 
Annotator
agreement
Arts 137 (70%) 58 (30%) 195 0.841 
Education 127 (64%) 70 (36%) 197 0.716 
Health 125 (64%) 69 (36%) 194 0.833 
Science 103 (52%) 94 (48%) 197 0.618 
Sports 154 (79%) 41 (21%) 195 0.877 
Total 646 (66%) 332 (34%) 978 0.777 
Table 1: Labeled dataset statistics. 
 
Table 1 reports the statistics of the annotated 
dataset. The overall inter-annotator percentage 
agreement between Mechanical Turk workers 
and final annotation is 0.777, indicating that 
the task is difficult, but feasible for humans to 
annotate manually.  
The statistics of our labeled sample show 
that the vast majority of the questions in all 
categories except for Science are subjective in 
nature. The relatively high ratio of subjective 
questions in the Science category is surprising. 
However, we find that users often post polem-
ics and statements instead of questions, using 
CQA as a forum to share their opinions on 
controversial topics. Overall, we were struck 
by the expressed need in Subjective informa-
tion, even for categories such as Health and 
Education, where objective information would 
intuitively seem more desirable.  
4.2 Features Used in Experiments 
For the subjectivied experiments to follow, 
we attempt to capture the linguistic 
characteristics identified in previous work 
(Section 5) in a lightweight and robust manner, 
due to the informal and noisy nature of CQA. 
In particular, we use the following feature 
classes, computed separately over question and 
answer content: 
? Character 3-grams  
? Words 
? Word with Character 3-grams 
? Word n-grams (n<=3, i.e. Wi, WiWi+1,  
WiWi+1Wi+2) 
                                                 
5http://answers.yahoo.com/question/?qid=20060829074901AA
DBRJ4  
6 http://answers.yahoo.com/question/?qid=1006012003651  
7 Available at http://ir.mathcs.emory.edu/datasets/. 
? Word and POS n-gram (n<=3, i.e. Wi, 
WiWi+1, Wi POSi+1, POSiWi+1 , 
POSiPOSi+1, etc.).  
We use the character 3-grams features to 
overcome spelling errors and problems of ill-
formatted or ungrammatical questions, and 
the POS information to capture common pat-
terns across domains, as words, especially the 
content words, are quite diverse in different 
topical domains. For word and character 3-
gram features, we consider two different ver-
sions: case-sensitive and case-insensitive. 
Case-insensitive features are assumed to be 
helpful for mitigating negative effects of ill-
formatted text. 
Moreover, we experimented with three 
term weighting schemes: Binary, TF, and 
TF*IDF. Term frequency (TF) exhibited bet-
ter performance in our development experi-
ments, so we use this weighting scheme for 
all the experiments in Section 4. Regarding 
features: both words and structure of the text 
(e.g., word order) can be used to infer subjec-
tivity. Therefore, the features we employ, 
such as words and word n-grams, are ex-
pected to be useful as a (coarse) proxy to 
grammatical and phrase features. Unlike tra-
ditional work on news-like text, the text of 
CQA and has poor spelling, grammar, and 
heavily uses non-standard abbreviations, 
hence our decision to use character n-grams.  
4.3 Experimental Setting 
Metrics: Since the prediction  on both sub-
jective questions and objective questions is 
equally important, we use the macro-
averaged F1 measure as the evaluation met-
ric. This is computed as the macro average of 
F1 measures computed for the Subjective and 
Objective classes individually. The F1 meas-
ure for either class is computed 
as
RecallPrecision 
Recall Precision 2
 
+
?? . 
 
Methods compared: We compare our ap-
proach with both the base supervised learning, 
as well as GE, a state-of-the-art semi-
supervised method:  
? Supervised: we use the LibSVM im-
plementation (Chang and Lin, 2001) 
with linear kernel.   
941
? GE: This is a state-of-the-art semi-
supervised learning algorithm, General-
ized Expectation (GE), introduced in 
(McCallum et al, 2007) that incorporates 
model expectations into the objective 
functions for parameter estimation. 
? CoCQA: Our method (Section 3).  
 
For semi-supervised learning experiments, 
we selected a random subset of 2,000 unla-
beled questions for each of the topical catego-
ries, for the total of 10,000 unlabeled questions. 
4.4 Experimental Results 
First we report the performance of our Super-
vised baseline system with a variety of fea-
tures, reporting the average results of 5-fold 
cross validation. Then we investigate the per-
formance to our new CoCQA framework under 
a variety of settings. 
4.4.1 Supervised Learning 
Table 2 reports the classification perform-
ance for varying units of representation (e.g., 
question text vs. answer text) and the varying 
feature sets. We used case-insensitive features 
and TF (term frequency within the text unit) as 
feature weights, as these two settings achieved 
the best results in our development experi-
ments. The rows show performance consider-
ing only the question text (question), the best 
answer (best_ans), text of all answers to a 
question (all_ans), the text of the question and 
the best answer (q_bestans), and the text of 
the question with all answers (q_allans), re-
spectively.  In particular, using the words in 
the question alone achieves F1 of 0.717, com-
pared to using words in the question and the 
best answers text (F1 of 0.695). For compari-
son, a na?ve baseline that always guesses the 
majority class (Subjective) obtains F1 of 0.398. 
With character 3-gram, our system achieves 
performance comparable with words as fea-
tures, but combining them together does not 
improve performance. We observe a slight 
gain with more complicated features, e.g. word 
n-gram, or word and POS n-grams, but the 
gain is not significant, and hence not worth the 
increased complexity of the feature generation. 
Finally, combining question text with answer 
text does not improve performance.  
Interestingly, the best answer itself is not as 
effective as the question for subjectivity 
analysis, nor is using all of the answers sub-
mitted. One possible reason is that approxi-
mately 40% of the best answers were chosen 
by the community and not the asker herself, 
are hence not necessarily representative of the 
asker intent.  
 
Feature
set
 
Unit 
Char 
3-
gram 
Word 
Word+ 
Char 
3-gram 
Word 
n-gram 
(n<=3) 
Word 
POS 
n-gram
(n<=3) 
question 0.700 0.717 0.694 0.716 0.720 
best_ans 0.587 0.597 0.578 0.580 0.565 
all_ans 0.603 0.628 0.607 0.648 0.630 
q_bestans 0.681 0.695 0.662 0.687 0.712 
q_allans 0.679 0.677 0.676 0.708 0.689 
Na?ve (majority class) baseline:  0.398 
Table 2. Performance of predicting question 
orientation on the mixed dataset with varying 
feature sets (Supervised). 
 
Table 3 reports the supervised subjectivity 
classification performance for each question 
category with word features. The overall clas-
sification results are significantly lower com-
pared to training and testing on the mixture of 
the questions drawn from all categories, 
likely caused by the small amount of labeled 
training data for each category. Another pos-
sibility is that the subjectivity clues are not 
topical and hence are not category dependent, 
with the possible exception of the questions 
in the Health domain.  
 
Category Arts Edu. Health Science Sports
F1 0.448 0.572 0.711 0.647 0.441 
Table 3. Experiment results on sub-categories 
with supervised SVM (q_bestans features).  
 
As words are simple and effective features 
in this experiment, we will use them in the 
subsequent experiments. Furthermore, the 
feature set using the words in the question 
with best answer together (q_bestans) exhibit 
higher performance than question with all 
answers (q_allans). Thus, we will only con-
sider questions and best answers in the fol-
lowing experiments with GE and CoCQA. 
4.4.2 Semi-Supervised Learning 
We now focus on investigating the effec-
tiveness of CoCQA, our co-training-based 
framework for community question answer-
ing analysis. Table 4 summarizes the main 
942
results of this section. The values for CoCQA 
are derived with the parameter settings: K=100, 
X=0.001. These optimal settings are chosen 
after comprehensive experiments with differ-
ent combinations, described later in this sec-
tion. GE does not exhibit a significant im-
provement over Supervised. In contrast, 
CoCQA performs significantly better than the 
purely supervised method, with F1 of 0.745 
compared to the F1 of 0.717 for Supervised. 
While it may seem surprising that a semi-
supervised method outperforms a supervised 
one, note that we use all of the available la-
beled data as provided to the Supervised 
method, as well as a large amount of unlabeled 
data, that is ultimately responsible for the per-
formance improvement. 
  
Features 
Method 
Question Question+ Best Answer 
Supervised 0.717 0.695 
GE 0.712 (-0.7%) 0.717 (+3.2%) 
CoCQA 0.731 (+1.9%) 0.745 (+7.2%) 
Table 4. Performance of CoCQA, GE, and Su-
pervised with the same feature and data settings.  
 
As an added advantage, CoCQA approach is 
also practical. In a realistic application, we 
have two different situations: offline and 
online. With online processing, we may not 
have best answers available to predict ques-
tion?s orientation, whereas we can employ in-
formation from best answers in offline setting. 
Co-training is a solution that is applicable to 
both situations. With CoCQA, we have two 
classifiers using the question text and the best 
answer text, respectively. We can use both of 
them to obtain better results in the offline set-
ting, while in online setting, we can use the 
text of the question alone. In contrast, GE may 
not have this flexibility.  
We now analyze the performance of 
CoCQA under a variety of settings to derive 
optimal parameters and to better understand 
the performance. Figure 3 reports the perform-
ance of CoCQA with varying the K parameter 
from 20 to 200. In this experiment, we fix X to 
be 0.001. The combination of question and 
best answer is superior to that of question and 
all answers. When K is 100, the system obtains 
the best result, 0.745.  
Figure 4 reports the number of co-training 
iterations needed to converge to optimal per-
formance. After 13 iterations (and 2500 unla-
beled examples added), CoCQA achieves op-
timal performance, and eventually terminates 
after an additional 3 iterations. While a vali-
dation set should have been used for CoCQA 
parameter tuning, Figures 3 and 4 indicate 
that CoCQA is not sensitive to the specific 
parameter settings. In particular, we observe 
that any K is greater than 100, and for any 
number of iterations R is greater than 10, 
CoCQA exhibits in effectively equivalent per-
formance. 
 
0.64
0.65
0.66
0.67
0.68
0.69
0.7
0.71
0.72
0.73
0.74
0.75
0.76
20 40 60 80 100 120 140 160 180 200K: # labeled examples added on each 
co-training iteration
F
1
CoCQA(Question and Best Answer)
Supervised Q_bestans
CoCQA(Question and All Answers)
Supervised Q_allans
 
Figure 3: Performance of CoCQA for varying 
the K (number of examples added on each it-
eration of co-training). 
 
Figure 5 reports the performance of 
CoCQA for varying the number of labeled 
examples from 50 to 400 (that is, up to 50% 
of the available labeled training data). Note 
that for this comparison we use the same fea-
ture sets  (words in question and best answer 
text), but using only the (varying) fractions of 
the manually labeled data. Surprisingly, 
CoCQA exhibits comparable performance of 
F1=0.685 with only 200 labeled examples are 
used, compared to the F1=0.695 for Super-
vised with all 800 labeled training examples 
on this feature set. In other words, CoCQA is 
able to achieve comparable performance to 
supervised SVM with only one quarter of the 
labeled training data. 
 
943
0.71
0.72
0.73
0.74
0.75
161377776666
# co-training iterations
F
1
0
500
1000
1500
2000
2500
3000
3500
T
ot
al
 #
 U
n
la
b
el
ed
 A
d
d
ed
CoCQA (Question + Best Answer)
Supervised
Total # Unlabeled
 
Figure 4: Performance and the total number of 
unlabeled examples added for varying number 
of co-training iterations (K=100, using q_bestans 
features) 
 
 
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
50 100 150 200 250 300 350 400
# of labeled data used
F1
CoCQA (Question + Best Answer)
Supervised Q_Best Ans
 
Figure 5: Performance of CoCQA with varying 
number of labeled examples used, compared to 
Supervised method, on same features. 
5 Related Work 
Question analysis, especially question classifi-
cation, has been long studied in the question 
answering research community. However, 
most of the previous research primarily con-
sidered factual questions, with the notable ex-
ception of the most recent TREC opinion QA 
track. Furthermore, the questions were specifi-
cally designed for benchmark evaluation. A 
related thread of research considered deep 
analysis of the questions (and corresponding 
sentences) by manually classifying questions 
along several orientation dimensions, notably 
(Stoyanov et al, 2005).  In contrast, our work 
focuses on analyzing real user questions 
posted in a question answering community. 
These questions are often complex or subjec-
tive, and are typically difficult to answer 
automatically as the question author probably 
was not able to find satisfactory answers with 
quick web search. 
Automatic complex question answering has 
been an active area of research, ranging from 
simple modification to factoid QA techniques 
(e.g., Soricut and Brill, 2003) to knowledge 
intensive approaches for specific domains 
(e.g., Harabagiu et al 2001, Fushman and Lin 
2007). However, the technology does not yet 
exist to automatically answer open-domain 
complex and subjective questions. While 
there has been some recent research (e.g., 
Agichtein et al 2008, Bian et al 2008) on 
retrieving high quality answers from CQA 
archives, the subjectivity orientation of the 
questions has not been considered as a feature 
for ranking.  
A related corresponding problem is com-
plex QA evaluation. Recent efforts at auto-
matic evaluation show that even for well-
defined, objective, complex questions, 
evaluation is extremely labor-intensive and 
introduces many challenges (Lin and 
Fushman 2006, Lin and Zhang 2007). As part 
of our contribution we showed that it is feasi-
ble to use the Amazon Mechanical Turk ser-
vice for evaluation by combining large degree 
of annotator redundancy (5 annotators per 
question) with more sparse but higher-quality 
expert annotation. 
The problem of automatic subjective ques-
tion answering has recently started to be ad-
dressed in the question answering commu-
nity, most recently as the first opinion QA 
track in (Dang et al, 2007). Unlike the con-
trolled TREC opinion track (introduced in 
2007), many of the questions in Yahoo! An-
swers community are inherently subjective, 
complex, ill-formed, or all of the above. To 
our knowledge, this paper is the first large-
scale study of subjective/objective orientation 
of information needs, and certainly the first in 
the CQA environment. 
A closely related research thread is subjec-
tivity analysis at document and sentence 
level. For example, reference (Yu, H., and 
Hatzivassiloglou, V. 2003; Somasundaran et 
944
al. 2007) attempted to classify sentences into 
those reporting facts or opinions. Also related 
is research on sentiment analysis (e.g., Pang et 
al., 2004) where the goal is to classify a sen-
tence or text fragment as being overall positive 
or negative. More generally, (Wiebe et al 
2004) and subsequent work focused on the 
analysis of subjective language in narrative 
text, primarily news. Our problem is quite dif-
ferent in the sense that we are trying to iden-
tify the orientation of a question. Nevertheless, 
our baseline method is similar to the methods 
and features used for sentiment analysis, and 
one of our contributions is evaluating the use-
fulness of the established features and tech-
niques to the new CQA setting. 
In order to predict question orientation, we 
build on co-training, one of the known semi-
supervised learning techniques. Many models 
and techniques have been proposed for classi-
fication, including support vector machines, 
decision tree based techniques, boosting-based 
techniques, and many others. We use LIBSVM 
(Chang and Lin, 2001) as a robust implemen-
tation of SVM algorithms. 
In summary, while we draw on many tech-
niques in question answering, natural language 
processing, and text classification, our work 
differs from previous research in that a) de-
velop a novel co-training based algorithm for 
question and answer classification; b) we ad-
dress a relatively new problem of automatic 
question subjectivity prediction; c) demon-
strate the effectiveness of our techniques in the 
new CQA setting and d) explore the character-
istics unique to CQA ? while showing good 
results for a quite difficult task. 
6 Conclusions 
We presented CoCQA, a co-training frame-
work for modeling the textual interactions in 
question answer communities. Unlike previous 
work, we have focused on real user questions 
(often noisy, ungrammatical, and vague) sub-
mitted in Yahoo! Answers, a popular commu-
nity question answering portal. We demon-
strated CoCQA for one particularly important 
task of automatically identifying question sub-
jectivity orientation, showing that CoCQA is 
able to exploit the structure of questions and 
corresponding answers. Despite the inherent 
difficulties of subjectivity analysis for real user 
questions, we have shown that by applying 
CoCQA to this task we can significantly im-
prove prediction performance, and substan-
tially reduce the size of the required training 
data, while outperforming a general state-of-
the-art semi-supervised algorithm that does 
not take advantage of the CQA characteris-
tics.  
In the future we plan to explore more so-
phisticated features such semantic concepts 
and relationships (e.g., derived from WordNet 
or Wikipedia), and richer syntactic and lin-
guistic information. We also plan to explore 
related variants of semi-supervised learning 
such as co-boosting methods to further im-
prove classification performance. We will 
also investigate other applications of our co-
training framework to tasks such as sentiment 
analysis in community question answering 
and similar social media content. 
Acknowledgments 
This research was partially supported by the 
Emory University Research Committee 
(URC) grant, and by the Emory College Seed 
grant. We thank the Yahoo! Answers team for 
providing access to the Answers API, and 
anonymous reviewers for their excellent sug-
gestions. 
References 
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and 
Mishne, G. 2008. Finding High-Quality Content in 
Social Media with an Application to Community-
Based Question Answering. WSDM2008  
Bian, J., Liu, Y., Agichtein, E., and H. Zha. 2008, to 
appear. Finding the Right Facts in the Crowd: Fac-
toid Question Answering over Social Media, Pro-
ceedings of the Inter-national World Wide Web Con-
ference (WWW), 2008  
Blum, A., and Mitchell, T. 1998. Combining Labeled 
and Unlabeled Data with Co-Training. Proc. of the 
Annual Conference on Computational Learning 
Theory.  
Chang, C. C. and Lin, C. J. 2001. LIBSVM : a library 
for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm.  
Chapelle, O., Scholkopf, B., and Zien, A. 2006. Semi-
supervised Learning. The MIT Press, Cambridge, 
Mas-sachusetts.  
Dang, H. T., Kelly, D., and Lin, J. 2007. Overview of 
the TREC 2007 Question Answering track. In Pro-
ceedings of TREC-2007.  
945
Demner-Fushman, D. and Lin, J. 2007. Answering clini-
cal questions with knowledge-based and statistical 
techniques. Computational Linguistics, 33(1):63?103.  
Harabagiu, S., Moldovan, D., Pasca, M., Surdeanu, M. , 
Mihalcea, R., Girju, R., Rusa, V., Lacatusu, F., 
Morarescu, P., and Bunescu, R. 2001. Answering 
Complex, List and Context Questions with LCC's 
Question-Answering Server. In Proc. of TREC 2001.  
Lin, J. and Demner-Fushman, D. 2006. Methods for 
automatically evaluating answers to complex ques-
tions. In-formation Retrieval, 9(5):565?587  
Lin, J. and Zhang, P. 2007. Deconstructing nuggets: the 
stability and reliability of complex question answering 
evaluation. In Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 327?334.  
Mann, G., and McCallum, A. 2007. Simple, Robust, 
Scalable Semi-supervised Learning via Expectation 
Regularization. Proceedings of ICML 2007.  
Pang, B., and Lee, L. 2004. A Sentimental Education: 
Sen-timent Analysis Using Subjective Summarization 
Based on Minimum Cuts. In Proc. of ACL.  
Prager, J. 2006. Open-Domain Question-Answering. 
Foundations and Trends in Information Retrieval.  
Sindhwani, V., Keerthi, S. 2006. Large Scale Semi-
supervised Linear SVMs. Proceedings of SIGIR 2006.  
Somasundaran, S., Wilson, T., Wiebe, J. and Stoyanov, 
V. 2007. QA with Attitude: Exploiting Opinion Type 
Analysis for Improving Question Answering in On-
line Discussions and the News. In proceedings of In-
ternational Conference on Weblogs and Social Media 
(ICWSM-2007).  
Soricut, R. and Brill, E. 2004. Automatic question an-
swering: Beyond the factoid. Proceedings of HLT-
NAACL.  
Stoyanov, V., Cardie, C., and Wiebe, J. 2005. Multi-
Perspective question answering using the OpQA cor-
pus. In Proceedings of EMNLP.  
Tri, N. T., Le, N. M., and Shimazu, A. 2006. Using 
Semi-supervised Learning for Question Classification. 
In Proceedings of ICCPOL-2006.  
Wiebe, J., Wilson, T., Bruce R., Bell M., and Martin M. 
2004. Learning subjective language. Computational 
Linguistics, 30 (3).  
Yu, H., and Hatzivassiloglou, V. 2003. Towards Answer-
ing Opinion Questions: Separating Facts from Opin-
ions and Identifying the Polarity of Opinion Sentences. 
In Proceedings of EMNLP-2003.  
Zhang, D., and Lee, W.S. 2003. Question Classification 
Using Support Vector Machines. Proceedings of the 
26th Annual International ACM SIGIR Conference on 
Re-search and Development in Information Retrieval.  
Zhu, X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report 1530, Computer Sciences, 
University of Wisconsin-Madison. 
 
946
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 97?100,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
You?ve Got Answers: Towards Personalized Models for Predicting Success
in Community Question Answering
Yandong Liu and Eugene Agichtein
Emory University
{yliu49,eugene}@mathcs.emory.edu
Abstract
Question answering communities such as Ya-
hoo! Answers have emerged as a popular al-
ternative to general-purpose web search. By
directly interacting with other participants, in-
formation seekers can obtain specific answers
to their questions. However, user success in
obtaining satisfactory answers varies greatly.
We hypothesize that satisfaction with the con-
tributed answers is largely determined by the
asker?s prior experience, expectations, and
personal preferences. Hence, we begin to de-
velop personalized models of asker satisfac-
tion to predict whether a particular question
author will be satisfied with the answers con-
tributed by the community participants. We
formalize this problem, and explore a variety
of content, structure, and interaction features
for this task using standard machine learning
techniques. Our experimental evaluation over
thousands of real questions indicates that in-
deed it is beneficial to personalize satisfaction
predictions when sufficient prior user history
exists, significantly improving accuracy over
a ?one-size-fits-all? prediction model.
1 Introduction
Community Question Answering (CQA) has re-
cently become a viable method for seeking infor-
mation online. As an alternative to using general-
purpose web search engines, information seekers
now have an option to post their questions (often
complex, specific, and subjective) on Community
QA sites such as Yahoo! Answers, and have their
questions answered by other users. Hundreds of mil-
lions of answers have already been posted for tens of
millions of questions in Yahoo! Answers. However,
the success of obtaining satisfactory answers in the
available CQA portals varies greatly. In many cases,
the questions posted by askers go un-answered, or
are answered poorly, never obtaining a satisfactory
answer.
In our recent work (Liu et al, 2008) we have in-
troduced a general model for predicting asker sat-
isfaction in community question answering. We
found that previous asker history is a significant fac-
tor that correlates with satisfaction. We hypothesize
that asker?s satisfaction with contributed answers is
largely determined by the asker expectations, prior
knowledge and previous experience with using the
CQA site. Therefore, in this paper we begin to ex-
plore how to personalize satisfaction prediction -
that is, to attempt to predict whether a specific in-
formation seeker will be satisfied with any of the
contributed answers. Our aim is to provide a ?per-
sonalized? recommendation to the user that they?ve
got answers that satisfy their information need.
To the best of our knowledge, ours is the first ex-
ploration of personalizing prediction of user satis-
faction in complex and subjective information seek-
ing environments. While information seeker sat-
isfaction has been studied in ad-hoc IR context
(see (Kobayashi and Takeda, 2000) for an overview),
previous studies have been limited by the lack of re-
alistic user feedback. In contrast, we deal with com-
plex information needs and community-provided
answers, trying to predict subjective ratings pro-
vided by users themselves. Furthermore, while au-
tomatic complex QA has been an active area of re-
search, ranging from simple modification to factoid
QA technique (e.g., (Soricut and Brill, 2004)) to
knowledge intensive approaches for specialized do-
mains, the technology does not yet exist to automat-
ically answer open domain, complex, and subjective
questions. Hence, this paper contributes to both the
understanding of complex question answering, and
explores evaluation issues in a new setting.
The rest of the paper is organized as follows. We
describe the problem and our approach in Section
2, including our initial attempt at personalizing sat-
isfaction prediction. We report results of a large-
scale evaluation over thousands of real users and
97
tens of thousands of questions in Section 3. Our
results demonstrate that when sufficient prior asker
history exists, even simple personalized models re-
sult in significant improvement over a general pre-
diction model. We discuss our findings and future
work in Section 4.
2 Predicting Asker Satisfaction in CQA
We first briefly review the life of a question in a
QA community. A user (the asker) posts a question
by selecting a topical category (e.g., ?History?), and
then enters the question and, optionally, additional
details. After a short delay the question appears in
the respective category list of open questions. At
this point, other users can answer the question, vote
on other users? answers, or interact in other ways.
The asker may be notified of the answers as they are
submitted, or may check the contributed answers pe-
riodically. If the asker is satisfied with any of the
answers, she can choose it as best, and rate the an-
swer by assigning stars. At that point, the question
is considered as closed by asker. For more detailed
treatment of user interactions in CQA see (Liu et
al., 2008). If the asker rates the best answer with
at least three out of five ?stars?, we believe the asker
is satisfied with the response. But often the asker
never closes the answer personally, and instead, af-
ter a period of time, the question is closed automat-
ically. In this case, the ?best? answer may be cho-
sen by the votes, or alternatively by automatically
predicting answer quality (e.g., (Jeon et al, 2006)
or (Agichtein et al, 2008)). While the best answer
chosen automatically may be of high quality, it is un-
known if the asker?s information need was satisfied.
Based on our exploration we believe that the main
reasons for not ?closing? a question are a) the asker
loses interest in the information and b) none of the
answers are satisfactory. In both cases, the QA com-
munity has failed to provide satisfactory answers in
a timely manner and ?lost? the asker?s interest. We
consider this outcome to be ?unsatisfied?. We now
define asker satisfaction more precisely:
Definition 1 An asker in a QA community is consid-
ered satisfied iff: the asker personally has closed the
question and rated the best answer with at least 3
?stars?. Otherwise, the asker is unsatisfied.
This definition captures a key aspect of asker satis-
faction, namely that we can reliably identify when
the asker is satisfied but not the converse.
2.1 Asker Satisfaction Prediction Framework
We now briefly review our ASP (Asker Satisfac-
tion Prediction) framework that learns to classify
whether a question has been satisfactorily answered,
originally introduced in (Liu et al, 2008). ASP em-
ploys standard classification techniques to predict,
given a question thread, whether an asker would be
satisfied. A sample of features used to represent this
problem is listed in Table 1. Our features are or-
ganized around the basic entities in a question an-
swering community: questions, answers, question-
answer pairs, users, and categories. In total, we de-
veloped 51 features for this task. A sample of the
features used are listed in the Figure 1.
? Question Features: Traditional question answer-
ing features such as the wh-type of the question
(e.g., ?what? or ?where?), and whether the ques-
tion is similar to other questions in the category.
? Question-Answer Relationship Features: Over-
lap between question and answer, answer length,
and number of candidate answers. We also use
features such as the number of positive votes
(?thumbs up? in Yahoo! Answers), negative votes
(?thumbs down?), and derived statistics such as
the maximum of positive or negative votes re-
ceived for any answer (e.g., to detect cases of bril-
liant answers or, conversely, blatant abuse).
? Asker User History: Past asker activity history
such as the most recent rating, average past satis-
faction, and number of previous questions posted.
Note that only the information available about the
asker prior to posting the question was used.
? Category Features: We hypothesized that user
behavior (and asker satisfaction) varies by topi-
cal question category, as recently shown in refer-
ence (Agichtein et al, 2008). Therefore we model
the prior of asker satisfaction for the category,
such as the average asker rating (satisfaction).
? Text Features: We also include word unigrams and
bigrams to represent the text of the question sub-
ject, question detail, and the answer content. Sep-
arate feature spaces were used for each attribute to
keep answer text distinct from question text, with
frequency-based filtering.
Classification Algorithms: We experimented with
a variety of classifiers in the Weka framework (Wit-
ten and Frank, 2005). In particular, we com-
pared Support Vector Machines, Decision trees, and
Boosting-based classifiers. SVM performed the best
98
Feature Description
Question Features
Q: Q punctuation density Ratio of punctuation to words in the question
Q: Q KL div wikipedia KL divergence with Wikipedia corpus
Q: Q KL div category KL divergence with ?satisfied? questions in category
Q: Q KL div trec KL divergence with TREC questions corpus
Question-Answer Relationship Features
QA: QA sum pos vote Sum of positive votes for all the answers
QA: QA sum neg vote Sum of negative votes for all the answers
QA: QA KL div wikipedia KL Divergence of all answers with Wikipedia corpus
Asker User History Features
UH: UH questions resolved Number of questions resolved in the past
UH: UH num answers Number of all answers this user has received in the past
UH: UH more recent rating Rating for the last question before current question
UH: UH avg past rating Average rating given when closing questions in the past
Category Features
CA: CA avg time to close Average interval between opening and closing
CA: CA avg num answers Average number of answers for that category
CA: CA avg asker rating Average rating given by asker for category
CA: CA avg num votes Average number of ?best answer? votes in category
Table 1: Sample features: Question (Q), Question-
Answer Relationship (QA), Asker history (UH), and Cat-
egory (CA).
of the three during development, so we report results
using SVM for all the subsequent experiments.
2.2 Personalizing Asker Satisfaction Prediction
We now describe our initial attempt at personalizing
the ASP framework described above to each asker:
? ASP Pers+Text: We first consider the naive per-
sonalization approach where we train a separate
classifier for each user. That is, to predict a par-
ticular asker?s satisfaction with the provided an-
swers, we apply the individual classifier trained
solely on the questions (and satisfaction labels)
provided in the past by that user.
? ASP Group: A more robust approach is to train a
classifier on the questions from the group of users
similar to each other. Our current grouping was
done simply by the number of questions posted,
essentially grouping users with similar levels of
?activity?. As we will show below, text features
only help for users with at least 20 previous ques-
tions. So, we only include text features for groups
of users with at least 20 questions.
Certainly, more sophisticated personalization mod-
els and user clustering methods could be devised.
However, as we show next, even the simple models
described above prove surprisingly effective.
3 Experimental Evaluation
Wewant to predict, for a given user and their current
question whether the user will be satisfied, accord-
ing to our definition in Section 2. In other words, our
?truth? labels are based on the rating subsequently
given to the best answer by the asker herself. It is
usually more valuable to correctly predict whether
a user is satisfied (e.g., to notify a user of success).
#Questions per Asker # Questions # Answers # Users
1 132,279 1,197,089 132,279
2 31,692 287,681 15,846
3-4 23,296 213,507 7,048
5-9 15,811 143,483 2,568
10-14 5,554 54,781 481
15-19 2,304 21,835 137
20-29 2,226 23,729 93
30-49 1,866 16,982 49
50-100 842 4,528 14
Total: 216,170 1,963,615 158,515
Table 2: Distribution of questions, answers and askers
.
Hence, we focus on the Precision, Recall, and F1
values for the satisfied class.
Datasets: Our data was based on a snapshot of Ya-
hoo! Answers crawled in early 2008, containing
216,170 questions posted in 100 topical categories
by 158,515 askers, with associated 1,963,615 an-
swers in total. More detailed statistics, arranged by
the number of questions posted by each asker are
reported in (Table 2). The askers with only one
question (i.e., no prior history) dominate the dataset,
as many users try the service once and never come
back. However, for personalized satisfaction, at least
some prior history is needed. Therefore, in this early
version of our work, we focus on users who have
posted at least 2 questions - i.e., have the minimal
history of at least one prior question. In the future,
we plan to address the ?cold start? problem of pre-
dicting satisfaction of new users.
Methods compared:
? ASP: A ?one-size-fits-all? satisfaction predictor
that is trained on 10,000 randomly sampled ques-
tions with only non-textual features (Section 2.1).
? ASP+Text: The ASP classifier with text features.
? ASP Pers+Text and ASP Group: A personal-
ized classifiers described in Section 2.2.
3.1 Experimental Results
Figure 1 reports the satisfaction prediction accu-
racy for ASP, ASP Text, ASP Pers+Text, and
ASP Group for groups of askers with varying num-
ber of previous questions posted. Surprisingly,
for ASP Text, textual features only become help-
ful for users with more than 20 or 30 previous
questions posted and degrade performance other-
wise. Also note that baseline ASP classifier is
not able to achieve higher accuracy even for users
with large amount of past history. In contrast,
the ASP Pers+Text classifier, trained only on the
past question(s) of each user, achieves surprisingly
good accuracy ? often significantly outperforming
the ASP and ASP Text classifiers. The improve-
ment is especially dramatic for users with at least
99
Figure 1: Precision, Recall, and F1 of ASP, ASP Text, ASP Pers+Text, and ASP Group for predicting satisfaction of
askers with varying number of questions
20 previous questions. Interestingly, the simple
strategy of grouping users by number of previous
questions (ASP Group) is even more effective, re-
sulting in accuracy higher than both other meth-
ods for users with moderate amount of history. Fi-
nally, for users with only 2 questions total (that is,
only 1 previous question posted) the performance
of ASP Pers+Text is surprisingly high. We found
that the classifier simply ?memorizes? the outcome
of the only available previous question, and uses it
to predict the rating of the current question.
To better understand the improvement of person-
alized models, we report the most significant fea-
tures, sorted by Information Gain (IG), for three
sample ASP Pers+Text models (Table 3). Interest-
ingly, whereas for Pers 1 and Pers 2, textual features
such as ?good luck? in the answer are significant, for
Pers 3 non-textual features are most significant.
We also report the top 10 features with the high-
est information gain for the ASP and ASP Group
models (Table 4). Interestingly, while asker?s aver-
age previous rating is the top feature for ASP, the
length of membership of the asker is the most impor-
tant feature for ASP Group, perhaps allowing the
classifier to distinguish more expert users from the
active newbies. In summary, we have demonstrated
promising preliminary results on personalizing sat-
isfaction prediction even with relatively simple per-
sonalization models.
Pers 1 (97 questions) Pers 2 (49 questions) Pers 3 (25 questions)
UH total answers received Q avg pos votes Q content kl trec
UH questions resolved ?would? in answer Q content kl wikipedia
?good luck? in answer ?answer? in question UH total answers received
?is an? in answer ?just? in answer UH questions resolved
?want to? in answer ?me? in answer Q content kl asker all cate
?we? in answer ?be? in answer Q prev avg rating
?want in? answer ?in the? in question CA avg asker rating
?adenocarcinoma? in question CA History ?anybody? in question
?was? in question ?who is? in question Q content typo density
?live? in answer ?those? in answer Q detail len
Table 3: Top 10 features by Information Gain for three
sample ASP Pers+Text models
.
IG ASP IG ASP Group
0.104117 Q prev avg rating 0.30981 UH membersince in days
0.102117 Q most recent rating 0.25541 Q prev avg rating
0.047222 Q avg pos vote 0.22556 Q most recent rating
0.041773 Q sum pos vote 0.15237 CA avg num votes
0.041076 Q max pos vote 0.14466 CA avg time close
0.03535 A ques timediff in minutes 0.13489 CA avg asker rating
0.032261 UH membersince in days 0.13175 CA num ans per hour
0.031812 CA avg asker rating 0.12437 CA num ques per hour
0.03001 CA ratio ans ques 0.09314 Q avg pos vote
0.029858 CA num ans per hour 0.08572 CA ratio ans ques
Table 4: Top 10 features by information gain for ASP
(trained for all askers) and ASP Group (trained for the
group of askers with 20 to 29 questions)
4 Conclusions
We have presented preliminary results on personal-
izing satisfaction prediction, demonstrating signif-
icant accuracy improvements over a ?one-size-fits-
all? satisfaction prediction model. In the future we
plan to explore the personalization more deeply fol-
lowing the rich work in recommender systems and
collaborative filtering, with the key difference that
the asker satisfaction, and each question, are unique
(instead of shared items such as movies). In sum-
mary, our work opens a promising direction towards
modeling personalized user intent, expectations, and
satisfaction.
References
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media with an application to community-based
question answering. In Proceedings of WSDM.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with non-
textual features. In Proceedings of SIGIR.
Mei Kobayashi and Koichi Takeda. 2000. Information
retrieval on the web. ACM Computing Surveys, 32(2).
Y. Liu, J. Bian, and E. Agichtein. 2008. Predicting in-
formation seeker satisfaction in community question
answering. In Proceedings of SIGIR.
R. Soricut and E. Brill. 2004. Automatic question an-
swering: Beyond the factoid. In HLT-NAACL.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
man, 2nd edition.
100
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1011?1021,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The Answer is at your Fingertips: Improving Passage Retrieval for Web
Question Answering with Search Behavior Data
Mikhail Ageev?
Moscow State University
mageev@yandex.ru
Dmitry Lagun
Emory University
dlagun@emory.edu
Eugene Agichtein
Emory University
eugene@mathcs.emory.edu
Abstract
Passage retrieval is a crucial first step of au-
tomatic Question Answering (QA). While ex-
isting passage retrieval algorithms are effec-
tive at selecting document passages most sim-
ilar to the question, or those that contain
the expected answer types, they do not take
into account which parts of the document the
searchers actually found useful. We propose,
to the best of our knowledge, the first success-
ful attempt to incorporate searcher examina-
tion data into passage retrieval for question an-
swering. Specifically, we exploit detailed ex-
amination data, such as mouse cursor move-
ments and scrolling, to infer the parts of the
document the searcher found interesting, and
then incorporate this signal into passage re-
trieval for QA. Our extensive experiments and
analysis demonstrate that our method signif-
icantly improves passage retrieval, compared
to using textual features alone. As an addi-
tional contribution, we make available to the
research community the code and the search
behavior data used in this study, with the hope
of encouraging further research in this area.
1 Introduction
Automated Question Answering (QA), is an attrac-
tive variation of search where the QA system auto-
matically returns an answer to a user?s question, in-
stead of a list of document results. Passage retrieval
is a first critical step of QA system, where candi-
date passages are identified and scored as likely to
contain an answer. While significant progress has
been made recently on incorporating syntactic and
semantic analysis for improving the QA system per-
formance, this analysis is typically applied only on
the (limited) set of candidate passages retrieved. The
main reason is that it is generally not practical to
perform deep analysis on all documents in a large
collection, and not yet feasible for the Web at large.
?Work done at Emory University.
In the web search setting, automated question
answering presents additional challenges and op-
portunities. On the downside, the questions and
queries from real users are often not grammatical
or well-formed, differing from the questions used
in the traditional TREC Question Answering evalua-
tions (Kelly and Lin, 2007; Sun et al, 2005). On the
upside, by interacting with a search engine, the mil-
lions of searchers implicitly provide additional clues
about usefulness of documents, result ranking, and
other aspects of the search process. In this paper,
we explore making use of the search behavior data
to improve passage retrieval for automated Question
Answering on the web.
Our basic observation is that when a user is at-
tempting to answer a question, he or she will more
carefully examine the parts of the document that
contain an answer. This observation is intuitive,
and is strongly supported by numerous eye track-
ing studies (e.g., Buscher et al (2008) and Buscher
et al (2009a)). Based on this, we hypothesize that
the passages containing the answers can be automat-
ically identified from the naturalistic searcher behav-
ior, and this prediction can be subsequently used to
improve passage ranking. To the best of our knowl-
edge, our work is the first to successfully incorporate
searcher examination into passage ranking for Ques-
tion Answering.
Our approach is primarily aimed at recurring (re-
peated) questions, which comprise a large fraction
of the search volume (while the exact statistics vary,
over 50% of search queries are submitted by multi-
ple users). For such questions, a system would track
the clicked result URLs, as well as the user interac-
tions on the landing pages. Then the system would
use this information to present the improved results
to new users who ask the same (or similar) ques-
tion. Intuitively, our method uses the same general
idea of result click data mining, used by the major
search engines to improve result ranking, but takes
1011
it a step further to exploit user interactions on the ac-
tual landing pages. A key point to emphasize is that
our approach exploits the natural browsing behavior
of the users, not requiring any additional effort from
the searchers.
Specifically, our contributions include:
? A novel approach to passage retrieval for ques-
tion answering, that naturally integrates textual
and behavioral evidence.
? A robust infrastructure for connecting fine-
grained searcher behavior to precise page con-
tents.
? Thorough experiments over hundreds of search
sessions and thousands of page views, demon-
strating significant improvements to passage
retrieval by harnessing the user?s page exami-
nation data.
Next we describe related work, to place our contri-
bution in context.
2 Related Work
Our work brings together two areas of research: pas-
sage retrieval for question answering, and mining
searcher behavior data.
Passage retrieval has long been recognized as
the first crucial step of automatic question answer-
ing. In some cases, passage retrieval can even serve
as the final product of a Question Answering sys-
tem (Clarke et al, 2000). As another example, re-
dundancy in the retrieved passages has been used by
the AskMSR system (Brill et al, 2002) to select an-
swers. Tellex et al (2003) report a thorough com-
parison of passage retrieval methods for QA, up to
2003. Additional improvements have been achieved
by using deeper analysis of the text. For exam-
ple, Cui et al (2005) exploited dependency relations
between the question terms, Aktolga et al (2011)
incorporated syntactic structure and answer typing,
while Harabagiu et al (2005) used semantic analy-
sis at all stages of the question answering process. In
this paper, we pursue a complementary direction, by
exploiting searcher examination behavior, with the
assumption that human searchers can easily zoom in
on relevant passages as part of normal searching.
It has been previously recognized that searcher in-
teractions could be valuable for question answering,
and a task on Complex Interactive QA has been ran
as part of TREC 2007 (Kelly and Lin, 2007). Our
work goes much further by considering not only ex-
plicit interactions, but also the searcher examination
behavior (i.e., detailed information on which text
passages were examined) ? which, as we show, pro-
vides additional valuable information for passage re-
trieval. Furthermore, it has been recognized that the
questions used in traditional TREC QA evaluation
may not be reflective of the ?real? questions, posed
by users (Bernardi and Kirschner, 2010). Our paper
uses a subset of the real questions posted by users on
Community Question Answering (CQA) sites, and
searches and interactions from real users ? which
makes our task unique and more challenging than
the previous settings.
In particular, our work builds on the rich his-
tory of using eye tracking technology to identify
areas of interest and attention, and to study read-
ing behavior. In the context of web search docu-
ment examination, Buscher et al (2008) extracted
sub-documents by tracking eye movements as im-
plicit feedback and expanded search queries to im-
prove the search result ranking. Buscher et al also
studied the prediction of salient Web page regions
using eye-tracking (Buscher et al, 2009a). This
work, and others, have shown that user attention can
help identify regions of documents of particular rel-
evance or usefulness for the query. While eye track-
ing equipment limits the applicability of these find-
ings to lab studies, these studies served as inspira-
tion to our work to detect the inferred areas of in-
terest. Specifically, we use mouse cursor tracking
as a natural proxy for user?s attention, to replace the
requirement for eye tracking equipment. As origi-
nally reported by Rodden et al (2008), the authors
discovered the coordination between a user?s eye
movements and mouse movements when scanning
a web search results page. This work was further
extended by Huang et al (2012) to predict the gaze
position from mouse cursor movement, with mean
error of about 150 px. In summary, there is mount-
ing evidence that the user?s attention in web search
can be approximated using mouse cursor, scrolling,
and other interaction data. In particular, Hijikata
(2004) proposed a method to extract text passages
of Web pages based on the user?s mouse activity and
found that extracted passages based on mouse ac-
1012
tivity such as text tracing, link pointing, link clicking
and text selection enable more accurate extraction of
key words of interest than using the whole text of the
page. Recently, White and Buscher (2012) proposed
a method that uses text selections as implicit feed-
back for document ranking. Most closely related to
this work is a contemporaneous effort on improv-
ing web search result summaries, or snippets, by
exploiting searcher behavior on the examined doc-
uments, described by Ageev et al (2013). How-
ever, to the best of our knowledge, there has been
no prior work on modeling searcher interaction on
result documents to improve Question Answering
performance, and in particular the passage retrieval
step.
3 Problem Statement and Approach
This section first states the problem we are address-
ing more precisely. Then, we describe the key parts
of our approach (Section 3.2), and the required in-
frastructure we had to develop to accomplish the re-
quired data collection (Section 3.4).
3.1 Problem Statement
Our goal is to incorporate the searcher behavior (in
particular, page examination) into passage retrieval.
That is, by analyzing the searcher behavior data, we
aim to identify the parts of the page that contain rel-
evant passages for answering a question. Specifi-
cally, given a question, a set of queries generated
by searchers attempting to answer this question, and
a set of documents retrieved by a search engine for
each of the queries, our goal to retrieve a set of pas-
sages that contain correct answers for the question.
That is, our goal is to identify, from searcher be-
havior, the passages in the documents most likely to
contain correct answers to a question, which could
then be incorporated into a fully automated question
answering system, or returned to the user directly,
for example, by incorporating these passages into
the result abstracts or ?snippets?.
3.2 Approach
Our approach accomplishes the goal above by in-
corporating both textual and behavioral evidence.
Specifically, we combine together traditional text-
based passage retrieval features, and the inferred
user interest in specific parts of a document based
on searcher behavior.
First, a passage score is obtained from the QA-
SYS system (Ng and Kan, 2010), resulting in a
strong text-only baseline that generates candidate
passages. Separately, examination behavior data is
collected over the landing pages, using our logging
infrastructure described in the next section. Then, a
behavior model is trained to identify the passages
of interest to the user, based on user examination
data (Section 4.2). Finally, the behavior-based pre-
diction of interest in each candidate passage is com-
bined with the original (text-based) passage score,
in order to generate the final behavior-biased pas-
sage ranking (Section 4.3). Note that by decoupling
the behavior modeling from the candidate genera-
tion method, our approach can be used with any
other passage retrieval approach that provides scores
for the candidate passages (that could be combined
with the behavior scores for the final ranking step).
While general and flexible, our approach makes
two key assumptions, resulting in potential limi-
tations. First, our approach is primarily targeted
(and evaluated for) informational questions ? that
is, questions for which the user expects to find an
answer in the text of the page. For other question
classes (e.g., opinion), passage retrieval might have
to be optimized differently. We also assume that
the user interactions on landing pages can be col-
lected by a search engine or a third party. This is
not far-fetched: already, browser plug-ins and tool-
bars collect some form of user interactions on web
pages, major organizations can (and sometimes do)
use proxies, and common page widgets like banner
ads and visit counters commonly inject JavaScript to
monitor basic user interactions ? and can be easily
extended to collect the examination data described
in this paper. The privacy and security of these meth-
ods are beyond the scope of this paper, we merely
point out that these behavior gathering tools, as-
sumed by our approach, already exist and are al-
ready widely deployed. The interested reader can
obtain an overview of the relevant privacy issues
and proposed solutions in references (Mayer and
Mitchell, 2012; Krishnamurthy and Wills, 2009).
3.3 Acquiring Search Behavior Data
Our infrastructure for acquiring search behavior was
developed with two goals in mind: (1) to obtain be-
havior data similar to real-world search, with the
ability to track fine-grained search behavior such as
1013
a mouse cursor movement (as there are no publicly
available data of this kind); (2) to create a controlled
and clean ground truth set, to train our system and
evaluate the effectiveness of our approach.
To collect sufficient amount of search behavior
data, we adapted for our task the publicly available
UFindIt architecture, described in reference Ageev
et al (2011). The participants played several search
contests, or ?games?, each consisting of 12 search
tasks (questions) to solve. The stated goal of the
game was to submit the highest possible number of
correct answers within the allotted time. After the
searcher decided that they found the answer, they
were instructed to type the answer together with the
supporting URL into the corresponding fields in the
game interface. Each search session (for one ques-
tion) was completed by either submitting an answer
or clicking the ?skip question? button to pass to the
next question.
Participants were recruited through the Amazon
Mechanical Turk (MTurk) service. As a first step,
the workers had to solve a ReCaptcha puzzle to
verify that they are human and not an automated
?bot?. A browser verification check was performed
to confirm that the browser was compatible with our
JavaScript tracking code. During the data postpro-
cessing stage, we filtered out the users who did not
answer even the easy, trivial questions, as it indi-
cated either poor understanding of the game rules,
or an attempt to make a quick buck without effort.
In order to capture all of the participants? search
actions, they were instructed to use only our search
interface (and not a separate browser window). The
search interface performed the web searches using
the public API of a popular web search engine, and
showed result pages to the users using the original
page design, layout and stylesheets, so the user?s
search experience is not affected.
3.4 Page Examination Behavior Logging
A key part of our system is a mechanism for collect-
ing searcher interactions on web pages, and tying
them precisely to the page content at the word level.
As the HTML page passed through the proxy, a
JavaScript code is embedded to track the user?s inter-
actions, including mouse movements and scrolling,
as well as the properties of the visited page. The be-
havioral (interaction) events are logged by the search
interface proxy and written to the server log.
To connect the tracked mouse cursor positions
to exact text passages we employed the following
trick. After the HTML page is rendered in the
browser window, our JavaScript code modifies the
page DOM tree so that each word is wrapped by a
separate DOM Element. Then for each DOM El-
ement, the window coordinates of that element are
evaluated and saved in an Element?s attribute. The
processed HTML page is then saved to the server by
an asynchronous request. The saved coordinates are
updated if the page layout is changed due to resize
window event or AJAX action.
As a result of this instrumentation, for each page
visit we know the searcher?s intent (question), a
search engine query that the user issued, a URL
and HTML page, the bounding boxes of each word
in the HTML text, and all of the searcher actions,
e.g., mouse movement coordinates, mouse clicks,
and scrolling.
4 Behavior-Biased Passage Retrieval
We now present the details of our behavior-biased
passage retrieval algorithm (BePR). First, we de-
scribe the text-only retrieval system. Then, we in-
troduce our method for inferring the most interesting
or useful parts of the document from user behavior
(Section 4.2).
4.1 Text-Based Passage Retrieval
We adopt an open-source question answering frame-
work QANUS (Ng and Kan, 2010) (version
v29Nov2012). The QANUS distribution contains
the fully functional factoid QA system QA-SYS that
we use as a baseline for our experiments. QA-
SYS implements many of the state-of-the-art ques-
tion answering techniques, and is similar to a top-
performing QA system from TREC (Sun et al,
2005). The QA-SYS distribution is configured for
processing documents and questions in TREC QA
format, and we adopted QA-SYS for answer extrac-
tion from web documents. QA-SYS takes a set of
documents and a question as an input, and processes
the input in three stages: (1) information source
preparation, (2) question processing, and (3) answer
retrieval.
In the first stage, the downloaded HTML pages
are pre-processed with Natural Language Tool Kit
(NLTK, Bird (2006)). Extracted text is divided into
sentences using Punkt unsupervised sentence split-
1014
ter (Kiss and Strunk, 2006). The QA-SYS performs
Part of Speech tagging using Stanford POS tagger
(Toutanova et al, 2003), and Named Entity Recog-
nition using Stanford NER (Finkel et al, 2005), and
then builds a Lucene index over the set of input
documents. In the second stage the QA-SYS per-
forms POS tagging, NE recognition, and question
type classification for an input question.
To answer a question, QA-SYS creates a query
from the question, performs the search over the in-
dexed text collection, and retrieves top 50 docu-
ments. Each document is split by sentences, and
for each sentence a QA-SYS Passage Retrieval Score
(TextScore) is computed as a linear combination
of term frequency score, proximity score, and term
coverage score. After that 40 passages with the high-
est TextScore are retrieved, for each passage QA-
SYS performs pattern based answer extraction based
on the identified expected answer type of the ques-
tion.
As the focus of this paper is to improve Passage
Retrieval performance, we use the TextScore sen-
tence ranking as a baseline, and improve on it by
adding the new search behavioral features indicating
the passage relevance, as described next.
4.2 Inferring Relevant Passages from Search
Behavior
To rank passages by their ?interestingness? ? that is,
to identify the passages that have been carefully ex-
amined by the searcher, we use a learning-to-rank
approach, and apply regression algorithms to predict
the probability that a specific passage is interesting
for a user. A passage is labeled as ?interesting?, if
the user submitted an answer in the current session,
and both the passage and the answer have at least
one common word, after stemming and stop-word
removal.
For each passage, a set of behavior features that
could represent passage interestingness is created.
To associate behavioral features with a given doc-
ument passage, we match the sequence of behav-
ior events and the set of bounding boxes for each
word and DOM Element of a page. For efficiency,
we build a spatial R-Tree index of these bounding
boxes, which allows us to quickly find the matching
DOM Elements for each event.
One key feature is the duration of the time in-
terval when a mouse cursor was hovering over the
Feature Description
MouseOverTime Time duration when the mouse
cursor was over the text passage
MouseNearTime Time duration when the mouse
cursor was close to the text
passage in the window
(x? 100px, y ? 70px)
MouseOverEvents The number of mouse events
during MouseOverTime
MouseNearEvents The number of mouse events
during MouseNearTime
DispTime Time duration when the text
passage has been visible in
the browser window
(depends on scrollbar position)
DispMiddleTime Time duration when the text
passage was visible in the middle
part of the browser window
Table 1: Behavior features for text passages
specific text passage, or very close to the passage.
We also take a scrollbar and event count features
from papers (Buscher et al, 2009b), and (Guo and
Agichtein, 2012) to detect evidence of ?reading? vs.
?skimming? behavior, and adopt those features to
represent the behavior near the specific location of
a page. The full set of our passage behavior features
are reported in Table 1.
To implement the passage ranker, we experi-
mented with a variety of learning-to-rank (LTR) al-
gorithms, and chose two implementations of Regres-
sion Trees, due to their strong performance for gen-
eral web search ranking tasks. The first algorithm
is Regression Tree (Friedman et al, 2001), and the
second is Gradient Boosting Regression Tree algo-
rithm (Friedman, 2001). They are named BePR-
BTree, and BePR-GBM respectively.
The dataset consists of a set of questions, with as-
sociated search behavior data collected from all the
users who tried to find an answer to this question,
the answers submitted by the users, and a set of val-
idated answers. These sets are divided into train-
ing, validation, and test, so that the training and val-
idation set URLs are disjoint, and the test set have
no intersection with training and validation set by
URLs, questions, and users. The training set is cre-
ated from only those page visits where the document
text has non-empty intersection with the user?s an-
swer, and the answer is correct. The trained regres-
1015
sion algorithm is applied to all page visits in the test
set. When the trained model is applied at test time,
it has no information about the user?s intent, the cor-
rect answer, or the current query, but rather uses only
the behavioral features of the current page visit to
identify the ?interesting? passages.
The predicted probability of passage interesting-
ness is averaged over all the users and page visits,
and the resulting passage interestingness is then used
as the BScore of the passage. Note that BScore
is defined for only visited pages; to incorporate the
overall clickthrough information (i.e., the fraction of
the time a page was visited, indicating relevance),
we introduce a generalized version, designated as
BSscoreAll, defined as: ? ?CTR+(1??)?BScore,
where CTR is the clickthrough rate for the page,
defined as the fraction of time the result was clicked
for all searches. Intuitively, this version reduces the
weight of the behavior score for the pages with in-
sufficient behavior data by ?backing off? to the doc-
ument clickthrough rate, according to the parame-
ter ?. For the cases where only the visited pages
are considered (ignoring the searches when the page
was not visited), ? is set to 0, reverting the score
to the original BScore definition. The resulting
behavior-based passage score is then used as the ag-
gregate value of searcher interest in the passage for
the combined passage retrieval step, described next.
4.3 Combining Textual and Behavioral
Evidence
The final step in our approach is to combine the
text-based score TextScore(f) for a sentence (Sec-
tion 4.1) with the interestingness score BScore(f)
(Section 4.2), inferred from the examination data. In
our current implementation we combine these scores
by linear combination:
FScore(f) =? ?BScore(f)
+ (1? ?) ? TextScore(f)
Other more sophisticated ways to combine text
and behavior evidence are possible, such as jointly
learning over both text and behavior features. How-
ever, we chose to follow the simpler linear approach
for interpretability of the results (e.g., by varying the
? parameter).
5 Data Collection and Experimental Setup
This section presents the methodology used for se-
lecting the questions (Section 5.1), the correspond-
ing search behavior data (Section 5.2), and the ex-
perimental collections and metrics (Section 5.3).
5.1 Questions
The search tasks were selected from community
question answering sites such as wiki.answers.com
and Yahoo! Answers by the researchers. The cri-
teria used were that the question should be clearly
stated, had a clear answer, and that finding this an-
swer was not a trivial task, that is, the answer was
not retrieved simply by submitting the question ver-
batim to Google, Bing, or Yahoo! Search engines.
Overall, 36 such questions were selected, posing (as
it turned out) greatly varying levels of difficulty for
participants. These questions were randomly split
into three game rounds of 12 questions each.
5.2 Browsing Behavior Dataset
The search behavior data for each of the questions
above was acquired as described in Section 3.3. A
total of 270 participants finished the game. Af-
ter filtering out users who did not follow the game
rules, we have 3047 search sessions performed by
265 users. Our data for these users consists of 7800
queries, 3910 unique queries, 8574 SERP clicks on
1544 distinct URLs. For 5683 page visits (66%)
and 883 distinct URLs the on-page behavioral data
is collected. For the rest 34% of page visits the be-
havioral data were not collected due to conflicts be-
tween our JavaScript tracking code and other code
presented on the page. For each page view there
are about 400 atomic browsing events (mouse move-
ments, scrolling, key pressing) on average. All the
source and derived data are available at http://
ir.mathcs.emory.edu/intent.
The dataset is divided into training, validation,
and test set in the following way. The behavior
dataset for the first game is divided randomly into
equal-sized training and validation sets that are dis-
joint by URLs. The training set was used to train
the regression algorithm for predicting passage at-
tractiveness, and the validation set was used to ex-
plore the influence of behavior weight ? on passage
retrieval performance, and to select the parameter ?
for using on a test set. The validation set consists
of 254 different URLs spread over 11 questions, and
1016
for each of them there is a collected browsing be-
havior.
The test set consists of 441 URLs spread over 24
questions, and the test set has no intersection with
training and validation set by URLs, questions, and
users.
5.3 Candidate Document Selection Strategies
The first step for question answering is a selection
of a candidate document set. In our settings, we
may select a subset of web documents in a differ-
ent way. We explore passage retrieval effectiveness
using three different strategies of document set se-
lection.
? For each question select All documents that
are in top 10 documents returned by a search
engine for any query that was issued during
search for the specific question. For our dataset
this gives around 500 candidate documents per
question on average.
? For each question select only documents that
were Clicked by a user. This restricts a can-
didate document set to set of most promising
documents. For our dataset this gives around
25 candidate documents per question on aver-
age.
? For each pair of question and Relevant docu-
ment apply passage retrieval to the specific doc-
ument. In this experiment we label a document
?Relevant? if a correct answer was extracted
from it. In a real-world scenario, while doc-
ument relevance could be estimated by a va-
riety of click-based methods, we address the
challenge of how to actually extract the cor-
rect answer from the document, automatically,
with the help of the natural behavior data. We
perform this experiment to estimate the perfor-
mance of passage retrieval for the case when
relevant documents are known with high confi-
dence.
Evaluation Metrics: We evaluate passage retrieval
performance by standard Mean Reciprocal Rank
(MRR), and Mean Average Precision (MAP) met-
rics for top 20 retrieved sentences (Voorhees and
Tice, 1999). We also evaluate ROUGE-1 met-
ric (Lin, 2004) for the first retrieved passage.
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0 0.1 0.2 0.3 0.4 0.5
ROUG
E
fragment score
user's answer ROUGE-1user's answer ROUGE-2all correct answers ROUGE-1all correct answers ROUGE-2
Figure 1: The actual passage interestingness, measured
by intersection with user?s answer, vs. the passage rele-
vance score BScore predicted from behavior data
6 Results
We now present the empirical results. First, we re-
port the intermediate result of using behavior data to
infer the interesting (useful) passages in the docu-
ment. Then, we report the main results of the paper
where the quality of the generated snippets with and
without using behavior data is compared using hu-
man judgments.
6.1 Prediction of Passage Interestingness
This experiment evaluates how well we can predict
interesting passages by observing a user?s on-page
behavior. We suppose that the passage is interesting
if it is related to the answer for the question. For
each visited page, we collect the user?s answer (if
submitted), and all correct answers from all users
who answered this question. Then, we compare
those answers to each text passage in the document
using ROUGE metrics (Lin, 2004).
Figure 1 shows the relationship between the in-
terestingness of a passage and behavior score. The
graph shows that when the score is high (? 0.5),
then average intersection between the passage and
user?s answer is much higher than those when the
passage score is low. All ROUGE-N metrics sig-
nificantly grow when the behavior score grows, al-
though ROUGE-2 over all correct answers are al-
ways very small (it grows from 0.003 to 0.007).
ROUGE-1 is much greater than ROUGE-2 for high
scores, as the interesting passage might contain use-
ful information for the answer, but the user reformu-
lates the obtained information and submits reformu-
lated answer. The ROUGE-N metrics for a user?s
answer are much greater than those for all correct
1017
Feature Feature Importance
DispMiddleTime 0.51
MouseOverTime 0.34
DispTime 0.12
MouseNearTime 0.02
MouseOverEvents 0.01
MouseNearEvents 0.01
Table 2: Feature importance for behavioral features, as
measured by Gini coefficient
answers, as other users might obtain valuable infor-
mation from other documents, and some questions
have distinct correct answers.
Behavior Feature Importance Analysis: To esti-
mate relative importance of behavior features we
evaluated the Gini importance index (Breiman,
1996) for each behavior feature from the Table 1.
The Table 2 shows that the most important features
are the time duration when the text passage was vis-
ible in the middle part of the scrolling window, and
the time duration when the mouse cursor was over
the text passage. The first feature has been shown
to be a good feature for re-ranking search results
in reference (Buscher et al, 2009b), and we have
shown that it is also useful for passage retrieval. The
MouseOverTime feature has been previously shown
to be correlated with examination time, measured
by eye-tracking experiments (Guo and Agichtein,
2010), and it helps us detect local behavior in the
neighborhood of a specific text passage.
Analysis of Searcher Attention: In order to better
understand what characteristics of the textual pas-
sages attract the searcher?s attention, we explored
21 linguistic features for each sentence. Our fea-
tures were designed to estimate text readability, and
the overlap of a passage with the query that was
used to find the document. We implemented the
readability features from (Kanungo and Orr, 2009),
and query matching features from (Metzler and Ka-
nungo, 2008). Table 3 reports the top 10 features
with the highest absolute value of the correlation co-
efficient with passage interestingness scoreBScore.
Interestingly, the most highly correlated features are
related to readability, while query matching features
are less important.
Feature description corr
Number of distinct words in the passage 0.31
Total number of words in the passage 0.28
Number of letter ([a-zA-z]) characters 0.27
Relative location of the passage in the document -0.25
Number of unique words in the passage -0.24
divided by total number of words
Number of punctuation characters -0.20
Number of words with first letter capitalized -0.17
Overlap of query terms expanded 0.15
with synonyms and the passage
Absolute count of query terms 0.15
matched in the passage
Average position of query term within the passage -0.14
Table 3: Correlation of passage interestingness BScore
with linguistic properties of a sentence
Figure 2: MRR for passage retrieval for varying behav-
ior weight ? and interestingness prediction algorithms
BePR-DTree, and BePR-GBM
6.2 Passage Retrieval with Behavior Data
This section reports the main results of the paper.
First, we describe the parameter tuning, followed by
the main performance results.
Parameter Tuning: To tune the passage retrieval
performance, we use the validation set to find the
optimal value for ?. Figure 2 reports the passage
retrieval MRR for varying ?, for two learning al-
gorithms BePR-GBM and BePR-DTree. The figure
shows that both BePR-GBM and BePR-BTree im-
prove over the QA-SYS baseline. BePR-GBM algo-
rithm achieves the best performance with ? = 0.8,
and also exhibits more robust behavior compared to
BePR-BTree, so we use BePR-GBM with ? = 0.8
for the main experiments described next. Similarly,
using the training and validation sets, we optimized
1018






	
  	





QA-SYS
BePR







	
  	



	




QA-SYS
BePR







	
  	



QA-SYS
BePR
Figure 3: Passage retrieval MRR (a), ROUGE1 (b), and MAP (c) for the BePR and QA-SYS systems, on the test set.
the value of the clickthrough rate weight ? = 0.05
(used for the BScoreAll score) for the All document
set only (as for the Clicked and Relevant document
sets, ? is always set to 0 by construction).
Main retrieval results: We now compare the base-
line algorithm for passage retrieval implemented in
QA-SYS system and described in section 4.1 with
the BePR algorithm (section 4.2-4.3) that combines
the textual passage score and the behavior score us-
ing the ? parameter for the relative weight of the
behavior evidence.
Figure 3 reports the main results of the paper,
namely the MRR, ROUGE-1@1 and MAP pas-
sage retrieval metrics for the baseline QA-SYS al-
gorithm, and BePR-GBM, on the test set. As the
figure shows, BePR achieves higher performance
on all metrics, and for all document sets. The im-
provements are statistically significant (p < 0.01)
for experiments with Clicked and Relevant docu-
ment sets. Not surprisingly, the improvements are
smallest when All documents are considered, as un-
clicked documents do not provide any associated be-
havior data. As the results show, our simple back-off
strategy (using the document clickthrough rate with
the ? parameter) is moderately successful, but could
be further refined in the future.
Finally, we illustrate how behavior features affect
passage ranking. Let?s consider a question ?How
many Swedes speak English as a percentage??. The
perfect relevant page for this question is a Wikipedia
page ?Languages of Sweden?. A sentence ?Main
foreign language(s): English 89%, German 30%,
French 11%.? contains an answer to the question, but
it has only a small intersection with question terms,
and QA-SYS ranks this question in the 13th place.
Other sentences that contain a country name, a num-
ber, or have more terms that match the question are
ranked higher. In contrast, as searchers examined
this sentence carefully to find the answer, BePR is
able to promote this sentence to the second place in
the ranking.
7 Resources and Data
All the code and the collected data used in this
research are available at http://ir.mathcs.
emory.edu/intent/. The dataset contains the
set of questions used for the experiments, and user?s
behavior: queries submitted by users to search
engine, result pages, visited URLs, downloaded
landing pages, on-page browsing behavior (mouse
movements, scrollbar events, resize actions, clicks).
By sharing our code and data, we hope to encourage
further research in this area.
8 Conclusions and Future Work
We presented the first successful approach to incor-
porating naturalistic searcher behavior data into pas-
sage retrieval for question answering. Specifically,
we developed a robust method to infer searcher in-
terest in specific parts of the document, which could
then be combined with more traditional textual fea-
tures used for passage retrieval. Our results show
significant improvements over a strong baseline, de-
rived from a competitive Question Answering sys-
tem.
To implement the proposed method in a real-
world search engine for Web QA, the proposed in-
frastructure and/or the released data could be used
as a training set for the algorithm that predicts frag-
ment interestingness from user behavior. Such a
system would need to track document examination
data. This can already be done by incorporating our
released tracking code or a similar method into a
1019
browser toolbar, banner ad system, visit counters or
other JavaScript widgets that already track user vis-
its. While we acknowledge user privacy as an im-
portant concern, it is beyond the scope of this work.
In the future, we plan to extend this work to more
precisely pinpoint the answer location on a page,
and consequently incorporate searcher behavior into
subsequent answer extraction and ranking stages of
question answering. We also plan to further investi-
gate the examination data to better understand how
searchers find correct (and incorrect) answers using
both general web search engines and QA systems ?
in order to inform and further improve query sug-
gestion, result snippet generation, and result ranking
algorithms.
Acknowledgments
This work was supported by the National Science
Foundation grant IIS-1018321, the DARPA grant
D11AP00269, the Yahoo! Faculty Research En-
gagement Program, and by the Russian Foundation
for Basic Research Grant 12-07-31225.
References
Mikhail Ageev, Qi Guo, Dmitry Lagun, and Eugene
Agichtein. 2011. Find it if you can: a game for mod-
eling different types of web search success using inter-
action data. In Proceedings of the 34th international
ACM SIGIR conference on Research and development
in Information Retrieval, SIGIR ?11, pages 345?354,
New York, NY, USA. ACM.
Mikhail Ageev, Dmitry Lagun, and Eugene Agichtein.
2013. Improving search result summaries by using
searcher behavior data. In Proceedings of the 36th in-
ternational ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?13.
Elif Aktolga, James Allan, and David A. Smith. 2011.
Passage reranking for question answering using syn-
tactic structures and answer types. In Proceedings
of the 33rd European conference on Advances in in-
formation retrieval, ECIR?11, pages 617?628, Berlin,
Heidelberg. Springer-Verlag.
Raffaella Bernardi and Manuel Kirschner. 2010. From
artificial questions to real user interaction logs: Real
challenges for interactive question answering systems.
In Proceedings of Workshop on Web Logs and Ques-
tion Answering, pages 8?15.
Steven Bird. 2006. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, COLING-ACL ?06, pages 69?72,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Leo Breiman. 1996. Bagging predictors. Mach. Learn.,
24(2):123?140.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the askmsr question-answering system. In
Proc. of ACL, EMNLP ?02, pages 257?264, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Georg Buscher, Andreas Dengel, and Ludger van Elst.
2008. Query expansion using gaze-based feedback on
the subdocument level. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?08, pages 387?394, New York, NY, USA. ACM.
Georg Buscher, Edward Cutrell, and Meredith Ringel
Morris. 2009a. What do you see when you?re surf-
ing?: using eye tracking to predict salient regions of
web pages. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ?09,
pages 21?30. ACM.
Georg Buscher, Ludger van Elst, and Andreas Dengel.
2009b. Segment-level display time as implicit feed-
back: a comparison to eye tracking. In Proceedings of
the 32nd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?09, pages 67?74, New York, NY, USA. ACM.
Charles Clarke, Gordon Cormack, Derek Kisman, and
Thomas Lynam. 2000. Question answering by pas-
sage selection (multitext experiments for trec-9). In
Proceedings of the Ninth Text REtrieval Conference
(TREC-9).
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In Proceedings
of the 28th annual international ACM SIGIR confer-
ence on Research and development in information re-
trieval, SIGIR ?05, pages 400?407, New York, NY,
USA. ACM.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
2001. The elements of statistical learning, volume 1.
Springer Series in Statistics.
Jerome H. Friedman. 2001. Greedy function approxi-
mation: A gradient boosting machine. The Annals of
Statistics, 29(5):pp. 1189?1232.
Qi Guo and Eugene Agichtein. 2010. Towards predicting
web searcher gaze position from mouse movements.
In CHI ?10 Extended Abstracts on Human Factors in
Computing Systems, CHI EA ?10, pages 3601?3606,
New York, NY, USA. ACM.
1020
Qi Guo and Eugene Agichtein. 2012. Beyond dwell
time: estimating document relevance from cursor
movements and other post-click searcher behavior. In
Proceedings of the 21st international conference on
World Wide Web, WWW ?12, pages 569?578, New
York, NY, USA. ACM.
Sanda Harabagiu, Dan Moldovan, Christine Clark,
Mitchell Bowden, Andrew Hickl, and Patrick Wang.
2005. Employing two question answering systems in
trec-2005. In Proceedings of the fourteenth text re-
trieval conference.
Yoshinori Hijikata. 2004. Implicit user profiling for on
demand relevance feedback. In Proceedings of the 9th
international conference on Intelligent user interfaces,
IUI ?04, pages 198?205, New York, NY, USA. ACM.
Jeff Huang, Ryen White, and Georg Buscher. 2012. User
see, user point: gaze and cursor alignment in web
search. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ?12, pages
1341?1350, New York, NY, USA. ACM.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proceedings
of the Second ACM International Conference on Web
Search and Data Mining, WSDM ?09, pages 202?211,
New York, NY, USA. ACM.
Diane Kelly and Jimmy Lin. 2007. Overview of the trec
2006 ciqa task. In ACM SIGIR Forum, volume 41,
pages 107?116. ACM.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multilin-
gual sentence boundary detection. Comput. Linguist.,
32(4):485?525, December.
Balachander Krishnamurthy and Craig Wills. 2009. Pri-
vacy diffusion on the web: a longitudinal perspective.
In Proceedings of the 18th international conference
on World wide web, WWW ?09, pages 541?550, New
York, NY, USA. ACM.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Jonathan R. Mayer and John C. Mitchell. 2012. Third-
party web tracking: Policy and technology. In Pro-
ceedings of the 2012 IEEE Symposium on Security
and Privacy, SP ?12, pages 413?427, Washington, DC,
USA. IEEE Computer Society.
D. Metzler and T. Kanungo. 2008. Machine learned sen-
tence selection strategies for query-biased summariza-
tion. In SIGIR Learning to Rank Workshop.
Jun-Ping Ng and Min-Yen Kan. 2010. Qanus:
An open-source question-answering platform
http://www.comp.nus.edu.sg/ junping/docs/qanus.pdf.
Kerry Rodden, Xin Fu, Anne Aula, and Ian Spiro. 2008.
Eye-mouse coordination patterns on web search re-
sults pages. In CHI ?08 Extended Abstracts on Human
Factors in Computing Systems, CHI EA ?08, pages
2997?3002, New York, NY, USA. ACM.
Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng
Chua, and Min-Yen Kan. 2005. Using syntactic and
semantic relation analysis in question answering. In
TREC.
Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative evalu-
ation of passage retrieval algorithms for question an-
swering. In Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in informaion retrieval, SIGIR ?03, pages 41?
47, New York, NY, USA. ACM.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ellen Voorhees and Dawn M Tice. 1999. The trec-8
question answering track evaluation. In Proceedings
of The Eighth Text REtrieval Conference (TREC-8),
http://trec. nist. gov/pubs/trec8/t8 proceedings. html.
Ryen W. White and Georg Buscher. 2012. Text se-
lections as implicit relevance feedback. In Proceed-
ings of the 35th international ACM SIGIR conference
on Research and development in information retrieval,
pages 1151?1152, New York, NY, USA. ACM.
1021
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 361?364,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Query Ambiguity Revisited: Clickthrough Measures for Distinguishing
Informational and Ambiguous Queries
Yu Wang
Math & Computer Science Department
Emory University
yu.wang@emory.edu
Eugene Agichtein
Math & Computer Science Department
Emory University
eugene@mathcs.emory.edu
Abstract
Understanding query ambiguity in web search
remains an important open problem. In this
paper we reexamine query ambiguity by ana-
lyzing the result clickthrough data. Previously
proposed clickthrough-based metrics of query
ambiguity tend to conflate informational and
ambiguous queries. To distinguish between
these query classes, we introduce novel met-
rics based on the entropy of the click distri-
butions of individual searchers. Our exper-
iments over a clickthrough log of commer-
cial search engine demonstrate the benefits of
our approach for distinguishing informational
from truly ambiguous queries.
1 Introduction
Since query interpretation is the first crucial step in
the operation of the web search engines, more re-
liable query intent classification, such as detecting
whether a query is ambiguous, could allow a search
engine to provide more diverse results, better query
suggestions, or otherwise improve user experience.
In this paper we re-examine query ambiguity
in connection with searcher clickthrough behavior.
That is, we posit that clickthrough information could
provide important evidence for classifying query
ambiguity. However, we find that previously pro-
posed clickthrough-based measures tend to conflate
informational and ambiguous queries. We propose a
novel clickthrough measure for query classification,
user click entropy, and show that it helps distinguish
between informational and truly ambiguous queries.
Previous research on this topic focused on binary
classification of query ambiguity. Notably, (Tee-
van et al, 2008) used click entropy as a proxy for
query ambiguity to estimate the potential for search
personalization. (Mei and Church, 2008) considered
click entropy as measure of search difficulty. More
broadly, clickthrough information has been used for
many other tasks such as improving search rank-
ing (Zhu and Mishne, 2009), learning semantic cat-
egories (Komachi et al, 2009), and for topical query
classification (Li et al, 2008). However, our work
sheds new light on distinguishing between informa-
tional and ambiguous queries, by using clickthrough
data. Our contributions include:
? More precise definition of query ambiguity in
terms of clickthrough behavior (Section 2).
? Entropy-based formalization of resulting click be-
haviors (Section 3).
? Empirical validation of our methods on a large
real query and clickthrough log (Section 4).
2 Defining Query Ambiguity
In this study we focus on two orthogonal query in-
tent dimensions, adapted from the top level of user
goal taxonomies such as (Rose and Levinson, 2004).
Specifically, a query could be ambiguous or unam-
biguous; as well as informational or navigational.
Consider the example queries of each type below:
Ambiguous Unambiguous
Informational ?al pacino? ?lyrics?
Navigational ?people? ?google?
The query ?al pacino?, the name of a famous ac-
tor, is a typical ambiguous and informational query.
In the clickthrough logs that we examined, the most
popular searcher destinations include sites with pic-
tures of Al Pacino, movie sites, and biography sites ?
corresponding to different informational intents. In
contrast, the query ?lyrics? has an unambiguous in-
formational intent, which is to explore websites with
song lyrics. For the ambiguous navigational query
?people?, popular destinations include people.com,
Yahoo People or People?s United Bank. Finally, the
361
query ?google? is unambiguous and navigational,
with over 94% of the clicks on the Google?s home-
page.
Definitions of query classes: we now more for-
mally define the query classes we consider:
? Clear: Unambiguous navigational query, such as
?google?.
? Informational: Unambiguous informational
query, such as ?lyrics?
? Ambiguous: Ambiguous informational or navi-
gational query, such as ?people? or ?al pacino?.
The key challenge in distinguishing the last two
classes, Informational and Ambiguous, is that the
overall clickthrough patterns for these classes are
similar: in both cases, there are clicks on many re-
sults, without a single dominant result for the query.
3 Clickthrough Measures for
Distinguishing Ambiguous and
Informational Queries
In this section we describe the features used to rep-
resent a query for intent classification, listed in Ta-
ble 1. In addition to popular features such as click-
through frequency and query length, we introduce
novel features related to user click entropy, to cap-
ture the distinction between informational and am-
biguous queries.
Overall Entropy: Previous methods for query classi-
fication utilize entropy of all result clicks for a query,
or overall entropy (the uncertainty associated with
obtaining a click on any specific result), defined as:
H(Rq) = ?
?
r?Rq
p(r) log p(r)
Rq is the set of results r, clicked by all users after
submitting the query q. For example, a clear query
?target? has the overall entropy of 0.36, and most
results corresponding to this query point to Target?s
company website. The click log data shows that
85% of the users click the Target website for this
query. In contrast, an unclear query ?lyrics? has the
overall entropy of 2.26. However, overall entropy
is insufficient for distinguishing between informa-
tional and ambiguous queries. To fill this gap, we
introduce new clickthrough metrics to detect such
ambiguous queries.
User Entropy: Recall, that both informational
queries and ambiguous queries could have high
1 2 30
5
10
15
(a) Overall Entropy
Fr
eq
ue
nc
y
Ambiguous
Informational
0.15 0.3 0.450
5
10
15
(b) User Entropy
Fr
eq
ue
nc
y InformationalAmbiguous
Figure 1: Frequency of ambiguous and informational
queries by Overall Entropy (a) and User Entropy (b).
overall entropy, making it difficult to distinguish
them. Thus, we introduce a new metric, user en-
tropy of a query q H(Uq), as the average entropy of
a distribution of clicks for each searcher:
H(Uq) =
?
?
u?Uq
?
r?Ru
p(r) log p(r)
|Uq|
where Uq is the set of users who have submitted the
query q, and Ru is the set of results r, clicked by
the user u. For the example informational query
?lyrics?, a single user may click many different
URLs, thereby increasing user entropy of this query
to 0.317. While for an ambiguous query, which has
multiple meanings, a user typically searches for only
one meaning of this query at a time, so the results
clicked by each user will concentrate on one topic.
For example, the query ?people? is ambiguous, and
has the overall entropy of 1.73 due to the variety
of URLs clicked. However, a particular user usu-
ally clicks only one of the websites, resulting in low
user entropy of 0.007. Figure 1 illustrates the dif-
ference in the distributions of informational and am-
biguous queries according to their overall and user
entropy values: more informational queries tend to
have medium to high User Entropy values, com-
pared to the truly ambiguous queries.
Domain Entropy: One problem with the above mea-
sures is that clickthrough data for individual URLs
is sparse. A common approach is to backoff to
the URLs domain, with the assumption that URLs
within the same domain usually relate to the same
topic or concept. Therefore, domain entropy H(Dq)
of a query may be more robust, and is defined as:
H(Dq) = ?
?
d?Dq
p(d) log p(d)
where Dq are the domains of all URL clicked for
q. For example, the query ?excite? is a navigational
and clear query, as all the different clicked URLs for
this query are within the same domain, excite.com.
362
Query Feature Description
QueryLength Number of tokens (words) in the query
ClickFrequency Number of total clicks for this query
OverallEntropy Entropy of all URLs for this query
UserEntropy* Average entropy of the URLs clicked by one user for this query
OverallDomainEntropy Entropy of all URL domains for this query
UserDomainEntropy* Average entropy of URL domains clicked by one user for this query
RelativeUserEntropy* Fraction of UserEntropy divided by OverallEntropy
RelativeOverallEntropy* Fraction of OverallEntropy divided by UserEntropy
RelativeUserDomainEntropy* Fraction of UserDomainEntropy divided by OverallDomainEntropy
RelativeOverallDomainEntropy* Fraction of OverallDomainEntropy divided by UserDomainEntropy
Table 1: Features used to represent a query (* indicates features derived from User Entropy).
While this query has high Overall and User Entropy
values, the Domain Entropy is low, as all the clicked
URLs for this query are within the same domain.
The features described here can then be used as
input to many available classifiers. In particular, we
use the Weka toolkit1, as described below.
4 Experimental Results
We describe the dataset and annotation process, and
then present and analyze the experimental results.
Dataset: We use an MSN Search query log
(from 2006 Microsoft data release) with 15 million
queries, from US users, sampled over one month.
Queries with click frequency under 10 are discarded.
As a result, 84,703 unique queries remained, which
form our universe of queries. To separately analyze
queries with different frequencies, we divide the
queries into three groups: low frequency group (10-
100 clicks), medium frequency group (100-1000
clicks) and high frequency group (over 1000 clicks).
From each group, we draw a random sample of 50
queries for manual labeling, for the total of 150
queries. Each query was labeled by three members
of our lab. The inter-annotator agreeement was 85%,
and Cohen?s Kappa value was 0.77.
Table 2 reports the distribution of query classes in
our dataset. Note that low frequency queries dom-
inate, but are equally represented in the data sam-
ples used for classification training and prediction
(we will separately analyze performance on differ-
ent query frequency groups).
Results: Table 3 shows that best classification re-
quired User Entropy features. The Weka classifiers
were Naive Bayes (NB), Logistic Regression (Lo-
gistic), and Support Vector Machines (SVM).
1http://www.cs.waikato.ac.nz/ml/weka/
Clear Informational Ambiguous Frequency (%)
High 76% 8% 16% 255 (0.3%)
Medium 52% 20% 28% 3802 (4.5%)
Low 32% 46% 22% 80646 (95.2%)
Table 2: Frequency distribution of different query types
All Clear Informational Ambiguous
Ac. Pre. Rec. Pre. Rec. Pre. Rec.
All features
NB 0.72 0.90 0.85 0.77 0.54 0.42 0.61
Logistic 0.77 0.84 0.98 0.68 0.73 0.59 0.30
SVM 0.76 0.79 1.00 0.69 0.78 0.71 0.15
Without user entropy
NB 0.73 0.85 0.95 0.63 0.73 0.39 0.21
Logistic 0.73 0.84 0.95 0.63 0.68 0.47 0.27
SVM 0.74 0.79 1.00 0.65 0.76 0.50 0.09
Table 3: Classification performance by query type
High Mid Low
Ac. Ac. Ac. Pre. Rec.
All features
NB 0.76 0.76 0.74 0.80 0.74
Logistic 0.78 0.76 0.70 0.68 0.7
SVM 0.78 0.72 0.79 0.69 0.72
Without user entropy
NB 0.80 0.76 0.70 0.66 0.70
Logistic 0.80 0.82 0.66 0.63 0.66
SVM 0.80 0.78 0.68 0.62 0.68
Table 4: Classification performance by query frequency
Recall, that low frequency queries dominate our
dataset, so we focus on performance of low fre-
quency queries, as reported in Table 4. The respec-
tive ?2 values are reported in (Table 5). The features
UserDomainEntropy and UserEntropy correlate the
most with manual query intent labels.
As an alternative to direct multiclass classification
described above, we first classify clear vs. unclear
queries, and only then attempt to distinguish am-
biguous and informational queries (within the un-
363
Feature ?2 (multiclass) ?2 (binary)
UserDomainEntropy 132.9618 23.3629
UserEntropy 128.0111 21.6112
RelativeOverallEntropy 96.6842 20.0255
RelativeUserEntropy 98.6842 20.0255
OverallEntropy 96.1205 0
Table 5: ?2 values of top five features for multiclass clas-
sification (clear vs. informational vs. ambiguous) and for
and for binary classification (informational vs. ambigu-
ous), given the manual unclear label.
Overall Informational Ambiguous
Ac. Pre. Rec. Pre. Rec.
With User Entropy features
NB 0.72 0.82 0.60 0.65 0.85
Logistic 0.71 0.74 0.70 0.69 0.73
SVM 0.65 0.64 0.73 0.64 0.55
Without User Entropy features
NB 0.66 0.65 0.76 0.67 0.55
Logistic 0.68 0.69 0.73 0.68 0.64
SVM 0.68 0.67 0.81 0.72 0.55
Table 6: Binary classification performance for queries
manually labeled as unclear.
clear category). For classification between clear
and unclear queries, the accuracy was 90%, preci-
sion was 91%, and recall was 90%. The results for
subsequently classifying ambiguous vs. information
queries are reported in Table 6. For this task, User
Entropy features are beneficial, while the ?2 value or
Overall Entropy is 0, supporting our claim that User
Entropy is more useful for distinguishing informa-
tional from ambiguous queries.
Discussion: Interestingly, User Entropy does not
show a large effect on classification of High and
Medium frequency queries. However, as Table 2
indicates, High and Medium frequency queries are
largely clear (76% and 52%, respectively). As dis-
cussed above, User Entropy helps classify unclear
queries, but there are fewer such queries among
the High frequency group, which also tend to have
larger click entropy in general.
An ambiguous query is difficult to detect when
most users interpret it only one way. For instance,
query ?ako? was annotated as ambiguous, as it could
refer to different popular websites, such as the site
for Army Knowledge Online and the company site
for A.K.O., Inc. However, most users select the re-
sult for the Army Knowledge Online site, making
the overall entropy low, resulting in prediction as
a clear query. On the positive side, we find that
User Entropy helps detect ambiguous queries, such
as ?laguna beach?, which was labeled ambiguous as
it could refer to both a geographical location and a
popular MTV show. As a result, while the Overall
Entropy value of the clickthrough is high, the low
User Entropy value identifies the query as truly am-
biguous and not informational.
In summary, our techniques are of most help
for Low frequency queries and moderately helpful
for Medium frequency queries. These results are
promising, as Low frequency queries make up the
majority of queries processed by search engines, and
also contain the highest proportion of informational
queries, which our techniques can identify.
5 Conclusions
We explored clickthrough-based metrics for dis-
tinguishing between ambiguous and informational
queries - which, while exhibiting similar overall
clickthrough distributions, can be more accurately
identified by using our User Entropy-based features.
We demonstrated substantial improvements for low-
frequency queries, which are the most frequent in
query logs. Hence, our results are likely to have no-
ticeable impact in a real search setting.
Acknowledgments: This work was partially sup-
ported by grants from Yahoo! and Microsoft.
References
M. Komachi, S. Makimoto, K. Uchiumi, and M. Sassano.
2009. Learning semantic categories from clickthrough
logs. In Proc. of ACL-IJCNLP.
X. Li, Y.Y. Wang, and A.Acero. 2008. Learning query
intent from regularized click graphs. In SIGIR, pages
339?346.
Q. Mei and K. Church. 2008. Entropy of search logs:
how hard is search? with personalization? with back-
off? In Proc. of WSDM, pages 45?54.
D. E. Rose and D. Levinson. 2004. Understanding user
goals in web search. In Proc. of WWW, pages 13?19.
J. Teevan, S. T. Dumais, and D. J. Liebling. 2008. To per-
sonalize or not to personalize: modeling queries with
variation in user intent. In Proc. of SIGIR, pages 163?
170.
G. Zhu and G. Mishne. 2009. Mining rich session con-
text to improve web search. In Proc. of KDD, pages
1037?1046.
364
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 1?2,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The ?Nays? Have It: Exploring Effects of Sentiment in Collaborative
Knowledge Sharing
Ablimit Aji , Eugene Agichtein
Mathematics & Computer Science Department
Emory University
{aaji,eugene}@mathcs.emory.edu
Abstract
In this paper we study what effects sentiment
have on the temporal dynamics of user inter-
action and content generation in a knowledge
sharing setting. We try to identify how senti-
ment influences interaction dynamics in terms
of answer arrival, user ratings arrival, commu-
nity agreement and content popularity. Our
study suggests that ?Negativity Bias? triggers
more community attention and consequently
more content contribution. Our findings pro-
vide insight into how users interact in online
knowledge sharing communities, and helpful
for improving existing systems.
1 Introduction
Recently, Collaborative Knowledge Sharing sites( or
CQA sites), such as Naver and Yahoo! Answers
have exploded in popularity. Already, for many in-
formation needs, these sites are becoming valuable
alternatives to search engines. Previous studies iden-
tified visibility as an important factor for content
popularity and developed models in static settings.
However, when users post social media content, they
might either explicitly or implicitly express their
personal attitudes or sentiment. The following ex-
ample illustrates a question with negative sentiment.
Q :Obama keeps saying we need to sacrifice.
What sacrifices has he and the gov made collec-
tively and individually?
A1: Our hard earned tax dollars. 17 ?, 2 ?
A2: None and they never will. 18 ?, 2 ?
Psychological studies (Smith et al, 2008) suggest
that our brain has ?Negativity Bias? - that is, people
automatically devote more attention to negative in-
formation than to positive information. Thus, our
attitudes may be more heavily influenced by nega-
tive opinions. Our hypothesis is that this kind of hu-
man cognitive bias would have measurable effects
on how users respond to information need in CQA
communities. Our goal in this paper is to under-
stand how question sentiment influence the dynam-
ics of the user interactions in CQA - that is, to un-
derstand how users respond to questions of different
sentiment, how question sentiment affects commu-
nity agreement on best answer and question popu-
larity.
2 Sentiment Influence
While (Aji et al, 2010) suggests that question cat-
egory has a patent influence on interaction dynam-
ics, we mainly focus on sentiment in this exploratory
study, for the reason that sentiment is a high level
but prominent facet in every piece of content. We
focused on how may sentiment effect the following
dimensions:
? Answer Arrival: Measured as number of an-
swers arrived every minute.
? Vote Arrival: Measured as number of votes ar-
rived per answer.
? Community Agreement: Mean Reciprocal Rank
(MRR), computed by ranking the answers in or-
der of decreasing ?Thumbs up? ratings, and iden-
tifying the rank of the actual ?best? answer, as se-
lected by the asker.
MRR = 1|Q|
N
?
i=1
1
ranki
(1)
where ranki is the rank of the best answer among
the answers submitted for question i.
? Answer Length, Question Length: We examine
whether questions with different sentiment exhibit
variations in question and answer length.
? Interest ?Stars?: How many users marked ques-
tion as interesting.
3 Dataset Description
For our study we tracked a total of approximately
10,000 questions, sampled from 20 categories from
Yahoo! Answers. Specifically, each new question in
our tracking list crawled every five minutes until it?s
closed. As a result, we obtained approximately 22
1
million question-answer-feedback snapshots in to-
tal. Since labeling all the questions would be ex-
pensive, we randomly selected 2000 questions from
this dataset for human labeling. We then utilized the
Amazon Mechanical Turk Service1. Five workers
labeled each question as either positive, negative or
neutral; the ratings were filtered by using majority
opinion (at least 3 out of 5 labels). Overall statistics
of this dataset are reported in Table 1. The overall
inter-rater agreement was 65%.
Positive Negative Neutral Total
379 173 548 1,100
Table 1: Statistics of the Temporal dataset
.4 Results and Discussion
Figure 1 reports answer arrival dynamics for ques-
tion with varying sentiment. Answers to negative
questions arrive substantially faster than answers to
positive or neutral questions, whereas the difference
between positive and neutral questions are minor.
This strongly confirms the ?Negative Bias? effect.
Given the fact that questions stay in the category
front page relatively same amount of time where
their visibility contributes potential answers, on av-
erage, negative sentiment questions managed to get
more answers than two other types of questions (4.3
vs. 3.3 and 3.5). It seems, sentiment expressed in a
question contributes to the answer arrival more than
visibility.
 0 0.5
 1 1.5
 2 2.5
 3 3.5
 4 4.5
 1  10  100  1000
Ans
wer
s
time (in minutes)
negative sentimentneutral sentimentpositive sentiment
Figure 1: Cumulative answer arrival
Figure 2 reports rating arrival dynamics. Interest-
ingly, positive ratings arrive much faster to negative
questions, whereas positive and negative ratings ar-
rive roughly at the same rate for positive and neutral
questions. While this might be partially due to the
fact that negative sentiment questions are more ?at-
tention grabbing? than other types of questions, we
conjecture that this effect is caused by the selection
bias of the raters participating in negative question
threads, who tend to support answers that strongly
1http://www.mturk.com
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 1  10  100  1000
Rat
ing
s
time (in minutes)
negative +neutral +positive +negative -neutral -positive -
Figure 2: Cumulative user ratings arrival
agree (or strongly disagree) with the question asker.
Surprisingly, community agreement(MRR) on the
Type MRR QLength ALength Stars
Negative 0.47 78 49 0.25
Positive 0.56 58 52 0.16
Neutral 0.57 52 47 0.15
Table 2: Agreement, Question length, Answer Length
and Star count averaged over question type
best answer is lower for negative sentiment ques-
tions. On average, negative sentiment questions
were marked as interesting more than positive or
neutral questions were marked as interesting. Al-
though this may sound counterintuitive, it is not sur-
prising if we recall how the ?Negative Bias? influ-
ences user behavior and may increase implicit ?visi-
bility?. All the above mentioned differences are sta-
tistically significant(t-test p = 0.05).
In summary, our preliminary exploration indi-
cates that sentiment may have a powerful effect on
the content contribution dynamics in collaborative
question answering, and is a promising direction for
further study of knowledge sharing communities.
Acknowledgments
We thank HP Social Computing Labs for support.
References
Ablimit Aji. Eugene Agichtein. 2010. Deconstructing
Interaction Dynamics in Knowledge Sharing Commu-
nities. International Conference on Social Computing,
Behavioral Modeling, & Prediction.
Gabor Szabo. Bernardo Huberman. 2008. Predicting the
popularity of online content. HP Labs Technical Re-
port.
Kristina Lerman. 2007. Social Information Processing
in Social News Aggregation. IEEE Internet Comput-
ing: Special Issue on Social Search.
N. Kyle Smith Jeff T. Larsen Tanya L. Chartrand John
T. Cacioppo 2006. Affective Context Moderates the
Attention Bias Toward Negative Information. Journal
of Personality and Social Psychology.
2
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 9?10,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Automatic Question Answering over Social Media by Learning 
Question Equivalence Patterns 
 
Tianyong Hao1 Wenyin Liu Eugene Agichtein 
City University of Hong Kong City University of Hong Kong  Emory University 
81 Tat Chee Avenue 81 Tat Chee Avenue 201 Dowman Drive 
Kowloon, Hong Kong SAR Kowloon, Hong Kong SAR Atlanta, Georgia 30322 USA 
haotianyong@gmail.com csliuwy@cityu.edu.hk eugene@mathcs.emory.edu 
 
 
 
Abstract 
Many questions submitted to Collaborative 
Question Answering (CQA) sites have been 
answered before. We propose an approach to 
automatically generating an answer to such 
questions based on automatically learning to 
identify ?equivalent? questions. Our main 
contribution is an unsupervised method for 
automatically learning question equivalence 
patterns from CQA archive data. These pat-
terns can be used to match new questions to 
their equivalents that have been answered be-
fore, and thereby help suggest answers auto-
matically. We experimented with our method 
approach over a large collection of more than 
200,000 real questions drawn from the Yahoo! 
Answers archive, automatically acquiring 
over 300 groups of question equivalence pat-
terns. These patterns allow our method to ob-
tain over 66% precision on automatically 
suggesting answers to new questions, signifi-
cantly outperforming conventional baseline 
approaches to question matching.  
1 Introduction 
Social media in general exhibit a rich variety of 
information sources. Question answering (QA) has 
been particularly amenable to social media, as it 
allows a potentially more effective alternative to 
web search by directly connecting users with the 
information needs to users willing to share the in-
formation directly (Bian, 2008). One of the useful 
by-products of this process is the resulting large 
archives of data ? which in turn could be good 
sources of information for automatic question an-
swering. Yahoo! Answers, as a collaborative QA 
system (CQA), has acquired an archive of more 
than 40 Million Questions and 500 Million an-
swers, as of 2008 estimates.  
The main premise of this paper is that there are 
many questions that are syntactically different 
while semantically similar. The key problem is 
how to identify such question groups. Our method 
is based on the key observation that when the best 
non-trivial answers chosen by asker in the same 
domain are exactly the same, the corresponding 
questions are semantically similar. Based on this 
observation, we propose answering new method 
for learning question equivalence patterns from 
CQA archives. First, we retrieve ?equivalent? 
question groups from a large dataset by grouping 
them by the text of the best answers (as chosen by 
the askers). The equivalence patterns are then gen-
erated by learning common syntactic and lexical 
patterns for each group. To avoid generating pat-
terns from questions that were grouped together by 
chance, we estimate the group?s topic diversity to 
filter the candidate patterns. These equivalence 
patterns are then compared against newly submit-
ted questions. In case of a match, the new question 
can be answered by proposing the ?best? answer 
from a previously answered equivalent question.  
We performed large-scale experiments over a 
more than 200,000 questions from Yahoo! An-
swers. Our method generated over 900 equivalence 
patterns in 339 groups and allows to correctly sug-
gest an answer to a new question, roughly 70% of 
the time ? outperforming conventional similarity-
based baselines for answer suggestion.  
Moreover, for the newly submitted questions, 
our method can identify equivalent questions and 
generate equivalent patterns incrementally, which 
can greatly improve the feasibility of our method. 
2 Learning Equivalence Patterns 
While most questions that share exactly the same 
?best? answer are indeed semantically equivalent, 
some may share the same answer by chance. To 
???????????????? 
1 Work done while visiting Emory University 
 
9
filter out such cases, we propose an estimate of 
Topical Diversity (TD), calculated based on the 
shared topics for all pairs of questions in the group. 
If the diversity is larger than a threshold, the ques-
tions in this group are considered not equivalent, 
and no patterns are generated. To calculate this 
measure, we consider as topics the ?notional 
words? (NW) in the question, which are the head 
nouns and the heads of verb phrases recognized by 
the OpenNLP parser. Using these words as ?top-
ics?, TD for a group of questions G is calculated as:  
? ?-
= =
<-?-=
1
1 2
)()1()1(
2)(
n
i
n
j ji
ji jiQQ
QQ
nnGTD U
I           
where Qi and Qj are the notional words in each 
question in within group G with n questions total.     
Based on the question groups, we can generate 
equivalence patterns to extend the matching cover-
age ? thus retrieving similar questions with differ-
ent syntactic structure. OpenNLP is used to 
generate the basic syntactic structures by phrase 
chunking. After that, only the chunks which con-
tain NWs are analyzed to acquire the phrase labels 
as the syntactic pattern. Table 1 shows an example 
of a generated pattern. 
 
Question: What was the first book you discovered that 
made you think reading wasn't a complete waste of time?  
Pattern: [NP]-[VP]-[NP]-[NP]-[VP]-[VP]-[NP]-[VP]-? 
NW: (Disjoint: read waste time) (Shared: book think) 
Question: What book do you think everyone should have 
at home?  
Pattern: [NP]-[NP]-[VP]-[NP]-[VP]-[PP]-[NP] 
NW: (Disjoint: do everyone have home) (Shared: book 
think) 
Table 1. A group of equivalence patterns 
3 Experimental Evaluation 
Our dataset is 216,563 questions and 2,044,296 
answers crawled from Yahoo! Answers. From this 
we acquired 833 groups of similar questions dis-
tributed in 65 categories. After filtering by topical 
diversity, 339 groups remain to generate equiva-
lence patterns. These groups contain 979 questions, 
with, 2.89 questions per group on average.  
After that, we split our data into 413 questions 
for training (200 groups) and 566 questions, with 
randomly selected an additional 10,000 questions, 
for testing (the remainder) to compare three vari-
ants of our system Equivalence patterns only (EP), 
Notional words only (NW), and the weighted com-
bination (EP+NW). To match question, both 
equivalence patterns and notional words are used 
with different weights. The weight of pattern, dis-
joint NW and shared NW are 0.7, 0.4 and 0.6 after 
parameter training. We then compare the variants 
and results are reported in Table 2, showing that 
EP+NW achieves the highest performance.  
 
 Recall Precision F1 score 
EP 0.811 0.385 0.522 
NW 0.378 0.559 0.451 
EP+NW 0.726 0.663 0.693 
Table 2. Performance comparison of three variants 
 
Using EP+NW as our best method, we now 
compare it to traditional similarity-based methods 
on whole question set. TF*IDF-based vector space 
model (TFIDF), and a more highly tuned Cosine 
model (that only keeps the same ?notional words? 
filtered by phrase chunking) are used as baselines. 
Figure 3 reports the results, which indicate that 
EP+NW, outperforms both Cosine and TFIDF me-
thods on all metrics. 
 
0
0.1
0 .2
0 .3
0 .4
0 .5
0 .6
0 .7
0 .8
Recall Precisio n F1
TFIDF(NW)
COSINE
EP+NW
 Figure 3. Performance of EP+NW vs. baselines 
 
Our work expands on previous significant ef-
forts on CQA retrieval (e.g., Bian et al, Jeon et al, 
Kosseim et al). Our contribution is a new unsu-
pervised and effective method for learning ques-
tion equivalence patterns that exploits the structure 
of the collaborative question answering archives ? 
an important part of social media. 
4 References 
Bian, J., Liu, Y., Agichtein, E., and Zha, H. 2008. Find-
ing the right facts in the crowd: factoid question an-
swering over social media. WWW. 
Jeon, J., Croft, B.W. and Lee, J.H. 2005. Finding simi-
lar questions in large question and answer archives 
Export Find Similar. CIKM. 
Kosseim, L. and Yousefi, J. 2008.Improving the per-
formance of question answering with semantically 
equivalent answer patterns, Journal of Data & 
Knowledge Engineering. 
10
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88?93,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Towards Tracking Political Sentiment through Microblog Data
Yu Wang
Emory University
yu.wang@emory.edu
Tom Clark
Emory University
tclark7@emory.edu
Eugene Agichtein
Emory University
eugene@mathcs.emory.edu
Jeffrey Staton
Emory University
jkstato@emory.edu
Abstract
People express and amplify political opin-
ions in Microblogs such as Twitter, espe-
cially when major political decisions are
made. Twitter provides a useful vehicle for
capturing and tracking popular opinion on
burning issues of the day. In this paper,
we focus on tracking the changes in polit-
ical sentiment related to the U.S. Supreme
Court (SCOTUS) and its decisions, fo-
cusing on the key dimensions on support,
emotional intensity, and polarity. Mea-
suring changes in these sentiment dimen-
sions could be useful for social and politi-
cal scientists, policy makers, and the pub-
lic. This preliminary work adapts existing
sentiment analysis techniques to these new
dimensions and the specifics of the cor-
pus (Twitter). We illustrate the promise
of our work with an important case study
of tracking sentiment change building up
to, and immediately following one recent
landmark Supreme Court decision. This
example illustrates how our work could
help answer fundamental research ques-
tions in political science about the nature
of Supreme Court power and its capacity
to influence public discourse.
1 Background and Motivation
Political opinions are a popular topic in Mi-
croblogs. On June 26th, 2013, when the U.S.
Supreme Court announced the decision on the un-
constitutionality of the ?Defense of Marriage Act?
(DOMA), there were millions of Tweets about the
users? opinions of the decision. In their Tweets,
people not only voice their opinions about the is-
sues at stake, expressing different dimensions of
sentiment, such as support or opposition to the de-
cision, or anger or happiness. Thus, simply ap-
plying traditional sentiment analysis scales such
as ?positive? vs. ?negative? classification would
not be sufficient to understand the public reaction
to political decisions.
Research on mass opinion and the Supreme
Court is valuable as it could shed light on the fun-
damental and related normative concerns about the
role of constitutional review in American gover-
nance, which emerge in a political system possess-
ing democratic institutions at cross-purposes. One
line of thought, beginning with Dahl (Dahl, 1957),
suggests that the Supreme Court of the United
States has a unique capacity among major institu-
tions of American government to leverage its legit-
imacy in order to change mass opinion regarding
salient policies. If the Dahl?s hypothesis is correct,
then the Supreme Court?s same-sex marriage deci-
sions should have resulted in a measurable change
in opinion. A primary finding about implication of
Dahl?s hypothesis is that the Court is polarizing,
creating more supportive opinions of the policies
it reviews among those who supported the pol-
icy before the decision and more negative opin-
ions among those who opposed the policy prior to
the decision (Franklin and Kosaki, 1989) (Johnson
and Martin, 1998).
We consider Twitter as important example of
social expression of opinion. Recent studies of
content on Twitter have revealed that 85% of Twit-
ter content is related to spreading and commenting
on headline news (Kwak et al., 2010); when users
talk about commercial brands in their Tweets,
about 20% of them have personal sentiment in-
volved (Jansen et al., 2009). These statistical evi-
dences imply that Twitter has became a portal for
public to express opinions. In the context of pol-
itics, Twitter content, together with Twitter users?
88
information, such as user?s profile and social net-
work, have shown reasonable power of detecting
user?s political leaning (Conover et al., 2011) and
predicting elections (Tumasjan et al., 2010). Al-
though promising, the effectiveness of using Twit-
ter content to measure public political opinions re-
mains unclear. Several studies show limited corre-
lation between sentiment on Twitter and political
polls in elections (Mejova et al., 2013) (O?Connor
et al., 2010). Our study mainly focuses on inves-
tigating sentiment on Twitter about U.S. Supreme
Court decisions.
We propose more fine-grained dimensions for
political sentiment analysis, such as supportive-
ness, emotional intensity and polarity, allowing
political science researchers, policy makers, and
the public to better comprehend the public reaction
to major political issues of the day. As we describe
below, these different dimensions of discourse on
Twitter allows examination of the multiple ways in
which discourse changes when the Supreme Court
makes a decision on a given issue of public policy.
Our dimensions also open the door to new avenues
of theorizing about the nature of public discourse
on policy debates.
Although general sentiment analysis has made
significant advances over the last decade (Pang et
al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wil-
son et al., 2009), and with the focus on certain
aspects, such as intensity (Wilson et al., 2004),
irony detection (Carvalho et al., 2009) and sar-
casm detection (Davidov et al., 2010), analyzing
Microblog content such as Twitter remains a chal-
lenging research topic (Reyes et al., 2012) (Vanin
et al., 2013) (Agarwal et al., 2011). Unlike previ-
ous work, we introduce and focus on sentiment di-
mensions particularly important for political anal-
ysis of Microblog text, and extend and adapt clas-
sification techniques accordingly. To make the
data and sentiment analysis results accessible for
researchers in other domain, we build a website to
visualize the sentiment dynamics over time and let
users download the data. Users could also define
their own topics of interest and perform deeper
analysis with keyword filtering and geolocation
filtering.
We present a case study in which our results
might be used to answer core questions in polit-
ical science about the nature of Supreme Court
influence on public opinion. Political scientists
have long been concerned with whether and how
Supreme Court decisions affect public opinion and
discourse about political topics (Hoekstra, 2003)
(Johnson and Martin, 1998) (Gibson et al., 2003).
Survey research on the subject has been limited in
two ways. Survey analysis, including panel de-
signs, rely on estimates near but never on the date
of particular decisions. In addition, all survey-
based research relies on estimates derived from an
instrument designed to elicit sentiment ? survey
responses, useful as they are, do not reflect well
how public opinion is naturally expressed. Our
analysis allows for the examination of public opin-
ion as it is naturally expressed and in a way that is
precisely connected to the timing of decisions.
Next, we state the problem more formally, and
outline our approach and implementation.
2 Problem Statement and Approach
2.1 Political Sentiment Classification
We propose three refinements to sentiment analy-
sis to quantify political opinions. Specifically, we
pose the following dimensions as particularly im-
portant for politics:
? Support: Whether a Tweet is Opposed, Neu-
tral, or Supportive regarding the topic.
? Emotional Intensity: Whether a Tweet is
emotionally Intense or Dispassionate.
? Sentiment Polarity: Whether a Tweet?s tone
is Angry, Neutral, or Pleased.
2.2 Approach
In this work, each of the proposed measures is
treated as a supervised classification problem. We
use multi-class classification algorithms to model
Support and Sentiment Polarity, and binary classi-
fication for Emotional Intensity and Sarcasm. Sec-
tion 3.2 describes the labels used to train the super-
vised classification models. Notice some classes
are more interesting than the others. For exam-
ple, the trends or ratio of opposed vs. supportive
Microblogs are more informative than the factual
ones. Particularly, we pay more attention to the
classes of opposed, supportive, intense, angry, and
pleased.
2.3 Classifier Feature Groups
To classify the Microblog message into the classes
of interest, we develop 6 groups of features:
Popularity: Number of times the message has been
89
posted or favored by users. As for a Tweet, this
feature means number of Retweets and favorites.
Capitalization and Punctuation.
N-gram of text: Unigram, bigram, and trigram of
the message text.
Sentiment score: The maximum, minimum, aver-
age and sum of sentiment score of terms and each
Part-of-Speech tags in the message text.
Counter factuality and temporal compression dic-
tionary: This feature counts the number of times
such words appear in the message text.
Political dictionary: Number of times a political-
related word appears in the message text.
We compute sentiment scores based on Senti-
WordNet
1
, a sentiment dictionary constructed on
WordNet.
2
Political dictionary is built upon
political-related words in WordNet. As in this pa-
per, we construct a political dictionary with 56
words and phrases, such as ?liberal?, ?conserva-
tive?, and ?freedom? etc.
3 Case Study: DOMA
Our goal is to build and test classifiers that can dis-
tinguish political content between classes of inter-
est. Particularly, we focus on classifying Tweets
related to one of the most popular political topics,
?Defence of Marriage Act? or DOMA, as the tar-
get. The techniques can be easily generalized to
other political issues in Twitter.
3.1 Dataset
In order to obtain relevant Tweets, we use Twit-
ter streaming API to track representative key-
words which include ?DOMA?, ?gay marriage?,
?Prop8?, etc. We track all matched Tweets gen-
erated from June 16th to June 29th, immedi-
ately prior and subsequent to the DOMA decision,
which results in more than 40 thousand Tweets per
day on average.
3.2 Human Judgments
With more than 0.5 million potential DOMA rele-
vant Tweets collected, we randomly sampled 100
Tweets per day from June 16th to June 29th, and
1,400 Tweets were selected in total. Three re-
search assistants were trained and they showed
high agreement on assigning labels of relevance,
support, emotional intensity, and sentiment polar-
ity after training. Each Tweet in our samples was
1
http://sentiwordnet.isti.cnr.it/
2
http://wordnet.princeton.edu/
labeled by all three annotators. After the label-
ing, we first removed ?irrelevant? Tweets (if the
Tweet was assigned ?irrelevant? label by at least
one annotator), and then the tweets with no major
agreement among annotators on any of the senti-
ment dimensions were removed. As a result, 1,151
tweets with what we consider to be reliable labels
remained in our dataset (which we expect to share
with the research community).
3.2.1 Annotator Agreement
The Fleiss? Kappa agreement for each scale is re-
ported in Table 1 and shows that labelers have an
almost perfect agreement on relevance. Support,
emotional intensity, and sentiment polarity, show
either moderate or almost perfect agreement.
Measure Fleiss? Kappa
Relevance 0.93
Support 0.84
Intensity 0.54
Polarity 0.49
Table 1: Agreement (Fleiss? Kappa) of Human Labels.
3.3 Classification Performance Results
We reproduce the same feature types as previous
work and develop the political dictionary feature
for this particular task. We experimented with a
variety of automated classification algorithms, and
for this preliminary experiment report the perfor-
mance of Na??ve Bayes algorithm (simple, fast, and
shown to be surprisingly robust to classification
tasks with sparse and noisy training data). 10-fold
cross validation are performed to test the general-
izability of the classifiers. Table 2 reports the aver-
age precision, recall and accuracy for all measures.
Sarcasm is challenging to detect in part due to the
lack of positive instances. One goal in this study
is to build a model that captures trends among the
different classes. In Section 3.4, we will show that
the trends of different measures estimated by the
trained classifier align with the human annotated
ones over time.
3.4 Visualizing Sentiment Before and After
DOMA
One natural application of the automated politi-
cal sentiment analysis proposed in this paper is
tracking public sentiment around landmark U.S.
Supreme Court decisions. To provide a more re-
liable estimate, we apply our trained classifier on
all relevant Tweets in our collection. More than
90
Value Prec. (%) Rec. (%) Accuracy(%)
Supportive (48%) 73 74
Neutral (45%) 76 67 68
Opposed (7%) 17 30
Intense (31%) 56 60
73
Dispassionate (69%) 81 79
Pleased (10%) 48 31
Neutral (79%) 84 78 69
Angry (11%) 24 45
Table 2: Performance of Classifiers on Each Class.
2.5 million Tweets are estimated in four proposed
measures. Figure 1 shows the distribution of on-
topic Tweet count over time. The Supreme Court
decision triggered a huge wave of Tweets, and the
volume went down quickly since then.
0
100,000
200,000
300,000
400,000
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Nu
mb
er o
f Tw
eet
s 
Date 
Figure 1: Number of ?Gay Marriage? Tweets Over Time.
Figures 2 and 3 visualize both the human la-
beled trends and the ones obtained by the classi-
fier for the classes ?Supportive? and ?Intense?. In
both figures, the peaks in the predicted labels gen-
erally align with the human-judged ones. We can
see the supportiveness and intensity are both rela-
tively high before the decision, and then they de-
cline gradually after the Supreme Court decision.
Figure 3 shows the volume of intensive Tweets
detected by our trained model has a burst on June
22rd, which is not captured by human labeled
data. To investigate this, we manually checked all
Tweets estimated as ?intensive? on June 22rd. It
turns out most of the Tweets are indeed intensive.
The reason of the burst is that one Tweet was heav-
ily retweeted on that day. We do not disclose the
actual tweet due to its offensive content.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Human Labeled
Estimated
Figure 2: Percentage of ?Supportive? Tweets Over Time.
Figure 4 plots the trends of ?supportive? and
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Human Labeled
Estimated
Figure 3: Percentage of ?Intense? Tweets Over Time.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
% of 
Oppose
d Tw
eets
 
% o
f Suppo
rtive 
Twe
ets
 Supportive Opposed
Figure 4: Comparison between ?Supportive? and ?Op-
posed? Trends.
?opposed? Tweets in different scales. According
to the Supreme Court decision, the ?supportive?
group wins the debate. Interestingly, instead of
responding immediately, the ?loser? group react
and start Tweeting 2 days after the decision. These
trends indicate that ?winner? and ?loser? in the de-
bate react differently in time and intensity dimen-
sions.
We believe that our estimates of sentiment can
be used in various ways by political scientists.
The ?positivity bias? (Gibson and Caldeira, 2009)
model of Supreme Court opinion suggests that
the Court can move public opinion in the direc-
tion of its decisions. Our results possibly indicate
the opposite, the ?polarizing? model suggested by
(Franklin and Kosaki, 1989) and (Johnson and
Martin, 1998), where more negative opinions are
observed after the decision (in Figure 4), at least
for a short period. By learning and visualize polit-
ical sentiments, we could crystalize the nature of
the decision that influences the degree to which the
Supreme Court can move opinion in the direction
of its decisions.
4 An Open Platform for Sharing and
Analyzing Political Sentiments
Figure 5 shows a website
3
that visualizes politi-
cal sentiments over time. The website shows sev-
eral popular U.S. Supreme Court cases, such as
?gay marriage?, ?voting right act?, ?tax cases?,
3
http://www.courtometer.com
91
etc., and general topics, such as ?Supreme Court?
and ?Justices?. Each of the topics is represented
by a list of keywords developed by political sci-
ence experts. The keywords are also used to track
relevant Tweets through Twitter streaming API. To
let users go deeper in analyzing public opinions,
the website provides two types of real-time filter-
ing: keywords and location of Tweet authors. Af-
ter applying filters, a subset of matched Tweets are
generated as subtopics and their sentiments are vi-
sualized. The example filtering in Figure 5 shows
the process of creating subtopic ?voting right act?
out of a general topic ?Supreme Court? by using
keyword ?VRA?. We can see that the volume of
negative Tweets of ?voting right act? is higher than
the positive ones, compared to the overall senti-
ment of the general Supreme Court topic. Once an
interesting subtopic is found, users can download
the corresponding data and share with other users.
Topic ?Supreme Court? 
Nu
mb
er 
of 
Tw
eet
s 
Nu
mb
er 
of 
Tw
eet
s 
Subtopic ?Voting Right Act? 
Filtered by keyword: ?VRA? 
Figure 5: We build a website that visualizes political sen-
timents over time and let users create ?subtopics? by using
keyword and location filters.
5 Conclusions
In this paper we considered the problem of polit-
ical sentiment analysis. We refined the notion of
sentiment, as applicable to the political domain,
and explored the features needed to perform auto-
mated classification to these dimensions, on a real
corpus of tweets about one U.S. Supreme Court
case. We showed that our existing classifier can
already be useful for exploratory political analy-
sis, by comparing the predicted sentiment trends to
those derived from manual human judgments, and
then applying the classifier on a large sample of
tweets ? with the results providing additional ev-
idence for an important model of Supreme Court
opinion formation from political science.
This work provides an important step towards
robust sentiment analysis in the political domain,
and the data collected in our study is expected to
serve as a stepping stone for subsequent explo-
ration. In the future, we plan to refine and im-
prove the classification performance by exploring
additional features, in particular in the latent topic
space, and experimenting with other political sci-
ence topics.
ACKNOWLEDGMENTS The work of Yu Wang
and Eugene Agichtein was supported in part by
DARPA grants N11AP20012 and D11AP00269,
and by the Google Research Award.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
Analysis of Twitter Data. In Proceedings of the
Workshop on Language in Social Media (LSM).
Paula Carvalho, Lu??s Sarmento, M?ario J. Silva, and
Eug?enio de Oliveira. 2009. Clues for detecting
irony in user-generated contents: oh...!! it?s ?so
easy? ;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion.
M.D. Conover, B. Goncalves, J. Ratkiewicz, A. Flam-
mini, and F. Menczer. 2011. Predicting the Political
Alignment of Twitter Users In Proceedings of IEEE
third international conference on social computing
Robert Dahl. 1957. Decision-Making in a Democracy:
The Supreme Court as National Policy-Maker. Jour-
nal of Public Law.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised Recognition of Sarcastic Sentences
in Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL).
Charles H. Franklin, and Liane C. Kosaki. 1989. Re-
publican Schoolmaster: The U.S. Supreme Court,
Public Opinion, and Abortion. The American Po-
litical Science Review.
James L Gibson, and Gregory A Caldeira. 2009. Cit-
izens, courts, and confirmations: Positivity theory
and the judgments of the American people. Prince-
ton University Press.
James L Gibson, Gregory A Caldeira, and Lester Keny-
atta Spence. 2003. Measuring Attitudes toward the
92
United States Supreme Court. American Journal of
Political Science.
Valerie Hoekstra. 2003. Public Reaction to Supreme
Court Decisions. Cambridge University Press.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Micro-blogging As Online Word
of Mouth Branding. in CHI ?09 Extended Abstracts
on Human Factors in Computing Systems.
Timothy R. Johnson, and Andrew D. Martin. 1998.
The Public?s Conditional Response to Supreme
Court Decisions. American Political Science Re-
view 92(2):299-309.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a Social Network
or a News Media?. in Proceedings of the 19th Inter-
national Conference on World Wide Web (WWW).
Yu-Ru Lin, Drew Margolin, Brian Keegan, and David
Lazer. 2013. Voices of Victory: A Computational
Focus Group Framework for Tracking Opinion Shift
in Real Time. In Proceedings of International World
Wide Web Conference (WWW).
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies.
Yelena Mejova, Padmini Srinivasan, and Bob Boynton.
2013. GOP Primary Season on Twitter: ?Popular?
Political Sentiment in Social Media. In Proceedings
of the Sixth ACM International Conference on Web
Search and Data Mining (WSDM).
B. O?Connor, R. Balasubramanyan, B. R. Routledge,
and N. A. Smith. 2010. From tweets to polls: Link-
ing text sentiment to public opinion time series. In
Proceedings of International AAAI Conference on
Weblogs and Social Media (ICWSM).
Bo Pang, and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Antonio Reyes, Paolo Rosso, and Tony Veale. 2012.
A multidimensional approach for detecting irony in
Twitter. Language Resources and Evaluation.
Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion Graphs for Po-
larity and Discourse Classification. TextGraphs-
4: Graph-based Methods for Natural Language Pro-
cessing.
Aline A. Vanin, Larissa A. Freitas, Re-nata Vieira, and
Marco Bochernitsan. 2013. Some clues on irony
detection in tweets. In Proceedings of International
World Wide Web Conference (WWW).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. In Proceedings of Conference
on Artificial Intelligence (AAAI).
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
Elections with Twitter: What 140 Characters Re-
veal about Political Sentiment. In Proceedings of
the Fourth International AAAI Conference on We-
blogs and Social Media (ICWSM).
93
