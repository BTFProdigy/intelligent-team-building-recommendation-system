METER: MEasuring TExt Reuse
Paul Clough and Robert Gaizauskas and Scott S.L. Piao and Yorick Wilks
Department of Computer Science
University of She?eld
Regent Court, 211 Portobello Street,
She?eld, England, S1 4DP
finitial.surname@dcs.shef.ac.ukg
Abstract
In this paper we present results from
the METER (MEasuring TExt Reuse)
project whose aim is to explore issues
pertaining to text reuse and derivation,
especially in the context of newspapers
using newswire sources. Although the
reuse of text by journalists has been
studied in linguistics, we are not aware
of any investigation using existing com-
putational methods for this particular
task. We investigate the classication
of newspaper articles according to their
degree of dependence upon, or deriva-
tion from, a newswire source using a
simple 3-level scheme designed by jour-
nalists. Three approaches to measur-
ing text similarity are considered: n-
gram overlap, Greedy String Tiling,
and sentence alignment. Measured
against a manually annotated corpus of
source and derived news text, we show
that a combined classier with fea-
tures automatically selected performs
best overall for the ternary classica-
tion achieving an average F
1
-measure
score of 0.664 across all three cate-
gories.
1 Introduction
A topic of considerable theoretical and practical
interest is that of text reuse: the reuse of existing
written sources in the creation of a new text. Of
course, reusing language is as old as the retelling
of stories, but current technologies for creating,
copying and disseminating electronic text, make
it easier than ever before to take some or all of
any number of existing text sources and reuse
them verbatim or with varying degrees of mod-
ication.
One form of unacceptable text reuse, plagia-
rism, has received considerable attention and
software for automatic plagiarism detection is
now available (see, e.g. (Clough, 2000) for a re-
cent review). But in this paper we present a
benign and acceptable form of text reuse that
is encountered virtually every day: the reuse of
news agency text (called copy) in the produc-
tion of daily newspapers. The question is not
just whether agency copy has been reused, but
to what extent and subject to what transforma-
tions. Using existing approaches from computa-
tional text analysis, we investigate their ability
to classify newspapers articles into categories in-
dicating their dependency on agency copy.
2 Journalistic reuse of a newswire
The process of gathering, editing and publish-
ing newspaper stories is a complex and spe-
cialised task often operating within specic pub-
lishing constraints such as: 1) short deadlines;
2) prescriptive writing practice (see, e.g. Evans
(1972)); 3) limits of physical size; 4) readability
and audience comprehension, e.g. a tabloid's
vocabulary limitations; 5) journalistic bias, e.g.
political and 6) a newspaper's house style. Of-
ten newsworkers, such as the reporter and edi-
tor, will rely upon news agency copy as the basis
of a news story or to verify facts and assess the
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159.
                         Proceedings of the 40th Annual Meeting of the Association for
importance of a story in the context of all those
appearing on the newswire. Because of the na-
ture of journalistic text reuse, dierences will
arise between reused news agency copy and the
original text. For example consider the follow-
ing:
Original (news agency) A drink-driver who
ran into the Queen Mother's o?cial Daim-
ler was ned $700 and banned from driving
for two years.
Rewrite (tabloid) A DRUNK driver who
ploughed into the Queen Mother's limo was
ned $700 and banned for two years yes-
terday.
This simple example illustrates the types of
rewrite that can occur even in a very short
sentence. The rewrite makes use of slang and
exaggeration to capture its readers' attention
(e.g. DRUNK, limo, ploughed). Deletion (e.g.
from driving) has also been used and the addi-
tion of yesterday indicates when the event oc-
curred. Many of the transformations we ob-
served between moving from news agency copy
to the newspaper version have also been re-
ported by the summarisation community (see,
e.g., McKeown and Jing (1999)).
Given the value of the information news agen-
cies supply, the ease with which text can be
reused and commercial pressures, it would be
benecial to be able to identify those news sto-
ries appearing in the newspapers that have relied
upon agency copy in their production. Potential
uses include: 1) monitoring take-up of agency
copy; 2) identifying the most reused stories ; 3)
determining customer dependency upon agency
copy and 4) new methods for charging customers
based upon the amount of copy reused. Given
the large volume of news agency copy output
each day, it would be infeasible to identify and
quantify reuse manually; therefore an automatic
method is required.
3 A conceptual framework
To begin to get a handle on measuring text
reuse, we have developed a document-level clas-
sication scheme, indicating the level at which
a newspaper story as a whole is derived from
agency copy, and a lexical-level classication
scheme, indicating the level at which individ-
ual word sequences within a newspaper story
are derived from agency copy. This framework
rests upon the intuitions of trained journalists
to judge text reuse, and not on an explicit lex-
ical/syntactic denition of reuse (which would
presuppose what we are setting out to discover).
At the document level, newspaper stories
are assigned to one of three possible categories
coarsely reecting the amount of text reused
from the news agency and the dependency of
the newspaper story upon news agency copy
for the provision of \facts". The categories in-
dicate whether a trained journalist can iden-
tify text rewritten from the news agency in
a candidate derived newspaper article. They
are: 1) wholly-derived (WD): all text in
the newspaper article is rewritten only from
news agency copy; 2) partially-derived (PD):
some text is derived from the news agency, but
other sources have also been used; and 3) non-
derived (ND): news agency has not been used
as the source of the article; although words may
still co-occur between the newspaper article and
news agency copy on the same topic, the jour-
nalist is condent the news agency has not been
used.
At the lexical or word sequence level, individ-
ual words and phrases within a newspaper story
are classied as to whether they are used to ex-
press the same information as words in news
agency copy (i.e. paraphrases) and or used to
express information not found in agency copy.
Once again, three categories are used, based on
the judgement of a trained journalist: 1) verba-
tim: text appearing word-for-word to express
the same information; 2) rewrite: text para-
phrased to create a dierent surface appearance,
but express the same information and 3) new:
text used to express information not appearing
in agency copy (can include verbatim/rewritten
text, but being used in a dierent context).
3.1 The METER corpus
Based on this conceptual framework, we have
constructed a small annotated corpus of news
texts using the UK Press Association (PA) as
the news agency source and nine British daily
newspapers
1
who subscribe to the PA as candi-
date reusers. The METER corpus (Gaizauskas
et al, 2001) is a collection of 1716 texts (over
500,000 words) carefully selected from a 12
month period from the areas of law and court
reporting (769 stories) and showbusiness (175
stories). 772 of these texts are PA copy and 944
from the nine newspapers. These texts cover 265
dierent stories from July 1999 to June 2000 and
all newspaper stories have been manually classi-
ed at the document-level. They include 300
wholly-derived, 438 partially-derived and 206
non-derived (i.e. 77% are thought to have used
PA in some way). In addition, 355 have been
classied according to the lexical-level scheme.
4 Approaches to measuring text
similarity
Many problems in computational text analy-
sis involve the measurement of similarity. For
example, the retrieval of documents to full a
user information need, clustering documents ac-
cording to some criterion, multi-document sum-
marisation, aligning sentences from one lan-
guage with those in another, detecting exact and
near duplicates of documents, plagiarism detec-
tion, routing documents according to their style
and identifying authorship attribution. Meth-
ods typically vary depending upon the match-
ing method, e.g. exact or partial, the degree
to which natural language processing techniques
are used and the type of problem, e.g. search-
ing, clustering, aligning etc. We have not had
time to investigate all of these techniques, nor
is there space here to review them. We have
concentrated on just three: ngram overlap mea-
sures, Greedy String Tiling, and sentence align-
ment. The rst was investigated because it of-
fers perhaps the simplest approach to the prob-
lem. The second was investigated because it has
been successfully used in plagiarism detection, a
problem which at least supercially is quite close
1
The newspapers include ve popular papers (e.g. The
Sun, The Daily Mail, Daily Star, Daily Mirror) and four
quality papers (e.g. Daily Telegraph, The Guardian, The
Independent and The Times).
to the text reuse issues we are investigating. Fi-
nally, alignment (treating the derived text as a
\translation" of the rst) seemed an intriguing
idea, and contrasts, certainly with the ngram ap-
proach, by focusing more on local, as opposed to
global measures of similarity.
4.1 Ngram Overlap
An initial, straightforward approach to assessing
the reuse between two texts is to measure the
number of shared word ngrams. This method
underlies many of the approaches used in copy
detection including the approach taken by Lyon
et al (2001).
They measure similarity using the set-
theoretic measures of containment and resem-
blance of shared trigrams to separate texts writ-
ten independently and those with su?cient sim-
ilarity to indicate some form of copying.
We treat each document as a set of overlap-
ping n-word sequences (initially considering only
n-word types) and compute a similarity score
from this. Given two sets of ngrams, we use
the set-theoretic containment score to measure
similarity between the documents for ngrams of
length 1 to 10 words. For a source text A and
a possibly derived text B represented by sets of
ngrams S
n
(A) and S
n
(B) respectively, the pro-
portion of ngrams in B also in A, the ngram con-
tainment C
n
(A;B), is given by:
C
n
(A;B) =
j S
n
(A) \ S
n
(B) j
j S
n
(B) j
(1)
Informally containment measures the number
of matches between the elements of ngram sets
S
n
(A) and S
n
(B), scaled by the size of S
n
(B).
In other words we measure the proportion of
unique n-grams in B that are found in A. The
score ranges from 0 to 1, indicating none to all
newspaper copy shared with PA respectively.
We also compare texts by counting only those
ngrams with low frequency, in particular those
occurring once. For 1-grams, this is the same as
comparing the hapax legomena which has been
shown to discriminate plagiarised texts from
those written independently even when lexical
overlap between the texts is already high (e.g.
70%) (Finlay, 1999). Unlike Finlay's work, we
nd that repetition in PA copy
2
drastically re-
duces the number of shared hapax legomena
thereby inhibiting classication of derived and
non-derived texts. Therefore we compute the
containment of hapax legomena (hapax contain-
ment) by comparing words occurring once in the
newspaper, i.e. those 1-grams in S
1
(B) that oc-
cur once with all 1-grams in PA copy, S
1
(A).
This containment score represents the number
of newspaper hapax legomena also appearing at
least once in PA copy.
4.2 Greedy String-Tiling
Greedy String-Tiling (GST) is a substring
matching algorithm which computes the degree
of similarity between two strings, for exam-
ple software code, free text or biological subse-
quences (Wise, 1996). Compared with previous
algorithms for computing string similarity, such
as the Longest Common Subsequence or
Levenshtein distance, GST is able to deal with
transposition of tokens (in earlier approaches
transposition is seen as a number of single inser-
tions/deletions rather than a single block move).
The GST algorithm performs a 1:1 matching
of tokens between two strings so that as much of
one token stream is covered with maximal length
substrings from the other (called tiles). In our
problem, we consider how much newspaper text
can be maximally covered by words from PA
copy. A minimum match length (MML) can be
used to avoid spurious matches (e.g. of 1 or
2 tokens) and the resulting similarity between
the strings can be expressed as a quantitative
similarity match or a qualitative list of common
substrings. Figure 1 shows the result of GST for
the example in Section 2.
Figure 1: Example GST results (MML=3)
2
As stories unfold, PA release copy with new, as well
as previous versions of the story
Given PA copy A, a newspaper text B and a
set of maximal matches, tiles, of a given length
between A and B, the similarity, gstsim(A,B),
is expressed as:
gstsim(A;B) =
P
i2tiles
length
i
j B j
(2)
4.3 Sentence alignment
In the past decade, various alignment algorithms
have been suggested for aligning multilingual
parallel corpora (Wu, 2000). These algorithms
have been used to map translation equivalents
across dierent languages. In this specic case,
we investigate whether alignment can map de-
rived texts (or parts of them) to their source
texts. PA copy may be subject to various
changes during text reuse, e.g. a single sen-
tence may derive from parts of several source
sentences. Therefore, strong correlations of sen-
tence length between the derived and source
sentences cannot be guaranteed. As a result,
sentence-length based statistical alignment al-
gorithms (Brown et al, 1991; Gale and Church,
1993) are not appropriate for this case. On the
other hand, cognate-based algorithms (Simard
et al, 1992; Melamed, 1999) are more e?cient
for coping with change of text format. There-
fore, a cognate-based approach is adopted for
the METER task. Here cognates are dened as
pairs of terms that are identical, share the same
stems, or are substitutable in the given context.
The algorithm consists of two principal com-
ponents: a comparison strategy and a scoring
function. In brief, the comparison works as fol-
lows (more details may be found in Piao (2001)).
For each sentence in the candidate derived text
DT the sentences in the candidate source text
ST are compared in order to nd the best match.
A DT sentence is allowed to match up to three
possibly non-consecutive ST sentences. The
candidate pair with the highest score (see be-
low) above a threshold is accepted as a true
alignment. If no such candidate is found, the
DT sentence is assumed to be independent of
the ST. Based on individual DT sentence align-
ments, the overall possibility of derivation for
the DT is estimated with a score ranging be-
tween 0 and 1. This score reects the propor-
tion of aligned sentences in the newspaper text.
Note that not only may multiple sentences in
the ST be aligned with a single sentence in the
DT, but also multiple sentences in the DT may
be aligned with one sentence in the ST.
Given a candidate derived sentence DS and
a proposed (set of) source sentence(s) SS, the
scoring function works as follows. Three basic
measures are computed for each pair of candi-
date DS and SS: SNG is the sum of lengths
of the maximum length non-overlapping shared
n-grams with n  2; SWD is the number of
matched words sharing stems not in an n-gram
guring in SNG; and SUB is the number of
substitutable terms (mainly synonyms) not g-
uring in SNG or SWD. Let L
1
be the length of
the candidate DS and L
2
the length of candidate
SS. Then, three scores PD, PS (Dice score) and
PV S are calculated as follows:
PSD =
SWD + SNG + SUB
L
1
PS =
2(SWD + SNG + SUB)
L
1
+ L
2
PSNG =
SNG
SWD + SNG + SUB
These three scores reect dierent aspects of
relations between the candidate DS and SS:
1. PSD: The proportion of the DS which is
shared material.
2. PS: The proportion of shared terms in DS
and SS. This measure prefers SS's which not
only contain many terms in the DS, but also
do not contain many additional terms.
3. PSNG: The proportion of matching n-
grams amongst the shared terms. This
measure captures the intuition that sen-
tences sharing not only words, but word se-
quences are more likely to be related.
These three scores are weighted and combined
together to provide an alignment metric WS
(weighted score), which is calculated as follows:
WS = ?
1
PSD+ ?
2
PS + ?
3
PSNG
where ?
1
+?
2
+?
3
= 1. The three weighting vari-
ables ?
i
(i = 1; 2; 3) have been determined empir-
ically and are currently set to: ?
1
= 0:85; ?
2
=
0:05; ?
3
= 0:1.
5 Reuse Classiers
To evaluate the previous approaches for measur-
ing text reuse at the document-level, we cast the
problem into one of a supervised learning task.
5.1 Experimental Setup
We used similarity scores as attributes for a ma-
chine learning algorithm and used the Weka 3.2
software (Witten and Frank, 2000). Because of
the small number of examples, we used tenfold
cross-validation repeated 10 times (i.e. 10 runs)
and combined this with stratication to ensure
approximately the same proportion of samples
from each class were used in each fold of the
cross-validation. All 769 newspaper texts from
the courts domain were used for evaluation and
randomly permuted to generate 10 sets. For
each newspaper text, we compared PA source
texts from the same story to create results in
the form: newspaper; class; score. These results
were ordered according to each set to create the
same 10 datasets for each approach thereby en-
abling comparison.
Using this data we rst trained ve single-
feature Naive Bayes classiers to do the ternary
classication task. The feature in each case was
a variant of one of the three similarity measures
described in Section 4, computed between the
two texts in the training set. The target classi-
cation value was the reuse classication category
from the corpus. A Naive Bayes classier was
used because of its success in previous classi-
cation tasks, however we are aware of its naive
assumptions that attributes are assumed inde-
pendent and data to be normally distributed.
We evaluated results using the F
1
-measure
(harmonic mean of precision and recall given
equal weighting). For each run, we calculated
the average F
1
score across the classes. The
overall average F
1
-measure scores were com-
puted from the 10 runs for each class (a single
accuracy measure would su?ce but the Weka
package outputs F
1
-measures). For the 10 runs,
the standard deviation of F
1
scores was com-
puted for each class and F
1
scores between all
approaches were tested for statistical signi-
cance using 1-way analysis of variance at a 99%
condence-level. Statistical dierences between
results were identied using Bonferroni analy-
sis
3
.
After examining the results of these single fea-
ture classiers, we also trained a \combined"
classier using a correlation-based lter ap-
proach (Hall and Smith, 1999) to select the com-
bination of features giving the highest classica-
tion score ( correlation-based ltering evaluates
all possible combinations of features). Feature
selection was carried for each fold during cross-
validation and features used in all 10 folds were
chosen as candidates. Those which occurred in
at least 5 of the 10 runs formed the nal selec-
tion.
We also tried splitting the training data into
various binary partitions (e.g. WD/PD vs. ND)
and training binary classiers, using feature se-
lection, to see how well binary classication
could be performed. Eskin and Bogosian (1998)
have observed that using cascaded binary clas-
siers, each of which splits the data well, may
work better on n-ary classication problems
than a single n-way classier. We then com-
puted how well such a cascaded classier should
perform using the best binary classier results.
5.2 Results
Table 1 shows the results of the single ternary
classiers. The baseline F
1
measure is based
upon the prior probability of a document falling
into one of the classes. The gures in parenthe-
sis are the standard deviations for the F
1
scores
across the ten evaluation runs. The nal row
shows the results for combining features selected
using the correlation-based lter.
Table 2 shows the result of training binary
classiers using feature selection to select the
most discriminating features for various binary
splits of the training data.
For both ternary and binary classiers feature
selection produced better results than using all
3
Using SPSS v10.0 for Windows.
Approach Category Avg F-measure
Baseline WD 0.340 (0.000)
PD 0.444 (0.000)
ND 0.216 (0.000)
total 0.333 (0.000)
3-gram WD 0.631 (0.004)
containment PD 0.624 (0.004)
ND 0.549 (0.005)
total 0.601 (0.003)
GST Sim WD 0.669 (0.004)
MML = 3 PD 0.633 (0.003)
ND 0.556 (0.004)
total 0.620 (0.002)
GST Sim WD 0.681 (0.003)
MML = 1 PD 0.634 (0.003)
ND 0.559 (0.008)
total 0.625 (0.004)
1-gram WD 0.718 (0.003)
containment PD 0.643 (0.003)
ND 0.551 (0.006)
total 0.638 (0.003)
Alignment WD 0.774 (0.003)
PD 0.624 (0.005)
ND 0.537 (0.007)
total 0.645 (0.004)
hapax WD 0.736 (0.003)
containment PD 0.654 (0.003)
ND 0.549 (0.010)
total 0.646 (0.004)
hapax cont. WD 0.756 (0.002)
1-gram cont. PD 0.599 (0.006)
alignment ND 0.629 (0.008)
(\combined") total 0.664 (0.004)
Table 1: A summary of classication results
possible features, with the one exception of the
binary classication between PD and ND.
5.3 Discussion
From Table 1, we nd that all classier results
are signicantly higher than the baseline (at
p < 0:01) and all dierences are signicant ex-
cept between hapax containment and alignment.
The highest F-measure for the 3-class problem
is 0.664 for the \combined" classier, which is
signicantly greater than 0.651 obtained with-
out. We notice that highest WD classication
is with alignment at 0.774, highest PD classi-
cation is 0.654 with hapax containment and
highest ND classication is 0.629 with combined
features. Using hapax containment gives higher
results than 1-gram containment alone and in
fact provides results as good as or better than
the more complex sentence alignment and GST
approaches.
Previous research by (Lyon et al, 2001) and
(Wise, 1996) had shown derived texts could be
distinguished using trigram overlap and tiling
with a match length of 3 or more, respectively.
Attributes Category Avg F
1
Correlation- alignment WD 0.942 (0.008)
based ND 0.909 (0.011)
lter total 0.926 (0.010)
alignment PD/ND 0.870 (0.003)
WD 0.770 (0.003)
total 0.820 (0.002)
alignment WD 0.778 (0.003)
PD 0.812 (0.002)
total 0.789 (0.002)
hapax cont. WD/PD 0.882 (0.002)
alignment ND 0.649 (0.007)
1-gram cont. total 0.763 (0.002)
1-gram PD 0.802 (0.002)
GST mml 3 ND 0.638 (0.007)
GST mml 1 total 0.720 (0.004)
alignment
GST mml 1 WD/ND 0.672 (0.002)
alignment PD 0.662 (0.003)
total 0.668 (0.003)
Table 2: Binary Classiers with feature selection
However, our results run counter to this be-
cause the highest classication scores are ob-
tained with 1-grams and an MML of 1, i.e. as
n or MML length increases, the F
1
scores de-
crease. We believe this results from two factors
which are characteristic of reuse in journalism.
First, since even ND texts are thematically sim-
ilar (same events being described) there is high
likelihood of coincidental overlap of ngrams of
length 3 or more (e.g. quoted speech). Secondly,
when journalists rewrite it is rare for them not
to vary the source.
For the intended application { helping the PA
to monitor text reuse { the cost of dierent mis-
classications is not equal. If the classier makes
a mistake, it is better that WD and ND texts are
mis-classied as PD, and PD as WD. Given the
dierence in distribution of documents across
classes where PD contains the most documents,
the classier will be biased towards this class
anyway as required. Table 3 shows the confu-
sion matrix for the combined ternary classier.
WD PD ND
WD 203 55 4
PD 79 192 70
ND 3 53 109
Table 3: Confusion matrix for combined ternary
classier
Although the overall F
1
-measure score is low
(0.664), mis-classication of both WD as ND
and ND as WD is also very low, as most mis-
classications are as PD. Note the high mis-
classication of PD as both WD and ND, re-
ecting the di?culty of separating this class.
From Table 2, we nd alignment is a selected
feature for each binary partition of the data.
The highest binary classication is achieved be-
tween the WD and ND classes using alignment
only, and the highest three scores show WD is
the easiest class to separate from the others.
The PD class is the hardest to isolate, reect-
ing the mis-classications seen in Table 3.
To predict how well a cascaded binary classi-
er will perform we can reason as follows. From
the preceding discussion we see that WD can
be separated most accurately; hence we choose
WD versus PD/ND as the rst binary classier.
This forces the second classier to be PD versus
ND. From the results in Table 2 and the follow-
ing equation to compute the F
1
measure for a
two-stage binary classier
WD + (PD=ND)(
PD+ND
2
)
2
we obtain an overall F
1
measure for ternary clas-
sication of 0.703, which is signicantly higher
than the best single stage ternary classier.
6 Conclusions
In this paper we have investigated text reuse in
the context of the reuse of news agency copy, an
area of theoretical and practical interest. We
present a conceptual framework in which we
measure reuse and based on which the METER
corpus has been constructed. We have presented
the results of using similarity scores, computed
using n-gram containment, Greedy String Tiling
and an alignment algorithm, as attributes for
a supervised learning algorithm faced with the
task of learning how to classify newspaper sto-
ries as to whether they are wholly, partially or
non-derived from a news agency source. We
show that the best single feature ternary clas-
sier uses either alignment or simple hapax con-
tainment measures and that a cascaded binary
classier using a combination of features can
outperform this.
The results are lower than one might like,
and reect the problems of measuring journalis-
tic reuse, stemming from complex editing trans-
formations and the high amount of verbatim
text overlapping as a result of thematic simi-
larity and \expected" similarity due to, e.g., di-
rect/indirect quotes. Given the relative close-
ness of results obtained by all approaches we
have considered, we speculate that any compar-
ison method based upon lexical similarity will
probably not improve classication results by
much. Perhaps improved performance at this
task may possible by using more advanced nat-
ural language processing techniques, e.g. better
modeling of the lexical variation and syntactic
transformation that goes on in journalistic reuse.
Nevertheless the results we have obtained are
strong enough in some cases (e.g. wholly derived
texts can be identied with > 80% accuracy) to
begin to be exploited.
In summary measuring text reuse is an excit-
ing new area that will have a number of appli-
cations, in particular, but not limited to, mon-
itoring and controlling the copy produced by a
newswire.
7 Future work
We are adapting the GST algorithm to deal with
simple rewrites (e.g. synonym substitution) and
to observe the eects of rewriting upon nding
longest common substrings. We are also experi-
menting using the more detailed METER corpus
lexical-level annotations to investigate how well
the GST and ngrams approaches can identify
reuse at this level.
A prototype browser-based demo of both the
GST algorithm and alignment program, allow-
ing users to test arbitrary text pairs for simi-
larity, is now available
4
and will continue to be
enhanced.
Acknowledgements
The authors would like to acknowledge the
UK Engineering and Physical Sciences Re-
search Council for funding the METER project
(GR/M34041). Thanks also to Mark Hepple for
helpful comments on earlier drafts.
4
See http://www.dcs.shef.ac.uk/nlp/meter.
References
P.F. Brown, J.C. Lai, and R.L. Mercer. 1991. Aligning
sentences in parallel corpora. In Proceedings of the
29th Annual Meeting of the Assoc. for Computational
Linguistics, pages 169{176, Berkeley, CA, USA.
P Clough. 2000. Plagiarism in natural and programming
languages: An overview of current tools and technolo-
gies. Technical Report CS-00-05, Dept. of Computer
Science, University of She?eld, UK.
E. Eskin and M. Bogosian. 1998. Classifying text docu-
ments using modular categories and linguistically mo-
tivated indicators. In AAAI-98 Workshop on Learning
for Text Classication.
H. Evans. 1972. Essential English for Journalists, Edi-
tors and Writers. Pimlico, London.
S. Finlay. 1999. Copycatch. Master's thesis, Dept. of
English. University of Birmingham.
R. Gaizauskas, J. Foster, Y. Wilks, J. Arundel,
P. Clough, and S. Piao. 2001. The meter corpus:
A corpus for analysing journalistic text reuse. In Pro-
ceedings of the Corpus Linguistics 2001 Conference,
pages 214|223.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpus. Computational Lin-
guistics, 19:75{102.
M.A. Hall and L.A. Smith. 1999. Feature selection for
machine learning: Comparing a correlation-based l-
ter approach to the wrapper. In Proceedings of the
Florida Articial Intelligence Symposium (FLAIRS-
99), pages 235{239.
C. Lyon, J. Malcolm, and B. Dickerson. 2001. Detecting
short passages of similar text in large document collec-
tions. In Conference on Empirical Methods in Natural
Language Processing (EMNLP2001), pages 118{125.
K. McKeown and H. Jing. 1999. The decomposition of
human-written summary sentences. In SIGIR 1999,
pages 129{136.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics, pages
107{130.
Scott S.L. Piao. 2001. Detecting and measuring text
reuse via aligning texts. Research Memorandum
CS-01-15, Dept. of Computer Science, University of
She?eld.
M. Simard, G. Foster, and P. Isabelle. 1992. Using
cognates to align sentences in bilingual corpora. In
Proceedings of the 4th Int. Conf. on Theoretical and
Methodological Issues in Machine Translation, pages
67{81, Montreal, Canada.
M. Wise. 1996. Yap3: Improved detection of similarities
in computer programs and other texts. In Proceedings
of SIGCSE'96, pages 130{134, Philadelphia, USA.
I.H. Witten and E. Frank. 2000. Datamining - practi-
cal machine learning tools and techniques with Java
implementations. Morgan Kaufmann.
D. Wu. 2000. Alignment. In R. Dale and H. Moisl and
H. Somers (eds.), A Handbook of Natural Language
Processing, pages 415{458. New York: Marcel Dekker.
Multilingual interactive experiments with Flickr
Paul Clough
Department of
Information Studies
University of Sheffield
Sheffield, UK
p.d.clough@sheffield.ac.uk
Julio Gonzalo
Departamento de Lenguajes
y Sistemas Informa?ticos
UNED
Madrid, Spain
julio@lsi.uned.es
Jussi Karlgren
Swedish Institute of
Computer Science
Stockholm
Sweden
jussi@sics.se
Abstract
This paper presents a proposal for iCLEF
2006, the interactive track of the CLEF
cross-language evaluation campaign. In
the past, iCLEF has addressed applications
such as information retrieval and ques-
tion answering. However, for 2006 the
focus has turned to text-based image re-
trieval from Flickr. We describe Flickr, the
challenges this kind of collection presents
to cross-language researchers, and suggest
initial iCLEF tasks.
1 Information Retrieval Evaluation by
User Experiment
Information retrieval systems, especially text re-
trieval systems, have benefited greatly from a
fairly strict and straight-laced evaluation scheme,
which enables system designers to run tests on
versions of their system using a test collection of
pre-assessed data. These relevance-oriented ex-
periments shed light on comparative system per-
formance and enable both introduction of new al-
gorithms and incremental optimization. However,
batch-oriented system evaluation based on large
amounts of data, abstracted away from situational
constraints, variation in usage, and interactiveness
issues only addresses some of the bottlenecks to
build a successful system.
The CLEF1 Interactive Track (iCLEF2) is de-
voted to the comparative study of user inclusive
cross-language search strategies. Over the past
5 years, iCLEF has studied three cross-language
search tasks: retrieval of documents, answers and
1http://www.clef-campaign.org/
2http://nlp.uned.es/iCLEF/
annotated images (Gonzalo and Oard, 2002; Gon-
zalo et al, 2005). All tasks involve the user in-
teracting with information systems in a language
different from that of the document collection.
Although iCLEF experiments continue produc-
ing interesting research results, which may have
a substantial impact on the way effective cross-
language search assistants are built, participation
in this track has remained low across the five years
of existence of the track. Interactive studies, how-
ever, remain as a recognized necessity in most
CLEF tracks.
Therefore, to encourage greater participation in
2006 our focus has turned to FLICKR3, a large-
scale, web-based image database with the poten-
tial for offering both challenging and realistic mul-
tilingual search tasks for interactive experiments.
Our aim in selecting a primarily non-textual tar-
get to study textual retrieval is based on some of
the multi-lingual and dynamic characteristics of
FLICKR. We will outline them below.
2 The Flickr system
The majority of Web image search is text-based
and the success of such approaches often de-
pends on reliably identifying relevant text associ-
ated with a particular image. FLICKR is an on-
line tool for managing and sharing personal pho-
tographs and currently contains over five million
freely accessible images. These are available via
the web, updated daily by a large number of users
and available to all web users (users can access
FLICKR for free, although limited to the upload of
20MB of photos per month).
3http://www.flickr.com/
70
2.1 Photographs in the collection
It is estimated that the complete FLICKR database
contains 37 million photos with approximately
200,000 images added daily by 1.2 million mem-
bers4. FLICKR provides both private and pub-
lic image storage, and photos which are shared
(around 5 million) can be protected under a Cre-
ative Commons (CC) licensing5 agreement (an al-
ternative to full copyright). Images from a wide
variety of topics can be accessed through FLICKR,
including people, places, landscapes, objects, ani-
mals and events. This makes the collection a rich
resource for image retrieval research.
2.2 Annotations
In FLICKR, photos are annotated by authors with
freely chosen keywords in a naturally multilingual
manner: most authors use keywords in their native
language; some combine more than one language.
In addition, photographs have titles, descriptions,
collaborative annotations, and comments in many
languages. Figure 5 provides an example photo
with multilingual annotations; Figure 5 shows
what the query ?cats? retrieves from the database,
compared with what the query ?chats? retrieves.
Annotations are used by the authors to organize
their images, and by any user to search on. Key-
words assigned to the images can include place
names and subject matter, and photos can also
be submitted to online discussion groups. This
provides additional metadata to the image which
can also be used for retrieval. An explore util-
ity provided by FLICKR makes use of this user-
generated data (plus other information such as
Clickthroughs) to define an ?interestingness? view
of images6.
3 Flickr at iCLEF 2006
Many images are accompanied by text, enabling
the use of both text and visual features for image
retrieval and its evaluation (Mu?ller et al, 2006,
see e.g.). Images are naturally language indepen-
dent and often successfully retrieved with asso-
ciated texts. This has been explored as part of
ImageCLEF (Clough et al, 2005) for areas such
as information access to medical images and his-
toric photographs. The way in which users search
4These figures are accurate as of October 2005:
http://www.wired.com/news/ebiz/0,1272,68654,00.html
5http://creativecommons.org/image/flickr,
http://flickr.com/creativecommons/
6http://www.flickr.com/explore/interesting
for images provides an interesting application for
user-centered design and evaluation. As an iCLEF
task, searching for images from FLICKR presents
a new multilingual challenge which, to date, has
not been explored. Challenges include:
? Different types of associated text, e.g. key-
words, titles, comments and description
fields.
? Collective classification and annotation us-
ing freely selected keywords (known as folk-
sonomies) resulting in non-uniform and sub-
jective categorization of images.
? Annotations in multiple languages.
Given the multilingual nature of the FLICKR
annotations, translating the user?s search request
would provide the opportunity of increasing the
number of images found and make more of the
collection accessible to a wider range of users
regardless of their language skills. The aim of
iCLEF using FLICKR will be to determine how
cross-language technologies could enhance ac-
cess, and explore the user interaction resulting
from this.
4 Proposed tasks
For iCLEF, participants to this evaluation cam-
paign will be provided with the following:
? A subset of the Flickr collection including an-
notations and photographs7.
? Example (realistic) search tasks. Ideally
these search tasks will reflect real user needs
which could be derived from log files, studies
or similar retrieval tasks.
? A framework in which to run an evaluation.
5 Summary
Flickr will allow us to create an extremely in-
teresting interactive task based on truly hetero-
geneous annotations (that will in turn hopefully
attract more participants). Using images from
within a Web environment is a realistic and con-
temporary search challenge and allows many im-
portant research questions to be addressed from
7We are currently in negotiations with Yahoo! (owners
of Flickr) and Flickr to provide researchers with legitimate
access to a subset of the collection.
71
a quickly developing field. User-centered studies
are required within both text and image retrieval,
but are often neglected as they require more effort
and time from participating groups than a system-
centered comparison that can often be run with-
out human intervention. Still, user-centered eval-
uation cannot be replaced and the influence of the
user on the results is in general stronger than the
influence of the system itself.
References
Paul Clough, Henning Mu?ller, and Mark Sanderson.
2005. The clef 2004 cross language image retrieval
track. In Carol Peters, Paul Clough, Julio Gon-
zalo, Gareth Jones, Michael Kluck, and Bernardo
Magnini, editors, Multilingual Information Access
for Text, Speech and Images: Results of the Fifth
CLEF Evaluation Campaign, number 3491/2005 in
Lecture Notes in Computer Science, pages 597?613.
Springer, Heidelberg, Germany.
Julio Gonzalo and Doug Oard. 2002. The clef
2002 interactive track. In Advances in Cross-
Language Information Retrieval, number 2785 in
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
Julio Gonzalo, Paul Clough, and A Vallin. 2005.
Overview of the clef 2005 interactive track. In
Working notes of the CLEF workshop, Vienna, Aus-
tria, September.
Henning Mu?ller, Paul Clough, William Hersh, Thomas
Deselaers, Thomas Lehmann, and Antoine Geiss-
buhler. 2006. Using heterogeneous annotation and
visual information for the benchmarking of image
retrieval systems. In SPIE conference Photonics
West, Electronic Imaging, special session on bench-
marking image retrieval systems, San Diego, Febru-
ary.
72
Figure 1: Example multilingual annotations in Flickr.
Figure 2: Retrieval of ?cats? (left) and ?chats? (right).
73
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PATHS: A System for Accessing Cultural Heritage Collections
Eneko Agirre?, Nikolaos Aletras?, Paul Clough?, Samuel Fernando?,
Paula Goodale?, Mark Hall?, Aitor Soroa? and Mark Stevenson?
(?) IXA NLP Group, University of the Basque Country
Manuel Lardizabal, 1, 20.018 Donostia, Basque Country
(?) Department of Computer Science, Sheffield University
211 Portobello, Sheffield S1 4DP, United Kingdom
Abstract
This paper describes a system for navigat-
ing large collections of information about
cultural heritage which is applied to Eu-
ropeana, the European Library. Euro-
peana contains over 20 million artefacts
with meta-data in a wide range of Euro-
pean languages. The system currently pro-
vides access to Europeana content with
meta-data in English and Spanish. The pa-
per describes how Natural Language Pro-
cessing is used to enrich and organise this
meta-data to assist navigation through Eu-
ropeana and shows how this information is
used within the system.
1 Introduction
Significant amounts of information about cultural
heritage has been digitised in recent years and is
now easily available through online portals. How-
ever, this vast amount of material can also be over-
whelming for many users since they are provided
with little or no guidance on how to find and inter-
pret this information. Potentially useful and rel-
evant content is hidden from the users who are
typically offered simple keyword-based searching
functionality as the entry point into a cultural her-
itage collection. The situation is very different
within traditional mechanisms for viewing cultural
heritage (e.g. museums) where artefacts are or-
ganised thematically and users guided through the
collection.
This paper describes a system that allows users
to explore large cultural heritage collections. Nav-
igation is based around the metaphor of pathways
(or trails) through the collection, an approach that
has been widely explored as an alternative to stan-
dard keyword-based search (Furuta et al, 1997;
Reich et al, 1999; Shipman et al, 2000; White and
Huang, 2010). Pathways are sets of artefacts or-
ganised around a theme which form access points
to the collection.
Pathways are a useful way to access informa-
tion about cultural heritage. Users accessing these
collections are often unfamiliar with their content,
making keyword-based search unsuitable since
they are unable to formulate appropriate queries
(Wilson et al, 2010). Non-keyword-based search
interfaces have been shown to be suitable for ex-
ploratory search (Marchionini, 2006). Pathways
support this exploration by echoing the organised
galleries and guided tours found in museums.
2 Related Work
Heitzman et al (1997) describe the ILEX system
which acts as a guide through the jewellery col-
lection of the National Museum of Scotland. The
user explores the collection through a set of web
pages which provide descriptions of each artefact
that are personalised for each user. The system
makes use of information about the artefacts the
user has viewed to build up a model of their in-
terests and uses this to customise the descriptions
of each artefact and provide recommendations for
further artefacts in which they may be interested.
Grieser et al (2007) also explore providing rec-
ommendations based on the artefacts a user has
viewed so far. They make use of a range of tech-
niques including language modelling, geospatial
modelling and analysis of previous visitors? be-
haviour to provide recommendations to visitors to
the Melbourne Museum.
Grieser et al (2011) explore methods for de-
termining the similarity between museum arte-
facts, commenting that this is useful for navigation
through these collections and important for per-
sonalisation (Bowen and Filippini-Fantoni, 2004;
O?Donnell et al, 2001), recommendation (Bohn-
ert et al, 2009; Trant, 2009) and automatic tour
generation (Finkelstein et al, 2002; Roes et al,
2009). They also use exhibits from Melbourne
151
Museum and apply a range of approaches to deter-
mine the similarity between them, including com-
paring descriptions and measuring physical dis-
tance between them in the museum.
These approaches, like many of the systems
that have been developed for online access to cul-
tural heritage (e.g. (Hage et al, 2010)), are based
around virtual access to a concrete physical space
(i.e. a museum). They often provide tours which
are constrained by the physical layout of the mu-
seum, such as virtual museum visits. However,
these approaches are less suitable for unstructured
collections such as databases of cultural heritage
artefacts collected from multiple institutions or
artefacts not connected with existing physical pre-
sentation (e.g. in a museum). The PATHS sys-
tem is designed for these types of collections and
makes use of natural language analysis to sup-
port navigation. In particular, similarity between
artefacts is computed automatically (see Section
4.1), background information automatically added
to artefact descriptions (see Section 4.2) and a hi-
erarchy of artefacts generated (see Section 4.3).
3 Cultural Heritage Data
The PATHS system has been applied to data from
Europeana1. This is a web-portal to collections
of cultural heritage artefacts provided by a wide
range of European institutions. Europeana cur-
rently provides access to over 20 million artefacts
including paintings, films, books, archival records
and museum objects. The artefacts are provided
by around 1,500 institutions which range from
major institutions, including the Rijksmuseum in
Amsterdam, the British Library and the Louvre,
to smaller organisations such as local museums.
It therefore contains an aggregation of digital con-
tent from several sources and is not connected with
any one physical museum.
The PATHS system makes use of three collec-
tions from Europeana. The first of these con-
tains artefacts from content providers in the United
Kingdom which has meta-data in English. The
artefacts in the remaining two collections come
from institutions in Spain and have meta-data in
Spanish.
CultureGrid Culture Grid2 is a digital content
provider service from the Collection Trust3.
1http://www.europeana.eu
2http://www.culturegrid.org.uk
3http://www.collectionstrust.org.uk
It contains information about over one mil-
lion artefacts from 40 different UK content
providers such as national and regional mu-
seums and libraries.
Cervantes Biblioteca Virtual Miguel De Cer-
vantes4 contains digitalised Spanish text in
various formats. In total, the online library
contains about 75,000 works from a range of
periods in Spanish history.
Hispana The Biblioteca Nacional de Espan?a5
contains information about a diverse set of
content including text and drawings. The ma-
terial is collected from different providers in
Spain including museums and libraries.
Europeana stores metadata for each artefact in
an XML-based format which includes information
such as its title, the digital format, the collection,
the year of creation and also a short description of
each artefact. However, this meta-data is created
by the content providers and varies significantly
across artefacts. Many of the artefacts have only
limited information associated with them, for ex-
ample a single word title. In addition, the content
providers that contribute to Europeana use differ-
ent hierarchical structures to organise their collec-
tions (e.g. Library of Congress Subject Headings6
and the Art and Architecture Thesaurus7), or do
not organise their content into any structure. Con-
sequently the various hierarchies that are used in
Europeana only cover some of the artefacts and
are not compatible with each other.
3.1 Filtering Data
Analysis of the artefacts in these three collections
revealed that many have short and uninformative
titles or lack a description. This forms a challenge
to language processing techniques since the arte-
fact?s meta-data does not contain enough informa-
tion to model it accurately.
The collections were filtered by removing any
artefacts that have no description and have either
fewer than four words in their title or have a title
that is repeated more than 100 times in the col-
lection. Table 1 shows the number of artefacts
in each of the Europeana collections before and
4http://www.cervantesvirtual.com
5http://www.bne.es
6http://authorities.loc.gov/
7http://www.getty.edu/research/tools/
vocabularies/aat/
152
after this filter has been applied. Applying the
heuristic leads to the removal of around 31% of the
artefacts, although the number varies significantly
across the collections with 61% of the artefacts in
CultureGrid being removed and only 1% of those
in Hispana.
Collection Lang. Total Filtered
CultureGrid Eng. 1,207,781 466,958
Hispana Sp. 1,235,133 1,219,731
Cervantes Sp. 19,278 14,983
2,462,192 1,701,672
Table 1: Number of artefacts in Europeana collec-
tions before and after filtering
4 Data Processing
A range of pre-preprocessing steps were carried
out on these collections to provide additional in-
formation to support navigation in the PATHS sys-
tem.
4.1 Artefact Similarity
We begin by computing the similarity between
the various artefacts in the Europeana collections.
This information is useful for navigation and rec-
ommendation but is not available in the Europeana
collections since they are drawn from a diverse
range of sources.
Similarity is computed using an approach de-
scribed by Aletras et al (2012). in which the top-
ics generated from each artefact?s metadata using
a topic model are compared. Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) is a widely used
type of topic model in which documents can be
viewed as probability distributions over topics, ?.
The similarity between a pair of documents can be
estimated by comparing their topic distributions.
This is achieved by viewing each distribution as
a vector of probabilities and then computing the
cosine of the angle between them:
sim(a, b) =
~?a.~?b
|~?a| ? | ~?b|
(1)
where ~?a is the vector created from the probability
distribution generated by LDA for document a.
This approach is evaluated using a set of 295
pairs of artefacts for which human judgements
of similarity were obtained using crowdsourcing
(Aletras et al, 2012). Pearson correlation between
the similarity scores and human judgements was
0.53.
The similarity between all the artefacts in the
collection is computed in a pairwise fashion. The
25 artefacts with the highest score are retained for
each artefact.
4.2 Background Links
The metadata associated with Europeana artefacts
is often very limited. Consequently links to rele-
vant articles in Wikipedia were added to each the
meta-data of each artefact using Wikipedia Miner
(Milne and Witten, 2008) to provide background
information. In addition to the link, Wikipedia
Miner returns a confidence value between 0 and
1 for each link based on the context of the item.
The accuracy of the links added by Wikipedia
Miner were evaluated using the meta-data associ-
ated with 21 randomly selected artefacts. Three
annotators analysed the links added and found that
a confidence value of 0.5 represented a good bal-
ance between accuracy and coverage. See Fer-
nando and Stevenson (2012) for further details.
4.3 Hierarchies
The range of hierarchies used by the various col-
lections that comprise the Europeana collection
make navigation difficult (see Section 3). Con-
sequently, the Wikipedia links added to the arte-
fact meta-data were used to automatically gener-
ate hierarchies that the cover the entire collection.
These hierarchies are used by the PATHS system
to assist browsing and exploration.
Two approaches are used to generate hierarchies
of Europeana artefacts (WikiFreq and WikiTax).
These are combined to generate the WikiMerge hi-
erarchy which is used in the PATHS system.
WikiFreq uses link frequencies across the en-
tire collection to organise the artefacts. The first
stage in the hierarchy generation process is to
compute the frequency with which each linked
Wikipedia article appears in the collection. The
links in each artefact are these analysed to con-
struct a hierarchy consisting of Wikipedia articles.
The links in the meta-data associated with each
artefact are ordered based on their frequency in the
entire collection and that set of links then inserted
into the hierarchy. For example, if the set of or-
dered links for an artefact is a1, a2, a3 ? ? ? an then
the artefact is then inserted into the hierarchy un-
der the branch a1 ? a2 ? a3 ? ? ? ? an, with
a1 at the top level in the tree and the artefact ap-
pearing under the node an. If this branch does not
already exist in the tree then it is created.
153
The hierarchy is pruned to removing nodes with
fewer than 20 artefacts in them. In addition, if a
node has more than 20 child nodes, only the 20
most frequent are used.
WikiTax uses the Wikipedia Taxonomy
(Ponzetto and Strube, 2011), a taxonomy derived
from Wikipedia categories. Europeana artefacts
are inserted into this taxonomy using the links
added by Wikipedia Miner with each artefact
being added to the taxonomy for all categories
listed in the links. This leads to a taxonomy in
which artefacts can occur in multiple locations.
Each approach was used to generate hierarchies
from the Europeana collections. The resulting hi-
erarchies were evaluated via online surveys, see
Fernando et al (2012) for further details. It was
found that WikiFreq performed well at placing
items into the correct location in the taxonomy and
grouping together similar items under the same
node. However, the overall structure of WikiTax
was judged to be more coherent and comprehensi-
ble.
WikiMerge combines combines WikiFreq and
WikiTax. WikiFreq is used to link each artefact
to Wikipedia articles a1 . . . an, but only the link
to the most specific article, an, is retained. The
an articles are linked to their parent WikiTax top-
ics based on the Wikipedia categories the articles
belong to. The resulting hierarchy is pruned re-
moving all WikiTax topics that do not have a Wik-
iFreq child or have only one child topic. Finally
top-level topics in the combined hierarchy are then
linked to their respective Wikipedia root node.
The resulting WikiMerge hierarchy has Wik-
iFreq topics as its leaves and WikiTax topics as
its interior and root nodes. Experiments showed
that this approach was successful in combining
the strengths of the two methods (Fernando et al,
2012).
5 The PATHS System
The PATHS system provides access to the Euro-
peana collections described in Section 3 by mak-
ing use of the additional information generated us-
ing the approaches described in Section 4. The in-
terface of the PATHS system has three main areas:
Paths enables users to navigate via pathways (see
Section 5.1).
Search supports discovery of both collection arte-
facts and pathways through keyword search
(see Section 5.2).
Explore enables users to explore the collections
using a variety of types of overview (see Sec-
tion 5.3).
5.1 Paths Area
This area provides users with access to Europeana
through pathways or trails. These are manually
generated sets of artefacts organised into a tree
structure which are designed to showcase the con-
tent available to the user in an organised way.
These can be created by users and can be pub-
lished for others to follow. An example path-
way on the topic ?railways? is shown in Figure
1. A short description of the pathway?s content is
shown towards the top of the figure and a graphical
overview of its contents at the bottom.
Figure 1: Example pathway on the topic ?rail-
ways?
Figure 2 shows as example artefact as displayed
in the system. The example artefact is a portrait
of Catherine the Great. The left side of the figure
shows information extracted directly from the Eu-
ropeana meta-data for this artefact. The title and
textual description are shown towards the top left
together with a thumbnail image of the artefact.
Other information from the meta-data is shown be-
neath the ?About this item? heading. The right
side of the figure shows additional information
Figure 2: Example artefact displayed in system in-
terface. Related artefacts and background links are
displayed on right hand side
154
Figure 3: Example visualisations of hierarchy: thesaurus view (top left), tag cloud (top right), map views
(bottom)
about the artefact generated using the approaches
described in Sections 4.1 and 4.2. Related arte-
facts are shown to the user one at a time, click-
ing on the thumbnail image leads to the equivalent
page for the related artefact. Below this are links
to the Wikipedia articles that are identified in the
text of the article?s title and description.
5.2 Search Area
This area allows users to search for artefacts and
pathways using standard keyword search imple-
mented using Lucene (McCandless et al, 2010).
5.3 Explore Area
The system provides a variety of ways to view
the hierarchies generated using the approach de-
scribed in Section 4.3. Figure 3 shows how these
are displayed for a section of the hierarchy with
the label ?Society?. The simplest view (shown in
the top left of Figure 3) is a thesaurus type view
in which levels of the hierarchy are represented by
indentation. The system also allows levels of the
hierarchy to be viewed as a tag cloud (top right of
Figure 3). The final representation of the hierar-
chy is as a map, shown in the bottom of Figure 3.
In this visualisation categories in the hierarchy are
represented as ?islands? on the map. Zooming in
on the map provides more detail about that area of
the hierarchy.
6 Summary and Future Developments
This paper describes a system for navigating Eu-
ropeana, an aggregation of collections of cultural
heritage artefacts. NLP analysis is used to organ-
ise the collection and provide additional informa-
tion. The results of this analysis are provided to
the user through an online interface which pro-
vides access to English and Spanish content in Eu-
ropeana.
Planned future development of this system in-
cludes providing recommendations and more per-
sonalised access. Similarity information (Sec-
tion 4.1) can be used to provide information from
which the recommendations can be made. Person-
alised access will make use of information about
individual users (e.g. from their browsing be-
haviour or information they provide about their
preferences) to generate individual views of Eu-
ropeana.
155
Online Demo
The PATHS system is available at
http://explorer.paths-project.eu/
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082
References
N. Aletras, M. Stevenson, and P. Clough. 2012. Com-
puting similarity between items in a digital library
of cultural heritage. Journal of Computing and Cul-
tural Heritage, 5(4):no. 16.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
F. Bohnert, D. Schmidt, and I. Zuckerman. 2009. Spa-
tial Process for Recommender Systems. In Proc. of
IJCAI 2009, pages 2022?2027, Pasadena, CA.
J. Bowen and S. Filippini-Fantoni. 2004. Personaliza-
tion and the Web from a Museum Perspective. In
Proc. of Museums and the Web 2004, pages 63?78.
Samuel Fernando and Mark Stevenson. 2012. Adapt-
ing Wikification to Cultural Heritage. In Proceed-
ings of the 6th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 101?106, Avignon, France.
Samuel Fernando, Mark Hall, Eneko Agirre, Aitor
Soroa, Paul Clough, and Mark Stevenson. 2012.
Comparing taxonomies for organising collections of
documents. In Proc. of COLING 2012, pages 879?
894, Mumbai, India.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Trans. on Information Systems, 20(1):116?131.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proc. of the Eighth ACM conference on Hypertext,
pages 167?176, New York, NY.
K. Grieser, T. Baldwin, and S. Bird. 2007. Dynamic
Path Prediction and Recommendation in a Museum
Environment. In Proc. of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 49?56, Prague, Czech Republic.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using Ontological and Document Similarity
to Estimate Museum Exhibit Relatedness. Journal
of Computing and Cultural Heritage, 3(3):1?20.
W.R. van Hage, N. Stash, Y. Wang, and L.M. Aroyo.
2010. Finding your way through the Rijksmuseum
with an adaptive mobile museum guide. In Proc. of
ESWC 2010, pages 46?59.
J. Heitzman, C. Mellish, and J. Oberlander. 1997. Dy-
namic Generation of Museum Web Pages: The In-
telligent Labelling Explorer. Archives and Museum
Informatics, 11(2):117?125.
G. Marchionini. 2006. Exploratory Search: from Find-
ing to Understanding. Comm. ACM, 49(1):41?46.
M. McCandless, E. Hatcher, and O. Gospodnetic.
2010. Lucene in Action. Manning Publications.
D. Milne and I. Witten. 2008. Learning to Link with
Wikipedia. In Proc. of CIKM 2008, Napa Valley,
California.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hy-
pertext generation system. Natural Language En-
gineering, 7:225?250.
S.P. Ponzetto and M. Strube. 2011. Taxonomy in-
duction based on a collaboratively built knowledge
repository. Artificial Intelligence, 175(9-10):1737?
1756.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
I. Roes, N. Stash, Y. Wang, and L. Aroyo. 2009. A
personalized walk through the museum: the CHIP
interactive tour guide. In Proc. of the 27th Interna-
tional Conference on Human Factors in Computing
Systems, pages 3317?3322, Boston, MA.
F. Shipman, R. Furuta, D. Brenner, C. Chung, and
H. Hsieh. 2000. Guided paths through web-based
collections: Design, experiences, and adaptations.
Journal of the American Society for Information Sci-
ence, 51(3):260?272.
J. Trant. 2009. Tagging, folksonomies and art mu-
seums: Early experiments and ongoing research.
Journal of Digital Information, 10(1).
R. White and J. Huang. 2010. Assessing the scenic
route: measuring the value of search trails in web
logs. In Proc. of SIGIR 2010, pages 587?594.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
156
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 54?58,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Detecting Text Reuse with Modified and Weighted N-grams
Rao Muhammad Adeel Nawab?, Mark Stevenson? and Paul Clough?
?Department of Computer Science and ?iSchool
University of Sheffield, UK.
{r.nawab@dcs, m.stevenson@dcs, p.d.clough@} .shef.ac.uk
Abstract
Text reuse is common in many scenarios and
documents are often based, at least in part, on
existing documents. This paper reports an ap-
proach to detecting text reuse which identifies
not only documents which have been reused
verbatim but is also designed to identify cases
of reuse when the original has been rewrit-
ten. The approach identifies reuse by compar-
ing word n-grams in documents and modifies
these (by substituting words with synonyms
and deleting words) to identify when text has
been altered. The approach is applied to a cor-
pus of newspaper stories and found to outper-
form a previously reported method.
1 Introduction
Text reuse is the process of creating new docu-
ment(s) using text from existing document(s). Text
reuse is standard practice in some situations, such as
journalism. Applications of automatic detection of
text reuse include the removal of (near-)duplicates
from search results (Hoad and Zobel, 2003; Seo and
Croft, 2008), identification of text reuse in journal-
ism (Clough et al, 2002) and identification of pla-
giarism (Potthast et al, 2011).
Text reuse is more difficult to detect when the
original text has been altered. We propose an ap-
proach to the identification of text reuse which is
intended to identify reuse in such cases. The ap-
proach is based on comparison of word n-grams, a
popular approach to detecting text reuse. However,
we also account for synonym replacement and word
deletion, two common text editing operations (Bell,
1991). The relative importance of n-grams is ac-
counted for using probabilities obtained from a lan-
guage model. We show that making use of modified
n-grams and their probabilities improves identifica-
tion of text reuse in an existing journalism corpus
and outperforms a previously reported approach.
2 Related Work
Approaches for identifying text reuse based on
word-level comparison (such as the SCAM copy de-
tection system (Shivakumar and Molina, 1995)) tend
to identify topical similarity between a pair of doc-
uments, whereas methods based on sentence-level
comparison (e.g. the COPS copy detection sys-
tem (Brin et al, 1995)) are unable to identify when
text has been reused if only a single word has been
changed in a sentence.
Comparison of word and character n-grams has
proven to be an effective method for detecting text
reuse (Clough et al, 2002; Ceden?o et al, 2009; Chiu
et al, 2010). For example, Ceden?o et al (2009)
showed that comparison of word bigrams and tri-
grams are an effective method for detecting reuse in
journalistic text. Clough et al (2002) also applied
n-gram overlap to identify reuse of journalistic text,
combining it with other approaches such as sentence
alignment and string matching algorithms. Chiu et
al. (2010) compared n-grams to identify duplicate
and reused documents on the web. Analysis of word
n-grams has also proved to be an effective method
for detecting plagiarism, another form of text reuse
(Lane et al, 2006).
However, a limitation of n-gram overlap approach
is that it fails to identify reuse when the original
54
text has been altered. To overcome this problem we
propose using modified n-grams, which have been
altered by deleting or substituting words in the n-
gram. The modified n-grams are intended to im-
prove matching with the original document.
3 Determining Text Reuse with N-gram
Overlap
3.1 N-grams Overlap (NG)
Following Clough et al (2002), the asymmetric con-
tainment measure (eqn 1) was used to quantify the
degree of text within a document (A) that is likely to
have been reused in another document (B).
scoren(A,B) =
?
ngram?B
count(ngram,A)
?
ngram?B
count(ngram,B)
(1)
where count(ngram,A) is the number of times
ngram appears in document A. A score of 1 means
that document B is contained in document A and a
score of 0 that none of the n-grams in B occur in A.
3.2 Modified N-grams
N-gram overlap has been shown to be useful for
measuring text reuse as derived texts typically share
longer n-grams (? 3 words). However, the approach
breaks down when an original document has been
altered. To counter this problem we applied vari-
ous techniques for modifying n-grams that allow for
word deletions (Deletions) and word substitutions
(WordNet and Paraphrases), two common text edit-
ing operations.
Deletions (Del) Assume that w1, w2, ...wn is an
n-gram. Then a set of modified n-grams can be cre-
ated by removing one of the w2 ... wn?1. The first
and last words in the n-gram are not removed since
they will also be generated as shorter n-grams. An
n-gram will generate n ? 2 deleted n-grams and no
deleted n-grams will be generated for unigrams and
bigrams.
Substitutions Further n-grams can be created by
substituting one of the words in an n-gram with one
of its synonyms from WordNet (WN). For words
with multiple senses we use synonyms from all
senses. Modified n-grams are created by substitut-
ing one of the words in the n-gram with one of its
synonyms from WordNet.
Similarly to the WordNet approach, n-grams can
be created by substituting one of the words with an
equivalent term from a paraphrase lexicon, which
we refer to as Paraphrases (Para). A paraphrase
lexicon was generated automatically (Burch, 2008)
and ten lexical equivalents (the default setting) pro-
duced for each word. Modified n-grams were cre-
ated by substituting one of the words in the n-gram
with one of the lexical equivalents.
3.3 Comparing Modified N-grams
The modified n-grams are applied in the text reuse
score by generating modified n-grams for the docu-
ment that is suspected to contain reused text. These
n-grams are then compared with the original docu-
ment to determine the overlap. However, the tech-
niques in Section 3.2 generate a large number of
modified n-grams which means that the number
of n-grams that overlap with document A can be
greater than the total number of n-grams in B, lead-
ing to similarity scores greater than 1. To avoid this
the n-gram overlap counts are constrained in a simi-
lar way that they are clipped in BLEU and ROUGE
(Papineni et al, 2002; Lin, 2004).
For each n-gram in B, a set of modified n-grams,
mod(ngram), is created.1 The count for an in-
dividual n-gram in B, exp count(ngram,B), can
be computed as the number of times any n-gram in
mod(ngram) occurs in A, see equation 2.
?
ngram? ?mod(ngram)
count(ngram?, A) (2)
However, the contribution of this count to the text
reuse score has to be bounded to ensure that the com-
bined count of the modified n-grams appearing in
A does not exceed the number of times the origi-
nal n-gram occurs in B. Consequently the text reuse
score, scoren(A,B), is computed using equation 3.
?
ngram
?B
min(exp count(ngram,A), count(ngram,B))
?
ngram?B
count(ngram,B)
(3)
3.4 Weighting N-grams
Probabilities of each n-gram, obtained using a lan-
guage model, are used to increase the importance of
1This is the set of n-grams that could have been created by
modifing an n-gram in B and includes the original n-gram itself.
55
rare n-grams and decrease the contribution of com-
mon ones. N-gram probabilities are computed us-
ing the SRILM language modelling toolkit (Stolcke,
2002). The score for each n-gram is computed as
its Information Content (Cover and Thomas, 1991),
ie. ?log(P ). When the language model (LM) is
applied the scores associated with each n-gram are
used instead of counts in equations 2 and 3.
4 Experiments
4.1 METER Corpus
The METER corpus (Gaizauskas et al, 2001) con-
tains 771 Press Association (PA) articles, some of
which were used as source(s) for 945 news stories
published by nine British newspapers.
These 945 documents are classified as Wholly De-
rived (WD), Partially Derived (PD) and Non De-
rived (ND). WD means that the newspaper article
is likely derived entirely from the PA source text;
PD reflects the situation where some of the newspa-
per article is derived from the PA source text; news
stories likely to be written independently of the PA
source fall into the category of ND. In our experi-
ments, the 768 stories from court and law reporting
were used (WD=285, PD=300, ND=183) to allow
comparison with Clough et al (2002). To provide a
collection to investigate binary classification we ag-
gregated the WD and PD cases to form a Derived set.
Each document was pre-processed by converting to
lower case and removing all punctuation marks.
4.2 Determining Reuse
The text reuse task aims to distinguish between lev-
els of text reuse, i.e. WD, PD and ND. Two versions
of a classification task were used: binary classifica-
tion distinguishes between Derived (i.e. WD ? PD)
and ND documents, and ternary classification distin-
guishes all three levels of reuse.
A Naive Bayes classifier (Weka version 3.6.1) and
10-fold cross validation were used for the experi-
ments. Containment similarity scores between all
PA source texts and news articles on the same story
were computed for word uni-grams, bi-grams, tri-
grams, four-grams and five-grams. These five simi-
larity scores were used as features. Performance was
measured using precision, recall and F1 measures
with the macro-average reported across all classes.
The language model (Section 3.4) was trained us-
ing 806,791 news articles from the Reuters Corpus
(Rose et al, 2002). A high proportion of the news
stories selected were related to the topics of enter-
tainment and legal reports to reflect the subjects of
the new articles in the METER corpus.
5 Results and Analysis
Tables 1 and 2 show the results of the binary
and ternary classification experiments respectively.
?NG? refers to the comparison of n-grams in each
document (Section 3.1), while ?Del?, ?WN? and
?Para? refer to the modified n-grams created us-
ing deletions, WordNet and paraphrases respectively
(Section 3.2). The prefix ?LM? (e.g. ?LM-NG?) in-
dicates that the n-grams are weighted using the lan-
guage model probability scores (Section 3.4).
For the binary classification task (Table 1) it can
be observed that including modified n-grams im-
proves performance. This improvement is observed
when each of the three types of modified n-grams
is applied individually, with a greater increase being
observed for the n-grams created using the WordNet
and paraphrase approaches. Further improvement is
observed when different types of modified n-grams
are combined with the best performance obtained
when all three types are used. All improvements
over the baseline approach (NG) are statistically
significant (Wilcoxon signed-rank test, p < 0.05).
These results demonstrate that the various types of
modified n-grams all contribute to identifying when
text is being reused since they capture different types
of rewrite operations.
In addition, performance consistently improves
when n-grams are weighted using language model
scores. The improvement is significant for all types
of n-grams. This demonstrates that the information
provided by the language model is useful in deter-
mining the relative importance of n-grams.
Several of the results are higher than those re-
ported by Clough et al (2002) (F1=0.763), despite
the fact their approach supplements n-gram overlap
with additional techniques such as sentence align-
ment and string search algorithms.
Results of the ternary classification task are
shown in Table 2. Results show a similar pattern
to those observed for the binary classification task
56
Approach P R F1
NG 0.836 0.706 0.732
LM-NG 0.846 0.722 0.746
Del 0.851 0.745 0.767
LM-Del 0.858 0.765 0.785
WN 0.876 0.801 0.817
LM-WN 0.879 0.810 0.825
Para 0.884 0.821 0.834
LM-Para 0.888 0.831 0.843
Del+WN 0.889 0.835 0.847
LM-Del+WN 0.884 0.848 0.855
Del+Para 0.892 0.841 0.853
LM-Del+Para 0.896 0.849 0.860
WN+Para 0.894 0.848 0.858
LM-WN+Para 0.896 0.865 0.871
Del+WN+Para 0.897 0.856 0.865
LM-Del+WN+Para 0.903 0.876 0.882
(Clough et al, 2002) ? ? 0.763
Table 1: Results for binary classification
and the best result is also obtained when all three
types of modified n-grams are included and n-grams
are weighted with probability scores. Once again
weighting n-grams with language model scores im-
proves results for all types of n-gram and this im-
provement is significant. Results for several types of
n-gram are also better than those reported by Clough
et al (2002) (F1=0.664).
Results for all approaches are lower for the
ternary classification. This is because the binary
classification task involves distinguishing between
two classes of documents which are relatively dis-
tinct (derived and non-derived) while the ternary
task divides the derived class into two (WD and PD)
which are more difficult to separate (see Table 3
showing confusion matrix for the approach which
gave best results for ternary classification).
6 Conclusion
This paper describes an approach to the analysis of
text reuse which is based on comparison of n-grams.
This approach is augmented by modifying the n-
grams in various ways and weighting them with
probabilities derived from a language model. Evalu-
ation is carried out on a standard data set containing
examples of reused journalistic texts. Making use of
Approach P R F1
NG 0.596 0.557 0.551
LM-NG 0.615 0.579 0.574
Del 0.612 0.584 0.579
LM-Del 0.633 0.611 0.606
WN 0.644 0.636 0.631
LM-WN 0.649 0.640 0.635
Para 0.662 0.653 0.647
LM-Para 0.669 0.659 0.654
Del+WN 0.655 0.649 0.643
LM-Del+WN 0.668 0.656 0.650
Del+Para 0.665 0.658 0.652
LM-Del+Para 0.661 0.662 0.655
WN+Para 0.668 0.661 0.655
LM-WN+Para 0.680 0.675 0.668
Del+WN+Para 0.669 0.666 0.660
LM-Del+WN+Para 0.688 0.689 0.683
(Clough et al, 2002) ? ? 0.664
Table 2: Results for ternary classification
Classified as WD PD ND
WD 139 94 14
PD 57 206 54
ND 1 13 191
Table 3: Confusion matrix when ?LM-Del+WN+Para?
approach used for ternary classification
modified n-grams with appropriate weights is found
to improve performance when detecting text reuse
and the approach described here outperforms an ex-
isting approach. In future we plan to experiment
with other methods for modifying n-grams and also
to apply this approach to other types of text reuse.
Acknowledgments
This work was funded by the COMSATS Institute
of Information Technology, Islamabad, Pakistan un-
der the Faculty Development Program (FDP) and a
Google Research Award.
References
Alberto B. Ceden?o, Paolo Rosso, and Jose M. Bened
2009. Reducing the Plagiarism Detection Search
Space on the basis of the Kullback-Leibler Distance
Proceedings of CICLing-09, 523?534.
57
Allan Bell 1991. The Language of News Media. Black-
well.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
901?904.
Chin-Yew Lin. 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Proceedings of the ACL-
04 Workshop, 74?81.
Chris Callison-Burch. 2008. Syntactic Constraints on
Paraphrases Extracted from Parallel Corpora. In Pro-
ceedings of EMNLP?08, 196?205.
Jangwon Seo and W. Bruce Croft. 2008. Local Text
Reuse Detection. In Proceedings of SIGIR?08, 571?
578. In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, 571?578.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
J. Zhu. 2002. Bleu: A Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL?02, 311?318.
Martin Potthast, Andreas Eiselt, Alberto Barro?n-Ceden?o,
Benno Stein and Paolo Rosso. 2011. Overview of the
3rd International Competition on Plagiarism Detec-
tion. Notebook Papers of CLEF 11 Labs and Work-
shops.
Narayanan Shivakumar and Hector G. Molina. 1995.
SCAM: A Copy Detection Mechanism for Digital Doc-
uments. Proceedings of the 2nd Annual Conference
on the Theory and Practice of Digital Libraries, Texas,
USA.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. Measuring Text Reuse. In Pro-
ceedings of ACL?02, Philadelphia, USA, 152?159.
Peter C. R. Lane, Caroline M. Lyon, and James A. Mal-
colm. 2006. Demonstration of the Ferret plagiarism
detector. Proceedings of the 2nd International Plagia-
rism Conference, Newcastle, UK.
Robert Gaizauskas, Jonathan Foster, Yorick Wilks, John
Arundel, Paul Clough, and Scott S.L. Piao. 2001. The
METER Corpus: A Corpus for Analysing Journalistic
Text Reuse. In Proceedings of the Corpus Linguistics
Conference, 214?223.
Sergey Brin, James Davis and Hector G. Molina. 1995.
Copy Detection Mechanisms for Digital Documents.
Proceedings ACM SIGMOD?95, 398?409.
Stanford Chiu, Ibrahim Uysal, Bruce W. Croft. 2010.
Evaluating text reuse discovery on the web. In Pro-
ceedings of the third symposium on Information inter-
action in context, 299?304.
Thomas M. Cover, Joy A. Thomas. 1991. Elements of
Information Theory. Wiley, New York, USA.
Timothy C. Hoad and Justin Zobel. 2003. Methods
for Identifying Versioned and Plagiarized Documents.
Journal of the American Society for Information Sci-
ence and Technology, 54(3):203?215.
Tony Rose, Mark Stevenson, Miles Whitehead. 2002.
The Reuters Corpus Volume 1 - from Yesterday?s news
to tomorr ow?s language resources. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC-02), 827?832.
58
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 94?100,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Enabling the Discovery of Digital Cultural Heritage Objects through
Wikipedia
Mark M Hall
Paul D Clough
Information School
Sheffield University
Sheffield, UK
m.mhall@shef.ac.uk
p.d.clough@shef.ac.uk
Oier Lopez de Lacalle1,2
1IKERBASQUE
Basque Foundation for Science
Bilbao, Spain
2School of Informatics
University of Edinburgh
Edinburgh, UK
oier.lopezdelacalle@gmail.es
Aitor Soroa
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Spain
a.soroa@ehu.es
e.agirre@ehu.es
Abstract
Over the past years large digital cultural
heritage collections have become increas-
ingly available. While these provide ad-
equate search functionality for the expert
user, this may not offer the best support for
non-expert or novice users. In this paper
we propose a novel mechanism for intro-
ducing new users to the items in a collection
by allowing them to browse Wikipedia arti-
cles, which are augmented with items from
the cultural heritage collection. Using Eu-
ropeana as a case-study we demonstrate the
effectiveness of our approach for encourag-
ing users to spend longer exploring items
in Europeana compared with the existing
search provision.
1 Introduction
Large amounts of digital cultural heritage (CH)
information have become available over the past
years, especially with the rise of large-scale ag-
gregators such as Europeana1, the European ag-
gregator for museums, archives, libraries, and gal-
leries. These large collections present two chal-
lenges to the new user. The first is discovering
the collection in the first place. The second is
then discovering what items are present in the
collection. In current systems support for item
discovery is mainly through the standard search
paradigm (Sutcliffe and Ennis, 1998), which is
well suited for CH professionals who are highly
familiar with the collections, subject areas, and
have specific search goals. However, for new
users who do not have a good understanding of
what is in the collections, what search keywords
1http://www.europeana.eu
to use, and have vague search goals, this method
of access is unsatisfactory as this quote from
(Borgman, 2009) exemplifies:
?So what use are the digital libraries, if
all they do is put digitally unusable in-
formation on the web??
Alternative item discovery methodolo-
gies are required to introduce new users to
digital CH collections (Geser, 2004; Steem-
son, 2004). Exploratory search models
(Marchionini, 2006; Pirolli, 2009) that en-
able switching between collection overviews
(Hornb[Pleaseinsertintopreamble]k and Hertzum,
2011) and detailed exploration within the
collection are frequently suggested as more
appropriate.
We propose a novel mechanism that enables
users to discover an unknown, aggregated collec-
tion by browsing a second, known collection. Our
method lets the user browse through Wikipedia
and automatically augments the page(s) the user
is viewing with items drawn from the CH collec-
tion, in our case Europeana. The items are chosen
to match the page?s content and enable the user to
acquire an overview of what information is avail-
able for a given topic. The goal is to introduce
new users to the digital collection, so that they can
then successfully use the existing search systems.
2 Background
Controlled vocabularies are often seen as a
promising discovery methodology (Baca, 2003).
However, in the case of aggregated collections
such as Europeana, items from different providers
are frequently aligned to different vocabularies,
requiring an integration of the two vocabularies in
94
order to present a unified structure. (Isaac et al,
2007) describe the use of automated methods for
aligning vocabularies, however this is not always
successfully possible. A proposed alternative is
to synthesise a new vocabulary to cover all aggre-
gated data, however (Chaudhry and Jiun, 2005)
highlight the complexities involved in then link-
ing the individual items to the new vocabulary.
To overcome this automatic clustering and vi-
sualisations based directly on the meta-data have
been proposed, such as 2d semantic maps (An-
drews et al, 2001), automatically generated tree
structures (Chen et al, 2002), multi-dimensional
scaling (Fortuna et al, 2005; Newton et al, 2009),
self-organising maps (Lin, 1992), and dynamic
taxonomies (Papadakos et al, 2009). However
none of these have achieved sufficient success to
find widespread use as exploration interfaces.
Faceted search systems (van Ossenbruggen et
al., 2007; Schmitz and Black, 2008) have arisen
as a flexible alternative for surfacing what meta-
data is available in a collection. Unlike the meth-
ods listed above, faceted search does not require
complex pre-processing and the values to display
for a facet can be calculated on the fly. However,
aggregated collections frequently have large num-
bers of potential facets and values for these facets,
making it hard to surface a sufficiently large frac-
tion to support resource discovery.
Time-lines such as those proposed by (Luo et
al., 2012) do not suffer from these issues, but are
only of limited value if the user?s interest cannot
be focused through time. A user interested in ex-
amples of pottery across the ages or restricted to
a certain geographic area is not supported by a
time-line-based interface.
The alternative we propose is to use a second
collection that the user is familiar with and that
acts as a proxy to the unfamiliar collection. (Villa
et al, 2010) describe a similar approach where
Flickr is used as the proxy collection, enabling
users to search an image collection that has no
textual meta-data.
In our proposed approach items from the unfa-
miliar collection are surfaced via their thumbnail
images and similar approaches for automatically
retrieving images for text have been tried by (Zhu
et al, 2007; Borman et al, 2005). (Zhu et al,
2007) report success rates that approach the qual-
ity of manually selected images, however their
approach requires complex pre-processing, which
Figure 1: Architectural structure of the Wikiana sys-
tem
the dynamic nature of discovery prohibits.
Wikipedia was chosen as the discovery inter-
face as it is known to have good content cover-
age and frequently appears at the top of search
results (Schweitzer, 2008) for many topics, its
use has been studied (Lim, 2009; Lucassen and
Schraagen, 2010), and it is frequently used as
an information source for knowledge modelling
(Suchanek et al, 2008; Milne and Witten, 2008),
information extraction (Weld et al, 2009; Ni et
al., 2009), and similarity calculation (Gabrilovich
and Markovitch, 2007).
3 Discovering Europeana through
Wikipedia
As stated above our method lets users browse
Wikipedia and at the same time exposes them to
items taken from Europeana, enabling them to
discover items that exist in Europeana.
The Wikipedia article is augmented with Euro-
peana items at two levels. The article as a whole
is augmented with up to 20 items that in a pre-
processing step have been linked to the article and
at the same time each paragraph in the article is
augmented with one item relating to that para-
graph.
Our system (Wikiana, figure 1) sits between
the user and the data-providers (Wikipedia, Eu-
ropeana, and the pre-computed article augmenta-
tion links). When the user requests an article from
Wikiana, the system fetches the matching article
from Wikipedia and in a first step strips every-
thing except the article?s main content. It then
queries the augmentation database for Europeana
items that have been linked to the article and se-
lects the top 20 items from the results, as detailed
below. It then processes each paragraph and uses
95
Figure 2: Screenshot of the augmented article
?Mediterranean Sea? with the pre-processed article-
level augmentation at the top and the first two para-
graphs augmented with items as returned by the Euro-
peana API.
keywords drawn from the paragraphs (details be-
low) to query Europeana?s OpenSearch API for
items. A random item is selected from the result-
set and a link to its thumbnail image inserted into
the paragraph. The augmented article is then sent
to the user?s browser, which in turn requests the
thumbnail images from Europeana?s servers (fig.
2).
The system makes heavy use of caching to
speed up the process and also to reduce the
amount of load on the backend systems.
3.1 Article augmentation
To create the article-level augmentations we first
create a Wikipedia ?dictionary?, which maps
strings to Wikipedia articles. The mapping is cre-
ated by extracting all anchor texts from the inter-
article hyperlinks2 and mapping these to the ar-
ticles they link to. For instance, the string ?ro-
man coin? is used as an anchor in a link to the
Wikipedia article Roman currency3. Where
the same string points to multiple articles we se-
lect the most frequent article as the target. In the
case of ties an article is selected arbitrarily.
In a second step, we scan the subset of Eu-
ropeana selected for a European project, which
comprises SCRAN and Culture Grid collections
for English. The items in this sub-set are then
linked to Wikipedia articles. The sub-set of Euro-
2We used the 2008 Wikipedia dump to construct the dic-
tionary.
3http://en.wikipedia.org/wiki/Roman_
currency
<record>
<dc:identifier>http://www.kirkleesimage...</dc:identifier>
<dc:title>Roman Coins found in 1820..., Lindley</dc:title>
<dc:source>Kirklees Image Archive OAI Feed</dc:source>
<dc:language>EN-GB</dc:language>
<dc:subject>Kirklees</dc:subject>
<dc:type>Image</dc:type>
</record>
Figure 3: Example of an ESE record, some fields have
been omitted for clarity.
peana that was processed followed the Europeana
Semantic Elements (ESE) specifications4. Figure
3 shows an example of an ESE record describ-
ing a photograph of a Roman coin belonging to
the Kirklees Image Archive. We scan each ESE
record and try to match the ?dc:title? field with
the dictionary entries. In the example in figure
3, the item will be mapped to the Wikipedia ar-
ticle Roman currency because the string ?ro-
man coins? appears in the title.
As a result, we create a many-to-many mapping
between Wikipedia articles and Europeana items.
The Wikiana application displays at most 20 im-
ages per article, thus the Europeana items need to
be ranked. The goal is to rank interesting items
higher, with ?interestingness? defined as how un-
usual the items are in the collection. This metric
is an adaption of the standard inverse-document-
frequency formula used widely in Information
Retrieval and is adapted to identify items that have
meta-data field-values that are infrequent in the
collection. As in original IDF we diminish the
weight of values that occur very frequently in
the collection, the non-interesting items, and in-
creases the weight of values that occur rarely, the
interesting items. More formally the interesting-
ness ?i of an item i is calculated as follows:
?i =
#{titlei}
?title
log
Ntitle
c(titlei) + 1
+
#{desci}
?desc
log
Ndesc
c(desci) + 1
+
#{subji}
?subj
log
Nsubj
c(subji) + 1
where #{fieldi} is the length in words of the
field of the given item i, ?field is the average length
in words of the field in the collection, Nfield is the
total number of items containing that field in the
4http://version1.europeana.eu/web/
guest/technical-requirements
96
The Roman Empire (Latin: Imperium Romanum) was
the post-Republican period of the ancient Roman civ-
ilization, characterised by an autocratic form of gov-
ernment and large territorial holdings in Europe and
around the Mediterranean.
?Latin language? OR ?Ro-
man Republic? OR ?An-
cient Rome? or ?Autoc-
racy?
Figure 4: Example paragraph with the Wikipedia hy-
perlinks in bold. Below the search keywords extracted
from the hyperlinks and the resulting thumbnail image.
entire collection, and c(fieldi) is the frequency of
the value in that field.
Items are ranked by descending ?i and the for
the top 20 items, the thumbnails for the items are
added to the top of the augmented page.
3.2 Paragraph augmentation
The items found in the article augmentation tend
to be very focused on the article itself, thus to pro-
vide the user with a wider overview of available
items, each paragraph is also augmented. This
augmentation is done dynamically when an arti-
cle is requested. As stated above the augmen-
tation iterates over all paragraphs in the article
and for each article determines its core keywords.
As in the article augmentation the Wikipedia hy-
perlinks are used to define the core keywords, as
the inclusion of the link in the paragraph indi-
cates that this is a concept that the author felt was
relevant enough to link to. For each paragraph
the Wikipedia hyperlinks are extracted, the under-
scores replaced by spaces and these are then used
as the query keywords. The keywords are com-
bined using ?OR? and enclosed in speech-marks
to ensure only exact phrase matches are returned
and then submitted to Europeana?s OpenSearch
API (fig. 4). From the result set an item is ran-
domly selected and the paragraph is augmented
with the link to the item, the item?s thumbnail im-
age and its title. If there are no hyperlinks in a
paragraph or the search returns no results, then no
augmentation is performed for that paragraph.
4 Evaluation
The initial evaluation focuses on the paragraph
augmentation, as the quality of that heavily de-
pends on the results provided by Europeana?s API
and on a log-analysis looking at how users com-
Question Yes No
Familiar 18 18
Appropriate 9 27
Supports 4 32
Visually interesting 13 23
Find out more 3 33
Table 1: Evaluation experiment results reduced from
the 5-point Likert-like scale to a yes/no level.
ing to Europeana from Wikiana behave.
4.1 Paragraph augmentation evaluation
For the paragraph augmentation evaluation 18
wikipedia articles were selected from six topics
(Place, Person, Event, Time period, Concept, and
Work of Art). From each article the first para-
graph and a random paragraph were selected for
augmentation, resulting in a total set of 36 aug-
mented paragraphs. In the experiment interface
the participants were shown the text paragraph,
the augmented thumbnail image, and five ques-
tions (?How familiar are you with the topic??,
?How appropriate is the image??, ?How well does
the image support the core ideas of the para-
graph??, ?How visually interesting is the image??,
and ?How likely are you to click on the image
to find out more??). Each question used a five-
point Likert-like scale for the answers, with 1 as
the lowest score and 5 the highest. Neither the
topic nor the paragraph selection have a statisti-
cally significant influence on the results. To sim-
plify the analysis the results have been reduced
to a yes/no level, where an image is classified as
?yes? for that question if more than half the partic-
ipants rated the image 3 or higher on that question
(table 1).
Considering the simplicity of the augmentation
approach and the fact that the search API is not
under our control, the results are promising. 9
out of 36 (25%) of the items were classified as
appropriate. The non-appropriate images are cur-
rently being analysed to determine whether there
are shared characteristics in the query structure or
item meta-data that could be used to improve the
query or filter out non-appropriate result items.
The difficulty with automatically adding items
taken from Europeana is also highlighted by the
fact that only 13 of the 36 (36%) items were clas-
sified as interesting. While no correlation could
be found between the two interest and appro-
97
Category Sessions 1st q. Med 3rd q.
Wikiana 88 6 11 15.25
All users 577642 3 8 17
Table 2: Summary statistics for the number of items
viewed in per session for users coming from our sys-
tem (Wikiana) and for all Europeana users.
priate results, only one of the 23 uninteresting
items was judged appropriate, while 8 out of 9
of the appropriate items were also judged to be
interesting. We are now looking at whether the
item meta-data might allow filtering uninteresting
items, as they seem unlikely to be appropriate.
Additionally the approach taken by (Zhu et al,
2007), where multiple images are shown per para-
graph, is also being investigated, as this might re-
duce the impact of non-appropriate items.
4.2 Log analysis
Although the paragraph augmentation results are
not as good as we had hoped, a log analysis shows
that the system can achieve its goal of introduc-
ing new users to an unknown CH collection (Eu-
ropeana). The system has been available online
for three months, although not widely advertised,
and we have collected Europeana?s web-logs for
the same period. Using the referer information in
the logs we can distinguish users that came to Eu-
ropeana through our system from all other Euro-
peana users. Based on this classification the num-
ber of items viewed per session were calculated
(table 2). To prevent the evaluation experiment
influencing the log analysis only logs acquired be-
fore the experiment date were used.
Table 2 clearly shows that users coming
through our system exhibit different browsing pat-
terns. The first quartile is higher, indicating that
Wikiana users do not leave Europeana as quickly,
which is further supported by the fact that 30% of
the general users leave Europeana after viewing
three items or less, while for Wikiana users it is
only 19%. At the same time the third quartile is
lower, showing that Wikiana users are less likely
to have long sessions on Europeana. The differ-
ence in the session length distributions has also
been validated using a Kolmogorov-Smirnov test
(p = 0.00287, D = 0.1929).
From this data we draw the hypothesis that
Wikiana is at least in part successfully attracting
users to Europeana that would normally not visit
or not stay and that it successfully helps users
overcome that first hurdle that causes almost one
third of all Europeana users to leave after viewing
three or less items.
5 Conclusion and Future Work
Recent digitisation efforts have led to large dig-
ital cultural heritage (CH) collections and while
search facilities provide access to users familiar
with the collections there is a lack of methods for
introducing new users to these collections. In this
paper we propose a novel method for discover-
ing items in an unfamiliar collection by brows-
ing Wikipedia. As the user browses Wikipedia
articles, these are augmented with a number of
thumbnail images of items taken from the un-
known collection that are appropriate to the ar-
ticle?s content. This enables the new user to be-
come familiar with what is available in the collec-
tion without having to immediately interact with
the collection?s search interface.
An early evaluation of the very straightforward
augmentation process revealed that further work
is required to improve the appropriateness of the
items used to augment the Wikipedia articles. At
the same time a log analysis of Europeana brows-
ing sessions showed that users introduced to Eu-
ropeana through our system were less likely to
leave after viewing less than three items, pro-
viding clear indication that the methodology pro-
posed in this paper is successful in introducing
new users to a large, aggregated CH collection.
Future work will focus on improving the qual-
ity of the augmentation results by including more
collections into the article-level augmentation and
by introducing an ?interestingness? ranking into
the paragraph augmentation. We will also look at
evaluating the system in a task-based setting and
with existing, comparable systems.
Acknowledgements
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013)
under grant agreement n? 270082. We ac-
knowledge the contribution of all project part-
ners involved in PATHS (see: http://www.
paths-project.eu).
98
References
Keith Andrews, Christian Gutl, Josef Moser, Vedran
Sabol, and Wilfried Lackner. 2001. Search result
visualisation with xfind. In uidis, page 0050. Pub-
lished by the IEEE Computer Society.
Murtha Baca. 2003. Practical issues in applying meta-
data schemas and controlled vocabularies to cultural
heritage information. Cataloging & Classification
Quarterly, 36(3?4):47?55.
Christine L. Borgman. 2009. The digital future is
now: A call to action for the humanities. Digital
humanities quarterly, 3(4).
Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Picnet: Augmenting semantic resources with pic-
torial representations. In Knowledge Collection
from Volunteer Contributors: Papers from the 2005
Spring Symposium, volume Technical Report SS-
05-03. American Association for Artificial Intelli-
gence.
Abdus Sattar Chaudhry and Tan Pei Jiun. 2005. En-
hancing access to digital information resources on
heritage: A case of development of a taxonomy at
the integrated museum and archives system in sin-
gapore. Journal of Documentation, 61(6):751?776.
Chaomei Chen, Timothy Cribbin, Jasna Kuljis, and
Robert Macredie. 2002. Footprints of information
foragers: behaviour semantics of visual exploration.
International Journal of Human-Computer Studies,
57(2):139 ? 163.
Blaz Fortuna, Marko Grobelnik, and Dunja Mladenic.
2005. Visualization of text document corpus. In-
formatica, 29:497?502.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings
of the 20th international joint conference on Artif-
ical intelligence, IJCAI?07, pages 1606?1611, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Guntram Geser. 2004. Resource discovery - position
paper: Putting the users first. Resource Discovery
Technologies for the Heritage Sector, 6:7?12.
Kasper Hornb?k and Morten Hertzum. 2011. The no-
tion of overview in information visualization. In-
ternational Journal of Human-Computer Studies,
69(7-8):509 ? 525.
Antoine Isaac, Stefan Schlobach, Henk Matthezing,
and Claus Zinn. 2007. Integrated access to cul-
tural heritage resources through representation and
alignment of controlled vocabularies. Library Re-
view, 67(3):187?199.
Sook Lim. 2009. How and why do college students
use wikipedia? Journal of the American Society for
Information Science and Technology, 60(11):2189?
2202.
Xia Lin. 1992. Visualization for the document space.
In Proceedings of the 3rd conference on Visualiza-
tion ?92, VIS ?92, pages 274?281, Los Alamitos,
CA, USA. IEEE Computer Society Press.
Teun Lucassen and Jan Maarten Schraagen. 2010.
Trust in wikipedia: how users trust information
from an unknown source. In Proceedings of the 4th
workshop on Information credibility, WICOW ?10,
pages 19?26, New York, NY, USA. ACM.
Dongning Luo, Jing Yang, Milos Krstajic, William
Ribarsky, and Daniel A. Keim. 2012. Eventriver:
Visually exploring text collections with temporal
references. Visualization and Computer Graphics,
IEEE Transactions on, 18(1):93 ?105, jan.
Gary Marchionini. 2006. Exploratory search: From
finding to understanding. Communications of the
ACM, 49(4):41?46.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, CIKM ?08, pages 509?518, New
York, NY, USA. ACM.
Glen Newton, Alison Callahan, and Michel Dumon-
tier. 2009. Semantic journal mappiong for search
visualization in a large scale article digital library.
In Second Workshop on Very Large Digital Li-
braries at ECDL 2009.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng
Chen. 2009. Mining multilingual topics from
wikipedia. In Proceedings of the 18th international
conference on World wide web, WWW ?09, pages
1155?1156, New York, NY, USA. ACM.
Panagiotis Papadakos, Stella Kopidaki, Nikos Arme-
natzoglou, and Yannis Tzitzikas. 2009. Ex-
ploratory web searching with dynamic taxonomies
and results clustering. In Maristella Agosti,
Jose? Borbinha, Sarantos Kapidakis, Christos Pa-
patheodorou, and Giannis Tsakonas, editors, Re-
search and Advanced Technology for Digital Li-
braries, volume 5714 of Lecture Notes in Computer
Science, pages 106?118. Springer Berlin / Heidel-
berg.
Peter Pirolli. 2009. Powers of 10: Modeling
complex information-seeking systems at multiple
scales. Computer, 42(3):33?40.
Patrick L Schmitz and Michael T Black. 2008. The
delphi toolkit: Enabling semantic search for mu-
seum collections. In Museums and the Web 2008:
the international conference for culture and her-
itage on-line.
Nick J. Schweitzer. 2008. Wikipedia and psychology:
Coverage of concepts and its use by undergraduate
students. Teaching of Psychology, 35(2):81?85.
Michael Steemson. 2004. Digicult experts seek out
discovery technologies for cultural heritage. Re-
source Discovery Technologies for the Heritage
Sector, 6:14?20.
99
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203 ? 217. World Wide Web Conference
2007Semantic Web Track.
Alistair Sutcliffe and Mark Ennis. 1998. Towards a
cognitive theory of information retrieval. Interact-
ing with Computers, 10:321?351.
Jacco van Ossenbruggen, Alia Amin, Lynda Hard-
man, Michiel Hildebrand, Mark van Assem, Borys
Omelayenko, Guus Schreiber, Anna Tordai, Vic-
tor de Boer, Bob Wielinga, Jan Wielemaker, Marco
de Niet, Jos Taekema, Marie-France van Orsouw,
and Annemiek Teesing. 2007. Searching and an-
notating virtual heritage collections with semantic-
web technologies. In Museums and the Web 2007.
Robert Villa, Martin Halvey, Hideo Joho, David Han-
nah, and Joemon M. Jose. 2010. Can an interme-
diary collection help users search image databases
without annotations? In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 303?312, New York, NY, USA. ACM.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2009. Using wikipedia to bootstrap open informa-
tion extraction. SIGMOD Rec., 37:62?68, March.
Xiaojin Zhu, Andrew B. Goldberg, Mohamed Eldawy,
Charles A. Dyer, and Bradley Strock. 2007. A text-
to-picture synthesis system for augmenting commu-
nication. In The integrated intelligence track of
the 22nd AAAI Conference on Artificial Intelligence
(AAAI-07).
100
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 1?10,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Generating Paths through Cultural Heritage Collections
Samuel Fernando1, Paula Goodale2, Paul Clough2,
Mark Stevenson1, Mark Hall2, Eneko Agirre3
1Department of Computer Science, University of Sheffield
2Information School, University of Sheffield
3Computer Science Department, University of the Basque Country
{s.fernando, p.goodale, p.d.clough,
r.m.stevenson, m.mhall}@sheffield.ac.uk
e.agirre@ehu.es
Abstract
Cultural heritage collections usually or-
ganise sets of items into exhibitions or
guided tours. These items are often
accompanied by text that describes the
theme and topic of the exhibition and pro-
vides background context and details of
connections with other items. The PATHS
project brings the idea of guided tours
to digital library collections where a tool
to create virtual paths are used to assist
with navigation and provide guides on par-
ticular subjects and topics. In this pa-
per we characterise and analyse paths of
items created by users of our online sys-
tem. The analysis highlights that most
users spend time selecting items relevant
to their chosen topic, but few users took
time to add background information to the
paths. In order to address this, we con-
ducted preliminary investigations to test
whether Wikipedia can be used to au-
tomatically add background text for se-
quences of items. In the future we would
like to explore the automatic creation of
full paths.
1 Introduction
Paths (or trails) have been studied as a means of
assisting users with the navigation of digital col-
lections as an alternative to standard keyword-
based search (Furuta et al, 1997; Reich et al,
1999; Shipman et al, 2000; White and Huang,
2010). Paths can be particularly useful to users
who are unfamilar with the content of digital col-
lections (e.g. historical documents) and may find
it difficult to formulate appropriate queries (Wil-
son et al, 2010). Paths can be used to assist users
with the navigation of collections through the pro-
vision of narratives and subject guides. From an
educational perspective paths can provide tangible
learning objects, created by teachers and followed
by students. Alternatively from a cultural her-
itage perspective paths can be used to create activ-
ity trails and guided tours support exploration by
visitors through collections of cultural artefacts.
This echoes the organised galleries and guided
tours found in physical museums. The existance
of tools, such as Walden?s paths1, Trailmeme2 and
Storify3, provide functionalities for users to record
and share paths through web resources and digital
libraries. From this perspective everyone can take
on role of curator and provide access to their own
personal collections.
We have developed an online system called
PATHS that allows curators and end-users to cre-
ate and view paths to navigate through the Eu-
ropeana4 cultural heriage collection. As part of
evaluations of the prototype PATHS system par-
ticipants have created paths on various topics. In
this paper we describe a number of these paths and
their characteristics. Analysing paths that are cre-
ated manually and characterising them can be seen
as a first step towards developing methods to sup-
port the creation of paths automatically and semi-
automatically. Within the context of the PATHS
project this is being considered to deal with the
following limitations of manual creation of paths.
Firstly, the effort required in generating them often
means that a sufficient number of paths on a vari-
ety of topics are not available. Secondly, the man-
ual creation of paths is a very time-consuming pro-
cess that would benefit from computational sup-
port in whatever form this might take. This pa-
per presents initial work in automatically creat-
ing paths and provides the following novel con-
1http://www.csdl.tamu.edu/walden/
2http://open.xerox.com/Services/
xerox-trails
3http://storify.com/
4http://www.europeana.eu/
1
tributions: (1) we present results of user stud-
ies describing what people want from paths and
how they use them to navigate digital collections;
(2) we analyse a set of manually-created paths
to identify their properties and be able to charac-
terise them; and (3) we present work on automati-
cally generating background text for sequences of
items, thus providing an efficient way to enrich
paths with additional information with little man-
ual input required.
The paper is structured as follows: Section 2 de-
scribes related work on the use of narratives in cul-
tural heritage and previous approaches to automat-
ically generate paths; Section 3 defines the prob-
lem of generating paths and describes the datasets
used in the experiments; Section 4 presents analy-
sis of manually-created paths; Section 5 shows re-
sults of using automatic methods to generate back-
ground text; and finally Section 6 concludes the
paper and provides avenues for further work.
2 Related Work
2.1 Narratives and Cultural Heritage
The potential of narrative in digital CH to sup-
port learning, creativity and exploration is clear,
providing opportunities for supporting a more ac-
tive user interaction, including deeper engagement
with context, representation of the collecting pro-
cess, and facilitation of a more entertaining expe-
rience of learning (Mulholland and Collins, 2002).
Walker et al (2013) also propose narrative as a
major element of interaction and informal learn-
ing, suggesting that meaning is made when the
links between people and artefacts, and interpreta-
tion and ideas are surfaced, especially within so-
cial groups. Their experiments involve the use
of mobile and handheld technologies in a physi-
cal museum environment, capturing audio annota-
tions, but have much in common with experimen-
tal systems designed for path creation online. In a
similar vein the StoryBank project utilises collec-
tions of photographs and audio narratives to create
and share stories as information in the developing
world (Frohlich and Rachovides, 2008).
Whilst technologies have aided the creation and
sharing of narratives in physical cultural encoun-
ters, Manovich (1999) critiques the lack of narra-
tive in digital cultural environments, offering that
online collections and many CH web sites are
databases with constantly changing content that
inevitably lack a cohesive and persistent story.
However, since ?narrative is constructed by link-
ing elements of this database in a particular or-
der? (Manovich, 1999), it is possible to offer users
any number of explicit ?trajectories? (narratives)
through a digital information space, and by merg-
ing database and narrative in this way, creating
a more dynamic, discovery-led experience. This
view might be interpreted at its simplest level as
a virtual representation of the guided tours rou-
tinely offered in physical CH spaces, and indeed
there is a small strand of research into the creation
of systems for generating and exploring online ex-
hibitions and tours from items held within digital
collections. A scenario of users creating and edit-
ing trails in a CH context is described by Walker
(2006), including functionality for collecting, or-
dering and annotating museum objects.
2.2 Automatically Creating Paths
Generation of implicit trails through physical and
virtual museum spaces has been related to the
learning process (Peterson and Levene, 2003). In
this example, trails are automatically created by
users as they navigate their way through an infor-
mation space, and may be used for individual or
collaborative purposes. Research on the applica-
tion of curated pathways in web environments has
often focused on providing trails pre-prepared by
experts (e.g. curators, educationalists) as a means
of assisting novice users to navigate information
online (Shipman et al, 2000). Indeed, it has been
found that domain knowledge or expertise can
considerably enhance the quality of trails created
(Yuan and White, 2012). Automatic extraction
and generation of trails in information spaces has
been explored as a means of harnessing the wis-
dom of crowds, using the mass actions of earlier
user behaviour to establish relevance, and recom-
mend content or navigation routes to later users.
Such trails can be readily mined from search en-
gine transaction logs and have been shown to pro-
vide added value (White and Huang, 2010; Has-
san and White, 2012; Liao et al, 2012). West and
Leskovec (2012) take this notion a stage further
and attempt to identify wayfinding strategies em-
ployed by browsers in Wikipedia, with the goal of
assisting future users in their navigation by surfac-
ing potentially useful hyperlinks.
Guided tours or pathways are essentially more
structured, purposeful forms of trails, taking the
user through a specific sequence of information
2
nodes and may also be automatically generated,
rather than manually curated as in the examples
above. Wheeldon and Levene (2003) offer an al-
gorithm for generating trails from site-search, en-
abling elements of structure and context to be in-
corporated into the trails created in this way, but
noting potential scalability issues for web scale
search tasks. In the CH domain, a small num-
ber of projects have attempted to automatically
generate digital content in the form of exhibi-
tions, tours and trails. Ma?kela? et al (2007) de-
scribe a system which utilises semantically an-
notated content to generate personalised ?exhi-
bitions? from a structured narrative-based search
query. Similarly, Zdrahal et al (2008) demonstrate
how pathways can be generated through a collec-
tion of semantically related documents to provide
a means of exploration, using non-NLP cluster-
ing and path creation techniques. Sophisticated
approaches such as linear programming and evo-
lutionary algorithms have also been proposed for
generating summaries and stories (McIntyre and
Lapata, 2010; Woodsend and Lapata, 2010). In
contrast, Wang et al (2007) use a recommender
system approach to generate museum tours on
the basis of ratings stored within a dynamic user
model, and Pechenizkiy and Calders (2007) pro-
pose the additional use of data mining techniques
on log data to improve this type of tour personali-
sation.
In summary, online tours and trails are made
possible either through manually curated content
generated through the efforts of experts or other
end users, or have been automatically generated
from the mining of large scale search logs, or from
collections benefitting from semantically-linked
content and/or detailed user models.
3 Methodology
This study brings together work from several ar-
eas of the PATHS project. An analysis of what
paths might be used for and what form they are ex-
pected to take, has had implications for the system
design and functionality and evaluation measures.
A user study focused upon evaluation of the first
prototype has provided manually-created paths as
a basis for analysing path content and attributes,
which in turn informs the desired characteristics
of automated paths and the algorithm designed for
generating paths automatically.
3.1 Utilisation of Paths
Initial user requirements interviews with 22 ex-
pert users in the heritage, education and profes-
sional domains found a strong affinity with the
path metaphor, revealing a range of different in-
terpretations of what it means in the CH context
and how they could be employed in an online en-
vironment to engage with key audiences. Eight
interpretations of the path metaphor emerged:
1. Path as search history
2. Path as information seeking journey
3. Path as linked metadata
4. Path as a starting point or way in
5. Path as a route through
6. Path as augmented reality
7. Path as information literacy journey / learn-
ing process
8. Path as transaction process
The first three of these are closest to the idea
of hypertext trails, with trails defined by user in-
teraction in 1 and 2, and trails defined automati-
cally, by the system in 3. Variations 4-6 are more
creative interpretations, all suggesting opportuni-
ties for guiding the user into and through collec-
tions, encouraging exploration and/or offering an
immersive experience, conducive with our initial
vision for the PATHS system.
In addition to expert-defined routes, 5 also in-
corporates the idea of users being able to see and
follow ?well-trodden path? defined by the cumula-
tive interactions of other users, thus extending the
opportunities for utilizing search histories. Con-
versely, 7 and 8 are both process oriented, al-
though 7 is experiential, user-defined, learning-
oriented, typified by trial and error and unique to
the individual, whilst 8 is a rigid process designed
to escort all users consistently through a standard
process of pre-defined steps.
A strong emphasis was placed on path content
being carefully selected or ?curated? by the path-
creator, with the addition of context and interpre-
tation so that the objects within the path convey
a narrative or meaning. Content may be derived
from one collection, but there were seen to be sig-
nificant benefits from including objects from di-
verse collections, along with other materials from
external web sites.
Paths facilitate topic-based information re-
trieval typified by the berry-picking mode of in-
teraction (Bates, 1989), rather than known item
searching. Furthermore, paths may be a useful tool
3
for personal information management in both for-
mal and informal research scenarios, enabling the
user to record, reuse and share their research activ-
ity, or helping them to organize their ideas. Cre-
ativity is also encouraged, as user-generated paths
provide the means to repurpose CH objects into
users? own narratives for private or public con-
sumption.
A summary of specific user scenarios high-
lighted by participants is given below:
? Teachers/lecturers presentations and class-
room activities
? Museum personnel curating collections, giv-
ing an overview, or covering a topic in depth
? Leisure users browsing, collecting interest-
ing and/or visually appealing content
? Researchers to aid image-based research,
sharing and discussing findings with fellow
researchers and supervisors
? Non-academic specialists (e.g. local histori-
ans) collecting and sharing items of interest
with other enthusiasts
3.2 Defining the Problem
To create a path or narrative that guides a user
through a set of items from a collection, whether
as a manual process or automatically, there are
three main activities: (1) the selection of items to
include in the path; (2) the arrangement of items
to form a path or narrative and (3) the annota-
tion of the path to with descriptive text and back-
ground information. We envision techniques to
automate the entire process; however, a first step is
to analyse existing manually-created paths to iden-
tify their characteristics and inform the automatic
creation of similar structures.
3.3 User Study
The manually generated paths used for this study
were created as part of a more detailed user study
to evaluate the first prototype, conducted using
a protocol informed by the Interactive IR eval-
uation framework (Borlund, 2003). Twenty-two
users, including subject experts, students and gen-
eral users (subject novices), each completed a 2-
hour session, during which they participated in the
following activities:
? Profile questionnaire and cognitive style test
? Familiarisation with the system
? 4x short information seeking tasks (5 minutes
each)
? 1x long simulated work task - path creation
(30 minutes)
? Task feedback questionnaire
? Session/system feedback questionnaire
? Think-after interview based upon the com-
plex task
Of most interest here is the simulated work task,
with associated observations, feedback and reflec-
tions. This task focused on the creation of a path,
using a scenario adapted to the type of user. Free-
dom was given in choosing a subject for the path,
and limited instructions were provided in what
might be needed to complete the task, for exam-
ple:
?Imagine you are a student who has been asked
to create a path as part of a university assignment.
You have been asked to use primary source ma-
terials to create a mini online exhibition suitable
for a target group within the general public and/or
school visitor categories. Your goal is to introduce
a historical or art-focussed topic in a popular, ac-
cessible way, and to encourage further use and ex-
ploration of cultural heritage resources.?
Data on the tasks was captured via log files, as
well as screen recording and observations using
the Morae usability software. Detailed analysis
was undertaken of user behaviour in the process of
completing the task, and of the paths created, from
both quantitative and qualitative perspectives.
4 Analysing Manually-created Paths
In this section we describe the results of analysing
the 22 paths created manually in the PATHS pro-
totype system.
4.1 User behaviour
On average users spend 25.3 mins on creating a
path (min=11.7; max=33.6) with an average of
201 mouse clicks (min=53; max=380). From the
observations, it was noted that some participants
spent quite a lot of time thinking about the task
and pondering their next move, whilst others en-
gaged in more rapid fire activity in the face of
uncertainty. Analysis of the screen recordings
showed a variety of primary interaction styles for
this task, with a fairly even split between serial
searching (33%) and serial browsing (39%), as the
two most popular strategies. Serial searching in-
volves repetitive search and reformulation, with
only a page or two of search results viewed before
searching again, and serial browsing involves very
4
few searches, with large numbers of search re-
sults pages viewed (over 50 pages in some cases).
These are then in effect, polar opposites of interac-
tion. Only 6% engaged primarily in exploring be-
haviour (using the explore and similar items con-
tent), and 22% of participants occupied the middle
ground, utilising a mix of search, browse and ex-
plore, with no strong preference for any one style.
4.2 Properties of paths
The mean number of items in a path was 10.7 (std
dev=6.7 items) with a minimum of 5 items and
maximum of 29 items. Most popular bin is 6-
10 items in a path (59%). We found 85% of the
items included in the paths included an image with
the metadata. The paths created were manually
categorised by theme to ascertain whether there
are any distinct preferences for the subject mat-
ter of content included. The most popular cate-
gories were paths about places (23%), art subjects
(23%) and history subjects (32%). These themes
are likely to have been influenced at least partly
by what content is currently available in our col-
lection, although the amount of art-related content
is much less than for history, and also appear to
have been influenced by the topics covered in ex-
isting paths in the system (e.g. places, topics re-
lated to the world wars). There were, however a
significant number of expert users who attempted
to build paths related to their own research inter-
ests, with varying degrees of success.
4.3 Descriptions and ordering
Once items have been selected and they have been
transferred in the path creation workspace, users
have the opportunity to modify and enhance their
path with a number of tools for adding content and
metadata, and for re-ordering the content. On cre-
ating the path, most users immediately went to the
metadata fields and added information for the path
description and duration fields, as well as a num-
ber of tags (or keywords). A short 1-2 line de-
scription of the path appears to be the norm and
was added in 91% of cases. Tags were added by
82% of users and a duration by only 46% of users.
It is clear from further investigation that the tags
were added incorrectly (without commas between
them) by a significant number of users and a tip
for successful use is required.
The items within a path can be annotated with
the user?s own contextual information, and can be
re-ordered into a more meaningful sequence, such
as a chronological or narrative sequence. These
more advanced features were used by significantly
fewer users, which could indicate a learning issue,
a lack of need, or a time constraint. On reviewing
the paths created by our evaluation participants it
is found that in 41% of cases, contextual informa-
tion was not added to any items in the path. There
are however 32% in which annotations were added
to all items (generally these were shorter paths
with fewer items), and a further 27% where anno-
tations were added to some or most of the items.
In 72% of cases the items in the paths created
were re-ordered to some degree, with 17% spend-
ing a considerable amount of time on this activity.
This finding is encouraging, as the default is for
items to be included in the path in the order they
were saved to the workspace, and re-ordering in-
dicates that users are thinking about their path as a
whole and trying to make sense of the information
it is intended to convey. Typical types of ordering
included chronology (32%), narrative (23%), ge-
ography (for example, a walking tour - 9%), theme
(9%) and ?interestingness? (5%).
5 Enriching paths with background
information
This section describes preliminary work on the
task of semi-automated path creation. In par-
ticular we describe efforts to enrich paths with
background contextual information using relevant
Wikipedia articles. The related work described
in Section 2.2 shows that there have been previ-
ous efforts to automatically select cultural heritage
items to form paths, trails and exhibitions. How-
ever to our knowledge no significant effort has
been made to automatically annotate such paths
with descriptive or contextual information. The
interviews described in Section 3.1 highlighted
the importance CH experts placed on having ad-
ditional information to give context for the items
in the path. It was also noted during the manual
path-creation exercise (Section 4.3) that a signif-
icant number of the users did not add any such
information to the path. The reasons for this are
unclear, but nevertheless there seems to be suffi-
cient motivation to devise automatic methods for
this task. Although the methods have previously
been well established in other tasks5 , we believe
5INEX Tweet Contextualization Track (https:
//inex.mmci.uni-saarland.de/tracks/qa/)
and Link-the-wiki Track (http://www.inex.otago.
ac.nz/tracks/wiki-link/wiki-link.asp)
5
this is the first time they have been applied for the
task of annotating sequences of items in this way.
5.1 Method
Manually generated paths contain sequences of
items selected from Europeana on some topic or
theme. Creators provide their own title, subject
keywords and description for the path. To aid
creation of paths we explore whether background
information could be generated automatically for
such paths. An approach is presented here which
shows promise as a potential way to achieve this
task. The input for this approach is a sequence of
items and a key Wikipedia article which describes
the overall topic of the path. The output comprises
sentences taken from a relevant Wikipedia article.
The aim is for this output to provide useful and
interesting additional background information re-
lated to the items and theme of the path. In this
paper experiments are focussed on how to select
good quality text to present as additional informa-
tion for the path. For this reason the key Wikipedia
article is manually chosen, and the task is to find a
good approach for selecting the most relevant sen-
tences from this key article for the text.
Two methods are tested in this paper. The first
method simply takes the first n sentences of the
article and outputs this. Since Wikipedia articles
are always structured to have a summary of the
article in the first paragraph we can expect this text
to perform well as a summary of the path topic.
The second method is more advanced and at-
tempts to find text in the article that is relevant to
the actual items that have been chosen for the path.
This approach uses the Wikipedia Miner software
(Milne and Witten, 2008) to add inline links to
the text in the items for this approach. This soft-
ware disambiguates terms in the text and then de-
tects links using various features such as the com-
monness of the term, the overall relatedness of the
terms in the text and so on. The result is text en-
riched with inline links to relevant Wikipedia arti-
cles. Each link also has an associated confidence
value which indicates how sure the software is that
the link is correctly disambiguated and relevant to
the text.
The approach works as follows for a sequence
of items S and a key article K. First Wikipedia
Miner is run over the items in S. The text input to
Wikipedia Miner comprises the title, subject and
description fields of each item. The output is a set
of article titles W comprising the titles of all the
linked articles which were found in the text fields
of S. For each title in W we also have the associ-
ated confidence value for the link as calculated by
Wikipedia Miner. The next step is to select from
K the most relevant sentences to output as the gen-
erated text. For each sentence in K a score is as-
signed if any of the words in the sentence match
one of the titles in W . The score is then simply the
sum of the confidence values associated with these
titles. The top scoring sentences are then output as
the background text. This method can be consid-
ered to be a kind of query based summarisation
(Jurafsky and Martin, 2008).
5.2 Results
The automatic approaches for generating back-
ground text were run over the items in the 22 man-
ually created paths described in the previous sec-
tion. To ensure a fair test the user-added text was
was discarded and only the original Europeana
metadata for the items was used as source for the
methods.
For each path a single key Wikipedia article was
manually found which best captured the overall
theme or topic of the path. For all paths at least
one reasonably good article was found, although
for some paths the articles were better matches for
the path topic than for others. Some examples are
shown in Table 1.
With the input ready, the approaches described
were run over the items and key articles to gener-
ate the additional text. This was limited to 3 sen-
tences to provide a short but useful piece of text
for viewers of the paths. For example one path
was entitled ?Bronze Age metallurgy?. The user
provided description was as follows:
Bronze Age metalsmiths produce a variety of
objects, however, we learn as much from the tools,
leftover debris and the way in which objects were
deposited as we do from the finished objects them-
selves.
The key article chosen for the path was ?Bronze
Age?. The items in the path included various ar-
tifacts and working debris from the Bronze Age.
For the first approach the output is the first 3 sen-
tences from this article:
The Bronze Age is a period characterized by
the use of copper and its alloy bronze as the chief
hard materials in the manufacture of some imple-
ments and weapons. Chronologically, it stands be-
6
Path title Key Wikipedia article
Canals in England Canals of the United Kingdom
A brief history of Sheffield History of Sheffield
Art and Criticism of John Ruskin John Ruskin
Early medieval features in English
Parish Churches
Architecture of the medieval cathedrals
of England
Brooches through the ages Brooch
Table 1: Key articles for paths
tween the Stone Age and Iron Age. The term Stone
Age implies the inability to smelt any ore, the term
Bronze Age implies the inability to smelt iron ore
and the term Iron Age implies the ability to man-
ufacture artifacts in any of the three types of hard
material.
For the second approach the top 3 highest scor-
ing sentences were output:
A region could be in the Bronze Age either by
smelting its own copper and alloying with tin or
by trading for bronze from production areas else-
where. The Bronze Age in Ireland commenced
around 2000 BC, when copper was alloyed with
tin and used to manufacture Ballybeg type flat axes
and associated metalwork. The Bronze Age is a
period characterized by the use of copper and its
alloy bronze as the chief hard materials in the
manufacture of some implements and weapons.
These sentences scored most highly since they
contained the most high-confidence terms from
the items, for example terms such as ?copper?, ?al-
loy? and ?Bronze Age?.
5.3 Evaluation
To evaluate the two approaches, 5 annotators were
presented with the paths and the text and asked to
rate each path on 3 dimensions:
? The relevance of the text to the theme and
items of the path. Text which relates strongly
to the path is scored highly while off-topic or
irrelevant text is given a low score.
? The coherence or quality of the text itself.
Text which appears well-written and well-
structured is scored highly, while poorly writ-
ten or incoherent text is given a low score.
? The contextualisation of the text in relation
to the path. To achieve a high score the
text should offer useful or interesting addi-
tional information which is not found else-
where within the content, i.e. the text helps
to provide a context for items in the path.
Annotators were asked to grade from A (very
good) to E (very poor) on each dimension. The
results are shown in Figure 1. The results for
the first 3 sentences are shown as First3 and for
the weighted approach as Weighted. For each di-
mension, the distribution of judgements across the
paths is shown. The First3 approach was found
to be superior in every dimension. For relevance
scores 90% of the scores were either A or B com-
pared to 63% for the Weighted approach. Sim-
ilarly for the coherence judgements 97% were A
or B compared to 62% for the weighted approach.
The reason for this superior performance seems to
be that the first few sentences of Wikipedia arti-
cles are deliberately created to give a short sum-
mary introduction of the topic of the article. This
explains the high scores for relevance and coher-
ence.
Both approaches scored lower on the contex-
tualisation dimension, with First3 getting 67%
A or B grades and the Weighted approach get-
ting 43%. There may be several reasons for this.
Firstly one problem is that the auto-generated text
sometimes repeats information that is already in
the path and item descriptions; thus the text fails
to meet the requirement of ?useful additional in-
formation?. Secondly the text is sometimes quite
general and vague, rather than focussing on spe-
cific details which might be most relevant to the
items chosen for the path.
To measure the agreement among the annotators
the following approach was used. First the scores
were converted to numeric values; A to 1, B to 2
and so on. Then the scores for each annotator were
compared to the average of the scores of all the
other annotators. The correlation was computing
using Spearman?s correlation coefficient. These
scores were then averaged amongst all annotators
to give a final agreement value. The results are
shown in Table 2.
7
Figure 1: Comparing the results of the two methods.
First3 Weighted
Relevance 0.57 0.57
Coherence 0.28 0.56
Contextualisation 0.56 0.78
Table 2: Agreement amongst annotators.
For both approaches there was good agreement
on the Relevance dimension. For the Coherence
dimension the First3 approach got quite a low
score. This may be because one annotator gave
lower scores for all paths, while the others all gave
consistently high scores, which seems to have
skewed the correlation co-efficient. For the con-
textualisation dimension the correlation scores for
high for both approaches, and the Weighted ap-
proach in particular achieved a very high agree-
ment value.
6 Conclusions
This paper presented results of interviews about
creating paths through cultural heritage collec-
tions. These results inform us on how people
want to navigate through cultural heritage collec-
tions using the path metaphor, how they wish to
make use of paths for their work and education,
and what information and qualities they consider
it important for a path to contain. The paper also
presents results from studies using the PATHS pro-
totype software where users were able to search
and explore a large digital library collection and
create their own paths of items from the collection
on topics of their interest.
From the interviews it was clear that the experts
considered it important that the paths contain ad-
ditional information to convey contextual informa-
tion to understand the meaning of the items in the
path. The results from the user studies showed that
this need was not being met in a significant num-
ber of cases; users were putting items together on
a topic but adding little or no descriptive text about
the topic and the items in the path. Therefore we
identified this as a key task which might benefit
from automatic methods. The simpler approach
which output the first n sentences from the key
Wikipedia article was found to generate the best
results. The resulting generated text was found to
be relevant and coherent. In most cases the text
was also found to add useful context about the
topic.
Future work will further refine the text genera-
tion approach. The approach depends on success-
fully identifying a good key article for each path.
In these experiments the key article was manually
chosen, however we are devising methods to se-
lect this article automatically. To correct the prob-
lem with repeated information a filtering approach
could eliminate information that is already con-
tained within the paths.
Acknowledgments
The research leading to these results was car-
ried out as part of the PATHS project (http:
//paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082.
References
Marcia J Bates. 1989. The design of browsing and
berrypicking techniques for the online search inter-
8
face. Online Information Review.
Pia Borlund. 2003. The IIR evaluation model: a
framework for evaluation of interactive information
retrieval systems. Information research, 8(3).
David M Frohlich and Dorothy Rachovides. 2008. Us-
ing digital stories for local and global information
sharing. In Community and International Develop-
ment, CHI 2008 Workshop.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proceedings of the eighth ACM conference on Hy-
pertext, pages 167?176, New York, NY.
Ahmed Hassan and Ryen W White. 2012. Task tours:
helping users tackle complex search tasks. In Pro-
ceedings of the 21st ACM international conference
on Information and knowledge management, pages
1885?1889. ACM.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing (2nd Edition) (Prentice
Hall Series in Artificial Intelligence). Prentice Hall.
Zhen Liao, Yang Song, Li-wei He, and Yalou Huang.
2012. Evaluating the effectiveness of search task
trails. In Proceedings of the 21st international con-
ference on World Wide Web, pages 489?498. ACM.
Eetu Ma?kela?, Osma Suominen, and Eero Hyvo?nen.
2007. Automatic exhibition generation based on
semantic cultural content. In Proc. of the Cultural
Heritage on the Semantic Web Workshop at ISWC+
ASWC, volume 2007.
Lev Manovich. 1999. Database as symbolic form.
Convergence: The International Journal of Re-
search into New Media Technologies, 5(2):80?99.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1562?
1572. Association for Computational Linguistics.
D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM confer-
ence on Information and knowledge management,
pages 509?518. ACM.
Paul Mulholland and Trevor Collins. 2002. Using dig-
ital narratives to support the collaborative learning
and exploration of cultural heritage. In Database
and Expert Systems Applications, 2002. Proceed-
ings. 13th International Workshop on, pages 527?
531. IEEE.
Mykola Pechenizkiy and Toon Calders. 2007. A
framework for guiding the museum tours person-
alization. In Proceedings of the Workshop on Per-
sonalised Access to Cultural Heritage (PATCH07),
pages 11?28.
Don Peterson and Mark Levene. 2003. Trail records
and navigational learning. London review of Educa-
tion, 1(3):207?216.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
Frank M Shipman, Richard Furuta, Donald Brenner,
Chung-Chi Chung, and Hao-wei Hsieh. 2000.
Guided paths through web-based collections: De-
sign, experiences, and adaptations. Journal of
the American Society for Information Science,
51(3):260?272.
K. Walker, A. Main, and Fass. J. 2013. User-
Generated Trails in Third Places. In HCI-3P Work-
shop on Human Computer Interaction for Third
Places at Computer Human Interaction 2013.
Kevin Walker. 2006. Story structures. building nar-
rative trails in museums. In Technology-Mediated
Narrative Environments for Learning, pages 103?
114. Sense Publishers.
Yiwen Wang, Lora M Aroyo, Natalia Stash, and Lloyd
Rutledge. 2007. Interactive user modeling for per-
sonalized access to museum collections: The ri-
jksmuseum case study. In User Modeling 2007,
pages 385?389. Springer.
Robert West and Jure Leskovec. 2012. Human
wayfinding in information networks. In Proceed-
ings of the 21st international conference on World
Wide Web, pages 619?628. ACM.
Richard Wheeldon and Mark Levene. 2003. The best
trail algorithm for assisted navigation of web sites.
In Web Congress, 2003. Proceedings. First Latin
American, pages 166?178. IEEE.
Ryen W White and Jeff Huang. 2010. Assessing the
scenic route: measuring the value of search trails in
web logs. In Proceedings of the 33rd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 587?594. ACM.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 565?574. Associ-
ation for Computational Linguistics.
Xiaojun Yuan and Ryen White. 2012. Building the
trail best traveled: effects of domain knowledge on
web search trailblazing. In Proceedings of the 2012
ACM annual conference on Human Factors in Com-
puting Systems, pages 1795?1804. ACM.
9
Zdenek Zdrahal, Paul Mulholland, and Trevor Collins.
2008. Exploring pathways across stories. In Proc.
of International Conference on Distributed Human-
Machine Systems.
10
