Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2260?2269, Dublin, Ireland, August 23-29 2014.
?One Entity per Discourse? and ?One Entity per Collocation?
Improve Named-Entity Disambiguation
Ander Barrena*, Eneko Agirre*, Bernardo Cabaleiro**, Anselmo Pe
?
nas**, Aitor Soroa*
*IXA NLP Group / University of the Basque Country, Basque Country
abarrena014@ikasle.ehu.es, e.agirre@ehu.es, a.soroa@ehu.es
**UNED NLP & IR Group, Madrid
anselmo@lsi.uned.es, bcabaleiro@lsi.uned.es
Abstract
The ?one sense per discourse? (OSPD) and ?one sense per collocation? (OSPC) hypotheses have
been very influential in Word Sense Disambiguation. The goal of this paper is twofold: (i) to
explore whether these hypotheses hold for entities, that is, whether several mentions in the same
discourse (or the same collocation) tend to refer to the same entity or not, and (ii) test their impact
in Named-Entity Disambiguation (NED). Our experiments show consistent results on different
collections and three state-of-the-art NED system. OSPD hypothesis holds in around 96%-98%
of documents whereas OSPC hypothesis holds in 91%-98% of collocations. Furthermore, a
simple NED post-processing in which the majority entity is promoted, produces a gain in perfor-
mance in all cases, reaching up to 8 absolute points of improvement in F-measure. These results
show that NED systems would benefit of considering these hypotheses into their implementation.
1 Introduction
The ?one sense per discourse? (OSPD) hypothesis was introduced by Gale et al. (1992), and stated that a
word tends to preserve its meaning when occurring multiple times in a discourse. They estimated that the
probability of two occurrences of the same polysemous noun drawn from one document having the same
sense to be around 94% for documents from Grolier encyclopedia, and 96% for documents from Brown,
based on word senses from the Oxford Advanced Learner?s Dictionary and a handful of examples. A few
years later, Krovetz (1998) reported 66% on larger corpora (SemCor and DSO) annotated with WordNet
senses by third parties, but, unfortunately, he only reported how many polysemous nouns occurred with
a single sense in all documents, not in each document. In the context of statistical machine translation,
Carpuat (2009) reported that, 80% of the time, words occurring multiple times in a source document are
translated into a single word in the target language.
In the case of entities, OSPD is closely related to coreference, where the task is to find whether two
different mentions (perhaps using different surface strings like ?John? and ?he?) in a document refer
to the same entity or not. For instance, the coreference system presented by (Lee et al., 2013), uses a
heuristic which links mentions in a document that share the same surface string: ?This sieve [heuristic]
accounts for approximately 16 CoNLL F1 points improvement, which proves that a signicant percentage
of mentions in text are indeed repetitions of previously seen concepts?. Our paper actually quantifies the
amount of those repetitions for entities, providing additional evidence for the heuristic.
The ?one sense per collocation? (OSPC) hypothesis was introduced by Yarowsky (1993), stating that
a word tends to preserve its meaning when occurring with the same collocate. Yarowsky tested his
hypothesis for several definitions of collocate, including positional collocates (word to left or right)
and syntactic collocations (governing verb of object, governing verb of subject, modifying adjective).
He reported entropy on train data, as well as disambiguation performance on unseen data, with the
precision ranging between 90% and 99% for a handful of words with two distinct homograph senses,
like, e.g. ?bass? or ?colon?. In larger-scale research, Martinez and Agirre (2000) measured the precision
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2260
Abbott Beefs Up Litigation Reserves NORTH CHICAGO, Ill. (AP) Abbott Laboratories Inc., bracing
for a costly settlement in a federal investigation involving the prostate-cancer drug Lupron, said Friday
it was increasing litigation reserves by $344 million. As part of the announcement, Abbott said it had
restated its quarterly results and is now reporting a loss of $319.9 million for the first three months
of this year rather than a profit. The move comes amid long-running negotiations between the U.S.
Department of Justice and TAP Pharmaceutical Products, the 50-50 joint venture between Abbott and
Takeda Chemical Industries of Japan that made Lupron. Abbott said in January ...
Figure 1: Example of OSPD for entities. All occurrences of ?Abbott? refer to ?Abbott Laboratories?.
of similar collocations on corpora (Semcor and DSO) annotated by third parties with finer-grained senses
from WordNet, reporting lower figures around 70%.
In this paper, we take a collocation to be a word (or multiword term) that co-occurs with the target
named-entity more often than would be expected by chance. In our case we use syntactic dependencies
to extract co-occurring terms.
These two hypotheses have been very influential, and have inspired multiple heuristics and methods
in Word Sense Disambiguation research (Agirre and Edmonds, 2007, Chapters 5,7,10,11). In this work
we are going to show that both hypotheses hold for named-entities as well, and that the hypotheses can
be used to post-process the output of any Named-Entity Disambiguation system (NED) to improve its
performance. NED, also known as Entity Linking, takes as input a named-entity mention in context and
assigns it a specific entity from a given entity repository (Hachey et al., 2012; Daiber et al., 2013).
In the first part of this work we are going to test whether the two hypotheses hold for entity mentions
with respect to a repository of entities extracted from Wikipedia. For instance, do all occurrences of
mention ?Abbott? in a document refer to the same entity? Do all occurrences of mention ?CPI? as
subject of verb ?rise? refer to the same entity? Do all occurrences of ?CDU? in relation to ?Merkel? refer
to the same entity? The examples in Figures 1 and 2 show evidence that this is indeed the case. The
experiments aim at quantifying in which degree OSPD and OSPC hypotheses hold for entities
1
.
In the second part of the paper, we will explore a simple method to incorporate OSPD and OSPC
hypotheses to any existing NED system, showing their potential. After running the NED system, we take
its output and observe, for each mention string, which is the entity returned most often for a given docu-
ment (or collocation), assigning to all occurrences the majority entity. We tested the improvements with a
freely available NED system (Daiber et al., 2013), a reimplementation of a strong Bayesian NED system
(Han and Sun, 2011) and an in-house graph-based system. We got statistically significant improvements
for all systems and ?one sense? hypotheses that we tested, with a couple exceptions.
In order to check the OSPD and OSPC hypotheses for entities, we first looked into existing datasets.
AIDA (Hoffart et al., 2011)
2
is a publicly available hand-tagged corpus based on the CoNLL named-
entity recognition and disambiguation task dataset. AIDA contains links of all entity mentions in full
documents, so it is a natural fit for OSPD. We estimated OSPD based on more than 4,000 mentions that
occur multiple times in a document. For completeness, we also estimated OSPD at the collection level.
OSPD and OSPC are independent of each other, as one is applied at the document level and the other
at the corpus level, focusing on the entities that occur with a specific collocation. Multiple occurrences
of a target string in a document usually occur with different collocations, and conversely, multiple occur-
rences of a target string with a specific collocation typically occur in different documents. Note also that
singletons (entities that are only mentioned once in a document) are not affected by OSPD, but could be
affected by OSPC.
In order to estimate OSPC, no available corpus existed, so we decided to base our dataset on the TAC
KBP 2009 Entity Linking dataset
3
(TAC2009 for short) (Ji et al., 2010). The TAC2009 dataset involves
138 mention strings, which have been annotated in several documents drawn primarily from Gigaword
4
.
1
For the sake of clarity we will also refer to OSPD and OSPC for entities as OSPD and OSPC.
2
http://www.mpi-inf.mpg.de/yago-naga/aida/downloads.html
3
http://www.nist.gov/tac/2013/KBP/EntityLinking/index.html
4
http://catalog.ldc.upenn.edu/LDC2003T05
2261
CPI subject-of rise:
China?s consumer price index, or CPI, rose 2.8 percent last December.
In the 10 months to October, the CPI rose 1.35 percent, the core price index grew 1.13 percent ...
Measured on a month-on-month basis, March CPI rose 2.3 percent from February, ...
... still lower than in China, Hong Kong and Singapore, whose CPIs have rised 8.0 percent, ...
The core CPI rose 0.2 percent, in line with Wall Street expectations.
Angela Merkel has CDU:
... who share power with Merkel?s CDU nationally in an uneasy ? grand coalition ? ...
Economy Minister Michael Glos, also from the CSU, the sister party to Merkel?s CDU ...
In the past Merkel?s CDU had been able to rely on the CSU?s strength in Bavaria ...
... but while her conservative CDU wanted new legal tools to do so, ...
The new development has put a further strain on Merkel?s CDU ...
Figure 2: Examples of OSPC for entities, showing five examples for a syntactic collocation (top row)
and fie examples for a more specific proposition (bottom row). ?CPI? might refer to ?Comunist Party
of India? or ?Consumer Price Index?, among others, but refers to the second in all cases. ?CDU? can
refer to the German ?Christian Democratic Union? or ?Catholic Distance University?, among others, but
refers to the first in all cases.
We extracted several syntactic collocations for those 138 mention strings from Gigaword, and hand-
annotated them, yielding an estimate for the OSPC. Note that TAC2009 only provides the annotation for
a specific mention in a document, so we had to annotate by hand the rest of occurrences in the documents.
For instance, we analyzed examples of ?CPI? as subject of the verb ?rise? (cf. Figure 2). Some of the
syntactic collocations like the subjects of verb ?has? seemed very uninformative, so we decided to also
check the OSPC hypothesis on more specific collocations, involving more complete argument structures.
For instance, we checked ?ABC? occurring as subject of ?has? with object ?radio?. We call this more
specific collocations propositions (Pe?nas and Hovy, 2010).
The paper is structured as follows. We will first present the resources used in this study. Section 3
presents the results of OSPD. Section 3.1 extends OSPD when, instead of documents, we take the com-
plete collection. Section 4 presents the study of OSPC both for syntactic dependencies and propositions.
Section 5 presents the experiments where OSPD and OSPC are used to improve the performance of
existing systems. Finally, we draw the conclusions and future work.
2 Resources used
AIDA is based on the corpus used in the CONLL named-entity recognition and classification task, where
all entities in full documents had been linked to the referred Wikipedia articles (using the 2010 Wikipedia
dump). We use the full AIDA dataset, with 1,393 documents, 34,140 disambiguated entity mentions,
where 27,240 are linked to a Wikipedia article. All in all there are 6,877 distinct mention strings (types)
which are linked at least once to a Wikipedia article. The rest refer to articles not in Wikipedia (NIL
instances), and were discarded. This corpus covers news from a sample of a few days spanning from
1996-05-28 to 1996-12-07.
In order to prepare our dataset for OSPC, we chose the dataset of the TAC KBP 2009 Entity Linking
competition, as this dataset have been extensively used in Entity Linking evaluation. In addition, the cor-
pus used in the task was very large, allowing us to mine relevant collocations (see below). We manually
annotated the occurrences in the extracted collocations, producing two datasets, one for each kind of col-
location (cf. Section 4). Note that the TAC KBP organizers only annotated one specific mention in each
target document. For completeness, we also tagged the rest of the occurrences of the target mentions in
the documents, thus allowing us to provide OSPD estimated based on TAC2009 data as well. This is
the third dataset that we annotated by hand. The hand-annotation was performed by a single person, and
later reviewed by the rest of the authors. The three annotation datasets are publicly available
5
. Hand-
5
http://ixa2.si.ehu.es/OEPDC
2262
NHasN ?U.S. dollar?
NPN ?condition of anonymity?
NVN ?official tells AFP?
NVNPN ?article maintains interest within layout?
NVPN ?others steal from input?
VNPN ?includes link to website?
Table 1: List of the six patterns used to extract propositions, with some examples.
tagging is costly, so we tagged around 250 examples of syntactic collocations and around 250 examples
of propositions.
Note that both AIDA and TAC2009 contain mentions that were not linked to a Wikipedia article
because the mention referred to an entity which was not listed in the entity inventory. We ignored all
those cases (called NIL cases), as we would need to investigate, for each NIL, which actual entity they
refer to.
The collocations were extracted from the TAC KBP collection (Ji et al., 2010), comprising 1.7 mil-
lion documents, 1.3 millions from newswire and 0.5 millions from the web. We have parsed them with
the Stanford CoreNLP software (Klein and Manning, 2003), obtaining around 650 million dependen-
cies (De Marneffe and Manning, 2008). We selected subject, object, prepositional complements and
adjectival modifiers as the source for syntactic collocations. In order to provide more specific collo-
cations, we implemented the syntactic patterns proposed in (Pe?nas and Hovy, 2010), which produce
so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the
six patterns used in this work, together with some examples.
In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which
lists, for each string mention, which entities it can refer to. We followed the construction method of
(Spitkovsky and Chang, 2012), which checked article titles, redirects, disambiguation pages and hyper-
links to find mention strings that can be used to refer to entities. Contrary to them, we could not access
hyperlinks in the web, so we could use only those in Wikipedia. According to our dictionary, the am-
biguity of the mentions that we are studying is very high, 26.4 entities on average for the mentions in
AIDA, and 62.6 entities on average for the mentions in TAC2009.
3 One entity per discourse
In order to estimate OSPD we divided the number of times a mention string referred to different entities
in the document with the number of times a mention string occurred multiple times in the document. In
the denominator and numerator we count each mention-document pair once.
Regarding AIDA, we found 12,084 occurrences of mentions which occurred more than once in a
document, making 4,265 unique mention-document pairs
6
(cf. Table 2). In the vast majority of the
cases those mentions refer to a single entity in the document, and only in 170 cases the mentions in the
document refer to several entities. The last row in Table 2 shows the ratio between those values, 96.01%,
showing that OSPD is strong in this dataset.
We also checked OSPD in the TAC2009 dataset. Out of the 138 distinct mention strings used in the
task, we discarded those only linked to NIL (that is, no corresponding Wikipedia article existed) and
those which were not ambiguous (that is, they had only one entity in the dictionary, cf. Section 2). That
leaves 105 mention strings, occurring 1,776 times in 918 different documents, which we annotated by
hand. The 105 strings occurred 1,776 times in 918 documents. Removing the cases where the mention
occurred only once, we were left with 1,173 occurrences, which make 334 unique mention-document
pairs, of which only 6 occurred with more than one sense (rightmost row in Table 2). This yields an
estimate for OSPD of 98.2%.
6
By unique mention-document pairs we mean that we only count once for a mention occurring multiple times in a document.
For instance if mention Smith occurs 10 times in the whole corpus, 8 times in document A and 2 times in document B, we count
two unique mention-document pairs.
2263
AIDA TAC2009
Mention-document pairs 4,265 334
Ambiguous pairs 170 6
OSPD 96.0% 98.2%
Table 2: One entity per discourse: per document statistics in AIDA and TAC2009 datasets. Pairs stand
for the number of unique mention-document pairs. The 4,265 pairs in AIDA correspond to 12,084
occurrences of mentions, and the 334 pairs in TAC2009 correspond to 1,173 occurrences.
All mentions First mention
AIDA TAC2009 AIDA TAC2009
Mention types 3,363 105 2,731 105
Ambiguous types 475 26 454 25
OSPD (collections) 85.9% 75.2% 83.4% 76.2%
Table 3: One entity per collection: statistics in AIDA and TAC2009. In the first two columns (?All
mentions?) we consider all mention types (3, 363 types in AIDA correspond to 23, 726 occurrences of
mentions, and 105 types in TAC2009 correspond to 1, 776 occurrences). In the second two columns
(?First mention?) we leave only the first mention of each document (in this case, there are 2, 731 mention
types in AIDA which correspond to 15, 275 occurrences, and 105 types in TAC2009 corresponding to
941 occurrences).
Finally, we also thought about measuring OSPD on the Wikipedia articles, where many mentions
have been manually linked to their respective article. Unfortunately, we noted that Wikipedia guidelines
explicitly prevent authors linking a mention multiple times: Generally, a link should appear only once
in an article, but if helpful for readers, links may be repeated in infoboxes, tables, image captions,
footnotes, and at the first occurrence after the lead
7
. The fact that Wikipedia editors did not explicitly
state exceptions to the above rule (e.g. for cases where the word or phrase is used to refer to two different
articles, thus breaking the OSPD hypothesis) is remarkable, and might indicate that Wikipedia editors
had not felt the need to challenge the OSPD hypothesis.
3.1 One entity per collection
We took the opportunity to also explore ?one entity per collection?, which gives an idea of what is
the spread of entities for whole document collections. In this case, there is no need to count mention-
document pairs, as there is one single document, the collection, so we estimate the hypothesis according
to mention types. The first two columns in table 3 shows that, overall, mentions which occurred more
than once in the collection tend to refer to the same entity 85.9% of the time in AIDA, and 75.2% of the
time in TAC2009.
As we know that multiple mentions in a document tend to refer to one entity, the second two columns
in table 3 offers the statistics when factoring out multiple occurrences of mention in a document, that is,
leaving the first mention in each document. The statistics are very similar, with minor variations.
We think that the lower estimate for TAC2009 is an artifact of how the TAC KBP organizers set up the
dataset, as they were explicitly looking for cases where the target string would refer to different entities,
making the task more challenging for NED systems. This fact does not affect OSPD for documents, as
those strings still tend to refer to a single entity per document, but given the need to find occurrences
for different entities, the organizers (Ji et al., 2010) did focus on strings occurring with different entities
across the document collection. This is in contrast with AIDA, where they tagged all named-entities
occurring in the target documents. Had the organizers of TAC2009 focused on a random choice of
strings and documents, the one entity per collection would also hold to the high degree exhibited in
AIDA, as the genre of most of the documents is also news (as in AIDA).
7
http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking#What_generally_
should_be_linked
2264
Syn. coll. Propositions
Mention-collocation pairs 58 61
Ambiguous pairs 5 1
OSPC 91.4% 98.4%
Table 4: One entity per collocation: statistics for syntactic collocations and propositions. The 58
mention-collocation pairs correspond to 262 occurrences, and the 61 mention-proposition pairs to 279.
4 One entity per collocation
In order to estimate OSPC for syntactic collocations, we manually annotated several occurrences of the
138 mention strings of the TAC2009 dataset. Hand-tagging mention entities is a costly process, so we
chose (at random) one syntactic dependency relation for each of the 138 mention strings that occurred
more than five times in the corpus. We then hand-tagged at random five occurrences of each collocation
(cf. Figure 2). This method would provide a maximum of 5 examples for each of the 138 mentions, but
after checking the minimum frequency of the collocations, the quality of the context, repeated sentences,
mentions that are not ambiguous in the dictionary, and whether the mention could be attached to an
entity in the database, the actual number was lower. All in all we found 58 mention-collocation pairs
(262 occurrences) for syntactic collocations (cf. middle column in Table 4). Only 5 mentions referred to
more than one entity per collocation, yielding that OSPC for syntactic collocation is around 91.4%.
To gather the dataset for propositions, we followed the same method as for the syntactic collocations,
that is, we chose (at random) one propositions involving one of the 138 mention strings that occurred
more than five times in the corpus, and hand-tagged at random five occurrences of each proposition
(cf. Figure 2. As with syntactic collocations, we also found a limited number of mentions filling the
desired properties. That left 61 mention-collocation pairs (279 occurrences) for propositions (cf. right
column in Table 4). Only 1 mention referred to more than one entity per proposition, yielding OSPC
for propositions around 98.4%. This shows that the more specific the context is, the stronger is the link
between mention and entity.
5 Improving performance
In order to check whether any of the ?one sense? hypothesis above could improve the performance of
a NED system, we followed a simple procedure: After running the NED system, we take its output
and observe, for each mention string, which is the entity returned most often for a given document (or
collocation), assigning to all occurrences the majority entity. In case of ties, we return the entity with the
highest support from the NED system. We tested the improvements on three NED systems: the freely
available DBpedia Spotlight, a reimplementation of a strong Bayesian NED system and a graph-based
system.
DBpedia Spotlight is a freely available NED system (Daiber et al., 2013), based on a generative proba-
bilistic model (Han and Sun, 2011). Nowadays it is one of the most widely used NED systems and attains
performances close to state-of-the-art (Daiber et al., 2013)We used the default values of the parameters
for all the experiments in this paper.
We also tested an in-house reimplementation of the generative probabilistic model presented in (Han
and Sun, 2011). This is a state-of-the-art system which got the same accuracy as the best participant
(72.0) when evaluated in the non-NIL subset of TAC2013.
UKB is a freely-available system for performing Word Sense Disambiguation and Similarity based
on random walks on graphs (Agirre and Soroa, 2009). Instead of using it on WordNet, we represented
Wikipedia as a graph, where vertices are the wikipedia articles and edges represents bidirectional hy-
perlinks among Wikipedia pages, effectively implementing a NED system. We used a Wikipedia dump
from 2013 in our experiments. UKB is a competitive, state-of-the-art system which attained a score of
69.0 when evaluated in the non-NIL subset of the TAC2013 dataset.
The input of the systems is the context of each mention to be disambiguated, in the form of a 100
token window centered in the target mention. In NED, the identification of the correct mention to be
2265
Mention in context Entity
Abbott Beefs Up Litigation ... Abbot Kinney
Abbott Laboratories Inc., bracing ... Abbott Laboratories
Abbott said it had restated ... Abbott Laboratories
venture between Abbott and Takeda ... Abbott Laboratories
Abbott said in January ... Abbott Laboratories
Figure 3: Applying OSPD: Each of the five occurrences of Abbott in the document in Figure 1 has been
tagged independently by a NED systems, which return the correct entity in all but one case (precision
80%). Applying OSPD would return the correct entity (Abbott Laboratories) in all cases, improving
precision to 100%.
AIDA TAC 2009
Prec. Recall F1 Prec. Recall F1
Spotlight 83.24 63.90 72.30 64.48 46.44 53.99
+ OSPD Discourse 84.17 70.01 76.44 64.65 48.50 55.42
+ OSPD Collection 84.02 74.64 79.05 56.24 47.98 51.78
UKB 70.09 69.03 69.55 67.70 67.64 67.67
+ OSPD Discourse 71.30 70.23 70.76 70.21 70.21 70.21
+ OSPD Collection 75.79 74.64 75.21 68.84 68.84 68.84
(Han and Sun, 2011) 65.71 65.11 65.41 65.49 65.49 65.49
+ OSPD Discourse 67.77 67.37 67.57 66.27 66.27 66.27
+ OSPD Collection 74.29 73.89 74.09 68.24 68.24 68.24
Table 5: Applying OSPD: NED performance on AIDA and TAC2009 OSPD datasets, including each of
the three NED systems, and the results after applying OSPD at the document and collections levels. Bold
marks best result for each system.
disambiguated is part of the problem. AIDA does provide gold mentions, but TAC2009 only provides a
query string which might be just a substring of the real mention in the document. We treated both corpus
in the same way. In the case of DBpedia Spotlight we use the built-in mention spotter. In the case of our
in-house implementations, we use the longest string that matches a valid entity mention in the system, as
given by the dictionary (cf. Section 3).
Some of the NED systems do not return an entity for all mentions, so we evaluate precision, recall and
the harmonic mean (F1 measure). Statistical significance has been estimated using Wilcoxon. We reused
the same corpora as in the previous sections for the evaluation, and also removed all NIL mentions (i.e.
mentions which refer to an entity not in Wikipedia).
5.1 One entity per discourse
We report the improvements using OSPD for both document and collection levels. At the document
level, we relabel mentions that occur multiple times in a document using the entity returned most times
by the NED system in that document. Figure 3 illustrates the idea for a NED system on the same sample
document as in Figure 1. At the collection level, we relabel mentions using the entity returned most
times by the NED systems in the whole collection.
Table 5 reports the results of the performance as evaluated on mentions occurring multiple times in
the AIDA and TAC2009 datasets. The numbers in the left part of the table correspond to the perfor-
mance as evaluated on mentions occurring multiple times in AIDA documents. Note that the number of
occurrences where OSPD at the collection level can be applied is larger (a superset of those for OSPD
at the document level), as, for instance, a mention string occurring once in three different documents
won?t be affected by OSPD at the document level, but it could be relabeled at the collection level. We
were especially interested in making the numbers between OSPD at the document and collection levels
2266
CPI subject-of rise Angela Merkel has CDU:
Consumer price index Christian Democratic Union (Germany)
Consumer price index Catholic Distance University
Communist Party of India Christian Democratic Union (Germany)
Communist Party of India Christian Democratic Union (Germany)
Consumer price index Christian Democratic Union (Germany)
Figure 4: Applying OSPC: A NED system system tagged each example in Figure 2 independently. For
CPI, the precision is 60%, but after relabeling with OSPC it would be 100%. For CDU, the improvement
is from 80% to 100%.
Syntactic collocations Propositions
prec. recall F1 prec. recall F1
Spotlight 82.46 66.41 73.57 74.67 60.22 66.67
+ OSPC 82.63 67.18 74.11 74.79 62.72 68.23
UKB 75.86 75.57 75.72 67.87 67.38 67.63
+ OSPC 78.54 78.24 78.39 68.59 68.10 68.35
(Han and Sun, 2011) 75.57 75.57 75.57 71.33 71.33 71.33
+ OSPC 78.24 78.24 78.24 73.12 73.12 73.12
Table 6: Applying OSPC: NED performance on TAC2009, including each of the three NED systems,
and the results after applying OSPC for syntactic collocations and propositions. Bold is used for best
results for each system.
directly comparable, and therefore report the results on the same occurrences, that is, the occurrences
where OSPD at the document level can be applied.
The results show a small but consistent improvement for OSPD at the document level in precision,
recall and F1 for the three NED systems, around 1 or 2 absolute points. The improvements when applying
OSPD at the collection level are also consistent, but remarkably larger, between 5 and 9 absolute points.
All improvements are statistically significant (p-value below 0.01).
Table 5 also reports the results after applying OSPD to TAC2009 instances which occurred more
than once in a document. Results for OSPD at document level and collection level follow the same
methodology as for AIDA. The improvement at the collection level is not so consistent, with a loss in
performance for Spotlight, a small improvement for UKB, and a larger improvement for (Han and Sun,
2011). All differences across the table are statistically significant (p-value below 0.01).
While the OSPD at the document level is strong in both corpora, Section 3.1 showed that the OSPD
at the collection level is only strong in AIDA, with a much lower estimate in TAC2009. This fact
would explain why the improvement with OSPD at the collection level is not consistent. Following
the rationale in Section 3.1, we think that had the organizers of the task chosen strings and documents
at random, the improvement in TAC 2009 at the collection level would be also as high as in AIDA. The
high improvement in AIDA at the collection level compared to the more modest improvement at the
document level, despite having a lower OSPD estimate (cf. Section 3.1), could be caused by the fact that
there are more occurrences and evidence in favor of the majority entity.
5.2 One entity per collocation
Figure 4 shows the application of OSPC to the output of a NED system to two sample collocations in our
dataset. In this case, the application of OSPC would increase precision to 100%. The actual result on the
datasets produced in Section 4 for syntactic collocations and propositions is reported on table 6.
Regarding syntactic collocations, table 6 shows that the improvement is small but consistent for the
three systems on precision, recall and F1, ranging from 0.5 to 2.5 absolute points in F1 score. The results
for propositions also show the same trend, with consistent improvements across the table. All differences
2267
in the two tables are statistically significant (p-value < 0.01), except for UKB.
6 Conclusions and future work
Our study shows that OSPD holds for 96%-98% (in the AIDA and TAC2009 datasets, respectively)
of the mentions that occur multiple times in documents. We also measured OSPD at the collection
level (86% and 75%, respectively). OSPC holds for 91% of the mentions that occur multiple times in
the syntactic collocations that we studied, and 98% of the mentions that occur multiple times in more
specific collocations. We reused the publicly available AIDA dataset for estimating OSPD. In addition,
we created a dataset to study OSPC based on the TAC KBP Entity Linking 2009 task dataset, which is
publicly available
8
.
We carefully chose to estimate both OPSD and OSPC on TAC2009, in order to make the numbers
between OSPD and OSPC comparable. The OSPD numbers for AIDA are very similar to those obtained
on TAC2009, providing complementary evidence. Although the high estimate of OSPD for entities was
somehow expected, the high estimate of OSPC for the syntactic collocations, especially the propositions,
was somehow unexpected, given the high ambiguity rate of the discussed strings, and the fact that the
ambiguity included similar entities, like for instance ?ABC? which can refer, among other 190 entities,
to the American Broadcasting Company or the Australian Broadcasting Corporation.
Our results also show that a simple application of the OSPD and OSPC hypotheses to the output of
three different NED systems improves the results in all cases. Remarkably, the highest performance gain,
8 absolute points, was for OSPD at the collection level in the AIDA corpus.
The results presented here could be largely dependent on the domain and genre of the documents,
as well as the definition of collocation. Our work is a strong basis for claiming that OSPD and OSPC
hold for entities, but the evidence could be further extended exploring alternative operationalization of
collocations and a larger breadth of genres and domains.
For the future we would like to check whether these hypotheses can be further used to improve current
NED systems. The OSPD hypothesis can be used to jointly disambiguate all occurrences of a mention
in a document. The OSPC hypothesis could be used to acquire important disambiguation features, or to
perform large-scale joint entity linking. The OSPD for whole collections could be useful for documents
on specific domains, and for domain adaptation scenarios.
Acknowledgements
This work was partially funded by MINECO (CHIST-ERA READERS project ? PCIN-2013-002- C02-
01) and the European Commission (QTLEAP ? FP7-ICT-2013.4.1-610516, OPENER ? FP7-ICT-2011-
SME-DCL-296451). Ander Barrena is supported by a PhD grant from the University of the Basque
Country.
References
Eneko Agirre and Philip Edmonds. 2007. Word Sense Disambiguation: Algorithms and Applications. Springer
Publishing Company, Incorporated, 1st edition.
Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings
of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ?09,
pages 33?41.
Marine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ?09, pages 19?27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes. 2013. Improving efficiency and accuracy
in multilingual entity extraction. In Proceedings of the 9th International Conference on Semantic Systems,
I-SEMANTICS ?13, pages 121?124, New York, NY, USA. ACM.
8
http://ixa2.si.ehu.es/OEPDC
2268
Marie-Catherine De Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. URL
http://nlp. stanford. edu/software/dependencies manual. pdf.
William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of
the workshop on Speech and Natural Language, HLT ?91, page 233237, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2012. Evaluating Entity
Linking with Wikipedia. Artif. Intell., 194:130?150, January.
Xianpei Han and Le Sun. 2011. A Generative Entity-mention Model for Linking Entities with Knowledge Base.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, volume 1, pages 945?954.
Johannes Hoffart, Mohamed A. Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stephan Thater, and Gerdhard Weikum. 2011. Robust Disambiguation of Named Entities in Text.
In Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, United Kingdom
2011, pages 782?792.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the tac 2010 knowledge
base population track. In Third Text Analysis Conference (TAC 2010).
Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
Robert Krovetz. 1998. More than one sense per discourse. In NEC Princeton NJ Labs., Research Memorandum.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics,
39(4).
David Martinez and Eneko Agirre. 2000. One sense per collocation and genre/topic variations. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large
corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics -
Volume 13, EMNLP ?00, page 207215, Stroudsburg, PA, USA. Association for Computational Linguistics.
Anselmo Pe?nas and Eduard Hovy. 2010. Filling knowledge gaps in text for machine reading. In Proceedings
of the 23rd International Conference on Computational Linguistics: Posters, pages 979?987. Association for
Computational Linguistics.
Valentin I. Spitkovsky and Angel X. Chang. 2012. A Cross-lingual Dictionary for English Wikipedia Concepts.
Eighth International Conference on Language Resources and Evaluation (LREC 2012).
David Yarowsky. 1993. One sense per collocation. In Proceedings of the workshop on Human Language Tech-
nology, HLT ?93, page 266271, Stroudsburg, PA, USA. Association for Computational Linguistics.
2269
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 107?116,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Temporally Anchored Relation Extraction
Guillermo Garrido, Anselmo Pen?as, Bernardo Cabaleiro, and A?lvaro Rodrigo
NLP & IR Group at UNED
Madrid, Spain
{ggarrido,anselmo,bcabaleiro,alvarory}@lsi.uned.es
Abstract
Although much work on relation extraction
has aimed at obtaining static facts, many of
the target relations are actually fluents, as their
validity is naturally anchored to a certain time
period. This paper proposes a methodologi-
cal approach to temporally anchored relation
extraction. Our proposal performs distant su-
pervised learning to extract a set of relations
from a natural language corpus, and anchors
each of them to an interval of temporal va-
lidity, aggregating evidence from documents
supporting the relation. We use a rich graph-
based document-level representation to gener-
ate novel features for this task. Results show
that our implementation for temporal anchor-
ing is able to achieve a 69% of the upper
bound performance imposed by the relation
extraction step. Compared to the state of the
art, the overall system achieves the highest
precision reported.
1 Introduction
A question that arises when extracting a relation is
how to capture its temporal validity: Can we assign a
period of time when the obtained relation held? As
pointed out in (Ling and Weld, 2010), while much
research in automatic relation extraction has focused
on distilling static facts from text, many of the tar-
get relations are in fact fluents, dynamic relations
whose truth value is dependent on time (Russell and
Norvig, 2010).
The Temporally anchored relation extraction
problem consists in, given a natural language text
document corpus, C, a target entity, e, and a target
relation, r, extracting from the corpus the value of
that relation for the entity, and a temporal interval
for which the relation was valid.
In this paper, we introduce a methodological ap-
proach to temporal anchoring of relations automat-
ically extracted from unrestricted text. Our system
(see Figure 1) extracts relational facts from text us-
ing distant supervision (Mintz et al, 2009) and then
anchors the relation to an interval of temporal va-
lidity. The intuition is that a distant supervised sys-
tem can effectively extract relations from the source
text collection, and a straightforward date aggrega-
tion can then be applied to anchor them. We pro-
pose a four step process for temporal anchoring:
(1) represent temporal evidence; (2) select tempo-
ral information relevant to the relation; (3) decide
how a relational fact and its relevant temporal in-
formation are themselves related; and (4) aggregate
imprecise temporal intervals across multiple docu-
ments. In contrast with previous approaches that
aim at intra-document temporal information extrac-
tion (Ling and Weld, 2010), we focus on mining
a corpus aggregating temporal evidences across the
supporting documents.
We address the following research questions:
(1) Validate whether distant supervised learning is
suitable for the task, and evaluate its shortcomings.
(2) Explore whether the use of features extracted
from a document-level rich representation could im-
prove distant supervised learning. (3) Compare the
use of document metadata against temporal expres-
sions within the document for relation temporal an-
choring. (4) Analyze how, in a pipeline architecture,
the propagation of errors limits the overall system?s
107
Training
(1) IR candidate document 
retrieval
(3) Distant supervised
      learning
(5) Relation Extraction
(6) Temporal Anchoring
Document
Collection
Document 
Index
(2) Document 
Representation
(4) Classifiers
Knowledge
Base
Training seeds
< entity, relation name, value >
Training
examples
+ / -
relation 
instances
unlabelled
candidate
Training
Application
Date Extraction
output:
temporally 
anchored 
relations
Date 
Aggregation
Input: Query 
entity
Figure 1: System overview diagram.
performance.
The representation we use for temporal informa-
tion is detailed in section 2; the rich document-level
representation we exploit is described in section 3.
For a query entity and target relation, the system first
performs relation extraction (section 4); then, we
find and aggregate time constraint evidence for the
same relation across different documents, to estab-
lish a temporal validity anchor interval (section 5).
Empirical comparative evaluation of our approach is
introduced in section 6; while some related work is
shown in section 7 and conclusions in section 8.
2 Temporal Anchors
We will denominate relation instance a triple
?entity, relation name, value?. We aim at anchor-
ing relation instances to their temporal validity. We
need a representation flexible enough to capture the
imprecise temporal information available in text,
but expressed in a structured style. Allen?s (1983)
interval-based algebra for temporal representation
and reasoning, underlies much research, such as the
Tempeval challenges (Verhagen et al, 2007; Puste-
jovsky and Verhagen, 2009). Our task is different,
as we focus on obtaining the temporal interval as-
sociated to a fact, rather than reasoning about the
temporal relations among the events appearing in a
single text.
Let us assume that each relation instance is valid
during a certain temporal interval, I = [t0, tf ]. This
sharp temporal interval fails to capture the impreci-
sion of temporal boundaries conveyed in natural lan-
guage text. The Temporal Slot Filling task at TAC-
KBP 2011 (Ji et al, 2011) proposed a 4-tuple rep-
resentation that we will refer to as imprecise anchor
intervals. An imprecise temporal interval is defined
as an ordered 4-tuple of time points: (t1, t2, t3, t4),
with the following semantics: the relation is true for
a period which starts at some point between t1 and
t2 and ends between t3 and t4. It should hold that:
t1 ? t2, t3 ? t4, and t1 ? t4. Any of the four
endpoints can be left unconstrained (t1 or t3 would
be ??, and t2 or t4 would be +?). This represen-
tation is flexible and expressive, although it cannot
capture certain types of information (Ji et al, 2011).
3 Document Representation
We use a rich document representation that employs
a graph structure obtained by augmenting the syn-
tactic dependency analysis of the document with se-
mantic information.
A document D is represented as a document
graph GD; with node set VD and edge set, ED. Each
node v ? VD represents a chunk of text, which is a
sequence of words1. Each node is labeled with a
dictionary of attributes, some of which are common
for every node: the words it contains, their part-of-
speech annotations (POS) and lemmas. Also, a rep-
resentative descriptor, which is a normalized string
value, is generated from the chunks in the node. Cer-
tain nodes are also annotated with one or more types.
There are three families of types: Events (verbs
that describe an action, annotated with tense, polar-
ity and aspect); standardized Time Expressions; and
Named Entities, with additional annotations such as
gender or age.
Edges in the document graph, e ? ED, represent
four kinds of relations between the nodes:
? Syntactic: a dependency relation.
? Coreference: indicates that two chunks refer to
1Most chunks consist in one word; we join words into a
chunk (and a node) in two cases: a multi-word named entity
and a verb and its auxiliaries.
108
David[NNP,David]
NER: PERSON
DESCRIPTOR: 
David
POS: N 
Julia[NNP,Julia]
CLASS:WIFE
NER: PERSON
DESCRIPTOR: 
Julia
POS: N
GENDER:FEMALE 
September[NNP,September] 1979[CD,1979]
NER:DATE
TIMEVALUE:197909
DESCRIPTOR: September 1979
POS: NNP 
wife[NN,wife]
DESCRIPTOR: 
wife
POS: NN 
is[VBZ,be] celebrating[VBG,celebrate]
ASPECT:PROGRESSIVE
TENSE:PRESENT
POLARITY:POS
DESCRIPTOR: celebrate
POS: V 
birthday[NN,birthday]
DESCRIPTOR: 
birthday
POS: NN 
was[VBD,be] born[VBN,bear]
ASPECT:NONE
TENSE:PAST
POLARITY:POS
DESCRIPTOR: bear
POS: V 
arg0
hasClass
prep_in
arg1
arg1
has
INCLUDES
has_wife
Figure 2: Collapsed document graph representation, GC ,
for the sample text document ?David?s wife, Julia, is cel-
ebrating her birthday. She was born in September 1979?.
the same discourse referent.
? Semantic relations between two nodes, such as
hasClass, hasProperty and hasAge.
? Temporal relations between events and time ex-
pressions.
The processing includes dependency parsing,
named entity recognition and coreference reso-
lution, done with the Stanford CoreNLP soft-
ware (Klein and Manning, 2003); and events and
temporal information extraction, via the TARSQI
Toolkit (Verhagen et al, 2005).
The document graph GD is then further trans-
formed into a collapsed document graph, GC . Each
node of GC clusters together coreferent nodes, rep-
resenting a discourse referent. Thus, a node u in GC
is a cluster of nodes u1, . . . , uk of GD. There is an
edge (u, v) in GC if there was an edge between any
of the nodes clustered into u and any of the nodes
v1, . . . , vk? . The coreference edges do not appear in
this representation. Additional semantic information
is also blended into this representation: normaliza-
tion of genitives, semantic class indicators inferred
from appositions and genitives, and gender annota-
tion inferred from pronouns. A final graph example
can be seen in Figure 2.
4 Distant Supervised Relation Extraction
To perform relation extraction, our proposal fol-
lows a distant supervision approach (Mintz et al,
2009), which has also inspired other slot filling sys-
tems (Agirre et al, 2009; Surdeanu et al, 2010).
We capture long distance relations by introducing
a document-level representation and deriving novel
features from deep syntactic and semantic analysis.
Seed harvesting. From a reference Knowledge
Base (KB), we extract a set of relation triples
or seeds: ?entity, relation, value?, where the
relation is one of the target relations. Our
document-level distant supervision assumption is
that if entity and value are found in a document
graph (see section 3), and there is a path connect-
ing them, then the document expresses the relation.
Relation candidates gathering. From a seed triple,
we retrieve candidate documents that contain both
the entity and value, within a span of 20 tokens,
using a standard IR approach. Then, entity and
value are matched to the document graph represen-
tation. We first use approximate string comparison
to find nodes matching the seed entity. After an en-
tity node has been found we use local breadth-first-
search (BFS) to find a matching value and the short-
est connecting path between them. We enforce the
Named Entity type of entity and value to match a
expected type, predefined for the relation.
Our procedure traverses the document graph look-
ing for entity and value nodes meeting those condi-
tions; when found, we generate features for a pos-
itive example for the relation2. If we encounter a
node that matches the expected NE type of the rela-
tion, but does not match the seed value, we generate
a negative example for that relation.
Training. From positive and negative examples, we
generate binary features; some of them are inspired
by previous work (Surdeanu and Ciaramita, 2007;
Mintz et al, 2009; Riedel et al, 2010; Surdeanu et
al., 2010), and others are novel, taking advantage of
our graph representation. Table 1 summarizes our
choice of features. Features appearing in less than 5
training examples were discarded.
Relation instance extraction. Given an input entity
and a target relation, we aim at finding a filler value
for a relation instance. This task is known as Slot
Filling. From the set of retrieved documents relevant
to the query entity, represented as document graphs,
2From the collapsed document graph representation we ob-
tained an average of 9213 positive training examples per slot;
from the uncollapsed document graph, a slightly lower average
of 8178.5 positive examples per slot.
109
Feature name Description
path dependency path between ENTITY and
VALUE in the sentence
X-annotation NE annotations for X
X-pos Part-of-speech annotations for X
X-gov Governor of X in the dependency path
X-mod Modifiers of X in the dependency path
X-has age X is a NE, with an age attribute
X-has class-C X is a NE, with a class C
X-property-P X is a NE, and it has a property P
X-has-Y X is a NE, with a possessive relation with
another NE, Y
X-is-Y X is a NE, in a copula with another NE, Y
X-gender-G X is a NE, and it has gender G
V -tense Tense of the verb V in the path
V -aspect Aspect of the verb V in the path
V -polarity Polarity (positive or negative) of the verb V
Table 1: Features included in the model. X stands for
ENTITY and VALUE. Verb features are generated from
the verbs, V , identified in the path between ENTITY and
VALUE.
we locate matching entities and start a local BFS of
candidate values, generating for them an unlabelled
example. For each of the relations to extract, a bi-
nary classifier (extractor) decides whether the exam-
ple is a valid relation instance. For each particular
relation classifier, only candidates with the expected
entity and value types for the relation were used in
the application phase. Each extractor was a SVM
classifier with linear kernel (Joachims, 2002). All
learning parameters were set to their default values.
The classification process yields a predicted class
label, plus a real number indicating the margin. We
performed an aggregation phase to sum the mar-
gins over distinct occurrences of the same extracted
value. The rationale is that when the same value is
extracted from more than one document, we should
accumulate that evidence.
The output of this phase is the set of extracted re-
lations (positive for each of the classifiers), plus the
documents where the same fact was detected (sup-
porting documents).
5 Temporal Anchoring of Relations
In this section, we propose and discuss a unified
methodological approach for temporal anchoring of
relations. We assume the input is a relation instance
and a set of supporting documents. The task is es-
tablishing a imprecise temporal anchor interval for
the relation.
We present a four-step methodological approach:
(1) representation of intra-document temporal infor-
mation; (2) selection of relevant temporal informa-
tion for the relation; (3) mapping of the link between
relational fact and temporal information into an in-
terval; and (4) aggregation of imprecise intervals.
Temporal representation. The first methodologi-
cal step is to obtain and represent the available intra-
document temporal information; the input is a doc-
ument, and the task is to identify temporal signals
and possible links among them. We use the term link
for a relation between a temporal expression (a date)
and an event; we want to avoid confusion with the
term relation (a relational fact extracted from text).
In our particular implementation:
? We use TARSQI to extract temporal expressions
and link them to events. In particular, TARSQI
uses the following temporal links: included, si-
multaneous, after, before, begun by or ended.
? We focus also on the syntactic pattern [Event-
preposition-Time] within the lexical context of the
candidate entity and value.
? Both are normalized into one from a set of prede-
fined temporal links: within, throughout, begin-
ning, ending, after and before.
Selection of temporal evidence. For each docu-
ment and relational instance, we have to select those
temporal expressions that are relevant.
a. Document-level metadata. The default value
we use is the document creation time (DCT),
if available. The underlying assumption is that
there is a within link from each fact expressed in
the text and the document creation time.
b. Temporal expressions. Temporal evidence
comes also from the temporal expressions
present in the context of a relation. In our par-
ticular implementation, we followed a straight-
forward approach, looking for the time expres-
sion closest in the document graph to the short-
est path between the entity and value nodes. This
search is performed via a limited depth BFS,
starting from the nodes in the path, in order from
value to entity.
Mapping of temporal links into intervals. The
third step is deciding how a relational fact and its rel-
evant temporal information are themselves related.
We have to map this information, expressed in text,
110
Temporal link Constraints mapping
Before t4 = first
After t1 = last
Within and Throughout t2 = first and t3 = last
Beginning t1 = first and t2 = last
Ending t3 = first and t4 = last
Table 2: Mapping from time expression and temporal re-
lation to temporal constraints.
to a temporal representation. We will use the impre-
cise anchor intervals described is section 2.
Let T be a temporal expression identified in the
document or its metadata. Now, the mapping of tem-
poral constraints depends on the temporal link to the
time expression identified; also, the semantics of the
event have to be considered in order to decide the
time period associated to a relation instance. This
step is important because the event could refer just to
the beginning of the relation, its ending, or both. For
instance, it is obvious that having the event marry
is different to having the event divorce, when decid-
ing the temporal constraints associated to the spouse
relation.
Table 2 shows our particular mapping between
temporal links and constraints. In particular, for the
default document creation time, we suppose that a
relation which appears in a document with creation
time d held true at least in that date; that is, we are
assuming a within link, and we map t2 = d, t3 = d.
Inter-document temporal evidence aggregation.
The last step is aggregating all the time constraints
found for the same relation and value across differ-
ent documents. If we found that a relation started af-
ter two dates d and d?, where d? > d, the closest con-
straint to the real start of the relation is d?. Mapped to
temporal constraints, it means that we would choose
the biggest t1 possible. Following the same reason-
ing, we would want to maximize t3. On the other
side, when a relation started before two dates d2 and
d?2, where d
?
2 > d2, the closest constraint is d2 and
we would choose the smallest t2. In summary, we
will maximize t1 and t3 and minimize t2 and t4, so
we will narrow the margins.
6 Evaluation
We have used for our evaluation the dataset com-
piled within the TAC-KBP 2011 Temporal Slot Fill-
ing Task (Ji et al, 2011). We employed as initial
KB the one distributed to participants in the task,
which has been compiled from Wikipedia infoboxes.
It contains 898 triples ?entity, slot type, value? for
100 different entities and up to 8 different slots (re-
lations) per entity3. This gold standard contains the
correct responses pooled from the participant sys-
tems plus a set of responses manually found by
annotators. Each triple has associated a temporal
anchor. The relations had to be extracted from a
domain-general collection of 1.7 million documents.
Our system was one of the five that took part in
the task.We have evaluated the overall system and
the two main components of the architecture: Rela-
tion Extraction, and Temporal Anchoring of the re-
lations. Due to space limitations, the description of
our implementation is very concise; refer to (Garrido
et al, 2011) for further details.
6.1 Evaluation of Relation Extraction
System response in the relation extraction step con-
sists in a set of triples ?entity, slot type, value?.
Performance is measured using precision, recall and
F-measure (harmonic mean) with respect to the 898
triples in the key. Target relations (slots) are poten-
tially list-valued, that is, more than one value can
be valid for a relation (possibly at different points
in time). Only correct values yield any score, and
redundant triples are ignored.
Experiments. We run two different system settings
for the relation extraction step. They differ in the
document representation used (detailed in section3),
in order to empirically assess whether clustering of
discourse referents into single nodes benefits the ex-
traction. In SETTING 1, each document is repre-
sented as a document graph, GD, while in SETTING
2 collapsed document graph representation, GC , is
employed.
Results. Results are shown in Table 3 in the col-
umn Relation Extraction. Both settings have a sim-
ilar performance with a slight increase in the case
of graphs with clustered referents. Although preci-
sion is close to 0.5, recall is lower than 0.1. We have
studied the limits of the assumptions our approach
3There are 7 person relations: cities of residence, state-
orprovinces of residence, countries of residence, employee of,
member of, title, spouse, and an organization relation:
top members/employees.
111
is based on. First, our standard retrieval component
performance limits the overall system?s. As a matter
of example, if we retrieve the first 100 documents
per entity, we find relevant documents only for 62%
of the triples in the key. This number means that no
matter how good relation extraction method is, 38%
of relations will not be found.
Second, the distant supervision assumption un-
derlying our approach is that for a seed relation in-
stance ?entity, relation, value?, any textual men-
tion of entity and value expresses the relation. It
has been shown that this assumption is more often
violated when training knowledge base and docu-
ment collection are of different type, e.g. Wikipedia
and news-wire (Riedel et al, 2010). We have real-
ized that a more determinant factor is the relation
itself and the type of arguments it takes. We ran-
domly sampled 100 training examples per relation,
and manually inspected them to assess if they were
indeed mentions of the relation. While for the re-
lation cities of residence only 30% of the training
examples are expressing the relation, for spouse the
number goes up to 59%. For title, up to 90% of the
examples are correct. This fact explains, at least par-
tially, the zeros we obtain for some relations.
6.2 Evaluation of Temporal Anchoring
Under the evaluation metrics proposed by TAC-KBP
2011, if the value of the relation instance is judged
as correct, the score for temporal anchoring depends
on how well the returned interval matches the one
provided in the key. More precisely, let the correct
imprecise anchor interval in the gold standard key
be Sk = (k1, k2, k3, k4) and the system response be
S = (r1, r2, r3, r4). The absence of a constraint in
t1 or t3 is treated as a value of ??; the absence of
a constraint in t2 or t4 is treated as a value of +?.
Then, let di = |ki ? ri|, for i ? 1, . . . , 4, be the
difference, a real number measured in years. The
score for the system response is:
Q(S) =
1
4
4?
i=1
1
1 + di
The score for a target relation Q(r) is computed
by summing Q(S) over all unique instances of the
relation whose value is correct. If the gold standard
contains N responses, and the system output M re-
sponses, then precision is: P = Q(r)/M , and recall:
R = Q(r)/N ; F1 is the harmonic mean of P and R.
Experiments. We evaluated two different set-
tings for the temporal anchoring step; both use
the collapsed document graph representation, GC
(SETTING 2). The goal of the experiment is two-
fold. First, test the strength of the document creation
time as evidence for temporal anchoring. Second,
test how hard this metadata-level baseline is to beat
using contextual temporal expressions.
The SETTING 2-I assumes a within temporal link
between the document creation time and any relation
expressed inside the document, and aggregates this
information across the documents that we have iden-
tified as supporting the relation. The SETTING 2-II
considers documents content in order to extract tem-
poral links from the context of the text that expresses
the relation. If no temporal expression is found, the
date of the document is used as default. Temporal
links from all supporting documents are mapped into
intervals and aggregated as detailed in section 5.
The performance on relation extraction is an up-
per bound for temporal anchoring, attainable if tem-
poral anchoring is perfect. Thus, we also evaluate
the temporal anchoring performance as the percent-
age the final system achieves with respect to the re-
lation extraction upper bound.
Results. Results are shown in Table 3 under column
Temporal Anchoring. They are low, due to the upper
bound that error propagation in candidate retrieval
and relation extraction imposes upon this step: tem-
porally anchoring alone achives 69% of its upper
bound. This value corresponds to the baseline SET-
TING 2-I, showing its strength. The difference with
SETTING 2-II shows that this baseline is difficult
to beat by considering temporal evidence inside the
document content. There is a reason for this. The
temporal link mapping into time intervals does not
depend only on the type of link, but also on the se-
mantics of the text that expresses the relation as we
pointed out above. We have to decide how to trans-
form the link between relation and temporal expres-
sion into a temporal interval. Learning a model for
this is a hard open research problem that has a strong
adversary in the baseline proposed.
112
Relation Extraction Temporal Anchoring
SETTING 1 SETTING 2 SETTING 2-I SETTING 2-II
P R F P R F P R F % P R F %
(1) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(2) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(3) 0.33 0.02 0.03 0 0 0 0 0 0 0 0 0 0 0
(4) 0.22 0.09 0.13 0.29 0.11 0.16 0.23 0.09 0.13 79 0.21 0.08 0.11 72
(5) 0.53 0.13 0.20 0.54 0.12 0.19 0.34 0.07 0.12 63 0.30 0.06 0.11 56
(6) 0.70 0.12 0.20 0.75 0.13 0.22 0.57 0.10 0.16 76 0.50 0.08 0.14 67
(7) 0.50 0.06 0.10 0.50 0.07 0.12 0.29 0.04 0.07 58 0.25 0.04 0.06 50
(8) 0.25 0.04 0.07 0.20 0.04 0.07 0.15 0.03 0.05 75 0.06 0.01 0.02 30
(9) 0.42 0.08 0.14 0.45 0.08 0.14 0.31 0.06 0.10 69 0.27 0.05 0.09 60
Table 3: Results of experiments for each relation: (1) per:stateorprovinces of residence; (2) per:employee of; (3)
per:countries of residence; (4) per:member of; (5) per:title; (6) org:top members/employees; (7) per:spouse; (8)
per:cities of residence; (9) overall results (calculated as a micro-average).
System # Filled Precision Recall F1
BLENDER2 1206 0.1789 0.3030 0.2250
BLENDER1 1116 0.1796 0.2942 0.2231
BLENDER3 1215 0.1744 0.2976 0.2199
IIRG1 346 0.2457 0.1194 0.1607
Setting 2-1 167 0.2996 0.0703 0.1139
Setting 2-2 167 0.2596 0.0609 0.0986
Stanford 12 5140 0.0233 0.1680 0.0409
Stanford 11 4353 0.0238 0.1453 0.0408
USFD20112 328 0.0152 0.0070 0.0096
USFD20113 127 0.0079 0.0014 0.0024
Table 4: System ID, number of filled responses of the
system, precision, recall and F measure.
6.3 Comparative Evaluation
Our approach was compared with the other four
participants at the KBP Temporal Slot Filling Task
2011. Table 4 shows results sorted by F-measure in
comparison to our two settings (described above).
These official results correspond to a previous
dataset containing 712 triples4.
As shown in column Filled our approach returns
less triples than other systems, explaining low recall.
However, our system achieves the highest precision
for the complete task of temporally anchored rela-
tion extraction. Despite low recall, our system ob-
tains the third best F1 value. This is a very promis-
ing result, since several directions can be explored
to consider more candidates and increase recall.
7 Related Work
Compiling a Knowledge Base of temporally an-
chored facts is an open research challenge (Weikum
et al, 2011). Despite the vast amount of research fo-
cusing on understanding temporal expressions and
4Slot-fillers from human assessors were not considered
their relation to events in natural language, the com-
plete problem of temporally anchored relation ex-
traction remains relatively unexplored. Also, while
much research has focused on single-document ex-
traction, it seems clear that extracting temporally an-
chored relations needs the aggregation of evidences
across multiple documents.
There have been attempts to extend an existing
knowledge base. Wang et al (2010) use regular
expressions to mine Wikipedia infoboxes and cat-
egories and it is not suited for unrestricted text. An
earlier attempt (Zhang et al, 2008), is specific for
business and difficult to generalize to other relations.
Two recent promising works are more related to our
research. Wang et al (2011) uses manually defined
patterns to collect candidate facts and explicit dates,
and re-rank them using a graph label propagation al-
gorithm; their approach is complementary to ours,
as our aim is not to harvest temporal facts but to
extract the relations in which a query entity takes
part; unlike us, they require entity, value, and a ex-
plicit date to appear in the same sentence. Talukdar
et al (2012) focus on the partial task of temporally
anchoring already known facts, showing the useful-
ness of the document creation time as temporal sig-
nal, aggregated across documents.
Earlier work has dealt mainly with partial aspects
of the problem. The TempEval community focused
on the classification of the temporal links between
pairs of events, or an event and a temporal expres-
sion; using shallow features (Mani et al, 2003; La-
pata and Lascarides, 2004; Chambers et al, 2007),
or syntactic-based structured features (Bethard and
Martin, 2007; Pus?cas?u, 2007; Cheng et al, 2007).
Aggregating evidence across different documents
113
to temporally anchor facts has been explored in set-
tings different to Information Extraction, such as
answering of definition questions (Pas?ca, 2008) or
extracting possible dates of well-known historical
events (Schockaert et al, 2010).
Temporal inference or reasoning to solve con-
flicting temporal expressions and induce temporal
order of events has been used in TempEval (Tatu
and Srikanth, 2008; Yoshikawa et al, 2009) and
ACE (Gupta and Ji, 2009) tasks, but focused on
single-document extraction. Ling et al (2010), use
cross-event joint inference to extract temporal facts,
but only inside a single document.
Evaluation campaigns, such as ACE and TAC-
KBP 2011 have had an important role in promoting
this research. While ACE required only to identify
time expressions and classify their relation to events,
KBP requires to infer explicitly the start/end time of
relations, which is a realistic approach in the context
of building time-aware knowledge bases. KBP rep-
resents an important step for the evaluation of tem-
poral information extraction systems. In general, the
participant systems adapted existing slot filling sys-
tems, adding a temporal classification component:
distant supervised (Chen et al, 2010; Surdeanu et
al., 2010) on manually-defined patterns (Byrne and
Dunnion, 2010).
8 Conclusions
This paper introduces the problem of extracting,
from unrestricted natural language text, relational
knowledge anchored to a temporal span, aggregat-
ing temporal evidence from a collection of docu-
ments. Although compiling time-aware knowledge
bases is an important open challenge (Weikum et
al., 2011), it has remained unexplored until very re-
cently (Wang et al, 2011; Talukdar et al, 2012).
We have elucidated the two challenges of the task,
namely relation extraction and temporal anchoring
of the extracted relations.
We have studied how, in a pipeline architecture,
the propagation of errors limits the overall system?s
performance. The performance attainable in the full
task is limited by the quality of the output of the
three main phases: retrieval of candidate passages/
documents, extraction of relations and temporal an-
choring of those.
We have also studied the limits of the distant su-
pervision approach to relation extraction, showing
empirically that its performance depends not only
on the nature of reference knowledge base and doc-
ument corpus (Riedel et al, 2010), but also on the
relation to be extracted. Given a relation between
two arguments, if it is not dominant among textual
expressions of those arguments, the distant supervi-
sion assumption will be more often violated.
We have introduced a novel graph-based docu-
ment level representation, that has allowed us to gen-
erate new features for the task of relation extraction,
capturing long distance structured contexts. Our re-
sults show how, in a document level syntactic repre-
sentation, it yields better results to collapse corefer-
ent nodes.
We have presented a methodological approach
to temporal anchoring composed of: (1) intra-
document temporal information representation; (2)
selection of relation-dependent relevant temporal in-
formation; (3) mapping of temporal links to an inter-
val representation; and (4) aggregation of imprecise
intervals.
Our proposal has been evaluated within a frame-
work that allows for comparability. It has been able
to extract temporally anchored relational informa-
tion with the highest precision among the partici-
pant systems taking part in the competitive evalu-
ation TAC-KBP 2011.
For the temporal anchoring sub-problem, we have
demonstrated the strength of the document creation
time as a temporal signal. It is possible to achieve
a performance of 69% of the upper-bound imposed
by relation extraction by assuming that any relation
mentioned in a document held at the document cre-
ation time (there is a within link between the rela-
tional fact and the document creation time). This
baseline has proved stronger than extracting and an-
alyzing the temporal expressions present in the doc-
ument content.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Innovation, through
the project Holopedia (TIN2010-21128-C02), and
the Regional Government of Madrid, through the
project MA2VICMR (S2009/TIC1542).
114
References
Eneko Agirre, Angel X. Chang, Daniel S. Jurafsky,
Christopher D. Manning, Valentin I. Spitkovsky, and
Eric Yeh. 2009. Stanford-UBC at TAC-KBP. In TAC
2009, November.
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26:832?843,
November.
Steven Bethard and James H. Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and se-
mantic features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 129?132, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Lorna Byrne and John Dunnion. 2010. UCD IIRG at
TAC 2010 KBP Slot Filling Task. In Proceedings of
the Third Text Analysis Conference (TAC 2010). NIST,
November.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 173?176, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Wen-Pin Lin, Matthew Snover, Javier Artiles, Marissa
Passantino, and Heng Ji. 2010. CUNY-BLENDER
TAC-KBP2010: Entity linking and slot filling system
description. In Proceedings of the Third Text Analysis
Conference (TAC 2010). NIST, November.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: temporal relation identifi-
cation using dependency parsed tree. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 245?248, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Peas,
varo Rodrigo, and Damiano Spina. 2011. A distant
supervised learning system for the TAC-KBP Slot Fill-
ing and Temporal Slot Filling Tasks. In Text Analysis
Conference, TAC 2011 Proceedings Papers.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event propaga-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, ACLShort ?09, pages 369?372,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In Text Analysis Conference, TAC 2011 Work-
shop, Notebook Papers.
T. Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines ? Methods, Theory, and
Algorithms. Kluwer/Springer. We used Joachim?s
SVMLight implementation available at http://
svmlight.joachims.org/.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 2003, pages 423?430.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT 2004.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence (AAAI-10).
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in news.
In NAACL-Short?03.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL 2009, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M Pas?ca. 2008. Answering Definition Questions via
Temporally-Anchored Text Snippets. Proc. of IJC-
NLP2008.
Georgiana Pus?cas?u. 2007. Wvali: temporal relation
identification by syntactico-semantic analysis. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 484?487,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Pustejovsky and Marc Verhagen. 2009. SemEval-
2010 task 13: evaluating events, time expressions,
and temporal relations (TempEval-2). In Proceed-
ings of the Workshop on Semantic Evaluations: Re-
cent Achievements and Future Directions, DEW ?09,
pages 112?116, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Jose? Balca?zar, Francesco Bonchi,
Aristides Gionis, and Miche`le Sebag, editors, Machine
Learning and Knowledge Discovery in Databases,
volume 6323 of LNCS, pages 148?163. Springer
Berlin / Heidelberg.
Stuart J. Russell and Peter Norvig. 2010. Artificial Intel-
ligence - A Modern Approach (3. internat. ed.). Pear-
son Education.
Steven Schockaert, Martine De Cock, and Etienne Kerre.
2010. Reasoning about fuzzy temporal information
from the web: towards retrieval of historical events.
Soft Computing - A Fusion of Foundations, Method-
ologies and Applications, 14:869?886.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07, March.
115
Mihai Surdeanu, David McClosky, Julie Tibshirani, John
Bauer, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2010. A simple distant
supervision approach for the tac-kbp slot filling task.
In Proceedings of the Third Text Analysis Conference
(TAC 2010), Gaithersburg, Maryland, USA, Novem-
ber. NIST.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
Proceedings of the Fifth ACM International Confer-
ence on Web Search and Data Mining (WSDM), Seat-
tle, Washington, USA, February. Association for Com-
puting Machinery.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING?08.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Seok Bae Jang, Jessica Littman, Anna
Rumshisky, John Phillips, and James Pustejovsky.
2005. Automating temporal annotation with TARSQI.
In ACLdemo?05.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal re-
lation identification. In SemEval?07.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of the 13th Inter-
national Conference on Extending Database Technol-
ogy, EDBT ?10, pages 697?700, New York, NY, USA.
ACM.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from textual
web sources by constrained label propagation. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM ?11,
pages 837?846, New York, NY, USA. ACM.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence.
In Malu Castellanos, Umeshwar Dayal, Volker Markl,
Wil Aalst, John Mylopoulos, Michael Rosemann,
Michael J. Shaw, and Clemens Szyperski, editors, En-
abling Real-Time Business Intelligence, volume 84
of Lecture Notes in Business Information Processing,
pages 1?6. Springer Berlin Heidelberg.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09, pages 405?413, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Qi Zhang, Fabian M. Suchanek, Lihua Yue, and Gerhard
Weikum. 2008. TOB: Timely ontologies for business
relations. In 11th International Workshop on the Web
and Databases, WebDB.
116
