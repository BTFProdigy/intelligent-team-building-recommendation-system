Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 213?216,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with a Multi-Phase Model 
Zhou Junsheng 
State Key Laboratory for Novel Software Tech-
nology, Nanjing University,  China 
Deptartment of Computer Science, Nanjing 
Normal University, China 
zhoujs@nlp.nju.edu.cn 
He Liang 
State Key Laboratory for Novel Software 
Technology, Nanjing University, China 
 hel@nlp.nju.edu.cn 
Dai Xinyu 
State Key Laboratory for Novel Software Tech-
nology, Nanjing University,  China 
 dxy@nlp.nju.edu.cn 
Chen Jiajun 
State Key Laboratory for Novel Software 
Technology, Nanjing University, China 
 chenjj@nlp.nju.edu.cn 
 
Abstract 
Chinese named entity recognition is one 
of the difficult and challenging tasks of 
NLP. In this paper, we present a Chinese 
named entity recognition system using a 
multi-phase model. First, we segment the 
text with a character-level CRF model. 
Then we apply three word-level CRF 
models to the labeling person names, lo-
cation names and organization names in 
the segmentation results, respectively. 
Our systems participated in the NER tests 
on open and closed tracks of  Microsoft 
Research (MSRA). The actual evaluation 
results show that our system performs 
well on both the open tracks and closed 
tracks. 
1 Introduction 
Named entity recognition (NER) is a fundamen-
tal component for many NLP applications, such 
as Information extraction, text Summarization, 
machine translation and so forth. In recent years, 
much attention has been focused on the problem 
of recognition of Chinese named entities. The 
problem of Chinese named entity recognition is 
difficult and challenging, In addition to the chal-
lenging difficulties existing in the counterpart 
problem in English, this problem also exhibits 
the following more difficulties: (1) In a Chinese 
document, the names do not have ?boundary to-
kens? such as the capitalized initial letters for a 
person name in an English document. (2) There 
is no space between words in Chinese text, so we 
have to segment the text before NER is per-
formed. 
In this paper, we report a Chinese named en-
tity recognition system using a multi-phase 
model which includes a basic segmentation 
phase and three named entity recognition phases. 
In our system, the implementations of basic seg-
mentation components and named entity recogni-
tion component are both based on conditional 
random fields (CRFs) (Lafferty et al, 2001). At 
last, we apply the rule method to recognize some 
simple and short location names and organization 
names in the text. We will describe each of these 
phases in more details below. 
2 Chinese NER with multi-level models 
2.1 Recognition Process 
The input to the recognition algorithm is Chinese 
character sequence that is not segmented and the 
output is recognized entity names. The process of 
recognition of Chinese NER is illustrated in fig-
ure 1. First, we segment the text with a character-
level CRF model. After basic segmentation, a 
small number of named entities in the text, such 
as ?????, ??????????? and so on, 
which are segmented as a single word. These 
simple single-word entities will be labeled with 
some rules in the last phase. However, a great 
number of named entities in the text, such as ??
???????????, ????????, are 
not yet segmented as a single word. Then, differ-
ent from (Andrew et al 2003), we apply three 
trained CRFs models with carefully designed and 
selected features to label person names, location 
names and organization names in the segmenta-
tion results, respectively. At last phase, we apply 
some rules to tag some names not recognized by 
CRFs models, and adjust part of the organization 
names recognized by CRFs models. 
213
 2.2 Word segmentation 
We implemented the basic segmentation compo-
nent with linear chain structure CRFs. CRFs are  
undirected graphical models that encode a condi-
tional probability distribution using a given set of 
features. In the special case in which the desig-
nated output nodes of the graphical model are 
linked by edges in a linear chain, CRFs make a 
first-order Markov independence assumption 
among output nodes, and thus correspond to fi-
nite state machines (FSMs). CRFs define the 
conditional probability of a state sequence given 
an input sequence as 
??
???
?= ??
= =
??
T
t
K
k
ttkk
o
tossf
Z
osP
1 1
1 ),,,(exp
1
)|( ?  
Where ),,,( 1 tossf ttk ?  is an arbitrary feature 
function over its arguments, and ?k is a learned 
weight for each feature function.  
Based on CRFs model, we cast the segmenta-
tion problem as a sequence tagging problem. Dif-
ferent from (Peng et al, 2004), we represent the 
positions of a hanzi (Chinese character) with four 
different tags: B for a hanzi that starts a word, I 
for a hanzi that continues the word, F for a hanzi 
that ends the word, S for a hanzi that occurs as a 
single-character word. The basic segmentation is 
a process of labeling each hanzi with a tag given 
the features derived from its surrounding context. 
The features used in our experiment can be bro-
ken into two categories: character features and 
word features. The character features are instan-
tiations of the following templates, similar to 
those described in (Ng and Jin, 2004), C refers to 
a Chinese hanzi. 
(a) Cn (n = ?2,?1,0,1,2 ) 
(b) CnCn+1( n = ?2,?1,0,1)  
(c) C?1C1 
(d) Pu(C0 ) 
In addition to the character features, we came 
up with another type word context feature which 
was found very useful in our experiments. The 
feature captures the relationship between the 
hanzi and the word which contains the hanzi. For 
a two-hanzi word, for example, the first hanzi 
??? within the word ???? will have the fea-
ture WC0=TWO_F set to 1, the second hanzi 
??? within the same word ???? will have the 
feature WC0=TWO_L set to 1. For the three-
hanzi word, for example, the first hanzi ??? 
within a word ????? will have the feature 
WC0=TRI_F set to 1, the second hanzi ??? 
within the same word ????? will have the 
feature WC0=TRI_M set to 1, and the last hanzi 
??? within the same word ????? will have 
the feature WC0=TRI_L set to 1. Similarly, the 
feature can be extended to a four-hanzi word. 
2.3 Named entity tagging with CRFs 
After basic segmentation, we use three word-
level CRFs models to label person names, loca-
tion names and organization names, respectively. 
The important factor in applying CRFs model to 
name entity recognition is how to select the 
proper features set. Most of entity names do not 
have any common structural characteristics ex-
cept for containing some feature words,  such as 
????, ????, ??? , ??? and so on. In addi-
tion, for person names, most names include a 
common surname, e.g. ???, ???. But as a 
proper noun, the occurrence of an entity name 
has the specific context. In this section, we only 
present our approach to organization name rec-
ognition. For example, the context information of 
organization name mainly includes the boundary 
words and some title words (e.g. ??????). 
By analyzing a large amount of entity name cor-
pora, we find that the indicative intensity of dif-
ferent boundary words vary greatly. So we divide 
the left and right boundary words into two 
classes according to the indicative intensity. Ac-
cordingly we construct the four boundary words 
lexicons. To solve the problem of the selection 
and classification of boundary words, we make 
use of mutual Information I(x, y). If there is a 
genuine association between x and y, then I(x, y) 
>>0. If there is no interesting relationship be-
Unsegmented text 
Word Segmentation 
Person Names Recognition 
Recognized Named Entities 
Processing with Rules 
Location Names Recognition 
Organization Names Recognition  
Fig1. Chinese NER process 
214
tween x and y, then I(x, y)?0. If x and y are in 
complementary distribution, then I(x, y) << 0. 
By using mutual information, we compute the 
association between boundary word and the type 
of organization name, then select and classify the 
boundary words. Some example boundary words 
for organization names are listed in table 1. 
Table 1. The classified boundary words for ORG names 
Based on the consideration given in preceding 
section, we constructed a set of atomic feature 
patterns, listed in table 2. Additionally, we de-
fined a set of conjunctive feature patterns, which 
could form effective feature conjunctions to ex-
press complicated contextual information. 
Table 2.  Atomic feature patterns for ORG names 
Atomic pattern Meaning of pattern 
CurWord Current word 
LocationName Check if current word is a location 
name 
PersonName Check if current word is a person 
name 
KnownORG Check if current word is a known 
organization name 
ORGFeature 
 
ScanFeatureWord_8 
Check if current word is a feature 
word of ORG name 
Check if there exist a feature word 
among eight words behind the 
current word 
LeftBoundary1_-2 
LeftBoundary2_-2 
Check if there exist a first-class or 
second-class left boundary word 
among two words before the cur-
rent word 
RightBoundary1_+2 
RightBoundary2_+2 
Check if there exist a first-class or 
second-class right boundary word 
among two words behind the cur-
rent word 
2.4 Processing with rules 
There exists some single-word named entities 
that aren?t tagged by CRFs models. We recog-
nize these single-word named entities with some 
rules. We first construct two known location 
names and organization names dictionaries and 
two feature words lists for location names and 
organization names. In closed track, we collect 
known location names and organization names 
only from training corpus. The recognition proc-
ess is described below. For each word in the text, 
we first check whether it is a known location or 
organization names according to the known loca-
tion names and organization names dictionaries. 
If it isn?t a known name, then we further check 
whether it is a known word. If it is not a known 
word also, we next check whether the word ends 
with a feature word of location or organization 
names. If it is, we label it as a location or organi-
zation name. 
In addition, we introduce some rules to adjust 
organization names recognized by CRF model 
based on the labeling specification of MRSA 
corpus. For example, the string ???????
???? ? is recognized as an organization 
name, but the string should be divided into two 
names: a location name (?????) and a or-
ganization name (?????????), according 
to label specification, so we add some rules to 
adjust it. 
3 Experimental results 
We participated in the three GB tracks in the 
third international Chinese language processing 
bakeoff: NER msra-closed, NER msra-open and 
WS msra-open. In the closed track, we con-
structed all dictionaries only with the words ap-
pearing in the training corpus. In the closed track, 
we didn?t use the same feature characters lists for 
location names and organization names as in the 
open tracks and we collected the feature charac-
ters from the training data in the closed track. We 
constructed feature characters lists for location 
names and organization names by the following 
approach. First, we extract all suffix string for all 
location names and organization names in the 
training data and count the occurrence of these 
suffix strings in all location names and organiza-
tion names. Second, we check every suffix string 
to judge whether it is a known word. If a suffix 
string is not a known word, we discard it. Finally, 
in the remaining suffix words, we select the fre-
quently used suffix words as the feature charac-
ters whose counts are greater than the threshold. 
We set different thresholds for single-character 
feature words and multi-character feature words. 
Similar approaches were taken to the collection 
of common Chinese surnames in the closed track.  
While making training data for segmentation 
model, we adopted different tagging methods for 
organization names in the closed track and in the 
open track. In the closed track, we regard every 
organization name, such as ????????
??, as a single word. But, in the open track, we 
segment a long organization name into several 
words. For example, the organization name ??
Type Class Examples
First-class ???6.0006?Left boundary 
word Second-class ???3.1161?
First -class ???5.4531?Right boundary 
word Second-class  ???2.0135?
215
???????? would be divided into three 
words: ?????, ???? and ?????. The 
different tagging methods at segmentation phase 
would bring different effect to organization 
names recognition. The size of training data used 
in the open tracks is same as the closed tracks. 
We have not employed any additional training 
data in the open tracks. Table 3 shows the per-
formance of our systems for NER in the bakeoff.  
Table 3: Named entity recognition outcome 
Track P  R F  Per-F Loc-F Org-F
NER msra  
closed      
88.94  84.20 86.51 90.09 85.45 83.10
NER msra 
open 
90.76 89.22 89.99 92.61 90.99 83.97
For the separate word segmentation task(WS), 
the above NER task is performed first. Then we 
added several additional processing steps on the 
result of named entity recognition. As we all 
know, disambiguation problem is one of the key 
issue in Chinese words segmentation. In this task, 
some ambiguities were resolved through a rule-
set which was automatically constructed based 
on error driven learning theory. The pre-
constructed rule-set stored many pseudo-
ambiguity strings and gave their correct segmen-
tations. After analyzing the result of our NER 
based on CRFs model, we noticed that it presents 
a high recall on out-of-vocabulary. But at the 
same time, some characters and words were 
wrongly combined as new words which caused 
the losing of the precision of OOV and the recall 
of IV. To this phenomenon, we adopted an un-
conditional rule, that if a word, except recog-
nized name entity, was detected as a new word 
and its length was more than 6 (Chinese Charac-
ters), and it should be segmented as several in-
vocabulary words based on the combination of  
FMM and BMM methods. Table 4 shows the 
result of our systems for word segmentation in 
the bakeoff. 
Table 4: Word segmentation outcome 
Track P R F OOV-R IV-R
WS msra 
open 0.975 0.976 0.975 0.811 0.981
4 Conclusion 
We have presented our Chinese named entity 
recognition system with a multi-phase model and 
its result for Msra_open and mrsa_closed tracks. 
Our open and closed GB track experiments show 
that its performance is competitive. We will try 
to select more useful feature functions into the 
existing segmentation model and named entity 
recognition model in future work. 
Reference 
Aitao Chen. 2003. Chinese Word Segmentation Using 
Minimal Linguistic Knowledge. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing.  
Andrew McCallum, Wei Li. 2003. Early Results for 
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced 
Lexicons. Proceedings of the Seventh CoNLL con-
ference, Edmonton,  
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In Proc. 
ICML 01. 
 Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese Part-
of-Speech Taging: One-at-a-Time or All at Once? 
Word-based or Character based? In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, Spain. 
Peng, Fuchun, Fangfang Feng, and Andrew 
McCallum. 2004. Chinese Segmentation and New 
Word Detection using Conditional Random Fields . 
In Proceedings of the Twentith International Con-
ference on Computaional Linguistics.  
 
216
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 557?567, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
 
 
 
Exploiting Chunk-level Features to Improve Phrase Chunking 
 
Junsheng Zhou    Weiguang Qu     Fen Zhang 
Jiangsu Research Center of Information Security & Privacy Technology 
School of Computer Science and Technology 
Nanjing Normal University. Nanjing, China, 210046 
Email:{zhoujs,wgqu}@njnu.edu.cn  zf9646@126.com 
  
  
Abstract 
Most existing systems solved the phrase 
chunking task with the sequence labeling 
approaches, in which the chunk candidates 
cannot be treated as a whole during parsing 
process so that the chunk-level features 
cannot be exploited in a natural way. In this 
paper, we formulate phrase chunking as a 
joint segmentation and labeling task. We 
propose an efficient dynamic programming 
algorithm with pruning for decoding, 
which allows the direct use of the features 
describing the internal characteristics of 
chunk and the features capturing the 
correlations between adjacent chunks. A 
relaxed, online maximum margin training 
algorithm is used for learning. Within this 
framework, we explored  a variety of 
effective feature representations for 
Chinese phrase chunking. The 
experimental results show that the use of 
chunk-level features can lead to significant 
performance improvement, and that our 
approach achieves state-of-the-art 
performance. In particular, our approach is 
much better at recognizing long and 
complicated phrases. 
1 Introduction 
Phrase chunking is a Natural Language Processing 
task that consists in dividing a text into 
syntactically correlated parts of words. Theses 
phrases are non-overlapping, i.e., a word can only 
be a member of one chunk (Abney, 1991). 
Generally speaking, there are two phrase chunking 
tasks, including text chunking (shallow parsing), 
and noun phrase (NP) chunking. Phrase chunking 
provides a key feature that helps on more 
elaborated NLP tasks such as parsing, semantic 
role tagging and information extraction.  
    There is a wide range of research work on 
phrase chunking based on machine learning 
approaches. However, most of the previous work 
reduced phrase chunking to sequence labeling 
problems either by using the classification models, 
such as SVM (Kudo and Matsumoto, 2001), 
Winnow and voted-perceptrons (Zhang et al2002; 
Collins, 2002), or by using the sequence labeling 
models, such as Hidden Markov Models (HMMs) 
(Molina and Pla, 2002) and Conditional Random 
Fields (CRFs) (Sha and Pereira, 2003). When 
applying the sequence labeling approaches to 
phrase chunking, there exist two major problems. 
Firstly, these models cannot treat globally a 
sequence of continuous words as a chunk 
candidate, and thus cannot inspect the internal 
structure of the candidate, which is an important 
aspect of information in modeling phrase chunking. 
In particular, it makes impossible the use of local 
indicator function features of the type "the chunk 
consists of POS tag sequence p1...,pk". For example, 
the Chinese NP "?? /NN(agriculture) ??
/NN(production) ?/CC(and) ??/NN(rural) ??
/NN(economic) ?? /NN(development)" seems 
relatively difficult to be correctly recognized by a 
sequence labeling approach due to its length. But if 
we can treat the sequence of words as a whole and 
describe the formation pattern of POS tags of this 
chunk with a regular expression-like form 
"[NN]+[CC][NN]+", then it is more likely to be 
correctly recognized, since this pattern might better 
express the characteristics of its constituents. As 
another example, consider the recognition of 
special terms. In Chinese corpus, there exists a 
kind of NPs called special terms, such as "? ??
557
  
 
(Life) ?? (Forbidden Zone) ? ", which are 
bracketed with the particular punctuations like "
? , ? , ? , ? , ? , ?". When recognizing the 
special terms, it is difficult for the sequence 
labeling approaches to guarantee the matching of 
particular punctuations appearing at the starting 
and ending positions of a chunk. For instance, the 
chunk candidate "? ??(Life) ??(Forbidden 
Zone)? is considered to be an invalid chunk. But 
it is easy to check this kind of punctuation 
matching in a single chunk by introducing a chunk-
level feature. 
Secondly, the sequence labeling models cannot 
capture the correlations between adjacent chunks, 
which should be informative for the identification 
of chunk boundaries and types. In particular, we 
find that some headwords in the sentence are 
expected to have a stronger dependency relation 
with their preceding headwords in preceding 
chunks than with their immediately preceding 
words within the same chunk. For example, in the 
following sentence: 
" [??/PN(Bilateral)]_NP [??/NN(economic 
and trade) ??/NN(relations)]_NP [?/AD(just) 
??/AD(steadily) ??/VV(develop)]_VP " 
if we can find the three headwords "??", "??" 
and "??" located in the three adjacent chunks 
with some head-finding rules, then the headword 
dependency expressed by headword bigrams or 
trigrams should be helpful to recognize these 
chunks in this sentence.  
In summary, the inherent deficiency in applying 
the sequence labeling approaches to phrase 
chunking is that the chunk-level features one 
would expect to be very informative cannot be 
exploited in a natural way.  
In this paper, we formulate phrase chunking as a 
joint segmentation and labeling problem, which 
offers advantages over previous learning methods 
by providing a natural formulation to exploit the 
features describing the internal structure of a chunk 
and the features capturing the correlations between 
the adjacent chunks.  
Within this framework, we explored  a variety of 
effective feature representations for Chinese phrase 
chunking. The experimental results on Chinese 
chunking corpus as well as English chunking 
corpus show that the use of chunk-level features 
can lead to significant performance improvement, 
and that our approach performs better than other 
approaches based on the sequence labeling models. 
2 Related Work 
In recent years, many chunking systems based on 
machine learning approaches have been presented. 
Some approaches rely on k-order generative 
probabilistic models, such as HMMs (Molina and 
Pla, 2002). However, HMMs learn a generative 
model over input sequence and labeled sequence 
pairs. It has difficulties in modeling multiple non-
independent features of the observation sequence. 
To accommodate multiple overlapping features on 
observations, some other approaches view the 
phrase chunking as a sequence of classification 
problems, including support vector machines 
(SVMs) (Kudo and Matsumoto 2001) and a variety 
of other classifiers (Zhang et al2002). Since these 
classifiers cannot trade off decisions at different 
positions against each other, the best classifier 
based shallow parsers are forced to resort to 
heuristic combinations of multiple classifiers. 
Recently, CRFs were widely employed for phrase 
chunking, and presented comparable or better 
performance than other state-of-the-art models 
(Sha and Pereira 2003; McDonald et al005). 
Further, Sun et al2008) used the latent-dynamic 
conditional random fields (LDCRF) to explicitly 
learn the hidden substructure of shallow phrases, 
achieving state-of-the-art performance over the 
NP-chunking task on the CoNLL data. 
Some similar approaches based on classifiers or 
sequence labeling models were also used for 
Chinese chunking (Li et al2003; Tan et al2004; 
Tan et al2005). Chen et al2006) conducted an 
empirical study of Chinese chunking on a corpus, 
which was extracted from UPENN Chinese 
Treebank-4 (CTB4). They compared the 
performances of the state-of-the-art machine 
learning models for Chinese chunking, and 
proposed some Tag-Extension and novel voting 
methods to improve performance.  
In this paper, we model phrase chunking with a 
joint segmentation and labeling approach, which 
offer advantages over previous learning methods 
by explicitly incorporating the internal structural 
feature and the correlations between the adjacent 
chunks. To some extent, our model is similar to 
Semi-Markov Conditional Random Fields (called a 
Semi-CRF), in which the segmentation and 
558
  
 
labeling can also be done directly (Sarawagi and 
Cohen, 2004). However, Semi-CRF just models 
label dependency, and it cannot capture more 
correlations between adjacent chunks, as is done in 
our approach. The limitation of Semi-CRF leads to 
its relatively low performance.   
3 Problem Formulation 
3.1 Chunk Types 
Unlike English chunking, there is not a 
benchmarking corpus for Chinese chunking. We 
follow the studies in (Chen et al006) so that a 
more direct comparison with state-of-the-art 
systems for Chinese chunking would be possible. 
There are 12 types of chunks: ADJP, ADVP, CLP, 
DNP, DP, DVP, LCP, LST, NP, PP, QP and VP in 
the chunking corpus (Xue et al2000). The 
training and test corpus can be extracted from 
CTB4 with a public tool, as depicted in (Chen et al
2006). 
3.2 Sequence Labeling Approaches to Phrase 
Chunking 
The standard approach to phrase chunking is to use 
tagging techniques with a BIO tag set. Words in 
the input text are tagged with one of B for the 
beginning of a contiguous segment, I for the inside 
of a contiguous segment, or O for outside a 
segment. For instance, the sentence (word 
segmented and POS tagged) "?/NR(He) ??
/VV(reached) ? ? /NR(Beijing) ? ?
/NN(airport) ?/PU" will be tagged as follows: 
Example 1: 
S1: [NP ?][VP ??][NP ??/??][O ?] 
S2: ?/B-NP ??/B-VP ??/B-NP ??/I-
NP ?/O 
Here S1 denotes that the sentence is tagged with 
chunk types, and S2 denotes that the sentence is 
tagged with chunk tags based on the BIO-based 
model. With the data representation like the S2, the 
problem of phrase chunking can be reduced to a 
sequence labeling task. 
3.3 Phrase Chunking via a Joint 
Segmentation and Labeling Approach 
To tackle the problems with the sequence labeling 
approaches to phrase chunking, we formulate it as 
a joint problem, which maps a Chinese sentence x 
with segmented words and POS tags to an output y 
with tagged chunk types, like the S1 in Example 1. 
The joint model considers all possible chunk 
boundaries and corresponding chunk types in the 
sentence, and chooses the overall best output. This 
kind of parser reads the input sentences from left to 
right, predicts whether current segment of 
continuous words is some type of chunk. After one 
chunk is found, parser move on and search for next 
possible chunk. 
Given a sentence x, let y denote an output tagged 
with chunk types, and GEN a function that 
enumerates a set of segmentation and labeling 
candidates GEN(x) for x. A parser is to solve the 
following ?argmax? problem: 
( )
| |
[1.. ]( ) 1
? arg max ( )
arg max ( )
T
y GEN x
y
T
iy GEN x i
y w y
  w yf
?
? =
= ?F
= ??
 
 
where F  and f  are global and local feature maps 
and w is the parameter vector to learn. The inner 
product [1.. ]( )T iw yf?  can be seen as the confidence 
score of whether yi is a chunk. The parser takes into 
account confidence score of each chunk, by using 
the sum of local scores as its criteria. Markov 
assumption is necessary for computation, so f  is 
usually defined on a limited history. 
The main advantage of the joint segmentation 
and labeling approach to phrase chunking is to 
allow for integrating both the internal structural 
features and the correlations between the adjacent 
chunks for prediction. The two basic components 
of our model are decoding and learning algorithms, 
which are described in the following sections. 
4 Decoding 
The inference technique is one of the most 
important components for a joint segmentation and 
labeling model. In this section, we propose a 
dynamic programming algorithm with pruning to 
efficiently produce the optimal output. 
4.1 Algorithm Description 
Given an input sentence x, the decoding algorithm 
searches for the highest-scored output with 
recognized chunks. The search space of combined 
candidates in the joint segmentation and labeling 
task is very large, which is an exponential growth 
(1)
559
  
 
in the number of possible candidates with 
increasing sentence size. The rate of growth is 
O(2nTn) for the joint system, where n is the length 
of the sentence and T is the number of chunk types. 
It is natural to use some greedy heuristic search 
algorithms for inference in some similar joint 
problems (Zhang and Clark, 2008; Zhang and 
Clark, 2010). However, the greedy heuristic search 
algorithms only explore a fraction of the whole 
space (even with beam search) as opposed to 
dynamic programming. Additionally, a specific 
advantage of the dynamic programming algorithm 
is that constraints required in a valid prediction 
sequence can be handled in a principled way. We 
show that dynamic programming is in fact possible 
for this joint problem, by introducing some 
effective pruning schemes. 
   To make the inference tractable, we first make a 
first-order Markov assumption on the features used 
in our model. In other words, we assume that the 
chunk ci and the corresponding label ti are only 
associated with the preceding chunk ci-1 and the 
label ti-1. Suppose that the input sentence has n 
words and the constant M is the maximum chunk 
length in the training corpus. Let V(b,e,t) denote 
the highest-scored segmentation and labeling with 
the last chunk starting at word index b, ending at 
word index e and the last chunk type being t. One 
way to find the highest-scored segmentation and 
labeling for the input sentence is to first calculate 
the V(b,n-1,t) for all possible start position b?(n-
M)..n-1, and all possible chunk type t, respectively, 
and then pick the highest-scored one from these 
candidates. In order to compute V(b,n-1,t), the last 
chunk needs to be combined with all possible 
different segmentations of words (b-M)..b-1 and all 
possible different chunk types so that the highest-
scored can be selected. According to the principle 
of optimality, the highest-scored among the 
segmentations of words (b-M)..b-1 and all possible 
chunk types with the last chunk being word b? ..b-
1 and the last chunk type being t ?  will also give the highest score when combined with the word 
b..n-1 and tag t. In this way, the search task is 
reduced recursively into smaller subproblems, 
where in the base case the subproblems V(0,e,t) for 
e?0..M-1, and each possible chunk type t, are 
solved in straightforward manner. And the final 
highest-scored segmentation and labeling can be 
found by solving all subproblems in a bottom-up 
fashion. 
   The pseudo code for this algorithm is shown in 
Figure 1. It works by filling an n by n by T table 
chart, where n is the number of words in the input 
sentence sent, and T is the number of chunk types. 
chart[b,e,t] records the value of subproblem 
V(b,e,t). chart[0, e, t] can be computed directly for 
e = 0..M-1 and for chunk type t=1..T. The final 
output is the best among chart[b,n-1,t], with b= 
n-M..n-1, and t=1..T.  
Inputs: sentence sent (word segmented and POS 
tagged) 
Variables:  
word index b for the start of chunk; 
word index e for the end of chunk; 
word index p for the start of the previous chunk. 
chunk type index t for the current chunk; 
chunk type index t ?  for the previous chunk; 
Initialization: 
for e = 0.. M-1: 
   for t =1..T: 
     chart[0,e,t] ?single chunk sent[0,e] and type t 
Algorithm: 
for e = 0..n-1: 
  for b = (e-M)..e: 
    for t =1..T: 
       chart[b,e,t]?the highest scored segmentation            
               and labeling among those derived by 
               combining chart[p,b-1, t ? ] with sent[b,e] 
               and chunk type t, for p = (b-M)..b-1, 
                t ? =1..T. 
Outputs: the highest scored segmentation and 
labeling among chart[b,n-1,t], for b=n-M..n-1, t 
=1..T. 
Figure 1: A dynamic-programming algorithm for 
phrase chunking. 
4.2 Pruning 
The time complexity of the above algorithm is 
O(M2T2n), where M is the maximum chunk size. It 
is linear in the length of sentence. However, the 
constant in the O is relatively large. In practice, the 
search space contains a large number of invalid 
partial candidates, which make the algorithm slow. 
In this section we describe three partial output 
pruning schemes which are helpful in speeding up 
the algorithm. 
560
  
 
Firstly, we collect chunk type transition 
information between chunk types by observing 
every pair of adjacent chunks in the training corpus, 
and record a chunk type transition matrix. For 
example, from the Chinese Treebank that we used 
for our experiments, a transition from chunk type 
ADJP to ADVP does not occur in the training 
corpus, the corresponding matrix element is set to 
false, true otherwise. During decoding, the chunk 
type transition information is used to prune 
unlikely combinations between current chunk and 
the preceding chunk by their chunk types. 
Secondly, a POS tag dictionary is used to record 
POS tags associated with each chunk type. 
Specifically, for each chunk type, we record all 
POS tags appearing in this type of chunk in the 
training corpus. During decoding, a segment of 
continuous words that contains only allowed POS 
tags according to the POS tag dictionary will be 
considered to be a valid chunk candidate. 
Finally, the system records the maximum 
number of words for each type of chunk in the 
training corpus. For example, in the Chinese 
Treebank, most types of chunks have one to three 
words. The few chunk types that are seen with 
length bigger than ten are NP, QP and ADJP. 
During decoding, the chunk candidate whose 
length is greater than the maximum chunk length 
associated with its chunk type will be discarded. 
For the above pruning schemes, development 
tests show that it improves the speed significantly, 
while having a very small negative influence on 
the accuracy. 
5 Learning 
5.1 Discriminative Online Training 
By defining features, a candidate output y is 
mapped into a global feature vector, in which each 
dimension represents the count of a particular 
feature in the sentence. The learning task is to set 
the parameter values w using the training examples 
as evidence. 
   Online learning is an attractive method for the 
joint model since it quickly converges within a few 
iterations (McDonald, 2006). We focus on an 
online learning algorithm called MIRA, which is a 
relaxed, online maximum margin training 
algorithm with the desired accuracy and scalability 
properties (Crammer, 2004). Furthermore, MIRA 
is very flexible with respect to the loss function. 
Any loss function on the output is compatible with 
MIRA since it does not require the loss to factor 
according to the output, which enables our model 
to be optimized with respect to evaluation metrics 
directly. Figure 2 outlines the generic online 
learning algorithm (McDonald, 2006) used in our 
framework. 
MIRA updates the parameter vector w with two 
constraints: (1) the positive example must have a 
higher score by a given margin, and (2) the change 
to w should be minimal. This second constraint is 
to reduce fluctuations in w. In particular, we use a 
generalized version of MIRA (Crammer et al
2005; McDonald, 2006) that can incorporate k-best 
decoding in the update procedure.  
Input: Training set 1{( , )}Tt t tS x y ==  
1: w(0) = 0; v = 0; i = 0 
2: for iter = 1 to N do 
3:    for t = 1 to T do 
4:       w(i+1) = update w(i) according to (xt, yt) 
5:       v = v + w(i+1) 
6:       i = i + 1 
7:    end for 
8: end for 
9: w = v/(N ? T) 
Output: weight vector w 
Figure 2: Generic Online Learning Algorithm 
In each iteration, MIRA updates the weight 
vector w by keeping the norm of the change in the 
weight vector as small as possible. Within this 
framework, we can formulate the optimization 
problem as follows (McDonald, 2006): 
( 1) ( )
( )
argmin
. . ( ; ) :
( ) ( ) ( , )
i i
w
i
k t
T T
t t
w w w
s t   y best x w
     w y w y L y y
+ = -
?" ?
? ??F - ?F ?
 
where ( )( ; )ik tbest x w  represents a set of top k-best 
outputs for xt given the weight vector w(i). In our 
implementation, the top k-best outputs are obtained 
with a straightforward k-best extension to the 
decoding algorithm in section 4.1. The above 
quadratic programming (QP) problem can be 
solved using Hildreth?s algorithm (Yair Censor, 
1997). Replacing Eq. (2) into line 4 of the 
algorithm in Figure 2, we obtain k-best MIRA. 
As shown in (McDonald, 2006), parameter 
averaging can effectively avoid overfitting. The 
(2)
561
  
 
final weight vector w is the average of the weight 
vectors after each iteration. 
5.2 Loss Function 
For the joint segmentation and labeling task, there 
are two alternative loss functions: 0-1 loss and F1 
loss. 0-1 loss gives credit only when the entire 
output sequence is correct: there is no notion of 
partially correct solutions. The most common loss 
function for joint segmentation and labeling 
problems is F1 measure over chunks. This is the 
geometric mean of precision and recall over the 
(properly-labeled) chunk identification task, 
defined as follows. 
2 | |?( , ) 1 | | | |
F y yL y y y y
??- ?+?
 
where the cardinality of y is simply the number of 
chunks identified. The cardinality of the 
intersection is the number of chunks in common. 
As can be seen in the definition, one is penalized 
both for identifying too many chunks (penalty in 
the denominator) and for identifying too few 
(penalty in the numerator).  
In our experiments, we will compare the 
performance of the systems with different loss 
functions. 
5.3 Features 
Table 1 shows the feature templates for the joint 
segmentation and labeling model. In the row for 
feature templates, c, t, w and p are used to 
represent a chunk, a chunk type, a word and a POS 
tag, respectively. And c0 and c?1 represent the 
current chunk and the previous chunk respectively. 
Similarly, w?1, w0 and w1 represent the previous 
word, the current word and the next word, 
respectively.  
Although it is slightly less natural to do so, part 
of the features used in the sequence labeling 
models can also be represented in our approach. 
Therefore the features employed in our model can 
be divided into three types: the features similar to 
those used in the sequence labeling models (called 
SL-type features), the features describing internal 
structure of a chunk (called Internal-type features), 
and the features capturing the correlations between 
the adjacent chunks (called Correlation-type 
features). 
Firstly, some features associated with a single 
label (here refers to label "B" and "I") used in the 
sequence labeling models are also represented in 
our model. In Table 1, templates 1-4 are SL-type 
features, where label(w) denotes the label 
indicating the position of the word w in the current 
chunk; len(c) denotes the length of chunk c. For 
example, given an NP chunk "??(Beijing) ??
(Airport)", which includes two words, the value of 
label("??") is "B" and the value of label("??") 
is "I". Bigram(w) denotes the word bigrams formed 
by combining the word to the left of w and the one 
to the right of w. And the same meaning is for 
biPOS(w). Template specitermMatch(c) is used to 
check the punctuation matching within chunk c for 
the special terms, as illustrated in section 1. 
Secondly, in our model, we have a chance to 
treat the chunk candidate as a whole during 
decoding, which means that we can employ more 
expressive features in our model than in the 
sequence labeling models. In Table 1, templates 5-
13 concern the Internal-type features, where 
start_word(c) and end_word(c) represent the first 
word and the last word of chunk c, respectively. 
Similarly, start_POS(c) and end_POS(c) represent 
the POS tags associated with the first word and the 
last word of chunk c, respectively. These features 
aim at expressing the formation patterns of the 
current chunk with respect to words and POS tags. 
Template internalWords(c) denotes the 
concatenation of words in chunk c, while 
internalPOSs(c) denotes the sequence of POS tags 
in chunk c using regular expression-like form, as 
illustrated in section 1.  
Finally, in Table 1, templates 14-28 concern the 
Correlation-type features, where head(c) denotes 
the headword extracted from chunk c, and 
headPOS(c) denotes the POS tag associated with 
the headword in chunk c. These features take into 
account various aspects of correlations between 
adjacent chunks. For example, we extracted the 
headwords located in adjacent chunks to form 
headword bigrams to express semantic dependency 
between adjacent chunks. To find the headword 
within every chunk, we referred to the head-
finding rules from (Bikel, 2004), and made a 
simple modification to them.  For instance, the 
head-finding rule for NP in (Bikel, 2004) is as 
follows:  
(NP (r NP NN NT NR QP) (r))  
Since the phrases are non-overlapping in our task, 
we simply remove the overlapping phrase tags NP 
    (3)
562
  
 
and QP from the rule, and then the rule is modified 
as follows:  
    (NP (r NN NT NR) (r))   
Additionally, the different bigrams formed by 
combining the first word (or POS) and last word 
(or POS) located in two adjacent chunks can also 
capture some correlations between adjacent chunks, 
and templates 17-22 are designed to express this 
kind of bigram information. 
ID Feature template 
1 wlabel(w) t0  
for all w in c0 
2 bigram (w) label(w)t0  
for all w in c0 
3 biPOS(w) label(w)t0  
for all w in c0 
4 w-1w1label(w0) t0 ,  where len(c0)=1 
5 start_word(c0)t0 
6 start_POS(c0)t0 
7 end_word(c0)t0 
8 end_POS(c0)t0 
9 wend_word (c0) t0 
 where 0 0_ ( )w c  and w end word c? ?  
10 pend_POS (c0) t0 
 where 0 0_ ( )p c  and p end POS c? ?  
11 internalPOSs(c0) t0 
12 internalWords(c0) t0 
13 specitermMatch(c0) 
14 t-1t0 
15 head(c-1)t-1head(c0)t0 
16 headPOS(c-1)t-1headPOS(c0)t0 
17 end_word(c-1)t-1start_word(c0)t0 
18 end_POS(c-1)t-1start_POS(c0)t0 
19 end_word(c-1)t-1end_word(c0)t0 
20 end_POS(c-1)t-1end_POS(c0)t0 
21 start_word(c-1)t-1start_word(c0)t0 
22 start_POS(c-1)t-1start_POS(c0)t0 
23 end_word(c-1)t0 
24 end_POS(c-1)t0 
25 t-1t0start_word(c0) 
26 t-1t0start_POS(c0) 
27 internalWords(c-1) t-1 internalWords(c0) t0
28 internalPOSs(c-1) t-1 internalPOSs(c0) t0 
Table 1: Feature templates. 
6 Experiments 
6.1 Data Sets and Evaluation 
Following previous studies on Chinese chunking in 
(Chen et al2006), our experiments were 
performed on the CTB4 dataset. The dataset 
consists of 838 files. In the experiments, we used 
the first 728 files (FID from chtb 001.fid to chtb 
899.fid) as training data, and the other 110 files 
(FID from chtb 900.fid to chtb 1078.fid) as testing 
data. The training set consists of 9878 sentences, 
and the test set consists of 5920 sentences. The 
standard evaluation metrics for this task are 
precision p (the fraction of output chunks matching 
the reference chunks), recall r (the fraction of 
reference chunks returned), and the F-measure 
given by F = 2pr/(p + r). 
Our model has two tunable parameters: the 
number of training iterations N; the number of top 
k-best outputs. Since we were interested in finding 
an effective feature representation at chunk-level 
for phrase chunking, we fixed N = 10 and k = 5 for 
all experiments. In the following experiments, our 
model has roughly comparable training time to the 
sequence labeling approach based on CRFs. 
6.2 Chinese NP chunking 
NP is the most important phrase in Chinese 
chunking and about 47% phrases in the CTB4 
Corpus are NPs. In this section, we present the 
results of our approach to NP recognition.  
Table 2 shows the results of the two systems 
using the same feature representations as defined 
in Table 1, but using different loss functions for 
learning. As shown, learning with F1 loss can 
improve the F-score by 0.34% over learning with 
0-1 loss. It is reasonable that the model optimized 
with respect to evaluation metrics directly can 
achieve higher performance. 
Loss Function Precision Recall F1 
0-1 loss 91.39 90.93 91.16 
F1 loss 92.03 90.98 91.50 
Table 2: Experimental results on Chinese NP 
chunking. 
6.3 Chinese Text Chunking 
There are 12 different types of phrases in the 
chunking corpus. Table 3 shows the results from 
563
  
 
two different systems with different loss functions 
for learning. Observing the results in Table 3, we 
can see that learning with F1 loss can improve the 
F-score by 0.36% over learning with 0-1 loss, 
similar to the case in NP recognition. More 
specifically, learning with F1 loss provides much 
better results for ADJP, ADVP, DVP, NP and VP, 
respectively. And it yields equivalent or 
comparable results to 0-1 loss in other categories.
 
 F1 loss 0-1 loss 
precision recall F1 precision recall F1 
ADJP 87.86 87.09 87.47 86.74 86.55 86.64 
ADVP 90.66 78.73 84.27 91.91 76.68 83.61 
CLP 0.00 0.00 0.00 1.32 5.88 2.15 
DNP 99.42 99.93 99.68 99.42 99.95 99.69 
DP 99.46 99.76 99.61 99.46 99.76 99.61 
DVP 99.61 99.61 99.61 99.22 99.61 99.42 
LCP 99.74 99.96 99.85 99.74 99.93 99.84 
LST 87.50 52.50 65.63 87.50 52.50 65.63 
NP 91.87 91.01 91.44 91.34 90.52 90.93 
PP 99.57 99.77 99.67 99.57 99.77 99.67 
QP 96.45 96.64 96.55 96.45 97.07 96.76 
VP 90.14 90.39 90.26 89.92 89.79 89.85 
ALL 92.54 91.68 92.11 92.30 91.20 91.75 
Table 3: Experimental results on Chinese text chunking. 
6.4 Comparison with Other Models 
Chen et al2006) compared the performance of 
the state-of-the-art machine learning models for 
Chinese chunking, and found that the SVMs 
approach yields higher accuracy than respective 
CRFs, Transformation-based Learning (TBL) 
(Megyesi, 2002), and Memory-based Learning 
(MBL) (Sang, 2002) approaches.  
In this section, we give a comparison and 
analysis between our model and other state-of-the-
art machine learning models for Chinese NP 
chunking and text chunking tasks. Performance of 
our model and some of the best results from the 
state-of-the-art systems are summarized in Table 4. 
Row "Voting" refers to the phrase-based voting 
methods based on four basic systems, which are 
respectively SVMs, CRFs, TBL and MBL, as 
depicted in (Chen et al2006). Observing the 
results in Table 4, we can see that for both NP 
chunking and text chunking tasks, our model 
achieves significant performance improvement 
over those state-of-the-art systems in terms of the 
F1-score, even for the voting methods. For text 
chunking task, our approach improves performance 
by 0.65% over SVMs, and 0.43% over the voting 
method, respectively. 
 Method F1 
NP 
chunking 
CRFs 89.72 
SVMs 90.62 
Voting 91.13 
Ours 91.50 
Text 
chunking 
CRFs 90.74 
SVMs 91.46 
Voting 91.68 
Ours 92.11 
Table 4: Comparisons of chunking performance for 
Chinese NP chunking and text chunking. 
In particular, for NP chunking task, the F1-score 
of our approach is improved by 0.88% in 
comparison with SVMs, the best single system. 
Further, we investigated the likely cause for 
performance improvement by comparing the 
recognized results from our system and SVMs 
564
  
 
respectively. We first sorted NPs by their length, 
and then calculated the F1-scores associated with 
different lengths for the two systems respectively. 
Figure 3 shows the comparison of F1-scores of the 
two systems by the chunk length. In the Chinese 
chunking corpus, the max NP length is 27, and 
the mean NP length is 1.5. Among all NPs, the 
NPs with the length 1 account for 81.22%. For the 
NPs with the length 1, our system gives slight 
improvement by 0.28% over SVMs. From the 
figure, we can see that the performance gap grows 
rapidly with the increase of the chunk length. In 
particular, the gap between the two systems is 
27.73% when the length hits 4. But the gap begins 
to become smaller with further growth of the 
chunk length. The reasons may include the 
following two aspects. First, the number of NPs 
with the greater length is relatively small in the 
corpus. Second, the NPs with greater length in 
Chinese corpus often exhibit some typical rules.  
For example, an NP with length 8 is given as 
follows. 
 "??/NN(cotton) ?/PU ??/NN(oil) ?/PU ?
?/NN(drug) ?/PU ??/NN(vegetable) ?/ETC 
(et al  
The NP consists of a sequence of nouns simply 
separated by a punctuation "?". So it is also easy 
to be recognized by the sequence labeling 
approach based on SVMs. In summary, the above 
investigation indicates that our system is better at 
recognizing the long and complicated phrases 
compared with the sequence labeling approaches. 
35
45
55
65
75
85
95
1 2 3 4 5 6 7 8 >8The length of NP
F-
sc
or
e
our system
SVM
 
Figure 3: Comparison of F1-scores of NP 
recognition on Chinese corpus by the chunk length. 
6.5 Impact of Different Types of Features 
Our phrase chunking model is highly dependent 
upon chunk-level information. To establish the 
impact of each type of feature (SL-type, Internal-
type, Correlation-type), we look at the 
improvement in F1-score brought about by adding 
each type of features. Table 5 shows the accuracy 
with various features added to the model.  
First consider the effect of the SL-type features. 
If we use only the SL-type features, the system 
achieves slightly lower performance than CRFs or 
SVMs, as shown in Table 4. Since the SL-type 
features consist of the features associated with 
single label, not including the features associated 
with label bigrams. Then, adding the Internal-type 
features to the system results in significant 
performance improvement on NP chunking and on 
text chunking, achieving 2.53% and 1.37%, 
respectively. Further, if Correlation-type features 
are used, the F1-scores on NP chunking and on text 
chunking are improved by 1.01% and 0.66%, 
respectively. The results show a significant impact 
due to the use of Internal-type features and 
Correlation-type features for both NP chunking 
and text chunking. 
Task Type Feature Type F1 
NP chunking 
SL-type 87.96 
+Internal-type 90.49 
+Correlation-type 91.50 
Text chunking
SL-type 90.08 
+Internal-type 91.45 
+Correlation-type 92.11 
Table 5: Test F1-scores for different types of 
features on Chinese corpus. 
6.6 Performance on Other Languages 
We mainly focused on Chinese chunking in this 
paper. However, our approach is generally 
applicable to other languages including English, 
except that the definition of feature templates may 
be language-specific. To validate this point, we 
evaluated our system on the CoNLL 2000 data set, 
a public benchmarking corpus for English 
chunking (Sang and Buchholz 2000). The training 
set consists of 8936 sentences, and the test set 
consists of 2012 sentences. 
We conducted both the NP-chunking and text 
chunking experiments on this data set with our 
approach, using the same feature templates as in 
Chinese chunking task excluding template 13. To 
find the headword within every chunk, we referred 
to the head-finding rules from (Collins, 1999), and 
made a simple modification to them in a similar 
way as in Chinese. As we can see from Table 6, 
565
  
 
our model is able to achieve better performance 
compared with state-of-the-art systems. Table 6 
also shows state-of-the-art performance for both 
NP-chunking and text chunking tasks. LDCRF's 
results presented in (Sun et al2008) are the state-
of-the-art for the NP chunking task, and SVM's 
results presented in (Wu et al2006) are the state-
of-the-art for the text chunking task. 
Moreover, the performance should be further 
improved if some additional features tailored for 
English chunking are employed in our model. For 
example, we can introduce an orthographic feature 
type called Token feature and the affix feature into 
the model, as used in  (Wu et al2006). 
 Method Precision Recall F1 
NP 
chunking 
Ours 94.79 94.65 94.72
LDCRF 94.65 94.03 94.34
Text 
chunking 
Ours 94.31 94.12 94.22
SVMs 94.12 94.13 94.12
Table 6: Performance on English corpus. 
7 Conclusions and Future Work 
In this paper we have presented a novel approach 
to phrase chunking by formulating it as a joint 
segmentation and labeling problem. One important 
advantage of our approach is that it provides a 
natural formulation to exploit chunk-level features. 
The experimental results on both Chinese chunking 
and English chunking tasks show that the use of 
chunk-level features can lead to significant 
performance improvement and that our approach 
outperforms the best in the literature. 
Future work mainly includes the following two 
aspects. Firstly, we will explore applying external 
information, such as semantic knowledge, to 
represent the chunk-level features, and then 
incorporate them into our model to improve the 
performance. Secondly, we plan to apply our 
approach to other joint segmentation and labeling 
tasks, such as clause identification and named 
entity recognition. 
Acknowledgments 
This research is supported by Projects 61073119, 
60773173 under the National Natural Science 
Foundation of China, and project BK2010547 
under the Jiangsu Natural Science Foundation of 
China. We would also like to thank the excellent 
and insightful comments from the three 
anonymous reviewers. 
References  
Steven P. Abney. 1991. Parsing by chunks. In Robert C. 
Berwick, Steven P. Abney, and Carol Tenny, editors, 
Principle-Based Parsing , pages 257-278. Kluwer 
Academic Publishers. 
Daniel M, Bikel. 2004. On the Parameter Space of 
Generative Lexicalized Statistical Parsing Models. 
Ph.D. thesis, University of Pennsylvania. 
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006. 
An empirical study of Chinese chunking. In 
Proceedings of the COLING/ACL 2006 Main 
Conference Poster Sessions, pages 97-104. 
Michael Collins. 2002. Discriminative training methods 
for hidden Markov models: Theory and experiments 
with perceptron algorithms. In Proc. EMNLP-02. 
Michael Collins. 1999. Head-Driven Statistical Models 
for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Koby Crammer. 2004. Online Learning of Complex 
Categorial Problems. Hebrew University of 
Jerusalem, PhD Thesis.  
Taku Kudo and Yuji Matsumoto. 2001. Chunking with 
support vector machines. In Proceedings of 
NAACL01.  
Koby Crammer, Ryan McDonald, and Fernando Pereira. 
2005. Scalable large-margin online learning for 
structured classification. In NIPS Workshop on 
Learning With Structured Outputs. 
Heng Li, Jonathan J. Webster, Chunyu Kit, and 
Tianshun Yao. 2003. Transductive hmm based 
chinese text chunking. In Proceedings of IEEE 
NLPKE2003, pages 257-262, Beijing, China. 
Ryan McDonald, Femando Pereira, Kiril Ribarow, and 
Jan Hajic. 2005. Non-projective dependency parsing 
using spanning tree algorithms. In Proceedings of 
HLT/EMNLP, pages 523-530.  
Ryan. McDonald, K. Crammer, and F. Pereira, 2005. 
Flexible Text Segmentation with Structured 
Multilabel Classification. In Proceedings 
HLT/EMNLP, pages 987- 994. 
Ryan McDonald. 2006. Discriminative Training and 
Spanning Tree Algorithms for Dependency Parsing. 
University of Pennsylvania, PhD Thesis. 
Beata Megyesi. 2002. Shallow parsing with pos taggers 
and linguistic features. Journal of Machine Learning 
Research, 2:639-668. 
566
  
 
Antonio Molina and Ferran Pla. 2002. Shallow parsing 
using specialized hmms. Journal of Machine 
Learning Research., 2:595- 613. 
 E.F.T.K Sang and S. Buchholz. 2000. Introduction to 
the CoNLL-2000 shared task: Chunking. In 
Proceedings CoNLL-00, pages 127-132. 
Sunita Sarawagi and W. Cohen. 2004. Semi-markov 
conditional random fields for information extraction. 
In Proceedings of NIPS 17, pages 1185?1192.  
Fei Sha and Fernando Pereira. 2003. Shallow parsing 
with conditional random fields. In Proceedings of 
HLT-NAACL03. 
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, 
and Jun?ichi Tsujii. 2008. Modeling Latent-Dynamic 
in Shallow Parsing: A Latent Conditional Model with 
Improved Inference. In Proceedings of the 22nd 
International Conference on Computational 
Linguistics, pages 841?848. 
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo 
Zhu. 2004. Chinese chunk identification using svms 
plus sigmoid. In IJCNLP, pages 527-536. 
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo 
Zhu. 2005. Applying conditional random fields to 
chinese shallow parsing. In Proceedings of CICLing-
2005, pages 167-176. 
Erik F. Tjong Kim Sang. 2002. Memory-based shallow 
parsing. JMLR, 2(3):559-594. 
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006. 
A general and multi-lingual phrase chunking model 
based on masking method. In Proceedings of 7th 
International Conference on Intelligent Text 
Processing and Computational Linguistics, pages 
144-155. 
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony 
Kroch. 2000. The bracketing guidelines for the penn 
chinese treebank. Technical report, University of 
Pennsylvania. 
Stavros A. Zenios Yair Censor. 1997. Parallel 
Optimization: Theory, Algorithms, and Applications. 
Oxford University Press. 
Tong Zhang, F. Damerau, and D. Johnson. 2002. Text 
chunking based on a generalization of winnow. 
Journal of Machine Learning Research, 2:615-637.  
Yue Zhang and Stephen Clark. 2008. Joint word 
segmentation and POS tagging using a single 
perceptron. In Proceedings of ACL/HLT, pages 888-
896. 
Yue Zhang and Stephen Clark. 2010. A fast decoder for 
joint word segmentation and POS-tagging using a 
single discriminative model. In Proceedings of 
EMNLP, pages 843-852. 
 
567
