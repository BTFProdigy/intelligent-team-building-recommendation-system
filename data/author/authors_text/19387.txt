Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2097?2106, Dublin, Ireland, August 23-29 2014.
An Analysis of Causality between Events and
its Relation to Temporal Information
Paramita Mirza
Fondazione Bruno Kessler,
University of Trento
Trento, Italy
paramita@fbk.eu
Sara Tonelli
Fondazione Bruno Kessler
Trento, Italy
satonelli@fbk.eu
Abstract
In this work we present an annotation framework to capture causality between events, inspired
by TimeML, and a language resource covering both temporal and causal relations. This data set
is then used to build an automatic extraction system for causal signals and causal links between
given event pairs. The evaluation and analysis of the system?s performance provides an insight
into explicit causality in text and the connection between temporal and causal relations.
1 Introduction
Causality is a concept that has been widely investigated from a philosophical, psychological and logical
point of view, but how to model its recognition and representation in NLP-centered applications is
still an open issue. However, information on causality could be beneficial to a number of natural
language processing tasks such as question answering, text summarization, decision support, etc. The
lack of information extraction systems focused on causality may depend also on the lack of unified
annotation guidelines and standard benchmarks, which usually foster the comparison of different systems
performances. Specific phenomena related to causality, such as causal arguments (Bonial et al., 2010),
causal discourse relations (The PDTB Research Group, 2008) or causal relations between nominals (Girju
et al., 2007), have been investigated, but no unified framework has been proposed to capture causal
relations between events, as opposed to the existing TimeML standard for temporal relations (Pustejovsky
et al., 2010).
The work presented in this paper copes with this issue by i) proposing an annotation framework to
model causal relations between events and ii) detailing the development and the evaluation of a supervised
system based on such framework.
We take advantage of the formalization work carried out for the TimeML standard, in which events,
temporal relations and temporal signals have been carefully defined and annotated. We propose to model
causal relations in a similar way to temporal relations, inheriting from TimeML the notion of event,
relation and signal, even though our approach to causality is well rooted in the force dynamic model by
Talmy (1985).
Besides, we focus our preliminary annotation on TimeBank (Pustejovsky et al., 2006), a corpus widely
used by the research community working on temporal processing. This should possibly enable the
adaptation of existing temporal processing systems to the analysis of causal information, given that we
rely on well-known standards and data. On the other hand, this makes it easier for us to straightforwardly
investigate the relation between temporal and causal information, given that a causing event should always
take place before a resulting event.
2 Related Work
Research on the extraction of event relations has concerned both the analysis of the temporal ordering
of events and the recognition of causality relations. However, the two research lines have progressed
quite independently from each other. Recent works on temporal relations mostly revolve around the last
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2097
TempEval-3
1
shared task on temporal and event processing. The task organizers released some data sets
annotated with events, time expressions and temporal relations in TimeML format (Pustejovsky et al.,
2003), mainly used for training and evaluation purposes. The results of TempEval-3 reported by UzZaman
et al. (2013) show that, even though the performance of systems for extracting TimeML events and
time expressions is quite good (>80% F-score), the overall performance of end-to-end event extraction
pipelines is negatively affected by the poor performance of modules for temporal relation extraction. In
fact, the state-of-the-art performance on the temporal relation extraction task yields only around 36%
F-score (Bethard, 2013).
The problem of detecting causality between events is as challenging as recognizing their temporal
order, but less analyzed from an NLP perspective. Besides, it has mostly focused on specific types of
event pairs and causal expressions in text, and has failed to provide a global account of causal phenomena
that can be captured with NLP techniques. SemEval-2007 Task 4 ?Classification of Semantic Relations
between Nominals? (Girju et al., 2007) gives access to a corpus containing nominal causal relations among
others, as causality is one of the considered semantic relations in the task. Bethard et al. (2008) collected
1,000 conjoined event pairs connected by and from the Wall Street Journal corpus. The event pairs
were annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE,
NO-REL). They use 697 event pairs to train a classification model for causal relations, and use the rest
for evaluating the system, which results in 37.4% F-score. Rink et al. (2010) perform textual graph
classification using the same corpus, and make use of manually annotated temporal relation types as a
feature to build a classification model for causal relations between events. This results in 57.9% F-score,
15% improvement in performance compared with the system without the additional feature of temporal
relations.
The interaction between temporal and causal information, and the contribution of temporal information
to the identification of causal links, are also one of the issues investigated in this paper. However, we aim
at providing a more comprehensive account of how causal relations can be explicitly expressed in a text,
and we do not limit our analysis to specific connectives.
Do et al. (2011) developed an evaluation corpus by collecting 20 news articles from CNN, allowing
the detection of causality between verb-verb, verb-noun, and noun-noun triggered event pairs. Causality
between event pairs is measured by taking into account Point-wise Mutual Information (PMI) between
the cause and the effect. They also incorporate discourse information, specifically the connective types
extracted from the Penn Discourse TreeBank (PDTB), and achieve a performance of 46.9% F-score.
Unfortunately, the data set is not freely available, hence, comparing our work with theirs is not possible.
The most recent work of Riaz and Girju (2013) focuses on the identification of causal relations between
verbal events. They rely on the unambiguous discourse markers because and but to automatically collect
training instances of cause and non-cause event pairs, respectively. The result is a knowledge base of
causal associations of verbs, which contains three classes of verb pairs: strongly causal, ambiguous and
strongly non-causal.
The lack of a standard benchmark to evaluate systems for the extraction of causal relations between
events makes it difficult to compare the performance of different systems, and to identify the state-of-the-
art approach to this particular task. For this reason, we annotated TimeBank, a freely available corpus,
with the aim of making it available to the research community for further evaluations.
3 Data annotation
In order to develop a classifier for the detection of causal relations between events, we first define
annotation guidelines for explicit causality and then manually annotate a data set for training and testing.
3.1 Annotation scheme
Since one of the goals of this work is to investigate the interaction between temporal and causal information,
we define an annotation scheme strongly inspired by the TimeML standard for events, time expressions
and temporal relations. First, we inherit from TimeML the definition of events, which includes all types
1
http://www.cs.york.ac.uk/semeval-2013/task1/
2098
of actions (punctual and durative) and states. Hence, we do not limit our annotation only to specific PoS
such as verbal or nominal events.
Similar to the <TLINK> tag in TimeML for temporal relations, we introduce the <CLINK> tag to
mark a causal relation between two events. Both TLINKs and CLINKs mark directional relations, i.e.
they involve a source and a target event. However, while a list of relation types is part of the attributes for
TLINKs (e.g. BEFORE, AFTER, INCLUDES, etc.), for CLINKs only one relation type is foreseen, going
from a source (the cause, indicated with
S
in the examples) to a target (the effect, indicated with
T
).
We also introduce the notion of causal signals through the <C-SIGNAL> tag. <SIGNAL>s have
been introduced in TimeML to annotate temporal prepositions and other temporal connectives and
subordinators. If a SIGNAL marks the presence of a temporal relation in a text, its ID is added to the
attributes of such TLINK. In a similar way, C-SIGNALs are used to mark-up textual elements signalling
the presence of causal relations, which include all causal uses of prepositions (e.g. because of, as a result
of, due to), conjunctions (e.g. because, since, so that), adverbial connectors (e.g. so, therefore, thus)
and clause-integrated expressions (e.g. the reason why, the result is, that is why). Also for CLINKs it
is possible to assign a c-signalID attribute, in case a C-SIGNAL marks the causal relation between two
events in text.
Concerning the notion of causality, it is particularly challenging to provide guidelines that clearly
define how to identify it in text, since causality exists as a psychological tool for understanding the world
independently of language and it is not necessarily grounded in text (van de Koot and Neeleman, 2012).
There have been several attempts in the psychology field to model causality, including the counterfactual
model (Lewis, 1973), the probabilistic contrast model (Cheng and Novick, 1991; Cheng and Novick,
1992) and the dynamics model (Wolff and Song, 2003; Wolff et al., 2005; Wolff, 2007), which is based on
Talmy?s force dynamic account of causality (Talmy, 1985; Talmy, 1988). We choose to lean our guidelines
on the latter model, since it accounts also for different ways in which causal concepts are lexicalized.
Specifically, Wolff (2007) claims that causation covers three main types of causal concepts, i.e. CAUSE,
ENABLE and PREVENT. These causal concepts are lexicalized through three types of verbs listed in
Wolff and Song (2003): i) CAUSE-type verbs, e.g. cause, prompt, force; ii) ENABLE-type verbs, e.g.
allow, enable, help; and iii) PREVENT-type verbs, e.g. block, prevent, restrain. These categories of
causation and the corresponding verbs are taken into account in our guidelines (Tonelli et al., 2014).
We assign a CLINK if, given two annotated events, there is an explicit causal construction linking them.
Such construction can be expressed in one of the following ways:
1. Expressions containing affect verbs (affect, influence, determine, change, etc.), e.g. Ogun ACN
crisis
S
influences the launch
T
of the All Progressive Congress.
2. Expressions containing link verbs (link, lead, depend on, etc.), e.g. An earthquake
T
in North
America was linked to a tsunami
S
in Japan.
3. Basic constructions involving causative verbs of CAUSE, ENABLE and PREVENT type, e.g. The
purchase
S
caused the creation
T
of the current building.
4. Periphrastic constructions involving causative verbs of CAUSE, ENABLE and PREVENT type,
e.g. The blast
S
caused the boat to heel
T
violently. With ?periphrastic? we mean constructions
where a causative verb (caused) takes an embedded clause or predicate as a complement expressing
a particular result (heel).
5. Expressions containing CSIGNALs, e.g. Its shipments declined
T
as a result of a reduction
S
in
inventories by service centers.
We annotate both intra- and inter-sentential causal relations between events, provided that one of
the above constructions is present. We do not annotate causal relations that are implicit and must be
inferred by annotators, because they may be highly ambiguous and would probably affect inter-annotator
agreement.
2099
3.2 Corpus statistics
Based on the guidelines above, we manually annotated causality in the TimeBank corpus taken from
TempEval-3, containing 183 documents with 6,811 annotated events in total.
2
We chose this corpus
because gold events were already present, between which we could add causal links. Besides, one of our
research goals is the analysis of the interaction between temporal and causal information, and TimeBank
already presents full manual annotation of temporal information according to TimeML standard.
However, during annotation, we noticed that some events involved in causal relations were not annotated,
probably because the corpus was originally built focusing on events involved in temporal relations.
Therefore, we annotated also 137 new events, which led to around 56% increase in the number of
annotated CLINKs.
The total number of annotated CSIGNALs is 171 and there are 318 CLINKs, much less than the number
of TLINKs found in the corpus, which is 5,118. Besides, not all documents contain causality relations
between events. From the total number of documents in TimeBank, only 109 (around 60%) of them
contain explicit causal links and only 87 (around 47%) of them contain CSIGNALs. We also found that
there is no temporal signal (marked by <SIGNAL> tag) annotated in TimeBank, which is unfortunate
since it could help in disambiguating causal signals from temporal signals.
Annotation was performed using the CAT tool (Bartalesi Lenzi et al., 2012), a web-based application
with a plugin to import annotated data in TimeML and add information on top of it. The agreement reached
by two annotators on a subset of 5 documents is 0.844 Dice?s coefficient on C-SIGNALs (micro-average
over markables) and of 0.73 on CLINKs. The built corpus is then used as training and test data in the
experiments for the classification of CSIGNALs and CLINKs, as described in Section 4. This preliminary
analysis on the corpus, however, shows that explicit causal relations between events are less frequently
found in texts than temporal ones. This may lead to data sparseness problems.
4 Experiments
Using the 183 documents from TimeBank manually enriched with causal information for training and
testing, we implement two different classifiers: the first one is a CSIGNAL labeler, that takes in input
information on events and temporal expressions as annotated in the original TimeBank, and classifies
whether a token is part of a causal signal or not (Section 4.1). The second one is a CLINK classifier,
which given an event pair detects whether they are connected by an explicit causal link (Section 4.2). Both
experiments are carried out based on five-fold cross-validation. The overall approach is largely inspired
by our existing framework for the classification of temporal relations (Mirza and Tonelli, 2014).
4.1 Automatic Extraction of CSIGNALs
The task of recognizing CSIGNALs can be seen as a text chunking task, i.e. using a classifier to determine
whether a token is part of a causal signal or not. Since the extent of causal signals can be expressed by
multi-word expressions, we employ the IOB tagging convention to annotate the data, where each token
can either be classified into B-CSIGNAL, I-CSIGNAL or O (for other). We build our classification model
using the Support Vector Machine (SVM) implementation provided by YamCha
3
, a generic, customizable,
and open source text chunker. In order to provide the classifier a feature vector to learn from, we perform
the two following steps:
1. Run the TextPro tool (Pianta et al., 2008) to get information on base NP chunking and whether a
token is part of named entity or not.
2. Run Stanford CoreNLP tool
4
to get information on lemma, part-of-speech (PoS) tags and dependency
relations between tokens.
In the end, the feature vector includes token, lemma, PoS tag, NP chunking, dependency path, and
several binary features, indicating whether a token is: i) an event or part of a temporal expression,
2
The annotated data set is available at http://hlt.fbk.eu/technologies/causal-timebank
3
http://chasen.org/?taku/software/yamcha/
4
http://nlp.stanford.edu/software/corenlp.shtml
2100
according to gold TimeML annotation; ii) part of a named entity or not; and iii) part of a specific discourse
connective type.
Dependency information is encoded as the dependency path between the current token and its governor.
For example, in ?He fell because the chair was broken?, there is a dependency relation mark (broken,
because), where mark indicates the presence of a finite clause subordinate to another clause (de Marneffe
and Manning, 2008). Thus, we encode the dependency feature for the token because as mark (broken). If
the governor is an event, e.g. broken is annotated as an event, the dependency feature is represented as
mark (EVENT) instead.
The mentioned binary features are introduced to exclude the corresponding token as a candidate token
for a causal signal. In other words, if a token is part of a named entity or an event, it is very unlikely that it
will be part of a causal signal. The same holds for all connective types that do not express causal relations,
e.g. temporal or concessive ones. In order to obtain this information, we include in the feature vector the
information about discourse connectives acquired using the addDiscourse tool (Pitler and Nenkova, 2009),
which identifies connectives and assigns them to one of four semantic classes in the framework of the
Penn Discourse Treebank (The PDTB Research Group, 2008): TEMPORAL, EXPANSION, CONTINGENCY
and COMPARISON. Note that causality is part of the CONTINGENCY class.
System Precision Recall F-score
Rule-based (baseline) 54.33% 40.35% 46.31%
Supervised chunking 91.03% 41.76% 57.26%
Table 1: Evaluation of CSIGNAL extraction system
Table 1 shows the performance of our classification model in a five-fold cross-validation setting, which
yields a good precision but a poorer recall, summing up into 57.26% F-score. We also compare our
supervised model with a baseline rule-based system, which labels as CSIGNALs all causal connectors
listed in our annotation guidelines and those appearing in specific syntactic constructions. For instance,
from and by are always labeled as CSIGNAL when they are governed by a passive verb annotated as
event and govern another event, as in the sentence ?The building was damaged
T
by the earthquake
S
.?
Note that this is quite a strong baseline, since the rule-based algorithm embeds some of the intuitions on
syntactic dependencies expressed also as features in the supervised approach.
4.2 Automatic Extraction of CLINKs
Similar to causal signal extraction, we approach the problem of detecting causal links between events as a
supervised classification task. Given an ordered pair or events (e
1
,e
2
), the classifier has to decide whether
there is a causal relation between them or not. However, since we also consider the directionality of the
causal link, an event pair (e
1
,e
2
) is classified into 3 classes: CLINK (where e
1
is the source and e
2
is the
target), CLINK-R (with the reverse order or source and target) or NO-REL. Again, we use YamCha to build
the classifier. This time, a feature vector is built for each pair of events and not for each token as in the
previous classification task.
As candidate event pairs, we take into account every possible combination of events in a sentence
in a forward manner. For example, if we have e
1
, e
2
and e
3
in a sentence (in this order), the candidate
event pairs are (e
1
,e
2
), (e
1
,e
3
) and (e
2
,e
3
). We also include as candidate event pairs the combination
of each event in a sentence with events in the following one. This is necessary to account for inter-
sentential causality, under the simplifying assumption that causality may occur only between events in
two consecutive sentences.
We implement a number of features, some of which are computed independently based on either e
1
or
e
2
, e.g. lemma, PoS, while some others are pairwise features, which are computed based on both elements,
e.g. dependency path, signals in between, etc. The implemented features are as follows:
String and grammatical features. The tokens and lemmas of e
1
and e
2
, along with their PoS and a
binary feature indicating whether e
1
and e
2
have the same PoS tags.
Textual context. The sentence distance and event distance of e
1
and e
2
. Sentence distance measures
2101
how far e
1
and e
2
are from each other in terms of sentences, i.e. 0 if they are in the same sentence. The
event distance corresponds to the number of events occurring between e
1
and e
2
(i.e. if they are adjacent,
the distance is 0).
Event attributes. Event attributes as specified in TimeML annotation, which consist of class, tense,
aspect and polarity. Events being a noun, adjective and preposition do not have tense and aspect attributes
in TimeML. Therefore, we retrieve this information by extracting the tense and aspect of the verbs that
govern them, based on their dependency relation. We also include four binary features representing
whether e
1
and e
2
have the same event attributes or not. These features, especially the tense and aspect
one, are very relevant for detecting causality. For instance, if e
1
is in the future tense and e
2
in the past
tense, there cannot be a causal relation connecting e
1
(as source) and e
2
(as target or result).
Dependency information. We include as features i) the dependency path that exists between e
1
and
e
2
, ii) the type of causative verb connecting them (if any) and iii) binary features indicating whether
e
1
/e
2
is the root of the sentence. This information is based on the collapsed representation of dependency
relations provided by the parsing module of Stanford CoreNLP. Consider the sentence ?Profit from coal
fell
T
to $41 million from $58 million, partly because of a miners? strike
S
.? Based on the collapsed
typed dependencies, we would obtain a direct relation between fell and strike, which is prep because of
(fell, strike). This information combined with the classification of because of as a causal signal would
straightforwardly identify the relation connecting the two events as causal.
Causal signals. We take into account the annotated CSIGNALs connecting two candidate events. We
look for causal signals occurring between e
1
and e
2
, or before e
1
. We also include the position of the
signals (between or before) as feature, since it is crucial to determine the direction of the causality of
a given ordered event pair. This is particularly evident if you consider the position of causal signals
in the following examples: i) ?The building collapsed
T
because of the earthquake
S
? vs. ii) ?Because
of the earthquake
S
the building collapsed
T
.? This feature is also very relevant in connection with the
Textual context, since two events being in two different sentences are linked by an explicit causal relation
only in specific cases, for instance if there is a CSIGNAL in between, typically at the beginning of the
second sentence. Note that in case of several CSIGNALs occurring between e
1
and e
2
, we take the
closest CSIGNAL to e
2
, as in the sentence ?The building was damaged
S
by the earthquake
,
thus, people
moved
T
away?. The dependency path between the causal signal and e
1
/e
2
is also important to determine
the correct involved events in the causal relations. For instance, in the sentence ?They decided
T
to move
because of the earthquake
S
?, the involved event is decided instead of move.
Temporal relations (TLINKs). Rink et al. (2010) showed that including temporal relation information
in detecting causal links results in improving classification performance. Nevertheless, they only analyze
this phenomenon when causality is expressed by the conjunction and. We decided to include this
information in the feature set by specifying the temporal relation type connecting e
1
and e
2
, if any, to see
whether TLINKs help in improving causality detection also in a more comprehensive setting.
We evaluate our approach in a five-fold cross-validation setting, and we compare the performance of
our classifier with a baseline rule-based system. This relies on an algorithm that, given a term t belonging
to affect, link, causative verbs (basic and periphrastic constructions) or causal signals (as listed in the
annotation guidelines), looks for specific dependency constructions where t is connected to two events. If
such dependencies are found, a CLINK is automatically set between the two events identifying the source
and the target of the relation. Further details on the baseline system and its evaluation can be found in
Mirza et al. (2014).
In our experimental setting, we evaluate two versions of the CLINK classifier: the first includes as
features the gold annotated CSIGNALs in the classification model, while the second takes in input
the CSIGNALs automatically annotated by the classifier described in Section 4.1. We also evaluate
the contribution of dependency, CSIGNAL and TLINK features by excluding each of them from the
classification model.
Evaluation results are reported in Table 2. We observe that the baseline is always outperformed by the
other classifiers. CSIGNAL is the most important feature, with a particularly high impact on recall. The
2102
intuition behind this result is that, if a CSIGNAL is present, it is a strong indicator of a causal relation being
present in the surrounding context. This is similar to what Derczynski and Gaizauskas (2012) report for
temporal information, showing that temporal signals provide useful information in TLINK classification.
Dependency information contributes to the performance of the classifier, but is less relevant than TLINK
information. A more detailed analysis of the relation between temporal and causal information is reported
in the following section. The significantly decreasing recall of the classifier using the automatic extracted
CSIGNALs as features is most probably caused by the low recall of the CSIGNAL extraction system.
System Precision Recall F-score
Rule-based (baseline) 36.79% 12.26% 18.40%
Supervised classification (with gold CSIGNALs) 74.67% 35.22% 47.86%
- without dependency feature 65.77% 30.82% 41.97%
- without CSIGNAL feature 57.53% 13.21% 21.48%
- without TLINK feature 61.59% 29.25% 39.66%
Supervised classification (with automatic CSIGNALs) 67.29% 22.64% 33.88%
Table 2: Performance of CLINK extraction system
5 Discussion
We further analyse the output of the automatic extraction systems, in order to understand some phenomena
triggering the results.
5.1 Recognizing CSIGNALs
When we manually inspect the output of the CSIGNAL extraction system, we find that the false positives
are actually the causal signals that annotators missed in the corpus, and not ambiguous connectives. The
system surprisingly yields better precision than human annotation, finding new correct signals.
The recall, however, suffers most probably from data sparseness. It is possible that during the cross-
validation experiments some splits do not have enough data to learn from, recalling that only around 47%
of the documents contain annotated CSIGNALs. Furthermore, 20% of the false negative cases are due to
classifier?s mistakes in detecting the causal signal by, which is highly ambiguous. Our assumption with
the rule-based system that ?by is likely to be a causal signal when it is used to modify a passive verb? is
too restrictive, since by can convey a causal meaning even if the target event is not in the passive voice, as
in the example ?The embargo is meant to cripple
T
Iraq by cutting
S
off its exports of oil and imports of
food and military supplies.?
Another ambiguous causal signal that the classifier fails to detect is the conjunction and. We believe
that more training data, and perhaps more lexical information on the tokens connected by the conjunction
and, are needed for the classifier to be able to disambiguate them.
5.2 Detecting CLINKs
We found that most of the mistakes done by the classifier, as well as by the rule-based system, are caused
by the dependency parser output that tends to establish a dependency relation between a causative verb or
causal signal and the closest verb. For example, in the sentence ?StatesWest Airlines withdrew
T
its offer
to acquire Mesa Airlines because the Farmington carrier did not respond
S
to its offer?, the dependency
parser identify because as the mark of acquire instead of withdrew.
Moreover, also for this task data sparseness is definitely an issue. One possible solution would be to
annotate more data, for instance the AQUAINT data set used for TempEval-3 competition (UzZaman et
al., 2013). Another possibility would be to automatically generate additional data from the Penn Discourse
TreeBank corpus, where causality is one of the discourse relations annotated between argument pairs.
However, a further processing step would be needed to identify inside the argument spans the events
between which a relation holds, which may introduce some errors.
2103
Regarding the directionality of causal relations, the classifier is generally quite precise. 112 out of 150
CLINKs detected by the classifier actually match a causal relation present in the gold annotated data.
Only 8 of them have been classified with the wrong direction. We believe that using the TLINK types as
features contributes to this good performance in disambiguating causality direction (CLINK vs. CLINK-R).
5.3 Interaction between temporal and causal information
We provide in Table 3 some statistics on the overlaps between causal links and temporal relation types
from the gold data. The Others class in the table includes SIMULTANEOUS, IS INCLUDED, BEGUN BY
and DURING INV relations. These counts were obtained by overlapping the temporal information in
TimeBank with the causal information manually added for our experiments. In total, only 32% of the gold
causal links have the underlying temporal relations. Note that the annotators could not see the temporal
links already present in the data, therefore they were not biased by TLINKs when assessing causal links.
BEFORE AFTER IBEFORE IAFTER Others Total
CLINK 15 5 0 0 4 24
CLINK-R 1 67 0 3 8 79
Table 3: Statistics of CLINKs overlapping with TLINKs
The data confirm our intuition that temporal information is a strong constraint when detecting causal
relations, with the BEFORE class having the most overlaps with CLINK and AFTER with CLINK-R. This
is in line with the outcome of our feature analysis reported in Table 2, suggesting that feeding temporal
information into a causal relation classifier yields an improvement in performance. However, the converse
would be less effective, since the occurrences of explicit causal relations are by far less frequent than
temporal ones. Besides, we found that the few cases where CLINKs overlap with AFTER relation are not
due to annotation mistakes, as in the example ?But some analysts questioned
T
how much of an impact
the retirement package will have, because few jobs will end
S
up being eliminated.?
Finally, the performance achieved by our system in causal relation extraction (with gold C-SIGNALs)
is 47.86% F-score, which is better than the performance of the state-of-the-art temporal relation extraction
system with 36.26% (Bethard, 2013). This probably depends on the fact that extracting CLINKs is a
simpler task compared with TLINK extraction: in the first case 3 classes are considered, while temporal
relation types are classified into 14 classes.
6 Conclusions
In this paper, we presented a framework for annotating causal signals and causal relations between events.
Besides, we implemented and evaluated two supervised systems, one classifying C-SIGNALs and the
other CLINKs.
With the first task, we showed that while recognizing unambiguous causal signals is very trivial,
ambiguous signals such as by and and are very difficult to identify because they occur in diverse syntactic
constructions. We definitely need more data to learn from, and perhaps use more lexical information
on the words connected by such causal signals as features. The knowledge base of causal associations
between verbs developed by Riaz and Girju (2013) may be a useful resource to provide such information,
and we will explore this possibility in the future.
We found that the low recall achieved by the CLINK classifier is probably affected by wrong dependen-
cies identified by the Stanford parser. In the future, we would like to test also the C&C tool (Curran et al.,
2007) to extract dependency relations, since it has a better coverage of long-range dependencies. We have
also shown that causal signals are very important in detecting explicit causal links holding between two
events. Finally, we showed that temporal relation types help in disambiguating the direction of causality,
i.e. to determine the source and target event. However, the converse may not hold, since the causal links
in the data set are very sparse, and only 2% of the total TLINKs overlap with CLINKs.
2104
Acknowledgements
The research leading to this paper was partially supported by the European Union?s 7th Framework
Programme via the NewsReader Project (ICT-316404). We also thank Rachele Sprugnoli and Manuela
Speranza for their contribution in defining the annotation guidelines.
References
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele Sprugnoli. 2012. CAT: the CELCT Annotation Tool. In
Proceedings of LREC 2012.
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a Corpus of Temporal-
Causal Structure. In European Language Resources Association (ELRA), editor, Proceedings of the Sixth Inter-
national Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may.
Steven Bethard. 2013. ClearTK-TimeML: A minimalist approach to TempEval 2013. In Proceedings of the Sev-
enth International Workshop on Semantic Evaluation, SemEval ?13, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Claire Bonial, Olga Babko-Malaya, Jinho D. Choi, Jena Hwang, and Martha Palmer. 2010. PropBank An-
notation Guidelines, Version 3.0. Technical report, Center for Computational Language and Education Re-
search, Institute of Cognitive Science, University of Colorado at Boulder. http://clear.colorado.
edu/compsem/documents/propbank_guidelines.pdf.
Patricia W. Cheng and Laura R. Novick. 1991. Causes versus enabling conditions. Cognition, 40(1-2):83 ? 120.
Patricia W. Cheng and Laura R. Novick. 1992. Covariation in natural causal induction. Psychological Review,
99(2):365?382.
James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically Motivated Large-Scale NLP with C&C and
Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions, pages 33?36, Prague, Czech Republic, June. Association
for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages
1?8. Association for Computational Linguistics.
Leon Derczynski and Robert J. Gaizauskas. 2012. Using Signals to Improve Automatic Classification of Temporal
Relations. CoRR, abs/1203.5055.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally Supervised Event Causality Identification.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 294?303,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-
2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-2007), pages 13?18, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
David Lewis. 1973. Causation. The Journal of Philosophy, 70(17):pp. 556?567.
Paramita Mirza and Sara Tonelli. 2014. Classifying Temporal Relations with Simple Features. In Proceedings of
the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308?317,
Gothenburg, Sweden, April. Association for Computational Linguistics.
Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating Causality in the
TempEval-3 Corpus. In Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality
in Language (CAtoCL), pages 10?19, Gothenburg, Sweden, April. Association for Computational Linguistics.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli. 2008. The TextPro Tool Suite. In Proceedings of
the 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco.
European Language Resources Association.
2105
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ?09, pages 13?16, Stroudsburg, PA,
USA. Association for Computational Linguistics.
James Pustejovsky, Jos?e Casta?no, Robert Ingria, Roser Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust specification of event and temporal expressions in text. In Proceedings of the
Fifth International Workshop on Computational Semantics (IWCS-5).
James Pustejovsky, Jessica Littman, Roser Saur??, and Marc Verhagen. 2006. Timebank 1.2 documentation. Tech-
nical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent Romary. 2010. ISO-TimeML: An international stan-
dard for semantic annotation. In Proceedings o the Fifth International Workshop on Interoperable Semantic
Annotation.
Mehwish Riaz and Roxana Girju. 2013. Toward a better understanding of causality between verbal events: Ex-
traction and analysis of the causal power of verb-verb associations. In Proceedings of the SIGDIAL 2013
Conference, pages 21?30, Metz, France, August. Association for Computational Linguistics.
Bryan Rink, Cosmin Adrian Bejan, and Sanda M. Harabagiu. 2010. Learning Textual Graph Patterns to Detect
Causal Event Relations. In Proceedings of the Twenty-Third International FLAIRS Conference.
Leonard Talmy. 1985. Force dynamics in language and thought. Chicago Linguistic Society, 21:293?337.
Leonard Talmy. 1988. Force dynamics in language and cognition. Cognitive science, 12(1):49?100.
The PDTB Research Group. 2008. The PDTB 2.0. Annotation Manual. Technical Report IRCS-08-01, Institute
for Research in Cognitive Science, University of Pennsylvania.
Sara Tonelli, Rachele Sprugnoli, and Manuela Speranza. 2014. Newsreader guidelines for annotation
at document level. Technical Report NWR-2014-2, Fondazione Bruno Kessler. http://www.newsreader-
project.eu/files/2013/01/NWR-2014-2.pdf.
Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013.
Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Proceedings
of the Seventh International Workshop on Semantic Evaluation, SemEval ?13, pages 1?9, Atlanta, Georgia,
USA, June. Association for Computational Linguistics.
H. van de Koot and A. Neeleman, 2012. The Theta System: Argument Structure at the Interface, chapter The
Linguistic Expression of Causation, pages 20 ? 51. Oxford University Press: Oxford.
Phillip Wolff and Grace Song. 2003. Models of causation and the semantics of causal verbs. Cognitive Psychology,
47(3):276?332.
Phillip Wolff, Bianca Klettke, Tatyana Ventura, and Grace Song. 2005. Expressing causation in english and other
languages. Categorization inside and outside the laboratory: Essays in honor of Douglas L. Medin, pages
29?48.
Phillip Wolff. 2007. Representing causation. Journal of experimental psychology: General, 136(1):82.
2106
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308?317,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Classifying Temporal Relations with Simple Features
Paramita Mirza
Fondazione Bruno Kessler
and University of Trento
Trento, Italy
paramita@fbk.eu
Sara Tonelli
Fondazione Bruno Kessler
Trento, Italy
satonelli@fbk.eu
Abstract
Approaching temporal link labelling as a
classification task has already been ex-
plored in several works. However, choos-
ing the right feature vectors to build the
classification model is still an open is-
sue, especially for event-event classifica-
tion, whose accuracy is still under 50%.
We find that using a simple feature set re-
sults in a better performance than using
more sophisticated features based on se-
mantic role labelling and deep semantic
parsing. We also investigate the impact of
extracting new training instances using in-
verse relations and transitive closure, and
gain insight into the impact of this boot-
strapping methodology on classifying the
full set of TempEval-3 relations.
1 Introduction
In recent years, temporal processing has gained in-
creasing attention within the NLP community, in
particular since TempEval evaluation campaigns
have been organized on this topic (Verhagen et
al., 2007; Verhagen et al., 2010; UzZaman et al.,
2013). In particular, the classification of tem-
poral relations holding between entities such as
events and temporal expressions (timex) is crucial
to build event timelines and to reconstruct the plot
of a story. This could be exploited in decision sup-
port systems and document archiving applications,
among others.
In this work we focus on the problem of clas-
sifying temporal relation types, assuming that the
links between events and time expressions are al-
ready established. This task is part of Tempeval-3
evaluation campaign, hence we follow the guide-
lines and the dataset provided by the organizers,
so that we can compare our system with other
systems participating in the challenge. Recent
works have tried to address this complex classifi-
cation task by using sophisticated features, based
on deep parsing, semantic role labelling and dis-
course parsing (D?Souza and Ng, 2013; Laokulrat
et al., 2013). We argue that a simpler approach,
based on lexico-syntactic features, achieves com-
parable results, while reducing the processing time
needed to extract the features. Besides, the perfor-
mance of complex NLP tools may strongly vary
when moving to new domains, affecting in turn the
classification performance, while our approach is
likely to be more stable across different domains.
Our features include some basic information on
the position, the attributes and the PoS tags of
events and timexes, as well as other information
obtained from external lexical resources such as a
list of typical event durations and a list of temporal
signals. The few processing steps required include
PoS-tagging, dependency parsing and the seman-
tic tagging of connectives (based on the parser out-
put).
We also investigate the impact of extending the
number of training instances through inverse rela-
tions and transitive closure, which is a ?simplified?
version of temporal closure covering only entities
connected via the same relation type.
2 Related Work
The task we deal with in this paper was proposed
as part of the TempEval-3 shared task (UzZaman
et al., 2012). Compared to previous TempEval
campaigns, the TempEval-3 task involved recog-
nizing the full set of temporal relations in TimeML
(14 types) instead of a reduced set, increasing the
task complexity. This specific temporal relation
classification task becomes the main focus of this
paper.
Supervised classification of temporal relation
types has already been explored in some earlier
works. Mani et al. (2006) built a MaxEnt classi-
fier to label the temporal links using training data
308
which were bootstrapped by applying temporal
closure. Chambers et al. (2007) focused on clas-
sifying the temporal relation type of event-event
pairs using previously learned event attributes as
features. However, both works use a reduced set
of temporal relations, obtained by collapsing the
relation types that inverse each other into a single
type.
Our work is most similar to the recent work
by D?Souza and Ng (2013). The authors perform
the same task on the full set of temporal rela-
tions, but adopt a much more complex approach.
They utilize lexical relations extracted from the
Merriam-Webster dictionary and WordNet (Fell-
baum, 1998), as well as semantic and discourse
features. They also introduce 437 hand-coded
rules to build a hybrid classification model.
Since we conduct our experiments based on
TempEval-3 task setup, this work is also compa-
rable with the systems participating in the task.
UzZaman et al. (2013) report that three groups
submitted at least one system run to the task.
The best performing one (Laokulrat et al., 2013)
uses, among others, sentence-level semantic in-
formation from a deep syntactic parser, namely
predicate-argument structure features. Another
system (Chambers, 2013) is composed of four
MaxEnt classifiers, two of which have been
trained for event-event links (inter- and intra-
sentence) and two for event-time links. The third-
ranked system (Kolya et al., 2013), instead, im-
plements a much simpler set of features account-
ing for event tense, modality and aspect, event and
timex context, etc.
3 Temporal Link Labelling
In this section we detail the task of temporal re-
lation labelling, the features implemented in our
classification system and the strategy adopted to
bootstrap new training data.
3.1 Task description
The full set of temporal relations specified in
TimeML version 1.2.1 (Saur?? et al., 2006) con-
tains 14 types of relations, as illustrated in Table 1.
Among them there are six pairs of relations that in-
verse each other.
Note that according to TimeML 1.2.1 annota-
tion guidelines, the difference between DURING
and IS INCLUDED (also their inverses) is that
DURING relation is specified when an event per-
sists throughout a temporal duration (e.g. John
drove for 5 hours), while IS INCLUDED relation
is specified when an event happens within a tem-
poral expression (e.g. John arrived on Tuesday).
a |???| a is BEFORE b
b |???| b is AFTER a
a |???| a is IBEFORE b
b |???| b is IAFTER a
a |??| a BEGINS b
b |????| b is BEGUN BY a
a |??| a ENDS b
b |????| b is ENDED BY a
a |??| a is DURING b
b |??????| b is DURING INV a
a |??????| a INCLUDES b
b |??| b IS INCLUDED in a
a |???|
a is SIMULTANEOUS with b
b |???|
a |???| b a is IDENTITY with b
Table 1: Temporal relations in TimeML annota-
tion
In TimeML annotation, temporal links are used
to (i) establish the temporal order of two events
(event-event pair), (ii) anchor an event to a time
expression (event-timex pair) and (iii) establish the
temporal order of two time expressions (timex-
timex pair).
The problem of determining the label of a given
temporal link can be regarded as a classification
problem. Given an ordered pair of entities (e
1
,
e
2
) that could be either event-event, event-timex
or timex-timex pair, the classifier has to assign a
certain label, namely one of the 14 temporal rela-
tion types. We train a classification model for each
category of entity pair, as suggested in several pre-
vious works (Mani et al., 2006; Chambers, 2013).
However, because there are very few examples
of timex-timex pairs in the training corpus, it is not
possible to train the classification model for these
particular pairs. Moreover, they only add up to
3.2% of the total number of extracted entity pairs;
therefore, we decided to disregard these pairs.
3.2 Feature set
We implement a number of features for tempo-
ral relation classification. Some of them are ba-
sic ones which take into account morpho-syntactic
information on events and time expressions, their
textual context and their attributes. Others rely
on semantic information such as typical event du-
rations and connective type. However, we avoid
complex processing of data. Such semantic infor-
mation is based on external lists of lexical items
309
and on the output of the addDiscourse tagger
(Pitler and Nenkova, 2009).
Some features are computed independently
based on either e
1
or e
2
, while some others are
pairwise features, which are computed based on
both elements. Some pairwise features are only
relevant for event-event pairs, for example, the
information on discourse connectives and the
binary features representing whether two events
have the same event attributes or not. Similarly,
the features related to time expression attributes
are only relevant for event-timex pairs, since
this information can only be obtained if e
2
is a
time expression. The selection of features that
contribute to the improvement of event-event
and event-timex classification will be detailed in
Section 4.3.
String features. The tokens and lemmas of
e
1
and e
2
.
Grammatical features. The part of speech
(PoS) tags of e
1
and e
2
, and a binary feature
indicating whether e
1
and e
2
have the same
PoS tag. The binary feature only applies to
event-event pairs since we do not include the
PoS tag of a time expression in the feature set
of event-timex pairs. The grammatical informa-
tion is obtained using the Stanford CoreNLP tool.
1
Textual context. The textual order, sentence
distance and entity distance of e
1
and e
2
. Textual
order is the appearance order of e
1
and e
2
in the
text, while sentence distance measures how far e
1
and e
2
are from each other in terms of sentences,
i.e. 0 if they are in the same sentence. The entity
distance is only measured if e
1
and e
2
are in the
same sentence, and corresponds to the number of
entities occurring between e
1
and e
2
(i.e. if they
are adjacent, the distance is 0).
Entity attributes. Event attributes and time
expression attributes of e
1
and e
2
as specified
in TimeML annotation. Event attributes consist
of class, tense, aspect and polarity, while the
attributes of a time expression are its type, value
and dct (indicating whether a time expression
is the document creation time or not). Events
falling under the category of noun, adjective and
1
http://nlp.stanford.edu/software/
corenlp.shtml
preposition do not have tense and aspect attributes
in TimeML. We retrieve this information by
extracting the tense and aspect of the verbs that
govern them, based on their dependency relation.
For event-event pairs we also include four binary
features representing whether e
1
and e
2
have the
same event attributes or not.
Dependency relations. Similar to D?Souza
and Ng (2013), we use the information related to
the dependency relation between e
1
and e
2
. We
include as features (i) the type of the dependency
relation that exists between them, (ii) the depen-
dency order which is either governor-dependent
or dependent-governor and (iii) binary features
indicating whether e
1
/e
2
is the root of the sen-
tence. This information is based on the collapsed
representation of dependency relations provided
by the parsing module of Stanford CoreNLP.
Consider the sentence ?John left the office and
drove back home for 20 minutes?. Using the
collapsed typed dependencies we could get the di-
rect relations between the existing entities, which
are conj and(left, drove) and prep for(drove,
minutes).
Event durations. To our knowledge, we are
the first to exploit event duration information
as features for temporal relation classification.
In fact, duration can be expressed not only by
a predicate?s tense and aspect but also by its
aktionsart, i.e. the inherent temporal information
connected to the meaning of a predicate. The
typical event duration allows us to infer, for
instance, that a punctual event is more likely to
be contained in a durative one. If we consider the
sentence ?State-run television broadcast footage
of Cuban exiles protesting in Miami?, this feature
would tell us that broadcast lasts for hours while
protesting lasts for days, thus contributing in
determining the direction of DURING relation
between the events.
The approximate duration for an event is
obtained from the list of 1000 most frequent
verbs and their duration distributions compiled
by Gusev et al. (2011).
2
The types of duration
include seconds, minutes, hours, days, weeks,
months, years and decades. We also add the
duration difference between e
1
and e
2
as a feature
2
The list is available at http://cs.stanford.edu/
people/agusev/durations/
310
with the value varied between same, less or more.
Similar to tense and aspect attributes for events,
the duration of events under the category of noun,
adjective and preposition are estimated by the
governing verb. As for time expressions, their
durations are estimated from their type and value
attributes using a set of simple rules, e.g. the
duration of Thursday morning (with the type of
TIME and the value of xxxx-xx-xxTMO) is hours.
Temporal signals. Derczynski and Gaizauskas
(2012) show the importance of temporal signals
in temporal link labelling. We take this into
account by integrating in our features the list of
signals extracted from TimeBank 1.2 corpus
3
. We
believe that the system performance will benefit
from distinguishing between event-related signals
and timex-related signals, therefore we manually
split the signals into two separate lists. Signals
such as when, as and then are commonly used
to temporally connect events, while signals such
as at, for and within more likely occur with time
expressions. There are also signals that are used
in both cases such as before, after and until, and
those kind of signals are added to both lists.
Besides the signal token, the position of the sig-
nal with respect to the events or time expressions
is also an important feature. Consider the position
of a signal in the sentences (i) ?John taught high
school before he worked at a bank? and (ii)
?Before John taught high school, he worked at a
bank?, which is crucial to determine the order of
John?s occupations. We also include in the feature
set whether a signal occurs at the beginning of a
sentence, as it is usually used to temporally relate
events in different sentences, e.g. ?John taught
high school. Previously, he worked at a bank.?
Temporal discourse connectives. Consider
the following sentences:
(i) ?John has been taking that driving course
since the accident that took place last week.?
(ii) ?John has been taking that driving course
since he wants to drive better.?
In order to label the temporal link holding be-
tween two events, it is important to know whether
there are temporal connectives in the surrounding
3
The list is available at http://www.timeml.org/
site/timebank/browser_1.2/displayTags.
php?treshold=1&tagtype=signal&sort=alpha
context, because they may contribute in identify-
ing the relation type. For instance, it may be rele-
vant to distinguish whether since is used as a tem-
poral or a causal cue (example (i) and (ii) resp.).
This information about discourse connectives is
acquired using the addDiscourse tool (Pitler and
Nenkova, 2009), which identifies connectives and
assigns them to one of four semantic classes: Tem-
poral, Expansion, Contingency and Comparison.
Note that this is a much shallower approach than
the one proposed by D?Souza and Ng (2013), who
perform full discourse parsing.
We include as feature whether a discourse con-
nective belonging to the Temporal class occurs in
the textual context of e
1
and e
2
. Similar to tem-
poral signals, we also include in the feature set the
position of the discourse connective with respect
to the events.
3.3 Inverse Relations and Transitive Closure
Since Mani et al. (2006) demonstrate that boot-
strapping training data through temporal closure
results in quite significant improvements, we try
to provide the classifier with more data to learn
from using the inverse relations and closure-based
inferred relations.
There are six pairs of relation types in TimeML
that inverse each other (see Table 1). By switch-
ing the order of the entities in a given pair and la-
belling the pair with the inverse relation type, we
basically multiply the number of training data.
As for temporal closure, there have been at-
tempts to apply it to improve temporal relation
classification. Mani et al. (2006) use SputLink
(Verhagen, 2005), which was developed based on
Allen?s closure inference (Allen, 1983), to infer
the relations based on temporal closure. UzZaman
and Allen (2011b) employ Timegraph (Gerevini et
al., 1995) to implement the scorer for TempEval-3
evaluation, since precision and recall for temporal
relation classification are computed based on the
closure graph.
We use a simpler approach to obtain the closure
graph of temporal relations, by applying the tran-
sitive closure only within the same relation type,
e.g. e
1
BEFORE e
2
? e
2
BEFORE e3 ? e
1
BE-
FORE e3. It can be seen as partial temporal clo-
sure since it produces only a subset of the rela-
tions produced by temporal closure, which covers
more complex cases, e.g. e
1
BEFORE e
2
? e
2
IN-
CLUDES e3? e
1
BEFORE e3.
311
As shown in Fischer and Meyer (1971), the
problem of finding the transitive closure of a di-
rected acyclic graph can be reduced to a boolean
matrix multiplication. For each temporal relation
type, we build its boolean matrix with the size of
n ? n, n being the number of entities in a text.
Given a temporal relation type R and its boolean
matrix M , the transitive closure-based relations of
R can be inferred from the matrix M
2
by extract-
ing its non-zero elements.
4 Experiment Description
4.1 Dataset
Since we want to compare our work with ex-
isting approaches to temporal relation classifica-
tion, we use the same training and test data as
in Tempeval-3 challenge
4
. Two types of train-
ing data were made available in the challenge:
TBAQ-cleaned and TE3-Silver-data. The former
includes a cleaned and improved version of the
AQUAINT TimeML corpus, containing 73 news
report documents, and the TimeBank corpus, with
183 news articles. TE3-Silver-data, instead, is a
600K word corpus annotated by the best perform-
ing systems at Tempeval-2, which we do not use
in our experiments.
Our test data is the newly created TempEval-
3-platinum evaluation corpus that was anno-
tated/reviewed by the Tempeval-3 task organizers.
The distribution of the relation types in all previ-
ously mentioned datasets is shown in Table 2. We
report also the statistics obtained after applying in-
verse relations and transitive closure, that increase
the number of training instances.
It is worth noticing that DURING INV relation
does not exist in the training data but appears in the
test data. In this case, inverse relations help in au-
tomatically acquiring training instances. The BE-
FORE relation corresponds to the majority class
and makes the instance distribution quite unbal-
anced, especially in the TBAQ corpus. Finally,
five event-timex instances in the TBAQ training
data are labeled with IDENTITY relation and can
be assumed to be falsely annotated.
4.2 Experimental Setup
We build our classification models using the Sup-
port Vector Machine (SVM) implementation pro-
4
http://www.cs.york.ac.uk/
semeval-2013/task1/index.php?id=data
vided by YamCha
5
. The experiment involves con-
ducting 5-fold cross validation on the TimeBank
corpus to find the best combination of features for
the event-event and event-timex classifiers. We
first run our experiments using YamCha default
parameters (pairwise method for multi-class clas-
sification and polynomial kernel of degree 2). Af-
ter identifying the best feature sets for the two
classifiers, we evaluate them using different ker-
nel degrees (from 1 to 4).
4.3 Feature Engineering
In order to select from our initial set of features
only those that improve the accuracy of the event-
event and event-timex classifiers, we incremen-
tally add them to the baseline (the model with
string feature only), and compute their contribu-
tion. Table 3 shows the results of this selection
process, by including the average accuracy from
the 5-fold cross validation.
In Table 3, we report the feature contributions of
the entity attributes and dependency relations sets
in more details, because within those categories
only some of the features have a positive impact on
accuracy. Instead, for features within textual con-
text, signal and discourse categories, incremen-
tally adding each feature results in increasing ac-
curacy, therefore we report only the overall accu-
racy of the feature group. Similarly, for duration
features, adding each feature incrementally results
in decreasing accuracy.
Regarding entity attributes, it can be seen that
aspect and class features have no positive im-
pact on the accuracy of event-event classifica-
tion, along with pairwise features same class and
same polarity. As for event-timex classification,
all event attributes except for polarity contribute
to accuracy improvements. Among time expres-
sion attributes, only the information about whether
a time expression is a document creation time or
not (dct feature) helps improving the classifier.
The dependency order feature does not give
positive contribution to the accuracy in both cases.
Besides, information on whether an event is the
root of the sentence (dependency is root feature)
is not relevant for event-timex classification.
Adding the temporal signal feature very slightly
improves the accuracy of event-event classifica-
tion, not as much as its contribution to event-timex
5
http://chasen.org/
?
taku/software/
yamcha/
312
Relation
event-event event-timex
train test train test
TB TBAQ TBAQ-I TBAQ-IC TE3-P TB TBAQ TBAQ-I TBAQ-IC TE3-P
BEFORE 490 2,115 2,938 5,685 226 661 1,417 1,925 2,474 96
AFTER 458 823 2,938 5,685 167 205 509 1,925 2,474 29
IBEFORE 22 60 103 105 1 2 3 8 8 5
IAFTER 27 43 103 105 2 4 5 8 8 6
BEGINS 24 44 86 85 0 20 65 89 89 1
BEGUN BY 24 42 86 85 1 22 24 89 89 1
ENDS 12 17 79 79 1 47 59 120 120 2
ENDED BY 44 62 79 79 0 57 61 120 120 2
DURING 46 80 80 84 1 197 200 200 201 1
DURING INV 0 0 80 84 0 0 0 200 201 1
INCLUDES 170 308 724 7,246 40 288 1,104 2,945 3,404 42
IS INCLUDED 212 416 724 7,246 47 897 1,841 2,945 3,404 125
SIMULTANEOUS 456 519 519 518 81 58 58 58 58 6
IDENTITY 534 742 742 742 15 4 5 5 5 0
Total 2,519 5,271 9,281 27,828 582 2,462 5,351 10,637 12,655 317
Table 2: The distribution of each relation type in the datasets for both event-event and event-timex pairs.
TB stands for TimeBank corpus, TBAQ denotes the combination of TimeBank and AQUAINT corpora,
TBAQ-I denotes the TBAQ corpus augmented with inverse relations, TBAQ-IC is the TBAQ corpus
with both inverse relations and transitive closure, and TE3-P is the TempEval-3-platinum evaluation
corpus.
classification. However, together with the tem-
poral discourse feature, they positively affect ac-
curacy, confirming previous findings (Derczynski
and Gaizauskas, 2012).
Surprisingly, adding event duration feature de-
creases the accuracy in both cases. This might be
caused by the insufficient coverage of the event
duration resource, since around 20% of the train-
ing pairs contain at least an event whose duration
is unknown. Moreover, we employ the approxi-
mate duration of a verb event as a feature without
considering the context and discourse. For exam-
ple, according to the distributions in the duration
resource, the event attack has two likely durations,
minutes and decades, with decades being slightly
more probable than minutes. In the sentence ?Is-
rael has publicly declared that it will respond to
an Iraqi attack on Jordan.?, the classifier fails to
recognize the IBEFORE relation between attack
and respond (attack happens immediately before
respond), because the duration feature of attack is
recognized as decades, while in this context the
attack most probably occurs within seconds.
According to the analysis of the different fea-
ture contributions, we define the best classification
models for both event-event pairs and event-timex
pairs as the models using combinations of features
that have positive impacts on the accuracy, based
on Table 3. Given the best performing sets of fea-
tures, we further experiment with different kernel
degrees in the same 5-fold cross validation sce-
nario.
The best classifier performances are achieved
with the polynomial kernel of degree 4, both for
event-event and event-timex classification. The
accuracy for event-event classification is 43.69%,
while for event-timex classification it is 66.62%.
However, using a high polynomial kernel degree
introduces more complexity in training the classi-
fication model, thus more time is required to train
such models.
D?Souza and Ng (2013) evaluate their system
on the same corpus, but with a slightly different
setting. They also split TimeBank into 5 folds,
but they only use two of them to perform 2-fold
cross validation, while they use another part of the
corpus to develop rules for their hybrid system.
Their best configuration gives 46.8% accuracy for
event-event classification and 65.4% accuracy for
event-timex classification. Although the two ap-
proaches are not directly comparable, we can as-
sume that the systems? performance are likely to
be very similar, with a better accuracy on event-
event classification by D?Souza and Ng (2013) and
a better performance on event-timex pairs by our
system. Probably, the hybrid system by D?Souza
and Ng, which integrates supervised classification
and manual rules, performs better on event-event
classification because it is a more complex task
than event-timex classification, where simple lex-
ical and syntactic features are still very effective.
313
event-event event-timex
Feature Accuracy Feature Accuracy
majority class 22.17% - majority class 36.42% -
string 31.07% - string 58.27% -
+grammatical 36.15% 5.08% +grammatical 61.30% 3.03%
+textual context 39.44% 3.29% +textual context 61.71% 0.41%
+tense 41.10% 1.66% +tense 63.10% 1.39%
+aspect 41.10% 0.00% +aspect 64.51% 1.41%
+class 39.96% -1.14% +class 65.30% 0.79%
+polarity 40.44% 0.48% +polarity 64.88% -0.42%
+same tense 40.55% 0.11% +dct 65.21% 0.33%
+same aspect 40.63% 0.08% +type 64.99% -0.22%
+same class 40.63% 0.00% +value 64.60% -0.39%
+same polarity 40.47% -0.16%
+ dependency 42.15% 1.68% +dependecy 65.60% 1.00%
+dependency order 41.99% -0.16% +dependency order 65.47% -0.13%
+dependency is root 42.63% 0.64% +dependency is root 65.22% -0.25%
+temporal signal 42.66% 0.03% +temporal signal 65.43% 0.21%
+temporal discourse 42.82% 0.16%
+duration 41.47% -1.35% +duration 64.19% -1.24%
Table 3: Feature contributions for event-event and event-timex classification. Features in italics have a
negative impact on accuracy and are not included in the final feature set.
5 Evaluation
We perform two types of evaluation. In the first
one, we evaluate the system performance with the
best feature sets and the best parameter configu-
ration using the four training sets presented in Ta-
ble 2. Our test set is the TempEval-3-platinum cor-
pus. The goal of this first evaluation is to specifi-
cally investigate the effect of enriching the training
data with inverse relations and transitive closure.
We compute the system accuracy as the percent-
age of the correct labels out of all annotated links.
In the second evaluation, we compare our sys-
tem to the systems participating in the task on tem-
poral relation classification at TempEval-3. The
test set is again TempEval-3-platinum, i.e. the
same one used in the competition. The task or-
ganizers introduced an evaluation metric (UzZa-
man and Allen, 2011a) capturing temporal aware-
ness in terms of precision, recall and F1-score. To
compute precision and recall, they verify the cor-
rectness of annotated temporal links using tempo-
ral closure, by checking the existence of the iden-
tified relations in the closure graph. In order to
replicate this type of evaluation, we use the scorer
made available to the task participants.
5.1 Evaluation of the Effects of Inverse
Relations and Transitive Closure
Table 4 shows the classifiers? accuracies achieved
using different training sets. After performing a
randomization test between the best performing
classifier and the others, we notice that on event-
event classification the improvement is significant
(p < 0.005) only between TBAQ and TimeBank.
This shows that only extending the TimeBank cor-
pus by adding AQUAINT is beneficial. In all other
cases, the differences are not significant. Applying
inverse relations and transitive closure extends the
number of training instances but makes the dataset
more unbalanced, thus it does not result in a sig-
nificant improvement.
Training data event-event event-timex
TimeBank 42.61% 71.92%
TBAQ 48.28% 73.82%
TBAQ-I 47.77% 74.45%
TBAQ-IC 46.39% 74.45%
Table 4: Classifier accuracies with different train-
ing data
This result is in contrast with the improvement
brought about by temporal closure reported in
Mani et al. (2006). The difference between our
approach and Mani et al.?s is that (i) we apply only
the transitive closure instead of the full temporal
one, and (ii) our classification task includes 14 re-
lations, while the other authors classify 6 relations.
In our future work, we will investigate whether the
benefits of closure are affected by the number of
relations, or whether our simplified version is ac-
tually outperformed by the full one.
Furthermore, we plan to investigate the effect of
over-sampling to handle highly skewed datasets,
for instance by applying inverse relations and tran-
sitive closure only to minority classes.
314
5.2 Evaluation of the System Performance in
TempEval-3 task
We train our classifiers for event-event pairs and
event-timex pairs by exploiting the best feature
combination and best configuration acquired from
the experiment, and using the best reported dataset
for each classifier as the training data. Even
though it has been shown that inverse relations and
transitive closure do not bring significantly posi-
tive impact to the accuracy, using the TBAQ-IC
corpus as the training set for event-timex classifi-
cation is still the best option. The two classifiers
are part of a temporal classification system called
TRelPro.
We compare in Table 5 the performance of
TRelPro to the other systems participating in
Tempeval-3 task, according to the figures reported
in (UzZaman et al., 2013). TRelPro is the best per-
forming system both in terms of precision and of
recall.
System F1 Precision Recall
TRelPro 58.48% 58.80% 58.17%
UTTime-1, 4 56.45% 55.58% 57.35%
UTTime-3, 5 54.70% 53.85% 55.58%
UTTime-2 54.26% 53.20% 55.36%
NavyTime-1 46.83% 46.59% 47.07%
NavyTime-2 43.92% 43.65% 44.20%
JU-CSE 34.77% 35.07% 34.48%
Table 5: Tempeval-3 evaluation on temporal rela-
tion classification
In order to analyze which are the most com-
mon errors made by TRelPro, we report in Table 6
the number of true positives (tp), false positives
(fp) and false negatives (fn) scored by the system
on each temporal relation. The system generally
fails to recognize IBEFORE, BEGINS, ENDS and
DURING relations, along with their inverse rela-
tions, primarily because of the skewed distribution
of instances in the training data, especially in com-
parison with the majority classes. This explains
also the large number of false positives labelled for
the BEFORE class (event-event pairs) and for the
IS INCLUDED class (event-timex pairs), which
are the majority classes for the two pairs respec-
tively.
6 Conclusion
We have described an approach to temporal link
labelling using simple features based on lexico-
syntactic information, as well as external lexical
resources listing temporal signals and event dura-
Relation
event-event event-timex
tp fp fn tp fp fn
BEFORE 186 186 40 82 17 14
AFTER 63 40 104 14 7 15
IBEFORE 0 0 1 0 0 5
IAFTER 0 0 2 0 0 6
BEGINS 0 0 0 0 0 1
BEGUN BY 0 0 0 0 0 1
ENDS 0 0 1 0 0 2
ENDED BY 1 1 0 0 0 2
DURING 0 0 1 0 2 1
DURING INV 0 0 0 0 0 1
INCLUDES 1 2 39 27 13 15
IS INCLUDED 2 4 45 114 40 11
SIMULTANEOUS 20 33 61 0 0 6
IDENTITY 9 35 6 0 1 0
Table 6: Relation type distribution for TempEval-
3-platinum test data, annotated with TRelPro. The
tp fields indicate the numbers of correctly anno-
tated instances, while the fp/fn fields correspond
to false positives/negatives.
tions. We find that by using a simple feature set we
can build a system that outperforms the systems
built using more sophisticated features, based on
semantic role labelling and deep semantic parsing.
This may depend on the fact that more complex
features are usually extracted from the output of
NLP systems, whose performance impacts on the
quality of such features.
We find that bootstrapping the training data with
inverse relations and transitive closure does not
help improving the classifiers? performances sig-
nificantly as it was reported in previous works, es-
pecially in event-event classification where the ac-
curacy decreases instead. In the future, we will
further investigate the reason of this difference.
We will also explore other variants of closure, as
well as over-sampling techniques to handle the
highly skewed dataset introduced by closure.
Finally, the overall performance of our system,
using the best models for both event-event and
event-timex classification, outperforms the other
systems participating in the TempEval-3 task. This
confirms our intuition that using simple features
and reducing the amount of complex semantic and
discourse information is a valuable alternative to
more sophisticated approaches.
Acknowledgments
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project
(ICT-316404).
315
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?
843, November.
Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meeting
of the ACL on Interactive Poster and Demonstra-
tion Sessions, ACL ?07, pages 173?176, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associ-
ation for Computational Linguistics.
Leon Derczynski and Robert J. Gaizauskas. 2012. Us-
ing Signals to Improve Automatic Classification of
Temporal Relations. CoRR, abs/1203.5055.
Jennifer D?Souza and Vincent Ng. 2013. Classify-
ing Temporal Relations with Rich Linguistic Knowl-
edge. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 918?927.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Michael J. Fischer and Albert R. Meyer. 1971.
Boolean matrix multiplication and transitive closure.
In SWAT (FOCS), pages 129?131. IEEE Computer
Society.
Alfonso Gerevini, Lenhart Schubert, and Stephanie
Schaeffer. 1995. The temporal reasoning tools
Timegraph I-II. International Journal of Artificial
Intelligence Tools, 4(1-2):281?299.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Juraf-
sky. 2011. Using query patterns to learn the dura-
tion of events. In Proceedings of the Ninth Inter-
national Conference on Computational Semantics,
IWCS ?11, pages 145?154, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Anup Kumar Kolya, Amitava Kundu, Rajdeep Gupta,
Asif Ekbal, and Sivaji Bandyopadhyay. 2013.
Ju cse: A crf based approach to annotation of tem-
poral expression, event and temporal relations. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64?72, Atlanta,
Georgia, USA, June. Association for Computational
Linguistics.
Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Uttime:
Temporal relation classification using deep syntactic
features. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 88?92,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
ACL-44, pages 753?760, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Emily Pitler and Ani Nenkova. 2009. Using syn-
tax to disambiguate explicit discourse connectives
in text. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 13?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Roser Saur??, Jessica Littman, Robert Gaizauskas, An-
drea Setzer, and James Pustejovsky, 2006. TimeML
Annotation Guidelines, Version 1.2.1.
Naushad UzZaman and James Allen. 2011a. Tem-
poral Evaluation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
351?356, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Naushad UzZaman and James F. Allen. 2011b. Tem-
poral evaluation. In ACL (Short Papers), pages 351?
356. The Association for Computer Linguistics.
Naushad UzZaman, Hector Llorens, James F. Allen,
Leon Derczynski, Marc Verhagen, and James Puste-
jovsky. 2012. TempEval-3: Evaluating Events,
Time Expressions, and Temporal Relations. CoRR,
abs/1206.5333.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identification. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations, Se-
mEval ?07, pages 75?80, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
316
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, SemEval
?10, pages 57?62, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Marc Verhagen. 2005. Temporal closure in an annota-
tion environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
317
Proceedings of the ACL 2014 Student Research Workshop, pages 10?17,
Baltimore, Maryland USA, June 22-27 2014.
c
?2014 Association for Computational Linguistics
Extracting Temporal and Causal Relations between Events
Paramita Mirza
Fondazione Bruno Kessler
University of Trento
Trento, Italy
paramita@fbk.eu
Abstract
A notably challenging problem related to
event processing is recognizing the rela-
tions holding between events in a text, in
particular temporal and causal relations.
While there has been some research on
temporal relations, the aspect of causality
between events from a Natural Language
Processing (NLP) perspective has hardly
been touched. We propose an annotation
scheme to cover different types of causality
between events, techniques for extracting
such relations and an investigation into the
connection between temporal and causal re-
lations. In this thesis work we aim to focus
especially on the latter, because causality
is presumed to have a temporal constraint.
We conjecture that injecting this presump-
tion may be beneficial for the recognition
of both temporal and causal relations.
1 Introduction
With the rapid growth of information available on
the world wide web, especially in the form of un-
structured and natural texts, information extraction
(IE) becomes one of the most prominent fields in
NLP research. IE aims to provide ways to automat-
ically extract the available information and store
them in a structured representation of knowledge.
The stored knowledge can then be useful for many
NLP applications, such as question answering, tex-
tual entailment, summarization, and focused infor-
mation retrieval systems.
There are several subtasks within information
extraction related to the type of knowledge one
wishes to extract from the text, event extraction be-
ing one of them. Event extraction is considered to
be a non-trivial task, due to the fact that mentions
of an event in text could be highly varied in terms
of sentence construction, and that the attributes de-
scribing an event are usually mentioned in several
sentences. However, the most challenging problem
in the context of event extraction is identifying the
relationship between events.
Events are usually anchored to temporal expres-
sions. The temporal attribute of an event can be
used to determine the temporal relationship be-
tween events. This information can be useful for
the ordering of event sequence in a timeline, e.g.
for the better presentation of news or history texts.
Moreover, in multi-document summarization of
news articles, the relative order of events is impor-
tant to merge and present information from multi-
ple sources correctly.
A more complex type of relationship between
events is causality. Identifying the causal relation
between events is an important step in predicting
occurrence of future events, and can be very ben-
eficial in risk analysis as well as decision making
support.
There is an overlap between causal and tem-
poral relations, since by the definition of causal-
ity, the first event (cause) must happen BEFORE
the second event (effect). We claim that a system
for extracting both temporal and causal relations,
may benefit from integrating this presumption. The
main focus of this research work will be (i) investi-
gating ways to utilize this presumption in building
an integrated event relation extraction system, in
addition to (ii) exploring ways to develop a robust
extraction component for each type of relations
(temporal and causal).
2 Background
In NLP, the definition of an event can be varied de-
pending on the target application. In topic detection
and tracking (Allan, 2002), the term event is used
interchangeably with topic, which describes some-
thing that happens and is usually used to identify a
cluster of documents, e.g. Olympics, wars. On the
other hand, information extraction provides finer
granularity of event definitions, in which events
10
are entities that happen/occur within the scope of a
document.
There are several annotation frameworks for
events and temporal expressions that can be viewed
as event models,
1
TimeML (Pustejovsky et al,
2003b) and ACE (Consortium, 2005) being the
prominent ones.
Both TimeML and ACE define an event as
something that happens/occurs or a state that
holds true, which can be expressed by a verb,
a noun, an adjective, as well as a nominaliza-
tion either from verbs or adjectives. Consider
the following passage annotated with events and
temporal expressions (TIMEX). ?A Philippine
volcano, dormant
EVENT
for six centuries
TIMEX
,
exploded
EVENT
last Monday
TIMEX
. During the
eruption
EVENT
, lava, rocks and red-hot ash are
spewed
EVENT
onto surrounding villages. The
explosion
EVENT
claimed
EVENT
at least 30 lives.?
The most important attribute of TimeML that
differs from ACE is the separation of the repre-
sentation of events and temporal expressions from
the anchoring or ordering dependencies. Instead
of treating a temporal expression as an event ar-
gument, TimeML introduces temporal link annota-
tions to establish dependencies (temporal relations)
between events and temporal expressions (Puste-
jovsky et al, 2003b). This annotation is important
in (i) anchoring an event to a temporal expression
(event time-stamping) and (ii) determining the tem-
poral order between events. This distinctive feature
of TimeML becomes our main consideration in
choosing the event model for our research.
Moreover, TimeML is the annotation framework
used in TempEval-3
2
, the most recent shared task
on temporal and event processing. The ultimate
goal of this evaluation campaign is the automatic
identification of temporal expressions, events, and
temporal relations within a text (UzZaman et al,
2012).
The main tasks defined in TempEval-3 include:
the automatic extraction of TimeML entities, i.e.
temporal expressions and events, and the end-to-
end automatic extraction of both TimeML enti-
ties and temporal links/relations. The result of
TempEval-3 reported by UzZaman et al (2013)
1
There are other event models based on web ontology
(RDFS+OWL) such as LODE (Shaw et al, 2009), SEM (van
Hage et al, 2011) and DOLCE (Gangemi et al, 2002), which
encode knowledge about events as triples. Such models can
be seen as ways to store the extracted knowledge to perform
the reasoning on.
2
http://www.cs.york.ac.uk/semeval-2013/task1/
shows that even though the performances of sys-
tems for extracting TimeML entities are quite good
(>80% F-score), the overall performance of end-
to-end event extraction systems suffers from the
low performance of the temporal relation extrac-
tion system. The state-of-the-art performance on
the temporal relation extraction task yields only
around 36% F-score. This becomes the main rea-
son of focusing our research on the extraction of
event relations.
3 Research Problem
We consider two types of event relations to be ex-
tracted from text, which are temporal relations and
causal relations. Causal relations are related to
temporal relations since there is a temporal con-
straint in causality, i.e. the cause must precede the
effect. Considering this presumption, and the as-
sumption that there are good enough systems to
extract temporal expressions and events, we define
two main problems that will be addressed in this
research work:
1. Given a text annotated with entities (temporal
expressions and events), how to automatically
extract temporal and causal relations between
them.
2. Given the temporal constraint of causality,
how to utilize the interaction between tempo-
ral relations and causal relations for building
an integrated event relation extraction system
for both types of relations.
4 Research Methodology
There are several aspects of the mentioned prob-
lems that will become our guidelines in continuing
our research in this topic. The following sections
will give a more detailed description of these as-
pects including the arising challenges, some pre-
liminary results to address the challenges and our
future research directions.
4.1 Temporal Relation Extraction
As previously mentioned, we consider the TimeML
annotation framework because it explicitly encodes
the temporal links between entities (events and tem-
poral expressions) in a text. In TimeML, each tem-
poral link has a temporal relation type assigned to
it. There are 14 types of temporal relations speci-
fied in TimeML version 1.2.1 (Saur?? et al, 2006),
11
which are defined based on Allen?s interval algebra
(Allen, 1983), as illustrated in Table 1.
a |???| a is BEFORE b
b |???| b is AFTER a
a |???| a is IBEFORE b
b |???| b is IAFTER a
a |??| a BEGINS b
b |????| b is BEGUN BY a
a |??| a ENDS b
b |????| b is ENDED BY a
a |??| a is DURING b
b |??????| b is DURING INV a
a |??????| a INCLUDES b
b |??| b IS INCLUDED in a
a |???|
a is SIMULTANEOUS with b
b |???|
a |???| b a is IDENTITY with b
Table 1: Temporal relations in TimeML annotation
Recalling the low performances of currently
available systems on the temporal relation extrac-
tion task, including the state-of-the-art systems ac-
cording to TempEval-3, it is still insufficient to use
the existing temporal relation extraction systems
to support real world applications, such as creat-
ing event timelines and temporally-based question
answering. Therefore, as the first step we take as
an objective finding ways to improve the current
state-of-the-art performance on temporal relation
extraction task.
The common approach towards temporal rela-
tion extraction is dividing the task into two sub-
tasks: identifying the pairs of entities having a tem-
poral link and determining the relation types. The
problem of identifying the entity pairs is usually
simplified. In TempEval-3, the possible pairs of
entities that can have a temporal link are defined as
(i) main events of consecutive sentences, (ii) pairs
of events in the same sentence, (iii) an event and a
time expression in the same sentence, and (iv) an
event and the document creation time (UzZaman
et al, 2013). The problem of determining the label
of a given temporal link is usually regarded as a
classification problem. Given an ordered pair of
entities (e
1
, e
2
) that could be either event-event,
event-timex or timex-timex pair, the classifier has
to assign a certain label representing the temporal
relation type.
We focus on the latter subtask of classifying
temporal relation types, assuming that the links
between events and time expressions are already
established. Several recent works have tried to ad-
dress this complex multi-class classification task
by using sophisticated features based on deep pars-
ing, semantic role labelling and discourse parsing
(D?Souza and Ng, 2013; Laokulrat et al, 2013). In
Mirza and Tonelli (2014) we have shown that a sim-
pler approach, based on lexico-syntactic features,
can achieve comparable results.
A classification model is trained for each cate-
gory of entity pair, i.e. event-event, event-timex
and timex-timex, as suggested in several previous
works (Mani et al, 2006; Chambers, 2013). How-
ever, because there are very few examples of timex-
timex pairs in the training corpus, it is not possible
to train a classifier for these particular pairs. More-
over, they only add up to 3.2% of the total number
of extracted entity pairs; therefore, we decided to
disregard these pairs.
We follow the guidelines and the dataset pro-
vided by the organizers of TempEval-3 so that we
can compare our system with other systems partici-
pating in the challenge. The TBAQ-cleaned corpus
is the training set provided for the task, consisting
of the TimeBank (Pustejovsky et al, 2003a) and
the AQUAINT corpora. It contains around 100K
words in total, with 11K words annotated as events
(UzZaman et al, 2013).
Simple Feature Set. We implement a number of
features including the commonly used ones (UzZa-
man et al, 2013), which take into account morpho-
syntactic information on events and time expres-
sions, their textual context and their attributes.
Other features rely on semantic information such
as typical event durations and explicit temporal
connective types. However, we avoid complex pro-
cessing of data. Such semantic information is based
on external lists of lexical items and on the output
of the addDiscourse tagger (Pitler and Nenkova,
2009). We build our classification models using the
Support Vector Machine (SVM) implementation
provided by YamCha
3
.
We perform feature engineering in order to se-
lect from our initial set of features only those that
improve the accuracy of the classifiers. This allows
us to select the best classification models for both
event-event pairs and event-timex pairs.
Inverse Relations and Closure. Motivated by the
finding of Mani et al (2006) that bootstrapping the
training data through a temporal closure method
results in quite significant improvements, we in-
vestigate the effect of enriching the training data
with inverse relations and closure-based inferred
3
http://chasen.org/?taku/software/yamcha/
12
relations.
However, we adopt a simpler approach to obtain
the closure graph of temporal relations, by applying
the transitive closure only within the same relation
type, e.g. e
1
BEFORE e
2
? e
2
BEFORE e3 ? e
1
BEFORE e3. It produces only a subset of the rela-
tions produced by the temporal closure (Verhagen,
2005; Gerevini et al, 1995). The problem of find-
ing the transitive closure of a directed acyclic graph
can be reduced to a boolean matrix multiplication
(Fischer and Meyer, 1971).
Training data event-event event-timex
TBAQ 48.28% 73.82%
TBAQ-I 47.77% 74.45%
TBAQ-IC 46.39% 74.45%
Table 2: Classifier accuracies with different
training data: TBAQ (TimeBank+AQUAINT),
TBAQ-I (TBAQ+inverse relations) and TBAQ-IC
(TBAQ+inverse relations+transitive closure).
Evaluation and Analysis. Our test data is the
newly annotated TempEval-3-platinum evaluation
corpus provided by TempEval-3 organizers, so that
we can compare our system with other systems
participating in the task. First, to investigate the
effect of enriching the training data with inverse
relations and transitive closure, we evaluate the sys-
tem performance trained with different datasets, as
shown in Table 2. A randomization test between
the best performing classifier and the others shows
that by extending the training data with inverse
relations and transitive closure, the improvement
are not significant. Applying inverse relations and
transitive closure extends the number of training in-
stances but makes the already skewed dataset more
imbalanced, thus it does not result in a significant
improvement.
We then train our classifiers for event-event pairs
and event-timex pairs by exploiting the best feature
combination and using the best reported dataset
for each classifier as the training data. The two
classifiers are part of our temporal classification
system called TRelPro.
Compared with the performances of other sys-
tems participating in TempEval-3 reported in UzZa-
man et al (2013), TRelPro is the best performing
system both in terms of precision and of recall.
The result of our system using simpler features
confirms the finding reported in UzZaman et al
(2013), that a system using basic morpho-syntactic
features is hard to beat with systems using more
complex semantic features, if not used properly.
System F1 Precision Recall
TRelPro 58.48% 58.80% 58.17%
UTTime-1, 4 56.45% 55.58% 57.35%
UTTime-3, 5 54.70% 53.85% 55.58%
UTTime-2 54.26% 53.20% 55.36%
NavyTime-1 46.83% 46.59% 47.07%
NavyTime-2 43.92% 43.65% 44.20%
JU-CSE 34.77% 35.07% 34.48%
Table 3: TempEval-3 evaluation on the classifica-
tion of temporal relation types
4.2 Causal Relation Extraction
Unlike the temporal order that has a clear defini-
tion, there is no consensus in the NLP community
on how to define causality. Causality is not a lin-
guistic notion, meaning that although language can
be used to express causality, causality exists as
a psychological tool for understanding the world
independently of language (van de Koot and Neele-
man, 2012). There have been several attempts in
the psychology field to model causality, including
the counterfactual model (Lewis, 1973), proba-
bilistic contrast model (Cheng and Novick, 1991;
Cheng and Novick, 1992) and the dynamics model
(Wolff and Song, 2003; Wolff et al, 2005; Wolff,
2007), which is based on Talmy?s force dynamic
account of causality (Talmy, 1985; Talmy, 1988).
In information extraction, modelling causality
is only the first step in order to have guidelines
to recognize causal relations in a text. In order
to have an automatic extraction system for causal
relations (particularly using a data-driven approach)
and most importantly to evaluate the performance
of the developed extraction system, it is important
that a language resource annotated with causality
is available.
Even though there are several corpora anno-
tated with causality, e.g. Penn Discourse Tree-
bank (PDTB) (Prasad et al, 2007) and PropBank
(Palmer et al, 2005),
4
we are not aware of any
standard benchmarking corpus for evaluating event
causality extraction, as it is available for temporal
relations in TimeML. This motivates us to create
a language resource annotated with both temporal
and causal relations in a unified annotation scheme,
for the main purpose of investigating the interac-
tion between both types of relations. It becomes
the objective of the second stage of our research, in
4
PDTB annotates causality between related clauses, while
PropBank annotates causality between a verb and its cause
clause.
13
addition to building an automatic extraction system
for event causality using the developed corpus.
In Mirza et al (2014) we have proposed annota-
tion guidelines for causality between events, based
on the TimeML definition of events, which consid-
ers all types of actions (punctual and durative) and
states as events. Parallel to the <TLINK> tag in
TimeML for temporal relations, we introduced the
<CLINK> tag to signify a causal link. We also
introduced the notion of causal signals through the
<C-SIGNAL> tag, parallel to the <SIGNAL> tag
in TimeML indicating temporal cues.
C-SIGNAL. C-SIGNAL is used to mark-up textual
elements signalling the presence of causal relations,
which include all causal uses of: prepositions (e.g.
because of, as a result of, due to), conjunctions
(e.g. because, since, so that), adverbial connectors
(e.g. so, therefore, thus) and clause-integrated ex-
pressions (e.g. the reason why, the result is, that is
why).
CLINK. A CLINK is a directional relation where
the causing event is the source (indicated with
S
in the examples) and the caused event is the target
(indicated with
T
). The annotation of CLINKs also
includes the c-signalID attribute, with the value of
the ID of C-SIGNAL marking the causal relation
(if available).
Wolff (2007) has shown that the dynamics model
covers three main types of causal concepts, i.e.
CAUSE, ENABLE and PREVENT. The model has
been tested by linking it with natural language,
Wolff and Song (2003) show that the three causal
concepts can be lexicalized as verbs : (i) CAUSE-
type verbs, e.g. cause, prompt, force; (ii) ENABLE-
type verbs, e.g. allow, enable, help; and (iii)
PREVENT-type verbs, e.g. block, prevent, restrain.
Its connection with natural language becomes the
main reason of basing our annotation guidelines
for causality on the dynamics model.
We limit the annotation of CLINKs to the pres-
ence of an explicit causal construction linking two
events, which can be one of the following cases:
1. Expressions containing affect verbs (affect,
influence, determine, change), e.g. Ogun ACN
crisis
S
influences the launch
T
of the All Pro-
gressive Congress.
2. Expressions containing link verbs (link, lead,
depend on), e.g. An earthquake
T
in North
America was linked to a tsunami
S
in Japan.
3. Basic construction of causative verbs, e.g.
The purchase
S
caused the creation
T
of the
current building.
4. Periphrastic construction of causative
verbs, e.g. The blast
S
caused the boat to
heel
T
violently, where the causative verb
(caused) takes an embedded verb (heel) ex-
pressing a particular result.
5. Expressions containing causative conjunc-
tions and prepositions, which are annotated
as C-SIGNALs.
Note that for causative verbs we consider sets of
verbs from all types of causal concepts including
CAUSE-type, ENABLE-type and PREVENT-type
verbs.
Manual Annotation of Event Causality. Having
the annotation guidelines, we are about to complete
the annotation of event causality. We have anno-
tated a subset of training corpus from TempEval-3
used in the temporal relation extraction, i.e. Time-
Bank. The agreement reached by two annotators on
a subset of 5 documents is 0.844 Dice?s coefficient
on C-SIGNALs (micro-average over markables)
and 0.73 on CLINKs.
After completing causality annotation, the next
step will be to build an automatic extraction system
for causal relations. We will consider to use a su-
pervised learning approach, as well as the similar
features employed for temporal relation classifica-
tion task, in addition to lexical information (e.g.
WordNet (Fellbaum, 1998), VerbOcean (Chklovski
and Pantel, 2004)) and the existing causal signals.
4.3 Integrated Event Relation Extraction
During the last stage of our research work, we will
investigate the interaction between temporal and
causal relations, given the temporal constraint of
causality. The ultimate goal is to build an integrated
event relation extraction system, that is capable of
automatically extracting both temporal and causal
relations from text.
Few works have investigated the interaction be-
tween these two types of relations. The corpus
analysis conducted by Bethard et al (2008) shows
that although it is expected that almost every causal
relation would have an underlying before relation,
in reality, 32% of causal relations in the corpus are
not accompanied by underlying before relations.
One of the possible causes is that the considered
event pairs are conjoined event pairs under the am-
biguous and conjunctive.
14
Consider the sentence ?The walls were
shaking
T
because of the earthquake
S
.? Looking
at the explicit causal mark because, there is a causal
relation between the events shaking and earthquake.
However, according to Allen?s interval algebra or
the TimeML annotation framework we cannot say
that event earthquake occurred BEFORE the event
shaking, because both events happen almost at
the same time (could be SIMULTANOUS), and in
both frameworks there is no overlap in BEFORE
relations. During our manual annotation process,
we encountered the case where the cause event
happens after the effect, as in ?Some analysts
questioned
T
how much of an impact the retirement
package will have, because few jobs will end
S
up being eliminated.? Further investigations are
needed to address this issue.
Rink et al (2010) makes use of manually anno-
tated temporal relation types as a feature to build
a classification model for causal relations between
events. This results in 57.9% of F1-Score, 15% im-
provement of performance in comparison with the
system without the additional feature of temporal
relations. The significant increase of performance
proves that the temporal relations between causal
events have a significant role in discovering causal
relations. On the other hand, a brief analysis into
our preliminary result on temporal relation extrac-
tion shows that there is a possibility to employ
causality to improve the temporal relation classifi-
cation of event-event pairs, specifically to reduce
the number of false positives and false negatives of
BEFORE and AFTER relations scored by the sys-
tem. Our hypothesis is that temporal and causal
relations can be of mutual benefit to the extraction
of each other.
Taking into account different classification
frameworks and possible configurations for the in-
tegrated system, for example, cascading the tempo-
ral and causal relation extraction systems, or one
system for both relation types in one pass, we will
explore the possibilities and evaluate their perfor-
mances. Furthermore, there is a possibility to ex-
ploit a global optimization algorithm, as explored
by Chambers and Jurafsky (2008) and Do et al
(2012), to improve the performance of a pairwise
classification system.
One possible classification algorithm under our
considerations, which can be used for extracting
both temporal and causal relations in one pass,
is General Conditional Random Fields (CRFs).
General CRFs allow us to train a classification
model with arbitrary graphical structure, e.g. a
two-dimensional CRF can be used to perform both
noun phrase chunking and PoS tagging at the same
time. And its skip-chain mechanism allows us to
create a chain of entity pairs, which may improve
the classification performance.
5 Conclusion
Event extraction has become one of the most in-
vestigated tasks of information extraction, since
it is the key to many applications in natural lan-
guage processing such as personalized news sys-
tems, question answering and document summa-
rization. The extraction of relations that hold be-
tween events is one of the subtasks within event ex-
traction gaining more attention in the recent years,
given the beneficial and promising applications.
We have presented a research plan covering the
topic of automatic extraction of two event relation
types, i.e. temporal and causal relations, from natu-
ral language texts. While there has been a clearly
defined framework for temporal relation extraction
task, namely TempEval-3, there is none for causal
relation extraction. Furthermore, since causality
has a temporal constraint, we are interested in in-
vestigating the interaction between temporal and
causal relations, in the context of events.
We propose a three-stage approach to cover this
research topic. The first stage includes improv-
ing the state-of-the-art performance on temporal
relation extraction. During the second stage we
propose an annotation scheme to create a corpus
for causal relations, based on the established anno-
tation framework for events and temporal relations,
namely TimeML. The created language resource
will then be used to build the automatic extraction
system for causal relations, and also to provide
the benchmarking evaluation corpus. Finally, the
last stage includes investigating the interaction be-
tween temporal and causal relations, in order to
build an integrated system for event relation ex-
traction, which is the ultimate goal of this research
work.
Acknowledgments
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project (ICT-
316404). We also thank Google for travel and
conference support for this paper.
15
References
James Allan, editor. 2002. Topic Detection and
Tracking: Event-based Information Organization.
Kluwer Academic Publishers, Norwell, MA, USA.
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?
843, November.
Steven Bethard, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a corpus
of temporal-causal structure. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco, May. European Language Resources Associ-
ation (ELRA).
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 698?706, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Patricia W. Cheng and Laura R. Novick. 1991. Causes
versus enabling conditions. Cognition, 40(1-2):83 ?
120.
Patricia W. Cheng and Laura R. Novick. 1992. Co-
variation in natural causal induction. Psychological
Review, 99(2):365?382.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained semantic
verb relations. In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 33?40,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Linguistic Data Consortium, 2005. ACE (Automatic
Content Extraction) English Annotation Guidelines
for Events.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
inference for event timeline construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ?12, pages 677?687, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 918?927.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Michael J. Fischer and Albert R. Meyer. 1971.
Boolean matrix multiplication and transitive closure.
In SWAT (FOCS), pages 129?131. IEEE Computer
Society.
Aldo Gangemi, Nicola Guarino, Claudio Masolo,
Alessandro Oltramari, and Luc Schneider. 2002.
Sweetening ontologies with dolce. In Proceedings
of the 13th International Conference on Knowledge
Engineering and Knowledge Management. Ontolo-
gies and the Semantic Web, EKAW ?02, pages 166?
181, London, UK, UK. Springer-Verlag.
Alfonso Gerevini, Lenhart Schubert, and Stephanie
Schaeffer. 1995. The temporal reasoning tools time-
graph i-ii. International Journal of Artificial Intelli-
gence Tools, 4(1-2):281?299.
Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Uttime: Tem-
poral relation classification using deep syntactic fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 88?92,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
David Lewis. 1973. Causation. The Journal of Philos-
ophy, 70(17):pp. 556?567.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics,
ACL-44, pages 753?760, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Paramita Mirza and Sara Tonelli. 2014. Classifying
temporal relations with simple features. In Proceed-
ings of the 14th Conference of the European Chapter
of the Association for Computational Linguistics.
Paramita Mirza, Rachele Sprugnoli, Sara Tonelli, and
Manuela Speranza. 2014. Annotating causality in
the tempeval-3 corpus. In Proceedings of the EACL-
2014 Workshop on Computational Approaches to
Causality in Language (CAtoCL).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71?106,
March.
Emily Pitler and Ani Nenkova. 2009. Using syn-
tax to disambiguate explicit discourse connectives
in text. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 13?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
16
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan
Lee, Aravind Joshi, Livio Robaldo, and Bonnie L
Webber. 2007. The penn discourse treebank 2.0 an-
notation manual. Technical report.
J. Pustejovsky, P. Hanks, R. Sauri, A. See,
R. Gaizauskas, A. Setzer, D. Radev, B. Sund-
heim, D. Day, L. Ferro, et al 2003a. The timebank
corpus. In Corpus Linguistics, volume 2003,
page 40.
James Pustejovsky, Jos?e Casta?no, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003b. Timeml: Robust specification of
event and temporal expressions in text. In Proceed-
ings of the Fifth International Workshop on Compu-
tational Semantics (IWCS-5).
Bryan Rink, Cosmin Adrian Bejan, and Sanda M.
Harabagiu. 2010. Learning textual graph patterns
to detect causal event relations. In FLAIRS Confer-
ence.
Roser Saur??, Jessica Littman, Robert Gaizauskas, An-
drea Setzer, and James Pustejovsky, 2006. TimeML
Annotation Guidelines, Version 1.2.1.
Ryan Shaw, Rapha?el Troncy, and Lynda Hardman.
2009. Lode: Linking open descriptions of events.
In Proceedings of the 4th Asian Conference on The
Semantic Web, ASWC ?09, pages 153?167, Berlin,
Heidelberg. Springer-Verlag.
Leonard Talmy. 1985. Force dynamics in lan-
guage and thought. Chicago Linguistic Society,
21:293337.
Leonard Talmy. 1988. Force dynamics in language
and cognition. Cognitive Science, 12(1):49?100.
Naushad UzZaman, Hector Llorens, James F. Allen,
Leon Derczynski, Marc Verhagen, and James Puste-
jovsky. 2012. Tempeval-3: Evaluating events,
time expressions, and temporal relations. CoRR,
abs/1206.5333.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Proceedings of the Seventh Interna-
tional Workshop on Semantic Evaluation, SemEval
?13, pages 1?9, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
H van de Koot and A Neeleman, 2012. The Theta Sys-
tem: Argument Structure at the Interface, chapter
The Linguistic Expression of Causation, pages 20 ?
51. Oxford University Press: Oxford.
Willem Robert van Hage, V?eronique Malais?e, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011.
Design and use of the simple event model (sem).
Journal of Web Semantics, 9(2):128?136.
Marc Verhagen. 2005. Temporal closure in an annota-
tion environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Phillip Wolff and Grace Song. 2003. Models of cau-
sation and the semantics of causal verbs. Cognitive
Psychology, 47(3):276 ? 332.
Phillip Wolff, Bianca Klettke, Tatyana Ventura, and
Grace Song, 2005. Categorization inside and out-
side the laboratory: Essays in honor of Douglas L.
Medin. APA decade of behavior series, chapter Ex-
pressing Causation in English and Other Languages,
pages 29?48. Washington, DC, US: American Psy-
chological Association, xx, 316 pp.
Phillip Wolff. 2007. Representing causation. Journal
of Experiment Psychology: General, 136:82?111.
17
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10?19,
Gothenburg, Sweden, April 26, 2014.
c?2014 Association for Computational Linguistics
Annotating causality in the TempEval-3 corpus
Paramita Mirza
FBK, Trento, Italy
University of Trento
paramita@fbk.eu
Rachele Sprugnoli
FBK, Trento, Italy
University of Trento
sprugnoli@fbk.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Manuela Speranza
FBK, Trento, Italy
manspera@fbk.eu
Abstract
While there is a wide consensus in the NLP
community over the modeling of temporal
relations between events, mainly based on
Allen?s temporal logic, the question on how
to annotate other types of event relations, in
particular causal ones, is still open. In this
work, we present some annotation guide-
lines to capture causality between event
pairs, partly inspired by TimeML. We then
implement a rule-based algorithm to auto-
matically identify explicit causal relations
in the TempEval-3 corpus. Based on this
annotation, we report some statistics on the
behavior of causal cues in text and perform
a preliminary investigation on the interac-
tion between causal and temporal relations.
1 Introduction
The annotation of events and event relations in
natural language texts has gained in recent years in-
creasing attention, especially thanks to the develop-
ment of TimeML annotation scheme (Pustejovsky
et al., 2003), the release of TimeBank (Pustejovsky
et al., 2006) and the organization of several eval-
uation campaigns devoted to automatic temporal
processing (Verhagen et al., 2007; Verhagen et al.,
2010; UzZaman et al., 2013).
However, while there is a wide consensus in the
NLP community over the modeling of temporal
relations between events, mainly based on Allen?s
interval algebra (Allen, 1983), the question on how
to model other types of event relations is still open.
In particular, linguistic annotation of causal rela-
tions, which have been widely investigated from
a philosophical and logical point of view, are still
under debate. This leads, in turn, to the lack of
a standard benchmark to evaluate causal relation
extraction systems, making it difficult to compare
systems performances, and to identify the state-of-
the-art approach for this particular task.
Although several resources exist in which causal-
ity has been annotated, they cover only few aspects
of causality and do not model it in a global way,
comparable to what as been proposed for temporal
relations in TimeML. See for instance the annota-
tion of causal arguments in PropBank (Bonial et al.,
2010) and of causal discourse relations in the Penn
Discourse Treebank (The PDTB Research Group,
2008).
In this work, we propose annotation guidelines
for causality inspired by TimeML, trying to take ad-
vantage of the clear definition of events, signals and
relations proposed by Pustejovsky et al. (2003). Be-
sides, as a preliminary investigation of causality in
the TempEval-3 corpus, we perform an automatic
analysis of causal signals and relations observed in
the corpus. This work is a first step towards the an-
notation of the TempEval-3 corpus with causality,
with the final goal of investigating the strict connec-
tion between temporal and causal relations. In fact,
there is a temporal constraint in causality, i.e. the
cause must occur BEFORE the effect. We believe
that investigating this precondition on a corpus ba-
sis can contribute to improving the performance of
temporal and causal relation extraction systems.
2 Existing resources on Causality
Several attempts have been made to annotate causal
relations in texts. A common approach is to look
for specific cue phrases like because or since or to
look for verbs that contain a cause as part of their
meaning, such as break (cause to be broken) or
kill (cause to die) (Khoo et al., 2000; Sakaji et al.,
2008; Girju et al., 2007). In PropBank (Bonial et
al., 2010), causal relations are annotated in the form
of predicate-argument relations, where ARGM-CAU
is used to annotate ?the reason for an action?, for
example: ?They [
PREDICATE
moved] to London
[
ARGM-CAU
because of the baby].?
Another scheme annotates causal relations be-
tween discourse arguments, in the framework of
10
the Penn Discourse Treebank (PDTB). As opposed
to PropBank, this kind of relations holds only be-
tween clauses and do not involve predicates and
their arguments. In PDTB, the Cause relation type
is classified as a subtype of CONTINGENCY.
Causal relations have also been annotated as re-
lations between events in a restricted set of linguis-
tic constructions (Bethard et al., 2008), between
clauses in text from novels (Grivaz, 2010), or in
noun-noun compounds (Girju et al., 2007).
Several types of annotation guidelines for causal
relations have been presented, with varying de-
grees of reliability. One of the simpler approaches
asks annotators to check whether the sentence they
are reading can be paraphrased using a connective
phrase such as and as a result or and as a conse-
quence (Bethard et al., 2008).
Another approach to annotate causal relations
tries to combine linguistic tests with semantic rea-
soning tests. In Grivaz (2010), the linguistic para-
phrasing suggested by Bethard et al. (2008) is
augmented with rules that take into account other
semantic constraints, for instance if the potential
cause occurs before or after the potential effect.
3 Annotation of causal information
As part of a wider annotation effort aimed to an-
notate texts at the semantic level (Tonelli et al.,
2014), we propose guidelines for the annotation of
causal information. In particular, we define causal
relations between events based on the TimeML def-
inition of events (ISO TimeML Working Group,
2008), as including all types of actions (punctual
and durative) and states. Syntactically, events can
be realized by a wide range of linguistic expres-
sions such as verbs, nouns (which can realize even-
tualities in different ways, for example through a
nominalization process of a verb or by possessing
an eventive meaning), and prepositional construc-
tions.
Following TimeML, our annotation of events in-
volved in causal relations includes the polarity
attribute (see Section 3.3); in addition to this, we
have defined the factuality and certainty
event attributes, which are useful to infer informa-
tion about actual causality between events.
Parallel to the TimeML tag <SIGNAL> as an
indicator for temporal links, we have also intro-
duced the notion of causal signals through the use
of the <C-SIGNAL> tag.
3.1 C-SIGNAL
The <C-SIGNAL> tag is used to mark-up textual
elements that indicate the presence of a causal rela-
tion (i.e. a CLINK, see 3.2). Such elements include
all causal uses of:
? prepositions, e.g. because of, on account of,
as a result of, in response to, due to, from, by;
? conjunctions, e.g. because, since, so that,
hence, thereby;
? adverbial connectors, e.g. as a result, so,
therefore, thus;
? clause-integrated expressions, e.g. the result
is, the reason why, that?s why.
The extent of C-SIGNALs corresponds to the
whole expression, so multi-token extensions are
allowed.
3.2 CLINK (Causal Relations)
For the annotation of causal relations between
events, we use the <CLINK> tag, a directional
one-to-one relation where the causing event is the
source (the first argument, indicated as
S
in the
examples) and the caused event is the target (the
second argument, indicated as
T
). The annotation
of CLINKs includes the c-signalID attribute,
whose value is the ID of the C-SIGNAL indicating
the causal relation (if available).
A seminal research in cognitive psychology
based on the force dynamics theory (Talmy, 1988)
has shown that causation covers three main kinds of
causal concepts (Wolff, 2007), which are CAUSE,
ENABLE, and PREVENT, and that these causal
concepts are lexicalized as verbs (Wolff and Song,
2003): (i) CAUSE-type verbs: bribe, cause, com-
pel, convince, drive, have, impel, incite, induce,
influence, inspire, lead, move, persuade, prompt,
push, force, get, make, rouse, send, set, spur, start,
stimulate; (ii) ENABLE-type verbs: aid, allow, en-
able, help, leave, let, permit; (iii) PREVENT-type
verbs: bar, block, constrain, deter, discourage, dis-
suade, hamper, hinder, hold, impede, keep, prevent,
protect, restrain, restrict, save, stop. CAUSE, EN-
ABLE, and PREVENT categories of causation and
the corresponding verbs are taken into account in
our guidelines.
As causal relations are often not overtly ex-
pressed in text (Wolff et al., 2005), we restrict the
annotation of CLINKs to the presence of an explicit
11
causal construction linking two events in the same
sentence
1
, as detailed below:
? Basic constructions for CAUSE, ENABLE
and PREVENT categories of causation as
shown in the following examples:
The purchase
S
caused the creation
T
of the cur-
rent building
The purchase
S
enabled the diversification
T
of
their business
The purchase
S
prevented a future transfer
T
? Expressions containing affect verbs, such as
affect, influence, determine, and change. They
can be usually rephrased using cause, enable,
or prevent:
Ogun ACN crisis
S
affects the launch
T
of the
All Progressives Congress? Ogun ACN cri-
sis causes/enables/prevents the launch of the
All Progressives Congress
? Expressions containing link verbs, such as
link, lead, and depend on. They can usually
be replaced only with cause and enable:
An earthquake
T
in North America was linked
to a tsunami
S
in Japan ? An earthquake
in North America was caused/enabled by a
tsunami in Japan
*An earthquake in North America was pre-
vented by a tsunami in Japan
? Periphrastic causatives are generally com-
posed of a verb that takes an embedded clause
or predicate as a complement; for example,
in the sentence The blast
S
caused the boat
to heel
T
violently, the verb (i.e. caused) ex-
presses the notion of CAUSE while the em-
bedded verb (i.e. heel) expresses a particular
result. Note that the notion of CAUSE can
be expressed by verbs belonging to the three
categories previously mentioned (which are
CAUSE-type verbs, ENABLE-type verbs and
PREVENT-type verbs).
? Expressions containing causative conjunc-
tions and prepositions as listed in Section
3.1. Causative conjunctions and prepositions
are annotated as C-SIGNALs and their ID is
1
A typical example of implicit causal construction is rep-
resented by lexical causatives; for example, kill has the em-
bedded meaning of causing someone to die (Huang, 2012). In
the present guidelines, these cases are not included.
to be reported in the c-signalID attribute
of the CLINK.
2
In some contexts, the coordinating conjunction
and can imply causation; given the ambiguity of
this construction and the fact that it is not an ex-
plicit causal construction, however, we do not an-
notate CLINKs between two events connected by
and. Similarly, the temporal conjunctions after and
when can also implicitly assert a causal relation
but should not be annotated as C-SIGNALs and no
CLINKs are to be created (temporal relations have
to be created instead).
3.3 Polarity, factuality and certainty
The polarity attribute, present both in TimeML
and in our guidelines, captures the grammatical
category that distinguishes affirmative and negative
events. Its values are NEG for events which are
negated (for instance, the event cause in Serotonin
deficiency
S
may not cause depression
T
) and POS
otherwise.
The annotation of factuality that we added
to our guidelines is based on the situation to which
an event refers. FACTUAL is used for facts, i.e. sit-
uations that have happened, COUNTERFACTUAL
is used for counterfacts, i.e. situations that have no
real counterpart as they did not take place, NON-
FACTUAL is used for possibilities, i.e. speculative
situations, such as future events, events for which
it is not possible to determine whether they have
happened, and general statements.
The certainty attribute expresses the binary
distinction between certain (value CERTAIN) and
uncertain (value UNCERTAIN) events. Uncer-
tain events are typically marked in the text by the
presence of modals or modal adverbs (e.g. per-
haps, maybe) indicating possibility. In the sentence
Drinking
S
may cause memory loss
T
, the causal con-
nector cause is an example of a NON-FACTUAL
and UNCERTAIN event.
In the annotation algorithm presented in the fol-
lowing section, only the polarity attribute is
taken into account, given that information about
factuality and certainty of events is not annotated
in the TempEval-3 corpus. In particular, at the
time of the writing the algorithm considers only the
polarity of causal verbal connectors, because this
information is necessary to extract causal chains
2
The absence of a value for the c-signalID attribute
means that the causal relation is encoded by a verb.
12
between events in a text. However, adding informa-
tion on the polarity of the single events involved in
the relations would make possible also the identifi-
cation of positive and negative causes and effects.
4 Automatic annotation of explicit
causality between events
In order to verify the soundness of our annotation
framework for event causality, we implement some
simple rules based on the categories and linguistic
cues listed in Section 3. Our goal is two-fold: first,
we want to check how accurate rule-based identifi-
cation of (explicit) event causality can be. Second,
we want to have an estimate of how frequently
causality can be explicitly found in text.
The dataset we annotate has been released for
the TempEval-3 shared task
3
on temporal and event
processing. The TBAQ-cleaned corpus is the train-
ing set provided for the task, consisting of the Time-
Bank (Pustejovsky et al., 2006) and the AQUAINT
corpora. It contains around 100K words in total,
with 11K words annotated as events (UzZaman et
al., 2013). We choose this corpus because gold
events are already provided, and because it allows
us to perform further analyses on the interaction
between temporal and causal relations.
Our automatic annotation pipeline takes as in-
put the TBAQ-cleaned corpus with gold annotated
events and tries to automatically recognize whether
there is a causal relation holding between them.
The annotation algorithm performs the following
steps in sequence:
1. The TBAQ-cleaned corpus is PoS-tagged and
parsed using the Stanford dependency parser
(de Marneffe and Manning, 2008).
2. The corpus is further analyzed with the ad-
dDiscourse tagger (Pitler and Nenkova, 2009),
which automatically identifies explicit dis-
course connectives and their sense, i.e. EX-
PANSION, CONTINGENCY, COMPARISON
and TEMPORAL. This is used to disambiguate
causal connectives (e.g. we consider only the
occurrences of since when it is a causal con-
nective, meaning that it falls into CONTIN-
GENCY class instead of TEMPORAL).
3. Given the list of affect, link, causative verbs
(basic and periphrastic constructions) and
causal signals listed in Sections 3.1 and 3.2,
3
http://www.cs.york.ac.uk/semeval-2013/task1/
the algorithm looks for specific dependency
constructions where the causal verb or signal
is connected to two events, as annotated in the
TBAQ-cleaned corpus.
4. If such dependencies are found, a CLINK is
automatically set between the two events iden-
tifying the source (
S
) and the target (
T
) of the
relation.
5. When a causal connector corresponds to an
event, the algorithm uses the polarity of the
event to assign a polarity to the causal link.
Specific approaches to detect when ambiguous
connectors have a causal meaning are implemented,
as in the case of from and by, where the algorithm
looks for specific structures. For instance, in ?The
building was damaged
T
by the earthquake
S
?, by is
governed by a passive verb annotated as event.
Also the preposition due to is ambiguous as
shown in the following sentences where it acts as a
causal connector only in b):
a) It had been due to expire Friday evening.
b) It cut
T
the dividend due to its third-quarter loss
S
of $992,000.
The algorithm performs the disambiguation by
checking the dependency structures: in sentence a)
there is only one dependency relation xcomp(due,
expire), while in sentence b) the dependency rela-
tions are xcomp(cut, due) and prep to(due, loss).
Besides, both cut and loss are annotated as events.
We are aware that this type of automatic anno-
tation may be prone to errors because it takes into
account only a limited list of causal connectors.
Besides, it only partially accounts for possible am-
biguities of causal cues and may suffer from pars-
ing errors. However, this allows us to make some
preliminary remarks on the amount of causal in-
formation found in the TempEval-3 corpus. Some
statistics are reported in the following subsection.
4.1 Statistics of Automatic Annotation
Basic construction. In Table 1 we report some
statistics on the non-periphrastic structures
identified starting from verbs expressing the three
categories of causation. Note that for the verbs
have, start, hold and keep, even though they
connect two events, we cannot say that there
is always a causal relation between them, as
exemplified in the following sentence taken from
the corpus:
a) Gen. Schwarzkopf secretly picked
S
Saturday
13
night as the optimal time to start the offensive
T
.
b) On Tuesday, the National Abortion and
Reproductive Rights Action League plans
S
to hold
a news conference
T
to screen a TV advertisement.
Types Verbs CLINK
CAUSE
have 1
start 2
cause 1
compel 1
PREVENT
hold 1
keep 3
block 7
prevent 1
ENABLE - -
Total 17
Table 1: Statistics of CLINKs with basic construc-
tion
Affect verbs. The algorithm does not annotate
any causal relation containing affect verbs mostly
because the majority of the 36 affect verb occur-
rences found in the corpus connect two elements
that are not events, as in ?These big stocks greatly
influence the Nasdaq Composite Index.?
Link verbs. In total, we found 50 occurrences of
link verbs in the corpus, but the algorithm identifies
only 4 causal links. Similar to affect verbs, this is
mainly due to the fact that two events are not found
to be involved in the relation. For instance, the
system associated only one CLINK to link (out
of 12 occurrences of the verb) and no CLINKs
to depend (which occurs 3 times). Most of the
CLINKs identified are signaled by the verb lead;
for example, ?Pol Pot is considered responsible for
the radical policies
S
that led to the deaths
T
of as
many as 1.7 million Cambodians.?
Periphrastic causative verbs. Overall, there are
around 1K potential occurrences of periphrastic
causative verbs in the corpus. However, the algo-
rithm identifies only around 14% of them as part
of a periphrastic construction, as shown in Table 2.
This is because some verbs are often used in non-
periphrastic structures, e.g. make, have, get, keep
and hold. Among the 144 cases of periphrastic con-
structions, 41 causal links are found by our rules.
In Table 2, for each verb type, we report the list
of verbs that appear in periphrastic constructions
in the corpus, specifying the number of CLINKs
identified by the system for each of them.
Some other CAUSE-type (move, push, drive, in-
fluence, compel, spur), PREVENT-type (hold, save,
impede, deter, discourage, dissuade, restrict) and
ENABLE-type (aid) verbs occur in the corpus but
are not involved in periphrastic structures. Some
others do not appear in the corpus at all (bribe, im-
pel, incite, induce, inspire, rouse, stimulate, hinder,
restrain).
Types Verbs Periphr. CLINK All
CAUSE
have 34 0 239
make 6 2 125
get 1 0 50
lead 2 1 38
send 5 1 34
set 2 0 23
start 1 0 22
force 2 1 15
cause 3 2 12
prompt 3 2 6
persuade 2 1 3
convince 1 1 2
PREVENT
keep 1 1 58
stop 3 0 24
block 2 2 21
protect 2 1 15
prevent 6 2 12
hamper 1 0 2
bar 1 0 1
constrain 1 0 1
ENABLE
help 31 13 45
leave 2 2 45
allow 22 3 39
permit 2 1 6
enable 4 2 5
let 4 3 5
Total 144 41 848
Table 2: Statistics of periphrastic causative verbs
Causal signals. Similar to periphrastic causative
verbs, out of around 1.2K potential causal connec-
tors found in the corpus, only 194 are automatically
recognized as actual causal signals after disam-
biguation, as detailed in Table 3. Based on these
identified causal signals, the algorithm derives 111
CLINKs.
Even though the addDiscourse tool labels 11
occurrences of the adverbial connector so as having
a causal meaning, our algorithm does not annotate
any CLINKs for such connector. In most cases, it
is because it acts as an inter-sentential connector,
while we limit the annotation of CLINKs only to
events occurring within the same sentence.
CLINKs polarity. Table 4 shows the distribution
of the positive and negative polarity of the detected
CLINKs.
Only two cases of negated CLINKs are automat-
ically identified in the corpus. One example is the
following: ?Director of the U.S. Federal Bureau of
14
Types C-SIGNALs Causal CLINK All
prep.
because of 32 11 32
on account of 0 0 0
as a result of 13 9 13
in response to 7 1 7
due to 2 1 6
from 2 2 500
by 23 24 465
conj.
because 58 37 58
since 26 19 72
so that 5 4 5
adverbial
as a result 3 0 3
so 11 0 69
therefore 4 0 4
thus 6 2 6
hence 0 0 0
thereby 1 0 1
consequently 1 1 1
clausal
the result is 0 0 0
the reason why 0 0 0
that is why 0 0 0
Total 194 111 1242
Table 3: Statistics of causal signals in CLINKs
Investigation (FBI) Louis Freeh said here Friday
that U.S. air raid
T
on Afghanistan and Sudan is
not directly linked with the probe
S
into the August
7 bombings in east Africa.?
Connector types POS NEG
Basic
CAUSE 5 0
PREVENT 12 0
ENABLE - -
Affect verbs - -
Link verbs 3 1
Periphrastic
CAUSE 10 1
PREVENT 6 0
ENABLE 24 0
Total 60 2
Table 4: Statistics of CLINKs? polarity
CLINKs vs TLINKs. In total, the algorithm iden-
tifies 173 CLINKs in the TBAQ-cleaned corpus,
while the total number of TLINKs between pairs of
events is around 5.2K. For each detected CLINK
between an event pair, we identify the underlying
temporal relations (TLINKs) if any. We found that
from the total of CLINKs extracted, around 33%
of them have an underlying TLINK, as detailed in
Table 5. Most of them are CLINKs signaled by
causal signals.
For causative verbs, the BEFORE relation is the
only underlying temporal relation type, with the
exception of one SIMULTANEOUS relation.
As for C-SIGNALs, the distribution of temporal
relation types is less homogeneous, as shown in Ta-
ble 6. In most of the cases, the underlying temporal
relation is BEFORE. In few cases, CLINKs sig-
Connector types CLINK TLINK
Basic
CAUSE 5 2
PREVENT 12 0
ENABLE - -
Affect verbs - -
Link verbs 4 1
Periphrastic
CAUSE 11 1
PREVENT 6 0
ENABLE 24 0
C-SIGNALs 111 54
Total 173 58
Table 5: Statistics of CLINKs? overlapping with
TLINKs
naled by the connector because overlap with an AF-
TER relation, as in ?But some analysts questioned
T
how much of an impact the retirement package will
have, because few jobs will end
S
up being elimi-
nated.?
In some cases, CLINKs signaled by the con-
nector since match with a BEGINS relation. This
shows that since expresses merely a temporal and
not a causal link. As it has been discussed before,
the connector since is highly ambiguous and the
CLINK has been wrongly assigned because of a
disambiguation mistake of the addDiscourse tool.
5 Evaluation
We perform two types of evaluation. The first is
a qualitative one, and is carried out by manually
inspecting the 173 CLINKs that have been auto-
matically annotated. The second is a quantitative
evaluation, and is performed by comparing the au-
tomatic annotated data with a gold standard corpus
of 100 documents taken from TimeBank.
5.1 Qualitative Evaluation
The automatically annotated CLINKs have been
manually checked in order to measure the precision
of the adopted procedure. Out of 173 annotated
CLINKs, 105 were correctly identified obtaining a
precision of 0.61.
Details on precision calculated on the different
types of categories and linguistic cues defined in
Section 3.2 are provided in Table 7. Statistics show
that performances vary widely depending on the
category and linguistic cue taken into consideration.
In particular, relations expressing causation of PRE-
VENT type prove to be extremely difficult to be
correctly detected with a rule-based approach: the
algorithm precision is 0.25 for basic constructions
and 0.17 for periphrastic constructions.
During the manual evaluation, two main types
15
C-SIGNALs BEFORE AFTER IS INCLUDED BEGINS others
because of 5 - - - -
as a result of 2 - - - -
in response to 1 - - - -
due to 1 - - - -
by 11 - 1 2 3
because 14 2 1 - 1
since 4 1 - 3 -
so that 1 - - - -
thus 1 - - - -
Total 40 3 2 5 4
Table 6: Statistics of CLINKs triggered by C-SIGNALs overlapping with TLINKs
Connector types Extracted Correct P
Basic
CAUSE 5 3 0.60
PREVENT 12 3 0.25
ENABLE 0 n.a. n.a.
Affect Verbs 0 n.a. n.a.
Link Verbs 4 3 0.75
Periphrastic
CAUSE 11 8 0.73
PREVENT 6 1 0.17
ENABLE 24 17 0.71
C-SIGNALs 111 70 0.63
Total 173 105 0.61
Table 7: Precision of automatically annotated
CLINKs
of mistakes have been observed: the wrong iden-
tification of events involved in CLINKs and the
annotation of sentences that do not contain causal
relations.
The assignment of a wrong source or a wrong
target to a CLINK is primarily caused by the de-
pendency parser output that tends to establish a
connection between a causal verb or signal and the
closest previous verb. For example, in the sentence
?StatesWest Airlines said it withdrew
T
its offer to
acquire Mesa Airlines because the Farmington car-
rier did not respond
S
to its offer?, the CLINK is
annotated between respond and acquire instead of
between respond and withdrew. On the other hand,
dependency structure is very effective in identify-
ing cases where one event is the consequence or
the cause of multiple events, as in ?The president
offered to offset
T
Jordan?s costs because 40% of
its exports go
S
to Iraq and 90% of its oil comes
S
from there.? In this case, the algorithm annotates a
causal link between go and offset, and also between
comes and offset.
The annotation of CLINKs in sentences not con-
taining causal relations is strongly related to the
ambiguous nature of many verbs, prepositions and
conjunctions, which encode a causal meaning or
express a causal relation only in some specific
contexts. For instance, many mistakes are due to
the erroneous disambiguation of the conjunction
since. According to the addDiscourse tool, since is
a causal connector in around one third of the cases,
as in ?For now, though, that would be a theoretical
advantage since the authorities have admitted they
have no idea where Kopp is.? However, there are
many cases where the outcome of the tool is not
perfect, as in ?Since then, 427 fugitives have been
taken into custody or located, 133 of them as a
result of citizen assistance, the FBI said?, where
since acts as a temporal conjunction.
5.2 Quantitative Evaluation
In order to perform also a quantitative evaluation of
our automatic annotation, we manually annotated
100 documents taken from the TimeBank corpus
according to the annotation guidelines discussed
before. We then used this data set as a gold stan-
dard.
The agreement reached by two annotators on a
subset of 5 documents is 0.844 Dice?s coefficient
on C-SIGNALS (micro-average over markables)
and of 0.73 on CLINKS.
We found that there are several cases where the
algorithm failed to recognize causal links due to
events that were originally not annotated in Time-
Bank. Therefore, as we proceed with the manual
annotation, we also annotated missing events that
are involved in causal relations. Table 8 shows that,
in creating the gold standard, we annotated 61 new
events. As a result, we have around 52% increase
in the number of CLINKs. Nevertheless, explicit
causal relations between events are by far less fre-
quent than temporal ones, with an average of 1.4
relations per document.
If we compare the coverage of automatic anno-
tation with the gold standard data (without newly
added events, to be fair), we observe that automatic
annotation covers around 76% of C-SIGNALs and
only around 55% of CLINKs. This is due to the
limitation of the algorithm that only considers a
16
Annotation EVENT C-SIGNAL CLINK
manual 3933 78 144
manual-w/o new events 3872 78 95
automatic 3872 59 52
Table 8: Statistics of causality annotation in manual
versus automatic annotation
precision recall F1-score
C-SIGNAL 0.64 0.49 0.55
CLINK 0.42 0.23 0.30
Table 9: Automatic annotation performance
small list of causal connectors. Some examples of
manually annotated causal signals that are not in
the list used by the algorithm include due mostly
to, thanks in part to and in punishment for.
Finally, we evaluate the performance of the algo-
rithm for automatic annotation (shown in Table 9)
by computing precision, recall and F1 on gold stan-
dard data without newly added events. We observe
that our rule-based approach is too rigid to capture
the causal information present in the data. In partic-
ular, it suffers from low recall as regards CLINKs.
We believe that this issue may be alleviated by
adopting a supervised approach, where the list of
verbs and causal signals would be included in a
larger feature set, considering among others the
events? position, their PoS tags, the dependency
path between the two events, etc.
6 Conclusions
In this paper, we presented our guidelines for an-
notating causality between events. We further tried
to automatically identify in TempEval-3 corpus the
types of causal relations described in the guide-
lines by implementing some simple rules based on
causal cues and dependency structures.
In a manual revision of the annotated causal
links, we observe that the algorithm obtains a pre-
cision of 0.61, with some issues related to the class
of PREVENT verbs. Some mistakes are introduced
by the tools used for parsing and for disambiguat-
ing causal signals, which in turn impact on our
annotation algorithm. Another issue, more related
to recall, is that in the TBAQ-cleaned corpus not all
events are annotated, because it focuses originally
on events involved in temporal relations. There-
fore, the number of causal relations identified auto-
matically would be higher if we did not take into
account this constraint.
From the statistics presented in Section 4.1, we
can observe that widely used verbs such as have or
keep express causality relations only in few cases.
The same holds for affect verbs, which are never
found in the corpus with a causal meaning, and for
link verbs. This shows that the main sense of causal
verbs usually reported in the literature is usually
the non-causal one.
Recognizing CLINKs based on causal signals is
more straightforward, probably because very fre-
quent ones such as because of and as a result are
not ambiguous. Others, such as by, can be identi-
fied based on specific syntactic constructions.
As for the polarity of CLINKs, which is a very
important feature to discriminate between actual
and negated causal relations, this phenomenon is
not very frequent (only 2 cases) and can be easily
identified through dependency relations.
We chose to automatically annotate TBAQ-
cleaned corpus because one of our goals was to
investigate how TLINKs and CLINKs interact.
However, this preliminary study shows that there
are only few overlaps between the two relations,
again with C-SIGNALs being more informative
than causal verbs. This may be biased by the fact
that, according to our annotation guidelines, only
explicit causal relations are annotated. Introducing
also the implicit cases would probably increase the
overlap between TLINKs and CLINKs, because
annotator would be allowed to capture the tempo-
ral constrains existing in causal relations even if
the are not overtly expressed.
In the near future, we will complete the manual
annotation of TempEval-3 corpus with causal in-
formation in order to have enough data for training
a supervised system, in which we will incorpo-
rate the lessons learnt with this first analysis. We
will also investigate the integration of the proposed
guidelines into the Grounded Annotation Format
(Fokkens et al., 2013), a formal framework for cap-
turing semantic information related to events and
participants at a conceptual level.
Acknowledgments
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project (ICT-
316404).
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26(11):832?
843, November.
17
Steven Bethard, William Corvey, Sara Klingenstein,
and James H. Martin. 2008. Building a corpus of
temporal-causal structure. In European Language
Resources Association (ELRA), editor, Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08), Marrakech, Morocco, may.
Claire Bonial, Olga Babko-Malaya, Jinho D.
Choi, Jena Hwang, and Martha Palmer.
2010. Propbank annotation guidelines, De-
cember. http://www.ldc.upenn.edu/
Catalog/docs/LDC2011T03/propbank/
english-propbank.pdf.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Association for Com-
putational Linguistics.
Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, Luciano Ser-
afini, Rachele Sprugnoli, and Jesper Hoeksema.
2013. GAF: A Grounded Annotation Framework
for Events. In Workshop on Events: Definition, De-
tection, Coreference, and Representation, pages 11?
20, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic
relations between nominals. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 13?18, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
C?ecile Grivaz. 2010. Human Judgements on Causation
in French Texts. In Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may. Euro-
pean Language Resources Association (ELRA).
Li-szu Agnes Huang. 2012. The Effectiveness of a
Corpus-based Instruction in Deepening EFL Learn-
ers? Knowledge of Periphrastic Causatives. TESOL
Journal, 6:83?108.
ISO TimeML Working Group. 2008. ISO TC37 draft
international standard DIS 24617-1, August 14.
http://semantic-annotation.uvt.nl/
ISO-TimeML-08-13-2008-vankiyong.
pdf.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medi-
cal database using graphical patterns. In In Proceed-
ings of 38th Annual Meeting of the ACL, Hong Kong,
2000, pages 336?343.
Emily Pitler and Ani Nenkova. 2009. Using syn-
tax to disambiguate explicit discourse connectives
in text. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 13?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
James Pustejovsky, J. Castano, R. Ingria, Roser Saur??,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003. TimeML: Robust specification of event and
temporal expressions in text. In Proceedings of the
Fifth International Workshop on Computational Se-
mantics.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006. Timebank 1.2 documenta-
tion. Technical report, Brandeis University, April.
Hiroki Sakaji, Satoshi Sekine, and Shigeru Masuyama.
2008. Extracting causal knowledge using clue
phrases and syntactic patterns. In Proceedings of the
7th International Conference on Practical Aspects
of Knowledge Management, PAKM ?08, pages 111?
122, Berlin, Heidelberg. Springer-Verlag.
Leonard Talmy. 1988. Force dynamics in language
and cognition. Cognitive science, 12(1):49?100.
The PDTB Research Group. 2008. The PDTB 2.0. An-
notation Manual. Technical Report IRCS-08-01, In-
stitute for Research in Cognitive Science, University
of Pennsylvania.
Sara Tonelli, Rachele Sprugnoli, and Manuela Sper-
anza. 2014. NewsReader Guidelines for Annotation
at Document Level, Extension of Deliverable
D3.1. Technical Report NWR-2014-2, Fondazione
Bruno Kessler. https://docs.google.
com/viewer?url=http%3A%2F%2Fwww.
newsreader-project.eu%2Ffiles%
2F2013%2F01%2FNWR-2014-2.pdf.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating events, time expressions, and temporal
relations. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Phillip Wolff and Grace Song. 2003. Models of cau-
sation and the semantics of causal verbs. Cognitive
Psychology, 47(3):276?332.
18
Phillip Wolff, Bianca Klettke, Tatyana Ventura, and
Grace Song. 2005. Expressing causation in english
and other languages. Categorization inside and out-
side the laboratory: Essays in honor of Douglas L.
Medin, pages 29?48.
Phillip Wolff. 2007. Representing causation. Journal
of experimental psychology: General, 136(1):82.
19
