Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 628?635, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
WBI-DDI: Drug-Drug Interaction Extraction using Majority Voting
Philippe Thomas Mariana Neves Tim Rockta?schel Ulf Leser
Humboldt-Universita?t zu Berlin
Knowledge Management in Bioinformatics
Unter den Linden 6
Berlin, 10099, Germany
{thomas,neves,trocktae,leser}@informatik.hu-berlin.de
Abstract
This work describes the participation of the
WBI-DDI team on the SemEval 2013 ? Task
9.2 DDI extraction challenge. The task con-
sisted of extracting interactions between pairs
of drugs from two collections of documents
(DrugBank and MEDLINE) and their clas-
sification into four subtypes: advise, effect,
mechanism, and int. We developed a two-step
approach in which pairs are initially extracted
using ensembles of up to five different clas-
sifiers and then relabeled to one of the four
categories. Our approach achieved the sec-
ond rank in the DDI competition. For interac-
tion detection we achieved F1 measures rang-
ing from 73 % to almost 76 % depending on
the run. These results are on par or even higher
than the performance estimation on the train-
ing dataset. When considering the four inter-
action subtypes we achieved an F1 measure of
60.9 %.
1 Introduction
A drug-drug interaction (DDI) can be described as
interplay between drugs taken during joint adminis-
tration. DDIs usually lead to an increase or decrease
in drug effects when compared to isolated treatment.
For instance, sildenafil (Viagra) in combination with
nitrates can cause a potentially live-threatening de-
crease in blood pressure (Cheitlin et al, 1999). It is
therefore crucial to consider potential DDI effects
when co-administering drugs to patients. As the
level of medication generally is raising all over the
world, the potential risk of unwanted side effects,
such as DDIs, is constantly increasing (Haider et al,
2007).
Only a fraction of knowledge about DDIs is
contained in specialized databases such as Drug-
Bank (Knox et al, 2011). These structured knowl-
edge bases are often the primary resource of infor-
mation for researchers. However, the majority of
new DDI findings are still initially reported in scien-
tific publications, which results in the situation that
structured knowledge bases lag behind recently pub-
lished research results. Thus, there is an urgent need
for researchers and database curators to cope with
the fast growth of biomedical literature (Hunter and
Cohen, 2006).
The SemEval 2013 ? Task 9.2 (Extraction of
Drug-Drug Interactions from BioMedical Texts)
is a competitive evaluation of methods for ex-
tracting mentions of drug-drug interactions from
texts (Segura-Bedmar et al, 2013). For training,
the organizers provide a corpus annotated with drug-
names and interactions between them. This corpus
is composed of 572 articles collected from Drug-
Bank and 142 PubMed abstracts. Interactions are
binary (always between two drugs) and undirected,
as target and agent roles are not annotated. Fur-
thermore, the two interacting drugs are always men-
tioned within the same sentence. In contrast to
the previous DDI-challenge 2011 (Segura-Bedmar
et al, 2011), four different DDI-subtypes (advise,
effect, mechanism, and int) have been introduced.
Details about the four subclasses can be found in the
task?s annotation guideline.
628
Figure 1: Workflow developed for the SemEval 2013
Task 9.2 challenge.
2 Methods
Binary relationship extraction is often tackled as a
pair-wise classification problem, where all
(n
2
)
co-
occurring entities in a sentence are classified as in-
teracting or not. To account for the four different
subtypes of DDIs, the problem definition could be
translated into a multiclass classification problem
between all co-occurring entities.
Contrary to that, we propose a two step strat-
egy: First, we detect general drug-drug interac-
tions regardless of subtype using a multitude of dif-
ferent machine-learning methods. The output of
these methods is aggregated using a majority vot-
ing approach. Second, detected interactions are re-
classified into one of the four possible DDI cate-
gories. The latter is referred to as DDI relabeling
throughout this paper. A detailed view on the pro-
posed workflow is depicted in Figure 1.
2.1 Preprocessing
Sentences have been parsed using Charniak-Johnson
PCFG reranking-parser (Charniak and Johnson,
2005) with a self-trained re-ranking model aug-
mented for biomedical texts (McClosky, 2010). Re-
sulting constituent parse trees have been converted
into dependency graphs using the Stanford con-
verter (De Marneffe et al, 2006). In the last step, we
created an augmented XML using the open source
Corpus Sentences
Pairs
Positive Negative Total
DrugBank 5,675 3,788 22,217 26,005
MEDLINE 1,301 232 1,555 1,787
Table 1: Basic statistics of the DDI training corpus shown
for DrugBank and MEDLINE separately.
framework from Tikk et al (2010). This XML file
encompasses tokens with respective part-of-speech
tags, constituent parse tree, and dependency parse
tree information. This format has been subsequently
transformed into a related XML format1 used by two
of the utilized classifiers. Properties of the training
corpus are shown for DrugBank and MEDLINE in
Table 1.
2.2 Machine Learning Methods
Tikk et al (2010) systematically analyzed nine dif-
ferent machine learning approaches for the extrac-
tion of undirected binary protein-protein interac-
tions. This framework has been successfully applied
to other domains, such as the I2B2 relation extrac-
tion challenge (Solt et al, 2010), the previous DDI
extraction challenge (Thomas et al, 2011), and to
the extraction of neuroanatomical connectivity state-
ments (French et al, 2012).
Drug entities are blinded by replacing the entity
name with a generic string to ensure the generality
of the approach. Without entity blinding drug names
are incorporated as features, which clearly affects
generalization capabilities of a classifier on unseen
entity mentions (Pyysalo et al, 2008).
We decided to use the following methods
provided by the framework: All-paths graph
(APG) (Airola et al, 2008), shallow lin-
guistic (SL) (Giuliano et al, 2006), subtree
(ST) (Vishwanathan and Smola, 2002), subset tree
(SST) (Collins and Duffy, 2001), and spectrum tree
(SpT) (Kuboyama et al, 2007) method. The SL
method uses only shallow linguistic features, i.e.,
token, stem, part-of-speech tag and morphologic
properties of the surrounding words. APG builds
a classifier using surface features and a weighting
1https://github.com/jbjorne/TEES/wiki/
Interaction-XML
629
scheme for dependency parse tree features. The
remaining three classifier (ST, SST, and SpT) build
kernel functions based on different subtree repre-
sentations on the constituent parse tree. To calculate
the constituent?tree kernels ST and SST we used
the SVM-LIGHT-TK toolkit (Moschitti, 2006).
Before applying these methods, constituent parse
trees have been reduced to the shortest-enclosed
parse following the recommendations from Zhang
et al (2006). For a more detailed description
of the different methods we refer to the original
publications.
In addition to the PPI framework, we also
employed the general purpose relationship ex-
traction tool ?Turku Event Extraction System?
(TEES) (Bjo?rne et al, 2011), a customized version
of the case-based reasoning system Moara (Neves
et al, 2009), and a self-developed feature based
classifier which is referred to as SLW. Regarding
TEES, we have used the edge extraction function-
ality for performing relationship extraction. TEES
considers features related to the tokens (e.g., part-of-
speech tags), dependency chains, dependency path
N-grams, entities (e.g., entity types) and external re-
sources, such as hypernyms in WordNet.
Moara is a case-based reasoning system for the
extraction of relationships and events. During train-
ing, interaction pairs are converted into cases and
saved into a HyperSQL database which are re-
trieved through case similarity during the classifica-
tion. Cases are composed by the following features:
the type of the entities (e.g. Brand and Group),
the part-of-speech tag of the tokens between the two
drugs (inclusive), the tags of the shortest depen-
dency path between the two drugs, and the lemma
of the non-entity tokens of the shortest dependency
path using BioLemmatizer (Liu et al, 2012). We
also consider the PHARE ontology (Coulet et al,
2011) in the lemma feature: When a lemma matches
any of the synonyms contained in this ontology, the
category of the respective term is considered instead.
Case similarity is calculated by exact feature match-
ing, except for the part-of-speech tags whose com-
parison is based on global alignment using insertion,
deletion, and substitution costs as proposed by Spa-
sic et al (2005).
SLW is inspired by SL (Giuliano et al, 2006;
Bunescu and Mooney, 2006) and uses the Breeze2
library. We generate n-grams over sequences of
arbitrary features (e.g. POS-tags, morphological
and syntactical features) to describe the global con-
text of an entity pair. Furthermore, we calculate
features from the local context of entities, but in
addition to SL, we include domain-specific fea-
tures used for identifying and classifying pharma-
cological substances (see our paper for DDI Task
9.1 (Rockta?schel et al, 2013)). In addition, we take
the name of the classes of a pair?s two entities as
feature to capture that entities of some class (e.g.
Brand and Group) are more likely to interact than
others (e.g. Brand and Brand).
2.3 Ensemble learning
Several community competitions previously noted
that combinations of predictions from different tools
help to achieve better results than one method
alone (Kim et al, 2009; Leitner et al, 2010). More
importantly, it is well known that ensembles increase
robustness by decreasing the risk of selecting a bad
classifier (Polikar, 2006). In this work we combined
the output of several classifiers by using majority
voting. The ensemble is used to predict DDIs re-
gardless of the four different subtypes. This com-
plies with the partial match evaluation criterion de-
fined by the competition organizers.
2.4 Relabeling
To account for DDI subtypes, we compared two ap-
proaches: (a) using the subtype prediction of TEES;
(b) training a multi-class classifier (SLW) on the
available training data for DDI subtypes. We de-
cided on using TEES, as it generated superior results
over SLW (data not shown). Thus, previously identi-
fied DDIs are relabeled into one of the four possible
subtypes using the most likely interaction subtype
from TEES.
3 Results
3.1 Cross validation
In order to compare the different approaches, we
performed document-wise 10-fold cross validation
(CV) on the training set. It has been shown that such
2http://www.scalanlp.org/
630
Type Pairs Precision Recall F1
total 3,119 78.6 78.6 78.6
effect 1,633 79.8 79.1 79.4
mechanism 1,319 79.8 79.2 79.4
advise 826 77.3 76.4 76.9
int 188 68.5 80.9 74.1
Table 4: Performance estimation for relabeling DDIs.
Pairs denotes the number of instances of this type in the
training corpus.
a setting provides more realistic performance esti-
mates than instance-wise CV (S?tre et al, 2008).
All approaches have been tested using the same
splits to ensure comparability. For APG, ST, SST,
and SpT we followed the parameter optimization
strategy defined by Tikk et al (2010). For TEES
and Moara, we used the cost parameter C (50000)
and best performing features, respectively, based on
the CV results. For SL and SLW, we used the default
parameters.
We performed several different CV experiments:
First, we performed CV on the two corpora (Drug-
Bank and MEDLINE) separately. Second, data
from the other corpus has been additionally used
during the training phase. This allows us to esti-
mate the impact of additional, but potentially differ-
ent text. CV results for DrugBank and MEDLINE
are shown in Table 2 and 3 respectively.
3.2 Relabeling
Performance of relabeling is evaluated by perform-
ing 10-fold CV on the training set using the same
splits as in previous analysis. Note that this experi-
ment is solely performed on positive DDI instances
to estimate separability of the four different DDI-
subtypes. Results for relabeling are shown in Ta-
ble 4.
3.3 Test dataset
For the test set we submitted results using the fol-
lowing three majority voting ensembles. For Run 1
we used Moara+SL+TEES, for Run 2 we used
APG+Moara+SL+SLW+TEES and for Run 3 we
used SL+SLW+TEES. Due to time constraints we
did not use different ensembles for the two corpora.
We rather decided to use ensembles which achieved
generally good results for both training corpora. All
classifiers, except APG, have been retrained on the
combination of MEDLINE and DrugBank using
the parameter setting yielding the highest F1 in the
training phase. For APG, we trained two different
models: One model is trained on MEDLINE and
DrugBank and one model is trained on DrugBank
solely. The first model is applied on the MEDLINE
test set and the latter on the DrugBank test set. Esti-
mated results on the training corpus and official re-
sults on the test corpus are shown in Table 5.
4 Discussion
4.1 Training dataset
Document-wise CV results for the DrugBank corpus
show no clear effect when using MEDLINE as ad-
ditional training data. By using MEDLINE during
the training phase we observe an average decrease of
0.3 percentage points (pp) in F1 and an average in-
crease of 0.7 pp in area under the receiver operating
characteristic curve (AUC). The strongest impact
can be observed for APG with a decrease of 2.3 pp
in F1. We therefore decided to train APG mod-
els for DrugBank without additional MEDLINE
data. For almost all ensembles (with the excep-
tion of APG+SpT+SL) we observe superior results
when using only DrugBank as training data. Inter-
estingly, this effect can mostly be attributed to an
average increase of 3.3 pp in recall, whereas preci-
sion remains fairly stable between ensembles using
DrugBank solely and those with additional training
data.
In contrast for MEDLINE, all methods largely
benefit from additional training data with an aver-
age increase of 9.8 pp and 3.6 pp for F1 and AUC re-
spectively. For the ensemble based approaches, we
observe an average increase of 13.8 pp for F1when
using DrugBank data in addition.
When ranking the different methods by F1 and
calculating correlation between the two differ-
ent corpora, we observe only a weak correlation
(Kendall?s ? = 0.286, p< 1). In other words, ma-
chine learning methods show varying performance-
ranks between the two corpora. This difference is
most pronounced for SL and SpT, with four ranks
difference between DrugBank and MEDLINE. It
is noteworthy that the two corpora are not directly
631
Regular CV Combined CV
Method P R F1 AUC P R F1 AUC
SL 61.5 79.0 69.1 92.8 62.1 78.4 69.2 93.0
APG 77.2 62.6 69.0 91.5 75.9 59.8 66.7 91.6
TEES 77.2 62.0 68.6 87.3 75.5 60.9 67.3 86.9
SLW 73.7 60.0 65.9 91.3 73.4 61.2 66.6 91.3
Moara 72.1 55.2 62.5 ? 72.0 54.7 62.1 ?
SpT 51.4 73.4 60.3 87.3 52.7 71.4 60.6 87.7
SST 51.9 61.2 56.0 85.4 55.1 57.1 56.0 86.1
ST 47.3 64.2 54.2 82.3 48.3 64.3 54.9 82.7
SL+SLW+TEES 76.1 69.9 72.7 ? 75.9 65.3 70.1 ?
APG+SL+TEES 79.3 69.9 74.2 ? 79.2 65.4 71.5 ?
Moara+SL+TEES 79.9 69.6 74.2 ? 79.6 65.1 71.6 ?
Moara+SL+APG 81.4 70.6 75.5 ? 81.3 70.3 75.3 ?
APG+Moara+SL+SLW+TEES 84.0 68.1 75.1 ? 83.7 64.2 72.6 ?
APG+SpT+TEES 76.8 68.0 72.1 ? 77.1 63.4 69.6 ?
APG+SpT+SL 68.7 74.8 71.5 ? 69.7 73.8 71.6 ?
Table 2: Cross validation results on DrugBank corpus. Regular CV is training and evaluation on DrugBank only.
Combined CV is training on DrugBank and MEDLINE and testing on DrugBank. Higher F1 between these two
settings are indicated in boldface for each method. Single methods are ranked by F1.
Regular CV Combined CV
Method P R F1 AUC P R F1 AUC
TEES 70.7 36.0 44.5 82.2 59.6 46.5 51.4 84.9
SpT 37.8 38.6 34.6 78.6 42.3 55.3 47.1 80.4
APG 46.5 44.3 42.4 82.3 38.1 62.2 46.4 82.8
SST 31.3 37.7 31.8 74.1 36.7 61.7 44.9 79.5
SL 43.7 40.1 38.7 78.9 34.7 67.1 44.7 81.1
SLW 58.0 14.3 20.4 73.4 50.1 38.0 42.0 82.4
Moara 49.8 31.9 37.6 ? 45.6 43.2 41.9 ?
ST 25.2 43.8 30.1 70.5 36.1 48.3 39.8 74.2
SL+SLW+TEES 73.6 29.0 37.6 ? 55.2 52.7 53.1 ?
APG+SL+TEES 60.7 37.9 43.4 ? 49.9 62.4 54.3 ?
Moara+SL+TEES 68.0 33.0 42.2 ? 62.1 55.5 57.4 ?
Moara+SL+APG 57.7 36.7 42.4 ? 48.3 60.9 52.8 ?
APG+Moara+SL+SLW+TEES 73.3 28.3 36.8 ? 60.6 54.4 56.5 ?
APG+SpT+TEES 58.5 37.4 41.7 ? 57.5 59.2 57.1 ?
APG+SpT+SL 48.3 39.9 40.0 ? 43.6 64.3 51.0 ?
Table 3: Cross validation results on MEDLINE corpus. Regular CV is training and evaluation on MEDLINE only.
Combined CV is training on DrugBank and MEDLINE and testing on MEDLINE. Higher F1 between these two
settings are indicated in boldface for each method. Single methods are ranked by F1.
632
Evaluation
Training Test
Run 1 Run 2 Run 3 Run 1 Run 2 Run 3
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1
Partial 78.7 67.3 72.6 82.9 66.4 73.7 75.2 67.6 71.2 84.1 65.4 73.6 86.1 65.7 74.5 80.1 72.2 75.9
Strict 65.7 56.1 60.5 70.0 56.0 62.2 63.0 56.7 59.7 68.5 53.2 59.9 69.5 53.0 60.1 64.2 57.9 60.9
-mechanism 61.8 49.7 55.1 68.1 50.0 57.7 59.2 50.3 54.4 72.2 51.7 60.2 74.9 52.3 61.6 65.3 58.6 61.8
-effect 68.8 57.9 62.9 71.8 57.6 63.9 66.1 57.4 61.5 63.7 57.5 60.4 63.6 55.8 59.5 60.7 61.4 61.0
-advise 64.6 60.5 62.5 68.2 59.7 63.6 61.1 61.5 61.3 73.3 53.4 61.8 74.5 55.7 63.7 69.0 58.4 63.2
-int 68.6 50.0 57.8 75.4 52.1 61.6 70.9 56.9 63.1 67.8 41.7 51.6 67.3 38.5 49.0 67.8 41.7 51.6
Table 5: Relation extraction results on the training and test set. Run 1 builds a majority voting on Moara+SL+TEES,
Run 2 on APG+Moara+SL+SLW+TEES, and Run 3 on SL+SLW+TEES. Partial characterizes only DDI detection
without classification of subtypes, whereas strict requires correct identification of subtypes as well.
comparable, as DrugBank is one order of magnitude
larger in terms of instances than the MEDLINE cor-
pus. Additionally, documents come from different
sources and it is tempting to speculate that there
might be a certain amount of domain specificity be-
tween DrugBank and MEDLINE sentences.
We tested for domain specificity by performing
cross-corpus experiments, i.e., we trained a classi-
fier on DrugBank, applied it on MEDLINE and vice
versa. When training on MEDLINE and testing
on DrugBank, we observe an average decrease of
about 15 pp in F1 in comparison to DrugBank in-
domain CV results. For the other setting, we observe
a lower decrease of approximately 5 pp in compari-
son to MEDLINE in-domain CV results.
From the current results, it seems that the doc-
uments from DrugBank and MEDLINE have dif-
ferent syntactic properties. However, this requires a
more detailed analysis of different aspects like dis-
tribution of sentence length, negations, or passives
between the two corpora (Cohen et al, 2010; Tikk
et al, 2013). We assume that transfer learning tech-
niques could improve results on both corpora (Pan
and Yang, 2010).
The DDI-relabeling capability of TEES is very
balanced with F1 measures ranging from 74.1 % to
79.4 % for all four DDI subclasses. This is unex-
pected since classes like ?effect? occur almost ten
times more often than classes like ?int? and classi-
fiers often have problems with predicting minority
classes.
4.2 Test dataset
On the test set, our best run achieves an F1 of 76 %
using the partial evaluation schema. This is slightly
better than the performance for DrugBank training
data shown in Table 2 and substantially better than
estimations for MEDLINE (see Table 3). With
F1 measures ranging between 74 % to 76 % only
minor performance differences can be observed be-
tween the three different ensembles.
When switching from partial to strict evaluation
scheme an average decrease of 15 pp in F1 can be ob-
served. As estimated on the training data, relabeling
performance is indeed very similar for the different
DDI-subtypes. Only for the class with the least in-
stances (int), a larger decrease in comparison to the
other three classes can be observed for the test set.
In general, results for test set are on par or higher
than results for the training set.
5 Conclusion
In this paper we presented our approach for the
SemEval 2013 ? Task 9.2 DDI extraction challenge.
Our strategy builds on a cascaded (coarse to fine
grained) classification strategy, where a majority
voting ensemble of different methods is initially
used to find generic DDIs. Predicted interactions
are subsequently relabeled into four different sub-
types. DDI extraction seems to be a more difficult
task for MEDLINE abstracts than for DrugBank ar-
ticles. In our opinion, this cannot be fully attributed
to the slightly higher ratio of positive instances in
DrugBank and points towards structural differences
between the two corpora.
Acknowledgments
This work was supported by the German Research
Foundation (DFG) [LE 1428/3-1] and the Federal
633
Ministry of Economics and Technology (BMWi)
[KF 2205209MS2].
References
A. Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala,
F. Ginter, and T. Salakoski. 2008. All-paths graph
kernel for protein-protein interaction extraction
with evaluation of cross-corpus learning. BMC
Bioinformatics, 9 Suppl 11:S2.
J. Bjo?rne, J. Heimonen, F. Ginter, A. Airola,
T. Pahikkala, and T. Salakoski. 2011. Extracting
Contextualized Complex Biological Events with
Rich Graph-Based Features Sets. Computational
Intelligence, 27(4):541?557.
R. C. Bunescu and R. J. Mooney. 2006. Sub-
sequence Kernels for Relation Extraction. Ad-
vances in Neural Information Processing Sys-
tems, 18:171.
E. Charniak and M. Johnson. 2005. Coarse-to-
Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proc. of ACL?05, pages 173?180.
M. D. Cheitlin, A. M. Hutter, R. G. Brindis, P. Ganz,
S. Kaul, R. O. Russell, and R. M. Zusman. 1999.
Use of sildenafil (viagra) in patients with cardio-
vascular disease. J Am Coll Cardiol, 33(1):273?
282.
K. Cohen, Helen L Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E Hunter.
2010. The structural and content aspects of ab-
stracts versus bodies of full text journal articles
are different. BMC Bioinformatics, 11:492.
M. Collins and N. Duffy. 2001. Convolution Kernels
for Natural Language. In Proc. of NIPS?01, pages
625?632.
A. Coulet, Y. Garten, M. Dumontier, R. Altman,
M. Musen, and N. Shah. 2011. Integration and
publication of heterogeneous text-mined relation-
ships on the semantic web. Journal of Biomedical
Semantics, 2(Suppl 2):S10.
M.C. De Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proc. of LREC
2006, pages 449?454.
L. French, S. Lane, L. Xu, C. Siu, C. Kwok, Y. Chen,
C. Krebs, and P. Pavlidis. 2012. Application and
evaluation of automated methods to extract neu-
roanatomical connectivity statements from free
text. Bioinformatics, 28(22):2963?2970.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting Shallow Linguistic Information for Re-
lation Extraction from Biomedical Literature. In
Proc. of EACL?06, pages 401?408.
S. I. Haider, K. Johnell, M. Thorslund, and J. Fast-
bom. 2007. Trends in polypharmacy and potential
drug-drug interactions across educational groups
in elderly patients in Sweden for the period 1992
- 2002. Int J Clin Pharmacol Ther, 45(12):643?
653.
L. Hunter and K. Cohen. 2006. Biomedical language
processing: what?s beyond PubMed? Mol Cell,
21(5):589?594.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of BioNLP?09 shared task on
event extraction. In Proc. of BioNLP?09, pages
1?9.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frol-
kis, A. Pon, K. Banco, C. Mak, V. Neveu,
Y. Djoumbou, R. Eisner, A. Chi Guo, and D. S
Wishart. 2011. Drugbank 3.0: a comprehensive
resource for ?omics? research on drugs. Nucleic
Acids Res, 39(Database issue):D1035?D1041.
T. Kuboyama, K. Hirata, H. Kashima, K. F. Aoki-
Kinoshita, and H. Yasuda. 2007. A Spectrum
Tree Kernel. Information and Media Technolo-
gies, 2(1):292?299.
F. Leitner, S.A. Mardis, M. Krallinger, G. Ce-
sareni, L.A. Hirschman, and A. Valencia. 2010.
An overview of BioCreative II. 5. IEEE
IEEE/ACM Transactions on Computational Biol-
ogy and Bioinformatics, pages 385?399.
H. Liu, T. Christiansen, W. Baumgartner, and
K. Verspoor. 2012. Biolemmatizer: a lemmatiza-
tion tool for morphological processing of biomed-
ical text. Journal of Biomedical Semantics, 3(1):3.
D. McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Brown University.
A. Moschitti. 2006. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees.
In Proc. of ECML?06, pages 318?329.
M. Neves, J.-M. Carazo, and A. Pascual-Montano.
2009. Extraction of biomedical events using case-
based reasoning. In Proc. of BioNLP?09, pages
68?76.
S. J. Pan and Q. Yang. 2010. A Survey on Transfer
634
Learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
R. Polikar. 2006. Ensemble Based Systems in Deci-
sion Making. IEEE Circuits and Systems Maga-
zine, 6(3):21?45.
S. Pyysalo, R. S?tre, J. Tsujii, and T. Salakoski.
2008. Why Biomedical Relation Extraction Re-
sults are Incomparable and What to do about it.
In Proc. of SMBM?08, pages 149?152.
T. Rockta?schel, T. Huber, M. Weidlich, and U. Leser.
2013. WBI-NER: The impact of domain-specific
features on the performance of identifying and
classifying mentions of drugs. In Proceedings of
the 7th International Workshop on Semantic Eval-
uation (SemEval 2013).
R. S?tre, K. Sagae, and J. Tsujii. 2008. Syntactic
features for protein-protein interaction extraction.
In Proc. of LBM?07.
I. Segura-Bedmar, P. Mart??nez, and M. Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts. In
Proc. of the 7th International Workshop on Se-
mantic Evaluation (SemEval 2013).
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros. 2011. The 1st ddiextraction-2011 chal-
lenge task: Extraction of drug-drug interactions
from biomedical text. In Proc. of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction
2011, pages 1?9.
I. Solt, F. P. Szidarovszky, and D. Tikk. 2010. Con-
cept, Assertion and Relation Extraction at the
2010 i2b2 Relation Extraction Challenge using
parsing information and dictionaries. In Proc. of
i2b2/VA Shared-Task.
I. Spasic, S. Ananiadou, and J. Tsujii. 2005. MaS-
TerClass: a case-based reasoning system for the
classification of biomedical terms. Bioinformat-
ics, 21(11):2748?2758.
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011. Relation extraction for drug-drug interac-
tions using ensemble learning. In Proc. of the
1st Challenge Task on Drug-Drug Interaction Ex-
traction 2011, pages 11?18.
D. Tikk, I. Solt, P. Thomas, and U. Leser. 2013.
A detailed error analysis of 13 kernel methods
for protein-protein interaction extraction. BMC
Bioinformatics, 14(1):12.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of
kernel methods to extract protein-protein interac-
tions from literature. PLoS Comput Biol, 6.
S. V. N. Vishwanathan and A. J. Smola. 2002. Fast
Kernels for String and Tree Matching. In Proc. of
NIPS?02, pages 569?576.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A
Composite Kernel to Extract Relations between
Entities with Both Flat and Structured Features.
In Proc. of ICML?06, pages 825?832.
635
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 1?9,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Not all links are equal: Exploiting Dependency Types for the Extraction of
Protein-Protein Interactions from Text
Philippe Thomas 1, Stefan Pietschmann 1, Ille?s Solt 2, Domonkos Tikk1,2 and Ulf Leser 1
1Knowledge Management in Bioinformatics, Institute for Computer Science,
Humboldt-University of Berlin,
Unter den Linden 6, 10099 Berlin, Germany
2Department of Telecommunications and Media Informatics,
Budapest University of Technology and Economics,
Magyar tudo?sok ko?ru?tja 2, 1117 Budapest, Hungary
{thomas,pietschm,solt,tikk,leser}@informatik.hu-berlin.de
Abstract
The extraction of protein-protein interactions
(PPIs) reported in scientific publications is one
of the most studied topics in Text Mining in
the Life Sciences, as such algorithms can sub-
stantially decrease the effort for databases cu-
rators. The currently best methods for this
task are based on analyzing the dependency
tree (DT) representation of sentences. Many
approaches exploit only topological features
and thus do not yet fully exploit the informa-
tion contained in DTs. We show that incor-
porating the grammatical information encoded
in the types of the dependencies in DTs no-
ticeably improves extraction performance by
using a pattern matching approach. We au-
tomatically infer a large set of linguistic pat-
terns using only information about interact-
ing proteins. Patterns are then refined based
on shallow linguistic features and the seman-
tics of dependency types. Together, these lead
to a total improvement of 17.2 percent points
in F1, as evaluated on five publicly available
PPI corpora. More than half of that improve-
ment is gained by properly handling depen-
dency types. Our method provides a general
framework for building task-specific relation-
ship extraction methods that do not require an-
notated training data. Furthermore, our obser-
vations offer methods to improve upon rela-
tion extraction approaches.
1 Introduction
Insights about protein-protein interactions (PPIs) are
vital to understand the biological processes within
organisms. Accordingly, several databases, such as
IntAct, DIP, or MINT, contain detailed information
about PPIs. This information is often manually har-
vested from peer reviewed publications (Ceol et al,
2010). However, it is assumed that a high amount
of PPIs is still hidden in publications. Therefore, the
automated extraction of PPIs from text has attracted
considerable attention from biology research.
A number of different techniques have been pro-
posed to solve the problem of extracting PPIs from
natural language text. These can be roughly or-
ganized into one of three classes: co-occurrence,
machine learning, and pattern matching (for a re-
cent survey, see (Zhou and He, 2008)). The co-
occurrence based approaches use only information
on the co-existence of protein mentions in a given
scope. They are easy to implement and allow for
efficient processing of huge amounts of texts, but
they are also prone to generate many false positives
because they cannot distinguish positive from neg-
ative pairs. The second class is based on machine
learning. Here, a statistical model is learned from a
set of positive and negative examples and then ap-
plied to unseen texts. In general, machine learning-
based methods to relation extraction perform very
well for any task where sufficient, representative and
high quality training data is available (Kazama et
al., 2002). This need for training data is their ma-
jor drawback, as annotated texts are, especially in
the Life Sciences, rather costly to produce. Fur-
thermore, they are prone to over-fit to the training
corpus, which renders evaluation results less infer-
able to real applications. A third class of methods
is based on pattern matching. Such methods work
with patterns constructed from linguistically anno-
1
tated text, which are matched against unseen text
to detect relationships. Patterns can either be in-
ferred from examples (Hakenberg et al, 2010; Liu
et al, 2010) or can be defined manually (Fundel et
al., 2007). Systems based on manually defined pat-
terns typically use few patterns, leading to high pre-
cision but low recall (Blaschke et al, 2002). In con-
trast, systems that learn patterns automatically often
produce more patterns and exhibit a better recall, at
the cost of a decrease in precision. To circumvent
this penalty, several works have tried to improve
patterns. E.g., SPIES (Hao et al, 2005) filters pat-
terns using the minimum description length (MDL)
method which improves its F1 by 6.72%.
Another classification of PPI extraction methods
is based on the sentence representation that is ap-
plied. The simplest such representation is the bag of
words (BoW) that occur in the sentence; more com-
plex representations are constituent trees, capturing
the syntactic structure of the sentence, and depen-
dency trees (DTs), which represent the main gram-
matical entities and their relationships to each other.
PPI extraction methods use various sentence repre-
sentation, e.g., are based only on BoW (Bunescu
and Mooney, 2006; Giuliano et al, 2006), use only
DTs (Erkan et al, 2007), or combine representa-
tions (Airola et al, 2008; Miwa et al, 2008).
In the last years, dependency trees have become
the most popular representation for relation extrac-
tion. DTs characterize, via their dependency links,
grammatical relationships among words. They are
particularly favored by kernel-based learning ap-
proaches, see e.g. (Culotta and Sorensen, 2004;
Erkan et al, 2007; Airola et al, 2008; Miwa et al,
2008; Kim et al, 2010) but also graph matching ap-
proaches using DTs have been proposed (Liu et al,
2010). However, these methods do not further utilize
the grammatical information encoded in the depen-
dency types (edge labels). Recently proposed meth-
ods like (Buyko et al, 2009; Rinaldi et al, 2010)
modify the DTs by e.g. trimming irrelevant depen-
dencies. In contrast to these approaches, our method
exploits the dependency types of DTs and performs
basic transformations on DTs; we use Stanford de-
pendencies, which are presumably the most often
used DT representation in PPI extraction.
The rest of this paper is organized as follows. We
describe our novel method for extracting PPIs from
text that is based on pattern matching in dependency
graphs. We evaluate our method against benchmark
PPI corpora, and discuss results with a focus on de-
pendency type information based methods.
2 Methods
Our approach consists of a series of steps: First, we
extract sentences from Medline and PMC open ac-
cess that contain pairs of genes/proteins known to
interact. Second, we convert each of those sentences
into DTs and derive putative tree patterns for each
pair. Having a set of such patterns, we apply a num-
ber of generalization methods to improve recall and
filtering methods to improve precision. We discern
between methods that are purely heuristic (termed
shallow) and steps that incorporate dependency type
information (termed grammatical). To predict PPIs
in unseen text, the resulting patterns are matched
against the corresponding DTs.
2.1 Extraction of PPI sentences
We apply the method described in (Hakenberg et al,
2006) to extract a set of sentences from Medline and
PMC potentially describing protein interactions. Es-
sentially, this method takes a database of PPIs (here
IntAct; (Aranda et al, 2010)) and searches all sen-
tences in Medline and PMC containing any of those
pairs. Proteins were tagged and normalized using
GNAT (Hakenberg et al, 2008). To avoid a possible
bias, articles contained in any of the five evaluation
corpora are excluded. This resulted in 763,027 in-
teracting protein pairs.
2.2 Pattern generation and matching
For each protein pair we generate a new sentence
and apply entity blinding, meaning that named enti-
ties are replaced by placeholders to avoid systemic
bias. Specifically, the mentions of the two proteins
known to interact are replaced by the placeholder
ENTITY A and any additional proteins in the same
sentence are replaced by ENTITY B. Tokens are
tagged with their part-of-speech (POS) using Med-
Post (Smith et al, 2004), which is specifically op-
timized for biomedical articles. Constituent parse
trees are generated using the Bikel parser (Bikel,
2002) and converted to DTs by the Stanford con-
verter (De Marneffe et al, 2006). In a DT, the short-
est path between two tokens is often assumed to con-
2
tain the most valuable information about their mu-
tual relationship. Therefore, we generate a pattern
from each DT by extracting the shortest, undirected
path between the two occurrences of ENTITY A.
The set of initial patterns is denoted by SIP.
We employ several methods to improve the qual-
ity of this initial set of patterns. We systemati-
cally evaluated possible constellations and identified
those that help in improving performance of PPI ex-
traction. The modifications are of two kinds. Pattern
generalizers are intended to elevate recall, whereas
pattern filters should raise precision. We present
two types of methods: Shallow methods are simple
heuristics whereas grammatical methods are rules
that exploit the information in dependency types.
We use a strict graph matching approach for pat-
tern matching. We consider a pattern to match a sub-
graph of a DT iff all their nodes and edges match
exactly, including edge labels and edge directions.
2.3 Pattern generalization
It is a common practice in NLP to apply some pre-
processing on patterns to reduce corpus-specificity.
In particular, we perform stemming (GST), re-
moval of common protein name prefixes and suf-
fixes (GPN), and replacement of interaction phrases
by single words (GIW). We summarize these steps as
shallow generalization steps. We only describe the
latter two (GPN, GIW) in more detail.
Protein names are often modified by suffixes like
-kinase, -receptor or -HeLa or by prefixes like
phospho- or anti-. These affixes are usually not
covered by entity blinding as the entity recognition
method does not consider them as part of the pro-
tein name. As such affixes are not relevant for rela-
tion extraction but do interfere with our exact graph
matching approach, we apply the GPN heuristic to
remove them.
Interactions between proteins can be expressed
very diversely in natural language. In almost all
cases there is at least one word that specifies the in-
teraction semantically, called the interaction word;
this is often a verb, such as ?binds? or ?phospho-
rylates?, but can as well be a noun, such as ?[in-
duced] phosphorylation?, or an adjective, such as
?binding?. The GIW heuristic generalizes patterns
by substituting all contained interaction words with
generic placeholders. We assembled a list of 851 in-
teraction words (including inflection variants) based
on (Temkin and Gilder, 2003; Hakenberg et al,
2006) that was further enriched manually. Based
on their POS-tags, interaction words are assigned to
one of the three placeholders IVERB, INOUN, IAD-
JECTIVE. We also experimented with a single inter-
action word placeholder, IWORD, handling the case
of incorrect POS-tags.
Unifying dependency types (GUD): The Stan-
ford typed dependency format contains 55 grammat-
ical relations organized in a generalization hierar-
chy. Therefore, it is a natural idea to treat simi-
lar (e.g., sibling) dependency types equally by re-
placing them with their common parent type. We
manually evaluated all dependency types to assess
whether such replacements are viable. The final list
of replacements is listed in Table 1. Note that we
used the so-called collapsed representation of de-
pendency types of the Stanford scheme. This means
that prepositional and conjunctive dependencies are
collapsed to form a single direct dependency be-
tween content words, and the type of this depen-
dency is suffixed with the removed word. In the GUD
generalizer, these dependency subtypes are substi-
tuted by their ancestors (e.g., prep, conj).
Dependency types Common type
subj, nsubj*, csubj* subj
obj, dobj, iobj, pobj obj
prep *, agent, prepc prep
nn, appos nn
Table 1: Unification of specific dependency types to
a single common type by the generalizer GUD. Note
that agent is merged with dependency type prep
as it is inferred for the preposition ?by?.
Collapsing dependency links (GCD): In addi-
tion to the collapsing performed by Stanford con-
verter, we remove edges that likely are irrelevant
for PPIs. We focused on removing the dependen-
cies nn (noun compound modifier) and appos (ap-
positional modifier). These grammatical construc-
tions have the same syntactic role but they carry
somewhat different meaning. They function as noun
phrase modifiers and often specify the subtype of
an entity, which is irrelevant for our task. As these
two dependency types convey no information about
3
the interaction itself, the dependency itself and the
corresponding noun can be removed, as long as the
noun is not an entity itself. As an example, this
generalizer is applied on the dependency parse tree
of the sentence ?ENTITY A protein recognized anti-
body (ENTITY A)? shown on Figure 1a. The modi-
fied parse tree is depicted on Figure 1b.
ENTITY-A
prote in
n n
recognized
nsubj
ant ibody
dobj
ENTITY-A
appos
(a) Original pattern
ENTITY_A
recognized
nsubj
ENTITY_A
dobj
(b) Generalized pattern
Figure 1: Dependency pattern before and after col-
lapsing nn and appos dependency links using the
generalizer GCD.
2.4 Pattern constraints
Due to the automatic construction method, our set
of patterns also contains samples derived from sen-
tences that do not actually describe an interaction
between proteins, although it does contain a pair of
interacting proteins. Such patterns lead to wrong
matches. As a countermeasure, we define con-
straints an extracted pattern has to fulfill. Patterns
not adhering to these constraints are removed from
the pattern set, thus increasing precision. Standard
(shallow) heuristics for doing so are the exclusion of
negation words (CNW) and the restriction to patterns
containing interaction-related words from a prede-
fined set (CIW). Patterns containing negations po-
tentially match two negative protein pairs. Such pat-
tern can be removed to prevent wrong extractions.
For negation words, the list described in (Fundel
et al, 2007) was used. Additionally, patterns con-
taining the dependency type conj no*, conj or, or
prep without are also removed. On top of those pre-
viously known approaches, we developed two new
filter to leverage the semantic richness of the DTs.
Dependency combination (CDC): Interaction
words are organized into the following categories:
verb, adjective and noun. Based on linguistic con-
siderations we define ?dependency patterns? for the
different word types. For example we assume that
interaction verbs describe an action that originates in
one protein and affects the other protein. Obviously,
the dependency combination subj with obj fulfills
this consideration (for an example see Figure 1b).
We manually evaluated a few DTs containing PPI
for each interaction word category (verb, noun, ad-
jective) and determined all combinations of depen-
dency types that are valid for the given category. The
resulting combinations are listed in Table 2.
Part of speech Dependency type combination
Noun
prep prep
prep nn
prep amod
nn nn
nn amod
Verb
prep subj
prep infmod
prep partmod
obj subj
obj infmod
obj partmod
Adjective amod
Table 2: Allowed dependency type combinations
based on classes of POS classes (constraint CDC).
subj = {nsubj, nsubjpass, xsubj, csubj,
csubjpass}, obj = {dobj, pobj, iobj} and
prep = {prep *, agent}
Syntax Filter (CSF): A particular case in PPI ex-
traction are sentences with enumerations, as shown
in Figure 2. Such (possibly quite long; the longest
enumeration we found contains not less than 9 pro-
teins) enumerations greatly increase the number of
protein pairs.
We observed that sentences in which the common
dependency type is prep between or nn often do
describe an association between the connected pro-
teins. Accordingly, such patterns are retained.
The remaining pairs inside enumerations often
do not describe an interaction between each other.
Therefore, we developed a special handling of enu-
merations, based on dependency types. If two pro-
teins have a common ancestor node connected by the
same dependency type, we assume that those pro-
teins do not interact with each other. Accordingly,
we remove all such patterns.
4
ENTITY_B
act ivates
nsubj
ENTITY_B
dobj
ENTITY_A
appos
ENTITY_A
appos
Figure 2: Dependency tree (DT) for the entity
blinded sentence ?ENTITY B activates ENTITY B,
ENTITY A, ENTITY A.? with the initial pattern
highlighted in bold red. Application of CSF removes
this pattern.
3 Results
For evaluation we use five manually annotated
benchmark corpora: AIMed, BioInfer, HPRD50,
IEPA, and LLL. Those have been unified to the
?greatest common factors? by Pyysalo et al (2008).
This means that protein names in the corpora are
tagged and that interactions are undirected and bi-
nary. A basic overview of the corpora can be found
in Table 1 of (Airola et al, 2008).
A sentence with n entities contains
(n
2
)
different
undirected entity pairs. For each entity pair in a
sentence, we generate a separate evaluation exam-
ple, apply entity blinding and generate the DT in
the same manner as previously described for gen-
erating the pattern set. All patterns are then matched
against the DTs of the sentences from the evalua-
tion corpora. If at least one pattern matches, the pair
is counted as positive otherwise as negative. From
this information we calculate precision, recall, and
F1 scores.
Table 3 shows results using the initial pattern set
and the different configurations of generalized / fil-
tered pattern sets. We evaluate the impact of shallow
and grammar-based methods separately. Recall that
Sshallow encompasses stemming (GST), substitution
of interaction words (GIW), suffix/prefix removal at
entity names (GPN), and interaction (CIW) and nega-
tion word filtering (CNW), while Sgrammar-based en-
compasses unification of dependency types (GUD),
collapsing dependency links (GCD), the dependency
combination constraint (CDC) and the syntax fil-
ter (CSF). In addition, results after application of
all generalizers Sgeneralizers, all constraints Sconstraints
0.0
0.2
0.4
0.6
0.8
1.0
AIMed
BioInfer
HPRD50
IEPA
LLL
Precision
RecallF1
Figure 3: Results for the five corpora using the set-
ting achieving highest overall F1 (Sall).
and the combination of both Sall are also included.
Corpus-specific results for the best setting in terms
of F1 (Sall) are shown in Figure 3.
Setting P R F1 #
Baseline Sinitial 23.2 34.8 27.8 478 k
Generalizers
GPN 23.4 37.0 28.7 423 k
GIW 26.2 45.3 33.2 453 k
GST 24.1 37.4 29.3 471 k
GUD 24.0 38.3 29.5 467 k
GCD 26.3 48.9 34.2 418 k
Constraints
CNW 23.4 34.8 28.0 455 k
CIW 42.5 17.2 24.5 322 k
CDC 39.5 15.9 22.7 318 k
CSF 28.2 32.6 30.3 419 k
Combinations
Sgeneralizers 28.2 69.0 39.9 290 k
Sconstraints 68.3 12.7 21.4 224 k
Sshallow 40.9 31.4 35.5 253 k
Sgrammar-based 33.2 42.1 37.2 264 k
Sall 38.2 54.8 45.0 152 k
Table 3: Performance of pattern sets on the ensem-
ble of all five corpora. # denotes the pattern set size.
4 Discussion
We presented a pattern-based approach to extract
protein-protein interactions from text. Our main
contribution in this paper was a systematic study on
the usage of dependency types within this approach.
We showed that using such knowledge, F1 on aver-
age improves by 9.4 percentage points (pp) (27.8 %
to 37.2 %) as measured on the five benchmark PPI
corpora.
Apart from this result, we note that our method
5
also has a number advantageous features: First, pat-
terns are learned from co-mentions of pairs of pro-
teins known to interact, and hence no annotated cor-
pus is necessary. This does not only make an ap-
plication of the method for new tasks easier and
cheaper, but also prevents over-fitting to a training
corpus. Note, that as shown recently by (Airola et
al., 2008; Tikk et al, 2010), essentially all state-of-
the-art machine learning methods show large per-
formance differences depending on whether or not
the evaluation and training examples are drawn from
the same corpus. In particular, cross-validation re-
sults of those are consistently more optimistic than
the more realistic cross-learning results. In contrast,
a pattern-based approach like ours is not prone to
over-fitting. Furthermore, debugging our method is
rather simple. Unlike when using a black-box ma-
chine learning method, whenever a false positive
match is found, one can pinpoint the specific pattern
producing it and take action.
The work most closely related to ours is
RelEx (Fundel et al, 2007). RelEx uses a small
set of fixed rules to extract directed PPIs from de-
pendency trees. Some of these rules also take ad-
vantage of dependency types, for instance, to prop-
erly treat enumerations. A reimplementation of
RelEx (Pyysalo et al, 2008) was recently evalu-
ated on the same corpora we used (see Table 7) and
was found to be on par with other systems, though
some of its measures were considerably worse than
those reported in the original publication. Com-
pared to our approach, RelEx is similar in that it
performs pattern matching on DTs using informa-
tion encoded in dependency types, however, there
are some notable differences: First, RelEx rules
were defined manually and are highly specific to
protein-protein interactions. It is not clear how these
could be adapted to other applications; in contrast,
we described a general method that performs pat-
tern learning from automatically generated exam-
ples. Second, it is not clear how RelEx results
could be further improved except by trial-and-error
with more rules. In contrast, our pattern learning
method offers a natural way of improvement by sim-
ply increasing the number of examples and hence the
number of patterns. We compared the results of our
approach using an increasingly larger pattern set and
observed a continuous increase in F1, due to a con-
tinuous improvement in recall. Consequently, using
more PPI databases would likely produce better re-
sults. Third, our generalization methods can be seen
as graph rewriting rules. The result of applying them
to a DT is, again, a DT; thus, they can easily be used
as pre-processing coupled with other PPI extraction
methods (a direction we are currently exploring). In
contrast, RelEx matches patterns against DTs, but
cannot be used to transform DTs.
In the following, we discuss the impact of the re-
finement methods individually and provide a brief
error analysis based on a random sample of false
negative pairs and a random sample of low preci-
sion patterns. We also compare our best results with
those of several state-of-the-art methods.
4.1 Generalizers and constraints
As can be seen in Table 3, all of the generalizers in-
creased recall and even provide minor improvement
in precision. For the combination of all five general-
izers (Sgeneralizers), an overall increase of 34.2 pp in
recall and 5 pp in precision was observed. From the
shallow generalizers, merging interaction phrases
(GIW) was proven to be the most effective, account-
ing for an increase of 10.5 pp in recall and 3 pp in
precision. As shown in Table 4, the general variant,
which merges all interaction phrases to a common
word, is slightly superior to the variant in which in-
teraction words are merged class-wise.
GIW variant P R F1
Specific 26.1 44.7 33.0
General 26.2 45.4 33.2
Table 4: Results for collapsing interaction word
variants (GIW).
For the grammar-based generalizer unifying de-
pendency types (GUD), each of the different variants
was evaluated separately (see Table 5). The com-
bination of the all different variants leads to an in-
crease of 3.5 pp in recall. As shown in Table 6, col-
lapsing the dependency types nn and appos (GCD)
also provides the highest improvement when applied
in combination.
In contrast to generalizers that alter patterns, con-
straints remove patterns from the pattern set. As
shown in Table 3, application of all constraints
6
GUD variant P R F1
subj 23.4 35.1 28.1
obj 23.3 34.9 27.9
prep 24.0 37.0 29.1
nn 23.1 35.6 28.1
sopn 24.0 38.3 29.5
Table 5: Dependency type aggregations used in gen-
eralizer GUD. sopn combines the dependency ag-
gregations for subj, obj, prep, and nn.
GCD variant P R F1
appos 23.6 38.1 29.2
nn 25.8 45.3 32.9
appos+nn 26.3 48.9 34.2
Table 6: Impact of collapsing the dependency types
appos and nn using generalizer GCD.
(Sconstraints) leads to an increase in precision of
45.1 pp at the cost of 22.1 pp reduced recall.
The shallow constraint that disallows patterns
with negation words (CNW) has comparably little
impact and removes only 5 % of the patterns. In con-
trast, the interaction word constraint (CIW) is less
conservative and removes more than 32.6 % of the
patterns, trading off an increase of 19.3 pp in preci-
sion to a decrease of 17.6 pp in recall.
Among the grammar-based constraints, the de-
pendency combination constraint CDC is superseded
by the syntax filter constraint (CSF) that removes
12 % of the patterns and increases precision about
5 pp while recall drops by only 2.2 pp. Note that CSF
is the only constraint substantially increasing F1.
As seen in Table 3, combinations of generalizers
and constraints yield almost fully additive improve-
ments. The combination of all shallow refinements
only (Sshallow) leads to an increase of 7.7 pp in F1,
whereas with the addition of our grammar-based re-
finements (Sall) a total increase of 17.2 pp in F1 is
achieved. We justify that the inclusion of depen-
dency type information adds a source of knowledge
that further improves on conventional methods such
as stemming or negation filtering.
4.2 Comparison with other methods
We compare the results of our best setting (Sall) with
the results of the recently published analysis of nine
state-of-the-art machine learning methods for PPI
extraction (Tikk et al, 2010). For these methods, a
cross-learning evaluation by training each kernel on
the ensemble of four corpora and evaluating it on the
fifth has been performed. Detailed results are given
in Table 7. In terms of F1 we achieve on BioInfer,
the corpus with most protein pairs, the second-best
result. On IEPA and LLL we achieve mid-range re-
sults and on AIMed and HPRD50 we yield results
below average. Overfitting remains a severe prob-
lem in ML based methods as these results are infe-
rior to those measured in cross-validation (Tikk et
al., 2010), though there are suggestions to overcome
this issue even in a ML setting (Miwa et al, 2009).
4.3 Error analysis
We randomly picked 30 gold standard sentences (all
corpora) containing false negatives pairs and investi-
gated all 72 false negative pairs included therein. For
29 positive pairs, possibly matching pattern were re-
moved by CDC, as the corresponding dependency
combination was not covered in our rule set. Fur-
ther 16 graphs passed the filtering, but our set of
sentences contained no adequate pattern. The third
largest fraction of errors (13 cases) are pairs which,
by our understanding, hardly describe an interaction.
In 11 cases, the dependency parse trees are incorrect
and therefore they do not provide the correct syntac-
tic information. For 7 pairs, the shortest path covers
insufficient syntactic information to decide whether
two proteins interact. For instance Figure 4 pro-
vides not enough information on the shortest path,
whereas the second shortest path would provide suf-
ficient information. Finally, three pairs were filtered
by the CIW filter, as their interaction words were
missing from our list.
We conclude that some constraints (especially
CDC and CIW) are too aggressive. Relaxation of
these syntactic rules should lead to higher recall.
We also analyzed the 30 patterns producing the
most false positives matches. 20 of them contained
an interaction verb, the remaining 10 an interaction
noun. The 10 noun patterns produced more than
twice as many false positives as the 20 verb patterns
while matching about 50 % less true positives. The
single noun pattern producing the most false posi-
tives (356) can be seen on Figure 5a. Among the 10,
four additional patterns can be seen as an extension
7
Method
AIMed BioInfer HPRD50 IEPA LLL
P R F1 P R F1 P R F1 P R F1 P R F1
Shallow linguistic (Giuliano et al, 2006) 28.3 86.6 42.6 62.8 36.5 46.2 56.9 68.7 62.2 71.0 52.5 60.4 79.0 57.3 66.4
Spectrum tree (Kuboyama et al, 2007) 20.3 48.4 28.6 38.9 48.0 43.0 44.7 77.3 56.6 41.6 9.6 15.5 48.2 83.5 61.2
k-band shortest path (Tikk et al, 2010) 28.6 68.0 40.3 62.2 38.5 47.6 61.7 74.2 67.4 72.8 68.7 70.7 83.7 75.0 79.1
Cosine distance (Erkan et al, 2007) 27.5 59.1 37.6 42.1 32.2 36.5 63.0 56.4 59.6 46.3 31.6 37.6 80.3 37.2 50.8
Edit distance (Erkan et al, 2007) 26.8 59.7 37.0 53.0 22.7 31.7 58.1 55.2 56.6 58.1 45.1 50.8 68.1 48.2 56.4
All-paths graph (Airola et al, 2008) 30.5 77.5 43.8 58.1 29.4 39.1 64.2 76.1 69.7 78.5 48.1 59.6 86.4 62.2 72.3
RelEx reimpl. (Pyysalo et al, 2008) 40.0 50.0 44.0 39.0 45.0 41.0 76.0 64.0 69.0 74.0 61.0 67.0 82.0 72.0 77.0
Our method (Sall) 25.8 62.9 36.6 43.4 50.3 46.6 48.3 51.5 49.9 67.5 58.2 62.5 70.3 70.7 70.5
Table 7: Cross-learning results. Classifiers are trained on the ensemble of four corpora and tested on the
fifth one (except for the rule-based RelEx). Best results are typeset in bold.
of this pattern leading to a total amount of 732 false
positives while only 172 true positives. This phe-
nomenon is caused by the way in which generaliz-
ers and constraints are currently applied. The unifi-
cation of different prep * dependency types to the
general prep (GUD) makes some dependency type
combinations indistinguishable, e.g. (prep to,
prep to) and (prep to, prep of). The depen-
dency type combination constraint (CDC) would dis-
allow a pattern containing the first combination, but
as it is not applied in the matching phase, its benefits
cannot be realized. A lesson learned from this exam-
ple is that constraints should also be applied in the
matching step as follows. After a successful match,
the constraints should be applied to the original un-
generalized counterparts of the matching subgraphs.
Similar conclusions can be drawn from examining
the verb pattern producing the most false positives
shown in Figure 5b.
5 Conclusion
We presented a pattern-based approach to extract
PPIs from literature. Its unique features are the ca-
pability of learning patterns from ?cheap? training
data, i.e., co-mentions of proteins known to inter-
act, and the use of linguistically motivated refine-
ments on the dependency structures of the DT it op-
erates on. We utilized the fact that not all depen-
dency types are of equal importance for relation ex-
traction; for instance, collapsing dependency links
(GCD) or unifying dependencies (GUD) considerably
improved extraction performance. However, as our
error analysis shows, especially our filtering meth-
ods deserve further study. Even without manually
annotated training data, our approach performs on
ENTITY_A ENTITY_Aconj_and
interac t
nsubj nsubj
Figure 4: Example dependency parse where the in-
formation extracted by the shortest path (highlighted
in bold red) is insufficient.
ENTITY_A
inoun
prep
ENTITY_A
prep
(a) Noun pattern
ENTITY_A
iverb
subj
ENTITY_A
prep
(b) Verb pattern
Figure 5: Patterns producing the most false posi-
tives. Depicted dependency types are generalized
according to GUD and GIW.
par with state-of-the-art machine learning methods
when evaluated in a cross-learning setting. In par-
ticular, it reaches the second best performance (in
terms of F-measure) of all approaches on the largest
PPI corpus.
Apart from presenting a volatile and elegant new
method for relationship extraction, we believe that
our study on using dependency type information
also will be helpful for advancing the performance
of other methods. For instance, we are currently
experimenting with using our DT-rewrite rules as a
preprocessor for a kernel-based extraction method.
Acknowledgments
PT was supported by BMBF, grant No 0315417B;
IS was supported by DAAD; DT was supported by
Alexander von Humboldt-Foundation.
8
References
A. Airola, S. Pyysalo, F Ginter J. Bjo?rne, and
T. Pahikkala. 2008. All-paths graph kernel for
protein-protein interaction extraction with evaluation
of cross-corpus learning. BMC Bioinformatics, 9:S2.
B. Aranda, P. Achuthan, Y. Alam-Faruque, I. Armean,
A. Bridge, C. Derow, M. Feuermann, et al 2010. The
IntAct molecular interaction database in 2010. Nu-
cleic Acids Res, 38:D525?D531, Jan.
DM. Bikel. 2002. Design of a Multi-lingual, Parallel-
processing Statistical Parsing Engine. In In Human
Language Technology Conference, pages 24?27.
C. Blaschke, L. Hirschman, and A. Valencia. 2002. In-
formation extraction in molecular biology. Briefings
in Bioinformatics, 3(2):154?165.
R. Bunescu and R. Mooney. 2006. Subsequence Kernels
for Relation Extraction. In Y. Weiss, B. Scho?lkopf,
and J. Platt, editors, Advances in Neural Information
Processing Systems 18, pages 171?178. MIT Press,
Cambridge, MA.
E. Buyko, E. Faessler, J. Wermter, and U. Hahn. 2009.
Event extraction from trimmed dependency graphs. In
BioNLP?09, pages 19?27.
A. Ceol, Chatr AA., L. Licata, D. Peluso, L. Briganti,
L. Perfetto, L. Castagnoli, and G. Cesareni. 2010.
MINT, the molecular interaction database: 2009 up-
date. Nucl. Acids Res., 38(suppl1):D532?539.
A. Culotta and JS. Sorensen. 2004. Dependency Tree
Kernels for Relation Extraction. In ACL?04, pages
423?429.
MC. De Marneffe, B.Maccartney, and CD. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In In LREC 2006.
G. Erkan, A. O?zgu?r, and DR. Radev. 2007. Semi-
Supervised Classification for Extracting Protein Inter-
action Sentences using Dependency Parsing. In Proc.
of EMNLP?CoNLL?07, pages 228?237.
K. Fundel, R. Ku?ffner, and R. Zimmer. 2007. RelEx ? re-
lation extraction using dependency parse trees. Bioin-
formatics, 23(3):365?371, February.
A. Giuliano, A. Lavelli, and L. Romano. 2006. Ex-
ploiting Shallow Linguistic Information for Relation
Extraction from Biomedical Literature. In Proc. of
EACL?06, pages 401?408, Trento, Italy. The Associ-
ation for Computer Linguistics.
J. Hakenberg, U. Leser, H. Kirsch, and D. Rebholz-
Schuhmann. 2006. Collecting a large corpus from all
of Medline. In SMBM?06, pages 89?92, April.
J. Hakenberg, C. Plake, R. Leaman, M. Schroeder,
and G. Gonzalez. 2008. Inter-species normaliza-
tion of gene mentions with GNAT. Bioinformatics,
24(16):i126?132.
J. Hakenberg, R. Leaman, NH. Vo, S.Jonnalagadda,
R. Sullivan, C. Miller, L. Tari, C. Baral, and G. Gon-
zalez. 2010. Efficient extraction of protein-protein
interactions from full-text articles. IEEE/ACM Trans
Comput Biol Bioinform, 7(3):481?494.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Discov-
ering patterns to extract protein-protein interactions
from the literature: Part II. Bioinformatics, 21:3294?
3300.
J. Kazama, T. Makino, Y. Ohta, and J. Tsujii. 2002.
Tuning support vector machines for biomedical named
entity recognition. In Proc. of BioNLP at ACL?02,
page 8.
S. Kim, J. Yoon, J Yang, and S. Park. 2010. Walk-
weighted subsequence kernels for protein-protein in-
teraction extraction. BMC Bioinformatics, 11(1):107.
R. Kuboyama, K. Hirata, H. Kashima, KF. Aoki-
Kinoshita, and H. Yasuda. 2007. A spectrum tree ker-
nel. Information and Media Technologies, 2(1):292?
299.
H. Liu, V. Keselj, and C. Blouin. 2010. Biological Event
Extraction using Subgraph Matching. In SMBM?10,
October.
M. Miwa, R. S?tre, Y. Miyao, T. Ohta, and J. Tsujii.
2008. Combining multiple layers of syntactic infor-
mation for protein-protein interaction extraction. In
SMBM?08, pages 101?108.
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009.
A Rich Feature Vector for Protein-Protein Interaction
Extraction from Multiple Corpora. In EMNLP?09,
pages 121?130.
S. Pyysalo, A. Airola, J. Heimonen, J. Bjrne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis of
five protein-protein interaction corpora. BMC Bioin-
formatics, 9 Suppl 3:S6.
F. Rinaldi, G. Schneider, K. Kaljurand, S. Clematide,
T. Vachon, and M. Romacker. 2010. Ontogene in
biocreative ii.5. IEEE/ACM Trans Comput Biol Bioin-
form, 7(3):472?480.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
Post: a part-of-speech tagger for bioMedical text.
Bioinformatics, 20(14):2320?2321, Sep.
JM. Temkin and MR. Gilder. 2003. Extraction of protein
interaction information from unstructured text using a
context-free grammar. Bioinformatics, 19(16):2046?
2053, Nov.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of
kernel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6:e1000837.
D. Zhou and Y. He. 2008. Extracting interactions be-
tween proteins from the literature. J Biomed Inform,
41(2):393?407, April.
9
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 35?43,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Improving Distantly Supervised Extraction of Drug-Drug and
Protein-Protein Interactions
Tamara Bobic?,1,2? Roman Klinger,1? Philippe Thomas,3 and Martin Hofmann-Apitius1,2
1Fraunhofer Institute for
Algorithms and Scientific
Computing (SCAI)
Schloss Birlinghoven
53754 Sankt Augustin
Germany
2Bonn-Aachen Center for
Information Technology
Dahlmannstra?e 2
53113 Bonn
Germany
3Computer Science Institut
Humboldt-Universita?t
Unter den Linden 6
10099 Berlin
Germany
{tbobic,klinger,hofmann-apitius}@scai.fraunhofer.de
thomas@informatik.hu-berlin.de
Abstract
Relation extraction is frequently and suc-
cessfully addressed by machine learning
methods. The downside of this approach
is the need for annotated training data, typi-
cally generated in tedious manual, cost inten-
sive work. Distantly supervised approaches
make use of weakly annotated data, like au-
tomatically annotated corpora.
Recent work in the biomedical domain
has applied distant supervision for protein-
protein interaction (PPI) with reasonable
results making use of the IntAct database.
Such data is typically noisy and heuristics
to filter the data are commonly applied. We
propose a constraint to increase the qual-
ity of data used for training based on the
assumption that no self-interaction of real-
world objects are described in sentences.
In addition, we make use of the Univer-
sity of Kansas Proteomics Service (KUPS)
database. These two steps show an increase
of 7 percentage points (pp) for the PPI cor-
pus AIMed. We demonstrate the broad appli-
cability of our approach by using the same
workflow for the analysis of drug-drug in-
teractions, utilizing relationships available
from the drug database DrugBank. We
achieve 37.31 % in F1 measure without man-
ually annotated training data on an indepen-
dent test set.
1 Introduction
Assuming co-mentioned entities to be related is
an approach of extracting relations of real-world
objects with limited precision. Extracting high
quality interaction pairs from free text allows for
?These authors contributed equally.
building networks, e. g. of proteins, which need
less manual curation to serve as a model for further
knowledge processing steps. Nevertheless, just as-
suming co-occurrence to model an interaction or
relation is common, as the development of inter-
action extraction systems can be time-consuming
and complex.
Currently, a lot of relation extraction (RE) sys-
tems rely on machine learning, namely classifying
pairs of entities to be related or not (Airola et al,
2008; Miwa et al, 2009; Kim et al, 2010). De-
spite the fact that machine learning has been most
successful in identifying relevant relations in text,
a drawback is the need for manually annotated
training data. Domain experts have to dedicate
time and effort to this tedious and labor-intensive
process.
Specific biomedical domains have been ex-
plored more extensively than others, thus creating
an imbalance in the number of existing corpora
for a specific RE task. Protein-protein interactions
(PPI) have been investigated the most, which gave
rise to a number of available corpora. Pyysalo et al
(2008) standardized five PPI corpora to a unified
XML format. Recently, a drug-drug-interaction
(DDI) corpus is made available in the same for-
mat, originally for the DDI Extraction Workshop1
(Segura-Bedmar et al, 2011b).
As a consequence of the overall scarcity of an-
notated corpora for RE in the biomedical domain,
the approach of distant supervision, e. g. to auto-
matically label a training set is emerging. Many
approaches make use of the distant supervision as-
sumption (Mintz et al, 2009; Riedel et al, 2010):
1Associated with the conference of the spanish society
for natural language processing (SEPLN) in 2011, http:
//labda.inf.uc3m.es/DDIExtraction2011/
35
If two entities participate in a relation,
all sentences that mention these two en-
tities express that relation.
Obviously, this assumption does not hold in gen-
eral, and therefore exceptions need to be detected
which are not used for training a model. Thomas et
al. (2011b) successfully used simple filtering tech-
niques in a distantly supervised setting to extract
PPI. In contrast to their work, we introduce a more
generic filter to detect frequent exceptions from
the distant supervision assumption and make use
of more data sources, by merging the interaction
information from IntAct and KUPS databases (dis-
cussed in Section 2.1). In addition, we present the
first system (to our knowledge), evaluating distant
supervision for drug-drug interaction with promis-
ing results.
1.1 Related work
Distant supervision approaches have received con-
siderable attention in the past few years. However,
most of the work is focusing on domains other
than biomedical texts.
Mintz et al (2009) use distant supervision to
learn to extract relations that are represented in
Freebase (Bollacker et al, 2008). Yao et al (2010)
use Freebase as a source of supervision, dealing
with entity identification and relation extraction
in a joint fashion. Entity types are restricted to
those compatible with selected relations. Riedel et
al. (2010) argue that distant supervision leads to
noisy training data that hurts precision and suggest
a two step approach to reduce this problem. They
identify the sentences which express the known re-
lations (?expressed-at-least-once? assumption) and
thus frame the problem of distant supervision as
an instance of constraint-driven semi-supervision,
achieving 31 % of error reduction.
Vlachos et al (2009) tackle the problem of
biomedical event extraction. The scope of their
interest is to identify different event types without
using a knowledge base as a source of supervision,
but explore the possibility of inferring relations
from the text based on the trigger words and de-
pendency parsing, without previously annotated
data.
Thomas et al (2011b) develop a distantly la-
beled corpus for protein-protein interaction extrac-
tion. Different strategies are evaluated to select
valuable training instances. Competitive results
are obtained, compared to purely supervised meth-
ods.
Very recent work examines the usability of
knowledge from PharmGKB (Gong et al, 2008)
to generate training sets that capture gene-drug,
gene-disease and drug-disease relations (Buyko et
al., 2012). They evaluate the RE for the three inter-
action classes in intrisic and extrinsic experimental
settings, reaching F1 measure of around 80 % and
up to 77.5 % respectively.
2 Resources
2.1 Interaction Databases
The IntAct database (Kerrien et al, 2012) con-
tains protein-protein interaction information. It is
freely available, manually curated and frequently
updated. It consists of 290,947 binary interaction
evidences, including 39,235 unique pairs of inter-
acting proteins for human species.2
In general, PPI databases are underanno-
tated and the overlap between them is marginal
(De Las Rivas and Fontanillo, 2010). Combining
several databases allows to cover a larger fraction
of known interactions resulting in a more complete
knowledge base. KUPS (Chen et al, 2010) is a
database that combines entries from three manu-
ally curated PPI databases (IntAct, MINT (Chatr-
aryamontri et al, 2007) and HPRD50 (Prasad et al,
2009)) and contains 185,446 positive pairs from
various model organisms, out of which 69,600
belong to human species.3 Enriching IntAct inter-
action information with the KUPS database leads
to 57,589 unique pairs.4
The database DrugBank (Knox et al, 2011)
combines detailed drug data with comprehensive
drug target information. It consists of 6,707 drug
entries. Apart from information about its targets,
for certain drugs known interactions with other
drugs are given. Altogether, we obtain 11,335
unique DDI pairs.
2.2 Corpora
For evaluation of protein-protein interaction, the
five corpora made available by Pyysalo et al
(2008) are used. Their properties, like size and ra-
tio of positive and negative examples, differ greatly,
2As of January 27th, 2012.
3As of August 16th, 2010.
4Only 45,684 out of 69,600 human PPI pairs are available
from the KUPS web service due to computational and storage
limitations (personal communication).
36
Corpus Positive pairs Negative pairs Total
AIMed 1000 (0.17) 4,834 (0.82) 5,834
BioInfer 2,534 (0.26) 7,132 (0.73) 9,666
HPRD50 163 (0.38) 270 (0.62) 433
IEPA 335 (0.41) 482 (0.59) 817
LLL 164 (0.49) 166 (0.50) 330
DDI train 2,400 (0.10) 21,411 (0.90) 23,811
DDI test 755 (0.11) 6,275 (0.89) 7,030
Table 1: Basic statistics of the five PPI and two DDI
corpora. Ratios are given in brackets.
the latter being the main cause of performance dif-
ferences when evaluating on these corpora. More-
over, annotation guidelines and contexts differ:
AIMed (Bunescu et al, 2005) and HPRD50 (Fun-
del et al, 2007) are human-focused, LLL (Nedel-
lec, 2005) on Bacillus subtilis, BioInfer (Pyysalo
et al, 2007) contains information from various or-
ganisms and IEPA (Ding et al, 2002) is made of
sentences that describe 10 selected chemicals, the
majority of which are proteins, and their interac-
tions.
For the purposes of DDI extraction, the corpus
published by Segura-Bedmar et al (2011b) is used.
This corpus is generated from web-documents de-
scribing drug effects. It is divided into a training
and testing set. An overview of the corpora is
given in Table 1.
3 Methods
In this section, the relation extraction system used
for classification of interacting pairs is presented.
Furthermore, the process of generating an automat-
ically labeled corpus is explained in more detail,
along with specific characteristics of the PPI and
DDI task.
3.1 Interaction Classification
We formulate the task of relation extraction as
feature-based classification of co-occurring enti-
ties in a sentence. Those are assigned to be either
related or not, without identifying the type of re-
lation. Our RE system is based on rich feature
vectors and the linear support vector machine clas-
sifier LibLINEAR, which has shown high perfor-
mance (in runtime as well as model accuracy) on
large and sparse data sets (Fan et al, 2008).
The approach is based on lexical features, op-
tionally with dependency parsing features created
using the Stanford parser (Marneffe et al, 2006).
Lexical features are bag-of-words (BOW) and n-
Methods P R F1
Thomas et al (2011a) 60.54 71.92 65.74
Chowdhury et al (2011) 58.59 70.46 63.98
Chowdhury and Lavelli (2011) 58.39 70.07 63.70
Bjo?rne et al (2011) 58.04 68.87 62.99
Minard et al (2011) 55.18 64.90 59.65
Our system (lex) 63.30 52.32 57.28
Our system (lex+dep) 66.46 56.69 61.19
Table 2: Comparison of fully supervised relations ex-
traction systems for DDI. (lex denotes the use of lexi-
cal features, lex+dep the additional use of dependency
parsing-based features.)
grams based, with n ? {1, 2, 3, 4}. They encom-
pass the local (window size 3) and global (window
size 13) context left and right of the entity pair,
along with the area between the entities (Li et al,
2010). Additionally, dictionary based domain spe-
cific trigger words are taken into account.
The respective dependency parse tree is in-
cluded through following the shortest dependency
path hypothesis (Bunescu and Mooney, 2005), by
using the syntactical and dependency information
of edges (e) and vertices (v). So-called v-walks
and e-walks of length 3 are created as well as n
grams along the shortest path (Miwa et al, 2010).
3.2 Automatically Labeling a Corpus in
General
One of the most important source of publications
in the biomedical domain is MEDLINE5, currently
containing more than 21 million citations.6 The
initial step is annotation of named entities ? in
our case performed by ProMiner (Hanisch et al,
2005), a tool proving state-of-the-art results in e. g.
the BioCreative competition (Fluck et al, 2007).
Based on the named entity recognition, only sen-
tences containing co-occurrences are further pro-
cessed. Based on the distant supervision assump-
tion, each pair of entities is labeled as related if
mentioned so in a structured interaction databases.
Note that this requires the step of entity normaliza-
tion.
3.3 Filtering Noise
A sentence may contain two entities of an inter-
acting pair (as known from a database), but does
not describe their interaction. Likewise, a sentence
5http://www.ncbi.nlm.nih.gov/pubmed/
6As of January, 2012.
37
may talk about a novel interaction which has not
been stored in the database. Therefore, filtering
strategies need to be employed to help in decid-
ing which pairs are annotated as being related and
which not.
Thomas et al (2011b) propose the use of trigger
words, i. e., an entity pair of a certain sentence is
marked as positive (related) if the database has in-
formation about their interaction and the sentence
contains at least one trigger word. Similarly, a
negative (non-related) example is a pair of entities
that does not interact according to the database
and their sentence does not contain any trigger
word. Pairs which do not fulfil both constraints are
discarded.
Towards improvement of the heuristics for re-
ducing noise, we introduce the constraint of ?auto-
interaction filtering? (AIF): If entities from an en-
tity pair both refer to the same real-world object,
the pair is labeled as not interacting. Even though
self-interactions are known for proteins and drugs,
such pairs can rarely be observed to describe an
interaction but rather are repeated occurences or
abbreviations. Moreover, the fundamental advan-
tage of AIF is that it requires no additional manual
effort.
3.4 Application on Protein-Protein
Interaction and Drug-Drug Interaction
In biomedical texts there are often mentions of
multiple proteins in the same sentence. However,
this co-occurrence does not necessarily signal that
the sentence is talking about their relation. Hence,
to reduce noise, a list of trigger words specific to
the problem is required. The rationale behind this
filter is that the interaction between two entities is
usually expressed by a specific (trigger) word. For
protein-protein-interactions, we use the trigger list
compiled by Thomas et al (2011b)7. In addition to
using IntAct alone, we introduce the use of KUPS
database (as described in Section 2.2).
For drug-drug-interaction, to our knowledge,
no DDI-specific trigger word list developed by
domain experts is available. Therefore, filtering
via such term occurrences is not applied in this
case.
7http://www2.informatik.hu-berlin.de/
?thomas/pub/2011/iwords.txt
4 Results
In this section, we start with an overview of state-
of-the-art results for fully supervised relation ex-
traction on PPI and DDI corpora (see Table 1).
Furthermore, experimental settings for distant su-
pervision are explained. Finally, we present spe-
cific results for models trained on distantly labeled
data, when evaluated on manually annotated PPI
and DDI corpora.
4.1 Performance overview of supervised RE
systems
Protein-protein interactions has been extensively
investigated in the past decade because of their bio-
logical significance. Machine learning approaches
have shown the best performance in this domain
(e. g. BioNLP (Cohen et al, 2011) and DDIExtrac-
tion Shared Task (Segura-Bedmar et al, 2011a)).
Table 3 gives a comparison of RE systems? per-
formances on 5 PPI corpora, determined by doc-
ument level 10-fold cross-validation.8 The use of
dependency parsing-based features increases the
F1 measure by almost 4 pp.
Table 2 shows results of the five best perform-
ing systems on the held out test data set of the
DDI extraction workshop (Segura-Bedmar et al,
2011b). In addition, the result of our system is
shown. Note that the first three systems use ensem-
ble based methods combining the output of several
different systems.
The results presented in Table 2 and 3 give a
performance overview of the RE system used in
distant learning strategies.
4.2 Experimental Setting
To avoid information leakage and biased classifi-
cation, all documents which are contained in the
test corpus are removed. For each experiment we
sample random subsets to reduce processing time.
This allows us to evaluate the impact of different
combinations of subset size and the ratio of related
and non-related (pos/neg) entity pairs, having in
mind the problem of imbalanced datasets (Chawla
et al, 2004). All experiments are performed five
times to reduce the influence of sampling differ-
ent subsets. This leads to more reliable precision,
recall, and F1 values.
8Separating into training and validation sets is performed
on document level, not on instance (entity pair) level. The
latter could lead to an unrealisticallly optimistic estimate
(Van Landeghem et al, 2008)
38
AIMed BioInfer HPRD50 IEPA LLL
P R F1 P R F1 P R F1 P R F1 P R F1
(Airola et al, 2008) 52.9 61.8 56.4 56.7 67.2 61.3 64.3 65.8 63.4 69.6 82.7 75.1 72.5 87.2 76.8
(Kim et al, 2010) 61.4 53.2 56.6 61.8 54.2 57.6 66.7 69.2 67.8 73.7 71.8 72.9 76.9 91.1 82.4
(Fayruzov et al, 2009) 39.0 34.0 56.0 72.0 76.0
(Liu et al, 2010) 54.7 59.8 64.9 62.1 78.1
(Miwa et al, 2009) 55.0 68.8 60.8 65.7 71.1 68.1 68.5 76.1 70.9 67.5 78.6 71.7 77.6 86.0 80.1
(Tikk et al, 2010) 47.5 65.5 54.5 55.1 66.5 60.0 64.4 67 64.2 71.2 69.3 69.3 74.5 85.3 74.5
Our s. (lex) 62.3 46.3 53.1 59.1 54.3 56.6 69.7 69.4 69.6 67.5 73.2 70.2 66.9 84.6 74.7
Our s. (lex+dep) 65.1 48.6 55.7 64.7 57.6 61.0 69.3 69.8 69.5 67.0 72.5 69.7 71.2 86.3 78.0
Table 3: Comparison of fully supervised relations extraction systems for PPI.
Strategy Pairs Positive pairs Sentences
1 3,304,033 511,665 (0.155) 842,339
2 5,560,975 1,389,036 (0.250) 1,172,920
3 2,764,626 359,437 (0.130) 780,658
4 3,454,805 650,455 (0.188) 896,344
Table 4: Statistics of the fours strategies used in distant
supervision for PPI task: 1) IntAct, 2) IntAct + KUPS,
3) IntAct + AIF, 4) IntAct + KUPS + AIF. Ratios are
given in brackets.
4.3 Protein-protein interaction
We explore four strategies to determine the impact
of using additional database knowledge (IntAct
and KUPS) and to test the utility of our novel
condition (AIF).
Table 4 shows the difference in retrieved num-
ber of sentences and protein pairs, including the
percentage of positive examples in the whole data
set. As expected, by using more background know-
ledge, the number of sentences and instances re-
trieved from MEDLINE rises. An increase of both
negative and positive pairs is observed, since a
relevant sentence can have negative pairs along
with the positive ones. After applying additional
interaction knowledge, the fraction of positive ex-
amples (see 3rd column in Table 4) increases from
15.5 % (IntAct) to 25 % (IntAct+KUPS). However,
employment of the AIF condition to both IntAct
and IntAct+KUPS strategies leads to a reduction
of these values (e. g. fraction of positive examples
reduces from 15.5 % to 13 % and from 25 % to
18.8 %).
For simplicity reasons all runs are performed
using only lexical features.
Table 5 shows the average values of distant super-
vision experiments carried out for the PPI task. A
significant correlation between pos/neg ratio and
precision/recall holds. This clearly indicates the
tendency of classifiers to assign more test instances
to the class more often observed during training.
In accordance with their class distribution, AIMed
reaches highest performance in case of lower frac-
tion of positive instances (i. e. 30 % or 40 %), while
for IEPA and LLL the optimal ratio is in favor of
the positive class (i. e. 70 % or 80 %).
Comparative results of the distant learning
strategies IntAct and IntAct+KUPS tested on five
PPI corpora indicate that additional knowledge
bases do not help per se. Supplementary employ-
ment of the KUPS database leads to a drop in
performances seen in four out of five test cases (a
decrease of 1.7 pp in F1 measure is most notably
observed in case of HPRD50). However, introduc-
tion of the novel filtering condition, in both strate-
gies IntAct+AIF and IntAct+KUPS+AIF, shows
a favorable effect on the precision and leads to an
increase of up to 6 pp in F1 measure, compared to
IntAct and IntAct+KUPS.
Applying AIF to the baseline IntAct increases
F1 measure of AIMed and HPRD50 from 34.4 %
to 37.8 % and from 56.1 % to 59.1 %, respectively.
An even larger impact is observed when compar-
ing IntAct+KUPS and IntAct+KUPS+AIF. For
AIMed, HPRD50 and IEPA an increase of around
6 pp is achieved, while F1 measure of BioInfer
and LLL is improved around 3 pp. Table 5 clearly
shows that IntAct+KUPS+AIF is outperforming
other strategies in all five test cases by achiev-
ing F1 measures of 39.0 % for AIMed, 52.0 % for
BioInfer, 60.2 % for HPRD50, 63.4 % for IEPA
and 69.3 % for LLL.
Analysis of the database (IntAct+KUPS) pairs
reveals that in total there are 5,550 (around 10 %)
proteins that interact with themselves, with 4,918
(89 %) originating from the KUPS database. This
indicates a number of instances that represent auto-
interacting proteins which contribute to increase of
false positives. Such proportion where a majority
of them come from KUPS explains the decrease
39
AIMed BioInfer HPRD50 IEPA LLL
Strategy pos/neg P R F1 P R F1 P R F1 P R F1 P R F1
IntAct
30-70 22.3 75.8 34.4 41.7 54.1 46.9 42.6 73.8 53.9 44.6 70.3 54.5 58.9 63.5 61.0
40-60 21.5 83.5 34.2 40.0 61.9 48.5 42.0 81.7 55.5 44.4 78.0 56.6 55.7 73.3 63.2
50-50 20.8 87.0 33.5 38.7 67.1 49.0 41.4 86.9 56.1 43.7 82.2 57.1 54.6 80.7 65.1
60-40 20.0 90.8 32.8 37.3 72.6 49.2 40.5 91.2 56.1 43.2 85.6 57.4 52.4 86.7 65.3
70-30 19.0 94.5 32.1 35.4 79.5 48.9 39.6 93.4 55.6 42.6 89.3 57.7 50.7 92.1 65.4
80-20 18.6 96.8 31.2 33.5 86.5 48.3 38.6 96.2 55.1 42.1 93.3 58.1 49.4 96.7 65.0
IntAct
+
KUPS
30-70 20.6 48.9 29.0 37.5 30.0 33.3 38.6 45.8 41.8 33.1 25.3 28.6 55.3 25.4 34.6
40-60 21.6 70.3 33.0 39.3 47.4 42.9 40.7 70.2 51.5 41.0 49.6 44.9 58.6 49.3 53.2
50-50 20.8 81.6 33.2 38.2 59.4 46.5 39.6 80.4 53.0 42.9 65.3 51.8 58.5 61.1 59.5
60-40 20.0 89.0 32.7 37.0 68.8 48.2 38.9 87.4 53.8 43.4 76.8 55.4 55.2 74.4 63.2
70-30 19.2 94.3 31.9 35.2 79.1 48.7 38.6 92.3 54.4 42.9 86.2 57.2 52.8 88.5 66.1
80-20 18.3 97.5 30.9 32.2 88.6 47.3 37.8 96.1 54.2 41.9 92.7 57.8 50.8 97.0 66.6
IntAct
+
AIF
30-70 25.1 76.7 37.8 42.8 54.1 47.7 45.7 75.7 57.0 49.9 77.2 60.6 58.4 69.5 63.4
40-60 24.5 78.9 37.4 42.3 56.5 48.3 46.1 79.2 58.3 49.2 79.0 60.7 58.2 72.8 64.6
50-50 23.9 81.1 36.9 42.3 59.2 49.2 45.9 83.1 59.1 49 81.6 61.2 57.8 75.5 65.3
60-40 23.1 83.8 36.1 41.8 63.3 50.3 44.9 85.3 58.8 48.4 84.7 61.6 56.8 79.2 66.1
70-30 22.1 85.8 35.2 40.8 66.4 50.5 43.9 86.5 58.2 47.6 87.9 61.8 56.3 82.1 66.7
80-20 21.3 88.3 34.3 39.6 69.9 50.5 42.9 89.8 58.1 46.0 91.6 61.3 54.0 84.9 66.0
IntAct
+
KUPS
+
AIF
30-70 26.6 72.1 38.8 43.8 50.8 47.0 48.1 78.6 59.7 51.1 75.3 60.9 60.2 63.7 61.8
40-60 26.0 77.8 39.0 43.2 55.4 48.5 47.6 82.5 60.4 50.7 80.6 62.2 58.8 68.7 63.3
50-50 25.5 81.6 38.8 44.8 56.2 49.8 46.0 83.9 59.4 51.4 78.7 62.2 60.3 72.2 65.6
60-40 24.6 84.1 38.0 44.5 60.0 51.1 45.6 88.6 60.2 50.6 83.8 63.1 59.4 77.8 67.3
70-30 23.6 86.7 37.1 43.3 64.4 51.8 44.3 90.5 59.5 49.3 88.8 63.4 59.4 83.3 69.3
80-20 22.1 90.4 35.5 41.0 71.3 52.0 42.5 93.4 58.4 46.8 91.8 62.0 56.2 88.2 68.6
Thomas et al (2011b) 22.3 81.3 35.0 38.7 76.0 51.2 45.6 92.9 61.2 42.6 88.3 57.3 53.7 93.3 68.1
Tikk et al (2010) 28.3 86.6 42.6 62.8 36.5 46.2 56.9 68.7 62.2 71.0 52.5 60.4 79.0 57.3 66.4
Our system 34.3 74.0 46.9 70.8 22.5 34.2 63.3 61.3 62.3 70.0 46.0 55.5 82.4 45.7 58.8
Co-occurrence 17.1 100 29.3 26.2 100 41.5 37.6 100 54.7 41.0 100 58.2 49.7 100 66.4
Table 5: Results achieved with lexical features, trained on 10,000 distantly labeled instances and tested on 5 PPI
corpora.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
AIMed BioInfer HPRD50 IEPA LLL DDI
F 1
Co-occurrence
IntAct/DrugBank
IntAct+KUPS
IntAct+AIF
IntAct+KUPS+AIF
Figure 1: Comparison of four distant learning strategies with co-occurrence baseline. ?IntAct/DrugBank? denotes
the database used as source of supervision for PPI corpora and DDI corpus, respectively.
40
of performance in strategy IntAct+KUPS and the
recovery after applying the AIF condition.
The strategy IntAct+KUPS+AIF results in a
higher quality of data used for training and
achieves the best performance in all five test cases
thus proving the effectiveness of the novel condi-
tion. More knowledge is beneficial, but only when
appropriate filtering of the data is applied.
Distantly supervised systems outperform
co-occurrence results for all five PPI corpora.
Considering the best performing strategy
(IntAct+KUPS+AIF), F1 measure of AIMed and
BioInfer, for which we assume to have the most
realistic pos/neg ratio, increased around 10 pp.
HPRD50, IEPA and LLL have an improvement of
5.5 pp, 5.2 pp and 2.9 pp respectively, due to high
fractions of positive instances (leading to a strong
co-occurrence baseline).
Cross-learning9 evaluation may be more realis-
tic to be compared to distant-learning than cross
validation (Airola et al, 2008). For AIMed and
HPRD50 our approach performs on a par with Tikk
et al (2010) or better (up to 6 pp for BioInfer).
4.4 Drug-drug interaction
The problem of drug-drug interactions has not
been previously explored in terms of distant super-
vision. It is noteworthy that DDI corpora are gener-
ated from web documents discussing drug effects
which are in general not contained in MEDLINE.
Hence, this evaluation corpus can be considered as
out-domain and provides additional insights on the
robustness of distant-supervision. The AIF setting
is not evaluated for the DDI task, because only 1
of all 11,335 unique pairs describes a self interac-
tion. In MEDLINE, only 7 sentences with multiple
mentions of this drug (Sulfathiazole, DrugBank
identifier DB06147) are found.
Table 6 gives an overview of the results for dis-
tant supervision on DDI, with the parameter of
size of the training corpus and the pos/neg ratio. A
slight increase in F1 measure can be observed with
additional training instances, both in case of using
just lexical features and when dependency based
features are additionally utilized (e. g. (lex+dep)
from 36.2 % (5k) to 37.3 % (25k) in F1 measure).
Accounting for dependency parsing features
leads to an increase of 0.5 pp in F1 measure, i. e.
from 36.5 % to 37.0 % (10k) and 36.7%? to 37.3 %
9For five PPI corpora: train on four, test on the remaining.
size pos/neg P R F1
5k
30-70 35.4 32.4 33.7
40-60 33.3 37.0 34.9
50-50 31.9 41.7 36.0
50-50 (lex+dep) 32.7 40.7 36.2
60-40 30.1 46.6 36.5
70-30 27.4 51.8 35.7
10k
30-70 36.0 34.4 34.9
40-60 34.2 38.9 36.3
50-50 32.9 41.0 36.5
50-50 (lex+dep) 33.8 41.1 37.0
60-40 30.8 44.8 36.4
70-30 28.2 48.7 35.6
25k
30-70 35.8 35.0 35.3
40-60 34.3 38.6 36.2
50-50 33.2 41.1 36.7
50-50 (lex+dep) 32.5 43.7 37.3
60-40 31.7 42.6 36.3
70-30 28.9 47.2 35.7
Co-occurrence 10.7 100 19.4
Table 6: Results for distant supervision with only lexi-
cal features on the DDI test corpus.
(25k)), the latter being our best result obtained for
weakly supervised DDI.
Compared to co-occurence, a gain of around
18 pp is achieved. Taking into account the high
class imbalance of the DDI test set (see Table 1),
which is most similar to AIMed corpus, the F1
measure of 37.3 % is encouraging.
Figure 1 shows the results of PPI and DDI experi-
ments in addition. The error bars denote the stan-
dard deviation over 5 differently sampled training
corpora.
5 Discussion
This paper presents the application of distant su-
pervision on the task to find protein-protein inter-
actions and drug-drug interactions. The first is
addressed using the databases IntAct and KUPS,
the second using DrugBank.
More database knowledge does not necessar-
ily have a positive impact on a trained model, ap-
propriate instance selection methods need to be
applied. This is demonstrated with the KUPS
database and the automatic curation via auto-
interaction filtering leading to state-of-the-art re-
sults for weakly supervised protein-protein inter-
action detection.
We present the first results of applying the dis-
tant supervision paradigm to drug-drug-interaction.
41
The results may seem comparatively limited in
comparison to protein-protein interaction, but are
encouraging when taking into account the imbal-
ance of the test corpus and its differing source
domain.
Future development of noise reduction ap-
proaches is important to make use of the full poten-
tial of available database knowledge. The results
shown are encouraging that manual annotation of
corpora can be avoided in other application areas
as well. Another future direction is the investiga-
tion of specifically difficult structures, e. g. listings
and enumerations of entities in a sentence.
Acknowledgments
We would like to thank the reviewers for their
valuable feedback. Thanks to Sumit Madan and
Theo Mevissen for fruitful discussions. T. Bobic?
was partially funded by the Bonn-Aachen Inter-
national Center for Information Technology (B-
IT) Research School. P. Thomas was funded by
the German Federal Ministry of Education and
Research (grant No 0315417B). R. Klinger was
partially funded by the European Community?s
Seventh Framework Programme [FP7/2007-2011]
under grant agreement no. 248726. We acknowl-
edge financial support provided by the IMI-JU,
grant agreement no. 115191 (Open PHACTS).
References
A. Airola, S. Pyysalo, J. Bjo?rne, T. Pahikkala, F. Ginter,
and T. Salakoski. 2008. All-paths Graph Kernel for
Protein-protein Interaction Extraction with Evalua-
tion of Cross-corpus Learning. BMC Bioinformatics,
9(Suppl 11):S2.
J. Bjo?rne, A. Airola, T. Pahikkala, and T. Salakoski.
2011. Drug-drug interaction extraction with RLS
and SVM classiffers. In Challenge Task on Drug-
Drug Interaction Extraction, pages 35?42.
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively created
graph database for structuring human knowledge. In
SIGMOD.
R. C. Bunescu and R. J. Mooney. 2005. A shortest
path dependency kernel for relation extraction. In
HLT and EMNLP.
R. C. Bunescu, R. Ge, R. J. Kate, E. M. Marcotte, R. J.
Mooney, A. K. Ramani, and Y. Wah Wong. 2005.
Comparative experiments on learning information
extractors for proteins and their interactions. Artif
Intell Med, 33(2):139?155, Feb.
E. Buyko, E. Beisswanger, and U. Hahn. 2012. The ex-
traction of pharmacogenetic and pharmacogenomic
relations?a case study using pharmgkb. PSB, pages
376?387.
A. Chatr-aryamontri, A. Ceol, L. M. Palazzi,
G. Nardelli, M.V. Schneider, L. Castagnoli, and
G. Cesareni. 2007. MINT: the Molecular INTer-
action database. Nucleic Acids Res, 35(Database
issue):D572?D574.
N. V. Chawla, N Japkowicz, and A. Kotcz. 2004. Ed-
itorial: special issue on learning from imbalanced
data sets. SIGKDD Explor. Newsl., 6:1?6.
X. Chen, J. C. Jeong, and P. Dermyer. 2010.
KUPS: constructing datasets of interacting and non-
interacting protein pairs with associated attributions.
Nucleic Acids Res, 39(Database issue):D750?D754.
F. M. Chowdhury and A. Lavelli. 2011. Drug-drug
interaction extraction using composite kernels. In
Challenge Task on Drug-Drug Interaction Extrac-
tion, pages 27?33.
F. M. Chowdhury, A. B. Abacha, A. Lavelli, and
P. Zweigenbaum. 2011. Two different machine
learning techniques for drug-drug interaction extrac-
tion. In Challenge Task on Drug-Drug Interaction
Extraction, pages 19?26.
K. B. Cohen, D. Demner-Fushman, S. Ananiadou,
J. Pestian, J. Tsujii, and B. Webber, editors. 2011.
Proceedings of the BioNLP.
J. De Las Rivas and C. Fontanillo. 2010. Protein-
protein interactions essentials: key concepts to build-
ing and analyzing interactome networks. PLoS Com-
put Biol, 6:e1000807+.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining MEDLINE: abstracts, sentences, or
phrases? Pac Symp Biocomput, pages 326?337.
E. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008. LIBLINEAR: A Library for Large Linear
Classification. Machine Learning Research, 9:1871?
1874.
T. Fayruzov, M. De Cock, C. Cornelis, and V. Hoste.
2009. Linguistic feature analysis for protein interac-
tion extraction. BMC Bioinformatics, 10(1):374.
J. Fluck, H. T. Mevissen, H. Dach, M. Oster, and
M. Hofmann-Apitius. 2007. ProMiner: Recognition
of Human Gene and Protein Names using regularly
updated Dictionaries. In BioCreative 2, pages 149?
151.
K. Fundel, R. Kuffner, and R. Zimmer. 2007. Relex-
relation extraction using dependency parse trees.
Bioinformatics, 23(3):365?371.
L. Gong, R. P. Owen, W. Gor, R. B. Altman, and T. E.
Klein. 2008. PharmGKB: an integrated resource of
pharmacogenomic data and knowledge. Curr Protoc
Bioinformatics, Chapter 14:Unit14.7.
D. Hanisch, K. Fundel, H. T. Mevissen, R. Zimmer,
and J. Fluck. 2005. ProMiner: rule-based protein
and gene entity recognition. BMC Bioinformatics,
6(Suppl 1):S14.
42
S. Kerrien, B. Aranda, L. Breuza, A. Bridge,
F. Broackes-Carter, C. Chen, M. Duesbury, M. Du-
mousseau, M. Feuermann, U. Hinz, C. Jandrasits,
R.C. Jimenez, J. Khadake, U. Mahadevan, P. Masson,
I. Pedruzzi, E. Pfeiffenberger, P. Porras, A. Raghu-
nath, B. Roechert, S. Orchard, and H. Hermjakob.
2012. The IntAct molecular interaction database in
2012. Nucleic Acids Res, 40:D841?D846.
S. Kim, J. Yoon, J. Yang, and S. Park. 2010. Walk-
weighted subsequence kernels for protein-protein
interaction extraction. BMC Bioinformatics, 11:107.
C. Knox, V. Law, T. Jewison, P. Liu, S. Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. Chi Guo, and D.S Wishart. 2011.
Drugbank 3.0: a comprehensive resource for ?omics?
research on drugs. Nucleic Acids Res, 39(Database
issue):D1035?D1041.
Y. Li, X. Hu, H. Lin, and Z. Yang. 2010. Learning
an enriched representation from unlabeled data for
protein-protein interaction extraction. BMC Bioin-
formatics, 11(Suppl 2):S7.
B. Liu, L. Qian, H. Wang, and G. Zhou. 2010.
Dependency-driven feature-based learning for ex-
tracting protein-protein interactions from biomedical
text. In COLING, pages 757?765.
M. C. De Marneffe, B. Maccartney, and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC.
A. L. Minard, L. Makour, A. L. Ligozat, and B. Grau.
2011. Feature Selection for Drug-Drug Interac-
tion Detection Using Machine-Learning Based Ap-
proaches. In Challenge Task on Drug-Drug Interac-
tion Extraction, pages 43?50.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In ACL-IJCNLP, pages 1003?1011.
M. Miwa, R. Saetre, Y. Miyao, and J. Tsujii. 2009.
A Rich Feature Vector for Protein-Protein Interac-
tion Extraction from Multiple Corpora. EMNLP,
1(1):121?130.
M. Miwa, R. Saetre, J. D. Kim, and J. Tsujii. 2010.
Event extraction with complex event classification
using rich features. J Bioinform Comput Biol,
8(1):131?146.
C. Nedellec. 2005. Learning language in logic-genic
interaction extraction challenge. In Proc. of the
ICML05 workshop: Learning Language in Logic
(LLL?05), volume 18, pages 97?99.
T. S. Prasad, R. Goel, K. Kandasamy, S. Keerthiku-
mar, S. Kumar, S. Mathivanan, D. Telikicherla,
R. Raju, B. Shafreen, A. Venugopal, L. Balakrish-
nan, A. Marimuthu, S. Banerjee, D. S. Somanathan,
A. Sebastian, S. Rani, S. Ray, C. J. Kishore, S. Kanth,
M. Ahmed, M. K. Kashyap, R. Mohmood, Y. L.
Ramachandra, V. Krishna, B. A.Rahiman, S. Mo-
han, P. Ranganathan, S. Ramabadran, R. Chaerkady,
and A. Pandey. 2009. Human Protein Refer-
ence Database?2009 update. Nucleic Acids Res,
37(Database issue):D767?D772.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. Bioinfer: A
corpus for information extraction in the biomedical
domain. BMC Bioinformatics, 8(50).
S. Pyysalo, A. Airola, J. Heimonen, J. Bjo?rne, F. Gin-
ter, and T. Salakoski. 2008. Comparative analysis
of five protein?protein interaction corpora. BMC
Bioinformatics, 9 Suppl 3:S6.
S. Riedel, L. Yao, and A. McCallum. 2010. Modeling
Relations and Their Mentions without Labeled Text.
In ECML PKDD.
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros, editors. 2011a. Proceedings of the 1st
Challenge Task on Drug-Drug Interaction Extrac-
tion.
I. Segura-Bedmar, P. Mart??nez, and D. Sanchez-
Cisneros. 2011b. The 1st DDIExtraction-2011 chal-
lenge task: Extraction of Drug-Drug Interactions
from biomedical texts. In Challenge Task on Drug-
Drug Interaction Extraction 2011, pages 1?9.
P. Thomas, M. Neves, I. Solt, D. Tikk, and U. Leser.
2011a. Relation Extraction for Drug-Drug Interac-
tions using Ensemble Learning. In Challenge Task
on Drug-Drug Interaction Extraction, pages 11?18.
P. Thomas, I. Solt, R. Klinger, and U. Leser. 2011b.
Learning Protein Protein Interaction Extraction us-
ing Distant Supervision. In Robust Unsupervised
and Semi-Supervised Methods in Natural Language
Processing, pages 34?41.
D. Tikk, P. Thomas, P. Palaga, J. Hakenberg, and
U. Leser. 2010. A comprehensive benchmark of ker-
nel methods to extract protein-protein interactions
from literature. PLoS Comput Biol, 6:e1000837.
S. Van Landeghem, Y. Saeys, B. De Baets, and
Y. Van de Peer. 2008. Extracting protein-protein
interactions from text using rich feature vectors and
feature selection. SMBM, pages 77?84.
A. Vlachos, P. Buttery, D. O? Se?aghdha, and T. Briscoe.
2009. Biomedical Event Extraction without Training
Data. In BioNLP, pages 37?40.
L. Yao, S. Riedel, and A. McCallum. 2010. Collec-
tive Cross-Document Relation Extraction Without
Labeled Data. In EMNLP.
43
