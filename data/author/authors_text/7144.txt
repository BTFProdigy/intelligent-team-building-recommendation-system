A hybrid approach to the development of dialogue systems
directed by semantics
Emilio Sanchis, Isabel Galiano, Fernando Garca, Antonio Cano
Departamento de Sistemas Informaticos y Computacion
Universidad Politecnica de Valencia
Camino de Vera s/n, 46020-Valencia, SPAIN
esanchis,mgaliano,fgarcia,acano@dsic.upv.es
Abstract
In this work we present an approach
to the development of the BA-
SURDE
1
dialogue system, which an-
swers telephone queries about rail-
way timetables in Spanish. We will
focus on the understanding and di-
alogue components which are mod-
eled under a stochastic framework.
The preliminary results from seman-
tic and dialogue interpretations of
user dialogue turns are also included
in this work.
1 Introduction
In the development of dialogue systems many
knowledge sources must be taken into ac-
count. The specic characteristics of each
knowledge source imply that dierent kinds
of models and dierent architectures can be
used. It is widely accepted that stochastic
models are a good representation for some
of these knowledge sources and some spe-
cic works have been done to represent the
semantic of the sentences and the dialogue
structure (Pieraccini et al, 1997)(Baggia et
al., 1999)(Lamel et al, 2000)(Martinez et al,
2000)(Segarra et al, 2001).
We present an approach in which the dia-
logue structure is represented by a stochastic
network of dialogue acts. One advantage of
this kind of network is that it can be learnt
from annotated training samples. Moreover,
it gives us a prediction of the next dialogue
acts which are expected from the user as well
1
Work partially funded by CICYT under project
TIC98-0423-C06
as some information about the possible dia-
logue acts that can be generated by the sys-
tem. The identication of the user dialogue
acts is done through the semantic represen-
tation of the sentence. This semantic inter-
pretation not only supplies the corresponding
dialogue act but also supplies the informa-
tion given about the query constraints, such
us Date, Departure city, etc.
To be able to provide the information re-
quested by the user, the system has to man-
age the values supplied by the user during the
conversation. We do this by means of a record
of current values that is updated after each
user turn and is used to generate the database
queries and to participate in the generation of
the dialogue turns of the system.
2 The Dialogue module
The dialogue model proposed is a stochastic
network which is automatically learnt from
a training set of dialogue samples obtained
by the Wizard of Oz technique (Figure 1).
A dialogue sample is a concatenation of di-
alogue acts which represents the translation
of a given user utterance into a sentence of a
dialogue act language.
One important decision is the denition of
the set of dialogue acts associated to the ap-
plication. If we establish a low number of
dialogue acts that are independent from the
task, we can expect a good modelization of
the dialogue structure and an easy identica-
tion of the dialogue acts which are generated
by the user; we could also change the applica-
tion without having to make many changes in
the dialogue model. However, in order for the
system to generate its dialogue turn, more in-
U:Closing:Nil
M:Closing:Nil
U:Opening:Nil
DataBase Query
DataBase Answer
DataBase
   GENERATION
        ANSWER
Arrival_city:Barcelona
...
Departure_city:Valencia
Date: 23/06/2001
M:Opening:Nil
M:Answer:Departure_time
U:Question:Departure_time
(Departure_time)
M:New_question:Nil
Record _of_Current_Values
Figure 1: An example of a part of the Dialogue model.
formation about the content of the sentences
is required.
If we increase the number of dialogue acts
so that each dialogue act has a more specic
meaning, then the variability of decisions (or
actions) associated to each state in the net-
work is reduced. In other words, a dialogue
act will have a very specic intention, but
we will need a huge number of labeled dia-
logues to learn the model. For example, if
the act is Question there are many kinds of
questions associated to it, but if the act is
Question:Departure time it only represents a
question about the departure time.
Since a balance between the number of la-
bels and the model structure is needed, within
the BASURDE project we have dened a set
of three-level dialogue acts. This set repre-
sents not only information about a general di-
alogue behaviour but also information about
the task (Martinez et al, 2000).
The rst level of each dialogue act labels
the dialogue behaviour. The labels we dene
at this level are generic for any task. The sec-
ond level is related to the semantic represen-
tation of a sentence and it is specic to the
task. In the Dialogue model presented here
only the rst two levels are considered.
The following labels have been de-
ned for the rst level: Opening, Clos-
ing, Undened, Not understood, Waiting,
A?rmation, Rejection, Question, Con-
rmation, Answer. The labels dened
for the second level are: Departure time,
Return departure time, Arrival time, Re-
turn arrival time, Price, Departure city,
Arrival city, Lenght of trip, Stops, Depar-
ture date, Arrival date, Train type, Services.
For example, a dialogue turn can be labeled
as follows:
Me puede decir el horario de los trenes a Valencia
el proximo lunes ?
(Can you tell me the timetable to Valencia for
next Monday ?)
(Question: Departure time)
The stochastic network representing the Di-
alogue model is obtained from a training set
of dialogues which are labeled in terms of dia-
logue act sequences. This network is built by
using the bigram probabilities.
The dialogue act network can be used in
two ways:
- To predict the next dialogue act of the
user; helping the recognition and understand-
ing processes.
- To decide the next action of the system.
As there are not enough samples to learn an
accurate model this decision making process
should be driven by the semantics.
Now we will describe how the dialogue
manager works. It has two main compo-
nents: the dialogue network and the record
of current values. The input of this mod-
ule is supplied by the Understanding module.
This input is a frame representation of the
semantic information obtained from the user
turn. We can extract the corresponding di-
alogue act from each input frame as well as
the constraints about the query given by the
user. The Dialogue Manager uses this infor-
mation in two ways: it determines the next di-
alogue transition to be made and updates the
record of current values using the constraints
obtained from the query.
The Dialogue Manager output, which is
also a frame representation, is sent to the an-
swer generator and then to the synthesizer.
The dynamics of this process is given by the
following Dialogue Manager algorithm:
/*Initialization*/
Put State=Opening
Init(Record of Current Values) /*Init(RCV)*/
Repeat
Sentence=obtain sentence from the user turn
Frame=extract meaning(Sentence)
State=Transition to(State,Frame)
RCV=Update(Frame)
/* actions of the manager */
if complete query(RCV)
then
Send Database query
State=Choose transition
else
select transitions permitted by RCV
State=Choose one of these selected transitions
Generate output frame
until State=Closing
The dialogue manager accepts the frames
obtained from the user turn as input. First
it modies the record of current values if nec-
essary. If there is enough information in this
record, a query to the database is made, an
output frame with the answer is generated,
and a transition in the dialogue network is
made. Otherwise the record of current values
is used to determine which transitions of the
dialogue network should be pruned, i.e. those
that are not compatible with the updated in-
formation. This situation occurs because the
model is learnt from a limited set of samples
and it is a bigram model with just one label
history, and then the constraints given in pre-
vious turns can not be taken into account.
For example, one of the transitions of the
network might imply asking the user about
the departure city and this information has
already been given in a previous turn. In
this case the corresponding transition would
be forbidden. After the set of allowed tran-
sitions is determined, one of them is selected
and the corresponding output frame is gener-
ated. The process nish when a Closing label
is found.
3 The Understanding module
We use frames to represent the meaning of
sentences. Each frame represents a concept
and can have some attributes associated to
it. We have dened 18 types of frames; some
of them are related to the task (for example
Departure time, Price, etc.), and others are
related to the general characteristics of the
dialogues (for example, Not understood, Af-
rmation, etc.). Note that each type of frame
has a corresponding dialogue act associated
to it.
We use a two phases approach for the un-
derstanding process (Figure 2). In the rst
phase a sequence of semantic units and its
corresponding segmentation is obtained from
an input sentence. In the second phase one or
more frames are extracted from this semantic
sentence.
The rst phase is implemented from a
stochastic transduction point of view (Segarra
et al, 2001); that is, the input sentence (in
words) is translated into an output sentence
of a semantic language. The vocabulary of
this semantic language is composed by a set
of semantic units, that represents meanings of
segments of words.
The segmentation of these sentences allows
us to dene two kind of stochastic models:
the Semantic model and the Semantic-Unit
model. The Semantic model represents the
allowed sequences of semantic units and their
probabilities. The Semantic-Unit model rep-
resents the allowed sequences of words and
their probabilities which are associated to
each semantic unit.
These models can be automatically learnt
from a set of annotated training samples. In
this work, we have used bigram models, but
any other type of stochastic model, like n-
grams or automata learnt by Grammatical
Inference techniques (Segarra et al, 2001),
could be used. These models can be inte-
grated into a unique understanding model;
each semantic unit of the Semantic model
ORTOGRAPHIC/
DECODING
SEMANTIC
GENERATION
FRAME
INPUT SENTENCE SECUENCE OF PAIRS 
SEGMENT/SEMANTIC UNIT FRAME
the timetable:<Departure_time>
Can you tell me:Query
to:Destination_marker
Destination_city: Valencia
Departure_date: 14/05/2001
(Departure_time)
Can you tell me the timetable 
to Valencia for next Monday ?
Valencia:Destination_city
for next Monday:Departure_date
Figure 2: Transduction approach in two phases.
is substituted by its corresponding Semantic-
Unit model.
As we already have the Dialogue model of
the system, we can obtain a specic Semantic
model for each user dialogue act. However,
the use of specic models can lead to a lack
of training samples and therefore to the use
of tied models. In our case, we have dened
only specic models for the rst level of our
set of dialogue labels.
A Viterbi decoding algorithm supplies the
most likely sequence of semantic units for the
input sentence and its corresponding segmen-
tation. As the output of the Understanding
module is a frame, a set of rules is applied to
obtain the corresponding frame. These rules
permit us to leave behind semantic units such
as courtesy, markers, etc., and select only the
semantic units that have a corresponding slot
associated to the frame.
4 Experimental Results
In this section we report the preliminary re-
sults from semantic and dialogue interpreta-
tions of user dialogue turns.
We dened a training set of 175 dialogues
with 1,141 user utterances and a test set of
40 dialogues with 268 user utterances from
the orthographic transcription of a set of 215
dialogues, obtained through a Wizard of Oz
technique. The number of words in these two
sets was 11,987 and the medium length of
the utterances was 10.5 words. The percent-
age of correctly understood sentences (correct
frames) was 80%, and the percentage of cor-
rect user dialogue act identication was 87%.
5 Conclusions
We have presented an approach for the devel-
opment of dialogue systems based on stochas-
tic models which are automatically learnt
from training samples. We have proposed a
system architecture which includes an Under-
standing module that extracts the semantics
of the user turns in terms of frames. A prelim-
inary implementation of the system has been
done, and preliminary results are reported.
We hope that the behaviour of the system
improves when we have more dialogue train-
ing samples. We will model other dialogue
situations which were not encountered in our
current training corpus.
References
Baggia P., Kelner A., Perennou E., Popovici C.,
Strum J., Wessel F. 1999. Language Model-
ing and Spoken Dialogue Systems the ARISE
experience. Eurospeech99, pp. 1767{1770.
Bonafonte A., Castell N., LLeida E., Mari~no J.B.,
Sanchis E., Torres M.I., Aibar P. 2000. Desar-
rollo de un sistema de dialogo oral en domin-
ios restringidos. I Jornadas en Tecnologa del
Habla, Sevilla.
Lamel L., Rosset S., Gauvain J.L., Bennacef S.,
Garnier-Rizet M., Prouts B. 2000. The LIMSI
Arise system. Speech Communication,31, pp.
339{353.
Martinez C., and Casacuberta F. 2000. A pattern
recognition approach to dialog labelling using
nite- state transducers. V Iberoamerican Sym-
posium on Pattern Recognition, pp. 669{677.
Pieraccini R, Levin E., and Eckert W. 1997.
AMICA: the AT&T Mixed Initiative Conver-
sational Architecture. Eurospeech97, pp. 1875{
1878.
Segarra E., Sanchis E., Galiano M., Garca F.,
Hurtado L.F. 2001. Extracting semantic infor-
mation through automatic learning techniques.
IX Spanish Symposium on Pattern Recognition
and Image Analysis (AERFAI), Castellon.
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 75?79,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
A methodology for obtaining concept graphs from word graphs
Marcos Calvo, Jon Ander Go?mez, Llu??s-F. Hurtado, Emilio Sanchis
Departament de Sistemes Informa`tics i Computacio?
Universitat Polite`cnica de Vale`ncia
Cam?? de Vera s/n, 46022, Vale`ncia, Spain
{mcalvo,jon,lhurtado,esanchis}@dsic.upv.es
Abstract
In this work, we describe a methodology
based on the Stochastic Finite State Trans-
ducers paradigm for Spoken Language Under-
standing (SLU) for obtaining concept graphs
from word graphs. In the edges of these con-
cept graphs, both semantic and lexical infor-
mation are represented. This makes these
graphs a very useful representation of the in-
formation for SLU. The best path in these con-
cept graphs provides the best sequence of con-
cepts.
1 Introduction
The task of SLU can be seen as the process that,
given an utterance, computes a semantic interpreta-
tion of the information contained in it. This semantic
interpretation will be based on a task-dependent set
of concepts.
An area where SLU systems are typically applied
is the construction of spoken dialog systems. The
goal of the SLU subsystem in the context of a dia-
log system is to process the information given by the
Automatic Speech Recognition (ASR) module, and
provide the semantic interpretation of it to the Dia-
log Manager, which will determine the next action
of the dialog. Thus, the work of the SLU module
can be split into two subtasks, the first of them is
the identification of the sequence of concepts and the
segments of the original sentence according to them,
and the other is the extraction of the relevant infor-
mation underlying to these labeled segments. In this
work we will focus on concept labeling, but we will
also consider the other subtask in our evaluation.
We can distinguish between the SLU systems that
work with the 1-best transcription and those that take
a representation of the n-best (Hakkani-Tu?r et al,
2006; Tur et al, 2002). The use of a word graph as
the input of the SLU module makes this task more
difficult, as the search space becomes larger. On the
other hand, the advantage of using them is that there
is more information that could help to find the cor-
rect semantic interpretation, rather than just taking
the best sentence given by the ASR.
In the recent literature, a variety of approaches for
automatic SLU have been proposed, like those ex-
plained in (Hahn et al, 2010; Raymond and Ric-
cardi, 2007; McCallum et al, 2000; Macherey et
al., 2001; Le?fe`vre, 2007; Lafferty et al, 2001). The
methodology that we propose in this paper is based
on Stochastic Finite State Transducers (SFST). This
is a generative approach that composes several trans-
ducers containing acoustic, lexical and semantic
knowledge. Our method performs this composition
on-the-fly, obtaining as a result a concept graph,
where semantic information is associated with seg-
ments of words. To carry out this step, we use a
different language model for each concept and also
study the use of lexical categorization and lemmas.
The best sequence of concepts can be determined by
finding the best path in the concept graph, with the
help of a language model of sequences of the con-
cepts.
The rest of this paper is structured as follows. In
Section 2, the theoretical model for SLU based on
SFST is briefly presented. Then, in Section 3 the
methodology for converting word graphs into con-
cept graphs is described. A experimentation to eval-
75
uate this methodology for the SLU task is shown in
Section 4. Finally, we draw some conclusions and
future work.
2 The SFST approach for SLU
The Bayes classifier for the SLU problem can be ex-
pressed as stated in equation 1, where C represents
a sequence of concepts or semantic labels and A is
the utterance that constitutes the input to the system.
C? = argmax
C
p(C|A) (1)
Taking into account the underlying sequence of
words W , and assuming that the acoustics may de-
pend onW but not onC, this equation can be rewrit-
ten as follows.
C? = argmax
C
max
W
p(A|W ) ? p(W,C) (2)
To compute the best sequence of concepts C? ex-
pressed as in equation 2, the proposal made by the
paradigm based on SFST is to search the best path in
a transducer ?SLU result of composing four SFST:
?SLU = ?G ? ?gen ? ?W2C ? ?SLM (3)
In this equation ?G is a SFST provided by
the ASR module where the acoustic probabilities
p(A|W ) are represented, ?gen introduces prior in-
formation of the task by means of a lexical cate-
gorization, ?W2C provides the probability of a se-
quence of words and labels it with a semantic label
and ?SLM modelizes a language model of sequences
of concepts.
3 From word graphs to concept graphs
The output of an ASR can be represented as a word
graph. This word graph can be enriched with se-
mantic information, obtaining a concept graph. This
concept graph constitutes a useful representation of
the possible semantics, considering the uncertainty
expressed in the original word graph. Finally, find-
ing the best path in the concept graph using a lan-
guage model of sequences of concepts provides as
a result the best sequence of concepts C?, the recog-
nized sentence W? , and its segmentation according
to C?.
3.1 Topology and semantics of the word graph
To perform the transformations for obtaining the
concept graph, the input graph given by the ASR
should represent the information in the following
way. First, its nodes will be labeled with times-
tamps. Also, for every two nodes i, j such that
i < j ? 1, there will be an edge from i to j labeled
with w and weight s if the ASR detected w between
the instants i and j ? 1 with an acoustic score s.
Finally, there may exist a ?-transition between any
pair of adjacent nodes. The score of this edge should
be computed by means of a smoothing method.
Defining the word graph in this way allows us to
model on it the distribution p(A|w), where A is the
sequence of acoustic frames between the initial and
final nodes of any edge, and w the word attached to
it. This probability distribution is represented in the
theoretical model by ?G.
3.2 Building the concept graph
The concept graph that is obtained has the following
features. First, its set of nodes is the same of the
word graph, and its meaning is kept. There is at most
one edge between every two nodes i and j (i < j)
labeled with the concept c. Every edge is labeled
with a pair (W, c), where W is a sequence of words
and c the concept that they represent. The weight
of the edge is maxW (p(Aji |W ) ?p(W |c)), where Aji
are the acoustic frames in the interval [i, j[ and W
the argument that maximizes the former expression.
In this specification appears the probability distri-
bution p(W |c), which can be estimated by using a
language model for each available concept.
This concept graph can be built using a Dynamic
Programming algorithm that finds for each concept
c and each pair of nodes i, j, with i < j, the
path from i to j on the word graph that maximizes
p(Aji |W )?p(W |c). In this case,W is the sequence of
words obtained by concatenating the words attached
to the edges of the path. Each of the ?best paths?
computed in this way will become an edge in the
resulting concept graph.
Thus, in the concept graph it is represented infor-
mation about possible sequences of words that might
have been uttered by the speaker, along with the con-
cepts each of these sequences expresses. This pair is
weighted with a score that is the result of combining
76
the acoustic score expressed in the word graph, and
the lexical and syntactic score given by the language
model, which is dependent on the current concept.
Furthermore, this information is enriched with tem-
poral information, since the initial and final nodes
of every edge represent the beginning and ending
timestamps of the sequence of words. Consequently,
this way of building the concept graph corresponds
to the transducer ?W2C of equation 3, since we find
sequences of words and attach them to a concept.
However, we also take advantage of and keep other
information, such as the temporal one.
4 Experiments and results
To evaluate this methodology, we have performed
SLU experiments using the concept graphs obtained
as explained in Section 3 and then finding the best
path in each of them. For this experimentation we
have used the DIHANA corpus (Bened?? et al, 2006).
This is a corpus of telephone spontaneous speech in
Spanish composed by 900 dialogs acquired by 225
speakers using the Wizard of Oz technique, with a
total of 6,229 user turns. All these dialogs simulate
real conversations in an automatic train information
phone service. The experiments reported here were
performed using the user turns of the dialogs, split-
ting them into a set of 1,340 utterances (turns) for
test and all the remaining 4,889 for training. Some
interesting statistics about the DIHANA corpus are
given in table 1.
Number of words 47,222
Vocabulary size 811
Average number of words per user turn 7.6
Number of concepts 30
Table 1: Characteristics of the DIHANA corpus.
In the DIHANA corpus, the orthographic tran-
scriptions of the utterances are semi-automatically
segmented and labeled in terms of semantic units.
This segmentation is used by our methodology as a
language model of sequences of words for each con-
cept. All the language models involved in this exper-
imentation are bigram models trained using Witten-
Bell smoothing and linear interpolation.
In our experimentation, we have considered three
different ways for building the ?gen transducer ex-
plained in Section 2. The first way consists of con-
sidering a transducer that given a word as its input,
outputs that word with probability 1. This means
that no generalization is being done.
The second ?gen transducer performs a lexical
categorization of some of the nouns of the vocab-
ulary. Some extra words have been added to some
lexical categories, in order to make the task more
realistic, as the lexical coverage is increased. Never-
theless, it also makes the task harder, as the size of
the vocabulary increases. We have used a total of 11
lexical categories.
Finally, the third ?gen, transducer we have gen-
erated performs the same lexical categorization but
it also includes a lemmatization of the verbs. This
process is normally needed for real-world systems
that work with spontaneous (and maybe telephonic)
speech.
We have generated three sets of word graphs to
take them as the input for the method. The first of
these sets, G1, is made up by the whole graphs ob-
tained from a word graph builder module that works
without using any language model. The Oracle
WER of these graphs is 4.10. With Oracle WER we
mean the WER obtained considering the sequence of
words S(G) corresponding to the path in the graph
G that is the nearest to the reference sentence.
The second set, G2, is composed by word graphs
that only contain the path corresponding to S(G) for
each graph G ? G1. These graphs give an idea of
the best results we could achieve if we could mini-
mize the confusion due to misrecognized words.
The third set, G3 is formed by a synthetic word
graph for each reference sentence, in which only that
sentence is contained. This set of graphs allows us
to simulate an experimentation on plain text.
For our evaluation, we have taken two measures.
First, we have evaluated the Concept Error Rate
(CER) over the best sequence of concepts. The def-
inition of the CER is analogous to that of the WER
but taking concepts instead of words. Second, we
have also evaluated the slot-level error (SLE). The
SLE is similar to the CER but deleting the non-
relevant segments (such as courtesies) and substitut-
ing the relevant concepts by a canonic value for the
sequence of words associated to them.
Tables 2, 3, and 4 show the results obtained using
the different ?gen transducers explained before.
77
Input word graphs CER SLE
G1 31.794 35.392
G2 11.230 9.104
G3 9.933 5.321
Table 2: CER and SLE without any categorization.
Input word graphs CER SLE
G1 34.565 38.760
G2 11.755 8.714
G3 9.633 4.516
Table 3: CER and SLE with lexical categorization.
From the results of Tables 2, 3, and 4 several facts
come to light. First, we can see that, in all the exper-
iments performed with the G1 set, the CER is lower
than the SLE, while with the other sets the CER is
larger than the SLE. It is due to the fact that the
whole graphs obtained from the word graph builder
have more lexical confusion than those from G2 and
G3, which are based on the reference sentence. This
lexical confusion may cause that a well-recognized
concept is associated to a misrecognized sequence
of words. This would imply that a hit would be con-
sidered for the CER calculation, while the value for
this slot is missed.
Other interesting fact is that, for the G1 set, the
more complex ?gen transducers give the worse re-
sults. This is because in these graphs there is a
significant confusion between phonetically similar
words, as the graphs were generated without any
language model. This phonetic confusion, combined
with the generalizations expressed by the lexical cat-
egorization and the lemmas, makes the task harder,
which leads to worse results. Nevertheless, in a real-
world application of this system these generaliza-
tions would be needed in order to have a larger cov-
erage of the lexicon of the language. The experi-
ments on G2 and G3 show that when the confusion
introduced in the graphs due to misrecognized words
is minimized, the use of lexical categorization and
lemmatization helps to improve the results.
5 Conclusions and future work
In this paper we have described a methodology,
based on the SFST paradigm for SLU, for obtaining
Input word graphs CER SLE
G1 36.536 40.640
G2 11.605 8.445
G3 9.458 4.064
Table 4: CER and SLE with lemmatization and lexical
categorization.
concept graphs from word graphs. The edges of the
concept graphs represent information about possible
sequences of words that might have been uttered by
the speaker, along with the concept each of these se-
quences expresses. Each of these edges is weighted
with a score that combines acoustic, lexical, syntac-
tic and semantic information. Furthermore, this in-
formation is enriched with temporal information, as
the nodes represent the beginning and ending of the
sequence of words. These concepts graphs consti-
tute a very useful representation of the information
for SLU.
To evaluate this methodology we have performed
an experimental evaluation in which different types
of lexical generalization have been considered. The
results show that a trade-off between the lexical con-
fusion expressed in the word graphs and the general-
izations encoded in the other transducers should be
achieved, in order to obtain the best results.
It would be interesting to apply this methodology
to word graphs generated with a language model,
although this way of generating the graphs would
not fit exactly the theoretical model. If a language
model is used to generate the graphs, then their lex-
ical confusion could be reduced, so better results
could be achieved. Other interesting task in which
this methodology could help is in performing SLU
experiments on a combination of the output of some
different ASR engines. All these interesting appli-
cations constitute a line of our future work.
Acknowledgments
This work has been supported by the Spanish Mi-
nisterio de Econom??a y Competitividad, under the
project TIN2011-28169-C05-01, by the Vicerrec-
torat d?Investigacio?, Desenvolupament i Innovacio?
de la Universitat Polite`cnica de Vale`ncia, under the
project PAID-06-10, and by the Spanish Ministerio
de Educacio?n under FPU Grant AP2010-4193.
78
References
Jose?-Miguel Bened??, Eduardo Lleida, Amparo Varona,
Mar??a-Jose? Castro, Isabel Galiano, Raquel Justo, In?igo
Lo?pez de Letona, and Antonio Miguel. 2006. Design
and acquisition of a telephone spontaneous speech di-
alogue corpus in Spanish: DIHANA. In Proceedings
of LREC 2006, pages 1636?1639, Genoa (Italy).
S. Hahn, M. Dinarelli, C. Raymond, F. Le?fe`vre,
P. Lehnen, R. De Mori, A. Moschitti, H. Ney, and
G. Riccardi. 2010. Comparing stochastic approaches
to spoken language understanding in multiple lan-
guages. Audio, Speech, and Language Processing,
IEEE Transactions on, 6(99):1569?1583.
D. Hakkani-Tu?r, F. Be?chet, G. Riccardi, and G. Tur.
2006. Beyond ASR 1-best: Using word confusion net-
works in spoken language understanding. Computer
Speech & Language, 20(4):495?514.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning, pages 282?289.
Citeseer.
F. Le?fe`vre. 2007. Dynamic bayesian networks and dis-
criminative classifiers for multi-stage semantic inter-
pretation. In Acoustics, Speech and Signal Processing,
2007. ICASSP 2007. IEEE International Conference
on, volume 4, pages 13?16. IEEE.
K. Macherey, F.J. Och, and H. Ney. 2001. Natural lan-
guage understanding using statistical machine transla-
tion. In European Conf. on Speech Communication
and Technology, pages 2205?2208. Citeseer.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy markov models for information extrac-
tion and segmentation. In Proceedings of the Seven-
teenth International Conference onMachine Learning,
pages 591?598. Citeseer.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language under-
standing. Proceedings of Interspeech2007, Antwerp,
Belgium, pages 1605?1608.
G. Tur, J. Wright, A. Gorin, G. Riccardi, and D. Hakkani-
Tu?r. 2002. Improving spoken language understanding
using word confusion networks. In Proceedings of the
ICSLP. Citeseer.
79
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 193?201,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Exploiting multiple hypotheses for Multilingual Spoken Language
Understanding
Marcos Calvo, Fernando Garc??a, Llu??s-F. Hurtado, Santiago Jime?nez, Emilio Sanchis
Departament de Sistemes Informa`tics i Computacio?
Universitat Polite`cnica de Vale`ncia, Vale`ncia, Spain
{mcalvo,fgarcia,lhurtado,sjimenez,esanchis}@dsic.upv.es
Abstract
In this work, we present an approach for
multilingual portability of Spoken Lan-
guage Understanding systems. The goal
of this approach is to avoid the effort of ac-
quiring and labeling new corpora to learn
models when changing the language. The
work presented in this paper is focused on
the learning of a specific translator for the
task and the mechanism of transmitting the
information among the modules by means
of graphs. These graphs represent a set of
hypotheses (a language) that is the input
to the statistical semantic decoder that pro-
vides the meaning of the sentence. Some
experiments in a Spanish task evaluated
with input French utterances and text are
presented. They show the good behavior
of the system, mainly when speech input
is considered.
1 Introduction
Spoken Language Understanding (SLU) is one of
the key modules in many voice-driven human-
computer interaction systems. Many successful
SLU systems that have been developed in the last
few years are based on statistical models automat-
ically learned from semantically labeled corpora
(Maynard and Lefe`vre, 2001; Segarra et al, 2002;
He and Young, 2006; Lefe`vre, 2007; De Mori et
al., 2008). One of the advantages of statistical
models is the capability of representing the vari-
ability of lexical realizations of concepts (mean-
ings). On the other hand, they are usually plain
models, that is, they can not represent a hierarchi-
cal semantic dependency, although there are some
works in this area (He and Young, 2003). How-
ever, this is not a problem in most Spoken Dialog
Systems since the semantic information to be ex-
tracted is not very hierarchically structured.
Another important aspect of these models is that
they can be learned from corpora. The corpora
used for training must be large enough to allow
an accurate estimation of the probabilities, and it
must represent the lexical and syntactic variabil-
ity that is used in the language to express the se-
mantics as much as possible. Although there are
some approaches based on semi-supervised or un-
supervised learning (Tu?r et al, 2005; Riccardi and
Hakkani-Tu?r, 2005; Ortega et al, 2010), the most
common approaches need to have a segmented
and labeled training corpus. This is the case of
discriminative models (like Conditional Random
Fields (Hahn et al, 2010)), and generative models
(such as Hidden Markov Models and Stochastic
Finite State Automata (Segarra et al, 2002; Hahn
et al, 2010)). In the case of supervised learn-
ing, it is necessary to define a set of concepts that
represent the semantic domain of the task and to
associate these concepts to the corresponding se-
quences of words in the sentences. This is the case
of the French MEDIA corpus (Bonneau-Maynard
et al, 2005), and the Spanish DIHANA corpus
(Bened?? et al, 2006). Since the corpus acquisi-
tion and labeling require a great manual effort, be-
ing able to reuse the corpus generated for a task
to easily develop SLU systems for other tasks, or
languages, is an important issue.
This work focuses on the problem of SLU porta-
bility between languages (Garc??a et al, 2012; He
et al, 2013; Jabaian et al, 2013). We propose a
semi-supervised approach for adapting the system
to tackle sentences that are uttered in a new lan-
guage. In order to learn a domain-specific transla-
tion model, a parallel corpus is automatically gen-
erated from the training set by using web transla-
tors. Due to the fact that the speech recognition
and the translation phases can generate many er-
rors, a mechanism to obtain the correct meaning
despite these errors is needed. This can be per-
formed by supplying many hypotheses between
193
the different stages, either as a set of n sentences
or as a graph that represents not only the original
sentences but also an adequate generalization of
them. This graph can be obtained from a Gram-
matical Inference process. We have also devel-
oped a specific algorithm to perform the semantic
decoding by taking graphs of words as the input
and considering statistical semantic models. We
have applied these techniques for the DIHANA
corpus, which is a task to access the information
of train timetables and fares in Spanish by phone.
This corpus was originally generated in Spanish,
and we have evaluated our system by using input
sentences in French.
2 Description of the system
One way of solving the SLU problem is to find the
sequence of concepts C? that best fits the seman-
tics contained in an utterance A. Considering a
stochastic modelization, it can be stated as:
C? = argmax
C
p(C|A) (1)
In the case of Multilingual SLU, the user utters
a sentence in a source language s, which is dif-
ferent to the language t of the original data of the
SLU task. Thus, either the uttered sentence or the
training data (or maybe both) should be translated
into a common language in order to be able to ap-
ply the semantic decoding process to the input ut-
terance. In our case, we recognize the input ut-
terance by using an Automatic Speech Recognizer
(ASR) in the source language, and we then trans-
late the hypotheses provided by the ASR into the
target language t by means of a statistical Machine
Translation system (see Figure 1). Consequently,
by considering both the input sentence Ws uttered
by the user and its translation into the target lan-
guage Wt, Equation (1) can be rewritten as:
C? = argmax
C
max
Ws,Wt
p(C,Ws,Wt|A) (2)
Equation (2) can be decomposed into several
factors, as shown in Equation (3). This is achieved
by applying the Bayes? Rule and making some
reasonable assumptions about the independence of
the variables.
C? = argmax
C
max
Ws,Wt
p(A|Ws) ? p(Ws|Wt) ? p(Wt|C) ? p(C)
p(A)
(3)
To perform this maximization, we propose a de-
coupled architecture, which sequentially applies
all the knowledge sources. One of the most impor-
tant drawbacks of decoupled architectures is that
the errors generated in one stage can not be recov-
ered in following phases. To overcome this prob-
lem, we propose an architecture in which the com-
munication between the modules is done by means
of structures that provide more than one hypothe-
sis, like n-best and graphs of words. A scheme of
this architecture is shown in Figure 1. Its modules
are the following:
1. First, the input utterance is processed by an
ASR in the source language, providing as its
output either the 1-best or a set of n-best tran-
scriptions. We have used a general purpose,
freely available web ASR, which means that
the ASR has no specific information about
the task.
2. These transcriptions are translated into the
target language by means of a state-of-the-
art Machine Translation system: MOSES
(Koehn et al, 2007). The translation mod-
els have been trained without using any man-
ually generated data. Instead, a set of freely
available web translators was used to trans-
late the training sentences of the corpus from
the target language (the original language of
the corpus sentences) into the source lan-
guage (the language of the speaker), thereby
building a parallel training corpus. MOSES
provides as its output a set of candidate trans-
lations (n-best) of the transcriptions supplied
by the ASR.
3. A graph of words is built from the n-best
provided by the translator. This graph is
built through a Grammatical Inference pro-
cess. This way the graph not only represents
the translations, but also a reasonable gener-
alization of them. This makes it possible for
the semantic decoder to consider some sen-
tences that were not in the initial set but that
are made of pieces of those sentences.
4. This graph of words is processed by a SLU
module that is able to tackle graphs. The se-
mantic model for this stage has been learned
using only the training data in the target lan-
guage. As an intermediate result, this process
builds a graph of concepts, which is a com-
pact representation of all the possible seman-
tics contained in the language represented by
194
Speech Translation
TEST
TRAINING
SLU
SLU 
Stage 1
SLU 
Stage 2
Graph of Concepts
w1  ...wi:Concept1
wi+1...wj:Concept2
...
wm+1...wn:ConceptkGraph of Words
Graph of 
Words
Builder
Frame
Converter
FRAME_NAME
slot1:value1
slot2:value2
...
slotm:valuem
ASR StatisticalTranslator
1-best/ 
n-best
1-best/ 
n-best
Spanish
DIHANA 
Corpus
(Training 
Set)
Translation
Model
Translation
Model Learning
(MOSES)
Semantic
Model
Learning
Web 
Translators
Semantic
Model
Figure 1: Scheme of the decoupled architecture.
the graph of words. The output of this mod-
ule is the best sequence of concepts C? and
also its underlying sequence of words W?t in
the target language and a segmentation of W?t
according to C?.
5. Finally, the segmentation obtained in the pre-
vious step is processed in order to convert
it into a frame representation. This involves
extracting the relevant information from the
segmentation and representing it in a canoni-
cal way.
Assuming that all the translations Wt that be-
long to the language represented by the graph of
words are a priori equiprobable in the target lan-
guage, we can rewrite Equation (3) as follows:
C? = argmax
C
max
Ws,Wt
p(A|Ws) ? p(Ws) ? p(Wt|Ws) ? p(Wt|C) ? p(C)
p(A)
(4)
The first three modules of the architecture can
be viewed as a speech translation process, where
the input is an utterance and the output is a set of
possible translations of this utterance, represented
as a graph of words. Each one of these translations
is weighted with the probability p(Wt|A). Consid-
ering that
p(Wt|A) ? maxWs
p(A|Ws) ? p(Ws) ? p(Wt|Ws)
p(A)
it stands that Equation (4) can be rewritten as:
C? = argmax
C
max
Wt
p(Wt|A) ?p(Wt|C) ?p(C) (5)
The fact that the communication between the
different modules is a set of hypotheses makes it
possible to apply the different constraints (acous-
tic, lexical, syntactic, and semantic) in a global
way, while the modular architecture allows local
pruning taking into account only a subset of the
knowledge sources. This way each of the modules
contributes to the computation of the global max-
imization, but it is not completely performed until
the end of the process.
3 Learning of the translation model
It has been shown that statistical models achieve
good performance in speech translation tasks
(Mathias and Byrne, 2006). Also, they have the
advantage that they can be adapted to a specific
task, as long as a large enough amount of paral-
lel training data is available in order to adequately
train the parameters of the Machine Translation
system. However, obtaining this task-specific
training data by translating the original data by
hand is very expensive and time-consuming. A
solution to this problem is to use several general-
purpose web translators (which are available on
195
the Internet) to automatically translate the task-
specific training sentences into another language.
Although these translators can generate many er-
rors, they are an interesting way to obtain several
hypotheses for a translation without much effort.
However, the use of these translators at testing
time is not very convenient due to the fact that the
system would depend on the Internet connection
and the reaction time of the corresponding web
pages. Another drawback is that it is impossible
to adapt them to a specific task, which could gen-
erate many errors that are important to the task.
The approach that we propose attempts to take
advantage of these resources, but for training pur-
poses. In other words, given the training sentences
in Spanish, they are translated into a new language
(French in this case) by using several web transla-
tors. This way we build a parallel corpus where
each sentence has different translations associated
to it. From this parallel corpus, we train a statisti-
cal translator that is specific for the task. It should
be noted that by means of this process, the learned
translator can represent and modelize the variabil-
ity generated by the different translators. How-
ever, due to the difficulty of the problem, this mod-
elization may not be enough. Therefore we can not
guarantee that the best translation obtained by the
model is consistent with the meaning of the origi-
nal sentence. This is why it is convenient to supply
more than one hypothesis to the semantic decod-
ing module in order to have the possibility of find-
ing the correct semantic meaning even when some
errors were generated in the recognition and trans-
lation processes. We think that separately process-
ing the n-best translated sentences (for each input
sentence) generated by the translator is not the best
solution. In contrast, it would be better to ade-
quately combine segments of different sentences.
Thus, we have developed a Grammatical Inference
mechanism to build a graph of words from a set of
hypotheses as described in the following section.
4 Generating the graphs of words
In this section, the process of obtaining the graphs
of words in the target language from multiple
translation hypotheses is explained. This process
is divided into two steps:
1. The translation hypotheses are aligned using
a Multiple Sequence Alignment (MSA) algo-
rithm. The result of the MSA process is an
alignment matrix.
2. The aligned sentences, represented by the
alignment matrix, are used to obtain a
weighted graph of words that will be the in-
put to the graph-based SLU module.
A Multiple Sequence Alignment is a process of
sequence alignment that involves more than two
sequences. It takes a set of sequences of symbols
(in our case, sequences of words) and provides the
alignment of the elements of the set that minimizes
the number of edit operations (substitutions, in-
sertions, and deletions) among all the symbols of
the sequences. In this work, a modification of the
ClustalW (Larkin et al, 2007) Multiple Sequence
Alignment software has been used.
The result of the MSA process is an alignment
matrix. Each row in this matrix represents a differ-
ent aligned sentence, and each column represents
the alignment of each symbol. The total number
of columns is usually greater than the length of
the longest sequence, since not all the symbols can
be aligned. The special symbol ?-? is used to rep-
resent the positions of non-alignment points in a
sentence.
A weighted directed acyclic graph of words
is created from the MSA alignment matrix, The
graph construction consists of creating as many
nodes as columns in the alignment matrix plus one
for the final state and as many arcs as cells in the
matrix that contain a symbol different to ?-?. The
arcs with the same source, destination, and symbol
are joined, and the weights are obtained by nor-
malizing these counters (Calvo et al, 2012).
Figure 2 shows a real example (extracted from
the test set) of the full process of obtaining the
graph of words. As the figure shows, the obtained
graph of words (where the arcs are labeled with
words and weighted with the normalized coun-
ters) represents a language which is a generaliza-
tion of the individual translations of the original
utterance. That is, this process is a Grammati-
cal Inference mechanism that represents sentences
with characteristics that are similar to those used
to build the graph. A full path from the initial
node to the final node in the graph may be seen
as an alternative translation of the original utter-
ance. For example, the correct translation of the
utterance ?el precio del billete del tren de las seis
treinta y cinco? was not among the candidates pro-
vided, but it can be recovered using this algorithm.
This graph builder module completes the se-
quence of modules that perform the speech trans-
196
source le prix du billet de train de six heures trente-cinq
utterance (the price of the train ticket for six thirty-five)
(el precio del billete del tren de las seis treinta y cinco)
le prix du billet train de sezer trente-cinq
multiple ASR le prix du billet train de six vers trente-cinq
outputs le prix du billet train de six onze trente-cinq
le prix du billet train des six heures trente cinq
el precio del billete de tren de sezer treinta y cinco
multiple el precio del billete de tren alrededor de las seis treinta y cinco
translations el precio del billete del tren de las seis once y treinta y cinco
el precio del billete de tren de las seis treinta de las cinco de la tarde
el precio del billete de tren - de sezer treinta - - - y cinco - - -
alignment el precio del billete de tren alrededor de las seis treinta - - y cinco - - -
matrix el precio del billete del tren - de las seis once y treinta y cinco - - -
el precio del billete de tren - de las seis treinta - de las cinco de la tarde
0 1el(1.00) 2precio(1.00) 3del(1.00) 4billete(1.00) 5de(0.75)del(0.25) 6tren(1.00) 7alrededor(0.25) 8de(0.75) de(1.00) 9las(0.75)sezer(0.25) 10seis(0.75)treinta(0.25) 11once(0.25)treinta(0.50) 14y(0.25)12y(0.33) 13de(0.33) y(0.33)treinta(1.00) las(0.50)y(0.50) 15cinco(0.25) 18cinco(0.75)16de(1.00) 17la(1.00) tarde(1.00)
Figure 2: Steps for obtaining the graph of words from the original utterance le prix du billet de train de
six heures trente-cinq, (the price of the train ticket for six thirty-five).
lation process. This process takes as its input
an utterance and outputs a weighted graph of
words, which represents the probability distribu-
tion p(Wt|A). In other words, each full path in
the graph of words (from the initial to the ending
node) is a candidate translation of the input utter-
ance, and is weighted with the probability of the
translation given the utterance.
5 Performing the semantic decoding
Our semantic decoding process is based on the
idea of finding segments of words contained in
the graph of words that are relevant to each of the
concepts of the task. In order to compactly repre-
sent this set of segments and the concepts they are
relevant to, a second graph is created, which we
have called a graph of concepts. This graph has
the same set of nodes as the graph of words, but
each arc represents that there is a path in the graph
of words between the initial and ending node of
the arc, which induces a sequence of words that is
relevant to some of the concepts of the task. Thus,
each of these arcs is labeled with the correspond-
ing sequence of words and the concept they repre-
sent. To assign a proper weight to the arcs, both
the weights represented in the graph of words and
the semantic model are considered. As the set of
nodes is the same as in the graph of words, we
will say that for every two nodes i, j, it stands that
i < j if i comes before j in the topological or-
der of the nodes in the graph of words (there is
a topological order because the graph of words is
directed and acyclic).
As stated in Equation (5), one of the important
factors in this approach is the probability of the
sequence of words in the target language given
the sequence of concepts p(Wt|C). This proba-
bility can be decomposed as the product of the
probabilities assigned by each concept of the se-
quence of concepts C to the segment of words that
is attached to it; that is,??ck?C p(Wtk |ck), where
Wtk is the sequence of words corresponding to
the concept ck in the segmentation. To compute
these probabilities, our semantic model includes
a set of bigram Language Models (LMs), one for
each concept in the task, which provide the prob-
ability of any sequence of words given the con-
cept. To train these LMs, the training sentences
of the corpus in the target language must be seg-
mented and labeled in terms of the concepts of
the task. The consequence of defining the seman-
tic model this way is that every arc from node i
to node j in the graph of concepts represents the
probability p(W i,jt |A)?p(W i,jt |c), where W i,jt and
c are the sequence of words and the concept at-
tached to the arc, respectively. Furthermore, each
full path (from the initial to the ending node) in
the graph of concepts represents the probability
197
p(Wt|A) ? p(Wt|C), for the sequence of concepts
C and the sentence Wt induced by the path.
The set of arcs of the graph of concepts can be
built by means of a Dynamic Programming (DP)
algorithm that finds the sequence of words that
maximizes the combined probability stated above,
for each pair of nodes i, j and each concept c.
Only the arc that represents the sequence of words
of maximum probability is needed because this in-
formation will afterwards be combined with the
probability of the sequence of concepts to find the
path of maximum probability (see Equation (5)),
and if there are many arcs between nodes i and j
corresponding to the concept c only the one with
maximum probability will be considered. This al-
lows us to prune the arcs of the graph of concepts
without any loss of information. For the DP al-
gorithm, we will consider a representation of the
LM corresponding to each concept as a Stochastic
Finite State Automaton (SFSA). Then, in the DP
process, for each concept c we will obtain the best
path from node i to node j in the graph of words
such that its underlying sequence of words arrives
to the state qc in the SFSA LMc (the LM of the
concept c). This can be achieved by means of the
following algorithm:
M(i, j, qc) =
?
?????
?????
1 if i = j ? qc is the initial state of LMc
0 if i = j ? qc is not the initial state of LMc
0 if j < i
max
?a?EGW :dest(a)=j
?(q?c,wd(a),qc)?LMc
M(i, src(a), q?c) ? p(q?c,wd(a), qc) ? wt(a)
otherwise
(6)
where dest(a) stands for the destination node of
the arc a in the graph of words, src(a) refers to its
source node, and wd(a) and wt(a) refer to the word
and the weight attached to the arc, respectively.
Also, (q?c,wd(a), qc) represents a transition from
the state q?c to the state qc labeled with wd(a) in
the SFSA that represents LMc.
It is worth noting that this process must be per-
formed for each concept in the task. Also, it is im-
portant for the algorithm to keep track of the words
that constitute the paths that maximize the expres-
sion for each cell. When this matrix has been
filled for a specific concept c, the cell that max-
imizes M(i, j, qc) for each pair i and j becomes
an arc in the graph of concepts between nodes i
and j. This arc is labeled with the sequence un-
derlying the winning path and the concept c and is
weighted with the score (probability) contained in
M(i, j, qc).
This process shapes the first stage of the SLU
process, which provides the graph of concepts as
a result. Then, this graph of concepts is processed
by a second stage. This second stage finds the path
in the graph that maximizes the combination of its
probability and the probability that a LM of bi-
grams of concepts gives to the sequence of con-
cepts underlying the path. The LM of bigrams of
concepts is also part of the semantic model, and
to train it we take advantage of the segmentation
and labeling in term of concepts provided by the
training corpus. Finding the best path this way
completely fulfills what is stated in Equation (5).
Also, this best path in the graph of concepts pro-
vides the best sequence of concepts C?, the under-
lying sequence of words W?t, and a segmentation
of W?t according to C?.
6 The DIHANA task and the semantic
representation
The DIHANA task consists of a telephone-based
information service for trains in Spanish. A set
of 900 dialogs was acquired by using the Wizard
of Oz technique. The number of user turns was
6,280 and the vocabulary was 823. As in many
other dialog systems (Minker, 1999), the semantic
representation chosen for the task is based on a
frame representation. Therefore, the final output
of the understanding process is one or more frames
with their corresponding attributes.
Even though the frame representation is the out-
put of the system, we propose an intermediate se-
mantic labeling that consists of assigning concepts
to segments of the sentence in a sequential way.
This is the output provided by the graph-based
SLU module.
In order to represent the meaning of the utter-
ances in terms of this intermediate semantic lan-
guage, a set of 31 concepts was defined. Some
of them are: query, affirmation, origin city, and
courtesy.
Each concept represents the meaning of words
(or sequences of words) in the sentences. For ex-
ample, the semantic unit query can be associated
to ?can you tell me?, ?please tell me?, ?what is?,
etc. This way, each sentence (sequence of words)
has a semantic sentence (sequence of concepts) as-
sociated to it, and there is an inherent segmenta-
tion. The advantage of this kind of representation
is that statistical models of the lexical realization
of concepts and the n-gram probabilities of the se-
198
Sentence hola buenos d??as quer??a saber los horarios de trenes para ir a Madrid
(hello good morning I?d like to know the train timetables to go to Madrid)
Semantic hola buenos d??as : courtesy
segments quer??a saber : query
los horarios de trenes para ir : <time>
a Madrid : destination city
Frame (TIME?)
DEST CITY : Madrid
Table 1: Example of the outputs of the SLU and Frame Converter modules.
quences of semantic units can be learned.
Finally, a set of rules are used to transduce this
intermediate representation into a frame. Since the
intermediate language is close to the frame repre-
sentation, only a small set of rules are required to
build the frame. This phase consists of the fol-
lowing: the deletion of irrelevant segments (such
as courtesies), the reordering of the relevant con-
cepts and attributes that appeared in the segmenta-
tion following an order which has been defined a
priori, the automatic instantiation of certain task-
dependent values, etc.
Table 1 shows an example of the semantic rep-
resentation in terms of the intermediate semantic
segmentation provided by the SLU module and the
final frame representation.
7 Experiments and results
To evaluate this architecture, we performed a set
of experiments with the DIHANA corpus. The
user turns of the corpus were split into a set of
4889 turns for training and 1227 turns for test. To
train the translation models, the training set was
automatically translated from Spanish into French
by four freely available web translators (Apertium,
Bing, Google, Lucy), which provided us a parallel
training corpus. The semantic model was learned
from the segmentation and labeling provided in
the DIHANA corpus for the training sentences in
Spanish. All the Language Models in the semantic
model were bigram models trained using Witten-
Bell smoothing.
For evaluation purposes, all the test set was
manually translated into French, and 500 turns
were uttered by four native French speakers. Thus,
we have carried out experiments both considering
as the input to our system the correct sentences in
French (which is the same than assuming a perfect
ASR) and the utterances. To recognize the utter-
ances the Google ASR was used, which for this
test set provides a Word Error Rate of 21.9% con-
sidering only the 1-best recognized sentence.
For this experimentation we have considered
three kinds of ASR outputs, namely, a Perfect ASR
(text input), the 1-best output, and finally the n-
best hypotheses (with n ranging from 1 to 20).
Also, we have configured the system in two dif-
ferent ways:
? Configuration 1: The output of the statistical
translation system are the n-best translations
for the input. Note that these n-best could
contain repeated translations, which may lead
to the reinforcement of some paths in the
graphs of words.
? Configuration 2: The output of the statistical
translation system is the set formed by the
best n different (unique) translations that it
can provide for the given input.
When the output of the ASR are n-best, we have
only considered the Configuration 1.
We have evaluated each experiment using two
measures: the Concept Error Rate (CER), which
corresponds to errors in the output of the SLU
module, and the Frame-Slot Error Rate (FSER),
which corresponds to errors in the slots of the
frames in the final output of the system.
Figures 3, 4, and 5 show the results obtained for
each of the ASR outputs and configurations con-
sidered. The horizontal axis represents the number
of hypotheses provided by the statistical translator.
As expected, in all the cases the FSER is lower
than the CER, as some errors at level of the con-
cept sequence are not relevant for the frame con-
version (for example, courtesies). In the case of
text input (Fig. 3), the best results are achieved
when just one or two hypotheses are provided
by the translator. This is because the translation
model has also been learned using correct sen-
tences, which makes the translation system more
robust for this kind of input. However, when con-
sidering speech as input (Figs. 4 and 5), the gen-
eralization provided by the graphs obtained using
a relatively large set of n-best translations leads to
199
a better behavior. This is due to the fact that the
errors introduced by the recognition of the speech
input increases the errors in the translation stage.
Thus, working with different alternatives makes it
possible to recover some of the errors. Table 2
shows the results obtained when optimizing the
FSER, and the number of hypotheses n used to
build the graphs that provide the best results.
Figures 3 and 4 also show that the parameters
that optimize FSER and CER may not be the same.
This behavior is due to the different nature of both
measures. While CER is defined in terms of the
sequence of concepts extracted by the SLU mod-
ule, FSER only takes into account those segments
that have relevant information.
It can be seen in Figures 3 and 4 that, for Con-
figuration 2, when n takes the value 18, both error
measures descend. However, after this, the errors
continue with their ascending tendency. The rea-
son for this is that with these parameters, the trans-
lations provided by the translator generate a graph
of words that allows the semantic model to better
recover the semantics of the sentence. However,
this effect is spurious, as for higher values of n the
error measures present higher values.
 15
 20
 25
 1  5  10  15  20
err
or
n-best
CER-Text Config. 1FSER-Text Config. 1CER-Text Config. 2FSER-Text Config. 2
Figure 3: Results obtained with the text input.
ASR output Config. CER FSER n
Text input Config. 1 21.50 14.03 2
Config. 2 21.37 14.08 1
1-best Config. 1 24.27 19.11 3
Config. 2 24.13 19.28 3
n-best Config. 1 22.40 19.63 7
Table 2: Results obtained optimizing the FSER.
8 Conclusions
We have presented an approach for developing
multilingual SLU systems without any manual ef-
 15
 20
 25
 1  5  10  15  20
err
or
n-best
CER-Voice (1-best) Config. 1FSER-Voice (1-best) Config. 1CER-Voice (1-best) Config. 2FSER-Voice (1-best) Config. 2
Figure 4: Results obtained with the voice input,
taking the 1-best from the ASR and the n-best
from MOSES.
 15
 20
 25
 1  5  10  15  20
err
or
n-best
CER-Voice (N-best) Config. 1FSER-Voice (N-best) Config. 1
Figure 5: Results obtained with the voice input,
taking the n-best from the ASR and the corre-
sponding 1-best from MOSES.
fort in the adaptation of the models. It has been
shown that the use of graphs of words, as a mech-
anism of generalization and transmission of hy-
potheses, is a good approach to recover from er-
rors generated in the different phases of the sys-
tem. As future work it may be interesting to ex-
plore other Grammatical Inference techniques to
combine the n-best hypotheses generated by both
the ASR and the translator. It would also be inter-
esting to study the behavior of this approach with
other languages that have greater differences than
Spanish and French, for example non-Latin lan-
guages like English and German.
Acknowledgements
This work is partially supported by the Spanish
MICINN under contract TIN2011-28169-C05-01,
and under FPU Grant AP2010-4193.
200
References
Jose?-Miguel Bened??, Eduardo Lleida, Amparo Varona,
Mar??a-Jose? Castro, Isabel Galiano, Raquel Justo,
In?igo Lo?pez de Letona, and Antonio Miguel. 2006.
Design and acquisition of a telephone spontaneous
speech dialogue corpus in Spanish: DIHANA.
In Proceedings of LREC 2006, pages 1636?1639,
Genoa (Italy).
H. Bonneau-Maynard, Sophie Rosset, C. Ayache,
A. Kuhn, and Djamel Mostefa. 2005. Semantic an-
notation of the French MEDIA dialog corpus. In
Proc. of InterSpeech 2005, pages 3457?3460, Portu-
gal.
Marcos Calvo, Llu??s-F Hurtado, Fernando Garc??a, and
Emilio Sanchis. 2012. A Multilingual SLU System
Based on Semantic Decoding of Graphs of Words.
In Advances in Speech and Language Technologies
for Iberian Languages, pages 158?167. Springer.
R. De Mori, F. Bechet, D. Hakkani-Tu?r, M. McTear,
G. Riccardi, and G. Tu?r. 2008. Spoken language
understanding: A survey. IEEE Signal Processing
magazine, 25(3):50?58.
F. Garc??a, L.-F. Hurtado, E. Segarra, E. Sanchis, and
G. Riccardi. 2012. Combining multiple translation
systems for spoken language understanding porta-
bility. In Spoken Language Technology Workshop
(SLT), 2012 IEEE, pages 194?198. IEEE.
S. Hahn, M. Dinarelli, C. Raymond, F. Lefe`vre,
P. Lehnen, R. De Mori, A. Moschitti, H. Ney, and
G. Riccardi. 2010. Comparing stochastic ap-
proaches to spoken language understanding in multi-
ple languages. IEEE Transactions on Audio, Speech,
and Language Processing, 6(99):1569?1583.
Yulan He and S. Young. 2003. Hidden vector
state model for hierarchical semantic parsing. In
Proceedings of IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP
?03), volume 1, pages 268?271.
Yulan He and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication, 48:262?275.
Xiaodong He, Li Deng, Dilek Hakkani-Tur, and
Gokhan Tur. 2013. Multi-style adaptive training for
robust cross-lingual spoken language understanding.
In Proc. ICASSP.
B. Jabaian, L. Besacier, and F. Lefe`vre. 2013.
Comparison and combination of lightly supervised
approaches for language portability of a spoken
language understanding system. Audio, Speech,
and Language Processing, IEEE Transactions on,
21(3):636?648.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
Moran C., R. Zens, C. Dyer, Bojar O., A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL?07), pages 177?180.
M. A. Larkin, G. Blackshields, N. P. Brown,
R. Chenna, P. A. McGettigan, H. McWilliam,
F. Valentin, I. M. Wallace, A. Wilm, R. Lopez, J. D.
Thompson, T. J. Gibson, and D. G. Higgins. 2007.
ClustalW and ClustalX version 2.0. Bioinformatics,
23(21):2947?2948.
F. Lefe`vre. 2007. Dynamic bayesian networks and dis-
criminative classifiers for multi-stage semantic in-
terpretation. In Proceedings of IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP?07), volume 4, pages 13?16. IEEE.
Lambert Mathias and William Byrne. 2006. Statis-
tical phrase-based speech translation. In Proceed-
ings of IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP?06), vol-
ume 1, pages 561?564. IEEE.
H. Bonneau Maynard and F. Lefe`vre. 2001. Investi-
gating Stochastic Speech Understanding. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU?01).
W. Minker. 1999. Stocastically-based semantic analy-
sis. In Kluwer Academic Publishers, Boston, USA.
Luc??a Ortega, Isabel Galiano, Llu??s-F. Hurtado, Emilio
Sanchis, and Encarna Segarra. 2010. A statistical
segment-based approach for spoken language under-
standing. In Proc. of InterSpeech 2010, pages 1836?
1839, Makuhari, Chiba, Japan.
G. Riccardi and D. Hakkani-Tu?r. 2005. Active learn-
ing: theory and applications to automatic speech
recognition. IEEE Transactions on Speech and Au-
dio Processing, 13(4):504 ? 511.
E. Segarra, E. Sanchis, M. Galiano, F. Garc??a, and
L. Hurtado. 2002. Extracting Semantic Information
Through Automatic Learning Techniques. IJPRAI,
16(3):301?307.
Gokhan Tu?r, Dilek Hakkani-Tu?r, and Robert E.
Schapire. 2005. Combining active and semi-
supervised learning for spoken language under-
standing. In Speech Communication, volume 45,
pages 171?186.
201
