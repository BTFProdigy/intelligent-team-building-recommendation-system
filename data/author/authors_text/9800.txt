Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1096?1104,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
The Feature Subspace Method for SMT System Combination 
 
Nan Duan1, Mu Li2, Tong Xiao3, Ming Zhou2 
  1Tianjin University       2Microsoft Research Asia       3Northeastern University 
     Tianjin, China                    Beijing, China                     Shenyang, China 
{v-naduan,muli,v-toxiao,mingzhou}@microsoft.com 
 
 
Abstract 
Recently system combination has been shown 
to be an effective way to improve translation 
quality over single machine translation sys-
tems. In this paper, we present a simple and ef-
fective method to systematically derive an en-
semble of SMT systems from one baseline li-
near SMT model for use in system combina-
tion. Each system in the resulting ensemble is 
based on a feature set derived from the fea-
tures of the baseline model (typically a subset 
of it). We will discuss the principles to deter-
mine the feature sets for derived systems, and 
present in detail the system combination mod-
el used in our work. Evaluation is performed 
on the data sets for NIST 2004 and NIST 2005 
Chinese-to-English machine translation tasks. 
Experimental results show that our method can 
bring significant improvements to baseline 
systems with state-of-the-art performance. 
1 Introduction 
Research on Statistical Machine Translation 
(SMT) has shown substantial progress in recent 
years. Since the success of phrase-based methods 
(Och and Ney, 2004; Koehn, 2004), models 
based on formal syntax (Chiang, 2005) or lin-
guistic syntax (Liu et al, 2006; Marcu et al, 
2006) have also achieved state-of-the-art perfor-
mance. As a result of the increasing numbers of 
available machine translation systems, studies on 
system combination have been drawing more and 
more attention in SMT research. 
 There have been many successful attempts to 
combine outputs from multiple machine transla-
tion systems to further improve translation quali-
ty. A system combination model usually takes n-
best translations of single systems as input, and 
depending on the combination strategy, different 
methods can be used. Sentence-level combina-
tion methods directly select hypotheses from 
original outputs of single SMT systems (Sim et 
al., 2007; Hildebrand and Vogel, 2008), while 
phrase-level or word?level combination methods 
are more complicated and could produce new 
translations different from any translations in the 
input (Bangalore et al, 2001; Jayaraman and La-
vie, 2005; Matusov et al, 2006; Sim et al, 
2007). 
 Among all the factors contributing to the suc-
cess of system combination, there is no doubt 
that the availability of multiple machine transla-
tion systems is an indispensable premise. Al-
though various approaches to SMT system com-
bination have been explored, including enhanced 
combination model structure (Rosti et al, 2007), 
better word alignment between translations 
(Ayan et al, 2008; He et al, 2008) and improved 
confusion network construction (Rosti et al, 
2008), most previous work simply used the en-
semble of SMT systems based on different mod-
els and paradigms at hand and did not tackle the 
issue of how to obtain the ensemble in a prin-
cipled way. To our knowledge the only work 
discussed this problem is Macherey and Och 
(2007), in which they experimented with build-
ing different SMT systems by varying one or 
more sub-models (i.e. translation model or dis-
tortion model) of an existing SMT system, and 
observed that changes in early-stage model train-
ing introduced most diversities in translation 
outputs.  
In this paper, we address the problem of build-
ing an ensemble of diversified machine transla-
tion systems from a single translation engine for 
system combination. In particular, we propose a 
novel Feature Subspace method for the ensemble 
construction based on any baseline SMT model 
which can be formulated as a standard linear 
function. Each system within the ensemble is 
based on a group of features directly derived 
from the baseline model with minimal efforts 
(which is typically a subset of the features used 
in the baseline model), and the resulting system 
is optimized in the derived feature space accor-
dingly. 
We evaluated our method on the test sets for 
NIST 2004 and NIST 2005 Chinese-to-English 
1096
machine translation tasks using two baseline 
SMT systems with state-of-the-art performance. 
Experimental results show that the feature sub-
space method can bring significant improve-
ments to both baseline systems. 
The rest of the paper is organized as follows. 
The motivation of our work is described on Sec-
tion 2. In Section 3, we first give a detailed de-
scription about feature subspace method, includ-
ing the principle to select subspaces from all 
possible options, and then an n-gram consensus ?
based sentence-level system combination method 
is presented. Experimental results are given in 
Section 4. Section 5 discusses some related is-
sues and concludes the paper. 
2 Motivation 
Our motivations for this work can be described 
in the following two aspects. 
The first aspect is related to the cost of build-
ing single systems for system combination. In 
previous work, the SMT systems used in combi-
nation differ mostly in two ways. One is the un-
derlying models adopted by individual systems. 
For example, using an ensemble of systems re-
spectively based on phrase-based models, hierar-
chical models or even syntax-based models is a 
common practice. The other is the methods used 
for feature function estimation such as using dif-
ferent word alignment models, language models 
or distortion models. For the first solution, build-
ing a new SMT system with different methodol-
ogy is by no means an easy task even for an ex-
perienced SMT researcher, because it requires 
not only considerable effects to develop but also 
plenty of time to accumulate enough experiences 
to fine tune the system. For the second alterna-
tive, usually it requires time-consuming re-
training for word alignment or language models. 
Also some of the feature tweaking in this solu-
tion is system or language specific, thus for any 
new systems or language pairs, human engineer-
ing has to be involved. For example, using dif-
ferent word segmentation methods for Chinese 
can generate different word alignment results, 
and based on which a new SMT system can be 
built. Although this may be useful to combina-
tion of Chinese-to-English translation, it is not 
applicable to most of other language pairs. 
Therefore it will be very helpful if there is a 
light-weight method that enables the SMT sys-
tem ensemble to be systematically constructed 
based on an existing SMT system. 
 
Source 
sentence 
?? ?? ?? ? ?? ?? 
?? ?? ?? 
Ref 
translation 
China's largest sea water desalini-
zation project settles in Zhoushan 
Default 
translation 
China 's largest desalination  
project in Zhoushan 
??????  
translation 
China 's largest sea water  
desalination project in Zhoushan 
Table 1: An example of translations generated 
from the same decoder but with different feature 
settings. 
 Chinese English ? ? ?  
1 ?? ?? desalination 0.4000 
2 ?? sea water 0.1748 
3 ?? desalination 0.0923 
Table 2: Parameters of related phrases for exam-
ples in Table 1. 
The second aspect motivating our work comes 
from the subspace learning method in machine 
learning literature (Ho, 1998), in which an en-
semble of classifiers are trained on subspaces of 
the full feature space, and final classification re-
sults are based on the vote of all classifiers in the 
ensemble. Lopez and Resnik (2006) also showed 
that feature engineering could be used to over-
come deficiencies of poor alignment. To illu-
strate the usefulness of feature subspace in the 
SMT task, we start with the example shown in 
Table 1. In the example, the Chinese source sen-
tence is translated with two settings of a hierar-
chical phrase-based system (Chiang, 2005). In 
the default setting all the features are used as 
usual in the decoder, and we find that the transla-
tion of the Chinese word ??  (sea water) is 
missing in the output. This can be explained with 
the data shown in Table 2. Because of noises and 
word alignment errors in the parallel training 
data, the inaccurate translation phrase 
?? ?? ? ????????????  is assigned with a 
high value of the phrase translation probability 
feature ?(?|?). Although the correct translation 
can also be composed by two phrases ?? ?
??? ????? and ?? ? ????????????, its over-
all translation score cannot beat the incorrect one 
because the combined phrase translation proba-
bility of these two phrases are much smaller 
than  ?(????????????|?? ??) . However, if 
we intentionally remove the ?(?|?) feature from 
the model, the preferred translation can be gener-
ated as shown in the result of ??????  because in 
1097
this way the bad estimation of ?(?|?)  for this 
phrase is avoided. 
This example gives us the hint that building 
decoders based on subspaces of a standard model 
could help with working around some negative 
impacts of inaccurate estimations of feature val-
ues for some input sentences. The subspace-
based systems are expected to work similarly to 
statistical classifiers trained on subspaces of a 
full feature space ? though the overall accuracy 
of baseline system might be better than any indi-
vidual systems, for a specific sentence some in-
dividual systems could generate better transla-
tions. It is expected that employing an ensemble 
of subspace-based systems and making use of 
consensus between them will outperform the 
baseline system. 
3 Feature Subspace Method for SMT 
System Ensemble Construction 
In this section, we will present in detail the me-
thod for systematically deriving SMT systems 
from a standard linear SMT model based on fea-
ture subspaces for system combination. 
3.1 SMT System Ensemble Generation 
Nowadays most of the state-of-the-art SMT sys-
tems are based on linear models as proposed in 
Och and Ney (2002). Let ?? (?, ?) be a feature 
function, and ??  be its weight, an SMT model ? 
can be formally written as: 
?? = argmax
?
 ???? (?, ?)
?
 (1) 
Noticing that Equation (1) is a general formu-
lation independent of any specific features, tech-
nically for any subset of features used in ? , a 
new SMT system can be constructed based on it, 
which we call a sub-system. 
Next we will use ? to denote the full feature 
space defined by the entire set of features used 
in ?, and ? ? ? is a feature subset that belongs 
to ?(?), the power set of ?. The derived sub-
system based on subset ? ? ? is denoted by ?? . 
Although in theory we can use all the sub-
systems derived from every feature subset 
in ?(?), it is still desirable to use only some of 
them in practice. The reasons for this are two-
fold. First, the number of possible sub-systems 
(2 ? ) is exponential to the size of ?. Even when 
the number of features in ? is relatively small, 
i.e. 10, there will be up to 1024 sub-systems in 
total, which is a large number for combination 
task. Larger feature sets will make the system 
combination practically infeasible. Second, not 
every sub-system could contribute to the system 
combination. For example, feature subsets only 
containing very small number of features will 
lead to sub-systems with very poor performance; 
and the language model feature is too important 
to be ignored for a sub-system to achieve reason-
ably good performance. 
In our work, we only consider feature sub-
spaces with only one difference from the features 
in ?. For each non- language model feature ?? , a 
sub-system ??  is built by removing ??  from  ? . 
Allowing for the importance of the language 
model (LM) feature to an SMT model, we do not 
remove any LM feature from any sub-system. 
Instead, we try to weaken the strength of a LM 
feature by lowering its n-gram order. For exam-
ple, if a 4-gram language model is used in the 
baseline system ?, then a trigram model can be 
used in one sub-system, and a bigram model can 
be used in another. In this way more than one 
sub-system can be derived based on one LM fea-
ture. When varying a language model feature, the 
one-feature difference principle is still kept: if 
we lower the order of a language model feature, 
no other features are removed or changed.  
The remaining issue of using weakened LM 
features is that the resulting ensemble is no long-
er strictly based on subspace of ?. However, this 
theoretical imperfection can be remedied by in-
troducing ?? , a super-space of ? to include all 
lower-order LM features. In this way, an aug-
mented baseline system ??  can be built based 
on  ?? , and the baseline system ? itself can also 
be viewed as a sub-system of ??. We will show 
in the experimental section that ??  actually per-
forms even slightly better than the original base-
line system ?, but results of sub-system combi-
nation are significantly better that both ? and ?? . 
After the sub-system ensemble is constructed, 
each sub-system tunes its feature weights inde-
pendently to optimize the evaluation metrics on 
the development set. 
Let ? = {?1 ,? ,??} be the set of sub-systems 
obtained by either removing one non-LM feature 
or changing the order of a LM feature, and ??  be 
the n-best list produced by ?? . Then ?(?), the 
translation candidate pool to the system combi-
nation model can be written as: 
?(?) = ??
?
 (2) 
The advantage of this method is that it allows 
us to systematically build an ensemble of SMT 
systems at a very low cost. From the decoding 
1098
perspective, all the sub-systems share a common 
decoder, with minimal extensions to the baseline 
systems to support the use of specified subset of 
feature functions to compute the overall score for 
translation hypotheses. From the model training 
perspective, all the non-LM feature functions can 
be estimated once for all sub-systems. The only 
exception is the language model feature, which 
may be of different values across multiple sub-
systems. However, since lower-order models 
have already been contained in higher-order 
model for the purpose of smoothing in almost all 
statistical language model implementations, there 
is also no extra training cost. 
3.2 System Combination Scheme 
In our work, we use a sentence-level system 
combination model to select best translation hy-
pothesis from the candidate pool  ?(?) . This 
method can also be viewed to be a hypotheses re-
ranking model since we only use the existing 
translations instead of performing decoding over 
a confusion network as done in the word-level 
combination method (Rosti et al, 2007). 
The score function in our combination model 
is formulated as follows: 
?? = ??????
??? ? 
?????? ? + ??? + ?(?,?(?)) 
(3) 
where ??? ?  is the language model score for ?, 
? is the length of ?, and ?(?,?(?)) is a transla-
tion consensus ?based scoring function. The 
computation of ?(?,?(?))  is further decom-
posed into weighted linear combination of a set 
of n-gram consensus ?based features, which are 
defined in terms of the order of n-gram to be 
matched between current candidate and other 
translation in ?(?). 
Given a translation candidate  ? , the n-gram 
agreement feature between ?  and other transla-
tions in the candidate pool is defined as: 
??
+(?,? ? ) =  ?? ?, ?
? 
? ? ?? ? ,? ???
 (4) 
where the function  ?? ?, ?
?  counts the occur-
rences of n-grams of ? in ? ? : 
?? ?, ?
? = ?(??
?+??1, ? ?)
 ? ??+1
?=1
 (5) 
    Here ?(?,?)  is the indicator function - 
? ??
?+??1 , ? ?  is 1 when the n-gram ??
?+??1  ap-
pears in ? ? , otherwise it is 0. 
In order to give the combination model an op-
portunity to penalize long but inaccurate transla-
tions, we also introduce a set of n-gram disa-
greement features in the combination model: 
??
?(?,? ? ) =  ( ? ? ? + 1? ??(?, ?
?))
? ? ?? ? ,? ???
 
(6) 
Because each order of n-gram match introduc-
es two features, the total number of features in 
the combination model will be 2? + 2 if ? or-
ders of n-gram are to be matched in computing 
?(?,?(?)). Since we also adopt a linear scor-
ing function in Equation (3), the feature weights 
of our combination model can also be tuned on a 
development data set to optimize the specified 
evaluation metrics using the standard Minimum 
Error Rate Training (MERT) algorithm (Och 
2003). 
Our method is similar to the work proposed by 
Hildebrand and Vogel (2008). However, except 
the language model and translation length, we 
only use intra-hypothesis n-gram agreement fea-
tures as Hildebrand and Vogel did and use addi-
tional intra-hypothesis n-gram disagreement fea-
tures as Li et al (2009) did in their co-decoding 
method. 
4 Experiments 
4.1 Data 
Experiments were conducted on the NIST evalu-
ation sets of 2004 (MT04) and 2005 (MT05) for 
Chinese-to-English translation tasks. Both corpo-
ra provide 4 reference translations per source 
sentence. Parameters were tuned with MERT 
algorithm (Och, 2003) on the NIST evaluation 
set of 2003 (MT03) for both the baseline systems 
and the system combination model. Translation 
performance was measured in terms of case-
insensitive NIST version of BLEU score which 
computes the brevity penalty using the shortest 
reference translation for each segment, and all 
the results will be reported in percentage num-
bers. Statistical significance is computed using 
the bootstrap re-sampling method proposed by 
Koehn (2004). Statistics of the data sets are 
summarized in Table 3. 
 
Data set #Sentences #Words 
MT03 (dev) 919 23,782 
MT04 (test) 1,788 47,762 
MT05 (test) 1,082 29,258 
Table 3: Data set statistics. 
1099
We use the parallel data available for the 
NIST 2008 constrained track of Chinese-to-
English machine translation task as bilingual 
training data, which contains 5.1M sentence 
pairs, 128M Chinese words and 147M English 
words after pre-processing. GIZA++ toolkit (Och 
and Ney, 2003) is used to perform word align-
ment in both directions with default settings, and 
the intersect-diag-grow method is used to gener-
ate symmetric word alignment refinement. The 
language model used for all systems is a 5-gram 
model trained with the English part of bilingual 
data and Xinhua portion of LDC English Giga-
word corpus version 3. In experiments, multiple 
language model features with the order ranging 
from 2 to 5 can be easily obtained from the 5-
gram one without retraining. 
4.2 System Description 
Theoretically our method is applicable to all li-
near model ?based SMT systems. In our experi-
ments, two in-house developed systems are used 
to validate our method. The first one (SYS1) is a 
system based on the hierarchical phrase-based 
model as proposed in (Chiang, 2005). Phrasal 
rules are extracted from all bilingual sentence 
pairs, while hierarchical rules with variables are 
extracted from selected data sets including 
LDC2003E14, LDC2003E07, LDC2005T06 and 
LDC2005T10, which contain around 350,000 
sentence pairs, 8.8M Chinese words and 10.3M 
English words. The second one (SYS2) is a re-
implementation of a phrase-based decoder with 
lexicalized reordering model based on maximum 
entropy principle proposed by Xiong et al 
(2006). All bilingual data are used to extract 
phrases up to length 3 on the source side. 
    In following experiments, we only consider 
removing common features shared by both base-
line systems for feature subspace generation. 
Rule penalty feature and lexicalized reordering 
feature, which are particular to SYS1 and SYS2, 
are not used. We list the features in consideration 
as follows: 
? PEF and PFE: phrase translation probabili-
ties ? ? ?  and ? ? ?  
? PEFLEX and PFELEX: lexical weights 
????  ? ?  and ????  ? ?  
? PP: phrase penalty 
? WP: word penalty 
? BLP: bi-lexicon pair counting how many 
entries of a conventional lexicon co-
occurring in a given translation pair 
? LM-n: language model with order n 
    Based on the principle described in Section 
3.1, we generate a number of feature subspaces 
for each baseline system as follows:  
? For non-LM features (PEF, PFE, PEFLEX, 
PFELEX, PP, WP and BLP), we remove one 
of them from the full feature space each 
time. Thus 7 feature subspaces are generated, 
which are denoted as  ?????? , ?????? , 
????????? , ????????? , ????? , ?????  and 
??????  respectively. The 5-gram LM feature 
is used in each of them. 
? For LM features (LM-n), we change the or-
der from 2 to 5 with all the other non-LM 
features present. Thus 4 LM-related feature 
subspaces are generated, which are denoted 
as ?????2, ?????3 , ?????4  and ?????5 re-
spectively. ?????5 is essentially the full fea-
ture space of  baseline system. 
   For each baseline system, we construct a total 
of 11 sub-systems by using above feature sub-
spaces. The baseline system is also contained 
within them because of using ?????5. We call 
all sub-systems are non-baseline sub-systems 
except the one derived by using ?????5. 
    By default, the beam size of 60 is used for all 
systems in our experiments. The size of n-best 
list is set to 20 for each sub-system, and for base-
line systems, this size is set to 220, which equals 
to the size of the combined n-best list generated 
by total 11 sub-systems. The order of n-gram 
agreement and disagreement features used in 
sentence-level combination model ranges from 
unigram to 4-gram. 
4.3 Evaluation of Oracle Translations 
We first evaluate the oracle performance on the 
n-best lists of baseline systems and on the com-
bined n-best lists of sub-systems generated from 
each baseline system. 
The oracle translations are obtained by using 
the metric of sentence-level BLEU score (Ye et 
al., 2007). Table 4 shows the evaluation results, 
in which Baseline stands for baseline system 
with a 5-gram LM feature, and FS stands for 11 
sub-systems derived from the baseline system.  
 
 SYS1 SYS2 
 BLEU/TER BLEU/TER 
MT04 
Baseline  49.68/0.6411 49.50/0.6349 
FS 51.05/0.6089 50.53/0.6056 
MT05 
Baseline 48.89/0.5946 48.37/0.5944 
FS 50.69/0.5695 49.81/0.5684 
Table 4: Oracle BLEU and TER scores on base-
line systems and their generated sub-systems. 
1100
For both SYS1 and SYS2, feature subspace 
method achieves higher oracle BLEU and lower 
TER scores on both MT04 and MT05 test sets, 
which gives the feature subspace method more 
potential to achieve higher performance than the 
baseline systems. 
We then investigate the ratio of translation 
candidates in the combined n-best lists of non-
baseline sub-systems that are not included in the 
baseline?s n-best list. Table 5 shows the statistics. 
 
 MT04 MT05 
SYS1 69.71% 69.69% 
SYS2 59.07% 58.54% 
Table 5: Ratio of unique translation candidates 
from non-baseline sub-systems. 
From Table 5 we can see that only less than 
half of the translation candidates of sub-systems 
overlap with those the of baseline systems. This 
result, together with the oracle BLEU and TER 
score estimation, helps eliminate the concern that 
no diversities or better translation candidates can 
be obtained by using sub-systems. 
4.4 Feature Subspace Method on Single 
SMT System 
Next we validate the effect of feature subspace 
method on single SMT systems. 
Figure 1 shows the evaluation results of dif-
ferent systems on the MT05 test set. From the 
figure we can see that the overall accuracy of 
baseline systems is better than any of their de-
rived sub-systems, and except the sub-system 
derived by using ?????2, the performance of all 
the systems are fairly similar. 
 
 
Figure 1: Performances of different systems. 
We then evaluate the system combination me-
thod proposed in Section 3.2 with all the sub-
systems for each baseline system. Table 6 shows 
the results on both MT04 and MT05 data sets, in 
which FS-Comb denotes the system combination 
using 11 sub-systems.  
From Table 6 we can see that by using FS-
Comb we obtain about 1.1~1.3 points of BLEU 
gains over baseline systems. We also include in 
Table 6 the results for Baseline+mLM, which 
stands for the augmented baseline system as de-
scribed in Section 3.1 using a bunch of LM fea-
tures from bigram to 5-gram. It can be seen that 
both augmented baseline systems outperform 
their corresponding baseline systems slightly but 
consistently on both data sets. 
 
 MT04 MT05 
SYS1 
Baseline 39.07 38.72 
Baseline+mLM 39.34+ 39.14+ 
FS-Comb 40.43++ 39.79++ 
SYS2 
Baseline 38.84 38.30 
Baseline+mLM 38.95* 38.63+ 
FS-Comb 39.92++ 39.49++ 
Table 6: Translation results of Baseline, Base-
line+mLM and FS-Comb (+: significant better 
than baseline system with ? < 0.05; ++: signifi-
cant better than baseline system with ? < 0.01; *: 
no significant improvement). 
We also investigate the results when we in-
crementally add the n-best list of each sub-
system into a candidate pool to see the effects 
when different numbers of sub-systems are used 
in combination. In order to decide the sequence 
of sub-systems to add, we first evaluate the per-
formance of pair-wise combinations between 
each sub-system and its baseline system on the 
development set. That is, for each sub-system, 
we combine its n-best list with the n-best list of 
its baseline system and perform system combina-
tion for MT03 data set. Then we rank the sub-
systems by the pair-wise combination perfor-
mance from high to low, and use this ranking as 
the sequence to add n-best lists of sub-systems. 
Each time when a new n-best list is added, the 
combination performance based on the enlarged 
candidate pool is evaluated. Figure 2 shows the 
results on both MT04 and MT05 test sets, in 
which SYS1-fs and SYS2-fs denote the sub-
systems derived from SYS1 and SYS2 respec-
tively, and X-axis is the number of sub-systems 
used for combination each time and Y-axis is the 
BLEU score. From the figure we can see that 
although in some cases the performance slightly 
drops when a new sub-system is added, generally 
using more sub-systems always leads to better 
results.  
31
32
33
34
35
36
37
38
39
SYS1 SYS2
Baseline
FS-PEF
FS-PFE
FS-PEFLEX
FS-PFELEX
FS-PP
FS-WP
FS-BLP
FS-LM-2
FS-LM-3
FS-LM-4
1101
Next we examine the performance of baseline 
systems when different beam sizes are used in 
decoding. The results on MT05 test set are 
shown in Figure3, where X-axis is the beam size. 
In Figure 3, SYS1+mLM and SYS2+mLM de-
note augmented baseline systems of SYS1 and 
SYS2 with multiple LM features. 
From Figure 3 we can see that augmented 
baseline systems (with multiple LM features) 
outperform the baseline systems (with only one 
LM feature) for all beam sizes ranging from 20 
to 220. In this experiment we did not observe any 
significant performance improvements when us-
ing larger beam sizes than the default setting, but 
using more sub-systems in combination almost 
always bring improvements. 
 
 
Figure 2: Performances on different numbers of 
sub-systems.  
 
Figure 3: Performances on different beam sizes. 
 MT04 MT05 
SYS1-fs 44.63% 46.12% 
SYS2-fs 47.54% 44.73% 
Table 7: Ratio of final translations coming from 
non-baseline sub-systems. 
Finally, we investigate the ratio of final trans-
lations coming from the n-best lists of non-
baseline sub-systems only. Table 7 shows the 
results on both MT04 and MT05 test sets, which 
indicate that almost half of the final translations 
are contributed by the non-baseline sub-systems. 
4.5 The Impact of n-best List Size 
In order to find the optimal size of n-best list for 
combination, we compare the combination re-
sults of using list sizes from 10-best up to 500-
best for each sub-system. 
In this experiment, system combination was 
performed on the combined n-best list from total 
11 sub-systems with different list size each time. 
Figure 4 shows the results on the MT03 dev set 
and the MT04 and MT05 test sets for both SYS1 
and SYS2. X-axis is the n-best list size of each 
sub-system. 
 
 
Figure 4: Performances on different n-best sizes. 
    We can see from the figure that for all data 
sets the optimal n-best list size is around 50, but 
the improvements are not significant over the 
results when 20-best translations are used. The 
reason for the small optimal n-best list size could 
be that the low-rank hypotheses might introduce 
more noises into the combined translation candi-
date pool for sentence-level combination (Hasan 
et al, 2007; Hildebrand and Vogel, 2008).  
4.6 Feature Subspace Method on Multiple 
SMT Systems 
In the last experiment, we investigate the effect 
of feature subspace method when multiple SMT 
systems are used in system combination.  
Evaluation results are reported in Table 8. The 
system combination method described in Section 
3.2 is used to combine outputs from two baseline 
systems (with only one 5-gram LM feature) and 
sub-systems generated from both baseline sys-
tems (22 in total), with their results denoted as 
Baseline Comb (both) and FS Comb (both) re-
spectively. We also include the combination re-
sults of sub-systems based on one baseline sys-
tem for reference in the table. 
 
38.0
38.5
39.0
39.5
40.0
40.5
1 2 3 4 5 6 7 8 9 10 11
SYS1-fs-05
SYS2-fs-05
SYS1-fs-04
SYS2-fs-04
38.0
38.5
39.0
39.5
2
0
4
0
6
0
8
0
1
0
0
1
2
0
1
4
0
1
6
0
1
8
0
2
0
0
2
2
0
SYS1
SYS2
SYS1-mLM
SYS2-mLM
39.0
39.5
40.0
40.5
41.0
41.5
42.0
10 20 50 100 200 500
SYS1-fs-05
SYS2-fs-05
SYS1-fs-04
SYS2-fs-04
SYS1-fs-03
SYS2-fs-03
1102
On both MT04 and MT05 test sets, the results 
of system combination based on sub-systems are 
significantly better than those of baseline sys-
tems, which show that our method can also help 
with system combination when more than one 
system are used. We can also see that using mul-
tiple systems based on different SMT models and 
using our subspace based method can help each 
other: the best performance can only be achieved 
when both are employed. 
 
 MT04 MT05 
Baseline Comb (both) 39.98 39.43 
FS-Comb (SYS1) 40.43 39.79 
FS-Comb (SYS2) 39.92 39.49 
FS Comb (both) 40.96 40.38 
Table 8: Performances of sentence-level combi-
nation on multiple SMT systems. 
5 Conclusion 
In this paper, we have presented a novel and ef-
fective Feature Subspace method for the con-
struction of an ensemble of machine translation 
systems based on a baseline SMT model which 
can be formulated as a standard linear function. 
Each system within the ensemble is based on a 
subset of features derived from the baseline 
model, and the resulting ensemble can be used in 
system combination to improve translation quali-
ty. Experimental results on NIST Chinese-to-
English translation tasks show that our method 
can bring significant improvements to two base-
line systems with state-of-the-art performance, 
and it is expected that our method can be em-
ployed to improve any linear model -based SMT 
systems. There is still much room for improve-
ments in the current work. For example, we still 
use a simple one-feature difference principle for 
feature subspace generation. In the future, we 
will explore more possibilities for feature sub-
spaces selection and experiment with our method 
in a word-level system combination model. 
 
References  
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008. 
Improving alignments for better confusion net-
works for combining machine translation systems. 
In Proc. COLING, pages 33-40. 
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In 
Proc. ASRU, pages 351-354. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
ACL, pages 263-270. 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 
98-107. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
8th AMTA conference, pages 254-261. 
Tin Kam Ho. 1998. The random subspace method for 
constructing decision forests. In IEEE Transactions 
on Pattern Analysis and Machine Intelligence, 
pages 832-844. 
Sasa Hasan, Richard Zens, and Hermann Ney. 2007. 
Are very large n-best lists useful for SMT? In 
Proc. NAACL, Short paper, pages 57-60. 
S. Jayaraman and A. Lavie. 2005. Multi-Engine Ma-
chine Translation Guided by Explicit Word Match-
ing. In 10th EAMT conference, pages 143-152. 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proc. EMNLP, 
pages 388-395. 
Philipp Koehn. 2004. Phrase-based Model for SMT. 
In Computational Linguistics, 28(1): pages 114-
133. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009. Collaborative Decoding: Partial 
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proc. ACL-IJCNLP. 
Adam Lopez and Philip Resnik. 2006. Word-Based 
Alignment, Phrase-Based Translation: What?s the 
link? In 7th AMTA conference, pages 90-99. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proc. ACL, pages 609-616. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phras-
es. In Proc. EMNLP, pages 44-52. 
Wolfgang Macherey and Franz Och. 2007. An Empir-
ical Study on Computing Consensus Translations 
from Multiple Machine Translation Systems. In 
Proc. EMNLP, pages 986-995. 
Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced 
hypotheses alignment. In Proc. EACL, pages 33-
40. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
1103
tical machine translation. In Proc. ACL, pages 295-
302. 
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 
160-167. 
Franz Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): pages 19-51. 
Franz Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine transla-
tion. Computational Linguistics, 30(4): pages 417-
449. 
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, 
Spyros Matsoukas, Richard Schwartz, and Bonnie 
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proc. NAACL, pages 
228-235. 
Antti-Veikko Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proc. 
ACL, pages 312-319. 
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with 
application to machine translation system combina-
tion. In Proc. Of the Third ACL Workshop on Sta-
tistical Machine Translation, pages 183-186. 
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. 
Woodland. 2007. Consensus network decoding for 
statistical machine translation system combination. 
In ICASSP, pages 105-108. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for 
statistical machine translation. In Proc. ACL, pages 
521-528. 
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level Machine Translation Evaluation as a 
Ranking Problem: One step aside from BLEU. In 
Proc. Of the Second ACL Workshop on Statistical 
Machine Translation, pages 240-247. 
1104
Proceedings of ACL-08: HLT, pages 89?96,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Measure Word Generation for English-Chinese SMT Systems 
 
 
Dongdong Zhang1, Mu Li1, Nan Duan2, Chi-Ho Li1, Ming Zhou1 
1Microsoft Research Asia 2Tianjin University 
Beijing, China Tianjin, China 
{dozhang,muli,v-naduan,chl,mingzhou}@microsoft.com 
 
 
 
 
 
 
Abstract 
Measure words in Chinese are used to indi-
cate the count of nouns. Conventional sta-
tistical machine translation (SMT) systems do 
not perform well on measure word generation 
due to data sparseness and the potential long 
distance dependency between measure words 
and their corresponding head words. In this 
paper, we propose a statistical model to gen-
erate appropriate measure words of nouns for 
an English-to-Chinese SMT system. We mod-
el the probability of measure word generation 
by utilizing lexical and syntactic knowledge 
from both source and target sentences. Our 
model works as a post-processing procedure 
over output of statistical machine translation 
systems, and can work with any SMT system. 
Experimental results show our method can 
achieve high precision and recall in measure 
word generation. 
1 Introduction 
In linguistics, measure words (MW) are words or 
morphemes used in combination with numerals or 
demonstrative pronouns to indicate the count of 
nouns1, which are often referred to as head words 
(HW). 
Chinese measure words are grammatical units 
and occur quite often in real text. According to our 
survey on the measure word distribution in the 
Chinese Penn Treebank and the test datasets distri-
buted by Linguistic Data Consortium (LDC) for 
Chinese-to-English machine translation evaluation, 
the average occurrence is 0.505 and 0.319 measure 
                                                 
1 The uncommon cases of verbs are not considered. 
words per sentence respectively. Unlike in Chinese, 
there is no special set of measure words in English. 
Measure words are usually used for mass nouns 
and any semantically appropriate nouns can func-
tion as the measure words. For example, in the 
phrase three bottles of water, the word bottles acts 
as a measure word. Countable nouns are almost 
never modified by measure words2. Numerals and 
indefinite articles are directly followed by counta-
ble nouns to denote the quantity of objects.  
Therefore, in the English-to-Chinese machine 
translation task we need to take additional efforts 
to generate the missing measure words in Chinese. 
For example, when translating the English phrase 
three books into the Chinese phrases ?????, 
where three corresponds to the numeral ??? and 
books corresponds to the noun ???, the Chinese 
measure word ??? should be generated between 
the numeral and the noun.  
In most statistical machine translation (SMT) 
models (Och et al, 2004; Koehn et al, 2003; 
Chiang, 2005), some of measure words can be 
generated without modification or additional 
processing. For example, in above translation, the 
phrase translation table may suggest the word three 
be translated into ???, ????, ????, etc, and 
the word books into ???, ????, ???? (scroll), 
etc. Then the SMT model selects the most likely 
combination ????? as the final translation re-
sult. In this example, a measure word candidate set 
consisting of ??? and ??? can be generated by 
bilingual phrases (or synchronous translation rules), 
and the best measure word ??? from the measure  
                                                 
2 There are some exceptional cases, such as ?100 head of cat-
tle?. But they are very uncommon. 
89
  
 
 
 
 
 
 
 
 
 
 
 
 
word candidate set can be selected by the SMT 
decoder. However, as we will show below, existing 
SMT systems do not deal well with the measure 
word generation in general due to data sparseness 
and long distance dependencies between measure 
words and their corresponding head words.  
Due to the limited size of bilingual corpora, 
many measure words, as well as the collocations 
between a measure and its head word, cannot be 
well covered by the phrase translation table in an 
SMT system. Moreover, Chinese measure words 
often have a long distance dependency to their 
head words which makes language model ineffec-
tive in selecting the correct measure words from 
the measure word candidate set. For example, in 
Figure 1 the distance between the measure word 
??? and its head word ???? (undertaking) is 15. 
In this case, an n-gram language model with n<15 
cannot capture the MW-HW collocation. Table 1 
shows the relative position?s distribution of head 
words around measure words in the Chinese Penn 
Treebank, where a negative position indicates that 
the head word is to the left of the measure word 
and a positive position indicates that the head word 
is to the right of the measure word. Although lots 
of measure words are close to the head words they 
modify, more than sixteen percent of measure 
words are far away from their corresponding head 
words (the absolute distance is more than 5). 
To overcome the disadvantage of measure word 
generation in a general SMT system, this paper 
proposes a dedicated statistical model to generate 
measure words for English-to-Chinese translation. 
We model the probability of measure word gen-
eration by utilizing rich lexical and syntactic 
knowledge from both source and target sentences. 
Three steps are involved in our method to generate 
measure words: Identifying the positions to gener-
ate measure words, collecting the measure word 
candidate set and selecting the best measure word. 
Our method is performed as a post-processing pro-
cedure of the output of SMT systems. The advan-
tage is that it can be easily integrated into any SMT 
system. Experimental results show our method can 
significantly improve the quality of measure word 
generation. We also compared the performance of 
our model based on different contextual informa-
tion, and show that both large-scale monolingual 
data and parallel bilingual data can be helpful to 
generate correct measure words. 
Position Occurrence Position Occurrence
1 39.5% -1 0 
2 15.7% -2 0 
3 4.7% -3 8.7% 
4 1.4% -4 6.8% 
5 2.1% -5 4.3% 
>5 8.8% <-5 8.0% 
Table 1. Position distribution of head words 
2 Our Method 
2.1 Measure word  generation in Chinese 
In Chinese, measure words are obligatory in cer-
tain contexts, and the choice of measure word 
usually depends on the head word?s semantics (e.g., 
shape or material). The set of Chinese measure 
words is a relatively close set and can be classified 
into two categories based on whether they have a 
corresponding English translation. Those not hav-
ing an English counterpart need to be generated 
during translation. For those having English trans-
lations, such as ??? (meter), ??? (ton), we just 
use the translation produced by the SMT system 
itself. According to our survey, about 70.4% of 
measure words in the Chinese Penn Treebank need 
Figure 1.  Example of long distance dependency between MW and its modified HW 
??/??/ 
??/ ?/ 
? ?? 
Pudong 's de-
velopment and 
opening up is a century-spanning 
/?/?
?/ 
for vigorously promoting shanghai 
and constructing a modern econom-
ic , trade , and financial center  undertaking
??/??/ ?/ ?? /??? /??
/ ?/ ??/ ? /??/ ??/ ?/ 
? 
. 
?
90
 to be explicitly generated during the translation 
process. 
In Chinese, there are generally stable linguistic 
collocations between measure words and their head 
words. Once the head word is determined, the col-
located measure word can usually be selected ac-
cordingly. However, there is no easy way to identi-
fy head words in target Chinese sentences since for 
most of the time an SMT output is not a well 
formed sentence due to translation errors. Mistake 
of head word identification may cause low quality 
of measure word generation. In addition, some-
times the head word itself is not enough to deter-
mine the measure word. For example, in Chinese 
sentences ???? 5??? (there are five people 
in his family) and ???? 5???????? (a 
total of five people attended the meeting), where 
??? (people) is the head word collocated with two 
different measure words ??? and ???, we cannot 
determine the measure word just based on the head 
word ???.   
2.2 Framework 
In our framework, a statistical model is used to 
generate measure words. The model is applied to 
SMT system outputs as a post-processing proce-
dure. Given an English source sentence, an SMT 
decoder produces a target Chinese translation, in 
which positions for measure word generation are 
identified. Based on contextual information con-
tained in both input source sentence and SMT sys-
tem?s output translation, a measure word candidate 
set M is constructed. Then a measure word selec-
tion model is used to select the best one from M. 
Finally, the selected measure word is inserted into 
previously determined measure word slot in the 
SMT system?s output, yielding the final translation 
result. 
2.3 Measure word position identification 
To identify where to generate measure words in the 
SMT outputs, all positions after numerals are 
marked at first since measure words often follow 
numerals. For other cases in which measure words 
do not follow numerals (e.g., ??? /? /??? 
(many computers), where ??? is a measure word 
and ???? (computers) is its head word), we just 
mine the set of words which can be followed by 
measure words from training corpus.  Most of 
words in the set are pronouns such as ??? (this), 
??? (that) and ???? (several). In the SMT out-
put, the positions after these words are also identi-
fied as candidate positions to generate measure 
words.  
2.4 Candidate measure word generation 
To avoid high computation cost, the measure word 
candidate set only consists of those measure words 
which can form valid MW-HW collocations with 
their head words. We assume that all the surround-
ing words within a certain window size centered on 
the given position to generate a measure word are 
potential head words, and require that a measure 
word candidate must collocate with at least one of 
the surrounding words. Valid MW-HW colloca-
tions are mined from the training corpus and a sep-
arate lexicon resource.  
There is a possibility that the real head word is 
outside the window of given size. To address this 
problem, we also use a source window centered on 
the position ps, which is aligned to the target meas-
ure word position pt. The link between ps and pt 
can be inferred from SMT decoding result. Thus, 
the chance of capturing the best measure word in-
creases with the aid of words located in the source 
window. For example, given the window size of 10, 
although the target head word ???? (undertaking) 
in Figure 1 is located outside the target window, its 
corresponding source head word undertaking can 
be found in the source window. Based on this 
source head word, the best measure word ??? will 
be included into the candidate measure word set. 
This example shows how bilingual information can 
enrich the measure word candidate set. 
Another special word {NULL} is always in-
cluded in the measure word candidate set. {NULL} 
represents those measure words having a corres-
ponding English translation as mentioned in Sec-
tion 2.1. If {NULL} is selected, it means that we 
need not generate any measure word at the current 
position. Thus, no matter what kinds of measure 
words they are, we can handle the issue of measure 
word generation in a unified framework.  
2.5 Measure word selection model 
After obtaining the measure word candidate set M, 
a measure word selection model is employed to 
select the best one from M. Given the contextual 
information C in both source window and target 
91
 window, we model the measure word selection as 
finding the measure word m* with highest post-
erior probability given C: 
?? = argmax????(?|?)                  (1) 
To leverage the collocation knowledge between 
measure words and head words, we extend (1) by 
introducing a hidden variable h where H represents 
all candidate head words located within the target 
window: 
     ?? = argmax??? ? ?(?, ?|?)???  
           = argmax??? ? ?(?|?)?(?|?, ?)???   (2) 
In (2), ?(?|?) is the head word selection proba-
bility and is empirically estimated according to the 
position distribution of head words in Table 1. 
?(?|?, ?) is the conditional probability of m given 
both h and C. We use maximum entropy model to 
compute ?(?|?, ?): 
            ?(?|?, ?) = exp(? ?? ??(?,?)? )? exp(? ?? ??(??,?)? )????      (3) 
Based on the different features used in the com-
putation of ?(?|?, ?) , we can train two sub-
models ? a monolingual model (Mo-ME) which 
only uses monolingual (Chinese) features and a 
bilingual model (Bi-ME) which integrates bilingual 
features. The advantage of the Mo-ME model is 
that it can employ an unlimited monolingual target 
training corpora, while the Bi-ME model leverages 
rich features including both the source and target 
information and may improve the precision. Com-
pared to the Mo-ME model, the Bi-ME model suf-
fers from small scale of parallel training data. To 
leverage advantages of both models, we use a 
combined model Co-ME, by linearly combing the 
monolingual and bilingual sub-models: 
?? = argmax??????????  + (1 ? ?)??????  
where ? ? [0,1] is a free parameter that can be op-
timized on held-out data and it was set to 0.39 in 
our experiments. 
2.6 Features 
The computation of Formula (3) involves the fea-
tures listed in Table 2 where the Mo-ME model 
only employs target features and the Bi-ME model 
leverages both target features and source features.  
For target features, n-gram language model 
score is defined as the sum of log n-gram probabil-
ities within the target window after the measure 
word is filled into the measure word slot. The 
MW-HW collocation feature is defined to be a 
function f1 to capture the collocation between a 
measure word and a head word. For features of 
surrounding words, the feature function f2 is de-
fined as 1 if a certain word exists at a certain posi-
tion, otherwise 0. For example, f2(?,-2)=1 means 
the second word on the left is ???. f2(?,3)=1 
means the third word on the right is ???. For 
punctuation position feature function f3, the feature 
value is 1 when there is a punctuation following 
the measure word, which indicates the target head 
word may appear to the left of measure word. Oth-
erwise, it is 0. In practice, we can also ignore the 
position part, i.e., a word appears anywhere within 
the window is viewed as the same feature. 
 Target features Source features 
n-gram language model 
score 
MW-HW collocation
MW-HW collocation surrounding words 
surrounding words source head word 
punctuation position POS tags 
Table 2. Features used in our model 
For source language side features, MW-HW col-
location and surrounding words are used in a simi-
lar way as does with target features. The source 
head word feature is defined to be a function f4 to 
indicate whether a word ei is the source head word 
in English according to a parse tree of the source 
sentence. Similar to the definition of lexical fea-
tures, we also use a set of features based on POS 
tags of source language. 
3 Model Training and Application 
3.1 Training 
We parsed English and Chinese sentences to get 
training samples for measure word generation 
model. Based on the source syntax parse tree, for 
each measure word, we identified its head word by 
using a toolkit from (Chiang and Bikel, 2002) 
which can heuristically identify head words for 
sub-trees. For the bilingual corpus, we also per-
form word alignment to get correspondences be-
tween source and target words. Then, the colloca-
tion between measure words and head words and 
their surrounding contextual information are ex-
tracted to train the measure word selection models. 
According to word alignment results, we classify 
92
 measure words into two classes based on whether 
they have non-null translations. We map Chinese 
measure words having non-null translations to a 
unified symbol {NULL} as mentioned in Section 
2.4, indicating that we need not generate these kind 
of measure words since they can be translated from 
English.  
In our work, the Berkeley parser (Petrov and 
Klein, 2007) was employed to extract syntactic 
knowledge from the training corpus. We ran GI-
ZA++ (Och and Ney, 2000) on the training corpus 
in both directions with IBM model 4, and then ap-
plied the refinement rule described in (Koehn et al, 
2003) to obtain a many-to-many word alignment 
for each sentence pair. We used the SRI Language 
Modeling Toolkit (Stolcke, 2002) to train a five-
gram model with modified Kneser-Ney smoothing 
(Chen and Goodman, 1998). The Maximum Entro-
py training toolkit from (Zhang, 2006) was em-
ployed to train the measure word selection model. 
3.2 Measure word generation 
As mentioned in previous sections, we apply our 
measure word generation module into SMT output 
as a post-processing step. Given a translation from 
an SMT system, we first determine the position pt 
at which to generate a Chinese measure word. Cen-
tered on pt, a surrounding word window with spe-
cified size is determined. From translation align-
ments, the corresponding source position ps aligned 
to pt can be referred.  In the same way, a source 
window centered on ps is determined as well. Then, 
contextual information within the windows in the 
source and the target sentence is extracted and fed 
to the measure word selection model. Meanwhile, 
the candidate set is obtained based on words in 
both windows. Finally, each measure word in the 
candidate set is inserted to the position pt, and its 
score is calculated based on the models presented 
in Section 2.5. The measure word with the highest 
probability will be chosen.  
There are two reasons why we perform measure 
word generation for SMT systems as a post-
processing step. One is that in this way our method 
can be easily applied to any SMT system. The oth-
er is that we can leverage both source and target 
information during the measure word generation 
process. We do not integrate our measure word 
generation module into the SMT decoder since 
there is only little target contextual information 
available during SMT decoding. Moreover, as we 
will show in experiment section, a pre-processing 
method does not work well when only source in-
formation is available. 
4 Experiments 
4.1 Data 
In the experiments, the language model is a Chi-
nese 5-gram language model trained with the Chi-
nese part of the LDC parallel corpus and the Xin-
hua part of the Chinese Gigaword corpus with 
about 27 million words. We used an SMT system 
similar to Chiang (2005), in which FBIS corpus is 
used as the bilingual training data. The training 
corpus for Mo-ME model consists of the Chinese 
Peen Treebank and the Chinese part of the LDC 
parallel corpus with about 2 million sentences. The 
Bi-ME model is trained with FBIS corpus, whose 
size is smaller than that used in Mo-ME model 
training. 
We extracted both development and test data set 
from years of NIST Chinese-to-English evaluation 
data by filtering out sentence pairs not containing 
measure words. The development set is extracted 
from NIST evaluation data from 2002 to 2004, and 
the test set consists of sentence pairs from NIST 
evaluation data from 2005 to 2006. There are 759 
testing cases for measure word generation in our 
test data consisting of 2746 sentence pairs. We use 
the English sentences in the data sets as input to 
the SMT decoder, and apply our proposed method 
to generate measure words for the output from the 
decoder. Measure words in Chinese sentences of 
the development and test sets are used as refer-
ences. When there are more than one measure 
words acceptable at some places, we manually 
augment the references with multiple acceptable 
measure words. 
4.2 Baseline 
Our baseline is the SMT output where measure 
words are generated by a Hiero-like SMT decoder 
as discussed in Section 1. Due to noises in the Chi-
nese translations introduced by the SMT system, 
we cannot correctly identify all the positions to 
generate measure words. Therefore, besides preci-
sion we examine recall in our experiments. 
4.3 Evaluation over SMT output 
Table 3 and Table 4 show the precision and recall 
of our measure word generation method. From the 
93
 experimental results, the Mo-ME, Bi-ME and Co-
ME models all outperform the baseline. Compared 
with the baseline, the Mo-ME method takes advan-
tage of a large size monolingual training corpus 
and reduces the data sparseness problem. The ad-
vantage of the Bi-ME model is being able to make 
full use of rich knowledge from both source and 
target sentences. Also as shown in Table 3 and Ta-
ble 4, the Co-ME model always achieve the best 
results when using the same window size since it 
leverages the advantage of both the Mo-ME and 
the Bi-ME models. 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
54.82% 
64.29% 67.15%  67.66% 
8 64.93% 68.50%  69.00% 
10 64.72% 69.40% 69.58%
12 65.46% 69.40% 69.76%
14 65.61% 69.69%  70.03% 
Table 3. Precision over SMT output 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
45.61% 
51.48% 53.69%  54.09% 
8 51.98% 54.75%  55.14% 
10 51.81% 55.44% 55.58%
12 52.38% 55.44% 55.72%
14 52.50% 55.67%  55.93% 
Table 4. Recall over SMT output 
We can see that the Bi-ME model can achieve 
better results than the Mo-ME model in both recall 
and precision metrics although only a small sized 
bilingual corpus is used for Bi-ME model training. 
The reason is that the Mo-ME model cannot cor-
rectly handle the cases where head words are lo-
cated outside the target window. However, due to 
word order differences between English and Chi-
nese, when target head words are outside the target 
window, their corresponding source head words 
might be within the source window. The capacity 
of capturing head words is improved when both 
source and target windows are used, which demon-
strates that bilingual knowledge is useful for meas-
ure word generation. 
We compare the results for each model with dif-
ferent window sizes. Larger window size can lead 
to better results as shown in Table 3 and Table 4 
since more contextual knowledge is used to model 
measure word generation. However, enlarging the 
window size does not bring significant improve-
ments, The major reason is that even a small win-
dow size is already able to cover most of measure 
word collocations, as indicated by the position dis-
tribution of head words in Table 1.  
The quality of the SMT output also affects the 
quality of measure word generation since our me-
thod is performed in a post-processing step over 
the SMT output. Although translation errors de-
grade the measure word generation accuracy, we 
achieve about 15% improvement in precision and a 
10% increase in recall over baseline. We notice 
that the recall is relatively lower. Part of the reason 
is some positions to generate measure words are 
not successfully identified due to translation errors. 
In addition to precision and recall, we also evaluate 
the Bleu score (Papineni et al, 2002) changes be-
fore and after applying our measure word genera-
tion method to the SMT output. For our test data, 
we only consider sentences containing measure 
words for Bleu score evaluation. Our measure 
word generation step leads to a Bleu score im-
provement of 0.32 where the window size is set to 
10, which shows that it can improve the translation 
quality of an English-to-Chinese SMT system. 
4.4 Evaluation over reference data 
To isolate the impact of the translation errors in 
SMT output on the performance of our measure 
word generation model, we conducted another ex-
periment with reference bilingual sentences in 
which measure words in Chinese sentences are 
manually removed. This experiment can show the 
performance upper bound of our method without 
interference from an SMT system. Table 5 shows 
the results. Compared to the results in Table 3, the 
precision improvement in the Mo-ME model is 
larger than that in the Bi-ME model, which shows 
that noisy translation of the SMT system has more 
serious influence on the Mo-ME model than the 
Bi-ME model. This also indicates that source in-
formation without noises is helpful for measure 
word generation. 
Wsize Mo-ME Bi-ME Co-ME 
6 71.63% 74.92% 75.72% 
8 73.80% 75.48% 76.20% 
10 73.80% 74.76% 75.48% 
12 73.80% 75.24% 75.96% 
14 73.56% 75.48% 76.44% 
Table 5. Results over reference data 
94
 4.5 Impacts of features 
In this section, we examine the contribution of 
both target language based features and source 
language based features in our model. Table 6 and 
Table 7 show the precision and recall when using 
different features. The window size is set to 10. In 
the tables, Lm denotes the n-gram language model 
feature, Tmh denotes the feature of collocation be-
tween target head words and the candidate measure 
word, Smh denotes the feature of collocation be-
tween source head words and the candidate meas-
ure word, Hs denotes the feature of source head 
word selection, Punc denotes the feature of target 
punctuation position, Tlex denotes surrounding 
word features in translation, Slex denotes surround-
ing word features in source sentence, and Pos de-
notes Part-Of-Speech feature. 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh 61.43% 49.22% 
+Punc 62.54% 50.08% 
+Tlex 64.80% 51.87% 
Table 6. Feature contribution in Mo-ME model 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh+Smh 64.50% 51.64% 
+Hs 65.32% 52.26% 
+Punc 66.29% 53.10% 
+Pos 66.53% 53.25% 
+Tlex 67.50% 54.02% 
+Slex 69.52% 55.54% 
Table 7. Feature contribution in Bi-ME model 
The experimental results show that all the fea-
tures can bring incremental improvements. The 
method with only Lm feature performs worse than 
the baseline. However, with more features inte-
grated, our method outperforms the baseline, 
which indicates each kind of features we selected 
is useful for measure word generation. According 
to the results, the feature of MW-HW collocation 
has much contribution to reducing the selection 
error of measure words given head words. The 
contribution of Slex feature explains that other sur-
rounding words in source sentence are also helpful 
since head word determination in source language 
might be incorrect due to errors in English parse 
trees. Meanwhile, the contribution from Smh, Hs 
and Slex features demonstrates that bilingual 
knowledge can play an important role for measure 
word generation. Compared with lexicalized fea-
tures, we do not get much benefit from the Pos 
features. 
4.6 Error analysis 
We conducted an error analysis on 100 randomly 
selected sentences from the test data. There are 
four major kinds of errors as listed in Table 8. 
Most errors are caused by failures in finding posi-
tions to generate measure words. The main reason 
for this is some hint information used to identify 
measure word positions is missing in the noisy 
output of SMT systems. Two kinds of errors are 
introduced by incomplete head word and MW-HW 
collocation coverage, which can be solved by en-
larging the size of training corpus. There are also 
head word selection errors due to incorrect syntax 
parsing. 
Error type Ratio 
unseen head word  32.14% 
unseen MW-HW collocation 10.71% 
missing MW position 39.29% 
incorrect HW selection 10.71% 
others 7.14% 
Table 8. Error distribution 
4.7 Comparison with other methods 
In this section we compare our statistical methods 
with the pre-processing method and the rule-based 
methods for measure word generation in a transla-
tion task.  
In pre-processing method, only source language 
information is available. Given a source sentence, 
the corresponding syntax parse tree Ts is first con-
structed with an English parser. Then the pre-
processing method chooses the source head word 
hs based on Ts. The candidate measure word with 
the highest probability collocated with hs is se-
lected as the best result, where the measure word 
candidate set corresponding to each head word is 
mined over a bilingual training corpus in advance. 
We achieved precision 58.62% and recall 49.25%, 
which are worse than the results of our post-
processing based methods. The weakness of the 
pre-processing method is twofold. One problem is 
data sparseness with respect to collocations be-
95
 tween English head words and Chinese measure 
words. The other problem comes from the English 
head word selection error introduced by using 
source parse trees.  
We also compared our method with a well-
known rule-based machine translation system ? 
SYSTRAN3. We translated our test data with SY-
STRAN?s English-to-Chinese translation engine. 
The precision and recall are 63.82% and 51.09% 
respectively, which are also lower than our method.  
5 Related Work  
Most existing rule-based English-to-Chinese MT 
systems have a dedicated module handling meas-
ure word generation. In general a rule-based me-
thod uses manually constructed rule patterns to 
predict measure words. Like most rule based ap-
proaches, this kind of system requires lots of hu-
man efforts of experienced linguists and usually 
cannot easily be adapted to a new domain. The 
most relevant work based on statistical methods to 
our research might be statistical technologies em-
ployed to model issues such as morphology gener-
ation (Minkov et al, 2007). 
6 Conclusion and Future Work 
In this paper we propose a statistical model for 
measure word generation for English-to-Chinese 
SMT systems, in which contextual knowledge 
from both source and target sentences is involved. 
Experimental results show that our method not on-
ly achieves high precision and recall for generating 
measure words, but also improves the quality of 
English-to-Chinese SMT systems. 
In the future, we plan to investigate more fea-
tures and enlarge coverage to improve the quality 
of measure word generation, especially reduce the 
errors found in our experiments. 
Acknowledgements 
Special thanks to David Chiang, Stephan Stiller 
and the anonymous reviewers for their feedback 
and insightful comments. 
References 
Stanley F. Chen and Joshua Goodman. 1998. An Empir-
ical study of smoothing techniques for language 
                                                 
3 http://www.systransoft.com/ 
modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technolo-
gy, 1998. 
David Chiang and Daniel M. Bikel. 2002. Recovering 
latent information in treebanks. Proceedings of COL-
ING '02, 2002.  
David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005, pages 263-270. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings of 
HLT-NAACL 2003, pages 127-133.  
Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 
2007. Generating complex morphology for machine 
translation. In Proceedings of 45th Annual Meeting 
of the ACL, pages 128-135. 
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of 38th An-
nual Meeting of the ACL, pages 440-447.  
Franz J. Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30:417-449. 
Kishore Papineni, Salim Roukos, ToddWard, and WeiJ-
ing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of 40th 
Annual Meeting of the ACL, pages 311-318. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of HLT-
NAACL, 2007. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of International 
Conference on Spoken Language Processing, volume 
2, pages 901-904.  
Le Zhang. MaxEnt toolkit. 2006. http://homepages.inf. 
ed.ac.uk/s0450736/maxent_toolkit.html  
96
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 585?592,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Collaborative Decoding: Partial Hypothesis Re-ranking 
Using Translation Consensus between Decoders 
 
Mu Li1, Nan Duan2, Dongdong Zhang1, Chi-Ho Li1, Ming Zhou1 
        1Microsoft Research Asia                                        2Tianjin University 
              Beijing, China                                                     Tianjin, China 
{muli,v-naduan,dozhang,chl,mingzhou}@microsoft.com 
 
 
Abstract 
This paper presents collaborative decoding 
(co-decoding), a new method to improve ma-
chine translation accuracy by leveraging trans-
lation consensus between multiple machine 
translation decoders. Different from system 
combination and MBR decoding, which post-
process the n-best lists or word lattice of ma-
chine translation decoders, in our method mul-
tiple machine translation decoders collaborate 
by exchanging partial translation results. Us-
ing an iterative decoding approach, n-gram 
agreement statistics between translations of 
multiple decoders are employed to re-rank 
both full and partial hypothesis explored in 
decoding. Experimental results on data sets for 
NIST Chinese-to-English machine translation 
task show that the co-decoding method can 
bring significant improvements to all baseline 
decoders, and the outputs from co-decoding 
can be used to further improve the result of 
system combination. 
1 Introduction 
Recent research has shown substantial improve-
ments can be achieved by utilizing consensus 
statistics obtained from outputs of multiple ma-
chine translation systems. Translation consensus 
can be measured either at sentence level or at 
word level. For example, Minimum Bayes Risk 
(MBR) (Kumar and Byrne, 2004) decoding over 
n-best list tries to find a hypothesis with lowest 
expected loss with respect to all the other transla-
tions, which can be viewed as sentence-level 
consensus-based decoding. Word based methods 
proposed range from straightforward consensus 
voting (Bangalore et al, 2001; Matusov et al, 
2006) to more complicated word-based system 
combination model (Rosti et al, 2007; Sim et al, 
2007). Typically, the resulting systems take out-
puts of individual machine translation systems as 
input, and build a new confusion network for 
second-pass decoding. 
There have been many efforts dedicated to ad-
vance the state-of-the-art performance by com-
bining multiple systems? outputs. Most of the 
work focused on seeking better word alignment 
for consensus-based confusion network decoding 
(Matusov et al, 2006) or word-level system 
combination (He et al, 2008; Ayan et al, 2008). 
In addition to better alignment, Rosti et al 
(2008) introduced an incremental strategy for 
confusion network construction; and Hildebrand 
and Vogel (2008) proposed a hypotheses re-
ranking model for multiple systems? outputs with 
more features including word translation proba-
bility and n-gram agreement statistics. 
A common property of all the work mentioned 
above is that the combination models work on 
the basis of n-best translation lists (full hypo-
theses) of existing machine translation systems. 
However, the n-best list only presents a very 
small portion of the entire search space of a Sta-
tistical Machine Translation (SMT) model while 
a majority of the space, within which there are 
many potentially good translations, is pruned 
away in decoding. In fact, due to the limitations 
of present-day computational resources, a consi-
derable number of promising possibilities have to 
be abandoned at the early stage of the decoding 
process. It is therefore expected that exploring 
additional possibilities beyond n-best hypotheses 
lists for full sentences could bring improvements 
to consensus-based decoding. 
In this paper, we present collaborative decod-
ing (or co-decoding), a new SMT decoding 
scheme to leverage consensus information be-
tween multiple machine translation systems. In 
this scheme, instead of using a post-processing 
step, multiple machine translation decoders col-
laborate during the decoding process, and trans-
lation consensus statistics are taken into account 
to improve ranking not only for full translations, 
but also for partial hypotheses. In this way, we 
585
expect to reduce search errors caused by partial 
hypotheses pruning, maximize the contribution 
of translation consensus, and result in better final 
translations. 
We will discuss the general co-decoding mod-
el, requirements for decoders that enable colla-
borative decoding and describe the updated mod-
el structures. We will present experimental re-
sults on the data sets of NIST Chinese-to-English 
machine translation task, and demonstrate that 
co-decoding can bring significant improvements 
to baseline systems.  We also conduct extensive 
investigations when different settings of co-
decoding are applied, and make comparisons 
with related methods such as word-level system 
combination of hypothesis selection from mul-
tiple n-best lists.  
The rest of the paper is structured as follows. 
Section 2 gives a formal description of the co-
decoding model, the strategy to apply consensus 
information and hypotheses ranking in decoding. 
In Section 3, we make detailed comparison be-
tween co-decoding and related work such as sys-
tem combination and hypotheses selection out of 
multiple systems.  Experimental results and dis-
cussions are presented in Section 4. Section 5 
concludes the paper. 
2 Collaborative Decoding 
2.1 Overview 
Collaborative decoding does not present a full 
SMT model as other SMT decoders do such as 
Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). 
Instead, it provides a framework that accommo-
dates and coordinates multiple MT decoders. 
Conceptually, collaborative decoding incorpo-
rates the following four constituents:  
1. Co-decoding model. A co-decoding model 
consists of a set of member models, which 
are a set of augmented baseline models. We 
call decoders based on member models 
member decoders, and those based on base-
line models baseline decoders. In our work, 
any Maximum A Posteriori (MAP) SMT 
model with log-linear formulation (Och, 
2002) can be a qualified candidate for a 
baseline model. The requirement for a log-
linear model aims to provide a natural way to 
integrate the new co-decoding features. 
2. Co-decoding features. Member models are 
built by adding additional translation consen-
sus -based co-decoding features to baseline 
models. A baseline model can be viewed as a 
special case of member model with all co-
decoding feature values set to 0. Accordingly, 
a baseline decoder can be viewed as a special 
setting of a member decoder. 
3. Decoder coordinating. In co-decoding, each 
member decoder cannot proceed solely based 
on its own agenda. To share consensus statis-
tics with others, the decoding must be per-
formed in a coordinated way.  
4. Model training. Since we use multiple inter-
related decoders and introduce more features 
in member models, we also need to address 
the parameter estimation issue in the frame-
work of co-decoding. 
In the following sub-sections we first establish a 
general model for co-decoding, and then present 
details of feature design and decoder implemen-
tation, as well as parameter estimation in the co-
decoding framework. We leave the investigation 
of using specific member models to the experi-
ment section. 
2.2 Generic Collaborative Decoding Model 
For a given source sentence f, a member model 
in co-decoding finds the best translation ?? 
among the set of possible candidate translations 
?(?) based on a scoring function ?: 
?? = argmax???(?)?(?) (1) 
In the following, we will use ??  to denote the 
???  member decoder, and also use the notation 
??(?) for the translation hypothesis space of f 
determined by ?? . The ?
??  member model can 
be written as: 
??  ? = ?? (?, ?) + ??(?,??(?))
?,???
 (2) 
where ?? (?, ?) is the score function of the ?
??  
baseline model, and each ??(?,??(?)) is a par-
tial consensus score function with respect to ??  
and is defined over e and ?? ? :  
?? ?,?? ?  = ?? ,?  ??,?(?,?? ? ) 
?
 (3) 
where each ?? ,?(?,?? ? ) is a feature function 
based on a consensus measure between e and 
?? ? , and ??,?  is the corresponding feature 
weight. Feature index l ranges over all consen-
sus-based features in Equation 3. 
2.3 Decoder Coordination 
Before discussing the design and computation of 
translation consensus -based features, we first 
586
describe the multiple decoder coordination issue 
in co-decoding. Note that in Equation 2, though 
the baseline score function ??  ?, ?  can be 
computed inside each decoder, the case of 
??(?,??(?))  is more complicated. Because 
usually it is not feasible to enumerate the entire 
hypothesis space for machine translation, we ap-
proximate ?? ?  with n-best hypotheses by 
convention. Then there is a circular dependency 
between co-decoding features and ??(?) : on 
one hand, searching for n-best approximation of 
??(?) requires using Equation 2 to select top-
ranked hypotheses; while on the other hand, Eq-
uation 2 cannot be computed until every ??(?) 
is available.  
We address this issue by employing a boot-
strapping method, in which the key idea is that 
we can use baseline models? n-best hypotheses 
as seeds, and iteratively refine member models? 
n-best hypotheses with co-decoding. Similar to a 
typical phrase-based decoder (Koehn, 2004), we 
associate each hypothesis with a coverage vector 
c to track translated source words in it. We will 
use ??(?,?) for the set of hypotheses associated 
with c, and we also denote with ??(?) =
 ??(?,?)?  the set of all hypotheses generated 
by member decoder ??  in decoding. The co-
decoding process can be described as follows: 
1. For each member decoder ?? , perform de-
coding with a baseline model, and memorize 
all translation hypotheses generated during 
decoding in ??(?); 
2. Re-group translation hypotheses in ??(?) 
into a set of buckets  ?? ?,?  by the cover-
age vector c associated with each hypothesis; 
3. Use member decoders to re-decode source 
sentence ? with member models. For mem-
ber decoder ?? , consensus-based features of 
any hypotheses associated with coverage 
vector c are computed based on current set-
ting of ?? ?,?  for all s but k. New hypo-
theses generated by ??  in re-decoding are 
cached in ??
? (?); 
4. Update all ??(?) with ??
? (?); 
5. Iterate from step 2 to step 4 until a preset 
iteration limit is reached. 
In the iterative decoding procedure described 
above, hypotheses of different decoders can be 
mutually improved. For example, given two de-
coders ?1  and ?2  with hypotheses sets ?1  and 
?2 , improvements on ?1  enable ?2  to improve 
?2, and in turn ?1 benefits from improved ?2, 
and so forth. 
Step 2 is used to facilitate the computation of 
feature functions ?? ,?(?,?? ? ) , which require 
both e and every hypothesis in ?? ?   should be 
translations of the same set of source words. This 
step seems to be redundant for CKY-style MT 
decoders (Liu et al, 2006; Xiong et al, 2006; 
Chiang, 2005) since the grouping is immediately 
available from decoders because all hypotheses 
spanning the same range of source sentence have 
been stacked together in the same chart cell. But 
to be a general framework, this step is necessary 
for some state-of-the-art phrase-based decoders 
(Koehn, 2007; Och and Ney, 2004) because in 
these decoders, hypotheses with different cover-
age vectors can co-exist in the same bin, or hypo-
theses associated with the same coverage vector 
might appear in different bins.  
Note that a member model does not enlarge 
the theoretical search space of its baseline model, 
the only change is hypothesis scoring. By re-
running a complete decoding process, member 
model can be applied to re-score all hypotheses 
explored by a decoder. Therefore step 3 can be 
viewed as full-scale hypothesis re-ranking be-
cause the re-ranking scope is beyond the limited 
n-best hypotheses currently cached in ?? .  
In the implementation of member decoders, 
there are two major modifications compared to 
their baseline decoders. One is the support for 
co-decoding features, including computation of 
feature values and the use of augmented co-
decoding score function (Equation 2) for hypo-
thesis ranking and pruning. The other is hypothe-
sis grouping based on coverage vector and a me-
chanism to effectively access grouped hypothes-
es in step 2 and step 3. 
2.4 Co-decoding Features 
We now present the consensus-based feature 
functions  ?? ,?(?,?? ? ) introduced in Equation 
3. In this work all the consensus-based features 
have the following formulation: 
?? ,? ?,?? ?  =  ? ?? ?? ??(?, ??)
????? ? 
 (4) 
where e is a translation of f by decoder ?? (? ?
?), ? ?  is a translation in ?? ?  and ? ?? ??  is 
the posterior probability of translation ? ?  deter-
mined by decoder ??  given source sentence f. 
??(?, ??) is a consensus measure defined on e and 
??, by varying which different feature functions 
can be obtained.  
587
Referring to the log-linear model formulation, 
the translation posterior ? ?? ??  can be com-
puted as: 
? ?? ?? =
exp ??? ??  
 exp ??? ???  ??? ??? ? 
 (5) 
where ??(?) is the score function given in Equa-
tion 2, and  ? is a scaling factor following the 
work of Tromble et al (2008) 
To compute the consensus measures, we fur-
ther decompose each ?? ?, ??  into n-gram 
matching statistics between e and ??. Here we do 
not discriminate among different lexical n-grams 
and are only concerned with statistics aggrega-
tion of all n-grams of the same order. For each n-
gram of order n, we introduce a pair of comple-
mentary consensus measure functions ??+ ?, ??  
and ??? ?, ??  described as follows: 
??+ ?, ?
?  is the n-gram agreement measure 
function which counts the number of occurrences 
in ? ?of n-grams in e. So the corresponding fea-
ture value will be the expected number of occur-
rences in ?? ?  of all n-grams in e:  
??+ ?, ?? = ?(??
?+??1 , ??)
 ? ??+1
?=1
 
where ?(?,?)  is a binary indicator function ? 
? ??
?+??1 , ??  is 1 if the n-gram ??
?+??1 occurs in 
? ?  and 0 otherwise. 
??? ?, ?
?  is the n-gram disagreement meas-
ure function which is complementary to 
??+ ?, ?
? : 
??? ?, ?? =  1? ? ??
?+??1 , ??  
 ? ??+1
?=1
 
This feature is designed because ??+ ?, ?
?  
does not penalize long translation with low pre-
cision. Obviously we have the following: 
??+ ?, ?? + ??? ?, ?? =  ? ? ? + 1 
So if the weights of agreement and disagree-
ment features are equal, the disagreement-based 
features will be equivalent to the translation 
length features. Using disagreement measures 
instead of translation length there could be two 
potential advantages: 1) a length feature has been 
included in the baseline model and we do not 
need to add one; 2) we can scale disagreement 
features independently and gain more modeling 
flexibility. 
Similar to a language model score, n-gram 
consensus -based feature values cannot be 
summed up from smaller hypotheses. Instead, it 
must be re-computed when building each new 
hypothesis. 
2.5 Model Training 
We adapt the Minimum Error Rate Training 
(MERT) (Och, 2003) algorithm to estimate pa-
rameters for each member model in co-decoding. 
Let ??  be the feature weight vector for member 
decoder ?? , the training procedure proceeds as 
follows: 
1. Choose initial values for ?1 ,? ,??   
2. Perform co-decoding using all member de-
coders on a development set D with 
?1 ,? ,?? . For each decoder ?? , find a new 
feature weight vector ??
?  which optimizes 
the specified evaluation criterion L on D us-
ing the MERT algorithm based on the n-best 
list ??  generated by ?? : 
??
? = argmax? ? (?|?,??  ,?)) 
where T denotes the translations selected by 
re-ranking the translations in ??  using a 
new feature weight vector ? 
3. Let ?1 = ?1
? ,? ,?? = ??
?  and repeat step 2 
until convergence or a preset iteration limit is 
reached. 
 
Figure 1. Model training for co-decoding 
In step 2, there is no global criterion to optim-
ize the co-decoding parameters across member 
models. Instead, parameters of different member 
models are tuned to maximize the evaluation cri-
teria on each member decoder?s own n-best out-
put.  Figure 1 illustrates the training process of 
co-decoding with 2 member decoders. 
Source sentence
decoder
1
decoder
2
?1
MERT
?2
MERT
co-decoding
ref
1?? 2??
588
2.6 Output Selection 
Since there is more than one model in co-
decoding, we cannot rely on member model?s 
score function to choose one best translation 
from multiple decoders? outputs because the 
model scores are not directly comparable. We 
will examine the following two system combina-
tion -based solutions to this task: 
? Word-level system combination (Rosti et al, 
2007) of member decoders? n-best outputs  
? Hypothesis selection from combined n-best 
lists as proposed in Hildebrand  and Vogel 
(2008) 
3 Experiments 
In this section we present experiments to eva-
luate the co-decoding method. We first describe 
the data sets and baseline systems. 
3.1 Data and Metric 
We conduct our experiments on the test data 
from the NIST 2005 and NIST 2008 Chinese-to-
English machine translation tasks. The NIST 
2003 test data is used for development data to 
estimate model parameters. Statistics of the data 
sets are shown in Table 1. In our experiments all 
the models are optimized with case-insensitive 
NIST version of BLEU score and we report re-
sults using this metric in percentage numbers. 
 
Data set # Sentences # Words 
NIST 2003 (dev) 919 23,782 
NIST 2005 (test) 1,082 29,258 
NIST 2008 (test) 1,357 31,592 
Table 1: Data set statistics 
We use the parallel data available for the 
NIST 2008 constrained track of Chinese-to-
English machine translation task as bilingual 
training data, which contains 5.1M sentence 
pairs, 128M Chinese words and 147M English 
words after pre-processing. GIZA++ is used to 
perform word alignment in both directions with 
default settings, and the intersect-diag-grow me-
thod is used to generate symmetric word align-
ment refinement. 
The language model used for all models (in-
clude decoding models and system combination 
models described in Section 2.6) is a 5-gram 
model trained with the English part of bilingual 
data and xinhua portion of LDC English Giga-
word corpus version 3. 
3.2 Member Decoders 
We use three baseline decoders in the experi-
ments. The first one (SYS1) is re-implementation 
of Hiero, a hierarchical phrase-based decoder. 
Phrasal rules are extracted from all bilingual sen-
tence pairs, while rules with variables are ex-
tracted only from selected data sets including 
LDC2003E14, LDC2003E07, LDC2005T06 and 
LDC2005T10, which contain around 350,000 
sentence pairs, 8.8M Chinese words and 10.3M 
English words. The second one (SYS2) is a BTG 
decoder with lexicalized reordering model based 
on maximum entropy principle as proposed by 
Xiong et al (2006). We use all the bilingual data 
to extract phrases up to length 3. The third one 
(SYS3) is a string-to-dependency tree ?based 
decoder as proposed by Shen et al (2008). For 
rule extraction we use the same setting as in 
SYS1. We parsed the language model training 
data with Berkeley parser, and then trained a de-
pendency language model based on the parsing 
output. All baseline decoders are extended with 
n-gram consensus ?based co-decoding features 
to construct member decoders. By default, the 
beam size of 20 is used for all decoders in the 
experiments. We run two iterations of decoding 
for each member decoder, and hold the value of 
?  in Equation 5 as a constant 0.05, which is 
tuned on the test data of NIST 2004 Chinese-to-
English machine translation task. 
3.3 Translation Results 
We first present the overall results of co-
decoding on both test sets using the settings as 
we described. For member decoders, up to 4-
gram agreement and disagreement features are 
used. We also implemented the word-level sys-
tem combination (Rosti et al, 2007) and the hy-
pothesis selection method (Hildebrand and Vogel, 
2008). 20-best translations from all decoders are 
used in the experiments for these two combina-
tion methods. Parameters for both system com-
bination and hypothesis selection are also tuned 
on NIST 2003 test data. The results are shown in 
Table 2. 
 
 NIST 2005 NIST 2008 
SYS1 38.66/40.08 27.67/29.19 
SYS2 38.04/39.93 27.25/29.14 
SYS3 39.50/40.32 28.75/29.68 
Word-level Comb 40.45/40.85 29.52/30.35 
Hypo Selection 40.09/40.50 29.02/29.71 
Table 2: Co-decoding results on test data 
589
In the Table 2, the results of a member decod-
er and its corresponding baseline decoder are 
grouped together with the later one for the mem-
ber decoders. On both test sets, every member 
decoder performs significantly better than its 
baseline decoder (using the method proposed in 
Koehn (2004) for statistical significance test).  
We apply system combination methods to the 
n-best outputs of both baseline decoders and 
member decoders. We notice that we can achieve 
even better performance by applying system 
combination methods to member decoders? n-
best outputs. However, the improvement margins 
are smaller than those of baseline decoders on 
both test sets. This could be the result of less di-
versified outputs from co-decoding than those 
from baseline decoders. In particular, the results 
for hypothesis selection are only slightly better 
than the best system in co-decoding.  
We also evaluate the performance of system 
combination using different n-best sizes, and the 
results on NIST 2005 data set are shown in Fig-
ure 2, where bl- and co- legends denote combina-
tion results of baseline decoding and co-decoding 
respectively. From the results we can see that 
combination based on co-decoding?s outputs per-
forms consistently better than that based on base-
line decoders? outputs for all n-best sizes we ex-
perimented with. However, we did not observe 
any significant improvements for both combina-
tion schemes when n-best size is larger than 20. 
 
Figure 2. Performance of system combination 
with different sizes of n-best lists 
One interesting observation in Table 2 is that 
the performance gap between baseline decoders 
is narrowed through co-decoding. For example, 
the 1.5 points gap between SYS2 and SYS3 on 
NIST 2008 data set is narrowed to 0.5. Actually 
we find that the TER score between two member 
decoders? outputs are significantly reduced (as 
shown in Table 3), which indicates that the out-
puts become more similar due to the use of con-
sensus information. For example, the TER score 
between SYS2 and SYS3 of the NIST 2008 out-
puts are reduced from 0.4238 to 0.2665.  
 
 NIST 2005 NIST 2008 
SYS1 vs. SYS2 0.3190/0.2274 0.4016/0.2686 
SYS1 vs. SYS3 0.3252/0.1840 0.4176/0.2469 
SYS2 vs. SYS3 0.3498/0.2171 0.4238/0.2665 
Table 3: TER scores between co-decoding  
translation outputs 
In the rest of this section we run a series of 
experiments to investigate the impacts of differ-
ent factors in co-decoding. All the results are 
reported on NIST 2005 test set.  
We start with investigating the performance 
gain due to partial hypothesis re-ranking. Be-
cause Equation 3 is a general model that can be 
applied to both partial hypothesis and n-best (full 
hypothesis) re-scoring, we compare the results of 
both cases. Figure 3 shows the BLEU score 
curves with up to 1000 candidates used for re-
ranking. In Figure 3, the suffix p denotes results 
for partial hypothesis re-ranking, and f for n-best 
re-ranking only. For partial hypothesis re-
ranking, obtaining more top-ranked results re-
quires increasing the beam size, which is not af-
fordable for large numbers in experiments. We 
work around this issue by approximating beam 
sizes larger than 20 by only enlarging the beam 
size for the span covering the entire source sen-
tence. From Figure 3 we can see that all decoders 
can gain improvements before the size of candi-
date set reaches 100. When the size is larger than 
50, co-decoding performs consistently and sig-
nificantly better than the re-ranking results on 
any baseline decoder?s n-best outputs.  
 
Figure 3. Partial hypothesis vs. n-best re-ranking 
results on NIST 2005 test data 
Figure 4 shows the BLEU scores of a two-
system co-decoding as a function of re-decoding 
iterations. From the results we can see that the 
results for both decoders converge after two ite-
rations.  
In Figure 4, iteration 0 denotes decoding with 
baseline model. The setting of iteration 1 can be 
viewed as the case of partial co-decoding, in 
39.5
39.8
40.0
40.3
40.5
40.8
41.0
41.3
10 20 50 100 200
bl-comb
co-comb
bl-hyposel
co-hyposel
38.0
38.5
39.0
39.5
40.0
40.5
41.0
41.5
10 20 50 100 200 500 1000
SYS1f
SYS2f
SYS3f
SYS1p
SYS2p
SYS3p
590
which one decoder uses member model and the 
other keeps using baseline model. The results 
show that member models help each other: al-
though improvements can be made using a single 
member model, best BLEU scores can only be 
achieved when both member models are used as 
shown by the results of iteration 2. The results 
also help justify the independent parameter esti-
mation of member decoders described in Section 
2.5, since optimizing the performance of one de-
coder will eventually bring performance im-
provements to all member decoders. 
 
Figure 4. Results using incremental iterations  
in co-decoding 
Next we examine the impacts of different con-
sensus-based features in co-decoding. Table 4 
shows the comparison results of a two-system 
co-decoding using different settings of n-gram 
agreement and disagreement features. It is clear-
ly shown that both n-gram agreement and disa-
greement types of features are helpful, and using 
them together is the best choice. 
 SYS1 SYS2 
Baseline 38.66 38.04 
+agreement ?disagreement 39.36 39.02 
?agreement +disagreement  39.12 38.67 
+agreement +disagreement 39.68 39.61 
Table 4: Co-decoding with/without n-gram 
agreement and disagreement features 
In Table 5 we show in another dimension the 
impact of consensus-based features by restricting 
the maximum order of n-grams used to compute 
agreement statistics. 
 SYS1 SYS2 
1 38.75 38.27 
2  39.21 39.10 
3 39.48 39.25 
4 39.68 39.61 
5 39.52 39.36 
6 39.58 39.47 
Table 5: Co-decoding with varied n-gram agree-
ment and disagreement features 
From the results we do not observe BLEU im-
provement for ? > 4. One reason could be that 
the data sparsity for high-order n-grams leads to 
over fitting on development data. 
We also empirically investigated the impact of 
scaling factor ? in Equation 5. It is observed in 
Figure 5 that the optimal value is between 0.01 ~ 
0.1 on both development and test data.  
 
Figure 5. Impact of scaling factor ?  
4 Discussion 
Word-level system combination (system combi-
nation hereafter) (Rosti et al, 2007; He et al, 
2008) has been proven to be an effective way to 
improve machine translation quality by using 
outputs from multiple systems. Our method is 
different from system combination in several 
ways. System combination uses unigram consen-
sus only and a standalone decoding model irrele-
vant to single decoders. Our method uses agree-
ment information of n-grams, and consensus fea-
tures are integrated into decoding models. By 
constructing a confusion network, system com-
bination is able to generate new translations dif-
ferent from any one in the input n-best lists, 
while our method does not extend the search 
spaces of baseline decoding models. Member 
decoders only change the scoring and ranking of 
the candidates in the search spaces. Results in 
Table 2 show that these two approaches can be 
used together to obtain further improvements. 
The work on multi-system hypothesis selec-
tion of Hildebrand and Vogel (2008) bears more 
resemblance to our method in that both make use 
of n-gram agreement statistics. They also empiri-
cally show that n-gram agreement is the most 
important factor for improvement apart from 
language models.  
Lattice MBR decoding (Tromble et al, 2008) 
also uses n-gram agreement statistics. Their work 
focuses on exploring larger evidence space by 
using a translation lattice instead of the n-best list. 
They also show the connection between expected 
n-gram change and corpus Log-BLEU loss. 
37.5
38.0
38.5
39.0
39.5
40.0
0 1 2 3 4
SYS1
SYS2
38.0
38.5
39.0
39.5
40.0
0 0.01 0.03 0.05 0.1 0.2 0.5 1
Dev SYS1
Dev SYS2
Test SYS1
Test SYS2
591
5 Conclusion 
Improving machine translation with multiple sys-
tems has been a focus in recent SMT research. In 
this paper, we present a framework of collabora-
tive decoding, in which multiple MT decoders 
are coordinated to search for better translations 
by re-ranking partial hypotheses using aug-
mented log-linear models with translation con-
sensus -based features. An iterative approach is 
proposed to re-rank all hypotheses explored in 
decoding. Experimental results show that with 
collaborative decoding every member decoder 
performs significantly better than its baseline 
decoder. In the future, we will extend our method 
to use lattice or hypergraph to compute consen-
sus statistics instead of n-best lists. 
References  
Necip Fazil Ayan,  Jing Zheng, and Wen Wang. 2008. 
Improving alignments for better confusion net-
works for combining machine translation systems. 
In Proc. Coling, pages 33-40. 
Srinivas Bangalore, German Bordel and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In 
Proc. ASRU, pages 351-354. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
ACL, pages 263-270. 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems.  In Proc. EMNLP, pages 
98-107. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
8th AMTA conference, pages 254-261. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. EMNLP. 
Philipp Koehn, 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
model. In Proc. 6th AMTA Conference, pages 115-
124. 
Philipp Koehn, Hieu Hoang, Alexandra Brich, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, Evan Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. In 
Proc. ACL, demonstration session. 
Shankar Kumar and William Byrne 2004. Minimum 
Bayes-Risk Decoding for Statistical Machine 
Translation. In HLT-NAACL, pages 169-176. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. ACL-Coling, pages 609-616. 
Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enchanced 
hypotheses alignment. In Proc. EACL, pages 33-
40. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. In Proc. ACL, pages 295-
302. 
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 
160-167. 
Franz Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), pages 417-
449 
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, 
Spyros Matsoukas, Richard Schwartz, and Bonnie 
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In HLT-NAACL, pages 
228-235 
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with 
application to machine translation system combina-
tion. In Proc. Of the Third ACL Workshop on Sta-
tistical Machine Translation, pages 183-186. 
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. 
Woodland. 2007. Consensus network decoding for 
statistical machine translation system combination. 
In ICASSP. 
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. HLT-ACL, pages 577-585. 
Roy W. Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice minimum 
bayes-risk decoding for statistical machine transla-
tion. In Proc. EMNLP, pages 620-629. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proc. ACL, pages 
521-528. 
 
592
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 304?312,
Beijing, August 2010
Translation Model Generalization using Probability Averaging 
for Machine Translation 
Nan Duan1, Hong Sun 
School of Computer Science and Technology 
Tianjin University 
v-naduan@microsoft.com 
v-hongsun@microsoft.com 
Ming Zhou 
Microsoft Research Asia 
 
mingzhou@microsoft.com 
Abstract 
Previous methods on improving transla-
tion quality by employing multiple SMT 
models usually carry out as a second-
pass decision procedure on hypotheses 
from multiple systems using extra fea-
tures instead of using features in existing 
models in more depth. In this paper, we 
propose translation model generalization 
(TMG), an approach that updates proba-
bility feature values for the translation 
model being used based on the model it-
self and a set of auxiliary models, aiming 
to enhance translation quality in the first-
pass decoding. We validate our approach 
on translation models based on auxiliary 
models built by two different ways. We 
also introduce novel probability variance 
features into the log-linear models for 
further improvements. We conclude that 
our approach can be developed indepen-
dently and integrated into current SMT 
pipeline directly. We demonstrate BLEU 
improvements on the NIST Chinese-to-
English MT tasks for single-system de-
codings, a system combination approach 
and a model combination approach.1 
1 Introduction 
Current research on Statistical Machine Transla-
tion (SMT) has made rapid progress in recent 
decades. Although differed on paradigms, such 
as phrase-based (Koehn, 2004; Och and Ney, 
2004), hierarchical phrase-based (Chiang, 2007) 
and syntax-based (Galley et al, 2006; Shen et 
al., 2008; Huang, 2008), most SMT systems fol-
                                                 
1 This work has been done while the author was visiting 
Microsoft Research Asia. 
low the similar pipeline and share common 
translation probability features which constitute 
the principal components of translation models. 
However, due to different model structures or 
data distributions, these features are usually as-
signed with different values in different transla-
tion models and result in translation outputs with 
individual advantages and shortcomings. 
In order to obtain further improvements, many 
approaches have been explored over multiple 
systems: system combination based on confu-
sion network (Matusov et al, 2006; Rosti et al, 
2007; Li et al, 2009a) develop on multiple N-
best outputs and outperform primary SMT sys-
tems; consensus-based methods (Li et al, 2009b; 
DeNero et al, 2010), on the other hand, avoid 
the alignment problem between translations can-
didates and utilize n-gram consensus, aiming to 
optimize special decoding objectives for hypo-
thesis selection. All these approaches act as the 
second-pass decision procedure on hypotheses 
from multiple systems by using extra features. 
They begin to work only after the generation of 
translation hypotheses has been finished. 
In this paper, we propose translation model 
generalization (TMG), an approach that takes 
effect during the first-pass decoding procedure 
by updating translation probability features for 
the translation model being used based on the 
model itself and a set of auxiliary models. Baye-
sian Model Averaging is used to integrate values 
of identical features between models. Our con-
tributions mainly include the following 3 aspects: 
? Alleviate the model bias problem based on 
translation models with different paradigms.  
Because of various model constraints, trans-
lation models based on different paradigms 
could have individual biases. For instance, 
phrase-based models prefer translation pairs 
with high frequencies and assign them high 
304
probability values; yet such pairs could be 
disliked or even be absent in syntax-based 
models because of their violation on syntac-
tic restrictions. We alleviate such model bias 
problem by using the generalized probability 
features in first-pass decoding, which com-
puted based on feature values from all trans-
lation models instead of any single one. 
? Alleviate the over-estimation problem based 
on translation models with an identical pa-
radigm but different training corpora.  
In order to obtain further improvements by 
using an existing training module built for a 
specified model paradigm, we present a ran-
dom data sampling method inspired by bag-
ging (Breiman, 1996) to construct transla-
tion model ensembles from a unique data set 
for usage in TMG. Compared to results of 
TMG based on models with different para-
digms, TMG based on models built in such a 
way can achieve larger improvements. 
? Novel translation probability variance fea-
tures introduced. 
We present how to compute the variance for 
each probability feature based on its values 
in different involved translation models with 
prior model probabilities. We add them into 
the log-linear model as new features to make 
current SMT models to be more flexible. 
The remainder of this paper is organized as 
follows: we review various translation models in 
Section 2. In Section 3, we first introduce Baye-
sian Model Averaging method for SMT tasks 
and present a generic TMG algorithm based on it. 
We then discuss two solutions for constructing 
TM ensembles for usage in TMG. We next in-
troduce probability variance features into current 
SMT models as new features. We evaluate our 
method on four state-of-the-art SMT systems, a 
system combination approach and a model com-
bination approach. Evaluation results are shown 
in Section 4. In Section 5, we discuss some re-
lated work. We conclude the paper in Section 6. 
2 Summary of Translation Models 
Translation Model (TM) is the most important 
component in current SMT framework. It 
provides basic translation units for decoders with 
a series of probability features for model 
scoring. Many literatures have paid attentions to 
TMs from different aspects: DeNeefe et al 
(2007) compared strengths and weaknesses of a 
phrase-based TM and a syntax-based TM from 
the statistic aspect; Zollmann et al (2008) made 
a systematic comparison of three TMs, including 
phrasal, hierarchical and syntax-based, from the 
performance aspect; and Auli et al (2009) made 
a systematic analysis of a phrase-based TM and 
a hierarchical TM from the search space aspect. 
Given a word-aligned training corpus, we 
separate a TM training procedure into two phas-
es: extraction phase and parameterization phase. 
Extraction phase aims to pick out all valid 
translation pairs that are consistent with pre-
defined model constraints. We summarize cur-
rent TMs based on their corresponding model 
constraints into two categories below: 
? String-based TM (string-to-string): reserves 
all translation pairs that are consistent with 
word alignment and satisfy length limitation. 
SMT systems using such TMs can benefit 
from a large convergence of translation pairs. 
? Tree-based TM (string-to-tree, tree-to-string 
or tree-to-tree): needs to obey syntactic re-
strictions in one side or even both sides of 
translation candidates. The advantage of us-
ing such TMs is that translation outputs 
trend to be more syntactically well-formed. 
Parameterization phase aims to assign a series 
of probability features to each translation pair. 
These features play the most important roles in 
the decision process and are shared by most cur-
rent SMT decoders. In this paper, we mainly 
focus on the following four commonly used do-
minant probability features including: 
? translation probability features in two direc-
tions:         and         
? lexical weight features in two directions: 
           and            
Both string-based and tree-based TMs are 
state-of-the-art models, and each extraction ap-
proach has its own strengths and weaknesses 
comparing to others. Due to different predefined 
model constraints, translation pairs extracted by 
different models usually have different distribu-
tions, which could directly affect the resulting 
probability feature values computed in parame-
305
terization phase. In order to utilize translation 
pairs more fairly in decoding, it is desirable to 
use more information to measure the quality of 
translation pairs based on different TMs rather 
than totally believing any single one. 
3 Translation Model Generalization 
We first introduce Bayesian Model Averaging 
method for SMT task. Based on it, we then for-
mally present the generic TMG algorithm. We 
also provide two solutions for constructing TM 
ensembles as auxiliary models. We last intro-
duce probability variance features based on mul-
tiple TMs for further improvements. 
3.1 Bayesian Model Averaging for SMT 
Bayesian Model Averaging (BMA) (Hoeting et 
al., 1999) is a technique designed to solve uncer-
tainty inherent in model selection.  
Specifically, for SMT tasks,   is a source sen-
tence,  is the training data,   is the  
th SMT 
model trained on    ,            represents 
the probability score predicted by   that   can 
be translated into a target sentence  . BMA pro-
vides a way to combine decisions of all     
SMT models by computing the final translation 
probability score              as follows: 
                               
 
   
  (1) 
where          is the prior probability that 
   is a true model. For convenience, we will 
omit all symbols    in following descriptions. 
Ideally, if all involved models           
share the same search space, then translation 
hypotheses could only be differentiated in prob-
ability scores assigned by different SMT models. 
In such case, BMA can be straightly developed 
on the whole SMT models in either span level or 
sentence level to re-compute translation scores 
of hypotheses for better rankings. However, be-
cause of various reasons, e.g. different pruning 
methods, different training data used, different 
generative capabilities of SMT models, search 
spaces between different models are always not 
identical. Thus, it is intractable to develop BMA 
on the whole SMT model level directly. 
As a tradeoff, we notice that translation pairs 
between different TMs share a relatively large 
convergence because of word length limitation. 
So we instead utilize BMA method to multiple 
TMs by re-computing values of probability fea-
tures between them, and we name this process as 
translation model generalization. 
3.2 A Generic BMA-based TMG Algorithm 
For a translation model  , TMG aims to re-
compute its values of probability features based 
on itself and   collaborative TMs          . 
We describe the re-computation process for an 
arbitrary feature              as follows: 
                            
 
   
  (2) 
where            is the feature value assigned 
by  . We denote   as the main model, and 
other collaborative TMs as auxiliary models. 
Figure 1 describes an example of TMG on two 
TMs, where the main model is a phrasal TM. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Equation 2 is a general framework that can be 
applied to all TMs. The only limitation is that 
the segmentation (or tokenization) standards for 
source (or target) training sentences should be 
identical for all models. We describe the generic 
TMG procedure in Algorithm 12. 
                                                 
2 In this paper, since all data sets used have relative large 
sizes and all SMT models have similar performances, we 
heuristically set al       equally to        . 
 
Figure 1. TMG applied to a phrasal TM (main 
model) and a syntax-based TM (auxiliary mod-
el). The value of a translation probability feature 
           ??  in TM1 is de-valued (from 0.6 
to 0.3), in which ?join the? is absent in TM2 be-
cause of its bad syntactic structure. 
           ??  
=0.6 
Phrase-based TM1 
(Main model) 
Syntax-based TM2 
(Auxiliary model) 
      =0.5       =0.5 
 
Generalized TM1 
           ??  
=0.6*0.5+0.0*0.5=0.3 
           ??  
=0.0 
306
Algorithm 1: TMG for a main model   
1: for the  th auxiliary TM do 
2:          run training procedure on    with specified 
model constraints and generate   
3: end for 
4: for each translation pair         in   do 
5:  for each probability feature           do 
6:          for each translation model   do 
7:      if          is contained in   then 
8:                                         
9    end if 
10:   end for 
11:  end for 
12: end for 
13: return the generalized   for SMT decoding 
 
3.3 Auxiliary Model Construction 
In order to utilize TMG, more than one TM as 
auxiliary models is needed. Building TMs with 
different paradigms is one solution. For exam-
ple, we can build a syntax-based TM as an aux-
iliary model for a phrase-based TM. However, it 
has to re-implement more complicated TM train-
ing modules besides the existing one. 
In this sub-section, we present an alternative 
solution to construct auxiliary model ensembles 
by using the existing training module with dif-
ferent training data extracted from a unique data 
set. We describe the general procedure for con-
structing   auxiliary models as follows: 
1) Given a unique training corpus  , we ran-
domly sample    bilingual sentence pairs 
without replacement and denote them as   . 
  is a number determined empirically; 
2) Based on  , we re-do word alignment and 
train an auxiliary model   using the exist-
ing training module; 
3) We execute Step 1 and Step 2 iteratively for 
  times, and finally obtain   auxiliary mod-
els. The optimal setting of   for TMG is al-
so determined empirically. 
With all above steps finished, we can perform 
TMG as we described in Algorithm 1 based on 
the   auxiliary models generated already. 
The random data sampling process described 
above is very similar to bagging except for it not 
allowing replacement during sampling. By mak-
ing use of this process, translation pairs with low 
frequencies have relatively high probabilities to 
be totally discarded, and in resulting TMs, their 
probabilities could be zero; meanwhile, transla-
tion pairs with high frequencies still have high 
probabilities to be reserved, and hold similar 
probability feature values in resulting TMs com-
paring to the main model. Thus, after TMG pro-
cedure, feature values could be smoothed for 
translation pairs with low frequencies, and be 
stable for translation pairs with high frequencies. 
From this point of view, TMG can also be seen 
as a TM smoothing technique based on multiple 
TMs instead of single one such as Foster et al 
(2006). We will see in Section 4 that TMG based 
on TMs generated by both of these two solutions 
can improve translation quality for all baseline 
decoders on a series of evaluation sets. 
3.4 Probability Variance Feature 
The re-computed values of probability features 
in Equation 2 are actually the feature expecta-
tions based on their values from all involved 
TMs. In order to give more statistical meanings 
to translation pairs, we also compute their cor-
responding feature variances based on feature 
expectations and TM-specified feature values 
with prior probabilities. We introduce such va-
riances as new features into the log-linear model 
for further improvements. Our motivation is to 
quantify the differences of model preferences 
between TMs for arbitrary probability features. 
The variance for an arbitrary probability fea-
ture         can be computed as follows: 
                     
       
 
   
  (3) 
where        is the feature expectation computed 
by Equation 2,       is the feature value pre-
dicted by  , and        is the prior probabil-
ity for  . Each probability feature now corres-
ponds to a variance score. We extend the origi-
nal feature set of   with variance features add-
ed in and list the updated set below: 
? translation probability expectation features 
in two directions:           and           
? translation probability variance features in 
two directions:          and          
? lexical weight expectation features in two 
directions:           
   and         
     
? lexical weight variance features in two di-
rections:          
   and        
     
307
4 Experiments 
4.1 Data Condition 
We conduct experiments on the NIST Chinese-
to-English MT tasks. We tune model parameters 
on the NIST 2003 (MT03) evaluation set by 
MERT (Och, 2003), and report results on NIST 
evaluation sets including the NIST 2004 (MT04), 
the NIST 2005 (MT05), the newswire portion of 
the NIST 2006 (MT06) and 2008 (MT08). Per-
formances are measured in terms of the case-
insensitive BLEU scores in percentage numbers. 
Table 1 gives statistics over these evaluation sets. 
 
 MT03 MT04 MT05 MT06 MT08 
Sent 919 1,788 1,082 616 691 
Word 23,788 48,215 29,263 17,316 17,424 
Table 1. Statistics on dev/test evaluation sets 
We use the selected data that picked out from 
the whole data available for the NIST 2008 con-
strained track of Chinese-to-English machine 
translation task as the training corpora, including 
LDC2003E07, LDC2003E14, LDC2005T06, 
LDC2005T10, LDC2005E83, LDC2006E26, 
LDC2006E34, LDC2006E85 and LDC2006E92, 
which contain about 498,000 sentence pairs after 
pre-processing. Word alignments are performed 
by GIZA++ (Och and Ney, 2000) in both direc-
tions with an intersect-diag-grow refinement. 
A traditional 5-gram language model (LM) 
for all involved systems is trained on the English 
side of all bilingual data plus the Xinhua portion 
of LDC English Gigaword Version 3.0. A lexi-
calized reordering model (Xiong et al, 2006) is 
trained on the selected data in maximum entropy 
principle for the phrase-based system. A tri-
gram target dependency LM (DLM) is trained 
on the English side of the selected data for the 
dependency-based hierarchical system. 
 
 
 
 
 
 
 
 
4.2 MT System Description 
We include four baseline systems. The first one 
(Phr) is a phrasal system (Xiong et al, 2006) 
based on Bracketing Transduction Grammar 
(Wu, 1997) with a lexicalized reordering com-
ponent based on maximum entropy model. The 
second one (Hier) is a hierarchical phrase-based 
system (Chiang, 2007) based on Synchronous 
Context Free Grammar (SCFG). The third one 
(Dep) is a string-to-dependency hierarchical 
phrase-based system (Shen et al, 2008) with a 
dependency language model, which translates 
source strings to target dependency trees. The 
fourth one (Synx) is a syntax-based system (Gal-
ley et al, 2006) that translates source strings to 
target syntactic trees. 
4.3 TMG based on Multiple Paradigms 
We develop TMG for each baseline system?s 
TM based on the other three TMs as auxiliary 
models. All prior probabilities of TMs are set 
equally to 0.25 heuristically as their similar per-
formances. Evaluation results are shown in Ta-
ble 2, where gains more than 0.2 BLEU points 
are highlighted as improved cases. Compared to 
baseline systems, systems based on generalized 
TMs improve in most cases (18 times out of 20). 
We also notice that the improvements achieved 
on tree-based systems (Dep and Synx) are rela-
tively smaller than those on string-based systems 
(Phr and Hier). A potential explanation can be 
that with considering more syntactic restrictions, 
tree-based systems suffer less than string-based 
systems on the over-estimation problem. We do 
not present further results with variance features 
added because of their consistent un-promising 
numbers. We think this may be due to the consi-
derable portion of non-overlapping translation 
pairs between main model and auxiliary models, 
which cause the variances not so accurate. 
 
 
 
 
 
 
 
 
 
 
 
  MT03(dev) MT04 MT05 MT06 MT08 Average 
Phr 
Baseline 40.45 39.21 38.03 34.24 30.21 36.43 
TMG 41.19(+0.74) 39.74(+0.53) 38.39(+0.36) 34.71(+0.47) 30.69(+0.48) 36.94(+0.51) 
Hier 
Baseline 41.30 39.63 38.83 34.63 30.46 36.97 
TMG 41.67(+0.37) 40.25(+0.62) 39.11(+0.28) 35.78(+1.15) 31.17(+0.71) 37.60(+0.63) 
Dep 
Baseline 41.10 39.81 39.47 35.72 30.50 37.32 
TMG 41.37(+0.27) 39.92(+0.11) 39.91(+0.44) 35.99(+0.27) 31.07(+0.57) 37.65(+0.33) 
Synx 
Baseline 41.02 39.88 39.47 36.41 32.15 37.79 
TMG 41.26(+0.24) 40.09(+0.21) 39.90(+0.43) 36.77(+0.36) 32.15(+0.00) 38.03(+0.24) 
Table 2. Results of TMG based on TMs with different paradigms 
 
308
4.4 TMG based on Single Paradigm 
We then evaluate TMG based on auxiliary mod-
els generated by the random sampling method. 
We first decide the percentage of training data 
to be sampled. We empirically vary this number 
by 20%, 40%, 60%, 80% and 90% and use each 
sampled data to train an auxiliary model. We 
then run TMG on the baseline TM with different 
auxiliary model used each time. For time saving, 
we only evaluate on MT03 for Phr in Figure 2. 
 
Figure 2. Affects of different percentages of data 
The optimal result is achieved when the per-
centage is 80%, and we fix it as the default value 
in following experiments. 
We then decide the number of auxiliary mod-
els used for TMG by varying it from 1 to 5. We 
list different results on MT03 for Phr in Figure 3. 
 
Figure 3. Affects of different numbers of auxi-
liary models 
 
 
 
 
 
 
 
 
 
 
The optimal result is achieved when the num-
ber of auxiliary models is 4, and we fix it as the 
default value in following experiments. 
We now develop TMG for each baseline sys-
tem?s TM based on auxiliary models constructed 
under default settings determined above. Evalua-
tion results are shown in Table 3. We also inves-
tigate the affect of variance features for perfor-
mance, whose results are denoted as TMG+Var. 
From Table 3 we can see that, compared to 
the results on baseline systems, systems using 
generalized TMs obtain improvements on almost 
all evaluation sets (19 times out of 20). With 
probability variance features added further, the 
improvements become even more stable than the 
ones using TMG only (20 times out of 20). Simi-
lar to the trend in Table 2, we also notice that 
TMG method is more preferred by string-based 
systems (Phr and Hier) rather than tree-based 
systems (Dep and Synx). This makes our con-
clusion more solidly that syntactic restrictions 
can help to alleviate the over-estimation problem. 
4.5 Analysis on Phrase Coverage 
We next empirically investigate on the transla-
tion pair coverage between TM ensembles built 
by different ways, and use them to analyze re-
sults got from previous experiments. Here, we 
only focus on full lexicalized translation entries 
between models. Those entries with variables 
are out of consideration in comparisons because 
of their model dependent properties. 
Phrase pairs in the first three TMs have a 
length limitation in source side up to 3 words, 
and each source phrase can be translated to at 
most 20 target phrases.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
40.0
40.5
41.0
41.5
0% 20% 40% 60% 80% 90%
Phr
40.0
40.5
41.0
41.5
42.0
0 1 2 3 4 5
Phr
  MT03(dev) MT04 MT05 MT06 MT08 Average 
Phr 
Baseline 40.45 39.21 38.03 34.24 30.21 36.43 
TMG 41.77(+1.32) 40.28(+1.07) 39.13(+1.10) 35.38(+1.14) 31.12(+0.91) 37.54(+1.11) 
TMG+Var 41.77(+1.32) 40.31(+1.10) 39.43(+1.30) 35.61(+1.37) 31.62(+1.41) 37.74(+1.31) 
Hier 
Baseline 41.30 39.63 38.83 34.63 30.46 36.97 
TMG 42.28(+0.98) 40.45(+0.82) 39.61(+0.78) 35.67(+1.04) 31.54(+1.08) 37.91(+0.94) 
TMG+Var 42.42(+1.12) 40.55(+0.92) 39.69(+0.86) 35.55(+0.92) 31.41(+0.95) 37.92(+0.95) 
Dep 
Baseline 41.10 39.81 39.47 35.72 30.50 37.32 
TMG 41.49(+0.39) 40.20(+0.39) 40.00(+0.53) 36.13(+0.41) 31.24(+0.74) 37.81(+0.49) 
TMG+Var 41.72(+0.62) 40.57(+0.76) 40.44(+0.97) 36.15(+0.43) 31.31(+0.81) 38.04(+0.72) 
Synx 
Baseline 41.02 39.88 39.47 36.41 32.15 37.79 
TMG 41.18(+0.16) 40.30(+0.42) 39.90(+0.43) 36.99(+0.58) 32.45(+0.30) 38.16(+0.37) 
TMG+Var 41.42(+0.40) 40.55(+0.67) 40.17(+0.70) 36.89(+0.48) 32.51(+0.36) 38.31(+0.52) 
Table 3. Results of TMG based on TMs constructed by random data sampling 
 
309
For the fourth TM, these two limitations are 
released to 4 words and 30 target phrases. We 
treat phrase pairs identical on both sides but with 
different syntactic labels in the fourth TM as a 
unique pair for conveniences in statistics.  
We first make statistics on TMs with different 
paradigms in Table 4. We can see from Table 4 
that only slightly over half of the phrase pairs 
contained by the four involved TMs are common, 
which is also similar to the conclusion drawn in 
DeNeefe et al (2006). 
 
Models #Translation Pair #Percentage 
Phr 1,222,909 50.6% 
Hier 1,222,909 50.6% 
Dep 1,087,198 56.9% 
Synx 1,188,408 52.0% 
Overlaps 618,371 - 
Table 4. Rule statistics on TMs constructed by 
different paradigms 
We then make statistics on TMs with identical 
paradigm in Table 5. For each baseline TM and 
its corresponding four auxiliary models con-
structed by random data sampling, we count the 
number of phrase pairs that are common be-
tween them and compute the percentage num-
bers based on it for each TM individually. 
 
Models TM0 TM1 TM2 TM3 TM4 
Phr 61.8% 74.0% 74.1% 73.9% 74.1% 
Hier 61.8% 74.0% 74.1% 73.9% 74.1% 
Dep 60.8% 73.6% 73.6% 73.5% 73.7% 
Synx 57.2% 68.4% 68.5% 68.5% 68.6% 
Table 5. Rule statistics on TMs constructed by 
random sampling (TM0 is the main model) 
Compared to the numbers in Table 4, we find 
that the coverage between baseline TM and 
sampled auxiliary models with identical para-
digm is larger than that between baseline TM 
and auxiliary models with different paradigms 
(about 10 percents). It is a potential reason can 
explain why results of TMG based on sampled 
auxiliary models are more effective than those 
based on auxiliary models built with different 
paradigms, as we infer that they share more 
common phrase pairs each other and make the 
computation of feature expectations and va-
riances to be more reliable and accurate. 
4.6 Improvements on System Combination 
Besides working for single-system decoding, we 
also perform a system combination method on 
N-best outputs from systems using generalized 
TMs. We re-implement a state-of-the-art word-
level System Combination  (SC) approach based 
on incremental HMM alignment proposed by Li 
et al (2009a). The default number of N-best 
candidates used is set to 20. 
We evaluate SC on N-best outputs generated 
from 4 baseline decoders by using different TM 
settings and list results in Table 6, where Base 
stands for combination results on systems using 
default TMs; Paras stands for combination re-
sults on systems using TMs generalized based 
on auxiliary models with different paradigms; 
and Samp stands for combination results on sys-
tems using TMs generalized based on auxiliary 
models constructed by the random data sampling 
method. For the Samp setting, we also include 
probability variance features computed based on 
Equation 3 in the log-linear model.  
 
SC MT03 MT04 MT05 MT06 MT08 
Base 44.20 42.30 41.22 37.77 33.07 
Paras 44.40 42.69 41.53 38.05 33.31 
Samp 44.80 42.95 42.10 38.39 33.67 
Table 6. Results on system combination 
From Table 6 we can see that system combi-
nation can benefit from TMG method. 
4.7 Improvements on Model Combination 
As an alternative, model combination is another 
effective way to improve translation perfor-
mance by utilizing multiple systems. We re-
implement the Model Combination (MC) ap-
proach (DeNero et al, 2010) using N-best lists 
as its inputs and develop it on N-best outputs 
used in Table 6. Evaluation results are presented 
in Table 7.  
 
MC MT03 MT04 MT05 MT06 MT08 
Base 42.31 40.57 40.31 38.65 33.88 
Paras 42.87 40.96 40.77 38.81 34.47 
Samp 43.29 41.29 41.11 39.28 34.77 
Table 7. Results on model combination 
310
From Table 7 we can see that model combina-
tion can also benefit from TMG method. 
5 Related Work 
Foster and Kuhn (2007) presented an approach 
that resembles more to our work, in which they 
divided the training corpus into different com-
ponents and integrated models trained on each 
component using the mixture modeling. Howev-
er, their motivation was to address the domain 
adaption problem, and additional genre informa-
tion should be provided for the corpus partition 
to create multiple models for mixture. We in-
stead present two ways for the model ensemble 
construction without extra information needed: 
building models by different paradigms or by a 
random data sampling technique inspired by a 
machine learning technique. Compared to the 
prior work, our approach is more general, which 
can also be used for model adaptation. We can 
also treat TMG as a smoothing way to address 
the over-estimation problem existing in almost 
all TMs. Some literatures have paid attention to 
this issue as well, such as Foster et al (2006) 
and Mylonakis and Sima ?an (2008). However, 
they did not leverage information between mul-
tiple models as we did, and developed on single 
models only. Furthermore, we also make current 
translation probability features to contain more 
statistical meanings by introducing the probabili-
ty variance features into the log-linear model, 
which are completely novel to prior work and 
provide further improvements. 
6 Conclusion and Future Work 
In this paper, we have investigated a simple but 
effective translation model generalization me-
thod that benefits by integrating values of prob-
ability features between multiple TMs and using 
them in decoding phase directly. We also intro-
duce novel probability variance features into the 
current feature sets of translation models and 
make the SMT models to be more flexible. We 
evaluate our method on four state-of-the-art 
SMT systems, and get promising results not only 
on single-system decodings, but also on a system 
combination approach and a model combination 
approach. 
Making use of different distributions of trans-
lation probability features is the essential of this 
work. In the future, we will extend TMG method 
to other statistical models in SMT framework, 
(e.g. LM), which could be also suffered from the 
over-estimation problem. And we will make fur-
ther research on how to tune prior probabilities 
of models automatically as well, in order to 
make our method to be more robust and tunable. 
References 
Auli Michael, Adam Lopez, Hieu Hoang, and Philipp 
Koehn. 2009. A Systematic Analysis of Translation 
Model Search Spaces. In 4th Workshop on Statis-
tical Machine Translation, pages 224-232. 
Breiman Leo. 1996. Bagging Predictors. Machine 
Learning. 
Chiang David. 2007. Hierarchical Phrase Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
DeNero John, Shankar Kumar, Ciprian Chelba, and 
Franz Och. 2010. Model Combination for Machine 
Translation. To appear in Proc. of the North Amer-
ican Chapter of the Association for Computational 
Linguistic. 
DeNeefe Steve, Kevin Knight, Wei Wang, and Da-
niel Marcu. 2007. What Can Syntax-based MT 
Learn from Phrase-based MT? In Proc. of Empiri-
cal Methods on Natural Language Processing, 
pages 755-763. 
Foster George, Roland Kuhn, and Howard Johnson. 
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In Proc. of Empirical Methods 
on Natural Language Processing, pages 53-61. 
Foster George and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In 2th Workshop on 
Statistical Machine Translation, pages 128-135. 
Galley Michel, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang, and Igna-
cio Thayer. 2006. Scalable Inference and Training 
of Context-Rich Syntactic Translation Models. In 
Proc. of 44th Meeting of the Association for Com-
putational Linguistics, pages: 961-968. 
Huang Liang. 2008. Forest Reranking: Discrimina-
tive Parsing with Non-Local Features. In Proc. of 
46th Meeting of the Association for Computational 
Linguistics, pages 586-594. 
Hoeting Jennifer, David Madigan, Adrian Raftery, 
and Chris Volinsky. 1999. Bayesian Model Aver-
aging: A tutorial. Statistical Science, Vol. 14, pag-
es 382-417. 
311
He Xiaodong, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Out-
puts from Machine Translation Systems. In Proc. 
of Empirical Methods on Natural Language 
Processing, pages 98-107. 
Koehn Philipp. 2004. Phrase-based Model for SMT. 
Computational Linguistics, 28(1): 114-133. 
Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi. 
2009a. Incremental HMM Alignment for MT sys-
tem Combination. In Proc. of 47th Meeting of the 
Association for Computational Linguistics, pages 
949-957. 
Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009b. Collaborative Decoding: Par-
tial Hypothesis Re-Ranking Using Translation 
Consensus between Decoders. In Proc. of 47th 
Meeting of the Association for Computational Lin-
guistics, pages 585-592. 
Liu Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009. 
Joint Decoding with Multiple Translation Models. 
In Proc. of 47th Meeting of the Association for 
Computational Linguistics, pages 576-584. 
 
Mylonakis Markos and Khalil Sima ?an. 2008. 
Phrase Translation Probabilities with ITG Priors 
and Smoothing as Learning Objective. In Proc. of 
Empirical Methods on Natural Language 
Processing, pages 630-639. 
Matusov Evgeny, Nicola Ueffi ng, and Hermann 
Ney. 2006. Computing consensus translation from 
multiple machine translation systems using en-
hanced hypotheses alignment. In Proc. of Euro-
pean Charter of the Association for Computational 
Linguistics, pages 33-40. 
Och Franz and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. In Proc. of 38th Meeting of 
the Association for Computational Linguistics, 
pages 440-447. 
Och Franz. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of 41th 
Meeting of the Association for Computational Lin-
guistics, pages 160-167. 
Och Franz and Hermann Ney. 2004. The Alignment 
template approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(4): 417-449. 
Shen Libin, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. of 46th Meeting of the Association for 
Computational Linguistics, pages 577-585. 
Wu Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-404. 
Xiong Deyi, Qun Liu, and Shouxun Lin. 2006. Max-
imum Entropy based Phrase Reordering Model for 
Statistical Machine Translation. In Proc. of 44th 
Meeting of the Association for Computational Lin-
guistics, pages 521-528. 
Zollmann Andreas, Ashish Venugopal, Franz Och, 
and Jay Ponte. 2008. A Systematic Comparison of 
Phrase-Based, Hierarchical and Syntax-
Augmented Statistical MT. In 23rd International 
Conference on Computational Linguistics, pages 
1145-1152. 
 
 
312
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 313?321,
Beijing, August 2010
Mixture Model-based Minimum Bayes Risk Decoding using Multiple 
Machine Translation Systems 
Nan Duan1 
School of Computer Science and Technology 
Tianjin University 
v-naduan@microsoft.com 
Mu Li, Dongdong Zhang, Ming Zhou 
Microsoft Research Asia 
muli@microsoft.com  
dozhang@microsoft.com 
mingzhou@microsoft.com 
Abstract 
We present Mixture Model-based Mini-
mum Bayes Risk (MMMBR) decoding, 
an approach that makes use of multiple 
SMT systems to improve translation ac-
curacy. Unlike existing MBR decoding 
methods defined on the basis of single 
SMT systems, an MMMBR decoder re-
ranks translation outputs in the combined 
search space of multiple systems using 
the MBR decision rule and a mixture dis-
tribution of component SMT models for 
translation hypotheses. MMMBR decod-
ing is a general method that is indepen-
dent of specific SMT models and can be 
applied to various commonly used search 
spaces. Experimental results on the NIST 
Chinese-to-English MT evaluation tasks 
show that our approach brings significant 
improvements to single system-based 
MBR decoding and outperforms a state-
of-the-art system combination method. 1 
1 Introduction 
Minimum Bayes Risk (MBR) decoding is be-
coming more and more popular in recent Statis-
tical Machine Translation (SMT) research. This 
approach requires a second-pass decoding pro-
cedure to re-rank translation hypotheses by risk 
scores computed based on model?s distribution. 
Kumar and Byrne (2004) first introduced 
MBR decoding to SMT field and developed it on 
the N-best list translations. Their work has 
shown that MBR decoding performs better than 
Maximum a Posteriori (MAP) decoding for dif-
ferent evaluation criteria. After that, many dedi-
                                                 
1 This work has been done while the author was visiting 
Microsoft Research Asia. 
cated efforts have been made to improve the per-
formances of SMT systems by utilizing MBR-
inspired methods. Tromble et al (2008) pro-
posed a linear approximation to BLEU score 
(log-BLEU) as a new loss function in MBR de-
coding and extended it from N-best lists to lat-
tices, and Kumar et al (2009) presented more 
efficient algorithms for MBR decoding on both 
lattices and hypergraphs to alleviate the high 
computational cost problem in Tromble et al?s 
work. DeNero et al (2009) proposed a fast con-
sensus decoding algorithm for MBR for both 
linear and non-linear similarity measures. 
All work mentioned above share a common 
setting: an MBR decoder is built based on one 
and only one MAP decoder. On the other hand, 
recent research has shown that substantial im-
provements can be achieved by utilizing consen-
sus statistics over multiple SMT systems (Rosti 
et al, 2007; Li et al, 2009a; Li et al, 2009b; 
Liu et al, 2009). It could be desirable to adapt 
MBR decoding to multiple SMT systems as well. 
In this paper, we present Mixture Model-
based Minimum Bayes Risk (MMMBR) decoding, 
an approach that makes use of multiple SMT 
systems to improve translation performance. In 
this work, we can take advantage of a larger 
search space for hypothesis selection, and em-
ploy an improved probability distribution over 
translation hypotheses based on mixture model-
ing, which linearly combines distributions of 
multiple component systems for Bayes risk 
computation. The key contribution of this paper 
is the usage of mixture modeling in MBR, which 
allows multiple SMT models to be involved in 
and makes the computation of n-gram consensus 
statistics to be more accurate. Evaluation results 
have shown that our approach not only brings 
significant improvements to single system-based 
MBR decoding but also outperforms a state-of-
the-art word-level system combination method. 
313
The rest of the paper is organized as follows: 
In Section 2, we first review traditional MBR 
decoding method and summarize various search 
spaces that can be utilized by an MBR decoder. 
Then, we describe how a mixture model can be 
used to combine distributions of multiple SMT 
systems for Bayes risk computation. Lastly, we 
present detailed MMMBR decoding model on 
multiple systems and make comparison with 
single system-based MBR decoding methods. 
Section 3 describes how to optimize different 
types of parameters. Experimental results will be 
shown in Section 4. Section 5 discusses some 
related work and Section 6 concludes the paper. 
2 Mixture Model-based MBR Decoding 
2.1 Minimum Bayes Risk Decoding 
Given a source sentence  , MBR decoding aims 
to find the translation with the least expected 
loss under a probability distribution. The objec-
tive of an MBR decoder can be written as: 
         
     
                 
    
  (1) 
where   denotes a search space for hypothesis 
selection;    denotes an evidence space for 
Bayes risk computation;      denotes a function 
that measures the loss between    and  ;      is 
the underlying distribution based on  . 
Some of existing work on MBR decoding fo-
cused on exploring larger spaces for both    
and   , e.g. from N-best lists to lattices or 
hypergraphs (Tromble et al, 2008; Kumar et al, 
2009). Various loss functions have also been 
investigated by using different evaluation crite-
ria for similarity computation, e.g. Word Error 
Rate, Position-independent Word Error Rate, 
BLEU and log-BLEU (Kumar and Byrne, 2004; 
Tromble et al, 2008). But less attention has 
been paid to distribution     . Currently, many 
SMT systems based on different paradigms can 
yield similar performances but are good at mod-
eling different inputs in the translation task 
(Koehn et al, 2004a; Och et al, 2004; Chiang, 
2007; Mi et al, 2008; Huang, 2008). We expect 
to integrate the advantages of different SMT 
models into MBR decoding for further im-
provements. In particular, we make in-depth in-
vestigation into MBR decoding concentrating on 
the translation distribution      by leveraging a 
mixture model based on multiple SMT systems. 
2.2 Summary of Translation Search Spaces 
There are three major forms of search spaces 
that can be obtained from an MAP decoder as a 
byproduct, depending on the design of the de-
coder: N-best lists, lattices and hypergraphs. 
An N-best list contains the   most probable 
translation hypotheses produced by a decoder. It 
only presents a very small portion of the entire 
search space of an SMT model. 
A hypergraph is a weighted acyclic graph 
which compactly encodes an exponential num-
ber of translation hypotheses. It allows us to 
represent both phrase-based and syntax-based 
systems in a unified framework. Formally, a 
hypergraph  is a pair      , where   is a 
set of hypernodes and   is a set of hyperedges. 
Each hypernode     corresponds to transla-
tion hypotheses with identical decoding states, 
which usually include the span       of the 
words being translated, the grammar symbol   
for that span and the left and right boundary 
words of hypotheses for computing language 
model (LM) scores. Each hyperedge     cor-
responds to a translation rule and connects a 
head node      and a set of tail nodes     . The 
number of tail nodes        is called the arity of 
the hyperedge   and the arity of a hypergraph is 
the maximum arity of its hyperedges. If the arity 
of a hyperedge   is zero,      is then called a 
source node. Each hypergraph has a unique root 
node and each path in a hypergraph induces a 
translation hypothesis. A lattice (Ueffing et al, 
2002) can be viewed as a special hypergraph, in 
which the maximum arity is one. 
2.3 Mixture Model for SMT 
We first describe how to construct a general dis-
tribution for translation hypotheses over multiple 
SMT systems using mixture modeling for usage 
in MBR decoding. 
Mixture modeling is a technique that has been 
applied to many statistical tasks successfully. 
For the SMT task in particular, given   SMT 
systems with their corresponding model distribu-
tions, a mixture model is defined as a probability 
distribution over the combined search space of 
all component systems and computed as a 
weighted sum of component model distributions: 
314
                      
 
   
  (2) 
In Equation 2,            are system weights 
which hold following constraints:        
and    
 
     ,            is the  
th distri-
bution estimated on the search space   based 
on the log-linear formulation: 
           
              
                     
  
where         is the score function of the  
th 
system for translation  ,          is a scaling 
factor that determines the flatness of the distri-
bution    sharp (    ) or smooth (    ). 
Due to the inherent differences in SMT mod-
els, translation hypotheses have different distri-
butions in different systems. A mixture model 
can effectively combine multiple distributions 
with tunable system weights. The distribution of 
a single model used in traditional MBR can be 
seen as a special mixture model, where   is one. 
2.4 Mixture Model for SMT 
Let              denote   machine translation 
systems,   denotes the search space produced 
by system    in MAP decoding procedure. An 
MMMBR decoder aims to seek a translation 
from the combined search space       that 
maximizes the expected gain score based on a 
mixture model         . We write the objec-
tive function of MMMBR decoding as: 
         
    
                
   
  (3) 
For the gain function     , we follow Trom-
ble et al (2008) to use log-BLEU, which is 
scored by the hypothesis length and a linear 
function of n-gram matches as: 
            
            
    
 
     
In this definition,   is a reference translation, 
     is the length of hypothesis   ,   is an n-
gram presented in   ,     
   is the number of 
times that  occurs in   , and       is an indi-
cator function which equals to 1 when   occurs 
in   and 0 otherwise.            are model 
parameters, where   is the maximum order of 
the n-grams involved. 
For the mixture model     , we replace it by 
Equation 2 and rewrite the total gain score for 
hypothesis    in Equation 3: 
                
   
 
                      
    
 
    
 
                  
   
 
    
 
                   
    
 
   
  
 
 
 
 
 
 
(4) 
In Equation 4, the total gain score on the com-
bined search space   can be further decom-
posed into each local search space    with a 
specified distribution           . This is a nice 
property and it allows us to compute the total 
gain score as a weighted sum of local gain 
scores on different search spaces. We expand the 
local gain score for    computed on search space 
   with            using log-BLEU as: 
                   
    
 
       
            
       
 
 
    
           
     
            
          
 
                           
We make two approximations for the situations 
when    : the first is                   
and the second is                      
          In fact, due to the differences in ge-
nerative capabilities of SMT models, training 
data selection and various pruning techniques 
used, search spaces of different systems are al-
ways not identical in practice. For the conveni-
ence of formal analysis, we treat all            
as ideal distributions with assumptions that all 
systems work in similar settings, and translation 
candidates are shared by all systems. 
The method for computing n-gram posterior 
probability          in Equation 5 depends on 
different types of search space  : 
? When   is an N-best list, it can be com-
puted immediately by enumerating all trans-
lation candidates in the N-best list: 
                         
    
  
315
? When   is a hypergraph (or a lattice) that 
encodes exponential number of hypotheses, 
it is often impractical to compute this proba-
bility directly.  In this paper, we use the al-
gorithm presented in Kumar et al (2009) 
which is described in Algorithm 12: 
                   
         
   
          
    
 
             
                         
       
 
             
                 
   
  
           counts the edge   with n-gram 
  that has the highest edge posterior proba-
bility relative to predecessors in the entire 
graph  , and          is the edge posterior 
probability that can be efficiently computed 
with standard inside and outside probabili-
ties      and      as: 
         
 
    
                
      
  
where     is the weight of hyperedge   in 
  ,      is the normalization factor that 
equals to the inside probability of the root 
node in  .  
 
Algorithm 1: Compute n-gram posterior proba-
bilities on hypergraph   (Kumar et al, 2009) 
1: sort hypernodes topologically 
2: compute inside/outside probabilities      and      
for each hypernode      
3: compute edge posterior probability          for 
each hyperedge       
4: for each hyperedge      do  
5:       merge n-grams on      and keep the highest 
probability when n-grams are duplicated 
6:      apply the rule of edge   to n-grams on      and 
propagate     gram prefixes/suffixes to      
7:          for each n-gram   introduced by   do  
8:      if                      then 
9:                                           
                     
10:           else 
11:                                 
12:   end if 
13:  end for   
14: end for 
15: return n-gram posterior probability set             
                                                 
2 We omit the similar algorithm for lattices because of their 
homogenous structures comparing to hypergraphs as we 
discussed in Section 2.2. 
Thus, the total gain score for hypothesis    on 
       can be further expanded as: 
   
 
                   
    
 
   
 
    
 
      
            
          
 
 
 
   
 
    
 
      
            
          
 
  
     
 
    
            
     
 
        
 
  
       
            
      
 
                            
where                   is a mixture n-
gram posterior probability. The most important 
fact derived from Equation 6 is that, the mixture 
of different distributions can be simplified to the 
weighted sum of n-gram posterior probabilities 
on different search spaces.  
We now derive the decision rule of MMMBR 
decoding based on Equation 6 below: 
         
    
    
            
      
 
  (7) 
We also notice that MAP decoding and MBR 
decoding are two different ways of estimating 
the probability        and each of them has 
advantages and disadvantages. It is desirable to 
interpolate them together when choosing the fi-
nal translation outputs. So we include each sys-
tem?s MAP decoding cost as an additional fea-
ture further and modify Equation 7 to: 
         
 ?  
    
            
      
 
 
             
       
 
  
 
 
 
 
(8) 
where       
        is the model cost as-
signed by the MAP decoder    for hypothesis  
 . 
Because the costs of MAP decoding on different 
SMT models are not directly comparable, we 
utilize the MERT algorithm to assign an appro-
priate weight    for each component system.  
Compared to single system-based MBR de-
coding, which obeys the decision rule below:  
         
     
    
            
         
 
   
316
MMMBR decoding has a similar objective func-
tion (Equation 8). The key difference is that, in 
MMMBR decoding, n-gram posterior probabili-
ty      is computed as              based on 
an ensemble of search spaces; meanwhile, in 
single system-based MBR decoding, this quanti-
ty is computed locally on single search space  . 
The procedure of MMMBR decoding on mul-
tiple SMT systems is described in Algorithm 2. 
 
Algorithm 2: MMMBR decoding on multiple 
SMT systems 
1: for each component system    do 
2:     run MAP decoding and generate the correspond-
ing search space   
3:  compute the n-gram posterior probability set 
            for   based on Algorithm 1 
4: end for 
5 compute the mixture n-gram posterior  probability 
                 for each  : 
6: for each unique n-gram   appeared in     do 
7:      for each search space   do 
8                   
9:         end for 
10: end for  
11: for each hyperedge   in     do 
12:     assign      to the edge   for all   contained in   
13: end for 
14: return the best path according to Equation 8 
 
3 A Two-Pass Parameter Optimization 
In Equation 8, there are two types of parameters: 
parameters introduced by the gain function      
and the model cost        , and system weights 
introduced by the mixture model     . Because 
Equation 8 is not a linear function when all pa-
rameters are taken into account, MERT algo-
rithm (Och, 2003) cannot be directly applied to 
optimize them at the same time. Our solution is 
to employ a two-pass training strategy, in which 
we optimize parameters for MBR first and then 
system weights for the mixture model. 
3.1 Parameter Optimization for MBR 
The inputs of an MMMBR decoder can be a 
combination of translation search spaces with 
arbitrary structures. For the sake of a general and 
convenience solution for optimization, we utilize 
the simplest N-best lists with proper sizes as 
approximations to arbitrary search spaces to 
optimize MBR parameters using MERT in the 
first-pass training. System weights can be set 
empirically based on different performances, or 
equally without any bias. Note that although we 
tune MBR parameters on N-best lists, n-gram 
posterior probabilities used for Bayes risk 
computation could still be estimated on 
hypergraphs for non N-best-based search spaces. 
3.2 Parameter Optimization for Mixture 
Model 
After MBR parameters optimized, we begin to 
tune system weights for the mixture model in the 
second-pass training. We rewrite Equation 8 as: 
         
 ?  
   
 
     
    
                                          
          
 
 
                                              
        
 
          
For each   , the aggregated score surrounded 
with braces can be seen as its feature value. Eq-
uation 9 now turns to be a linear function for all 
weights and can be optimized by the MERT. 
4 Experiments 
4.1 Data and Metric 
We conduct experiments on the NIST Chinese-
to-English machine translation tasks. We use the 
newswire portion of the NIST 2006 test set 
(MT06-nw) as the development set for parameter 
optimization, and report results on the NIST 
2008 test set (MT08). Translation performances 
are measured in terms of case-insensitive BLEU 
scores. Statistical significance is computed using 
the bootstrap re-sampling method proposed by 
Koehn (2004b). Table 1 gives data statistics. 
 
Data Set #Sentence #Word 
MT06-nw (dev) 616 17,316 
MT08 (test) 1,357 31,600 
Table 1. Statistics on dev and test data sets 
All bilingual corpora available for the NIST 
2008 constrained track of Chinese-to-English 
machine translation task are used as training data, 
which contain 5.1M sentence pairs, 128M Chi-
nese words and 147M English words after pre-
processing. Word alignments are performed by 
GIZA++ with an intersect-diag-grow refinement.  
317
A 5-gram language model is trained on the 
English side of all bilingual data plus the Xinhua 
portion of LDC English Gigaword Version 3.0. 
4.2 System Description 
We use two baseline systems. The first one 
(SYS1) is a hierarchical phrase-based system 
(Chiang, 2007) based on Synchronous Context 
Free Grammar (SCFG), and the second one 
(SYS2) is a phrasal system (Xiong et al, 2006) 
based on Bracketing Transduction Grammar 
(Wu, 1997) with a lexicalized reordering com-
ponent based on maximum entropy model. 
Phrasal rules shared by both systems are ex-
tracted on all bilingual data, while hierarchical 
rules for SYS1 only are extracted on a selected 
data set, including LDC2003E07, LDC2003E14, 
LDC2005T06, LDC2005T10, LDC2005E83, 
LDC2006E26, LDC2006E34, LDC2006E85 and 
LDC2006E92, which contain about 498,000 sen-
tence pairs. Translation hypergraphs are generat-
ed by each baseline system during the MAP de-
coding phase, and 1000-best lists used for 
MERT algorithm are extracted from hyper-
graphs by the k-best parsing algorithm (Huang 
and Chiang, 2005). We tune scaling factor to 
optimize the performance of HyperGraph-based 
MBR decoding (HGMBR) on MT06-nw for 
each system (0.5 for SYS1 and 0.01 for SYS2). 
4.3 MMMBR Results on Multiple Systems 
We first present the overall results of MMMBR 
decoding on two baseline systems. 
To compare with single system-based MBR 
methods, we re-implement N-best MBR, which 
performs MBR decoding on 1000-best lists with 
the fast consensus decoding algorithm (DeNero 
et al, 2009), and HGMBR, which performs 
MBR decoding on a hypergraph (Kumar et al, 
2009). Both methods use log-BLEU as the loss 
function. We also compare our method with 
IHMM Word-Comb, a state-of-the-art word-level 
system combination approach based on incre-
mental HMM alignment proposed by Li et al 
(2009b). We report results of MMMBR decod-
ing on both N-best lists (N-best MMMBR) and 
hypergraphs (Hypergraph MMMBR) of two 
baseline systems. As MBR decoding can be used 
for any SMT system, we also evaluate MBR-
IHMM Word-Comb, which uses N-best lists 
generated by HGMBR on each baseline systems. 
The default beam size is set to 50 for MAP de-
coding and hypergraph generation. The setting 
of N-best candidates used for (MBR-) IHMM 
Word-Comb is the same as the one used in Li et 
al. (2009b). The maximum order of n-grams in-
volved in MBR model is set to 4. Table 2 shows 
the evaluation results. 
 
 MT06-nw MT08 
 SYS1 SYS2 SYS1 SYS2 
MAP 38.1 37.1 28.5 28.0 
N-best MBR 38.3 37.4 29.0 28.1 
HGMBR 38.3 37.5 29.1 28.3 
IHMM 
Word-Comb 
39.1 29.3 
MBR-IHMM 
Word-Comb 
39.3 29.7 
N-best 
MMMBR 
39.0* 29.4* 
Hypergraph 
MMMBR 
39.4*+ 29.9*+ 
Table 2. MMMBR decoding on multiple sys-
tems (*: significantly better than HGMBR with 
      ; +: significantly better than IHMM 
Word-Comb with       ) 
From Table 2 we can see that, compared to 
MAP decoding, N-best MBR and HGMBR only 
improve the performance in a relative small 
range (+0.1~+0.6 BLEU), while MMMBR de-
coding on multiple systems can yield significant 
improvements on both dev set (+0.9 BLEU on 
N-best MMMBR and +1.3 BLEU on Hyper-
graph MMMBR) and test set (+0.9 BLEU on N-
best MMMBR and +1.4 BLEU on Hypergraph 
MMMBR); compared to IHMM Word-Comb, 
N-best MMMBR can achieve comparable results 
on both dev and test sets, while Hypergraphs 
MMMBR can achieve even better results (+0.3 
BLEU on dev and +0.6 BLEU on test); com-
pared to MBR-IHMM Word-Comb, Hypergraph 
MMMBR can also obtain comparable results 
with tiny improvements (+0.1 BLEU on dev and 
+0.2 BLEU on test). However, MBR-IHMM 
Word-Comb has ability to generate new hypo-
theses, while Hypergraph MMMBR only choos-
es translations from original search spaces. 
We next evaluate performances of MMMBR 
decoding on hypergraphs generated by different 
beam size settings, and compare them to (MBR-) 
318
IHMM Word-Comb with the same candidate 
size and HGMBR with the same beam size. We 
list the results of MAP decoding for comparison. 
The comparative results on MT08 are shown in 
Figure 1, where X-axis is the size used for all 
methods each time, Y-axis is the BLEU score, 
MAP-  and HGMBR-  stand for MAP decoding 
and HGMBR decoding for the  th system. 
 
Figure 1. MMMBR vs. (MBR-) IHMM Word-
Comb and HGMBR with different sizes 
From Figure 1 we can see that, MMMBR de-
coding performs consistently better than both 
(MBR-) IHMM Word-Comb and HGMBR on 
all sizes. The gains achieved are around +0.5 
BLEU compared to IHMM Word-Comb, +0.2 
BLEU compared to MBR-IHMM Word-Comb, 
and +0.8 BLEU compared to HGMBR. Com-
pared to MAP decoding, the best result (30.1) is 
obtained when the size is 100, and the largest 
improvement (+1.4 BLEU) is obtained when the 
size is 50. However, we did not observe signifi-
cant improvement when the size is larger than 50.  
We then setup an experiment to verify that the 
mixture model based on multiple distributions is 
more effective than any individual distributions 
for Bayes risk computation in MBR decoding. 
We use Mix-HGMBR to denote MBR decoding 
performed on single hypergraph of each system 
in the meantime using a mixture model upon 
distributions of two systems for Bayes risk com-
putation. We compare it with HGMBR and 
Hypergraph MMMBR and list results in Table 3. 
 
 MT08 
 SYS1 SYS2 
HGMBR 29.1 28.3 
Mix-HGMBR 29.4 28.9 
Hypergraph MMMBR 29.9 
Table 3. Performance of MBR decoding on dif-
ferent settings of search spaces and distributions 
It can be seen that based on the same search 
space, the performance of Mix-HGMBR is sig-
nificantly better than that of HGMBR (+0.3/+0.6 
BLEU on dev/test). Yet the performance is still 
not as good as Hypergraph, which indicates the 
fact that the mixture model and the combination 
of search spaces are both helpful to MBR decod-
ing, and the best choice is to use them together. 
We also empirically investigate the impacts of 
different system weight settings upon the per-
formances of Hypergraph MMMBR on dev set 
in Figure 2, where X-axis is the weight    for 
SYS1, Y-axis is the BLEU score. The weight    
for SYS2 equals to      as only two systems 
involved. The best evaluation result on dev set is 
achieved when the weight pair is set to 0.7/0.3 
for SYS1/SYS2, which is also very close to the 
one trained automatically by the training strategy 
presented in Section 3.2. Although this training 
strategy can be processed repeatedly, the per-
formance is stable after the 1st round finished. 
 
Figure 2. Impacts of different system weights in 
the mixture model 
4.4 MMMBR Results on Identical Systems 
with Different Translation Models 
Inspired by Macherey and Och (2007), we ar-
range a similar experiment to test MMMBR de-
coding for each baseline system on an ensemble 
of sub-systems built by the following two steps. 
Firstly, we iteratively apply the following 
procedure 3 times: at the  th time, we randomly 
sample 80% sentence pairs from the total bilin-
gual data to train a translation model and use it 
to build a new system based on the same decod-
er, which is denoted as sub-system- . Table 4 
shows the evaluation results of all sub-systems 
on MT08, where MAP decoding (the former 
ones) and corresponding HGMBR (the latter 
ones) are grouped together by a slash. We set al 
beam sizes to 20 for a time-saving purpose. 
27.5
28.0
28.5
29.0
29.5
30.0
30.5
10 20 50 100 150
MAP-1
MAP-2
HGMBR-1
HGMBR-2
IHMM
MBR-IHMM
MMMBR
38.5
38.7
38.9
39.1
39.3
39.5
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
MMMBR
319
 MT08 
 SYS1 SYS2 
Baseline 28.4/29.0 27.6/27.8 
sub-system-1 28.1/28.5 26.8/27.3 
sub-system-2 28.3/28.4 27.0/27.1 
sub-system-3 27.7/28.0 27.3/27.6 
Table 4. Performance of sub-systems 
Secondly, starting from each baseline system, 
we gradually add one more sub-system each 
time and perform Hypergraph MMMBR on 
hypergraphs generated by current involved sys-
tems. Table 5 shows the evaluation results. 
 
 MT08 
 SYS1 SYS2 
MAP 28.4 27.6 
HGMBR 29.0 27.8 
Hypergraph MMMBR 
+ sub-system-1 29.1 27.9 
+ sub-system-2 29.1 28.1 
+ sub-system-3 29.3 28.3 
Table 5. Performance of Hypergraph MMMBR 
on multiple sub-systems 
We can see from Table 5 that, compared to 
the results of MAP decoding, MMMBR decod-
ing can achieve significant improvements when 
more than one sub-system are involved; however, 
compared to the results of HGMBR on baseline 
systems, there are few changes of performance 
when the number of sub-systems increases. One 
potential reason is that the translation hypotheses 
between multiple sub-systems under the same 
SMT model hold high degree of correlation, 
which is discussed in Macherey and Och (2007). 
We also evaluate MBR-IHMM Word-Comb 
on N-best lists generated by each baseline sys-
tem with its corresponding three sub-systems. 
Evaluation results are shown in Table 6, where 
Hypergraph MMMBR still outperforms MBR-
IHMM Word-Comb on both baseline systems. 
 
 MT08 
 SYS1 SYS2 
MBR-IHMM Word-Comb 29.1 28.0 
Hypergraph MMMBR 29.3 28.3 
Table 6. Hypergraph MMMBR vs. MBR-IHMM 
Word-Comb with multiple sub-systems 
5 Related Work 
Employing consensus between multiple systems 
to improve machine translation quality has made 
rapid progress in recent years. System combina-
tion methods based on confusion networks (Ros-
ti et al, 2007; Li et al, 2009b) have shown 
state-of-the-art performances in MT benchmarks. 
Different from them, MMMBR decoding me-
thod does not generate new translations. It main-
tains the essential of MBR methods to seek 
translations from existing search spaces. Hypo-
thesis selection method (Hildebrand and Vogel, 
2008) resembles more our method in making use 
of n-gram statistics. Yet their work does not be-
long to the MBR framework and treats all sys-
tems equally. Li et al (2009a) presents a co-
decoding method, in which n-gram agreement 
and disagreement statistics between translations 
of multiple decoders are employed to re-rank 
both full and partial hypotheses during decoding. 
Liu et al (2009) proposes a joint-decoding me-
thod to combine multiple SMT models into one 
decoder and integrate translation hypergraphs 
generated by different models. Both of the last 
two methods work in a white-box way and need 
to implement a more complicated decoder to 
integrate multiple SMT models to work together; 
meanwhile our method can be conveniently used 
as a second-pass decoding procedure, without 
considering any system implementation details. 
6 Conclusions and Future Work 
In this paper, we have presented a novel 
MMMBR decoding approach that makes use of 
a mixture distribution of multiple SMT systems 
to improve translation accuracy. Compared to 
single system-based MBR decoding methods, 
our method can achieve significant improve-
ments on both dev and test sets. What is more, 
MMMBR decoding approach also outperforms a 
state-of-the-art system combination method.  We 
have empirically verified that the success of our 
method comes from both the mixture modeling 
of translation hypotheses and the combined 
search space for translation selection. 
In the future, we will include more SMT sys-
tems with more complicated models into our 
MMMBR decoder and employ more general 
MERT algorithms on hypergraphs and lattices 
(Kumar et al, 2009) for parameter optimization. 
320
References 
Chiang David. 2007. Hierarchical Phrase Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
DeNero John, David Chiang, and Kevin Knight. 2009. 
Fast Consensus Decoding over Translation 
Forests. In Proc. of 47th Meeting of the Associa-
tion for Computational Linguistics, pages 567-575. 
Hildebrand Almut Silja and Stephan Vogel. 2008. 
Combination of Machine Translation Systems 
via Hypothesis Selection from Combined N-
best lists. In Proc. of the Association for Machine 
Translation in the Americas, pages 254-261. 
Huang Liang and David Chiang. 2005. Better k-best 
Parsing. In Proc. of 7th International Conference 
on Parsing Technologies, pages 53-64. 
Huang Liang. 2008. Forest Reranking: Discrimin-
ative Parsing with Non-Local Features. In 
Proc. of 46th Meeting of the Association for Com-
putational Linguistics, pages 586-594. 
Koehn Philipp. 2004a. Phrase-based Model for 
SMT. Computational Linguistics, 28(1): 114-133. 
Koehn Philipp. 2004b. Statistical Significance 
Tests for Machine Translation Evaluation. In 
Proc. of Empirical Methods on Natural Language 
Processing, pages 388-395. 
Kumar Shankar and William Byrne. 2004. Minimum 
Bayes-Risk Decoding for Statistical Machine 
Translation. In Proc. of the North American 
Chapter of the Association for Computational Lin-
guistics, pages 169-176.  
Kumar Shankar, Wolfgang Macherey, Chris Dyer, 
and Franz Och. 2009. Efficient Minimum Error 
Rate Training and Minimum Bayes-Risk De-
coding for Translation Hypergraphs and Lat-
tices. In Proc. of 47th Meeting of the Association 
for Computational Linguistics, pages 163-171. 
Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009a. Collaborative Decoding: 
Partial Hypothesis Re-Ranking Using Trans-
lation Consensus between Decoders. In Proc. 
of 47th Meeting of the Association for Computa-
tional Linguistics, pages 585-592. 
Liu Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009. 
Joint Decoding with Multiple Translation 
Models. In Proc. of 47th Meeting of the Associa-
tion for Computational Linguistics, pages 576-584. 
Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi. 
2009b. Incremental HMM Alignment for MT 
system Combination. In Proc. of 47th Meeting of 
the Association for Computational Linguistics, 
pages 949-957. 
Mi Haitao, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proc. of 46th Meeting of 
the Association for Computational Linguistics, 
pages 192-199. 
Macherey Wolfgang and Franz Och. 2007. An Em-
pirical Study on Computing Consensus Trans-
lations from multiple Machine Translation 
Systems. In Proc. of Empirical Methods on Natu-
ral Language Processing, pages 986-995. 
Och Franz. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proc. of 
41th Meeting of the Association for Computational 
Linguistics, pages 160-167. 
Och Franz and Hermann Ney. 2004. The Alignment 
template approach to Statistical Machine 
Translation. Computational Linguistics, 30(4): 
417-449. 
Rosti Antti-Veikko, Spyros Matsoukas, and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proc. 
of 45th Meeting of the Association for Computa-
tional Linguistics, pages 312-319. 
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk Decoding for Statistical Machine Trans-
lation. In Proc. of Empirical Methods on Natural 
Language Processing, pages 620-629. 
Ueffing Nicola, Franz Och, and Hermann Ney. 2002. 
Generation of Word Graphs in Statistical Ma-
chine Translation. In Proc. of Empirical Me-
thods on Natural Language Processing, pages 
156-163. 
Wu Dekai. 1997. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Pa-
rallel Corpora. Computational Linguistics, 
23(3): 377-404. 
Xiong Deyi, Qun Liu, and Shouxun Lin. 2006. Max-
imum Entropy based Phrase Reordering 
Model for Statistical Machine Translation. In 
Proc. of 44th Meeting of the Association for Com-
putational Linguistics, pages 521-528. 
321
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 445?454, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Forced Derivation Tree based Model Training to
Statistical Machine Translation
Nan Duan
Microsoft Research Asia
nanduan@microsoft.com
Mu Li
Microsoft Research Asia
muli@microsoft.com
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Abstract
A forced derivation tree (FDT) of a sentence
pair {f, e} denotes a derivation tree that can
translate f into its accurate target translation
e. In this paper, we present an approach that
leverages structured knowledge contained in
FDTs to train component models for statistical
machine translation (SMT) systems. We first
describe how to generate different FDTs for
each sentence pair in training corpus, and then
present how to infer the optimal FDTs based
on their derivation and alignment qualities. As
the first step in this line of research, we verify
the effectiveness of our approach in a BTG-
based phrasal system, and propose four FDT-
based component models. Experiments are
carried out on large scale English-to-Japanese
and Chinese-to-English translation tasks, and
significant improvements are reported on both
translation quality and alignment quality.
1 Introduction
Most of today?s SMT systems depends heavily on
parallel corpora aligned at the word-level to train
their different component models. However, such
annotations do have their drawbacks in training.
On one hand, word links predicted by automatic
aligners such as GIZA++ (Och and Ney, 2004) often
contain errors. This problem gets even worse on lan-
guage pairs that differ substantially in word orders,
such as English and Japanese/Korean/German. The
descent of the word alignment quality will lead to
inaccurate component models straightforwardly.
On the other hand, several component models
are designed to supervise the decoding procedures,
which usually rely on training examples extracted
from word-aligned sentence pairs, such as distortion
models (Tillman, 2004; Xiong et al2006; Galley
and Manning, 2008) and sequence models (Banchs
et al2005; Quirk and Menezes, 2006; Vaswani et
al., 2011). Ideally, training examples of models are
expected to match most of the situations that could
be met in decoding procedures. But actually, plain
structures of word alignments are too coarse to pro-
vide enough knowledge to ensure this expectation.
This paper presents an FDT-based model training
approach to SMT systems by leveraging structured
knowledge contained in FDTs. An FDT of a sen-
tence pair {f, e} denotes a derivation tree that can
translate f into its accurate target translation e. The
principle advantage of this work is two-fold. First,
using alignments induced from the 1-best FDTs of
all sentence pairs, the overall alignment quality of
training corpus can be improved. Second, compar-
ing to word alignments, FDTs can provide richer
structured knowledge for various component models
to extract training instances. Our FDT-based mod-
el training approach performs via three steps: (1)
generation, where an FDT space composed of dif-
ferent FDTs is generated for each sentence pair in
training corpus by the forced decoding technique;
(2) inference, where the optimal FDTs are extract-
ed from the FDT space of each sentence pair based
on both derivation and alignment qualities measured
by a memory-based re-ranking model; (3) training,
where various component models are trained based
on the optimal FDTs extracted in the inference step.
Our FDT-based model training approach can be
adapted to SMT systems with arbitrary paradigms.
445
As the first step in this line of research, our approach
is verified in a phrase-based SMT system on both
English-to-Japanese and Chinese-to-English transla-
tion tasks . Significant improvements are reported
on both translation quality (up to 1.31 BLEU) and
word alignment quality (up to 3.15 F-score).
2 Forced Derivation Tree for SMT
A forced derivation tree (FDT) of a sentence pair
{f, e} can be defined as a pair G =< D,A >:
? D denotes a derivation that can translate f into
e accurately, using a set of translation rules.
? A denotes a set of word links (i, j) indicating
that ei ? e aligns to fj ? f .
In this section, we first describe how to gener-
ate FDTs for each sentence pair in training corpus,
which is denoted as the generation step, and then
present how to select the optimal FDT for each sen-
tence pair, which is denoted as the inference step.
We leave a real application of FDTs to the model
training in a phrase-based SMT system in Section 3.
2.1 Generation
We first describe how to generate multiple FDTs for
each sentence pair in training corpus C based on the
forced decoding (FD) technique, which performs via
the following four steps:
1. Train component models needed for a specific
SMT paradigm M based on training corpus C;
2. Perform MERT on the development data set to
obtain a set of optimized feature weights;
3. For each {f, e} ? C, translate f into accurate e
based onM, component models trained in step
1, and feature weights optimized in step 2;
4. For each {f, e} ? C, output the hypergraph
(Huang and Chiang, 2005) H(f, e) generated
in step 3 as its FDT space.
In step 3: (1) all partial hypotheses that do not match
any sequence in e will be discarded; (2) derivations
covering identical source and target words but with
different alignments will be kept as different partial
candidates, as they can produce different FDTs for
the same sentence pair. For each {f, e}, the proba-
bility of each G ? H(f, e) is computed as:
p(G|H(f, e)) = exp{?(G)}?
G??H(f,e) exp{?(G?)}
(1)
where ?(G) is the FD model score assigned to G.
For each sentence pair, different alignment candi-
dates can be induced from its different forced deriva-
tion trees generated in the generation step, because
FD can use phrase pairs with different internal word
links extracted from other sentence pairs to recon-
struct the given sentence pair, which could lead to
better word alignment candidates.
2.2 Inference
Given an FDT spaceH(f, e), we propose a memory-
based re-ranking model (MRM), which selects the
best FDT G? as follows:
G? = argmax
G?H(f,e)
exp{
?
i ?ihi(G)}
?
G??H(f,e) exp{
?
i ?ihi(G?)}
= argmax
G?H(f,e)
?
i
?ihi(G) (2)
where hi(G) is feature function and ?i is its feature
weight. Here, memory means the whole translation
history that happened in the generation step will be
used as the evidence to help us compute features.
From the definition we can see that the quality of
an FDT directly relates to two aspects: its derivation
D and alignments A. So two kinds of features are
used to measure the overall quality of each FDT.
(I) The features in the first category measure the
derivation quality of each FDT, including:
? h(e?|f?), source-to-target translation probability
of a translation rule r = {f? , e?}.
h(e?|f?) =
?
{f,e}?C fracH(f,e)(f? , e?)
?
{f,e}?C
?
e?? fracH(f,e)(f? , e??)
(3)
fracH(f,e)(f? , e?) denotes the fractional count of
r used in generating H(f, e):
fracH(f,e)(f? , e?) =
?
G?H(f,e)
1r(G)p(G|H(f, e))
1r(G) is an indicator function that equals 1
when r is used in G and 0 otherwise. In prac-
tice, we use pH(f,e)(r) of r to approximate
446
fracH(f,e)(f? , e?) when the size of H(f, e) is too
large to enumerate all FDTs:
pH(f,e)(r) =
?(r)O(head(r))
?
v?tail(r) I(v)
Z(f)
where ?(r) is the weight of translation rule r
in the FDT space H(f, e), Z is a normalization
factor that equals to the inside probability of
the root node in H(f, e), I(v) and O(v) are
the standard inside and outside probabilities of
a node v inH(f, e), head(r) and tail(r) are the
head node and a set of tail nodes of a translation
rule r in H(f, e) respectively.
? h(f? |e?), target-to-source translation probability
of a translation rule r = {f? , e?}.
h(f? |e?) =
?
{f,e}?C fracH(f,e)(f? , e?)
?
{f,e}?C
?
f? ? fracH(f,e)(f? ?, e?)
(4)
? h#(r), smoothed usage count for translation
rule r = {f? , e?} in the whole generation step.
h#(r) =
1
1 + e{?
?
{f,e}?C fracH(f,e)(f? ,e?)}
(5)
In this paper, the sigmoid function is used to
make sure that the feature values of different
translation rules are in a proper value range.
? hr(G), number of translation rules used in G.
? hd(G), structure-based score of G. For FDTs
generated by phrase-based paradigms, it can be
computed by distortion models; while for FDTs
generated by syntax-based paradigms, it can be
computed by either parsing models or syntactic
LMs (Charniak et al2003).
The overfitting issue in the generation step can be
alleviated by leveraging memory-based features in
the inference step. h#(r) is used to penalize those
long translation rules which tend to occur in only a
few training sentences and are used few times in FD,
hr(G) adjust our MRM to prefer FDTs consisting of
more translation rules, hd(G) is used to select FDTs
with better parse tree-like structures, which can be
induced from their derivations directly.
(II) The features in the second category measure
the alignment quality of each FDT, including:
? word pair translation probabilities trained from
IBM models (Brown et al1993);
? log-likelihood ratio (Moore, 2005);
? conditional link probability (Moore, 2005);
? count of unlinked words;
? counts of inversion and concatenation.
Many alignment-inspired features can be used in
MRM. This paper only uses those commonly-used
ones that have already been proved useful in many
previous work (Moore, 2005; Moore et al2006;
Fraser and Marcu, 2006; Liu et al2010).
Following the common practice in SMT research,
the MERT algorithm (Och, 2003) is used to tune fea-
ture weights in MRM. Due to the fact that all FDTs
of each sentence pair share identical translation, we
cannot use BLEU as the error criterion any more.
Instead, alignment F-score is used as the alterna-
tive. We will show in Section 5 that after the in-
ference step, alignment quality can be improved by
replacing original alignments of each sentence pair
with alignments induced from its 1-best FDT. Future
work could experiment with other error criterions,
such as reordering-based loss functions (Birch et al
2010; Talbot et al2011; Birch and Osborne, 2011)
or span F1 (DeNero and Uszkoreit, 2011).
3 Training in Phrase-based SMT
As the first step in this line of research, we explore
the usage of FDT-based model training method in
a phrase-based SMT system (Xiong et al2006),
which employs Bracketing Transduction Grammar
(BTG) (Wu, 1997) to parse parallel sentences. The
reason of choosing this system is due to the promi-
nent advantages of BTG, such as the simplicity of
the grammar and the good coverage of syntactic di-
versities between different language pairs. We first
describe more details of FDTs under BTG. Then,
four FDT-based component models are presented.
3.1 BTG-based FDT
Given a sentence pair f = {f0, ..., fJ} and e =
{e0, ..., eI} in training corpus, its FDT G generat-
ed based on BTG is a binary tree, which is presented
by a set of terminal translation states T and a set of
non-terminal translation states N , where:
447
Figure 1: S = {f?[i,j), e?[i?,j?), A?,m,m?,R} is denoted
by the dark-shaded rectangle pair. It can be split into two
child translation states, Sl, which is denoted by the light-
shaded rectangle pair, and Sr, which is denoted by the
white rectangle pair. Dash lines within rectangle pairs
denote their internal alignments and solid lines with rows
denote BTG rules. (a) uses [?] to combine two translation
states, while (b) uses ???. Both Sl and Sr belong to T ?N .
? each terminal translation state S ? T is a 3-
tuple {f?[i,j), e?[i?,j?), A?}, in which f?[i,j) denotes
the word sequence that covers the source span
[i, j) of f , e?[i?,j?) denotes the target translation
of f?[i,j), which is the word sequence that covers
the target span [i?, j?) of e at the same time, A? is
a set of word links that aligns f?[i,j) and e?[i?,j?).
? each non-terminal translation state S ? N is a
5-tuple {f?[i,j), e?[i?,j?), A?,m,m?,R}1. The first
3 elements have the same meanings as in T ,
whilem andm? denote two split points that di-
vide S into two child translation states, Sl and
Sr, R denotes a BTG rule, which is either a [?]
operation or a ??? operation2. The relationship
between Sl, Sr and S is illustrated in Figure 1.
All terminal translation states of the sentence pair
{f, e} are disjoint but cover f[0,J+1) and e[0,I+1) at
the same time, where J = |f | and I = |e|, and
all non-terminal translation states correspond to the
partial decoding states generated during decoding.
3.2 FDT-based Translation Model
First, an FDT-based translation model (FDT-TM) is
presented for our BTG-based system.
1We sometimes omit m, m? and R for a simplicity reason.
2A [?] operation combines the translations of two consecu-
tive source spans [i,m) and [m, j) in a monotonic way; while
a ??? operation combines them in an inverted way.
Given sentence pairs in training corpus with their
corresponding FDT spaces, we train FDT-TM in t-
wo different ways: (1) The first only uses the 1-best
FDT of each sentence pair. Based on each align-
ment A induced from each 1-best FDT G, all possi-
ble bilingual phrases are extracted. Then, the max-
imum likelihood estimation (MLE) is used to com-
pute probabilities and generate an FDT-TM. (2) The
second uses the n-best FDTs of each sentence pair,
which is motivated by several studies (Venugopal et
al., 2008; Liu et al2009). For each sentence pair
{f, e}, we first induce n alignments {A1, ...,An}
from the top n FDTs ? = {G1, ...,Gn} ? H(f, e).
Each Ak is annotated with the posterior probability
of its corresponding FDT Gk as follows:
p(Ak|Gk) =
exp{
?
i ?ihi(Gk)}
?
Gk???
exp{
?
i ?ihi(Gk?)}
(6)
where
?
i ?ihi(Gk) is the model score assigned to
Gk by MRM. Then, all possible bilingual phrases
are extracted from the expanded training corpus built
using n-best alignments for each sentence pair. The
count of each phrase pair is now computed as the
sum of posterior probabilities, instead of the sum of
absolute frequencies. Last, MLE is used to compute
probabilities and generate an FDT-TM.
3.3 FDT-based Distortion Model
In Xiong?s BTG system, training instances of the
distortion model (DM) are pruned based on heuris-
tic rules, aiming to keep the training size acceptable.
But this will cause the examples remained cannot
cover all reordering cases that could be met in real
decoding procedures. To overcome this drawback,
we propose an FDT-based DM (FDT-DM).
Given the 1-best FDT G of a sentence pair {f, e},
all non-terminal translation states {S1, ...,SK} are
first extracted. For each Sk, we split it into two
child translation states Skl and Skr. A training in-
stance can be then obtained, using the BTG opera-
tionR ? Sk as its class label and boundary words of
two translation blocks (f?Skl , e?Skl) and (f?Skr , e?Skr)
contained in Skl and Skr as its features. Last, the
FDT-DM is trained based on all training instances
by a MaxEnt toolkit, which can cover both local and
global reordering situations due to its training in-
stance extraction mechanism. Figure 2 shows an ex-
ample of extracting training instances from an FDT.
448
Figure 2: An example of extracting training instances
from an FDT, where solid lines with rows denote BTG
operations and dash lines denote alignments. Two in-
stances can be extracted from this FDT, where 0 and 1
denote a [?] operation and a ??? operation respectively. In
DM training, the number (0 or 1) in each instance is used
as a label, while boundary words are extracted from each
instance?s two phrase pairs and used as lexical features.
3.4 FDT-based Source Language Model
We next propose an FDT-based source language
model (FDT-SLM).
Given the 1-best FDT G of a sentence pair {f, e},
we first extract a reordered source word sequence
f ? = {f ?0, ..., f ?J} from G, based on the order of ter-
minal translation states in G which covers the target
translation e from left to right. This procedure can
be illustrated by Algorithm 1. Then, all reordered
source sentences of training corpus are used to train
a source LM. During decoding, each time when a
new hypothesis is generated, we obtain its reordered
source word sequence as well, compute a LM score
based on FDT-SLM and use it as a new feature:
hSLM (f ?) =
J
?
k=1
p(f ?k|f ?k?n+1, ..., f ?k?1) (7)
3.5 FDT-based Rule Sequence Model
The last contribution in this section is an FDT-based
rule sequence model (FDT-RSM).
Given the 1-best FDT G of a sentence pair {f, e},
we first extract a sequence of translation rule appli-
cations {r1, ..., rK} based on Algorithm 2, where
Algorithm 1: Sequence Extraction in FDT-SLM
1 let f ? = ?;
2 let S? = {S1? , ...,SK?} represents an ordered
sequence of terminal translation states whose
target phrases cover e from left to right orderly;
3 foreach S ? S? in the left-to-right order do
4 extract f?[i,j) from S;
5 append f?[i,j) to f ?;
6 append a blank space to f ?;
7 end
8 return f ? as a reordered source word sequence.
rk = (f?[i,j), e?[i?,j?)) denotes the kth phrase pair. Fig-
ure 3 gives an example of extracting a rule sequence
from an FDT. An FDT-RSM is trained based on all
rule sequences extracted from training corpus. Dur-
ing decoding, each time when a new hypothesis is
generated, we compute an FDT-RSM score based on
its rule sequence and use it as a new feature:
hRSM (f, e) =
K
?
k=1
p(rk|rk?n+1, ..., rk?1) (8)
Algorithm 2: Sequence Extraction in FDT-RSM
1 let r? = ?;
2 let S? = {S1? , ...,SK?} represents an ordered
sequence of terminal translation states whose
target phrases cover e from left to right orderly;
3 foreach S ? S? in the left-to-right order do
4 extract a phrase pair (f?[i,j), e?[i?,j?)) from S;
5 add rk = (f?[i,j), e?[i?,j?)) to r?;
6 end
7 return r? as a rule sequence.
The main difference between FDT-SLM and
FDT-RSM is that the former is trained based on
monolingual n-grams; while the latter is trained
based on bilingual phrases. Although these two
models are trained and computed in an LM style,
they are used as reordering features, because they
help SMT decoder find better decoding sequences.
Of course, the usage of FDTs need not be limit-
ed to the BTG-based system, and we consider using
FDTs generated by SCFG-based systems or tradi-
tional left-to-right phrase-based systems in future.
449
Figure 3: An example of extracting a rule sequence from
an FDT. In order to generate the correct target translation,
the desired rule sequence should be r2 ? r3 ? r1.
4 Related Work
4.1 Forced Decoding/Alignment
Schwartz (2008) used forced decoding to leverage
multilingual corpus to improve translation quality;
Shen et al2008) used forced alignment to train
a better phrase segmentation model; Wuebker et al
(2010) used forced alignment to re-estimate trans-
lation probabilities using a leaving-one-out strategy.
We consider the usage of FD in Section 2.1 to be
a direct extension of these approaches, but one that
generates FDTs for parallel data rather than focusing
on phrase segmentation or probability estimation.
4.2 Pre-reordering
Pre-reordering (PRO) techniques (Collins et al
2005; Xu et al2009; Genzel et al2010; Lee et
al., 2010) used features from syntactic parse trees
to reorder source sentences at training and transla-
tion time. A parser is often indispensable to provide
syntactic information for such methods. Recently,
DeNero and Uszkoreit (2011) proposed an approach
that induced parse trees automatically from word-
aligned training corpus to perform PRO for a phrase-
based SMT system, instead of relying on treebanks.
First, binary parse trees are induced from word-
aligned training corpus. Based on them, a monolin-
gual parsing model and a tree reordering model are
trained to pre-reorder source words into the target-
language-like order. Their work is distinct from ours
because it focused on inducing sentence structures
for the PRO task, but mirrors ours in demonstrating
that there is a potential role for structure-based train-
ing corpus in SMT model training.
4.3 Distortion Models
Lexicalized distortion models (Tillman, 2004; Zens
and Ney, 2006; Xiong et al2006; Galley and Man-
ning, 2008;) are widely used in phrase-based SMT
systems. Training instances of these models are ex-
tracted from word-aligned sentence pairs. Due to ef-
ficiency reasons, only parts of all instances are kept
and used in DM training, which cannot cover all pos-
sible reordering situations that could be met in de-
coding. In FDT-DM, by contrast, training instances
are extracted from FDTs. Such instances take both
local and global reordering cases into consideration.
4.4 Sequence Models
Feng et al2010) proposed an SLM in a phrase-
based SMT system. They used it as a reordering
feature in the sense that it helped the decoder to find
correct decoding sequences. The difference between
their model and our FDT-SLM is that, in their work,
the reordered source sequences are extracted based
on word alignments only; while in our FDT-SLM,
such sequences are obtained based on FDTs.
Quirk and Menezes (2006) proposed a Minimal
Translation Unit (MTU) -based sequence model and
used it in their treelet system; Vaswani et al2011)
proposed a rule Markov model to capture dependen-
cies between minimal rules for a top-down tree-to-
string system. The key difference between FDT-
RSM and previous work is that the rule sequences
are extracted from FDTs, and no parser is needed.
5 Experiments
5.1 Data and Metric
Experiments are carried out on English-to-Japanese
(E-J) and Chinese-to-English (C-E) MT tasks.
For E-J task, bilingual data used contains 13.3M
sentence pairs after pre-processing. The Japanese
side of bilingual data is used to train a 4-gram LM.
The development set (dev) which contains 2,000
sentences is used to optimize the log-linear SMT
model. Two test sets are used for evaluation, which
contain 5,000 sentences (test-1) and 999 sentences
450
(test-2) respectively. In all evaluation data sets, each
source sentence has only one reference translation.
For C-E task, bilingual data used contains 0.5M
sentence pairs with high translation quality, includ-
ing LDC2003E07, LDC2003E14, LDC2005T06,
LDC2005T10, LDC2005E83, LDC2006E26, LD-
C2006E34, LDC2006E85 and LDC2006E92. A 5-
gram LM is trained on the Xinhua portion of LDC
English Gigaword Version 3.0. NIST 2004 (MT04)
data set is used as dev set, and evaluation results
are measured on NIST 2005 (MT05) and NIST 2008
(MT08) data sets. In all evaluation data sets, each
source sentence has four reference translations.
Default word alignments for both SMT tasks are
performed by GIZA++ with the intersect-diag-grow
refinement. Translation quality is measured in terms
of case-insensitive BLEU (Papineni et al2002) and
reported in percentage numbers.
5.2 Baseline System
The phrase-based SMT system proposed by Xiong
et al2006) is used as the baseline system, with a
MaxEnt principle-based lexicalized reordering mod-
el integrated, which is used to handle reorderings in
decoding. The maximum lengths for the source and
target phrases are 5 and 7 on E-J task, and 3 and 5
on C-E task. The beam size is set to 20.
5.3 Translation Quality on E-J Task
We first evaluate the effectiveness of our FDT-based
model training approach on E-J translation task, and
present evaluation results in Table 1, in which BTG
denotes the performance of the baseline system.
FDT-TM denotes the improved system that uses
FDT-TM proposed in Section 3.2 instead of original
phrase table. As described in Section 3.2, we tried
different sizes of n-best FDTs to induce alignments
for phrase extraction and found the optimal choice
is 5. Besides, in order to make full use of the train-
ing corpus, for those sentence pairs that are failed in
FD, we just use their original word alignments to ex-
tract bilingual phrases. We can see from Table 1 that
FDT-TM outperforms the BTG system significantly.
FDT-DM denotes the improved system that us-
es FDT-DM proposed in Section 3.3 instead of o-
riginal distortion model. Comparing to baseline D-
M which has length limitation on training instances,
training examples of FDT-DM are extracted from 1-
best FDTs without any restriction. This makes our
new DM can cover both local and global reordering
situations that might be met in decoding procedures.
We can see from Table 1 that using FDT-DM, sig-
nificant improvements can be achieved.
FDT-SLM denotes the improved system that uses
FDT-SLM proposed in Section 3.4 as an addition-
al feature, in which the maximum n-gram order is
4. However, from Table 1 we notice that with FDT-
SLM integrated, only 0.2 BLEU improvements can
be obtained. We analyze decoding-logs and find that
the reordered source sequences of n-best translation-
s are very similar, which, we think, can explain why
improvements of using this model are so limited.
FDT-RSM denotes the improved system that us-
es FDT-RSM proposed in Section 3.5 as an addi-
tional feature. The maximum order of this model
is 3. From Table 1 we can see that FDT-RSM out-
performs BTG significantly, with up to 0.48 BLEU
improvements. Comparing to FDT-SLM, FDT-RSM
performs slightly better as well. We think it is due
to the fact that bilingual phrases can provide more
discriminative power than monolingual n-grams do.
Last, all these four FDT-based models (FDT-TM,
FDT-DM, FDT-SLM and FDT-RSM) are put togeth-
er to form an improved system that is denoted as
FDT-ALL. It can provide an averaged 1.2 BLEU im-
provements on these three evaluation data sets.
BLEU dev test-1 test-2
BTG 20.60 20.27 13.15
FDT-TM 21.21 20.71(+0.44) 13.98(+0.83)
FDT-DM 21.13 20.79(+0.52) 14.25(+1.10)
FDT-SLM 20.84 20.50(+0.23) 13.36(+0.21)
FDT-RSM 21.07 20.75(+0.48) 13.59(+0.44)
FDT-ALL 21.83 21.34(+1.07) 14.46(+1.31)
PRO 21.89 21.81 14.69
Table 1: FDT-based model training on E-J task.
Pre-reordering (PRO) is often used on language
pairs, e.g. English and Japanese, with very different
word orders. So we compare our method with PRO
as well. We re-implement the PROmethod proposed
by Genzel (2010) and show its results in Table 1. On
dev and test-2, FDT-ALL performs comparable to
PRO, with no syntactic information needed at all.
451
5.4 Translation Quality on C-E Task
We then evaluate the effectiveness of our FDT-based
model training approach on C-E translation task, and
present evaluation results in Table 2, from which we
can see significant improvements as well.
BLEU MT03 MT05 MT08
BTG 38.73 38.01 23.78
FDT-TM 39.14 38.31(+0.30) 24.30(+0.52)
FDT-DM 39.27 38.56(+0.55) 24.50(+0.72)
FDT-SLM 38.97 38.22(+0.21) 24.04(+0.26)
FDT-RSM 39.06 38.33(+0.32) 24.13(+0.35)
FDT-ALL 39.59 38.72(+0.71) 24.67(+0.89)
Table 2: FDT-based model training on C-E task
Comparing to numbers in Table 1, the gains com-
ing from the first two FDT-based models become s-
mall on C-E task. This might be due to the fact that
the word alignment quality in C-E task is more reli-
able than that in E-J task for TM and DM training.
5.5 Effect on Alignment Quality
We compare the qualities of alignments predicted by
GIZA++ and alignments induced from 1-best FDTs.
For E-J task, 575 English-Japanese sentence pairs
are manually annotated with word alignments. 382
sentence pairs are used as the dev set, and the other
193 sentence pairs are used as the test set. For C-E
task, 491 Chinese-English sentence pairs are manu-
ally annotated with word alignments. 250 sentence
pairs are used as the dev set, and the other 241 sen-
tence pairs are used as the test set. Both Japanese
and Chinese sentences are adapted to our own word
segmentation standards respectively. Table 3 shows
the comparison results. Comparing to C-E language
pair (S-V-O), E-J language pair (S-O-V) has much
lower F-scores, due to its very different word order.
F-score from GIZA++ from 1-best FDTs
devEJ 54.75% 57.93%(+3.18%)
testEJ 55.32% 58.47%(+3.15%)
devCE 81.32% 83.37%(+2.05%)
testCE 80.61% 82.51%(+1.90%)
Table 3: Comparison of alignment qualities predicted by
GIZA++ and induced from 1-best FDTs.
From Table 3 we can see that the F-score im-
proves on all language pairs when using alignments
induced from 1-best FDTs, rather than GIZA++.
5.6 Effect on Classification Accuracy
In the BTG system, the MaxEnt model is used as a
binary classifier to predict reordering operations of
neighbor translation blocks. As the baseline DM and
our FDT-DM have different mechanisms on training
instance extraction procedures, we compare the clas-
sification accuracies of these two DMs in Table 4 to
show the effect of different training instances. The
MaxEnt toolkit (Zhang, 2004) is used to optimize
feature weights using the l-BFGS method (Byrd et
al., 1995). We set the iteration number to 200 and
Gaussian prior to 1 for avoiding overfitting. Table
4 shows that when using training instances extract-
ed from FDTs, classification accuracy of reorderings
improves on both E-J and C-E tasks. This is because
FDTs can provide more deterministic and structured
knowledge for training instance extraction, which
can cover both local and global reordering cases.
baseline DM FDT-based DM
E-J 93.67% 95.60%(+1.93%)
C-E 95.85% 97.52%(+1.67%)
Table 4: Comparison of classification accuracies of DMs
based on instances extracted by different mechanisms.
6 Conclusions
In this paper, we have presented an FDT-based mod-
el training approach to SMT. As the first step in this
research direction, we have verified our method on a
phrase-based SMT system, and proposed four FDT-
based component models. Experiments on both E-J
and C-E tasks have demonstrated the effectiveness
of our approach. Summing up, comparing to plain
word alignments, FDTs provide richer structured
knowledge for more accurate SMT model training.
Several potential research topics can be explored in
future. For example, FDTs can be used in a pre-
reordering framework. This is feasible in the sense
that FDTs can provide both tree-like structures and
reordering information. We also plan to adapt our
FDT-based model training approach to SCFG-based
and traditional left-to-right phrase-based systems.
452
References
Peter Brown, Stephen Pietra, Vincent Pietra, and Robert
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation, Computa-
tional Linguistics.
Rafael Banchs, Josep Crego, Adri?a Gispert, Patrik Lam-
bert, and Jos Mario. 2005. Statistical Machine Trans-
lation of Euparl Data by using Bilingual N-grams, In
Proceedings of the ACL Workshop on Building and
Using Parallel Texts.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing, Machine Translation.
Alexandra Birch and Miles Osborne. 2011. Reordering
metrics for MT, In Proceedings of the Association for
Computational Linguistics.
Richard Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou
Zhu. 1995. A limited memory algorithm for bound
constrained optimization, SIAM Journal of Science
and Statistical Computing.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based Language Models for Statistical
Machine Translation, MT Summit.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation, In Proceedings of the Association for
Computational Linguistics.
John DeNero and Jakob Uszkoreit. 2011. Inducing Sen-
tence Structure from Parallel Corpora for Reordering,
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A
Source-side Decoding Sequence Model for Statistical
Machine Translation, In Proceedings of the Confer-
ence of the Association for Machine Translation.
Alexander Fraser and Daniel Marcu. 2006. Semi-
Supervised Training for Statistical Word Alignment, In
Proceedings of the International Conference on Com-
putational Linguistics and Annual Meeting of the As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion, In Proceedings of the Conference on Computa-
tional Linguistics.
Liang Huang and David Chiang. 2005. Better k-best
Parsing, In Proceedings of International Conference
on Parsing Technologies,.
Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo.
2010. Constituent Reordering and Syntax Models for
English-to-Japanese Statistical Machine Translation,
In Proceedings of the Conference on Computational
Linguistics.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted Alignment Matrices for Statistical Ma-chine
Translation, In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discrimi-
native Word Alignment by Linear Modeling, Compu-
tational Linguistics.
Robert Moore. 2005. A Discriminative Framework for
Bilingual Word Alignment, In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
Robert Moore, Wen-tau Yih, and Andreas Bode. 2006.
Improved Discriminative Bilingual Word Alignmen-
t, In Proceedings of the International Conference on
Computational Linguistics and Annual Meeting of the
Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based Translation, In Proceedings of the Association
for Computational Linguistics.
Galley Michel and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model, In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Franz Och. 2003. Minimum Error Rate Training in S-
tatistical Machine Translation, In Proceedings of the
Association for Computational Linguistics.
Franz Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation,
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation, In Proceedings of the
Association for Computational Linguistics.
Chris Quirk and Arul Menezes. 2006. Do we need phras-
es? Challenging the conventional wisdom in Statisti-
cal Machine Translation, In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Lane Schwartz. 2008. Multi-Source Translation Method-
s, In Proceedings of the Conference of the Association
for Machine Translation.
Wade Shen, Brian Delaney, Tim Anderson, and Ray Sly-
h. 2008. The MIT-LL/AFRL IWSLT-2008 MT System,
International Workshop on Spoken Language Transla-
tion.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering, In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov Models for Fast Tree-to-
String Translation, In Proceedings of the Association
for Computational Linguistics.
453
Ashish Venugopal, Andreas Zollmann, Noah Smith, and
Stephan Vogel. 2008. Wider Pipelines: N-best Align-
ments and Parses in MT Training, In Proceedings of
the Conference of the Association for Machine Trans-
lation.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training Phrase Translation Models with Leaving-
One-Out, In Proceedings of the Association for Com-
putational Linguistics.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora,
Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for s-
tatistical machine translation, In Proceedings of the
Association for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMT for Subject-Object-Verb Languages, In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion, In Proceedings of the Workshop on Statistical
Machine Translation.
Le Zhang. 2004. Maximum Entropy Modeling Toolkit
for Python and C++.
454
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645?650,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Joint Relational Embeddings for Knowledge-based Question Answering
Min-Chul Yang
?
Nan Duan
?
Ming Zhou
?
Hae-Chang Rim
?
?
Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
?
Microsoft Research Asia, Beijing, China
mcyang@nlp.korea.ac.kr
{nanduan, mingzhou}@microsoft.com
rim@nlp.korea.ac.kr
Abstract
Transforming a natural language (NL)
question into a corresponding logical form
(LF) is central to the knowledge-based
question answering (KB-QA) task. Un-
like most previous methods that achieve
this goal based on mappings between lex-
icalized phrases and logical predicates,
this paper goes one step further and pro-
poses a novel embedding-based approach
that maps NL-questions into LFs for KB-
QA by leveraging semantic associations
between lexical representations and KB-
properties in the latent space. Experimen-
tal results demonstrate that our proposed
method outperforms three KB-QA base-
line methods on two publicly released QA
data sets.
1 Introduction
Knowledge-based question answering (KB-QA)
involves answering questions posed in natural
language (NL) using existing knowledge bases
(KBs). As most KBs are structured databases,
how to transform the input question into its corre-
sponding structured query for KB (KB-query) as
a logical form (LF), also known as semantic pars-
ing, is the central task for KB-QA systems. Pre-
vious works (Mooney, 2007; Liang et al., 2011;
Cai and Yates, 2013; Fader et al., 2013; Berant et
al., 2013; Bao et al., 2014) usually leveraged map-
pings between NL phrases and logical predicates
as lexical triggers to perform transformation tasks
in semantic parsing, but they had to deal with two
limitations: (i) as the meaning of a logical pred-
icate often has different natural language expres-
sion (NLE) forms, the lexical triggers extracted for
a predicate may at times are limited in size; (ii)
entities detected by the named entity recognition
(NER) component will be used to compose the
logical forms together with the logical predicates,
so their types should be consistent with the pred-
icates as well. However, most NER components
used in existing KB-QA systems are independent
from the NLE-to-predicate mapping procedure.
We present a novel embedding-based KB-QA
method that takes all the aforementioned lim-
itations into account, and maps NLE-to-entity
and NLE-to-predicate simultaneously using sim-
ple vector operations for structured query con-
struction. First, low-dimensional embeddings of
n-grams, entity types, and predicates are jointly
learned from an existing knowledge base and from
entries <entity
subj
, NL relation phrase, entity
obj
>
that are mined from NL texts labeled as KB-
properties with weak supervision. Each such en-
try corresponds to an NL expression of a triple
<entity
subj
, predicate, entity
obj
> in the KB. These
embeddings are used to measure the semantic as-
sociations between lexical phrases and two prop-
erties of the KB, entity type and logical predicate.
Next, given an NL-question, all possible struc-
tured queries as candidate LFs are generated and
then they are ranked by the similarity between the
embeddings of observed features (n-grams) in the
NL-question and the embeddings of logical fea-
tures in the structured queries. Last, answers are
retrieved from the KB using the selected LFs.
The contributions of this work are two-fold: (1)
as a smoothing technique, the low-dimensional
embeddings can alleviate the coverage issues of
lexical triggers; (2) our joint approach integrates
entity span selection and predicate mapping tasks
for KB-QA. For this we built independent entity
embeddings as the additional component, solving
the entity disambiguation problem.
2 Related Work
Supervised semantic parsers (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Mooney,
2007) heavily rely on the <sentence, semantic an-
645
notation> pairs for lexical trigger extraction and
model training. Due to the data annotation re-
quirement, such methods are usually restricted to
specific domains, and struggle with the coverage
issue caused by the limited size of lexical triggers.
Studies on weakly supervised semantic parsers
have tried to reduce the amount of human supervi-
sion by using question-answer pairs (Liang et al.,
2011) or distant supervision (Krishnamurthy and
Mitchell, 2012) instead of full semantic annota-
tions. Still, for KB-QA, the question of how to
leverage KB-properties and analyze the question
structures remains.
Bordes et al. (2012) and Weston et al. (2013) de-
signed embedding models that connect free texts
with KBs using the relational learning method
(Weston et al., 2010). Their inputs are often
statement sentences which include subject and ob-
ject entities for a given predicate, whereas NL-
questions lack either a subject or object entity that
is the potential answer. Hence, we can only use
the information of a subject or object entity, which
leads to a different training instance generation
procedure and a different training criterion.
Recently, researchers have developed open do-
main systems based on large scale KBs such as
FREEBASE
1
(Cai and Yates, 2013; Fader et al.,
2013; Berant et al., 2013; Kwiatkowski et al.,
2013; Bao et al., 2014; Berant and Liang, 2014;
Yao and Van Durme, 2014). Their semantic
parsers for Open QA are unified formal and scal-
able: they enable the NL-question to be mapped
into the appropriate logical form. Our method ob-
tains similar logical forms, but using only low-
dimensional embeddings of n-grams, entity types,
and predicates learned from texts and KB.
3 Setup
3.1 Relational Components for KB-QA
Our method learns semantic mappings between
NLEs and the KB
2
based on the paired relation-
ships of the following three components: C de-
notes a set of bag-of-words (or n-grams) as context
features (c) for NLEs that are the lexical represen-
tations of a logical predicate (p) in KB; T denotes
a set of entity types (t) in KB and each type can be
used as the abstract expression of a subject entity
1
http://www.freebase.com
2
For this paper, we used a large scale knowledge base that
contains 2.3B entities, 5.5K predicates, and 18B assertions.
A 16-machine cluster was used to host and serve the whole
data.
(s) that occurs in the input question; P denotes a
set of logical predicates (p) in KB, each of which
is the canonical form of different NLEs sharing an
identical meaning (bag-of-words; c).
Based on the components defined above, the
paired relationships are described as follows: T -
P can investigate the relationship between sub-
ject entity and logical predicate, as object entity
is always missing in KB-QA; C-T can scruti-
nize subject entity?s attributes for the entity span
selection such as its positional information and
relevant entity types to the given context, which
may solve the entity disambiguation problem in
KB-QA; C-P can leverage the semantic overlap
between question contexts (n-gram features) and
logical predicates, which is important for mapping
NL-questions to their corresponding predicates.
3.2 NLE-KB Pair Extraction
This section describes how we extract the semantic
associated pairs of NLE-entries and KB-triples to
learn the relational embeddings (Section 4.1).
<Relation Mention, Predicate> Pair (MP)
Each relation mention denotes a lexical phrase
of an existing KB-predicate. Following informa-
tion extraction methods, such as PATTY (Nakas-
hole et al., 2012), we extracted the <relation
mention, logical predicate> pairs from English
WIKIPEDIA
3
, which is closely connected to our
KB, as follows: Given a KB-triple <entity
subj
,
logical predicate, entity
obj
>, we extracted NLE-
entries <entity
subj
, relation mention, entity
obj
>
where relation mention is the shortest path be-
tween entity
subj
and entity
obj
in the dependency
tree of sentences. The assumption is that any re-
lation mention (m) in the NLE-entry containing
such entity pairs that occurred in the KB-triple is
likely to express the predicate (p) of that triple.
With obtaining high-qualityMP pairs, we kept
only relation mentions that were highly associated
with a predicate measured by the scoring function:
S(m, p) = PMI(e
m
; e
p
) + PMI(u
m
;u
p
) (1)
where e
x
is the set of total pairs of both-side
entities of entry x (m or p) and u
x
is the set
of unique (distinct) pairs of both-side entities of
entry x. In this case, the both-side entities in-
dicate entity
subj
and entity
obj
. For a frequency-
based probability, PMI(x; y) = log
P (x,y)
P (x)P (y)
3
http://en.wikipedia.org/
646
(Church and Hanks, 1990) can be re-written as
PMI(x; y) = log
|x
?
y|?C
|x|?|y|
, where C denotes the
total number of items shown in the corpus. The
function is partially derived from the support score
(Gerber and Ngonga Ngomo, 2011), but we fo-
cus on the correlation of shared entity pairs be-
tween relation mentions and predicates using the
PMI computation.
<Question Pattern, Predicate> Pair (QP)
Since WIKIPEDIA articles have no information to
leverage interrogative features which highly de-
pend on the object entity (answer), it is difficult to
distinguish some questions that are composed of
only different 5W1H words, e.g., {When|Where}
was Barack Obama born? Hence, we used the
method of collecting question patterns with human
labeled predicates that are restricted by the set of
predicates used inMP (Bao et al., 2014).
4 Embedding-based KB-QA
Our task is as follows. First, our model learns the
semantic associations of C-T , C-P , and T -P (Sec-
tion 3.1) based on NLE-KB pairs (Section 3.2),
and then predicts the semantic-related KB-query
which can directly find the answer to a given NL-
question.
For our feature space, given an NLE-KB pair,
the NLE (relation mention in MP or question
pattern in QP) is decomposed into n-gram fea-
tures: C = {c | c is a segment of NLE}, and
the KB-properties are represented by entity type
t of entity
subj
and predicate p. Then we can ob-
tain a training triplet w = [C, t, p]. Each feature
(c ? C, t ? T , p ? P) is encoded in the distributed
representation which is n-dimensional embedding
vectors (E
n
): ?x, x
encode
? E(x) ? E
n
.
All n-gram features (C) for an NLE are merged
into one embedding vector to help speed up the
learning process: E(C) =
?
c?C
E(c)/|C|. This
feature representation is inspired by previous work
in embedding-based relation extraction (Weston et
al., 2013), but differs in the following ways: (1)
entity information is represented on a separate em-
bedding, but its positional information remains as
symbol ?entity?; (2) when the vectors are com-
bined, we use the average of each index to normal-
ize features.
For our joint relational approach, we focus on
the set of paired relationships R = {C-t, C-p, t-
p} that can be semantically leveraged. Formally,
these features are embedded into the same latent
space (E
n
) and their semantic similarities can be
computed by a dot product operation:
Sim(a, b) = Sim(r
ab
) = E(a)
?
E(b) (2)
where r
ab
denotes a paired relationship a-b (or (a,
b)) in the above set R. We believe that our joint re-
lational learning can smooth the surface (lexical)
features for semantic parsing using the aligned en-
tity and predicate.
4.1 Joint Relational Embedding Learning
Our ranking-based relational learning is based on
a ranking loss (Weston et al., 2010) that supports
the idea that the similarity scores of observed pairs
in the training set (positive instances) should be
larger than those of any other pairs (negative in-
stances):
?i, ?y
?
6= y
i
, Sim(x
i
, y
i
) > 1+Sim(x
i
, y
?
) (3)
More precisely, for each triplet w
i
= [C
i
, t
i
, p
i
]
obtained from an NLE-KB pair, the relationships
R
i
= {C
i
-t
i
, C
i
-p
i
, t
i
-p
i
} are trained under the
soft ranking criterion, which conducts Stochastic
Gradient Descent (SGD). We thus aim to minimize
the following:
?i,?y
?
6= y
i
,max(0, 1?Sim(x
i
, y
i
)+Sim(x
i
, y
?
))
(4)
Our learning strategy is as follows. First, we ini-
tialize embedding space E
n
by randomly giving
mean 0 and standard deviation 1/n to each vec-
tor. Then for each training triplet w
i
, we select the
negative pairs against positive pairs (C
i
-t
i
, C
i
-p
i
,
and t
i
-p
i
) in the triplet. Last, we make a stochastic
gradient step to minimize Equation 4 and update
E
n
at each step.
4.2 KB-QA using Embedding Models
Our goal for KB-QA is to translate a given NL-
question to a KB-query with the form <subject
entity, predicate, ?>, where ? denotes the an-
swer entity we are looking for. The decoding pro-
cess consists of two stages. The first stage in-
volves generating all possible KB-queries (K
q
) for
an NL-question q. We first extract n-gram fea-
tures (C
q
) from the NL-question q. Then for a
KB-query k
q
, we find all available entity types
(t
q
) of the identified subject entities (s
q
) using
the dictionary-based entity detection on the NL-
question q (all of spans can be candidate entities),
and assign all items of predicate set (P) as the can-
didate predicates (p
q
). Like the training triplets,
647
q where is the city of david?
?
k(q) [The City of David, contained by, ?]
C
q
n-grams of ?where is ?entity? ??
t
q
location
p
q
contained by
Table 1: The corresponding KB-query
?
k(q) for a
NL-question q and its decoding triplet w
q
.
we also represent the above features as the triplet
form w
q
i
= [C
q
i
, t
q
i
, p
q
i
] which is directly linked to
a KB-query k
q
i
= [s
q
i
, p
q
i
, ?]. The second stage
involves ranking candidate KB-queries based on
the similarity scores between the following paired
relationships from the triplet w
q
i
: R
q
i
= {C
q
i
-t
q
i
,
C
q
i
-p
q
i
, t
q
i
-p
q
i
}. Unlike in the training step, the sim-
ilarities of C
q
i
-t
q
i
and C
q
i
-p
q
i
are computed by sum-
mation of all pairwise elements (each context em-
bedding E(c), not E(C), with each paired E(t) or
E(p)) for a more precise measurement. Since sim-
ilarites of R
q
are calculated on different scales, we
normalize each value using Z-score (Z(x) =
x??
?
)
(Kreyszig, 1979). The final score is measured by:
Sim
q2k
(q, k
q
) =
?
r?R
q
Z(Sim(r)) (5)
Then, given any NL-question q, we can predict the
corresponding KB-query
?
k(q):
?
k(q) = argmax
k?K
q
Sim
q2k
(q, k) (6)
Last, we can retrieve an answer from the KB using
a structured query
?
k(q). Table 1 shows an example
of our decoding process.
Multi-related Question Some questions in-
clude two-subject entities, both of which are cru-
cial to understanding the question. For the ques-
tion who plays gandalf in the lord of the rings?
Gandalf (character) and The Lord Of The
Rings (film) are explicit entities that should be
joined to a pair of the two entities (implicit entity).
More precisely, the two entities can be combined
into one concatenated entity (character-in-film)
using our manual rule, which compares the possi-
ble pairs of entity types in the question with the
list of pre-defined entity type pairs that can be
merged into a concatenated entity. Our solution
enables a multi-related question to be transformed
to a single-related question which can be directly
translated to a KB-query. Then, the two entity
# Entries Accuracy
MP pairs 291,585 89%
QP pairs 4,764 98%
Table 2: Statistics of NLE-KB pairs
mentions are replaced with the symbol ?entity?
(who play ?entity? in ?entity? ?). We re-
gard the result of this transformation as one of the
candidate KB-queries in the decoding step.
5 Experiments
Experimental Setting We first performed pre-
processing, including lowercase transformation,
lemmatization and tokenization, on NLE-KB pairs
and evaluation data. We used 71,310 n-grams
(uni-, bi-, tri-), 990 entity types, and 660 predi-
cates as relational components shown in Section
3.1. The sum of these three numbers (72,960)
equals the size of the embeddings we are going
to learn. In Table 2, we evaluated the quality of
NLE-KB pairs (MP and QP) described in Sec-
tion 3.2. We can see that the quality ofQP pairs is
good, mainly due to human efforts. Also, we ob-
tained MP pairs that have an acceptable quality
using threshold 3.0 for Equation 1, which lever-
ages the redundancy information in the large-scale
data (WIKIPEDIA). For our embedding learning,
we set the embedding dimension n to 100, the
learning rate (?) for SGD to 0.0001, and the it-
eration number to 30. To make the decoding
procedure computable, we kept only the popular
KB-entity in the dictionary to map different entity
mentions into a KB-entity.
We used two publicly released data sets for QA
evaluations: Free917 (Cai and Yates, 2013) in-
cludes the annotated lambda calculus forms for
each question, and covers 81 domains and 635
Freebase relations; WebQ. (Berant et al., 2013)
provides 5,810 question-answer pairs that are built
by collecting common questions from Web-query
logs and by manually labeling answers. We used
the previous three approaches (Cai and Yates,
2013; Berant et al., 2013; Bao et al., 2014) as our
baselines.
Experimental Results Table 3 reports the over-
all performances of our proposed KB-QA method
on the two evaluation data sets and compares them
with those of the three baselines. Note that we
did not re-implement the baseline systems, but just
borrowed the evaluation results reported in their
648
Methods Free917 WebQ.
Cai and Yates (2013) 59.00% N/A
Berant et al. (2013) 62.00% 31.40%
Bao et al. (2014) N/A 37.50%
Our method 71.38% 41.34%
Table 3: Accuracy on the evaluation data
Methods Free917 WebQ.
Our method 71.38% 41.34%
w/o T -P 70.65% 40.55%
w/o C-T 67.03% 38.44%
w/o C-P 31.16% 19.24%
Table 4: Ablation of the relationship types
papers. Although the KB used by our system is
much larger than FREEBASE, we still think that
the experimental results are directly comparable
because we disallow all the entities that are not in-
cluded in FREEBASE.
Table 3 shows that our method outperforms the
baselines on both Free917 and WebQ. data sets.
We think that using the low-dimensional embed-
dings of n-grams rather than the lexical triggers
greatly improves the coverage issue. Unlike the
previous methods which perform entity disam-
biguation and predicate prediction separately, our
method jointly performs these two tasks. More
precisely, we consider the relationships C-T and
C-P simultaneously to rank candidate KB-queries.
In Table 1, the most independent NER in KB-QA
systems may detect David as the subject entity,
but our joint approach can predict the appropriate
subject entity The City of David by leveraging
not only the relationships with other components
but also other relationships at once. The syntax-
based (grammar formalism) approaches such as
Combinatory Categorial Grammar (CCG) may ex-
perience errors if a question has grammatical er-
rors. However, our bag-of-words model-based ap-
proach can handle any question as long as the
question contains keywords that can help in un-
derstanding it.
Table 4 shows the contributions of the relation-
ships (R) between relational components C, T ,
and P . For each row, we remove the similarity
from each of the relationship types described in
Section 3.1. We can see that the C-P relationship
plays a crucial role in translating NL-questions to
KB-queries, while the other two relationships are
slightly helpful.
Result Analysis Since the majority of questions
in WebQ. tend to be more natural and diverse, our
method cannot find the correct answers to many
questions. The errors can be caused by any of
the following reasons. First, some NLEs cannot
be easily linked to existing KB-predicates, mak-
ing it difficult to find the answer entity. Second,
some entities can be mentioned in several different
ways, e.g., nickname (shaq?Shaquille O?neal)
and family name (hitler?Adolf Hitler). Third, in
terms of KB coverage issues, we cannot detect the
entities that are unpopular. Last, feature represen-
tation for a question can fail when the question
consists of rare n-grams.
The two training sets shown in Section 3.2 are
complementary: QP pairs provide more oppor-
tunities for us to learn the semantic associations
between interrogative words and predicates. Such
resources are especially important for understand-
ing NL-questions, as most of them start with such
5W1H words; on the other hand, MP pairs en-
rich the semantic associations between context in-
formation (n-gram features) and predicates.
6 Conclusion
In this paper, we propose a novel method that
transforms NL-questions into their corresponding
logical forms using joint relational embeddings.
We also built a simple and robust KB-QA system
based on only the learned embeddings. Such em-
beddings learn the semantic associations between
natural language statements and KB-properties
from NLE-KB pairs that are automatically ex-
tracted from English WIKIPEDIA using KB-triples
with weak supervision. Then, we generate all pos-
sible structured queries derived from latent logical
features of the given NL-question, and rank them
based on the similarity scores between those re-
lational attributes. The experimental results show
that our method outperforms the latest three KB-
QA baseline systems. For our future work, we will
build concept-level context embeddings by lever-
aging latent meanings of NLEs rather than their
surface n-grams with the aligned logical features
on KB.
Acknowledgement This research was sup-
ported by the Next-Generation Information
Computing Development Program through the
National Research Foundation of Korea (NRF)
funded by the Ministry of Science, ICT & Future
Planning (NRF-2012M3C4A7033344).
649
References
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 967?976. Association for Computa-
tional Linguistics.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, pages 1415?1425. Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In In Proceedings of 15th International
Conference on Artificial Intelligence and Statistics.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Association for Computational Lin-
guistics (ACL), pages 423?433. The Association for
Computer Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22?29, March.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Association for Computational Lin-
guistics (ACL), pages 1608?1618. The Association
for Computer Linguistics.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In 1st Workshop
on Web Scale Knowledge Extraction @ ISWC 2011.
E. Kreyszig. 1979. Advanced Engineering Mathemat-
ics. Wiley.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 754?765, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 590?599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
RaymondJ. Mooney. 2007. Learning for semantic
parsing. In Alexander Gelbukh, editor, Computa-
tional Linguistics and Intelligent Text Processing,
volume 4394 of Lecture Notes in Computer Science,
pages 311?324. Springer Berlin Heidelberg.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1135?1145, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine
Learning, 81(1):21?35, October.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366?1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 956?966, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, AAAI?96, pages 1050?1055. AAAI Press.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
650
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1258?1267,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 
Hypothesis Mixture Decoding for Statistical Machine Translation 
 
 
Nan Duan, Mu Li, and Ming Zhou 
School of Computer Science and Technology Natural Language Computing Group 
Tianjin University Microsoft Research Asia 
Tianjin, China Beijing, China 
v-naduan@microsoft.com {muli,mingzhou}@microsoft.com 
 
 
 
 
 
Abstract 
This paper presents hypothesis mixture decoding 
(HM decoding), a new decoding scheme that 
performs translation reconstruction using hypo-
theses generated by multiple translation systems. 
HM decoding involves two decoding stages: 
first, each component system decodes indepen-
dently, with the explored search space kept for 
use in the next step; second, a new search space 
is constructed by composing existing hypotheses 
produced by all component systems using a set 
of rules provided by the HM decoder itself, and 
a new set of model independent features are 
used to seek the final best translation from this 
new search space. Few assumptions are made by 
our approach about the underlying component 
systems, enabling us to leverage SMT models 
based on arbitrary paradigms. We compare our 
approach with several related techniques, and 
demonstrate significant BLEU improvements in 
large-scale Chinese-to-English translation tasks. 
1 Introduction 
Besides tremendous efforts on constructing more 
complicated and accurate models for statistical 
machine translation (SMT) (Och and Ney, 2004; 
Chiang, 2005; Galley et al, 2006; Shen et al, 2008; 
Chiang 2010), many researchers have concentrated 
on the approaches that improve translation quality 
using information between hypotheses from one or 
more SMT systems as well. 
System combination is built on top of the N-best 
outputs generated by multiple component systems 
(Rosti et al, 2007; He et al, 2008; Li et al, 2009b) 
which aligns multiple hypotheses to build confu-
sion networks as new search spaces, and outputs 
the highest scoring paths as the final translations. 
Consensus decoding, on the other hand, can be 
based on either single or multiple systems: single 
system based methods (Kumar and Byrne, 2004; 
Tromble et al, 2008; DeNero et al, 2009; Kumar 
et al, 2009) re-rank translations produced by a 
single SMT model using either n-gram posteriors 
or expected n-gram counts. Because hypotheses 
generated by a single model are highly correlated, 
improvements obtained are usually small; recently, 
dedicated efforts have been made to extend it from 
single system to multiple systems (Li et al, 2009a; 
DeNero et al, 2010; Duan et al, 2010). Such me-
thods select translations by optimizing consensus 
models over the combined hypotheses using all 
component systems? posterior distributions. 
Although these two types of approaches have 
shown consistent improvements over the standard 
Maximum a Posteriori (MAP) decoding scheme, 
most of them are implemented as post-processing 
procedures over translations generated by MAP 
decoders. In this sense, the work of Li et al (2009a) 
is different in that both partial and full hypotheses 
are re-ranked during the decoding phase directly 
using consensus between translations from differ-
ent SMT systems. However, their method does not 
change component systems? search spaces. 
This paper presents hypothesis mixture decoding 
(HM decoding), a new decoding scheme that per-
forms translation reconstruction using hypotheses 
generated by multiple component systems. HM 
decoding involves two decoding stages: first, each 
component system decodes the source sentence 
independently, with the explored search space kept 
for use in the next step; second, a new search 
space is constructed by composing existing hypo-
1258
 theses produced by all component systems using a 
set of rules provided by the HM decoder itself, and 
a new set of component model independent fea-
tures are used to seek the final best translation 
from this new constructed search space. 
We evaluate by combining two SMT models 
with state-of-the-art performances on the NIST 
Chinese-to-English translation tasks. Experimental 
results show that our approach outperforms the 
best component SMT system by up to 2.11 BLEU 
points. Consistent improvements can be observed 
over several related decoding techniques as well, 
including word-level system combination, colla-
borative decoding and model combination. 
2 Hypothesis Mixture Decoding 
2.1 Motivation and Overview 
SMT models based on different paradigms have 
emerged in the last decade using fairly different 
levels of linguistic knowledge. Motivated by the 
success of system combination research, the key 
contribution of this work is to make more effective 
use of the extended search spaces from different 
SMT models in decoding phase directly, rather 
than just post-processing their final outputs. We 
first begin with a brief review of single system 
based SMT decoding, and then illustrate major 
challenges to this end. 
Given a source sentence  , an SMT decoder 
seeks for a target translation   that best matches   
as its translation by maximizing the following 
conditional probability: 
       
                   
                            
 
where      is the feature vector that includes a set 
of system specific features,   is the weight vector, 
     is a derivation that can yield   and is defined 
as a sequence of translation rule applications    . 
Figure 1 illustrates a decoding example, in which 
the final translation is generated by recursively 
composing partial hypotheses that cover different 
ranges of the source sentence until the whole input 
sentence is fully covered, and the feature vector of 
the final translation is the aggregation of feature 
vectors of all partial hypotheses used.1 
However, hypotheses generated by different 
SMT systems cannot be combined directly to form 
new translations because of two major issues: 
The first one is the heterogeneous structures of 
different SMT models. For example, a string-to-
tree system cannot use hypotheses generated by a 
phrase-based system in decoding procedure, as 
such hypotheses are based on flat structures, which 
cannot provide any additional information needed 
in the syntactic model. 
The second one is the incompatible feature 
spaces of different SMT models. For example, 
even if a phrase-based system can use the lexical 
forms of hypotheses generated by a syntax-based 
system without considering syntactic structures, 
the feature vectors of these hypotheses still cannot 
be aggregated together in any trivial way, because 
the feature sets of SMT models based on different 
paradigms are usually inconsistent. 
To address these two issues discussed above, we 
propose HM decoding that performs translation 
reconstruction using hypotheses generated by mul-
tiple component systems. 2  Our method involves 
two decoding stages depicted as follows: 
1. Independent decoding stage, in which each 
component system decodes input sentences 
independently based on its own model and 
search algorithm, and the explored search 
spaces (translation forests) are kept for use in 
the next stage. 
                                                 
1 There are also features independent of translation deriva-
tions, such as the language model feature. 
2 In this paper, we will constrain our discussions within CKY-
style decoders, in which we find translations for all spans of 
the source sentence. Although standard implementations of 
phrase-based decoders fall out of this scope, they can be still 
re-written to work in the CKY-style bottom-up manner at the 
cost of 1) only BTG-style reordering allowed, and 2) higher 
time complexity. As a result, any phrase-based SMT system 
can be used as a component in our HM decoding method. 
China ?s economic growth 
[-2.48, 4] 
 
China 
[-0.36, 1] 
? 
 
?? ?? ?? 
?s  
[-0.69, 1] 
economic 
[-0.51, 1] 
growth 
[-0.92, 1] 
China ?s 
[-1.05, 2] 
economic growth 
[-1.43, 2] 
Figure 1: A decoding example of a phrase-based 
SMT system. Each hypothesis is annotated with a 
feature vector, which includes a logarithmic probabil-
ity feature and a word count feature. 
1259
  
2. HM decoding stage, where a mixture search 
space is constructed for translation derivations 
by composing partial hypotheses generated by 
all component systems, and a new decoding 
model with a set of enriched feature functions 
are used to seek final translations from this 
newly generated search space. 
HM decoding can use lexicalized hypotheses of 
arbitrary SMT models to derive translation, and a 
set of component model independent features are 
used to compute translation confidence. We dis-
cuss mixture search space construction, details of 
model and feature designs as well as HM decoding 
algorithms in Section 2.2, 2.3 and 2.4 respectively. 
2.2 Mixture Search Space Construction 
Let        denote  component MT systems, 
  
 
 denote the span of a source sentence   starting 
at position   and ending at position  . We use 
     
   denoting the search space of   
 
 predicted 
by  , and     
   denoting the mixture search 
space of   
 
 constructed by the HM decoder, which 
is defined recursively as follows: 
?      
       
  . This rule adds all compo-
nent systems? search spaces into the mixture 
search space for use in HM decoding. Thus 
hypotheses produced by all component sys-
tems are still available to the HM decoder. 
?      
        
        
  , in which      
     and    
        
   .   is a translation 
rule provided by HM decoder that composes a 
new hypothesis using smaller hypotheses in 
the search spaces     
           
   . These 
rules further extend     
   with hypotheses 
generated by the HM decoder itself. 
Figure 2 shows an example of HM decoding, in 
which hypotheses generated by two SMT systems 
are used together to compose new translations. 
Since search space pruning is the indispensable 
procedure for all SMT systems, we will omit its 
explicit expression in the following descriptions 
and algorithms for convenience. 
2.3 Models and Features 
Following the common practice in SMT research, 
we use a linear model to formulate the preference 
of translation hypotheses in the mixture search 
space    . Formally, we are to find a translation 
  that maximizes the weighted linear combination 
of a set of real-valued features as follows: 
         
      
           
 
  
where         is an HM decoding feature with its 
corresponding feature weight   . 
In this paper, the HM decoder does not assume 
the availability of any internal knowledge of the 
underlying component systems. The HM decoding 
features are independent of component models as 
well, which fall into two categories: 
The first category contains a set of consensus-
based features, which are inspired by the success 
of consensus decoding approaches. These features 
are described in details as follows: 
1)            : the n-gram posterior feature of 
  computed based on the component search 
space      generated by  : 
                            
   
 
                
      
             is 
the posterior probability of an n-gram   in 
     ,       is the number of times that   
occurs in  ,       equals to 1 when   occurs 
in  , and 0 otherwise. 
Figure 2: An example of HM decoding, in which the 
translations surrounded by the dotted lines are newly 
generated hypotheses. Hypotheses light-shaded come 
from a phrase-based system, and hypotheses dark-
shaded come from a syntax-based system. 
economic growth of China 
economic growth China ?s 
? ?? ?? ?? 
development of economy 
China ?s development of economy 
China ?s economic growth 
of China 
development of economy of China 
 
? Rules provided by 
the HM decoder 
1260
 2)          
    : the stemmed n-gram posterior 
feature of   computed based on the stemmed 
component search space  
    . A word stem 
dictionary that includes 22,660 entries is used 
to convert   and      into their stem forms 
   and  
     by replacing each word into its 
stem form. This feature is computed similarly 
to that of            . 
3)           : the n-gram posterior feature of   
computed based on the mixture search space 
     generated by the HM decoder: 
                          
   
 
               
                 is the 
posterior probability of an n-gram   in    , 
        is the posterior probability of one 
translation    given   based on    . 
4)        : the length posterior feature of the 
specific target hypothesis with length   based 
on the mixture search space     generated 
by the HM decoder: 
            
    
                  
 
Note here that features in            and         
will be computed when the computations of all the 
remainder features in two categories have already 
finished for each   in    , and they will be used 
to update current HM decoding model scores. 
Consensus features based on component search 
spaces have already shown effectiveness (Kumar 
et al, 2009; DeNero et al, 2010; Duan et al, 
2010). We leverage consensus features based on 
the mixture search space newly generated in HM 
decoding as well. The length posterior feature (Zen 
and Ney, 2006) is used to adjust the preference of 
HM decoder for longer or shorter translations, and 
the stemmed n-gram posterior features are used to 
provide more discriminative power for HM decod-
ing and to decrease the effects of morphological 
changes in words for more accurate computation 
of consensus statistics. 
The second feature category contains a set of 
general features. Although there are more features 
that can be incorporated into HM decoding besides 
the ones we list below, we only utilize the most 
representative ones for convenience: 
1)             : the word count feature. 
2)         : the language model feature. 
3)           : the dictionary-based feature that 
counts how many lexicon pairs can be found 
in a given translation pair      . 
4)           and          : reordering features 
that penalize the uses of straight and inverted 
BTG rules during the derivation of   in HM 
decoding. These two features are specific to 
BTG-based HM decoding (Section 2.4.1): 
                   
      
 
                   
      
 
5)            and           : reordering fea-
tures that penalize the uses of hierarchical and 
glue rules during the derivation of   in HM 
decoding. These two features are specific to 
SCFG-based HM decoding (Section 2.4.2): 
                  
      
 
                    
      
 
  is the hierarchical rule set provided by the 
HM decoder itself,       equals to 1 when   
is provided by  , and 0 otherwise. 
6)          : the feature that counts how many 
n-grams in   are newly generated by the HM 
decoder, which cannot be found in all existing 
component search spaces: 
                          
 
   
 
   
 
          
 
     equals to 1 when   does 
not exist in      
 
   , and 0 otherwise. 
The MERT algorithm (Och, 2003) is used to 
tune weights of HM decoding features. 
2.4 Decoding Algorithms 
Two CKY-style algorithms for HM decoding are 
presented in this subsection. The first one is based 
on BTG (Wu, 1997), and the second one is based 
on SCFG, similar to Chiang (2005). 
1261
 2.4.1 BTG-based HM Decoding 
The first algorithm, BTG-HMD, is presented in 
Algorithm 1, where hypotheses of two consecutive 
source spans are composed using two BTG rules: 
? Straight rule    . It combines translations of 
two consecutive blocks into a single larger 
block in a straight order. 
? Inverted rule   . It combines translations of 
two consecutive blocks into a single larger 
block in an inverted order. 
These two rules are used bottom-up until the 
whole source sentence is fully covered. We use 
two reordering rule penalty features,           and 
         , to penalize the uses of these two rules. 
 
Algorithm 1: BTG-based HM Decoding 
1: for each component model   do 
2:  output the search space      for the input   
3: end for 
4: for     to       do 
5:  for all     s.t.       do 
6:       
 
      
7:   for all   s.t.       do 
8:    for        
   and          
 
  do 
9:     add                  to    
 
  
10:     add                 to    
 
  
11:    end for 
12:   end for 
13:   for each hypothesis         
 
      do 
14:    compute HM decoding features for   
15:    add   to    
 
  
16:   end for 
17:   for each hypothesis       
 
  do 
18: 
   
compute the n-gram and length posterior 
features for   based on    
 
  
19:    update current HM decoding score of   
20:   end for 
21:  end for 
22: end for 
23: return         with the maximum model score 
 
In BTG-HMD, in order to derive translations for 
a source span   
 
, we compose hypotheses of any 
two smaller spans   
  and     
 
 using two BTG 
rules in line 9 and 10,              denotes the 
operations that firstly combine    and    using one 
BTG rule   and secondly compute HM decoding 
features for the newly generated hypothesis  . We 
compute HM decoding features for hypotheses 
contained in all existing component search spaces 
      
 
      as well, and add them to     
 
 . 
From line 17 to 20, we update current HM decod-
ing scores for all hypotheses in    
 
  using the 
n-gram and length posterior features computed 
based on    
 
 . When the whole source sentence 
is fully covered, we return the hypothesis with the 
maximum model score as the final best translation. 
2.4.2 SCFG-based HM Decoding 
The second algorithm, SCFG-HMD, is presented 
in Algorithm 2. An additional rule set , which is 
provided by the HM decoder, is used to compose 
hypotheses. It includes hierarchical rules extracted 
using Chiang (2005)?s method and glue rules. Two 
reordering rule penalty features,            and 
          , are used to adjust the preferences of 
using hierarchical rules and glue rules. 
 
Algorithm 2: SCFG-based HM Decoding 
1: for each component model   do 
2:  output the search space      for the input   
3: end for 
4: for     to       do 
5:  for all     s.t.       do 
6:       
 
      
7:   for each rule     that matches   
 
do 
8:    for           and           do 
9:     add                to    
 
  
10:    end for 
11:   end for 
12:   for each hypothesis         
 
      do 
13:    compute HM decoding features for   
14:    add   to    
 
  
15:   end for 
16:   for each hypothesis       
 
  do 
17: 
   
compute the n-gram and length posterior 
features for   based on    
 
  
18:    update current HM decoding score of   
19:   end for 
20:  end for 
21: end for 
22: return         with the maximum model score 
 
Compared to BTG-HMD, the key differences in 
SCFG-HMD are located from line 7 to 11, where 
the translation for a given span   
 
 is generated by 
replacing the non-terminals in a hierarchical rule 
    with their corresponding target translations, 
    is the source span that is covered by the  
th non-
terminal of  ,        is the search space for     
predicted by the HM decoder. 
1262
 3 Comparisons to Related Techniques 
3.1 Model Combination and Mixture Model 
based MBR Decoding 
Model combination (DeNero et al, 2010) is an 
approach that selects translations from a conjoint 
search space using information from multiple SMT 
component models; Duan et al (2010) presents a 
similar method, which utilizes a mixture model to 
combine distributions of hypotheses from different 
systems for Bayes-risk computation, and selects 
final translations from the combined search spaces 
using MBR decoding. Both of these two methods 
share a common limitation: they only re-rank the 
combined search space, without the capability to 
generate new translations. In contrast, by reusing 
hypotheses generated by all component systems in 
HM decoding, translations beyond any existing 
search space can be generated. 
3.2 Co-Decoding and Joint Decoding 
Li et al (2009a) proposes collaborative decoding, 
an approach that combines translation systems by 
re-ranking partial and full translations iteratively 
using n-gram features from the predictions of other 
member systems. However, in co-decoding, all 
member systems must work in a synchronous way, 
and hypotheses between different systems cannot 
be shared during decoding procedure; Liu et al 
(2009) proposes joint-decoding, in which multiple 
SMT models are combined in either translation or 
derivation levels. However, their method relies on 
the correspondence between nodes in hypergraph 
outputs of different models. HM decoding, on the 
other hand, can use hypotheses from component 
search spaces directly without any restriction. 
3.3 Hybrid Decoding 
Hybrid decoding (Cui et al, 2010) resembles our 
approach in the motivation. This method uses the 
system combination technique in decoding directly 
to combine partial hypotheses from different SMT 
models. However, confusion network construction 
brings high computational complexity. What?s 
more, partial hypotheses generated by confusion 
network decoding cannot be assigned exact feature 
values for future use in higher level decoding, and 
they only use feature values of 1-best hypothesis 
as an approximation. HM decoding, on the other 
hand, leverages a set of enriched features, which 
are computable for all the hypotheses generated by 
either component systems or the HM decoder. 
4 Experiments 
4.1 Data and Metric 
Experiments are conducted on the NIST Chinese-
to-English MT tasks. The NIST 2004 (MT04) data 
set is used as the development set, and evaluation 
results are reported on the NIST 2005 (MT05), the 
newswire portions of the NIST 2006 (MT06) and 
2008 (MT08) data sets. All bilingual corpora 
available for the NIST 2008 constrained data track 
of Chinese-to-English MT task are used as training 
data, which contain 5.1M sentence pairs, 128M 
Chinese words and 147M English words after pre-
processing. Word alignments are performed using 
GIZA++ with the intersect-diag-grow refinement. 
The English side of bilingual corpus plus Xinhua 
portion of the LDC English Gigaword Version 3.0 
are used to train a 5-gram language model. 
Translation performance is measured in terms of 
case-insensitive BLEU scores (Papineni et al, 
2002), which compute the brevity penalty using 
the shortest reference translation for each segment. 
Statistical significance is computed using the boot-
strap re-sampling approach proposed by Koehn 
(2004). Table 1 gives some data statistics. 
 
Data Set #Sentence #Word 
MT04(dev) 1,788 48,215 
MT05 1,082 29,263 
MT06 616 17,316 
MT08 691 17,424 
Table 1: Statistics on dev and test data sets 
4.2 Component Systems 
For convenience of comparing HM decoding with 
several related decoding techniques, we include 
two state-of-the-art SMT systems as component 
systems only: 
? PB. A phrase-based system (Xiong et al, 
2006) with one lexicalized reordering model 
based on the maximum entropy principle. 
? DHPB. A string-to-dependency tree-based 
system (Shen et al, 2008), which translates 
source strings to target dependency trees. A 
target dependency language model is used as 
an additional feature. 
1263
 Phrasal rules are extracted on all bilingual data, 
hierarchical rules used in DHPB and reordering 
rules used in SCFG-HMD are extracted from a 
selected data set3. Reordering model used in PB is 
trained on the same selected data set as well. A 
trigram dependency language model used in 
DHPB is trained with the outputs from Berkeley 
parser on all language model training data. 
4.3 Contrastive Techniques 
We compare HM decoding with three multiple-
system based decoding techniques: 
? Word-Level System Combination (SC). We 
re-implement an IHMM alignment based sys-
tem combination method proposed by Li et al 
(2009b). The setting of the N-best candidates 
used is the same as the original paper. 
? Co-decoding (CD). We re-implement it based 
on Li et al (2009a), with the only difference 
that only two models are included in our re-
implementation, instead of three in theirs. For 
each test set, co-decoding outputs three results, 
two for two member systems, and one for the 
further system combination. 
? Model Combination (MC). Different from co-
decoding, MC produces single one output for 
each input sentence. We re-implement this 
method based on DeNero et al (2010) with 
two component models included. 
4.4 Comparison to Component Systems 
We compared HM decoding with two component 
SMT systems first (in Table 2). 30 features are 
used to annotate each hypothesis in HM decoding, 
including: 8 n-gram posterior features computed 
from PB/DHPB forests for      ; 8 stemmed 
n-gram posterior features computed from stemmed 
PB/DHPB forests for      ; 4 n-gram post-
erior features and 1 length posterior feature com-
puted from the mixture search space of HM de-
coder for      ; 1 LM feature; 1 word count 
feature; 1 dictionary-based feature; 2 grammar-
specified rule penalty features for either BTG-
HMD or SCFG-HMD; 4 count features for newly 
generated n-grams in HM decoding for      . 
All n-gram posteriors are computed using the effi-
cient algorithm proposed by Kumar et al (2009). 
                                                 
3 LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, 
LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 
and LDC2006E92 
 
Model 
BLEU% 
MT04 MT05 MT06 MT08 
PB 38.93 38.21 33.59 29.62 
DHPB 39.90 39.76 35.00 30.43 
BTG-HMD 41.24* 41.26* 36.76* 31.69* 
SCFG-HMD 41.31* 41.19* 36.63* 31.52* 
Table 2: HM decoding vs. single component system 
decoding (*: significantly better than each component 
system with   < 0.01) 
From table 2 we can see, both BTG-HMD and 
SCFG-HMD outperform decoding results of the 
best component system (DHPB) with significant 
improvements: +1.50, +1.76, and +1.26 BLEU 
points on MT05, MT06, and MT08 for BTG-HMD; 
+1.43, +1.63 and +1.09 BLEU points on MT05, 
MT06, and MT08 for SCFG-HMD. We also notice 
that BTG-HMD performs slight better than SCFG-
HMD on test sets. We think the potential reason is 
that more reordering rules are used in SCFG-HMD 
to handle phrase movements than BTG-HMD do; 
however, current HM decoding model lacks the 
ability to distinguish the qualities of different rules. 
We also investigate on the effects of different 
HM-decoding features. For the convenience of 
comparison, we divide them into five categories: 
? Set-1. 8 n-gram posterior features based on 2 
component search spaces plus 3 commonly 
used features (1 LM feature, 1 word count 
feature and 1 dictionary-based feature). 
? Set-2. 8 stemmed n-gram posterior features 
based on 2 stemmed component search spaces. 
? Set-3. 4 n-gram posterior features and 1 
length posterior feature based on the mixture 
search space of the HM decoder. 
? Set-4. 2 grammar-specified reordering rule 
penalty features. 
? Set-5. 4 count features for unseen n-grams 
generated by HM decoder itself. 
Except for the dictionary-based feature, all the 
features contained in Set-1 are used by the latest 
multiple-system based consensus decoding tech-
niques (DeNero et al, 2010; Duan et al, 2010). 
We use them as the starting point. Each time, we 
add one more feature set and describe the changes 
of performances by drawing two curves for each 
HM decoding algorithm on MT08 in Figure 3. 
1264
  
Figure 3: Effects of using different sets of HM decoding 
features on MT08 
With Set-1 used only, HM-decoding has already 
outperformed the best component system, which 
shows the strong contributions of these features as 
proved in related work; small gains (+0.2 BLEU 
points) are achieved by using 8 stemmed n-gram 
posterior features in Set-2, which shows consensus 
statistics based on n-grams in their stem forms are 
also helpful; n-gram and length posterior features 
based on mixture search space bring improvements 
as well; reordering rule penalty features and count 
features for unseen n-grams boost newly generated 
hypotheses specific for HM decoding, and they 
contribute to the overall improvements. 
4.5 Comparison to System Combination 
Word-level system combination is state-of-the-art 
method to improve translation performance using 
outputs generated by multiple SMT systems. In 
this paper, we compare our HM decoding with the 
combination method proposed by Li et al (2009b). 
Evaluation results are shown in Table 3. 
 
Model 
BLEU% 
MT04 MT05 MT06 MT08 
SC 41.14 40.70 36.04 31.16 
BTG-HMD 41.24 41.26+ 36.76+ 31.69+ 
SCFG-HMD 41.31+ 41.19+ 36.63+ 31.52+ 
Table 3: HM decoding vs. system combination (+: sig-
nificantly better than SC with   < 0.05) 
Compared to word-level system combination, 
both BTG-HMD and SCFG-HMD can provide 
significant improvements. We think the potential 
reason for these improvements is that, system 
combination can only use a small portion of the 
component systems? search spaces; HM decoding, 
on the other hand, can make full use of the entire 
translation spaces of all component systems. 
4.6 Comparison to Consensus Decoding 
Consensus decoding is another decoding technique 
that motivates our approach. We compare our HM 
decoding with two latest multiple-system based 
consensus decoding approaches, co-decoding and 
model combination. We list the comparison results 
in Table 4, in which CD-PB and CD-DHPB denote 
the translation results of two member systems in 
co-decoding respectively, CD-Comb denotes the 
results of further combination using outputs of 
CD-PB and CD-DHPB, MC denotes the results of 
model combination. 
 
Model 
BLEU% 
MT04 MT05 MT06 MT08 
CD-PB 40.39 40.34 35.20 30.39 
CD-DHPB 40.81 40.56 35.73 30.87 
CD-Comb 41.27 41.02 36.37 31.54 
MC 41.19 40.96 36.30 31.43 
BTG-HMD 41.24 41.26+ 36.76+ 31.69 
SCFG-HMD 41.31 41.19 36.63+ 31.52 
Table 4: HM decoding vs. consensus decoding (+: sig-
nificantly better than the best result of consensus decod-
ing methods with   < 0.05) 
Table 4 shows that after an additional system 
combination procedure, CD-Comb performs slight 
better than MC. Both BTG-HMD and SCFG-
HMD perform consistent better than CD and MC 
on all blind test sets, due to its richer generative 
capability and usage of larger search spaces. 
4.7 System Combination over BTG-HMD 
and SCFG-HMD Outputs 
As BTG-HMD and SCFG-HMD are based on two 
different decoding grammars, we could perform 
system combination over the outputs of these two 
settings (SCBTG+SCFG) for further improvements as 
well, just as Li et al (2009a) did in co-decoding. 
We present evaluation results in Table 5. 
 
Model 
BLEU% 
MT04 MT05 MT06 MT08 
BTG-HMD 41.24 41.26 36.76 31.69 
SCFG-HMD 41.31 41.19 36.63 31.52 
SCBTG+SCFG 41.74+ 41.53+ 37.11+ 32.06+ 
Table 5: System combination based on the outputs of 
BTG-HMD and SCFG-HMD (+: significantly better 
than the best HM decoding algorithm (SCFG-HMD) 
with   < 0.05) 
30.5
30.7
30.9
31.1
31.3
31.5
31.7
31.9
Set-1 Set-2 Set-3 Set-4 Set-5
BTG-HMD
SCFG-HMD
1265
 After system combination, translation results are 
significantly better than all decoding approaches 
investigated in this paper: up to 2.11 BLEU points 
over the best component system (DHPB), up to 
1.07 BLEU points over system combination, up to 
0.74 BLEU points over co-decoding, and up to 
0.81 BLEU points over model combination. 
4.8 Evaluation of Oracle Translations 
In the last part, we evaluate the quality of oracle 
translations on the n-best lists generated by HM 
decoding and all decoding approaches discussed in 
this paper. Oracle performances are obtained using 
the metric of sentence-level BLEU score proposed 
by Ye et al (2007), and each decoding approach 
outputs its 1000-best hypotheses, which are used 
to extract oracle translations. 
 
Model 
BLEU% 
MT04 MT05 MT06 MT08 
PB 49.53 48.36 43.69 39.39 
DHPB 50.66 49.59 44.68 40.47 
SC 51.77 50.84 46.87 42.11 
CD-PB 50.26 50.10 45.65 40.52 
CD-DHPB 51.91 50.61 46.23 41.01 
CD-Comb 52.10 51.00 46.95 42.20 
MC 52.03 51.22 46.60 42.23 
BTG-HMD 52.69+ 51.75+ 47.08 42.71+ 
SCFG-HMD 52.94+ 51.40 47.27+ 42.45+ 
SCBTG+SCFG 53.58+ 52.03+ 47.90+ 43.07+ 
Table 6: Oracle performances of different methods (+: 
significantly better than the best multiple-system based 
decoding method (CD-Comb) with   < 0.05) 
Results are shown in Table 6: compared to each 
single component system, decoding methods based 
on multiple SMT systems can provide significant 
improvements on oracle translations; word-level 
system combination, collaborative decoding and 
model combination show similar performances, in 
which CD-Comb performs best; BTG-HMD, 
SCFG-HMD and SCBTG+SCFG can obtain significant 
improvements than all the other approaches, and 
SCBTG+SCFG performs best on all evaluation sets. 
5 Conclusion 
In this paper, we have presented the hypothesis 
mixture decoding approach to combine multiple 
SMT models, in which hypotheses generated by 
multiple component systems are used to compose 
new translations. HM decoding method integrates 
the advantages of both system combination and 
consensus decoding techniques into a unified 
framework. Experimental results across different 
NIST Chinese-to-English MT evaluation data sets 
have validated the effectiveness of our approach. 
In the future, we will include more SMT models 
and explore more features, such as syntax-based 
features, helping to improve the performance of 
HM decoding. We also plan to investigate more 
complicated reordering models in HM decoding. 
References  
David Chiang. 2005. A Hierarchical Phrase-based 
Model for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 263-270. 
David Chiang. 2010. Learning to Translate with Source 
and Target Syntax. In Proceedings of the Association 
for Computational Linguistics, pages 1443-1452. 
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and 
Tiejun Zhao. 2010. Hybrid Decoding: Decoding with 
Partial Hypotheses Combination over Multiple SMT 
Systems. In Proceedings of the International Confe-
rence on Computational Linguistics, pages 214-222. 
John DeNero, David Chiang, and Kevin Knight. 2009. 
Fast Consensus Decoding over Translation Forests. 
In Proceedings of the Association for Computational 
Linguistics, pages 567-575. 
John DeNero, Shankar Kumar, Ciprian Chelba and 
Franz Och. 2010. Model Combination for Machine 
Translation. In Proceedings of the North American 
Association for Computational Linguistics, pages 
975-983. 
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 
2010. Mixture Model-based Minimum Bayes Risk 
Decoding using Multiple Machine Translation Sys-
tems. In Proceedings of the International Conference 
on Computational Linguistics, pages 313-321. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang, and Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 961-968. 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Outputs 
from Machine Translation Systems. In Proceedings 
of the Conference on Empirical Methods on Natural 
Language Processing, pages 98-107. 
1266
 Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceedings of 
the Conference on Empirical Methods on Natural 
Language Processing, pages 388-395. 
Shankar Kumar and William Byrne. 2004. Minimum 
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proceedings of the North American Asso-
ciation for Computational Linguistics, pages 169-
176. 
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and 
Franz Och. 2009. Efficient Minimum Error Rate 
Training and Minimum Bayes-Risk Decoding for 
Translation Hypergraphs and Lattices. In Proceed-
ings of the Association for Computational Linguis-
tics, pages 163-171. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009a. Collaborative Decoding: Partial 
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proceedings of the Associ-
ation for Computational Linguistics, pages 585-592. 
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi. 
2009b. Incremental HMM Alignment for MT system 
Combination. In Proceedings of the Association for 
Computational Linguistics, pages 949-957. 
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. 
Joint Decoding with Multiple Translation Models. In 
Proceedings of the Association for Computational 
Linguistics, pages 576-584. 
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of the 
Association for Computational Linguistics, pages 
160-167. 
Franz Och and Hermann Ney. 2004. The Alignment 
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(4): 417-449. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Weijing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, pages 
311-318. 
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A 
new String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model. 
In Proceedings of the Association for Computational 
Linguistics, pages 577-585. 
Antti-Veikko Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proceed-
ings of the Association for Computational Linguistics, 
pages 312-319. 
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk 
Decoding for Statistical Machine Translation. In 
Proceedings of the Conference on Empirical Me-
thods on Natural Language Processing, pages 620-
629. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Corpora. 
Computational Linguistics, 23(3): 377-404. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
the Association for Computational Linguistics, pages 
521-528. 
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation as a 
Ranking Problem: one step aside from BLEU. In 
Proceedings of the Second Workshop on Statistical 
Machine Translation, pages 240-247. 
1267
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 41?46,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Paraphrasing Adaptation for Web Search Ranking
Chenguang Wang?
School of EECS
Peking University
wangchenguang@pku.edu.cn
Nan Duan
Microsoft Research Asia
nanduan@microsoft.com
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Ming Zhang
School of EECS
Peking University
mzhang@net.pku.edu.cn
Abstract
Mismatch between queries and documents
is a key issue for the web search task. In
order to narrow down such mismatch, in
this paper, we present an in-depth inves-
tigation on adapting a paraphrasing tech-
nique to web search from three aspect-
s: a search-oriented paraphrasing mod-
el; an NDCG-based parameter optimiza-
tion algorithm; an enhanced ranking mod-
el leveraging augmented features comput-
ed on paraphrases of original queries. Ex-
periments performed on the large scale
query-document data set show that, the
search performance can be significantly
improved, with +3.28% and +1.14% ND-
CG gains on dev and test sets respectively.
1 Introduction
Paraphrasing is an NLP technique that generates
alternative expressions to convey the same mean-
ing of the input text in different ways. Researcher-
s have made great efforts to improve paraphrasing
from different perspectives, such as paraphrase ex-
traction (Zhao et al, 2007), paraphrase generation
(Quirk et al, 2004), model optimization (Zhao et
al., 2009) and etc. But as far as we know, none of
previous work has explored the impact of using a
well designed paraphrasing engine for web search
ranking task specifically.
In web search, mismatches between queries and
their relevant documents are usually caused by ex-
pressing the same meaning in different natural lan-
guage ways. E.g., X is the author of Y and Y was
written by X have identical meaning in most cas-
es, but they are quite different in literal sense. The
capability of paraphrasing is just right to alleviate
such issues. Motivated by this, this paper presents
? This work has been done while the author was visiting
Microsoft Research Asia.
an in-depth study on adapting paraphrasing to web
search. First, we propose a search-oriented para-
phrasing model, which includes specifically de-
signed features for web queries that can enable a
paraphrasing engine to learn preferences on dif-
ferent paraphrasing strategies. Second, we opti-
mize the parameters of the paraphrasing model ac-
cording to the Normalized Discounted Cumulative
Gain (NDCG) score, by leveraging the minimum
error rate training (MERT) algorithm (Och, 2003).
Third, we propose an enhanced ranking model by
using augmented features computed on paraphras-
es of original queries.
Many query reformulation approaches have
been proposed to tackle the query-document mis-
match issue, which can be generally summarized
as query expansion and query substitution. Query
expansion (Baeza-Yates, 1992; Jing and Croft,
1994; Lavrenko and Croft, 2001; Cui et al, 2002;
Yu et al, 2003; Zhang and Yu, 2006; Craswell and
Szummer, 2007; Elsas et al, 2008; Xu et al, 2009)
adds new terms extracted from different sources to
the original query directly; while query substitu-
tion (Brill and Moore, 2000; Jones et al, 2006;
Guo et al, 2008; Wang and Zhai, 2008; Dang
and Croft, 2010) uses probabilistic models, such
as graphical models, to predict the sequence of
rewritten query words to form a new query. Com-
paring to these works, our paraphrasing engine al-
ters queries in a similar way to statistical machine
translation, with systematic tuning and decoding
components. Zhao et al (2009) proposes an uni-
fied paraphrasing framework that can be adapted
to different applications using different usability
models. Our work can be seen as an extension a-
long this line of research, by carrying out in-depth
study on adapting paraphrasing to web search.
Experiments performed on the large scale data
set show that, by leveraging additional matching
features computed on query paraphrases, signif-
icant NDCG gains can be achieved on both dev
41
(+3.28%) and test (+1.14%) sets.
2 Paraphrasing for Web Search
In this section, we first summarize our paraphrase
extraction approaches, and then describe our para-
phrasing engine for the web search task from three
aspects, including: 1) a search-oriented paraphras-
ing model; 2) an NDCG-based parameter opti-
mization algorithm; 3) an enhanced ranking model
with augmented features that are computed based
on the extra knowledge provided by the paraphrase
candidates of the original queries.
2.1 Paraphrase Extraction
Paraphrases can be mined from various resources.
Given a bilingual corpus, we use Bannard and
Callison-Burch (2005)?s pivot-based approach to
extract paraphrases. Given a monolingual cor-
pus, Lin and Pantel (2001)?s method is used to ex-
tract paraphrases based on distributional hypoth-
esis. Additionally, human annotated data can al-
so be used as high-quality paraphrases. We use
Miller (1995)?s approach to extract paraphrases
from the synonym dictionary of WordNet. Word
alignments within each paraphrase pair are gener-
ated using GIZA++ (Och and Ney, 2000).
2.2 Search-Oriented Paraphrasing Model
Similar to statistical machine translation (SMT),
given an input query Q, our paraphrasing engine
generates paraphrase candidates1 based on a linear
model.
Q? = argmax
Q??H(Q)
P (Q?|Q)
= argmax
Q??H(Q)
M?
m=1
?mhm(Q,Q?)
H(Q) is the hypothesis space containing all para-
phrase candidates of Q, hm is the mth feature
function with weight ?m, Q? denotes one candi-
date. In order to enable our paraphrasing model
to learn the preferences on different paraphrasing
strategies according to the characteristics of web
queries, we design search-oriented features2 based
on word alignments within Q and Q?, which can
be described as follows:
1We apply CYK algorithm (Chappelier and Rajman,
1998), which is most commonly used in SMT (Chiang,
2005), to generating paraphrase candidates.
2Similar features have been demonstrated effective in
(Jones et al, 2006). But we use SMT-like model to gener-
ate query reformulations.
? Word Addition feature hWADD(Q,Q?),
which is defined as the number of words in
the paraphrase candidate Q? without being
aligned to any word in the original query Q.
? Word Deletion feature hWDEL(Q,Q?),
which is defined as the number of words in
the original query Q without being aligned
to any word in the paraphrase candidate Q?.
? Word Overlap feature hWO(Q,Q?), which is
defined as the number of word pairs that align
identical words between Q and Q?.
? Word Alteration feature hWA(Q,Q?), which
is defined as the number of word pairs that
align different words between Q and Q?.
? Word Reorder feature hWR(Q,Q?), which is
modeled by a relative distortion probability
distribution, similar to the distortion model in
(Koehn et al, 2003).
? Length Difference feature hLD(Q,Q?),
which is defined as |Q?| ? |Q|.
? Edit Distance feature hED(Q,Q?), which is
defined as the character-level edit distance
between Q and Q?.
Besides, a set of traditional SMT features
(Koehn et al, 2003) are also used in our paraphras-
ing model, including translation probability, lex-
ical weight, word count, paraphrase rule count3,
and language model feature.
2.3 NDCG-based Parameter Optimization
We utilize minimum error rate training (MERT)
(Och, 2003) to optimize feature weights of the
paraphrasing model according to NDCG. We de-
fine D as the entire document set. R is a rank-
ing model4 that can rank documents in D based
on each input query. {Qi,DLabeli }Si=1 is a human-
labeled development set. Qi is the ith query and
DLabeli ? D is a subset of documents, in which
the relevance between Qi and each document is
labeled by human annotators.
MERT is used to optimize feature weights
of our linear-formed paraphrasing model. For
3Paraphrase rule count is the number of rules that are used
to generate paraphrase candidates.
4The ranking model R (Liu et al, 2007) uses matching
features computed based on original queries and documents.
42
each query Qi in {Qi}Si=1, we first generate N-
best paraphrase candidates {Qji}Nj=1, and com-
pute NDCG score for each paraphrase based on
documents ranked by the ranker R and labeled
documents DLabeli . We then optimize the feature
weights according to the following criterion:
??M1 = argmin
?M1
{
S?
i=1
Err(DLabeli , Q?i;?M1 ,R)}
The objective of MERT is to find the optimal fea-
ture weight vector ??M1 that minimizes the error cri-
terionErr according to the NDCG scores of top-1
paraphrase candidates.
The error function Err is defined as:
Err(DLabeli , Q?i;?M1 ,R) = 1?N (DLabeli , Q?i,R)
where Q?i is the best paraphrase candidate accord-
ing to the paraphrasing model based on the weight
vector ?M1 , N (DLabeli , Q?i,R) is the NDCG score
of Q?i computed on the documents ranked byR of
Q?i and labeled document set DLabeli of Qi. The
relevance rating labeled by human annotators can
be represented by five levels: ?Perfect?, ?Excel-
lent?, ?Good?, ?Fair?, and ?Bad?. When comput-
ing NDCG scores, these five levels are commonly
mapped to the numerical scores 31, 15, 7, 3, 0 re-
spectively.
2.4 Enhanced Ranking Model
In web search, the key objective of the ranking
model is to rank the retrieved documents based on
their relevance to a given query.
Given a query Q and its retrieved document set
D = {DQ}, for each DQ ? D, we use the fol-
lowing ranking model to compute their relevance,
which is formulated as a weighted combination of
matching features:
R(Q,DQ) =
K?
k=1
?kFk(Q,DQ)
F = {F1, ..., FK} denotes a set of matching fea-
tures that measure the matching degrees between
Q and DQ, Fk(Q,DQ) ? F is the kth matching
feature, ?k is its corresponding feature weight.
How to learn the weight vector {?k}Kk=1 is a s-
tandard learning-to-rank task. The goal of learning
is to find an optimal weight vector {??k}Kk=1, such
that for any two documentsDiQ ? D andDjQ ? D,
the following condition holds:
R(Q,DiQ) > R(Q,DjQ)? rDiQ > rDjQ
where rDQ denotes a numerical relevance rating
labeled by human annotators denoting the rele-
vance between Q and DQ.
As the ultimate goal of improving paraphrasing
is to help the search task, we present a straight-
forward but effective method to enhance the rank-
ing modelR described above, by leveraging para-
phrase candidates of the original query as the extra
knowledge to compute matching features.
Formally, given a query Q and its N -best para-
phrase candidates {Q?1, ..., Q?N}, we enrich the o-
riginal feature vector F to {F,F1, ...,FN} for Q
and DQ, where all features in Fn have the same
meanings as they are in F, however, their feature
values are computed based onQ?n andDQ, instead
of Q and DQ. In this way, the paraphrase candi-
dates act as hidden variables and expanded match-
ing features between queries and documents, mak-
ing our ranking model more tunable and flexible
for web search.
3 Experiment
3.1 Data and Metric
Paraphrase pairs are extracted as we described in
Section 2.1. The bilingual corpus includes 5.1M
sentence pairs from the NIST 2008 constrained
track of Chinese-to-English machine translation
task. The monolingual corpus includes 16.7M
queries from the log of a commercial search en-
gine. Human annotated data contains 0.3M syn-
onym pairs from WordNet dictionary. Word align-
ments of each paraphrase pair are trained by
GIZA++. The language model is trained based
on a portion of queries, in which the frequency of
each query is higher than a predefined threshold,
5. The number of paraphrase pairs is 58M. The
minimum length of paraphrase rule is 1, while the
maximum length of paraphrase rule is 5.
We randomly select 2, 838 queries from the log
of a commercial search engine, each of which at-
tached with a set of documents that are annotat-
ed with relevance ratings described in Section 2.3.
We use the first 1, 419 queries together with their
annotated documents as the development set to
tune paraphrasing parameters (as we discussed in
Section 2.3), and use the rest as the test set. The
ranking model is trained based on the develop-
ment set. NDCG is used as the evaluation metric
of the web search task.
43
3.2 Baseline Systems
The baselines of the paraphrasing and the ranking
model are described as follows:
The paraphrasing baseline is denoted as BL-
Para, which only uses traditional SMT features
described at the end of Section 2.2. Weights are
optimized by MERT using BLEU (Papineni et al,
2002) as the error criterion. Development data are
generated based on the English references of NIST
2008 constrained track of Chinese-to-English ma-
chine translation task. We use the first reference
as the source, and the rest as its paraphrases.
The ranking model baseline (Liu et al, 2007) is
denoted as BL-Rank, which only uses matching
features computed based on original queries and
different meta-streams of web pages, including
URL, page title, page body, meta-keywords, meta-
description and anchor texts. The feature function-
s we use include unigram/bigram/trigram BM25
and original/normalized Perfect-Match. The rank-
ing model is learned based on SVM rank toolkit
(Joachims, 2006) with default parameter setting.
3.3 Impacts of Search-Oriented Features
We first evaluate the effectiveness of the search-
oriented features. To do so, we add these features
into the paraphrasing model baseline, and denote it
as BL-Para+SF, whose weights are optimized in
the same way with BL-Para. The ranking model
baseline BL-Rank is used to rank the documents.
We then compare the NDCG@1 scores of the best
documents retrieved using either original query, or
query paraphrases generated by BL-Para and BL-
Para+SF respectively, and list comparison results
in Table 1, where Cand@1 denotes the best para-
phrase candidate generated by each paraphrasing
model.
Test Set
BL-Para BL-Para+SF
Original Query Cand@1 Cand@1
27.28% 26.44% 26.53%
Table 1: Impacts of search-oriented features.
From Table 1, we can see, even using the best
query paraphrase, its corresponding NDCG score
is still lower than the NDCG score of the original
query. This performance dropping makes sense,
as changing user queries brings the risks of query
drift. When adding search-oriented features in-
to the baseline, the performance changes little, as
these two models are optimized based on BLEU
score only, without considering characteristics of
mismatches in search.
3.4 Impacts of Optimization Algorithm
We then evaluate the impact of our NDCG-based
optimization method. We add the optimization al-
gorithm described in Section 2.3 into BL-Para+SF,
and get a paraphrasing model BL-Para+SF+Opt.
The ranking model baseline BL-Rank is used.
Similar to the experiment in Table 1, we compare
the NDCG@1 scores of the best documents re-
trieved using query paraphrases generated by BL-
Para+SF and BL-Para+SF+Opt respectively, with
results shown in Table 2.
Test Set
BL-Para+SF BL-Para+SF+Opt
Original Query Cand@1 Cand@1
27.28% 26.53% 27.06%(+0.53%)
Table 2: Impacts of NDCG-based optimization.
Table 2 indicates that, by leveraging NDCG as
the error criterion for MERT, search-oriented fea-
tures benefit more (+0.53% NDCG) in selecting
the best query paraphrase from the whole para-
phrasing search space. The improvement is statis-
tically significant (p < 0.001) by t-test (Smucker
et al, 2007). The quality of the top-1 paraphrase
generated by BL-Para+SF+Opt is very close to the
original query.
3.5 Impacts of Enhanced Ranking Model
We last evaluate the effectiveness of the en-
hanced ranking model. The ranking model base-
line BL-Rank only uses original queries to com-
pute matching features between queries and docu-
ments; while the enhanced ranking model, denot-
ed as BL-Rank+Para, uses not only the original
query but also its top-1 paraphrase candidate gen-
erated by BL-Para+SF+Opt to compute augment-
ed matching features described in Section 2.4.
Dev Set
NDCG@1 NDCG@5
BL-Rank 25.31% 33.76%
BL-Rank+Para 28.59%(+3.28%) 34.25%(+0.49%)
Test Set
NDCG@1 NDCG@5
BL-Rank 27.28% 34.79%
BL-Rank+Para 28.42%(+1.14%) 35.68%(+0.89%)
Table 3: Impacts of enhanced ranking model.
From Table 3, we can see that NDCG@k (k =
1, 5) scores of BL-Rank+Para outperforms BL-
Rank on both dev and test sets. T-test shows that
44
the improvement is statistically significant (p <
0.001). Such end-to-end NDCG improvements
come from the extra knowledge provided by the
hidden paraphrases of original queries. This nar-
rows down the query-document mismatch issue to
a certain extent.
4 Conclusion and Future Work
In this paper, we present an in-depth study on us-
ing paraphrasing for web search, which pays close
attention to various aspects of the application in-
cluding choice of model and optimization tech-
nique. In the future, we will compare and com-
bine paraphrasing with other query reformulation
techniques, e.g., pseudo-relevance feedback (Yu et
al., 2003) and a conditional random field-based ap-
proach (Guo et al, 2008).
Acknowledgments
This work is supported by the National Natu-
ral Science Foundation of China (NSFC Grant
No. 61272343) as well as the Doctoral Program
of Higher Education of China (FSSP Grant No.
20120001110112).
References
Ricardo A Baeza-Yates. 1992. Introduction to data
structures and algorithms related to information re-
trieval.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL, pages 597?604.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of ACL, pages 286?293.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized cyk algorithm for parsing stochastic cfg.
In Workshop on Tabulation in Parsing and Deduc-
tion, pages 133?137.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270.
Nick Craswell and Martin Szummer. 2007. Random
walks on the click graph. In Proceedings of SIGIR,
SIGIR ?07, pages 239?246.
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of WWW, pages 325?
332.
Van Dang and Bruce W. Croft. 2010. Query reformu-
lation using anchor text. In Proceedings of WSDM,
pages 41?50.
Jonathan L. Elsas, Jaime Arguello, Jamie Callan, and
Jaime G. Carbonell. 2008. Retrieval and feedback
models for blog feed search. In Proceedings of SI-
GIR, pages 347?354.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Proceedings of SIGIR, SIGIR ?08, pages
379?386.
Yufeng Jing and W. Bruce Croft. 1994. An association
thesaurus for information retrieval. In In RIAO 94
Conference Proceedings, pages 146?160.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of KDD, pages 217?226.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of WWW, pages 387?396.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48?54.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance
based language models. In Proceedings of SIGIR,
pages 120?127.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Natural Lan-
guage Engineering, pages 343?360.
Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and
Hang Li. 2007. Letor: Benchmark dataset for re-
search on learning to rank for information retrieval.
In Proceedings of SIGIR workshop, pages 3?10.
George A Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM, pages 39?
41.
Franz Josef Och and Hermann Ney. 2000. Improved s-
tatistical alignment models. In Proceedings of ACL,
pages 440?447.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311?318.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Proceedings of EMNLP, pages
142?149.
Mark D Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of CIKM, pages 623?632.
45
Xuanhui Wang and ChengXiang Zhai. 2008. Mining
term association patterns from search logs for ef-
fective query reformulation. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, Proceedings of CIKM, pages
479?488.
Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query dependent pseudo-relevance feedback based
on wikipedia. In Proceedings of SIGIR, pages 59?
66.
Shipeng Yu, Deng Cai, Ji-Rong Wen, and Wei-Ying
Ma. 2003. Improving pseudo-relevance feedback in
web information retrieval using web page segmenta-
tion. In Proceedings of WWW, pages 11?18.
Wei Zhang and Clement Yu. 2006. Uic at trec 2006
blog track. In Proceedings of TREC.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
question paraphrases for qa from encarta logs. In
Proceedings of IJCAI, pages 1795?1800.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL, pages 834?842.
46
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 424?428,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Minimum Bayes Risk based Answer Re-ranking for Question Answering
Nan Duan
Natural Language Computing
Microsoft Research Asia
nanduan@microsoft.com
Abstract
This paper presents two minimum Bayes
risk (MBR) based Answer Re-ranking
(MBRAR) approaches for the question
answering (QA) task. The first approach
re-ranks single QA system?s outputs by
using a traditional MBR model, by mea-
suring correlations between answer can-
didates; while the second approach re-
ranks the combined outputs of multiple
QA systems with heterogenous answer ex-
traction components by using a mixture
model-based MBR model. Evaluation-
s are performed on factoid questions se-
lected from two different domains: Jeop-
ardy! and Web, and significant improve-
ments are achieved on all data sets.
1 Introduction
Minimum Bayes Risk (MBR) techniques have
been successfully applied to a wide range of nat-
ural language processing tasks, such as statisti-
cal machine translation (Kumar and Byrne, 2004),
automatic speech recognition (Goel and Byrne,
2000), parsing (Titov and Henderson, 2006), etc.
This work makes further exploration along this
line of research, by applying MBR technique to
question answering (QA).
The function of a typical factoid question an-
swering system is to automatically give answers to
questions in most case asking about entities, which
usually consists of three key components: ques-
tion understanding, passage retrieval, and answer
extraction. In this paper, we propose two MBR-
based Answer Re-ranking (MBRAR) approaches,
aiming to re-rank answer candidates from either
single and multiple QA systems. The first one
re-ranks answer outputs from single QA system
based on a traditional MBR model by measuring
the correlations between each answer candidates
and all the other candidates; while the second one
re-ranks the combined answer outputs from multi-
ple QA systems based on a mixture model-based
MBR model. The key contribution of this work is
that, our MBRAR approaches assume little about
QA systems and can be easily applied to QA sys-
tems with arbitrary sub-components.
The remainder of this paper is organized as fol-
lows: Section 2 gives a brief review of the QA task
and describe two types of QA systems with differ-
ent pros and cons. Section 3 presents twoMBRAR
approaches that can re-rank the answer candidates
from single and multiple QA systems respectively.
The relationship between our approach and pre-
vious work is discussed in Section 4. Section 5
evaluates our methods on large scale questions s-
elected from two domains (Jeopardy! and Web)
and shows promising results. Section 6 concludes
this paper.
2 Question Answering
2.1 Overview
Formally, given an input question Q, a typical fac-
toid QA system generates answers on the basis of
the following three procedures:
(1) Question Understanding, which determines
the answer type and identifies necessory informa-
tion contained in Q, such as question focus and
lexical answer type (LAT). Such information will
be encoded and used by the following procedures.
(2) Passage Retrieval, which formulates queries
based on Q, and retrieves passages from offline
corpus or online search engines (e.g. Google and
Bing).
(3) Answer Extraction, which first extracts an-
swer candidates from retrieved passages, and then
ranks them based on specific ranking models.
424
2.2 Two Types of QA Systems
We present two different QA sysytems, which are
distinguished from three aspects: answer typing,
answer generation, and answer ranking.
The 1st QA system is denoted as Type-
Dependent QA engine (TD-QA). In answer typing
phase, TD-QA assigns the most possible answer
type T? to a given question Q based on:
T? = argmax
T
P (T |Q)
P (T |Q) is a probabilistic answer-typing mod-
el that is similar to Pinchak and Lin (2006)?s
work. In answer generation phase, TD-QA uses
a CRF-based Named Entity Recognizer to detect
all named entities contained in retrieved passages
with the type T? , and treat them as the answer can-
didate space H(Q):
H(Q) =
?
k
Ak
In answer ranking phase, the decision rule de-
scribed below is used to rank answer candidate s-
pace H(Q):
A? = argmax
A?H(Q)
P (A|T? , Q)
= argmax
A?H(Q)
?
i
?i ? hi(A, T? , Q)
where {hi(?)} is a set of ranking features that
measure the correctness of answer candidates, and
{?i} are their corresponding feature weights.
The 2ed QA system is denoted as Type-
Independent QA engine (TI-QA). In answer typ-
ing phase, TI-QA assigns top N , instead of the
best, answer types TN (Q) for each question Q.
The probability of each type candidate is main-
tained as well. In answer generation phase, TI-
QA extracts all answer candidates from retrieved
passages based on answer types in TN (Q), by the
same NER used in TD-QA. In answer ranking
phase, TI-QA considers the probabilities of differ-
ent answer types as well:
A? = argmax
A?H(Q)
P (A|Q)
= argmax
A?H(Q)
?
T?TN (Q)
P (A|T,Q) ? P (T |Q)
On one hand, TD-QA can achieve relative high
ranking precision, as using a unique answer type
greatly reduces the size of the candidate list for
ranking. However, as the answer-typing model is
far from perfect, if prediction errors happen, TD-
QA can no longer give correct answers at all.
On the other hand, TI-QA can provide higher
answer coverage, as it can extract answer candi-
dates with multiple answer types. However, more
answer candidates with different types bring more
difficulties to the answer ranking model to rank the
correct answer to the top 1 position. So the rank-
ing precision of TI-QA is not as good as TD-QA.
3 MBR-based Answering Re-ranking
3.1 MBRAR for Single QA System
MBR decoding (Bickel and Doksum, 1977) aims
to select the hypothesis that minimizes the expect-
ed loss in classification. In MBRAR, we replace
the loss function with the gain function that mea-
sure the correlation between answer candidates.
Thus, the objective of the MBRAR approach for
single QA system is to find the answer candidate
that is most supported by other candidates under
QA system?s distribution, which can be formally
written as:
A? = argmax
A?H(Q)
?
Ak?H(Q)
G(A,Ak) ? P (Ak|H(Q))
P (Ak|H(Q)) denotes the hypothesis distribu-
tion estimated on the search space H(Q) based on
the following log-linear formulation:
P (Ak|H(Q)) =
exp(? ? P (Ak|Q))?
A??H exp(? ? P (A
? |Q))
P (Ak|Q) is the posterior probability of the answer
candidate Ak based on QA system?s ranking mod-
el, ? is a scaling factor which controls the distri-
bution P (?) sharp (when ? > 1) or smooth (when
? < 1).
G(A,Ak) is the gain function that denotes the
degree of how Ak supports A. This function can
be further expanded as a weighted combination of
a set of correlation features as: ?j ?j ?hj(A,Ak).
The following correlation features are used in
G(?):
? answer-level n-gram correlation feature:
hanswer(A,Ak) =
?
??A
#?(Ak)
where ? denotes an n-gram in A, #?(Ak)
denotes the number of times that ? occurs in
Ak.
425
? passage-level n-gram correlation feature:
hpassage(A,Ak) =
?
??PA
#?(PAk)
where PA denotes passages from which A
are extracted. This feature measures the de-
gree of Ak supports A from the context per-
spective.
? answer-type agreement feature:
htype(A,Ak) = ?(TA, TAi)
?(TA, TAk) denotes an indicator function that
equals to 1 when the answer types of A and
Ak are the same, and 0 otherwise.
? answer-length feature that is used to penalize
long answer candidates.
? averaged passage-length feature that is used
to penalize passages with a long averaged
length.
3.2 MBRAR for Multiple QA Systems
Aiming to apply MBRAR to the outputs from N
QA systems, we modify MBR components as fol-
lows.
First, the hypothesis space HC(Q) is built by
merging answer candidates of multiple QA sys-
tems:
HC(Q) =
?
i
Hi(Q)
Second, the hypothesis distribution is defined
as a probability distribution over the combined
search space of N component QA systems and
computed as a weighted sum of component model
distributions:
P (A|HC(Q)) =
N?
i=1
?i ? P (A|Hi(Q))
where ?1, ..., ?N are coefficients with following
constraints holds1: 0 ? ?i ? 1 and?Ni=1 ?i = 1,
P (A|Hi(Q)) is the posterior probability ofA esti-
mated on the ith QA system?s search spaceHi(Q).
Third, the features used in the gain function G(?)
can be grouped into two categories, including:
? system-independent features, which includes
all features described in Section 3.1 for single
system based MBRAR method;
1For simplicity, the coefficients are equally set: ?i =
1/N .
? system-dependent features, which measure
the correctness of answer candidates based
on information provided by multiple QA sys-
tems:
? system indicator feature hsys(A, QAi),
which equals to 1 when A is generated
by the ith system QAi, and 0 otherwise;
? system ranking feature hrank(A, QAi),
which equals to the reciprocal of the
rank position of A predicted by QAi. If
QAi fails to generate A, then it equals
to 0;
? ensemble feature hcons(A), which e-
quals to 1 when A can be generated by
all individual QA system, and 0 other-
wise.
Thus, the MBRAR for multiple QA systems can
be finally formulated as follows:
A? = argmax
A?HC(Q)
?
Ai?HC(Q)
G(A,Ai) ? P (Ai|HC(Q))
where the training process of the weights in the
gain function is carried out with Ranking SVM2
based on the method described in Verberne et al
(2009).
4 Related Work
MBR decoding have been successfully applied to
many NLP tasks, e.g. machine translation, pars-
ing, speech recognition and etc. As far as we
know, this is the first work that applies MBR prin-
ciple to QA.
Yaman et al (2009) proposed a classifica-
tion based method for QA task that jointly uses
multiple 5-W QA systems by selecting one opti-
mal QA system for each question. Comparing to
their work, our MBRAR approaches assume few
about the question types, and all QA systems con-
tribute in the re-ranking model. Tellez-Valero et
al. (2008) presented an answer validation method
that helps individual QA systems to automatical-
ly detect its own errors based on information from
multiple QA systems. Chu-Carroll et al (2003) p-
resented a multi-level answer resolution algorithm
to merge results from the answering agents at the
question, passage, and answer levels. Grappy et al
2We use SVMRank (Joachims, 2006) that can be found-
ed at www.cs.cornell.edu/people/tj/svm light/svm rank.html/
426
(2012) proposed to use different score combina-
tions to merge answers from different QA system-
s. Although all methods mentioned above leverage
information provided by multiple QA systems, our
work is the first time to explore the usage of MBR
principle for the QA task.
5 Experiments
5.1 Data and Metric
Questions from two different domains are used
as our evaluation data sets: the first data set in-
cludes 10,051 factoid question-answer pairs se-
lected from the Jeopardy! quiz show3; while the
second data set includes 360 celebrity-asking web
questions4 selected from a commercial search en-
gine, the answers for each question is labeled by
human annotators.
The evaluation metric Succeed@n is defined as
the number of questions whose correct answers
are successfully ranked to the top n answer can-
didates.
5.2 MBRAR for Single QA System
We first evaluate the effectiveness of our MBRAR
for single QA system. Given the N-best answer
outputs from each single QA system, together with
their ranking scores assigned by the corresponding
ranking components, we further perform MBRAR
to re-rank them and show resulting numbers on t-
wo evaluation data sets in Table 1 and 2 respec-
tively.
Both Table 1 and Table 2 show that, by lever-
aging our MBRAR method on individual QA sys-
tems, the rankings of correct answers are consis-
tently improved on both Jeopardy! and web ques-
tions.
Joepardy! Succeed@1 Succeed@2 Succeed@3
TD-QA 2,289 2,693 2,885
MBRAR 2,372 2,784 2,982
TI-QA 2,527 3,397 3,821
MBRAR 2,628 3,500 3,931
Table 1: Impacts of MBRAR for single QA system
on Jeopardy! questions.
We also notice TI-QA performs significantly
better than TD-QA on Jeopardy! questions, but
worse on web questions. This is due to fac-
t that when the answer type is fixed (PERSON for
3http://www.jeopardy.com/
4The answers of such questions are person names.
Web Succeed@1 Succeed@2 Succeed@3
TD-QA 97 128 146
MBRAR 99 130 148
TI-QA 95 122 136
MBRAR 97 126 143
Table 2: Impacts of MBRAR for single QA system
on web questions.
celebrity-asking questions), TI-QA will generate
candidates with wrong answer types, which will
definitely deteriorate the ranking accuracy.
5.3 MBRAR for Multiple QA Systems
We then evaluate the effectiveness of our MBRAR
for multiple QA systems. The mixture model-
based MBRAR method described in Section 3.2
is used to rank the combined answer outputs from
TD-QA and TI-QA, with ranking results shown in
Table 3 and 4.
From Table 3 and Table 4 we can see that, com-
paring to the ranking performances of single QA
systems TD-QA and TI-QA, MBRAR using two
QA systems? outputs shows significant improve-
ments on both Jeopardy! and web questions. Fur-
thermore, comparing to MBRAR on single QA
system, MBRAR onmultiple QA systems can pro-
vide extra gains on both questions sets as well.
Jeopardy! Succeed@1 Succeed@2 Succeed@3
TD-QA 2,289 2,693 2,885
TI-QA 2,527 3,397 3,821
MBRAR 2,891 3,668 4,033
Table 3: Impacts of MBRAR for multiple QA sys-
tems on Jeopardy! questions.
Web Succeed@1 Succeed@2 Succeed@3
TD-QA 97 128 146
TI-QA 95 122 136
MBRAR 108 137 152
Table 4: Impacts of MBRAR for multiple QA sys-
tems on web questions.
6 Conclusions and Future Work
In this paper, we present two MBR-based answer
re-ranking approaches for QA. Comparing to pre-
vious methods, MBRAR provides a systematic
way to re-rank answers from either single or multi-
ple QA systems, without considering their hetero-
geneous implementations of internal components.
427
Experiments on questions from two different do-
mains show that, our proposed method can sig-
nificantly improve the ranking performances. In
future, we will add more QA systems into our M-
BRAR framework, and design more features for
the MBR gain function.
References
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected Topics. Holden-
Day Inc.
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager,
and Abraham Ittycheriah. 2003. In Question An-
swering, Two Heads Are Better Than One. In pro-
ceeding of HLT-NAACL.
Vaibhava Goel and William Byrne. 2000. Minimum
bayes-risk automatic speech recognition, Computer
Speech and Language.
Arnaud Grappy, Brigitte Grau, and Sophie Ros-
set. 2012. Methods Combination and ML-based
Re-ranking of Multiple Hypothesis for Question-
Answering Systems, In proceeding of EACL.
Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time, In proceeding of KDD.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statisti-cal Machine
Translation. In proceeding of HLT-NAACL.
Christopher Pinchak and Dekang Lin. 2006. A Prob-
abilistic Answer Type Model. In proceeding of EA-
CL.
Ivan Titov and James Henderson. 2006. Bayes Risk
Minimization in Natural Language Parsing. Techni-
cal report.
Alberto Tellez-Valero, Manuel Montes-y-Gomez, Luis
Villasenor-Pineda, and Anselmo Penas. 2008. Im-
proving Question Answering by Combining Multiple
Systems via Answer Validation. In proceeding of CI-
CLing.
Suzan Verberne, Clst Ru Nijmegen, Hans Van Hal-
teren, Clst Ru Nijmegen, Daphne Theijssen, Ru Ni-
jmegen, Stephan Raaijmakers, Lou Boves, and Clst
Ru Nijmegen. 2009. Learning to rank qa data. e-
valuating machine learning techniques for ranking
answers to why-questions. In proceeding of SIGIR
workshop.
Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, Ralph
Grishman, Mary Harper, Kathleen R. McKe-
own, Adam Meyers, Kartavya Sharma. 2009.
Classification-Based Strategies for Combining Mul-
tiple 5-W Question Answering Systems. In proceed-
ing of INTERSPEECH.
428
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967?976,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Knowledge-Based Question Answering as Machine Translation
Junwei Bao
? ?
, Nan Duan
?
, Ming Zhou
?
, Tiejun Zhao
?
?
Harbin Institute of Technology
?
Microsoft Research
baojunwei001@gmail.com
{nanduan, mingzhou}@microsoft.com
tjzhao@hit.edu.cn
Abstract
A typical knowledge-based question an-
swering (KB-QA) system faces two chal-
lenges: one is to transform natural lan-
guage questions into their meaning repre-
sentations (MRs); the other is to retrieve
answers from knowledge bases (KBs) us-
ing generated MRs. Unlike previous meth-
ods which treat them in a cascaded man-
ner, we present a translation-based ap-
proach to solve these two tasks in one u-
nified framework. We translate questions
to answers based on CYK parsing. An-
swers as translations of the span covered
by each CYK cell are obtained by a ques-
tion translation method, which first gener-
ates formal triple queries as MRs for the
span based on question patterns and re-
lation expressions, and then retrieves an-
swers from a given KB based on triple
queries generated. A linear model is de-
fined over derivations, and minimum er-
ror rate training is used to tune feature
weights based on a set of question-answer
pairs. Compared to a KB-QA system us-
ing a state-of-the-art semantic parser, our
method achieves better results.
1 Introduction
Knowledge-based question answering (KB-QA)
computes answers to natural language (NL) ques-
tions based on existing knowledge bases (KBs).
Most previous systems tackle this task in a cas-
caded manner: First, the input question is trans-
formed into its meaning representation (MR) by
an independent semantic parser (Zettlemoyer and
Collins, 2005; Mooney, 2007; Artzi and Zettle-
moyer, 2011; Liang et al, 2011; Cai and Yates,
?
This work was finished while the author was visiting Mi-
crosoft Research Asia.
2013; Poon, 2013; Artzi et al, 2013; Kwiatkowski
et al, 2013; Berant et al, 2013); Then, the answer-
s are retrieved from existing KBs using generated
MRs as queries.
Unlike existing KB-QA systems which treat se-
mantic parsing and answer retrieval as two cas-
caded tasks, this paper presents a unified frame-
work that can integrate semantic parsing into the
question answering procedure directly. Borrow-
ing ideas from machine translation (MT), we treat
the QA task as a translation procedure. Like MT,
CYK parsing is used to parse each input question,
and answers of the span covered by each CYK cel-
l are considered the translations of that cell; un-
like MT, which uses offline-generated translation
tables to translate source phrases into target trans-
lations, a semantic parsing-based question trans-
lation method is used to translate each span into
its answers on-the-fly, based on question patterns
and relation expressions. The final answers can be
obtained from the root cell. Derivations generated
during such a translation procedure are modeled
by a linear model, and minimum error rate train-
ing (MERT) (Och, 2003) is used to tune feature
weights based on a set of question-answer pairs.
Figure 1 shows an example: the question direc-
tor of movie starred by Tom Hanks is translated to
one of its answers Robert Zemeckis by three main
steps: (i) translate director of to director of ; (ii)
translate movie starred by Tom Hanks to one of it-
s answers Forrest Gump; (iii) translate director of
Forrest Gump to a final answer Robert Zemeckis.
Note that the updated question covered by Cell[0,
6] is obtained by combining the answers to ques-
tion spans covered by Cell[0, 1] and Cell[2, 6].
The contributions of this work are two-fold: (1)
We propose a translation-based KB-QA method
that integrates semantic parsing and QA in one
unified framework. The benefit of our method
is that we don?t need to explicitly generate com-
plete semantic structures for input questions. Be-
967
Cell[0, 6] 
Cell[2, 6] 
Cell[0, 1] 
director of movie starred by Tom Hanks 
(ii) movie starred by Tom Hanks ? Forrest Gump 
(iii) director of Forrest Gump ? Robert Zemeckis 
(i) director of ? director of 
Figure 1: Translation-based KB-QA example
sides which, answers generated during the transla-
tion procedure help significantly with search space
pruning. (2) We propose a robust method to trans-
form single-relation questions into formal triple
queries as their MRs, which trades off between
transformation accuracy and recall using question
patterns and relation expressions respectively.
2 Translation-Based KB-QA
2.1 Overview
Formally, given a knowledge base KB and an N-
L question Q, our KB-QA method generates a set
of formal triples-answer pairs {?D,A?} as deriva-
tions, which are scored and ranked by the distribu-
tion P (?D,A?|KB,Q) defined as follows:
exp{
?
M
i=1
?
i
? h
i
(?D,A?,KB,Q)}
?
?D
?
,A
?
??H(Q)
exp{
?
M
i=1
?
i
? h
i
(?D
?
,A
?
?,KB,Q)}
? KB denotes a knowledge base
1
that stores a
set of assertions. Each assertion t ? KB is in
the form of {e
ID
sbj
, p, e
ID
obj
}, where p denotes
a predicate, e
ID
sbj
and e
ID
obj
denote the subject
and object entities of t, with unique IDs
2
.
? H(Q) denotes the search space {?D,A?}. D
is composed of a set of ordered formal triples
{t
1
, ..., t
n
}. Each triple t = {e
sbj
, p, e
obj
}
j
i
?
D denotes an assertion in KB, where i and
j denotes the beginning and end indexes of
the question span from which t is trans-
formed. The order of triples in D denotes
the order of translation steps from Q to A.
E.g., ?director of, Null, director of ?
1
0
, ?Tom
1
We use a large scale knowledge base in this paper, which
contains 2.3B entities, 5.5K predicates, and 18B assertions. A
16-machine cluster is used to host and serve the whole data.
2
Each KB entity has a unique ID. For the sake of conve-
nience, we omit the ID information in the rest of the paper.
Hanks, Film.Actor.Film, Forrest Gump?
6
2
and
?Forrest Gump, Film.Film.Director, Robert
Zemeckis?
6
0
are three ordered formal triples
corresponding to the three translation steps in
Figure 1. We define the task of transforming
question spans into formal triples as question
translation. A denotes one final answer ofQ.
? h
i
(?) denotes the i
th
feature function.
? ?
i
denotes the feature weight of h
i
(?).
According to the above description, our KB-
QA method can be decomposed into four tasks as:
(1) search space generation for H(Q); (2) ques-
tion translation for transforming question spans in-
to their corresponding formal triples; (3) feature
design for h
i
(?); and (4) feature weight tuning for
{?
i
}. We present details of these four tasks in the
following subsections one-by-one.
2.2 Search Space Generation
We first present our translation-based KB-QA
method in Algorithm 1, which is used to generate
H(Q) for each input NL question Q.
Algorithm 1: Translation-based KB-QA
1 for l = 1 to |Q| do
2 for all i, j s.t. j ? i = l do
3 H(Q
j
i
) = ?;
4 T = QTrans(Q
j
i
,KB);
5 foreach formal triple t ? T do
6 create a new derivation d;
7 d.A = t.e
obj
;
8 d.D = {t};
9 update the model score of d;
10 insert d toH(Q
j
i
);
11 end
12 end
13 end
14 for l = 1 to |Q| do
15 for all i, j s.t. j ? i = l do
16 for all m s.t. i ? m < j do
17 for d
l
? H(Q
m
i
) and d
r
? H(Q
j
m+1
) do
18 Q
update
= d
l
.A+ d
r
.A;
19 T = QTrans(Q
update
,KB);
20 foreach formal triple t ? T do
21 create a new derivation d;
22 d.A = t.e
obj
;
23 d.D = d
l
.D
?
d
r
.D
?
{t};
24 update the model score of d;
25 insert d toH(Q
j
i
);
26 end
27 end
28 end
29 end
30 end
31 returnH(Q).
968
The first half (from Line 1 to Line 13) gen-
erates a formal triple set T for each unary span
Q
j
i
? Q, using the question translation method
QTrans(Q
j
i
,KB) (Line 4), which takesQ
j
i
as the
input. Each triple t ? T returned is in the form of
{e
sbj
, p, e
obj
}, where e
sbj
?s mention occurs inQ
j
i
,
p is a predicate that denotes the meaning expressed
by the context of e
sbj
in Q
j
i
, e
obj
is an answer of
Q
j
i
based on e
sbj
, p and KB. We describe the im-
plementation detail of QTrans(?) in Section 2.3.
The second half (from Line 14 to Line 31) first
updates the content of each bigger spanQ
j
i
by con-
catenating the answers to its any two consecutive
smaller spans covered by Q
j
i
(Line 18). Then,
QTrans(Q
j
i
,KB) is called to generate triples for
the updated span (Line 19). The above operations
are equivalent to answering a simplified question,
which is obtained by replacing the answerable
spans in the original question with their corre-
sponding answers. The search spaceH(Q) for the
entire question Q is returned at last (Line 31).
2.3 Question Translation
The purpose of question translation is to translate
a span Q to a set of formal triples T . Each triple
t ? T is in the form of {e
sbj
, p, e
obj
}, where e
sbj
?s
mention
3
occurs inQ, p is a predicate that denotes
the meaning expressed by the context of e
sbj
in
Q, e
obj
is an answer to Q retrieved from KB us-
ing a triple query q = {e
sbj
, p, ?}. Note that if
no predicate p or answer e
obj
can be generated,
{Q, Null,Q} will be returned as a special triple,
which sets e
obj
to be Q itself, and p to be Null.
This makes sure the un-answerable spans can be
passed on to the higher-level operations.
Question translation assumes each span Q is a
single-relation question (Fader et al, 2013). Such
assumption simplifies the efforts of semantic pars-
ing to the minimum question units, while leaving
the capability of handling multiple-relation ques-
tions (Figure 1 gives one such example) to the out-
er CYK-parsing based translation procedure. Two
question translation methods are presented in the
rest of this subsection, which are based on ques-
tion patterns and relation expressions respectively.
2.3.1 Question Pattern-based Translation
A question pattern QP includes a pattern string
QP
pattern
, which is composed of words and a slot
3
For simplicity, a cleaned entity dictionary dumped from
the entire KB is used to detect entity mentions inQ.
Algorithm 2:QP-based Question Translation
1 T = ?;
2 foreach entity mention e
Q
? Q do
3 Q
pattern
= replace e
Q
inQ with [Slot];
4 foreach question patternQP do
5 ifQ
pattern
==QP
pattern
then
6 E = Disambiguate(e
Q
,QP
predicate
);
7 foreach e ? E do
8 create a new triple query q;
9 q = {e,QP
predicate
, ?};
10 {A
i
} = AnswerRetrieve(q,KB);
11 foreach A ? {A
i
} do
12 create a new formal triple t;
13 t = {q.e
sbj
, q.p,A};
14 t.score = 1.0;
15 insert t to T ;
16 end
17 end
18 end
19 end
20 end
21 return T .
symbol [Slot], and a KB predicate QP
predicate
,
which denotes the meaning expressed by the con-
text words in QP
pattern
.
Algorithm 2 shows how to generate formal
triples for a span Q based on question pattern-
s (QP-based question translation). For each en-
tity mention e
Q
? Q, we replace it with [Slot]
and obtain a pattern string Q
pattern
(Line 3). If
Q
pattern
can match one QP
pattern
, then we con-
struct a triple query q (Line 9) using QP
predicate
as its predicate and one of the KB entities re-
turned by Disambiguate(e
Q
,QP
predicate
) as it-
s subject entity (Line 6). Here, the objective of
Disambiguate(e
Q
,QP
predicate
) is to output a set
of disambiguated KB entities E in KB. The name
of each entity returned equals the input entity
mention e
Q
and occurs in some assertions where
QP
predicate
are the predicates. The underlying
idea is to use the context (predicate) information to
help entity disambiguation. The answers of q are
returned by AnswerRetrieve(q,KB) based on q
and KB (Line 10), each of which is used to con-
struct a formal triple and added to T for Q (from
Line 11 to Line 16). Figure 2 gives an example.
Question patterns are collected as follows: First,
5W queries, which begin with What, Where, Who,
When, or Which, are selected from a large scale
query log of a commercial search engine; Then, a
cleaned entity dictionary is used to annotate each
query by replacing all entity mentions it contains
with the symbol [Slot]. Only high-frequent query
patterns which contain one [Slot] are maintained;
969
?                    : who is the director of Forrest Gump 
?????????    : who is the director of [Slot] 
???????????: Film.Film.Director 
?                    : <Forrest Gump, Film.Film.Director, ?> 
?                     : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 
KB 
Figure 2: QP-based question translation example
Lastly, annotators try to manually label the most-
frequent 50,000 query patterns with their corre-
sponding predicates, and 4,764 question patterns
with single labeled predicates are obtained.
From experiments (Table 3 in Section 4.3) we
can see that, question pattern based question trans-
lation can achieve high end-to-end accuracy. But
as human efforts are needed in the mining proce-
dure, this method cannot be extended to large scale
very easily. Besides, different users often type the
questions with the same meaning in different NL
expressions. For example, although the question
Forrest Gump was directed by which moviemaker
means the same as the question Q in Figure 2, no
question pattern can cover it. We need to find an
alternative way to alleviate such coverage issue.
2.3.2 Relation Expression-based Translation
Aiming to alleviate the coverage issue occurring in
QP-based method, an alternative relation expres-
sion (RE) -based method is proposed, and will be
used when the QP-based method fails.
We define RE
p
as a relation expression set for
a given KB predicate p ? KB. Each relation ex-
pressionRE ? RE
p
includes an expression string
RE
expression
, which must contain at least one con-
tent word, and a weight RE
weight
, which denotes
the confidence thatRE
expression
can represent p?s
meaning in NL. For example, is the director of
is one relation expression string for the predicate
Film.Film.Director, which means it is usually used
to express this relation (predicate) in NL.
Algorithm 3 shows how to generate triples for
a question Q based on relation expressions. For
each possible entity mention e
Q
? Q and a K-
B predicate p ? KB that is related to a KB enti-
ty e whose name equals e
Q
, Sim(e
Q
,Q,RE
p
) is
computed (Line 5) based on the similarity between
question context and RE
p
, which measures how
likely Q can be transformed into a triple query
Algorithm 3:RE-based Question Translation
1 T = ?;
2 foreach entity mention e
Q
? Q do
3 foreach e ? KB s.t. e.name==e
Q
do
4 foreach predicate p ? KB related to e do
5 score = Sim(e
Q
,Q,RE
p
);
6 if score > 0 then
7 create a new triple query q;
8 q = {e, p, ?};
9 {A
i
} = AnswerRetrieve(q,KB);
10 foreach A ? {A
i
} do
11 create a new formal triple t;
12 t = {q.e
sbj
, q.p,A};
13 t.score = score;
14 insert t to T ;
15 end
16 end
17 end
18 end
19 end
20 sort T based on the score of each t ? T ;
21 return T .
q = {e, p, ?}. If this score is larger than 0, which
means there are overlaps betweenQ?s context and
RE
p
, then q will be used as the triple query of Q,
and a set of formal triples will be generated based
on q andKB (from Line 7 to Line 15). The compu-
tation of Sim(e
Q
,Q,RE
p
) is defined as follows:
?
n
1
|Q| ? n+ 1
? {
?
?
n
?Q,?
n
?
e
Q
=?
P (?
n
|RE
p
)}
where n is the n-gram order which ranges from 1
to 5, ?
n
is an n-gram occurring inQ without over-
lapping with e
Q
and containing at least one con-
tent word, P (?
n
|RE
p
) is the posterior probability
which is computed by:
P (?
n
|RE
p
) =
Count(?
n
,RE
p
)
?
?
?
n
?RE
p
Count(?
?
n
,RE
p
)
Count(?,RE
p
) denotes the weighted sum of
times that ? occurs inRE
p
:
Count(?,RE
p
) =
?
RE?RE
p
{#
?
(RE) ? RE
weight
}
where #
?
(RE) denotes the number of times that
? occurs inRE
expression
, andRE
weight
is decided
by the relation expression extraction component.
Figure 3 gives an example, where n-grams with
rectangles are the ones that occur in bothQ?s con-
text and the relation expression set of a given pred-
icate p = Film.F ilm.Director. Unlike the QP-
based method which needs a perfect match, the
970
?                                 : Forrest Gump was directed by which moviemaker 
????????????????????: is directed by 
was directed and written by 
is the moviemaker of 
was famous as the director of 
? 
?                                  : <Forrest Gump, Film.Film.Director, ?> 
?                                   : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 
KB 
Figure 3: RE-based question translation example
RE-based method allows fuzzy matching between
Q andRE
p
, and records this (Line 13) in generat-
ed triples, which is used as features later.
Relation expressions are mined as follows: Giv-
en a set of KB assertions with an identical predi-
cate p, we first extract all sentences from English
Wiki pages
4
, each of which contains at least one
pair of entities occurring in one assertion. Then,
we extract the shortest path between paired entities
in the dependency tree of each sentence as an RE
candidate for the given predicate. The intuition is
that any sentence containing such entity pairs oc-
cur in an assertion is likely to express the predi-
cate of that assertion in some way. Last, all rela-
tion expressions extracted are filtered by heuristic
rules, i.e., the frequency must be larger than 4, the
length must be shorter than 10, and then weighted
by the pattern scoring methods proposed in (Ger-
ber and Ngomo, 2011; Gerber and Ngomo, 2012).
For each predicate, we only keep the relation ex-
pressions whose pattern scores are larger than a
pre-defined threshold. Figure 4 gives one relation
expression extraction example. The statistics and
overall quality of the relation expressions are list-
ed in Section 4.1.
{ Forrest Gump , Robert Zemeckis }  
{ Titanic, James Cameron }  
{ The Dark Knight Rises , C hristopher  Nolan }  
Paired entity of a 
KB predicate  
??Film.Film.Director 
Passage retrieval  
from Wiki pages  
Relation expression 
weighting  
Robert Zemeckis  is the director of Forrest Gump  
James Cameron  is the moviemaker of Titanic 
The Dark Knight Rises is directed by C hristopher  Nolan  
is the director of           ||| 0.25  
is the moviemaker of   ||| 0.23  
is directed by                 ||| 0.20  
Figure 4: RE extraction example
4
http://en.wikipedia.org/wiki/Wikipedia:Database download
2.3.3 Question Decomposition
Sometimes, a question may provide multiple con-
straints to its answers. movie starred by Tom Han-
ks in 1994 is one such question. All the films as
the answers of this question should satisfy the fol-
lowing two constraints: (1) starred by Tom Hanks;
and (2) released in 1994. It is easy to see that such
questions cannot be translated to single triples.
We propose a dependency tree-based method to
handle such multiple-constraint questions by (i)
decomposing the original question into a set of
sub-questions using syntax-based patterns; and (ii)
intersecting the answers of all sub-questions as the
final answers of the original question. Note, ques-
tion decomposition only operates on the original
question and question spans covered by complete
dependency subtrees. Four syntax-based patterns
(Figure 5) are used for question decomposition. If
a question matches any one of these patterns, then
sub-questions are generated by collecting the path-
s between n
0
and each n
i
(i > 0) in the pattern,
where each n denotes a complete subtree with a
noun, number, or question word as its root node,
the symbol ? above prep
?
denotes this preposition
can be skipped in matching. For the question men-
tioned at the beginning, its two sub-questions gen-
erated are movie starred by Tom Hanks and movie
starred in 1994, as its dependency form matches
pattern (a). Similar ideas are used in IBM Wat-
son (Kalyanpur et al, 2012) as well.
???? 
?? 
????? 
?? 
????? 
?? 
? 
? 
???? 
?? ?? ???? 
(a) 
?? 
???? 
?? 
?? 
(c) 
and  ?? 
????? 
???? 
?? 
?? 
(d) 
????? and  ???? 
?? 
????? 
(b) 
Figure 5: Four syntax-based patterns for question
decomposition
As dependency parsing is not perfect, we gen-
erate single triples for such questions without con-
sidering constraints as well, and add them to the
search space for competition. h
syntax constraint
(?)
971
is used to boost triples that are converted from sub-
questions generated by question decomposition.
The more constraints an answer satisfies, the bet-
ter. Obviously, current patterns used can?t cover
all cases but most-common ones. We leave a more
general pattern mining method for future work.
2.4 Feature Design
The objective of our KB-QA system is to seek the
derivation ?
?
D,
?
A? that maximizes the probability
P (?D,A?|KB,Q) described in Section 2.1 as:
?
?
D,
?
A? = argmax
?D,A??H(Q)
P (?D,A?|KB,Q)
= argmax
?D,A??H(Q)
M
?
i=1
?
i
? h
i
(?D,A?,KB,Q)
We now introduce the feature sets {h
i
(?)} that are
used in the above linear model:
? h
question word
(?), which counts the number of
original question words occurring inA. It pe-
nalizes those partially answered questions.
? h
span
(?), which counts the number of spans
in Q that are converted to formal triples. It
controls the granularity of the spans used in
question translation.
? h
syntax subtree
(?), which counts the number
of spans inQ that are (1) converted to formal
triples, whose predicates are not Null, and
(2) covered by complete dependency subtrees
at the same time. The underlying intuition
is that, dependency subtrees of Q should be
treated as units for question translation.
? h
syntax constraint
(?), which counts the num-
ber of triples in D that are converted from
sub-questions generated by the question de-
composition component.
? h
triple
(?), which counts the number of triples
in D, whose predicates are not Null.
? h
triple
weight
(?), which sums the scores of all
triples {t
i
} in D as
?
t
i
?D
t
i
.score.
? h
QP
count
(?), which counts the number of
triples in D that are generated by QP-based
question translation method.
? h
RE
count
(?), which counts the number of
triples in D that are generated by RE-based
question translation method.
? h
staticrank
sbj
(?), which sums the static rank
scores of all subject entities in D?s triple set
as
?
t
i
?D
t
i
.e
sbj
.static rank.
? h
staticrank
obj
(?), which sums the static rank
scores of all object entities inD?s triple set as
?
t
i
?D
t
i
.e
obj
.static rank.
? h
confidence
obj
(?), which sums the confidence
scores of all object entities inD?s triple set as
?
t?D
t.e
obj
.confidence.
For each assertion {e
sbj
, p, e
obj
} stored in KB,
e
sbj
.static rank and e
obj
.static rank denote the
static rank scores
5
for e
sbj
and e
obj
respectively;
e
obj
.confidence rank represents the probability
p(e
obj
|e
sbj
, p). These three scores are used as fea-
tures to rank answers generated in QA procedure.
2.5 Feature Weight Tuning
Given a set of question-answer pairs {Q
i
,A
ref
i
}
as the development (dev) set, we use the minimum
error rate training (MERT) (Och, 2003) algorithm
to tune the feature weights ?
M
i
in our proposed
model. The training criterion is to seek the feature
weights that can minimize the accumulated errors
of the top-1 answer of questions in the dev set:
?
?
M
1
= argmin
?
M
1
N
?
i=1
Err(A
ref
i
,
?
A
i
;?
M
1
)
N is the number of questions in the dev set, A
ref
i
is the correct answers as references of the i
th
ques-
tion in the dev set,
?
A
i
is the top-1 answer candi-
date of the i
th
question in the dev set based on
feature weights ?
M
1
, Err(?) is the error function
which is defined as:
Err(A
ref
i
,
?
A
i
;?
M
1
) = 1? ?(A
ref
i
,
?
A
i
)
where ?(A
ref
i
,
?
A
i
) is an indicator function which
equals 1 when
?
A
i
is included in the reference set
A
ref
i
, and 0 otherwise.
3 Comparison with Previous Work
Our work intersects with two research directions:
semantic parsing and question answering.
Some previous works on semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Wong and Mooney,
5
The static rank score of an entity represents a general
indicator of the overall quality of that entity.
972
2007; Kwiatkowski et al, 2010; Kwiatkowski
et al, 2011) require manually annotated logical
forms as supervision, and are hard to extend result-
ing parsers from limited domains, such as GEO,
JOBS and ATIS, to open domains. Recent work-
s (Clarke and Lapata, 2010; Liang et al, 2013)
have alleviated such issues using question-answer
pairs as weak supervision, but still with the short-
coming of using limited lexical triggers to link NL
phrases to predicates. Poon (2013) has proposed
an unsupervised method by adopting grounded-
learning to leverage the database for indirect su-
pervision. But transformation from NL questions
to MRs heavily depends on dependency parsing
results. Besides, the KB used (ATIS) is limited as
well. Kwiatkowski et al (2013) use Wiktionary
and a limited manual lexicon to map POS tags to
a set of predefined CCG lexical categories, which
aims to reduce the need for learning lexicon from
training data. But it still needs human efforts to de-
fine lexical categories, which usually can not cover
all the semantic phenomena.
Berant et al (2013) have not only enlarged the
KB used for Freebase (Google, 2013), but also
used a bigger lexicon trigger set extracted by the
open IE method (Lin et al, 2012) for NL phrases
to predicates linking. In comparison, our method
has further advantages: (1) Question answering
and semantic parsing are performed in an join-
t way under a unified framework; (2) A robust
method is proposed to map NL questions to their
formal triple queries, which trades off the mapping
quality by using question patterns and relation ex-
pressions in a cascaded way; and (3) We use do-
main independent feature set which allowing us to
use a relatively small number of question-answer
pairs to tune model parameters.
Fader et al (2013) map questions to formal
(triple) queries over a large scale, open-domain
database of facts extracted from a raw corpus by
ReVerb (Fader et al, 2011). Compared to their
work, our method gains an improvement in two
aspects: (1) Instead of using facts extracted us-
ing the open IE method, we leverage a large scale,
high-quality knowledge base; (2) We can han-
dle multiple-relation questions, instead of single-
relation queries only, based on our translation
based KB-QA framework.
Espana-Bonet and Comas (2012) have proposed
an MT-based method for factoid QA. But MT in
there work means to translate questions into n-
best translations, which are used for finding simi-
lar sentences in the document collection that prob-
ably contain answers. Echihabi and Marcu (2003)
have developed a noisy-channel model for QA,
which explains how a sentence containing an an-
swer to a given question can be rewritten into that
question through a sequence of stochastic opera-
tions. Compared to the above two MT-motivated
QA work, our method uses MT methodology to
translate questions to answers directly.
4 Experiment
4.1 Data Sets
Following Berant et al (2013), we use the same
subset of WEBQUESTIONS (3,778 questions) as
the development set (Dev) for weight tuning in
MERT, and use the other part of WEBQUES-
TIONS (2,032 questions) as the test set (Test). Ta-
ble 1 shows the statistics of this data set.
Data Set # Questions # Words
WEBQUESTIONS 5,810 6.7
Table 1: Statistics of evaluation set. # Questions is
the number of questions in a data set, # Words is
the averaged word count of a question.
Table 2 shows the statistics of question patterns
and relation expressions used in our KB-QA sys-
tem. As all question patterns are collected with hu-
man involvement as we discussed in Section 2.3.1,
the quality is very high (98%). We also sample
1,000 instances from the whole relation expression
set and manually label their quality. The accuracy
is around 89%. These two resources can cover 566
head predicates in our KB.
# Entries Accuracy
Question Patterns 4,764 98%
Relation Expressions 133,445 89%
Table 2: Statistics of question patterns and relation
expressions.
4.2 KB-QA Systems
Since Berant et al (2013) is one of the latest
work which has reported QA results based on a
large scale, general domain knowledge base (Free-
base), we consider their evaluation result on WE-
BQUESTIONS as our baseline.
Our KB-QA system generates the k-best deriva-
tions for each question span, where k is set to 20.
973
The answers with the highest model scores are
considered the best answers for evaluation. For
evaluation, we follow Berant et al (2013) to al-
low partial credit and score an answer using the F1
measure, comparing the predicted set of entities to
the annotated set of entities.
One difference between these two systems is the
KB used. Since Freebase is completely contained
by our KB, we disallow all entities which are not
included by Freebase. By doing so, our KB pro-
vides the same knowledge as Freebase does, which
means we do not gain any extra advantage by us-
ing a larger KB. But we still allow ourselves to
use the static rank scores and confidence scores of
entities as features, as we described in Section 2.4.
4.3 Evaluation Results
We first show the overall evaluation results of our
KB-QA system and compare them with baseline?s
results on Dev and Test. Note that we do not re-
implement the baseline system, but just list their
evaluation numbers reported in the paper. Com-
parison results are listed in Table 3.
Dev (Accuracy) Test (Accuracy)
Baseline 32.9% 31.4%
Our Method 42.5% (+9.6%) 37.5% (+6.1%)
Table 3: Accuracy on evaluation sets. Accuracy is
defined as the number of correctly answered ques-
tions divided by the total number of questions.
Table 3 shows our KB-QA method outperforms
baseline on both Dev and Test. We think the po-
tential reasons of this improvement include:
? Different methods are used to map NL phras-
es to KB predicates. Berant et al (2013)
have used a lexicon extracted from a subset
of ReVerb triples (Lin et al, 2012), which
is similar to the relation expression set used
in question translation. But as our relation
expressions are extracted by an in-house ex-
tractor, we can record their extraction-related
statistics as extra information, and use them
as features to measure the mapping quality.
Besides, as a portion of entities in our KB
are extracted from Wiki, we know the one-
to-one correspondence between such entities
and Wiki pages, and use this information in
relation expression extraction for entity dis-
ambiguation. A lower disambiguation error
rate results in better relation expressions.
? Question patterns are used to map NL context
to KB predicates. Context can be either con-
tinuous or discontinues phrases. Although
the size of this set is limited, they can actually
cover head questions/queries
6
very well. The
underlying intuition of using patterns is that
those high-frequent questions/queries should
and can be treated and solved in the QA task,
by involving human effort at a relative small
price but with very impressive accuracy.
In order to figure out the impacts of question
patterns and relation expressions, another exper-
iment (Table 4) is designed to evaluate their in-
dependent influences, where QP
only
and RE
only
denote the results of KB-QA systems which only
allow question patterns and relation expressions in
question translation respectively.
Settings Test (Accuracy) Test (Precision)
QP
only
11.8% 97.5%
RE
only
32.5% 73.2%
Table 4: Impacts of question patterns and relation
expressions. Precision is defined as the num-
ber of correctly answered questions divided by the
number of questions with non-empty answers gen-
erated by our KB-QA system.
From Table 4 we can see that the accuracy of
RE
only
on Test (32.5%) is slightly better than
baseline?s result (31.4%). We think this improve-
ment comes from two aspects: (1) The quality of
the relation expressions is better than the quality
of the lexicon entries used in the baseline; and
(2) We use the extraction-related statistics of re-
lation expressions as features, which brings more
information to measure the confidence of map-
ping between NL phrases and KB predicates, and
makes the model to be more flexible. Meanwhile,
QP
only
perform worse (11.8%) than RE
only
, due
to coverage issue. But by comparing the precision-
s of these two settings, we find QP
only
(97.5%)
outperforms RE
only
(73.2%) significantly, due to
its high quality. This means how to extract high-
quality question patterns is worth to be studied for
the question answering task.
As the performance of our KB-QA system re-
lies heavily on the k-best beam approximation, we
evaluate the impact of the beam size and list the
comparison results in Figure 6. We can see that as
6
Head questions/queries mean the questions/queries with
high frequency and clear patterns.
974
we increase k incrementally, the accuracy increase
at the same time. However, a larger k (e.g. 200)
cannot bring significant improvements comparing
to a smaller one (e.g., 20), but using a large k has
a tremendous impact on system efficiency. So we
choose k = 20 as the optimal value in above ex-
periments, which trades off between accuracy and
efficiency.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
5 20 50 100 200
Accuracy on Test 
Accuracy
Figure 6: Impacts of beam size on accuracy.
Actually, the size of our system?s search space
is much smaller than the one of the semantic parser
used in the baseline.This is due to the fact that, if
triple queries generated by the question translation
component cannot derive any answer from KB, we
will discard such triple queries directly during the
QA procedure. We can see that using a small k
can achieve better results than baseline, where the
beam size is set to be 200.
4.4 Error Analysis
4.4.1 Entity Detection
Since named entity recognizers trained on Penn
TreeBank usually perform poorly on web queries,
We instead use a simple string-match method to
detect entity mentions in the question using a
cleaned entity dictionary dumped from our KB.
One problem of doing so is the entity detection
issue. For example, in the question who was Es-
ther?s husband ?, we cannot detect Esther as an
entity, as it is just part of an entity name. We need
an ad-hoc entity detection component to handle
such issues, especially for a web scenario, where
users often type entity names in their partial or ab-
breviation forms.
4.4.2 Predicate Mapping
Some questions lack sufficient evidences to detec-
t predicates. where is Byron Nelson 2012 ? is an
example. Since each relation expression must con-
tain at least one content word, this question cannot
match any relation expression. Except for Byron
Nelson and 2012, all the others are non-content
words.
Besides, ambiguous entries contained in rela-
tion expression sets of different predicates can
bring mapping errors as well. For the follow-
ing question who did Steve Spurrier play pro
football for? as an example, since the unigram
play exists in both Film.Film.Actor and Ameri-
can Football.Player.Current Team ?s relation ex-
pression sets, we made a wrong prediction, which
led to wrong answers.
4.4.3 Specific Questions
Sometimes, we cannot give exact answers to
superlative questions like what is the first book
Sherlock Holmes appeared in?. For this example,
we can give all book names where Sherlock
Holmes appeared in, but we cannot rank them
based on their publication date , as we cannot
learn the alignment between the constraint word
first occurred in the question and the predicate
Book.Written Work.Date Of First Publication
from training data automatically. Although we
have followed some work (Poon, 2013; Liang
et al, 2013) to handle such special linguistic
phenomena by defining some specific operators,
it is still hard to cover all unseen cases. We leave
this to future work as an independent topic.
5 Conclusion and Future Work
This paper presents a translation-based KB-QA
method that integrates semantic parsing and QA
in one unified framework. Comparing to the base-
line system using an independent semantic parser
with state-of-the-art performance, we achieve bet-
ter results on a general domain evaluation set.
Several directions can be further explored in the
future: (i) We plan to design a method that can
extract question patterns automatically, using ex-
isting labeled question patterns and KB as weak
supervision. As we discussed in the experiment
part, how to mine high-quality question patterns is
worth further study for the QA task; (ii) We plan
to integrate an ad-hoc NER into our KB-QA sys-
tem to alleviate the entity detection issue; (iii) In
fact, our proposed QA framework can be general-
ized to other intelligence besides knowledge bases
as well. Any method that can generate answers to
questions, such as the Web-based QA approach,
can be integrated into this framework, by using
them in the question translation component.
975
References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. In
EMNLP, pages 421?432.
Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettle-
moyer. 2013. Semantic parsing with combinatory
categorial grammars. In ACL (Tutorial Abstracts),
page 2.
Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, pages 1533?
1544.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL, pages 423?433.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
ACL.
Cristina Espana-Bonet and Pere R. Comas. 2012. Full
machine translation for factoid question answering.
In EACL, pages 20?29.
Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open information
extraction. In EMNLP, pages 1535?1545.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL, pages 1608?1618.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In ISWC.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2012.
Extracting multilingual natural-language patterns
for rdf predicates. In ESWC.
Google. 2013. Freebase. In http://www.freebase.com.
Aditya Kalyanpur, Siddharth Patwardhan, Branimir
Boguraev, Adam Lally, and Jennifer Chu-Carroll.
2012. Fact-based question decomposition in deep-
qa. IBM Journal of Research and Development,
56(3):13.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In EMNLP, pages 1223?1233.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In EMNLP, pages 1512?1523.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP, pages 1545?1556.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL, pages 590?599.
Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In AKBC-WEKEX, pages 84?
88.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In CICLing, pages 311?324.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL, pages 933?943.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In HLT-NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In ACL.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, Vol. 2, pages 1050?
1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678?687.
976
