Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 299?307,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Constraint Driven Learning For Transliteration Discovery
Ming-Wei Chang Dan Goldwasser Dan Roth Yuancheng Tu
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,goldwas1,danr,ytu}@uiuc.edu
Abstract
This paper introduces a novel unsupervised
constraint-driven learning algorithm for iden-
tifying named-entity (NE) transliterations in
bilingual corpora. The proposed method does
not require any annotated data or aligned cor-
pora. Instead, it is bootstrapped using a simple
resource ? a romanization table. We show that
this resource, when used in conjunction with
constraints, can efficiently identify translitera-
tion pairs. We evaluate the proposed method
on transliterating English NEs to three differ-
ent languages - Chinese, Russian and Hebrew.
Our experiments show that constraint driven
learning can significantly outperform existing
unsupervised models and achieve competitive
results to existing supervised models.
1 Introduction
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to some
target language while preserving its pronunciation in
the original language. Automatic NE transliteration
is an important component in many cross-language
applications, such as Cross-Lingual Information Re-
trieval (CLIR) and Machine Translation(MT) (Her-
mjakob et al, 2008; Klementiev and Roth, 2006a;
Meng et al, 2001; Knight and Graehl, 1998).
It might initially seem that transliteration is an
easy task, requiring only finding a phonetic mapping
between character sets. However simply matching
every source language character to its target lan-
guage counterpart is not likely to work well as in
practice this mapping depends on the context the
characters appear in and on transliteration conven-
tions which may change across domains. As a result,
current approaches employ machine learning meth-
ods which, given enough labeled training data learn
how to determine whether a pair of words consti-
tute a transliteration pair. These methods typically
require training data and language-specific expertise
which may not exist for many languages. In this pa-
per we try to overcome these difficulties and show
that when the problem is modeled correctly, a sim-
ple character level mapping is a sufficient resource.
In our experiments, English was used as the
source language, allowing us to use romanization ta-
bles, a resource commonly-available for many lan-
guages1. These tables contain an incomplete map-
ping between character sets, mapping every charac-
ter to its most common counterpart.
Our transliteration model takes a discriminative
approach. Given a word pair, the model determines
if one word is a transliteration of the other. The
features used by this model are character n-gram
matches across the two strings. For example, Fig-
ure 1 describes the decomposition of a word pair into
unigram features as a bipartite graph in which each
edge represents an active feature.
We enhance the initial model with constraints, by
framing the feature extraction process as a struc-
tured prediction problem - given a word pair, the set
of possible active features is defined as a set of latent
binary variables. The contextual dependency be-
1The romanization tables available at the Library of
Congress website (http://www.loc.gov/catdir/cpso/roman.html)
cover more than 150 languages written in various non-Roman
scripts
299
Figure 1: Top: The space of all possible features that can be
generated given the word pair. Bottom: A pruned features rep-
resentation generated by the inference process.
tween features is encoded as a set of constraints over
these variables. Features are extracted by finding
an assignment that maximizes the similarity score
between the two strings and conforms to the con-
straints. The model is bootstrapped using a roman-
ization table and uses a discriminatively self-trained
classifier as a way to improve over several training
iterations. Furthermore, when specific knowledge
about the source and target languages exists, it can
be directly injected into the model as constraints.
We tested our approach on three very differ-
ent languages - Russian, a Slavic language, He-
brew a Semitic language, and Chinese, a Sino-
Tibetan language. In all languages, using this sim-
ple resource in conjunction with constraints pro-
vided us with a robust transliteration system which
significantly outperforms existing unsupervised ap-
proaches and achieves comparable performance to
supervised methods.
The rest of the paper is organized as follows.
Sec. 2 briefly examines more related work. Sec. 3
explains our model and Sec. 4 provide a linguistic
intuition for it. Sec. 5 describes our experiments and
evaluates our results followed by sec. 6 which con-
cludes our paper.
2 Related Works
Transliteration methods typically fall into two cate-
gories: generative approaches (Li et al, 2004; Jung
et al, 2000; Knight and Graehl, 1998) that try to
produce the target transliteration given a source lan-
guage NE, and discriminative approaches (Gold-
wasser and Roth, 2008b; Bergsma and Kondrak,
2007; Sproat et al, 2006; Klementiev and Roth,
2006a), that try to identify the correct translitera-
tion for a word in the source language given several
candidates in the target language. Generative meth-
ods encounter the Out-Of-Vocabulary (OOV) prob-
lem and require substantial amounts of training data
and knowledge of the source and target languages.
Discriminative approaches, when used to for dis-
covering NE in a bilingual corpora avoid the OOV
problem by choosing the transliteration candidates
from the corpora. These methods typically make
very little assumptions about the source and target
languages and require considerably less data to con-
verge. Training the transliteration model is typi-
cally done under supervised settings (Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b), or
weakly supervised settings with additional tempo-
ral information (Sproat et al, 2006; Klementiev and
Roth, 2006a). Our work differs from these works
in that it is completely unsupervised and makes no
assumptions about the training data.
Incorporating knowledge encoded as constraints
into learning problems has attracted a lot of atten-
tion in the NLP community recently. This has been
shown both in supervised settings (Roth and Yih,
2004; Riedel and Clarke, 2006) and unsupervised
settings (Haghighi and Klein, 2006; Chang et al,
2007) in which constraints are used to bootstrap the
model. (Chang et al, 2007) describes an unsuper-
vised training of a Constrained Conditional Model
(CCM), a general framework for combining statisti-
cal models with declarative constraints. We extend
this work to include constraints over possible assign-
ments to latent variables which, in turn, define the
underlying representation for the learning problem.
In the transliteration community there are sev-
eral works (Ristad and Yianilos, 1998; Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b) that
show how the feature representation of a word pair
can be restricted to facilitate learning a string sim-
ilarity model. We follow the approach discussed
in (Goldwasser and Roth, 2008b), which considers
the feature representation as a structured prediction
problem and finds the set of optimal assignments (or
feature activations), under a set of legitimacy con-
straints. This approach stresses the importance of
interaction between learning and inference, as the
model iteratively uses inference to improve the sam-
ple representation for the learning problem and uses
the learned model to improve the accuracy of the in-
300
ference process. We adapt this approach to unsu-
pervised settings, where iterating over the data im-
proves the model in both of these dimensions.
3 Unsupervised Constraint Driven
Learning
In this section we present our Unsupervised Con-
straint Driven Learning (UCDL) model for discov-
ering transliteration pairs. Our task is in essence a
ranking task. Given a NE in the source language and
a list of candidate transliterations in the target lan-
guage, the model is supposed to rank the candidates
and output the one with the highest score. The model
is bootstrapped using two linguistic resources: a ro-
manization table and a set of general and linguistic
constraints. We use several iterations of self training
to learn the model. The details of the procedure are
explained in Algorithm 1.
In our model features are character pairs (cs, ct),
where cs ? Cs is a source word character and
ct ? Ct is a target word character. The feature
representation of a word pair vs, vt is denoted by
F (vs, vt). Each feature (cs, ct) is assigned a weight
W (cs, ct) ? R. In step 1 of the algorithm we initial-
ize the weights vector using the romanization table.
Given a pair (vs, vt), a feature extraction process
is used to determine the feature based representation
of the pair. Once features are extracted to represent
a pair, the sum of the weights of the extracted fea-
tures is the score assigned to the target translitera-
tion candidate. Unlike traditional feature extraction
approaches, our feature representation function does
not produce a fixed feature representation. In step
2.1, we formalize the feature extraction process as a
constrained optimization problem that captures the
interdependencies between the features used to rep-
resent the sample. That is, obtaining F (vs, vt) re-
quires solving an optimization problem. The techni-
cal details are described in Sec. 3.1. The constraints
we use are described in Sec. 3.2.
In step 2.2 the different candidates for every
source NE are ranked according to the similarity
score associated with their chosen representation.
This ranking is used to ?label? examples for a dis-
criminative learning process that learns increasingly
better weights, and thus improve the representation
of the pair: each source NE paired with its top
ranked transliteration is labeled as a positive exam-
ples (step 2.3) and the rest of the samples are consid-
ered as negative samples. In order to focus the learn-
ing process, we removed from the training set al
negative examples ruled-out by the constraints (step
2.4). As the learning process progresses, the initial
weights are replaced by weights which are discrimi-
natively learned (step 2.5). This process is repeated
several times until the model converges, and repeats
the same ranking over several iterations.
Input: Romanization table T : Cs ? Ct, Constraints
C, Source NEs: Vs, Target words: Vt
1. Initialize Model
Let W : Cs ? Ct ? R be a weight vector.
Initialize W using T by the following procedure
?(cs, ct), (cs, ct) ? T ? W(cs, ct) = 0,
?(cs, ct),?((cs, ct) ? T ) ?W(cs, ct) = ?1,
?cs,W(cs, ) = ?1, ?ct,W( , ct) = ?1.
2. Constraints driven unsupervised training
while not converged do
1. ?vs ? Vs, vt ? Vt, use C and W
to generate a representation F (vs, vt)
2. ?vs ? Vs, find the top ranking transliteration
pair (vs, v?t ) by solving
v?t = argmaxvt score(F (vs, vt)).
3. D = {(+, F (vs, v?t )) | ?vs ? Vs}.
4. ?vs ? Vs, vt ? Vt, if vt 6= v?t and
score(F (vs, vt)) 6= ??, then
D = D ? {(?, F (vs, vt))}.
5. W ? train(D)
end
Algorithm 1: UCDL Transliteration Framework.
In the rest of this section we explain this process
in detail. We define the feature extraction inference
process in Sec. 3.1, the constraints used in Sec. 3.2
and the inference algorithm in Sec. 3.3. The linguis-
tic intuition for our model is described in Sec. 4.
3.1 Finding Feature Representation as
Constrained Optimization
We use the formulation of Constrainted Conditional
Models (CCMs) (Roth and Yih, 2004; Roth and Yih,
2007; Chang et al, 2008). Previous work on CCM
models dependencies between different decisions in
structured prediction problems. Transliteration dis-
covery is a binary classification problem, however,
301
the underlying representation of each sample can be
modeled as a CCM, defined as a set of latent vari-
ables corresponding to the set of all possible features
for a given sample. The dependencies between the
features are captured using constraints.
Given a word pair, the set of all possible features
consists of all character mappings from the source
word to the target word. Since in many cases the
size of the words differ we augment each of the
words with a blank character (denoted as ? ?). We
model character omission by mapping the character
to the blank character. This process is formally de-
fined as an operator mapping a transliteration can-
didate pair to a set of binary variables, denoted as
All-Features (AF ).
AF = {(cs, ct)|cs ? vs ? { }, ct ? vt ? { }}
This representation is depicted at the top of Figure 1.
The initial sample representation (AF ) gener-
ates features by coupling substrings from the two
terms without considering the dependencies be-
tween the possible combinations. This representa-
tion is clearly noisy and in order to improve it we
select a subset F ? AF of the possible features.
The selection process is formulated as a linear op-
timization problem over binary variables encoding
feature activations in AF . Variables assigned 1 are
selected to be in F , and those assigned 0 are not.
The objective function maximized is a linear func-
tion over the variables in AF , each with its weight as
a coefficient, as in the left part of Equation 1 below.
We seek to maximize this linear sum subject to a set
of constraints. These represent the dependencies be-
tween selections and prior knowledge about possible
legitimate character mappings and correspond to the
right side of Equation 1. In our settings only hard
constraints are used and therefore the penalty (?) for
violating any of the constraints is set to ?. The spe-
cific constraints used are discussed in Sec. 3.2. The
score of the mapping F (vs, vt) can be written as fol-
lows:
1
|vt|
(W ? F (vs, vt)?
?
ci?C
?ci(F (vs, vt)) (1)
We normalize this score by dividing it by the size of
the target word, since the size of candidates varies,
normalization improved the ranking of candidates.
The result of the optimization process is a set F of
active features, defined in Equation 2. The result of
this process is described at the bottom of Figure 1.
F ?(vs, vt) = argmaxF?AF (vs,vt)score(F ). (2)
The ranking process done by our model can now be
naturally defined - given a source word vs, and a
set of candidates target words v0t , . . . , vnt , find the
candidate whose optimal representation maximizes
Equation 1. This process is defined in Equation 3.
v?t = argmaxvit
score(F (vs, vit)). (3)
3.2 Incorporating Mapping Constraints
We consider two types of constraints: language spe-
cific and general constraints that apply to all lan-
guages. Language specific constraints typically im-
pose a local restriction such as individually forcing
some of the possible character mapping decisions.
The linguistic intuition behind these constraints is
discussed in Section 4. General constraints encode
global restrictions, capturing the dependencies be-
tween different mapping decisions.
General constraints: To facilitate readability we
denote the feature mapping the i-th source word
character to the j-th target word character as a
Boolean variable aij that is 1 if that feature is active
and 0 otherwise.
? Coverage - Every character must be mapped
only to a single character or to the blank char-
acter. We can formulate this as: ?j aij = 1
and ?i aij = 1.
? No Crossing - Every character mapping, except
mapping to blank character, should preserve the
order of appearance in the source and target
words, or formally - ?i, j s.t. aij = 1 ? ?l <
i, ?k > j, alk = 0. Another constraint is ?i, j
s.t. aij = 1 ? ?l > i, ?k < j, alk = 0.
Language specific constraints
? Restricted Mapping: These constraints restrict
the possible local mappings between source
and target language characters. We maintain a
set of possible mappings {cs ? ?cs}, where
?cs ? Ct and {ct ? ?ct}, where ?ct ? Cs.
Any feature (cs, ct) such that cs /? ?ct or
ct /? ?cs is penalized in our model.
302
? Length restriction: An additional constraint
restricts the size difference between the two
words. We formulate this as follows: ?vs ?
Vs,?vt ? Vt, if ?|vt| > |vs| and ?|vs| > |vt|,
score(F (vs, vt)) = ??. Although ? can
take different values for different languages, we
simply set ? to 2 in this paper.
In addition to biasing the model to choose the
right candidate, the constraints also provide a com-
putational advantage: a given a word pair is elimi-
nated from consideration when the length restriction
is not satisfied or there is no way to satisfy the re-
stricted mapping constraints.
3.3 Inference
The optimization problem defined in Equation 2 is
an integer linear program (ILP). However, given
the structure of the problem it is possible to de-
velop an efficient dynamic programming algorithm
for it, based on the algorithm for finding the mini-
mal edit distance of two strings. The complexity of
finding the optimal set of features is only quadratic
in the size of the input pair, a clear improvement
over the ILP exponential time algorithm. The al-
gorithm minimizes the weighted edit distance be-
tween the strings, and produces a character align-
ment that satisfies the general constraints (Sec. 3.2).
Our modifications are only concerned with incorpo-
rating the language-specific constraints into the al-
gorithm. This can be done simply by assigning a
negative infinity score to any alignment decision not
satisfying these constraints.
4 Bootstrapping with Linguistic
Information
Our model is bootstrapped using two resources - a
romanization table and mapping constraints. Both
resources capture the same information - character
mapping between languages. The distinction be-
tween the two represents the difference in the con-
fidence we have in these resources - the romaniza-
tion table is a noisy mapping covering the character
set and is therefore better suited as a feature. Con-
straints, represented by pervasive, correct character
mapping, indicate the sound mapping tendency be-
tween source and target languages. For example,
certain n-gram phonemic mappings, such as r ? l
from English to Chinese, are language specific and
can be captured by language specific sound change
patterns.
Phonemes Constraints
Vowel i ? y; u ? w; a ? a
Nasal m ? m; m,n ? m
Approximant
r ? l; l, r ? l
l ? l; w ? h,w, f
h, o, u, v ? w; y ? y
Fricative v ? w, b, fs ? s, x, z; s, c ? s
Plosive
p ? b, p; p ? p
b ? b; t ? t
t, d ? d; q ? k
Table 1: All language specific constraints used in our English
to Chinese transliteration (see Sec. 3.2 for more details). Con-
straints in boldface apply to all positions, the rest apply only to
characters appearing in initial position.
These patterns have been used by other systems
as features or pseudofeatures (Yoon et al, 2007).
However, in our system these language specific rule-
of-thumbs are systematically used as constraints to
exclude impossible alignments and therefore gener-
ate better features for learning. We listed in Table 1
all 20 language specific constraints we used for Chi-
nese. There is a total of 24 constraints for Hebrew
and 17 for Russian.
The constraints in Table 1 indicate a systematic
sound mapping between English and Chinese un-
igram character mappings. Arranged by manners
of articulation each row of the table indicates the
sound change tendency among vowels, nasals, ap-
proximants (retroflex and glides), fricatives and plo-
sives. For example, voiceless plosive sounds such as
p, t in English, tend to map to both voiced (such as b,
d) and voiceless sounds in Chinese. However, if the
sound is voiceless in Chinese, its backtrack English
sound must be voiceless. This voice-voiceless sound
change tendency is captured by our constraints such
as p ? b, p and p ? p; t ? t.
5 Experiments and Analysis
In this section, we demonstrate the effectiveness
of constraint driven learning empirically. We start
by describing the datasets and experimental settings
and then proceed to describe the results. We eval-
uated our method on three very different target lan-
303
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  2  4  6  8  10  12  14  16  18  20
A
C
C
Number of Rounds
[KR 06] + temporal information[KR 06] All Cons. + unsupervsied learning
Figure 2: Comparison between our models and weakly su-
pervised learning methods (Klementiev and Roth, 2006b).
Note that one of the models proposed in (Klementiev and Roth,
2006b) takes advantage of the temporal information. Our best
model, the unsupervised learning with all constraints, outper-
forms both models in (Klementiev and Roth, 2006b), even
though we do not use any temporal information.
guages: Russian, Chinese, and Hebrew, and com-
pared our results to previously published results.
5.1 Experimental Settings
In our experiments the system is evaluated on its
ability to correctly identify the gold transliteration
for each source word. We evaluated the system?s
performance using two measures adopted in many
transliteration works. The first one is Mean Recip-
rocal Rank (MRR), used in (Tao et al, 2006; Sproat
et al, 2006), which is the average of the multiplica-
tive inverse of the rank of the correct answer. For-
mally, Let n be the number of source NEs. Let Gol-
dRank(i) be the rank the algorithm assigns to the
correct transliteration. Then, MRR is defined by:
MRR = 1n
n?
i=1
1
goldRank(i) .
Another measure is Accuracy (ACC) used in (Kle-
mentiev and Roth, 2006a; Goldwasser and Roth,
2008a), which is the percentage of the top rank can-
didates being the gold transliteration. In our im-
plementation we used the support vector machine
(SVM) learning algorithm with linear kernel as our
underlying learning algorithm (mentioned in part
2.5 of Algorithm 1) . We used the package LIB-
LINEAR (Hsieh et al, 2008) in our experiments.
Through all of our experiments, we used the 2-norm
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 0  2  4  6  8  10  12  14  16  18  20
M
R
R
Number of Rounds
[GR 08] 250 labeled ex. with cons[GR 08] 250 labeled ex. w/o consGeneral cons + unsupervised learningAll cons. + unsupervised learning
Figure 3: Comparison between our works and supervised
models in (Goldwasser and Roth, 2008b). We show the learn-
ing curves for Hebrew under two different settings: unsuper-
vised learning with general and all constraints. The results of
two supervised models (Goldwasser and Roth, 2008b) are also
included here. Note that our unsupervised model with all con-
straints is competitive to the supervised model with 250 labeled
examples. See the text for more comparisons and details.
hinge loss as our loss function and fixed the regular-
ization parameter C to be 0.5.
5.2 Datasets
We experimented using three different target lan-
guages Russian, Chinese, and Hebrew. We used En-
glish as the source language in all these experiments.
The Russian data set2, originally introduced in
(Klementiev and Roth, 2006b), is comprised of tem-
porally aligned news articles. The dataset contains
727 single word English NEs with a correspond-
ing set of 50,648 potential Russian candidate words
which include not only name entities, but also other
words appearing in the news articles.
The Chinese dataset is taken directly from an
English-Chinese transliteration dictionary, derived
from LDC Gigaword corpus3. The entire dictionary
consists of 74,396 pairs of English-Chinese NEs,
where Chinese NEs are written in Pinyin, a roman-
ized spelling system of Chinese. In (Tao et al, 2006)
a dataset which contains about 600 English NEs and
700 Chinese candidates is used. Since the dataset
is not publicly available, we created a dataset in a
similar way. We randomly selected approximately
600 NE pairs and then added about 100 candidates
which do not correspond to any of the English NE
2The corpus is available http://L2R.cs.uiuc.edu/?cogcomp.
3http://www.ldc.upenn.edu
304
Language UCDL Prev. works
Rus. (ACC) 73 63 (41) (KR?06)
Heb. (MRR) 0.899 0.894 (GR?08)
Table 2: Comparison to previously published results. UCDL
is our method, KR?06 is described in (Klementiev and Roth,
2006b) and GR?08 in (Goldwasser and Roth, 2008b). Note that
our results for Hebrew are comparable with a supervised sys-
tem.
previously selected.
The Hebrew dataset, originally introduced in
(Goldwasser and Roth, 2008a), consists of 300
English-Hebrew pairs extracted from Wikipedia.
5.3 Results
We begin by comparing our model to previously
published models tested over the same data, in two
different languages, Russian and Hebrew. For Rus-
sian, we compare to the model presented in (Kle-
mentiev and Roth, 2006b), a weakly supervised al-
gorithm that uses both phonetic information and
temporal information. The model is bootstrapped
using a set of 20 labeled examples. In their setting
the candidates are ranked by combining two scores,
one obtained using the transliteration model and a
second by comparing the relative occurrence fre-
quency of terms over time in both languages. Due
to computational tractability reasons we slightly
changed Algorithm 1 to use only a small subset of
the possible negative examples.
For Hebrew, we compare to the model presented
in (Goldwasser and Roth, 2008b), a supervised
model trained using 250 labeled examples. This
model uses a bigram model to represent the translit-
eration samples (i.e., features are generated by pair-
ing character unigrams and bigrams). The model
also uses constraints to restrict the feature extrac-
tion process, which are equivalent to the coverage
constraint we described in Sec. 3.2.
The results of these experiments are reported us-
ing the evaluation measures used in the original pa-
pers and are summarized in Table 2. The results
show a significant improvement over the Russian
data set and comparable performance to the super-
vised method used for Hebrew.
Figure 2 describes the learning curve of our
method over the Russian dataset. We compared our
algorithm to two models described in (Klementiev
and Roth, 2006b) - one uses only phonetic simi-
larity and the second also considers temporal co-
occurrence similarity when ranking the translitera-
tion candidates. Both models converge after 50 it-
erations. When comparing our model to their mod-
els, we found that even though our model ignores
the temporal information it achieves better results
and converges after fewer iterations. Their results
report a significant improvement when using tempo-
ral information - improving an ACC score of 41%
without temporal information to 63% when using
it. Since the temporal information is orthogonal to
the transliteration model, our model should similarly
benefit from incorporating the temporal information.
Figure 3 compares the learning curve of our
method to an existing supervised method over the
Hebrew data and shows we get comparable results.
Unfortunately, we could not find a published Chi-
nese dataset. However, our system achieved similar
results to other systems, over a different dataset with
similar number of training examples. For example,
(Sproat et al, 2006) presents a supervised system
that achieves a MRR score of 0.89, when evaluated
over a dataset consisting of 400 English NE and 627
Chinese words. Our results for a different dataset of
similar size are reported in Table 3.
5.4 Analysis
The resources used in our framework consist of
- a romanization table, general and language spe-
cific transliteration constraints. To reveal the impact
of each component we experimented with different
combination of the components, resulting in three
different testing configurations.
Romanization Table: We initialized the weight
vector using a romanization table and did not use any
constraints. To generate features we use a modified
version of our AF operator (see Sec. 3), which gen-
erates features by coupling characters in close posi-
tions in the source and target words. This configura-
tion is equivalent to the model used in (Klementiev
and Roth, 2006b).
+General Constraints: This configuration uses the
romanization table for initializing the weight vector
and general transliteration constraints (see Sec. 3.2)
for feature extraction.
+All Constraints: This configuration uses lan-
guage specific constraints in addition to the gen-
305
Settings Chinese Russian Hebrew
Romanization table 0.019 (0.5) 0.034 (1.0) 0.046 (1.7)
Romanization table +learning 0.020 (0.3) 0.048 (1.3) 0.028 (0.7)
+Gen Constraints 0.746 (67.1) 0.809 (74.3) 0.533 (45.0)
+Gen Constraints +learning 0.867 (82.2) 0.906 (86.7) 0.834 (76.0)
+All Constraints 0.801 (73.4) 0.849 (79.3) 0.743 (66.0)
+All Constraints +learning 0.889 (84.7) 0.931 (90.0) 0.899 (85.0)
Table 3: Results of an ablation study of unsupervised method for three target languages. Results for ACC are inside parentheses,
and for MRR outside. When the learning algorithm is used, the results after 20 rounds of constraint driven learning are reported.
Note that using linguistic constraints has a significant impact in the English-Hebrew experiments. Our results show that a small
amount of constraints can go a long way, and better constraints lead to better learning performance.
eral transliteration constraints to generate the feature
representation. (see Sec. 4).
+Learning: Indicates that after initializing the
weight vector, we update the weight using Algo-
rithm 1. In all of the experiments, we report the
results after 20 training iterations.
The results are summarized in Table 3. Due to the
size of the Russian dataset, we used a subset consist-
ing of 300 English NEs and their matching Russian
transliterations for the analysis presented here. Af-
ter observing the results, we discovered the follow-
ing regularities for all three languages. Using the
romanization table directly without constraints re-
sults in very poor performance, even after learning.
This can be used as an indication of the difficulty of
the transliteration problem and the difficulties ear-
lier works have had when using only romanization
tables, however, when used in conjunction with con-
straints results improve dramatically. For example,
in the English-Chinese data set, we improve MRR
from 0.02 to 0.746 and for the English-Russian data
set we improve 0.03 to 0.8. Interestingly, the results
for the English-Hebrew data set are lower than for
other languages - we achieve 0.53 MRR in this set-
ting. We attribute the difference to the quality of
the mapping in the romanization table for that lan-
guage. Indeed, the weights learned after 20 train-
ing iterations improve the results to 0.83. This im-
provement is consistent across all languages, after
learning we are able to achieve a MRR score of 0.87
for the English-Chinese data set and 0.91 for the
English-Russian data set. These results show that
romanization table contains enough information to
bootstrap the model when used in conjunction with
constraints. We are able to achieve results compa-
rable to supervised methods that use a similar set of
constraints and labeled examples.
Bootstrapping the weight vector using language
specific constraints can further improve the results.
They provide several advantages: a better starting
point, an improved learning rate and a better final
model. This is clear in all three languages, for exam-
ple results for the Russian and Chinese bootstrapped
models improve by 5%, and by over 20% for He-
brew. After training the difference is smaller- only
3% for the first two and 6% for Hebrew. Figure 3 de-
scribes the learning curve for models with and with-
out language specific constraints for the English-
Hebrew data set, it can be observed that using these
constraints the model converges faster and achieves
better results.
6 Conclusion
In this paper we develop a constraints driven ap-
proach to named entity transliteration. In doing it
we show that romanization tables are a very useful
resource for transliteration discovery if proper con-
straints are included. Our framework does not need
labeled data and does not assume that bilingual cor-
pus are temporally aligned. Even without using any
labeled data, our model is competitive to existing
supervised models and outperforms existing weakly
supervised models.
7 Acknowledgments
We wish to thank the reviewers for their insightful
comments. This work is partly supported by NSF
grant SoD-HCER-0613885 and DARPA funding un-
der the Bootstrap Learning Program.
306
References
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 656?663, Prague, Czech Republic,
June. Association for Computational Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 280?287, Prague,
Czech Republic, Jun. Association for Computational
Linguistics.
M. Chang, L. Ratinov, N. Rizzolo, and D. Roth. 2008.
Learning and inference with constraints. In Proc.
of the National Conference on Artificial Intelligence
(AAAI), July.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), June.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 353?362, Oct.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. of the Annual Meet-
ing of the North American Association of Computa-
tional Linguistics (NAACL).
U. Hermjakob, K. Knight, and H. Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 389?397, Columbus, Ohio, June.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML
?08: Proceedings of the 25th international conference
on Machine learning, pages 408?415, New York, NY,
USA. ACM.
S. Jung, S. Hong, and E. Paek. 2000. An english to
korean transliteration model of extended markov win-
dow. In Proc. the International Conference on Com-
putational Linguistics (COLING), pages 383?389.
A. Klementiev and D. Roth. 2006a. Named entity
transliteration and discovery from multilingual com-
parable corpora. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 82?88, June.
A. Klementiev and D. Roth. 2006b. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages USS,TL,ADAPT, July.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, pages 599?612.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In Proc. of the An-
nual Meeting of the Association of Computational Lin-
guistics (ACL), pages 159?166, Barcelona, Spain, July.
H. Meng, W. Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named en-
tities in english-chinese cross-langauge spoken doc-
ument retreival. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop,
pages 389?397.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 129?
137, Sydney, Australia.
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recogni-
tion and Machine Intelligence, 20(5):522?532, May.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
pages 1?8. Association for Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
R. Sproat, T. Tao, and C. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proc. of
the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 73?80, Sydney, Aus-
tralia, July.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entitly transliteration using tem-
poral and phonetic correlation. In Proc. of the Con-
ference on Empirical Methods for Natural Language
Processing (EMNLP), pages 250?257.
S. Yoon, K. Kim, and R. Sproat. 2007. Multilingual
transliteration using feature based phonetic method.
In Proc. of the Annual Meeting of the Association
of Computational Linguistics (ACL), pages 112?119,
Prague, Czech Republic, June.
307
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 65?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pipeline Framework for Dependency Parsing
Ming-Wei Chang Quang Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, quangdo2, danr}@uiuc.edu
Abstract
Pipeline computation, in which a task is
decomposed into several stages that are
solved sequentially, is a common compu-
tational strategy in natural language pro-
cessing. The key problem of this model
is that it results in error accumulation and
suffers from its inability to correct mis-
takes in previous stages. We develop
a framework for decisions made via in
pipeline models, which addresses these
difficulties, and presents and evaluates it
in the context of bottom up dependency
parsing for English. We show improve-
ments in the accuracy of the inferred trees
relative to existing models. Interestingly,
the proposed algorithm shines especially
when evaluated globally, at a sentence
level, where our results are significantly
better than those of existing approaches.
1 Introduction
A pipeline process over the decisions of learned
classifiers is a common computational strategy in
natural language processing. In this model a task
is decomposed into several stages that are solved
sequentially, where the computation in the ith
stage typically depends on the outcome of com-
putations done in previous stages. For example,
a semantic role labeling program (Punyakanok et
al., 2005) may start by using a part-of-speech tag-
ger, then apply a shallow parser to chunk the sen-
tence into phrases, identify predicates and argu-
ments and then classify them to types. In fact,
any left to right processing of an English sentence
may be viewed as a pipeline computation as it pro-
cesses a token and, potentially, makes use of this
result when processing the token to the right.
The pipeline model is a standard model of
computation in natural language processing for
good reasons. It is based on the assumption that
some decisions might be easier or more reliable
than others, and their outcomes, therefore, can be
counted on when making further decisions. Nev-
ertheless, it is clear that it results in error accu-
mulation and suffers from its inability to correct
mistakes in previous stages. Researchers have re-
cently started to address some of the disadvantages
of this model. E.g., (Roth and Yih, 2004) suggests
a model in which global constraints are taken into
account in a later stage to fix mistakes due to the
pipeline. (Punyakanok et al, 2005; Marciniak
and Strube, 2005) also address some aspects of
this problem. However, these solutions rely on the
fact that all decisions are made with respect to the
same input; specifically, all classifiers considered
use the same examples as their input. In addition,
the pipelines they study are shallow.
This paper develops a general framework for
decisions in pipeline models which addresses
these difficulties. Specifically, we are interested
in deep pipelines ? a large number of predictions
that are being chained.
A pipeline process is one in which decisions
made in the ith stage (1) depend on earlier deci-
sions and (2) feed on input that depends on earlier
decisions. The latter issue is especially important
at evaluation time since, at training time, a gold
standard data set might be used to avoid this issue.
We develop and study the framework in the con-
text of a bottom up approach to dependency pars-
ing. We suggest that two principles to guide the
pipeline algorithm development:
(i) Make local decisions as reliable as possible.
(ii) Reduce the number of decisions made.
Using these as guidelines we devise an algo-
65
rithm for dependency parsing, prove that it satis-
fies these principles, and show experimentally that
this improves the accuracy of the resulting tree.
Specifically, our approach is based on a shift-
reduced parsing as in (Yamada and Matsumoto,
2003). Our general framework provides insights
that allow us to improve their algorithm, and to
principally justify some of the algorithmic deci-
sions. Specifically, the first principle suggests to
improve the reliability of the local predictions,
which we do by improving the set of actions taken
by the parsing algorithm, and by using a look-
ahead search. The second principle is used to jus-
tify the control policy of the parsing algorithm ?
which edges to consider at any point of time. We
prove that our control policy is optimal in some
sense, and that the decisions we made, guided by
these, principles lead to a significant improvement
in the accuracy of the resulting parse tree.
1.1 Dependency Parsing and Pipeline Models
Dependency trees provide a syntactic reresenta-
tion that encodes functional relationships between
words; it is relatively independent of the grammar
theory and can be used to represent the structure
of sentences in different languages. Dependency
structures are more efficient to parse (Eisner,
1996) and are believed to be easier to learn, yet
they still capture much of the predicate-argument
information needed in applications (Haghighi et
al., 2005), which is one reason for the recent in-
terest in learning these structures (Eisner, 1996;
McDonald et al, 2005; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004).
Eisner?s work ? O(n3) parsing time generative
algorithm ? embarked the interest in this area.
His model, however, seems to be limited when
dealing with complex and long sentences. (Mc-
Donald et al, 2005) build on this work, and use
a global discriminative training approach to im-
prove the edges? scores, along with Eisner?s algo-
rithm, to yield the expected improvement. A dif-
ferent approach was studied by (Yamada and Mat-
sumoto, 2003), that develop a bottom-up approach
and learn the parsing decisions between consecu-
tive words in the sentence. Local actions are used
to generate a dependency tree using a shift-reduce
parsing approach (Aho et al, 1986). This is a
true pipeline approach, as was done in other suc-
cessful parsers, e.g. (Ratnaparkhi, 1997), in that
the classifiers are trained on individual decisions
rather than on the overall quality of the parser, and
chained to yield the global structure. Clearly, it
suffers from the limitations of pipeline process-
ing, such as accumulation of errors, but neverthe-
less, yields very competitive parsing results. A
somewhat similar approach was used in (Nivre and
Scholz, 2004) to develop a hybrid bottom-up/top-
down approach; there, the edges are also labeled
with semantic types, yielding lower accuracy than
the works mentioned above.
The overall goal of dependency parsing (DP)
learning is to infer a tree structure. A common
way to do that is to predict with respect to each
potential edge (i, j) in the tree, and then choose a
global structure that (1) is a tree and that (2) max-
imizes some score. In the context of DPs, this
?edge based factorization method? was proposed
by (Eisner, 1996). In other contexts, this is similar
to the approach of (Roth and Yih, 2004) in that
scoring each edge depends only on the raw data
observed and not on the classifications of other
edges, and that global considerations can be used
to overwrite the local (edge-based) decisions.
On the other hand, the key in a pipeline model
is that making a decision with respect to the edge
(i, j) may gain from taking into account deci-
sions already made with respect to neighboring
edges. However, given that these decisions are
noisy, there is a need to devise policies for reduc-
ing the number of predictions in order to make the
parser more robust. This is exemplified in (Ya-
mada and Matsumoto, 2003) ? a bottom-up ap-
proach, that is most related to the work presented
here. Their model is a ?traditional? pipeline model
? a classifier suggests a decision that, once taken,
determines the next action to be taken (as well as
the input the next action observes).
In the rest of this paper, we propose and jus-
tify a framework for improving pipeline process-
ing based on the principles mentioned above: (i)
make local decisions as reliably as possible, and
(ii) reduce the number of decisions made. We
use the proposed principles to examine the (Ya-
mada and Matsumoto, 2003) parsing algorithm
and show that this results in modifying some of
the decisions made there and, consequently, better
overall dependency trees.
2 Efficient Dependency Parsing
This section describes our DP algorithm and jus-
tifies its advantages as a pipeline model. We pro-
66
pose an improved pipeline framework based on the
mentioned principles.
For many languages such as English, Chinese
and Japanese (with a few exceptions), projective
dependency trees (that is, DPs without edge cross-
ings) are sufficient to analyze most sentences. Our
work is therefore concerned only with projective
trees, which we define below.
For words x, y in the sentence T we introduce
the following notations:
x ? y: x is the direct parent of y.
x ?? y: x is an ancestor of y;
x ? y: x ? y or y ? x.
x < y: x is to the left of y in T .
Definition 1 (Projective Language) (Nivre,
2003) ?a, b, c ? T, a ? b and a < c < b imply
that a ?? c or b ?? c.
2.1 A Pipeline DP Algorithm
Our parsing algorithm is a modified shift-reduce
parser that makes use of the actions described be-
low and applies them in a left to right manner
on consecutive pairs of words (a, b) (a < b) in
the sentence. This is a bottom-up approach that
uses machine learning algorithms to learn the pars-
ing decisions (actions) between consecutive words
in the sentences. The basic actions used in this
model, as in (Yamada and Matsumoto, 2003), are:
Shift: there is no relation between a and b, or
the action is deferred because the relationship be-
tween a and b cannot be determined at this point.
Right: b is the parent of a,
Left: a is the parent of b.
This is a true pipeline approach in that the clas-
sifiers are trained on individual decisions rather
than on the overall quality of the parsing, and
chained to yield the global structure. And, clearly,
decisions make with respect to a pair of words af-
fect what is considered next by the algorithm.
In order to complete the description of the algo-
rithm we need to describe which edge to consider
once an action is taken. We describe it via the no-
tion of the focus point: when the algorithm con-
siders the pair (a, b), a < b, we call the word a the
current focus point.
Next we describe several policies for determin-
ing the focus point of the algorithm following an
action. We note that, with a few exceptions, de-
termining the focus point does not affect the cor-
rectness of the algorithm. It is easy to show that
for (almost) any focus point chosen, if the correct
action is selected for the corresponding edge, the
algorithm will eventually yield the correct tree (but
may require multiple cycles through the sentence).
In practice, the actions selected are noisy, and a
wasteful focus point policy will result in a large
number of actions, and thus in error accumulation.
To minimize the number of actions taken, we want
to find a good focus point placement policy.
After S, the focus point always moves one word
to the right. After L or R there are there natural
placement policies to consider:
Start Over: Move focus to the first word in T .
Stay: Move focus to the next word to the right.
That is, for T = (a, b, c), and focus being a, an
L action will result is the focus being a, while R
action results in the focus being b.
Step Back: The focus moves to the previous word
(on the left). That is, for T = (a, b, c), and focus
being b, in both cases, a will be the focus point.
In practice, different placement policies have a
significant effect on the number of pairs consid-
ered by the algorithm and, therefore, on the fi-
nal accuracy1. The following analysis justifies the
Step Back policy. We claim that if Step Back
is used, the algorithm will not waste any action.
Thus, it achieves the goal of minimizing the num-
ber of actions in pipeline algorithms. Notice that
using this policy, when L is taken, the pair (a, b) is
reconsidered, but with new information, since now
it is known that c is the child of b. Although this
seems wasteful, we will show this is a necessary
movement to reduce the number of actions.
As mentioned above, each of these policies
yields the correct tree. Table 1 compares the three
policies in terms of the number of actions required
to build a tree.
Policy #Shift #Left #Right
Start over 156545 26351 27918
Stay 117819 26351 27918
Step back 43374 26351 27918
Table 1: The number of actions required to build
all the trees for the sentences in section 23 of Penn
Treebank (Marcus et al, 1993) as a function of
the focus point placement policy. The statistics are
taken with the correct (gold-standard) actions.
It is clear from Table 1 that the policies result
1Note that (Yamada and Matsumoto, 2003) mention that
they move the focus point back after R, but do not state what
they do after executing L actions, and why. (Yamada, 2006)
indicates that they also move focus point back after L.
67
Algorithm 2 Pseudo Code of the dependency
parsing algorithm. getFeatures extracts the fea-
tures describing the word pair currently consid-
ered; getAction determines the appropriate action
for the pair; assignParent assigns a parent for the
child word based on the action; and deleteWord
deletes the child word in T at the focus once the
action is taken.Let t represents for a word token
For sentence T = {t1, t2, . . . , tn}
focus= 1
while focus< |T | do
~v = getFeatures(tfocus, tfocus+1)
? = getAction(tfocus, tfocus+1, ~v)if ? = L or ? = R then
assignParent(tfocus, tfocus+1, ?)
deleteWord(T, focus, ?)
// performing Step Back here
focus = focus ? 1
else
focus = focus + 1
end if
end while
in very different number of actions and that Step
Back is the best choice. Note that, since the ac-
tions are the gold-standard actions, the policy af-
fects only the number of S actions used, and not
the L and R actions, which are a direct function
of the correct tree. The number of required ac-
tions in the testing stage shows the same trend and
the Step Back also gives the best dependency ac-
curacy. Algorithm 2 depicts the parsing algorithm.
2.2 Correctness and Pipeline Properties
We can prove two properties of our algorithm.
First we show that the algorithm builds the de-
pendency tree in only one pass over the sentence.
Then, we show that the algorithm does not waste
actions in the sense that it never considers a word
pair twice in the same situation. Consequently,
this shows that under the assumption of a perfect
action predictor, our algorithm makes the smallest
possible number of actions, among all algorithms
that build a tree sequentially in one pass.
Note that this may not be true if the action clas-
sifier is not perfect, and one can contrive examples
in which an algorithm that makes several passes on
a sentence can actually make fewer actions than a
single pass algorithm. In practice, however, as our
experimental data shows, this is unlikely.
Lemma 1 A dependency parsing algorithm that
uses the Step Back policy completes the tree when
it reaches the end of the sentence for the rst time.
In order to prove the algorithm we need the fol-
lowing definition. We call a pair of words (a, b) a
free pair if and only if there is a relation between
a and b and the algorithm can perform L or R ac-
tions on that pair when it is considered. Formally,
Definition 2 (free pair) A pair (a, b) considered
by the algorithm is a free pair, if it satises the
following conditions:
1. a ? b
2. a, b are consecutive in T (not necessary in
the original sentence).
3. No other word in T is the child of a or b. (a
and b are now part of a complete subtree.)
Proof. : It is easy to see that there is at least one
free pair in T , with |T | > 1. The reason is that
if no such pair exists, there must be three words
{a, b, c} s.t. a ? b, a < c < b and ?(a ? c ?
b ? c). However, this violates the properties of a
projective language.
Assume {a, b, d} are three consecutive words in
T . Now, we claim that when using Step Back, the
focus point is always to the left of all free pairs in
T . This is clearly true when the algorithm starts.
Assume that (a, b) is the first free pair in T and let
c be just to the left of a and b. Then, the algorithm
will not make a L or R action before the focus
point meets (a, b), and will make one of these ac-
tions then. It?s possible that (c, a ? b) becomes a
free pair after removing a or b in T so we need
to move the focus point back. However, we also
know that there is no free pair to the left of c.
Therefore, during the algorithm, the focus point
will always remain to the left of all free pairs. So,
when we reach the end of the sentence, every free
pair in the sentence has been taken care of, and the
sentence has been completely parsed. 2
Lemma 2 All actions made by a dependency
parsing algorithm that uses the Step Back policy
are necessary.
Proof. : We will show that a pair (a, b) will never
be considered again given the same situation, that
is, when there is no additional information about
relations a or b participate in. Note that if R or
68
L is taken, either a or b will become a child word
and be eliminate from further consideration by the
algorithm. Therefore, if the action taken on (a, b)
is R or L, it will never be considered again.
Assume that the action taken is S, and, w.l.o.g.
that this is the rightmost S action taken before a
non-S action happens. Note that it is possible that
there is a relation between a and b, but we can-
not perform R or L now. Therefore, we should
consider (a, b) again only if a child of a or b has
changed. When Step Back is used, we will con-
sider (a, b) again only if the next action is L. (If
next action is R, b will be eliminated.) This is true
because the focus point will move back after per-
forming L, which implies that b has a new child
so we are indeed in a new situation. Since, from
Lemma 1, the algorithm only requires one round.
we therefore consider (a, b) again only if the situ-
ation has changed. 2
2.3 Improving the Parsing Action Set
In order to improve the accuracy of the action pre-
dictors, we suggest a new (hierarchical) set of ac-
tions: Shift, Left, Right, WaitLeft, WaitRight. We
believe that predicting these is easier due to finer
granularity ? the S action is broken to sub-actions
in a natural way.
WaitLeft: a < b. a is the parent of b, but it?s
possible that b is a parent of other nodes. Action is
deferred. If we perform Left instead, the child of b
can not find its parents later.
WaitRight: a < b. b is the parent of a, but it?s
possible that a is a parent of other nodes. Similar
to WL, action is deferred.
Thus, we also change the algorithm to perform
S only if there is no relationship between a and b2.
The new set of actions is shown to better support
our parsing algorithm, when tested on different
placement policies. When WaitLeft or WaitRight
is performed, the focus will move to the next word.
It is very interesting to notice that WaitRight is
not needed in projective languages if Step Back
is used. This give us another strong reason to use
Step Back, since the classification becomes more
accurate ? a more natural class of actions, with a
smaller number of candidate actions.
Once the parsing algorithm, along with the fo-
cus point policy, is determined, we can train the
2Interestingly, (Yamada and Matsumoto, 2003) mention
the possibility of an additional single Wait action, but do not
add it to the model.
action classifiers. Given an annotated corpus, the
parsing algorithm is used to determine the action
taken for each consecutive pair; this is used to train
a classifier to predict one of the five actions. The
details of the classifier and the feature used are
given in Section 4.
When the learned model is evaluated on new
data, the sentence is processed left to right and the
parsing algorithm, along with the action classifier,
are used to produce the dependency tree. The eval-
uation process is somewhat more involved, since
the action classifier is not used as is, but rather via
a look ahead inference step described next.
3 A Pipeline Model with Look Ahead
The advantage of a pipeline model is that it can use
more information, based on the outcomes of previ-
ous predictions. As discussed earlier, this may re-
sult in accumulating error. The importance of hav-
ing a reliable action predictor in a pipeline model
motivates the following approach. We devise a
look ahead algorithm and use it as a look ahead
policy, when determining the predicted action.
This approach can be used in any pipeline
model but we illustrate it below in the context of
our dependency parser.
The following example illustrates a situation in
which an early mistake in predicting an action
causes a chain reaction and results in further mis-
takes. This stresses the importance of correct early
decisions, and motivates our look ahead policy.
Let (w, x, y, z) be a sentence of four words, and
assume that the correct dependency relations are
as shown in the top part of Figure 1. If the system
mistakenly predicts that x is a child of w before y
and z becomes x?s children, we can only consider
the relationship between w and y in the next stage.
Consequently, we will never find the correct parent
for y and z. The previous prediction error propa-
gates and impacts future predictions. On the other
hand, if the algorithm makes a correct prediction,
in the next stage, we do not need to consider w and
y. As shown, getting useful rather than misleading
information in a pipeline model, requires correct
early predictions. Therefore, it is necessary to uti-
lize some inference framework to that may help
resolving the error accumulation problem.
In order to improve the accuracy of the action
prediction, we might want to examine all possible
combinations of action sequences and choose the
one that maximizes some score. It is clearly in-
69
X YW Z
X
YW Z
Figure 1: Top figure: the correct dependency rela-
tions between w, x, y and z. Bottom figure: if the
algorithm mistakenly decides that x is a child of w
before deciding that y and z are x?s children, we
cannot find the correct parent for y and z.
tractable to find the global optimal prediction se-
quences in a pipeline model of the depth we con-
sider. Therefore, we use a look ahead strategy,
implemented via a local search framework, which
uses additional information but is still tractable.
The local search algorithm is presented in Algo-
rithm 3. The algorithm accepts three parameters,
model, depth and State. We assume a classifier
that can give a confidence in its prediction. This is
represented here by model.
As our learning algorithm we use a regularized
variation of the perceptron update rule, as incorpo-
rated in SNoW (Roth, 1998; Carlson et al, 1999),
a multi-class classifier that is tailored for large
scale learning tasks and has been used successfully
in a large number of NLP tasks (e.g., (Punyakanok
et al, 2005)). SNoW uses softmax over the raw
activation values as its confidence measure, which
can be shown to produce a reliable approximation
of the labels? conditional probabilities.
The parameter depth is to determine the depth
of the search procedure. State encodes the config-
uration of the environment (in the context of the
dependency parsing this includes the sentence, the
focus point and the current parent and children for
each word). Note that State changes when a pre-
diction is made and that the features extracted for
the action classifier also depend on State.
The search algorithm will perform a search of
length depth. Additive scoring is used to score
the sequence, and the first action in this sequence
is selected and performed. Then, the State is up-
dated, the new features for the action classifiers are
computed and search is called again.
One interesting property of this framework is
that it allows that use of future information in ad-
dition to past information. The pipeline model nat-
urally allows access to all the past information.
Algorithm 3 Pseudo code for the look ahead algo-
rithm. y represents a action sequence. The func-
tion search considers all possible action sequences
with |depth| actions and returns the sequence with
the highest score.
Algo predictAction(model, depth, State)
x = getNextFeature(State)
y = search(x, depth, model, State)
lab = y[1]
State = update(State, lab)
return lab
Algo search(x, depth, model, State)
maxScore = ??
F = {y | ?y? = depth}
for y in F do
s = 0, TmpState = State
for i = 1 . . . depth do
x = getNextFeature(TmpState)
s = s+ score(y[i], x, model)
TmpState = update(TmpState, y[i])
end for
if s > maxScore then
y? = y
maxScore = s
end if
end for
return y?
Since the algorithm uses a look ahead policy, it
also uses future predictions. The significance of
this becomes clear in Section 4.
There are several parameters, in addition to
depth that can be used to improve the efficiency of
the framework. For example, given that the action
predictor is a multi-class classifier, we do not need
to consider all future possibilities in order to de-
cide the current action. For example, in our exper-
iments, we only consider two actions with highest
score at each level (which was shown to produce
almost the same accuracy as considering all four
actions).
4 Experiments and Results
We use the standard corpus for this task, the Penn
Treebank (Marcus et al, 1993). The training set
consists of sections 02 to 21 and the testing set is
section 23. The POS tags for the evaluation data
sets were provided by the tagger of (Toutanova et
al., 2003) (which has an accuracy of 97.2% section
70
23 of the Penn Treebank).
4.1 Features for Action Classification
For each word pair (w1, w2) we use the words,their POS tags and also these features of the chil-
dren of w1 and w2. We also include the lexiconand POS tags of 2 words before w1 and 4 wordsafter w2 (as in (Yamada and Matsumoto, 2003)).The key additional feature we use, relative to (Ya-
mada and Matsumoto, 2003), is that we include
the previous predicted action as a feature. We
also add conjunctions of above features to ensure
expressiveness of the model. (Yamada and Mat-
sumoto, 2003) makes use of polynomial kernels
of degree 2 which is equivalent to using even more
conjunctive features. Overall, the average number
of active features in an example is about 50.
4.2 Evaluation
We use the same evaluation metrics as in (McDon-
ald et al, 2005). Dependency accuracy (DA) is the
proportion of non-root words that are assigned the
correct head. Complete accuracy (CA) indicates
the fraction of sentences that have a complete cor-
rect analysis. We also measure that root accuracy
(RA) and leaf accuracy (LA), as in (Yamada and
Matsumoto, 2003). When evaluating the result,
we exclude the punctuation marks, as done in (Mc-
Donald et al, 2005) and (Yamada and Matsumoto,
2003).
4.3 Results
We present the results of several of the experi-
ments that were intended to help us analyze and
understand several of the design decisions in our
pipeline algorithm.
To see the effect of the additional action, we
present in Table 2 a comparison between a system
that does not have the WaitLeft action (similar
to the (Yamada and Matsumoto, 2003) approach)
with one that does. In both cases, we do not use the
look ahead procedure. Note that, as stated above,
the action WaitRight is never needed for our pars-
ing algorithm. It is clear that adding WaitLeft in-
creases the accuracy significantly.
Table 3 investigates the effect of the look ahead,
and presents results with different depth param-
eters (depth= 1 means ?no search?), showing a
consistent trend of improvement.
Table 4 breaks down the results as a function
of the sentence length; it is especially noticeable
that the system also performs very well for long
method DA RA CA LA
w/o WaitLeft 90.27 90.73 39.28 93.87
w WaitLeft 90.53 90.76 39.74 93.94
Table 2: The significant of the action WaitLeft .
method DA RA CA LA
depth=1 90.53 90.76 39.74 93.94depth=2 90.67 91.51 40.23 93.96depth=3 90.69 92.05 40.52 93.94depth=4 90.79 92.26 40.68 93.95
Table 3: The effect of different depth settings.
sentences, another indication for its global perfor-
mance robustness.
Table 5 shows the results with three settings of
the POS tagger. The best result is, naturally, when
we use the gold standard also in testing. How-
ever, it is worthwhile noticing that it is better to
train with the same POS tagger available in test-
ing, even if its performance is somewhat lower.
Table 6 compares the performances of several
of the state of the art dependency parsing systems
with ours. When comparing with other depen-
dency parsing systems it is especially worth notic-
ing that our system gives significantly better accu-
racy on completely parsed sentences.
Interestingly, in the experiments, we allow the
parsing algorithm to run many rounds to parse a
sentece in the testing stage. However, we found
that over 99% sentences can be parsed in a single
round. This supports for our justification about the
correctness of our model.
5 Further Work and Conclusion
We have addressed the problem of using learned
classifiers in a pipeline fashion, where a task is de-
composed into several stages and stage classifiers
are used sequentially, where each stage may use
the outcome of previous stages as its input. This
is a common computational strategy in natural lan-
guage processing and is known to suffer from error
accumulation and an inability to correct mistakes
in previous stages.
Sent. Len. DA RA CA LA
<11 93.4 96.7 85.2 94.6
11-20 92.4 93.7 56.1 94.7
21-30 90.4 91.8 32.5 93.4
31-40 90.4 89.8 16.8 94.0
>40 89.7 87.9 8.7 93.3
Table 4: The effect of sentences length. The ex-
periment is done with depth = 4.
71
Train-Test DA RA CA LA
gold?pos 90.7 92.0 40.8 93.8
pos?pos 90.8 92.3 40.7 94.0
gold?gold 92.0 93.9 43.6 95.0
Table 5: Comparing different sources of POS tag-
ging in a pipeline model. We set depth= 4 in all
the experiments of this table.
System DA RA CA LA
Y&M03 90.3 91.6 38.4 93.5
N&S04 87.3 84.3 30.4 N/A
M&C&P05 90.9 94.2 37.5 N/ACurrent Work 90.8 92.3 40.7 94.0
Table 6: The comparison between the current
work with other dependency parsing systems.
We abstracted two natural principles, one which
calls for making the local classifiers used in the
computation more reliable and a second, which
suggests to devise the pipeline algorithm in such
a way that minimizes the number of decisions (ac-
tions) made.
We study this framework in the context of de-
signing a bottom up dependency parsing. Not only
we manage to use this framework to justify several
design decisions, but we also show experimentally
that following these results in improving the accu-
racy of the inferred trees relative to existing mod-
els. Interestingly, we can show that the trees pro-
duced by our algorithm are relatively good even
for long sentences, and that our algorithm is do-
ing especially well when evaluated globally, at a
sentence level, where our results are significantly
better than those of existing approaches ? perhaps
showing that the design goals were achieved.
Our future work includes trying to generalize
this work to non-projective dependency parsing,
as well as attempting to incorporate additional
sources of information (e.g., shallow parsing in-
formation) into the pipeline process.
6 Acknowledgements
We thank Ryan McDonald for providing the anno-
tated data set and to Vasin Punyakanok for useful
comments and suggestions.
This research is supported by the Advanced
Research and Development Activity (ARDA)?s
Advanced Question Answering for Intelligence
(AQUAINT) Program and a DOI grant under the
Reflex program.
References
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, techniques, and tools. In Addison-Wesley Pub-lishing Company, Reading, MA.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science Depart-
ment, May.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. the Inter-national Conference on Computational Linguistics (COL-ING), pages 340?345, Copenhagen, August.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust textual
inference via graph matching. In Proceedings of HumanLanguage Technology Conference and Conference on Em-pirical Methods in Natural Language Processing, pages
387?394, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
T. Marciniak and M. Strube. 2005. Beyond the pipeline: Dis-
crete optimization in NLP. In Proceedings of the NinthConference on Computational Natural Language Learn-ing (CoNLL-2005), pages 136?143, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330,
June.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc. ofthe Annual Meeting of the ACL, pages 91?98, Ann Arbor,
Michigan.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In COLING2004, pages 64?70.
Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. In IWPT, Nancy, France.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity
of syntactic parsing for semantic role labeling. In Proc.of the International Joint Conference on Artificial Intelli-gence (IJCAI), pages 1117?1123.
A. Ratnaparkhi. 1997. A linear observed time statistical
parser based on maximum entropy models. In EMNLP-97, The Second Conference on Empirical Methods in Nat-ural Language Processing, pages 1?10.
D. Roth and W. Yih. 2004. A linear programming for-
mulation for global inference in natural language tasks.
In Hwee Tou Ng and Ellen Riloff, editors, Proc. of theAnnual Conference on Computational Natural LanguageLearning (CoNLL), pages 1?8. Association for Computa-
tional Linguistics.
D. Roth. 1998. Learning to resolve natural language ambi-
guities: A unified approach. In Proc. National Conferenceon Artificial Intelligence, pages 806?813.
K. Toutanova, D. Klein, and C. Manning. ?2003?. Feature-
rich part-of-speech tagging with a cyclic dependency net-
work. In Proceedings of HLT-NAACL 03.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In IWPT2003.
H. Yamada. 2006. Private communication.
72
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280?287,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Guiding Semi-Supervision with Constraint-Driven Learning
Ming-Wei Chang Lev Ratinov Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, ratinov2, danr}@uiuc.edu
Abstract
Over the last few years, two of the main
research directions in machine learning of
natural language processing have been the
study of semi-supervised learning algo-
rithms as a way to train classifiers when the
labeled data is scarce, and the study of ways
to exploit knowledge and global information
in structured learning tasks. In this paper,
we suggest a method for incorporating do-
main knowledge in semi-supervised learn-
ing algorithms. Our novel framework unifies
and can exploit several kinds of task specic
constraints. The experimental results pre-
sented in the information extraction domain
demonstrate that applying constraints helps
the model to generate better feedback during
learning, and hence the framework allows
for high performance learning with signif-
icantly less training data than was possible
before on these tasks.
1 Introduction
Natural Language Processing (NLP) systems typi-
cally require large amounts of knowledge to achieve
good performance. Acquiring labeled data is a dif-
ficult and expensive task. Therefore, an increasing
attention has been recently given to semi-supervised
learning, where large amounts of unlabeled data are
used to improve the models learned from a small
training set (Collins and Singer, 1999; Thelen and
Riloff, 2002). The hope is that semi-supervised or
even unsupervised approaches, when given enough
knowledge about the structure of the problem, will
be competitive with the supervised models trained
on large training sets. However, in the general
case, semi-supervised approaches give mixed re-
sults, and sometimes even degrade the model per-
formance (Nigam et al, 2000). In many cases, im-
proving semi-supervised models was done by seed-
ing these models with domain information taken
from dictionaries or ontology (Cohen and Sarawagi,
2004; Collins and Singer, 1999; Haghighi and Klein,
2006; Thelen and Riloff, 2002). On the other hand,
in the supervised setting, it has been shown that
incorporating domain and problem specific struc-
tured information can result in substantial improve-
ments (Toutanova et al, 2005; Roth and Yih, 2005).
This paper proposes a novel constraints-based
learning protocol for guiding semi-supervised learn-
ing. We develop a formalism for constraints-based
learning that unifies several kinds of constraints:
unary, dictionary based and n-ary constraints, which
encode structural information and interdependencies
among possible labels. One advantage of our for-
malism is that it allows capturing different levels of
constraint violation. Our protocol can be used in
the presence of any learning model, including those
that acquire additional statistical constraints from
observed data while learning (see Section 5. In the
experimental part of this paper we use HMMs as the
underlying model, and exhibit significant reduction
in the number of training examples required in two
information extraction problems.
As is often the case in semi-supervised learning,
the algorithm can be viewed as a process that im-
proves the model by generating feedback through
280
labeling unlabeled examples. Our algorithm pushes
this intuition further, in that the use of constraints
allows us to better exploit domain information as a
way to label, along with the current learned model,
unlabeled examples. Given a small amount of la-
beled data and a large unlabeled pool, our frame-
work initializes the model with the labeled data and
then repeatedly:
(1) Uses constraints and the learned model to label
the instances in the pool.
(2) Updates the model by newly labeled data.
This way, we can generate better ?training? ex-
amples during the semi-supervised learning process.
The core of our approach, (1), is described in Sec-
tion 5. The task is described in Section 3 and the
Experimental study in Section 6. It is shown there
that the improvement on the training examples via
the constraints indeed boosts the learned model and
the proposed method significantly outperforms the
traditional semi-supervised framework.
2 Related Work
In the semi-supervised domain there are two main
approaches for injecting domain specific knowledge.
One is using the prior knowledge to accurately tailor
the generative model so that it captures the domain
structure. For example, (Grenager et al, 2005) pro-
poses Diagonal Transition Models for sequential la-
beling tasks where neighboring words tend to have
the same labels. This is done by constraining the
HMM transition matrix, which can be done also for
other models, such as CRF. However (Roth and Yih,
2005) showed that reasoning with more expressive,
non-sequential constraints can improve the perfor-
mance for the supervised protocol.
A second approach has been to use a small high-
accuracy set of labeled tokens as a way to seed and
bootstrap the semi-supervised learning. This was
used, for example, by (Thelen and Riloff, 2002;
Collins and Singer, 1999) in information extraction,
and by (Smith and Eisner, 2005) in POS tagging.
(Haghighi and Klein, 2006) extends the dictionary-
based approach to sequential labeling tasks by prop-
agating the information given in the seeds with con-
textual word similarity. This follows a conceptually
similar approach by (Cohen and Sarawagi, 2004)
that uses a large named-entity dictionary, where the
similarity between the candidate named-entity and
its matching prototype in the dictionary is encoded
as a feature in a supervised classifier.
In our framework, dictionary lookup approaches
are viewed as unary constraints on the output states.
We extend these kinds of constraints and allow for
more general, n-ary constraints.
In the supervised learning setting it has been es-
tablished that incorporating global information can
significantly improve performance on several NLP
tasks, including information extraction and semantic
role labeling. (Punyakanok et al, 2005; Toutanova
et al, 2005; Roth and Yih, 2005). Our formalism
is most related to this last work. But, we develop a
semi-supervised learning protocol based on this for-
malism. We also make use of soft constraints and,
furthermore, extend the notion of soft constraints to
account for multiple levels of constraints? violation.
Conceptually, although not technically, the most re-
lated work to ours is (Shen et al, 2005) that, in
a somewhat ad-hoc manner uses soft constraints to
guide an unsupervised model that was crafted for
mention tracking. To the best of our knowledge,
we are the first to suggest a general semi-supervised
protocol that is driven by soft constraints.
We propose learning with constraints - a frame-
work that combines the approaches described above
in a unified and intuitive way.
3 Tasks, Examples and Datasets
In Section 4 we will develop a general framework
for semi-supervised learning with constraints. How-
ever, it is useful to illustrate the ideas on concrete
problems. Therefore, in this section, we give a brief
introduction to the two domains on which we tested
our algorithms. We study two information extrac-
tion problems in each of which, given text, a set of
pre-defined fields is to be identified. Since the fields
are typically related and interdependent, these kinds
of applications provide a good test case for an ap-
proach like ours.1
The first task is to identify fields from citations
(McCallum et al, 2000) . The data originally in-
cluded 500 labeled references, and was later ex-
tended with 5,000 unannotated citations collected
from papers found on the Internet (Grenager et al,
2005). Given a citation, the task is to extract the
1The data for both problems is available at:
http://www.stanford.edu/ grenager/data/unsupie.tgz
281
(a) [ AUTHOR Lars Ole Andersen . ] [ TITLE Program analysis and specialization for the C programming language . ] [TECH-REPORT PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , ] [ DATE May 1994 . ]
(b) [ AUTHOR Lars Ole Andersen . Program analysis and ] [TITLE specialization for the ] [EDITOR C ] [ BOOKTITLE
Programming language ] [ TECH-REPORT . PhD thesis , ] [ INSTITUTION DIKU , University of Copenhagen , May ] [ DATE
1994 . ]
Figure 1: Error analysis of a HMM model. The labels are annotated by underline and are to the right of
each open bracket. The correct assignment was shown in (a). While the predicted label assignment (b) is
generally coherent, some constraints are violated. Most obviously, punctuation marks are ignored as cues
for state transitions. The constraint ?Fields cannot end with stop words (such as ?the?)? may be also good.
fields that appear in the given reference. See Fig. 1.
There are 13 possible fields including author, title,
location, etc.
To gain an insight to how the constraints can guide
semi-supervised learning, assume that the sentence
shown in Figure 1 appears in the unlabeled data
pool. Part (a) of the figure shows the correct la-
beled assignment and part (b) shows the assignment
labeled by a HMM trained on 30 labels. However,
if we apply the constraint that state transition can
occur only on punctuation marks, the same HMM
model parameters will result in the correct labeling
(a). Therefore, by adding the improved labeled as-
signment we can generate better training samples
during semi-supervised learning. In fact, the punc-
tuation marks are only some of the constraints that
can be applied to this problem. The set of constraints
we used in our experiments appears in Table 1. Note
that some of the constraints are non-local and are
very intuitive for people, yet it is very difficult to
inject this knowledge into most models.
The second problem we consider is extracting
fields from advertisements (Grenager et al, 2005).
The dataset consists of 8,767 advertisements for
apartment rentals in the San Francisco Bay Area
downloaded in June 2004 from the Craigslist web-
site. In the dataset, only 302 entries have been la-
beled with 12 fields, including size, rent, neighbor-
hood, features, and so on. The data was prepro-
cessed using regular expressions for phone numbers,
email addresses and URLs. The list of the con-
straints for this domain is given in Table 1. We im-
plement some global constraints and include unary
constraints which were largely imported from the
list of seed words used in (Haghighi and Klein,
2006). We slightly modified the seedwords due to
difference in preprocessing.
4 Notation and Definitions
Consider a structured classification problem, where
given an input sequence x = (x1, . . . , xN ), the taskis to find the best assignment to the output variables
y = (y1, . . . , yM ). We denote X to be the space ofthe possible input sequences and Y to be the set of
possible output sequences.
We define a structured output classifier as a func-
tion h : X ? Y that uses a global scoring function
f : X ?Y ? R to assign scores to each possible in-
put/output pair. Given an input x, a desired function
f will assign the correct output y the highest score
among all the possible outputs. The global scoring
function is often decomposed as a weighted sum of
feature functions,
f(x, y) =
M
?
i=1
?ifi(x, y) = ? ? F (x, y).
This decomposition applies both to discriminative
linear models and to generative models such as
HMMs and CRFs, in which case the linear sum
corresponds to log likelihood assigned to the in-
put/output pair by the model (for details see (Roth,
1999) for the classification case and (Collins, 2002)
for the structured case). Even when not dictated by
the model, the feature functions fi(x, y) used arelocal to allow inference tractability. Local feature
function can capture some context for each input or
output variable, yet it is very limited to allow dy-
namic programming decoding during inference.
Now, consider a scenario where we have a set
of constraints C1, . . . , CK . We define a constraint
C : X ? Y ? {0, 1} as a function that indicates
whether the input/output sequence violates some de-
sired properties. When the constraints are hard, the
solution is given by
argmax
y?1C(x)
? ? F (x, y),
282
(a)-Citations
1) Each field must be a consecutive list of words, and can
appear at most once in a citation.
2) State transitions must occur on punctuation marks.
3) The citation can only start with author or editor.
4) The words pp., pages correspond to PAGE.
5) Four digits starting with 20xx and 19xx are DATE.
6) Quotations can appear only in titles.
7) The words note, submitted, appear are NOTE.
8) The words CA, Australia, NY are LOCATION.
9) The words tech, technical are TECH REPORT.
10) The words proc, journal, proceedings, ACM are JOUR-NAL or BOOKTITLE.
11) The words ed, editors correspond to EDITOR.
(b)-Advertisements
1) State transitions can occur only on punctuation marks or
the newline symbol.
2) Each field must be at least 3 words long.
3) The words laundry, kitchen, parking are FEATURES.
4) The words sq, ft, bdrm are SIZE.
5) The word $, *MONEY* are RENT.
6) The words close, near, shopping are NEIGHBORHOOD.
7) The words laundry kitchen, parking are FEATURES.
8) The (normalized) words phone, email are CONTACT.
9) The words immediately, begin, cheaper are AVAILABLE.
10) The words roommates, respectful, drama are ROOM-MATES.
11) The words smoking, dogs, cats are RESTRICTIONS.
12) The word http, image, link are PHOTOS.
13) The words address, carlmont, st, cross are ADDRESS.
14) The words utilities, pays, electricity are UTILITIES.
Table 1: The list of constraints for extracting fields
from citations and advertisements. Some constraints
(represented in the first block of each domain) are
global and are relatively difficult to inject into tradi-
tional models. While all the constraints hold for the
vast majority of the data, some of them are violated
by some correct labeled assignments.
where 1C(x) is a subset of Y for which all Ci as-sign the value 1 for the given (x, y).
When the constraints are soft, we want to in-
cur some penalty for their violation. Moreover, we
want to incorporate into our cost function a mea-
sure for the amount of violation incurred by vi-
olating the constraint. A generic way to capture
this intuition is to introduce a distance function
d(y, 1Ci(x)) between the space of outputs that re-spect the constraint,1Ci(x), and the given output se-quence y. One possible way to implement this dis-
tance function is as the minimal Hamming distance
to a sequence that respects the constraint Ci, that is:
d(y, 1Ci(x)) = min(y??1C(x)) H(y, y?). If the penaltyfor violating the soft constraint Ci is ?i, we write the
score function as:
argmax
y
? ? F (x, y) ?
K
?
i=1
?id(y, 1Ci(x)) (1)
We refer to d(y, 1C(x)) as the valuation of theconstraint C on (x, y). The intuition behind (1) is as
follows. Instead of merely maximizing the model?s
likelihood, we also want to bias the model using
some knowledge. The first term of (1) is used to
learn from data. The second term biases the mode
by using the knowledge encoded in the constraints.
Note that we do not normalize our objective function
to be a true probability distribution.
5 Learning and Inference with Constraints
In this section we present a new constraint-driven
learning algorithm (CODL) for using constraints to
guide semi-supervised learning. The task is to learn
the parameter vector ? by using the new objective
function (1). While our formulation allows us to
train also the coefficients of the constraints valua-
tion, ?i, we choose not to do it, since we view this asa way to bias (or enforce) the prior knowledge into
the learned model, rather than allowing the data to
brush it away. Our experiments demonstrate that the
proposed approach is robust to inaccurate approxi-
mation of the prior knowledge (assigning the same
penalty to all the ?i ).We note that in the presence of constraints, the
inference procedure (for finding the output y that
maximizes the cost function) is usually done with
search techniques (rather than Viterbi decoding,
see (Toutanova et al, 2005; Roth and Yih, 2005) for
a discussion), we chose beamsearch decoding.
The semi-supervised learning with constraints is
done with an EM-like procedure. We initialize the
model with traditional supervised learning (ignoring
the constraints) on a small labeled set. Given an un-
labeled set U , in the estimation step, the traditional
EM algorithm assigns a distribution over labeled as-
signmentsY of each x ? U , and in the maximization
step, the set of model parameters is learned from the
distributions assigned in the estimation step.
However, in the presence of constraints, assigning
the complete distributions in the estimation step is
infeasible since the constraints reshape the distribu-
tion in an arbitrary way. As in existing methods for
training a model by maximizing a linear cost func-
tion (maximize likelihood or discriminative maxi-
283
mization), the distribution over Y is represented as
the set of scores assigned to it; rather than consid-
ering the score assigned to all y?s, we truncate the
distribution to the top K assignments as returned
by the search. Given a set of K top assignments
y1, . . . , yK , we approximate the estimation step by
assigning uniform probability to the top K candi-
dates, and zero to the other output sequences. We
denote this algorithm top-K hard EM. In this pa-
per, we use beamsearch to generate K candidates
according to (1).
Our training algorithm is summarized in Figure 2.
Several things about the algorithm should be clari-
fied: the Top-K-Inference procedure in line 7, the
learning procedure in line 9, and the new parameter
estimation in line 9.
The Top-K-Inference is a procedure that returns
the K labeled assignments that maximize the new
objective function (1). In our case we used the top-
K elements in the beam, but this could be applied
to any other inference procedure. The fact that the
constraints are used in the inference procedure (in
particular, for generating new training examples) al-
lows us to use a learning algorithm that ignores the
constraints, which is a lot more efficient (although
algorithms that do take the constraints into account
can be used too). We used maximum likelihood es-
timation of ? but, in general, perceptron or quasi-
Newton can also be used.
It is known that traditional semi-supervised train-
ing can degrade the learned model?s performance.
(Nigam et al, 2000) has suggested to balance the
contribution of labeled and unlabeled data to the pa-
rameters. The intuition is that when iteratively esti-
mating the parameters with EM, we disallow the pa-
rameters to drift too far from the supervised model.
The parameter re-estimation in line 9, uses a similar
intuition, but instead of weighting data instances, we
introduced a smoothing parameter ? which controls
the convex combination of models induced by the la-
beled and the unlabeled data. Unlike the technique
mentioned above which focuses on naive Bayes, our
method allows us to weight linear models generated
by different learning algorithms.
Another way to look the algorithm is from the
self-training perspective (McClosky et al, 2006).
Similarly to self-training, we use the current model
to generate new training examples from the unla-
Input:
Cycles: learning cycles
Tr = {x, y}: labeled training set.
U : unlabeled dataset
F : set of feature functions.
{?i}: set of penalties.
{Ci}: set of constraints.
?: balancing parameter with the supervised model.
learn(Tr, F ): supervised learning algorithm
Top-K-Inference:
returns top-K labeled scored by the cost function (1)CODL:
1. Initialize ?0 = learn(Tr, F ).2. ? = ?0.3. For Cycles iterations do:
4. T = ?
5. For each x ? U
6. {(x, y1), . . . , (x, yK)} =
7. Top-K-Inference(x, ?, F, {Ci}, {?i})
8. T = T ? {(x, y1), . . . , (x, yK)}
9. ? = ??0 + (1 ? ?)learn(T, F )
Figure 2: COnstraint Driven Learning (CODL). In
Top-K-Inference, we use beamsearch to find the K-
best solution according to Eq. (1).
beled set. However, there are two important differ-
ences. One is that in self-training, once an unlabeled
sample was labeled, it is never labeled again. In
our case all the samples are relabeled in each iter-
ation. In self-training it is often the case that only
high-confidence samples are added to the labeled
data pool. While we include all the samples in the
training pool, we could also limit ourselves to the
high-confidence samples. The second difference is
that each unlabeled example generates K labeled in-
stances. The case of one iteration of top-1 hard EM
is equivalent to self training, where all the unlabeled
samples are added to the labeled pool.
There are several possible benefits to using K > 1
samples. (1) It effectively increases the training set
by a factor of K (albeit by somewhat noisy exam-
ples). In the structured scenario, each of the top-K
assignments is likely to have some good components
so generating top-K assignments helps leveraging
the noise. (2) Given an assignment that does not sat-
isfy some constraints, using top-K allows for mul-
tiple ways to correct it. For example, consider the
output 11101000 with the constraint that it should
belong to the language 1?0?. If the two top scoring
corrections are 11111000 and 11100000, consider-
ing only one of those can negatively bias the model.
284
6 Experiments and Results
In this section, we present empirical results of our
algorithms on two domains: citations and adver-
tisements. Both problems are modeled with a sim-
ple token-based HMM. We stress that token-based
HMM cannot represent many of our constraints. The
function d(y, 1C(x)) used is an approximation of aHamming distance function, discussed in Section 7.
For both domains, and all the experiments, ? was
set to 0.1. The constraints violation penalty ? is set
to ? log 10?4 and ? log 10?1 for citations and ad-
vertisements, resp.2 Note that all constraints share
the same penalty. The number of semi-supervised
training cycles (line 3 of Figure 2) was set to 5. The
constraints for the two domains are listed in Table 1.
We trained models on training sets of size vary-
ing from 5 to 300 for the citations and from 5 to
100 for the advertisements. Additionally, in all the
semi-supervised experiments, 1000 unlabeled exam-
ples are used. We report token-based3 accuracy on
100 held-out examples (which do not overlap neither
with the training nor with the unlabeled data). We
ran 5 experiments in each setting, randomly choos-
ing the training set. The results reported below are
the averages over these 5 runs.
To verify our claims we implemented several
baselines. The first baseline is the supervised learn-
ing protocol denoted by sup. The second baseline
was a traditional top-1 Hard EM also known as
truncated EM4 (denoted by H for Hard). In the third
baseline, denoted H&W, we balanced the weight
of the supervised and unsupervised models as de-
scribed in line 9 of Figure 2. We compare these base-
lines to our proposed protocol, H&W&C, where we
added the constraints to guide the H&W protocol.
We experimented with two flavors of the algorithm:
the top-1 and the top-K version. In the top-K ver-
sion, the algorithm uses K-best predictions (K=50)
for each instance in order to update the model as de-
scribed in Figure 2.
The experimental results for both domains are in
given Table 2. As hypothesized, hard EM sometimes
2The guiding intuition is that ?F (x, y) corresponds to a log-
likelihood of a HMM model and ? to a crude estimation of the
log probability that a constraint does not hold. ? was tuned on
a development set and kept fixed in all experiments.
3Each token (word or punctuation mark) is assigned a state.
4We also experimented with (soft) EM without constraints,
but the results were generally worse.
(a)- Citations
N Inf. sup. H H&W H&W&C H&W&C
(Top-1) (Top-K)
5 no I 55.1 60.9 63.6 70.6 71.0
I 66.6 69.0 72.5 76.0 77.8
10 no I 64.6 66.8 69.8 76.5 76.7
I 78.1 78.1 81.0 83.4 83.8
15 no I 68.7 70.6 73.7 78.6 79.4
I 81.3 81.9 84.1 85.5 86.2
20 no I 70.1 72.4 75.0 79.6 79.4
I 81.1 82.4 84.0 86.1 86.1
25 no I 72.7 73.2 77.0 81.6 82.0
I 84.3 84.2 86.2 87.4 87.6
300 no I 86.1 80.7 87.1 88.2 88.2
I 92.5 89.6 93.4 93.6 93.5
(b)-Advertisements
N Inf. sup. H H&W H&W&C H&W&C
(Top-1) (Top-K)
5 no I 55.2 61.8 60.5 66.0 66.0
I 59.4 65.2 63.6 69.3 69.6
10 no I 61.6 69.2 67.0 70.8 70.9
I 66.6 73.2 71.6 74.7 74.7
15 no I 66.3 71.7 70.1 73.0 73.0
I 70.4 75.6 74.5 76.6 76.9
20 no I 68.1 72.8 72.0 74.5 74.6
I 71.9 76.7 75.7 77.9 78.1
25 no I 70.0 73.8 73.0 74.9 74.8
I 73.7 77.7 76.6 78.4 78.5
100 no I 76.3 76.2 77.6 78.5 78.6
I 80.4 80.5 81.2 81.8 81.7
Table 2: Experimental results for extracting fields
from citations and advertisements. N is the number
of labeled samples. H is the traditional hard-EM and
H&W weighs labeled and unlabeled data as men-
tioned in Sec. 5. Our proposed model is H&W&C,
which uses constraints in the learning procedure. I
refers to using constraints during inference at eval-
uation time. Note that adding constraints improves
the accuracy during both learning and inference.
degrade the performance. Indeed, with 300 labeled
examples in the citations domain, the performance
decreases from 86.1 to 80.7. The usefulness of in-
jecting constraints in semi-supervised learning is ex-
hibited in the two right most columns: using con-
straints H&W&C improves the performance over
H&W quite significantly.
We carefully examined the contribution of us-
ing constraints to the learning stage and the testing
stage, and two separate results are presented: test-
ing with constraints (denoted I for inference) and
without constraints (no I). The I results are consis-
tently better. And, it is also clear from Table 2,
that using constraints in training always improves
285
the model and the amount of improvement depends
on the amount of labeled data.
Figure 3 compares two protocols on the adver-
tisements domain: H&W+I, where we first run the
H&W protocol and then apply the constraints dur-
ing testing stage, and H&W&C+I, which uses con-
straints to guide the model during learning and uses
it also in testing. Although injecting constraints in
the learning process helps, testing with constraints is
more important than using constraints during learn-
ing, especially when the labeled data size is large.
This confirms results reported for the supervised
learning case in (Punyakanok et al, 2005; Roth and
Yih, 2005). However, as shown, our proposed al-
gorithm H&W&C for training with constraints is
critical when the amount labeled data is small.
Figure 4 further strengthens this point. In the cita-
tions domain, H&W&C+I achieves with 20 labeled
samples similar performance to the supervised ver-
sion without constraints with 300 labeled samples.
(Grenager et al, 2005) and (Haghighi and Klein,
2006) also report results for semi-supervised learn-
ing for these domains. However, due to differ-
ent preprocessing, the comparison is not straight-
forward. For the citation domain, when 20 labeled
and 300 unlabeled samples are available, (Grenager
et al, 2005) observed an increase from 65.2% to
71.3%. Our improvement is from 70.1% to 79.4%.
For the advertisement domain, they observed no im-
provement, while our model improves from 68.1%
to 74.6% with 20 labeled samples. Moreover, we
successfully use out-of-domain data (web data) to
improve our model, while they report that this data
did not improve their unsupervised model.
(Haghighi and Klein, 2006) also worked on one of
our data sets. Their underlying model, Markov Ran-
dom Fields, allows more expressive features. Nev-
ertheless, when they use only unary constraints they
get 53.75%. When they use their final model, along
with a mechanism for extending the prototypes to
other tokens, they get results that are comparable to
our model with 10 labeled examples. Additionally,
in their framework, it is not clear how to use small
amounts of labeled data when available. Our model
outperforms theirs once we add 10 more examples.
 0.65
 0.7
 0.75
 0.8
 0.85
100252015105
H+N+I
H+N+C+I
Figure 3: Comparison between H&W+I and
H&W&C+I on the advertisements domain. When
there is a lot of labeled data, inference with con-
straints is more important than using constraints dur-
ing learning. However, it is important to train with
constraints when the amount of labeled data is small.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
100252015105
sup. (300)
H+N+C+I
Figure 4: With 20 labeled citations, our algorithm
performs competitively to the supervised version
trained on 300 samples.
7 Soft Constraints
This section discusses the importance of using soft
constraints rather than hard constraints, the choice
of Hamming distance for d(y, 1C(x)) and how weapproximate it. We use two constraints to illustrate
the ideas. (C1): ?state transitions can only occur onpunctuation marks or newlines?, and (C2): ?the field
TITLE must appear?.
First, we claim that defining d(y, 1C(x)) to bethe Hamming distance is superior to using a binary
value, d(y, 1C(x)) = 0 if y ? 1C(x) and 1 other-wise. Consider, for example, the constraint C1 inthe advertisements domain. While the vast majority
of the instances satisfy the constraint, some violate
it in more than one place. Therefore, once the binary
distance is set to 1, the algorithm looses the ability to
discriminate constraint violations in other locations
286
of the same instance. This may hurt the performance
in both the inference and the learning stage.
Computing the Hamming distance exactly can
be a computationally hard problem. Further-
more, it is unreasonable to implement the ex-
act computation for each constraint. Therefore,
we implemented a generic approximation for the
hamming distance assuming only that we are
given a boolean function ?C(yN ) that returnswhether labeling the token xN with state yN vio-lates constraint with respect to an already labeled
sequence (x1, . . . , xN?1, y1, . . . , yN?1). Then
d(y, 1C(x)) =
?N
i=1 ?C(yi). For example,consider the prefix x1, x2, x3, x4, which con-tains no punctuation or newlines and was labeled
AUTH, AUTH, DATE, DATE. This labeling
violates C1, the minimal hamming distance is 2, andour approximation gives 1, (since there is only one
transition that violates the constraint.)
For constraints which cannot be validated based
on prefix information, our approximation resorts to
binary violation count. For instance, the constraint
C2 cannot be implemented with prefix informationwhen the assignment is not complete. Otherwise, it
would mean that the field TITLE should appear as
early as possible in the assignment.
While (Roth and Yih, 2005) showed the signif-
icance of using hard constraints, our experiments
show that using soft constraints is a superior op-
tion. For example, in the advertisements domain,
C1 holds for the large majority of the gold-labeledinstances, but is sometimes violated. In supervised
training with 100 labeled examples on this domain,
sup gave 76.3% accuracy. When the constraint vio-
lation penalty ? was innity (equivalent to hard con-
straint), the accuracy improved to 78.7%, but when
the penalty was set to ?log(0.1), the accuracy of the
model jumped to 80.6%.
8 Conclusions and Future Work
We proposed to use constraints as a way to guide
semi-supervised learning. The framework devel-
oped is general both in terms of the representation
and expressiveness of the constraints, and in terms
of the underlying model being learned ? HMM in
the current implementation. Moreover, our frame-
work is a useful tool when the domain knowledge
cannot be expressed by the model.
The results show that constraints improve not
only the performance of the final inference stage but
also propagate useful information during the semi-
supervised learning process and that training with
the constraints is especially significant when the
number of labeled training data is small.
Acknowledgments: This work is supported by NSF SoD-
HCER-0613885 and by a grant from Boeing. Part of this work
was done while Dan Roth visited the Technion, Israel, sup-
ported by a Lady Davis Fellowship.
References
W. Cohen and S. Sarawagi. 2004. Exploiting dictionaries in
named entity extraction: Combining semi-markov extraction
processes and data integration methods. In Proc. of the ACMSIGKDD.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proc. of EMNLP.
M. Collins. 2002. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proc. of EMNLP.
T. Grenager, D. Klein, and C. Manning. 2005. Unsupervised
learning of field segmentation models for information extrac-
tion. In Proc. of the Annual Meeting of the ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learning for
sequence models. In Proc. of HTL-NAACL.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy markov models for information extraction and seg-
mentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proceedings of HLT-NAACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Text
classification from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103?134.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learn-
ing and inference over constrained output. In Proc. of IJCAI.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML.
D. Roth. 1999. Learning in natural language. In Proc. of IJCAI,
pages 898?904.
W. Shen, X. Li, and A. Doan. 2005. Constraint-based entity
matching. In Proc. of AAAI).
N. Smith and J. Eisner. 2005. Contrastive estimation: Training
log-linear models on unlabeled data. In Proc. of the AnnualMeeting of the ACL.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern contexts.
In Proc. of EMNLP.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of theAnnual Meeting of the ACL.
287
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 186?190, New York City, June 2006. c?2006 Association for Computational Linguistics
A Pipeline Model for Bottom-Up Dependency Parsing
Ming-Wei Chang Quang Do Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{mchang21, quangdo2, danr}@uiuc.edu
Abstract
We present a new machine learning frame-
work for multi-lingual dependency pars-
ing. The framework uses a linear, pipeline
based, bottom-up parsing algorithm, with
a look ahead local search that serves to
make the local predictions more robust.
As shown, the performance of the first
generation of this algorithm is promising.
1 System Description
1.1 Parsing as a Pipeline
Pipeline computation is a common computational
strategy in natural language processing, where a task
is decomposed into several stages that are solved se-
quentially. For example, a semantic role labeling
program may start by using a part-of-speech tagger,
than apply a shallow parser to chunk the sentence
into phrases, and continue by identifying predicates
and arguments and then classifying them.
(Yamada and Matsumoto, 2003) proposed a
bottom-up dependency parsing algorithm, where the
local actions, chosen from among Shift, Left, Right,
are used to generate a dependency tree using a
shift-reduce parsing approach. Moreover, they used
SVMs to learn the parsing decisions between pairs
of consecutive words in the sentences 1. This is
a true pipeline approach in that the classifiers are
trained on individual decisions rather than on the
overall quality of the parser, and chained to yield the
1A pair of words may become consecutive after the words
between them become the children of these two words
global structure. It suffers from the limitations of
pipeline processing, such as accumulation of errors,
but nevertheless, yields very competitive parsing re-
sults.
We devise two natural principles for enhancing
pipeline models. First, inference procedures should
be incorporated to make robust prediction for each
stage. Second, the number of predictions should
be minimized to prevent error accumulation. Ac-
cording to these two principles, we propose an im-
proved pipeline framework for multi-lingual depen-
dency parsing that aims at addressing the limitations
of the pipeline processing. Specifically, (1) we use
local search, a look ahead policy, to improve the ac-
curacy of the predicted actions, and (2) we argue that
the parsing algorithm we used minimizes the num-
ber of actions (Chang et al, 2006).
We use the set of actions: Shift, Left, Right, Wait-
Left, WaitRight for the parsing algorithm. The pure
Wait action was suggested in (Yamada and Mat-
sumoto, 2003). However, here we come up with
these five actions by separating actions Left into
(real) Left and WaitLeft, and Right into (real) Right
and WaitRight. Predicting these turns out to be eas-
ier due to finer granularity. We then use local search
over consecutive actions and better exploit the de-
pendencies among them.
The parsing algorithm is a modified shift-reduce
parser (Aho et al, 1986) that makes use of the ac-
tions described above and applies them in a left
to right manner on consecutive word pairs (a, b)
(a < b) in the word list T . T is initialized as the full
sentence. Latter, the actions will change the contents
of T . The actions are used as follows:
186
Shift: there is no relation between a and b.
Right: b is the parent of a,
Left: a is the parent of b
WaitLeft: a is the parent of b, but it?s possible that
b is a parent of other nodes. Action is deferred.
The actions control the procedure of building
trees. When Left or Right is performed, the algo-
rithm has found a parent and a child. Then, the func-
tion deleteWord will be called to eliminate the child
word, and the procedure will be repeated until the
tree is built. In projective languages, we discovered
that action WaitRight is not needed. Therefore, for
projective languages, we just need 4 actions.
In order to complete the description of the algo-
rithm we need to describe which pair of consecu-
tive words to consider once an action is taken. We
describe it via the notion of the focus point, which
represents the index of the current word in T . In
fact, determining the focus point does not affect the
correctness of the algorithm. It is easy to show that
any pair of consecutive words in the sentence can
be considered next. If the correct action is chosen
for the corresponding pair, this will eventually yield
the correct tree (but may necessitate multiple cycles
through the sentence).
In practice, however, the actions chosen will be
noisy, and a wasteful focus point policy will result
in a large number of actions, and thus in error accu-
mulation. To minimize the number of actions taken,
we want to find a good focus point placement policy.
There are many natural placement policies that we
can consider (Chang et al, 2006). In this paper, ac-
cording to the policy we used, after S and WL, the
focus point moves one word to the right. After L or
R, we adopt the policy Step Back: the focus moves
back one word to the left. Although the focus place-
ment policy here is similar to (Yamada and Mat-
sumoto, 2003), they did not explain why they made
this choice. In (Chang et al, 2006), we show that
the policy movement used here minimized the num-
ber of actions during the parsing procedure. We can
also show that the algorithm can parse a sentence
with projective relationships in only one round.
Once the parsing algorithm, along with the focus
point policy, is determined, we can train the action
classifiers. Given an annotated corpus, the parsing
algorithm is used to determine the action taken for
each consecutive pair; this is used to train a classifier
Algorithm 1 Pseudo Code of the dependency pars-
ing algorithm. getFeatures extracts the features
describing the currently considered pair of words;
getAction determines the appropriate action for the
pair; assignParent assigns the parent for the child
word based on the action; and deleteWord deletes the
word which become child once the action is taken.
Let t represents for a word and its part of speech
For sentence T = {t1, t2, . . . , tn}
focus= 1
while focus< |T | do
~v = getFeatures(tfocus, tfocus+1)
? = getAction(tfocus, tfocus+1, ~v)
if ? = L or ? = R then
assignParent(tfocus, tfocus+1, ?)
deleteWord(T, focus, ?)
// performing Step Back here
focus = focus ? 1
else
focus = focus + 1
end if
end while
to predict one of the four actions. The details of the
classifier and the features are given in Section 3.
When we apply the trained model on new data,
the sentence is processed from left to right to pro-
duce the predicted dependency tree. The evaluation
process is somewhat more involved, since the action
classifier is not used as it is, but rather via a local
search inference step. This is described in Section 2.
Algorithm 1 depicts the pseudo code of our parsing
algorithm.
Our algorithm is designed for projective lan-
guages. For non-projective relationships in some
languages, we convert them into near projective
ones. Then, we directly apply the algorithm on mod-
ified data in training stage. Because the sentences in
some language, such as Czech, etc. , may have multi
roots, in our experiment, we ran multiple rounds of
Algorithm 1 to build the tree.
1.2 Labeling the Type of Dependencies
In our work, labeling the type of dependencies is
a post-task after the phase of predicting the head
for the tokens in the sentences. This is a multi-
class classification task. The number of the de-
187
pendency types for each language can be found in
the organizer?s introduction paper of the shared task
of CoNLL-X. In the phase of learning dependency
types, the parent of the tokens, which was labeled
in the first phase, will be used as features. The pre-
dicted actions can help us to make accurate predic-
tions for dependency types.
1.3 Dealing with Crossing Edges
The algorithm described in previous section is pri-
marily designed for projective languages. To deal
with non-projective languages, we use a similar ap-
proach of (Nivre and Nilsson, 2005) to map non-
projective trees to projective trees. Any single
rooted projective dependency tree can be mapped
into a projective tree by the Lift operation. The
definition of Lift is as follows: Lift(wj ? wk) =
parent(wj) ? wk, where a ? b means that a is the
parent of b, and parent is a function which returns
the parent word of the given word. The procedure is
as follows. First, the mapping algorithm examines if
there is a crossing edge in the current tree. If there is
a crossing edge, it will perform Lift and replace the
edge until the tree becomes projective.
2 Local Search
The advantage of a pipeline model is that it can use
more information that is taken from the outcomes
of previous prediction. However, this may result in
accumulating error. Therefore, it is essential for our
algorithm to use a reliable action predictor. This mo-
tivates the following approach for making the local
prediction in a pipeline model more reliable. Infor-
mally, we devise a local search algorithm and use it
as a look ahead policy, when determining the pre-
dicted action.
In order to improve the accuracy, we might want
to examine all the combinations of actions proposed
and choose the one that maximizes the score. It is
clearly intractable to find the global optimal predic-
tion sequence in a pipeline model of the depth we
consider. The size of the possible action sequence
increases exponentially so that we can not examine
every possibility. Therefore, a local search frame-
work which uses additional information, however, is
suitable and tractable.
The local search algorithm is presented in Al-
Algorithm 2 Pseudo code for the local search al-
gorithm. In the algorithm, y represents the a action
sequence. The function search considers all possible
action sequences with |depth| actions and returns
the sequence with highest score.
Algo predictAction(model, depth, State)
x = getNextFeature(State)
y = search(x, depth, model, State)
lab = y[1]
State = update(State, lab)
return lab
Algo search(x, depth, model, State)
maxScore = ??
F = {y | ?y? = depth}
for y in F do
s = 0, TmpState = State
for i = 1 . . . depth do
x = getNextFeature(TmpState)
s = s + log(score(y[i], x))
TmpState = update(TmpState, y[i])
end for
if s > maxScore then
y? = y
maxScore = s
end if
end for
return y?
gorithm 2. The algorithm accepts two parameters,
model and depth. We assume a classifier that can
give a confidence in its prediction. This is repre-
sented here by model. depth is the parameter de-
termining the depth of the local search. State en-
codes the configuration of the environment (in the
context of the dependency parsing this includes the
sentence, the focus point and the current parent and
children for each node). Note that the features ex-
tracted for the action classifier depends on State, and
State changes by the update function when a predic-
tion is made. In this paper, the update function cares
about the child word elimination, relationship addi-
tion and focus point movement.
The search algorithm will perform a search of
length depth. Additive scoring is used to score the
sequence, and the first action in this sequence is per-
formed. Then, the State is updated, determining the
188
next features for the action classifiers and search is
called again.
One interesting property of this framework is that
we use future information in addition to past infor-
mation. The pipeline model naturally allows access
to all the past information. But, since our algorithm
uses the search as a look ahead policy, it can produce
more robust results.
3 Experiments and Results
In this work we used as our learning algorithm a
regularized variation of the perceptron update rule
as incorporated in SNoW (Roth, 1998; Carlson et
al., 1999), a multi-class classifier that is specifically
tailored for large scale learning tasks. SNoW uses
softmax over the raw activation values as its confi-
dence measure, which can be shown to be a reliable
approximation of the labels? probabilities. This is
used both for labeling the actions and types of de-
pendencies. There is no special language enhance-
ment required for each language. The resources pro-
vided for 12 languages are described in: (Hajic? et
al., 2004; Chen et al, 2003; Bo?hmova? et al, 2003;
Kromann, 2003; van der Beek et al, 2002; Brants
et al, 2002; Kawata and Bartels, 2000; Afonso et
al., 2002; Dz?eroski et al, 2006; Civit Torruella and
Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer et
al., 2003; Atalay et al, 2003).
3.1 Experimental Setting
The feature set plays an important role in the qual-
ity of the classifier. Basically, we used the same
feature set for the action selection classifiers and
for the label classifiers. In our work, each exam-
ple has average fifty active features. For each word
pair (w1, w2), we used their LEMMA, the POSTAG
and also the POSTAG of the children of w1 and
w2. We also included the LEMMA and POSTAG
of surrounding words in a window of size (2, 4).
We considered 2 words before w1 and 4 words af-
ter w2 (we agree with the window size in (Yamada
and Matsumoto, 2003)). The major difference of
our feature set compared with the one in (Yamada
and Matsumoto, 2003) is that we included the pre-
vious predicted action. We also added some con-
junctions of the above features to ensure expressive-
ness of the model. (Yamada and Matsumoto, 2003)
made use of the polynomial kernel of degree 2 so
they in fact use more conjunctive features. Beside
these features, we incorporated the information of
FEATS for the languages when it is available. The
columns in the data files we used for our work are
the LEMMA, POSTAG, and the FEATS, which is
treated as atomic. Due to time limitation, we did not
apply the local search algorithm for the languages
having the FEATS features.
3.2 Results
Table 1 shows our results on Unlabeled Attachment
Scores (UAS), Labeled Attachment Scores (LAS),
and Label Accuracy score (LAC) for 12 languages.
Our results are compared with the average scores
(AV) and the standard deviations (SD), of all the sys-
tems participating in the shared task of CoNLL-X.
Our average UAS for 12 languages is 83.54%
with the standard deviation 6.01; and 76.80% with
the standard deviation 9.43 for average LAS.
4 Analysis and Discussion
We observed that our UAS for Arabic is generally
lower than for other languages. The reason for the
low accuracy of Arabic is that the sentence is very
long. In the training data for Arabic, there are 25%
sentences which have more than 50 words. Since
we use a pipeline model in our algorithm, it required
more predictions to complete a long sentence. More
predictions in pipeline models may result in more
mistakes. We think that this explains our relatively
low Arabic result. Moreover, in our current system,
we use the same window size (2,4) for feature ex-
traction in all languages. Changing the windows size
seems to be a reasonable step when the sentences are
longer.
For Czech, one reason for our relatively low result
is that we did not use the whole training corpus due
to time limitation 2 . Actually, in our experiment
on the development set, when we increase the size
of training data in the training phase we got signif-
icantly higher result than the system trained on the
smaller data. The other problem for Czech is that
Czech is one of the languages with many types of
part of speech and dependency types, and also the
2Training our system for most languages takes 30 minutes
or 1 hour for both phases of labeling HEAD and DEPREL. It
takes 6-7 hours for Czech with 50% training data.
189
Language UAS LAS LAC
Ours AV SD Ours AV SD Ours AV SD
Arabic 76.09 73.48 4.94 60.92 59.94 6.53 75.69 75.12 5.49
Chinese 89.60 84.85 5.99 85.05 78.32 8.82 87.28 81.66 7.92
Czech 81.78 77.01 6.70 72.88 67.17 8.93 80.42 76.59 7.69
Danish 86.85 84.52 8.97 80.60 78.31 11.34 86.51 84.50 4.35
Dutch 76.25 75.07 5.78 72.91 70.73 6.66 80.15 77.57 5.92
German 86.90 82.60 6.73 84.17 78.58 7.51 91.03 86.26 6.01
Japanese 90.77 89.05 5.20 89.07 85.86 7.09 92.18 89.90 5.36
Portuguese 88.60 86.46 4.17 83.99 80.63 5.83 88.84 85.35 5.45
Slovene 80.32 76.53 4.67 69.52 65.16 6.78 79.26 76.31 6.40
Spanish 83.09 77.76 7.81 79.72 73.52 8.41 89.26 85.71 4.56
Swedish 89.05 84.21 5.45 82.31 76.44 6.46 84.82 80.00 6.24
Turkish 73.15 69.35 5.51 60.51 55.95 7.71 73.75 69.59 7.94
Table 1: Our results are compared with the average scores. UAS=Unlabeled Attachment Score,
LAS=Labeled Attachment Score, LAC=Label Accuracy, AV=Average score, and SD=standard deviation.
length of the sentences in Czech is relatively long.
These facts make recognizing the HEAD and the
types of dependencies more difficult.
Another interesting aspect is that we have not
used the information about the syntactic and/or mor-
phological features (FEATS) properly. For the lan-
guages for which FEATS is available, we have a
larger gap, compared with the top system.
5 Further Work and Conclusion
In the shared task of CoNLL-X, we have shown that
our dependency parsing system can do well on mul-
tiple languages without requiring special knowledge
for each of the languages.
From a technical perspective, we have addressed
the problem of using learned classifiers in a pipeline
fashion, where a task is decomposed into several
stages and classifiers are used sequentially to solve
each stage. This is a common computational strat-
egy in natural language processing and is known to
suffer from error accumulation and an inability to
correct mistakes in previous stages. We abstracted
two natural principles, one which calls for making
the local classifiers used in the computation more
reliable and a second, which suggests to devise the
pipeline algorithm in such a way that it minimizes
the number of actions taken.
However, since we tried to build a single approach
for all languages, we have not fully utilized the capa-
bilities of our algorithms. In future work we will try
to specify both features and local search parameters
to the target language.
Acknowledgement This research is supported by
NSF ITR IIS-0428472, a DOI grant under the Reflex
program and ARDA?s Advanced Question Answer-
ing for Intelligence (AQUAINT) program.
References
A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, techniques, and tools. In Addison-Wesley
Publishing Company, Reading, MA.
A. Carlson, C. Cumby, J. Rosen, and D. Roth. 1999.
The SNoW learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer Science De-
partment, May.
M. Chang, Q. Do, and D. Roth. 2006. Local search
for bottom-up dependency parsing. Technical report,
UIUC Computer Science Department.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 806?813.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT2003.
190
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767?777,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
The Necessity of Combining Adaptation Methods
Ming-Wei Chang, Michael Connor and Dan Roth
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,connor2,danr}@uiuc.edu
Abstract
Problems stemming from domain adaptation
continue to plague the statistical natural lan-
guage processing community. There has been
continuing work trying to find general purpose
algorithms to alleviate this problem. In this
paper we argue that existing general purpose
approaches usually only focus on one of two
issues related to the difficulties faced by adap-
tation: 1) difference in base feature statistics
or 2) task differences that can be detected with
labeled data.
We argue that it is necessary to combine these
two classes of adaptation algorithms, using
evidence collected through theoretical analy-
sis and simulated and real-world data exper-
iments. We find that the combined approach
often outperforms the individual adaptation
approaches. By combining simple approaches
from each class of adaptation algorithm, we
achieve state-of-the-art results for both Named
Entity Recognition adaptation task and the
Preposition Sense Disambiguation adaptation
task. Second, we also show that applying an
adaptation algorithm that finds shared repre-
sentation between domains often impacts the
choice in adaptation algorithm that makes use
of target labeled data.
1 Introduction
While recent advances in statistical modeling for
natural language processing are exciting, the prob-
lem of domain adaptation remains a big challenge.
It is widely known that a classifier trained on one do-
main (e.g. news domain) usually performs poorly on
a different domain (e.g. medical domain) (Jiang and
Zhai, 2007; Daume? III, 2007). The inability of cur-
rent statistical models to handle multiple domains is
one of the key obstacles hindering the progress of
NLP.
Several general purpose algorithms have been
proposed to address the domain adaptation prob-
lem: (Blitzer et al, 2006; Jiang and Zhai, 2007;
Daume? III, 2007; Finkel and Manning, 2009). It
is widely believed that the drop in performance of
statistical models on new domains is due to the
shift of the joint distribution of labels and examples,
P (Y,X), from domain to domain, where X repre-
sents the input space and Y represents the output
space. In general, we can separate existing adap-
tation algorithms into two categories:
Focuses on P (X) This type of adaptation algo-
rithm attempts to resolve the difference between the
feature space statistics of two domains. While many
different techniques have been proposed, the com-
mon goal of these algorithms is to find a better
shared representation that brings the source domain
and the target domain closer. Often these algorithms
do not use labeled examples in the target domain.
The works (Blitzer et al, 2006; Huang and Yates,
2009) all belong to this category.
Focuses on P (Y |X) These adaptation algorithms
assume that there exists a small amount of labeled
data for the target domain. Instead of training two
weight vectors independently (one for source and
the other for the target domain), these algorithms try
to relate the source and target weight vectors. This is
often achieved by using a special designed regular-
ization term. The works (Chelba and Acero, 2004;
Daume? III, 2007; Finkel and Manning, 2009) belong
to this category.
767
It is important to give the definition of an adapta-
tion framework. An adaptation framework is speci-
fied by the data/resources used and a specific learn-
ing algorithm. For example, a framework that used
only source labeled examples and one that used both
source and target labeled examples should be con-
sidered as two different frameworks, even though
they might use exactly the same training algorithm.
Note that the goal of a good adaptation framework is
to perform well on the target domain and quite often
we only need to change the data/resource used to in-
crease the performance without changing the train-
ing algorithm. We refer to frameworks that do not
use target labeled data and focus on P (X) as Unla-
beled Adaptation Frameworks and refer to frame-
works that use algorithms that focus on P (Y |X) as
Labeled Adaptation Frameworks.
The major difference between unlabeled adapta-
tion frameworks and labeled adaptation frameworks
is the use of target labeled examples. Unlabeled
adaptation frameworks do not use target labeled ex-
amples1, while the labeled adaptation frameworks
make use of target labeled examples. Under this
definition, we consider that a model trained on both
source and target labeled examples (later referred as
S+T) is a labeled adaptation framework.
It is important to combine the labeled and unla-
beled adaptation frameworks for two reasons:
? Mutual Benefit: We analyze these two types
of frameworks and find that they address dif-
ferent adaptation issues. Therefore, it is often
beneficial to apply them together.
? Complex Interaction: Another, probably
more important issue, is that these two types
of frameworks are not independent. Different
representations will impact howmuch a labeled
adaptation algorithm can transfer information
between domains. Therefore, in order to have a
clear picture of what is the best labeled adapta-
tion framework, it is necessary to analyze these
two domain adaptation frameworks together.
In this paper, we assume we have both a small
amount of target labeled data and a large amount
1Note that we still use labeled data from source domain in
an unlabeled adaptation framework.
of unlabeled data so that we can perform both unla-
beled and labeled adaptation. The goal of our paper
is to point out the necessity of applying these two
adaptation frameworks together. To the best of our
knowledge, this is the first paper that both theoreti-
cally and empirically analyzes the interdependence
between the impact of labeled and unlabeled adap-
tation frameworks.
The contribution of this paper is as follows:
? Propose a theoretical analysis of the ?Frustrat-
ingly Easy? (FE) framework (Daume? III, 2007)
(Section 3).
The theoretical analysis shows that for FE to be
effective the domains must already be ?close?.
At some threshold of ?closeness? it is better to
switch from FE to just pool all training together
as one domain.
? Demonstrate the complex interaction between
unlabeled and labeled approaches (Section 4)
We construct artificial experiments that demon-
strate how applying unlabeled adaptation may
impact the behavior of two labeled adaptation
approaches.
? Empirically analyze the interaction on real
datasets (Section 5).
We show that in general combining both ap-
proaches on the tasks of preposition sense
disambiguation and named entity recognition
works better than either individual method.
Our approach not only achieves state-of-the-
art results on these two tasks but it also re-
veals something surprising ? finding a bet-
ter shared representation often makes a sim-
ple source+target approach the best adaptation
framework in practice.
2 Two Adaptation Aspects: A Review
Why do we need two types of adaptation frame-
works? First, unlabeled adaptation frameworks are
necessary since many features only exist in one do-
main. Therefore, it is important to develop algo-
rithms that find features which work across domains.
On the other hand, labeled adaptation frameworks
768
are also required because we would like to take ad-
vantages of target labeled data. Even though differ-
ent domains may have different definitions for la-
bels (say in named entity recognition, specific defi-
nition of PER/LOC/ORG may change), labeled data
should still be useful. We summarize these distinc-
tions in Table 1.
While these two aspects of adaptation both saw
significant progress in the past few years, little anal-
ysis has been done on the interaction between these
two types of algorithms2.
In order to have a deep analysis, it is necessary to
choose specific adaptation algorithms for each as-
pect of adaptation framework. While we mainly
conduct analysis on the algorithms we picked, we
would like to point out that the necessity of com-
bining these two types of adaptation algorithms has
been largely ignored in the community.
As our example adaptation algorithms we se-
lected:
Labeled adaptation: FE framework One of the
most popular adaptation frameworks that requires
the use of labeled target data is the ?Frustrat-
ingly Easy? (FE) adaptation framework (Daume? III,
2007). However, why and when this framework
works remains unclear in the NLP community. The
FE framework can be viewed as an framework that
extends the feature space, and it requires source and
target labeled data to work. We denote n as the
total number of features3 and m is the number of
the ?domains?, where one of the domains is the tar-
get domain. The FE framework creates a global
weight vector in Rn(m+1), an extended space for all
domains. The representation x of the t-th domain
is mapped by ?t(x) ? Rn(m+1). In the extended
space, the first n features consist of the ?shared?
block, which is always active across all tasks. The
(t+1)-th block (the (nt+1)-th to the (nt+n)-th fea-
tures) is a ?specific? block, and is only active when
2Among the previously mentioned work, (Jiang and Zhai,
2007) is a special case given that it discusses both aspects of
adaptation algorithms. However, little analysis on the interac-
tion of the two aspects is discussed in that paper
3We assume that the number of features in each domain is
equal.
extracting examples from the task t. More formally,
?t(x) =
2
6
4 x|{z}
shared
(t?1) blocks
z }| {
0 . . .0 x
|{z}
specific
(m?t) blocks
z }| {
0 . . .0
3
7
5 . (1)
A single weight vector w? is obtained by training on
the modified labeled data {yti ,?t(x
t
i)}
m
t=1. Given
that this framework only extends the feature space,
in this paper, we also call it the feature extension
framework (still called FE). We will see in Section 3
that this framework is equivalent to applying a reg-
ularization trick that bridges the source and the tar-
get domains. As it will become clear in Section 3,
in fact, this framework is only effective when there
is target labeled data and hence belongs to labeled
adaptation frameworks.
Although FE framework is quite popular in the
community, there are other even simpler labeled
adaptation frameworks that allow the use of tar-
get labeled data. For example, one of the simplest
frameworks is the S+T framework, which simply
trains a single model on the pooled and unextended
source and target training data.
Unlabeled adaptation: Adding cluster-like fea-
tures Recall that unlabeled adaptation frameworks
find the features that ?work? across domain. In this
paper, we find such features in two steps. First,
we use word clusters generated from unlabeled text
and/or third party resources that spans domains.
Then, for every feature template that contains a
word, we append another feature template that uses
the word?s cluster instead of the word itself. This
technique is used in many recent works including
dependency parsing and NER (Koo et al, 2008;
Ratinov and Roth, 2009). Note that the unlabeled
text need not come from the source or target do-
main. In fact, in this paper, we use clusters gen-
erated with the Reuters 1996 dataset, a superset of
the CoNLL03 NER dataset (Koo et al, 2008; Liang,
2005). We adopt the Brown cluster algorithm to find
the word cluster (Brown et al, 1992; Liang, 2005).
We can use other resources to create clusters as well.
For example, in the NER domain, we also include
gazetteers4 as an unlabeled cluster resource, which
can bring the domains together quite effectively.
4Our gazetteers comes from (Ratinov and Roth, 2009).
769
Framework Labeled Data Unlabeled Data Common Approach
Unlabeled Adaptation
(Focus on P (X))
Source Encompasses Source and Target.
May use other third party resources
(dictionaries, gazetteers, etc.).
Generate features that span domains us-
ing unlabeled data and/or third party re-
sources.
Labeled Adaptation
(Focus on P (Y |X))
Source and Target None Train classifier(s) using both source and
target training data, relating the two.
Table 1: Comparison between two general adaptation frameworks discussed in this paper. Each framework is specified by its setting
(data required) and its learning algorithm. Multiple previous adaptation approaches fit in one of either framework.
While other more complex algorithms (Ando and
Zhang, 2005; Blitzer et al, 2006) for finding bet-
ter shared representation (without using labeled tar-
get data) have been proposed, we find that using
straightforward clustering features is quite effective
in general.
3 Analysis of the FE Framework
In this section, we propose a simple yet informative
analysis of the FE algorithm from the perspective of
multi-task learning. Note that we ignore the effect
of unlabeled adaptation in this section, and focus on
the analysis of the FE framework as a representative
labeled adaptation framework.
3.1 Mistake Bound Analysis
While (Daume? III, 2007) proposed this framework
for adaptation, a very similar idea had been proposed
in (Evgeniou and Pontil, 2004) as a novel regular-
ization term for multitask learning with support vec-
tor machines. Assume that w1,w2, . . . ,wm are the
weight vector for the first domain to the m-th do-
main, respectively. The baseline approach is to as-
sume that each weight vector is independent. As-
sume that we adopt a SVM-like optimization prob-
lem that consider all m tasks, the baseline approach
is equivalent to using the following regularization
term in the objective function:
?m
t=1 ?wt?
2.
In (Evgeniou and Pontil, 2004; Daume? III, 2007),
they assume that wt = u + vt, for t = 1, . . .m,
where vt is the specific weight vector for t-th do-
main and u is a shared weight vector across all do-
mains. The new regularization term then becomes
?u?2 +
m?
t=1
?vt?2. (2)
Note that these two regularization terms are differ-
ent, given that the new regularization term makes
w1,w2, . . . ,wm not independent anymore. It fol-
lows that
wTt x = (u+ vt)
Tx = w?T?t(x),
where
w?T =
[
uT vT1 . . . v
T
m
]
.
and ?w??2 equals to Eq. (2). Therefore, we can think
feature extension framework as a learning frame-
work that adopts Eq. (2) as its regularization term.
The FE framework was in fact originally designed
for the problem of multitask learning so in the fol-
lowing, we propose a simple mistake bound analysis
based on the multitask setting, where we calculate
the mistakes on all domains5. We focus on multi-
task setting for two reasons: 1) the analysis is very
easy and intuitive, and 2) in Section 4.1, we empiri-
cally confirm that the analysis holds for the adapta-
tion setting.
In the following, we assume that the training
algorithm used in the FE framework is the on-
line perceptron learning algorithm (Novikoff, 1963).
This allows us to analyze the mistake bound of the
FE framework with the perceptron algorithm. The
bound can give us an insight on when and why one
should adopt the FE framework. By using the stan-
dard mistake bound theorem (Novikoff, 1963), we
show:
Theorem 1. Let Dt be the labeled data of domain t.
Assume that there exist w1,w2, . . . ,wm such that
ywTt x ? ?,?(x, y) ? Dt,
and assume that max(x,y)?Dt ?x? ? R
2,?t =
1 . . .m. Then, the number of mistakes made with
online perceptron training (Novikoff, 1963) and the
5In the adaptation setting, one generally only cares about the
performance on the target domain.
770
FE framework is bounded by
2R2
?2
(
m?
t=1
?wt?2 ?
?
?m
t=1 wt?
2
m + 1
). (3)
Proof. Define w? as a vector in Rn(m+1). We claim
that there exists a set Sw? such that for all w? ? Sw?,
w?T?t(x) = wTt x for any domain t = 1 . . .m. Note
that?t(x) is defined in Eq. (1). We can construct Sw?
in the following way:
Sw? = {
[
s (w1 ? s) . . . (wm ? s)
]
| s ? Rn},
where s is an arbitrary vector with n elements.
In order to obtain the best possible bound, we
would like to find the most compressed weight vec-
tor in Sw?, w? = minw??Sw? ?w??
2.
The optimization problem has an analytical solu-
tion:
?w??2 =
m?
t=1
?wt?2 ? ?
m?
t=1
wt?2/(m + 1).
The proof is completed by the standard mis-
take bound theorem and the following fact:
maxx ??t(x)?2 = 2maxx ?x?2 ? 2R2.
3.2 Mistake Bound Comparison
In the following, we would like to explore under
what circumstances the FE framework can work bet-
ter than individual models and the S+T framework
using Theorem 1. The analysis is done based on the
assumption that all frameworks use the perceptron
algorithm.
Before showing the bound analysis, note that the
framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization over
these three frameworks (FE, S+T, and the base-
line)6. However, our goal in this paper is different:
we try to provide a deep discussion onwhen and why
one should use a particular framework.
Here, we compare the mistake bounds of the fea-
ture sharing framework to that of the baseline ap-
proach, which learns each task independently7. In
6The framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization of Eq. (1). It
allows the user to weight each block of features. If we put zero
weight on the shared block, it becomes the baseline approach.
On the other hand, if we put zero weight on all task-specific
blocks, the framework becomes the S+T approach.
7Note that mistake bound results can be generalized to gen-
eralization bound results. See (Zhang, 2002).
order to make the comparison easier, we make some
simplifying assumptions. First, we assume that the
problem contains only two tasks, 1 and 2. We also
assume that ?w1? = ?w2? = a. These assump-
tions greatly reduce the complexity of the analysis
and can give us greater insight into the comparisons.
Following the assumptions and Theorem 1, the
mistake bound for the FE frameworks is
4(2? cos(w1,w2))R2a2/(3?2) (4)
This line of analysis leads to interesting bound com-
parisons for two cases. In the first case, we assume
that task 1 and task 2 are essentially the same. In the
second, more common case, we assume that they are
different.
First, when we know a priori that task 1 and task
2 are essentially the same, we can combine the train-
ing data from the two tasks and train them as a sin-
gle task. Therefore, given that we do not need to
expand the feature space, the number of mistakes is
now bounded by R2a2/?2. Note that this bound is
in fact better than (4) with cos(w1,w2) = 1. There-
fore, if we know a priori that these two tasks are the
same, training a single model is better than using the
feature shared approach.
In practice, it is often the case that the two tasks
are not the same. In this case, the number of mis-
takes of an independent approach on both task 1 and
2 will be bounded by the summation of the mistake
bounds of task 1 and task 2. Therefore, using the
independent approach, the number of mistakes for
the perceptron algorithm on both tasks is bounded
by 2R2a2/?2. The following results can be obtained
by directly comparing the two bounds,
Corollary 1. Assume there exists w1 and w2 which
separate D1 and D2 respectively with functional
margin ?, and ?w1? = ?w2? = a. In this case:
(4) will be smaller than the bound of individual ap-
proach, 2R2a2/?2, if and only if cos(w1,w2) =
(wT1 w2)/(?w1??w2?) >
1
2 .
If we assume that there is no difference in
P (X) between domains and hence we can treat
cos(w1,w2) as the similarity between two tasks, the
above argument suggests:
? If the two tasks are very different, the baseline
approach (building two models) is better than
FE and S+T.
771
? If the tasks are similar enough, FE is better than
baseline and S+T.
? If the tasks are almost the same, S+T becomes
better than FE and baseline.
In Section 4.1, we will evaluate whether these claims
can be justified empirically.
4 Artificial Data Experiment Study
In this section we will present artificial experiments.
We have two primary goals: 1) verifying the analysis
proposed in Section 3, and 2) showing that the repre-
sentation shift will impact the behavior of the FE al-
gorithm. The second point will be verified again in
the real world experiments in Section 5.
Data Generation In the following artificial ex-
periments we experiment with domain adaptation
by generating training and test data for two tasks,
source and target, where we can control the differ-
ence between task definitions. The general proce-
dure can be divided into two steps: 1) generating
weight vectors z1 and z2 (for source and target re-
spectively), and 2) randomly generating labeled in-
stances for training and testing using z1 and z2.
The different experiments start with the same ba-
sic z1 and z2, but then may alter these weights to
introduce task dissimilarities or similarities. The ba-
sic z1 and z2 are both generated by a multivariate
Gaussian distribution with mean z and a diagonal
covariance matrix ?I:
z1 ? N (z, ?I), z2 ? N (z, ?I),
where N is the normal distribution and z is random
vector with zero mean. Note that z is only used to
generate z1 and z2. There is one parameter, ?, that
controls the variance of the Gaussian distribution.
Hence we use ? to roughly control the ?angle? of z1
and z2. When ? is close to zero, z1 and z2 will be
very similar. On the other hand, when ? is large, z1
and z2 can be very different. In these experiments,
we vary ? between 0.01 and 5 so that we are exper-
imenting only with tasks where the weight the task
difference is the ?angle? or cosine between z1 and
z2. Once we obtain the z1 and z2, we normalize
them to the unit length.
After selecting z1 and z2, we then generate la-
beled instances (x, y) for the source task in the fol-
lowing way. For each example x, we randomly gen-
erate n binary features, where each feature has 20%
chance to be active. We then label the example by
y = sign(zT1 x),
The data for the target task is generated similarly
with z2. In these experiments, we fix the number of
features n to be 500 and generate 100 source train-
ing examples and 40 target training examples, along
with 1000 target testing examples. This matches the
reasonable case in NLP where there are more fea-
tures than training examples and each feature vector
is sparse. In all of the experiments, we report the
averaged testing error rate on the target testing data.
4.1 Experiment 1, FE algorithm
Goal The goal here is to verify our theoretical
analysis in Section 3. Note that we do not introduce
representation shift in this experiment and assume
that both source and target domains use exactly the
same features.
Result Figure 1(a) shows the performance of the
three training algorithms as variance decreases and
thus cosine between weight vectors (or measure of
task similarity) goes to 1. Note that FE labeled adap-
tation framework beats TGT once the task cosine
passes approximately 0.6. Initially FE slightly out-
performs S+T until the tasks are close enough to-
gether that it is better to treat all the data as coming
from one task. Note that while the experiments are
based on the adaptation setting, the results match our
analysis based on the multitask setting in Section 3.
4.2 Experiment 2, Unseen Features
Goal So far we have not considered the difference
in P (X) between domains. In the previous exper-
iment, we used only cosine as our task similarity
measurement to decide what is the best framework.
However, task similarity should consider the differ-
ence in both P (X) and P (Y |X), and the cosine
measurement is not sufficient for this. Here we con-
struct a simple example to show that even a simple
representation shift can change the behavior of the
labeled adaptation framework. This case shows that
S+T can be better than FE even when the tasks are
not similar according to the cosine measurement.
772
 0.3
 0.32
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Cosine
Tgt
S+T
FE
(a) Basic Similarity
 0.32
 0.325
 0.33
 0.335
 0.34
 0.345
 0.35
 0.355
 0.36
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Original Cosine
Tgt
S+T
FE
(b) Shared Features
Figure 1: Artificial Experiment comparing labeled adaptation performance vs. cosine between base weight vectors that defines
two tasks, before and after cross-domain shared features are added. Figure (a) shows results from experiment 1. For FE adaptation
algorithm to work the tasks need to be close (cosine > 0.6), and if the tasks are close enough (cosine ? 1, dividing line) then
it is better to just pool source and target training data together (the S+T algorithm). Figure (b) shows results for experiment 3
when shared features are added to the base weight vectors as used in experiment 1. Here the cosine similarity measure is between
the base task weight vectors before the shared features have been added. Both labeled adaptation algorithms effectively use the
shared features to improve over just training on target. With shared features added the dividing line where S+T improves over
FE decreases so even for tasks that are initially further apart, once clusters are added the S+T algorithm does better than FE. Each
point represents the average of 2000 training runs with random initial z1 and z2 generating data.
Result The second experiment deals with the case
where features may appear in only one domain but
should be treated like known features in the other
domain. An example of this are out of vocabulary
words that may not exist in a small target train-
ing task, but have synonyms in the source train-
ing data. In this case if we had features grouping
words (say by word meanings) then we would re-
cover this cross-domain information. In this experi-
ment we want to explore which adaptation algorithm
performs best before these features are applied.
To simulate this case we start with similar weight
vectors z1 and z2 (sampled with variance = 0.00001,
cos(z1,z2) ? 1), but then shift some set of dimen-
sions so that they represent features that appear only
in one domain.
z1 = (a1,b1) ? z?1 = (0,b1,a1)
z2 = (a2,b2) ? z?2 = (a2,b2,0)
By changing the ratio of the size of the dissimilar
subset a to the similar subset b we can make the
two weight vectors z?1 and z
?
2 more or less similar.
Using these two new weight vectors we can proceed
as above, generating training and testing data.
Figure 2 shows the performance of the three algo-
 0.315
 0.32
 0.325
 0.33
 0.335
 0.34
 0.345
 0.35
 0.355
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Cosine
Tgt
S+T
FE
Figure 2: Artificial Experiment where unknown features are
included in source or target domains, but not the other. The
simple S+T adaptation framework is best able to exploit the
set of shared features so performs best over the whole space of
similarity in this setting.
rithms on this data as the number of unrelated fea-
tures are decreased. Over the entire range the com-
bined algorithm S+T does better since it more ef-
ficiently exploits the shared similar b subset of the
feature space. When the FE algorithm tries to cre-
ate the shared features, it considers both the similar
subset b and dissimilar subset a. However, since
a should not be shared, FE algorithm becomes less
773
effective than the S+T algorithm. See the bound
comparison in Section 3.2 for more intuitions. With
this experiment we have demonstrated that there is
a need to consider label and unlabeled adaptation
frameworks together.
4.3 Experiment 3, Shared Features
Goal A good unlabeled adaptation framework
should try to find features that ?work? across do-
mains. However, it is not clear how these newly
added features will impact the behavior of the la-
beled adaptation frameworks. In this experiment, we
show that the new shared features will bring the do-
mains together, and hence make S+T a very strong
adaptation framework.
Result For the third experiment we start with the
same setup as in the first experiment, but then aug-
ment the initial weight vector with additional shared
weights. These shared weights correspond to the in-
troduction of features that appear in both domains
and have the same meaning relative to the tasks, the
ideal result of unlabeled adaptation methods.
To generate this case we again start with z1 and
z2 of varying similarity as in section 4.1, then gen-
erate a random weight vector for shared features and
append this to both weight vectors.
zs ? N (0, I), z??1 = (z1, ?zs), z
??
2 = (z2, ?zs),
where ? is used to put increased importance on the
shared weight vectors by increasing the total weight
of that section relative to the base z1 and z2 subsets.
In our experiments we use 100 shared features to the
500 base features and set ? to 2.
Figure 1(b) shows the performance of the labeled
adaptation algorithms once shared features had been
added. Here the x-axis is the cosine between the
original task weight vectors, demonstrating how the
shared features improve performance on potentially
dissimilar tasks. Whereas in the first experiment
FE does not improve over just training on target data
until the cosine is greater than 0.6, once shared fea-
tures have been added then both FE and S+T use
these features to learn with originally dissimilar
tasks. Furthermore the shared features tend to push
the tasks ?closer? so that S+T improves over FE ear-
lier. Comparing to Figure 1(a), there are regions
where before shared features are added it is better
to use FE, and after shared features are added it is
better to use S+T. This shows that labeled adapta-
tion and unlabeled are not independent. Therefore,
it is important to combine these two aspects to see
the real contribution of each adaptation framework.
In these three artificial experiments we have
demonstrated cases where both FE or S+T are
the best algorithm before and after representation
changes like those created with unlabeled adaptation
are imposed. This fact points to the perhaps obvi-
ous conclusion that there is not a single best adapta-
tion algorithm, and the determination of specific best
practices depends on task similarity (in both P (X)
and P (Y |X)), especially after being brought closer
together with other adaptation approaches. If there
is one common trend it is that often once two tasks
have been brought close together using a shared rep-
resentation, then the tasks are now close enough
such that the simple S+T algorithm does well.
5 Real World Experiments
In Section 4, we have shown through artificial data
experiments that labeled and unlabeled adaptation
algorithms are not independent. In this section, we
focus on experiments with real datasets.
For the labeled adaptation algorithms, we have the
following options:
? TGT: Only uses target labeled training dataset.
? FE: Uses both labeled datasets.
? FE+: Uses both labeled datasets. A modifica-
tion of the FE algorithm, equivalent to multi-
plying the ?shared? part of the FE feature vec-
tor (Eq. (1)) by 10 (Finkel andManning, 2009).
? S+T: Uses both source and target labeled
datasets to train a single model with all labeled
data directly.
Throughout all of our experiments, we use SVMs
trained with a modified java implementation8 of
LIBLINEAR as our underlying learning classi-
fier (Hsieh et al, 2008). For the tasks that require
structures, we model each individual decision using
8Our code is modified from the version available on http:
//www.bwaldvogel.de/liblinear-java/
774
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Token F1
(a) MUC7 Dev 58.6 70.5 74.3 73.1
(a) + cluster 77.5 82.5 83.3 83.3
(b) MUC7 Train 73.0 78.2 80.1 78.7
(b) + cluster 85.4 86.4 86.2 86.5
Table 2: NER Experiments. We bold face the best accuracy
in a row and underline the runner up. Both unlabeled adapta-
tion algorithms (adding cluster features) and labeled adaptation
algorithm (using source labeled data) help the performance sig-
nificantly. Moreover, adding cluster-like features also changes
the behavior of the labeled adaptation algorithms. Note that
after adding cluster features, S+T becomes quite competitive
with (or slightly better than) the FE+ approach. The size of
MUC7 develop set is roughly 20% of the size of the MUC7
training set.
a local SVM classifier then make our prediction us-
ing a greedy approach from left to right. While we
could use a more complex model such as Condi-
tional Random Field (Lafferty et al, 2001), as we
will see later, our simple model generates state-of-
the-art results for many tasks. Regarding parameter
selection, we selected the SVM regularization pa-
rameter for the baseline model (TGT) and then fix it
for all algorithms9.
Named Entity Recognition Our first task is
Named Entity Recognition (NER). The source do-
main is from the CoNLL03 shared task (Tjong
Kim Sang and De Meulder, 2003) and the target do-
main is from the MUC7 dataset. The goal of this
adaptation system is to maximize the performance
on the test data of MUC7 dataset with CoNLL train-
ing data and (some) MUC7 labeled data. As an unla-
beled adaptation method to address feature sparsity,
we add cluster-like features based on the gazetteers
and word clustering resources used in (Ratinov and
Roth, 2009) to bridge the source and target domain.
We experiment with both MUC development and
training set as our target labeled sets.
The experimental results are in Table 2. First, no-
tice that addressing the feature sparsity issue helps
the performance significantly. Adding cluster-like
9We use L2-hinge loss for all of the experiments, with
C = 2?4 for NER experiments and C = 2?5 for the PSD
experiments.
features improves the Token-F1 by around 10%. On
the other hand, adding target labeled data also helps
the results significantly. Moreover, using both tar-
get labeled data and cluster-like shared representa-
tion are mutually beneficial in all cases.
Importantly, adding cluster-like features changes
the behavior of the labeled adaptation algorithms.
When the cluster-like features are not added, the
FE+ algorithm is in general the best labeled adap-
tation framework. This result agrees with the re-
sults showed in (Finkel and Manning, 2009), where
the authors show that FE+ is the best labeled adap-
tation framework in their settings. However, after
adding the cluster-like features, the simple S+T ap-
proach becomes very competitive to both FE and
FE+. This matches our analysis in Section 4: re-
solving features sparsity will change the behavior of
labeled adaptation frameworks.
We compare the simple S+T algorithm with
cluster-like features to other published results on
adapting from CoNLL dataset to MUC7 dataset in
table 3. Past works on this setting often only fo-
cus on one class of adaption approach. For example,
(Ratinov and Roth, 2009) only use the cluster-like
features to address the feature sparsity problem, and
(Finkel and Manning, 2009) only use target labeled
data without using gazetteers and word-cluster in-
formation. Notice that because of combining two
classes of adaption algorithms, our approach is sig-
nificantly better than these two systems10.
Preposition Sense Disambiguation We also test
the combination of unlabeled and labeled adaption
on the task of Preposition Sense Disambiguation.
Here the data contains multiple prepositions where
each preposition has many different senses. The
goal is to predict the right sense for a given prepo-
sition in the testing data. The source domain is the
SemEval 2007 preposition WSD Task and the target
domain is from the dataset annotated in (Dahlmeier
et al, 2009). Our feature design mainly comes
from (Tratz and Hovy, 2009) (who do not evalu-
ate their system on our target data). As our un-
10The work (Ratinov and Roth, 2009) also combines their
system with several document-level features. While it is possi-
ble to add these features in our system, we do not include any
global features for the sake of simplicity. Note that our sys-
tem is competitive to (Ratinov and Roth, 2009) even though our
system does not use global features.
775
Systems Cluster? TGT? P.F1 T.F1
Our NER y y 84.1 86.5
FM09 n y 79.98 N/A
RR09 y n N/A 83.2
RR09 + global y n N/A 86.2
Table 3: Comparisons between different NER systems. P.F1
and T.F1 represent the phrase-level and token-level F1 score,
respectively. We use ?Cluster?? to indicate if cluster features
are used and use ?TGT?? to indicate if target labeled data is
used. Previous systems often only use one class of adaptation
algorithms. Using both adaptation aspects makes our system
perform significantly better than FM09 and RR09.
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Accuracy
10% Tgt 43.8 48.2 51.3 49.7
10% Tgt + Cluster 44.9 50.5 51.8 52.0
100% Tgt 59.5 60.5 60.3 61.2
100% Tgt + Cluster 61.3 62.0 61.2 62.1
Table 4: Preposition Sense Disambiguation. We mark the best
accuracy in a row using the bold font and underline the runner
up. Note that both adding cluster features and adding source la-
beled data help the performance significantly. Moreover, adding
clusters also changes the behavior of the labeled adaptation al-
gorithms.
labeled adaptation approach we augment all word
based features with cluster information from sepa-
rately generated hierarchical Brown clusters (Brown
et al, 1992).
The experimental results are in Table 4. Note that
we see phenomena similar to what happened in the
NER experiments. First, both labeled and unlabeled
adaptation improves the system. When only 10% of
the target labeled data is used, the inclusion of the
source labeled data helps significantly. When there
is more labeled data, labeled and unlabeled adaption
have similar impact. Again, using unlabeled adap-
tion changes the behavior of the labeled adaption al-
gorithms.
In Table 5, we compare our system to (Dahlmeier
et al, 2009), who do not use the SemEval data but
jointly train their preposition sense disambiguation
system with a semantic role labeling system. With
both labeled and unlabeled adaption, our system is
significantly better.
Systems ACC
Our PSD (S+T and cluster) 62.1
DNS09 56.5
DNS09 + SRL 58.8
Table 5: Comparison between different PSD systems. Note
that after adding cluster features and source labeled data with
S+T approach, our system outperforms the state-of-the-art sys-
tem proposed in (Dahlmeier et al, 2009), even though they
jointly learn a PSD and SRL system together.
6 Conclusion
In this paper, we point out the necessities of com-
bining labeled and unlabeled adaptation algorithms.
We analyzed the FE algorithm both theoretically
and empirically, demonstrating that it requires both
a minimal amount of task similarity to work, and
past a certain level of similarity other, simpler ap-
proaches are better. More importantly, through arti-
ficial data experiments we found that applying unla-
beled adaptation algorithms may change the behav-
ior of labeled adaptation algorithms as representa-
tions change, and hence affect the choice of labeled
adaptation algorithm. Experiments with real-world
datasets confirmed that combinations of both adap-
tation methods provide the best results, often allow-
ing the use of simple labeled adaptation approaches.
In the future, we hope to develop a joint algorithm
which addresses both labeled and unlabeled adapta-
tion at the same time.
Acknowledgment We thank Vivek Srikumar for provid-
ing the baseline implementation of preposition sense disam-
biguation. We also thank anonymous reviewers for their use-
ful comments. University of Illinois gratefully acknowledges
the support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government.
References
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res.
776
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. In KDD.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In NAACL.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Novikoff. 1963. On convergence proofs for percep-
trons. In Proceeding of the Symposium on the Mathe-
matical Theory of Automata.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003.
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In
NAACL.
Tong Zhang. 2002. Covering number bounds of certain
regularized linear function classes. J. Mach. Learn.
Res.
777
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429?437,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Discriminative Learning over Constrained Latent Representations
Ming-Wei Chang and Dan Goldwasser and Dan Roth and Vivek Srikumar
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang,goldwas1,danr,vsrikum2}@uiuc.edu
Abstract
This paper proposes a general learning frame-
work for a class of problems that require learn-
ing over latent intermediate representations.
Many natural language processing (NLP) de-
cision problems are defined over an expressive
intermediate representation that is not explicit
in the input, leaving the algorithm with both
the task of recovering a good intermediate rep-
resentation and learning to classify correctly.
Most current systems separate the learning
problem into two stages by solving the first
step of recovering the intermediate representa-
tion heuristically and using it to learn the final
classifier. This paper develops a novel joint
learning algorithm for both tasks, that uses the
final prediction to guide the selection of the
best intermediate representation. We evalu-
ate our algorithm on three different NLP tasks
? transliteration, paraphrase identification and
textual entailment ? and show that our joint
method significantly improves performance.
1 Introduction
Many NLP tasks can be phrased as decision prob-
lems over complex linguistic structures. Successful
learning depends on correctly encoding these (of-
ten latent) structures as features for the learning sys-
tem. Tasks such as transliteration discovery (Kle-
mentiev and Roth, 2008), recognizing textual en-
tailment (RTE) (Dagan et al, 2006) and paraphrase
identification (Dolan et al, 2004) are a few proto-
typical examples. However, the input to such prob-
lems does not specify the latent structures and the
problem is defined in terms of surface forms only.
Most current solutions transform the raw input into
a meaningful intermediate representation1, and then
encode its structural properties as features for the
learning algorithm.
Consider the RTE task of identifying whether the
meaning of a short text snippet (called the hypoth-
esis) can be inferred from that of another snippet
(called the text). A common solution (MacCartney
et al, 2008; Roth et al, 2009) is to begin by defining
an alignment over the corresponding entities, pred-
icates and their arguments as an intermediate rep-
resentation. A classifier is then trained using fea-
tures extracted from the intermediate representation.
The idea of using a intermediate representation also
occurs frequently in other NLP tasks (Bergsma and
Kondrak, 2007; Qiu et al, 2006).
While the importance of finding a good inter-
mediate representation is clear, emphasis is typi-
cally placed on the later stage of extracting features
over this intermediate representation, thus separat-
ing learning into two stages ? specifying the la-
tent representation, and then extracting features for
learning. The latent representation is obtained by an
inference process using predefined models or well-
designed heuristics. While these approaches often
perform well, they ignore a useful resource when
generating the latent structure ? the labeled data for
the final learning task. As we will show in this pa-
per, this results in degraded performance for the ac-
tual classification task at hand. Several works have
considered this issue (McCallum et al, 2005; Gold-
wasser and Roth, 2008b; Chang et al, 2009; Das
and Smith, 2009); however, they provide solutions
1In this paper, the phrases ?intermediate representation? and
?latent representation? are used interchangeably.
429
that do not easily generalize to new tasks.
In this paper, we propose a unified solution to the
problem of learning to make the classification deci-
sion jointly with determining the intermediate rep-
resentation. Our Learning Constrained Latent Rep-
resentations (LCLR) framework is guided by the in-
tuition that there is no intrinsically good intermedi-
ate representation, but rather that a representation is
good only to the extent to which it improves perfor-
mance on the final classification task. In the rest of
this section we discuss the properties of our frame-
work and highlight its contributions.
Learning over Latent Representations This pa-
per formulates the problem of learning over latent
representations and presents a novel and general so-
lution applicable to a wide range of NLP applica-
tions. We analyze the properties of our learning
solution, thus allowing new research to take advan-
tage of a well understood learning and optimization
framework rather than an ad-hoc solution. We show
the generality of our framework by successfully ap-
plying it to three domains: transliteration, RTE and
paraphrase identification.
Joint Learning Algorithm In contrast to most
existing approaches that employ domain specific
heuristics to construct intermediate representations
to learn the final classifier, our algorithm learns to
construct the optimal intermediate representation to
support the learning problem. Learning to represent
is a difficult structured learning problem however,
unlike other works that use labeled data at the in-
termediate level, our algorithm only uses the binary
supervision supplied for the final learning problem.
Flexible Inference Successful learning depends
on constraining the intermediate representation with
task-specific knowledge. Our framework uses the
declarative Integer Linear Programming (ILP) infer-
ence formulation, which makes it easy to define the
intermediate representation and to inject knowledge
in the form of constraints. While ILP has been ap-
plied to structured output learning, to the best of our
knowledge, this is the first work that makes use of
ILP in formalizing the general problem of learning
intermediate representations.
2 Preliminaries
We introduce notation using the Paraphrase Iden-
tification task as a running example. This is the bi-
nary classification task of identifying whether one
sentence is a paraphrase of another. A paraphrase
pair from the MSR Paraphrase corpus (Quirk et al,
2004) is shown in Figure 1. In order to identify
that the sentences paraphrase each other , we need
to align constituents of these sentences. One possi-
ble alignment is shown in the figure, in which the
dotted edges correspond to the aligned constituents.
An alignment can be specified using binary variables
corresponding to every edge between constituents,
indicating whether the edge is included in the align-
ment. Different activations of these variables induce
the space of intermediate representations.
The notification was first reported Friday by MSNBC.
MSNBC.com first reported the CIA request on Friday.
Figure 1: The dotted lines represent a possible intermediate
representation for the paraphrase identification task. Since dif-
ferent representation choices will impact the binary identifica-
tion decision directly, our approach chooses the representation
that facilitates the binary learning task.
To formalize this setting, let x denote the input
to a decision function, which maps x to {?1, 1}.
We consider problems where this decision depends
on an intermediate representation (for example, the
collection of all dotted edges in Figure 1), which can
be represented by a binary vector h.
In the literature, a common approach is to sepa-
rate the problem into two stages. First, a genera-
tion stage predicts h for each x using a pre-defined
model or a heuristic. This is followed by a learn-
ing stage, in which the classifier is trained using h.
In our example, if the generation stage predicts the
alignment shown, then the learning stage would use
the features computed based on the alignments. For-
mally, the two-stage approach uses a pre-defined in-
ference procedure that finds an intermediate repre-
sentation h?. Using features ?(x,h?) and a learned
weight vector ?, the example is classified as positive
if ?T?(x,h?) ? 0.
However, in the two stage approach, the latent
representation, which is provided to the learning al-
gorithm, is determined before learning starts, and
without any feedback from the final task. It is dic-
tated by the intuition of the developer. This approach
makes two implicit assumptions: first, it assumes
430
the existence of a ?correct? latent representation and,
second, that the model or heuristic used to generate
it is the correct one for the learning problem at hand.
3 Joint Learning with an Intermediate
Representation
In contrast to two-stage approaches, we use the
annotated data for the final classification task to
learn a suitable intermediate representation which,
in turn, helps the final classification.
Choosing a good representation is an optimization
problem that selects which of the elements (features)
of the representation best contribute to success-
ful classification given some legitimacy constraints;
therefore, we (1) set up the optimization framework
that finds legitimate representations (Section 3.1),
and (2) learn an objective function for this optimiza-
tion problem, such that it makes the best final deci-
sion (Section 3.2.)
3.1 Inference
Our goal is to correctly predict the final label
rather than matching a ?gold? intermediate repre-
sentation. In our framework, attempting to learn the
final decision drives both the selection of the inter-
mediate representation and the final predictions.
For each x, let ?(x) be the set of all substructures
of all possible intermediate representations. In Fig-
ure 1, this could be the set of all alignment edges
connecting the constituents of the sentences. Given
a vocabulary of such structures of sizeN , we denote
intermediate representations by h ? {0, 1}N , which
?select? the components of the vocabulary that con-
stitute the intermediate representation. We define
?s(x) to be a feature vector over the substructure
s, which is used to describe the characteristics of s,
and define a weight vector u over these features.
Let C denote the set of feasible intermediate repre-
sentations h, specified by means of linear constraints
over h. While ?(x) might be large, the set of those
elements in h that are active can be constrained by
controlling C. After we have learned a weight vec-
tor u that scores intermediate representations for the
final classification task, we define our decision func-
tion as
fu(x) = max
h?C
uT
?
s??(x)
hs?s(x), (1)
and classify the input as positive if fu(x) ? 0.
In Eq. (1), uT?s(x) is the score associated with
the substructure s, and fu(x) is the score for the en-
tire intermediate representation. Therefore, our de-
cision function fu(x) ? 0 makes use of the interme-
diate representation and its score to classify the in-
put. An input is labeled as positive if its underlying
intermediate structure allows it to cross the decision
threshold. The intermediate representation is cho-
sen to maximize the overall score of the input. This
design is especially beneficial for many phenomena
in NLP, where only positive examples have a mean-
ingful underlying structure. In our paraphrase iden-
tification example, good alignments generally exist
only for positive examples.
One unique feature of our framework is that we
treat Eq. (1) as an Integer Linear Programming
(ILP) instance. A concrete instantiation of this set-
ting to the paraphrase identification problem, along
with the actual ILP formulation is shown in Section
4.
3.2 Learning
We now present an algorithm that learns the
weight vector u. For a loss function ` : R ? R,
the goal of learning is to solve the following opti-
mization problem:
min
u
?
2
?u?2 +
?
i
` (?yifu(xi)) (2)
Here, ? is the regularization parameter. Substituting
Eq. (1) into Eq. (2), we get
min
u
?
2
?u?2+
?
i
`
?
??yi max
h?C
uT
?
s??(x)
hs?s(xi)
?
? (3)
Note that there is a maximization term inside the
global minimization problem, making Eq. (3) a non-
convex problem. The minimization drives u towards
smaller empirical loss while the maximization uses
u to find the best representation for each example.
The algorithm for Learning over Constrained La-
tent Representations (LCLR) is listed in Algorithm
1. In each iteration, first, we find the best feature
representations for all positive examples (lines 3-5).
This step can be solved with an off-the-shelf ILP
solver. Having fixed the representations for the pos-
itive examples, we update the u by solving Eq. (4)
at line 6 in the algorithm. It is important to observe
431
Algorithm 1 LCLR :The algorithm that optimizes (3)
1: initialize: u? u0
2: repeat
3: for all positive examples (xi, yi = 1) do
4: Find h?i ? arg maxh?C
?
s
hsuT?s(xi)
5: end for
6: Update u by solving
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?C
uT
?
s
hs?s(xi)) (4)
7: until convergence
8: return u
that for positive examples in Eq. (4), we use the in-
termediate representations h? from line 4.
Algorithm 1 satisfies the following property:
Theorem 1 If the loss function ` is a non-
decreasing function, then the objective function
value of Eq. (3) is guaranteed to decrease in every
iteration of Algorithm 1. Moreover, if the loss func-
tion is also convex, then Eq. (4) in Algorithm 1 is
convex.
Due to the space limitation, we omit the proof.
Theoretically, we can use any loss function that
satisfies the conditions of the theorem. In the exper-
iments in this paper, we use the squared-hinge loss
function: `(?yfu(x)) = max(0, 1? yfu(x))2.
Recall that Eq. (4) is not the traditional SVM or
logistic regression formulation. This is because in-
side the inner loop, the best representation for each
negative example must be found. Therefore, we
need to perform inference for every negative exam-
ple when updating the weight vector solution. In-
stead of solving a difficult non-convex optimization
problem (Eq. (3)), LCLR iteratively solves a series
of easier problems (Eq. (4)). This is especially true
for our loss function because Eq. (4) is convex and
can be solved efficiently.
We use a cutting plane algorithm to solve Eq. (4).
A similar idea has been proposed in (Joachims et al,
2009). The algorithm for solving Eq. (4) is presented
as Algorithm 2. This algorithm uses a ?cache? Hj
to store all intermediate representations for negative
examples that have been seen in previous iterations
Algorithm 2 Cutting plane algorithm to optimize Eq. (4)
1: for each negative example xj , Hj ? ?
2: repeat
3: for each negative example xj do
4: Find h?j ? arg maxh?C
?
s hsu
T?s(xj)
5: Hj ? Hj ? {h?j}
6: end for
7: Solve
min
u
?
2
?u?2 +
?
i:yi=1
`(?uT
?
s
h?i,s?s(xi))
+
?
i:yi=?1
`(max
h?Hj
uT
?
s
hs?s(xi)) (5)
8: until no new element is added to any Hj
9: return u
(lines 3-6) 2. The difference between Eq. (5) in line
7 of Algorithm 2 and Eq. (4) is that in Eq. (5), we do
not search over the entire space of intermediate rep-
resentations. The search space for the minimization
problem Eq. (5) is restricted to the cache Hj . There-
fore, instead of solving the minimization problem
Eq. (4), we can now solve several simpler problems
shown in Eq. (5). The algorithm is guaranteed to
stop (line 8) because the space of intermediate rep-
resentations is finite. Furthermore, in practice, the
algorithm needs to consider only a small subset of
?hard? examples before it converges.
Inspired by (Hsieh et al, 2008), we apply an effi-
cient coordinate descent algorithm for the dual for-
mulation of (5) which is guaranteed to find its global
minimum. Due to space considerations, we do not
present the derivation of dual formulation and the
details of the optimization algorithm.
4 Encoding with ILP: A Paraphrase
Identification Example
In this section, we define the latent representation
for the paraphrase identification task. Unlike the ear-
lier example, where we considered the alignment of
lexical items, we describe a more complex interme-
diate representation by aligning graphs created using
semantic resources.
An input example is represented as two acyclic
2In our implementation, we keep a global cache Hj for each
negative example xj . Therefore, in Algorithm 2, we start with
a non-empty cache improving the speed significantly.
432
graphs, G1 and G2, corresponding to the first
and second input sentences. Each vertex in the
graph contains word information (lemma and part-
of-speech) and the edges denote dependency rela-
tions, generated by the Stanford parser (Klein and
Manning, 2003). The intermediate representation
for this task can now be defined as an alignment be-
tween the graphs, which captures lexical and syntac-
tic correlations between the sentences.
We use V (G) and E(G) to denote the set of ver-
tices and edges in G respectively, and define four
hidden variable types to encode vertex and edge
mappings between G1 and G2.
? The word-mapping variables, denoted by
hv1,v2 , define possible pairings of vertices,
where v1 ? V (G1) and v2 ? V (G2).
? The edge-mapping variables, denoted by
he1,e2 , define possible pairings of the graphs
edges, where e1 ? E(G1) and e2 ? E(G2).
? The word-deletion variables hv1,? (or h?,v2) al-
low for vertices v1 ? V (G1) (or v2 ? V (G2))
to be deleted. This accounts for omission of
words (like function words).
? The edge-deletion variables, he1,? (or h?,e2) al-
low for deletion of edges from G1 (or G2).
Our inference problem is to find the optimal set of
hidden variable activations, restricted according to
the following set of linear constraints
? Each vertex inG1 (orG2) can either be mapped
to a single vertex in G2 (or G1) or marked as
deleted. In terms of the word-mapping and
word-deletion variables, we have
?v1 ? V (G1);hv1,? +
?
v2?V (G2)
hv1,v2 = 1 (6)
?v2 ? V (G2);h?,v2 +
?
v1?V (G1)
hv1,v2 = 1 (7)
? Each edge in G1 (or G2) can either be mapped
to a single edge in G2 (or G1) or marked as
deleted. In terms of the edge-mapping and
edge-deletion variables, we have
?e1 ? E(G1);he1,? +
?
e2?E(G2)
he1,e2 = 1 (8)
?e2 ? E(G2);h?,e2 +
?
e1?E(G1)
he1,e2 = 1 (9)
? The edge mappings can be active if, and only
if, the corresponding node mappings are ac-
tive. Suppose e1 = (v1, v?1) ? E(G1) and
e2 = (v2, v?2) ? E(G2), where v1, v
?
1 ? V (G1)
and v2, v?2 ? V (G2). Then, we have
hv1,v2 + hv?1,v?2 ? he1,e2 ? 1 (10)
hv1,v2 ? he1,e2 ;hv?1,v?2 ? he1,e2 (11)
These constraints define the feasible set for the in-
ference problem specified in Equation (1). This in-
ference problem can be formulated as an ILP prob-
lem with the objective function from Equation (1):
max
h
?
s
hsuT?s(x)
subject to (6)-(11); ?s;hs ? {0, 1} (12)
This example demonstrates the use of integer linear
programming to define intermediate representations
incorporating domain intuition.
5 Experiments
We applied our framework to three different NLP
tasks: transliteration discovery (Klementiev and
Roth, 2008), RTE (Dagan et al, 2006), and para-
phrase identification (Dolan et al, 2004).
Our experiments are designed to answer the fol-
lowing research question: ?Given a binary classifi-
cation problem defined over latent representations,
will the joint LCLR algorithm perform better than a
two-stage approach?? To ensure a fair comparison,
both systems use the same feature functions and def-
inition of intermediate representation. We use the
same ILP formulation in both configurations, with a
single exception ? the objective function parameters:
the two stage approach uses a task-specific heuristic,
while LCLR learns it iteratively.
The ILP formulation results in very strong two
stage systems. For example, in the paraphrase iden-
tification task, even our two stage system is the cur-
rent state-of-the-art performance. In these settings,
the improvement obtained by our joint approach is
non-trivial and can be clearly attributed to the su-
periority of the joint learning algorithm. Interest-
ingly, we find that our more general approach is bet-
ter than specially designed joint approaches (Gold-
wasser and Roth, 2008b; Das and Smith, 2009).
Since the objective function (3) of the joint ap-
proach is not convex, a good initialization is re-
quired. We use the weight vector learned by the two
433
stage approach as the starting point for the joint ap-
proach. The algorithm terminates when the relative
improvement of the objective is smaller than 10?5.
5.1 Transliteration Discovery
Transliteration discovery is the problem of iden-
tifying if a word pair, possibly written using two
different character sets, refers to the same underly-
ing entity. The intermediate representation consists
of all possible character mappings between the two
character sets. Identifying this mapping is not easy,
as most writing systems do not perfectly align pho-
netically and orthographically; rather, this mapping
can be context-dependent and ambiguous.
For an input pair of words (w1, w2), the interme-
diate structure h is a mapping between their charac-
ters, with the latent variable hij indicating if the ith
character in w1 is aligned to the jth character in w2.
The feature vector associated with the variable hij
contains unigram character mapping, bigram char-
acter mapping (by considering surrounding charac-
ters). We adopt the one-to-one mapping and non-
crossing constraint used in (Chang et al, 2009).
We evaluated our system using the English-
Hebrew corpus (Goldwasser and Roth, 2008a),
which consists of 250 positive transliteration pairs
for training, and 300 pairs for testing. As negative
examples for training, we sample 10% from random
pairings of words from the positive data. We report
two evaluation measurements ? (1) the Mean Recip-
rocal Rank (MRR), which is the average of the mul-
tiplicative inverse of the rank of the correct answer,
and (2) the accuracy (Acc), which is the percentage
of the top rank candidates being correct.
We initialized the two stage inference process as
detailed in (Chang et al, 2009) using a Romaniza-
tion table to assign uniform weights to prominent
character mappings. This initialization procedure
resembles the approach used in (Bergsma and Kon-
drak, 2007). An alignment is first built by solving
the constrained optimization problem. Then, a sup-
port vector machine with squared-hinge loss func-
tion is used to train a classifier using features ex-
tracted from the alignment. We refer to this two
stage approach as Alignment+Learning.
The results summarized in Table 1 show the sig-
nificant improvement obtained by the joint approach
(95.4% MRR) compared to the two stage approach
Transliteration System Acc MRR
(Goldwasser and Roth,
2008b)
N/A 89.4
Alignment + Learning 80.0 85.7
LCLR 92.3 95.4
Table 1: Experimental results for transliteration. We compare
a two-stage system: ?Alignment+Learning? with LCLR, our
joint algorithm. Both ?Alignment+Learning? and LCLR use
the same features and the same intermediate representation def-
inition.
(85.7%). Moreover, LCLR outperforms the joint
system introduced in (Goldwasser and Roth, 2008b).
5.2 Textual Entailment
Recognizing Textual Entailment (RTE) is an im-
portant textual inference task of predicting if a given
text snippet, entails the meaning of another (the hy-
pothesis). In many current RTE systems, the entail-
ment decision depends on successfully aligning the
constituents of the text and hypothesis, accounting
for the internal linguistic structure of the input.
The raw input ? the text and hypothesis ? are
represented as directed acyclic graphs, where ver-
tices correspond to words. Directed edges link verbs
to the head words of semantic role labeling argu-
ments produced by (Punyakanok et al, 2008). All
other words are connected by dependency edges.
The intermediate representation is an alignment be-
tween the nodes and edges of the graphs. We used
three hidden variable types from Section 4 ? word-
mapping, word-deletion and edge-mapping, along
with the associated constraints as defined earlier.
Since the text is typically much longer than the hy-
pothesis, we create word-deletion latent variables
(and features) only for the hypothesis.
The second column of Table 2 lists the resources
used to generate features corresponding to each hid-
den variable type. For word-mapping variables, the
features include a WordNet based metric (WNSim),
indicators for the POS tags and negation identifiers.
We used the state-of-the-art coreference resolution
system of (Bengtson and Roth, 2008) to identify the
canonical entities for pronouns and extract features
accordingly. For word deletion, we use only the POS
tags of the corresponding tokens (generated by the
LBJ POS tagger3) to generate features. For edge
3
http://L2R.cs.uiuc.edu/?cogcomp/software.php
434
Hidden RTE Paraphrase
Variable features features
word-mapping WordNet, POS,
Coref, Neg
WordNet, POS,
NE, ED
word-deletion POS POS, NE
edge-mapping NODE-INFO NODE-INFO,
DEP
edge-deletion N/A DEP
Table 2: Summary of latent variables and feature resources for
the entailment and paraphrase identification tasks. See Section
4 for an explanation of the hidden variable types. The linguistic
resources used to generate features are abbreviated as follows ?
POS: Part of speech, Coref: Canonical coreferent entities; NE:
Named Entity, ED: Edit distance, Neg: Negation markers, DEP:
Dependency labels, NODE-INFO: corresponding node align-
ment resources, N/A: Hidden variable not used.
Entailment System Acc
Median of TAC 2009 systems 61.5
Alignment + Learning 65.0
LCLR 66.8
Table 3: Experimental results for recognizing textual entail-
ment. The first row is the median of best performing systems of
all teams that participated in the RTE5 challenge (Bentivogli et
al., 2009). Alignment + Learning is our two-stage system im-
plementation, and LCLR is our joint learning algorithm. Details
about these systems are provided in the text.
mapping variables, we include the features of the
corresponding word mapping variables, scaled by
the word similarity of the words forming the edge.
We evaluated our system using the RTE-5
data (Bentivogli et al, 2009), consisting of 600 sen-
tence pairs for training and testing respectively, in
which positive and negative examples are equally
distributed. In these experiments the joint LCLR al-
gorithm converged after 5 iterations.
For the two stage system, we used WN-
Sim to score alignments during inference. The
word-based scores influence the edge variables
via the constraints. This two-stage system (the
Alignment+Learning system) is significantly better
than the median performance of the RTE-5 submis-
sions. Using LCLR further improves the result by al-
most 2%, a substantial improvement in this domain.
5.3 Paraphrase Identification
Our final task is Paraphrase Identification, dis-
cussed in detail at Section 4. We use all the four
hidden variable types described in that section. The
features used are similar to those described earlier
Paraphrase System Acc
Experiments using (Dolan et al, 2004)
(Qiu et al, 2006) 72.00
(Das and Smith, 2009) 73.86
(Wan et al, 2006) 75.60
Alignment + Learning 76.23
LCLR 76.41
Experiments using Extended data set
Alignment + Learning 72.00
LCLR 72.75
Table 4: Experimental Result For Paraphrasing Identification.
Our joint LCLR approach achieves the best results compared
to several previously published systems, and our own two stage
system implementation (Alignment + Learning). We evaluated
the systems performance across two datasets: (Dolan et al,
2004) dataset and the Extended dataset, see the text for details.
Note that LCLR outperforms (Das and Smith, 2009), which is a
specifically designed joint approach for this task.
for the RTE system and are summarized in Table 2.
We used the MSR paraphrase dataset of (Dolan
et al, 2004) for empirical evaluation. Additionally,
we generated a second corpus (called the Extended
dataset) by sampling 500 sentence pairs from the
MSR dataset for training and using the entire test
collection of the original dataset. In the Extended
dataset, for every sentence pair, we extended the
longer sentence by concatenating it with itself. This
results in a more difficult inference problem because
it allows more mappings between words. Note that
the performance on the original dataset sets the ceil-
ing on the second one.
The results are summarized in Table 4. The first
part of the table compares the LCLR system with
a two stage system (Alignment + Learning) and
three published results that use the MSR dataset.
(We only list single systems in the table4) Inter-
estingly, although still outperformed by our joint
LCLR algorithm, the two stage system is able per-
form significantly better than existing systems for
that dataset (Qiu et al, 2006; Das and Smith, 2009;
Wan et al, 2006). We attribute this improvement,
consistent across both the ILP based systems, to the
intermediate representation we defined.
We hypothesize that the similarity in performance
between the joint LCLR algorithm and the two stage
4Previous work (Das and Smith, 2009) has shown that com-
bining the results of several systems improves performance.
435
(Alignment + Learning) systems is due to the limited
intermediate representation space for input pairs in
this dataset. We evaluated these systems on the more
difficult Extended dataset. Results indeed show that
the margin between the two systems increases as the
inference problem becomes harder.
6 Related Work
Recent NLP research has largely focused on two-
stage approaches. Examples include RTE (Zanzotto
and Moschitti, 2006; MacCartney et al, 2008; Roth
et al, 2009); string matching (Bergsma and Kon-
drak, 2007); transliteration (Klementiev and Roth,
2008); and paraphrase identification (Qiu et al,
2006; Wan et al, 2006).
(MacCartney et al, 2008) considered construct-
ing a latent representation to be an independent task
and used manually labeled alignment data (Brockett,
2007) to tune the inference procedure parameters.
While this method identifies alignments well, it does
not improve entailment decisions. This strengthens
our intuition that the latent representation should be
guided by the final task.
There are several exceptions to the two-stage ap-
proach in the NLP community (Haghighi et al,
2005; McCallum et al, 2005; Goldwasser and Roth,
2008b; Das and Smith, 2009); however, the interme-
diate representation and the inference for construct-
ing it are closely coupled with the application task.
In contrast, LCLR provides a general formulation
that allows the use of expressive constraints, mak-
ing it applicable to many NLP tasks.
Unlike other latent variable SVM frameworks
(Felzenszwalb et al, 2009; Yu and Joachims, 2009)
which often use task-specific inference procedure,
LCLR utilizes the declarative inference framework
that allows using constraints over intermediate rep-
resentation and provides a general platform for a
wide range of NLP tasks.
The optimization procedure in this work and
(Felzenszwalb et al, 2009) are quite different.
We use the coordinate descent and cutting-plane
methods ensuring we have fewer parameters and
the inference procedure can be easily parallelized.
Our procedure also allows different loss functions.
(Cherry and Quirk, 2008) adopts the Latent SVM al-
gorithm to define a language model. Unfortunately,
their implementation is not guaranteed to converge.
In CRF-like models with latent variables (McCal-
lum et al, 2005), the decision function marginal-
izes over the all hidden states when presented with
an input example. Unfortunately, the computational
cost of applying their framework is prohibitive with
constrained latent representations. In contrast, our
framework requires only the best hidden representa-
tion instead of marginalizing over all possible repre-
sentations, thus reducing the computational effort.
7 Conclusion
We consider the problem of learning over an inter-
mediate representation. We assume the existence of
a latent structure in the input, relevant to the learn-
ing problem, but not accessible to the learning algo-
rithm. Many NLP tasks fall into these settings and
each can consider a different hidden input structure.
We propose a unifying thread for the different prob-
lems and present a novel framework for Learning
over Constrained Latent Representations (LCLR).
Our framework can be applied to many different la-
tent representations such as parse trees, orthographic
mapping and tree alignments. Our approach con-
trasts with existing work in which learning is done
over a fixed representation, as we advocate jointly
learning it with the final task.
We successfully apply the proposed framework to
three learning tasks ? Transliteration, Textual En-
tailment and Paraphrase Identification. Our joint
LCLR algorithm achieves superior performance in
all three tasks. We attribute the performance im-
provement to our novel training algorithm and flex-
ible inference procedure, allowing us to encode do-
main knowledge. This presents an interesting line of
future work in which more linguistic intuitions can
be encoded into the learning problem. For these rea-
sons, we believe that our framework provides an im-
portant step forward in understanding the problem
of learning over hidden structured inputs.
Acknowledgment We thank James Clarke and Mark Sam-
mons for their insightful comments. This research was partly sponsored
by the Army Research Laboratory (ARL) (accomplished under Cooper-
ative Agreement Number W911NF-09-2-0053) and by Air Force Re-
search Laboratory (AFRL) under prime contract no. FA8750-09-C-
0181. Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not necessarily
reflect the view of the ARL or of AFRL.
436
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In Proc. of TAC Work-
shop.
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In ACL.
C. Brockett. 2007. Aligning the RTE 2006 corpus.
In Technical Report MSR-TR-2007-77, Microsoft Re-
search.
M. Chang, D. Goldwasser, D. Roth, and Y. Tu. 2009.
Unsupervised constraint driven learning for transliter-
ation discovery. In NAACL.
C. Cherry and C. Quirk. 2008. Discriminative, syntactic
language modeling through latent svms. In Proc. of
the Eighth Conference of AMTA.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge.
D. Das and N. A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
ACL.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING.
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrimina-
tively trained part based models. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In ACL. Short
Paper.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In EMNLP.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In HLT-EMNLP.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural svms. Machine
Learning.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
NIPS.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Cyril Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Translation.
B. MacCartney, M. Galley, and C. D. Manning. 2008.
A phrase-based alignment model for natural language
inference. In EMNLP.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In UAI.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
EMNLP.
D. Roth, M. Sammons, and V.G. Vydiswaran. 2009. A
framework for entailed relation recognition. In ACL.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Using
dependency-based features to take the p?ara-farceo?ut
of paraphrase. In Proc. of the Australasian Language
Technology Workshop (ALTW).
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In ICML.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In ACL.
437
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 688?698,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unified Expectation Maximization
Rajhans Samdani
University of Illinois
rsamdan2@illinois.edu
Ming-Wei Chang
Microsoft Research
minchang@microsoft.com
Dan Roth
University of Illinois
danr@illinois.edu
Abstract
We present a general framework containing a
graded spectrum of Expectation Maximization
(EM) algorithms called Unified Expectation
Maximization (UEM.) UEM is parameterized
by a single parameter and covers existing al-
gorithms like standard EM and hard EM, con-
strained versions of EM such as Constraint-
Driven Learning (Chang et al, 2007) and Pos-
terior Regularization (Ganchev et al, 2010),
along with a range of new EM algorithms.
For the constrained inference step in UEM we
present an efficient dual projected gradient as-
cent algorithm which generalizes several dual
decomposition and Lagrange relaxation algo-
rithms popularized recently in the NLP litera-
ture (Ganchev et al, 2008; Koo et al, 2010;
Rush and Collins, 2011). UEM is as efficient
and easy to implement as standard EM. Fur-
thermore, experiments on POS tagging, infor-
mation extraction, and word-alignment show
that often the best performing algorithm in the
UEM family is a new algorithm that wasn?t
available earlier, exhibiting the benefits of the
UEM framework.
1 Introduction
Expectation Maximization (EM) (Dempster et al,
1977) is inarguably the most widely used algo-
rithm for unsupervised and semi-supervised learn-
ing. Many successful applications of unsupervised
and semi-supervised learning in NLP use EM in-
cluding text classification (McCallum et al, 1998;
Nigam et al, 2000), machine translation (Brown et
al., 1993), and parsing (Klein and Manning, 2004).
Recently, EM algorithms which incorporate con-
straints on structured output spaces have been pro-
posed (Chang et al, 2007; Ganchev et al, 2010).
Several variations of EM (e.g. hard EM) exist in
the literature and choosing a suitable variation is of-
ten very task-specific. Some works have shown that
for certain tasks, hard EM is more suitable than reg-
ular EM (Spitkovsky et al, 2010). The same issue
continues in the presence of constraints where Poste-
rior Regularization (PR) (Ganchev et al, 2010) cor-
responds to EM while Constraint-Driven Learning
(CoDL)1 (Chang et al, 2007) corresponds to hard
EM. The problem of choosing between EM and hard
EM (or between PR and CoDL) remains elusive,
along with the possibility of simple and better alter-
natives, to practitioners. Unfortunately, little study
has been done to understand the relationships be-
tween these variations in the NLP community.
In this paper, we approach various EM-based
techniques from a novel perspective. We believe that
?EM or Hard-EM?? and ?PR or CoDL?? are not the
right questions to ask. Instead, we present a unified
framework for EM, Unified EM (UEM), that covers
many EM variations including the constrained cases
along with a continuum of new ones. UEM allows us
to compare and investigate the properties of EM in a
systematic way and helps find better alternatives.
The contributions of this paper are as follows:
1. We propose a general framework called Uni-
fied Expectation Maximization (UEM) that
presents a continuous spectrum of EM algo-
rithms parameterized by a simple temperature-
like tuning parameter. The framework covers
both constrained and unconstrained EM algo-
rithms. UEM thus connects EM, hard EM, PR,
and CoDL so that the relation between differ-
ent algorithms can be better understood. It also
enables us to find new EM algorithms.
2. To solve UEM (with constraints), we propose
1To be more precise, (Chang et al, 2007) mentioned using
hard constraints as well as soft constraints in EM. In this paper,
we refer to CoDL only as the EM framework with hard con-
straints.
688
a dual projected subgradient ascent algorithm
that generalizes several dual decomposition
and Lagrange relaxation algorithms (Bertsekas,
1999) introduced recently in NLP (Ganchev et
al., 2008; Rush and Collins, 2011).
3. We provide a way to implement a family of
EM algorithms and choose the appropriate one,
given the data and problem setting, rather than
a single EM variation. We conduct experi-
ments on unsupervised POS tagging, unsuper-
vised word-alignment, and semi-supervised in-
formation extraction and show that choosing
the right UEM variation outperforms existing
EM algorithms by a significant margin.
2 Preliminaries
Let x denote an input or observed features and h be
a discrete output variable to be predicted from a fi-
nite set of possible outputs H(x). Let P?(x,h) be
a probability distribution over (x,h) parameterized
by ?. Let P?(h|x) refer to the conditional probabil-
ity of h given x. For instance, in part-of-speech tag-
ging, x is a sentence, h the corresponding POS tags,
and ? could be an HMM model; in word-alignment,
x can be an English-French sentence pair, h the
word alignment between the sentences, and ? the
probabilistic alignment model. Let ?(h = h?) be
the Kronecker-Delta distribution centered at h?, i.e.,
it puts a probability of 1 at h? and 0 elsewhere.
In the rest of this section, we review EM and
constraints-based learning with EM.
2.1 EM Algorithm
To obtain the parameter ? in an unsupervised way,
one maximizes log-likelihood of the observed data:
L(?) = logP?(x) = log
?
h?H(x)
P?(x,h) . (1)
EM (Dempster et al, 1977) is the most common
technique for learning ?, which maximizes a tight
lower bound onL(?). While there are a few different
styles of expressing EM, following the style of (Neal
and Hinton, 1998), we define
F (?, q) = L(?)?KL(q, P?(h|x)), (2)
where q is a posterior distribution over H(x) and
KL(p1, p2) is the KL divergence between two dis-
tributions p1 and p2. Given this formulation, EM can
be shown to maximize F via block coordinate ascent
alternating over q (E-step) and ? (M-step) (Neal and
Hinton, 1998). In particular, the E-step for EM can
be written as
q = arg min
q??Q
KL(q?, P?(h|x)) , (3)
where Q is the space of all distributions. While EM
produces a distribution in the E-step, hard EM is
thought of as producing a single output given by
h? = arg max
h?H(x)
P?(h|x) . (4)
However, one can also think of hard EM as pro-
ducing a distribution given by q = ?(h = h?). In
this paper, we pursue this distributional view of both
EM and hard EM and show its benefits.
EM for Discriminative Models EM-like algo-
rithms can also be used in discriminative set-
tings (Bellare et al, 2009; Ganchev et al, 2010)
specifically for semi-supervised learning (SSL.)
Given some labeled and unlabeled data, such algo-
rithms maximize a modified F (?, q) function:
F (?, q) = Lc(?)? c1???
2 ? c2KL(q, P?(h|x)) , (5)
where, q, as before, is a probability distribution over
H(x), Lc(?) is the conditional log-likelihood of the
labels given the features for the labeled data, and c1
and c2 are constants specified by the user; the KL
divergence is measured only over the unlabeled data.
The EM algorithm in this case has the same E-step
as unsupervised EM, but the M-step is different. The
M-step is similar to supervised learning as it finds ?
by maximizing a regularized conditional likelihood
of the data w.r.t. the labels ? true labels are used for
labeled data and ?soft? pseudo labels based on q are
used for unlabeled data.
2.2 Constraints in EM
It has become a common practice in the NLP com-
munity to use constraints on output variables to
guide inference. Few of many examples include
type constraints between relations and entities (Roth
and Yih, 2004), sentential and modifier constraints
during sentence compression (Clarke and Lapata,
2006), and agreement constraints between word-
alignment directions (Ganchev et al, 2008) or var-
ious parsing models (Koo et al, 2010). In the con-
689
text of EM, constraints can be imposed on the pos-
terior probabilities, q, to guide the learning proce-
dure (Chang et al, 2007; Ganchev et al, 2010).
In this paper, we focus on linear constraints over
h (potentially non-linear over x.) This is a very gen-
eral formulation as it is known that all Boolean con-
straints can be transformed into sets of linear con-
straints over binary variables (Roth and Yih, 2007).
Assume that we have m linear constraints on out-
puts where the kth constraint can be written as
uk
Th ? bk .
Defining a matrix U as UT =
[
u1T . . . umT
]
and a vector b as bT = [b1, . . . , bm], we write down
the set of all feasible2 structures as
{h | h ? H(x),Uh ? b} .
Constraint-Driven Learning (CoDL) (Chang et
al., 2007) augments the E-step of hard EM (4) by
imposing these constraints on the outputs.
Constraints on structures can be relaxed to expec-
tation constraints by requiring the distribution q to
satisfy them only in expectation. Define expecta-
tion w.r.t. a distribution q over H(x) as Eq[Uh] =?
h?H(x) q(h)Uh. In the expectation constraints
setting, q is required to satisfy:
Eq[Uh] ? b .
The space of distributions Q can be modified as:
Q = {q | q(h) ? 0, Eq[Uh] ? b,
?
h?H(x)
q(h) = 1}.
Augmenting these constraints into the E-step of
EM (3), gives the Posterior Regularization (PR)
framework (Ganchev et al, 2010). In this paper, we
adopt the expectation constraint setting. Later, we
show that UEM naturally includes and generalizes
both PR and CoDL.
3 Unified Expectation Maximization
We now present the Unified Expectation Maximiza-
tion (UEM) framework which captures a continuum
of (constrained and unconstrained) EM algorithms
2Note that this set is a finite set of discrete variables not to
be confused with a polytope. Polytopes are also specified as
{z|Az ? d} but are over real variables whereas h is discrete.
Algorithm 1 The UEM algorithm for both the genera-
tive (G) and discriminative (D) cases.
Initialize ?0
for t = 0, . . . , T do
UEM E-step:
qt+1 ? arg minq?QKL(q, P?t(h|x); ?)
UEM M-step:
G: ?t+1 = arg max? Eqt+1 [logP?(x,h)]
D: ?t+1 = arg max? Eqt+1 [logP?(h|x)]? c1???
2
end for
including EM and hard EM by modulating the en-
tropy of the posterior. A key observation underlying
the development of UEM is that hard EM (or CoDL)
finds a distribution with zero entropy while EM (or
PR) finds a distribution with the same entropy as P?
(or close to it). Specifically, we modify the objective
of the E-step of EM (3) as
q = arg min
q??Q
KL(q?, P?(h|x); ?) , (6)
where KL(q, p; ?) is a modified KL divergence:
KL(q, p; ?) =
?
h?H(x)
?q(h) log q(h)?q(h) log p(h). (7)
In other words, UEM projects P?(h|x) on the
space of feasible distributions Q w.r.t. a metric3
KL(?, ?; ?) to obtain the posterior q. By simply vary-
ing ?, UEM changes the metric of projection and ob-
tains different variations of EM including EM (PR,
in the presence of constraints) and hard EM (CoDL.)
The M-step for UEM is exactly the same as EM (or
discriminative EM.)
The UEM Algorithm: Alg. 1 shows the UEM al-
gorithm for both the generative (G) and the discrimi-
native (D) case. We refer to the UEM algorithm with
parameter ? as UEM? .
3.1 Relationship between UEM and Other EM
Algorithms
The relation between unconstrained versions of EM
has been mentioned before (Ueda and Nakano,
1998; Smith and Eisner, 2004). We show that the
relationship takes novel aspects in the presence of
constraints. In order to better understand different
UEM variations, we write the UEM E-step (6) ex-
plicitly as an optimization problem:
3The term ?metric? is used very loosely. KL(?, ?; ?) does
not satisfy the mathematical properties of a metric.
690
Framework ? = ?? ? = 0 ? ? (0, 1) ? = 1 ? =?? 1
Constrained Hard EM Hard EM (NEW) UEM? Standard EM Deterministic
Annealing EM
Unconstrained CoDL (Chang et
al., 2007)
(NEW) EM
with Lin. Prog.
(NEW) constrained
UEM?
PR (Ganchev et al,
2010)
Table 1: Summary of different UEM algorithms. The entries marked with ?(NEW)? have not been proposed before.
Eq. (8) is the objective function for all the EM frameworks listed in this table. Note that, in the absence of constraints,
? ? (??, 0] corresponds to hard EM (Sec. 3.1.1.) Please see Sec. 3.1 for a detailed explanation.
min
q
?
h?H(x)
?q(h) log q(h)? q(h) logP?(h|x)(8)
s.t. Eq[Uh] ? b,
q(h) ? 0,?h ? H(x),
?
h?H(x) q(h) = 1 .
We discuss below, both the constrained and the
unconstrained cases. Tab. 1 summarizes different
EM algorithms in the UEM family.
3.1.1 UEM Without Constraints
The E-step in this case, computes a q obeying
only the simplex constraints:
?
h?H(x) q(h) = 1.
For ? = 1, UEM minimizes KL(q, P?(h|x); 1)
which is the same as minimizing KL(q, P?(h|x))
as in the standard EM (3). For ? = 0, UEM
is solving arg minq?Q
?
h?H(x)?q(h) logP?(h|x)
which is a linear programming (LP) problem. Due to
the unimodularity of the simplex constraints (Schri-
jver, 1986), this LP outputs an integral q =
?
(
h = arg maxh?H(x) P?(h|x)
)
which is the same
as hard EM (4). It has already been noted in the liter-
ature (Kearns et al, 1997; Smith and Eisner, 2004;
Hofmann, 2001) that this formulation (correspond-
ing to our ? = 0) is the same as hard EM. In fact,
for ? ? 0, UEM stays the same as hard EM be-
cause of negative penalty on the entropy. The range
? ? (0, 1) has not been discussed in the literature,
to the best of our knowledge. In Sec. 5, we show
the impact of using UEM?for ? ? {0, 1}. Lastly,
the range of ? from? to 1 has been used in deter-
ministic annealing for EM (Rose, 1998; Ueda and
Nakano, 1998; Hofmann, 2001). However, the focus
of deterministic annealing is solely to solve the stan-
dard EM while avoiding local maxima problems.
3.1.2 UEM With Constraints
UEM and Posterior Regularization (? = 1) For
? = 1, UEM solves arg minq?QKL (q, P?(h|x))
which is the same as Posterior Regulariza-
tion (Ganchev et al, 2010).
UEM and CoDL (? = ??) When ? ? ??
then due to an infinite penalty on the entropy of the
posterior, the entropy must become zero. Thus, now
the E-step, as expressed by Eq. (8), can be written as
q = ?(h = h?) where h? is obtained as
arg max
h?H(x)
logP?(h|x) (9)
s.t. Uh ? b ,
which is the same as CoDL. This combinatorial
maximization can be solved using the Viterbi algo-
rithm in some cases or, in general, using Integer Lin-
ear Programming (ILP.)
3.2 UEM with ? ? [0, 1]
Tab. 1 lists different EM variations and their associ-
ated values ?. This paper focuses on values of ? be-
tween 0 and 1 for the following reasons. First, the E-
step (8) is non-convex for ? < 0 and hence compu-
tationally expensive; e.g., hard EM (i.e. ? = ??)
requires ILP inference. For ? ? 0, (8) is a convex
optimization problem which can be solved exactly
and efficiently. Second, for ? = 0, the E-step solves
max
q
?
h?H(x) q(h) logP?(h|x) (10)
s.t. Eq[Uh] ? b,
q(h) ? 0, ?h ? H(x),
?
h?H(x) q(h) = 1 ,
which is an LP-relaxation of hard EM (Eq. (4)
and (9)). LP relaxations often provide a decent
proxy to ILP (Roth and Yih, 2004; Martins et al,
2009). Third, ? ? [0, 1] covers standard EM/PR.
3.2.1 Discussion: Role of ?
The modified KL divergence can be related to
standard KL divergence as KL(q, P?(h|x); ?) =
691
KL(q, P?(y|x)) + (1? ?)H(q) ? UEM (6) mini-
mizes the former during the E-step, while Standard
EM (3) minimizes the latter. The additional term
(1 ? ?)H(q) is essentially an entropic prior on the
posterior distribution q which can be used to regu-
larize the entropy as desired.
For ? < 1, the regularization term penalizes the
entropy of the posterior thus reducing the probability
mass on the tail of the distribution. This is signifi-
cant, for instance, in unsupervised structured predic-
tion where the tail can carry a substantial amount of
probability mass as the output space is massive. This
notion aligns with the observation of (Spitkovsky
et al, 2010) who criticize EM for frittering away
too much probability mass on unimportant outputs
while showing that hard EM does much better in
PCFG parsing. In particular, they empirically show
that when initialized with a ?good? set of parame-
ters obtained by supervised learning, EM drifts away
(thus losing accuracy) much farther than hard-EM.
4 Solving Constrained E-step with
Lagrangian Dual
In this section, we discuss how to solve the E-
step (8) for UEM. It is a non-convex problem for
? < 0; however, for ? = ?? (CoDL) one can use
ILP solvers. We focus here on solving the E-step for
? ? 0 for which it is a convex optimization problem,
and use a Lagrange relaxation algorithm (Bertsekas,
1999). Our contributions are two fold:
? We describe an algorithm for UEM with con-
straints that is as easy to implement as PR or
CoDL. Existing code for constrained EM (PR
or CoDL) can be easily extended to run UEM.
? We solve the E-step (8) using a Lagrangian
dual-based algorithm which performs projected
subgradient-ascent on dual variables. Our al-
gorithm covers Lagrange relaxation and dual
decomposition techniques (Bertsekas, 1999)
which were recently popularized in NLP (Rush
and Collins, 2011; Rush et al, 2010; Koo et al,
2010). Not only do we extend the algorithmic
framework to a continuum of algorithms, we
also allow, unlike the aforementioned works,
general inequality constraints over the output
variables. Furthermore, we establish new and
interesting connections between existing con-
strained inference techniques.
4.1 Projected Subgradient Ascent with
Lagrangian Dual
We provide below a high-level view of our algo-
rithm, omitting the technical derivations due to lack
of space. To solve the E-step (8), we introduce dual
variables ? ? one for each expectation constraint in
Q. The subgradient O? of the dual of Eq. (8) w.r.t.
? is given by
O? ? Eq[Uh]? b . (11)
For ? > 0, the primal variable q can be written in
terms of ? as
q(h) ? P?t(h|x)
1
? e?
?TUh
? . (12)
For ? = 0, the q above is not well defined and so
we take the limit ? ? 0 in (12) and since lp norm
approaches the max-norm as p??, this yields
q(h) = ?(h = arg max
h??H(x)
P?(h
?|x)e??
TUh?). (13)
We combine both the ideas by setting q(h) =
G(h, P?t(?|x), ?TU, ?) where
G(h, P,v, ?) =
?
?
??
?
??
P (h)
1
? e
? vh?
?
h? P (h
?)
1
? e
? vh
?
?
? > 0 ,
?(h= argmax
h??H(x)
P (h?)e?vh
?
) ? = 0 .
(14)
Alg. 2 shows the overall optimization scheme.
The dual variables for inequality constraints are re-
stricted to be positive and hence after a gradient up-
date, negative dual variables are projected to 0.
Note that for ? = 0, our algorithm is a Lagrange
relaxation algorithm for approximately solving the
E-step for CoDL (which uses exact arg max infer-
ence). Lagrange relaxation has been recently shown
to provide exact and optimal results in a large num-
ber of cases (Rush and Collins, 2011). This shows
that our range of algorithms is very broad ? it in-
cludes PR and a good approximation to CoDL.
Overall, the required optimization (8) can be
solved efficiently if the expected value computation
in the dual gradient (Eq. (11)) w.r.t. the posterior q
in the primal (Eq (14)) can be performed efficiently.
In cases where we can enumerate the possible out-
puts h efficiently, e.g. multi-class classification, we
692
Algorithm 2 Solving E-step of UEM? for ? ? 0.
1: Initialize and normalize q; initialize ? = 0.
2: for t = 0, . . . , R or until convergence do
3: ?? max (?+ ?t (Eq[Uh]? b) , 0)
4: q(h) = G(h, P?t(?|x), ?TU, ?)
5: end for
can compute the posterior probability q explicitly
using the dual variables. In cases where the out-
put space is structured and exponential in size, e.g.
word alignment, we can optimize (8) efficiently if
the constraints and the model P?(h|x) decompose
in the same way. To elucidate, we give a more con-
crete example in the next section.
4.2 Projected Subgradient based Dual
Decomposition Algorithm
Solving the inference (8) using Lagrangian dual can
often help us decompose the problem into compo-
nents and handle complex constraints in the dual
space as we show in this section. Suppose our
task is to predict two output variables h1 and h2
coupled via linear constraints. Specifically, they
obey Ueh1 = Ueh2 (agreement constraints) and
Uih1 ? Uih2 (inequality constraints)4 for given
matrices Ue and Ui. Let their respective probabilis-
tic models be P 1?1 and P
2
?2 . The E-step (8) can be
written as
arg min
q1,q2
A(q1, q2; ?) (15)
s.t. Eq1 [Ueh
1] = Eq2 [Ueh
2]
Eq1 [Uih
1] ? Eq2 [Uih
2] ,
where A(q1, q2; ?) = KL(q1(h1), P 1?1(h
1|x); ?) +
KL(q2(h2), P 2?2(h
2|x); ?).
The application of Alg. 2 results in a dual decom-
position scheme which is described in Alg. 3.
Note that in the absence of inequality constraints
and for ? = 0, our algorithm reduces to a simpler
dual decomposition algorithm with agreement con-
straints described in (Rush et al, 2010; Koo et al,
2010). For ? = 1 with agreement constraints, our
algorithm specializes to an earlier proposed tech-
nique by (Ganchev et al, 2008). Thus our algo-
rithm puts these dual decomposition techniques with
4The analysis remains the same for a more general formu-
lation with a constant offset vector on the R.H.S. and different
matrices for h1 and h2.
Algorithm 3 Projected Subgradient-based Lagrange
Relaxation Algorithm that optimizes Eq. (15)
1: Input: Two distributions P 1?1 and P
2
?2 .
2: Output: Output distributions q1 and q2 in (15)
3: Define ?T =
[
?e
T ?i
T
]
and UT =
[
Ue
T Ui
T
]
4: ?? 0
5: for t = 0, . . . , R or until convergence do
6: q1(h1)? G(h1, P 1?1(?|x), ?
TU, ?)
7: q2(h2)? G(h2, P 2?2(?|x),??
TU, ?)
8: ?e ? ?e + ?t(?Eq1 [Ueh
1] + Eq2 [Ueh
2])
9: ?i ? ?i + ?t(?Eq1 [Uih
1] + Eq2 [Uih
2])
10: ?i ? max(?i, 0) {Projection step}
11: end for
12: return (q1, q2)
agreement constraints on the same spectrum. More-
over, dual-decomposition is just a special case of
Lagrangian dual-based techniques. Hence Alg. 2
is more broadly applicable (see Sec. 5). Lines 6-9
show that the required computation is decomposed
over each sub-component.
Thus if computing the posterior and expected val-
ues of linear functions over each subcomponent is
easy, then the algorithm works efficiently. Con-
sider the case when constraints decompose linearly
over h and each component is modeled as an HMM
with ?S as the initial state distribution, ?E as em-
mision probabilities, and ?T as transition probabil-
ities. An instance of this is word alignment over
language pair (S, T ) modeled using an HMM aug-
mented with agreement constraints which constrain
alignment probabilities in one direction (P?1 : from
S to T ) to agree with the alignment probabilities in
the other direction (P?2 : from T to S.) The agree-
ment constraints are linear over the alignments, h.
Now, the HMM probability is given by
P?(h|x) = ?S(h0)
?
i ?E(xi|hi)?T (hi+1|hi)
where vi denotes the ith component of a vector v.
For ? > 0, the resulting q (14) can be expressed
using a vector ? =+/-?TU (see lines 6-7) as
q(h) ?
(
?S(h0)
?
i
?E(xi|hi)?T (hi+1|hi)
) 1
?
e
?
i ?ihi
?
?
?
i
?S(h0)
1
?
(
?E(xi|hi)e?ihi
) 1
? ?T (hi+1|hi)
1
? .
The dual variables-based term can be folded into
the emission probabilities, ?E . Now, the resulting q
can be expressed as an HMM by raising ?S , ?E , and
693
?T to the power 1/? and normalizing. For ? = 0, q
can be computed as the most probable output. The
required computations in lines 6-9 can be performed
using the forward-backward algorithm or the Viterbi
algorithm. Note that we can efficiently compute ev-
ery step because the linear constraints decompose
nicely along the probability model.
5 Experiments
Our experiments are designed to explore tuning ?
in the UEM framework as a way to obtain gains
over EM and hard EM in the constrained and uncon-
strained cases. We conduct experiments on POS-
tagging, word-alignment, and information extrac-
tion; we inject constraints in the latter two. In all the
cases we use our unified inference step to implement
general UEM and the special cases of existing EM
algorithms. Since both of our constrained problems
involve large scale constrained inference during the
E-step, we use UEM0 (with a Lagrange relaxation
based E-step) as a proxy for ILP-based CoDL .
As we vary ? over [0, 1], we circumvent much of
the debate over EM vs hard EM (Spitkovsky et al,
2010) by exploring the space of EM algorithms in a
?continuous? way. Furthermore, we also study the
relation between quality of model initialization and
the value of ? in the case of POS tagging. This is
inspired by a general ?research wisdom? that hard
EM is a better choice than EM with a good initial-
ization point whereas the opposite is true with an
?uninformed? initialization.
Unsupervised POS Tagging We conduct exper-
iments on unsupervised POS learning experiment
with the tagging dictionary assumption. We use a
standard subset of Penn Treebank containing 24,115
tokens (Ravi and Knight, 2009) with the tagging dic-
tionary derived from the entire Penn Treebank. We
run UEM with a first order (bigram) HMM model5.
We consider initialization points of varying quality
and observe the performance for ? ? [0, 1].
Different initialization points are constructed as
follows. The ?posterior uniform? initialization is
created by spreading the probability uniformly over
all possible tags for each token. Our EM model on
5(Ravi and Knight, 2009) showed that a first order HMM
model performs much better than a second order HMM model
on unsupervised POS tagging
-0.15-0.1-0.05 0 0.05 1
.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Relative performance to EM (Gamma=1)
Gamm
a
unifor
m pos
terior 
initiali
zer
5 labe
led ex
ample
s initia
lizer
10 lab
eled e
xampl
es init
ializer
20 lab
eled e
xampl
es init
ializer
40 lab
eled e
xampl
es init
ializer
80 lab
eled e
xampl
es init
ializer
Figure 1: POS Experiments showing the relation between
initial model parameters and ?. We report the relative per-
formance compared to EM (see Eq. (16)). The posterior
uniform initialization does not use any labeled examples.
As the no. of labeled examples used to create the initial
HMM model increases, the quality of the initial model
improves. The results show that the value of the best ? is
sensitive to the initialization point and EM (? = 1) and
hard EM (? = 0) are often not the best choice.
this dataset obtains 84.9% accuracy on all tokens
and 72.3% accuracy on ambiguous tokens, which
is competitive with results reported in (Ravi and
Knight, 2009). To construct better initialization
points, we train a supervised HMM tagger on hold-
out labeled data. The quality of the initialization
points is varied by varying the size of the labeled
data over {5, 10, 20, 40, 80}. Those initialization
points are then fed into different UEM algorithms.
Results For a particular ?, we report the perfor-
mance of UEM? w.r.t. EM (? = 1.0) as given by
rel(?) =
Acc(UEM?)?Acc(UEM?=1.0)
Acc(UEM?=1.0)
(16)
where Acc represents the accuracy as evaluated on
the ambiguous words of the given data. Note that
rel(?) ? 0, implies performance better or worse
than EM. The results are summarized in Figure 1.
Note that when we use the ?posterior uniform?
initialization, EM wins by a significant margin. Sur-
prisingly, with the initialization point constructed
with merely 5 or 10 examples, EM is not the best
algorithm anymore. The best result for most cases is
obtained at ? somewhere between 0 (hard EM) and 1
(EM). Furthermore, the results not only indicate that
a measure of ?hardness? of EM i.e. the best value
694
of ?, is closely related to the quality of the ini-
tialization point but also elicit a more fine-grained
relationship between initialization and UEM.
This experiment agrees with (Merialdo, 1994),
which shows that EM performs poorly in the semi-
supervised setting. In (Spitkovsky et al, 2010), the
authors show that hard EM (Viterbi EM) works bet-
ter than standard EM. We extend these results by
showing that this issue can be overcome with the
UEM framework by picking appropriate ? based on
the amount of available labeled data.
Semi-Supervised Entity-Relation Extraction
We conduct semi-supervised learning (SSL) ex-
periments on entity and relation type prediction
assuming that we are given mention boundaries.
We borrow the data and the setting from (Roth and
Yih, 2004). The dataset has 1437 sentences; four
entity types: PER, ORG, LOC, OTHERS and;
five relation types LIVE IN, KILL, ORG BASED IN,
WORKS FOR, LOCATED IN. We consider relations
between all within-sentence pairs of entities. We
add a relation type NONE indicating no relation
exists between a given pair of entities.
We train two log linear models for entity type and
relation type prediction, respectively via discrimina-
tive UEM. We work in a discriminative setting in
order to use several informative features which we
borrow from (Roth and Small, 2009). Using these
features, we obtain 56% average F1 for relations and
88% average F1 for entities in a fully supervised set-
ting with an 80-20 split which is competitive with
the reported results on this data (Roth and Yih, 2004;
Roth and Small, 2009). For our SSL experiments,
we use 20% of data for testing, a small amount, ?%,
as labeled training data (we vary ?), and the remain-
ing as unlabeled training data. We initialize with a
classifier trained on the given labeled data.
We use the following constraints on the posterior.
1) Type constraints: For two entities e1 and e2, the
relation type ?(e1, e2) between them dictates a par-
ticular entity type (or in general, a set of entity types)
for both e1 and e2. These type constraints can be
expressed as simple logical rules which can be con-
verted into linear constraints. E.g. if the pair (e1, e2)
has relation type LOCATED IN then e2 must have en-
tity type LOC. This yields a logical rule which is
converted into a linear constraint as
0.3	 ?0.32	 ?
0.34	 ?0.36	 ?
0.38	 ?0.4	 ?
0.42	 ?0.44	 ?
0.46	 ?0.48	 ?
5	 ? 10	 ? 20	 ?
Avg.	 ?F1
	 ?for	 ?rel
a?ons	 ?
%	 ?of	 ?labeled	 ?data	 ?
Sup.	 ?Bas.	 ? PR	 ?
CoDL	 ? UEM	 ?
Figure 2: Average F1 for relation prediction for varying
sizes of labeled data comparing the supervised baseline,
PR, CoDL, and UEM. UEM is statistically significantly
better than supervised baseline and PR in all the cases.
(?(e1, e2) == LOCATED IN) ? (e2 == LOC)
? q (LOCATED IN; e1, e2) ? q (LOC; e2) .
Refer to (Roth and Yih, 2004) for more statistics on
this data and a list of all the type constraints used.
2) Expected count constraints: Since most entity
pairs are not covered by the given relation types, the
presence of a large number of NONE relations can
overwhelm SSL. To guide learning in the right direc-
tion, we use corpus-wide expected count constraints
for each non-NONE relation type. These constraints
are very similar to the label regularization technique
mentioned in (Mann and McCallum, 2010). Let Dr
be the set of entity pairs as candidate relations in the
entire corpus. For each non-NONE relation type ?,
we impose the constraints
L? ?
?
(e1,e2)?Dr
q(?; e1, e2) ? U? ,
where L? and U? are lower and upper bound on the
expected number of ? relations in the entire corpus.
Assuming that the labeled and the unlabeled data are
drawn from the same distribution, we obtain these
bounds using the fractional counts of ? over the la-
beled data and then perturbing it by +/- 20%.
Results We use Alg. 2 for solving the constrained
E-step. We report results averaged over 10 random
splits of the data and measure statistical significance
using paired t-test with p = 0.05. The results for
relation prediction are shown in Fig. 2. For each
trial, we split the labeled data into half to tune the
value of ?. For ? = 5%, 10%, and 20%, the average
695
value of gamma is 0.52, 0.6, and 0.57, respectively;
the median values are 0.5, 0.6, and 0.5, respectively.
For relation extraction, UEM is always statistically
significantly better than the baseline and PR. The
difference between UEM and CoDL is small which
is not very surprising because hard EM approaches
like CoDL are known to work very well for discrim-
inative SSL. We omit the graph for entity predic-
tion because EM-based approaches do not outper-
form the supervised baseline there. However, no-
tably, for entities, for ? = 10%, UEM outperforms
CoDL and PR and for 20%, the supervised baseline
outperforms PR statistically significantly.
Word Alignment Statistical word alignment is a
well known structured output application of unsu-
pervised learning and is a key step towards ma-
chine translation from a source language S to a tar-
get language T . We experiment with two language-
pairs: English-French and English-Spanish. We
use Hansards corpus for French-English trans-
lation (Och and Ney, 2000) and Europarl cor-
pus (Koehn, 2002) for Spanish-English translation
with EPPS (Lambert et al, 2005) annotation.
We use an HMM-based model for word-
alignment (Vogel et al, 1996) and add agreement
constraints (Liang et al, 2008; Ganchev et al, 2008)
to constrain alignment probabilities in one direction
(P?1 : from S to T ) to agree with the alignment prob-
abilities in the other direction (P?2 : from T to S.)
We use a small development set of size 50 to tune
the model. Note that the amount of labeled data we
use is much smaller than the supervised approaches
reported in (Taskar et al, 2005; Moore et al, 2006)
and unsupervised approaches mentioned in (Liang et
al., 2008; Ganchev et al, 2008) and hence our results
are not directly comparable. For the E-step, we use
Alg. 3 with R=5 and pick ? from {0.0, 0.1, . . . , 1.0},
tuning it over the development set.
During testing, instead of running HMM mod-
els for each direction separately, we obtain posterior
probabilities by performing agreement constraints-
based inference as in Alg. 3. This results in a
posterior probability distribution over all possible
alignments. To obtain final alignments, follow-
ing (Ganchev et al, 2008) we use minimum Bayes
risk decoding: we align all word pairs with poste-
rior marginal alignment probability above a certain
Size EM PR CoDL UEM EM PR CoDL UEM
En-Fr Fr-En
10k 23.54 10.63 14.76 9.10 19.63 10.71 14.68 9.21
50k 18.02 8.30 10.08 7.34 16.17 8.40 10.09 7.40
100k 16.31 8.16 9.17 7.05 15.03 8.09 8.93 6.87
En-Es Es-En
10k 33.92 22.24 28.19 20.80 31.94 22.00 28.13 20.83
50k 25.31 19.84 22.99 18.93 24.46 20.08 23.01 18.95
100k 24.48 19.49 21.62 18.75 23.78 19.70 21.60 18.64
Table 2: AER (Alignment Error Rate) comparisons
for French-English (above) and Spanish-English (below)
alignment for various data sizes. For French-English set-
ting, tuned ? for all data-sizes is either 0.5 or 0.6. For
Spanish-English, tuned ? for all data-sizes is 0.7.
threshold, tuned over the development set.
Results We compare UEM with EM, PR, and
CoDL on the basis of Alignment Error Rate (AER)
for different sizes of unlabeled data (See Tab. 2.)
See (Och and Ney, 2003) for the definition of AER.
UEM consistently outperforms EM, PR, and CoDL
with a wide margin.
6 Conclusion
We proposed a continuum of EM algorithms
parameterized by a single parameter. Our frame-
work naturally incorporates constraints on output
variables and generalizes existing constrained and
unconstrained EM algorithms like standard and
hard EM, PR, and CoDL. We provided an efficient
Lagrange relaxation algorithm for inference with
constraints in the E-step and empirically showed
how important it is to choose the right EM version.
Our technique is amenable to be combined with
many existing variations of EM (Berg-Kirkpatrick
et al, 2010). We leave this as future work.
Acknowledgments: We thank Joa?o Grac?a for provid-
ing the code and data for alignment with agreement. This
research is sponsored by the Army Research Laboratory
(ARL) under agreement W911NF-09-2-0053, Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-018, and an
ONR Award on Guiding Learning and Decision Making
in the Presence of Multiple Forms of Information. Any
opinions, findings, conclusions or recommendations are
those of the authors and do not necessarily reflect the
views of the funding agencies.
696
References
K. Bellare, G. Druck, and A. McCallum. 2009. Alter-
nating projections for learning with expectation con-
straints. In UAI.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In ACL, HLT ?10.
D. P. Bertsekas. 1999. Nonlinear Programming. Athena
Scientific, 2nd edition.
P. Brown, S. D. Pietra, V. D. Pietra, and R. Mercer. 1993.
The mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In ACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society.
K. Ganchev, J. Graca, and B. Taskar. 2008. Better align-
ments = better translations. In ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
T. Hofmann. 2001. Unsupervised learning by probabilis-
tic latent semantic analysis. MlJ.
M. Kearns, Y. Mansour, and A. Y. Ng. 1997. An
information-theoretic analysis of hard and soft assign-
ment methods for clustering. In ICML.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: models of dependency and
constituency. In ACL.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
P. Lambert, A. De Gispert, R. Banchs, and J. Marino.
2005. Guidelines for word alignment evaluation and
manual alignment. Language Resources and Evalua-
tion.
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In NIPS.
G. S. Mann and A. McCallum. 2010. Generalized
expectation criteria for semi-supervised learning with
weakly labeled data. JMLR, 11.
A. Martins, N. A. Smith, and E. Xing. 2009. Concise
integer linear programming formulations for depen-
dency parsing. In ACL.
A. K. McCallum, R. Rosenfeld, T. M. Mitchell, and A. Y.
Ng. 1998. Improving text classification by shrinkage
in a hierarchy of classes. In ICML.
B. Merialdo. 1994. Tagging text with a probabilistic
model. Computational Linguistics.
R. C. Moore, W. Yih, and A. Bode. 2006. Improved
discriminative bilingual word alignment. In ACL.
R. M. Neal and G. E. Hinton. 1998. A new view of
the EM algorithm that justifies incremental, sparse and
other variants. In M. I. Jordan, editor, Learning in
Graphical Models.
K. Nigam, A. K. Mccallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. CL, 29.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. ACL, 1(August).
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related op-
timization problems. In IEEE, pages 2210?2239.
D. Roth and K. Small. 2009. Interactive feature space
construction using semantic information. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
H. T. Ng and E. Riloff, editors, CoNLL.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In L. Getoor and B. Taskar, editors, In-
troduction to Statistical Relational Learning.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
A. Schrijver. 1986. Theory of linear and integer pro-
gramming. John Wiley & Sons, Inc.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
V. I. Spitkovsky, H. Alshawi, D. Jurafsky, and C. D. Man-
ning. 2010. Viterbi training improves unsupervised
dependency parsing. In CoNLL.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
HLT-EMNLP.
N. Ueda and R. Nakano. 1998. Deterministic annealing
em algorithm. Neural Network.
697
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In COLING.
698
Proceedings of NAACL-HLT 2013, pages 1020?1030,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
To Link or Not to Link? A Study on End-to-End
Tweet Entity Linking
Stephen Guo
Stanford University
sdguo@cs.stanford.edu
Ming-Wei Chang Emre K?c?man
Microsoft Research
{minchang, emrek}@microsoft.com
Abstract
Information extraction from microblog posts
is an important task, as today microblogs cap-
ture an unprecedented amount of information
and provide a view into the pulse of the world.
As the core component of information extrac-
tion, we consider the task of Twitter entity
linking in this paper.
In the current entity linking literature, mention
detection and entity disambiguation are fre-
quently cast as equally important but distinct
problems. However, in our task, we find that
mention detection is often the performance
bottleneck. The reason is that messages on
micro-blogs are short, noisy and informal texts
with little context, and often contain phrases
with ambiguous meanings.
To rigorously address the Twitter entity link-
ing problem, we propose a structural SVM
algorithm for entity linking that jointly op-
timizes mention detection and entity disam-
biguation as a single end-to-end task. By com-
bining structural learning and a variety of first-
order, second-order, and context-sensitive fea-
tures, our system is able to outperform exist-
ing state-of-the art entity linking systems by
15% F1.
1 Introduction
Microblogging services, such as Twitter and Face-
book, are today capturing the largest volume ever
recorded of fine-grained discussions spanning a
huge breadth of topics, from the mundane to the his-
toric. The micro-blogging service Twitter reports
that it alone captures over 340M short messages,
or tweets, per day.1 From such micro-blogging ser-
vices? data streams, researchers have reported min-
ing insights about a variety of domains, from elec-
tion results (Tumasjan et al, 2010) and democracy
movements (Starbird and Palen, 2012) to health is-
sues and disease spreading (Paul and Dredze, 2011;
Sadilek et al, 2012), as well as tracking prod-
uct feedback and sentiment (Asur and Huberman,
2010).
A critical step in mining information from a
micro-blogging service, such as Twitter, is the iden-
tification of entities in tweets. In order to mine
the relationship between drugs, symptoms and side-
effects, or track the popularity of politicians or sen-
timent about social issues, we must first be able to
identify the topics and specific entities being dis-
cussed. The challenge is that messages on micro-
blogs are short, noisy, and informal texts with little
context, and often contain phrases with ambiguous
meanings. For example, ?one day? may be either a
set phrase or a reference to a movie. Given such
difficulties, current mining and analysis of micro-
blogs lists limits its application to certain domains
with easy-to-recognize, unambiguous entities in or-
der to avoid noise in the extraction results.
We begin this paper with a thorough investigation
of mention detection and entity disambiguation for
social media, focused on the Twitter micro-blogging
service. Mention detection is the task of extraction
surface form candidates that can link to an entity in
the domain of interest. Entity disambiguation is the
task of linking an extracted mention to a specific def-
inition or instance of an entity in a knowledge base.
1http://blog.twitter.com/2012/03/twitter-turns-six.html
1020
While mention detection and entity disambigua-
tion are frequently cast as equally important but dis-
tinct and separate problems, we find that mention
detection is where today?s systems and our base-
line techniques incur the most failures. Detecting
the correct entity mention is a significant challenge
given mis-capitalizations, incorrect grammar, and
ambiguous phrases. In (Ritter et al, 2011), the au-
thors report their system achieves 0.64 to 0.67 F1 on
named entity segmentation results with 34K tokens
of labeled examples. On the other hand, once the
correct entity mention is detected, a trivial disam-
biguation that maps to the most popular entity2 will
achieve 85% accuracy in our set.
Our primary contribution in this paper is a re-
casting and merging of the tasks of mention detec-
tion and entity disambiguation into a single end-
to-end entity linking task. We achieve significant
improvements by applying structural learning tech-
niques to jointly optimize the detection and disam-
biguation of entities. Treating detection and disam-
biguation as a single task also enables us to apply a
large set of new features, conventionally used only
for disambiguation, to the initial detection of men-
tions. These features, derived from external knowl-
edge bases, include entity popularity and inter-entity
relations from external knowledge bases, and are not
well utilized in current mention detection systems.
For example, consider the following partial tweet:
(1) The town is so, so good. And don?t
worry Ben, we already forgave you
for Gigli. Really.
Determining whether or not ?The town? is a mention
of a location or other specific entity based solely on
lexical and syntactic features is challenging. Know-
ing ?The Town? is the name of a recent movie helps,
and we can we be more confident if we know that
Ben Affleck is an actor in the movie, and Gigli is
another of his movies.
To train and evaluate our system, we created three
separate annotated data sets of approximately 500
tweets each. These data sets are hand annotated
with entity links to Wikipedia. We evaluate our sys-
tem by comparing its performance at detecting en-
2What we mean here is ?the most linked entity?. See Sec-
tion 3 for details.
tities to the performance of two state-of-the-art en-
tity linking systems, Cucerzan (Cucerzan, 2007) and
TagMe (Ferragina and Scaiella, 2010), and find that
our system outperforms them significantly by 15%
in absolute F1.
The rest of this paper describes related work, our
structured learning approach to entity linking, and
our experimental results.
2 Related Work
Building an entity linking system requires solving
two interrelated sub-problems: mention detection
and entity disambiguation. The significant portion
of recent work in the literature (Ratinov et al, 2011;
Davis et al, 2012; Sil et al, 2012; Demartini et al,
2012; Wang et al, 2012; Han and Sun, 2011; Han
et al, 2011) focuses solely upon the entity linking
problem. The entity linking systems of these studies
assume that entity mentions are provided by a sepa-
rate mention detection system. In contrast, our study
jointly identifies and disambiguates entity mentions
within tweets (short text fragments).
A subset of existing literature targets end-to-end
linking (Cucerzan, 2007; Milne and Witten, 2008;
Kulkarni et al, 2009; Ferragina and Scaiella, 2010;
Han and Sun, 2011; Meij et al, 2012), but there
are quite a few differences between our work and
each of these systems. Some systems (Milne and
Witten, 2008; Kulkarni et al, 2009; Han and Sun,
2011) heavily depend on Wikipedia text and might
not work well in short and noisy tweets. Many sys-
tems (Mihalcea and Csomai, 2007; Cucerzan, 2007;
Milne and Witten, 2008; Ferragina and Scaiella,
2010) treat mention detection and entity disam-
biguation as two different problems. (Meij et al,
2012) is the most related to our paper. While their
system also considers mention detection and entity
disambiguation together, they do not consider entity-
to-entity relationships and do not incorporate con-
textual words from tweets.
An area of work closely related to the mention
detection problem is the Named Entity Recogni-
tion (NER) problem, the identification of textual
phrases which belong to core categories (Person,
Location, Organization). It is well-known that NER
systems trained on well-written documents perform
very poorly on short, noisy text, such as tweets (Rit-
1021
ter et al, 2011). There have been a few recent stud-
ies proposing Twitter-specific NER systems (Li et
al., 2012; Ritter et al, 2011).
3 Preliminaries
For performing entity linking on Twitter, we choose
Wikipedia as our external knowledge base of enti-
ties.
Entity We define an entity as a nonambiguous, ter-
minal page (e.g., The Town (the film)) in Wikipedia
(i.e., a Wikipedia page that is not a category, dis-
ambiguation, list, or redirect page). We define an
anchor phrase (surface form) as the textual phrase
(e.g., the town) which can potentially link to some
entities. We define an entity mention as an anchor
phrase and the context (?the town? in the exam-
ple tweet in Section 1), where its semantic meaning
umambiguously represents a specific entity. Note
that an entity may be represented by multiple sur-
face forms.
Wikipedia Lexicon Construction Following the
assumptions used in most prior entity linking re-
search, we assume that surface forms of entities can
be found as anchor phrases in Wikipedia. In or-
der to construct a Wikipedia lexicon, we first collect
all anchors phases in Wikipedia. For each anchor
phrase (surface form) s, we construct a lexicon en-
try by gathering the set of entities {e1, e2, . . . eK}
that can be linked from s. We also collect the num-
ber of times anchor a links to the entity ei, d(s, ei).
We define P (ei|s) = d(s, ei)/d(s), where d(s) rep-
resents the number of times s appears in Wikipedia.
We refer e? as the most linked entity for anchor s if
e? = arg maxe P (ei|s).
Candidate Generation Given a tweet t, we ex-
tract all k-grams of size ? k. For each k-gram,
we find all entities where this k-gram is an anchor
phrase. If a k-gram is an anchor phrase for at least
one entity, then the k-gram is a candidate entity
mention. In general, we identify many candidate
phrase per tweet; let U(t) = {c1, c2, . . .} denote
the set of candidates in tweet t. We refer to s(c)
as the surface form (e.g., the anchor phrase) of c.
Compared to the anchor phrase, the candidate also
carries the context and position information. Let
E(ci) = {e1, e2, . . . ,NIL} denote the set of entities
which candidate i may be linked to, plus the addi-
tional special token NIL. Note that the size of E(ci)
is always at least 2.
Task Definition First, our system generates candi-
date entity mentions, textual phrases which can pos-
sibly be entity mentions. Our system then performs
filtering and optimization to process the list of can-
didates. For each candidate, our system links the
candidate to a special NIL token or links the candi-
date to its corresponding entity in Wikipedia. More
formally, given a tweet t and its candidate set U(t),
the goal of the system is to predict yi ? E(ci),?ci ?
U(t).
Comparison to the TAC KBP Competition It is
important to state that our definition of the entity
linking problem differs significantly from the entity
linking problem as defined by the TAC KBP com-
petition (Ji et al, 2010; Ji et al, 2011). In the TAC,
there is no true mention detection problem; every
candidate in the TAC is an entity mention that rep-
resents an entity. Another difference is that the TAC
allows for an entity mention to map to an entity not
in the external knowledge base (Wikipedia); our sys-
tem does not provide special handling of this case.
Comparison to Named Entity Recognition
There are also important differences between our
task and the canonical NER task. For example,
NER systems identify common names, such as
?Robert,? as entities. In our task, we only consider a
prediction as a success if the system can determine
which person in Wikipedia ?Robert? is referring to.
In other words, our definition of entities depends
on the given knowledge base, rather than human
judgment. Hence, it is difficult to make a fair system
comparison of our system to NER systems.
4 Entity Linking as Structural Learning
In our framework, we use structural learning as a
tool to capture the relationship between entities. We
define yi as the output for ci, where yi ? E(ci). Let
T = |U(t)| and y = {y1, y2, . . . , yT }. The fea-
ture function for the whole assignment can be writ-
ten as ?(t, U(t),y). The score for the assignment
y can be obtained as the linear product between the
weight vector w and the feature vector. For an input
example, the prediction can be found by solving the
1022
inference problem:
y? = arg max
y
wT?(t, U(t),y) (1)
We use a Structural SVM (SSVM) (Taskar et
al., 2004; Tsochantaridis et al, 2005; Chang et al,
2010) as our learning algorithm. To train the weight
vector w, we minimize the objective function of the
SSVM
min
w
?w?2
2
+ C
l?
i=1
?2i (2)
where l is the number of labeled examples and
wT?(ti, c(ti),yi)
??(yi,y) + wT?(ti, c(ti),y)? ?i, ?i,y
We denote yi as the gold assignment for xi and de-
fine ?(yi,y) as the Hamming distance between two
assignments yi and y.
4.1 Features
Feature definitions are very important as they define
the shapes of the structures. Our feature vector is
defined as
?(t, U(t),y) =
?
i
?(t, ci, yi)+
?
i<j
?(t, ci, yi, cj , yj)
where ci and cj is the i-th and j-th candidates in
U(t), respectively.
First, we assign ?(t, ci,NIL) to be a special bias
feature. The corresponding weight value behaves as
a threshold to cut-off mentions. Recall in our defini-
tion that yi = NIL represents that the candidate ci is
not a mention.
The first order features for ?(t, ci, e) are de-
scribed as follows. In general, we can classify our
features into two types: mention-specific features
and entity-specific features. For a given candidate
ci, mention-specific features only consider the sur-
face form of ci and the tweet t. Entity-specific fea-
tures also consider the knowledge base content of
the entity e. Prior work in the entity linking liter-
ature has primarily focused on entity-specific fea-
tures, as most prior work solves entity disambigua-
tion with given mentions.
Base and Capitalization Rate Our base features
are from two resources. Let s(c) denote the sur-
face form of candidate c. The link probability
Pl(s(c)) and P (e|s(c)) features are extracted from
Wikipedia. We explained P (e|s(c)) in Section 3.
Link probability Pl(s(c)) is the probability that a
phrase is used as an anchor in Wikipedia. We also
add a third feature that captures normalized link
count. Besides these three features, we also have
a feature to indicate if a is a stop word, and a fea-
ture indicating the number of tokens in a. The view
count and P (e|s) features are entity-specific, while
the other three features are mention-specific.
For each phrase s(c), we also collect statistics
about the probability that a phrase is capitalized in
Wikipedia. We refer to this feature as the capitaliza-
tion rate feature, Pc(s(c)).
Popularity Feature We have access to 300GBs
of Wikipedia page view counts, representing one
months worth of page view information, we use
this as popularity data.3 As mentioned in Sec-
tion 3, we find that the most often linked Wikipedia
articles might not be the most popular ones on
Twitter. Using page view statistics helps our sys-
tem correct this bias. We define another prob-
ability based on page view statistics Pv(ei|c) =
v(ei)/(
?
e?E(c)/{NIL} v(e)), where v(e) represents
the view count for the page e.
Context Capitalization Our context capitaliza-
tion features indicate if the current candidate, the
word before, and the word after the candidate are
capitalized.
Entity Type and Tf-idf We use the procedure pro-
posed in (Ratinov et al, 2011) to extract keyword
phrases from categories for each Wikipedia page,
and then build a rule-based system using keyword
phrases to classify if each entity page belongs to one
of the following entity types: Person, Location, Or-
ganization, TV Show, Book/Magazine and Movie.4
For a given candidate c and an entity e, the associ-
ated binary feature becomes active if the entity be-
longs to a specific entity type. There are six entity
type features in our system.
3http://dammit.lt/wikistats
4The entity type prediction accuracy of our rule-based sys-
tem on the development set is around 95%.
1023
Features Descriptions
Base Pl(si), P (e|s), normalized link counts, stop
word, # tokens
Cap. Rate Pc(si)
Popularity Pv(e|s), normalized page view count,
Pv(e|s)P (e|s)
Context Cap. Three features indicating if the current candi-
date and the words before and after are capi-
talized
Entity Type Six binary features for each entity type
Tf-idf Two features for the similarity between the
word vectors of the entity and the tweet
Second-Order Jac(ei, ej), P (ei|si)P (ej |sj), Pc(si)Pc(sj),
Pl(si)Pl(sj)
Table 1: Summary of the features used in our structural
learning systems.
We also include tf-idf features in our system. For
each Wikipedia page, we collect the top 100 tf-idf
words. We add one feature that is the dot product
between the tf-idf word vector of e and the words of
tweet t. We include a second feature that represents
the average tf-idf score of all words that appear in
both e and t.
Second-order features We include four very sim-
ple second-order features ?(t, ci, ei, cj , ej) to cap-
ture more complex relations between entities and
candidates. The first feature is the Jaccard distance
between two Wikipedia pages ei and ej . Let ?(ei)
denote the set of Wikipedia pages that contain a hy-
perlink to ei. We define the Jaccard distance be-
tween ei and ej as:
Jac(ei, ej) =
|?(ei) ? ?(ej)|
|?(ei) ? ?(ej)|
This feature has a similar effect as the normal-
ized Google distance (Cilibrasi and Vitanyi, 2007),
which has been used for many entity linking sys-
tems. Let us use the following shorthand: si = s(ci)
and sj = s(cj). We have also included three features
P (ei|si)P (ej |sj), Pc(si)Pc(sj) and Pl(si)Pl(sj) to
increase the expressivity of our model.
4.2 Mining Additional Contextual Words
Unlike mention detection systems used in other NLP
tasks, there are no lexical features in our system.
Lexical features are important as they can capture
semantic meaning precisely. However, given that
we do not have many labeled examples, lexical fea-
tures can lead to overfitting. The diverse language
in tweets also make it more difficult to use lexical
features.
Our solution for this problem is to use a very sim-
ple method to mine context words for different enti-
ties from a large, unlabeled tweet corpus. The algo-
rithm works as follows:
1. Train an end-to-end entity linking system and
then apply it to a large, unlabeled tweet corpus
2. Extract contextual words for each entity type
based on the pseudo-labeled data.
3. Train the entity linking system again with new
contextual features.
In this paper, we only use the word before and the
word after as our contextual word for a candidate.
Note that while there are ambiguous phrases on the
surface (e.g., ?friends? can be a TV show or just
a regular phrase), certain phrases are unambiguous
(e.g., ?CSI : Miami?). As contextual words are often
shared within the same entity type (e.g. ?watching?
is likely to appear before a tv show), those words can
potentially improve our final system.
Let wi denote the i-th word in the tweet and ti
denote the entity type for the i-th word.5 We use a
very simple rule to select a set of left context words
Q(R) for entity type R.
Q(R) = {wi | P (ti+1 = R|wi) > r, d(wi) > z}
where d(wi) represent the number of times the word
wi appears in the unlabeled set. The first rule is to
simply find a word which is more likely to be fol-
lowed by an entity. The second rule filter outs noisy
words (e.g., Twitter handles) in the unlabeled set.
The right context words are also extracted in a simi-
lar way.
To train the second end-to-end entity linking sys-
tem, we add one additional feature for the contextual
words. For the feature vector ?(t, ci, e), the context
feature is active if the candidate ci is capitalized6 and
the context words around ci belongs to Q(R), given
R is the entity type for the entity e.
5The tag ti belongs to the entity type R if our system links a
candidate c to an entity with type R and c covers the word wi.
6The word ?watching? can be a TV show while most of the
time it is not. These common makes this contextual feature
noisy. We found that the context feature can only be reliably
applied when the candidate is capitalized.
1024
4.3 Cohesiveness Score
There are several ways to consider entity-entity co-
hesiveness besides using the second-order features
directly. In our model, we also consider a modi-
fied cohesiveness score proposed in (Ferragina and
Scaiella, 2010). The idea behind the cohesiveness
score is to estimate the correlations between differ-
ent entities by using weighted Jaccard scores.7
There are two rounds in the procedure of com-
puting the cohesiveness score. We first estimate ap-
proximately the most probable entity for each candi-
date given all the other candidates in the same tweet.
In the second round, the cohesiveness score is then
produced with respect to the most probable entity
computed in the first round.
More formally, in the first round, we compute the
relevance score for each candidate and entity pair:
Rel(e, c|t) =
?
c? 6=c
?
e??E(c?) P (e
?|c?)Jac(e, e?)
|U(t)|
.
Then, the cohesiveness score is computed by
Scoh(e, c|t) =
?
c? 6=c Jac(e, e?(c
?))P (e?(c?)|c?)
|U(t)|
,
where the e?(c?) = arg maxe?E(c?)Rel(e, c
?|t). We
then put the cohesiveness score as a feature for each
(e, c) pair. In practice, we found that the cohesive-
ness score in the model can significantly increase the
disambiguation ability of the model without using
the second-order information.
4.4 Inference
In order to train and test the SSVM model, one needs
to solve both the inference problem Eq. (3) and the
loss-augmented inference problem. Without second-
order features, the inference and loss-augmented in-
ference problems can be easily solved, given that
each component can be solved independently by
y?i = arg max
y?E(ci)
wT?(t, ci, y) (3)
While the inference problem can be solved inde-
pendently, the training algorithm still considers the
whole assignment together in the training procedure.
7In our experiments, we only apply the cohesiveness score
technique on candidates which pass the filtering procedure. See
section 5 for more details for our filtering process.
Data #Tweets #Cand #Men. P@1
Train 473 8212 218 85.3%
Test 1 500 8950 249 87.7%
Test 2 488 7781 332 89.6%
Table 2: Labeled example statistics. ?#Cand? represents
the total number of candidates we found in this dataset.
?#Men.? is the total number of mentions that disam-
biguate to an entity. The top-1 rate (P@1) represents the
proportion of the mentions that disambiguate to the most
linked entity in Wikipedia.
With the second-order features, the inference
problem becomes NP-hard. While one can resort to
using integer linear programming to find the optimal
solution, we choose not to do so. We instead use the
beam search algorithm. Our beam search algorithm
first arranges the candidates from left to right, and
then solve the inference problems approximately.
5 Experiments
We collected unlabeled Twitter data from two re-
sources and then asked human annotators to label
each tweet with a set of entities present. Our anno-
tators ignored the following: duplicate entities per
tweet, ambiguous entity mentions, and entities not
present in Wikipedia. We next describe the two sets
of Twitter data used as our training data and test-
ing data. In addition to these two datasets, we also
randomly sampled another 200 tweets as our devel-
opment set.
Ritter We sampled 473 and 500 tweets8 from the
data used in (Ritter et al, 2011) to be our training
data and test data, respectively. We did not use any
labels generated by (Ritter et al, 2011); our annota-
tors completely re-annotated each tweets with its set
of entities. We refer to the first set as Train and the
second set as Test 1.
Entertainment To check if our system has the
ability to generalize across different domains, we
sampled another 488 tweets related to entertain-
ment entities. Our main focus was to extract
tweets that contained TV shows, Movies, and
8We originally labeled 1000 tweets but then found 27 re-
peated tweets in the dataset. Therefore, we remove those 27
tweets in the training set.
1025
Books/Magazines. Identifying tweets from a spe-
cific domain is a research topic on its own, so we
followed (Dalvi et al, 2012), and used a keyword
matching method.9 After sampling this set of tweets,
we asked our annotators to label the data in the same
way as before (all entities are labeled, not just en-
tertainment entities). We refer to this tweet set as
Test 2.
After sampling, all tweets were then normalized
in the following way. First, we removed all retweet
symbols (RT) and special symbols, as these are to-
kens that may easily confuse NER systems. We
treated punctuation as separate tokens. Hashtags (#)
play a very important role in tweets as they often
carry critical information. We used the following
web service10 to break the hashtags into tokens (e.g.,
the service will break ?#TheCloneWars? into ?the
clone wars?) (Wang et al, 2011).
The statistics of our labeled examples are pre-
sented in Table 2. First, note that the average number
of mentions per tweet is well below 1. In fact, many
tweets are personal conversations and do not carry
any entities that can be linked to Wikipedia. Still,
many candidates are generated (such as ?really?) for
those tweets, given that those candidates can still po-
tentially link to an entity (?really? could be a TV
channel). Therefore, it is very important to include
tweets without entities in the training set because we
do not want our system to create unnecessary links
to entities.
Another interesting thing to note is the percent-
age of entity mentions that disambiguate directly to
their most often linked entities in Wikipedia. If we
simply disambiguate each entity mention to its most
linked entity in Wikipedia, we can already achieve
85% to 90% accuracy, if mention detection is per-
fectly accurate. However, mention detection is a dif-
ficult problem as only about 3% of candidates are
valid entity mentions.
It is worthwhile to mention that, as per (Ferragina
and Scaiella, 2010), for computational efficiency,
9We use the following word list :?movie?, ?tv?, ?episode?,
?film?, ?actor?, ?actors?, ?actress?, ?director?, ?directors?,
?movies?, ?episodes?, ?book?, ?novel?, ?reading?, ?read?,
?watch?, ?watching?, ?show?, ?books?, ?novels?, ?movies?,
?author? and ?authors?.
10http://web-ngram.research.microsoft.
com/info/break.html
we apply several preprocessing steps before running
our entity linking system. First, for each anchor in
Wikipedia, we gather all entities it can disambiguate
to and remove from that anchor?s entity set al enti-
ties that are linked less than 2% of the time. Second,
we apply a modified filtering procedure similar to
that proposed in (Ferragina and Scaiella, 2010) to
filter the set of candidates per tweet.
Evaluation Our annotated datasets contain enti-
ties from many Wikipedia categories. For eval-
uation, we primarily focus on entities belonging
to a set of six core categories (Person, Location,
Organization, TV Show, Book/Magazine, Movie).
We believe it is necessary to focus upon core en-
tities, rather than considering all possible entities
in Wikipedia. Most common words in the English
language have their own Wikpedia page, but most
words are not important enough to be considered en-
tities. In general, there is a large degree of subjectiv-
ity when comparing different entity linking datasets;
different researchers have their own interpretation of
what constitutes an entity. For example, we exam-
ined the annotation used in (Meij et al, 2012) and
found it to be extremely lenient, when compared to
our own beliefs of what is an entity. Therefore, we
believe evaluating performance on restricted entity
types is the only fair way to compare different end-
to-end entity linking systems.
We evaluate the performance of our system on
a per-tweet basis, by comparing the set of anno-
tated ?gold? entities with the set of entities predicted
by our system, and computing performance metrics
(precision, recall, F1). We choose to evaluate our
system on a per-tweet basis, as opposed to a per-
entity basis, because we wish to avoid the issue of
matching segmentations. For example, it is quite
common to observe multiple overlapping phrases in
a tweet that should be linked to the same entity (e.g.,
?President Obama? and ?Obama?). When evaluat-
ing our system, we compute performance metrics for
both all entities and core entities.11
Parameters In our implementation, we fixed the
regularization parameter C = 10. When beam-
11To decide if an entity is a core entity or not, we use the
following procedure. For the gold entities, the annotators also
annotate type of the entity. We decide the entity type of the
predicted entities using the procedure described in Section 4.1.
1026
Model
Test 1 Test 2
P R F1 P R F1
Cucerzan 64.8 42.2 51.1 64.9 39.7 49.5
TagMe 38.8 69.0 49.7 34.9 70.3 46.7
SSVM 78.8 59.9 68.0 75.0 57.7 65.2
Table 3: Comparisons between different end-to-end en-
tity linking systems. We evaluate performance on core
entities, as it is the only fair way to compare different
systems.
search is used, the beam size is set to be 50, and
we only consider the top 10 candidates for each can-
didate to speed the inference process. In the context
word mining algorithm, r = 0.5% and z = 1000.
5.1 Results
In the following, we analyze the contributions of
each component in our system and compare our final
systems to other existing end-to-end entity linking
systems.
System Comparison We compare our final sys-
tem to other state-of-the-art systems in Table 3.
CUCERZAN represents a modified implementation
of the system in (Cucerzan, 2007). TagMe is an end-
to-end linking system that focuses on short texts,
including tweets. Our system significantly outper-
forms these two systems in both precision and re-
call. Note that CUCERZAN?s system is a state-of-
the-art system on well-written documents with pro-
vided entity mentions. The system (Cucerzan, 2007)
has been extended by the authors and won the TAC
KBP competition in 2010 (Ji et al, 2010).
There are two possible reasons to explain why our
system outperforms CUCERZAN. First, their men-
tion detection is a carefully designed system targeted
toward documents, not tweets. Their system has seg-
mentation issues when applied to Twitter, as it relies
heavily upon capitalization when identifying candi-
date entity mentions. Second, their system heav-
ily depends on the fact that related entities should
appear together within documents. However, given
that tweets are very short, some of their most impor-
tant features are not suitable for the Twitter domain.
Our system outperforms TagMe because we use a
more sophisticated machine learning approach, as
compared to their system. TagMe links too many
Structural SVM
Test 1 Test 2
All Core All Core
Base 35.9 42.9 47.7 52.5
+Cap. Rate 38.4 45.6 49.9 53.7
+Popularity 41.3 47.9 50.3 55.1
+Context Cap 43.7 52.0 50.7 54.8
+Entity Type 47.9 57.0 53.5 59.0
+Tfidf 53.2 63.1 56.8 61.9
Table 4: Feature Study: F1 for entity linking perfor-
mance. ?All? means evaluation on all annotated entities.
?Core? means evaluation only on our six entity types.
Each row contains all additional features of the row above
it.
spurious entity mentions for common words. This is
a result of their algorithm?s over-emphasis on entity-
entity co-occurrence features.
Feature Study We study the contributions of each
feature group in our system in Table 4. We summa-
rize our discoveries as follows:
First, we find collecting statistics from a large cor-
pus helps the system significantly. In addition to
P (e|s), we find that capitalization rate features of-
fer around 3% to 4% F1 improvement in Test 1.
Similarly, popularity features are also important, as
it corrects bias existing in Wikipedia link statistics.
Compared to lexical features, using statistical fea-
tures offers a great advantage of reducing the need
for large amounts of labeled data.
We also find entity related features (Popularity,
Entity Type, Tf-idf) are crucial. Given that between
85% to 90% of our mentions should directly disam-
biguate to the most often linked entities, one might
think entity-specific features are not important in
our task. Interestingly, entity-specific features are
among the most important features. The discovery
confirms our hypothesis: it is critical to consider
mention detection and entity disambiguation as a
single problem, rather than as separate problems in
a two staged approach used by many other entity
linking systems. Note that capitalization rate and
context capitalization features are mention-specific.
Additionally, we find that mixing mention-specific
features and entity-specific features results in a bet-
ter model.
1027
Entity Type Words appearing before
the mention
Words appearing after the
mention
Person wr, dominating, rip, quar-
terback, singer, featuring,
defender, rb, minister, ac-
tress, twitition, secretary
tarde, format, noite, suf-
fers, dire, admits, sen-
ators, urges, performs,
joins
TV Show sbs, assistir, assistindo,
otm, watching, nw,
watchn, viagra, watchin,
ver
skit, performances,
premieres, finale, par-
ody, marathon, season,
episodes, spoilers, sketch
Table 5: An example of context words that are automati-
cally extracted from 20 million unlabeled tweets. For the
sake of brevity, we only display context words for two
categories. Note that there are misspelled words (such
as ?watchn?) and abbreviations (such as nw) that do not
appear in well-written documents.
Advance Models
Test 1 Test 2
All Core All Core
SSVM (Table 4) 53.2 63.1 56.8 61.9
+Context 53.9 64.6 58.6 63.4
+Cohesiveness 55.6 66.5 59.7 65.1
+2nd order 58.1 68.0 60.6 65.2
Table 6: Evaluation results (F1) of the advanced models.
?+ Context? is the model that uses additional context fea-
tures extracted from 20 millions unlabeled tweets. ?+ Co-
hesiveness? is the model with both additional context and
cohesiveness features. ?+2nd order? is our final model
(which incorporates context, cohesiveness, and second-
order features).
Mining Context Words We verify the effective-
ness of adding contextual features that are extracted
automatically from large unlabeled data. We apply
our system (with all first-order features) on a set of
20 million unlabeled tweets we collected. Context
words are then extracted using the simple rules de-
scribed in Section 4. We list the top 10 words we
extracted in Table 5. Due to space limitations, we
only list the words for the Person and TV Show cat-
egories. The results are interesting as we are able
to find common misspelled words and abbreviations
used in Twitter. For example, we find that ?watchn?
means ?watching? and ?nw? means ?now watching,?
and they are usually words found before TV shows.
We also find tweeters frequently use abbreviations
for people?s jobs. For example, ?wr? means ?wide
receiver? and ?rb? means ?running back.? When
mined context is added into our system, the perfor-
mance improves significantly (Table 6). We note
that extending context mining algorithms in a large-
scale, principled approach is an important next re-
search topic.
Capturing Entity-Entity Relationships In this
paper, we use two methods to capture the relation-
ship between entities: adding the cohesiveness score
and using second order information. Until now, we
only considered features that can be extracted from
only one entity. Past research has shown that consid-
ering features that involve multiple entities can im-
prove entity linking performance, given that related
entities are more likely to appear together in a doc-
ument. When these type of features are added, we
need to perform beamsearch, as the exact inference
procedure can be prohibitively expensive.
As displayed in Table 6, we find that either adding
the cohesiveness score or using second order infor-
mation can improve prediction. Using both methods
improves the model even more. Comparing compu-
tation overhead, computing cohesiveness is signifi-
cantly more cost-effective than using second-order
information.
6 Conclusion
In this paper, we propose a structural SVM method
to address the problem of end-to-end entity linking
on Twitter. By considering mention detection and
entity disambiguation together, we build a end-to-
end entity linking system that outperforms current
state-of-the-art systems.
There are plenty of research problems left to be
addressed. Developing a better algorithm for min-
ing contextual words is an important research topic.
It would also be interesting to design a method that
jointly learns NER models and entity linking mod-
els.
References
S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. arXiv preprint arXiv:1003.5699.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML).
R.L. Cilibrasi and P.M.B. Vitanyi. 2007. The google
similarity distance. Knowledge and Data Engineering,
IEEE Transactions on, 19(3):370?383.
1028
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
the 2007 Joint Conference of EMNLP-CoNLL, pages
708?716.
N. Dalvi, R. Kumar, and B. Pang. 2012. Object match-
ing in tweets with spatial models. In Proceedings of
the fifth ACM international conference on Web search
and data mining, WSDM ?12, pages 43?52, New York,
NY, USA. ACM.
A. Davis, A. Veloso, A. S. da Silva, W. Meira, Jr., and
A. H. F. Laender. 2012. Named entity disambiguation
in streaming data. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 815?824, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
G. Demartini, D. E. Difallah, and P. Cudre?-Mauroux.
2012. Zencrowd: leveraging probabilistic reasoning
and crowdsourcing techniques for large-scale entity
linking. In The International World Wide Web Con-
ference, pages 469?478, New York, NY, USA. ACM.
P. Ferragina and U. Scaiella. 2010. Tagme: on-the-fly
annotation of short text fragments (by wikipedia enti-
ties). In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, CIKM ?10, pages 1625?1628, New York, NY,
USA. ACM.
X. Han and L. Sun. 2011. A generative entity-mention
model for linking entities with knowledge base. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 945?
954, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
ceedings of the 34th international ACM SIGIR con-
ference on Research and development in Information
Retrieval, SIGIR ?11, pages 765?774, New York, NY,
USA. ACM.
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis.
2010. Overview of the tac 2010 knowledge base pop-
ulation track. In Proceedings of the TAC 2010 Work-
shop.
H. Ji, R. Grishman, and Dang. 2011. Overview of the tac
2011 knowledge base population track. In Proceed-
ings of the TAC 2011 Workshop.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, Proceedings
of International Conference on Knowledge Discovery
and Data Mining (KDD), pages 457?466, New York,
NY, USA. ACM.
C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and B.-S.
Lee. 2012. Twiner: named entity recognition in tar-
geted twitter stream. In Proceedings of the 35th inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, Proceedings of In-
ternational Conference on Research and Development
in Information Retrieval, SIGIR, pages 721?730, New
York, NY, USA. ACM.
E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding
semantics to microblog posts. In Proceedings of the
fifth ACM international conference on Web search and
data mining, pages 563?572, New York, NY, USA.
ACM.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In Proceedings
of ACM Conference on Information and Knowledge
Management (CIKM), pages 233?242. ACM.
D. Milne and I. H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 509?518, New York, NY, USA. ACM.
M.J. Paul and M. Dredze. 2011. You are what you tweet:
Analyzing twitter for public health. In Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM 2011).
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1375?1384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 1524?1534, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Sadilek, H. Kautz, and V. Silenzio. 2012. Model-
ing spread of disease from social interactions. In Sixth
AAAI International Conference on Weblogs and Social
Media (ICWSM).
A. Sil, E. Cronin, P. Nie, Y. Yang, A.-M. Popescu,
and A. Yates. 2012. Linking named entities to
any database. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP), pages 116?127, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
K. Starbird and L. Palen. 2012. (how) will the revolu-
tion be retweeted?: information diffusion and the 2011
egyptian uprising. In Proceedings of the acm 2012
conference on computer supported cooperative work,
pages 7?16. ACM.
1029
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin
markov networks. In The Conference on Advances in
Neural Information Processing Systems (NIPS).
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453?1484, September.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter: What
140 characters reveal about political sentiment. In
Proceedings of the fourth international aaai confer-
ence on weblogs and social media, pages 178?185.
K. Wang, C. Thrasher, and B.J.P. Hsu. 2011. Web scale
nlp: a case study on url word breaking. In The Inter-
national World Wide Web Conference, pages 357?366.
ACM.
C. Wang, K. Chakrabarti, T. Cheng, and S. Chaudhuri.
2012. Targeted disambiguation of ad-hoc, homoge-
neous sets of named entities. In The International
World Wide Web Conference, pages 719?728, New
York, NY, USA. ACM.
1030
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744?1753,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Question Answering Using Enhanced Lexical Semantic Models
Wen-tau Yih Ming-Wei Chang Christopher Meek Andrzej Pastusiak
Microsoft Research
Redmond, WA 98052, USA
{scottyih,minchang,meek,andrzejp}@microsoft.com
Abstract
In this paper, we study the answer
sentence selection problem for ques-
tion answering. Unlike previous work,
which primarily leverages syntactic analy-
sis through dependency tree matching, we
focus on improving the performance us-
ing models of lexical semantic resources.
Experiments show that our systems can
be consistently and significantly improved
with rich lexical semantic information, re-
gardless of the choice of learning algo-
rithms. When evaluated on a bench-
mark dataset, the MAP and MRR scores
are increased by 8 to 10 points, com-
pared to one of our baseline systems using
only surface-form matching. Moreover,
our best system also outperforms pervious
work that makes use of the dependency
tree structure by a wide margin.
1 Introduction
Open-domain question answering (QA), which
fulfills a user?s information need by outputting di-
rect answers to natural language queries, is a chal-
lenging but important problem (Etzioni, 2011).
State-of-the-art QA systems often implement a
complicated pipeline architecture, consisting of
question analysis, document or passage retrieval,
answer selection and verification (Ferrucci, 2012;
Moldovan et al, 2003). In this paper, we focus
on one of the key subtasks ? answer sentence se-
lection. Given a question and a set of candidate
sentences, the task is to choose the correct sen-
tence that contains the exact answer and can suf-
ficiently support the answer choice. For instance,
although both of the following sentences contain
the answer ?Jack Lemmon? to the question ?Who
won the best actor Oscar in 1973?? only the first
sentence is correct.
A1: Jack Lemmon won the Academy Award for
Best Actor for Save the Tiger (1973).
A2: Oscar winner Kevin Spacey said that Jack
Lemmon is remembered as always making
time for other people.
One of the benefits of answer sentence selec-
tion is that the output can be provided directly to
the user. Instead of outputting only the answer, re-
turning the whole sentence often adds more value
as the user can easily verify the correctness with-
out reading a lengthy document.
Answer sentence selection can be naturally re-
duced to a semantic text matching problem. Con-
ceptually, we would like to measure how close
the question and sentence can be matched seman-
tically. Due to the variety of word choices and
inherent ambiguities in natural languages, bag-of-
words approaches with simple surface-form word
matching tend to produce brittle results with poor
prediction accuracy (Bilotti et al, 2007). As a
result, researchers put more emphasis on exploit-
ing both the syntactic and semantic structure in
questions/sentences. Representative examples in-
clude methods based on deeper semantic anal-
ysis (Shen and Lapata, 2007; Moldovan et al,
2007) and on tree edit-distance (Punyakanok et
al., 2004; Heilman and Smith, 2010) and quasi-
synchronous grammar (Wang et al, 2007) that
match the dependency parse trees of questions and
sentences. However, such approaches often re-
quire more computational resources. In addition
to applying a syntactic or semantic parser during
run-time, finding the best matching between struc-
tured representations of sentences is not trivial.
For example, the computational complexity of tree
matching is O(V 2L4), where V is the number of
nodes and L is the maximum depth (Tai, 1979).
Instead of focusing on the high-level seman-
tic representation, we turn our attention in this
work to improving the shallow semantic compo-
1744
nent, lexical semantics. We formulate answer se-
lection as a semantic matching problem with a la-
tent word-alignment structure as in (Chang et al,
2010) and conduct a series of experimental stud-
ies on leveraging recently proposed lexical seman-
tic models. Our main contributions in this work
are two key findings. First, by incorporating the
abundant information from a variety of lexical se-
mantic models, the answer selection system can
be enhanced substantially, regardless of the choice
of learning algorithms and settings. Compared to
the previous work, our latent alignment model im-
proves the result on a benchmark dataset by a wide
margin ? the mean average precision (MAP) and
mean reciprocal rank (MRR) scores are increased
by 25.6% and 18.8%, respectively. Second, while
the latent alignment model performs better than
unstructured models, the difference diminishes af-
ter adding the enhanced lexical semantics infor-
mation. This may suggest that compared to in-
troducing complex structured constraints, incorpo-
rating shallow semantic information is both more
effective and computationally inexpensive in im-
proving the performance, at least for the specific
word alignment model tested in this work.
The rest of the paper is structured as follows.
We first survey the related work in Sec. 2. Sec. 3
defines the problem of answer sentence selection,
along with the high-level description of our solu-
tion. The enhanced lexical semantic models and
the learning frameworks we explore are presented
in Sec. 4 and Sec. 5, respectively. Our experimen-
tal results on a benchmark QA dataset is shown in
Sec. 6. Finally, Sec. 7 concludes the paper.
2 Related Work
While the task of question answering has a long
history dated back to the dawn of artificial in-
telligence, early systems like STUDENT (Wino-
grad, 1977) and LUNAR (Woods, 1973) are typ-
ically designed to demonstrate natural language
understanding for a small and specific domain.
The Text REtrieval Conference (TREC) Question
Answering Track was arguably the first large-
scale evaluation of open-domain question answer-
ing (Voorhees and Tice, 2000). The task is de-
signed in an information retrieval oriented setting.
Given a factoid question along with a collection
of documents, a system is required to return the
exact answer, along with the document that sup-
ports the answer. In contrast, the Jeopardy! TV
quiz show provides another open-domain question
answering setting, in which IBM?s Watson system
famously beat the two highest ranked players (Fer-
rucci, 2012). Questions in this game are presented
in a statement form and the system needs to iden-
tify the true question and to give the exact answer.
A short sentence or paragraph to justify the answer
is not required in either TREC-QA or Jeopardy!
As any QA system can virtually be decomposed
into two major high-level components, retrieval
and selection (Echihabi and Marcu, 2003), the an-
swer selection problem is clearly critical. Limiting
the scope of an answer to a sentence is first high-
lighted by Wang et al (2007), who argued that it
was more informative to present the whole sen-
tence instead of a short answer to users.
Observing the limitations of the bag-of-words
models, Wang et al (2007) proposed a syntax-
driven approach, where each pair of question and
sentence are matched by their dependency trees.
The mapping is learned by a generative probabilis-
tic model based on a Quasi-synchronous Gram-
mar formulation (Smith and Eisner, 2006). This
approach was later improved by Wang and Man-
ning (2010) with a tree-edit CRF model that learns
the latent alignment structure. In contrast, gen-
eral tree matching methods based on tree-edit dis-
tance have been first proposed by Punyakanok et
al. (2004) for a similar answer selection task. Heil-
man and Smith (2010) proposed a discriminative
approach that first computes a tree kernel func-
tion between the dependency trees of the question
and candidate sentence, and then learns a classifier
based on the tree-edit features extracted.
Although lexical semantic information derived
from WordNet has been used in some of these
approaches, the research has mainly focused
on modeling the mapping between the syntac-
tic structures of questions and sentences, pro-
duced from syntactic analysis. The potential im-
provement from enhanced lexical semantic mod-
els seems to have been deliberately overlooked.1
3 Problem Definition
We consider the answer selection problem in a
supervised learning setting. For a set of ques-
tions {q1, ? ? ? , qm}, each question qi is associated
with a list of labeled candidate answer sentences
1For example, Heilman and Smith (2010) emphasized that
?The tree edit model, which does not use lexical semantics
knowledge, produced the best result reported to date.?
1745
What is the fastest car in the world?
The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet. 
Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan,
2001). Words connected by solid lines are clear synonyms or hyponym/hypernym; words with weaker
semantic association are linked by dashed lines.
{(yi1 , si1), (yi1 , si2), ? ? ? , (yin , sin)}, where yij =
1 indicates that sentence sij is a correct answer to
question qi, and 0 otherwise. Using this labeled
data, our goal is to learn a probabilistic classifier
to predict the label of a new, unseen pair of ques-
tion and sentence.
Fundamentally, what the classifier predicts is
whether the sentence ?matches? the question se-
mantically. In other words, does s have the an-
swer that satisfies the semantic constraints pro-
vided in the question? Without representing the
question and sentence in logic or syntactic trees,
we take a word-alignment view for solving this
problem. We assume that there is an underly-
ing structure h that describes how q and s can
be associated through the relations of the words
in them. Figure 1 illustrates this setting using a
revised example from (Harabagiu and Moldovan,
2001). In this figure, words connected by solid
lines are clear synonyms or hyponym/hypernym;
words connected by dashed lines indicate that they
are weakly related. With this alignment structure,
features like the degree of mapping or whether all
the content words in the question can be mapped
to some words in the sentence can be extracted and
help improve the classifier. Notice that the struc-
ture representation in terms of word-alignment is
fairly general. For instance, if we assume a naive
complete bipartite matching, then effectively it re-
duces to the simple bag-of-words model.
Typically, the ?ideal? alignment structure is not
available in the data, and previous work exploited
mostly syntactic analysis (e.g., dependency trees)
to reveal the latent mapping structure. In this
work, we focus our study on leveraging the low-
level semantic cues from recently proposed lexical
semantic models. As will be shown in our experi-
ments, such information not only improves a latent
structure learning method, but also makes a simple
bipartite matching approach extremely strong.2
4 Lexical Semantic Models
In this section, we introduce the lexical seman-
tic models we adopt for solving the semantic
matching problem in answer selection. To go be-
yond the simple, limited surface-form matching,
we aim to pair words that are semantically re-
lated, specifically measured by models of word
relations including synonymy/antonymy, hyper-
nymy/hyponymy (the Is-A relation) and general se-
mantic word similarity.
4.1 Synonymy and Antonymy
Among all the word relations, synonymy is per-
haps the most basic one and needs to be handled
reliably. Although sets of synonyms can be eas-
ily found in thesauri or WordNet synsets, such
resources typically cover only strict synonyms.
When comparing two words, it is more useful to
estimate the degree of synonymy as well. For in-
stance, ship and boat are not strict synonyms be-
cause a ship is usually viewed as a large boat.
Knowing that two words are somewhat synony-
mous could be valuable in determining whether
they should be mapped.
In order to estimate the degree of synonymy, we
leverage a recently proposed polarity-inducing la-
tent semantic analysis (PILSA) model (Yih et al,
2012). Given a thesaurus, the model first con-
structs a signed d-by-n co-occurrence matrix W ,
where d is the number of word groups and n is
the size of the vocabulary. Each row consists of a
2Proposed by an anonymous reviewer, one justification of
this word-alignment approach, where syntactic analysis plays
a less important role, is that there are often few sensible com-
binations of words. For instance, knowing only the set of
words {?car?, ?fastest?, ?world?}, one may still guess cor-
rectly the question ?What is the fastest car in the world??
1746
group of synonyms and antonyms of a particular
sense and each column represents a unique word.
Values of the elements in each row vector are the
TFIDF values of the corresponding words in this
group. The notion of polarity is then induced by
making the values of words in the antonym groups
negative, and the matrix is generalized by a low-
rank approximation derived by singular-value de-
composition (SVD) in the end. This design has an
intriguing property ? if the cosine score of two col-
umn vectors are positive, then the two correspond-
ing words tend to be synonymous; if it?s negative,
then the two words are antonymous. The degree is
measured by the absolute value.
Following the setting described in (Yih et al,
2012), we construct a PILSA model based on the
Encarta thesaurus and enhance it with a discrimi-
native projection matrix training method. The es-
timated degrees of both synonymy and antonymy
are used our experiments.3
4.2 Hypernymy and Hyponymy
The Class-Inclusion or Is-A relation is commonly
observed between words in questions and answer
sentences. For example, to correctly answer the
question ?What color is Saturn??, it is crucial that
the selected sentence mentions a specific kind of
color, as in ?Saturn is a giant gas planet with
brown and beige clouds.? Another example is
?Who wrote Moonlight Sonata??, where compose
in ?Ludwig van Beethoven composed the Moon-
light Sonata in 1801.? is one kind of write.
Traditionally, WordNet taxonomy is the linguis-
tic resource for identifying hypernyms and hy-
ponyms, applied broadly to many NLP problems.
However, WordNet has a number of well-known
limitations including its rather limited or skewed
concept distribution and the lack of the coverage
of the Is-A relation (Song et al, 2011). For in-
stance, when a word refers to a named entity, the
particular sense and meaning is often not encoded.
As a result, relations such as ?Apple? is-a ?com-
pany? and ?Jaguar? is-a ?car? cannot be found in
WordNet. Similar to the case in synonymy, the
Is-A relation defined in WordNet does not provide
a native, real-valued degree of the relation, which
can only be roughly approximated using the num-
ber of links on the taxonomy path connecting two
3Mapping two antonyms may be desired if one of them is
in the scope of negation (Morante and Blanco, 2012; Blanco
and Moldovan, 2011). However, we do not attempt to resolve
the negation scope in this work.
concepts (Resnik, 1995).
In order to remedy these issues, we aug-
ment WordNet with the Is-A relations found in
Probase (Wu et al, 2012). Probase is a knowledge
base that establishes connections between 2.7 mil-
lion concepts, discovered automatically by apply-
ing Hearst patterns (Hearst, 1992) to 1.68 billion
Web pages. Its abundant concept coverage dis-
tinguishes it from other knowledge bases, such as
Freebase (Bollacker et al, 2008) and WikiTaxon-
omy (Ponzetto and Strube, 2007). Based on the
frequency of term co-occurrences, each Is-A rela-
tion from Probase is associated with a probability
value, indicating the degree of the relation.
We verified the quality of Probase Is-A relations
using a recently proposed SemEval task of rela-
tional similarity (Jurgens et al, 2012) in a com-
panion paper (Zhila et al, 2013), where a subset
of the data is to measure the degree of two words
having a class-inclusion relation. Probase?s pre-
diction correlates well with the human annotations
and achieves a high Spearman?s rank correlation
coefficient score, ? = 0.619. In comparison, the
previous best system (Rink and Harabagiu, 2012)
in the task only reaches ? = 0.233. These appeal-
ing qualities make Probase a robust lexical seman-
tic model for hypernymy/hyponymy.
4.3 Semantic Word Similarity
The third lexical semantic model we introduce tar-
gets a general notion of word similarity. Unlike
synonymy and hyponymy, word similarity is only
loosely defined when two words can be associated
by some implicit relation.4 The general word sim-
ilarity model can be viewed as a ?back-off? so-
lution when the exact lexical relation (e.g., part-
whole and attribute) is not available or cannot be
accurately detected.
Among various word similarity models (Agirre
et al, 2009; Reisinger and Mooney, 2010;
Gabrilovich and Markovitch, 2007; Radinsky et
al., 2011), the vector space models (VSMs) based
on the idea of distributional similarity (Turney
and Pantel, 2010) are often used as the core com-
ponent. Inspired by (Yih and Qazvinian, 2012),
which argues the importance of incorporating het-
erogeneous vector space models for measuring
word similarity, we leverage three different VSMs
in this work: Wiki term-vectors, recurrent neural
4Instead of making the distinction, word similarity here
refers to the larger set of relations commonly covered by word
relatedness (Budanitsky and Hirst, 2006).
1747
network language model (RNNLM) and a concept
vector space model learned from click-through
data. Semantic word similarity is estimated using
the cosine score of the corresponding word vectors
in these VSMs.
Contextual term-vectors created using the
Wikipedia corpus have shown to perform well
on measuring word similarity (Reisinger and
Mooney, 2010). Following the setting suggested
by Yih and Qazvinian (2012), we create term-
vectors representing about 1 million words by ag-
gregating terms within a window of [?10, 10] of
each occurrence of the target word. The vectors
are further refined by applying the same vocabu-
lary and feature pruning techniques.
A recurrent neural network language
model (Mikolov et al, 2010) aims to esti-
mate the probability of observing a word given its
preceding context. However, one by-product of
this model is the word embedding learned in its
hidden-layer, which can be viewed as capturing
the word meaning in some latent, conceptual
space. As a result, vectors of related words tend
to be close to each other. For this word similarity
model, we take a 640-dimensional version of
RNNLM vectors, which is trained using the
Broadcast News corpus of 320M words.5
The final word relatedness model is a projec-
tion model learned from the click-through data of
a commercial search engine (Gao et al, 2011).
Unlike the previous two models, which are cre-
ated or trained using a text corpus, the input for
this model is pairs of aggregated queries and ti-
tles of pages users click. This parallel data is
used to train a projection matrix for creating the
mapping between words in queries and documents
based on user feedback, using a Siamese neural
network (Yih et al, 2011). Each row vector of
this matrix is the dense vector representation of
the corresponding word in the vocabulary. Perhaps
due to its unique information source, we found this
particular word embedding seems to complement
the other two VSMs and tends to improve the word
similarity measure in general.
5 Learning QA Matching Models
In this section, we investigate the effectiveness of
various learning models for matching questions
and sentences, including the bag-of-words setting
5http://www.fit.vutbr.cz/?imikolov/
rnnlm/
and the framework of learning latent structures.
5.1 Bag-of-Words Model
The bag-of-words model treats each question and
sentence as an unstructured bag of words. When
comparing a question with a sentence, the model
first matches each word in the question to each
word in the sentence. It then aggregates features
extracted from each of these word pairs to rep-
resent the whole question/sentence pair. A bi-
nary classifier can be trained easily using any ma-
chine learning algorithm in this standard super-
vised learning setting.
Formally, let x = (q, s) be a pair of question q
and sentence s. Let Vq = {wq1 , wq2 , ? ? ? , wqm}
and Vs = {ws1 , ws2 , ? ? ? , wsn} be the sets of
words in q and s, respectively. Given a word pair
(wq, ws), where wq ? Vq and ws ? Vs, feature
functions ?1, ? ? ? , ?d map it to a d-dimensional
real-valued feature vector.
We consider two aggregate functions for defin-
ing the feature vectors of the whole ques-
tion/answer pair: average and max.
?avgj (q, s) =
1
mn
?
wq?Vq
ws?Vs
?j(wq, ws) (1)
?maxj (q, s) = maxwq?Vq
ws?Vs
?j(wq, ws) (2)
Together, each question/sentence pair is repre-
sented by a 2d-dimensional feature vector.
We tested two learning algorithms in this set-
ting: logistic regression and boosted decision
trees (Friedman, 2001). The former is the log-
linear model widely used in the NLP community
and the latter is a robust non-linear learning algo-
rithm that has shown great empirical performance.
The bag-of-words model does not require an ad-
ditional inference stage as in structured learning,
which may be computationally expensive. Nev-
ertheless, its lack of structure information could
limit the expressiveness of the model and make it
difficult to capture more sophisticated semantics
in the sentences. To address this concern, we in-
vestigate models of learning latent structures next.
5.2 Learning Latent Structures
One obvious issue of the bag-of-words model is
that words in the unrelated part of the sentence
may still be paired with words in the question,
which introduces noise to the final feature vector.
1748
This is observed in many question/sentence pairs,
such as the one below.
Q: Which was the first movie that James Dean
was in?
A: James Dean, who began as an actor on TV
dramas, didn?t make his screen debut until
1951?s ?Fixed Bayonet.?
While this sentence correctly answers the ques-
tion, the fact that James Dean began as a TV
actor is unrelated to the question. As a result,
an ?ideal? word alignment structure should not
link words in this clause to those in the ques-
tion. In order to leverage the latent structured in-
formation, we adapt a recently proposed frame-
work of learning constrained latent representa-
tions (LCLR) (Chang et al, 2010). LCLR can be
viewed as a variant of Latent-SVM (Felzenszwalb
et al, 2009) with different learning formulations
and a general inference framework. The idea of
LCLR is to replace the decision function of a stan-
dard linear model ?T?(x) with
arg max
h
?T?(x, h), (3)
where ? represents the weight vector and h repre-
sents the latent variables.
In this answer selection task, x = (q, s) rep-
resents a pair of question q and candidate sen-
tence s. As described in Sec. 3, h refers to the
latent alignment between q and s. The intuition
behinds Eq. (3) is: candidate sentence s correctly
answers question q if and only if the decision can
be supported by the best alignment h.
The objective function of LCLR is defined as:
min?
1
2 ||?||
2 + C
?
i
?2i
s.t. ?i ? 1? yi maxh ?
T?(x, h)
Note that the alignment is latent, so LCLR uses
the binary labels in the training data as feedback
to find the alignment for each example.
The computational difficulty of the inference
problem (Eq. (3)) largely depends on the con-
straints we enforce in the alignment. Complicated
constraints may result in a difficult inference prob-
lem, which can be solved by integer linear pro-
gramming (Roth and Yih, 2007). In this work,
we considered several sets of constraints for the
alignment task, including a two-layer phrase/word
alignment structure, but found that they generally
performed the same. Therefore, we chose the
many-to-one alignment6, where inference can be
solved exactly using a simple greedy algorithm.
6 Experiments
We present our experimental results in this sec-
tion by first introducing the data and evaluation
metrics, followed by the results of existing sys-
tems and some baseline methods. We then show
the positive impact of adding information of word
relations from various lexical semantics models,
with some discussion on the limitation of the
word-matching approach.
6.1 Data & Evaluation Metrics
The answer selection dataset we used was orig-
inally created by Wang et al (2007) based on
the QA track of past Text REtrieval Confer-
ences (TREC-QA). Questions in this dataset are
short factoid questions, such as ?What is Crips?
gang color?? In average, each question is associ-
ated with approximately 33 answer candidate sen-
tences. A pair of question and sentence is judged
positive if the sentence contains the exact answer
key and can provide sufficient context as support-
ing evidence.
The training set of the data contains manu-
ally labeled 5,919 question/sentence pairs from
TREC 8-12. The development and testing sets
are both from TREC 13, which contain 1,374
and 1,866 pairs, respectively. The task is treated as
a sentence ranking problem for each question and
thus evaluated in Mean Average Precision (MAP)
and Mean Reciprocal Rank (MRR), using the offi-
cial TREC evaluation program. Following (Wang
et al, 2007), candidate sentences with more than
40 words are removed from evaluation, as well as
questions with only positive or negative candidate
sentences.
6.2 Baseline Methods
Several systems have been proposed and tested
using this dataset. Wang et al (2007) pre-
sented a generative probabilistic model based on
a Quasi-synchronous Grammar formulation and
was later improved by Wang and Manning (2010)
with a tree-edit CRF model that learns the la-
tent alignment structure. In contrast, Heilman and
6Each word in the question needs to be linked to a word
in the sentence. Each word in the sentence can be linked to
zero or multiple words in the question.
1749
System MAP MRR
Wang et al (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
Table 1: Test set results of existing methods, taken
from Table 3 of (Wang and Manning, 2010).
Dev Test
Baseline MAP MRR MAP MRR
Random 0.5243 0.5816 0.4708 0.5286
Word Cnt 0.6516 0.7216 0.6263 0.6822
Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071
Table 2: Results of three baseline methods.
Smith (2010) proposed a discriminative approach
that first computes a tree kernel function between
the dependency trees of the question and candidate
sentence, and then learns a classifier based on the
tree-edit features extracted. Table 1 summarizes
their results on the test set. All these systems in-
corporated lexical semantics features derived from
WordNet and named entity features.
In order to further estimate the difficulty of
this task and dataset, we tested three simple base-
lines. The first is random scoring, which sim-
ply assigns a random score to each candidate sen-
tence. The second one, word count, is to count
how many words in the question that also occur in
the answer sentence, after removing stopwords7,
and lowering the case. Finally, the last base-
line method, weighted word count, is basically the
same as identical word matching, but the count is
re-weighted using the IDF value of the question
word. This is similar to the BM25 ranking func-
tion (Robertson et al, 1995). The results of these
three methods are shown in Table 1.
Somewhat surprisingly, we find that word count
is fairly strong and performs comparably to previ-
ous systems.8 In addition, weighting the question
words with their IDF values further improves the
results.
6.3 Incorporating Rich Lexical Semantics
We test the effectiveness of adding rich lexical
semantics information by creating examples of
different feature sets. As described in Sec. 5,
7We used a list of 101 stopwords, including articles, pro-
nouns and punctuation.
8The finding has been confirmed by the lead author
of (Wang et al, 2007).
all the features are based on the properties of
the pair of a word from the question and a
word from the candidate sentence. Stopwords
are first removed from both questions and sen-
tences and all words are lower-cased. Features
used in the experiments can be categorized into
six types: identical word matching (I), lemma
matching (L), WordNet (WN), enhanced Lexi-
cal Semantics (LS), Named Entity matching (NE)
and Answer type checking (Ans). Inspired by
the weighted word count baseline, all features ex-
cept (Ans) are weighted by the IDF value of the
question word. In other words, the IDF values help
decide the importance of word pairs to the model.
Staring from the our baseline model, weighted
word count, the identical word matching (I) fea-
ture checks whether the pair of words are the
same. Instead of checking the surface form of
the word, lemma matching (L) verifies whether
the two words have the same lemma form. Ar-
guably the most common source of word rela-
tions, WordNet (WN) provides the primitive fea-
tures of whether two words could belong to the
same synset in WordNet, could be antonyms and
whether one is a hypernym of the other. Alter-
natively, the enhanced lexical semantics (LS) fea-
tures apply the models described in Sec. 4 to the
word pair and use their estimated degree of syn-
onymy, antonymy, hyponymy and semantic relat-
edness as features. Named entity matching (NE)
checks whether two words are individually part
of some named entities with the same type. Fi-
nally, when the question word is the WH-word, we
check if the paired word belongs to some phrase
that has the correct answer type using simple rules,
such as ?Who should link to a word that is part of
a named entity of type Person.? We created exam-
ples in each round of experiments by augmenting
these features in the same order, and observed how
adding different information helped improve the
model performance.
Three models are included in our study. For
the unstructured, bag-of-words setting, we tested
logistic regression (LR) and boosted decision
trees (BDT). As mentioned in Sec. 5, the features
for the whole question/sentence pair are the aver-
age and max of features of all the word pairs. For
the structured-output setting, we used the frame-
work of learning constrained latent representa-
tion (LCLR) and required that each question word
needed to be mapped to a word in the sentence.
1750
LR BDT LCLR
Feature set MAP MRR MAP MRR MAP MRR
1: I 0.6531 0.7071 0.6323 0.6898 0.6629 0.7279
2: I+L 0.6744 0.7223 0.6496 0.6923 0.6815 0.7270
3: I+L+WN 0.7039 0.7705 0.6798 0.7450 0.7316 0.7921
4: I+L+WN+LS 0.7339 0.8107 0.7523 0.8455 0.7626 0.8231
5: All 0.7374 0.8171 0.7495 0.8450 0.7648 0.8255
Table 3: Test results of various models and feature groups. Logistic regression (LR) and boosted decision
trees (BDT) are the two unstructured models. LCLR is the algorithm for learning latent structures.
Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhanced
Lexical Semantics (LS). All includes these four plus Named Entity matching (NE) and Answer type
checking (Ans).
Hyper-parameters are selected using the ones that
achieve the best MAP score on the development
set. Results of these models and feature sets are
presented in Table 3.
We make two observations from the results.
First, while incorporating more information of the
word pairs in general helps, it is clear that map-
ping words beyond surface-form matching with
the help of WordNet (Line #3 vs. #2) is impor-
tant. Moreover, when richer information from
other lexical semantic models is available, the per-
formance can be further improved (Line #4 vs.
#3). Overall, by simply incorporating more in-
formation on word relations, we gain approxi-
mately 10 points in both MAP and MRR com-
pared to surface-form matching (Line #4 vs. #2),
consistently across all three models. However,
adding more information like named entity match-
ing and answer type verification does not seem to
help much (Line #5 vs. #4). Second, while the
structured-output model usually performs better
than both unstructured models (LCLR vs. LR &
BDT), the performance gain diminishes after more
information of word pairs is available (e.g., Lines
#4 and #5).
6.4 Limitation of Word Matching Models
Although we have demonstrated the benefits of
leveraging various lexical semantic models to help
find the association between words, the problem of
question answering is nevertheless far from solved
using the word-based approach. Examining the
output of the LCLR model with all features on the
development set, we found that there were three
main sources of errors, including uncovered or in-
accurate entity relations, the lack of robust ques-
tion analysis and the need of high-level semantic
representation and inference. While the first two
can be improved by, say, using a better named en-
tity tagger, incorporating other knowledge bases
and building a question classifier, how to solve the
third problem is tricky. Below is an example:
Q: In what film is Gordon Gekko the main char-
acter?
A: He received a best actor Oscar in 1987 for his
role as Gordon Gekko in ?Wall Street?.
This is a correct answer sentence because ?win-
ning a best actor Oscar? implies that the role Gor-
don Gekko is the main character. It is hard to be-
lieve that a pure word-matching model would be
able to solve this type of ?inferential question an-
swering? problem.
7 Conclusions
In this paper, we present an experimental study
on solving the answer selection problem using en-
hanced lexical semantic models. Following the
word-alignment paradigm, we find that the rich
lexical semantic information improves the models
consistently in the unstructured bag-of-words set-
ting and also in the framework of learning latent
structures. Another interesting finding we have
is that while the latent structured model, LCLR,
performs better than the other two unstructured
models, the difference diminishes after more in-
formation, including the enhanced lexical seman-
tic knowledge and answer type verification, has
been incorporated. This may suggest that adding
shallow semantic information is more effective
than introducing complex structured constraints,
at least for the specific word alignment model we
experimented with in this work.
1751
In the future, we plan to explore several di-
rections. First, although we focus on improv-
ing TREC-style open-domain question answering
in this work, we would like to apply the pro-
posed technology to other QA scenarios, such
as community-based QA (CQA). For instance,
the sentence matching technique can help map a
given question to some questions in an existing
CQA database (e.g., Yahoo! Answers). More-
over, the answer sentence selection scheme could
also be useful in extracting the most related sen-
tences from the answer text to form a summary
answer. Second, because the task of answer sen-
tence selection is very similar to paraphrase de-
tection (Dolan et al, 2004) and recognizing tex-
tual entailment (Dagan et al, 2006), we would like
to investigate whether systems for these tasks can
be improved by incorporating enhanced lexical se-
mantic knowledge as well. Finally, we would like
to improve our system for the answer sentence se-
lection task and for question answering in general.
In addition to following the directions suggested
by the error analysis presented in Sec. 6.4, we plan
to use logic-like semantic representations of ques-
tions and sentences, and explore the role of lexical
semantics for handling questions that require in-
ference.
Acknowledgments
We are grateful to Mengqiu Wang for providing
the dataset and helping clarify some issues in the
experiments. We also thank Chris Burges and Hoi-
fung Poon for valuable discussion and the anony-
mous reviewers for their useful comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas?ca and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of NAACL, pages
19?27.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of SIGIR, pages 351?358.
E. Blanco and D. Moldovan. 2011. Semantic repre-
sentation of negation using focus detection. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011).
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In ACM Conference on Management of Data
(SIGMOD), pages 1247?1250.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13?47,
March.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In Proceedings of NAACL.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge, volume 3944. Springer-Verlag, Berlin.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 16?23.
Oren Etzioni. 2011. Search needs a shake-up. Nature,
476(7358):25?26.
P. Felzenszwalb, R. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
99(1).
D. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3.4):1?1.
J. Friedman. 2001. Greedy function approximation:
a gradient boosting machine. Annals of Statistics,
29(5):1189?1232.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In AAAI Conference on Artificial
Intelligence (AAAI).
J. Gao, K. Toutanova, and W. Yih. 2011.
Clickthrough-based latent semantic models for web
search. In Proceedings of SIGIR, pages 675?684.
S. Harabagiu and D. Moldovan. 2001. Open-domain
textual question answering. Tutorial of NAACL-
2001.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
M. Heilman and N. Smith. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and
answers to questions. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 1011?1019.
1752
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 356?364.
T. Mikolov, M. Karafia?t, L. Burget, J. Cernocky?, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 1045?1048.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analy-
sis in an open-domain question answering system.
ACM Transactions on Information Systems (TOIS),
21(2):133?154.
D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges.
2007. COGEX: A semantically and contextually en-
riched logic prover for question answering. Journal
of Applied Logic, 5(1):49?69.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: Resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 265?274.
S. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In AAAI Confer-
ence on Artificial Intelligence (AAAI).
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping
dependencies trees: An application to question an-
swering. In International Symposium on Artificial
Intelligence and Mathematics (AI & Math).
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis.
In WWW ?11, pages 337?346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In Proceed-
ings of NAACL.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International
Joint Conference on Artificial Intelligence (IJCAI).
B. Rink and S. Harabagiu. 2012. UTD: Determining
relational similarity using lexical patterns. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 413?418.
S. Robertson, S. Walker, S. Jones, M. Hancock-
Beaulieu, and M. Gatford. 1995. Okapi at TREC-3.
In Text REtrieval Conference (TREC), pages 109?
109.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12?21.
D. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of the HLT-NAACL
Workshop on Statistical Machine Translation, pages
23?30.
Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. 2011.
Short text conceptualization using a probabilistic
knowledgebase. In International Joint Conference
on Artificial Intelligence (IJCAI), pages 2330?2336.
K. Tai. 1979. The tree-to-tree correction problem. J.
ACM, 26(3):422?433, July.
P. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141?
188.
E. Voorhees and D. Tice. 2000. Building a question
answering test collection. In Proceedings of SIGIR,
pages 200?207.
M. Wang and C. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In Proceed-
ings of COLING.
M. Wang, N. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
T. Winograd. 1977. Five lectures on artificial intelli-
gence. In A. Zampolli, editor, Linguistic Structures
Processing, pages 399?520. North Holland.
W. Woods. 1973. Progress in natural language under-
standing: An application to lunar geology. In Pro-
ceedings of the National Computer Conference and
Exposition (AFIPS), pages 441?450.
W. Wu, H. Li, H. Wang, and K. Zhu. 2012. Probase:
a probabilistic taxonomy for text understanding. In
ACM Conference on Management of Data (SIG-
MOD), pages 481?492.
W. Yih and V. Qazvinian. 2012. Measuring word relat-
edness using heterogeneous vector space models. In
Proceedings of NAACL-HLT 2012, pages 616?620.
W. Yih, K. Toutanova, J. Platt, and C. Meek. 2011.
Learning discriminative projections for text similar-
ity measures. In ACL Conference on Natural Lan-
guage Learning (CoNLL), pages 247?256.
W. Yih, G. Zweig, and J. Platt. 2012. Polarity inducing
latent semantic analysis. In Proceedings of EMNLP-
CoNLL, pages 1212?1222.
A. Zhila, W. Yih, C. Meek, G. Zweig, and T. Mikolov.
2013. Combining heterogeneous models for mea-
suring relational similarity. In Proceedings of HLT-
NAACL.
1753
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 7,
Baltimore, Maryland, USA, 22 June 2014. c?2014 Association for Computational Linguistics
Wikification and Beyond:  
The Challenges of Entity and Concept Grounding 
Dan Roth Heng Ji 
University of Illinois at Urbana-Champaign Rensselaer Polytechnic Institute 
danr@illinois.edu jih@rpi.edu 
  
Ming-Wei Chang Taylor Cassidy 
Microsoft Research Army Research Lab & IBM Research 
minchang@microsoft.com taylor.cassidy.ctr@mail.mil 
 
 
 
  
1 Introduction 
Contextual disambiguation and grounding of 
concepts and entities in natural language are es-
sential to progress in many natural language un-
derstanding tasks and fundamental to many ap-
plications. Wikification aims at automatically 
identifying concept mentions in text and linking 
them to referents in a knowledge base (KB) (e.g., 
Wikipedia). Consider the sentence, "The Times 
report on Blumenthal (D) has the potential to 
fundamentally reshape the contest in the Nutmeg 
State.". A Wikifier should identify the key enti-
ties and concepts and map them to an encyclope-
dic resource (e.g., ?D? refers to Democratic Par-
ty, and ?the Nutmeg State? refers to Connecticut.  
   Wikification benefits end-users and Natural 
Language Processing (NLP) systems. Readers 
can better comprehend Wikified documents as 
information about related topics is readily acces-
sible. For systems, a Wikified document eluci-
dates concepts and entities by grounding them in 
an encyclopedic resource or an ontology. Wikifi-
cation output has improved NLP down-stream 
tasks, including coreference resolution, user in-
terest discovery , recommendation and search. 
  This task has received increased attention in 
recent years from the NLP and Data Mining 
communities, partly fostered by the U.S. NIST 
Text Analysis Conference Knowledge Base Pop-
ulation (KBP) track, and several versions of it 
has been studied. These include Wikifying all 
concept mentions in a single text document; 
Wikifying a cluster of co-referential named enti-
ty mentions that appear across documents (Entity 
Linking), and Wikifying a whole document to a 
single concept. Other works relate this task to 
coreference resolution within and across docu-
ments and in the context of multiple text genres. 
2 Content Overview 
This tutorial will motivate Wikification as a 
broad paradigm for cross-source linking for 
knowledge enrichment. We will discuss multiple 
dimensions of the task definition, present the 
building blocks of a state-of-the-art Wikifier, 
share key lessons learned from analysis of re-
sults, and discuss recently proposed ideas for 
advancing work in this area in response to key 
challenges. We will touch on new research areas 
including interactive Wikification, social media, 
and censorship. The tutorial will be useful for all 
those with interests in cross-source information 
extraction and linking, knowledge acquisition, 
and the use of acquired knowledge in NLP. We 
will provide a concise roadmap of recent per-
spectives and results, and point to some of our 
available Wikification resources.  
3 Outline 
? Introduction and Motivation 
? Methodological presentation of a skeletal Wik-
ification system 
o Mention and candidate identification 
o Knowledge representation  
o Local and global context analysis 
o Role of Machine Learning 
? Obstacles & Advanced Methods 
o Joint modeling 
o Collective inference 
o Scarcity of supervision signals 
o Diverse text genres and social media 
? Remaining Challenges and Future Work 
o Rich semantic knowledge acquisition 
o Cross-lingual Wikification 
References 
http://nlp.cs.rpi.edu/kbp/2014/elreading.html 7
Transactions of the Association for Computational Linguistics, 1 (2013) 207?218. Action Editor: Ben Taskar.
Submitted 10/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Dual Coordinate Descent Algorithms for Efficient
Large Margin Structured Prediction
Ming-Wei Chang Wen-tau Yih
Microsoft Research
Redmond, WA 98052, USA
{minchang,scottyih}@microsoft.com
Abstract
Due to the nature of complex NLP problems,
structured prediction algorithms have been
important modeling tools for a wide range of
tasks. While there exists evidence showing
that linear Structural Support Vector Machine
(SSVM) algorithm performs better than struc-
tured Perceptron, the SSVM algorithm is still
less frequently chosen in the NLP community
because of its relatively slow training speed.
In this paper, we propose a fast and easy-to-
implement dual coordinate descent algorithm
for SSVMs. Unlike algorithms such as Per-
ceptron and stochastic gradient descent, our
method keeps track of dual variables and up-
dates the weight vector more aggressively. As
a result, this training process is as efficient as
existing online learning methods, and yet de-
rives consistently better models, as evaluated
on four benchmark NLP datasets for part-of-
speech tagging, named-entity recognition and
dependency parsing.
1 Introduction
Complex natural language processing tasks are in-
herently structured. From sequence labeling prob-
lems like part-of-speech tagging and named entity
recognition to tree construction tasks like syntactic
parsing, strong dependencies exist among the la-
bels of individual components. By modeling such
relations in the output space, structured output pre-
diction algorithms have been shown to outperform
significantly simple binary or multi-class classi-
fiers (Lafferty et al, 2001; Collins, 2002; McDonald
et al, 2005).
Among the existing structured output prediction
algorithms, the linear Structural Support Vector
Machine (SSVM) algorithm (Tsochantaridis et al,
2004; Joachims et al, 2009) has shown outstanding
performance in several NLP tasks, such as bilingual
word alignment (Moore et al, 2007), constituency
and dependency parsing (Taskar et al, 2004b; Koo
et al, 2007), sentence compression (Cohn and La-
pata, 2009) and document summarization (Li et al,
2009). Nevertheless, as a learning method for NLP,
the SSVM algorithm has been less than popular al-
gorithms such as the structured Perceptron (Collins,
2002). This may be due to the fact that cur-
rent SSVM implementations often suffer from sev-
eral practical issues. First, state-of-the-art imple-
mentations of SSVM such as cutting plane meth-
ods (Joachims et al, 2009) are typically compli-
cated.1 Second, while methods like stochastic gradi-
ent descent are simple to implement, tuning learning
rates can be difficult. Finally, while SSVM mod-
els can achieve superior accuracy, this often requires
long training time.
In this paper, we propose a novel optimiza-
tion method for efficiently training linear SSVMs.
Our method not only is easy to implement, but
also has excellent training speed, competitive with
both structured Perceptron (Collins, 2002) and
MIRA (Crammer et al, 2005). When evaluated on
several NLP tasks, including POS tagging, NER and
dependency parsing, this optimization method also
outperforms other approaches in terms of prediction
accuracy. Our final algorithm is a dual coordinate
1Our algorithm is easy to implement mainly because we use
the square hinge loss function.
207
descent (DCD) algorithm for solving a structured
output SVM problem with a 2-norm hinge loss func-
tion. The algorithm consists of two main compo-
nents. One component behaves analogously to on-
line learning methods and updates the weight vector
immediately after inference is performed. The other
component is similar to the cutting plane method
and updates the dual variables (and the weight vec-
tor) without running inference. Conceptually, this
hybrid approach operates at a balanced trade-off
point between inference and weight update, per-
forming better than with either component alone.
Our contributions in this work can be summarized
as follows. Firstly, our proposed algorithm shows
that even for structured output prediction, an SSVM
model can be trained as efficiently as a structured
Perceptron one. Secondly, we conducted a careful
experimental study on three NLP tasks using four
different benchmark datasets. When compared with
previous methods for training SSVMs (Joachims
et al, 2009), our method achieves similar perfor-
mance using less training time. When compared to
commonly used learning algorithms such as Percep-
tron and MIRA, the model trained by our algorithm
performs consistently better when given the same
amount of training time. We believe our method can
be a powerful tool for many different NLP tasks.
The rest of our paper is organized as follows.
We first describe our approach by formally defining
the problem and notation in Sec. 2, where we also
review some existing, closely-related structured-
output learning algorithms and optimization tech-
niques. We introduce the detailed algorithmic de-
sign in Sec. 3. The experimental comparisons of
variations of our approach and the existing methods
on several NLP benchmark tasks and datasets are re-
ported in Sec. 4. Finally, Sec. 5 concludes the paper.
2 Background and Related Work
We first introduce notations used throughout this pa-
per. An input example is denoted by x and an out-
put structure is denoted by y. The feature vector
?(x,y) is a function defined over an input-output
pair (x,y). We focus on linear models with predic-
tions made by solving the decoding problem:
arg max
y?Y(xi)
wT?(xi,y). (1)
The set Y(xi) represents all possible (exponentially
many) structures that can be generated from the ex-
ample xi. Let yi be the true structured label of xi.
The difference between the feature vectors of the
correct label yi and y is denoted as ?yi,y(xi) ?
?(xi,yi) ? ?(xi,y). We define ?(yi,y) as a dis-
tance function between two structures.
2.1 Perceptron and MIRA
Structured Perceptron First introduced by
Collins (2002), the structured Perceptron algorithm
runs two steps iteratively: first, it finds the best
structured prediction y for an example with the
current weight vector using Eq. (1); then the
weight vector is updated according to the difference
between the feature vectors of the true label and
the prediction: w ? w + ?yi,y(xi). Inspired by
Freund and Schapire (1999), Collins (2002) also
proposed the averaged structured Perceptron, which
maintains an averaged weight vector throughout the
training procedure. This technique has been shown
to improve the generalization ability of the model.
MIRA The Margin Infused Relaxed Algo-
rithm (MIRA), which was introduced by Crammer
et al (2005), explicitly uses the notion of margin to
update the weight vector. The MIRA updates the
weight vector by calculating the step size using
min
w
1
2?w ?w0?
2
S.T. wT?yi,y(xi) ? ?(y,yi),?y ? Hk,
where Hk is a set containing the best-k structures
according to the weight vector w0. MIRA is a
very popular method in the NLP community and has
been applied to NLP tasks like word segmentation
and part-of-speech tagging (Kruengkrai et al, 2009),
NER and chunking (Mejer and Crammer, 2010) and
dependency parsing (McDonald et al, 2005).
2.2 Structural SVM
Structural SVM (SSVM) is a maximum margin
model for the structured output prediction setting.
Training SSVM is equivalent to solving the follow-
ing global optimization problem:
min
w
?w?2
2 + C
l?
i=1
L(xi,yi,w), (2)
208
where l is the number of labeled examples and
L(xi,yi,w) = `
(
max
y
[
?(yi,y)?wT?yxi,y(xi)
])
The typical choice of ` is `(a) = at. If t = 2 is used,
we refer to the SSVM defined in Eq. (2) as the L2-
Loss SSVM. If hinge loss (t = 1) is used in Eq. (2),
we refer to it as the L1-Loss SSVM. Note that the
function ? is not only necessary,2 but also enables
us to use more information on the differences be-
tween the structures in the training phase. For ex-
ample, using Hamming distance for sequence label-
ing is a reasonable choice, as the model can express
finer distinctions between structures yi and y.
When training an SSVM model, we often need to
solve the loss-augmented inference problem,
arg max
y?Y(xi)
[
wT?(xi,y) + ?(yi,y)
]
. (3)
Note that it is a different inference problem than the
decoding problem in Eq. (1).
Algorithms for training SSVM Cutting
plane (CP) methods (Tsochantaridis et al, 2004;
Joachims et al, 2009) have been the dominant
method for learning the L1-Loss SSVM. Eq. (2)
contains an exponential number of constraints.
The cutting plane (CP) methods iteratively select
a subset of active constraints for each example
then solve a sub-problem which contains active
constraints to improve the model. CP has proven
useful for solving SSVMs. For instance, Yu and
Joachims (2009) proposed using CP methods to
solve a 1-slack variable formulation, and showed
that solving for a 1-slack variable formulation
is much faster than solving the l-slack variable
one (Eq. (2)). Chang et al (2010) also proposed
a variant of cutting plane method for solving the
L2-Loss SSVM. This method uses a dual coordinate
descent algorithm to solve the sub-problems. We
call their approach the CPD method.
Several other algorithms also aim at solv-
ing the L1-Loss SSVM. Stochastic gradient de-
scent (SGD) (Bottou, 2004; Shalev-Shwartz et al,
2007) is a technique for optimizing general con-
vex functions and has been applied to solving the
2Without ?(y,yi) in Eq. 2, the optimal w would be zero.
L1-Loss SSVM (Ratliff et al, 2007). Taskar et
al. (2004a) proposed a structured SMO algorithm.
Because the algorithm solves the dual formulation
of the L1-Loss SSVM, it requires picking a vio-
lation pair for each update. In contrast, because
each dual variable can be updated independently in
our DCD algorithm, the implementation is relatively
simple. The extragradient algorithm has also been
applied to solving the L1-Loss SSVM (Taskar et al,
2005). Unlike our DCD algorithm, the extragradient
method requires the learning rate to be specified.
The connections between dual methods and the
online algorithms have been previously discussed.
Specifically, Shalev-Shwartz and Singer (2006) con-
nects the dual methods to a wide range of online
learning algorithms. In (Martins et al, 2010), the au-
thors apply similar techniques on L1-Loss SSVMs
and show that the proposed algorithm can be faster
than the SGD algorithm.
Exponentiated Gradient (EG) descent (Kivinen
and Warmuth, 1995; Collins et al, 2008) has re-
cently been applied to solving the L1-Loss SSVM.
Compared to other SSVM learners, EG requires
manual tuning of the step size. In addition, EG re-
quires solution of the sum-product inference prob-
lem, which can be more expensive than solving
Eq. (3) (Taskar et al, 2006). Very recently, Lacoste-
Julien et al (2013) proposed a block-coordinate de-
scent algorithm for the L1-Loss SSVM based on the
Frank-Wolfe algorithm (FW-Struct), which has been
shown to outperform the EG algorithm significantly.
Similar to our DCD algorithm, FW calculates the
optimal learning rate when updating the dual vari-
ables.
The Sequential Dual Method (SDM) (Shevade et
al., 2011) is probably the most related to this paper.
SDM solves the L1-Loss SSVM problem using mul-
tiple updating policies, which is similar to our ap-
proach. However, there are several important differ-
ences in the detailed algorithmic design. As will be
clear in Sec. 3, our dual coordinate descent (DCD)
algorithm is very simple, while SDM (which is not
a DCD algorithm) uses a complicated procedure to
balance different update policies. By targeting the
L2-Loss SSVM formulation, our methods can up-
date the weight vector more efficiently, since there
are no equality constraints in the dual.
209
3 Dual Coordinate Descent Algorithms for
Structural SVM
In this work, we focus on solving the dual of linear
L2-Loss SSVM, which can be written as follows:
min
?i,y?0
1
2?
?
i,y
?i,y?yi,y(xi)?2 (4)
+ 14C
?
i
(
?
y?Y(xi)
?i,y)2 ?
?
i,y
?(y,yi)?i,y.
In the above equation, a dual variable ?i,y is asso-
ciated with a structure y ? Y(xi). Therefore, the
total number of dual variables can be quite large: its
upper bound is lB, where B = maxi |Y(xi)|.
The connection between the dual variables and
the weight vector w at optimal solutions is through
the following equation:
w =
l?
i=1
?
y?Y(xi)
?i,y?yi,y(xi). (5)
Advantages of L2-Loss SSVM The use of the
2-norm hinge loss function eliminates the need of
equality constraints3; only non-negative constraints
(?i,y ? 0) remain. This is important because now
each dual variable can be updated without changing
values of the other dual variables. We can then up-
date one single dual variable at a time. As a result,
this dual formulation allows us to design a simple
and principled dual coordinate descent (DCD) opti-
mization method.
DCD algorithms consist of two iterative steps:
1. Pick a dual variable ?i,y.
2. Update the dual variable and the weight vector.
Go to 1.
In the normal binary classification case, how to
select dual variables to solve is not an issue as
choosing them randomly works effectively in prac-
tice (Hsieh et al, 2008). However, this is not a prac-
tical scheme for training SSVM models given that
the number of dual variables in Eq. (4) can be very
large because of the exponentially many legitimate
output structures. To address this issue, we intro-
duce the concept of working set below.
3For L1-Loss SSVM, there are the equality constraints:?
y?Y(xi) ?i,y = C, ?i.
Working Set The number of non-zero variables in
the optimal ? can be small when solving Eq. (4).
Hence, it is often feasible to use a small working set
Wi for each example to keep track of the structures
for non-zero ??s. More formally,
Wi = {y | ?y ? Y(xi), ?i,y > 0}.
Intuitively, the working set Wi records the output
structures that are similar to the true structure yi.We
set al dual variables to be zero initially (therefore,
w = 0 as well), soWi = ? for all i. Then the algo-
rithm starts to build the working set in the training
procedure. After training, the weight vector is com-
puted using dual variables in the working set and
thus equivalent to
w =
l?
i=1
?
y?Wi
?i,y?yi,y(xi). (6)
Connections to Structured Perceptron The pro-
cess of updating a dual variable is in fact very simi-
lar to the update rule used in Perceptron and MIRA.
Take structured Perceptron for example, its weight
vector can be determined using the following equa-
tion:
wperc =
l?
i=1
?
y??(xi)
?i,y?yi,y(xi), (7)
where ?(xi) is the set containing all structures Per-
ceptron predicts for xi during training, and ?i,y is
the number of times Perceptron predicts y for xi
during training. By comparing Eq. (6) and Eq. (7), it
is clear that SSVM is just a more principled way to
update the weight vector, as ??s are computed based
on the notion of margin.4
Updating Dual Variables and Weights After
picking a dual variable ?i,y?, we first show how to
update it optimally. Recall that a dual variable ?i,y?
is associated with the i-th example and a structure y?.
The optimal update size d for ?i,y? can be calculated
analytically from the following optimization prob-
4Of course, Wi could be very different from ?(xi), the con-
struction of the working sets will be discussed in Sec. 3.1.
210
Algorithm 1 UPDATEWEIGHT(i,w):
Update the weight vector w and the dual variables
in the working set of the i-th example. C is the reg-
ularization parameter defined in Eq. (2).
1: Shuffle the elements inWi (but retain the newest
member of the working set to be updated first.
See Theorem 1 for the reasons.)
2: for y? ? Wi do
3: d? ?(y?,yi)?w
T?yi,y?(xi)?
?
y?Wi ?i,y
2C
??yi,y?(xi)?2+ 12C
4: ?? ? max(?i,y? + d, 0)
5: w? w + (?? ? ?i,y?)?yi,y?(xi)
6: ?i,y? ? ??
7: end for
lem (derived from Eq. (4)):
min
d???i,y?
1
2?w + d?yi,y?(x)?
2+
1
4C (d+
?
y?Wi
?i,y)2 ? d?(yi, y?), (8)
where the w is defined in Eq. (6). Compared to
stochastic gradient descent, DCD algorithms keep
track of dual variables and do not need to tune the
learning rate.
Instead of updating one dual variable at a time,
our algorithm updates all dual variables once in the
working set. This step is important for the conver-
gence of the DCD algorithms.5 The exact update
algorithm is presented in Algorithm 1. Line 3 cal-
culates the optimal step size (the analytical solution
to the above optimization problem). Line 4 makes
sure that dual variables are non-negative. Lines 5
and 6 update the weight vectors and the dual vari-
ables. Note that every update ensures Eq. (4) to be
no greater than the original value.
3.1 Two DCD Optimization Algorithms
Now we are ready to present two novel DCD algo-
rithms for L2-Loss SSVM: DCD-Light and DCD-
SSVM.
3.1.1 DCD-Light
The basic idea of DCD-Light is just like online
learning algorithms. Instead of doing inference for
5Specifically, updating all of the structures in the working
set is a necessary condition for our algorithms to converge.
the whole batch of examples before updating the
weight vector in each iteration, as done in CPD and
1-slack variable formulation of SVM-Struct, DCD-
Light updates the model weights after solving the in-
ference problem for each individual example. Algo-
rithm 2 depicts the detailed steps. In Line 5, the loss-
augmented inference (Eq. (3)) is performed; then
the weight vector is updated in Line 9 ? all of the
structures and dual variables in the working set are
used to update the weight vector. Note that there is
a ? parameter in Line 6 to control how precise we
would like to solve this SSVM problem. As sug-
gested in (Hsieh et al, 2008), we shuffle the exam-
ples in each iteration (Line 3) as it helps the algo-
rithm converge faster.
DCD-Light has several noticeable differences
when compared to the most popular online learn-
ing method, averaged Perceptron. First, DCD-Light
performs the loss-augmented inference (Eq. (3)) at
Line 5 instead of the argmax inference (Eq. (1)).
Second, the algorithm updates the weight vector
with all structures in the working set. Finally, DCD-
light does not average the weight vectors.
3.1.2 DCD-SSVM
Observing that DCD-Light does not fully utilize
the saved dual variables in the working set, we pro-
pose a hybrid approach called DCD-SSVM, which
combines ideas from DCD-Light and cutting plane
methods. In short, after running the updates on a
batch of examples, we refine the model by solving
the dual variables further in the current working sets.
The key advantage of keeping track of these dual
variables is that it allows us to update the saved dual
variables without performing any inference, which
is often an expensive step in structured prediction
algorithms.
DCD-SSVM is summarized in Algorithm 3.
Lines 10 to 16 are from DCD-Light. In Lines 3 to
8, we grab the idea from cutting plane methods by
updating the weight vector using the saved dual vari-
ables in the working sets without any inference (note
that Lines 3 to 8 do not have any effect at the first
iteration). By revisiting the dual variables, we can
derive a better intermediate model, resulting in run-
ning the inference procedure less frequently. Similar
to DCD-Light, we also shuffle the examples in each
iteration.
211
Algorithm 2 DCD-Light: The lightweight dual co-
ordinate descent algorithm for optimizing Eq. (4).
1: w? 0,Wi ? ?,?i
2: for t = 1 . . . T do
3: Shuffle the order of the training examples
4: for i = 1 . . . l do
5: y?? arg maxy wT?(xi,y) + ?(y,yi)
6: if ?(y?,yi)?wT?yi,y?(xi)?
?
y?Wi
?i,y
2C ? ?then
7: Wi ?Wi ? {y?}
8: end if
9: UPDATEWEIGHT(i,w) {Algo. 1}
10: end for
11: end for
DCD algorithms are similar to column generation
algorithms for linear programming (Desrosiers and
Lu?bbecke, 2005), where the master problem is to
solve the dual problem that focuses on the variables
in the working sets, and the subproblem is to find
new variables for the working sets. In Sec. 4, we
will demonstrate the importance of balancing these
two problems by comparing DCD-SSVM and DCD-
Light.
3.2 Convergence Analysis
We now present the theoretic analysis of both DCD-
Light and DCD-SSVM, and address two main top-
ics: (1) whether the working sets will grow expo-
nentially and (2) the convergence rate. Due to the
lack of space, we show only the main theorems.
Leveraging Theorem 5 in (Joachims et al, 2009),
we can prove that the DCD algorithms only add a
limited number of variables in the working sets, and
have the following theorem.
Theorem 1. The number of times that DCD-Light
or DCD-SSVM adds structures into working sets is
bounded by O
(2(R2+ 12C )lC?2
?2
)
, where R2 is de-
fined as maxi,y? ??yi,y?(xi)?2, and ? is the upper
bound of ?(yi, y?), ?yi, y? ? Y(xi).
We discuss next the convergence rates of our
DCD algorithms under two different conditions ?
when the working sets are fixed and the general case.
If the working sets are fixed in DCD algorithms,
they become cyclic dual coordinate descent meth-
Algorithm 3 DCD-SSVM: a hybrid dual coor-
dinate descent algorithm that combines ideas from
DCD-Light and cutting plane algorithms.
1: w? 0,Wi ? ?,?i
2: for t = 1 . . . T do
3: for j = 1 . . . r do
4: Shuffle the order of the training examples
5: for i = 1 . . . l do
6: UPDATEWEIGHT(i,w) {Algo. 1}
7: end for
8: end for
9: Shuffle the order of the training examples
10: for i = 1 . . . l do
11: y?? arg maxy wT?(xi,y) + ?(y,yi)
12: if ?(y?,yi)?wT?yi,y?(xi)?
?
y?Wi
?i,y
2C ? ?then
13: Wi ?Wi ? {y?}
14: end if
15: UPDATEWEIGHT(i,w) {Algo. 1}
16: end for
17: end for
ods. In this case, we denote the minimization prob-
lem Eq. (4) as F (?). For fixed working sets {Wi},
we denote FS(?) as the minimization problem that
focuses on the dual variables in the working set only.
By applying the results from (Luo and Tseng, 1993;
Wang and Lin, 2013) to L2-Loss SSVM, we have
the following theorem.
Theorem 2. For any given non-empty working sets
{Wi}, if the DCD algorithms do not extend the
working sets (i.e., line 6-8 in Algorithm 2 are not ex-
ecuted), then the DCD algorithms will obtain the -
optimal solution for FS(?) in O(log(1 )) iterations.
Based on Theorem 1 and Theorem 2, we have the
following theorem.
Theorem 3. DCD-SSVM obtains an -optimal solu-
tion in O( 12 log(1 )) iterations.
To the best of our knowledge, this is the first con-
vergence analysis result for L2-Loss SSVM. Com-
pared to other theoretic analysis results for L1-Loss
SSVM, a tighter bound might exist given a better
theoretic analysis. We leave this for future work.6
6Noticeably, the use of working sets complicates the theo-
212
0 20 40 60 80 10078
80
82
84
86
Training Time (seconds)
Tes
tF1
(a) Test F1 vs. Time in NER-CoNLL
0 100 200 300 400
96.6
96.8
97
97.2
Training Time (seconds)
Tes
tA
cc
(b) Test Acc vs. Time in POS
0 2,000 4,000 6,00086
88
90
Training Time (seconds)
Tes
tA
cc
(c) Test Acc vs. Time in DP-WSJ
0 20 40 60 80 100
2,000
4,000
6,000
Training Time (seconds)
Prim
alO
bjec
tive
Val
ue
(d) Primal Objective Value in NER-CoNLL
0 100 200 300 400 500
1
1.5
2 ?104
Training Time (seconds)
Prim
alO
bjec
tive
Val
ue
(e) Primal Objective Value in POS
0 2,000 4,000 6,000
2
4
6 ?104
Training Time (seconds)
Prim
alO
bjec
tive
Val
ue
DCD-SSVM
DCD-Light
CPD
(f) Primal Objective Value in DP-WSJ
Figure 1: We plot the testing performance (top row) and the primal objective function (bottom row) versus training
time for three optimization methods for learning the L2-Loss SSVM. In general, DCD-SSVM is the best algorithm for
both the objective function and the testing performance.
4 Experiments
In order to verify the effectiveness of the proposed
algorithm, we conduct a set of experiments on dif-
ferent optimization and learning algorithms. Before
going to the experimental results, we first introduce
the tasks and settings used in the experiments.
4.1 Tasks and Data
We evaluated our method and existing structured
output learning approaches on named entity recog-
nition (NER), part-of-speech tagging (POS) and de-
pendency parsing (DP) on four benchmark datasets.
NER-MUC7 MUC-7 data contains a subset of
North American News Text Corpora annotated with
many types of entities. We followed the settings
in (Ratinov and Roth, 2009) and consider three main
entities categories: PER, LOC and ORG. We evalu-
ated the results using phrase-level F1.
retic analysis significantly. Also note that Theorem 2 shows
that if we put all possible structures in the working sets (i.e.,
F (?) = FS(?)), then the DCD algorithms can obtain -optimal
solution in O(log( 1 )) iterations.
NER-CoNLL This is the English dataset from the
CoNLL 2003 shared task (T. K. Sang and De Meul-
der, 2003). The data set labels sentences from the
Reuters Corpus, Volume 1 (Lewis et al, 2004) with
four different entity types: PER, LOC, ORG and
MISC. We evaluated the results using phrase-level
F1.
POS-WSJ The standard set for evaluating the per-
formance of a part-of-speech tagger. The training,
development and test sets consist of sections 0-18,
19-21 and 22-24 of the Penn Treebank data (Marcus
et al, 1993), respectively. We evaluated the results
by token-level accuracy.
DP-WSJ We took sections 02-21 of Penn Tree-
bank as the training set, section 00 as the develop-
ment set and section 23 as the test set. We imple-
ment a simple version of hash kernel to speed up of
training procedure for this task (Bohnet, 2010). We
reported the unlabeled attachment accuracy for this
task (McDonald et al, 2005).
213
0 10 20 3076
77
78
79
80
Training Time (seconds)
Tes
tF1
(a) Test F1 vs. Time in NER-MUC7
0 20 40 60 80 100 120
80
82
84
86
Training Time (seconds)
Tes
tF1
(b) Test F1 vs. Time in NER-CoNLL
0 200 400 60095
96
97
Training Time (seconds)
Tes
tA
cc
DCD-SSVM
FW-Struct
SVM-Struct
(c) Test Acc vs. Time in POS-WSJ
Figure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct. Note that
DCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct.
4.2 Features and Inference Algorithms
For the sequence labeling tasks, NER and POS,
we followed the discriminative HMM settings used
in (Joachims et al, 2009) and defined the features as
?(x,y) =
N?
i=l
?
?????
?emi(xi, yi)
[yi = 1][yi?1 = 1]
[yi = 1][yi?1 = 2]
. . .
[yi = k][yi?1 = k]
?
?????
,
where ?emi is the feature vector dedicated to the i-th
token (or, the emission features), N represents the
number of tokens in this sequence, yi represents the
i-th token in the sequence y, [yi = 1] is the indictor
variable and k is the number of tags.
The inference problems are solved by the Viterbi
algorithm. The emission features used in both POS
and NER are the standard ones, including word fea-
tures, word-shape features, etc. For NER, we used
additional simple gazetteer features7 and word clus-
ter features (Turian et al, 2010)
For dependency parsing, we followed the setting
described in (McDonald et al, 2005) and used sim-
ilar features. The decoding algorithm is the first-
order Eisner?s algorithm (Eisner, 1997).
4.3 Algorithms and Implementation Detail
For all SSVM algorithms (including SGD), C was
chosen among the set {0.01, 0.05, 0.1, 0.5, 1, 5} ac-
cording to the accuracy/F1 on the development set.
For each task, the same features were used by all
7Adding Wikipedia gazetteers would likely increase the per-
formance significantly (Ratinov and Roth, 2009).
algorithms. For NER-MUC7, NER-CoNLL and
POS-WSJ, we ran the online algorithms and DCD-
SSVM for 25 iterations. For DP-WSJ, we only let
the algorithms run for 10 iterations as the inference
procedure is very expensive computationally. The
algorithms in the experiments are:
DCD Our dual coordinate descent method on the
L2-Loss SSVM. For DCD-SSVM, r is set to be 5.
For both DCD-Light and DCD-SSVM , we follow
the suggestion in (Joachims et al, 2009): if the value
of a dual variable becomes zero, its corresponding
structure will be removed from the working set to
improve the speed.
SVM-Struct We used the latest (v3.10) of SVM-
HMM.8 This version uses the cutting plane method
on a 1-slack variable formulation (Joachims et al,
2009) for the L1-Loss SSVM. SVM-Struct was im-
plemented in C and all the other algorithms are im-
plemented in C#. We did not apply SVM-Struct to
DP-WSJ because there is no native implementation.
Perceptron This refers to the averaged structured
Perceptron method introduced by Collins (2002). To
speed up the convergence rate, we shuffle the train-
ing examples at each iteration.
MIRA Margin Infused Relaxed Algorithm
(MIRA) (Crammer et al, 2005) is the online
learning algorithm that explicitly uses the notion
of margin to update the weight vector. We use
1-best MIRA in our experiments. To increase
8http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
214
the convergence speed, we shuffle the training
examples at each iteration. Following (McDonald et
al., 2005), we did not tune the C parameter for the
MIRA algorithm.
SGD Stochastic gradient descent (SGD) (Bottou,
2004) is a technique for optimizing general convex
functions. In this paper, we use SGD as an alterna-
tive baseline for optimizing the L1-Loss SSVM ob-
jective function (Eq. (2) with higne loss).9 When us-
ing SGD, the learning rate must be carefully tuned.
Following (Bottou, 2004), the learning rate is ob-
tained by
?0
(1.0 + (?0T/C))0.75
,
where C is the regularization parameter, T is the
number of updates so far and ?0 is the initial step
size. The parameter ?0 was selected among the set
{2?1, 2?2, 2?3, 2?4, 2?5} by running the SGD al-
gorithm on a set of 1000 randomly sampled exam-
ples, and then choosing the ?0 with lowest primal
objective function on these examples.
FW-Struct FW-Struct represents the Frank-Wolfe
algorithm for the L1-Loss SSVM (Lacoste-Julien et
al., 2013).
In order to improve the training speed, we cached
all the feature vectors generated by the gold la-
beled data once computed. This applied to all al-
gorithms except SVM-Struct, which has its own
caching mechanism. We report the performance
of the averaged weight vectors for Perceptron and
MIRA.
4.4 Results
We present the experimental results below on com-
paring different dual coordinate descent methods,
as well as comparing our main algorithm, DCD-
SSVM, with other structured learning approaches.
4.4.1 Comparisons of DCD Methods
We compared three DCD methods: DCD-Light,
DCD-SSVM and CPD. CPD is a cutting plane
method proposed by Chang et al (2010), which uses
9To compare with SGD using its best setting, we report only
the results of SGD on the L1-Loss SSVM as we found tuning
the step size for the L2-Loss SSVM is more difficult.
a dual coordinate descent algorithm to solve the in-
ternal sub-problems. We specifically included CPD
as it also targets at the L2-Loss SSVM.
Because different optimization strategies will
reach the same objective values eventually, compar-
ing them on prediction accuracy of the final models
is not meaningful. Instead, here we compare how
fast each algorithm converges as shown in Figure 1.
Each marker on the line in this figure represents one
iteration of the corresponding algorithm. Generally
speaking, CPD improves the model very slowly in
the early stages, but much faster after several iter-
ations. In comparison, DCD-Light often behaves
much better initially, and DCD-SSVM is generally
the most efficient algorithm here.
The reason behind the slow performance of CPD
is clear. During early rounds of the algorithm,
the weight vector is far from optimal, so it spends
too much time using ?bad? weight vectors to find
the most violated structures. On the other hand,
DCD-Light updates the weight vector more fre-
quently, so it behaves much better in general. DCD-
SSVM spends more time on updating models during
each batch, but keeps the same amount of time doing
inference as DCD-Light. As a result, it finds a better
trade-off between inference and learning.
4.4.2 DCD-SSVM, SVM-Struct and FW-Struct
Joachims et al (2009) proposed a 1-slack vari-
able method for the L1-Loss SSVM. They showed
that solving a 1-slack variable formulation is an
order-of-magnitude faster than solving the original
formulation (l-slack variables formulation). Nev-
ertheless, from Figure 2, we can see the clear ad-
vantage of DCD-SSVM over SVM-Struct. Al-
though using 1-slack variable has improved the
learning speed, SVM-Struct still converges slower
than DCD-SSVM. In addition, the performance of
models trained by SVM-Struct in the early stage is
quite unstable, which makes early stopping an in-
effective strategy in practice when training time is
limited.
We also compared our algorithms to FW-Struct.
Our results agree with (Lacoste-Julien et al, 2013),
which shows that the FW-Struct outperforms the
SVM-Struct. In our experiments, we found that our
DCD algorithms were competitive, sometimes con-
verged faster than the FW-Struct.
215
0 10 20 3076
77
78
79
80
Training Time (seconds)
Tes
tF1
(a) Test F1 vs. Time in NER-MUC7
0 100 200 300 400
96.6
96.8
97
97.2
Training Time (seconds)
Tes
tA
cc
(b) Test Acc vs. Time in POS-WSJ
0 2,000 4,000 6,00088
89
90
91
Training Time (seconds)
Tes
tA
cc
DCD-SSVM
PERP
MIRA
SGD
(c) Test Acc vs. Time in DP-WSJ
Figure 3: Comparisons between DCD-SSVM and popular online learning algorithms. Note that the results diverge
when comparing Perceptron and MIRA. In general, DCD-SSVM is the most stable algorithm.
Task/Data DCD Percep MIRA SGD
NER-MUC7 79.4 78.5 78.8 77.8
NER-CoNLL 85.6 85.3 85.1 84.2
POS-WSJ 97.1 96.9 96.9 96.9
DP-WSJ 90.8 90.3 90.2 90.9
Table 1: Performance of online learning algorithms and
the DCD-SSVM algorithm on the testing sets. NER is
measured by F1 while others by accuracy.
4.4.3 DCD-SSVM, MIRA, Perceptron and
SGD
As in binary classification, large-margin methods
like SVMs often perform better than algorithms like
Perceptron and SGD (Hsieh et al, 2008; Shalev-
Shwartz and Zhang, 2013), here we observe similar
behaviors in the structured output domain. Table 1
shows the final test accuracy numbers or F1 scores of
models trained by algorithms including Perceptron,
MIRA and SGD, compared to those of the SSVM
models trained by DCD-SSVM. Among the bench-
mark datasets and tasks we have experimented with,
DCD-SSVM derived the most accurate models, ex-
cept for DP-WSJ when compared to SGD.
Perhaps a more interesting comparison is on
the training speed, which can be observed in Fig-
ure 3. Compared to other online algorithms, DCD-
SSVM can take advantage of cached dual variables
and structures. We show that the training speed of
DCD-SSVM can be competitive to that of the on-
line learning algorithms, unlike SVM-Struct. Note
that SGD is not very stable for NER-MUC7, even
though we tuned the step size very carefully.
5 Conclusion
In this paper, we present a novel approach for learn-
ing the L2-Loss SSVM model. By combining the
ideas of dual coordinate descent and cutting plane
methods, the hybrid approach, DCD-SSVM outper-
forms other SSVM training methods both in terms
of objective value reduction and testing error rate
reduction. As demonstrated in our experiments
on several NLP tasks, our approach also tends to
learn more accurate models than commonly used
structured learning algorithms, including structured
Perceptron, MIRA and SGD. Perhaps more inter-
estingly, our SSVM learning method is very effi-
cient: the model training time is competitive to on-
line learning algorithms such as structured Percep-
tron and MIRA. These unique qualities make DCD-
SSVM an excellent choice for solving a variety of
complex NLP problems.
In the future, we would like to compare our algo-
rithm to other structured prediction approaches, such
as conditional random fields (Lafferty et al, 2001)
and exponential gradient descent methods (Collins
et al, 2008). Expediting the learning process fur-
ther by leveraging approximate inference is also an
interesting direction to investigate.
Acknowledgments
We sincerely thank John Platt, Lin Xiao and Kaiwei Chang for
the discussions and feedback. We are grateful to Po-Wei Wang
and Chih-Jen Lin for providing their work on convergence rate
analysis on feasible descent methods. We also thank the review-
ers for their detailed comments on this paper.
216
References
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, Proceedings the International Conference
on Computational Linguistics (COLING).
L. Bottou. 2004. Stochastic learning. In Olivier Bous-
quet and Ulrike von Luxburg, editors, Advanced Lec-
tures on Machine Learning, Lecture Notes in Artifi-
cial Intelligence, LNAI 3176, pages 146?168. Springer
Verlag, Berlin.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML).
T. Cohn and M. Lapata. 2009. Sentence compression
as tree transduction. Journal of AI Research, 34:637?
674, April.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L.
Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin Markov
networks. Journal of Machine Learning Research, 9.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP).
K. Crammer, R. Mcdonald, and F. Pereira. 2005. Scal-
able large-margin online learning for structured clas-
sification. Technical report, Department of Computer
and Information Science, University of Pennsylvania.
J. Desrosiers and M. E. Lu?bbecke. 2005. A primer in
column generation. In Column Generation, pages 1?
32. Springer.
J. M. Eisner. 1997. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
the International Conference on Computational Lin-
guistics (COLING), pages 340?345.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the Perceptron algorithm. Machine
Learning, 37(3):277?296.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear SVM. In Proceedings
of the International Conference on Machine Learning
(ICML), New York, NY, USA. ACM.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural SVMs. Machine
Learning, 77(1):27?59.
J. Kivinen and M. K. Warmuth. 1995. Exponentiated
gradient versus gradient descent for linear predictors.
In ACM Symp. of the Theory of Computing.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proceedings of the 2007 Joint Conference of
EMNLP-CoNLL, pages 141?150.
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang,
K. Torisawa, and H. Isahara. 2009. An error-driven
word-character hybrid model for joint chinese word
segmentation and pos tagging. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 513?521.
S. Lacoste-Julien, M. Jaggi, M. W. Schmidt, and
P. Pletscher. 2013. Stochastic block-coordinate Frank-
Wolfe optimization for structural SVMs. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML).
D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1:
A new benchmark collection for text categorization
research. Journal of Machine Learning Research,
5:361?397.
L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. 2009.
Enhancing diversity, coverage and balance for summa-
rization through structure learning. In Proceedings of
the 18th international conference on World wide web,
The International World Wide Web Conference, pages
71?80, New York, NY, USA. ACM.
Z.-Q. Luo and P. Tseng. 1993. Error bounds and conver-
gence analysis of feasible descent methods: A general
approach. Annals of Operations Research, 46(1):157?
178.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330, June.
A. F. Martins, K. Gimpel, N. A. Smith, E. P. Xing, M. A.
Figueiredo, and P. M. Aguiar. 2010. Learning struc-
tured classifiers with dual coordinate ascent. Technical
report, Technical report CMU-ML-10-109.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 91?98, Ann
Arbor, Michigan.
A. Mejer and K. Crammer. 2010. Confidence in
structured-prediction using confidence-weighted mod-
els. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, Pro-
ceedings of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 971?
981.
217
R. C. Moore, W. Yih, and A. Bode. 2007. Improved dis-
criminative bilingual word alignment. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL), Jun.
N. Ratliff, J. Andrew (Drew) Bagnell, and M. Zinkevich.
2007. (Online) subgradient methods for structured
prediction. In Proceedings of the International Work-
shop on Artificial Intelligence and Statistics, March.
S. Shalev-Shwartz and Y. Singer. 2006. Online learn-
ing meets optimization in the dual. In Proceedings of
the Annual ACM Workshop on Computational Learn-
ing Theory (COLT).
S. Shalev-Shwartz and T. Zhang. 2013. Stochastic dual
coordinate ascent methods for regularized loss min-
imization. Journal of Machine Learning Research,
14:567?599.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: primal estimated sub-gradient solver for SVM.
In Zoubin Ghahramani, editor, Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 807?814. Omnipress.
S. Shevade, P. Balamurugan, S. Sundararajan, and
S. Keerthi. 2011. A sequential dual method for struc-
tural SVMs. In IEEE International Conference on
Data Mining(ICDM).
E. F. T. K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Walter Daelemans and
Miles Osborne, editors, Proceedings of CoNLL-2003,
pages 142?147. Edmonton, Canada.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-
margin markov networks. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004b. Max-margin parsing. In Proceedings
of the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
B. Taskar, S. Lacoste-julien, and M. I. Jordan. 2005.
Structured prediction via the extragradient method. In
The Conference on Advances in Neural Information
Processing Systems (NIPS).
B. Taskar, S. Lacoste-Julien, and M. I Jordan. 2006.
Structured prediction, dual extragradient and bregman
projections. Journal of Machine Learning Research,
7:1627?1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. In Proceedings
of the International Conference on Machine Learning
(ICML).
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 384?394, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Po-Wei Wang and Chih-Jen Lin. 2013. Iteration com-
plexity of feasible descent methods for convex opti-
mization. Technical report, National Taiwan Univer-
sity.
C. Yu and T. Joachims. 2009. Learning structural SVMs
with latent variables. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML).
218
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18?27,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Driving Semantic Parsing from the World?s Response
James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61820
{clarkeje,goldwas1,mchang21,danr}@illinois.edu
Abstract
Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.
1 Introduction
Semantic Parsing, the process of converting text
into a formal meaning representation (MR), is one
of the key challenges in natural language process-
ing. Unlike shallow approaches for semantic in-
terpretation (e.g., semantic role labeling and in-
formation extraction) which often result in an in-
complete or ambiguous interpretation of the natu-
ral language (NL) input, the output of a semantic
parser is a complete meaning representation that
can be executed directly by a computer program.
Semantic parsing has mainly been studied in the
context of providing natural language interfaces
to computer systems. In these settings the target
meaning representation is defined by the seman-
tics of the underlying task. For example, provid-
ing access to databases: a question posed in nat-
ural language is converted into a formal database
query that can be executed to retrieve information.
Example 1 shows a NL input query and its corre-
sponding meaning representation.
Example 1 Geoquery input text and output MR
?What is the largest state that borders Texas??
largest(state(next to(const(texas))))
Previous works (Zelle and Mooney, 1996; Tang
and Mooney, 2001; Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) employ
machine learning techniques to construct a seman-
tic parser. The learning algorithm is given a set of
input sentences and their corresponding meaning
representations, and learns a statistical semantic
parser ? a set of rules mapping lexical items and
syntactic patterns to their meaning representation
and a score associated with each rule. Given a sen-
tence, these rules are applied recursively to derive
the most probable meaning representation. Since
semantic interpretation is limited to syntactic pat-
terns identified in the training data, the learning
algorithm requires considerable amounts of anno-
tated data to account for the syntactic variations
associated with the meaning representation. An-
notating sentences with their MR is a difficult,
time consuming task; minimizing the supervision
effort required for learning is a major challenge in
scaling semantic parsers.
This paper proposes a new model and learning
paradigm for semantic parsing aimed to alleviate
the supervision bottleneck. Following the obser-
vation that the target meaning representation is to
be executed by a computer program which in turn
provides a response or outcome; we propose a re-
sponse driven learning framework capable of ex-
ploiting feedback based on the response. The feed-
back can be viewed as a teacher judging whether
the execution of the meaning representation pro-
duced the desired response for the input sentence.
18
This type of supervision is very natural in many
situations and requires no expertise, thus can be
supplied by any user.
Continuing with Example 1, the response gen-
erated by executing a database query would be
used to provide feedback. The feedback would be
whether the generated response is the correct an-
swer for the input question or not, in this case New
Mexico is the desired response.
In response driven semantic parsing, the learner
is provided with a set of natural language sen-
tences and a feedback function that encapsulates
the teacher. The feedback function informs the
learner whether its interpretation of the input sen-
tence produces the desired response. We consider
scenarios where the feedback is provided as a bi-
nary signal, correct +1 or incorrect ?1.
This weaker form of supervision poses a chal-
lenge to conventional learning methods: semantic
parsing is in essence a structured prediction prob-
lem requiring supervision for a set of interdepen-
dent decisions, while the provided supervision is
binary, indicating the correctness of a generated
meaning representation. To bridge this difference
we propose two novel learning algorithms suited
to the response driven setting.
Furthermore, to account for the many syntac-
tic variations associated with the MR, we propose
a new model for semantic parsing that allows us
to learn effectively and generalize better. Cur-
rent semantic parsing approaches extract parsing
rules mapping NL to their MR, restricting pos-
sible interpretations to previously seen syntactic
patterns. We replace the rigid inference process
induced by the learned parsing rules with a flex-
ible framework. We model semantic interpreta-
tion as a sequence of interdependent decisions,
mapping text spans to predicates and use syntac-
tic information to determine how the meaning of
these logical fragments should be composed. We
frame this process as an Integer Linear Program-
ming (ILP) problem, a powerful and flexible in-
ference framework that allows us to inject rele-
vant domain knowledge into the inference process,
such as specific domain semantics that restrict the
space of possible interpretations.
We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), a
database consisting of U.S. geographical informa-
tion, and natural language questions. Our experi-
mental results show that our model with response
driven learning can outperform existing models
trained with annotated logical forms.
The key contributions of this paper are:
Response driven learning for semantic parsing
We propose a new learning paradigm for learn-
ing semantic parsers without any annotated mean-
ing representations. The supervision for learning
comes from a binary feedback signal based a re-
sponse generated by executing a meaning repre-
sentation. This type of supervision signal is nat-
ural to produce and can be acquired from non-
expert users.
Novel training algorithms Two novel train-
ing algorithms are developed within the response
driven learning paradigm. The training algorithms
are applicable beyond semantic parsing and can be
used in situations where it is possible to obtain bi-
nary feedback for a structured learning problem.
Flexible semantic interpretation process We
propose a novel flexible semantic parsing model
that can handle previously unseen syntactic varia-
tions of the meaning representation.
2 Semantic Parsing
The goal of semantic parsing is to produce a func-
tion F : X ? Z that maps from the space natural
language input sentences, X , to the space of mean-
ing representations, Z . This type of task is usu-
ally cast as a structured output prediction problem,
where the goal is to obtain a model that assigns the
highest score to the correct meaning representa-
tion given an input sentence. However, in the task
of semantic parsing, this decision relies on identi-
fying a hidden intermediate representation (or an
alignment) that captures the way in which frag-
ments of the text correspond to the meaning repre-
sentation. Therefore, we formulate the prediction
function as follows:
z? = Fw(x) = argmax
y?Y ,z?Z
wT?(x,y, z) (1)
Where ? is a feature function that describes the
relationships between an input sentence x, align-
ment y and meaning representation z. w is the
weight vector which contains the parameters of the
model. We refer to the argmax above as the in-
ference problem. The feature function combined
with the nature of the inference problem defines
the semantic parsing model. The key to producing
19
What is the largest Texas?
largest( const(texas))))
New Mexico
x:
y:
z:
r:
that bordersstate
state( next_to(
Figure 1: Example input sentence, meaning repre-
sentation, alignment and answer for the Geoquery
domain
a semantic parser involves defining a model and a
learning algorithm to obtain w.
In order to exemplify these concepts we con-
sider the Geoquery domain. Geoquery contains a
query language for a database of U.S. geograph-
ical facts. Figure 1 illustrates concrete examples
of the terminology introduce. The input sentences
x are natural language queries about U.S. geog-
raphy. The meaning representations z are logical
forms which can be executed on the database to
obtain a response which we denote with r. The
alignment y captures the associations between x
and z.
Building a semantic parser involves defining the
model (feature function ? and inference problem)
and a learning strategy to obtain weights (w) as-
sociated with the model. We defer discussion of
our model until Section 4 and first focus on our
learning strategy.
3 Structured Learning with Binary
Feedback
Previous approaches to semantic parsing have
assumed a fully supervised setting where
a training set is available consisting of ei-
ther: input sentences and logical forms
{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,
2005)) or input sentences, logical forms
and a mapping between their constituents
{(xl,yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).
Given such training examples a weight vector w
can be learned using structured learning methods.
Obtaining, through annotation or other means, this
form of training data is an expensive and difficult
process which presents a major bottleneck for
semantic parsing.
To reduce the burden of annotation we focus
on a new learning paradigm which uses feedback
from a teacher. The feedback signal is binary
(+1,?1) and informs the learner whether a pre-
dicted logical form z? when executed on the target
Algorithm 1 Direct Approach (Binary Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Bl ? {} for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: add (?(xl, y?, z?)/|xl|, f) to Bl
7: end for
8: w? BinaryLearn(B) where B = ?lBl
9: until no Bl has new unique examples
10: return w
domain produces the desired response or outcome.
This is a very natural method for providing super-
vision in many situations and requires no exper-
tise. For example, a user can observe the response
and provide a judgement. The general form of
the teacher?s feedback is provided by a function
Feedback : X ? Z ? {+1,?1}.
For the Geoquery domain this amounts to
whether the logical form produces the correct re-
sponse r for the input sentence. Geoquery has the
added benefit that the teacher can be automated
if we have a dataset consisting of input sentences
and response pairs {(xl, rl)}Nl=1. Feedback eval-
uates whether a logical form produces a response
matching r:
Feedback (xl, z) =
{
+1 if execute(z) = rl
?1 otherwise
We are now ready to present our learning
with feedback algorithms that operate in situations
where input sentences, {xl}Nl=1, and a teacher
feedback mechanism, Feedback , are available. We
do not assume the availability of logical forms.
3.1 Direct Approach (Binary Learning)
In general, a weight vector can be considered
good if when used in the inference problem (Equa-
tion (1)) it scores the correct logical form and
alignment (which may be hidden) higher than all
other logical forms and alignments for a given in-
put sentence. The intuition behind the direct ap-
proach is that the feedback function can be used to
subsample the space of possible structures (align-
ments and logical forms (Y ? Z)) for a given in-
put x. The feedback mechanism indicates whether
the structure is good (+1) or bad (?1). Using this
20
intuition we can cast the problem of learning a
weight vector for Equation (1) as a binary classifi-
cation problem where we directly consider struc-
tures the feedback assigns +1 as positive examples
and those assigned ?1 as negative.
We represent the input to the binary classifier
as the feature vector ?(x,y, z) normalized by the
size of the input sentence1 |x|, and the label as the
result from Feedback (x, z).
Algorithm 1 outlines the approach in detail. The
first stage of the algorithm iterates over all the
training input sentences and computes the best
logical form z? and alignment y? by solving the in-
ference problem (line 4). The feedback function
is queried (line 5) and a training example for the
binary predictor created using the normalized fea-
ture vector from the triple containing the sentence,
alignment and logical form as input and the feed-
back as the label. This training example is added
to the working set of training examples for this in-
put sentence (line 6). All the feedback training ex-
amples are used to train a binary classifier whose
weight vector is used in the next iteration (line 8).
The algorithm repeats until no new unique training
examples are added to any of the working sets for
any input sentence. Although the number of possi-
ble training examples is very large, in practice the
algorithm is efficient and converges quickly. Note
that this approach is capable of using a wide va-
riety of linear classifiers as the base learner (line
8).
A policy is required to specify the nature of
the working set of training examples (Bl) used for
training the base classifier. This is pertinent in line
6 of the algorithm. Possible policies include: al-
lowing duplicates in the working set (i.e., Bl is
a multiset), disallowing duplicates (Bl is a set),
or only allowing one example per input sentence
(?Bl? = 1). We adopt the first approach in this
paper.2
3.2 Aggressive Approach (Structured
Learning)
There is important implicit information which
the direct approach ignores. It is implicit that
when the teacher indicates an input paired with
an alignment and logical form is good (+1 feed-
1Normalization is required to ensure that each sentence
contributes equally to the binary learning problem regardless
of the sentence?s length.
2The working set Bl for each sentence may contain multi-
ple positive examples with the same and differing alignments.
Algorithm 2 Aggressive Approach (Structured
Learning)
Input: Sentences {xl}Nl=1,
Feedback : X ?Z ? {+1, 1},
initial weight vector w
1: Sl ? ? for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: y?, z? = argmaxy,z w
T?(xl,y, z)
5: f = Feedback (xl, z?)
6: if f is +1 then
7: Sl ? {(xl, y?, z?)}
8: end if
9: end for
10: w? StructLearn(S,?) where S = ?lSl
11: until no Sl has changed
12: return w
back) that in order to repeat this behavior all other
competing structures should be made suboptimal
(or bad). To leverage this implicit information
we adopt a structured learning strategy in which
we consider the prediction as the optimal structure
and all others as suboptimal. This is in contrast to
the direct approach where only structures that have
explicitly received negative feedback are consid-
ered subopitmal.
When a structure is found with positive feed-
back it is added to the training pool for a struc-
tured learner. We consider this approach aggres-
sive as the structured learner implicitly considers
all other structures as being suboptimal. Negative
feedback indicates that the structure should not be
added to the training pool as it will introduce noise
into the learning process.
Algorithm 2 outlines the learning in more detail.
As before, y? and z? are predicted using the cur-
rent weight vector and feedback received (lines 4
and 5). When positive feedback is received a new
training instance for a structured learner is created
from the input sentence and prediction (line 7) this
training instance replaces any previous instance
for the input sentence. When negative feedback
is received the training pool Sl is not updated. A
weight vector is learned using a structured learner
where the training data S contains at most one ex-
ample per input sentence. In the first iteration of
the outer loop the training data S will contain very
few examples. In each subsequent iteration the
newly learned weight vector allows the algorithm
to acquire new examples. This is repeated until no
21
new examples are added or changed in S.
Like the direct approach, this learning frame-
work is makes very few assumptions about the
type of structured learner used as a base learner
(line 10).3
4 Model
Semantic parsing is the process of converting a
natural language input into a formal logic repre-
sentation. This process is performed by associat-
ing lexical items and syntactic patterns with logi-
cal fragments and composing them into a complete
formula. Existing approaches rely on extracting
a set of parsing rules, mapping text constituents
to a logical representation, from annotated train-
ing data and applying them recursively to obtain
the meaning representation. Adapting to new data
is a major limitation of these approaches as they
cannot handle inputs containing syntactic patterns
which were not observed in the training data. For
example, assume the training data produced the
following set of parsing rules:
Example 2 Typical parsing rules
(1) NP [?x.capital(x)]? capital
(2) PP [ const(texas)]? of Texas
(3) NNP [ const(texas)]? Texas
(4) NP [capital(const(texas))]?
NP[?x.capital(x)] PP [ const(texas)]
At test time the parser is given the sentences in
Example 3. Despite the lexical similarity in these
examples, the semantic parser will correctly parse
the first sentence but fail to parse the second be-
cause the lexical items belong to different a syn-
tactic category (i.e., the word Texas is not part of a
preposition phrase in the second sentence).
Example 3 Syntactic variations of the same MR
Target logical form: capital(const(texas))
Sentence 1: ?What is the capital of Texas??
Sentence 2: ?What is Texas? capital??
The ability to adapt to unseen inputs is one
of the key challenges in semantic parsing. Sev-
eral works (Zettlemoyer and Collins, 2007; Kate,
2008) have addressed this issue explicitly by man-
ually defining syntactic transformation rules that
can help the learned parser generalize better. Un-
fortunately these are only partial solutions as a
3Mistake driven algorithms that do not enforce margin
constraints may not be able to generalize using this proto-
col since they will repeat the same prediction at training time
and therefore will not update the model.
manually constructed rule set cannot cover the
many syntactic variations.
Given the previous example, we observe
that it is enough to identify that the function
capital(?) and the constant const(texas)
appear in the target MR, since there is only a single
way to compose these entities into a single formula
? capital(const(texas)).
Motivated by this observation we define our
meaning derivation process over the rules of the
MR language and use syntactic information as a
way to bias the MR construction process. That
is, our inference process considers the entire space
of meaning representations irrespective of the pat-
terns observed in the training data. This is possi-
ble as the MRs are defined by a formal language
and formal grammar.4 The syntactic information
present in the natural language is used as soft ev-
idence (features) which guides the inference pro-
cess to good meaning representations.
This formulation is a major shift from existing
approaches that rely on extracting parsing rules
from the training data. In existing approaches
the space of possible meaning representations is
constrained by the patterns in the training data
and syntactic structure of the natural language in-
put. Our formulation considers the entire space of
meaning representations and allows the model to
adapt to previously unseen data and always pro-
duce a semantic interpretation by using the pat-
terns observed in the input.
We frame our semantic interpretation process
as a constrained optimization process, maximiz-
ing the objective function defined by Equation 1
which relies on extracting lexical and syntactic
features instead of parsing rules. In the remain-
der of this section we explain the components of
our inference model.
4.1 Target Meaning Representation
Following previous work, we capture the se-
mantics of the Geoquery domain using a sub-
set of first-order logic consisting of typed con-
stants and functions. There are two types: en-
tities E in the domain and numeric values N .
Functions describe a functional relationship over
types (e.g., population : E ? N ). A com-
plete logical form is constructed through func-
tional composition; in our formalism this is per-
4This is true for all meaning representations designed to
be executed by a computer system.
22
formed by the substitution operator. For ex-
ample, given the function next to(x) and
the expression const(texas), substitution re-
places the occurrence of the free variable x, with
the expression, resulting in a new logical form:
next to(const(texas)). Due to space lim-
itations we refer the reader to (Zelle and Mooney,
1996) for a detailed description of the Geoquery
domain.
4.2 Semantic Parsing as Constrained
Optimization
Recall that the goal of semantic parsing is to pro-
duce the following function (Equation (1)):
Fw(x) = argmax
y,z
wT?(x,y, z)
However, given that y and z are complex struc-
tures it is necessary to decompose the structure
into a set of smaller decisions to facilitate efficient
inference.
In order to define our decomposition we intro-
duce additional notation: c is a constituent (or
word span) in the input sentence x and D is the
set of all function and constant symbols in the do-
main. The alignment y is defined as a set of map-
pings between constituents and symbols in the do-
main y = {(c, s)} where s ? D.
We decompose the construction of an alignment
and logical form into two types of decisions:
First-order decisions. A mapping between con-
stituents and logical symbols (functions and con-
stants).
Second-order decisions. Expressing how logi-
cal symbols are composed into a complete logical
interpretation. For example, whether next to
and state forms next to(state(?)) or
state(next to(?)).
Note that for all possible logical forms and
alignments there exists a one-to-one mapping to
these decisions.
We frame the inference problem as an Integer
Linear Programming (ILP) problem (Equation (2))
in which the first-order decisions are governed by
?cs, a binary decision variable indicating that con-
stituent c is aligned with logical symbol s. And
?cs,dt capture the second-order decisions indicat-
ing the symbol t (associated with constituent d)
is an argument to function s (associated with con-
stituent c).
Fw(x) = argmax
?,?
?
c?x
?
s?D
?cs ?wT?1(x, c, s)
+
?
c,d?x
?
s,t?D
?cs,dt ?wT?2(x, c, s, d, t) (2)
It is clear that there are dependencies between
the ?-variables and ?-variables. For example,
given that ?cs,dt is active, the corresponding ?-
variables ?cs and ?dt must also be active. In order
to ensure a consistent solution we introduce a set
of constraints on Equation (2). In addition we add
constraints which leverage the typing information
inherent in the domain to eliminate logical forms
that are invalid in the Geoquery domain. For ex-
ample, the function length only accepts river
types as input. The set of constraints are:
? A given constituent can be associated with
exactly one logical symbol.
? ?cs,dt is active if and only if ?cs and ?dt are
active.
? If ?cs,dt is active, s must be a function and
the types of s and t should be consistent.
? Functional composition is directional and
acyclic.
The flexibility of ILP has previously been advan-
tageous in natural language processing tasks (Roth
and Yih, 2007) as it allows us to easily incorporate
such constraints.
4.3 Features
The inference problem defined in Equation (2)
uses two feature functions: ?1 and ?2.
First-order decision features ?1 Determining
if a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.5 This lexicon is
small and only used as a starting point. Existing
approaches rely on annotated logical forms to ex-
tend the lexicon. However, in our setting we do
not have access to annotated logical forms, instead
we rely on external knowledge to supply further
5The lexicon contains on average 1.42 words per func-
tion and 1.07 words per constant. For example the function
next to has the lexical entries: borders, next, adjacent and
the constant illinois the lexical item illinois.
23
information. We add features which measure the
lexical similarity between a constituent and a logi-
cal symbol?s surface forms (as defined by the lexi-
con). Two metrics are used: stemmed word match
and a similarity metric based on WordNet (Miller
et al, 1990) which allows our model to account
for words not in the lexicon. The WordNet met-
ric measures similarity based on synonymy, hy-
ponymy and meronymy (Do et al, 2010). In the
case where the constituent is a preposition, which
are notorious for being ambiguous, we add a fea-
ture that considers the current lexical context (one
word to the left and right) in addition to word sim-
ilarity.
Second-order decision features ?2 Determin-
ing how to compose two logical symbols relies on
syntactic information, in our model we use the de-
pendency tree (Klein and Manning, 2003) of the
input sentence. Given a second-order decision
?cs,dt, the dependency feature takes the normal-
ized distance between the head words in the con-
stituents c and d. A set of features also indicate
which logical symbols are usually composed to-
gether, without considering their alignment to text.
5 Experiments
In this section we describe our experimental setup,
which includes the details of the domain, re-
sources and parameters.
5.1 Domain and Corpus
We evaluate our system on the Geoquery domain
as described previously. The domain consists of
a database and Prolog query language for U.S.
geographical facts. The corpus contains of 880
natural language queries paired with Prolog log-
ical form queries ((x, z) pairs). We follow previ-
ous approaches and transform these queries into a
functional representation. We randomly select 250
sentences for training and 250 sentences for test-
ing.6 We refer to the training set as Response 250
(R250) indicating that each example x in this data
set has a corresponding desired database response
r. We refer the testing set as Query 250 (Q250)
where the examples only contain the natural lan-
guage queries.
6Our inference problem is less constrained than previous
approaches thus we limit the training data to 250 examples
due to scalability issues. We also prune the search space by
limiting the number of logical symbol candidates per word
(on average 13 logical symbols per word).
Precision and recall are typically used as eval-
uation metrics in semantic parsing. However, as
our model inherently has the ability to map any
input sentence into the space of meaning repre-
sentations the trade off between precision and re-
call does not exist. Thus, we report accuracy: the
percentage of meaning representations which pro-
duce the correct response. This is equivalent to
recall in previous work (Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007).
5.2 Resources and Parameters
Feedback Recall that our learning framework
does not require meaning representation annota-
tions. However, we do require a Feedback func-
tion that informs the learner whether a predicted
meaning representation when executed produces
the desired response for a given input sentence.
We automatically generate a set of natural lan-
guage queries and response pairs {(x, r)} by exe-
cuting the annotated logical forms on the database.
Using this data we construct an automatic feed-
back function as described in Section 3.
Domain knowledge Our learning approaches
require an initial weight vector as input. In or-
der to provide an initial starting point, we initialize
the weight vector using a similar procedure to the
one used in (Zettlemoyer and Collins, 2007) to set
weights for three features and a bias term. The
weights were developed on the training set using
the feedback function to guide our choices.
Underlying Learning Algorithms In the direct
approach the base linear classifier we use is a lin-
ear kernel Support Vector Machine with squared-
hinge loss. In the aggressive approach we de-
fine our base structured learner to be a structural
Support Vector Machine with squared-hinge loss
and use hamming distance as the distance func-
tion. We use a custom implementation to op-
timize the objective function using the Cutting-
Plane method, this allows us to parrallelize the
learning process by solving the inference problem
for multiple training examples simultaneously.
6 Results
Our experiments are designed to answer three
questions:
1. Is it possible to learn a semantic parser with-
out annotated logical forms?
24
Algorithm R250 Q250
NOLEARN 22.2 ?
DIRECT 75.2 69.2
AGGRESSIVE 82.4 73.2
SUPERVISED 87.6 80.4
Table 1: Accuracy of learned models on R250 data and
Q250 (testing) data. NOLEARN: using initialized weight
vector, DIRECT: using feedback with the direct approach,
AGGRESSIVE: using feedback with the aggressive approach,
SUPERVISED: using gold 250 logical forms for training.
Note that none of the approaches use any annotated logical
forms besides the SUPERVISED approach.
Algorithm # LF Accuracy
AGGRESSIVE ? 73.2
SUPERVISED 250 80.4
W&M 2006 ? 310 ? 60.0
W&M 2007 ? 310 ? 75.0
Z&C 2005 600 79.29
Z&C 2007 600 86.07
W&M 2007 800 86.59
Table 2: Comparison against previously published results.
Results show that with a similar number of logical forms
(# LF) for training our SUPERVISED approach outperforms
existing systems, while the AGGRESSIVE approach remains
competitive without using any logical forms.
2. How much performance do we sacrifice by
not restricting our model to parsing rules?
3. What, if any, are the differences in behaviour
between the two learning with feedback ap-
proaches?
We first compare how well our model performs
under four different learning regimes. NOLEARN
uses a manually initialized weight vector. DIRECT
and AGGRESSIVE use the two response driven
learning approaches, where a feedback function
but no logical forms are provided. As an up-
per bound we train the model using a fully SU-
PERVISED approach where the input sentences are
paired with hand annotated logical forms.
Table 1 shows the accuracy of each setup. The
model without learning (NOLEARN) gives a start-
ing point with an accuracy of 22.2%. The re-
sponse driven learning methods perform substan-
tially better than the starting point. The DIRECT
approach which uses a binary learner reaches an
accuracy of 75.2% on the R250 data and 69.2% on
the Q250 (testing) data. While the AGGRESSIVE
approach which uses a structured learner sees a
bigger improvement, reaching 82.4% and 73.2%
respectively. This is only 7% below the fully SU-
PERVISED upper bound of the model.
To answer the second question, we compare a
supervised version of our model to existing se-
mantic parsers. The results are in Table 2. Al-
though the numbers are not directly comparable
due to different splits in the data7, we can see that
with a similar number of logical forms for train-
ing our SUPERVISED approach outperforms ex-
isting systems (Wong and Mooney, 2006; Wong
and Mooney, 2007), while the AGGRESSIVE ap-
proach remains competitive without using any log-
ical forms. Our SUPERVISED model is still very
competitive with other approaches (Zettlemoyer
and Collins, 2007; Wong and Mooney, 2007),
which used considerably more annotated logical
forms in the training phase.
In order to answer the third question, we turn
our attention to the differences between the two
response driven learning approaches. The DIRECT
and AGGRESSIVE approaches use binary feedback
to learn, however they utilize the signal differently.
DIRECT uses the signal directly to learn a bi-
nary classifier capable of replicating the feedback,
whereas AGGRESSIVE learns a structured predic-
tor that can repeatedly obtain the logical forms
for which positive feedback was received. Thus,
although the AGGRESSIVE outperforms the DI-
RECT approach the concepts each approach learns
may be different. Analysis over the training data
shows that in 66.8% examples both approaches
predict a logical form that gives the correct an-
swer. While AGGRESSIVE correctly answers an
additional 16% which DIRECT gets incorrect. In
the opposite direction, DIRECT correctly answers
8.8% that AGGRESSIVE does not. Leaving only
8.4% of the examples that both approaches pre-
dict incorrect logical forms. This suggests that an
approach which combines DIRECT and AGGRES-
SIVE may be able to improve even further.
Figure 2 shows the accuracy on the entire train-
ing data (R250) at each iteration of learning. We
see that the AGGRESSIVE approach learns to cover
more of the training data and at a faster rate than
DIRECT. Note that the performance of the DI-
RECT approach drops at the first iteration. We hy-
pothesize this is due to imbalances in the binary
feedback dataset (too many negative examples) in
the first iteration.
7It is relatively difficult to compare different approaches
in the Geoquery domain given that many existing papers do
not use the same data split.
25
70 1 2 3 4 5 6
90
0
10
20
30
40
50
60
70
80
Learning Iterations
A
cc
ur
ac
y 
on
 R
es
po
ns
e 
25
0
Direct Approach
Aggressive Approach
Initialization
Figure 2: Accuracy on training set as number of learning
iterations increases.
7 Related Work
Learning to map sentences to a meaning repre-
sentation has been studied extensively in the NLP
community. Early works (Zelle and Mooney,
1996; Tang and Mooney, 2000) employed induc-
tive logic programming approaches to learn a se-
mantic parser. More recent works apply statisti-
cal learning methods to the problem. In (Ge and
Mooney, 2005; Nguyen et al, 2006), the input to
the learner consists of complete syntactic deriva-
tions for the input sentences annotated with logi-
cal expressions. Other works (Wong and Mooney,
2006; Kate and Mooney, 2006; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009) try to alleviate the
annotation effort by only taking sentence and log-
ical form pairs to train the models. Learning is
then defined over hidden patterns in the training
data that associate logical symbols with lexical
and syntactic elements.
In this work we take an additional step to-
wards alleviating the difficulty of training seman-
tic parsers and present a world response based
training protocol. Several recent works (Chen and
Mooney, 2008; Liang et al, 2009; Branavan et
al., 2009) explore using an external world context
as a supervision signal for semantic interpretation.
These works operate in settings different to ours as
they rely on an external world state that is directly
referenced by the input text. Although our frame-
work can also be applied in these settings we do
not assume that the text can be grounded in a world
state. In our experiments the input text consists of
generalized statements which describe some infor-
mation need that does not correspond directly to a
grounded world state.
Our learning framework closely follows recent
work on learning from indirect supervision. The
direct approach resembles learning a binary clas-
sifier over a latent structure (Chang et al, 2010a);
while the aggressive approach has similarities with
work that uses labeled structures and a binary
signal indicating the existence of good structures
to improve structured prediction (Chang et al,
2010b).
8 Conclusions
In this paper we tackle one of the key bottlenecks
in semantic parsing ? providing sufficient super-
vision to train a semantic parser. Our solution is
two fold, first we present a new training paradigm
for semantic parsing that relies on natural, hu-
man level supervision. Second, we suggest a new
model for semantic interpretation that does not
rely on NL syntactic parsing rules, but rather uses
the syntactic information to bias the interpretation
process. This approach allows the model to gener-
alize better and reduce the required amount of su-
pervision. We demonstrate the effectiveness of our
training paradigm and interpretation model over
the Geoquery domain, and show that our model
can outperform fully supervised systems.
Acknowledgements We are grateful to Rohit Kate and
Raymond Mooney for their help with the Geoquery dataset.
Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembu
and the three anonymous reviewers for their insightful com-
ments. This material is based upon work supported by the
Air Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181 and by DARPA under the Bootstrap
Learning Program. Any opinions, findings, and conclusion or
recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the AFRL
or DARPA.
References
S.R.K. Branavan, H. Chen, L. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning for map-
ping instructions to actions. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained la-
tent representations. In Proc. of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
26
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010b. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In Proc. of
the International Conference on Machine Learning
(ICML).
Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydis-
waran. 2010. Robust, Light-weight Approaches to
compute Lexical Similarity. Computer Science Re-
search and Technical Reports, University of Illinois.
http://hdl.handle.net/2142/15462.
R. Ge and R. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
R. Kate and R. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).
R. Kate. 2008. Transforming meaning representation
grammars to improve semantic parsing. In Proc. of
the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Proc. of the Conference on Advances in
Neural Information Processing Systems (NIPS).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography.
L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic
parsing with structured svm ensemble classification
models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.
L. Tang and R. Mooney. 2000. Automated construc-
tion of database interfaces: integrating statistical and
relational learning for semantic parsing. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming
for semantic parsing. In Proc. of the European Con-
ference on Machine Learning (ECML).
Y.-W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proc. of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).
Y.-W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proc. of the National Conference on Artificial In-
telligence (AAAI).
L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. of
the Annual Conference in Uncertainty in Artificial
Intelligence (UAI).
L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and on Computational Natural Language
Learning (EMNLP-CoNLL).
L. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
27
