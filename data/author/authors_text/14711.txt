Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353?1361,
Beijing, August 2010
A Monolingual Tree-based Translation Model for Sentence Simplification?
Zhemin Zhu1
Department of Computer Science
Technische Universita?t Darmstadt
Delphine Bernhard2
LIMSI-CNRS
Iryna Gurevych1
Department of Computer Science
Technische Universita?t Darmstadt
1http://www.ukp.tu-darmstadt.de 2delphine.bernhard@limsi.fr
Abstract
In this paper, we consider sentence sim-
plification as a special form of translation
with the complex sentence as the source
and the simple sentence as the target.
We propose a Tree-based Simplification
Model (TSM), which, to our knowledge,
is the first statistical simplification model
covering splitting, dropping, reordering
and substitution integrally. We also de-
scribe an efficient method to train our
model with a large-scale parallel dataset
obtained from the Wikipedia and Simple
Wikipedia. The evaluation shows that our
model achieves better readability scores
than a set of baseline systems.
1 Introduction
Sentence simplification transforms long and dif-
ficult sentences into shorter and more readable
ones. This helps humans read texts more easily
and faster. Reading assistance is thus an impor-
tant application of sentence simplification, espe-
cially for people with reading disabilities (Carroll
et al, 1999; Inui et al, 2003), low-literacy read-
ers (Watanabe et al, 2009), or non-native speakers
(Siddharthan, 2002).
Not only human readers but also NLP ap-
plications can benefit from sentence simplifica-
tion. The original motivation for sentence sim-
plification is using it as a preprocessor to facili-
tate parsing or translation tasks (Chandrasekar et
al., 1996). Complex sentences are considered as
stumbling blocks for such systems. More recently,
sentence simplification has also been shown help-
ful for summarization (Knight and Marcu, 2000),
? This work has been supported by the Emmy Noether
Program of the German Research Foundation (DFG) under
the grant No. GU 798/3-1, and by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Program under
the grant No. I/82806.
sentence fusion (Filippova and Strube, 2008b), se-
mantic role labeling (Vickrey and Koller, 2008),
question generation (Heilman and Smith, 2009),
paraphrase generation (Zhao et al, 2009) and
biomedical information extraction (Jonnalagadda
and Gonzalez, 2009).
At sentence level, reading difficulty stems ei-
ther from lexical or syntactic complexity. Sen-
tence simplification can therefore be classified
into two types: lexical simplification and syntac-
tic simplification (Carroll et al, 1999). These two
types of simplification can be further implemented
by a set of simplification operations. Splitting,
dropping, reordering, and substitution are widely
accepted as important simplification operations.
The splitting operation splits a long sentence into
several shorter sentences to decrease the complex-
ity of the long sentence. The dropping operation
further removes unimportant parts of a sentence to
make it more concise. The reordering operation
interchanges the order of the split sentences (Sid-
dharthan, 2006) or parts in a sentence (Watanabe
et al, 2009). Finally, the substitution operation re-
places difficult phrases or words with their simpler
synonyms.
In most cases, different simplification opera-
tions happen simultaneously. It is therefore nec-
essary to consider the simplification process as
a combination of different operations and treat
them as a whole. However, most of the ex-
isting models only consider one of these opera-
tions. Siddharthan (2006) and Petersen and Osten-
dorf (2007) focus on sentence splitting, while sen-
tence compression systems (Filippova and Strube,
2008a) mainly use the dropping operation. As far
as lexical simplification is concerned, word sub-
stitution is usually done by selecting simpler syn-
onyms from Wordnet based on word frequency
(Carroll et al, 1999).
In this paper, we propose a sentence simplifica-
tion model by tree transformation which is based
1353
on techniques from statistical machine translation
(SMT) (Yamada and Knight, 2001; Yamada and
Knight, 2002; Graehl et al, 2008). Our model in-
tegrally covers splitting, dropping, reordering and
phrase/word substitution. The parameters of our
model can be efficiently learned from complex-
simple parallel datasets. The transformation from
a complex sentence to a simple sentence is con-
ducted by applying a sequence of simplification
operations. An expectation maximization (EM)
algorithm is used to iteratively train our model.
We also propose a method based on monolingual
word mapping which speeds up the training pro-
cess significantly. Finally, a decoder is designed to
generate the simplified sentences using a greedy
strategy and integrates language models.
In order to train our model, we further com-
pile a large-scale complex-simple parallel dataset
(PWKP) from Simple English Wikipedia1 and En-
glish Wikipedia2, as such datasets are rare.
We organize the remainder of the paper as fol-
lows: Section 2 describes the PWKP dataset. Sec-
tion 3 presents our TSM model. Sections 4 and 5
are devoted to training and decoding, respectively.
Section 6 details the evaluation. The conclusions
follow in the final section.
2 Wikipedia Dataset: PWKP
We collected a paired dataset from the English
Wikipedia and Simple English Wikipedia. The
targeted audience of Simple Wikipedia includes
?children and adults who are learning English lan-
guage?. The authors are requested to ?use easy
words and short sentences? to compose articles.
We processed the dataset as follows:
Article Pairing 65,133 articles from Simple
Wikipedia3 and Wikipedia4 were paired by fol-
lowing the ?language link? using the dump files
in Wikimedia.5 Administration articles were fur-
ther removed.
Plain Text Extraction We use JWPL (Zesch et
al., 2008) to extract plain texts from Wikipedia ar-
ticles by removing specific Wiki tags.
Pre-processing including sentence boundary
detection and tokenization with the Stanford
1http://simple.wikipedia.org
2http://en.wikipedia.org
3As of Aug 17th, 2009
4As of Aug 22nd, 2009
5http://download.wikimedia.org
Parser package (Klein and Manning, 2003),
and lemmatization with the TreeTagger (Schmid,
1994).
Monolingual Sentence Alignment As we need
a parallel dataset algned at the sentence level,
we further applied monolingual sentence align-
ment on the article pairs. In order to achieve
the best sentence alignment on our dataset, we
tested three similarity measures: (i) sentence-level
TF*IDF (Nelken and Shieber, 2006), (ii) word
overlap (Barzilay and Elhadad, 2003) and (iii)
word-based maximum edit distance (MED) (Lev-
enshtein, 1966) with costs of insertion, deletion
and substitution set to 1. To evaluate their perfor-
mance we manually annotated 120 sentence pairs
from the article pairs. Tab. 1 reports the precision
and recall of these three measures. We manually
adjusted the similarity threshold to obtain a recall
value as close as possible to 55.8% which was pre-
viously adopted by Nelken and Shieber (2006).
Similarity Precision Recall
TF*IDF 91.3% 55.4%
Word Overlap 50.5% 55.1%
MED 13.9% 54.7%
Table 1: Monolingual Sentence Alignment
The results in Tab. 1 show that sentence-level
TF*IDF clearly outperforms the other two mea-
sures, which is consistent with the results reported
by Nelken and Shieber (2006). We henceforth
chose sentence-level TF*IDF to align our dataset.
As shown in Tab. 2, PWKP contains more
than 108k sentence pairs. The sentences from
Wikipedia and Simple Wikipedia are considered
as ?complex? and ?simple? respectively. Both the
average sentence length and average token length
in Simple Wikipedia are shorter than those in
Wikipedia, which is in compliance with the pur-
pose of Simple Wikipedia.
Avg. Sen. Len Avg. Tok. Len #Sen.Pairs
complex simple complex simple -
25.01 20.87 5.06 4.89 108,016
Table 2: Statistics for the PWKP dataset
In order to account for sentence splitting, we al-
low 1 to n sentence alignment to map one complex
sentence to several simple sentences. We first per-
form 1 to 1 mapping with sentence-level TF*IDF
and then combine the pairs with the same complex
sentence and adjacent simple sentences.
3 The Simplification Model: TSM
We apply the following simplification operations
to the parse tree of a complex sentence: splitting,
1354
dropping, reordering and substitution. In this sec-
tion, we use a running example to illustrate this
process. c is the complex sentence to be simpli-
fied in our example. Fig. 1 shows the parse tree of
c (we skip the POS level).
c: August was the sixth month in the ancient Ro-
man calendar which started in 735BC.
NP VP
S
August was
NPinsixththe
SBAR
NP
NP PP
WHNP S
VP
started PP
in 735BC
ancient calendar whichthe Roman
month
Figure 1: Parse Tree of c
3.1 Splitting
The first operation is sentence splitting, which we
further decompose into two subtasks: (i) segmen-
tation, which decides where and whether to split
a sentence and (ii) completion, which makes the
new split sentences complete.
First, we decide where we can split a sentence.
In our model, the splitting point is judged by the
syntactic constituent of the split boundary word
in the complex sentence. The decision whether a
sentence should be split is based on the length of
the complex sentence. The features used in the
segmentation step are shown in Tab. 3.
Word Constituent iLength isSplit Prob.
?which? SBAR 1 true 0.0016
?which? SBAR 1 false 0.9984
?which? SBAR 2 true 0.0835
?which? SBAR 2 false 0.9165
Table 3: Segmentation Feature Table (SFT)
Actually, we do not use the direct constituent of
a word in the parse tree. In our example, the direct
constituent of the word ?which? is ?WHNP?. In-
stead, we use Alg. 1 to calculate the constituent
of a word. Alg. 1 returns ?SBAR? as the ad-
justed constituent for ?which?. Moreover, di-
rectly using the length of the complex sentence
is affected by the data sparseness problem. In-
stead, we use iLength as the feature which is
calculated as iLength = ceiling( comLengthavgSimLength),
where comLength is the length of the complex
sentence and avgSimLength is the average length
of simple sentences in the training dataset. The
?Prob.? column shows the probabilities obtained
after training on our dataset.
Algorithm 1 adjustConstituent(word, tree)
constituent? word.father;
father ? constituent.father;
while father 6= NULL AND constituent is the most
left child of father do
constituent? father;
father ? father.father;
end while
return constituent;
In our model, one complex sentence can be split
into two or more sentences. Since many splitting
operations are possible, we need to select the most
likely one. The probability of a segmentation op-
eration is calculated as:
P (seg|c) =
?
w:c
SFT (w|c) (1)
where w is a word in the complex sentence c and
SFT (w|c) is the probability of the word w in the
Segmentation Feature Table (SFT); Fig. 2 shows
a possible segmentation result of our example.
NP VP
S
August was
NPinsixththe
SBAR
NP
NP PP
WHNP S
VP
started PP
in 735BC
ancient calendar
which
the Roman
month
Figure 2: Segmentation
The second step is completion. In this step,
we try to make the split sentences complete and
grammatical. In our example, to make the second
sentence ?which started in 735BC? complete and
grammatical we should first drop the border word
?which? and then copy the dependent NP ?the
ancient Roman calendar? to the left of ?started?
to obtain the complete sentence ?the ancient Ro-
man calendar started in 735BC?. In our model,
whether the border word should be dropped or
retained depends on two features of the border
word: the direct constituent of the word and the
word itself, as shown in Tab. 4.
Const. Word isDropped Prob.
WHNP which True 1.0
WHNP which False Prob.Min
Table 4: Border Drop Feature Table (BDFT)
In order to copy the necessary parts to complete
the new sentences, we must decide which parts
should be copied and where to put these parts in
the new sentences. In our model, this is judged
by two features: the dependency relation and the
constituent. We use the Stanford Parser for pars-
ing the dependencies. In our example, the de-
1355
pendency relation between ?calendar? in the com-
plex sentence and the verb ?started? in the second
split sentence is ?gov nsubj?.6 The direct con-
stituent of ?started? is ?VP? and the word ?calen-
dar? should be put on the ?left? of ?started?, see
Tab. 5.
Dep. Const. isCopied Pos. Prob.
gov nsubj VP(VBD) True left 0.9000
gov nsubj VP(VBD) True right 0.0994
gov nsubj VP(VBD) False - 0.0006
Table 5: Copy Feature Table (CFT)
For dependent NPs, we copy the whole NP
phrase rather than only the head noun.7 In our
example, we copy the whole NP phrase ?the an-
cient Roman calendar? to the new position rather
than only the word ?calendar?. The probability of
a completion operation can be calculated as
P (com|seg) =
Y
bw:s
BDFT (bw|s)
Y
w:s
Y
dep:w
CFT (dep).
where s are the split sentences, bw is a border
word in s, w is a word in s, dep is a dependency
of w which is out of the scope of s. Fig. 3 shows
the most likely result of the completion operation
for our example.
NP VP
pt1
August was
NPinsixththe
NP
NP PPpt2
VP
started PP
in 735BC
ancient calendarthe RomanNP
ancient calendarthe Roman
month
Figure 3: Completion
3.2 Dropping and Reordering
We first apply dropping and then reordering to
each non-terminal node in the parse tree from top
to bottom. We use the same features for both drop-
ping and reordering: the node?s direct constituent
and its children?s constituents pattern, see Tab. 6
and Tab. 7.
Constituent Children Drop Prob.
NP DT JJ NNP NN 1101 7.66E-4
NP DT JJ NNP NN 0001 1.26E-7
Table 6: Dropping Feature Table (DFT)
6With Stanford Parser, ?which? is a referent of ?calender?
and the nsubj of ?started?. ?calender? thus can be considered
to be the nsubj of ?started? with ?started? as the governor.
7The copied NP phrase can be further simplified in the
following steps.
Constituent Children Reorder Prob.
NP DT JJ NN 012 0.8303
NP DT JJ NN 210 0.0039
Table 7: Reordering Feature Table (RFT)
The bits ?1? and ?0? in the ?Drop? column indi-
cate whether the corresponding constituent is re-
tained or dropped. The number in the ?Reorder?
column represents the new order for the children.
The probabilities of the dropping and reordering
operations can be calculated as Equ. 2 and Equ. 3.
P (dp|node) = DFT (node) (2)
P (ro|node) = RFT (node) (3)
In our example, one of the possible results is
dropping the NNP ?Roman?, as shown in Fig. 4.
NP VP
pt1
August was
NPinsixththe
NP
NP PPpt2
VP
started PP
in 735BC
ancient calendartheNP
ancient calendarthe
month
Figure 4: Dropping & Reordering
3.3 Substitution
3.3.1 Word Substitution
Word substitution only happens on the termi-
nal nodes of the parse tree. In our model, the
conditioning features include the original word
and the substitution. The substitution for a word
can be another word or a multi-word expression
(see Tab. 8). The probability of a word substitu-
tion operation can be calculated as P (sub|w) =
SubFT (Substitution|Origin).
Origin Substitution Prob.
ancient ancient 0.963
ancient old 0.0183
ancient than transport 1.83E-102
old ancient 0.005
Table 8: Substitution Feature Table (SubFT)
3.3.2 Phrase Substitution
Phrase substitution happens on the non-
terminal nodes and uses the same conditioning
features as word substitution. The ?Origin? con-
sists of the leaves of the subtree rooted at the
node. When we apply phrase substitution on a
non-terminal node, then any simplification opera-
tion (including dropping, reordering and substitu-
tion) cannot happen on its descendants any more
1356
because when a node has been replaced then its
descendants are no longer existing. Therefore, for
each non-terminal node we must decide whether a
substitution should take place at this node or at its
descendants. We perform substitution for a non-
terminal node if the following constraint is met:
Max(SubFT (?|node)) ?
Y
ch:node
Max(SubFT (?|ch)).
where ch is a child of the node. ??? can
be any substitution in the SubFT. The proba-
bility of the phrase substitution is calculated as
P (sub|node) = SubFT (Substitution|Origin).
Fig. 5 shows one of the possible substitution re-
sults for our example where ?ancient? is replaced
by ?old?.
NP VP
pt1
August was
NPinsixththe
NP
NP PPpt2
VP
started PP
in 735BC
old calendartheNP
old calendarthe
month
Figure 5: Substitution
As a result of all the simplification operations,
we obtain the following two sentences: s1 =
Str(pt1)=?August was the sixth month in the old
calendar.? and s2 = Str(pt2)=?The old calendar
started in 735BC.?
3.4 The Probabilistic Model
Our model can be formalized as a direct transla-
tion model from complex to simple P (s|c) multi-
plied by a language model P (s) as shown in Equ.
4.
s = argmax
s
P (s|c)P (s) (4)
We combine the parts described in the previous
sections to get the direct translation model:
P (s|c) =
?
?:Str(?(c))=s
(P (seg|c)P (com|seg)
(5)
?
node
P (dp|node)P (ro|node)P (sub|node)
?
w
(sub|w)).
where ? is a sequence of simplification operations
and Str(?(c)) corresponds to the leaves of a sim-
plified tree. There can be many sequences of op-
erations that result in the same simplified sentence
and we sum up all of their probabilities.
4 Training
In this section, we describe how we train the prob-
abilities in the tables. Following the work of
Yamada and Knight (2001), we train our model
by maximizing P (s|c) over the training corpus
with the EM algorithm described in Alg. 2, us-
ing a constructed graph structure. We develop the
Training Tree (Fig. 6) to calculate P (s|c). P (s|c)
is equal to the inside probability of the root in the
Training Tree. Alg. 3 and Alg. 4 are used to cal-
culate the inside and outside probabilities. We re-
fer readers to Yamada and Knight (2001) for more
details.
Algorithm 2 EM Training (dataset)
Initialize all probability tables using the uniform distribu-
tion;
for several iterations do
reset al cnt = 0;
for each sentence pair < c, s > in dataset do
tt = buildTrainingTree(< c, s >);
calcInsideProb(tt);
calcOutsideProb(tt);
update cnt for each conditioning feature in each
node of tt: cnt = cnt + node.insideProb ?
node.outsideProb/root.insideProb;
end for
updateProbability();
end for
root
sp
sp_res1 sp_res2
dp
ro
mp
mp_res1 mp_res2
sub
mp
mp_res
subsub
dp
ro
mp_res
root
sp
sp_res sp_res
dp
ro
ro_res ro_res
sub
ro_res
subsub
dp
ro
ro_res
sub_res
sub_res sub_res
Figure 6: Training Tree (Left) and Decoding Tree
(Right)
We illustrate the construction of the training
tree with our running example. There are two
kinds of nodes in the training tree: data nodes in
rectangles and operation nodes in circles. Data
nodes contain data and operation nodes execute
operations. The training is a supervised learning
1357
process with the parse tree of c as input and the
two strings s1 and s2 as the desired output. root
stores the parse tree of c and also s1 and s2. sp,
ro, mp and sub are splitting, reordering, mapping
and substitution operations. sp res and mp res
store the results of sp and mp. In our example,
sp splits the parse tree into two parse trees pt1
and pt2 (Fig. 3). sp res1 contains pt1 and s1.
sp res2 contains pt2 and s2. Then dp, ro and mp
are iteratively applied to each non-terminal node
at each level of pt1 and pt2 from top to down.
This process continues until the terminal nodes
are reached or is stopped by a sub node. The func-
tion of mp operation is similar to the word map-
ping operation in the string-based machine trans-
lation. It maps substrings in the complex sentence
which are dominated by the children of the current
node to proper substrings in the simple sentences.
Speeding Up The example above is only one
of the possible paths. We try all of the promis-
ing paths in training. Promising paths are the
paths which are likely to succeed in transform-
ing the parse tree of c into s1 and s2. We select
the promising candidates using monolingual word
mapping as shown in Fig. 7. In this example,
only the word ?which? can be a promising can-
didate for splitting. We can select the promising
candidates for the dropping, reordering and map-
ping operations similarly. With this improvement,
we can train on the PWKP dataset within 1 hour
excluding the parsing time taken by the Stanford
Parser.
We initialize the probabilities with the uniform
distribution. The binary features, such as SFT and
BDFT, are assigned the initial value of 0.5. For
DFT and RFT, the initial probability is 1N! , where
N is the number of the children. CFT is initial-
ized as 0.25. SubFT is initialized as 1.0 for any
substitution at the first iteration. After each itera-
tion, the updateProbability function recalculates
these probabilities based on the cnt for each fea-
ture.
Algorithm 3 calcInsideProb (TrainingTree tt)
for each node from level = N to root of tt do
if node is a sub node then
node.insideProb = P (sub|node);
else if node is a mp OR sp node then
node.insideProb =Qchild child.insideProb;else
node.insideProb =Pchild child.insideProb;end if
end for
Algorithm 4 calcOutsideProb (TrainingTree tt)
for each node from root to level = N of tt do
if node is the root then
node.outsideProb = 1.0;
else if node is a sp res OR mp res node then
{COMMENT: father are the fathers of the current
node, sibling are the children of father excluding
the current node}
node.outsideProb =
P
father
father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then
node.outsideProb = father.outsideProb ? 1.0;
else if node is a sp, ro, dp or sub node then
node.outsideProb = father.outsideProb ?
P (sp or ro or dp or sub|node);
end if
end for
August was the sixth in the ancient Roman calendar statedwhich in 735BC
August was the sixth in the old Roman calendar stated in 735BCThe old calendar.
.
.
Complex sentence
Simple sentences
month
month
Figure 7: Monolingual Word Mapping
5 Decoding
For decoding, we construct the decoding tree
(Fig. 6) similarly to the construction of the train-
ing tree. The decoding tree does not have mp op-
erations and there can be more than one sub nodes
attached to a single ro res. The root contains the
parse tree of the complex sentence. Due to space
limitations, we cannot provide all the details of the
decoder.
We calculate the inside probability and out-
side probability for each node in the decoding
tree. When we simplify a complex sentence, we
start from the root and greedily select the branch
with the highest outside probability. For the sub-
stitution operation, we also integrate a trigram
language model to make the generated sentences
more fluent. We train the language model with
SRILM (Stolcke, 2002). All the articles from the
Simple Wikipedia are used as the training corpus,
amounting to about 54 MB.
6 Evaluation
Our evaluation dataset consists of 100 complex
sentences and 131 parallel simple sentences from
PWKP. They have not been used for training.
Four baseline systems are compared in our eval-
uation. The first is Moses which is a state of
the art SMT system widely used as a baseline in
MT community. Obviously, the purpose of Moses
is cross-lingual translation rather than monolin-
1358
gual simplification. The goal of our comparison
is therefore to assess how well a standard SMT
system may perform simplification when fed with
a proper training dataset. We train Moses with the
same part of PWKP as our model. The second
baseline system is a sentence compression sys-
tem (Filippova and Strube, 2008a) whose demo
system is available online.8 As the compression
system can only perform dropping, we further ex-
tend it to our third and fourth baseline systems,
in order to make a reasonable comparison. In our
third baseline system, we substitute the words in
the output of the compression system with their
simpler synonyms. This is done by looking up
the synonyms in Wordnet and selecting the most
frequent synonym for replacement. The word fre-
quency is counted using the articles from Simple
Wikipedia. The fourth system performs sentence
splitting on the output of the third system. This
is simply done by splitting the sentences at ?and?,
?or?, ?but?, ?which?, ?who? and ?that?, and dis-
carding the border words. In total, there are 5
systems in our evaluation: Moses, the MT sys-
tem; C, the compression system; CS, the com-
pression+substitution system; CSS, the compres-
sion+substitution+split system; TSM, our model.
We also provide evaluation measures for the sen-
tences in the evaluation dataset: CW: complex
sentences from Normal Wikipedia and SW: par-
allel simple sentences from Simple Wikipedia.
6.1 Basic Statistics and Examples
The first three columns in Tab. 9 present the ba-
sic statistics for the evaluation sentences and the
output of the five systems. tokenLen is the aver-
age length of tokens which may roughly reflect the
lexical difficulty. TSM achieves an average token
length which is the same as the Simple Wikipedia
(SW). senLen is the average number of tokens in
one sentence, which may roughly reflect the syn-
tactic complexity. Both TSM and CSS produce
shorter sentences than SW. Moses is very close to
CW. #sen gives the number of sentences. Moses,
C and CS cannot split sentences and thus produce
about the same number of sentences as available
in CW.
Here are two example results obtained with our
TSM system.
Example 1. CW: ?Genetic engineering has ex-
panded the genes available to breeders to utilize
in creating desired germlines for new crops.? SW:
8http://212.126.215.106/compression/
?New plants were created with genetic engineer-
ing.? TSM: ?Engineering has expanded the genes
available to breeders to use in making germlines
for new crops.?
Example 2. CW: ?An umbrella term is a word that
provides a superset or grouping of related con-
cepts, also called a hypernym.? SW: ?An umbrella
term is a word that provides a superset or group-
ing of related concepts.? TSM: ?An umbrella term
is a word. A word provides a superset of related
concepts, called a hypernym.?
In the first example, both substitution and drop-
ping happen. TSM replaces ?utilize? and ?cre-
ating? with ?use? and ?making?. ?Genetic? is
dropped. In the second example, the complex sen-
tence is split and ?also? is dropped.
6.2 Translation Assessment
In this part of the evaluation, we use traditional
measures used for evaluating MT systems. Tab. 9
shows the BLEU and NIST scores. We use
?mteval-v11b.pl?9 as the evaluation tool. CW
and SW are used respectively as source and ref-
erence sentences. TSM obtains a very high BLEU
score (0.38) but not as high as Moses (0.55).
However, the original complex sentences (CW)
from Normal Wikipedia get a rather high BLEU
(0.50), when compared to the simple sentences.
We also find that most of the sentences generated
by Moses are exactly the same as those in CW:
this shows that Moses only performs few modi-
fications to the original complex sentences. This
is confirmed by MT evaluation measures: if we
set CW as both source and reference, the BLEU
score obtained by Moses is 0.78. TSM gets 0.55
in the same setting which is significantly smaller
than Moses and demonstrates that TSM is able to
generate simplifications with a greater amount of
variation from the original sentence. As shown in
the ?#Same? column of Tab. 9, 25 sentences gen-
erated by Moses are exactly identical to the com-
plex sentences, while the number for TSM is 2
which is closer to SW. It is however not clear how
well BLEU and NIST discriminate simplification
systems. As discussed in Jurafsky and Martin
(2008), ?BLEU does poorly at comparing systems
with radically different architectures and is most
appropriate when evaluating incremental changes
with similar architectures.? In our case, TSM and
CSS can be considered as having similar architec-
tures as both of them can do splitting, dropping
9http://www.statmt.org/moses/
1359
TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL
CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384
SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179
Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363
C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481
CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616
CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581
TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353
Table 9: Evaluation
and substitution. But Moses mostly cannot split
and drop. We may conclude that TSM and Moses
have different architectures and BLEU or NIST is
not suitable for comparing them. Here is an exam-
ple to illustrate this: (CW): ?Almost as soon as he
leaves, Annius and the guard Publius arrive to es-
cort Vitellia to Titus, who has now chosen her as
his empress.? (SW): ?Almost as soon as he leaves,
Annius and the guard Publius arrive to take Vitel-
lia to Titus, who has now chosen her as his em-
press.? (Moses): The same as (SW). (TSM): ?An-
nius and the guard Publius arrive to take Vitellia
to Titus. Titus has now chosen her as his empress.?
In this example, Moses generates an exactly iden-
tical sentence to SW, thus the BLUE and NIST
scores of Moses is the highest. TSM simplifies
the complex sentence by dropping, splitting and
substitution, which results in two sentences that
are quite different from the SW sentence and thus
gets lower BLUE and NIST scores. Nevertheless,
the sentences generated by TSM seem better than
Moses in terms of simplification.
6.3 Readability Assessment
Intuitively, readability scores should be suitable
metrics for simplification systems. We use the
Linux ?style? command to calculate the Flesch
and Lix readability scores. The results are pre-
sented in Tab. 9. ?PE? in the Flesch column stands
for ?Plain English? and the ?Grade? in Lix repre-
sents the school year. TSM achieves significantly
better scores than Moses which has the best BLEU
score. This implies that good monolingual trans-
lation is not necessarily good simplification. OOV
is the percentage of words that are not in the Ba-
sic English BE850 list.10 TSM is ranked as the
second best system for this criterion.
The perplexity (PPL) is a score of text proba-
bility measured by a language model and normal-
ized by the number of words in the text (Equ. 6).
10http://simple.wikipedia.org/wiki/
Wikipedia:Basic_English_alphabetical_
wordlist
PPL can be used to measure how tight the lan-
guage model fits the text. Language models con-
stitute an important feature for assessing readabil-
ity (Schwarm and Ostendorf, 2005). We train a
trigram LM using the simple sentences in PWKP
and calculate the PPL with SRILM. TSM gets the
best PPL score. From this table, we can conclude
that TSM achieves better overall readability than
the baseline systems.
PPL(text) = P (w1w2...wN )?
1
N (6)
There are still some important issues to be con-
sidered in future. Based on our observations, the
current model performs well for word substitution
and segmentation. But the completion of the new
sentences is still problematic. For example, we
copy the dependent NP to the new sentences. This
may break the coherence between sentences. A
better solution would be to use a pronoun to re-
place the NP. Sometimes, excessive droppings oc-
cur, e.g., ?older? and ?twin? are dropped in ?She
has an older brother and a twin brother...?. This
results in a problematic sentence: ?She has an
brother and a brother...?. There are also some er-
rors which stem from the dependency parser. In
Example 2, ?An umbrella term? should be a de-
pendency of ?called?. But the parser returns ?su-
perset? as the dependency. In the future, we will
investigate more sophisticated features and rules
to enhance TSM.
7 Conclusions
In this paper, we presented a novel large-scale par-
allel dataset PWKP for sentence simplification.
We proposed TSM, a tree-based translation model
for sentence simplification which covers splitting,
dropping, reordering and word/phrase substitution
integrally for the first time. We also described an
efficient training method with speeding up tech-
niques for TSM. The evaluation shows that TSM
can achieve better overall readability scores than
a set of baseline systems.
1360
References
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Process-
ing, pages 25?32.
Carroll, John, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL?99), pages 269?270.
Chandrasekar, R., Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simpli-
fication. In Proceedings of the Sixteenth Inter-
national Conference on Computational Linguistics
(COLING?96), pages 1041?1044.
Filippova, Katja and Michael Strube. 2008a. Depen-
dency tree based sentence compression. In Inter-
national Natural Language Generation Conference
(INLG?08), pages 25?32.
Filippova, Katja and Michael Strube. 2008b. Sen-
tence fusion via dependency graph compression. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 177?185.
Graehl, Jonathan, Kevin Knight, and Jonathan May.
2008. Training tree transducers. In Computational
Linguistics, volume 34, pages 391?427. MIT Press.
Heilman, M. and N. A. Smith. 2009. Question gener-
ation via overgenerating transformations and rank-
ing. Technical Report CMU-LTI-09-013, Language
Technologies Institute, Carnegie Mellon University.
Inui, Kentaro, Atsushi Fujita, Tetsuro Takahashi, Ryu
Iida, and Tomoya Iwakura. 2003. Text simplifi-
cation for reading assistance: A project note. In
Proceedings of the 2nd International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP), pages 9?16.
Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine.
Jurafsky, Daniel and James H. Martin. 2008. Speech
and Language Processing (2nd Edition). Prentice
Hall, 2 edition.
Klein, Dan and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NISP?02), pages 3?10.
Knight, Kevin and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI, pages 703?710.
Levenshtein. 1966. Binary code capable of correct-
ing deletions, insertions and reversals. In Soviet
Physics, pages 707?710.
Nelken, Rani and Stuart M. Shieber. 2006. To-
wards robust context-sensitive sentence alignment
for monolingual corpora. In Proceedings of 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 161?168.
Petersen, Sarah E. and Mari Ostendorf. 2007. Text
simplification for language learners: a corpus anal-
ysis. In Proc. of Workshop on Speech and Language
Technology for Education, pages 69?72.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49.
Schwarm, Sarah E. and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In ACL?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 523?530.
Siddharthan, Advaith. 2002. An architecture for a
text simplification system. In Proceedings of the
Language Engineering Conference (LEC?02), pages
64?71.
Siddharthan, Advaith. 2006. Syntactic simplifica-
tion and text cohesion. In Research on Language
& Computation, volume 4, pages 77?109. Springer
Netherlands, June.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. pages 901?904.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, June.
Watanabe, Willian Massami, Arnaldo Candido Junior,
Vin??cius Rodriguez Uze?da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra Maria Alu??sio. 2009. Facilita: reading as-
sistance for low-literacy readers. In SIGDOC ?09:
Proceedings of the 27th ACM international confer-
ence on Design of communication, pages 29?36.
ACM.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL?01: Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, pages 523?530.
Yamada, Kenji and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In ACL?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 303?310.
Zesch, Torsten, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings
of the Sixth International Language Resources and
Evaluation (LREC?08), pages 1646?1652.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li.
2009. Application-driven statistical paraphrase gen-
eration. In Proceedings of ACL-IJCNLP, pages
834?842, Suntec, Singapore, August.
1361
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 384?389, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UT-DB: An Experimental Study on Sentiment Analysis in Twitter
Zhemin Zhu Djoerd Hiemstra Peter Apers Andreas Wombacher
CTIT Database Group, University of Twente
Drienerlolaan 5, 7500 AE, Enschede, The Netherlands
{z.zhu, d.hiemstra, p.m.g.apers, A.Wombacher}@utwente.nl
Abstract
This paper describes our system for participat-
ing SemEval2013 Task2-B (Kozareva et al,
2013): Sentiment Analysis in Twitter. Given
a message, our system classifies whether the
message is positive, negative or neutral senti-
ment. It uses a co-occurrence rate model. The
training data are constrained to the data pro-
vided by the task organizers (No other tweet
data are used). We consider 9 types of fea-
tures and use a subset of them in our submitted
system. To see the contribution of each type of
features, we do experimental study on features
by leaving one type of features out each time.
Results suggest that unigrams are the most im-
portant features, bigrams and POS tags seem
not helpful, and stopwords should be retained
to achieve the best results. The overall results
of our system are promising regarding the con-
strained features and data we use.
1 Introduction
The past years have witnessed the emergence and
popularity of short messages such as tweets and
SMS messages. Comparing with the traditional gen-
res such as newswire data, tweets are very short and
use informal grammar and expressions. The short-
ness and informality make them a new genre and
bring new challenges to sentiment analysis (Pang et
al., 2002) as well as other NLP applications such
named entity recognition (Habib et al, 2013).
Recently a wide range of methods and features
have been applied to sentimental analysis over
tweets. Go et al (2009) train sentiment classi-
fiers using machine learning methods, such as Naive
Bayes, Maximum Entropy and SVMs, with different
combinations of features such as unigrams, bigrams
and Part-of-Speech (POS) tags. Microblogging fea-
tures such as hashtags, emoticons, abbreviations, all-
caps and character repetitions are also found help-
ful (Kouloumpis et al, 2011). Saif et al (2012)
train Naive Bayes models with semantic features.
Also the lexicon prior polarities have been proved
very useful (Agarwal et al, 2011). Davidov et al
(2010) utilize hashtags and smileys to build a large-
scale annotated tweet dataset automatically. This
avoids the need for labour intensive manual anno-
tation. Due to the fact that tweets are generated con-
stantly, sentiment analysis over tweets has some in-
teresting applications, such as predicting stock mar-
ket movement (Bollen et al, 2011) and predicting
election results (Tumasjan et al, 2010; O?Connor et
al., 2010).
But there are still some unclear parts in the lit-
erature. For example, it is unclear whether using
POS tags improves the sentiment analysis perfor-
mance or not. Conflicting results are reported (Pak
and Paroubek, 2010; Go et al, 2009). It is also
a little surprising that not removing stopwords in-
creases performance (Saif et al, 2012). In this pa-
per, we build a system based on the concept of co-
occurrence rate. 9 different types of features are con-
sidered. We find that using a subset of these features
achieves the best results in our system, so we use
this subset of features rather than all the 9 types of
features in our submitted system. To see the contri-
bution of each type of features, we perform experi-
ments by leaving one type of features out each time.
Results show that unigrams are the most important
384
features, bigrams and POS tags seem not helpful,
and retaining stopwords makes the results better.
The overall results of our system are also promis-
ing regarding the constrained features and data we
use.
2 System Description
2.1 Method
We use a supervised method which is similar to the
Naive Bayes classifier. The score of a tweet, denoted
by t, and a sentiment category, denoted by c, is cal-
culated according to the following formula:
Score(t, c) = [
n?
i=1
logCR(fi, c)] + logP (c),
where fi is a feature extracted from t. The sentiment
category c can be positive, negative or neutral. And
CR(fi, c) is Co-occurrence Rate (CR) of fi and c
which can be obtained as follows:
CR(f, c) =
P (fi, c)
P (fi)P (c)
?
#(fi, c)
#(fi)#(c)
,
where #(?) is the number of times that the pattern
? appears in the training dataset. Then the category
of the highest score arg maxc Score(t, c) is the pre-
diction.
This method assumes all the features are inde-
pendent which is also the assumption of the Naive
Bayes model. But our model excludes P (fi) be-
cause they are observations. Hence comparing with
Naive Bayes, our model saves the effort to model
feature distributions P (fi). Also this method can
be trained efficiently because it only depends on the
empirical distributions.
2.2 Features
To make our system general, we constrain to the text
features. That is we do not use the features outside
the tweet texts such as features related to the user
profiles, discourse information or links. The follow-
ing 9 types of features are considered:
1. Unigrams. We use lemmas as the form of un-
igrams. The lemmas are obtained by the Stan-
ford CoreNLP1 (Toutanova et al, 2003). Hash-
1http://nlp.stanford.edu/software/corenlp.shtml
tags and emoticons are also considered as un-
igrams. Some of the unigrams are stopwords
which will be discussed in the next section.
2. Bigrams. We consider two adjacent lemmas as
bigrams.
3. Named entities. We use the CMU Twitter Tag-
ger (Gimpel et al, 2011; Owoputi et al, 2013)2
to recognize named entities. The tokens cov-
ered by a named entity are not considered as
unigrams any more. Instead a named entity as
a whole is treated as a single feature.
4. Dependency relations. Dependency relations
are helpful to the sentiment prediction. Here we
give an example to explain this type of features.
In the tweet ?I may not be able to vote from
Britain but I COMPLETLEY support you!!!!? ,
the dependency relation between the word ?not?
and ?able? is ?NEG? which stands for nega-
tion, and the dependency relation between the
word ?COMPLETELY? and ?support? is ?ADV-
MOD? which means adverb modifier. For this
example, we add ?NEG able? and ?completely
support? as dependency features to our system.
We use Stanford CoreNLP (Klein and Man-
ning, 2003a; Klein and Manning, 2003b) to ob-
tain dependencies. And we only consider two
types of dependencies ?NEG? and ?ADVMOD?.
Other dependency relations are not helpful.
5. Lexicon prior polarity. The prior polarity of
lexicons have been proved very useful to sen-
timent analysis. Many lexicon resources have
been developed. But for a single lexicon re-
source, the coverage is limited. To achieve
better coverage, we merge three lexicon re-
sources. The first one is SentiStrength3 (Ku-
cuktunc et al, 2012). SentiStrength provides a
fine-granularity system for grading lexicon po-
larity which ranges from ?5 (most negative) to
+5 (most positive). Our grading system con-
sists of three categories: negative, neutral and
positive. So we map the words ranging from
?5 to ?1 in SentiStrength to negative in our
grading system, and the words ranging from
2http://www.ark.cs.cmu.edu/TweetNLP/
3http://sentistrength.wlv.ac.uk/
385
+1 to +5 to positive. The rest are mapped
to neutral. We do the same for the other two
lexicon resources: OpinionFinder4 (Wiebe et
al., 2005) and SentiWordNet5 (Esuli and Sebas-
tiani, 2006; Baccianella and Sebastiani, 2010).
6. Intensifiers. The tweets containing intensifiers
are more likely to be non-neutral. In the sub-
mitted system, we merge the boosters in Sen-
tiStrength and the intensifiers in OpinionFinder
to form a list of intensifiers. Some of these in-
tensifiers strengthen emotion (e.g. ?definitely?),
but others weaken emotion (e.g. ?slightly?).
They are distinguished and assigned with dif-
ferent labels {intensifier strengthen,
intensifier weaken}.
7. All-caps and repeat characters. All-caps6 and
repeat characters are common expressions in
tweets to make emphasis on the applied tokens.
They can be considered as implicit intensifiers.
In our system, we first normalize the repeat
characters. For example, happyyyy is nor-
malized to happy as there are ? 3 consequent
y. Then they are treated in the same way as
intensifier features discussed above.
8. Interrogative sentence. Interrogative sentences
are more likely to be neutral. So we add if a
tweet includes interrogative sentences as a fea-
ture to our system. The sentences ending with
a question mark ??? are considered as inter-
rogative sentences. We first use the Stanford
CoreNLP to find the sentence boundaries in a
tweet, then check the ending mark of each sen-
tence.
9. Imperative sentence. Intuitively, imperative
sentences are more likely to be negative. So
if a tweet contains imperative sentences can be
a feature. We consider the sentences start with
a verb as imperative sentences. The verbs are
identified by the CMU Twitter Tagger.
We further filter out the low-frequency features
which have been observed less than 3 times in the
4https://code.google.com/p/opinionfinder/
5http://sentiwordnet.isti.cnr.it/
6All characters of a token are in upper case.
training data. Because these features are not stable
indicators of sentiment. Our experiments show that
removing these low-frequency features increases the
accuracy.
2.3 Pre-processing
The pre-processing of our system includes two steps.
In the first step, we replace the abbreviations as de-
scribed in Section 2.3.1. In the second step, we use
the CMU Twitter Tagger to extract the features of
emoticons (e.g. :)), hashtags (e.g. #Friday), re-
ciepts (e.g. @Peter) and URLs, and remove these
symbols from tweet texts for further processing.
2.3.1 Replacing Abbreviations
Abbreviations are replaced by their original ex-
pressions. We use the Internet Lingo Dictionary
(Wasden, 2010) to obtain the original expressions
of abbreviations. This dictionary originally contains
748 acronyms. But we do not use the acronyms in
which all characters are digits. Because we find they
are more likely to be numbers than acronyms. This
results in 735 acronyms.
3 Experiments
Our system is implemented in Java and organized
as a pipeline consisting of a sequence of annotators
and extractors. This architecture is very similar to
the framework of UIMA (Ferrucci and Lally, 2004).
With such an architecture, we can easily vary the
configurations of our system.
3.1 Datasets
We use the standard dataset provided by Se-
mEval2013 Task2-B (Kozareva et al, 2013) for
training and testing. The training and develop-
ment data provided are merged together to train our
model. Originally, the training and development
data contain 9,684 and 1,654 instances, respectively.
But due to the policy of Twitter, only the tweet IDs
can be released publicly. So we need to fetch the ac-
tual tweets by their IDs. Some of the tweets are no
longer existing after they were downloaded for an-
notation. So the number of tweets used for training
is less than the original tweets provided by the orga-
nizers. In our case, we obtained 10,370 tweets for
training our model.
386
Class Precision Recall F-Score
Positive 74.86 60.05 66.64
Negative 47.80 59.73 53.11
Neutral 67.02 73.60 70.15
Avg (Pos & Neg) 61.33 59.89 59.87
Table 1: Submitted System on Twitter Data
Class Precision Recall F-Score
Positive 54.81 57.93 56.32
Negative 37.87 67.77 48.59
Neutral 80.78 58.11 67.60
Avg (Pos & Neg) 46.34 62.85 52.46
Table 2: Submitted System on SMS Data
There are two test datasets: Twitter and SMS. The
first dataset consists of 3,813 twitter messages and
the second dataset contains 2,094 SMS messages.
The purpose of having a separate test set of SMS
messages is to see how well systems trained on twit-
ter data will generalize to other types of data.
3.2 Results of Our Submitted System
We use a subset of features described in Section 2.2
in our submitted system: unigrams, named entities,
dependency relations, lexicon prior polarity, inten-
sifiers, all-caps and repeat characters, interrogative
and imperative sentences. The official results on the
two datasets are given in Table (1, 2). Our system is
ranked as #14/51 on the Twitter dataset and #18/44
on the SMS dataset.
3.3 Feature Contribution Analysis
To see the contribution of each type of features, we
vary the configuration of our system by leaving one
type of features out each time. The results are listed
in Table 3.
In Table 3, ?Y(T)? means the corresponding fea-
ture is used and the test dataset is the Twitter Data,
and ?N(sms)? means the corresponding feature is left
out and the test dataset is SMS Data.
From Table 3, we can see that unigrams are the
most important features. Leaving out unigrams
leads to a radical decrease of F-scores. On the Twit-
ter dataset, the F-score drops from 59.87 to 41.44,
and on the SMS dataset, the F-score drops from
52.64 to 35.09. And also filtering out the low-
Feature Y(T) N(T) Y(sms) N(sms)
Stopword 59.87 58.19 52.64 51.00
POS Tag 58.68 59.87 51.87 52.64
Bigram 58.47 59.87 51.94 52.64
Unigram 59.87 41.22 52.64 35.09
3 ? 59.87 57.66 52.64 51.20
Intensifier 59.87 59.47 52.64 52.39
Lexicon 59.87 58.33 52.64 51.26
Named Ent. 59.87 59.71 52.64 51.80
Interrogative 59.87 59.67 52.64 52.93
Imperative 59.87 59.54 52.64 52.14
Dependence 59.87 59.37 52.64 52.08
Table 3: Avg (Pos & Neg) of Leave-one-out Experiments
frequency features which happens less than 3 times
increases the F-scores on Twitter data from 57.66 to
59.87, and on SMS data from 51.20 to 52.64. Re-
moving stopwords decreases the scores by 1.66 per-
cent. This result is consistent with that reported by
Saif et al (2012). By taking a close look at the
stopwords we use, we find that some of the stop-
words are highly related to the sentiment polarity,
such as ?can?, ?no?, ?very? and ?want?, but others
are not, such as ?the?, ?him? and ?on?. Removing
the stopwords which are related to the sentiment
is obviously harmful. This means the stopwords
which originally developed for the purpose of in-
formation retrieval are not suitable for sentimental
analysis. Dependency relations are also helpful fea-
tures which increase F-scores by about 0.5 percent.
The POS tags and bigrams seem not helpful in our
experiments, which is consistent with the results re-
ported by (Kouloumpis et al, 2011).
4 Conclusions
We described the method and features used in our
system. We also did analysis on feautre contribu-
tion. Experiment results suggest that unigrams are
the most important features, POS tags and bigrams
seem not helpful, filtering out the low-frequency fea-
tures is helpful and retaining stopwords makes the
results better.
Acknowledgements
This work has been supported by the Dutch national
program COMMIT.
387
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Language
Resources Association (ELRA).
J. Bollen, H. Mao, and X. Zeng. 2011. Twitter mood
predicts the stock market. Journal of Computational
Science.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
David Ferrucci and Adam Lally. 2004. Uima: an archi-
tectural approach to unstructured information process-
ing in the corporate research environment. Nat. Lang.
Eng., 10(3-4):327?348, September.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 42?47, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
M. B. Habib, M. van Keulen, and Z. Zhu. 2013. Con-
cept extraction challenge: University of twente at
#msm2013. In Proceedings of the 3rd workshop on
?Making Sense of Microposts? (#MSM2013), Rio de
Janeiro, Brazil, Brazil, May. CEUR.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems, volume 15. MIT Press.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, ICWSM. The
AAAI Press.
Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara
Rosenthal, Veselin Stoyonov, and Theresa Wilson.
2013. Sentiment analysis in twitter. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation. Association for Computation Linguistics.
Onur Kucuktunc, B. Barla Cambazoglu, Ingmar Weber,
and Hakan Ferhatosmanoglu. 2012. A large-scale
sentiment analysis for yahoo! answers. In Proceed-
ings of the fifth ACM international conference on Web
search and data mining, WSDM ?12, pages 633?642,
New York, NY, USA. ACM.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM 2010, Washington, DC, USA, May
23-26, 2010.
Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Hello, who is calling?: Can words reveal the social
nature of conversations? In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, June.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing - Volume 10, EMNLP ?02, pages 79?
388
86, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hassan Saif, Yulan He, and Harith Alani. 2012. Seman-
tic sentiment analysis of twitter. In Proceedings of the
11th international conference on The Semantic Web -
Volume Part I, ISWC?12, pages 508?524, Berlin, Hei-
delberg. Springer-Verlag.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
A. Tumasjan, T.O. Sprenger, P.G. Sandner, and I.M.
Welpe. 2010. Predicting elections with twitter: What
140 characters reveal about political sentiment. In
Proceedings of the Fourth International AAAI Confer-
ence on Weblogs and Social Media, pages 178?185.
Lawrence Wasden. 2010. Internet lingo dictionary: A
parents guide to codes used in chat rooms, instant mes-
saging, text messaging, and blogs. Technical report,
Attorney General.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
1(2):0.
389
