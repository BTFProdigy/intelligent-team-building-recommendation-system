Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1031?1040,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Mining and Modeling Relations between
Formal and Informal Chinese Phrases from Web Corpora
Zhifei Li and David Yarowsky
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and yarowsky@cs.jhu.edu
Abstract
We present a novel method for discovering
and modeling the relationship between in-
formal Chinese expressions (including collo-
quialisms and instant-messaging slang) and
their formal equivalents. Specifically, we pro-
posed a bootstrapping procedure to identify
a list of candidate informal phrases in web
corpora. Given an informal phrase, we re-
trieve contextual instances from the web us-
ing a search engine, generate hypotheses of
formal equivalents via this data, and rank
the hypotheses using a conditional log-linear
model. In the log-linear model, we incorpo-
rate as feature functions both rule-based intu-
itions and data co-occurrence phenomena (ei-
ther as an explicit or indirect definition, or
through formal/informal usages occurring in
free variation in a discourse). We test our
system on manually collected test examples,
and find that the (formal-informal) relation-
ship discovery and extraction process using
our method achieves an average 1-best preci-
sion of 62%. Given the ubiquity of informal
conversational style on the internet, this work
has clear applications for text normalization
in text-processing systems including machine
translation aspiring to broad coverage.
1 Introduction
Informal text (e.g., newsgroups, online chat, blogs,
etc.) is the majority of all text appearing on the Inter-
net. Informal text tends to have very different style
from formal text (e.g., newswire, magazine, etc.).
In particular, they are different in vocabulary, syn-
tactic structure, semantic interpretation, discourse
Formal Informal
?? (BaiBai)[bye-bye] 88 (BaBa)
?? (XiHuan)[like] ?, (XiFan)[gruel]
?? (GeGe)[elder brother] GG
?? (GeMi)[fans] N? (FenSi)[a food]
Table 1: Example Chinese Formal-informal Relations.
The PinYin pronunciation is in parentheses and an op-
tional literal gloss is in brackets.
structure, and so on. On the other hand, certain re-
lations exist between the informal and formal text,
and informal text often has a viable formal equiva-
lent. Table 1 shows several naturally occurring ex-
amples of informal expressions in Chinese, and Ta-
ble 2 provides a more detailed inventory and charac-
terization of this phenomena1. The first example of
informal phrase ?88? is used very often in Chinese
on-line chat when a person wants to say ?bye-bye?
to the other person. This can be explained as fol-
lows. In Chinese, the standard equivalent to ?bye-
bye? is ???? whose PinYin is ?BaiBai?. Coin-
cidentally, the PinYin of ?88? is ?BaBa?. Because
?BaBa? and ?BaiBai? are near homophones, people
often use ?88? to represent ????, either for input
convenience or just for fun. The other relations in
Table 1 are formed due to similar processes as will
be described later.
Due to the often substantial divergence between
1For clarity, we represent Chinese words in the format: Chi-
nese characters (optional PinYin equivalent in parentheses and
optional English gloss in brackets).
1031
informal and formal text, a text-processing system
trained on formal text does not typically work well
on informal genres. For example, in a machine
translation system (Koehn et al, 2007), if the bilin-
gual training data does not contain the word ??
,? (the second example in Table 1), it leaves the
word untranslated. On the other hand, if the word
??,? does appear in the training data but it has
only a translation ?gruel? as that is the meaning in
the formal text, the translation system may wrongly
translate ??,? into ?gruel? for the informal text
where the word ??,? is more likely to mean
?like?. Therefore, as a text-normalization step, it
is desirable to transform the informal text into its
standard formal equivalent before feeding it into a
general-purpose text-processing system. Unfortu-
nately, there are many processes for generating in-
formal expressions in common use today. Such
transformations are highly flexible/diverse, and new
phrases are invented on the Internet every day due to
major news events, popular movies, TV shows, ra-
dio talks, political activities, and so on. Therefore,
it is of great interest to have a data-driven method
that can automatically find the relations between in-
formal and formal expressions.
In this paper, we present a novel method for dis-
covering and modeling the relationship between in-
formal Chinese expressions found in web corpora
and their formal equivalents. Specifically, we im-
plement a bootstrapping procedure to identify a
list of candidate informal phrases. Given an indi-
vidual informal phrase, we retrieve contextual in-
stances from the web using a search engine (in this
case, www.baidu.com), generate hypotheses of for-
mal equivalents via this data, and rank the hypothe-
ses using a conditional log-linear model. In the log-
linear model, we incorporate as feature functions
both rule-based intuitions and data co-occurrence
phenomena (either as an explicit or indirect defini-
tion, or through formal/informal usages occurring in
free variation in a discourse). We test our system on
manually collected test examples2, and find that the
(formal-informal) relationship discovery and extrac-
tion process using our method achieves an average
precision of more than 60%. This work has applica-
2The training and test examples are freely available at
http://www.cs.jhu.edu/?zfli.
tions for text normalization in many general-purpose
text-processing tasks, e.g., machine translation.
To the best of our knowledge, our work is the
first published machine-learning approach to pro-
ductively model the broad types of relationships be-
tween informal and formal expressions in Chinese
using web corpora.
2 Formal to Informal: Phenomena and
Examples
In this section, we describe the phenomena and pro-
vide examples of the relations between formal and
informal expressions in Chinese (we refer to the
relation as formal-informal phrases hereafter, even
in the case of single-word expressions). We man-
ually collected 908 formal-informal relations, and
classified these relations into four categories. We
collected these pairs by investigating multiple web-
pages where the formal-informal relations are man-
ually compiled, and then merged these seed relations
and removed duplicates. In this way, the 908 exam-
ples should give good coverage on the typical cat-
egories in the formal-informal relations. Also, the
distribution of the categories found in the 908 exam-
ples should be representative of the actual distribu-
tion of the formal-informal relations occurring in the
real text. Table 2 presents these categories and ex-
amples in each category. In the last column, the table
also shows the relative frequency of each category,
computed based on the 908 examples. Recall that
we represent Chinese words in the format: Chinese
characters (optional PinYin equivalent in parenthe-
ses and optional English gloss in brackets).
2.1 Homophone
In general, a homophone is a word that is pro-
nounced the same as another word but differs in
meaning and/or written-form. Here, we use the word
?homophone? in a loose way. In particular, we re-
fer an informal phrase as a homophone of a formal
phrase if its pronunciation is the same or similar to
the formal phrase. In the three examples belonging
to the homophone category in Table 2, the first ex-
ample is a true homophone, while the other two are
loose homophones. The third example represents a
major sub-class where the informal phrase is a num-
ber (e.g., 88).
1032
Category Formal Informal %
Homophone ?? (BanZhu) [system administrator] ?? (BanZhu) [bamboo] 4.2
?? (XiHuan)[like] ?, (XiFan)[gruel] 4.4
?? (BaiBai)[bye-bye] 88 (BaBa) 21
Abbreviation ?)? (MeiGuoJunDui)[american army] ? (MeiJun)[american army] 3.8
Acronym ?? (GeGe)[elder brother] GG 12.3
E?? (Nu?PengYou)[girl friend] GF 7.2
Transliteration ?? (GeMi)[fans] N? (FenSi)[a Chinese food]
2.3
\\ (XieXie)[thank you] 3Q (SanQiu)
Others ?n?N? (XiLaLiFenSi)[fans of Hilary] ?, (XiFan)[gruel]
44.8??jN? (AoBaMaFenSi)[fans of Obama] QN (OuFen)[a food]
? (ChaoQiang)[super strong] P?? (ZouZhaoGongXu)
Table 2: Chinese Formal-informal Relations: Categories and Examples. Literal glosses in brackets.
For illustrative purposes, we can present the
transformation path showing how the informal
phrase is obtained from the formal phrase. In par-
ticular, the transformation path for this category is
?Formal ? PinYin ? Informal (similar or same
PinYin as the formal phrase)?.
2.2 Abbreviation and Acronym
A Chinese abbreviation of a formal phrase is ob-
tained by selecting one or more characters from this
formal phrase, and the selected characters can be at
any position in the formal phrase (Li and Yarowsky,
2008; Lee, 2005; Yin, 1999). In comparison, an
acronym is a special form of abbreviation, where
only the first character of each word in the formal
phrase is selected to form the informal phrase. Table
2 presents three examples belonging to this category.
While the first example is an abbreviation, and the
other two examples are acronyms.
The transformation path for the second exam-
ple is ?Formal ? PinYin ? Acronym?, and the
transformation path for the third example is ?For-
mal? English? Acronym?. Clearly, they differ in
whether PinYin or English is used as a bridge.
2.3 Transliteration
A transliteration is transcribing a word or text writ-
ten in one writing system into another writing sys-
tem. Table 2 presents examples belonging to this
category. In the first example, the Chinese infor-
mal phrase ?N? (FenSi)[a Chinese food]? can be
thought as a transliteration of the English phase
?fans? as the pronunciation of ?fans? is quite sim-
ilar to the PinYin ?FenSi?.
The transformation path for this category is ?For-
mal? English? Chinese Transliteration?.
2.4 Others
Due to the inherently informal and flexible nature of
expressions in informal genre, the formation of an
informal phrase can be very complex or ad-hoc. For
example, an informal phrase can be generated by ap-
plying the above transformation rules jointly. More
importantly, many relations cannot be described us-
ing a simple set of rules. Table 2 presents three such
examples, where the first two examples are gener-
ated by applying rules jointly and the third example
is created by decomposing the Chinese characters in
the formal form. The statistics collected from the
904 examples tells us that about 45% of the relations
belonging to this category. This motivates us to use
a data-driven method to automatically discover the
relations between informal and formal phrases.
3 Data Co-occurrence
In natural language, related words tend to appear to-
gether (i.e., co-occurrence). For example, Bill Gates
1033
tends to appear together with Microsoft more of-
ten than expected by chance. Such co-occurrence
may imply the existence of a relationship, and is ex-
ploited in formal-informal relation discovery under
different conditions.
3.1 Data Co-occurrence in Definitions
In general, for many informal phrases in popular use,
there is likely to be an explicit definition somewhere
that provides or paraphrases its meaning for an unfa-
miliar audience. People have created dedicated def-
inition web-pages to explain the relations between
formal and informal phrases. For example, the first
example in Table 3 is commonly explained in many
dedicated definition web-pages on the Internet. On
the other hand, in some formal text (e.g., research
papers), people tend to define the informal phrase
before it is used frequently in the later part of the
text. The second example of Table 3 illustrates this
phenomena. Clearly, the definition text normally
contains salient patterns. For example, the first ex-
ample follows the ?informal4formal{??? defi-
nition pattern, while the second example follows the
pattern ?formal (informal)?. This gives us a reliable
way to seed and bootstrap a list of informal phrases
as will be discussed in Section 4.1.
Relation Definition Text
(E??, GF) GF4E??{??
(-??	?,
-?)
&?{ yf~-??
	? (-?)X~yy
??Z
Table 3: Data Co-occurrence in Definitions
3.2 Data Co-occurrence in Online Chat
Informal phrases appear in online chat very often for
input convenience or just for fun. Since different
people may have different ways or traditions to ex-
press semantically-equivalent phrases, one may find
many nearby data co-occurrence examples in chat
text. For example, in Table 4, after a series of mes-
sage exchanges, person A wants to end the conver-
sation and types ???? (meaning ?bye-bye?), per-
son B later includes the same semantic content, but
in a different (more or less formal) expression (e.g.
?88?).
...
Person A: ?X???"?
Person A: ??
Person B: 88
Table 4: Data Co-occurrence in Online Chat for Relation
(??, 88) meaning ?bye-bye?
3.3 Data Co-occurrence in News Articles
For some formal-informal relations, since both of
the informal and formal phrases have been used in
public very often and people are normally aware
of these relations, an author may use the informal
and formal phrases interchangeably without bother-
ing to explain the relations. This is particularly true
in news articles for some well-known relations. Ta-
ble 5 shows an example, where the abbreviation ??
??? (meaning ?winter olympics?) appears in the
title and its full-form ?????? appears in the
text of the same document. In general, the relative
distance between an informal phrase and its formal
phrase varies. For example, they may appear in the
same sentence, or in neighboring sentences.
Title ?????*R?<??
Text c???2?9??(V?c?
?)?20?????{?*R
?h?-10?t8?????
?.??t*y ?{??
Table 5: Data Co-occurrence in News Article for Relation
(????,???) meaning ?winter olympics?
4 Mining Relations between Informal and
Formal Phrases from Web
In this section, we describe an approach that auto-
matically discovers the relation between a formal
phrase and an informal phrase from web corpora.
Specifically, we propose a bootstrapping procedure
to identify a list of candidate informal phrases.
Given a target informal phrase, we retrieve a large
set of of instances in context from the Web, generate
candidate hypotheses (i.e, candidate formal phrases)
from the data, and rank the hypotheses by using a
conditional log-linear model. The log-linear model
is very flexible to incorporate both the rule- and data-
1034
driven intuitions (described in Sections 2 and 3, re-
spectively) into the model as feature functions.
4.1 Identifying Informal Phrases
Before finding the formal phrase corresponding to
an informal phrase, we first need to identify infor-
mal phrases of interest. For example, one can collect
informal phrases manually. However, this is too ex-
pensive as new relations between informal and for-
mal phrases emerge every day on the Internet. Alter-
natively, one can employ a large amount of formal
text (e.g., newswire) and informal text (e.g., Inter-
net blogs) to derive such a list as follows. Specifi-
cally, from the informal corpus we can extract those
phrases whose frequency in the informal corpus is
significantly different from that in the formal cor-
pus. However, such a list may be quite noisy, i.e.,
many of them are not informal phrases at all.
An alternative approach to extracting the infor-
mal phrases is to use a bootstrapping algorithm (e.g.,
Yarowsky (1995)). Specifically, we first manually
collect a small set of example relations. Then, using
these relations as a seed set, we extract the text pat-
terns (e.g., the definition pattern showing how the
informal and formal phrases co-occur in the data as
discussed in Section 3.1). With these patterns, we
identify many more new relations from the data and
augment them into the seed set. The procedure it-
erates. Using such an approach, we should be able
to extract a large list of formal-informal relations.
Clearly, the list extracted in this way may be quite
noisy, and thus it is important to exploit both the
data- and rule-driven intuitions to rank these rela-
tions properly.
4.2 Retrieving Data from Web
Given an informal phrase, we retrieve training data
from the web on the fly. Specifically, we first use
a search engine to identify a set of hyper-links that
point to web pages containing contexts relevant to
the informal phrase, and then follow the hyper-links
to download the web pages. The input to the search
engine is a text query. One can simply use the infor-
mal phrase as a query. However, this may lead to a
set of pages that have nothing to do with the infor-
mal phrase. For example, if we search the informal
phrase ?88? (the third example in Table 2) using the
well-known Chinese search engine www.baidu.com,
none of the top-10 pages are related to the infor-
mal phrase ?88?. To avoid this situation, one can
use a search engine that is dedicated to informal text
search (e.g., blogsearch.baidu.com). Alternatively,
one can use the general-purpose search engine but
expanding the query with domain information. For
example, for the informal phrase ?88?, we can use
a query ?88 d???, where ?d??? means
internet language.
4.3 Generating Candidate Hypotheses
Given an informal phrase, we generate a set of hy-
potheses which are candidate formal phrases corre-
sponding to the informal phrase. We considered two
general approaches to the generation of hypotheses.
Rule-driven Hypothesis Generation: One can
use the rules described in Section 2 to generate a
set of hypotheses. However, with this approach, one
may generate an exponential number of hypotheses.
For example, assuming the number of English words
starting with a given letter is O(|V |), we can generate
O(|V |n) hypotheses given an acronym containing n
letters. Another problem with this approach is that
a relation between an informal phrase and a formal
phrase may not be explained by a specific rule. In
fact, as shown in the last row of Table 2, such rela-
tions consist of 44.8% of all corpus instances.
Data-driven Hypothesis Generation: With data
retrieved from the Web, we can generate hypotheses
by enumerating the frequent n-grams co-occurring
with the informal phrase within certain distance.
This exploits the data co-occurrence phenomena de-
scribed in Section 3, that is, the formal phrase tends
to co-occur with the informal phrase nearby in the
data, for the multiple reasons described above. This
can deal with the cases where the relation between
an informal phrase and a formal phrase cannot be
explained by a rule. However, it also suffers from
the over-generation problem as in the rule-driven ap-
proach.
In this paper, we use the data-driven method to
generate hypotheses, and rank the hypotheses using
a conditional log-linear model that incorporates both
the rule and data intuitions as feature functions.
1035
4.4 Ranking Hypotheses: Conditional
Log-linear Model
Log-linear models are known for flexible incorpora-
tion of features into the model. Each feature func-
tion reflects a hint/intuition that can be used to rank
the hypotheses. In this subsection, we develop a
conditional log-linear model that incorporates both
the rule and data intuitions as feature functions.
4.4.1 Conditional Log-linear Model
Given an informal phrase (say x) and a candidate
formal phrase (say y), the model assigns the pair a
score (say s(x, y)), which will be used to rank the
hypothesis y. The score s(x, y) is a linear combina-
tion of the feature scores (say ?i(x, y)) over a set of
feature functions indexed by i. Formally,
s(x, y) =
K?
i=1
?i(x, y)? ?i (1)
where K is the number of feature functions defined
and ?i is the weight assigned to the i-th feature func-
tion (i.e., ?i). To learn the weight vector ~?, we first
define a probability measure,
P~?(y|x) =
1
Z(x, ~?)
es(x,y) (2)
where Z(x, ~?) is a normalization constant. Now, we
define the regularized log-likelihood (LLR) of the
training data (i.e, a set of pairs of (x, y)), as follows,
LLR(~?) =
N?
j=1
log P~?(yj |xj)?
||~?||2
2?2
(3)
whereN is the number of training examples, and the
regularization term ||~?||
2
2?2 is a Gaussian prior with a
variance ?2 (Roark et al, 2007). The optimal weight
vector ~?? is obtained by maximizing the regularized
log-likelihood (LLR), that is,
~?? = arg max
~?
LLR(~?) (4)
To maximize the above function, we use a limited-
memory variable method (Benson and More, 2002)
that is implemented in the TAO package (Benson et
al., 2002) and has been shown to be very effective in
various natural language processing tasks (Malouf,
2002).
During test time, the following decision rule is
normally used to predict the optimal formal phrase
y? for a given informal phrase x,
y? = arg max
y
s(x, y). (5)
4.4.2 Feature Functions
As mentioned before, we incorporate both the
rule- and data-driven intuitions as feature functions
in the log-linear model.
Rule-driven feature functions: Clearly, if a pair
(x, y) matches the rule patterns described in Table 2,
the pair has a high possibility to be a true formal-
informal relation. To reflect this intuition, we de-
velop several feature functions as follows.
? LD-PinYin(x, y): the Levenshtein distance on
PinYin of x and y. The distance between
two PinYin characters is weighted based on
the similarity of pronunciation, for example,
the weight w(l, n) is smaller than the weight
w(a, z).
? LEN-PinYin(x, y): the difference in the num-
ber of PinYin characters between x and y.
? Is-PinYin-Acronym(x, y): is x a PinYin
acronym of y? For example,
Is-PinYin-Acronym(GG,??)=1,
Is-PinYin-Acronym(GG,w?)=0.
? Is-CN-Abbreviation(x, y): is x a Chinese ab-
breviation of y? For example,
Is-CN-Abbreviation(?,?)?)=1,
Is-CN-Abbreviation(?,?)?)=0.
Data-driven feature functions: As described in
Section 3, the informal and formal phrases tends to
co-occur in the data. Here, we develop several fea-
ture functions to reflect this intuition.
? n-gram co-occurrence relative frequency: we
collect the n-grams that occur in the data within
a window of the occurrence of the informal
phrase, and compute their relative frequency
as feature values. Since different orders of
grams will have quite different statistics, we
define 7 features in this category: 1-gram, 2-
gram, 3-gram, 4-gram, 5-gram, 6to10-gram,
and 11to15-gram. Note that the order n of a
n-gram is in terms of number of Chinese char-
acters instead of words.
1036
? Features on a definition pattern: we have dis-
cussed definition patterns in Section 3.1. For
each definition pattern, we can define a feature
function saying that if the co-occurrence of x
and y satisfies the definition pattern, the feature
value is one, otherwise is zero.
? Features on the number of relevant web-pages:
another interesting feature function can be de-
fined as follows. For each candidate relation
(x, y), we use the pair as a query to search the
web, and treat the number of pages returned by
the search engine as a feature value.3 However,
these features are quite expensive as millions of
queries may need to be served.
5 Experimental Results
Recall that in Section 2 we categorize the formal-
informal relations based on the manually collected
relations. In this section, we use a subset of them for
training and testing. In particular, we use 252 exam-
ples to train the log-linear model that is described
in Section 4, and use 249 examples as test data to
compute the precision.4
Table 6 shows the weights5 learned for the var-
ious feature functions described in Section 4.4.
Clearly, different feature functions get quite differ-
ent weights. This is intuitive as the feature functions
may differ in the scale of the feature values or in
their importance in ranking the hypotheses. In fact,
this shows the importance of using the log-linear
model to learn the optimal weights in a principled
and automatic manner, instead of manually tuning
the weights in an ad-hoc way.
Tables 7-9 show the precision results for different
categories as described in Section 2, using the rule-
driven, data-driven, or both rule and data-driven fea-
tures, respectively. In the tables, the precision corre-
sponding to the ?top-N? is computed in the follow-
ing way: if the true hypothesis is among the top-N
hypotheses ranked by the model, we tag the classi-
fication as correct, otherwise as wrong. Clearly, the
3Note that the number of pages relevant to a query can be
easily obtained as most search engines return this number.
4Again, the training and test examples are freely available at
http://www.cs.jhu.edu/?zfli.
5Note that we do not use the features on definition patterns
and on the number of relevant web pages, for efficiency.
Category Feature Weight
Rule-driven
LD-PinYin 0.800
Len-PinYin 0.781
Is-PinYin-Acronym 7.594
Is-CN-Abbreviation 7.464
Data-driven
1-gram 14.506
2-gram 108.193
3-gram 82.975
4-gram 66.872
5-gram 42.258
6to10-gram 21.229
11to15-gram 0.985
Table 6: Optimal Weights in the Log-linear Model
larger the N is, the higher the precision is. Comput-
ing the top-N precision (instead of just computing
the usual top-1 precision) is meaningful especially
when we consider our relation extractor as an inter-
mediate step in an end-to-end text-processing sys-
tem (e.g., machine translation) since the final deci-
sion can be delayed to later stage based on more ev-
idence. In general, our model gets quite respectably
high precision for such a task (e.g., more than 60%
for top-1 and more than 85% for top-100) when us-
ing both data and rule-driven features, as shown in
Table 9. Moreover, the data-driven features are more
helpful than the rule-driven features (e.g, 25.3% ab-
solute improvement in 1-best precision), while the
combination of these features does boost the perfor-
mance of any individual feature set (e.g., 10.4% ab-
solute improvement in 1-best precision over the case
using data-driven features only).
We also carried out experiments (see Table 10)
in the bootstrapping procedure described in Section
4.1. In particular, we start from a seed set having
130 relations. We identify the frequent patterns from
the data retrieved from the web for these seed exam-
ples. Then, we use these patterns to identify many
more new possible formal-informal relations. After
the first iteration, we select the top 3000 pairs of re-
lations matched by the patterns. The recall of a man-
ually collected test set (having 750 pairs) on these
3000 pairs is around 30%, which is quite promising
given the highly noisy data.
1037
Category
Precision (%)
Top-1 Top-10 Top-50 Top-100
Homophone Same PinYin 31.6 47.4 68.4 73.7
Similar PinYin 15.0 35.0 45.0 50.0
Number 31.6 64.2 84.2 90.5
Abbreviation Chinese abbreviation 11.8 35.3 41.2 41.2
Acronym PinYin Acronym 39.3 82.1 91.1 92.9
English Acronym 3.1 6.3 9.4 28.1
Transliteration 10.0 20.0 20.0 20.0
Average 26.1 53.4 66.3 72.3
Table 7: Rule-driven Features only: Precision on Chinese Formal-informal Relation Extraction
Category
Precision (%)
Top-1 Top-10 Top-50 Top-100
Homophone Same PinYin 52.6 73.7 73.7 78.9
Similar PinYin 45.0 65.0 75.0 75.0
Number 66.3 86.3 94.7 96.8
Abbreviation Chinese abbreviation 0.0 23.5 47.1 47.1
Acronym PinYin Acronym 58.9 78.6 85.7 87.5
English Acronym 25.0 46.9 68.6 68.8
Transliteration 50.0 50.0 50.0 50.0
Average 51.4 71.1 81.1 82.7
Table 8: Data-driven Features only: Precision on Chinese Formal-informal Relation Extraction
Category
Precision (%)
Top-1 Top-10 Top-50 Top-100
Homophone Same PinYin 63.2 73.7 84.2 84.2
Similar PinYin 40.0 60.0 70.0 80.0
Number 81.1 91.6 95.8 96.8
Abbreviation Chinese abbreviation 11.8 41.2 52.9 52.9
Acronym PinYin Acronym 82.1 94.6 96.4 96.4
English Acronym 21.9 46.9 56.3 59.4
Transliteration 20.0 40.0 50.0 50.0
Average 61.8 77.1 83.1 84.7
Table 9: Both Data and Rule-drive Features: Precision on Chinese Formal-informal Relation Extraction
1038
Size of seed set 130
Size of candidate set 3000
Size of test set 750
Recall 30%
Table 10: Recall of Test Set on a Candidate Set Extracted
by a Bootstrapping Procedure
6 Related Work
Automatically extracting the relations between full-
form Chinese phrases and their abbreviations is an
interesting and important task for many NLP appli-
cations (e.g., machine translation, information re-
trieval, etc.). Recently, Chang and Lai (2004), Lee
(2005), Chang and Teng (2006), Li and Yarowsky
(2008) have investigated this task. Specifically,
Chang and Lai (2004) describes a hidden markov
model (HMM) to model the relationship between
a full-form phrase and its abbreviation, by treat-
ing the abbreviation as the observation and the full-
form words as states in the model. Using a set
of manually-created full-abbreviation relations as
training data, they report experimental results on
a recognition task (i.e., given an abbreviation, the
task is to obtain its full-form, or the vice versa).
Chang and Teng (2006) extends the work in Chang
and Lai (2004) to automatically extract the relations
between full-form phrases and their abbreviations,
where both the full-form phrase and its abbrevia-
tion are not given. Clearly, the method in (Chang
and Lai, 2004; Chang and Teng, 2006) is super-
vised because it requires the full-abbreviation rela-
tions as training data. Li and Yarowsky (2008) pro-
pose an unsupervised method to extract the relations
between full-form phrases and their abbreviations.
They exploit the data co-occurrence phenomena in
the newswire text, as we have done in this paper.
Moreover, they augment and improve a statistical
machine translation by incorporating the extracted
relations into the baseline translation system.
Other interesting work that addresses a similar
task as ours includes the work on homophones (e.g.,
Lee and Chen (1997)), abbreviations with their defi-
nitions (e.g., Park and Byrd (2001)), abbreviations
and acronyms in the medical domain (Pakhomov,
2002), and transliteration (e.g., (Knight and Graehl,
1998; Virga and Khudanpur, 2003; Li et al, 2004;
Wu and Chang, 2007)).
While all the above work deals with the rela-
tions occurring within the formal text, we consider
the formal-informal relations that occur across both
formal and informal text, and we extract the rela-
tions from the web corpora, instead from just formal
text. Moreover, our method is semi-supervised in
the sense that the weights of the feature functions
are tuned in a supervised log-linear model using a
small number of seed relations while the generation
and ranking of the hypotheses are unsupervised by
exploiting the data co-occurrence phenomena.
7 Conclusions
In this paper, we have first presented a taxonomy of
the formal-informal relations occurring in Chinese
text. We have then proposed a novel method for
discovering and modeling the relationship between
informal Chinese expressions (including colloqui-
alisms and instant-messaging slang) and their formal
equivalents. Specifically, we have proposed a boot-
strapping procedure to identify a list of candidate
informal phrases in web corpora. Given an infor-
mal phrase, we retrieved contextual instances from
the web using a search engine, generated hypothe-
ses of formal equivalents via this data, and ranked
the hypotheses using a conditional log-linear model.
In the log-linear model, we incorporated as feature
functions both rule-based intuitions and data co-
occurrence phenomena (either as an explicit or in-
direct definition, or through formal/informal usages
occurring in free variation in a discourse). We tested
our system on manually collected test examples,
and found that the (formal-informal) relationship
discovery and extraction process using our method
achieves an average 1-best precision of 62%. Given
the ubiquity of informal conversational style on the
internet, this work has clear applications for text nor-
malization in text-processing systems including ma-
chine translation aspiring to broad coverage.
Acknowledgments
We would like to thank Yi Su, Sanjeev Khudanpur,
and the anonymous reviewers for their helpful com-
ments. This work was partially supported by the De-
fense Advanced Research Projects Agency?s GALE
program via Contract No
?
HR0011-06-2-0001.
1039
References
S. J. Benson, L. C. McInnes, J. J. More, and J. Sarich.
2002. Tao users manual, Technical Report ANL/MCS-
TM-242-Revision 1.4, Argonne National Laboratory.
S. J. Benson and J. J. More. 2002. A limited memory vari-
able metric method for bound constrained minimiza-
tion. preprint ANL/ACSP909-0901, Argonne National
Laboratory.
Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary
study on probabilistic models for Chinese abbrevia-
tions. In Proceedings of the 3rd SIGHAN Workshop
on Chinese Language Processing, Barcelona, Spain
(2004),pages 9-16.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
Atomic Chinese Abbreviation Pairs: A Probabilistic
Model for Single Character Word Recovery. In Pro-
ceedings of the 5rd SIGHAN Workshop on Chinese
Language Processing, Sydney, Australia (2006), pages
17-24.
Kevin Knight and Jonathan Graehl. 1998. Machine
Transliteration. Computational Linguistics, 24(4):599-
612.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan,Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177-180.
H.W.D Lee. 2005. A study of automatic expansion of
Chinese abbreviations. MA Thesis, The University of
Hong Kong.
Yue-Shi Lee and Hsin-Hsi Chen. 1997. Applying Repair
Processing in Chinese Homophone Disambiguation.
In Proceedings of the Fifth Conference on Applied Nat-
ural Language Processing, pages 57-63.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source
channel model for machine transliteration. In Proceed-
ings of ACL 2004, pages 159-166.
Zhifei Li and David Yarowsky. 2008. Unsupervised
Translation Induction for Chinese Abbreviations using
Monolingual Corpora. In Proceedings of ACL 2008,
pages 425-433.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
CoNLL 2002, pages 49-55.
Serguei Pakhomov. 2002. Semi-Supervised Maximum
Entropy Based Approach to Acronym and Abbrevia-
tion Normalization in Medical Texts. In Proceedings
of ACL 2002, pages 160-167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP 2001, pages 126-133.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373-392.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of Proper Names in Cross lingual Information
Retrieval. In Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-language Named Entity
Recognition.
Jian-Cheng Wu and Jason S. Chang. 2007. Learning to
Find English to Chinese Transliterations on the Web.
In Proceedings of EMNLP-CoNLL 2007, pages 996-
1004.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of ACL 1995, pages 189-196.
Z.P. Yin. 1999. Methodologies and principles of Chi-
nese abbreviation formation. In Language Teaching
and Study, No.2 (1999) 73-82.
1040
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40?51,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
First- and Second-Order Expectation Semirings
with Applications to Minimum-Risk Training on Translation Forests
?
Zhifei Li and Jason Eisner
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com, jason@cs.jhu.edu
Abstract
Many statistical translation models can be
regarded as weighted logical deduction.
Under this paradigm, we use weights from
the expectation semiring (Eisner, 2002), to
compute first-order statistics (e.g., the ex-
pected hypothesis length or feature counts)
over packed forests of translations (lat-
tices or hypergraphs). We then introduce
a novel second-order expectation semir-
ing, which computes second-order statis-
tics (e.g., the variance of the hypothe-
sis length or the gradient of entropy).
This second-order semiring is essential for
many interesting training paradigms such
as minimum risk, deterministic anneal-
ing, active learning, and semi-supervised
learning, where gradient descent optimiza-
tion requires computing the gradient of en-
tropy or risk. We use these semirings in an
open-source machine translation toolkit,
Joshua, enabling minimum-risk training
for a benefit of up to 1.0 BLEU point.
1 Introduction
A hypergraph or ?packed forest? (Gallo et al,
1993; Klein and Manning, 2004; Huang and Chi-
ang, 2005) is a compact data structure that uses
structure-sharing to represent exponentially many
trees in polynomial space. A weighted hypergraph
also defines a probability or other weight for each
tree, and can be used to represent the hypothesis
space considered (for a given input) by a mono-
lingual parser or a tree-based translation system,
e.g., tree to string (Quirk et al, 2005; Liu et al,
2006), string to tree (Galley et al, 2006), tree to
tree (Eisner, 2003), or string to string with latent
tree structures (Chiang, 2007).
?
This research was partially supported by the Defense
Advanced Research Projects Agency?s GALE program via
Contract No HR0011-06-2-0001. We are grateful to Sanjeev
Khudanpur for early guidance and regular discussions.
Given a hypergraph, we are often interested in
computing some quantities over it using dynamic
programming algorithms. For example, we may
want to run the Viterbi algorithm to find the most
probable derivation tree in the hypergraph, or the k
most probable trees. Semiring-weighted logic pro-
gramming is a general framework to specify these
algorithms (Pereira and Warren, 1983; Shieber et
al., 1994; Goodman, 1999; Eisner et al, 2005;
Lopez, 2009). Goodman (1999) describes many
useful semirings (e.g., Viterbi, inside, and Viterbi-
n-best). While most of these semirings are used in
?testing? (i.e., decoding), we are mainly interested
in the semirings that are useful for ?training? (i.e.,
parameter estimation). The expectation semiring
(Eisner, 2002), originally proposed for finite-state
machines, is one such ?training? semiring, and can
be used to compute feature expectations for the E-
step of the EM algorithm, or gradients of the like-
lihood function for gradient descent.
In this paper, we apply the expectation semir-
ing (Eisner, 2002) to a hypergraph (or packed for-
est) rather than just a lattice. We then propose
a novel second-order expectation semiring, nick-
named the ?variance semiring.?
The original first-order expectation semiring al-
lows us to efficiently compute a vector of first-
order statistics (expectations; first derivatives) on
the set of paths in a lattice or the set of trees in a
hypergraph. The second-order expectation semir-
ing additionally computes a matrix of second-
order statistics (expectations of products; second
derivatives (Hessian); derivatives of expectations).
We present details on how to compute many in-
teresting quantities over the hypergraph using the
expectation and variance semirings. These quan-
tities include expected hypothesis length, feature
expectation, entropy, cross-entropy, Kullback-
Leibler divergence, Bayes risk, variance of hy-
pothesis length, gradient of entropy and Bayes
risk, covariance and Hessian matrix, and so on.
The variance semiring is essential for many in-
teresting training paradigms such as deterministic
40
annealing (Rose, 1998), minimum risk (Smith and
Eisner, 2006), active and semi-supervised learning
(Grandvalet and Bengio, 2004; Jiao et al, 2006).
In these settings, we must compute the gradient of
entropy or risk. The semirings can also be used for
second-order gradient optimization algorithms.
We implement the expectation and variance
semirings in Joshua (Li et al, 2009a), and demon-
strate their practical benefit by using minimum-
risk training to improve Hiero (Chiang, 2007).
2 Semiring Parsing on Hypergraphs
We use a specific tree-based system called Hiero
(Chiang, 2007) as an example, although the dis-
cussion is general for any systems that use a hy-
pergraph to represent the hypothesis space.
2.1 Hierarchical Machine Translation
In Hiero, a synchronous context-free grammar
(SCFG) is extracted from automatically word-
aligned corpora. An illustrative grammar rule for
Chinese-to-English translation is
X ? ?X
0
{ X
1
, X
1
of X
0
? ,
where the Chinese word { means of, and the
alignment, encoded via subscripts on the nonter-
minals, causes the two phrases around { to be
reordered around of in the translation. Given
a source sentence, Hiero uses a CKY parser to
generate a hypergraph, encoding many derivation
trees along with the translation strings.
2.2 Hypergraphs
Formally, a hypergraph is a pair ?V,E?, where V
is a set of nodes (vertices) and E is a set of hy-
peredges, with each hyperedge connecting a set of
antecedent nodes to a single consequent node.
1
In
parsing parlance, a node corresponds to an item
in the chart (which specifies aligned spans of in-
put and output together with a nonterminal label).
The root node corresponds to the goal item. A
hyperedge represents an SCFG rule that has been
?instantiated? at a particular position, so that the
nonterminals on the right and left sides have been
replaced by particular antecedent and consequent
items; this corresponds to storage of backpointers
in the chart.
We write T (e) to denote the set of antecedent
nodes of a hyperedge e. We write I(v) for the
1
Strictly speaking, making each hyperedge designate a
single consequent defines a B-hypergraph (Gallo et al, 1993).
X 0,2 the mat NA X 3,4 a cat NA
X 0,4 a cat the mat
X 0,4 the mat a cat
goal item
?
0
 ?
1
                   ?
2
               ?
3
on the mat                of              a cat
X??X0 ?X1,X1 onX0?
X??X0 ?X1,X1 of X0?X??X0 ?X1,X0 ?s X1?X??X0 ?X1,X0 X1?
X?????, the mat?
S??X0,X0? S??X0,X0?
X???, a cat?
Figure 1: A toy hypergraph in Hiero. When generating the
hypergraph, a trigram language model is integrated. Rect-
angles represent items, where each item is identified by the
non-terminal symbol, source span, and left- and right-side
language model states. An item has one or more incoming
hyperedges. A hyperedge consists of a rule, and a pointer to
an antecedent item for each non-terminal symbol in the rule.
set of incoming hyperedges of node v (i.e., hyper-
edges of which v is the consequent), which repre-
sent different ways of deriving v. Figure 1 shows
a simple Hiero-style hypergraph. The hypergraph
encodes four different derivation trees that share
some of the same items. By exploiting this shar-
ing, a hypergraph can compactly represent expo-
nentially many trees.
We observe that any finite-state automaton can
also be encoded as a hypergraph (in which every
hyperedge is an ordinary edge that connects a sin-
gle antecedent to a consequent). Thus, the meth-
ods of this paper apply directly to the simpler case
of hypothesis lattices as well.
2.3 Semiring Parsing
We assume a hypergraph HG, which compactly
encodes many derivation trees d ? D. Given HG,
we wish to extract the best derivations?or other
aggregate properties of the forest of derivations.
Semiring parsing (Goodman, 1999) is a general
framework to describe such algorithms. To define
a particular algorithm, we choose a semiring K
and specify a ?weight? k
e
? K for each hyper-
edge e. The desired aggregate result then emerges
as the total weight of all derivations in the hyper-
graph. For example, to simply count derivations,
one can assign every hyperedge weight 1 in the
semiring of ordinary integers; then each deriva-
tion also has weight 1, and their total weight is the
number of derivations.
We write K = ?K,?,?, 0, 1? for a semiring
with elements K, additive operation ?, multi-
41
plicative operation?, additive identity 0, and mul-
tiplicative identity 1. The ? operation is used to
obtain the weight of each derivation d by multi-
plying the weights of its component hyperedges e,
that is, k
d
=
?
e?d
k
e
. The ? operation is used
to sum over all derivations d in the hypergraph
to obtain the total weight of the hypergraph HG,
which is
?
d?D
?
e?d
k
e
.
2
Figure 2 shows how to
compute the total weight of an acyclic hypergraph
HG.
3
In general, the total weight is a sum over
exponentially many derivations d. But Figure 2
sums over these derivations in time only linear on
the size of the hypergraph. Its correctness relies
on axiomatic properties of the semiring: namely,
? is associative and commutative with identity 0,
? is associative with two-sided identity 1, and
? distributes over ? from both sides. The dis-
tributive property is what makes Figure 2 work.
The other properties are necessary to ensure that
?
d?D
?
e?d
k
e
is well-defined.
4
The algorithm in Figure 2 is general and can be
applied with any semiring (e.g., Viterbi). Below,
we present our novel semirings.
3 Finding Expectations on Hypergraphs
We now introduce the computational problems of
this paper and the semirings we use to solve them.
3.1 Problem Definitions
We are given a function p : D ? R
?0
, which
decomposes multiplicatively over component hy-
peredges e of a derivation d ? D: that is, p(d)
def
=
?
e?d
p
e
. In practice, p(d) will specify a probabil-
ity distribution over the derivations in the hyper-
2
Eisner (2002) uses closed semirings that are also
equipped with a Kleene closure operator
?
. For example, in
the real semiring ?R,+,?, 0, 1?, we define p
?
= (1 ? p)
?1
(= 1 + p + p
2
+ . . .) for |p| < 1 and is undefined other-
wise. The closure operator enables exact summation over the
infinitely many paths in a cyclic FSM, or trees in a hyper-
graph with non-branching cycles, without the need to iterate
around cycles to numerical convergence. For completeness,
we specify the closure operator for our semirings, satisfying
the axioms k
?
= 1 ? k ? k
?
= 1 ? k
?
? k, but we do not
use it in our experiments since our hypergraphs are acyclic.
3
We assume that HG has already been built by deductive
inference (Shieber et al, 1994). But in practice, the nodes? in-
side weights ?(v) are usually accumulated as the hypergraph
is being built, so that pruning heuristics can consult them.
4
Actually, the notation
?
e?d
k
e
assumes that ? is com-
mutative as well, as does the notation ?for u ? T (e)? in our
algorithms; neither specifies a loop order. One could how-
ever use a non-commutative semiring by ordering each hyper-
edge?s antecedents and specifying that a derivation?s weight
is the product of the weights of its hyperedges when visited in
prefix order. Tables 1?2 will not assume any commutativity.
INSIDE(HG,K)
1 for
v
in topological order on HG  each node
2  find ?(v)?
?
e?I(v)
(k
e
? (
?
u?T (e)
?(u)))
3 ?(v)? 0
4 for
e
? I(v)  each incoming hyperedge
5 k ? k
e
 hyperedge weight
6 for
u
? T (e)  each antecedent node
7 k ? k ? ?(u)
8 ?(v)? ?(v)? k
9 return ?(root)
Figure 2: Inside algorithm for an acyclic hypergraph HG,
which provides hyperedge weights k
e
? K. This computes
all ?inside weights? ?(v) ? K, and returns ?(root), which is
total weight of the hypergraph, i.e.,
?
d?D
?
e?d
k
e
.
OUTSIDE(HG,K)
1 for
v
in HG
2 ?(v)? 0
3 ?(root)? 1
4 for
v
in reverse topological order on HG
5 for
e
? I(v)  each incoming hyperedge
6 for
u
? T (e)  each antecedent node
7 ?(u)? ?(u)? (?(v)? k
e
?
8
?
w?T (e),w 6=u
?(w))
Figure 3: Computes the ?outside weights? ?(v). Can only be
run after INSIDE(HG) of Figure 2 has already computed the
inside weights ?(v).
graph. It is often convenient to permit this prob-
ability distribution to be unnormalized, i.e., one
may have to divide it through by some Z to get a
proper distribution that sums to 1.
We are also given two functions of interest r, s :
D ? R, each of which decomposes additively
over its component hyperedges e: that is, r(d)
def
=
?
e?d
r
e
, and s(d)
def
=
?
e?d
s
e
.
We are now interested in computing the follow-
ing quantities on the hypergraph HG:
Z
def
=
?
d?D
p(d) (1)
r
def
=
?
d?D
p(d)r(d) (2)
s
def
=
?
d?D
p(d)s(d) (3)
t
def
=
?
d?D
p(d)r(d)s(d) (4)
Note that r/Z, s/Z, and t/Z are expectations un-
der p of r(d), s(d), and r(d)s(d), respectively.
More formally, the probabilistic interpretation
is that D is a discrete sample space (consisting
42
INSIDE-OUTSIDE(HG,K,X )
1  Run inside and outside on HG with only k
e
weights
2
?
k ? INSIDE(HG,K)  see Figure 2
3 OUTSIDE(HG,K)  see Figure 3
4  Do a single linear combination to get x?
5 x?? 0
6 for
v
in HG  each node
7 for
e
? I(v)  each incoming hyperedge
8 k
e
? ?(v)
9 for
u
? T (e)  each antecedent node
10 k
e
? k
e
?(u)
11 x?? x?+ (k
e
x
e
)
12 return ?
?
k, x??
Figure 4: If every hyperedge specifies a weight ?k
e
, x
e
? in
some expectation semiring E
K,X
, then this inside-outside al-
gorithm is a more efficient alternative to Figure 2 for comput-
ing the total weight ?
?
k, x?? of the hypergraph, especially if the
x
e
are vectors. First, at lines 2?3, the inside and outside al-
gorithms are run using only the k
e
weights, obtaining only
?
k
(without x?) but also obtaining all inside and outside weights
?, ? ? K as a side effect. Then the second component x? of
the total weight is accumulated in lines 5?11 as a linear com-
bination of all the x
e
values, namely x? =
?
e
k
e
x
e
, where
k
e
is computed at lines 8?10 using ? and ? weights. The lin-
ear coefficient k
e
is the ?exclusive weight? for hyperedge e,
meaning that the product k
e
k
e
is the total weight in K of all
derivations d ? D that include e.
of all derivations in the hypergraph), p is a mea-
sure over this space, and r, s : D ? R are ran-
dom variables. Then r/Z and s/Z give the expec-
tations of these random variables, and t/Z gives
the expectation of their product t = rs, so that
t/Z ? (r/Z)(s/Z) gives their covariance.
Example 1: r(d) is the length of the translation
corresponding to derivation d (arranged by setting
r
e
to the number of target-side terminal words in
the SCFG rule associated with e). Then r/Z is
the expected hypothesis length. Example 2: r(d)
evaluates the loss of d compared to a reference
translation, using some additively decomposable
loss function. Then r/Z is the risk (expected loss),
which is useful in minimum-risk training. Exam-
ple 3: r(d) is the number of times that a certain
feature fires on d. Then r/Z is the expected fea-
ture count, which is useful in maximum-likelihood
training. We will generalize later in Section 4 to
allow r(d) to be a vector of features. Example 4:
Suppose r(d) and s(d) are identical and both com-
pute hypothesis length. Then the second-order
statistic t/Z is the second moment of the length
distribution, so the variance of hypothesis length
can be found as t/Z ? (r/Z)
2
.
3.2 Computing the Quantities
We will use the semiring parsing framework to
compute the quantities (1)?(4). Although each is a
sum over exponentially many derivations, we will
compute it in O(|HG|) time using Figure 2.
In the simplest case, let K = ?R,+,?, 0, 1?,
and define k
e
= p
e
for each hyperedge e. Then
the algorithm of Figure 2 reduces to the classical
inside algorithm (Baker, 1979) and computes Z.
Next suppose K is the expectation semiring
(Eisner, 2002), shown in Table 1. Define k
e
=
?p
e
, p
e
r
e
?. Then Figure 2 will return ?Z, r?.
Finally, suppose K is our novel second-order
expectation semiring, which we introduce in Ta-
ble 2. Define k
e
= ?p
e
, p
e
r
e
, p
e
s
e
, p
e
r
e
s
e
?.
Then the algorithm of Figure 2 returns ?Z, r, s, t?.
Note that, to compute t, one cannot simply con-
struct a first-order expectation semiring by defin-
ing t(d)
def
= r(d)s(d) because t(d), unlike r(d)
and s(d), is not additively decomposable over the
hyperedges in d.
5
Also, when r(d) and s(d) are
identical, the second-order expectation semiring
allows us to compute variance as t/Z ? (r/Z)
2
,
which is why we may call our second-order ex-
pectation semiring the variance semiring.
3.3 Correctness of the Algorithms
To prove our claim about the first-order expecta-
tion semiring, we first observe that the definitions
in Table 1 satisfy the semiring axioms. The
reader can easily check these axioms (as well
as the closure axioms in footnote 2). With a
valid semiring, we then simply observe that Fig-
ure 2 returns the total weight
?
d?D
?
e?d
k
e
=
?
d?D
?p(d), p(d)r(d)? = ?Z, r?. It is easy to
verify the second equality from the definitions
of ?, Z, and r. The first equality requires
proving that
?
e?d
k
e
= ?p(d), p(d)r(d)?
from the definitions of ?, k
e
, p(d), and r(d).
The main intuition is that ? can be used to
build up ?p(d), p(d)r(d)? inductively from the
k
e
: if d decomposes into two disjoint sub-
derivations d
1
, d
2
, then ?p(d), p(d)r(d)? =
?p(d
1
)p(d
2
), p(d
1
)p(d
2
)(r(d
1
) + r(d
2
))? =
?p(d
1
), p(d
1
)r(d
1
)? ? ?p(d
2
), p(d
2
)r(d
2
)?. The
base cases are where d is a single hyperedge e, in
which case ?p(d), p(d)r(d)? = k
e
(thanks to our
choice of k
e
), and where d is empty, in which case
5
However, in a more tricky way, the second-order expec-
tation semiring can be constructed using the first-order ex-
pectation semiring, as will be seen in Section 4.3.
43
Element ?p, r?
?p
1
, r
1
?? ?p
2
, r
2
? ?p
1
p
2
, p
1
r
2
+ p
2
r
1
?
?p
1
, r
1
?? ?p
2
, r
2
? ?p
1
+ p
2
, r
1
+ r
2
?
?p, r?
?
?p
?
, p
?
p
?
r?
0 ?0, 0?
1 ?1, 0?
Table 1: Expectation semiring: Each element in the semir-
ing is a pair ?p, r?. The second and third rows define the
operations between two elements ?p
1
, r
1
? and ?p
2
, r
2
?, and
the last two rows define the identities. Note that the multi-
plicative identity 1 has an r component of 0.
s
a
s
b
a+ b a ? b
s
a+b
`
a+b
s
a?b
`
a?b
+ + + `
a
+ log(1 + e
`
b
?`
a
) + `
a
+ `
b
+ - + `
a
+ log(1? e
`
b
?`
a
) - `
a
+ `
b
- + - `
a
+ log(1? e
`
b
?`
a
) - `
a
+ `
b
- - - `
a
+ log(1 + e
`
b
?`
a
) + `
a
+ `
b
Table 3: Storing signed values in log domain: each value a
(= s
a
e
`
a
) is stored as a pair ?s
a
, `
a
? where s
a
and `
a
are the
sign bit of a and natural logarithm of |a|, respectively. This
table shows the operations between two values a = s
a
2
`
a
and b = s
b
2
`
b
, assuming `
a
? `
b
. Note: log(1 + x) (where
|x| < 1) should be computed by the Mercator series x ?
x
2
/2+x
3
/3?? ? ? , e.g., using the math library function log1p.
?p(d), p(d)r(d)? = 1. It follows by induction that
?p(d), p(d)r(d)? =
?
e?d
k
e
.
The proof for the second-order expec-
tation semiring is similar. In particular,
one mainly needs to show that
?
e?d
k
e
=
?p(d), p(d)r(d), p(d)s(d), p(d)r(d)s(d)?.
3.4 Preventing Underflow/Overflow
In Tables 1?2, we do not discuss how to store p, r,
s, and t. If p is a probability, it often suffers from
the underflow problem. r, s, and tmay suffer from
both underflow and overflow problems, depending
on their scales.
To address these, we could represent p in the
log domain as usual. However, r, s, and t can be
positive or negative, and we cannot directly take
the log of a negative number. Therefore, we repre-
sent real numbers as ordered pairs. Specifically, to
represent a = s
a
e
`
a
, we store ?s
a
, `
a
?, where the
s
a
? {+,?} is the sign bit of a and the floating-
point number `
a
is the natural logarithm of |a|.
6
Table 3 shows the ??? and ?+?operations.
6
An alternative that avoids log and exp is to store a =
f
a
2
e
a
as ?f
a
, e
a
?, where f
a
is a floating-point number and
e
a
is a sufficiently wide integer. E.g., combining a 32-bit
f
a
with a 32-bit e
a
will in effect extend f
a
?s 8-bit internal
exponent to 32 bits by adding e
a
to it. This gives much more
dynamic range than the 11-bit exponent of a 64-bit double-
precision floating-point number, if vastly less than in Table 3.
4 Generalizations and Speedups
In this section, we generalize beyond the above
case where p, r, s are R-valued. In general, p may
be an element of some other semiring, and r and s
may be vectors or other algebraic objects.
When r and s are vectors, especially high-
dimensional vectors, the basic ?inside algorithm?
of Figure 2 will be slow. We will show how to
speed it up with an ?inside-outside algorithm.?
4.1 Allowing Feature Vectors and More
In general, for P,R, S, T , we can define the
first-order expectation semiring E
P,R
= ?P ?
R,?,?, 0, 1? and the second-order expectation
semiring E
P,R,S,T
= ?P ?R?S?T,?,?, 0, 1?,
using the definitions from Tables 1?2. But do
those definitions remain meaningful, and do they
continue to satisfy the semiring axioms?
Indeed they do when P = R, R = R
n
, S =
R
m
, T = R
n?m
, with rs defined as the outer
product rs
T
(a matrix) where s
T
is the trans-
pose of s. In this way, the second-order semiring
E
P,R,S,T
lets us take expectations of vectors and
outer products of vectors. So we can find means
and covariances of any number of linearly decom-
posable quantities (e.g., feature counts) defined on
the hypergraph.
We will consider some other choices in Sec-
tions 4.3?4.4 below. Thus, for generality, we con-
clude this section by stating the precise technical
conditions needed to construct E
P,R
and E
P,R,S,T
:
? P is a semiring
? R is a P -module (e.g, a vector space), mean-
ing that it comes equipped with an associative
and commutative addition operation with an
identity element 0, and also a multiplication
operation P?R? R, such that p(r
1
+r
2
) =
pr
1
+pr
2
, (p
1
+p
2
)r = p
1
r+p
2
r, p
1
(p
2
r) =
(p
1
p
2
)r
? S and T are also P -modules
? there is a multiplication operation R ? S ?
T that is bilinear, i.e., (r
1
+ r
2
)s = r
1
s +
r
2
s, r(s
1
+ s
2
) = rs
1
+ rs
2
, (pr)s = p(rs),
r(ps) = p(rs)
As a matter of notation, note that above and in
Tables 1?2, we overload ?+? to denote any of
the addition operations within P,R, S, T ; over-
load ?0? to denote their respective additive iden-
tities; and overload concatenation to denote any
of the multiplication operations within or between
44
Element ?p, r, s, t?
?p
1
, r
1
, s
1
, t
1
?? ?p
2
, r
2
, s
2
, t
2
? ?p
1
p
2
, p
1
r
2
+ p
2
r
1
, p
1
s
2
+ p
2
s
1
,
p
1
t
2
+ p
2
t
1
+ r
1
s
2
+ r
2
s
1
?
?p
1
, r
1
, s
1
, t
1
?? ?p
2
, r
2
, s
2
, t
2
? ?p
1
+ p
2
, r
1
+ r
2
, s
1
+ s
2
, t
1
+ t
2
?
?p, r, s, t?
?
?p
?
, p
?
p
?
r, p
?
p
?
s, p
?
p
?
(p
?
rs+ p
?
rs+ t)?
0 ?0, 0, 0, 0?
1 ?1, 0, 0, 0?
Table 2: Second-order expectation semiring (variance semiring): Each element in the semiring is a 4-tuple ?p, r, s, t?. The
second and third rows define the operations between two elements ?p
1
, r
1
, s
1
, t
1
? and ?p
2
, r
2
, s
2
, t
2
?, while the last two rows
define the identities. Note that the multiplicative identity 1 has r,s and t components of 0.
P,R, S, T . ?1? refers to the multiplicative identity
of P . We continue to use distinguished symbols
?,?, 0, 1 for the operations and identities in our
?main semiring of interest,? E
P,R
or E
P,R,S,T
.
To compute equations (1)?(4) in this more gen-
eral setting, we must still require multiplicative
or additive decomposability, defining p(d)
def
=
?
e?d
p
e
, r(d)
def
=
?
e?d
r
e
, s(d)
def
=
?
e?d
s
e
as be-
fore. But the
?
and
?
operators here now denote
appropriate operations within P , R, and S respec-
tively (rather than the usual operations within R).
4.2 Inside-Outside Speedup for First-Order
Expectation Semirings
Under the first-order expectation semiring E
R,R
n
,
the inside algorithm of Figure 2 will return ?Z, r?
where r is a vector of n feature expectations.
However, Eisner (2002, section 5) observes that
this is inefficient when n is large. Why? The
inside algorithm takes the trouble to compute an
inside weight ?(v) ? R ? R
n
for each node v
in the hypergraph (or lattice). The second com-
ponent of ?(v) is a presumably dense vector of
all features that fire in all subderivations rooted at
node v. Moreover, as ?(v) is computed in lines
3?8, that vector is built up (via the ? and ? oper-
ations of Table 1) as a linear combination of other
dense vectors (the second components of the vari-
ous ?(u)). These vector operations can be slow.
A much more efficient approach (usually) is
the traditional inside-outside algorithm (Baker,
1979).
7
Figure 4 generalizes the inside-outside
algorithm to work with any expectation semiring
E
K,X
.
8
We are given a hypergraph HG whose
edges have weights ?k
e
, x
e
? in this semiring (so
7
Note, however, that the expectation semiring requires
only the forward/inside pass to compute expectations, and
thus it is more efficient than the traditional inside-outside al-
gorithm (which requires two passes) if we are interested in
computing only a small number of quantities.
8
This follows Eisner (2002), who similarly generalized
the forward-backward algorithm.
now k
e
? K denotes only part of the edge weight,
not all of it). INSIDE-OUTSIDE(HG,K, X) finds
?
d?D
?
e?d
?k
e
, x
e
?, which has the form ?
?
k, x??.
But, INSIDE(HG,E
K,X
) could accomplish the
same thing. So what makes the inside-outside al-
gorithm more efficient? It turns out that x? can
be found quickly as a single linear combination
?
e
k
e
x
e
of just the feature vectors x
e
that ap-
pear on individual hyperedges?typically a sum
of very sparse vectors! And the linear coefficients
k
e
, as well as
?
k, are computed entirely within the
cheap semiring K. They are based on ? and ? val-
ues obtained by first running INSIDE(HG,K) and
OUTSIDE(HG,K), which use only the k
e
part of
the weights and ignore the more expensive x
e
.
It is noteworthy that the expectation semiring is
not used at all by Figure 4. Although the return
value ?
?
k, x?? is in the expectation semiring, it is
built up not by ? and ? but rather by computing
?
k and x? separately. One might therefore wonder
why the expectation semiring and its operations
are still needed. One reason is that the input to
Figure 4 consists of hyperedge weights ?k
e
, x
e
? in
the expectation semiring?and these weights may
well have been constructed using ? and ?. For
example, Eisner (2002) uses finite-state operations
such as composition, which do combine weights
entirely within the expectation semiring before
their result is passed to the forward-backward al-
gorithm. A second reason is that when we work
with a second-order expectation semiring in Sec-
tion 4.4 below, the
?
k, ?, and ? values in Figure 4
will turn out to be elements of a first-order expec-
tation semiring, and they must still be constructed
by first-order ? and ?, via calls to Figures 2?3.
Why does inside-outside work? Whereas the
inside algorithm computes
?
d?D
?
e?d
in any
semiring, the inside-outside algorithm exploits
the special structure of an expectation semir-
ing. By that semiring?s definitions of ? and ?
(Table 1),
?
d?D
?
e?d
?k
e
, x
e
? can be found as
45
??
d?D
?
e?d
k
e
,
?
d?D
?
e?d
(
?
e
?
?d,e
?
6=e
k
e
?
)x
e
?.
The first component (giving
?
k) is found
by calling the inside algorithm on just the
k
e
part of the weights. The second com-
ponent (giving x?) can be rearranged into
?
e
?
d: e?d
(
?
e
?
?d,e
?
6=e
k
e
?
)x
e
=
?
e
k
e
x
e
, where
k
e
def
=
?
d: e?d
(
?
e
?
?d,e
?
6=e
k
e
?
) is found from ?, ?.
The application described at the start of this
subsection is the classical inside-outside algo-
rithm. Here ?k
e
, x
e
?
def
= ?p
e
, p
e
r
e
?, and the al-
gorithm returns ?
?
k, x?? = ?Z, r?. In fact, that
x? = r can be seen directly: r =
?
d
p(d)r(d) =
?
d
p(d)(
?
e?d
r
e
) =
?
e
?
d: e?d
p(d)r
e
=
?
e
(k
e
k
e
)r
e
=
?
e
k
e
x
e
= x?. This uses the fact
that k
e
k
e
=
?
d: e?d
p(d).
4.3 Lifting Trick for Second-Order Semirings
We now observe that the second-order expectation
semiring E
P,R,S,T
can be obtained indirectly by
nesting one first-order expectation semiring inside
another! First ?lift? P to obtain the first-order ex-
pectation semiring K
def
= E
P,R
. Then lift this a sec-
ond time to obtain the ?nested? first-order expec-
tation semiring E
K,X
= E
(E
P,R
),(S?T )
, where we
equip X
def
= S ? T with the operations ?s
1
, t
1
? +
?s
2
, t
2
?
def
= ?s
1
+ s
2
, t
1
+ t
2
? and ?p, r??s, t?
def
=
?ps, pt+ rs?. The resulting first-order expectation
semiring has elements of the form ??p, r?, ?s, t??.
Table 4 shows that it is indeed isomorphic to
E
P,R,S,T
, with corresponding elements ?p, r, s, t?.
This construction of the second-order semiring
as a first-order semiring is a useful bit of abstract
algebra, because it means that known properties
of first-order semirings will also apply to second-
order ones. First of all, we are immediately guar-
anteed that the second-order semiring satisfies the
semiring axioms. Second, we can directly apply
the inside-outside algorithm there, as we now see.
4.4 Inside-Outside Speedup for
Second-Order Expectation Semirings
Given a hypergraph weighted by a second-order
expectation semiring E
P,R,S,T
. By recasting this
as the first-order expectation semiringE
K,X
where
K = E
P,R
and X = (S ? T ), we can again ap-
ply INSIDE-OUTSIDE(HG,K, X) to find the total
weight of all derivations.
For example, to speed up Section 3.2, we
may define ?k
e
, x
e
? = ??p
e
, p
e
r
e
?, ?p
e
s
e
, p
e
r
e
s
e
??
for each hyperedge e. Then the inside-outside
algorithm of Figure 4 will compute ?
?
k, x?? =
??Z, r?, ?s, t??, more quickly than the inside algo-
rithm of Figure 2 computed ?Z, r, s, t?.
Figure 4 in this case will run the inside and
outside algorithms in the semiring E
P,R
, so that
k
e
,
?
k, ?, ?, and k
e
will now be elements of P ?R
(not just elements of P as in the first-order case).
Finally it finds x? =
?
e
k
e
x
e
, where x
e
? S?T .
9
This is a particularly effective speedup over
the inside algorithm when R consists of scalars
(or small vectors) whereas S, T are sparse high-
dimensional vectors. We will see exactly this case
in our experiments, where our weights ?p, r, s, t?
denote (probability, risk, gradient of probability,
gradient of risk), or (probability, entropy, gradient
of probability, gradient of entropy).
5 Finding Gradients on Hypergraphs
In Sections 3.2 and 4.1, we saw how our semirings
helped find the sum Z of all p(d), and compute
expectations r, s, t of r(d), s(d), and r(d)s(d).
It turns out that these semirings can also com-
pute first- and second-order partial derivatives of
all the above results, with respect to a parameter
vector ? ? R
m
. That is, we ask how they are
affected when ? changes slightly from its current
value. The elementary values p
e
, r
e
, s
e
are now
assumed to implicitly be functions of ?.
Case 1: Recall that Z
def
=
?
d
p(d) is com-
puted by INSIDE(HG,R) if each hyperedge e has
weight p
e
. ?Lift? this weight to ?p
e
,?p
e
?, where
?p
e
? R
m
is a gradient vector. Now ?Z,?Z? will
be returned by INSIDE(HG,E
R,R
m
)? or, more
efficiently, by INSIDE-OUTSIDE(HG,R,R
m
).
Case 2: To differentiate a second
time, ?lift? the above weights again
to obtain ??p
e
,?p
e
?,??p
e
,?p
e
?? =
??p
e
,?p
e
?, ??p
e
,?
2
p
e
??, where ?
2
p
e
? R
m?m
is the Hessian matrix of second-order mixed
partial derivatives. These weights are in a
second-order expectation semiring.
10
Now
9
Figure 4 was already proved generally correct in Sec-
tion 4.2. To understand more specifically how ?s, t? gets
computed, observe in analogy to the end of Section 4.2 that
?s, t? =
?
d
?p(d)s(d), p(d)r(d)s(d)?
=
?
d
?p(d), p(d)r(d)??s(d), 0?
=
?
d
?p(d), p(d)r(d)?
?
e?d
?s
e
, 0?
=
?
e
?
d: e?d
?p(d), p(d)r(d)??s
e
, 0?
=
?
e
(k
e
k
e
)?s
e
, 0? =
?
e
k
e
?p
e
, p
e
r
e
??s
e
, 0?
=
?
e
k
e
?p
e
s
e
, p
e
r
e
s
e
? =
?
e
k
e
x
e
= x?.
10
Modulo the trivial isomorphism from ??p, r?, ?s, t?? to
?p, r, s, t? (see Section 4.3), the intended semiring both here
and in Case 3 is the one that was defined at the start of Sec-
tion 4.1, in which r, s are vectors and their product is defined
46
??p
1
, r
1
?, ?s
1
, t
1
??? ??p
2
, r
2
?, ?s
2
, t
2
?? = ??p
1
, r
1
? + ?p
2
, r
2
?, ?s
1
, t
1
? + ?s
2
, t
2
??
= ??p
1
+ p
2
, r
1
+ r
2
?, ?s
1
+ s
2
, t
1
+ t
2
??
??p
1
, r
1
?, ?s
1
, t
1
??? ??p
2
, r
2
?, ?s
2
, t
2
?? = ??p
1
, r
1
??p
2
, r
2
?, ?p
1
, r
1
??s
2
, t
2
? + ?p
2
, r
2
??s
1
, t
1
??
= ??p
1
p
2
, p
1
r
2
+ p
2
r
1
?, ?p
1
s
2
+ p
2
s
1
, p
1
t
2
+ p
2
t
1
+ r
1
s
2
+ r
2
s
1
??
Table 4: Constructing second-order expectation semiring as first-order. Here we show that the operations in E
K,X
are
isomorphic to Table 2?s operations in E
P,R,S,T
, provided that K
def
= E
P,R
and X
def
= S ? T is a K-module, in which addition is
defined by?s
1
, t
1
? + ?s
2
, t
2
?
def
= ?s
1
+ s
2
, t
1
+ t
2
?, and left-multiplication by K is defined by ?p, r??s, t?
def
= ?ps, pt+ rs?.
?Z,?Z,?Z,?
2
Z? will be returned by
INSIDE(HG,E
R,R
m
,R
m
,R
m?m), or more effi-
ciently by INSIDE-OUTSIDE(HG,E
R,R
m
,R
m
?
R
m?m
).
Case 3: Our experiments will need to find ex-
pectations and their partial derivatives. Recall that
?Z, r? is computed by INSIDE(HG,E
R,R
n
) when
the edge weights are ?p
e
, p
e
r
e
? with r
e
? R
n
. Lift
these weights to ??p
e
, p
e
r
e
?,??p
e
, p
e
r
e
?? =
??p
e
, p
e
r
e
?, ??p
e
, (?p
e
)r
e
+ p
e
(?r
e
)??.
Now ?Z, r,?Z,?r? will be returned
by INSIDE(HG,E
R,R
n
,R
m
,R
n?m) or by
INSIDE-OUTSIDE(HG,E
R,R
n
,R
m
? R
n?m
).
11
5.1 What Connects Gradients to Expectations?
In Case 1, we claimed that the same algorithm
will compute either gradients ?Z,?Z? or expec-
tations ?Z, r?, if the hyperedge weights are set to
?p
e
,?p
e
? or ?p
e
, p
e
r
e
? respectively.
12
This may
seem wonderful and mysterious. We now show in
two distinct ways why this follows from our setup
of Section 3.1. At the end, we derive as a special
case the well-known relationship between gradi-
ents and expectations in log-linear models.
From Expectations to Gradients One perspec-
tive is that our semiring fundamentally finds ex-
pectations. Thus, we must be finding ?Z by for-
mulating it as a certain expectation r. Specif-
ically, ?Z = ?
?
d
p(d) =
?
d
?p(d) =
to be rs
T
, a matrix. However, when using this semiring to
compute second derivatives (Case 2) or covariances, one may
exploit the invariant that r = s, e.g., to avoid storing s and to
compute r
1
s
2
+ s
1
r
2
in multiplication simply as 2 ? r
1
r
2
.
11
Or, if n > m, it is faster to instead use
INSIDE-OUTSIDE(HG,E
R,R
m
,R
n
? R
m?n
), swapping the
second and third components of the 4-tuple and trans-
posing the matrix in the fourth component. Alge-
braically, this changes nothing because E
R,R
n
,R
m
?R
n?m
and
E
R,R
m
,R
n
?R
m?n
are isomorphic, thanks to symmetries in Ta-
ble 2. This method computes the expectation of the gradient
rather than the gradient of the expectation?they are equal.
12
Cases 2?3 relied on the fact that this relationship still
holds even when the scalars Z, p
e
? R are replaced by more
complex objects that we wish to differentiate. Our discus-
sion below sticks to the scalar case for simplicity, but would
generalize fairly straightforwardly. Pearlmutter and Siskind
(2007) give the relevant generalizations of dual numbers.
?
d
p(d)r(d) = r, provided that r(d) =
(?p(d))/p(d). That can be arranged by defining
r
e
def
= (?p
e
)/p
e
.
13
So that is why the input weights
?p
e
, p
e
r
e
? take the form ?p
e
,?p
e
?.
From Gradients to Expectations An alterna-
tive perspective is that our semiring fundamen-
tally finds gradients. Indeed, pairs like ?p,?p?
have long been used for this purpose (Clifford,
1873) under the name ?dual numbers.? Oper-
ations on dual numbers, including those in Ta-
ble 1, compute a result in R along with its gradi-
ent. For example, our ? multiplies dual numbers,
since ?p
1
,?p
1
? ? ?p
2
,?p
2
? = ?p
1
p
2
, p
1
(?p
2
) +
(?p
1
)p
2
? = ?p
1
p
2
,?(p
1
p
2
)?. The inside algo-
rithm thus computes both Z and ?Z in a single
?forward? or ?inside? pass?known as automatic
differentiation in the forward mode. The inside-
outside algorithm instead uses the reverse mode
(a.k.a. back-propagation), where a separate ?back-
ward? or ?outside? pass is used to compute?Z.
How can we modify this machinery to pro-
duce expectations r? given some arbitrary r
e
of interest? Automatic differentiation may
be used on any function (e.g., a neural net),
but for our simple sum-of-products function
Z, it happens that ?Z = ?(
?
d
?
e
p
e
) =
?
d
?
e?d
(
?
e
?
?d,e
?
6=e
p
e
?
)?p
e
. Our trick is to
surreptitiously replace the ?p
e
in the input
weights ?p
e
,?p
e
? with p
e
r
e
. Then the output
changes similarly: the algorithms will instead
find
?
d
?
e?d
(
?
e
?
?d,e
?
6=e
p
e
?
)p
e
r
e
, which re-
duces to
?
d
?
e?d
p(d)r
e
=
?
d
p(d)
?
e?d
r
e
=
?
d
p(d)r(d) = r?.
Log-linear Models as a Special Case Replac-
ing ?p
e
with p
e
r
e
is unnecessary if ?p
e
already
equals p
e
r
e
. That is the case in log-linear models,
where p
e
def
= exp(r
e
? ?) for some feature vector r
e
associated with e. So there, ?Z already equals
r??yielding a key useful property of log-linear
13
Proof: r(d) =
?
e?d
r
e
=
?
e?d
(?p
e
)/p
e
=
?
e?d
? log p
e
= ?
?
e?d
log p
e
= ? log
?
e?d
p
e
=
? log p(d) = (?p(d))/p(d).
47
models, that ? logZ = (?Z)/Z = r?/Z, the vec-
tor of feature expectations (Lau et al, 1993).
6 Practical Applications
Given a hypergraph HG whose hyperedges e are
annotated with values p
e
. Recall from Section 3.1
that this defines a probability distribution over all
derivations d in the hypergraph, namely p(d)/Z
where p(d)
def
=
?
e?d
p
e
.
6.1 First-Order Expectation Semiring E
R,R
In Section 3, we show how to compute the ex-
pected hypothesis length or expected feature
counts, using the algorithm of Figure 2 with a
first-order expectation semiring E
R,R
. In general,
given hyperedge weights ?p
e
, p
e
r
e
?, the algorithm
computes ?Z, r? and thus r/Z, the expectation of
r(d)
def
=
?
e?d
r
e
. We now show how to compute a
few other quantities by choosing r
e
appropriately.
Entropy on a Hypergraph The entropy of the
distribution of derivations in a hypergraph
14
is
H(p) = ?
?
d?D
(p(d)/Z) log(p(d)/Z) (5)
= logZ ?
1
Z
?
d?D
p(d) log p(d)
= logZ ?
1
Z
?
d?D
p(d)r(d) = logZ ?
r
Z
provided that we define r
e
def
= log p
e
(so that
r(d) =
?
e?d
r
e
= log p(d)). Of course, we can
compute ?Z, r? as explained in Section 3.2.
Cross-Entropy and KL Divergence We may
be interested in computing the cross-entropy or
KL divergence between two distributions p and q.
For example, in variational decoding for machine
translation (Li et al, 2009b), p is a distribution
represented by a hypergraph, while q, represented
by a finite state automaton, is an approximation to
p. The cross entropy between p and q is defined as
H(p, q) = ?
?
d?D
(p(d)/Z
p
) log(q(d)/Z
q
) (6)
= logZ
q
?
1
Z
p
?
d?D
p(d) log q(d)
= logZ
q
?
1
Z
p
?
d?D
p(d)r(d) = logZ
q
?
r
Z
p
14
Unfortunately, it is intractable to compute the entropy of
the distribution over strings (each string?s probability is a sum
over several derivations). But Li et al (2009b, section 5.4) do
estimate the gap between derivational and string entropies.
where the first term Z
q
can be computed using
the inside algorithm with hyperedge weights q
e
,
and the numerator and denominator of the sec-
ond term using an expectation semiring with hy-
peredge weights ?p
e
, p
e
r
e
? with r
e
def
= log q
e
.
The KL divergence to p from q can be computed
as KL(p ? q) = H(p, q)? H(p).
Expected Loss (Risk) Given a reference sen-
tence y
?
, the expected loss (i.e., Bayes risk) of the
hypotheses in the hypergraph is defined as,
R(p) =
?
d?D
(p(d)/Z)L(Y(d), y
?
) (7)
where Y(d) is the target yield of d and L(y, y
?
) is
the loss of the hypothesis y with respect to the ref-
erence y
?
. The popular machine translation met-
ric, BLEU (Papineni et al, 2001), is not additively
decomposable, and thus we are not able to com-
pute the expected loss for it. Tromble et al (2008)
develop the following loss function, of which a lin-
ear approximation to BLEU is a special case,
L(y, y
?
) = ?(?
0
|y|+
?
w?N
?
w
#
w
(y)?
w
(y?)) (8)
where w is an n-gram type, N is a set of n-gram
types with n ? [1, 4], #
w
(y) is the number of oc-
currence of the n-gramw in y, ?
w
(y
?
) is an indica-
tor to check if y
?
contains at least one occurrence
of w, and ?
n
is the weight indicating the relative
importance of an n-gram match. If the hypergraph
is already annotated with n-gram (n ? 4) lan-
guage model states, this loss function is additively
decomposable. Using r
e
def
= L
e
where L
e
is the
loss for a hyperedge e, we compute the expected
loss,
R(p) =
?
d?D
p(d)L(Y(d), y
?
)
Z
=
r
Z
(9)
6.2 Second-Order Expectation Semirings
With second-order expectation semirings, we can
compute from a hypergraph the expectation and
variance of hypothesis length; the feature expec-
tation vector and covariance matrix; the Hessian
(matrix of second derivatives) of Z; and the gradi-
ents of entropy and expected loss. The computa-
tions should be clear from earlier discussion. Be-
low we compute gradient of entropy or Bayes risk.
Gradient of Entropy or Risk It is easy to see
that the gradient of entropy (5) is
?H(p) =
?Z
Z
?
Z?r ? r?Z
Z
2
(10)
48
We may compute ?Z, r,?Z,?r? as ex-
plained in Case 3 of Section 5 by using
k
e
def
= ?p
e
, p
e
r
e
,?p
e
, (?p
e
)r
e
+ p
e
?r
e
?
def
=
?p
e
, p
e
log p
e
,?p
e
, (1 + log p
e
)?p
e
?, where ?p
e
depends on the particular parameterization of the
model (see Section 7.1 for an example).
Similarly, the gradient of risk of (9) is
?R(p) =
Z?r ? r?Z
Z
2
(11)
We may compute ?Z, r,?Z,?r? using k
e
def
=
?p
e
, p
e
L
e
,?p
e
, L
e
?p
e
?.
7 Minimum-Risk Training for MT
We now show how we improve the training of a
Hiero MT model by optimizing an objective func-
tion that includes entropy and risk. Our objective
function could be computed with a first-order ex-
pectation semiring, but computing it along with its
gradient requires a second-order one.
7.1 The Model p
We assume a globally normalized linear model
for its simplicity. Each derivation d is scored by
score(d)
def
= ?(d) ? ? =
?
i
?
i
(d) ?
i
(12)
where ?(d) ? R
m
is a vector of features of d. We
then define the unnormalized distribution p(d) as
p(d) = exp(? ? score(d)) (13)
where the scale factor ? adjusts how sharply the
distribution favors the highest-scoring hypotheses.
7.2 Minimum-Risk Training
Adjusting ? or ? changes the distribution p. Mini-
mum error rate training (MERT) (Och, 2003) tries
to tune ? to minimize the BLEU loss of a decoder
that chooses the most probable output according
to p. (? has no effect.) MERT?s specialized line-
search addresses the problem that this objective
function is piecewise constant, but it does not scale
to a large number of parameters.
Smith and Eisner (2006) instead propose a dif-
ferentiable objective that can be optimized by gra-
dient descent: the Bayes risk R(p) of (7). This is
the expected loss if one were (hypothetically) to
use a randomized decoder, which chooses a hy-
pothesis d in proportion to its probability p(d). If
entropy H(p) is large (e.g., small ?), the Bayes risk
is smooth and has few local minima. Thus, Smith
and Eisner (2006) try to avoid local minima by
starting with large H(p) and decreasing it gradu-
ally during optimization. This is called determin-
istic annealing (Rose, 1998). As H(p) ? 0 (e.g.,
large ?), the Bayes risk does approach the MERT
objective (i.e. minimizing 1-best error).The objec-
tive is
minimize R(p)? T ? H(p) (14)
where the ?temperature? T starts high and is ex-
plicitly decreased as optimization proceeds.
7.3 Gradient Descent Optimization
Solving (14) for a given T requires computing the
entropy H(p) and risk R(p) and their gradients
with respect to ? and ?. Smith and Eisner (2006)
followed MERT in constraining their decoder to
only an n-best list, so for them, computing these
quantities did not involve dynamic programming.
We compare those methods to training on a hy-
pergraph containing exponentially many hypothe-
ses. In this condition, we need our new second-
order semiring methods and must also approxi-
mate BLEU (during training only) by an additively
decomposable loss (Tromble et al, 2008).
15
Our algorithms require that p(d) of (13) is mul-
tiplicatively decomposable. It suffices to define
?(d)
def
=
?
e?d
?
e
, so that all features are local
to individual hyperedges; the vector ?
e
indicates
which features fire on hyperedge e. Then score(d)
of (12) is additively decomposable:
score(d) =
?
e?d
score
e
=
?
e?d
?
e
? ? (15)
We can then set p
e
= exp(? ? score
e
), and ?p
e
=
?p
e
?(e), and use the algorithms described in Sec-
tion 6 to compute H(p) and R(p) and their gradi-
ents with respect to ? and ?.
16
15
Pauls et al (2009) concurrently developed a method to
maximize the expected n-gram counts on a hypergraph using
gradient descent. Their objective is similar to the minimum
risk objective (though without annealing), and their gradient
descent optimization involves in algorithms in computing ex-
pected feature/n-gram counts as well as expected products of
features and n-gram counts, which can be viewed as instances
of our general algorithms with first- and second-order semir-
ings. They focused on tuning only a small number (i.e. nine)
of features as in a regular MERT setting, while our experi-
ments involve both a small and a large number of features.
16
It is easy to verify that the gradient of a function f (e.g.
entropy or risk) with respect to ? can be written as a weighted
sum of gradients with respect to the feature weights ?
i
, i.e.
?
f
?
?
=
1
?
?
i
?
i
?
?
f
?
?
i
(16)
49
7.4 Experimental Results
7.4.1 Experimental Setup
We built a translation model on a corpus for
IWSLT 2005 Chinese-to-English translation task
(Eck and Hori, 2005), which consists of 40k pairs
of sentences. We used a 5-gram language model
with modified Kneser-Ney smoothing, trained on
the bitext?s English using SRILM (Stolcke, 2002).
7.4.2 Tuning a Small Number of Features
We first investigate how minimum-risk training
(MR), with and without deterministic annealing
(DA), performs compared to regular MERT. MR
without DA just fixes T = 0 and ? = 1 in (14).
All MR or MR+DA uses an approximated BLEU
(Tromble et al, 2008) (for training only), while
MERT uses the exact corpus BLEU in training.
The first five rows in Table 5 present the results
by tuning the weights of five features (? ? R
5
). We
observe that MR or MR+DA performs worse than
MERT on the dev set. This may be mainly because
MR or MR+DA uses an approximated BLEU while
MERT doesn?t. On the test set, MR or MR+DA
on an n-best list is comparable to MERT. But our
new approach, MR or MR+DA on a hypergraph,
does consistently better (statistically significant)
than MERT, despite approximating BLEU.
17
Did DA help? For both n-best and hypergraph,
MR+DA did obtain a better BLEU score than plain
MR on the dev set.
18
This shows that DA helps
with the local minimum problem, as hoped. How-
ever, DA?s improvement on the dev set did not
transfer to the test set.
7.4.3 Tuning a Large Number of Features
MR (with or without DA) is scalable to tune a
large number of features, while MERT is not. To
achieve competitive performance, we adopt a for-
est reranking approach (Li and Khudanpur, 2009;
Huang, 2008). Specifically, our training has two
stages. In the first stage, we train a baseline system
as usual. We also find the optimal feature weights
for the five features mentioned before, using the
method of MR+DA operating on a hypergraph. In
the second stage, we generate a hypergraph for
each sentence in the training data (which consists
of about 40k sentence pairs), using the baseline
17
Pauls et al (2009) concurrently observed a similar pat-
tern (i.e., MR performs worse than MERT on the dev set, but
performs better on a test set).
18
We also verified that MR+DA found a better objective
value (i.e., expected loss on the dev set) than MR.
Training scheme dev test
MERT (Nbest, small) 42.6 47.7
MR (Nbest, small) 40.8 47.7
MR+DA (Nbest, small) 41.6 47.8
NEW! MR (hypergraph, small) 41.3 48.4
NEW! MR+DA (hypergraph, small) 41.9 48.3
NEW! MR (hypergraph, large) 42.3 48.7
Table 5: BLEU scores on the Dev and test sets under different
training scenarios. In the ?small? model, five features (i.e.,
one for the language model, three for the translation model,
and one for word penalty) are tuned. In the ?large? model,
21k additional unigram and bigram features are used.
system. In this stage, we add 21k additional uni-
gram and bigram target-side language model fea-
tures (cf. Li and Khudanpur (2008)). For example,
a specific bigram ?the cat? can be a feature. Note
that the total score by the baseline system is also
a feature in the second-stage model. With these
features and the 40k hypergraphs, we run the MR
training to obtain the optimal weights.
During test time, a similar procedure is fol-
lowed. For a given test sentence, the baseline sys-
tem first generates a hypergraph, and then the hy-
pergraph is reranked by the second-stage model.
The last row in Table 5 reports the BLEU scores.
Clearly, adding more features improves (statisti-
cally significant) the case with only five features.
We plan to incorporate more informative features
described by Chiang et al (2009).
19
8 Conclusions
We presented first-order expectation semirings
and inside-outside computation in more detail
than (Eisner, 2002), and developed extensions to
higher-order expectation semirings. This enables
efficient computation of many interesting quanti-
ties over the exponentially many derivations en-
coded in a hypergraph: second derivatives (Hes-
sians), expectations of products (covariances), and
expectations such as risk and entropy along with
their derivatives. To our knowledge, algorithms
for these problems have not been presented before.
Our approach is theoretically elegant, like other
work in this vein (Goodman, 1999; Lopez, 2009;
Gimpel and Smith, 2009). We used it practically to
enable a new form of minimum-risk training that
improved Chinese-English MT by 1.0 BLEU point.
Our implementation will be released within the
open-source MT toolkit Joshua (Li et al, 2009a).
19
Their MIRA training tries to favor a specific oracle
translation?indeed a specific tree?from the (pruned) hyper-
graph. MR does not commit to such an arbitrary choice.
50
References
J. K. Baker. 1979. Trainable grammars for speech
recognition. In Jared J. Wolf and Dennis H. Klatt,
editors, Speech Communication Papers Presented at
the 97th Meeting of the Acoustical Society of Amer-
ica, MIT, Cambridge, MA, June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
W. K. Clifford. 1873. Preliminary sketch of bi-
quaternions. Proceedings of the London Mathemat-
ical Society, 4:381?395.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In Proc. of the
International Workshop on Spoken Language Trans-
lation.
Jason Eisner, Eric Goldlust, and Noah A. Smith.
2005. Compiling comp ling: practical weighted
dynamic programming and the dyna language. In
HLT/EMNLP, pages 281?290.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1?8.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL, pages
205?208.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and ap-
plications. Discrete Appl. Math., 42(2-3):177?201.
Kevin Gimpel and Noah A. Smith. 2009. Cube
summing, approximate inference with non-local fea-
tures, and dynamic programming without semirings.
In EACL, pages 318?326.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Y Grandvalet and Y Bengio. 2004. Semi-supervised
learning by entropy minimization. In NIPS, pages
529?536.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL, pages 586?
594.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In ACL, pages
209?216.
Dan Klein and Christopher D. Manning. 2004. Pars-
ing and hypergraphs. New developments in parsing
technology, pages 351?372.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modelling using the maxi-
mum entropy principle. In Proc. ARPA Human Lan-
guage Technologies Workshop, pages 81?86.
Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA, pages 133?142.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In ACL, pages 609?616.
Adam Lopez. 2009. Translation as weighted deduc-
tion. In EACL, pages 532?540.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. In ACL, pages
311?318.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In EMNLP.
B. A. Pearlmutter and J. M. Siskind. 2007. Lazy mul-
tivariate higher-order forward-mode ad. In Proceed-
ings of the 34th Annual Symposium on Principles of
Programming Languages (POPL), pages 155?160.
Fernando C. N. Pereira and David H. D. Warren. 1983.
Parsing as deduction. In ACL, pages 137?144.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL, pages 271?279.
Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and
related optimization problems. In Proceedings of
the IEEE, pages 2210?2239.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1994. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In ACL,
pages 787?794.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum-Bayes-
risk decoding for statistical machine translation. In
EMNLP, pages 620?629.
51
Proceedings of NAACL HLT 2009: Short Papers, pages 9?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Extraction of Oracle-best Translations from Hypergraphs
Zhifei Li and Sanjeev Khudanpur
Center for Language and Speech Processing and Department of Computer Science
The Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and khudanpur@jhu.edu
Abstract
Hypergraphs are used in several syntax-
inspired methods of machine translation to
compactly encode exponentially many trans-
lation hypotheses. The hypotheses closest to
given reference translations therefore cannot
be found via brute force, particularly for pop-
ular measures of closeness such as BLEU. We
develop a dynamic program for extracting the
so called oracle-best hypothesis from a hyper-
graph by viewing it as the problem of finding
the most likely hypothesis under an n-gram
language model trained from only the refer-
ence translations. We further identify and re-
move massive redundancies in the dynamic
program state due to the sparsity of n-grams
present in the reference translations, resulting
in a very efficient program. We present run-
time statistics for this program, and demon-
strate successful application of the hypothe-
ses thus found as the targets for discriminative
training of translation system components.
1 Introduction
A hypergraph, as demonstrated by Huang and Chi-
ang (2007), is a compact data-structure that can en-
code an exponential number of hypotheses gener-
ated by a regular phrase-based machine translation
(MT) system (e.g., Koehn et al (2003)) or a syntax-
based MT system (e.g., Chiang (2007)). While the
hypergraph represents a very large set of transla-
tions, it is quite possible that some desired transla-
tions (e.g., the reference translations) are not con-
tained in the hypergraph, due to pruning or inherent
deficiency of the translation model. In this case, one
is often required to find the translation(s) in the hy-
pergraph that are most similar to the desired transla-
tions, with similarity computed via some automatic
metric such as BLEU (Papineni et al, 2002). Such
maximally similar translations will be called oracle-
best translations, and the process of extracting them
oracle extraction. Oracle extraction is a nontrivial
task because computing the similarity of any one
hypothesis requires information scattered over many
items in the hypergraph, and the exponentially large
number of hypotheses makes a brute-force linear
search intractable. Therefore, efficient algorithms
that can exploit the structure of the hypergraph are
required.
We present an efficient oracle extraction algo-
rithm, which involves two key ideas. Firstly, we
view the oracle extraction as a bottom-up model
scoring process on a hypergraph, where the model is
?trained? on the reference translation(s). This is sim-
ilar to the algorithm proposed for a lattice by Dreyer
et al (2007). Their algorithm, however, requires
maintaining a separate dynamic programming state
for each distinguished sequence of ?state? words and
the number of such sequences can be huge, mak-
ing the search very slow. Secondly, therefore, we
present a novel look-ahead technique, called equiv-
alent oracle-state maintenance, to merge multiple
states that are equivalent for similarity computation.
Our experiments show that the equivalent oracle-
state maintenance technique significantly speeds up
(more than 40 times) the oracle extraction.
Efficient oracle extraction has at least three im-
portant applications in machine translation.
Discriminative Training: In discriminative train-
ing, the objective is to tune the model parameters,
e.g. weights of a perceptron model or conditional
random field, such that the reference translations are
preferred over competitors. However, the reference
translations may not be reachable by the translation
system, in which case the oracle-best hypotheses
should be substituted in training.
9
System Combination: In a typical system combi-
nation task, e.g. Rosti et al (2007), each compo-
nent system produces a set of translations, which
are then grafted to form a confusion network. The
confusion network is then rescored, often employ-
ing additional (language) models, to select the fi-
nal translation. When measuring the goodness of a
hypothesis in the confusion network, one requires
its score under each component system. However,
some translations in the confusion network may not
be reachable by some component systems, in which
case a system?s score for the most similar reachable
translation serves as a good approximation.
Multi-source Translation: In a multi-source
translation task (Och and Ney, 2001) the input is
given in multiple source languages. This leads
to a situation analogous to system combination,
except that each component translation system now
corresponds to a specific source language.
2 Oracle Extraction on a Hypergraph
In this section, we present the oracle extraction al-
gorithm: it extracts one or more translations in a hy-
pergraph that have the maximum BLEU score1 with
respect to the corresponding reference translation(s).
The BLEU score of a hypothesis h relative to a
reference r may be expressed in the log domain as,
log BLEU(r, h) = min
[
1? |r||h| , 0
]
+
4?
n=1
1
4 log pn.
The first component is the brevity penalty when
|h|<|r|, while the second component corresponds to
the geometric mean of n-gram precisions pn (with
clipping). While BLEU is normally defined at the
corpus level, we use the sentence-level BLEU for
the purpose of oracle extraction.
Two key ideas for extracting the oracle-best hy-
pothesis from a hypergraph are presented next.
2.1 Oracle Extraction as Model Scoring
Our first key idea is to view the oracle extraction
as a bottom-up model scoring process on the hy-
pergraph. Specifically, we train a 4-gram language
model (LM) on only the reference translation(s),
1We believe our method is general and can be extended to
other metrics capturing only n-gram dependency and other com-
pact data structures, e.g. lattices.
and use this LM as the only model to do a Viterbi
search on the hypergraph to find the hypothesis that
has the maximum (oracle) LM score. Essentially,
the LM is simply a table memorizing the counts of
n-grams found in the reference translation(s), and
the LM score is the log-BLEU value (instead of log-
probability, as in a regular LM). During the search,
the dynamic programming (DP) states maintained
at each item include the left- and right-side LM
context, and the length of the partial translation.
To compute the n-gram precisions pn incrementally
during the search, the algorithm also memorizes at
each item a vector of maximum numbers of n-gram
matches between the partial translation and the ref-
erence(s). Note however that the oracle state of an
item (which decides the uniqueness of an item) de-
pends only on the LM contexts and span lengths, not
on this vector of n-gram match counts.
The computation of BLEU also requires the
brevity penalty, but since there is no explicit align-
ment between the source and the reference(s), we
cannot get the exact reference length |r| at an inter-
mediate item. The exact value of brevity penalty is
thus not computable. We approximate the true refer-
ence length for an item with a product between the
length of the source string spanned by that item and
a ratio (which is between the lengths of the whole
reference and the whole source sentence). Another
approximation is that we do not consider the effect
of clipping, since it is a global feature, making the
strict computation intractable. This does not signifi-
cantly affect the quality of the oracle-best hypothesis
as shown later. Table 1 shows an example how the
BLEU scores are computed in the hypergraph.
The process above may be used either in a first-
stage decoding or a hypergraph-rescoring stage. In
the latter case, if the hypergraph generated by the
first-stage decoding does not have a set of DP states
that is a superset of the DP states required for ora-
cle extraction, we need to split the items of the first-
stage hypergraph and create new items with suffi-
ciently detailed states.
It is worth mentioning that if the hypergraph items
contain the state information necessary for extract-
ing the oracle-best hypothesis, it is straightforward
to further extract the k-best hypotheses in the hyper-
graph (according to BLEU) for any k ? 1 using the
algorithm of Huang and Chiang (2005).
10
Item |h| |r?| matches log BLEU
Item A 5 6.2 (3, 2, 2, 1) -0.82
Item B 10 9.8 (8, 7, 6, 5) -0.27
Item C 17 18.3 (12, 10, 9, 6) -0.62
Table 1: Example computation when items A and B are
combined by a rule to produce item C. |r?| is the approxi-
mated reference length as described in the text.
2.2 Equivalent Oracle State Maintenance
The process above, while able to extract the oracle-
best hypothesis from a hypergraph, is very slow due
to the need to maintain a dedicated item for each or-
acle state (i.e., a combination of left-LM state, right-
LM state, and hypothesis length). This is especially
true if the baseline system uses a LM whose order is
smaller than four, since we need to split the items in
the original hypergraph into many sub-items during
the search. To speed up the extraction, our second
key idea is to maintain an equivalent oracle state.
Roughly speaking, instead of maintaining a dif-
ferent state for different language model words, we
collapse them into a single state whenever it does not
affect BLEU. For example, if we have two left-side
LM states a b c and a b d, and we know that
the reference(s) do not have any n-gram ending with
them, then we can reduce them both to a b and ig-
nore the last word. This is because the combination
of neither left-side LM state (a b c or a b d) can
contribute an n-gram match to the BLEU computa-
tion, regardless of which prefix in the hypergraph
they combine with. Similarly, if we have two right-
side LM states a b c and d b c, and if we know
that the reference(s) do not have any n-gram starting
with either, then we can ignore the first word and re-
duce them both to b c. We can continue this reduc-
tion recursively as shown in Figures 1 and 2, where
IS-A-PREFIX(emi ) (or IS-A-SUFFIX(ei1)) checks if
emi (resp. ei1) is a prefix (suffix) of any n-gram in
the reference translation(s). For BLEU, 1 ? n ? 4.
This equivalent oracle state maintenance tech-
nique, in practice, dramatically reduces the number
of distinct items preserved in the hypergraph for or-
acle extraction. To understand this, observe that if
all hypotheses in the hypergraph together contain m
unique n-grams, for any fixed n, then the total num-
ber of equivalent items takes a multiplicative factor
that is O(m2) due to left- and right-side LM state
EQ-L-STATE (em1 )
1 els? em1
2 for i? m to 1  right to left
3 if IS-A-SUFFIX(ei1)
4 break  stop reducing els
5 else
6 els? ei?11  reduce state
7 return els
Figure 1: Equivalent Left LM State Computation.
EQ-R-STATE (em1 )
1 ers? em1
2 for i? 1 to m  left to right
3 if IS-A-PREFIX (emi )
4 break  stop reducing ers
5 else
6 ers? emi+1  reduce state
7 return ers
Figure 2: Equivalent Right LM State Computation.
maintenance of Section 2.1. This multiplicative fac-
tor under the equivalent state maintenance above is
O(m?2), where m? is the number of unique n-grams
in the reference translations. Clearly, m?  m by
several orders of magnitude, leading to effectively
much fewer items to process in the chart.
One may view this idea of maintaining equivalent
states more generally as an outside look-ahead dur-
ing bottom-up inside parsing. The look-ahead uses
some external information, e.g. IS-A-SUFFIX(?), to
anticipate whether maintaining a detailed state now
will be of consequence later; if not then the in-
side parsing eliminates or collapses the state into
a coarser state. The technique proposed by Li and
Khudanpur (2008a) for decoding with large LMs is
a special case of this general theme.
3 Experimental Results
We report experimental results on a Chinese to En-
glish task, for a system that is trained using a similar
pipeline and data resource as in Chiang (2007).
3.1 Goodness of the Oracle-Best Translations
Table 2 reports the average speed (seconds/sentence)
for oracle extraction. Hypergraphs were generated
with a trigram LM and expanded on the fly for 4-
gram BLEU computation.
11
Basic DP Collapse equiv. states speed-up
25.4 sec/sent 0.6 sec/sent ? 42
Table 2: Speed of oracle extraction from hypergraphs.
The basic dynamic program (Sec. 2.1) improves signifi-
cantly by collapsing equivalent oracle states (Sec. 2.2).
Table 3 reports the goodness of the oracle-best hy-
potheses on three standard data sets. The highest
achievable BLEU score in a hypergraph is clearly
much higher than in the 500-best unique strings.
This shows that a hypergraph provides a much better
basis, e.g., for reranking than an n-best list.
As mentioned in Section 2.1, we use several ap-
proximations in computing BLEU (e.g., no clipping
and approximate reference length). To justify these
approximations, we first extract 500-best unique or-
acles from the hypergraph, and then rerank the ora-
cles based on the true sentence-level BLEU. The last
row of Table 3 reports the reranked one-best oracle
BLEU scores. Clearly, the approximations do not
hurt the oracle BLEU very much.
Hypothesis space MT?04 MT?05 MT?06
1-best (Baseline) 35.7 32.6 28.3
500-unique-best 44.0 41.2 35.1
Hypergraph 52.8 51.8 37.8
500-best oracles 53.2 52.2 38.0
Table 3: Baseline and oracle-best 4-gram BLEU scores
with 4 references for NIST Chinese-English MT datasets.
3.2 Discriminative Hypergraph-Reranking
Oracle extraction is a critical component for
hypergraph-based discriminative reranking, where
millions of model parameters are discriminatively
tuned to prefer the oracle-best hypotheses over oth-
ers. Hypergraph-reranking in MT is similar to the
forest-reranking for monolingual parsing (Huang,
2008). Moreover, once the oracle-best hypothesis
is identified, discriminative models may be trained
on hypergraphs in the same way as on n-best lists
(cf e.g. Li and Khudanpur (2008b)). The results in
Table 4 demonstrate that hypergraph-reranking with
a discriminative LM or TM improves upon the base-
line models on all three test sets. Jointly training
both the LM and TM likely suffers from over-fitting.
Test Set MT?04 MT?05 MT?06
Baseline 35.7 32.6 28.3
Discrim. LM 35.9 33.0 28.2
Discrim. TM 36.1 33.2 28.7
Discrim. TM+LM 36.0 33.1 28.6
Table 4: BLEU scores after discriminative hypergraph-
reranking. Only the language model (LM) or the transla-
tion model (TM) or both (LM+TM) may be discrimina-
tively trained to prefer the oracle-best hypotheses.
4 Conclusions
We have presented an efficient algorithm to extract
the oracle-best translation hypothesis from a hyper-
graph. To this end, we introduced a novel technique
for equivalent oracle state maintenance, which sig-
nificantly speeds up the oracle extraction process.
Our algorithm has clear applications in diverse tasks
such as discriminative training, system combination
and multi-source translation.
References
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In Proc. of SSST.
L. Huang. 2008. Forest Reranking: Discriminative Pars-
ing with Non-Local Features. In Proc. of ACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. of IWPT.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
of ACL.
P. Koehn, F. J. Och, and D. Marcu.2003. Statistical
phrase-based translation. In Proc. of NAACL.
Z. Li and S. Khudanpur. 2008a. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. In Proc. SSST.
Z. Li and S. Khudanpur. 2008b. Large-scale Discrimina-
tive n-gram Language Models for Statistical Machine
Translation. In Proc. of AMTA.
F. Och and H. Ney. 2001. Statistical multisource transla-
tion. In Proc. MT Summit VIII.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
A.I. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In Proc. of ACL.
12
Proceedings of ACL-08: HLT, pages 425?433,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Translation Induction for Chinese Abbreviations
using Monolingual Corpora
Zhifei Li and David Yarowsky
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and yarowsky@cs.jhu.edu
Abstract
Chinese abbreviations are widely used in
modern Chinese texts. Compared with
English abbreviations (which are mostly
acronyms and truncations), the formation of
Chinese abbreviations is much more complex.
Due to the richness of Chinese abbreviations,
many of them may not appear in available par-
allel corpora, in which case current machine
translation systems simply treat them as un-
known words and leave them untranslated. In
this paper, we present a novel unsupervised
method that automatically extracts the relation
between a full-form phrase and its abbrevia-
tion from monolingual corpora, and induces
translation entries for the abbreviation by us-
ing its full-form as a bridge. Our method does
not require any additional annotated data other
than the data that a regular translation system
uses. We integrate our method into a state-of-
the-art baseline translation system and show
that it consistently improves the performance
of the baseline system on various NIST MT
test sets.
1 Introduction
The modern Chinese language is a highly abbrevi-
ated one due to the mixed use of ancient single-
character words with modern multi-character words
and compound words. According to Chang and Lai
(2004), approximately 20% of sentences in a typical
news article have abbreviated words in them. Ab-
breviations have become even more popular along
with the development of Internet media (e.g., online
chat, weblog, newsgroup, and so on). While En-
glish words are normally abbreviated by either their
Full-form Abbreviation Translation
&? ? ?? Hong Kong Governor
?\ ?/? ??? Security Council
Figure 1: Chinese Abbreviations Examples
first letters (i.e. acronyms) or via truncation, the for-
mation of Chinese abbreviations is much more com-
plex. Figure 1 shows two examples for Chinese ab-
breviations. Clearly, an abbreviated form of a word
can be obtained by selecting one or more characters
from this word, and the selected characters can be at
any position in the word. In an extreme case, there
are even re-ordering between a full-form phrase and
its abbreviation.
While the research in statistical machine trans-
lation (SMT) has made significant progress, most
SMT systems (Koehn et al, 2003; Chiang, 2007;
Galley et al, 2006) rely on parallel corpora to extract
translation entries. The richness and complexness
of Chinese abbreviations imposes challenges to the
SMT systems. In particular, many Chinese abbrevi-
ations may not appear in available parallel corpora,
in which case current SMT systems treat them as
unknown words and leave them untranslated. This
affects the translation quality significantly.
To be able to translate a Chinese abbreviation that
is unseen in available parallel corpora, one may an-
notate more parallel data. However, this is very
expensive as there are too many possible abbrevia-
tions and new abbreviations are constantly created.
Another approach is to transform the abbreviation
425
into its full-form for which the current SMT system
knows how to translate. For example, if the baseline
system knows that the translation for ?&??? is
?Hong Kong Governor?, and it also knows that ??
?? is an abbreviation of ?&? ?? , then it can
translate ???? to ?Hong Kong Governor?.
Even if an abbreviation has been seen in parallel
corpora, it may still be worth to consider its full-
form phrase as an additional alternative to the ab-
breviation since abbreviated words are normally se-
mantically ambiguous, while its full-form contains
more context information that helps the MT system
choose a right translation for the abbreviation.
Conceptually, the approach of translating an ab-
breviation by using its full-form as a bridge in-
volves four components: identifying abbreviations,
learning their full-forms, inducing their translations,
and integrating the abbreviation translations into the
baseline SMT system. None of these components is
trivial to realize. For example, for the first two com-
ponents, we may need manually annotated data that
tags an abbreviation with its full-form. We also need
to make sure that the baseline system has at least
one valid translation for the full-form phrase. On
the other hand, integrating an additional component
into a baseline SMT system is notoriously tricky as
evident in the research on integrating word sense
disambiguation (WSD) into SMT systems: different
ways of integration lead to conflicting conclusions
on whether WSD helps MT performance (Chan et
al., 2007; Carpuat and Wu, 2007).
In this paper, we present an unsupervised ap-
proach to translate Chinese abbreviations. Our ap-
proach exploits the data co-occurrence phenomena
and does not require any additional annotated data
except the parallel and monolingual corpora that the
baseline SMT system uses. Moreover, our approach
integrates the abbreviation translation component
into the baseline system in a natural way, and thus is
able to make use of the minimum-error-rate training
(Och, 2003) to automatically adjust the model pa-
rameters to reflect the change of the integrated sys-
tem over the baseline system. We carry out experi-
ments on a state-of-the-art SMT system, i.e., Moses
(Koehn et al, 2007), and show that the abbreviation
translations consistently improve the translation per-
formance (in terms of BLEU (Papineni et al, 2002))
on various NIST MT test sets.
2 Background: Chinese Abbreviations
In general, Chinese abbreviations are formed based
on three major methods: reduction, elimination and
generalization (Lee, 2005; Yin, 1999). Table 1
presents examples for each category.
Among the three methods, reduction is the most
popular one, which generates an abbreviation by
selecting one or more characters from each of the
words in the full-form phrase. The selected char-
acters can be at any position of the word. Table 1
presents examples to illustrate how characters at dif-
ferent positions are selected to generate abbrevia-
tions. While the abbreviations mostly originate from
noun phrases (in particular, named entities), other
general phrases are also abbreviatable. For example,
the second example ?Save Energy? is a verb phrase.
In an extreme case, reordering may happen between
an abbreviation and its full-form phrase. For exam-
ple, for the seventh example in Table 1, a monotone
abbreviation should be ?X??, however, ?X
?? is a more popular ordering in Chinese texts.
In elimination, one or more words of the origi-
nal full-form phrase are eliminated and the rest parts
remain as an abbreviation. For example, in the full-
form phrase ?8?L??, the word ?L?? is elim-
inated and the remaining word ?8?? alone be-
comes the abbreviation.
In generalization, an abbreviation is created
by generalizing parallel sub-parts of the full-form
phrase. For example, ??3 (three preventions)? in
Table 1 is an abbreviation for the phrase ?3?3
x3b//? (fire prevention, theft prevention,
and traffic accident prevention)?. The character ?3
(prevention)? is common to the three sub-parts of the
full-form, so it is being generalized.
3 Unsupervised Translation Induction for
Chinese Abbreviations
In this section, we describe an unsupervised method
to induce translation entries for Chinese abbrevia-
tions, even when these abbreviations never appear in
the Chinese side of the parallel corpora. Our basic
idea is to automatically extract the relation between
a full-form phrase and its abbreviation (we refer the
relation as full-abbreviation) from monolingual cor-
pora, and then induce translation entries for the ab-
breviation by using its full-form phrase as a bridge.
426
Category Full-form Abbreviation Translation
Reduction ?? L? ?L Peking University
?? ? ? Save Energy
&? ? ?? Hong Kong Governor
ib \? i? Foreign Minister
|? ?? ?? People?s Police
?\ ?/? ??? Security Council
? X ?? X? No.1 Nuclear Energy Power Plant
Elimination 8? L? 8? Tsinghua University
Generalization 3?3x3b//? ?3 Three Preventions
Table 1: Chinese Abbreviation: Categories and Examples
Our approach involves five major steps:
? Step-1: extract a list of English entities from
English monolingual corpora;
? Step-2: translate the list into Chinese using a
baseline translation system;
? Step-3: extract full-abbreviation relations from
Chinese monolingual corpora by treating the
Chinese translations obtained in Step-2 as full-
form phrases;
? Step-4: induce translation entries for Chinese
abbreviations by using their full-form phrases
as bridges;
? Step-5: augment the baseline system with
translation entries obtained in Step-4.
Clearly, the main purpose of Step-1 and -2 is to
obtain a list of Chinese entities, which will be treated
as full-form phrases in Step-3. One may use a named
entity tagger to obtain such a list. However, this re-
lies on the existence of a Chinese named entity tag-
ger with high-precision. Moreover, obtaining a list
using a dedicated tagger does not guarantee that the
baseline system knows how to translate the list. On
the contrary, in our approach, since the Chinese en-
tities are translation outputs for the English entities,
it is ensured that the baseline system has translations
for these Chinese entities.
Regarding the data resource used, Step-1, -2, and
-3 rely on the English monolingual corpora, paral-
lel corpora, and the Chinese monolingual corpora,
respectively. Clearly, our approach does not re-
quire any additional annotated data compared with
the baseline system. Moreover, our approach uti-
lizes both Chinese and English monolingual data
to help MT, while most SMT systems utilizes only
the English monolingual data to build a language
model. This is particularly interesting since we nor-
mally have enormous monolingual data, but a small
amount of parallel data. For example, in the transla-
tion task between Chinese and English, both the Chi-
nese and English Gigaword have billions of words,
but the parallel data has only about 30 million words.
Step-4 and -5 are natural ways to integrate the ab-
breviation translation component with the baseline
translation system. This is critical to make the ab-
breviation translation get performance gains over the
baseline system as will be clear later.
In the remainder of this section, we will present a
specific instantiation for each step.
3.1 English Entity Extraction from English
Monolingual Corpora
Though one can exploit a sophisticated named-entity
tagger to extract English entities, in this paper we
identify English entities based on the capitalization
information. Specifically, to be considered as an en-
tity, a continuous span of English words must satisfy
the following conditions:
? all words must start from a capital letter except
for function words ?of?, ?the?, and ?and?;
? each function word can appear only once;
? the number of words in the span must be
smaller than a threshold (e.g., 10);
? the occurrence count of this span must be
greater than a threshold (e.g., 1).
427
3.2 English Entity Translation
For the Chinese-English language pair, most MT re-
search is on translation from Chinese to English, but
here we need the reverse direction. However, since
most of statistical translation models (Koehn et al,
2003; Chiang, 2007; Galley et al, 2006) are sym-
metrical, it is relatively easy to train a translation
system to translate from English to Chinese, except
that we need to train a Chinese language model from
the Chinese monolingual data.
It is worth pointing out that the baseline system
may not be able to translate all the English enti-
ties. This is because the entities are extracted from
the English monolingual corpora, which has a much
larger vocabulary than the English side of the par-
allel corpora. Therefore, we should remove all the
Chinese translations that contain any untranslated
English words before proceeding to the next step.
Moreover, it is desirable to generate an n-best list
instead of a 1-best translation for the English entity.
3.3 Full-abbreviation Relation Extraction from
Chinese Monolingual Corpora
We treat the Chinese entities obtained in Section 3.2
as full-form phrases. To identify their abbreviations,
one can employ an HMM model (Chang and Teng,
2006). Here we propose a much simpler approach,
which is based on the data co-occurrence intuition.
3.3.1 Data Co-occurrence
In a monolingual corpus, relevant words tend to
appear together (i.e., co-occurrence). For example,
Bill Gates tends to appear together with Microsoft.
The co-occurrence may imply a relationship (e.g.,
Bill Gates is the founder of Microsoft). By inspec-
tion of the Chinese text, we found that the data
co-occurrence phenomena also applies to the full-
Title ?????*R?<??
Text c???2?9??(V?c?
?)?20?????{?*R?
h?-10?t8??????.
??t*y ?{??
Table 2: Data Co-occurrence Example for the Full-
abbreviation Relation (????,???) meaning
?winter olympics?
abbreviation relation. Table 2 shows an example,
where the abbreviation ????? appears in the title
while its full-form ?????? appears in the text
of the same document. In general, the occurrence
distance between an abbreviation and its full-form
varies. For example, they may appear in the same
sentence, or in the neighborhood sentences.
3.3.2 Full-abbreviation Relation Extraction
Algorithm
By exploiting the data co-occurrence phenom-
ena, we identify possible abbreviations for full-form
phrases. Figure 2 presents the pseudocode of the
full-abbreviation relation extraction algorithm.
Relation-Extraction(Corpus ,Full-list)
1 contexts ? NIL
2 for i ? 1 to length[Corpus]
3 sent1 ? Corpus[i ]
4 contexts ? UPDATE(contexts ,Corpus , i)
5 for full in sent1
6 if full in Full-list
7 for sent2 in contexts
8 for abbr in sent2
9 if RL(full , abbr ) = TRUE
10 Count[abbr , full]++
11 return Count
Figure 2: Full-abbreviation Relation Extraction
Given a monolingual corpus and a list of full-form
phrases (i.e., Full-list, which is obtained in Sec-
tion 3.2), the algorithm returns a Count that con-
tains full-abbreviation relations and their occurrence
counts. Specifically, the algorithm linearly scans
over the whole corpus as indicated by line 1. Along
the linear scan, the algorithm maintains contexts of
the current sentence (i.e., sent1), and the contexts
remember the sentences from where the algorithm
identifies possible abbreviations. In our implemen-
tation, the contexts include current sentence, the ti-
tle of current document, and previous and next sen-
tence in the document. Then, for each ngram (i.e.,
full) of the current sentence (i.e., sent1) and for each
ngram (i.e., abbr) of a context sentence (i.e., sent2),
the algorithm calls a function RL, which decides
whether the full-abbreviation relation holds between
full and abbr. If RL returns TRUE, the count table
428
(i.e., Count) is incremented by one for this relation.
Note that the filtering through the full-form phrases
list (i.e., Full-list) as shown in line 6 is the key to
make the algorithm efficient enough to run through
large-size monolingual corpora.
In function RL, we run a simple alignment algo-
rithm that links the characters in abbr with the words
in full. In the alignment, we assume that there is no
reordering between full and abbr. To be considered
as a valid full-abbreviation relation, full and abbr
must satisfy the following conditions:
? abbr must be shorter than full by a relative
threshold (e.g., 1.2);
? each character in abbr must be aligned to full;
? each word in full must have at least one charac-
ter aligned to abbr;
? abbr must not be a continuous sub-part of full;
Clearly, due to the above conditions, our approach
may not be able to handle all possible abbreviations
(e.g., the abbreviations formed by the generalization
method described in Section 2). One can modify
the conditions and the alignment algorithm to handle
more complex full-abbreviation relations.
With the count table Count, we can calculate the
relative frequency and get the following probability,
P (full|abbr) = Count[abbr, full]?Count[abbr, ?] (1)
3.4 Translation Induction for Chinese
Abbreviations
Given a Chinese abbreviation and its full-form, we
induce English translation entries for the abbrevia-
tion by using the full-form as a bridge. Specifically,
we first generate n-best translations for each full-
form Chinese phrase using the baseline system.1 We
then post-process the translation outputs such that
they have the same format (i.e., containing the same
set of model features) as a regular phrase entry in
1In our method, it is guaranteed that each Chinese full-form
phrase will have at least one English translation, i.e., the En-
glish entity that has been used to produce this full-form phrase.
However, it does not mean that this English entity is the best
translation that the baseline system has for the Chinese full-
form phrase. This is mainly due to the asymmetry introduced
by the different LMs in different translation directions.
the baseline phrase table. Once we get the transla-
tion entries for the full-form, we can replace the full-
form Chinese with its abbreviation to generate trans-
lation entries for the abbreviation. Moreover, to deal
with the case that an abbreviation may have several
candidate full-form phrases, we normalize the fea-
ture values using the following equation,
?j(e, abbr) = ?j(e, full)? P (full|abbr) (2)
where e is an English translation, and ?j is the j-th
model feature indexed as in the baseline system.
3.5 Integration with Baseline Translation
System
Since the obtained translation entries for abbrevia-
tions have the same format as the regular transla-
tion entries in the baseline phrase table, it is rela-
tively easy to add them into the baseline phrase ta-
ble. Specifically, if a translation entry (signatured by
its Chinese and English strings) to be added is not in
the baseline phrase table, we simply add the entry
into the baseline table. On the other hand, if the en-
try is already in the baseline phrase table, then we
merge the entries by enforcing the translation prob-
ability as we obtain the same translation entry from
two different knowledge sources (one is from par-
allel corpora and the other one is from the Chinese
monolingual corpora).
Once we obtain the augmented phrase table, we
should run the minimum-error-rate training (Och,
2003) with the augmented phrase table such that the
model parameters are properly adjusted. As will be
shown in the experimental results, this is critical to
obtain performance gain over the baseline system.
4 Experimental Results
4.1 Corpora
We compile a parallel dataset which consists of var-
ious corpora distributed by the Linguistic Data Con-
sortium (LDC) for NIST MT evaluation. The paral-
lel dataset has about 1M sentence pairs, and about
28M words. The monolingual data we use includes
the English Gigaword V2 (LDC2005T12) and the
Chinese Gigaword V2 (LDC2005T14).
4.2 Baseline System Training
Using the toolkit Moses (Koehn et al, 2007), we
built a phrase-based baseline system by following
429
the standard procedure: running GIZA++ (Och and
Ney, 2000) in both directions, applying refinement
rules to obtain a many-to-many word alignment, and
then extracting and scoring phrases using heuristics
(Och and Ney, 2004). The baseline system has eight
feature functions (see Table 8). The feature func-
tions are combined under a log-linear framework,
and the weights are tuned by the minimum-error-rate
training (Och, 2003) using BLEU (Papineni et al,
2002) as the optimization metric.
To handle different directions of translation be-
tween Chinese and English, we built two tri-
gram language models with modified Kneser-Ney
smoothing (Chen and Goodman, 1998) using the
SRILM toolkit (Stolcke, 2002).
4.3 Statistics on Intermediate Steps
As described in Section 3, our approach involves
five major steps. Table 3 reports the statistics for
each intermediate step. While about 5M English en-
tities are extracted and 2-best Chinese translations
are generated for each English entity, we get only
4.7M Chinese entities. This is because many of the
English entities are untranslatable by the baseline
system. The number of full-abbreviation relations2
extracted from the Chinese monolingual corpora is
51K. For each full-form phrase we generate 5-best
English translations, however only 210k (<51K?5)
translation entries are obtained. This is because the
baseline system may have less than 5 unique trans-
lations for some of the full-form phrases. Lastly, the
number of translation entries added due to abbrevi-
ations is very small compared with the total number
of translation entries (i.e., 50M).
Measure Value
number of English entities 5M
number of Chinese entities 4.7M
number of full-abbreviation relations 51K
number of translation entries added 210K
total number of translation entries 50M
Table 3: Statistics on Intermediate Steps
2Note that many of the ?abbreviations? extracted by our al-
gorithm are not true abbreviations in the linguistic sense, instead
they are just continuous-span of words. This is analogous to the
concept of ?phrase? in phrase-based MT.
4.4 Precision on Full-abbreviation Relations
Table 4 reports the precision on the extracted full-
abbreviation relations. We classify the relations into
several classes based on their occurrence counts. In
the second column, we list the fraction of the rela-
tions in the given class among all the relations we
have extracted (i.e., 51K relations). For each class,
we randomly select 100 relations, manually tag them
as correct or wrong, and then calculate the precision.
Intuitively, a class that has a higher occurrence count
should have a higher precision, and this is generally
true as shown in the fourth column of Table 4. In
comparison, Chang and Teng (2006) reports a preci-
sion of 50% over relations between single-word full-
forms and single-character abbreviations. One can
imagine a much lower precision on general relations
(e.g., the relations between multi-word full-forms
and multi-character abbreviations) that we consider
here. Clearly, our results are very competitive3.
Count Fraction (%) Precision (%)Baseline Ours
(0, 1] 35.2 8.9 42.6
(1, 5] 33.8 7.8 54.4
(5, 10] 10.7 8.9 60.0
(10, 100] 16.5 7.6 55.9
(100,+?) 3.8 12.1 59.9
Average Precision (%) 8.4 51.3
Table 4: Full-abbreviation Relation Extraction Precision
To further show the advantage of our relation ex-
traction algorithm (see Section 3.3), in the third col-
umn of Table 4 we report the results on a simple
baseline. To create the baseline, we make use of the
dominant abbreviation patterns shown in Table 5,
which have been reported in Chang and Lai (2004).
The abbreviation pattern is represented using the
format ?(bit pattern|length)? where the bit pattern
encodes the information about how an abbreviated
form is obtained from its original full-form word,
and the length represents the number of characters in
the full-form word. In the bit pattern, a ?1? indicates
that the character at the corresponding position of
the full-form word is kept in the abbreviation, while
a ?0? means the character is deleted. Now we dis-
3However, it is not a strict comparison because the dataset is
different and the recall may also be different.
430
Pattern Fraction (%) Example
(1|1) 100 (?,?)
(10|2) 87 (??,?)
(101|3) 44 (?/?,??)
(1010|4) 56 (??=?,?=)
Table 5: Dominant Abbreviation Patterns reported in
Chang and Lai (2004)
cuss how to create the baseline. For each full-form
phrase in the randomly selected relations, we gener-
ate a baseline hypothesis (i.e., abbreviation) as fol-
lows. We first generate an abbreviated form for each
word in the full-form phrase by using the dominant
abbreviation pattern, and then concatenate these ab-
breviated words to form a baseline abbreviation for
the full-form phrase. As shown in Table 4, the base-
line performs significantly worse than our relation
extraction algorithm. Compared with the baseline,
our relation extraction algorithm allows arbitrary ab-
breviation patterns as long as they satisfy the align-
ment constraints. Moreover, our algorithm exploits
the data co-occurrence phenomena to generate and
rank hypothesis (i.e., abbreviation). The above two
reasons explain the large performance gain.
It is interesting to examine the statistics on abbre-
viation patterns over the relations automatically ex-
tracted by our algorithm. Table 6 reports the statis-
tics. We obtain the statistics on the relations that
are manually tagged as correct before, and there are
in total 263 unique words in the corresponding full-
form phrases. Note that the results here are highly
biased to our relation extraction algorithm (see Sec-
tion 3.3). For the statistics on manually collected
examples, please refer to Chang and Lai (2004).
4.5 Results on Translation Performance
4.5.1 Precision on Translations of Chinese
Full-form Phrases
For the relations manually tagged as correct in
Section 4.4, we manually look at the top-5 transla-
tions for the full-form phrases. If the top-5 transla-
tions contain at least one correct translation, we tag
it as correct, otherwise as wrong. We get a precision
of 97.5%. This precision is extremely high because
the BLEU score (precision with brevity penalty) that
one obtains for a Chinese sentence is normally be-
tween 30% to 50%. Two reasons explain such a high
Pattern Fraction (%) Example
(1|1) 100 (?,?)
(10|2) 74.3 (??,?)
(01|2) 7.6 (??,?)
(11|2) 18.1 ( j, j)
(100|3) 58.5 (n.,)
(010|3) 3.1 (qu?,u)
(001|3) 4.6 (???,?)
(110|3) 13.8 (???,??)
(101|3) 3.1 (?/?,??)
(111|3) 16.9 ()?,)?)
Table 6: Statistics on Abbreviation Patterns
precision. Firstly, the full-form phrase is short com-
pared with a regular Chinese sentence, and thus it is
easier to translate. Secondly, the full-form phrase it-
self contains enough context information that helps
the system choose a right translation for it. In fact,
this shows the importance of considering the full-
form phrase as an additional alternative to the ab-
breviation even if the baseline system already has
translation entries for the abbreviation.
4.5.2 BLEU on NIST MT Test Sets
We use MT02 as the development set4 for mini-
mum error rate training (MERT) (Och, 2003). The
MT performance is measured by lower-case 4-gram
BLEU (Papineni et al, 2002). Table 7 reports the re-
sults on various NIST MT test sets. As shown in the
table, our Abbreviation Augmented MT (AAMT)
systems perform consistently better than the base-
line system (described in Section 4.2).
Task Baseline AAMTNo MERT With MERT
MT02 29.87 29.96 30.46
MT03 29.03 29.23 29.71
MT04 29.05 29.88 30.55
Average Gain +0.52 +1.18
Table 7: MT Performance measured by BLEU Score
As clear in Table 7, it is important to re-run MERT
(on MT02 only) with the augmented phrase table
in order to get performance gains. Table 8 reports
4On the dev set, about 20K (among 210K) abbreviation
translation entries are matched in the Chinese side.
431
the MERT weights with different phrase tables. One
may notice the change of the weight in word penalty
feature. This is very intuitive in order to prevent the
hypothesis being too long due to the expansion of
the abbreviations into their full-forms.
Feature Baseline AAMT
language model 0.137 0.133
phrase translation 0.066 0.023
lexical translation 0.061 0.078
reverse phrase translation 0.059 0.103
reverse lexical translation 0.112 0.090
phrase penalty -0.150 -0.162
word penalty -0.327 -0.356
distortion model 0.089 0.055
Table 8: Weights obtained by MERT
5 Related Work
Though automatically extracting the relations be-
tween full-form Chinese phrases and their abbrevi-
ations is an interesting and important task for many
natural language processing applications (e.g., ma-
chine translation, question answering, information
retrieval, and so on), not much work is available
in the literature. Recently, Chang and Lai (2004),
Chang and Teng (2006), and Lee (2005) have in-
vestigated this task. Specifically, Chang and Lai
(2004) describes a hidden markov model (HMM) to
model the relationship between a full-form phrase
and its abbreviation, by treating the abbreviation as
the observation and the full-form words as states in
the model. Using a set of manually-created full-
abbreviation relations as training data, they report
experimental results on a recognition task (i.e., given
an abbreviation, the task is to obtain its full-form, or
the vice versa). Clearly, their method is supervised
because it requires the full-abbreviation relations as
training data.5 Chang and Teng (2006) extends the
work in Chang and Lai (2004) to automatically ex-
tract the relations between full-form phrases and
their abbreviations. However, they have only con-
sidered relations between single-word phrases and
single-character abbreviations. Moreover, the HMM
model is computationally-expensive and unable to
exploit the data co-occurrence phenomena that we
5However, the HMM model aligns the characters in the ab-
breviation to the words in the full-form in an unsupervised way.
have exploited efficiently in this paper. Lee (2005)
gives a summary about how Chinese abbreviations
are formed and presents many examples. Manual
rules are created to expand an abbreviation to its full-
form, however, no quantitative results are reported.
None of the above work has addressed the Chi-
nese abbreviation issue in the context of a machine
translation task, which is the primary goal in this
paper. To the best of our knowledge, our work is
the first to systematically model Chinese abbrevia-
tion expansion to improve machine translation.
The idea of using a bridge (i.e., full-form) to ob-
tain translation entries for unseen words (i.e., abbre-
viation) is similar to the idea of using paraphrases in
MT (see Callison-Burch et al (2006) and references
therein) as both are trying to introduce generaliza-
tion into MT. At last, the goal that we aim to exploit
monolingual corpora to help MT is in-spirit similar
to the goal of using non-parallel corpora to help MT
as aimed in a large amount of work (see Munteanu
and Marcu (2006) and references therein).
6 Conclusions
In this paper, we present a novel method that
automatically extracts relations between full-form
phrases and their abbreviations from monolingual
corpora, and induces translation entries for these ab-
breviations by using their full-form as a bridge. Our
method is scalable enough to handle large amount
of monolingual data, and is essentially unsupervised
as it does not require any additional annotated data
than the baseline translation system. Our method
exploits the data co-occurrence phenomena that is
very useful for relation extractions. We integrate our
method into a state-of-the-art phrase-based baseline
translation system, i.e., Moses (Koehn et al, 2007),
and show that the integrated system consistently im-
proves the performance of the baseline system on
various NIST machine translation test sets.
Acknowledgments
We would like to thank Yi Su, Sanjeev Khudan-
pur, Philip Resnik, Smaranda Muresan, Chris Dyer
and the anonymous reviewers for their helpful com-
ments. This work was partially supported by the De-
fense Advanced Research Projects Agency?s GALE
program via Contract No
?
HR0011-06-2-0001.
432
References
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne, 2006. Improved Statistical Machine Translation
Using Paraphrases. In Proceedings of NAACL 2006,
pages 17-24.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation using Word Sense Disam-
biguation. In Proceedings of EMNLP 2007, pages 61-
72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of ACL 2007, pages
33-40.
Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary
study on probabilistic models for Chinese abbrevia-
tions. In Proceedings of the 3rd SIGHAN Workshop on
Chinese Language Processing, pages 9-16.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
Atomic Chinese Abbreviation Pairs: A Probabilistic
Model for Single Character Word Recovery. In Pro-
ceedings of the 5rd SIGHAN Workshop on Chinese
Language Processing, pages 17-24.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006, pages 961-968.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan,Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
strantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, Demonstration Session, pages 177-180.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48-54.
H.W.D Lee. 2005. A study of automatic expansion of
Chinese abbreviations. MA Thesis, The University of
Hong Kong.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting Parallel Sub-Sentential Fragments from Non-
Parallel Corpora. In Proceedings of ACL 2006, pages
81-88.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160-167.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440-447.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417-449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311-318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, pages
901-904.
Z.P. Yin. 1999. Methodologies and principles of Chi-
nese abbreviation formation. In Language Teaching
and Study, 2:73-82.
433
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593?601,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Variational Decoding for Statistical Machine Translation
Zhifei Li and Jason Eisner and Sanjeev Khudanpur
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com, jason@cs.jhu.edu, khudanpur@jhu.edu
Abstract
Statistical models in machine translation
exhibit spurious ambiguity. That is, the
probability of an output string is split
among many distinct derivations (e.g.,
trees or segmentations). In principle, the
goodness of a string is measured by the
total probability of its many derivations.
However, finding the best string (e.g., dur-
ing decoding) is then computationally in-
tractable. Therefore, most systems use
a simple Viterbi approximation that mea-
sures the goodness of a string using only
its most probable derivation. Instead,
we develop a variational approximation,
which considers all the derivations but still
allows tractable decoding. Our particular
variational distributions are parameterized
as n-gram models. We also analytically
show that interpolating these n-gram mod-
els for different n is similar to minimum-
risk decoding for BLEU (Tromble et al,
2008). Experiments show that our ap-
proach improves the state of the art.
1 Introduction
Ambiguity is a central issue in natural language
processing. Many systems try to resolve ambigu-
ities in the input, for example by tagging words
with their senses or choosing a particular syntax
tree for a sentence. These systems are designed to
recover the values of interesting latent variables,
such as word senses, syntax trees, or translations,
given the observed input.
However, some systems resolve too many ambi-
guities. They recover additional latent variables?
so-called nuisance variables?that are not of in-
terest to the user.1 For example, though machine
translation (MT) seeks to output a string, typical
MT systems (Koehn et al, 2003; Chiang, 2007)
1These nuisance variables may be annotated in training
data, but it is more common for them to be latent even there,
i.e., there is no supervision as to their ?correct? values.
will also recover a particular derivation of that out-
put string, which specifies a tree or segmentation
and its alignment to the input string. The compet-
ing derivations of a string are interchangeable for
a user who is only interested in the string itself, so
a system that unnecessarily tries to choose among
them is said to be resolving spurious ambiguity.
Of course, the nuisance variables are important
components of the system?s model. For example,
the translation process from one language to an-
other language may follow some hidden tree trans-
formation process, in a recursive fashion. Many
features of the model will crucially make reference
to such hidden structures or alignments.
However, collapsing the resulting spurious
ambiguity?i.e., marginalizing out the nuisance
variables?causes significant computational dif-
ficulties. The goodness of a possible MT out-
put string should be measured by summing up
the probabilities of all its derivations. Unfortu-
nately, finding the best string is then computation-
ally intractable (Sima?an, 1996; Casacuberta and
Higuera, 2000).2 Therefore, most systems merely
identify the single most probable derivation and
report the corresponding string. This corresponds
to a Viterbi approximation that measures the good-
ness of an output string using only its most proba-
ble derivation, ignoring all the others.
In this paper, we propose a variational method
that considers all the derivations but still allows
tractable decoding. Given an input string, the orig-
inal system produces a probability distribution p
over possible output strings and their derivations
(nuisance variables). Our method constructs a sec-
ond distribution q ? Q that approximates p as well
as possible, and then finds the best string accord-
ing to q. The last step is tractable because each
q ? Q is defined (unlike p) without reference to
nuisance variables. Notice that q here does not ap-
proximate the entire translation process, but only
2May and Knight (2006) have successfully used tree-
automaton determinization to exactly marginalize out some
of the nuisance variables, obtaining a distribution over parsed
translations. However, they do not marginalize over these
parse trees to obtain a distribution over translation strings.
593
the distribution over output strings for a particular
input. This is why it can be a fairly good approxi-
mation even without using the nuisance variables.
In practice, we approximate with several dif-
ferent variational families Q, corresponding to n-
gram (Markov) models of different orders. We
geometrically interpolate the resulting approxima-
tions q with one another (and with the original dis-
tribution p), justifying this interpolation as similar
to the minimum-risk decoding for BLEU proposed
by Tromble et al (2008). Experiments show that
our approach improves the state of the art.
The methods presented in this paper should be
applicable to collapsing spurious ambiguity for
other tasks as well. Such tasks include data-
oriented parsing (DOP), applications of Hidden
Markov Models (HMMs) and mixture models, and
other models with latent variables. Indeed, our
methods were inspired by past work on varia-
tional decoding for DOP (Goodman, 1996) and for
latent-variable parsing (Matsuzaki et al, 2005).
2 Background
2.1 Terminology
In MT, spurious ambiguity occurs both in regular
phrase-based systems (e.g., Koehn et al (2003)),
where different segmentations lead to the same
translation string (Figure 1), and in syntax-based
systems (e.g., Chiang (2007)), where different
derivation trees yield the same string (Figure 2).
In the Hiero system (Chiang, 2007) we are us-
ing, each string corresponds to about 115 distinct
derivations on average.
We use x to denote the input string, and D(x) to
consider the set of derivations then considered by
the system. Each derivation d ? D(x) yields some
translation string y = Y(d) in the target language.
We write D(x, y)
def
= {d ? D(x) : Y(d) = y} to
denote the set of all derivations that yield y. Thus,
the set of translations permitted by the model is
T(y)
def
= {y : D(x, y) 6= ?} (or equivalently,
T(y)
def
= {Y(d) : d ? D(x)}). We write y? for
the translation string that is actually output.
2.2 Maximum A Posterior (MAP) Decoding
For a given input sentence x, a decoding method
identifies a particular ?best? output string y?. The
maximum a posteriori (MAP) decision rule is
y? = argmax
y?T(x)
p(y | x) (1)
machine translation software
?  ?  ?  ?  ?  ?
machine translation software
?  ?  ?  ?  ?  ?
Figure 1: Segmentation ambiguity in phrase-based MT: two
different segmentations lead to the same translation string.
S->(? ?, machine) S->( ? , translation) S->( ? , software)
S->(? ?, machine)
?
S->( ? , software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(S0 ?  S1, S0 translation S1)
Figure 2: Tree ambiguity in syntax-based MT: two derivation
trees yield the same translation string.
(An alternative decision rule, minimum Bayes
risk (MBR), will be discussed in Section 4.)
To obtain p(y | x) above, we need to marginal-
ize over a nuisance variable, the derivation of y.
Therefore, the MAP decision rule becomes
y? = argmax
y?T(x)
?
d?D(x,y)
p(y, d | x) (2)
where p(y, d | x) is typically derived from a log-
linear model as follows,
p(y, d | x) =
e??s(x,y,d)
Z(x)
=
e??s(x,y,d)
?
y,d e
??s(x,y,d)
(3)
where ? is a scaling factor to adjust the sharp-
ness of the distribution, the score s(x, y, d) is a
learned linear combination of features of the triple
(x, y, d), and Z(x) is a normalization constant.
Note that p(y, d | x) = 0 if y 6= Y(d). Our deriva-
tion set D(x) is encoded in polynomial space, us-
ing a hypergraph or lattice.3 However, both |D(x)|
and |T(x)| may be exponential in |x|. Since the
marginalization needs to be carried out for each
member of T(x), the decoding problem of (2)
turns out to be NP-hard,4 as shown by Sima?an
(1996) for a similar problem.
3A hypergraph is analogous to a parse forest (Huang and
Chiang, 2007). (A finite-state lattice is a special case.) It can
be used to encode exponentially many hypotheses generated
by a phrase-based MT system (e.g., Koehn et al (2003)) or a
syntax-based MT system (e.g., Chiang (2007)).
4Note that the marginalization for a particular y would be
tractable; it is used at training time in certain training objec-
tive functions, e.g., maximizing the conditional likelihood of
a reference translation (Blunsom et al, 2008).
594
2.3 Viterbi Approximation
To approximate the intractable decoding problem
of (2), most MT systems (Koehn et al, 2003; Chi-
ang, 2007) use a simple Viterbi approximation,
y? = argmax
y?T(x)
pViterbi(y | x) (4)
= argmax
y?T(x)
max
d?D(x,y)
p(y, d | x) (5)
= Y
(
argmax
d?D(x)
p(y, d | x)
)
(6)
Clearly, (5) replaces the sum in (2) with a max.
In other words, it approximates the probability of
a translation string by the probability of its most-
probable derivation. (5) is found quickly via (6).
The Viterbi approximation is simple and tractable,
but it ignores most derivations.
2.4 N-best Approximation (or Crunching)
Another popular approximation enumerates the N
best derivations in D(x), a set that we call ND(x).
Modifying (2) to sum over only these derivations
is called crunching by May and Knight (2006):
y? = argmax
y?T(x)
pcrunch(y | x) (7)
= argmax
y?T(x)
?
d?D(x,y)?ND(x)
p(y, d | x)
3 Variational Approximate Decoding
The Viterbi and crunching methods above approx-
imate the intractable decoding of (2) by ignor-
ing most of the derivations. In this section, we
will present a novel variational approximation,
which considers all the derivations but still allows
tractable decoding.
3.1 Approximate Inference
There are several popular approaches to approxi-
mate inference when exact inference is intractable
(Bishop, 2006). Stochastic techniques such as
Markov Chain Monte Carlo are exact in the limit
of infinite runtime, but tend to be too slow for large
problems. By contrast, deterministic variational
methods (Jordan et al, 1999), including message-
passing (Minka, 2005), are inexact but scale up
well. They approximate the original intractable
distribution with one that factorizes better or has
a specific parametric form (e.g., Gaussian).
In our work, we use a fast variational method.
Variational methods generally work as follows.
When exact inference under a complex model p
is intractable, one can approximate the posterior
p(y | x) by a tractable model q(y), where q ? Q is
chosen to minimize some information loss such as
the KL divergence KL(p ? q). The simpler model
q can then act as a surrogate for p during inference.
3.2 Variational Decoding for MT
For each input sentence x, we assume that a base-
line MT system generates a hypergraph HG(x)
that compactly encodes the derivation set D(x)
along with a score for each d ? D(x),5 which we
interpret as p(y, d | x) (or proportional to it). For
any single y ? T(x), it would be tractable using
HG(x) to compute p(y | x) =
?
d p(y, d | x).
However, as mentioned, it is intractable to find
argmaxy p(y | x) as required by the MAP de-
coding (2), so we seek an approximate distribution
q(y) ? p(y | x).6
For a fixed x, we seek a distribution q ? Q that
minimizes the KL divergence from p to q (both
regarded as distributions over y):7
q? = argmin
q?Q
KL(p ? q) (8)
= argmin
q?Q
?
y?T(x)
(p log p? p log q) (9)
= argmax
q?Q
?
y?T(x)
p log q (10)
So far, in order to approximate the intractable
optimization problem (2), we have defined an-
other optimization problem (10). If computing
p(y | x) during decoding is computationally in-
tractable, one might wonder if the optimization
problem (10) is any simpler. We will show this is
the case. The trick is to parameterize q as a fac-
torized distribution such that the estimation of q?
and decoding using q? are both tractable through
efficient dynamic programs. In the next three sub-
sections, we will discuss the parameterization, es-
timation, and decoding, respectively.
3.2.1 Parameterization of q
In (10), Q is a family of distributions. If we se-
lect a large family Q, we can allow more com-
plex distributions, so that q? will better approxi-
mate p. If we select a smaller family Q, we can
5The baseline system may return a pruned hypergraph,
which has the effect of pruning D(x) and T(x) as well.
6Following the convention in describing variational infer-
ence, we write q(y) instead of q(y | x), even though q(y)
always depends on x implicitly.
7To avoid clutter, we denote p(y | x) by p, and q(y) by q.
We drop p log p from (9) because it is constant with respect
to q. We then flip the sign and change argmin to argmax.
595
guarantee that q? will have a simple form with
many conditional independencies, so that q?(y)
and y? = argmaxy q
?(y) are easier to compute.
Since each q(y) is a distribution over output
strings, a natural choice for Q is the family of
n-gram models. To obtain a small KL diver-
gence (8), we should make n as large as possible.
In fact, q? ? p as n ? ?. Of course, this last
point also means that our computation becomes
intractable as n??.8 However, if p(y | x) is de-
fined by a hypergraph HG(x) whose structure ex-
plicitly incorporates an m-gram language model,
both training and decoding will be efficient when
m ? n. We will give algorithms for this case that
are linear in the size of HG(x).9
Formally, each q ? Q takes the form
q(y) =
?
w?W
q(r(w) | h(w))cw(y) (11)
where W is a set of n-gram types. Each w ?W is
an n-gram, which occurs cw(y) times in the string
y, and w may be divided into an (n ? 1)-gram
prefix h(w) (the history) and a 1-gram suffix r(w)
(the rightmost or current word).
8Blunsom et al (2008) effectively do take n = ?, by
maintaining the whole translation string in the dynamic pro-
gramming state. They alleviate the computation cost some-
how by using aggressive beam pruning, which might be sen-
sible for their relatively small task (e.g., input sentences of
< 10 words). But, we are interested in improving the perfor-
mance for a large-scale system, and thus their method is not
a viable solution. Moreover, we observe in our experiments
that using a larger n does not improve much over n = 2.
9A reviewer asks about the interaction with backed-off
language models. The issue is that the most compact finite-
state representations of these (Allauzen et al, 2003), which
exploit backoff structure, are not purely m-gram for any
m. They yield more compact hypergraphs (Li and Khudan-
pur, 2008), but unfortunately those hypergraphs might not be
treatable by Fig. 4?since where they back off to less than an
n-gram, e is not informative enough for line 8 to find w.
We sketch a method that works for any language model
given by a weighted FSA, L. The variational family Q can
be specified by any deterministic weighted FSA, Q, with
weights parameterized by ?. One seeks ? to minimize (8).
Intersect HG(x) with an ?unweighted? version of Q in
which all arcs have weight 1, so that Q does not prefer
any string to another. By lifting weights into an expectation
semiring (Eisner, 2002), it is then possible to obtain expected
transition counts in Q (where the expectation is taken under
p), or other sufficient statistics needed to estimate ?.
This takes only time O(|HG(x)|) when L is a left-to-right
refinement of Q (meaning that any two prefix strings that
reach the same state in L also reach the same state in Q),
for then intersecting L or HG(x) with Q does not split any
states. That is the case when L and Q are respectively pure
m-gram and n-gram models withm ? n, as assumed in (12)
and Figure 4. It is also the case when Q is a pure n-gram
model and L is constructed not to back off beyond n-grams;
or when the variational family Q is defined by deliberately
taking the FSA Q to have the same topology as L.
The parameters that specify a particular q ? Q
are the (normalized) conditional probability distri-
butions q(r(w) | h(w)). We will now see how to
estimate these parameters to approximate p(? | x)
for a given x at test time.
3.2.2 Estimation of q?
Note that the objective function (8)?(10) asks us to
approximate p as closely as possible, without any
further smoothing. (It is assumed that p is already
smoothed appropriately, having been constructed
from channel and language models that were esti-
mated with smoothing from finite training data.)
In fact, if p were the empirical distribution over
strings in a training corpus, then q? of (10) is just
the maximum-likelihood n-gram model?whose
parameters, trivially, are just unsmoothed ratios of
the n-gram and (n?1)-gram counts in the training
corpus. That is, q?(r(w) | h(w)) = c(w)c(h(w)) .
Our actual job is exactly the same, except that p
is specified not by a corpus but by the hypergraph
HG(x). The only change is that the n-gram counts
c?(w) are no longer integers from a corpus, but are
expected counts under p:10
q?(r(w) | h(w)) =
c?(w)
c?(h(w))
= (12)
?
y cw(y)p(y | x)
?
y ch(w)(y)p(y | x)
=
?
y,d cw(y)p(y, d | x)
?
y,d ch(w)(y)p(y, d | x)
Now, the question is how to efficiently compute
(12) from the hypergraph HG(x). To develop the
intuition, we first present a brute-force algorithm
in Figure 3. The algorithm is brute-force since
it first needs to unpack the hypergraph and enu-
merate each possible derivation in the hypergraph
(see line 1), which is computationally intractable.
The algorithm then enumerates each n-gram and
(n ? 1)-gram in y and accumulates its soft count
into the expected count, and finally obtains the pa-
rameters of q? by taking count ratios via (12).
Figure 4 shows an efficient version that exploits
the packed-forest structure of HG(x) in com-
puting the expected counts. Specifically, it first
runs the inside-outside procedure, which annotates
each node (say v) with both an inside weight ?(v)
and an outside weight ?(v). The inside-outside
also finds Z(x), the total weight of all derivations.
With these weights, the algorithm then explores
the hypergraph once more to collect the expected
10One can prove (12) via Lagrange multipliers, with q?(? |
h) constrained to be a normalized distribution for each h.
596
Brute-Force-MLE(HG(x ))
1 for y , d in HG(x)  each derivation
2 forw in y  each n-gram type
3  accumulate soft count
4 c?(w) + = cw(y) ? p(y, d | x)
5 c?(h(w)) + = cw(y) ? p(y, d | x)
6 q? ? MLE using formula (12)
7 return q?
Figure 3: Brute-force estimation of q?.
Dynamic-Programming-MLE(HG(x ))
1 run inside-outside on the hypergraph HG(x)
2 for v in HG(x)  each node
3 for e ? B(v)  each incoming hyperedge
4 ce ? pe ? ?(v)/Z(x)
5 for u ? T (e)  each antecedent node
6 ce ? ce ? ?(u)
7  accumulate soft count
8 forw in e  each n-gram type
9 c?(w) + = cw(e) ? ce
10 c?(h(w)) + = cw(e) ? ce
11 q? ? MLE using formula (12)
12 return q?
Figure 4: Dynamic programming estimation of q?. B(v) rep-
resents the set of incoming hyperedges of node v; pe repre-
sents the weight of the hyperedge e itself; T (e) represents
the set of antecedent nodes of hyperedge e. Please refer to
the text for the meanings of other notations.
counts. For each hyperedge (say e), it first gets the
posterior weight ce (see lines 4-6). Then, for each
n-gram type (say w), it increments the expected
count by cw(e) ? ce, where cw(e) is the number of
copies of n-gram w that are added by hyperedge
e, i.e., that appear in the yield of e but not in the
yields of any of its antecedents u ? T (e).
While there may be exponentially many deriva-
tions, the hypergraph data structure represents
them in polynomial space by allowing multiple
derivations to share subderivations. The algorithm
of Figure 4 may be run over this packed forest
in time O(|HG(x)|) where |HG(x)| is the hyper-
graph?s size (number of hyperedges).
3.2.3 Decoding with q?
When translating x at runtime, the q? constructed
from HG(x) will be used as a surrogate for p dur-
ing decoding. We want its most probable string:
y? = argmax
y
q?(y) (13)
Since q? is an n-gram model, finding y? is equiv-
alent to a shortest-path problem in a certain graph
whose edges correspond to n-grams (weighted
with negative log-probabilities) and whose ver-
tices correspond to (n? 1)-grams.
However, because q? only approximates p, y? of
(13) may be locally appropriate but globally inade-
quate as a translation of x. Observe, e.g., that an n-
gram model q?(y) will tend to favor short strings
y, regardless of the length of x. Suppose x = le
chat chasse la souris (?the cat chases the mouse?)
and q? is a bigram approximation to p(y | x). Pre-
sumably q?(the | START), q?(mouse | the), and
q?(END | mouse) are all large in HG(x). So the
most probable string y? under q? may be simply
?the mouse,? which is short and has a high proba-
bility but fails to cover x.
Therefore, a better way of using q? is to restrict
the search space to the original hypergraph, i.e.:
y? = argmax
y?T(x)
q?(y) (14)
This ensures that y? is a valid string in the origi-
nal hypergraph HG(x), which will tend to rule out
inadequate translations like ?the mouse.?
If our sole objective is to get a good approxi-
mation to p(y | x), we should just use a single
n-gram model q? whose order n is as large as pos-
sible, given computational constraints. This may
be regarded as favoring n-grams that are likely to
appear in the reference translation (because they
are likely in the derivation forest). However, in or-
der to score well on the BLEU metric for MT eval-
uation (Papineni et al, 2001), which gives partial
credit, we would also like to favor lower-order n-
grams that are likely to appear in the reference,
even if this means picking some less-likely high-
order n-grams. For this reason, it is useful to in-
terpolate different orders of variational models,
y? = argmax
y?T(x)
?
n
?n ? log q
?
n(y) (15)
where n may include the value of zero, in which
case log q?0(y)
def
= |y|, corresponding to a conven-
tional word penalty feature. In the geometric inter-
polation above, the weight ?n controls the relative
veto power of the n-gram approximation and can
be tuned using MERT (Och, 2003) or a minimum
risk procedure (Smith and Eisner, 2006).
Lastly, note that Viterbi and variational approx-
imation are different ways to approximate the ex-
act probability p(y | x), and each of them has
pros and cons. Specifically, Viterbi approxima-
tion uses the correct probability of one complete
597
derivation, but ignores most of the derivations in
the hypergraph. In comparison, the variational ap-
proximation considers all the derivations in the hy-
pergraph, but uses only aggregate statistics of frag-
ments of derivations. Therefore, it is desirable to
interpolate further with the Viterbi approximation
when choosing the final translation output:11
y? = argmax
y?T(x)
?
n
?n ? log q
?
n(y)
+ ?v ? log pViterbi(y | x) (16)
where the first term corresponds to the interpolated
variational decoding of (15) and the second term
corresponds to the Viterbi decoding of (4).12 As-
suming ?v > 0, the second term penalizes transla-
tions with no good derivation in the hypergraph.13
For n ? m, any of these decoders (14)?
(16) may be implemented efficiently by using the
n-gram variational approximations q? to rescore
HG(x)?preserving its hypergraph topology, but
modifying the hyperedge weights.14 While the
original weights gave derivation d a score of
log p(d | x), the weights as modified for (16)
will give d a score of
?
n ?n ? log q
?
n(Y(d)) + ?v ?
log p(d | x). We then find the best-scoring deriva-
tion and output its target yield; that is, we find
argmaxy?T(x) via Y(argmaxd?D(x)).
4 Variational vs. Min-Risk Decoding
In place of the MAP decoding, another commonly
used decision rule is minimum Bayes risk (MBR):
y? = argmin
y
R(y) = argmin
y
?
y?
l(y, y?)p(y? | x)
(17)
11It would also be possible to interpolate with the N -best
approximations (see Section 2.4), with some complications.
12Zens and Ney (2006) use a similar decision rule as here
and they also use posterior n-gram probabilities as feature
functions, but their model estimation and decoding are over
an N -best, which is trivial in terms of computation.
13Already at (14), we explicitly ruled out translations y
having no derivation at all in the hypergraph. However,
suppose the hypergraph were very large (thanks to a large
or smoothed translation model and weak pruning). Then
(14)?s heuristic would fail to eliminate bad translations (?the
mouse?), since nearly every string y ? ?? would be derived
as a translation with at least a tiny probability. The ?soft? ver-
sion (16) solves this problem, since unlike the ?hard? (14), it
penalizes translations that appear only weakly in the hyper-
graph. As an extreme case, translations not in the hypergraph
at all are infinitely penalized (log pViterbi(y) = log 0 =
??), making it natural for the decoder not to consider them,
i.e., to do only argmaxy?T(x) rather than argmaxy??? .
14One might also want to use the q?n or smoothed versions
of them to rescore additional hypotheses, e.g., hypotheses
proposed by other systems or by system combination.
where l(y, y?) represents the loss of y if the true
answer is y?, and the risk of y is its expected
loss.15 Statistical decision theory shows MBR is
optimal if p(y? | x) is the true distribution, while
in practice p(y? | x) is given by a model at hand.
We now observe that our variational decoding
resembles the MBR decoding of Tromble et al
(2008). They use the following loss function, of
which a linear approximation to BLEU (Papineni
et al, 2001) is a special case,
l(y, y?) = ?(?0|y|+
?
w?N
?wcw(y)?w(y
?)) (18)
where w is an n-gram type, N is a set of n-gram
types with n ? [1, 4], cw(y) is the number of oc-
currence of the n-gram w in y, and ?w(y?) is an
indicator function to check if y? contains at least
one occurrence of w. With the above loss func-
tion, Tromble et al (2008) derive the MBR rule16
y? = argmax
y
(?0|y|+
?
w?N
?wcw(y)g(w | x))
(19)
where g(w | x) is a specialized ?posterior? proba-
bility of the n-gram w, and is defined as
g(w | x) =
?
y?
?w(y
?)p(y? | x) (20)
Now, let us divide N , which contains n-gram
types of different n, into several subsets Wn, each
of which contains only the n-grams with a given
length n. We can now rewrite (19) as follows,
y? = argmax
y
?
n
?n ? gn(y | x) (21)
by assuming ?w = ?|w| and,
gn(y | x)=
{
|y| if n = 0
?
w?Wn g(w | x)cw(y) if n > 0
(22)
Clearly, their rule (21) has a quite similar form
to our rule (15), and we can relate (20) to (12) and
(22) to (11). This justifies the use of interpolation
in Section 3.2.3. However, there are several im-
portant differences. First, the n-gram ?posterior?
of (20) is very expensive to compute. In fact, it re-
quires an intersection between each n-gram in the
lattice and the lattice itself, as is done by Tromble
15The MBR becomes the MAP decision rule of (1) if a so-
called zero-one loss function is used: l(y, y?) = 0 if y = y?;
otherwise l(y, y?) = 1.
16Note that Tromble et al (2008) only consider MBR for a
lattice without hidden structures, though their method can be
in principle applied in a hypergraph with spurious ambiguity.
598
et al (2008). In comparison, the optimal n-gram
probabilities of (12) can be computed using the
inside-outside algorithm, once and for all. Also,
g(w | x) of (20) is not normalized over the history
of w, while q?(r(w) | h(w)) of (12) is. Lastly, the
definition of the n-gram model is different. While
the model (11) is a proper probabilistic model, the
function of (22) is simply an approximation of the
average n-gram precisions of y.
A connection between variational decoding and
minimum-risk decoding has been noted before
(e.g., Matsuzaki et al (2005)), but the derivation
above makes the connection formal.
DeNero et al (2009) concurrently developed
an alternate to MBR, called consensus decoding,
which is similar to ours in practice although moti-
vated quite differently.
5 Experimental Results
We report results using an open source MT toolkit,
called Joshua (Li et al, 2009), which implements
Hiero (Chiang, 2007).
5.1 Experimental Setup
We work on a Chinese to English translation task.
Our translation model was trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora
distributed by LDC for the NIST MT evalua-
tion using a sampling method based on the n-
gram matches between training and test sets in
the foreign side. We also used a 5-gram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), trained on a data
set consisting of a 130M words in English Giga-
word (LDC2007T07) and the English side of the
parallel corpora. We use GIZA++ (Och and Ney,
2000), a suffix-array (Lopez, 2007), SRILM (Stol-
cke, 2002), and risk-based deterministic annealing
(Smith and Eisner, 2006)17 to obtain word align-
ments, translation models, language models, and
the optimal weights for combining these models,
respectively. We use standard beam-pruning and
cube-pruning parameter settings, following Chi-
ang (2007), when generating the hypergraphs.
The NIST MT?03 set is used to tune model
weights (e.g. those of (16)) and the scaling factor
17We have also experimented with MERT (Och, 2003), and
found that the deterministic annealing gave results that were
more consistent across runs and often better.
Decoding scheme MT?04 MT?05
Viterbi 35.4 32.6
MBR (K=1000) 35.8 32.7
Crunching (N=10000) 35.7 32.8
Crunching+MBR (N=10000) 35.8 32.7
Variational (1to4gram+wp+vt) 36.6 33.5
Table 1: BLEU scores for Viterbi, Crunching, MBR, and vari-
ational decoding. All the systems improve significantly over
the Viterbi baseline (paired permutation test, p < 0.05). In
each column, we boldface the best result as well as all results
that are statistically indistinguishable from it. In MBR, K is
the number of unique strings. For Crunching and Crunch-
ing+MBR, N represents the number of derivations. On av-
erage, each string has about 115 distinct derivations. The
variational method ?1to4gram+wp+vt? is our full interpola-
tion (16) of four variational n-gram models (?1to4gram?), the
Viterbi baseline (?vt?), and a word penalty feature (?wp?).
? of (3),18 and MT?04 and MT?05 are blind test-
sets. We will report results for lowercase BLEU-4,
using the shortest reference translation in comput-
ing brevity penalty.
5.2 Main Results
Table 1 presents the BLEU scores under Viterbi,
crunching, MBR, and variational decoding. Both
crunching and MBR show slight significant im-
provements over the Viterbi baseline; variational
decoding gives a substantial improvement.
The difference between MBR and Crunch-
ing+MBR lies in how we approximate the distri-
bution p(y? | x) in (17).19 For MBR, we take
p(y? | x) to be proportional to pViterbi(y? | x) if y?
is among the K best distinct strings on that mea-
sure, and 0 otherwise. For Crunching+MBR, we
take p(y? | x) to be proportional to pcrunch(y? | x),
which is based on the N best derivations.
5.3 Results of Different Variational Decoding
Table 2 presents the BLEU results under different
ways in using the variational models, as discussed
in Section 3.2.3. As shown in Table 2a, decod-
ing with a single variational n-gram model (VM)
as per (14) improves the Viterbi baseline (except
the case with a unigram VM), though often not
statistically significant. Moreover, a bigram (i.e.,
?2gram?) achieves the best BLEU scores among
the four different orders of VMs.
The interpolation between a VM and a word
penalty feature (?wp?) improves over the unigram
18We found the BLEU scores are not very sensitive to ?,
contrasting to the observations by Tromble et al (2008).
19We also restrict T(x) to {y : p(y | x) > 0}, using the
same approximation for p(y | x) as we did for p(y? | x).
599
(a) decoding with a single variational model
Decoding scheme MT?04 MT?05
Viterbi 35.4 32.6
1gram 25.9 24.5
2gram 36.1 33.4
3gram 36.0? 33.1
4gram 35.8? 32.9
(b) interpolation between a single variational
model and a word penalty feature
1gram+wp 29.7 27.7
2gram+wp 35.5 32.6
3gram+wp 36.1? 33.1
4gram+wp 35.7? 32.8?
(c) interpolation of a single variational model, the
Viterbi model, and a word penalty feature
1gram+wp+vt 35.6? 32.8?
2gram+wp+vt 36.5? 33.5?
3gram+wp+vt 35.8? 32.9?
4gram+wp+vt 35.6? 32.8?
(d) interpolation of several n-gram VMs, the
Viterbi model, and a word penalty feature
1to2gram+wp+vt 36.6? 33.6?
1to3gram+wp+vt 36.6? 33.5?
1to4gram+wp+vt 36.6? 33.5?
Table 2: BLEU scores under different variational decoders
discussed in Section 3.2.3. A star ? indicates a result that is
significantly better than Viterbi decoding (paired permutation
test, p < 0.05). We boldface the best system and all systems
that are not significantly worse than it. The brevity penalty
BP in BLEU is always 1, meaning that on average y? is no
shorter than the reference translation, except for the ?1gram?
systems in (a), which suffer from brevity penalties of 0.826
and 0.831.
VM dramatically, but does not improve higher-
order VMs (Table 2b). Adding the Viterbi fea-
ture (?vt?) into the interpolation further improves
the lower-order models (Table 2c), and all the im-
provements over the Viterbi baseline become sta-
tistically significant. At last, interpolation of sev-
eral variational models does not yield much fur-
ther improvement over the best previous model,
but makes the results more stable (Table 2d).
5.4 KL Divergence of Approximate Models
While the BLEU scores reported show the prac-
tical utility of the variational models, it is also
interesting to measure how well each individual
variational model q(y) approximates the distribu-
tion p(y | x). Ideally, the quality of approxima-
tion should be measured by the KL divergence
KL(p ? q)
def
= H(p, q) ? H(p), where the cross-
entropy H(p, q)
def
= ?
?
y p(y | x) log q(y), and
Measure H(p, ?) Hd(p) H(p)
bits/word q?1 q
?
2 q
?
3 q
?
4 ?
MT?04 2.33 1.68 1.57 1.53 1.36 1.03
MT?05 2.31 1.69 1.58 1.54 1.37 1.04
Table 3: Cross-entropies H(p, q) achieved by various ap-
proximations q. The notation H denotes the sum of cross-
entropies of all test sentences, divided by the total number
of test words. A perfect approximation would achieve H(p),
which we estimate using the true Hd(p) and a 10000-best list.
the entropy H(p)
def
= ?
?
y p(y | x) log p(y | x).
Unfortunately H(p) (and hence KL = H(p, q) ?
H(p)) is intractable to compute. But, since H(p)
is the same for all q, we can simply use H(p, q)
to compare different models q. Table 3 reports the
cross-entropies H(p, q) for various models q.
We also report the derivational entropy
Hd(p)
def
= ?
?
d p(d | x) log p(d | x).
20 From this,
we obtain an estimate of H(p) by observing that
the ?gap? Hd(p) ? H(p) equals Ep(y)[H(d | y)],
which we estimate from our 10000-best list.
Table 3 confirms that higher-order variational
models (drawn from a larger family Q) approxi-
mate p better. This is necessarily true, but it is
interesting to see that most of the improvement is
obtained just by moving from a unigram to a bi-
gram model. Indeed, although Table 3 shows that
better approximations can be obtained by using
higher-order models, the best BLEU score in Ta-
bles 2a and 2c was obtained by the bigram model.
After all, p cannot perfectly predict the reference
translation anyway, hence may not be worth ap-
proximating closely; but p may do a good job
of predicting bigrams of the reference translation,
and the BLEU score rewards us for those.
6 Conclusions and Future Work
We have successfully applied the general varia-
tional inference framework to a large-scale MT
task, to approximate the intractable problem of
MAP decoding in the presence of spurious am-
biguity. We also showed that interpolating vari-
ational models with the Viterbi approximation can
compensate for poor approximations, and that in-
terpolating them with one another can reduce the
Bayes risk and improve BLEU. Our empirical re-
sults improve the state of the art.
20Both H(p, q) and Hd(p) involve an expectation over ex-
ponentially many derivations, but they can be computed in
time only linear in the size of HG(x) using an expectation
semiring (Eisner, 2002). In particular, H(p, q) can be found
as ?
?
d?D(x) p(d | x) log q(Y(d)).
600
Many interesting research directions remain
open. To approximate the intractable MAP de-
coding problem of (2), we can use different vari-
ational distributions other than the n-gram model
of (11). Interpolation with other models is also
interesting, e.g., the constituent model in Zhang
and Gildea (2008). We might also attempt to min-
imize KL(q ? p) rather than KL(p ? q), in order
to approximate the mode (which may be prefer-
able since we care most about the 1-best transla-
tion under p) rather than the mean of p (Minka,
2005). One could also augment our n-gram mod-
els with non-local string features (Rosenfeld et al,
2001) provided that the expectations of these fea-
tures could be extracted from the hypergraph.
Variational inference can also be exploited to
solve many other intractable problems in MT (e.g.,
word/phrase alignment and system combination).
Finally, our method can be used for tasks beyond
MT. For example, it can be used to approximate
the intractable MAP decoding inherent in systems
using HMMs (e.g. speech recognition). It can also
be used to approximate a context-free grammar
with a finite state automaton (Nederhof, 2005).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In ACL, pages 40?47.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning. Springer.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. In ICGI, pages
15?24.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
ACL-IJCNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1?8.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the DOP model. In EMNLP, pages 143?152.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL, pages 144?151.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational meth-
ods for graphical models. In Learning in Graphical
Models. MIT press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL, pages 48?54.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09, pages 135?
139.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL, pages
976?985.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Tom Minka. 2005. Divergence measures and message
passing. In Microsoft Research Technical Report
(MSR-TR-2005-173). Microsoft Research.
Mark-Jan Nederhof. 2005. A general technique to
train language models on language models. Com-
put. Linguist., 31(2):173?186.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL, pages 440?
447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: A vehicle for linguistic-statistical integration.
Computer Speech and Language, 15(1).
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175?1180.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In ACL,
pages 787?794.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In ICSLP, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
EMNLP, pages 620?629.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In WMT06, pages 72?77.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In ACL, pages 209?217.
601
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 104?111,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Optimal Dialog in Consumer-Rating Systems using a POMDP Framework
Zhifei Li
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
zhifei.work@gmail.com
Patrick Nguyen, Geoffrey Zweig
Microsoft Corporation
1 Microsoft Way,
Redmond, WA 98052, USA
{panguyen,gzweig}@microsoft.com
Abstract
Voice-Rate is an experimental dialog system
through which a user can call to get prod-
uct information. In this paper, we describe
an optimal dialog management algorithm for
Voice-Rate. Our algorithm uses a POMDP
framework, which is probabilistic and cap-
tures uncertainty in speech recognition and
user knowledge. We propose a novel method
to learn a user knowledge model from a review
database. Simulation results show that the
POMDP system performs significantly better
than a deterministic baseline system in terms
of both dialog failure rate and dialog interac-
tion time. To the best of our knowledge, our
work is the first to show that a POMDP can
be successfully used for disambiguation in a
complex voice search domain like Voice-Rate.
1 Introduction
In recent years, web-based shopping and rating sys-
tems have provided a valuable service to consumers
by allowing them to shop products and share their
assessments of products online. The use of these
systems, however, requires access to a web interface,
typically through a laptop or desktop computer, and
this restricts their usefulness. While mobile phones
also provide some web access, their small screens
make them inconvenient to use. Therefore, there
arises great interests in having a spoken dialog in-
terface through which a user can call to get product
information (e.g., price, rating, review, etc.) on the
fly. Voice-Rate (Zweig et al, 2007) is such a sys-
tem. Here is a typical scenario under which shows
the usefulness of the Voice-Rate system. A user en-
ters a store and finds that a digital camera he has
not planned to buy is on sale. Before he decides
to buy the camera, he takes out his cell phone and
calls Voice-Rate to see whether the price is really
a bargain and what other people have said about
the camera. This helps him to make a wise deci-
sion. The Voice-Rate system (Zweig et al, 2007) in-
volves many techniques, e.g., information retrieval,
review summarization, speech recognition, speech
synthesis, dialog management, etc. In this paper, we
mainly focus on the dialog management component.
When a user calls Voice-Rate for the information
of a specific product, the system needs to identify,
from a database containing millions of products, the
exact product the user intends. To achieve this, the
system first solicits the user for the product name.
Using the product name as a query, the system then
retrieves from its database a list of products related
to the query. Ideally, the highest-ranked product
should be the one intended by the user. In reality,
this is often not the case due to various reasons. For
example, there might be a speech recognition error
or an information retrieval ranking error. Moreover,
the product name is usually very ambiguous in iden-
tifying an exact product. The product name that the
user says may not be exactly the same as the name
in the product database. For example, while the user
says ?Canon Powershot SD750?, the exact name
in the product database may be ?Canon Powershot
SD750 Digital Camera?. Even the user says the ex-
act name, it is possible that the same name may be
corresponding to different products in different cat-
egories, for instance books and movies.
Due to the above reasons, whenever the Voice-
Rate system finds multiple products matching the
user?s initial speech query, it initiates a dialog proce-
dure to identify the intended product by asking ques-
tions about the products. In the product database,
104
many attributes can be used to identify a product.
For example, a digital camera has the product name,
category, brand, resolution, zoom, etc. Given a list
of products, different attributes may have different
ability to distinguish the products. For example, if
the products belong to many categories, the category
attribute is very useful to distinguish the products. In
contrast, if all the products belong to a single cate-
gory, it makes no sense to ask a question on the cat-
egory. In addition to the variability in distinguishing
products, different attributes may require different
knowledge from the user in order for them to an-
swer questions about these attributes. For example,
while most users can easily answer a question on
category, they may not be able to answer a question
on the part number of a product, though the part
number is unique and perfect to distinguish prod-
ucts. Other variabilities are in the difficulty that the
attributes impose on speech recognition and speech
synthesis. Clearly, given a list of products and a set
of attributes, what questions and in what order to ask
is essential to make the dialog successful. Our goal
is to dynamically find such important attributes at
each stage/turn.
The baseline system (Zweig et al, 2007) asks
questions only on product name and category. The
order of questions is fixed: first ask questions on
product category, and then on name. Moreover, it
is deterministic and does not model uncertainly in
speech recognition and user knowledge. Partially
observable Markov decision process (POMDP) has
been shown to be a general framework to capture the
uncertainty in spoken dialog systems. In this paper,
we present a POMDP-based probabilistic system,
which utilizes rich product information and captures
uncertainty in speech recognition and user knowl-
edge. We propose a novel method to learn a user
knowledge model from a review database. Our sim-
ulation results show that the POMDP-based system
improves the baseline significantly.
To the best of our knowledge, our work is the first
to show that a POMDP can be successfully used for
disambiguation in a complex voice search domain
like Voice-Rate.
2 Voice-Rate Dialog System Overview
Figure 1 shows the main flow in the Voice-Rate sys-
tem with simplification. Specifically, when a user
calls Voice-Rate for the information of a specific
 
 
Yes 
Begin 
Information Retrieval 
Dialog Manager 
End 
Initial Speech Query 
List of Products 
Corrupted User Action 
Human 
Speech recognizer 
User Action 
Found 
product? No  
Play Rating 
Question 
? Intended product  
Figure 1: Flow Chart of Voice-Rate System
Step-1: remove products that do not match
the user action
Step-2: any category question to ask?
yes: ask the question and return
no: go to step-3
Step-3: ask a product name question
Table 1: Baseline Dialog Manager Algorithm
product, the system first solicits the user for the
product name. Treating the user input as a query
and the product names in the product database as
documents, the system retrieves a list of products
that match the user input based on TF-IDF mea-
sure. Then, the dialog manager dynamically gener-
ates questions to identify the specific intended prod-
uct. Once the product is found, the system plays
back its rating information. In this paper, we mainly
focus on the dialog manager component.
Baseline Dialog Manager: Table 1 shows the
baseline dialog manager. In Step-1, it removes all
the products that are not consistent with the user re-
sponse. For example, if the user answers ?camera?
when given a question on category, the system re-
moves all the products that do not belong to category
?camera?. In Step-2 and Step-3, the baseline system
asks questions about product name and product cat-
egory, and product category has a higher priority.
3 Overview of POMDP
3.1 Basic Definitions
A Partially Observable Markov Decision Process
(POMDP) is a general framework to handle uncer-
tainty in a spoken dialog system. Following nota-
105
tions in Williams and Young (2007), a POMDP is
defined as a tuple {S,A, T,R,O,Z, ?,~b0} where S
is a set of states s describing the environment; A is
a set of machine actions a operating on the environ-
ment; T defines a transition probability P (s? |s, a);
R defines a reward function r(s, a); O is a set of ob-
servations o, and an observation can be thought as
a corrupted version of a user action; Z defines an
observation probability P (o? |s? , a); ? is a geometric
discount factor; and~b0 is an initial belief vector.
The POMDP operates as follows. At each time-
step (a.k.a. stage), the environment is in some unob-
served state s. Since s is not known exactly, a distri-
bution (called a belief vector ~b) over possible states
is maintained where~b(s) indicates the probability of
being in a particular state s. Based on the current be-
lief vector ~b, an optimal action selection algorithm
selects a machine action a, receives a reward r, and
the environment transits to a new unobserved state
s? . The environment then generates an observation
o? (i.e., a user action), after which the system update
the belief vector ~b. We call the process of adjusting
the belief vector~b at each stage ?belief update?.
3.2 Applying POMDP in Practice
As mentioned in Williams and Young (2007), it is
not trivial to apply the POMDP framework to a
specific application. To achieve this, one normally
needs to design the following three components:
? State Diagram Modeling
? Belief Update
? Optimal Action Selection
The state diagram defines the topology of the
graph, which contains three kinds of elements: sys-
tem state, machine action, and user action. To drive
the transitions, one also needs to define a set of
models (e.g., user goal model, user action model,
etc.). The modeling assumptions are application-
dependent. The state diagram, together with the
models, determines the dynamics of the system.
In general, the belief update depends on the ob-
servation probability and the transition probability,
while the transition probability itself depends on the
modeling assumptions the system makes. Thus, the
exact belief update formula is application-specific.
Optimal action selection is essentially an opti-
mization algorithm, which can be defined as,
a? = argmax
a?A
G(P (a)), (1)
where A refers to a set of machine actions a.
Clearly, the optimal action selection requires three
sub-components: a goodness measure function G, a
prediction algorithm P , and a search algorithm (i.e.,
the argmax operator). The prediction algorithm is
used to predict the behavior of the system in the
future if a given machine action a was taken. The
search algorithm can use an exhaustive linear search
or an approximated greedy search depending on the
size of A (Murphy, 2000; Spaan and Vlassis, 2005).
4 POMDP Framework in Voice-Rate
In this section, we present our instantiation of
POMDP in the Voice-Rate system.
4.1 State Diagram Modeling
4.1.1 State Diagram Design
Table 2 summarizes the main design choices in
the state diagram for our application, i.e., identifying
the intended product from a large list of products.
As in Williams and Young (2007), we incorporate
both the user goal (i.e., the intended product) and
the user action in the system state. Moreover, to ef-
ficiently update belief vector and compute optimal
action, the state space is dynamically generated and
pruned. In particular, instead of listing all the possi-
ble combinations between the products and the user
actions, at each stage, we only generate states con-
taining the products and the user actions that are rel-
evant to the last machine action. Moreover, at each
stage, if the belief probability of a product is smaller
than a threshold, we prune out this product and all
its associated system states. Note that the intended
product may be pruned away due to an overly large
threshold. In the simulation, we will use a develop-
ment set to tune this threshold.
As shown in Table 2, five kinds of machine ac-
tions are defined. The questions on product names
are usually long, imposing difficulty in speech syn-
thesis/recgonition and user input. Thus, short ques-
tions (e.g., questions on category or simple at-
tributes) are preferable. This partly motivate us to
exploit rich product information to help the dialog.
Seven kinds of user actions are defined as shown
in Table 2. Among them, the user actions ?others?,
?not related?, and ?not known? are special. Specif-
ically, to limit the question length and to ensure the
106
Component Design Comments
System State (Product, User action) e.g., (HP Computer, Category: computer)
Machine Action Question on Category e.g., choose category: Electronics, Movie, Book
Question on Product name e.g., choose product name: Canon SD750 digital cam-
era, Canon Powershot A40 digital camera, Canon
SD950 digital camera, Others
Question on Attribute e.g., choose memory size: 64M, 128M, 256M
Confirmation question e.g., you want Canon SD750 camera, yes or no?
Play Rating e.g., I think you want Canon SD750 digital camera,
here is the rating!
User Action Category e.g., Movie
Product name e.g., Canon SD750 digital camera
Attribute value e.g., memory size: 64M
Others used when a question has too many possible options
Yes/No used for a confirmation question
Not related used if the intended product is unrelated to the question
Not known used if the user does not have required knowledge to
answer the question
Table 2: State Diagram Design in Voice-Rate
human is able to memorize all the options, we re-
strict the number of options in a single question to a
threshold N (e.g., 5). Clearly, given a list of prod-
ucts and a question, there might be more than N pos-
sible options. In such a case, we need to merge some
options into the ?others? class. The third example in
Table 2 shows an example with the ?others? option.
One may exploit a clustering algorithm (e.g., an it-
erative greedy search algorithm) to find an optimal
merge. In our system, we simply take the top-(N -1)
options (ranked by the belief probabilities) and treat
all the remaining options as ?others?.
The ?not related? option is required when some
candidate products are irrelevant to the question. For
example, when the system asks a question regarding
the attribute ?cpu speed? while the products contain
both books and computers, the ?not related? option
is required in case the intended product is a book.
Lastly, while some attributes are very useful to
distinguish the products, a user may not have enough
knowledge to answer a question on these attributes.
For example, while there is a unique part number for
each product, however, the user may not know the
exact part number for the intended product. Thus,
?not known? option is required whenever the system
expects the user is unable to answer the question.
4.1.2 Models
We assume that the user does not change his goal
(i.e., the intended product) along the dialog. We
also assume that the user rationally answers the
question to achieve his goal. Additionally, we as-
sume that the speech synthesis is good enough such
that the user always gets the right information that
the system intends to convey. The two main mod-
els that we consider include an observation model
that captures speech recognition uncertainty, and a
user knowledge model that captures the variability
of user knowledge required for answering questions
on different attributes.
Observation Model: Since the speech recogni-
tion engine we are using returns only a one-best and
its confidence value C ? [0, 1]. We define the obser-
vation function as follows,
P (a?u|au) =
{
C if a?u = au,
1?C
|Au|?1 otherwise.
(2)
where au is the true user action, a?u is the speech
recognition output (i.e., corrupted user action), and
Au is the set of user actions related to the last ma-
chine action.
User Knowledge Model: In most of the appli-
cations (Roy et al, 2000; Williams, 2007) where
107
the POMDP framework got applied, it is normally
assumed that the user needs only common sense to
answer the questions asked by the dialog system.
Our application is more complex as the product in-
formation is very rich. A user may have different
difficulty in answering different questions. For ex-
ample, while a user can easily answer a question on
category, he may not be able to answer a question
on the part number. Thus, we define a user knowl-
edge model to capture such uncertainty. Specifically,
given a question (say am) and an intended product
(say gu) in the user?s mind, we want to know how
likely the user has required knowledge to answer the
question. Formally, the user knowledge model is,
P (au|gu, am) =
?
??
??
P (unk|gu, am) if au=unk,
1? P (unk|gu, am) if au=truth,
0 otherwise.
(3)
where unk represents the user action ?not known?.
Clearly, given a specific product gu and a specific
question am, there is exactly one correct user ac-
tion (represented by truth in Equation 3), and its
probability is 1 ? P (unk|gu, am). Now, to obtain
a user knowledge model, we only need to obtain
P (unk|gu, am). As shown in Table 2, there are four
kinds of question-type machine actions am. We as-
sume that the user always has knowledge to answer
a question regarding the category and product name,
and thus P (unk|gu, am) for these types of machine
actions are zero regardless of what the specific prod-
uct gu is. Therefore, we only need to consider
P (unk|gu, am) when am is a question about an at-
tribute (say attr). Moreover, since there are millions
of products, to deal with the data sparsity issue, we
assume P (unk|gu, am) does not depends on a spe-
cific product gu, instead it depends on only the cate-
gory (say cat) of the product gu. Therefore,
P (unk|gu, am) ? P (unk|cat,attr). (4)
Now, we only need to get the probability
P (unk|cat,attr) for each attribute attr in each cate-
gory cat. To learn P (unk|cat,attr), one may collect
data from human, which is very expensive. Instead,
we learn this model from a database of online re-
views for the products. Our method is based on the
following intuition: if a user cares/knows about an
attribute of a product, he will mention either the at-
tribute name, or the attribute value, or both in his
review of this product. With this intuition, the occur-
rence frequency of a given attr in a given category
cat is collected from the review database, followed
by proper weighting, scaling and normalization, and
thus P (unk|cat,attr) is obtained.
4.2 Belief Update
Based on the model assumptions in Section 4.1.2,
the belief update formula for the state (gu, a?u) is,
~b(gu, a?u) = (5)
k ? P (a??u|a
?
u)P (a
?
u|gu, am)
?
au?A(gu)
~b(gu, au)
where k is a normalization constant. The P (a??u|a
?
u)
is the observation function as defined in Equation 2,
while P (a?u|gu, am) is the user knowledge model as
defined in Equation 3. The A(gu) represents the set
of user actions au related to the system states for
which the intended product is gu.
In our state representation, a single product gu
is associated with several states which differ in the
user action au, and the belief probability of gu is the
sum of the probabilities of these states. Therefore,
even there is a speech recognition error or an un-
intentional user mistake, the true product still gets
a non-zero belief probability (though the true/ideal
user action au gets a zero probability). Moreover,
the probability of the true product will get promoted
through later iterations. Therefore, our system has
error-handling capability, which is one of the major
advantages over the deterministic baseline system.
4.3 Optimal Action Selection
As mentioned in Section 3.2, the optimal action se-
lection involves three sub-components: a prediction
algorithm, a goodness measure, and a search algo-
rithm. Ideally, in our application, we should mini-
mize the time required to successfully identify the
intended product. Clearly, this is too difficult as
it needs to predict the infinite future and needs to
encode the time into a reward function. Therefore,
for simplicity, we predict only one-step forward, and
use the entropy as a goodness measure1. Formally,
1Due to this approximation, one may argue that our model
is more like the greedy information theoretic model in Paek and
Chickering (2005), instead of a POMDP model. However, we
believe that our model follows the POMDP modeling frame-
work in general, though it does not involve reinforcement learn-
ing currently.
108
the optimization function is as follows:
a? = argmin
a?A
H(Products | a), (6)
where H(Products | a) is the entropy over the belief
probabilities of the products if the machine action
a was taken. When predicting the belief vector us-
ing Equation 5, we consider only the user knowledge
model and ignore the observation function2.
In the above, we consider only the question-type
machine actions. We also need to decide when
to take the play rating action such that the dialog
will terminate. Specifically, we take the play rating
action whenever the belief probability of the most
probable product is greater than a threshold. More-
over, the threshold should depend on the number of
surviving products. For example, if there are fifty
surviving products and the most probable product
has a belief probability greater than 0.3, it is reason-
able to take the play rating action. This is not true
if there are only four surviving products. Also note
that if we set the thresholds to too small values, the
system may play the rating for a wrong product. We
will use a development set to tune these thresholds.
4.3.1 Machine Action Filtering during Search
We use an exhaustive linear search for the opera-
tor argmin in Equation 6. However, additional filter-
ing during the search is required.
Repeated Question: Since the speech response
from the user to a question is probabilistic, it is quite
possible that the system will choose the same ques-
tion that has been asked in previous stages3. Since
our product information is very rich, many differ-
ent questions have the similar capability to reduce
entropy. Therefore, during the search, we simply ig-
nore all the questions asked in previous stages.
?Not Related? Option: While reducing entropy
helps to reduce the confusion at the machine side, it
does not measure the ?weirdness? of a question to
the human. For example, when the intended product
is a book and the candidate products contain both
books and computers, it is quite possible that the
optimal action, based solely on entropy reduction,
2Note that we ignore the observation function only in the
prediction, not in real belief update.
3In a regular decision tree, the answer to a question is deter-
ministic. It never asks the same question as that does not lead to
any additional reduction of entropy. This problem is also due to
the fact we do not have an explicit reward function.
is a question on the attribute ?cpu speed?. Clearly,
such a question is very weird to the human as he is
looking for a book that has nothing related to ?cpu
speed?. Though the user may be able to choose the
?not related? option correctly after thinking for a
while, it degrades the dialog quality. Therefore, for
a given question, whenever the system predicts that
the user will have to choose the ?not related? option
with a probability greater than a threshold, we sim-
ply ignore such questions in the search. Clearly, if
we set the threshold as zero, we essentially elimi-
nates the ?not related? option. That is, at each stage,
we generate questions only on attributes that apply
to all the candidate products. Since we dynamically
remove products whose probability is smaller than
a threshold at each stage, the valid question set dy-
namically expands. Specifically, at the beginning,
only very general questions (e.g., questions on cate-
gory) are valid, then more refined questions become
valid (e.g., questions on product brand), and finally
very specific questions are valid (e.g, questions on
product model). This leads to very natural behav-
ior in identifying a product, i.e., coarse to fine4. It
also makes the system adapt to the user knowledge.
Specifically, as the user demonstrates deeper knowl-
edge of the products by answering the questions cor-
rectly, it makes sense to ask more refined questions
about the products.
5 Simulation Results
To evaluate system performance, ideally one should
ask people to call the system, and manually collect
the performance data. This is very expensive. Al-
ternatively, we develop a simulation method, which
is automatic and thus allow fast evaluation of the
system during development5. In fact, many design
choices in Section 4 are inspired by the simulation.
5.1 Simulation Model
Figure 2 illustrates the general framework for the
simulation. The process is very similar to that in
Figure 1 except that the human user and the speech
4While the baseline dialog manager achieves the similar be-
havior by manually enforcing the order of questions, the sys-
tem here automatically discovers the order of questions and the
question set is much more richer than that in the baseline.
5However, we agree that simulation is not without its limi-
tations and the results may not precisely reflect real scenarios.
109
  
Yes 
Begin 
Information Retrieval 
Dialog Manager 
? Baseline 
? POMDP 
End 
Initial Query 
List of Products 
Corrupted User Action 
Simulated User 
? Intended product  
? User knowledge model 
Simulated  
Speech Recognizer 
User Action 
Found 
product? No  
Play Rating 
Question 
Figure 2: Flow Chart in Simulation
recognizer are replaced with a simulated compo-
nent, and that the simulated user has access to a user
knowledge model. In particular, we generate the
user action and its corrupted version using random
number generators by following the models defined
in Equations 3 and 2, respectively. We use a fixed
value (e.g., 0.9) for C in Equation 2.
Clearly, our goal here is not to evaluate the good-
ness of the user knowledge model or the speech rec-
ognizer. Instead, we want to see how the probabilis-
tic dialog manger (i.e., POMDP) performs compared
with the deterministic baseline dialog manager, and
to see whether the richer attribute information helps
to reduce the dialog interaction time.
5.2 Data Resources
In the system, we use three data resources: a prod-
uct database, a review database, and a query-click
database. The product database contains detailed in-
formation for 0.2 million electronics and computer
related products. The review database is used for
learning the user knowledge model. The query-
click database contains 2289 pairs in the format (text
query, product clicked). One example pair is (Canon
Powershot A700, Canon Powershot A700 6.2MP
digital camera). We divide it into a development set
(1308 pairs) and a test set (981 pairs).
5.3 Results on Information Retrieval
For each initial query, the information retrieval
(IR) engine returns a list of top-ranked products.
Whether the intended product is in the returned list
depends on the size of the list. If the intended prod-
uct is in the list, the IR successfully recalled the
product. Table 3 shows the correlation between the
recall rate and the size of the returned list. Clearly,
the larger the list size is, the larger the recall rate is.
One may notice that the IR recall rate is low. This
is because the query-click data set is very noisy, that
is, the clicked product may be nothing to do with
the query. For example, (msn shopping, Handspring
Treo 270) is one of the pairs in our data set.
List Size Recall Rate (%)
50 38.36
100 41.46
150 43.5
Table 3: Information Retrieval Recall Rates on Test set
5.4 Dialog System Configuration and Tuning
As mentioned in Section 4, several parameters in the
system are configurable and tunable. Specifically,
we set the max number of options in a question as
5, and the threshold for ?not related? option as zero.
We use the development set to tune the following pa-
rameters: the threshold of the belief probability be-
low which the product is pruned, and the thresholds
above which the most probable product is played.
The parameters are tuned in a way such that no dia-
log error is made on the development set.
5.5 Results on Error Handling
Even the IR succeeds, the dialog system may not
find the intended product successfully. In particu-
lar, the baseline system does not have error handling
capability. Whenever the system makes a speech
recognition error or the user mistakenly answers a
question, the dialog system fails (either plays the rat-
ing for a wrong product or fails to find any product).
On the contrary, our POMDP framework has error
handling functionality due to its probabilistic na-
ture. Table 5 compares the dialog error rate between
the baseline and the POMDP systems. Clearly,
the POMDP system performs much better to han-
dle errors. Note that the POMDP system does not
eliminate dialog failures on the test set because the
thresholds are not perfect for the test set6. This is
due to two reasons: the system may prune the in-
tended product (reason-1), and the system may play
the rating for a wrong product (reason-2).
6Note that the POMDP system does not have dialog failures
on the development set as we tune the system in this way.
110
System Size Average MaxStages Characters Words Stages Characters Words
Baseline
50 2.44 524.0 82.3 11 2927 546
100 3.37 765.4 120.4 25 7762 1369
150 3.90 906.4 143.0 30 9345 1668
POMDP
50 1.57 342.8 54.3 4 2659 466
100 2.36 487.9 76.6 18 3575 597
150 2.59 541.3 85.0 19 4898 767
Table 4: Interaction Time Results on Test Set
Size Baseline POMDP (%)(%) Total Reason-1 Reason-2
50 13.8 8.2 4.2 4.0
100 17.7 2.7 1.2 1.5
150 19.3 4.7 0.7 4.0
Table 5: Dialog Failure Rate on Test Set
5.6 Results on Interaction Time
It is quite difficult to measure the exact interaction
time, so instead we measure it through the number of
stages/characters/words required during the dialog
process. Clearly, the number of characters is the one
that matches most closely to the true time. Table 4
reports the average and maximum numbers. In gen-
eral, the POMDP system performs much better than
the baseline system. One may notice the difference
in the number of stages between the baseline and
the POMDP systems is not as significant as in the
number of characters. This is because the POMDP
system is able to exploit very short questions while
the baseline system mainly uses the product name
question, which is normally very long. The long
question on product name also imposes difficulty in
speech synthesis, user input, and speech recognition,
though this is not reflected in the simulation.
6 Conclusions
In this paper, we have applied the POMDP frame-
work into Voice-Rate, a system through which a
user can call to get product information (e.g., price,
rating, review, etc.). We have proposed a novel
method to learn a user knowledge model from a re-
view database. Compared with a deterministic base-
line system (Zweig et al, 2007), the POMDP system
is probabilistic and is able to handle speech recogni-
tion errors and user mistakes, in which case the de-
terministic baseline system is doomed to fail. More-
over, the POMDP system exploits richer product in-
formation to reduce the interaction time required to
complete a dialog. We have developed a simulation
model, and shown that the POMDP system improves
the baseline system significantly in terms of both di-
alog failure rate and dialog interaction time. We also
implement our POMDP system into a speech demo
and plan to carry out tests through humans.
Acknowledgement
This work was conducted during the first author?s
internship at Microsoft Research; thanks to Dan Bo-
hus, Ghinwa Choueiter, Yun-Cheng Ju, Xiao Li,
Milind Mahajan, Tim Paek, Yeyi Wang, and Dong
Yu for helpful discussions.
References
K. Murphy. 2000. A survey of POMDP solution tech-
niques. Technical Report, U. C. Berkeley.
T. Paek and D. Chickering. 2005. The Markov assump-
tion in spoken dialogue management. In Proc of SIG-
dial 2005.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog
management for robots. In Proc of ACL 2000.
M. Spaan and N. Vlassis. 2005. Perseus: randomized
point-based value iteration for POMDPs. Journal of
Artificial Intelligence Research, 24:195-220.
J. Williams. 2007. Applying POMDPs to Dialog
Systems in the Troubleshooting Domain. In Proc
HLT/NAACL Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technology.
J. Williams and S. Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language 21(2): 231-
422.
G. Zweig, P. Nguyen, Y.C. Ju, Y.Y. Wang, D. Yu, and
A. Acero. 2007. The Voice-Rate Dialog System for
Consumer Ratings. In Proc of Interspeech 2007.
111
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10?18,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Scalable Decoder for Parsing-based Machine Translation
with Equivalent Language Model State Maintenance
Zhifei Li and Sanjeev Khudanpur
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and khudanpur@jhu.edu
Abstract
We describe a scalable decoder for parsing-
based machine translation. The decoder is
written in JAVA and implements all the es-
sential algorithms described in Chiang (2007):
chart-parsing, m-gram language model inte-
gration, beam- and cube-pruning, and unique
k-best extraction. Additionally, parallel
and distributed computing techniques are ex-
ploited to make it scalable. We also propose
an algorithm to maintain equivalent language
model states that exploits the back-off prop-
erty of m-gram language models: instead of
maintaining a separate state for each distin-
guished sequence of ?state? words, we merge
multiple states that can be made equivalent for
language model probability calculations due
to back-off. We demonstrate experimentally
that our decoder is more than 30 times faster
than a baseline decoder written in PYTHON.
We propose to release our decoder as an open-
source toolkit.
1 Introduction
Large-scale parsing-based statistical machine trans-
lation (MT) has made remarkable progress in the
last few years. The systems being developed differ
in whether they use source- or target-language syn-
tax. For instance, the hierarchical translation sys-
tem of Chiang (2007) extracts a synchronous gram-
mar from pairs of strings, Quirk et al (2005), Liu et
al. (2006) and Huang et al (2006) perform syntac-
tic analyses in the source-language, and Galley et al
(2006) use target-language syntax.
A critical component in parsing-based MT sys-
tems is the decoder, which is complex to imple-
ment and scale up. Most of the systems described
above employ tailor-made, dedicated decoders that
are not open-source, which results in a high barrier
to entry for other researchers in the field. How-
ever, with the algorithms proposed in (Huang and
Chiang, 2005; Chiang, 2007; Huang and Chiang,
2007), it is possible to develop a general-purpose de-
coder that can be used by all the parsing-based sys-
tems. In this paper, we describe an important first-
step towards an extensible, general-purpose, scal-
able, and open-source parsing-based MT decoder.
Our decoder is written in JAVA and implements all
the essential algorithms described in Chiang (2007):
chart-parsing, m-gram language model integration,
beam- and cube-pruning, and unique k-best extrac-
tion. Additionally, parallel and distributed comput-
ing techniques are exploited to make it scalable.
Straightforward integration of an m-gram lan-
guage model (LM) into a parsing-based decoder
substantially increases its computational complex-
ity. Therefore, it is important to develop efficient
methods for LM integration. We propose an algo-
rithm to maintain equivalent LM states by exploit-
ing the back-off property of m-gram LMs. Specifi-
cally, instead of maintaining a separate state for each
distinguished sequence of ?state? words, we merge
multiple states that can be made equivalent for LM
calculations by anticipating such back-off.
We demonstrate experimentally that our decoder
is 38 times faster than a previous decoder written in
PYTHON. Furthermore, the distributed computing
permits improving translation quality via large-scale
LMs. We have successfully use our decoder to trans-
late about a million sentences in a parallel corpus for
large-scale discriminative training experiments.
10
2 Parsing-based MT Decoder
In this section, we discuss the core algorithms imple-
mented in our decoder. These algorithms have been
discussed by Chiang (2007) in detail, and we reca-
pitulate the essential parts here for completeness.
2.1 Grammar Formalism
Our decoder assumes a probabilistic synchronous
context-free grammar (SCFG). Following the nota-
tion in Venugopal et al (2007), a probabilistic SCFG
comprises a set of source-language terminal sym-
bols TS , a set of target-language terminal symbols
TT , a shared set of nonterminal symbols N , and a
set of rules of the form
X ? ??, ?,?, w? , (1)
where X ? N , ? ? [N?TS ]? is a (mixed) sequence
of nonterminals and source terminals, ? ? [N?TT ]?
is a sequence of nonterminals and target terminals,
? is a one-to-one correspondence or alignment be-
tween the nonterminal elements of ? and ?, and
w ? 0 is a weight assigned to the rule. An illus-
trative rule for Chinese-to-English translation is
NP ? ?NP0{ NP1 , NP1 of NP0 ? ,
where the Chinese word { (pronounced de or di)
means of, and the alignment, encoded via subscripts
on the nonterminals, causes the two noun phrases
around { to be reordered around of in the transla-
tion. The rule weight is omitted in this example.
A bilingual SCFG derivation is analogous to a
monolingual CFG derivation. It begins with a pair
of aligned start symbols. At each step, an aligned
pair of nonterminals is rewritten as the two corre-
sponding components of a single rule. In this sense,
the derivations are generated synchronously.
Our decoder presently handles SCFGs of the kind
extracted by Heiro (Chiang, 2007), but is easily ex-
tensible to more general SCFGs and closely related
formalisms such as synchronous tree substitution
grammars (Eisner, 2003; Chiang, 2006).
2.2 MT Decoding as Chart Parsing
Given a source-language sentence f?, the decoder
must find the target-language yield e(D) of the best
derivation D among all derivations with source-
language yield f(D) = f?, i.e.
e? = e
(
arg max
D : f(D)=f?
w(D)
)
, (2)
where w(D) is the composite weight of D.
The parser may be treated as a deductive proof
system (Shieber et al, 1995). Formally (cf. (Chiang,
2007)), a parser defines a space of weighted items,
with some items designated as axioms and some as
goals, and a set of inference rules of the form
I1 : w1 ? ? ? Ik : wk
I : w ? ,
which states that if all the antecedent items Ii are
provable, respectively with weight wi, then the con-
sequent item I is provable with weight w, provided
the side condition ? holds. For a grammar with a
maximum of two (pairs of) nonterminals per rule1,
Figure 1 illustrates the resulting chart parsing proce-
dure, including the integration of an m-gram LM.
The actual decoding algorithm maintains a chart,
which contains an array of cells. Each cell in turn
maintains a list of proved items. The parsing process
starts with the axioms, and proceeds by applying the
inference rules to prove more and more items until
a goal item is proved. Whenever the parser proves a
new item, it adds the item to the appropriate chart
cell. It also maintains backpointers to antecedent
items, which are used for k-best extraction, as dis-
cussed in Section 2.4 below.
In a SCFG-based decoder, an item is identi-
fied by its source-language span, left-side non-
terminal label, and left- and right-context for the
target-language m-gram LM. Therefore, in a given
cell, the maximum possible number of items is
O(|N ||TT |2(m?1)), and the worst case decoding
complexity is
O
(
|N |K |TT |2K(m?1)n3
)
, (3)
where K is the maximum number of nonterminal
pairs per rule and n is the source-language sentence
length (Venugopal et al, 2007).
1For more general grammars with K ? 2 pairs of non-
terminals per rule, see Venugopal et al (2007).
11
X???,??:w (X ? ??, ?,w?) ? G
X??fji+1, ?? : w
[X, i, j; q(?)] : wp(?)
Z??f i1i+1Xfjj1+1, ?? : w [X,i1,j1;e1] : w1
[Z, i, j; q(?? )] : ww1p(?? ) ?
? = ?[e1/X]
Z??f i1i+1X1f
i2
j1+1Y2f
j
j2+1, ?? : w [X,i1,j1;e1] : w1 [Y,i2,j2;e2] : w2
[Z, i, j; q(?? )] : ww1w2p(?? ) ?
? = ?[e1/X1, e2/Y2]
Goal item: [S, 0, n; ?s?m?1 ? e?/s?]
Figure 1: Inference rules from Chiang (2007) for a parser with an m-gram LM. G denotes the translation grammar.
w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(?) provides the LM
probability for all complete m-grams in a string, while the function q(?) elides symbols whose m-grams have been
accounted for by p(?). Details about the functions p(?) and q(?) are provided in Section 4.
2.3 Pruning in a Decoder
Severe pruning is needed in order to make the decod-
ing computationally feasible for SCFGs with large
vocabularies TT and detailed nonterminal sets. In
our decoder, we incorporate two pruning techniques
described by (Chiang, 2007; Huang and Chiang,
2007). For beam pruning, in each cell, we discard
all items whose weight is worse, by a relative thresh-
old ?, than the weight of the best item in the same
cell. If too many items pass the threshold, a cell only
retains the top-b items by weight. When combining
smaller items to obtain a larger item by applying an
inference rule, we use cube-pruning to simulate k-
best extraction in each destination cell, and discard
combinations that lead to an item whose weight is
worse than the best item in that cell by a margin of
?.
2.4 k-best Extraction Over Hyper-graphs
For each source-language sentence f?, the output
of the chart-parsing algorithm may be treated as a
hyper-graph representing a set of likely hypotheses
D in (2). Briefly, a hyper-graph is a set of vertices
and hyper-edges, with each hyper-edge connecting
a set of antecedent vertices to a consequent vertex,
and a special vertex designated as the target vertex.
In parsing parlance, a vertex corresponds to an item
in the chart, a hyper-edge corresponds to a SCFG
rule with the nonterminals on the right-side replaced
by back-pointers to antecedent items, and the target
vertex corresponds to the goal item2.
Given a hyper-graph for a source-language sen-
tence f?, we use the k-best extraction algorithm of
Huang and Chiang (2005) to extract its k most likely
translations. Moreover, since many different deriva-
tions D in (2) may lead to the same target-language
yield e(D), we adopt the modification described in
Huang et al (2006) to efficiently generate the unique
k best translations of f?.
3 Parallel and Distributed Computing
Many applications of parsing-based MT entail the
use of SCFGs extracted from millions of bilin-
gual sentence pairs and LMs extracted from bil-
lions of words of target-language text. This requires
the decoder to make use of distributed computing
to spread the memory required to load large-scale
SCFGs and LMs onto multiple processors. Further-
more, techniques such as iterative minimum error-
rate training (Och et al, 2003) as well as web-based
MT services require the decoder to translate a large
number of source-language sentences per unit time.
This requires the decoder to make use of parallel
computing to utilize each individual multi-core pro-
cessor more effectively. We have incorporated two
such performance enhancements in our decoder.
2In a decoder integrating an m-gram LM, there may be mul-
tiple goal items due to different LM contexts. However, one can
image a single goal item identified by the span [0, n] and the
goal nonterminal S, but not by the LM contexts.
12
3.1 Parallel Decoding
We have enhanced our decoder to translate multiple
source-language sentences in parallel by exploiting
the ability of a multi-core processor to concurrently
run several threads that share memory. Specifi-
cally, given one (or more) document(s) containing
multiple source-language sentences, the decoder au-
tomatically splits the set of sentences into several
subsets, and initiates concurrent decoding threads;
once all the threads finish, the main thread merges
back the translations. Since all the threads naturally
share memory, the decoder needs to load the (large)
SCFG and LM into memory only once. This multi-
threading provides a very significant speed-up.
3.2 Distributed Language Models
It is not possible in some cases to load a very large
LM into memory on a single machine, particularly
if the SCFG is also very large. In other cases, load-
ing the LM each time the decoder runs may be too
time-consuming relative to the time required for de-
coding itself, such as in iterative decoding with up-
dated combination weights during minimum error-
rate training. It is therefore desirable to have dedi-
cated servers to load parts of the LM3 ? an idea that
has been exploited by (Zhang et al, 2006; Emami et
al., 2007; Brants et al, 2007).
Our implementation can load a (partitioned) LM
on different servers before initiating decoding. The
decoder remotely calls the servers to obtain individ-
ual LM probabilities, and linearly interpolates them
on the fly using a given set of interpolation weights.
With this architecture, one can deal with a very large
target-language text corpus by splitting it into many
parts and training separate LMs from each. The run-
time interpolation capability may also be used for
LM adaptation, e.g. for building document-specific
language models.
To mitigate potential network communication de-
lays inherent to a distributed LM, we implement a
simple cache mechanism in the decoder. The cache
saves the outcomes of the most recent LM calls,
including interpolated LM probabilities; the cache
is reset whenever its size exceeds a threshold. We
could have maintained a cache at each LM server
as well; however, the resultant saving is not signif-
3Similarly, distributing the SCFG is also possible.
icant because the trie data-structures used to imple-
ment m-gram LMs are quite fast relative to the cache
lookup overhead.
4 Equivalent LM-state Maintenance
It is clear from the complexity (3) of the inference
rules (Figure 1) that a straightforward integration
of an m-gram LM adds a multiplicative factor of
|TT |2K(m?1) to the computational complexity of the
decoder, where TT is the set of target-language ter-
minal symbols. We illustrate in this section how this
potentially very large multiplier can be dramatically
reduced by exploiting the structure of the LM.
4.1 Applying an m-gram LM in the Decoder
Integrating an LM into chart parsing requires two
functions p(?) and q(?) (see Figure 1) that oper-
ate on strings over TT ? {?}, where ? is a special
?placeholder? symbol for an elided part of a target-
language string.
The function p(e) calculates the LM probability
of the complete m-grams in e ? e1 . . . el, i.e.
p(e1 . . . el) =
?
m? i? l & ? 6? eii?(m?1)
PLM(ei |hi) , (4)
where hi = ei?(m?1) . . . ei?1 is the m?1-word
?LM history? of the target-language word ei.
Since the p-probability of e does not include the
LM probability for the partial m-grams (i.e., the first
(m? 1) words) of e, the exact weights of two items
[X, i, j; e] and [X, i, j; e?] in the chart are not avail-
able during the bottom-up pruning of Section 2.3.
Therefore, as an approximation, we also compute
p?(e) =
min{m?1, |e|}?
k=1
PLM(ek | e1 . . . ek?1), (5)
an estimate of the LM probability of the m?1-gram
prefix of e. This estimated probability is taken into
account for pruning purposes (only).
The function q(e1 . . . el) determines the left and
right LM states that must be maintained for future
computation of the exact LM probability, respec-
tively, of e1 . . . em?1 and el+1 . . . el+m?1.
q(e1 . . . el) (6)
=
{
e1 . . . el if l < m? 1,
e1 . . . em?1 ? el?(m?2) . . . el otherwise.
13
4.2 Back-off Parameterization of m-gram LMs
While many different methods are popular for esti-
mating m-gram LMs, most store the estimated LM
parameters in the ARPA back-off file format; using
the notation eji to denote a target-language word se-
quence ei ei+1 . . . ej , the LM probability calcula-
tion is carried out as
PBO(em | em?11 ) (7)
=
{
pi(em1 ) if em1 ? LM
?(em?11 )? PBO(em | em?12 ) otherwise,
where the lower order probability PBO(em | em?12 )
is recursively defined in the same way, and ?(em?11 )
is the back-off weight of the history. The LM file
contains the parameter pi(?) for each listed m-gram,
and the parameters pi(?) and ?(?) for each listed m?-
gram, 1 ? m? < m; for unlisted m?-grams, ?(?) = 1
by definition.
Observe from (7) that if em1 is not listed in the LM,
the back-off weight ?(?) is the same for all words
em, and the backed-off probability PBO(em | ?) is the
same for all words e1. Furthermore, as m grows, the
fraction of possible m-grams actually observed in a
training corpus diminishes rapidly.
4.3 The Equivalent LM State of an Item
The maximum possible number of items in a cell in-
creases exponentially with the LM order m, as dis-
cussed in Section 2.2. With pruning (cf. Section
2.3), we restrict the maximum number of items in
each cell to some threshold b. Intuitively, therefore,
if we increase the LM order m, we should also in-
crease the beam size b to reduce search errors. This
could slow down the decoder significantly.
Recall from the previous subsection, however,
that when m increases, the fraction of m-grams
that will need to back-off also increases. Moreover,
even for modest values of m, the decoder consid-
ers many ?unseen? m-grams (due to reordering and
translation combinations) that do not appear in natu-
ral texts, leading to frequent back-off during the LM
probability calculation (7). In this subsection, we
propose a method to collapse equivalent LM states
so that the decoder effectively considers many more
items in each cell without increasing beam size.
We merge multiple LM states (6) that already
have?or back-off to?the same ?LM history? in
the calculation (7) of LM probabilities, e.g. due
to different unlisted m-grams that back-off to the
same m?1-gram. For simplicity, we only consider
LM state merging by the function q(?) of (6) when
l ? m?1.
Though the equivalent LM state maintenance
technique is discussed here in the context of a
parsing-based MT decoder, it is also applicable to
standard left-to-right phrase-based decoders. In par-
ticular, the right-side equivalent LM state mainte-
nance proposed in Section 4.3.1 may be used.
4.3.1 Obtaining the Equivalent Right LM State
Recall that the right LM state ell?(m?2) of el1
serves as the ?LM history? for calculating the ex-
act LM probabilities of the yet-to-be-determined
word el+1. Recall further the computation (7) of
PBO(el+1 | ell?(m?2)).
? If the m-gram el+1l?(m?2) is not listed in the LM
for any word el+1, then the LM will back-off to
PBO(el+1 | ell?(m?3)), which does not depend
on the word el?(m?2).
? If the m?1-gram ell?(m?2) also is not listed in
the LM, then ?(ell?(m?2)) = 1.
If these two conditions hold true, q(?) may safely
elide the word el?(m?2) in (6) no matter what words
follow el1. The right LM state is thus reduced from
m? 1 words to m? 2 words.
The argument above can be applied recursively
to the resulting right LM state ell?(m?2)+i, where
i ? [0,m ? 2], leading to the equivalent right state
computation procedure of Figure 2. The procedure
IS-A-PREFIX(em?1 ) checks if its argument em?1 is a pre-
fix of any k-gram listed in the LM, k ? [m?,m].
4.3.2 Obtaining the Equivalent Left LM State
Recall that the left LM state em?11 of el1 is
the prefix whose exact LM probability is unknown
during bottom-up parsing, and is replaced by the
estimated probability p?(em?11 ) of (5) for pruning
purposes. Recall further the computation (7) of
PBO(em?1 | em?20 ).
? If the m-gram em?10 is not listed in the LM
for any word e0, then it will back-off to
14
EQ-R-STATE (ell?(m?2))
1 ers ? ell?(m?2)
2 for i ? 0 to m? 2 ? left to right
3 if IS-A-PREFIX (ell?(m?2)+i)
4 break ? stop reducing ers
5 else
6 ers ? ell?(m?2)+i+1 ? reduce state
7 return ers
Figure 2: Equivalent Right LM State Computation.
PBO(em?1 | em?21 ), which can be computed
right away based on em?11 without waiting for
the unknown e0. Moreover, the back-off weight
?(em?20 ) does not depend on the word em?1.
Therefore, q(?) may safely elide the word em?1, and
reduce the left LM state in (6) from em?11 to em?21 .
Also, p(?) should also co-opt PBO(em?1 | em?21 ) into
the complete m-gram probability of (4) and p?(?)
should exclude em?1 in (5).
The argument above can again be applied recur-
sively to the resulting left LM state ei1, i ? [1,m?1],
leading to the equivalent left state procedure of Fig-
ure 3. The procedure IS-A-SUFFIX(em?1 ) checks if
em?1 is a suffix of any listed k-gram in the LM, k ?
[m?,m]. In Figure 3, fin refers to the probability
that can be computed right away based on the state
itself, for co-opting into the complete m-gram prob-
ability of (4) as mentioned above.
4.3.3 Modified Cost Functions for Parsing
When carrying out the reduction of the left and
right LM states to their shortest equivalents, the for-
mula (4) for calculating the probability of the com-
plete m-grams in an item [X, i, j; e], where e = el1,
is modified as
p(el1)
= EQ-L-STATE(em?11 ).fin?
?
m? i? l & ? 6? eii?(m?1)
PLM(ei |hi)
with the further qualification that some care must be
taken later to incorporate the back-off weights of the
?LM histories? of the suffix of em?11 that went miss-
ing due to left LM state reduction.
EQ-L-STATE (em?11 )
1 els ? em?11
2 fin ? 1 ? update to final probability p
3 for i ? m? 1 to 1 ? right to left
4 if IS-A-SUFFIX(ei1)
5 break ? stop reducing els
6 else
7 fin ? PBO(ei | ei?11 )? fin
8 els ? ei?11 ? reduce state
9 return els, fin
Figure 3: Equivalent Left LM State Computation.
The estimated probability of the left LM state is
modified as
p?(e) =
{
p?(e) if |e| < m? 1
p?(EQ-L-STATE(em?11 ).els) otherwise,
with p? as defined in (5).
Finally, the LM state function is
q(el1)
=
?
???
???
e1 . . . el if l < m? 1
EQ-L-STATE(em?11 ).els ?
EQ-R-STATE(ell?(m?2)).ers otherwise.
4.3.4 Suffix and Prefix Look-Up
As done in the SRILM toolkit (Stolcke, 2002), a
back-off m-gram LM is stored using a reverse trie
data structure. We store the suffix and prefix infor-
mation in the same data structure without incurring
much additional memory cost. Specifically, the pre-
fix information is stored at the back-off state, while
the suffix information is stored as one bit alongside
the regular m-gram probability.
5 Experimental Results
In this section, we evaluate the performance of our
decoder on a Chinese to English translation task.
5.1 System Training
We use various parallel text corpora distributed by
the Linguistic Data Consortium (LDC) for the NIST
MT evaluation. The parallel data we select contains
about 570K Chinese-English sentence pairs, adding
15
up to about 19M words on each side. To train the
English language models, we use the English side
of the parallel text and a subset of the English Giga-
word corpus, for a total of about 130M words.
We use the GIZA toolkit (Och and Ney, 2000),
a suffix-array architecture (Lopez, 2007), the
SRILM toolkit (Stolcke, 2002), and minimum er-
ror rate training (Och et al, 2003) to obtain word-
alignments, a translation model, language models,
and the optimal weights for combining these mod-
els, respectively.
5.2 Improvements in Decoding Speed
We use a PYTHON implementation of a state-of-
the-art decoder as our baseline4 for decoder compar-
isons. For a direct comparison, we use exactly the
same models and pruning parameters. The SCFG
contains about 3M rules, the 5-gram LM explicitly
lists about 49M k-grams, k = 1, 2, . . . , 5, and the
pruning uses ? = 10, b = 30 and ? = 0.1.
Decoder Speed BLEU-4(sec/sent) MT ?03 MT ?05
Python 26.5 34.4% 32.7%
Java 1.2 34.5% 32.9%Java (parallel) 0.7
Table 1: Decoder Comparison: Translation speed and
quality on the 2003 and 2005 NIST MT benchmark tests.
As shown in Table 1, the JAVA decoder (without
explicit parallelization) is 22 times faster than the
PYTHON decoder, while achieving slightly better
translation quality as measured by BLEU-4 (Pap-
ineni et al, 2002). The parallelization further speeds
it up by a factor of 1.7, making the parallel JAVA de-
coder is 38 times faster than the PYTHON decoder.
We have used the decoder to successfully decode
about one million sentences for a large-scale dis-
criminative training experiment.
5.3 Impact of a Distributed Language Model
We use the SRILM toolkit to build eight 7-gram lan-
guage models, and load and call the LMs using a
4We are extremely thankful to Philip Resnik at University of
Maryland for allowing us the use of their PYTHON decoder as
the baseline. Thanks also go to David Chiang who originally
implement the decoder.
distributed LM architecture5 as discussed in Section
3.2. As shown in Table 2, the 7-gram distributed lan-
guage model (DLM) significantly improves trans-
lation performance over the 5-gram LM. However,
decoding is significantly slower (12.2 sec/sent when
using the non-parallel decoder) due to the added net-
work communication overhead.
LM type # k-grams MT ?03 MT ?05
5-gram LM 49 M 34.5% 32.9%
7-gram DLM 310 M 35.5% 33.9%
Table 2: Distributed language model: the 7-gram LM
cannot be loaded alongside the SCFG on a single ma-
chine; via distributed computing, it yields significant im-
provement in BLEU-4 over a 5-gram.
5.4 Utility of Equivalent LM States
To reduce the number of search errors, one may ei-
ther increase the beam size, or employ techniques
such as the equivalent LM state maintenance de-
scribed in Section 4. In this subsection, we compare
the tradeoff between the search effort (measured by
decoding time per sentence) and the search qual-
ity (measured by the average model cost of the best
translation found).
Intuitively, collapsing equivalent LM states is use-
ful only when the language model is very sparse, i.e.,
most of the evaluated m-grams will need to back-
off. A sparse LM is obtained in practice by using
a large order m relative to the amount of training
data. To test this intuition, we train a 7-gram LM
using only the English side of the parallel text (?
19M words). Figure 4 compares maintenance of
the full LM state v/s the equivalent LM state. The
beam size b for decoding with equivalent LM states
is fixed at 30; it is increased considerably?30, 50,
70, 90, 120, and 150?with the full LM state in
an effort to reduce search errors. It is clear from
the figure that collapsing items that differ due only
to equivalent LM states improves the search quality
considerably while actually reducing search effort.
This shows the effectiveness of equivalent LM state
maintenance.
5Since our distributed LM architecture dynamically interpo-
lates multiple LM scores, it cannot yet exploit the equivalent
LM state maintenance of Section 4, for different LMs will have
different reduced LM states. We will address this in the future.
16
Search Effort vs Search Quality
19.95
19.97
19.99
20.01
20.03
20.05
20.07
0 2 4 6 8 10
Number of Seconds per Sentence
Av
g M
od
el 
Co
st 
for
 on
e-b
es
ts
Baseline
EquivLM
 
beam size = 30 
Figure 4: Search quality with equivalent 7-gram LM state
maintenance (EquivLM) and without it (Baseline) as a
function of search effort as controlled by the beam size.
We also train a 3-gram LM using an English cor-
pus of about 130M words, and repeat the above ex-
periments. In this case, maintaining equivalent LM
states costs more decoding time than using the full
LM state to achieve the same search quality. This
is due partly to our inefficient implementation of the
prefix- and suffix-lookup required to determine the
equivalent LM state, and partly to the fact that with
130M words, a 3-gram LM backs off less frequently.
6 Conclusions
We have described a scalable decoder for parsing-
based machine translation. It is written in JAVA and
implements all the essential algorithms described
in Chiang (2007): chart-parsing, m-gram language
model integration, beam- and cube-pruning, and
unique k-best extraction. Additionally, parallel and
distributed computing techniques are exploited to
make it scalable. We demonstrate that our decoder
is 38 times faster than a baseline decoder written in
PYTHON, and that the distributed language model
is very useful to improve translation quality in a
large-scale task. We also describe an algorithm that
exploits the back-off property of an m-gram model
to maintain equivalent LM states, and show that bet-
ter search quality is obtained with less search effort
when the search space is organized to exploit this
equivalence. We plan to incorporate some additional
syntax-based components into the decoder and re-
lease it as an open-source toolkit.
Acknowledgments
We thank Philip Resnik, Chris Dyer, Smaranda
Muresan and Adam Lopez for very helpful discus-
sions, and the anonymous reviewers for their con-
structive comments. This research was partially sup-
ported by the Defense Advanced Research Projects
Agency?s GALE program via Contract No? HR0011-
06-2-0001.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2006. Large Language Models in
Machine Translation. In Proceedings of EMNLP 2007.
David Chiang. 2006. An Introduction to
Synchronous Grammars. Available at
http://www.isi.edu/?chiang/papers/synchtut.pdf.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP 2007.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of IWPT 2005.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proceedings of the ACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA 2006.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL 2006.
Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of EMNLP
2007.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000.
17
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of ACL 2005.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3-15.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, volume
2, pages 901-904.
Ashish Venugopal, Andreas Zollmann, Stephan Vo-
gel. 2007. An Efficient Two-Pass Approach to
Synchronous-CFG Driven Statistical MT. In Proceed-
ings of NAACL 2007.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list re-
ranking. In Proceedings of EMNLP 2006.
18
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Coling 2010: Poster Volume, pages 656?664,
Beijing, August 2010
Unsupervised Discriminative Language Model Training
for Machine Translation using Simulated Confusion Sets
Zhifei Li and Ziyuan Wang and Sanjeev Khudanpur and Jason Eisner
Center for Language and Speech Processing
Johns Hopkins University
zhifei.work@gmail.com,{zwang40, khudanpur, eisner}@jhu.edu
Abstract
An unsupervised discriminative training
procedure is proposed for estimating a
language model (LM) for machine trans-
lation (MT). An English-to-English syn-
chronous context-free grammar is derived
from a baseline MT system to capture
translation alternatives: pairs of words,
phrases or other sentence fragments that
potentially compete to be the translation
of the same source-language fragment.
Using this grammar, a set of impostor
sentences is then created for each En-
glish sentence to simulate confusions that
would arise if the system were to process
an (unavailable) input whose correct En-
glish translation is that sentence. An LM
is then trained to discriminate between
the original sentences and the impostors.
The procedure is applied to the IWSLT
Chinese-to-English translation task, and
promising improvements on a state-of-
the-art MT system are demonstrated.
1 Discriminative Language Modeling
A language model (LM) constitutes a crucial com-
ponent in many tasks such as machine translation
(MT), speech recognition, information retrieval,
handwriting recognition, etc. It assigns a pri-
ori probabilities to word sequences. In general,
we expect a low probability for an ungrammat-
ical or implausible word sequence. The domi-
nant LM used in such systems is the so-called
n-gram model, which is typically derived from a
large corpus of target language text via maximum
likelihood estimation, mitigated by some smooth-
ing or regularization. Due to the Markovian as-
sumptions implicit in n-gram models, however,
richer linguistic and semantic dependencies are
not well captured. Rosenfeld (1996) and Khu-
danpur and Wu (2000) address such shortcom-
ing by using maximum entropy models with long-
span features, while still working with a locally
normalized left-to-right LM. The whole-sentence
maximum entropy LM of Rosenfeld et al (2001)
proposes a globally normalized log-linear LM in-
corporating several sentence-wide features.
The n-gram as well as the whole-sentence
model are generative or descriptive models of
text. However, in a task like Chinese-to-English
MT, the de facto role of the LM is to discriminate
among the alternative English translations being
contemplated by the MT system for a particular
Chinese input sentence. We call the set of such
alternative translations a confusion set. Since a
confusion set is typically a minuscule subset of
the set of all possible word sequences, it is ar-
guably better to train the LM parameters so as to
make the best candidate in the confusion set more
likely than its competitors, as done by Roark et al
(2004) for speech recognition and by Li and Khu-
danpur (2008) for MT. Note that identifying the
best candidate requires supervised training data?
bilingual text in case of MT?which is expensive
in many domains (e.g. weblog or newsgroup) and
for most language pairs (e.g. Urdu-English).
We propose a novel discriminative LM in this
paper: a globally normalized log-linear LM that
can be trained in an efficient and unsupervised
manner, using only monolingual (English) text.
The main idea is to exploit (translation) un-
certainties inherent in an MT system to de-
rive an English-to-English confusion grammar
(CG), illustrated in this paper for a Hiero sys-
tem (Chiang, 2007). From the bilingual syn-
chronous context-free grammar (SCFG) used in
Hiero, we extract a monolingual SCFG, with rules
of the kind, X ? ?strong tea, powerful tea? or
656
X ? ?in X1, in the X1?. Thus our CG is also an
SCFG that generates pairs of English sentences
that differ from each other in ways that alterna-
tive English hypothesis considered during transla-
tion would differ from each other. This CG is then
used to ?translate? each sentence in the LM train-
ing corpus into what we call its confusion set ? a
set of other ?sentences? with which that sentence
would likely be confused by the MT system, were
it to be the target translation of a source-language
sentence. Sentences in the training corpus, each
paired with its confusion set, are then used to train
a discriminative LM to prefer the training sen-
tences over the alternatives in their confusion sets.
Since the monolingual CG and the bilingual
Hiero grammar are both SCFGs, the confusion
sets are isomorphic with translation hypergraphs
that are used by supervised discriminative train-
ing. The confusion sets thus simulate the super-
vised case, with a key exception: lack of any
(Chinese) source-language information. There-
fore, only target-side ?language model? probabil-
ities may be estimated from confusion sets.
We carry out this discriminative training proce-
dure, and empirically demonstrate promising im-
provements in translation quality.
2 Discriminative LM Training
2.1 Whole-sentence Maximum Entropy LM
We aim to train a globally normalized log-linear
language model p?(y) of the form
p?(y) = Z?1 ef(y)?? (1)
where y is an English sentence, f(y) is a vector
of arbitrary features of y, ? is the (weight) vec-
tor of model parameters, and Z def= ?y? ef(y?)?? is
a normalization constant. Given a set of English
training sentences {yi}, the parameters ? may be
chosen to maximize likelihood, as
?? = argmax
?
?
i
p?(yi). (2)
This is the so called whole-sentence maximum
entropy (WSME) language model1 proposed by
1Note the contrast with the maximum entropy n-gram
LM (Rosenfeld, 1996; Khudanpur and Wu, 2000), where the
normalization is performed for each n-gram history.
Rosenfeld et al (2001). Training the model of
(2) requires computing Z, a sum over all possible
word sequences y? with any length, which is com-
putationally intractable. Rosenfeld et al (2001)
approximate Z by random sampling.
2.2 Supervised Discriminative LM Training
In addition to the computational disadvantage, (2)
also has a modeling limitation. In particular, in
a task like MT, the primary role of the LM is to
discriminate among alternative translations of a
given source-language sentence. This set of alter-
natives is typically a minuscule subset of all pos-
sible target-language word sequences. Therefore,
a better way to train the global log-linear LM,
given bilingual text {(xi, yi)}, is to generate the
real confusion set N (xi) for each input sentence
xi using a specific MT system, and to adjust ? to
discriminate between the reference translation yi
and y? ? N (xi) (Roark et al, 2004; Li and Khu-
danpur, 2008).
For example, one may maximize the condi-
tional likelihood of the bilingual training data as
?? = argmax
?
?
i
p?(yi |xi) (3)
= argmax
?
?
i
ef(xi,yi)???
y??N (xi) ef(xi,y
?)?? ,
which entails summing over only the candidate
translations y? of the given input xi. Furthermore,
if the features f(xi, y) are depend on only the out-
put y, i.e. on the English-side features of the bilin-
gual text, the resulting discriminative model may
be interpreted as a language model.
Finally, in a Hiero style MT system, if f(xi, y)
depends on the target-side(s) of the bilingual rules
used to construct y from xi, we essentially have a
syntactic LM.
2.3 Unsupervised Discriminative Training
using Simulated Confusion Sets
While the supervised discriminative LM training
has both computational and modeling advantages
over the WSME LM, it relies on bilingual data,
which is expensive to obtain for several domains
and language pairs. For such cases, we propose
a novel discriminative language model, which is
657
still a global log-linear LM with the modeling ad-
vantage and computational efficiency of (3) but re-
quires only monolingual text {yi} for training ?.
Specifically, we propose to modify (3) as
?? = argmax
?
?
i
p?(yi | N (yi)) (4)
= argmax
?
?
i
ef(yi)???
y??N (yi) ef(y
?)?? ,
where N (yi) is a simulated confusion set for yi
obtained by applying a confusion grammar to yi,
as detailed in Section 3. Our hope is that N (yi)
resembles the actual confusion set N (xi) that an
MT system would generate if it were given the in-
put sentence xi.
Like (3), the maximum likelihood training of
(4) does not entail the expensive computation of a
global normalization constant Z, and is therefore
very efficient. Unlike (3) however, where the input
xi for each output yi is needed to create N (xi),
the model of (4) can be trained in an unsupervised
manner with only {yi}.
3 Unsupervised Discriminative Training
of the Language Model for MT
The following is thus the proposed procedure for
unsupervised discriminative training of the LM.
1. Extract a confusion grammar (CG) from the
baseline MT system.
2. ?Translate? each English sentence in the LM
training corpus, using the CG as an English-
to-English translation model, to generate a
simulated confusion set.
3. Train a discriminative language model on the
simulated confusion sets, using the corre-
sponding original English sentences as the
training references.
The trained model may then be used for actual MT
decoding. We next describe each step in detail.
3.1 Extracting a Confusion Grammar
We assume a synchronous context free grammar
(SCFG) formalism for the confusion grammar
(CG). While the SCFG used by the MT system
is bilingual, the CG we extract will be monolin-
gual, with both the source and target sides being
English. Some example CG rules are:
X ? ? strong tea , powerful tea ? ,
X ? ?X0 at beijing , beijing ?s X0 ? ,
X ? ?X0 of X1 , X0 of the X1 ? ,
X ? ?X0 ?s X1 , X1 of X0 ? .
Like a regular SCFG, a CG contains rules with
different ?arities? and reordering of the nontermi-
nals (as shown in the last example) capturing the
confusions that the MT system encounters when
choosing word senses, reordering patterns, etc.
3.1.1 Extracting a Confusion Grammar from
the Bilingual Grammar
The confusion grammar is derived from the MT
system?s bilingual grammar. In Hiero, the bilin-
gual rules are of the form X ? ?c, e?, where
both c and e may contain (a matched number of)
nonterminal symbols. For every c which appears
on the source-side of two different Hiero rules
X ? ?c, e1? and X ? ?c, e2?, we extract two CG
rules, X ? ?e1, e2? and X ? ?e2, e1?, to capture
the confusion the MT system would face were it
to encounter c in its input. For each Hiero rule
X ? ?c, e?, we also extract X ? ?e, e?, the iden-
tity rule. Therefore, if a pattern c appears with |E|
different translation options, we extract |E|2 dif-
ferent CG rules from c. In our current work, the
rules of the CG are unweighted.
3.1.2 Test-set Specific Confusion Grammars
If the bilingual grammar contains all the rules
that are extractable from the bilingual training cor-
pus, the resulting confusion grammar is likely to
be huge. As a way of reducing computation, the
bilingual grammar can be restricted to a specific
test set, and only rules used by the MT system for
translating the test set used for extracting the CG.2
To economize further, one may extract a CG
from the translation hypergraphs that are gener-
ated for the test-set. Recall that a node in a hy-
pergraph corresponds to a specific source (Chi-
nese) span, and the node has many incident hy-
peredges, each associated with a different bilin-
2Test-set specific CGs are of course only practical for off-
line applications.
658
gual rule. Therefore, all the bilingual rules asso-
ciated with the incoming hyperedges of a given
node translate the same Chinese string. At each
hypergraph node, we extract CG rules to represent
the competing English sides as described above.
Note that even though different rules associated
with a node may have different ?arity,? we extract
CG rules only from pairs of bilingual rules that
have the same arity.
A CG extracted from only the bilingual rule
pairs incident on the same node in the test hy-
pergraphs is, of course, much smaller than a CG
extracted from the entire bilingual grammar. It
is also more suitable for our task, since the test
hypergraphs have already benefited from a base-
line n-gram LM and pruning, removing all confu-
sions that are easily resolved (rightly or wrongly)
by other system components.
3.2 Generating Simulated Confusion Sets
For each English sentence y in the training cor-
pus, we use the extracted CG to produce a simu-
lated confusion set N (y). This is done like a reg-
ular MT decoding pass, because we can treat the
CG as a Hiero style ?translation? grammar3 for an
English-to-English translation system.
Since the CG is an SCFG, the confusion set
N (y) generated for a sentence y is a hypergraph,
encoding not only the alternative sentences y? but
also the hierarchical derivation tree for each y?
from y (e.g., which phrase in y has been re-
placed with what in y?). As usual, many differ-
ent derivation trees d may correspond to the same
string/sentence y? due to spurious ambiguity. We
use D(y) to denote the set of derivations d, which
is a hypergraph representation of N (y).
Figure 1 presents an example confusion hy-
pergraph for the English sentence y =?a cat on
the mat,? containing four alternative hypotheses:
3To make sure that we produce at least one derivation tree
for each y, we need to add to the CG the following two glue
rules, as done in Hiero (Chiang, 2007).
S ? ?X0 , X0 ? ,
S ? ?S0 X1 , S0 X1 ? .
We also add an out of vocabulary rule X ? ?word, oov? for
each word in y and set the cost of this rule to a high value so
that the OOV rule will get used only when the CG does not
know how to ?translate? the word.
X ? ? a cat , the cat ?
X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ?
X ? ?X0 on X1 , X1 on X0 ?
X ? ?X0 on X1 , X1 of X0 ?
S ? ?X0 , X0 ?
(a) An example confusion grammar.
a
0
  cat
1             
       on
2
         the
3
 mat
4
S??X0,X0?
X
0,5
X
0,2
X
3,5
X ? ? a cat , the cat ? X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ? X ? ?X0 on X1 , X1 of X0 ?
X ? ?X0 on X1 , X1 on X0 ?
S
0,5
(b) An example hypergraph generated by the confusion
grammar of (a) for the input sentence ?a cat on the mat.?
Figure 1: Example confusion grammar and simulated
confusion hypergraph. Given an input sentence y = ?a cat
on the mat,? the confusion grammar of (a) generates a hyper-
graph D(y) shown in (b), which represents the confusion set
N (y) containing four alternative sentences y?.
N (y) = { ?the cat the mat,? ?the cat ?s the mat,?
?the mat of the cat,? ?the mat on the cat?}.
Notice that each competitor y? ? N (y) can be
regarded as the result of a ?round-trip? translation
y ? x ? y?, in which we reconstruct a possible
Chinese source sentence x that our Hiero bilin-
gual grammar could translate into both y and y?.4
We will train our LM to prefer y, which was ac-
tually observed. Our CG-based round-trip forces
x? y? to use the same hierarchical segmentation
of x as y ? x did. This constraint leads to effi-
cient training but artificially reduces the diversity
4This is because of the way we construct our CG from the
Hiero grammar. However, the identity and glue rules in our
CG allow almost any portion of y to be preserved untrans-
lated through the entire y ? x ? y? process. Much of y
will necessarily be preserved in the situation where the CG is
extracted from a small test set and hence has few non-identity
rules. See (Li, 2010) for further discussion.
659
ofN (y). In other recent work (Li et al, 2010), we
have taken the round-trip view more seriously, by
imputing likely source sentences x and translating
them back to separate, weighted confusion forests
N (y), without any same-segmentation constraint.
3.3 Confusion-based Discriminative Training
With the training sentences yi and their simulated
confusion sets N (yi) ? represented as hyper-
graphs D(yi)) ? we can perform the discrimi-
native training using any of a number of proce-
dures such as MERT (Och, 2003) or MIRA as
used by Chiang et al (2009). In our paper, we
use hypergraph-based minimum risk (Li and Eis-
ner, 2009),
?? = argmin
?
?
i
Risk?(yi) (5)
= argmin
?
?
i
?
d?D(yi)
L(Y(d), yi)p?(d |D(yi)),
where L(y?, yi) is the loss (e.g negated BLEU) in-
curred by producing y? when the true answer is yi,
Y(d) is the English yield of a derivation d, and
p?(d |D(yi)) is defined as,
p?(d |D(yi)) = e
f(d)??
?
d?D(yi) ef(d)??
, (6)
where f(d) is a feature vector over d. We will
specify the features in Section 5, but in general
they should be defined such that the training will
be efficient and the actual MT decoding can use
them conveniently.
The objective of (5) is differentiable and thus
we can optimize ? by a gradient-based method.
The risk and its gradient on a hypergraph can
be computed by using a second-order expectation
semiring (Li and Eisner, 2009).
3.3.1 Iterative Training
In practice, the full confusion set N (y) defined
by a confusion grammar may be too large and we
have to perform pruning when training our model.
But the pruning itself may depend on the model
that we aim to train. How do we solve this circu-
lar dependency problem? We adopt the following
procedure. Given an initial model ?, we generate a
hypergraph (with pruning) for each y, and train an
optimal ?? of (5) on these hypergraphs. Then, we
use the optimal ?? to regenerate a hypergraph for
each y, and do the training again. This iterates un-
til convergence. This procedure is quite similar to
the k-best MERT (Och, 2003) where the training
involves a few iterations, and each iteration uses a
new k-best list generated using the latest model.
3.4 Applying the Discriminative LM
First, we measure the goodness of our language
model in a simulated task. We generate simulated
confusion sets N (y) for some held out English
sentences y, and test how well p?(d |D(y)) can
recover y from N (y). This is merely a proof of
concept, and may be useful in deciding which fea-
tures f(d) to employ for discriminative training.
The intended use of our model is, of course, for
actual MT decoding (e.g., translating Chinese to
English). Specifically, we can add the discrimina-
tive model into an MT pipeline as a feature, and
tune its weight relative to other models in the MT
system, including the baseline n-gram LM.
4 Related and Similar Work
The detailed relation between the proposed pro-
cedure and other language modeling techniques
has been discussed in Sections 1 and 2. Here, we
review two other methods that are related to our
method in a broader context.
4.1 Unsupervised Training of Global
Log-linear Models
Our method is similar to the contrastive estimation
(CE) of Smith and Eisner (2005) and its succes-
sors (Poon et al, 2009). In particular, our confu-
sion grammar is like a neighborhood function in
CE. Also, our goal is to improve both efficiency
and accuracy, just as CE does. However, there
are two important differences. First, the neigh-
borhood function in CE is manually created based
on human insights about the particular task, while
our neighborhood function, generated by the CG,
is automatically learnt (e.g., from the bilingual
grammar) and specific to the MT system being
used. Therefore, our neighborhood function is
more likely to be informative and adaptive to the
task. Secondly, when tuning ?, CE uses the maxi-
mum likelihood training, but we use the minimum
660
risk training of (5). Since our training uses a task-
specific loss function, it is likely to perform better
than maximum likelihood training.
4.2 Paraphrasing Models
Our method is also related to methods for train-
ing paraphrasing models (Quirk et al, 2004; Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Madnani et al, 2007). Specifically, the
form of our confusion grammar is similar to that
of the paraphrase model they use, and the ways
of extracting the grammar/model are also similar
as both employ a second language (e.g., Chinese
in our case) as a pivot. However, while a ?trans-
lation? rule in a paraphrase model is expected to
contain a pair of phrases that are good alterna-
tives for each other, a confusion rule in our CG
is based on an MT system processing unseen test
data and contains pairs of phrases that are typi-
cally bad (and only rarely good) alternatives for
each other.
The motivation and goal are also different. For
example, the goal of Bannard and Callison-Burch
(2005) is to extract paraphrases with the help of
parallel corpora. Callison-Burch et al (2006) aim
to improve MT quality by adding paraphrases in
the translation table, while Madnani et al (2007)
aim to improve the minimum error rate training by
adding the automatically generated paraphrases
into the English reference sets. In contrast, our
motivation is to train a discriminative language
model to improve MT (by using the confusion
grammar to decide what alternatives the model
should learn to discriminate).
5 Experimental Results
We have applied the confusion-based discrimina-
tive language model (CDLM) to the IWSLT 2005
Chinese-to-English text translation task5 (Eck and
Hori, 2005). We see promising improvements
over an n-gram LM for a solid Joshua-based
baseline system (Li et al, 2009).
5.1 Data Partitions for Training & Testing
Four kinds of data are used for CDLM training:
5This is a relatively small task compared to, say, the NIST
MT tasks. We worked on it for a proof-of-concept. Having
been successful, we are now investigating larger MT tasks.
# sentencesData Usage ZH EN
Set1 TM & LM training 40k 40k
Set2 Min-risk training 1006 1006?16
Set3 CDLM training ? 1006?16
Set4 Test 506 506?16
Table 1: Data sets used. Set1 contains translation-equivalent
Chinese-English sentence pairs, while for each Chinese sen-
tence in Set2 and Set4, there are 16 English translations. Set3
happens to be the English side of Set2 due to lack of ad-
ditional in-domain English text, but this is not noteworthy;
Set3 could be any in-domain target-language text corpus.
Set1 a bilingual training set on which 10 individ-
ual MT system components are trained,
Set2 a small bilingual, in-domain set for tuning
relative weights of the system components,
Set3 an in-domain monolingual target-language
corpus for CDLM training, and
Set4 a test set on which improvements in MT per-
formance is measured.
We partition the IWSLT data into four such sub-
sets as listed in Table 1.
5.2 Baseline MT System
Our baseline translation model components are
estimated from 40k pairs of utterances from the
travel domain, called Set1 in Table 1. We use a 5-
gram language model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998), trained on
the English side of Set1, as our baseline LM.
The baseline MT system comprises 10 com-
ponent models (or ?features?) that are standard
in Hiero (Chiang, 2007), namely the baseline
language model (BLM) feature, three baseline
translation model features, one word-insertion
penalty (WP) feature, and five arity features ?
three to count how many rules with an arity of
zero/one/two are used in a derivation, and two
to count how many times the unary and binary
glue rules are used in a derivation. The rela-
tive weights of these 10 features are tuned via
hypergraph-based minimum risk training (Li and
Eisner, 2009) on the bilingual data Set2.
The resulting MT system gives a BLEU score of
48.5% on Set4, which is arguably a solid baseline.
661
5.3 Unsupervised Training of the CDLM
We extract a test-set specific CG from the hyper-
graphs obtained by decoding Set2 and Set4, as de-
scribed in Section 3.1.2. The number of rules in
the bilingual grammar and the CG are about 167k
and 1583k respectively. The CG is used as the
?translation? model to generate confusion hyper-
graphs for sentences in Set3.
Two CDLMs, corresponding to different fea-
ture sets f(d) in equation (6), were trained.
Only n-gram LM Features: We consider a
CDLM with only two features f(d): a base-
line LM feature (BLM) that equals the 5-
gram probability of Y(d) and a word penalty
feature (WP) equal to the length of Y(d).
Target-side Rule Bigram Features6: For each
CG rule used in d, we extract counts of bi-
grams that appear on the target-side of the
CG rule. For example, if the confusion rule
X ? ?X0 of X1 , X0 of the X1 ? is used in
d, the bigram features in f(d) whose counts
are incremented are: ?X of,? ?of the? and
?the X .?7 Note that the indices on the non-
terminals in the rule have been removed. To
avoid very rare features, we only consider
the 250 most freqent terminal symbol (En-
glish words) in the English of Set1 and map
all other terminal symbols into a single class.
Finally, we replace the identities of words
with their dominant POS tags. These restric-
tions result in 525 target-side rule bigram
(TsRB) features f(d) in the model of (6).
For each choice of the feature vector f(d), be it
2- or 527-dimensional, we use the training proce-
dure of Section 3.3.1 to iteratively minimize the
objective of (5) and get the CDLM parameter ??.
Note that each English sentence in Set3 has 15
other paraphrases. We generate a separate confu-
sion hypergraph D(y) for each English sentence
y, but for each such hypergraph we use both y
and its 15 paraphrases as ?reference translations?
when computing the risk L(Y(d), {y}) in (5).8
6Note that these features are novel in MT.
7With these target-side rule-based features, our LM is es-
sentially a syntactic LM, not just an LM on English strings.
8We take unfair advantage of this unusual dataset to com-
5.4 Results on Monolingual Simulation
We first probe how our novel CDLM performs as
a language model itself. One usually uses the per-
plexity of the LM on some unseen text to measure
its goodness. But since we did not optimize the
CDLM for likelihood, we instead examine how
it performs in discriminating between a good En-
glish sentence and sentences with which the MT
system may confuse that sentence. The test is per-
formed as follows. For each test English sentence
y of Set4, the confusion grammar defines a full
confusion set N (y) via a hypergraph D(y). We
use a LM to pick the most likely y? from N (y),
and then compute its BLEU score by using y and
its 15 paraphrase sentences as references. The
higher the BLEU, the better is the LM in picking
out a good translation from N (y).
Table 2 shows the results9 under a regular n-
gram LM and the two CDLMs described in Sec-
tion 5.3.
The baseline LM (BLM) entails no weight op-
timization a la (5) on Set3. The CDLM with the
BLM and word pentaly (WP) features improves
over the baseline LM. Compared to either of them,
the CDLM with the target-side rule bigram fea-
tures (TsRB) performs dramatically better.
5.5 Results on MT Test Data
We now examine how our CDLM performs during
actual MT decoding. To incorporate the CDLM
into MT decoding, we add the log-probability (6)
of a derivation d under the CDLM as an additional
bat an unrelated complication?a seemingly problematic in-
stability in the minimum risk training procedure.
As an illustration of this problem, we note that in super-
vised tuning of the baseline MT system (|f(d)|=10) with
500 sentences from Set2, the BLEU score on Set4 varies from
38.6% to 44.2% to 47.8% if we use 1, 4 and 16 reference
translations during the supervised training respectively. We
choose a system tuned on 16 references on Set2 as our base-
line. In order not to let the unsupervised CDLM training
suffer from this unrelated limitation of the tuning procedure,
we give it too the benefit of being able to compute risk on
Set3 using y plus its 15 paraphrases.
We wish to emphasize that this trait of Set3 having 15
paraphrases for each sentence is otherwise unnecessary, and
does not detract much from the main claim of this paper.
9Note that the scores in Table 2 are very low compared to
scores for actual translation from Chinese shown in Table 3.
This is mainly because in this monolingual simulation, the
LM is the only model used to rank the y? ? N (y). Said dif-
ferently, y? is being chosen in Table 2 entirely for its fluency
with no consideration whatsoever for its adequacy.
662
LM used for Features used BLEU
rescoring BLM WP TsRB on Set4
Baseline LM X 12.8
CDLM X X 14.2
CDLM X X X 25.3
Table 2: BLEU scores in monolingual simulations. Rescor-
ing the confusion sets of English sentences created using the
CG shows that the CDLM with TsRB features recovers hy-
potheses much closer to the sentence that generated the con-
fusion set than does the baseline n-gram LM.
Model used Features used BLEU
for rescoring 10 models TsRB on Set4
Joshua X 48.5
+ CDLM X X 49.5
Table 3: BLEU scores on the test set. The baseline MT sys-
tem has ten models/features, and the proposed system has
one additional model, the CDLM. Note that for the CDLM,
only the TsRB features are used during MT decoding.
feature, on top of the 10 features already present
in baseline MT system (see Section 5.2). We then
(re)tune relative weights for these 11 features on
the bilingual data Set2 of Table 1.
Note that the MT system also uses the BLM and
WP features whose weights are now retuned on
Set2. Therefore, when integrating a CDLM into
MT decoding, it is mathematically equivalent to
use only the TsRB features of the CDLM, with
the corresponding weights as estimated alongside
its ?own? BLM and WP features during unsuper-
vised discriminative training on Set3.
Table 3 reports the results. A BLEU score im-
provement of 1% is seen, reinforcing the claim
that the unsupervised CDLM helps select better
translations from among the system?s alternatives.
5.6 Goodness of Simulated Confusion Sets
The confusion set N (y) generated by applying
the CG to an English sentence y aims to simulate
the real confusion set that would be generated by
the MT system if the system?s input was the Chi-
nese sentence whose English translation is y. We
investigate, in closing, how much the simulated
confusion set resembles to the real one. Since
we know the actual input-output pairs (xi, yi) for
Set4, we generate two confusion sets: the simu-
lated set N (yi) and the real one N (xi).
One way to measure the goodness of N (yi) as
a proxy for N (xi), is to extract the n-gram types
n-gram Precision Recall
unigram 36.5% 48.2%
bigram 10.1% 12.8%
trigram 3.7% 4.6%
4-gram 2.0% 2.4%
Table 4: n-gram precision and recall of simulated con-
fusion sets relative to the true confusions when translating
Chinese sentences. The n-grams are collected from k-best
strings in both cases, with k = 100. The precision and recall
change little when varying k.
witnessed in the two sets, and compute the ratio of
the number of n-grams in the intersection to the
number in their union. Another is to measure the
precision and recall of N (yi) relative to N (xi).
Table 4 presents such precision and recall fig-
ures. For convenience, the n-grams are collected
from the 100-best strings, instead of the hyper-
graph D(yi) and D(xi). Observe that the sim-
ulated confusion set does a reasonably good job
on the real unigram confusions but the simulation
needs improving for higher order n-grams.
6 Conclusions
We proposed a novel procedure to discrimina-
tively train a globally normalized log-linear lan-
guage model for MT, in an efficient and unsu-
pervised manner. Our method relies on the con-
struction of a confusion grammar, an English-to-
English SCFG that captures translation alterna-
tives that an MT system may face when choosing
a translation for a given input. For each English
training sentence, we use this confusion gram-
mar to generate a simulated confusion set, from
which we train a discriminative language model
that will prefer the original English sentence over
sentences in the confusion set. Our experiments
show that the novel CDLM picks better alterna-
tives than a regular n-gram LM from simulated
confusion sets, and improves performance in a
real Chinese-to-English translation task.
7 Acknowledgements
This work was partially supported by the National
Science Foundation via grants No? SGER-0840112
and RI-0963898, and by the DARPA GALE pro-
gram. The authors thank Brian Roark and Dami-
anos Karakos for insightful discussions.
663
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages
597?604, Morristown, NJ, USA. Association for
Computational Linguistics.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 17?
24, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Chen, Stanley F. and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
Chiang, David, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Eck, Matthias and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In Proc. of the
International Workshop on Spoken Language Trans-
lation.
Khudanpur, Sanjeev and Jun Wu. 2000. Maximum en-
tropy techniques for exploiting syntactic, semantic
and collocational dependencies in language model-
ing. In Computer Speech and Language, number 4,
pages 355?372.
Li, Zhifei and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
40?51, Singapore, August. Association for Compu-
tational Linguistics.
Li, Zhifei and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA, pages 133?142.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In WMT09,
pages 26?30.
Li, Zhifei, Ziyuan Wang, Jason Eisner, and Sanjeev
Khudanpur. 2010. Minimum imputed risk training
for machine translation. In review.
Li, Zhifei. 2010. Discriminative training and varia-
tional decoding in machine translation via novel al-
gorithms for weighted hypergraphs. PHD Disserta-
tion, Johns Hopkins University.
Madnani, Nitin, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL, pages
160?167.
Poon, Hoifung, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, pages 209?217, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 142?149.
Roark, Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 47?54,
Barcelona, Spain, July.
Rosenfeld, Roni, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Rosenfeld, Roni. 1996. A maximum entropy approach
to adaptive statistical language modeling. In Com-
puter Speech and Language, number 3, pages 187?
228.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics (ACL 2005), Ann Arbor, Michi-
gan.
664
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920?929,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimum Imputed Risk: Unsupervised Discriminative Training for
Machine Translation
Zhifei Li?
Google Research
Mountain View, CA 94043, USA
zhifei.work@gmail.com
Ziyuan Wang, Sanjeev Khudanpur
Johns Hopkins University
Baltimore, MD 21218, USA
zwang40,khudanpur@jhu.edu
Jason Eisner
Johns Hopkins University
Baltimore, MD 21218, USA
eisner@jhu.edu
Brian Roark
Oregon Health & Science University
Beaverton, Oregon 97006, USA
roark@cslu.ogi.edu
Abstract
Discriminative training for machine transla-
tion has been well studied in the recent past.
A limitation of the work to date is that it relies
on the availability of high-quality in-domain
bilingual text for supervised training. We
present an unsupervised discriminative train-
ing framework to incorporate the usually plen-
tiful target-language monolingual data by us-
ing a rough ?reverse? translation system. Intu-
itively, our method strives to ensure that prob-
abilistic ?round-trip? translation from a target-
language sentence to the source-language and
back will have low expected loss. Theoret-
ically, this may be justified as (discrimina-
tively) minimizing an imputed empirical risk.
Empirically, we demonstrate that augment-
ing supervised training with unsupervised data
improves translation performance over the su-
pervised case for both IWSLT and NIST tasks.
1 Introduction
Missing data is a common problem in statistics when
fitting the parameters ? of a model. A common strat-
egy is to attempt to impute, or ?fill in,? the missing
data (Little and Rubin, 1987), as typified by the EM
algorithm. In this paper we develop imputation tech-
niques when ? is to be trained discriminatively.
We focus on machine translation (MT) as our ex-
ample application. A Chinese-to-English machine
translation system is given a Chinese sentence x and
? Zhifei Li is currently working at Google Research, and
this work was done while he was a PHD student at Johns Hop-
kins University.
asked to predict its English translation y. This sys-
tem employs statistical models p?(y | x) whose pa-
rameters ? are discriminatively trained using bilin-
gual sentence pairs (x, y). But bilingual data for
such supervised training may be relatively scarce for
a particular language pair (e.g., Urdu-English), es-
pecially for some topics (e.g., technical manuals) or
genres (e.g., blogs). So systems seek to exploit ad-
ditional monolingual data, i.e., a corpus of English
sentences y with no corresponding source-language
sentences x, to improve estimation of ?. This is our
missing data scenario.1
Discriminative training of the parameters ? of
p?(y | x) using monolingual English data is a cu-
rious idea, since there is no Chinese input x to trans-
late. We propose an unsupervised training approach,
called minimum imputed risk training, which is con-
ceptually straightforward: First guess x (probabilis-
tically) from the observed y using a reverse English-
to-Chinese translation model p?(x | y). Then train
the discriminative Chinese-to-English model p?(y |
x) to do a good job at translating this imputed x
back to y, as measured by a given performance met-
ric. Intuitively, our method strives to ensure that
probabilistic ?round-trip? translation from a target-
language sentence to the source-language and back
again will have low expected loss.
Our approach can be applied in an application
scenario where we have (1) enough out-of-domain
bilingual data to build two baseline translation sys-
tems, with parameters ? for the forward direction,
and ? for the reverse direction; (2) a small amount
1Contrast this with traditional semi-supervised training that
looks to exploit ?unlabeled? inputs x, with missing outputs y.
920
of in-domain bilingual development data to discrim-
inatively tune a small number of parameters in ?;
and (3) a large amount of in-domain English mono-
lingual data.
The novelty here is to exploit (3) to discrimina-
tively tune the parameters ? of all translation model
components,2 p?(y|x) and p?(y), not merely train a
generative language model p?(y), as is the norm.
Following the theoretical development below, the
empirical effectiveness of our approach is demon-
strated by replacing a key supervised discriminative
training step in the development of large MT sys-
tems ? learning the log-linear combination of sev-
eral component model scores (viewed as features) to
optimize a performance metric (e.g. BLEU) on a set
of (x, y) pairs ? with our unsupervised discrimina-
tive training using only y. One may hence contrast
our approach with the traditional supervised meth-
ods applied to the MT task such as minimum error
rate training (Och, 2003; Macherey et al, 2008), the
averaged Perceptron (Liang et al, 2006), maximum
conditional likelihood (Blunsom et al, 2008), min-
imum risk (Smith and Eisner, 2006; Li and Eisner,
2009), and MIRA (Watanabe et al, 2007; Chiang et
al., 2009).
We perform experiments using the open-source
MT toolkit Joshua (Li et al, 2009a), and show that
adding unsupervised data to the traditional super-
vised training setup improves performance.
2 Supervised Discriminative Training via
Minimization of Empirical Risk
Let us first review discriminative training in the su-
pervised setting?as used in MERT (Och, 2003) and
subsequent work.
One wishes to tune the parameters ? of some
complex translation system ??(x). The function ??,
which translates Chinese x to English y = ??(x)
need not be probabilistic. For example, ? may be
the parameters of a scoring function used by ?, along
with pruning and decoding heuristics, for extracting
a high-scoring translation of x.
The goal of discriminative training is to mini-
mize the expected loss of ??(?), under a given task-
specific loss function L(y?, y) that measures how
2Note that the extra monolingual data is used only for tuning
the model weights, but not for inducing new phrases or rules.
bad it would be to output y? when the correct output
is y. For an MT system that is judged by the BLEU
metric (Papineni et al, 2001), for instance, L(y?, y)
may be the negated BLEU score of y? w.r.t. y. To be
precise, the goal3 is to find ? with low Bayes risk,
?? = argmin
?
?
x,y
p(x, y)L(??(x), y) (1)
where p(x, y) is the joint distribution of the input-
output pairs.4
The true p(x, y) is, of course, not known and,
in practice, one typically minimizes empirical risk
by replacing p(x, y) above with the empirical dis-
tribution p?(x, y) given by a supervised training set
{(xi, yi), i = 1, . . . , N}. Therefore,
?? = argmin
?
?
x,y
p?(x, y)L(??(x), y)
= argmin
?
1
N
N?
i=1
L(??(xi), yi). (2)
The search for ?? typically requires the use of nu-
merical methods and some regularization.5
3 Unsupervised Discriminative Training
with Missing Inputs
3.1 Minimization of Imputed Risk
We now turn to the unsupervised case, where we
have training examples {yi} but not their corre-
sponding inputs {xi}. We cannot compute the sum-
mand L(??(xi), yi) for such i in (2), since ??(xi)
requires to know xi. So we propose to replace
3This goal is different from the minimum risk training of
Li and Eisner (2009) in a subtle but important way. In both
cases, ?? minimizes risk or expected loss, but the expectation
is w.r.t. different distributions: the expectation in Li and Eisner
(2009) is under the conditional distribution p(y |x), while the
expectation in (1) is under the joint distribution p(x, y).
4In the terminology of statistical decision theory, p(x, y) is
a distribution over states of nature. We seek a decision rule
??(x) that will incur low expected loss on observations x that
are generated from unseen states of nature.
5To compensate for the shortcut of using the unsmoothed
empirical distribution rather than a posterior estimate of p(x, y)
(Minka, 2000), it is common to add a regularization term ||?||22
in the objective of (2). The regularization term can prevent over-
fitting to a training set that is not large enough to learn all pa-
rameters.
921
L(??(xi), yi) with the expectation
?
x
p?(x | yi)L(??(x), yi), (3)
where p?(? | ?) is a ?reverse prediction model? that
attempts to impute the missing xi data. We call the
resulting variant of (2) the minimization of imputed
empirical risk, and say that
?? = argmin
?
1
N
N?
i=1
?
x
p?(x | yi)L(??(x), yi) (4)
is the estimate with the minimum imputed risk6.
The minimum imputed risk objective of (4) could
be evaluated by brute force as follows.
1. For each unsupervised example yi, use the re-
verse prediction model p?(? | yi) to impute pos-
sible reverse translations Xi = {xi1, xi2, . . .},
and add each (xij , yi) pair, weighted by
p?(xij | yi) ? 1, to an imputed training set .
2. Perform the supervised training of (2) on the
imputed and weighted training data.
The second step means that we must use ?? to
forward-translate each imputed xij , evaluate the loss
of the translations y?ij against the corresponding true
translation yi, and choose the ? that minimizes the
weighted sum of these losses (i.e., the empirical risk
when the empirical distribution p?(x, y) is derived
from the imputed training set). Specific to our MT
task, this tries to ensure that probabilistic ?round-
trip? translation, from the target-language sentence
yi to the source-language and back again, will have
a low expected loss.7
The trouble with this method is that the reverse
model p? generates a weighted lattice or hyper-
graph Xi encoding exponentially many translations
of yi, and it is computationally infeasible to forward-
translate each xij ? Xi. We therefore investigate
several approximations to (4) in Section 3.4.
6One may exploit both supervised data {(xi, yi)} and unsu-
pervised data {yj} to perform semi-supervised training via an
interpolation of (2) and (4). We will do so in our experiments.
7Our approach may be applied to other tasks as well. For
example, in a speech recognition task, ?? is a speech recognizer
that produces text, whereas p? is a speech synthesizer that must
produce a distribution over audio (or at least over acoustic fea-
tures or phone sequences) (Huang et al, 2010).
3.2 The Reverse Prediction Model p?
A crucial ingredient in (4) is the reverse prediction
model p?(?|?) that attempts to impute the missing xi.
We will train this model in advance, doing the best
job we can from available data, including any out-
of-domain bilingual data as well as any in-domain
monolingual data8 x.
In the MT setting, ?? and p? may have similar pa-
rameterization. One translates Chinese to English;
the other translates English to Chinese.
Yet the setup is not quite symmetric. Whereas ??
is a translation system that aims to produce a single,
low-loss translation, the reverse version p? is rather
a probabilistic model. It is supposed to give an accu-
rate probability distribution over possible values xij
of the missing input sentence xi. All of these val-
ues are taken into account in (4), regardless of the
loss that they would incur if they were evaluated for
translation quality relative to the missing xi.
Thus, ? does not need to be trained to minimize
the risk itself (so there is no circularity). Ideally,
it should be trained to match the underlying condi-
tional distribution of x given y, by achieving a low
conditional cross-entropy
H(X |Y ) = ?
?
x,y
p(x, y) log p?(x | y). (5)
In practice, ? is trained by (empirically) minimiz-
ing ? 1M
?N
j=1 log p?(xj | yj) + 12?2 ???22 on some
bilingual data, with the regularization coefficient ?2
tuned on held out data.
It may be tolerable for p? to impute mediocre
translations xij . All that is necessary is that the (for-
ward) translations generated from the imputed xij
?simulate? the competing hypotheses that we would
see when translating the correct Chinese input xi.
3.3 The Forward Translation System ?? and
The Loss Function L(??(xi), yi)
The minimum empirical risk objective of (2) is
quite general and various popular supervised train-
ing methods (Lafferty et al, 2001; Collins, 2002;
Och, 2003; Crammer et al, 2006; Smith and Eisner,
8In a translation task from x to y, one usually does not make
use of in-domain monolingual data x. But we can exploit x to
train a language model p?(x) for the reverse translation system,
which will make the imputed xij look like true Chinese inputs.
922
2006) can be formalized in this framework by choos-
ing different functions for ?? and L(??(xi), yi). The
generality of (2) extends to our minimum imputed
risk objective of (4). Below, we specify the ?? and
L(??(xi), yi) we considered in our investigation.
3.3.1 Deterministic Decoding
A simple translation rule would define
??(x) = argmax
y
p?(y |x) (6)
If this ??(x) is used together with a loss function
L(??(xi), yi) that is the negated BLEU score9, our
minimum imputed risk objective of (4) is equivalent
to MERT (Och, 2003) on the imputed training data.
However, this would not yield a differentiable ob-
jective function. Infinitesimal changes to ? could re-
sult in discrete changes to the winning output string
??(x) in (6), and hence to the loss L(??(x), yi). Och
(2003) developed a specialized line search to per-
form the optimization, which is not scalable when
the number of model parameters ? is large.
3.3.2 Randomized Decoding
Instead of using the argmax of (6), we assume
during training that ??(x) is itself random, i.e. the
MT system randomly outputs a translation y with
probability p?(y |x). As a result, we will modify
our objective function of (4) to take yet another ex-
pectation over the unknown y. Specifically, we will
replace L(??(x), yi) in (4) with
?
y
p?(y |x)L(y, yi). (7)
Now, the minimum imputed empirical risk objective
of (4) becomes
?? = argmin
?
1
N
N?
i=1
?
x,y
p?(x | yi) p?(y |x)L(y, yi)
(8)
If the loss function L(y, yi) is a negated BLEU, this
is equivalent to performing minimum-risk training
described by (Smith and Eisner, 2006; Li and Eisner,
2009) on the imputed data.10
9One can manipulate the loss function to support other
methods that use deterministic decoding, such as Perceptron
(Collins, 2002) and MIRA (Crammer et al, 2006).
10Again, one may manipulate the loss function to support
other probabilistic methods that use randomized decoding, such
as CRFs (Lafferty et al, 2001).
The objective function in (8) is now differentiable,
since each coefficient p?(y |x) is a differentiable
function of ?, and thus amenable to optimization
by gradient-based methods; we use the L-BFGS al-
gorithm (Liu et al, 1989) in our experiments. We
perform experiments with the syntax-based MT sys-
tem Joshua (Li et al, 2009a), which implements
dynamic programming algorithms for second-order
expectation semirings (Li and Eisner, 2009) to effi-
ciently compute the gradients needed for optimizing
(8).
3.4 Approximating p?(x | yi)
As mentioned at the end of Section 3.1, it is com-
putationally infeasible to forward-translate each of
the imputed reverse translations xij . We propose
four approximations that are computationally feasi-
ble. Each may be regarded as a different approxima-
tion of p?(x | yi) in equations (4) or (8).
k-best. For each yi, add to the imputed training set
only the k most probable translations {xi1, . . . xik}
according to p?(x | yi). (These can be extracted
from Xi using standard algorithms (Huang and Chi-
ang, 2005).) Rescale their probabilities to sum to 1.
Sampling. For each yi, add to the training set k in-
dependent samples {xi1, . . . xik} from the distribu-
tion p?(x | yi), each with weight 1/k. (These can be
sampled from Xi using standard algorithms (John-
son et al, 2007).) This method is known in the liter-
ature as multiple imputation (Rubin, 1987).
Lattice. 11 Under certain special cases it is be pos-
sible to compute the expected loss in (3) exactly
via dynamic programming. Although Xi does con-
tain exponentially many translations, it may use a
?packed? representation in which these translations
share structure. This representation may further-
more enable sharing work in forward-translation, so
as to efficiently translate the entire set Xi and ob-
tain a distribution over translations y. Finally, the
expected loss under that distribution, as required by
equation (3), may also be efficiently computable.
All this turns out to be possible if (a) the poste-
rior distribution p?(x | yi) is represented by an un-
11The lattice approximation is presented here as a theoreti-
cal contribution, and we do not empirically evaluate it since its
implementation requires extensive engineering effort that is be-
yond the main scope of this paper.
923
ambiguous weighted finite-state automaton Xi, (b)
the forward translation system ?? is structured in a
certain way as a weighted synchronous context-free
grammar, and (c) the loss function decomposes in a
certain way. We omit the details of the construction
as beyond the scope of this paper.
In our experimental setting described below, (b) is
true (using Joshua), and (c) is true (since we use a
loss function presented by Tromble et al (2008) that
is an approximation to BLEU and is decomposable).
While (a) is not true in our setting because Xi is a
hypergraph (which is ambiguous), Li et al (2009b)
show how to approximate a hypergraph representa-
tion of p?(x | yi) by an unambiguous WFSA. One
could then apply the construction to this WFSA12,
obtaining an approximation to (3).
Rule-level Composition. Intuitively, the reason
why the structure-sharing in the hypergraphXi (gen-
erated by the reverse system) cannot be exploited
during forward translating is that when the forward
Hiero system translates a string xi ? Xi, it must
parse it into recursive phrases.
But the structure-sharing within the hypergraph of
Xi has already parsed xi into recursive phrases, in a
way determined by the reverse Hiero system; each
translation phrase (or rule) corresponding to a hy-
peredge. To exploit structure-sharing, we can use
a forward translation system that decomposes ac-
cording to that existing parse of xi. We can do that
by considering only forward translations that respect
the hypergraph structure of Xi. The simplest way to
do this is to require complete isomorphism of the
SCFG trees used for the reverse and forward trans-
lations. In other words, this does round-trip impu-
tation (i.e., from y to x, and then to y?) at the rule
level. This is essentially the approach taken by Li et
al. (2010).
3.5 The Log-Linear Model p?
We have not yet specified the form of p?. Following
much work in MT, we begin with a linear model
score(x, y) = ? ? f(x, y) =
?
k
?kfk(x, y) (9)
where f(x, y) is a feature vector indexed by k. Our
deterministic test-time translation system ?? simply
12Note that the forward translation of a WFSA is tractable by
using a lattice-based decoder such as that by Dyer et al (2008).
outputs the highest-scoring y for fixed x. At training
time, our randomized decoder (Section 3.3.2) uses
the Boltzmann distribution (here a log-linear model)
p?(y |x) =
e??score(x,y)
Z(x) =
e??score(x,y)?
y? e??score(x,y
?) (10)
The scaling factor ? controls the sharpness of the
training-time distribution, i.e., the degree to which
the randomized decoder favors the highest-scoring
y. For large ?, our training objective approaches
the imputed risk of the deterministic test-time sys-
tem while remaining differentiable.
In a task like MT, in addition to the input x and
output y, we often need to introduce a latent variable
d to represent the hidden derivation that relates x to
y. A derivation d represents a particular phrase seg-
mentation in a phrase-based MT system (Koehn et
al., 2003) and a derivation tree in a typical syntax-
based system (Galley et al, 2006; Chiang, 2007).
We change our model to assign scores not to an
(x, y) pair but to the detailed derivation d; in partic-
ular, now the function f that extracts a feature vector
can look at all of d. We replace y by d in (9)?(10),
and finally define p?(y|x) by marginalizing out d,
p?(y |x) =
?
d?D(x,y)
p?(d |x) (11)
where D(x, y) represents the set of derivations that
yield x and y.
4 Minimum Imputed Risk vs. EM
The notion of imputing missing data is familiar
from other settings (Little and Rubin, 1987), particu-
larly the expectation maximization (EM) algorithm,
a widely used generative approach. So it is instruc-
tive to compare EM with minimum imputed risk.
One can estimate ? by maximizing the log-
likelihood of the data {(xi, yi), i = 1, . . . , N} as
argmax
?
1
N
N?
i=1
log p?(xi, yi). (12)
If the xi?s are missing, EM tries to iteratively maxi-
mize the marginal probability:
argmax
?
1
N
N?
i=1
log
?
x
p?(x, yi). (13)
924
The E-step of each iteration comprises comput-
ing ?x p?t(x | yi) log p?(x, yi), the expected log-
likelihood of the complete data, where p?t(x | yi) is
the conditional part of p?t(x, yi) under the current
iterate ?t, and the M-step comprises maximizing it:
?t+1 = argmax
?
1
N
N?
i=1
?
x
p?t(x | yi) log p?(x, yi).
(14)
Notice that if we replace p?t(x|yi) with p?(x | yi)
in the equation above, and admit negated log-
likelihood as a loss function, then the EM update
(14) becomes identical to (4). In other words, the
minimum imputed risk approach of Section 3.1 dif-
fers from EM in (i) using an externally-provided and
static p?, instead of refining it at each iteration based
on the current p?t , and (ii) using a specific loss func-
tion, namely negated log-likelihood.
So why not simply use the maximum-likelihood
(EM) training procedure for MT? One reason is
that it is not discriminative: the loss function (e.g.
negated BLEU) is ignored during training.
A second reason is that training good joint models
p?(x, y) is computationally expensive. Contempo-
rary MT makes heavy use of log-linear probability
models, which allow the system designer to inject
phrase tables, linguistic intuitions, or prior knowl-
edge through a careful choice of features. Comput-
ing the objective function of (14) in closed form is
difficult if p? is an arbitrary log-linear model, be-
cause the joint probability p?(xi, yi) is then defined
as a ratio whose denominatorZ? involves a sum over
all possible sentence pairs (x, y) of any length.
By contrast, our discriminative framework will
only require us to work with conditional models.
While conditional probabilities such as p?(x | y) and
p?(y |x) are also ratios, computing their denomina-
tors only requires us to sum over a packed forest of
possible translations of a given y or x.13
In summary, EM would impute missing data us-
ing p?(x | y) and predict outputs using p?(y |x),
both being conditional forms of the same joint
model p?(x, y). Our minimum imputed risk train-
ing method is similar, but it instead uses a pair of
13Analogously, discriminative CRFs have become more pop-
ular than generative HMMs because they permit efficient train-
ing even with a wide variety of log-linear features (Lafferty et
al., 2001).
separately parameterized, separately trained mod-
els p?(x | y) and p?(y |x). By sticking to condi-
tional models, we can efficiently use more sophis-
ticated model features, and we can incorporate the
loss function when we train ?, which should improve
both efficiency and accuracy at test time.
5 Experimental Results
We report results on Chinese-to-English translation
tasks using Joshua (Li et al, 2009a), an open-source
implementation of Hiero (Chiang, 2007).
5.1 Baseline Systems
5.1.1 IWSLT Task
We train both reverse and forward baseline sys-
tems. The translation models are built using the cor-
pus for the IWSLT 2005 Chinese to English trans-
lation task (Eck and Hori, 2005), which comprises
40,000 pairs of transcribed utterances in the travel
domain. We use a 5-gram language model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998), trained on the English (resp. Chi-
nese) side of the bitext. We use a standard train-
ing pipeline and pruning settings recommended by
(Chiang, 2007).
5.1.2 NIST Task
For the NIST task, the TM is trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora dis-
tributed by LDC for the NIST MT evaluation using a
sampling method implemented in Joshua. We also
used a 5-gram language model, trained on a data set
consisting of a 130M words in English Gigaword
(LDC2007T07) and the bitext?s English side.
5.2 Feature Functions
We use two classes of features fk for discriminative
training of p? as defined in (9).
5.2.1 Regular Hiero Features
We include ten features that are standard in Hi-
ero (Chiang, 2007). In particular, these include
one baseline language model feature, three baseline
translation models, one word penalty feature, three
features to count how many rules with an arity of
925
zero/one/two are used in a derivation, and two fea-
tures to count how many times the unary and binary
glue rules in Hiero are used in a derivation.
5.2.2 Target-rule Bigram Features
In this paper, we do not attempt to discrimina-
tively tune a separate parameter for each bilingual
rule in the Hiero grammar. Instead, we train several
hundred features that generalize across these rules.
For each bilingual rule, we extract bigram fea-
tures over the target-side symbols (including non-
terminals and terminals). For example, if a bilingual
rule?s target-side is ?on the X1 issue of X2? where
X1 and X2 are non-terminals (with a position in-
dex), we extract the bigram features on the, the X ,
X issue, issue of, and of X . (Note that the posi-
tion index of a non-terminal is ignored in the fea-
ture.) Moreover, for the terminal symbols, we will
use their dominant POS tags (instead of the sym-
bol itself). For example, the feature the X becomes
DTX . We use 541 such bigram features for IWSLT
task (and 1023 such features for NIST task) that fire
frequently.
5.3 Data Sets for Discriminative Training
5.3.1 IWSLT Task
In addition to the 40,000 sentence pairs used to
train the baseline generative models (which are used
to compute the features fk), we use three bilingual
data sets listed in Table 1, also from IWSLT, for dis-
criminative training: one to train the reverse model
p? (which uses only the 10 standard Hiero features
as described in Section 5.2.1),14 one to train the for-
ward model ?? (which uses both classes of features
described in Section 5.2, i.e., 551 features in total),
and one for test.
Note that the reverse model ? is always trained us-
ing the supervised data of Dev ?, while the forward
model ? may be trained in a supervised or semi-
supervised manner, as we will show below.
In all three data sets, each Chinese sentence xi
has 16 English reference translations, so each yi is
actually a set of 16 translations. When we impute
data from yi (in the semi-supervised scenario), we
14Ideally, we should train ? to minimize the conditional
cross-entropy (5) as suggested in section 3.2. In the present
results, we trained ? discriminatively to minimize risk, purely
for ease of implementation using well versed steps.
Data set Purpose # of sentencesChinese English
Dev ? training ? 503 503?16
Dev ? training ? 503? 503?16
Eval ? testing 506 506?16
Table 1: IWSLT Data sets used for discriminative
training/test. Dev ? is used for discriminatively training
of the reverse model ?, Dev ? is for the forward model,
and Eval ? is for testing. The star ? for Dev ? empha-
sizes that some of its Chinese side will not be used in the
training (see Table 2 for details).
actually impute 16 different values of xi, by using
p? to separately reverse translate each sentence in
yi. This effectively adds 16 pairs of the form (xi, yi)
to the training set (see section 3.4), where each xi
is a different input sentence (imputed) in each case,
but yi is always the original set of 16 references.
5.3.2 NIST Task
For the NIST task, we use MT03 set (having 919
sentences) to tune the component parameters in both
the forward and reverse baseline systems. Addition-
ally, we use the English side of MT04 (having 1788
sentences) to perform semi-supervised tuning of the
forward model. The test sets are MT05 and MT06
(having 1082 and 1099 sentences, respectively). In
all the data sets, each source sentence has four refer-
ence translations.
5.4 Main Results
We compare two training scenarios: supervised and
semi-supervised. The supervised system (?Sup?)
carries out discriminative training on a bilingual data
set. The semi-supervised system (?+Unsup?) addi-
tionally uses some monolingual English text for dis-
criminative training (where we impute one Chinese
translation per English sentence).
Tables 2 and 3 report the results for the two tasks
under two training scenarios. Clearly, adding unsu-
pervised data improves over the supervised case, by
at least 1.3 BLEU points in IWSLT and 0.5 BLEU in
NIST.
5.5 Results for Analysis Purposes
Below, we will present more results on the IWSLT
data set to help us understand the behavior of the
926
Training scenario Test BLEU
Sup, (200, 200?16) 47.6
+Unsup, 101?16 Eng sentences 49.0
+Unsup, 202?16 Eng sentences 48.9
+Unsup, 303?16 Eng sentences 49.7?
Table 2: BLEU scores for semi-supervised training for
IWSLT task. The supervised system (?Sup?) is trained
on a subset of Dev ? containing 200 Chinese sentences
and 200?16 English translations. ?+Unsup? means that
we include additional (monolingual) English sentences
from Dev ? for semi-supervised training; for each En-
glish sentence, we impute the 1-best Chinese translation.
A star ? indicates a result that is signicantly better than
the ?Sup? baseline (paired permutation test, p < 0.05).
Training scenario Test BLEUMT05 MT06
Sup, (919, 919?4) 32.4 30.6
+Unsup, 1788 Eng sentences 33.0? 31.1?
Table 3: BLEU scores for semi-supervised training for
NIST task. The ?Sup? system is trained on MT03, while
the ?+Unsup? system is trained with additional 1788 En-
glish sentences from MT04. (Note that while MT04 has
1788?4 English sentences as it has four sets of refer-
ences, we only use one such set, for computational ef-
ficiency of discriminative training.) A star ? indicates a
result that is signicantly better than the ?Sup? baseline
(paired permutation test, p < 0.05).
methods proposed in this paper.
5.5.1 Imputation with Different Reverse
Models
A critical component of our unsupervised method
is the reverse translation model p?(x | y). We
wonder how the performance of our unsupervised
method changes when the quality of the reverse sys-
tem varies. To study this question, we used two dif-
ferent reverse translation systems, one with a lan-
guage model trained on the Chinese side of the bi-
text (?WLM?), and the other one without using such
a Chinese LM (?NLM?). Table 4 (in the fully unsu-
pervised case) shows that the imputed Chinese trans-
lations have a far lower BLEU score without the lan-
guage model,15 and that this costs us about 1 English
15The BLEU scores are low even with the language model
because only one Chinese reference is available for scoring.
Data size Imputed-CN BLEU Test-EN BLEUWLM NLM WLM NLM
101 11.8 3.0 48.5 46.7
202 11.7 3.2 48.9 47.6
303 13.4 3.5 48.8 47.9
Table 4: BLEU scores for unsupervised training
with/without using a language model in the reverse
system. A data size of 101 means that we use only
the English sentences from a subset of Dev ? containing
101 Chinese sentences and 101?16 English translations;
for each English sentence we impute the 1-best Chinese
translation. ?WLM? means a Chinese language model
is used in the reverse system, while ?NLM? means no
Chinese language model is used. In addition to reporting
the BLEU score on Eval ?, we also report ?Imputed-CN
BLEU?, the BLEU score of the imputed Chinese sentences
against their corresponding Chinese reference sentences.
BLEU point in the forward translations. Still, even
with the worse imputation (in the case of ?NLM?),
our forward translations improve as we add more
monolingual data.
5.5.2 Imputation with Different k-best Sizes
In all the experiments so far, we used the reverse
translation system to impute only a single Chinese
translation for each English monolingual sentence.
This is the 1-best approximation of section 3.4.
Table 5 shows (in the fully unsupervised case)
that the performance does not change much as k in-
creases.16 This may be because that the 5-best sen-
tences are likely to be quite similar to one another
(May and Knight, 2006). Imputing a longer k-best
list, a sample, or a lattice for xi (see section 3.4)
might achieve more diversity in the training inputs,
which might make the system more robust.
6 Conclusions
In this paper, we present an unsupervised discrimi-
native training method that works with missing in-
puts. The key idea in our method is to use a re-
verse model to impute the missing input from the ob-
served output. The training will then forward trans-
late the imputed input, and choose the parameters of
the forward model such that the imputed risk (i.e.,
16In the present experiments, however, we simply weighted
all k imputed translations equally, rather than in proportion to
their posterior probabilities as suggested in Section 3.4.
927
Training scenario Test BLEU
Unsup, k=1 48.5
Unsup, k=2 48.4
Unsup, k=3 48.9
Unsup, k=4 48.5
Unsup, k=5 48.4
Table 5: BLEU scores for unsupervised training with
different k-best sizes. We use 101?16 monolingual En-
glish sentences, and for each English sentence we impute
the k-best Chinese translations using the reverse system.
the expected loss of the forward translations with
respect to the observed output) is minimized. This
matches the intuition that the probabilistic ?round-
trip? translation from the target-language sentence
to the source-language and back should have low ex-
pected loss.
We applied our method to two Chinese to English
machine translation tasks (i.e. IWSLT and NIST).
We showed that augmenting supervised data with
unsupervised data improved performance over the
supervised case (for both tasks).
Our discriminative model used only a small
amount of training data and relatively few features.
In future work, we plan to test our method in settings
where there are large amounts of monolingual train-
ing data (enabling many discriminative features).
Also, our experiments here were performed on a lan-
guage pair (i.e., Chinese to English) that has quite
rich bilingual resources in the domain of the test
data. In future work, we plan to consider low-
resource test domains and language pairs like Urdu-
English, where bilingual data for novel domains is
sparse.
Acknowledgements
This work was partially supported by NSF Grants
No IIS-0963898 and No IIS-0964102 and the
DARPA GALE Program. The authors thank Markus
Dreyer, Damianos Karakos and Jason Smith for in-
sightful discussions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In ACL,
pages 1012?1020.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In IWSLT.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Dis-
criminative training methods for language models us-
ing conditional entropy criteria. In ICASSP.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In NAACL, pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL,
pages 48?54.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar. Zaidan. 2009a.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason
Eisner. 2010. Unsupervised discriminative language
928
model training for machine translation using simulated
confusion sets. In COLING, pages 556?664.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL, pages 761?
768.
R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis
with Missing Data. J. Wiley & Sons, New York.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the
limited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Thomas Minka. 2000. Empirical risk minimization is
an incomplete inductive principle. In MIT Media Lab
note.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
D. B. Rubin. 1987. Multiple Imputation for Nonresponse
in Surveys. J. Wiley & Sons, New York.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In ACL,
pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum-Bayes-risk de-
coding for statistical machine translation. In EMNLP,
pages 620?629.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
929
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
