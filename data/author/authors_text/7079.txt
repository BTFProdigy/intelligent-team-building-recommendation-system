Correction Grammars for Error Handling in a Speech Dialog System 
 
Hirohiko Sagawa Teruko Mitamura Eric Nyberg 
Language Technologies Institute, Carnegie Mellon University 
Pittsburgh, PA 15213, U.S.A. 
{hsagawa, teruko, ehn}@cs.cmu.edu 
 
 
 
Abstract 
Speech recognition errors are inevitable in a 
speech dialog system. This paper presents an 
error handling method based on correction 
grammars which recognize the correction 
utterances which follow a recognition error. 
Correction grammars are dynamically created 
from existing grammars and a set of 
correction templates. We also describe a 
prototype dialog system which incorporates 
this error handling method, and provide 
empirical evidence that this method can 
improve dialog success rate and reduce the 
number of dialog turns required for error 
recovery. 
1 Introduction 
In a dialog system, speech recognition errors are 
inevitable and often make smooth communication 
between a user and a system difficult. Figure 1 shows an 
example of a dialog between a user and a system which 
illustrates a system error. The system misrecognized 
?Tokyo? in the user?s utterance (U1) as ?Kyoto? (S3). If 
the system correctly recognized the user?s utterance, the 
user could answer ?yes? at U3 and the weather is 
reported (S6). However, in this case, the user must 
correct the error at U3 and the turns from S4 to U5 are 
required to recover from the error. The dialog system 
must recognize the user?s response to the system error 
(correction utterance). Otherwise, more turns (or a 
complete restart) will be required to correct the error. 
Therefore, an error handling method which corrects a 
system error and returns to the normal dialog flow 
smoothly is an important requirement for practical 
dialog systems.  
Recent work related to error handling in speech 
dialog systems has mainly focused on error detection. 
Walker et al (2000), Bosch et al (2001) and 
Kazemzadeh et al (2003) extracted several parameters 
(e.g., acoustic, lexical and semantic) from a dialog 
corpus, and analyzed the differences between correction 
utterances and the other utterances in a dialog. They 
also tried to detect system errors by using these 
parameters as input to machine learning methods. 
However, the issue of error recovery is not addressed. 
Danieli (1996) and LuperFoy & Duff (1996) 
proposed error detection methods based on plan 
matching. An error is detected when the intention or the 
parameter expressed in the user?s utterance is not 
consistent with the system?s assumptions and/or 
limitations. In these studies, the correction utterances 
are assumed to be recognized correctly. 
Kitaoka et al (2003) proposed a method to detect 
system errors based on the similarity of speech patterns 
and hypotheses overlapping in the recognition result. 
They also proposed a method to improve the recognition 
accuracy for correction utterances by selecting a speech 
recognition grammar according to the results of the 
error detection. 
The previous studies assumed that the rules for 
speech recognition or natural language processing of 
correction utterances were prepared in advance (Danieli , 
1996; LuperFoy & Duff, 1996). These rules are 
indispensable because the correction utterance often 
includes the information required to correct the error. 
The correction utterance depends on the dialog context, 
especially on the user?s utterances prior to the system 
error. Therefore it is difficult for the system designer to 
prepare these rules in advance when the dialog flow 
becomes complex. To solve this problem, a method that 
can automatically create the rules to interpret correction 
utterances is desirable. 
In this paper, we will propose a method to 
dynamically create the rules to recognize correction 
utterances and repair recognition errors based on the 
dialog context. A prototype dialog system which 
incorporates the proposed method has been developed, 
S1:  Please tell me the area. 
U1: Tokyo. 
S2: Please tell me the date. 
U2: Tomorrow. 
S3: Would you like to know the weather for Kyoto 
tomorrow? 
U3: No, Tokyo. 
S4: Did you say Tokyo? 
U4: Yes. 
S5:  Would you like to know the weather for Tokyo 
tomorrow? 
U5: Yes. 
S6: The weather for Tokyo tomorrow is fine. 
Correction utterance 
System error 
 
Figure 1. Example of a dialog with a system error 
and we present experimental results which show the 
effectiveness of the approach. 
2 CAMMIA Dialog System 
Our current approach focuses on dialog systems which 
incorporate speech recognition modules utilizing regular 
grammars. The CAMMIA system is an example of such 
a dialog system (Nyberg et al, 2002). 
The CAMMIA system is a client-server dialog 
management system based on VoiceXML. Each dialog 
scenario in this system is described in the format of 
DialogXML. The system has the initiative in the dialog, 
and dialogs are oriented around slot-filling for particular 
queries or requests. The server sends a VoiceXML data 
file to the client VoiceXML interpreter for a particular 
dialog turn, compiled from the DialogXML scenario 
according to the current dialog context. The VoiceXML 
data includes system prompts, names of grammar files 
and valid transitions to subsequent dialog states. The 
client interacts with the user according to the 
VoiceXML data.  
Figure 2 shows an example of a grammar rule used 
in the CAMMIA system. The regular grammar rule can 
be represented as a transition network. The following 
sentences are recognized by the rule in Figure 2: 
? I would like to know the weather for Tokyo. 
? I would like to know the weather for Tokyo tomorrow. 
3 Error Handling Based on Correction 
Grammars 
To recognize the user?s utterances in a dialog system, a 
grammar for potential user utterances must be prepared 
in advance for each dialog context. For error handling, it 
is also necessary to anticipate correction utterances and 
prepare a correction grammar. We propose a method to 
automatically create the correction grammar based on 
the current dialog context; error detection and repair is 
implemented using the correction grammar. 
To create the correction grammar, the system must 
know the user?s utterances prior to the error, because 
correction utterances typically depend on them. If the 
user?s utterances are consistent with what the system is 
expecting, the correction grammar can be generated 
based on the grammar previously in use by the speech 
recognizer. Therefore, the sequence of grammars used 
in the dialog so far is stored in the grammar history as 
the dialog context, and the correction grammar is 
created using the grammars in this history. 
Most of the forms of correction utterances can be 
expected in advance because correction utterances 
include many repetitions of words or phrases from 
previous turns (Kazemzadeh et al, 2003). We assume 
that the rules to generate the correction grammar can be 
prepared as templates; the correction grammar is created 
by inserting information extracted from the grammar 
history into a template. 
Figure 3 shows an example of a process flow in a 
dialog system which performs error handling based on a 
correction grammar. The ?system prompt n? is the 
process to output the n-th prompt to the user. The 
correction grammar is created based on the grammar 
used in the ?user response n-1?, which is the process to 
recognize the (n-1)-th user utterance, and it is used in 
the ?user response n? together with the ?grammar n? 
which is used to recognize the n-th normal user?s 
utterance. The system detects the error when the user?s 
utterance is recognized using the correction grammar, 
and then transits into the ?correction of errors? to 
modify the error. The grammar history in Figure 3 
stores only the grammar used in the last recognition 
process. The number of grammars stored in the history 
can be changed depending on the dialog management 
strategy and error handling requirements. 
4 Generation of Correction Grammar 
The correction grammar is created as follows. 
(1) Copying the grammar rules in the history 
The user often repeats the same utterance when the 
system misunderstood what s/he spoke. To detect 
when the user repeats exactly the same utterance, the 
grammar rules in the grammar history are copied into 
the correction grammar. 
(2) Inserting the rules in the history into the template 
When the user tries to correct the system error, some 
 
1 
2 
4 
?I would like to know 
the weather for? 
?Tokyo? 
?tomorrow? 
3 
?Tokyo? 
 
Figure 2. Example of the grammar rule used in the 
CAMMIA system 
 
System prompt n-1
Grammar n-1
Generation of 
correction grammar 
Correction 
grammar n-1 
Grammar n 
. 
. 
. 
. 
. 
. 
Template 
Recognized by 
correction grammar ? 
Yes 
No 
User response n-1
System prompt n 
User response n 
Correction of errors 
System prompt n+1 
 
Figure 3. Process flow: Error handling based on a 
correction grammar 
phrases are often added to the original utterance 
(Kitaoka, 2003). The template mentioned above is 
used to support this type of correction utterance. An 
example of the correction grammar rule generated by 
this method is shown in Figure 4. The ?null? in Figure 
4 implies a transition with no condition, and the ?X? 
shows where the original rule is embedded. In this 
example, the created grammar rule in Figure 4(c) 
corresponds to the following sentences: 
? No, I?d like to know the weather for Tokyo. 
? I said I?d like to know the weather for Tokyo. 
(3) Inserting slot-values into the template 
The user often repeats only words or phrases which 
the system is focusing on (Kazemzadeh et al, 2003). 
In a slot-filling dialog, these correspond to the slot 
values. Therefore, correction grammar rules are also 
created by extracting the slot values from the grammar 
in the history and inserting them into the template. If 
there are several slot values that can be corrected at 
the same time, all of their possible combinations and 
permutations are also generated. An example is shown 
in Figure 5. In Figure 5(b), the slot-values are 
?Tokyo? and ?tomorrow?. The grammar rule in Figure 
5(c) includes each slot value plus their combination(s), 
and represents the following sentences: 
? I said Tokyo. 
? I said tomorrow. 
? I said Tokyo tomorrow. 
5 Prototype System with Error Handling 
We have implemented the proposed error handling 
method for a set of Japanese dialog scenarios in the 
CAMMIA system. We added to this system: a) a 
process to create a correction grammar file when the 
system sends a grammar file to the client, b) a process to 
repair errors based on the recognition result, and c) 
transitions to the repair action when the user?s utterance 
is recognized by the correction grammar. 
There are two types of errors: task transition errors 
and slot value errors. If the error is a task transition error, 
the modification process cancels the current task and 
transits to the new task as specified by the correction 
utterance. When the error is a slot value error, the slot 
value is replaced by the value given in the correction 
utterance. However, if the new value is identical to the 
old one, we assume a recognition error and the second 
candidate in the recognition result is used. This 
technique requires a speech recognizer that can output 
N-best results; we used Julius for SAPI (Kyoto Univ., 
2002) for this experiment. 
6 Experiments 
We carried out an experiment to verify whether the 
proposed method works properly in a dialog system. In 
this experiment, dialog systems with and without the 
error handling method were compared. In this 
experiment, a weather information dialog was selected 
as the task for the subjects and about 1200 dialog 
instances were analyzed (both with and without error 
handling). The dialog flow was the same as shown in 
Figure 1. The grammar included 500 words for place 
names, and 69 words for the date. The subjects were 
instructed in advance on the utterance patterns allowed 
by the system, and used only those patterns during the 
experiment. A summary of the collected data is shown 
in Table 1. When error handling is disabled, the system 
returns to the place name query when the user denies the 
system?s confirmation, e.g. it returns from U3 to S1 in 
Figure 1. A comparison of the number of turns in these 
two systems is shown in Table 2. ?1 error? in Table 2 
means that the dialog included one error and ?2 errors? 
means that the same error was repeated. 
The success rate for the task and the average number 
of turns in the dialog (including errors) are tabulated.  
Dialogs including more than 3 errors were regarded as 
incomplete tasks in the calculation of the success rate. 
The results are shown in Table 3. 
 
1 
X 2 
?no? 
null 
?I said? 
 
(a) Template 
 
1 
3 
?I would like
 
to know 
the weather for? 
2 
?Tokyo? 
 
(b) Grammar rule in the history 
 
3 4 
?I would like
 
to 
know the  
weather for? 
?Tokyo? 
1 
2 5 
?no? 
null 
?I said? 
 
(c) Created correction grammar rule 
Figure 4. Correction grammar created by inserting 
the original rule into a template 
  
1 
X 2 
null 
?I said? 
 
(a) Template 
 
1 
3 
?I would like to know 
the weather for? 
2 
?Tokyo? 
?tomorrow? 
 
(b) Grammar rule in the history 
 
3 
4 
?Tokyo? 
1 
2 5 
null 
?I said? 
?tomorrow? 
?Tokyo? 
?tomorrow? 
 
(c) Created correction grammar rule 
Figure 5. Correction grammar rules created by 
inserting slot values into a template 
7 Discussion 
The task completion rate was improved from 86.4% to 
93.4% when the proposed error handling method was 
used. The average number of turns was reduced by 3 
turns as shown in Table 3. This result shows that the 
proposed error handling method was working properly 
and effectively. 
One reason that the success rate was improved is 
that the proposed method prevents the repetition of 
errors. When the error handling method is not 
implemented, errors can be easily repeated. The error 
handling method can avoid repeated errors by selecting 
the second candidate in the recognition result even when 
the correction utterance is also misrecognized. There 
were 7 dialogs in which the correction utterance was 
correctly recognized by selecting the second candidate. 
However, there were 13 dialogs in which the error 
was not repaired by one correction utterance. There are 
two explanations. One is that there are insertion errors 
in speech recognition which causes words not spoken to 
appear in the recognition result. For example, the 
system prompt S4 for U3 in Figure 1 becomes as 
follows: 
S4: Did you say Tokyo yesterday? 
In this case, the user has to speak more correction 
utterances. The second explanation is that the 
recognition result did not always include the correct 
result within the first two candidates. It is not clear that 
extending the repair mechanism to always consider 
additional recognition candidates (3rd, 4th, etc.) is a 
viable technique, given the drop off in recognition 
accuracy; more study is required. 
8 Conclusions 
In this paper, we proposed an error handling method 
based on dynamic generation of correction grammars to 
recognize the user corrections which follow system 
errors. The correction grammars detect system errors 
and also repair the dialog flow, improving task 
completion rates and reducing the average number of 
dialog turns. We developed a prototype dialog system 
using the proposed method, and demonstrated 
empirically that the success rate improved by 7.0%, and 
the number of turns was reduced by 3. 
The creation of rules for correction utterances based 
on the dialog history could be applicable to dialog 
systems which use speech recognition or natural 
language processing and other kinds of rules beyond 
regular grammars; we plan to study this in future work. 
We are also planning to develop an algorithm to 
improve the precision of corrections that are based on 
the set of recognition candidates for the correction 
utterance and an error recovery strategy. We also plan to 
apply the proposed method to other types of dialogs, 
such as user-initiative dialogs and mixed-initiative 
dialogs. 
References 
Abe Kazemzadeh, Sungbok Lee and Shrikanth 
Narayanan. 2003. Acoustic Correlates of User 
Response to Error in Human-Computer Dialogues, 
Proceedings of ASRU 2003: 215-220. 
Antal van den Bosch, Emiel Krahmer and Marc 
Swerts. 2001. Detecting problematic turns in 
human-machine interactions: Rule-Induction 
Versus Memory-Based Learning Approaches, 
Proceedings of ACL 2001: 499-507. 
Eric Nyberg, Teruko Mitamura, Paul Placeway, 
Michael Duggan, Nobuo Hataoka. 2002. Dynamic 
Dialog Management with VoiceXML, Proceedings 
of HLT-2002. 
Kyoto University, 2002, Julius Open-Source Real-
Time Large Vocabulary Speech Recognition 
Engine, http://julius.sourceforge.jp. 
Marilyn Walker, Jerry Wright and Irene Langkilde. 
2000. Using Natural Language Processing and 
Discourse Features to Identify Understanding 
Errors in a Spoken Dialogue System, Proceedings 
of ICML-2000: 1111-1118. 
Morena Danieli. 1996. On the Use of Expectations for 
Detecting and Repairing Human-Machine 
Miscommunication, Proceedings of AAAI 
Workshop on Detection, Repair and Prevention of 
Human-Machine Miscommunication: 87-93. 
Norihide Kitaoka, Kaoko Kakutani and Seiichi 
Nakagawa. 2003. Detection and Recognition of 
Correction Utterance in Spontaneously Spoken 
Dialog, Proceedings of EUROSPEECH 2003: 625-
628. 
Susann LuperFoy and David Duff. 1996. Disco: A 
Four-Step Dialog Recovery Program, The 
Proceedings of the AAAI Workshop on Detection, 
Repair and Prevention of Human-Machine 
Miscommunication: 73-76. 
Table 1. Summary of the collected data 
 w/o error handling w/ error handling 
# of users 2 male, 1 female 2 male, 1 female 
# of dialog 603 596 
# of error dialog 66 61 
 
Table 2. Number of turns in the dialog 
 No error 1 error 2 errors 
w/o error handling 13 19 
w/ error handling 7 11 13 
 
Table 3. Success rate and average number of turns 
 Success rate Ave. # turns 
w/o error handling 86.4% 14.6 
w/ error handling 93.4% 11.6 
 
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121?124,
Prague, June 2007. c?2007 Association for Computational Linguistics
Predicting Evidence of Understanding by Monitoring User?s Task 
Manipulation in Multimodal Conversations 
Yukiko I. Nakano? 
Yoshiko Arimoto?? 
?Tokyo University of Agri-
culture and Technology 
2-24-16 Nakacho, Koganei-
shi, Tokyo 184-8588, Japan 
{nakano, kmurata, meno-
moto}@cc.tuat.ac.jp 
Kazuyoshi Murata? 
Yasuhiro Asa??? 
??Tokyo University of 
Technology 
1404-1 Katakura, Hachioji, 
Tokyo 192-0981, Japan 
ar@mf.teu.ac.jp 
Mika Enomoto? 
Hirohiko Sagawa??? 
???Central Research Laboratory, 
Hitachi, Ltd. 
1-280, Higashi-koigakubo Kokub-
unji-shi, Tokyo 185-8601, Japan 
{yasuhiro.asa.mk, hiro-
hiko.sagawa.cu}@hitachi.com 
 
 
 
Abstract 
The aim of this paper is to develop ani-
mated agents that can control multimodal 
instruction dialogues by monitoring user?s 
behaviors. First, this paper reports on our 
Wizard-of-Oz experiments, and then, using 
the collected corpus, proposes a probabilis-
tic model of fine-grained timing dependen-
cies among multimodal communication 
behaviors: speech, gestures, and mouse 
manipulations. A preliminary evaluation 
revealed that our model can predict a in-
structor?s grounding judgment and a lis-
tener?s successful mouse manipulation 
quite accurately, suggesting that the model 
is useful in estimating the user?s under-
standing, and can be applied to determining 
the agent?s next action.  
1 Introduction 
In face-to-face conversation, speakers adjust their 
utterances in progress according to the listener?s 
feedback expressed in multimodal manners, such 
as speech, facial expression, and eye-gaze. In task-
manipulation situations where the listener manipu-
lates objects by following the speaker?s instruc-
tions, correct task manipulation by the listener 
serves as more direct evidence of understanding 
(Brennan 2000, Clark and Krych 2004), and affects 
the speaker?s dialogue control strategies.  
Figure 1 shows an example of a software in-
struction dialogue in a video-mediated situation 
(originally in Japanese). While the learner says 
nothing, the instructor gives the instruction in 
small pieces, simultaneously modifying her ges-
tures and utterances according to the learner?s 
mouse movements. 
To accomplish such interaction between human 
users and animated help agents, and to assist the 
user through natural conversational interaction, this 
paper proposes a probabilistic model that computes 
timing dependencies among different types of be-
haviors in different modalities: speech, gestures, 
and mouse events. The model predicts (a) whether 
the instructor?s current utterance will be success-
fully understood by the learner and grounded 
(Clark and Schaefer 1989), and (b) whether the 
learner will successfully manipulate the object in 
the near future. These predictions can be used as 
constraints in determining agent actions. For ex-
ample, if the current utterance will not be grounded, 
then the help agent must add more information. 
In the following sections, first, we collect hu-
man-agent conversations by employing a Wizard-
of-Oz method, and annotate verbal and nonverbal 
behaviors. The annotated corpus is used to build a 
Bayesian network model for the multimodal in-
struction dialogues. Finally, we will evaluate how 
?That? (204ms pause)
Pointing gesture <preparation>
<stroke>
Mouse move
Instructor:
Learner:
?at the most? (395ms pause)
?left-hand side?
Instructor:
Learner:
Instructor:
Mouse move  
Figure 1: Example of task manipulation dialogue 
121
accurately the model can predict the events in (a) 
and (b) mentioned above. 
2 Related work 
In their psychological study, Clark and Krych 
(2004) showed that speakers alter their utterances 
midcourse while monitoring not only the listener?s 
vocal signals, but also the listener?s gestural sig-
nals as well as through other mutually visible 
events. Such a bilateral process functions as a joint 
activity to ground the presented information, and 
task manipulation as a mutually visible event con-
tributes to the grounding process (Brennan 2000, 
Whittaker 2003). Dillenbourg, Traum, et al (1996) 
also discussed cross-modality in grounding: ver-
bally presented information is grounded by an ac-
tion in the task environment.  
Studies on interface agents have presented com-
putational models of multimodal interaction 
(Cassell, Bickmore, et al 2000). Paek and Horvitz 
(1999) focused on uncertainty in speech-based in-
teraction, and employed a Bayesian network to 
understand the user?s speech input. For user moni-
toring, Nakano, Reinstein, et al (2003) used a head 
tracker to build a conversational agent which can 
monitor the user?s eye-gaze and head nods as non-
verbal signals in grounding. 
These previous studies provide psychological 
evidence about the speaker?s monitoring behaviors 
as well as conversation modeling techniques in 
computational linguistics. However, little has been 
studied about how systems (agents) should monitor 
the user?s task manipulation, which gives direct 
evidence of understanding to estimate the user?s 
understanding, and exploits the predicted evidence 
as constraints in selecting the agent?s next action. 
Based on these previous attempts, this study pro-
poses a multimodal interaction model by focusing 
on task manipulation, and predicts conversation 
states using probabilistic reasoning. 
3 Data collection 
A data collection experiment was conducted using 
a Wizard-of-Oz agent assisting a user in learning a 
PCTV application, a system for watching and re-
cording TV programs on a PC.  
The output of the PC operated by the user was 
displayed on a 23-inch monitor in front of the user, 
and also projected on a 120-inch big screen, in 
front of which a human instructor was standing 
(Figure 2 (a)). Therefore, the participants shared 
visual events output from the PC (Figure 2 (b)) 
while sitting in different rooms. In addition, a rab-
bit-like animated agent was controlled through the 
instructor?s motion data captured by motion sen-
sors. The instructor?s voice was changed through a 
voice transformation system to make it sound like 
a rabbit agent. 
4 Corpus  
We collected 20 conversations from 10 pairs, and 
annotated 11 conversations of 6 pairs using the 
Anvil video annotating tool (Kipp 2004).   
Agent?s verbal behaviors: The agent?s (actually, 
instructor?s) speech data was split by pauses longer 
than 200ms. For each inter pausal unit (IPU), utter-
ance content type defined as follows was assigned.  
? Identification (id): identification of a target 
object for the next operation 
? Operation (op): request to execute a mouse 
click or a similar primitive action on the target 
? Identification + operation (idop): referring to 
identification and operation in one IPU 
In addition to these main categories, we also 
used:  State (referring to a state before/after an op-
eration), Function (explaining a function of the 
system), Goal (referring to a task goal to be ac-
complished), and Acknowledgment. The inter-
coder agreement for this coding scheme is very 
high K=0.89 (Cohen?s Kappa), suggesting that the 
assigned tags are reliable.  
Agent?s nonverbal behaviors: As the most salient 
instructor?s nonverbal behaviors in the collected 
data, we annotated agent pointing gestures: 
? Agent movement: agent?s position  movement 
? Agent touching target (att): agent?s touching 
the target object as a stroke of a pointing ges-
ture  
          (a) Instructor                          (b) PC output 
Figure 2: Wizard-of-Oz agent controlled by instructor 
122
User?s nonverbal behaviors: We annotated three 
types of mouse manipulation for the user?s task 
manipulation as follows:   
? Mouse movement: movement of the mouse 
cursor 
? Mouse-on-target: the mouse cursor is on the 
target object  
? Click target: click on the target object 
4.1 Example of collected data 
 An example of an annotated corpus is shown in 
Figure 3. The upper two tracks illustrate the 
agent?s verbal and nonverbal behaviors, and the 
other two illustrate the user?s behaviors. The agent 
was pointing at the target (att) and giving a se-
quence of identification descriptions [a1-3]. Since 
the user?s mouse did not move at all, the agent 
added another identification IPU [a4] accompanied 
by another pointing gesture. Immediately after that, 
the user?s mouse cursor started moving towards the 
target object. After finishing the next IPU, the 
agent finally requested the user to click the object 
in [a6]. Note that the collected Wizard-of-Oz con-
versations are very similar to the human-human 
instruction dialogues shown in Figure 1. While 
carefully monitoring the user?s mouse actions, the 
Wizard-of-Oz agent provided information in small 
pieces. If it was uncertain that the user was follow-
ing the instruction, the agent added more explana-
tion without continuing. 
5 Probabilistic model of user-agent mul-
timodal interaction 
5.1 Building a Bayesian network model 
To consider multiple factors for verbal and non-
verbal behaviors in probabilistic reasoning, we 
employed a Bayesian network technique, which 
can infer the likelihood of the occurrence of a tar-
get event based on the dependencies among multi-
ple kinds of evidence. We extracted the conversa-
tional data from the beginning of an instructor's 
identification utterance for a new target object to 
the point that the user clicks on the object. Each 
IPU was split at 500ms intervals, and 1395 inter-
vals were obtained. As shown in Figure 4, the net-
work consists of 9 properties concerning verbal 
and nonverbal behaviors for past, current, and fu-
ture interval(s).   
5.2 Predicting evidence of understanding 
As a preliminary evaluation, we tested how ac-
curately our Bayesian network model can predict 
an instructor?s grounding judgment, and the user?s 
mouse click. The following five kinds of evidence 
were given to the network to predict future states. 
As evidence for the previous three intervals (1.5 
sec), we used (1) the percentage of time the agent 
touched the target (att), (2) the number of the 
user?s mouse movements. Evidence for the current 
interval is (3) current IPU?s content type, (4) 
whether the end of the current interval will be the 
end of the IPU (i.e. whether a pause will follow 
after the current interval), and (5) whether the 
mouse is on the target object. 
Well, 
Yes View 
the TV right of 
Yes 
Beside the DVD There is a button 
starts with ?V?
Ah, yes Er, yes 
Press it 
This 
User
Agent
Speech
Gesture
Mouser actions
id id id id id+op
Mouse move
click
att att att
Mouse on 
target
[a2] [a3] [a4] [a5] [a6][a1]
ack ack ack ack
Speech
Off
On
 
Figure 3: Example dialogue between Wizard-of-Oz agent and user 
 
Figure 4: Bayesian network model 
123
(a) Predicting grounding judgment: We tested 
how accurately the model can predict whether the 
instructor will go on to the next leg of the instruc-
tion or will give additional explanations using the 
same utterance content type (the current message 
will not be grounded). 
The results of 5-fold cross-validation are shown 
in Table 1. Since 83% of the data are ?same con-
tent? cases, prediction for ?same content? is very 
accurate (F-measure is 0.90). However, it is not 
very easy to find ?content change? case because of 
its less frequency (F-measure is 0.68). It would be 
better to test the model using more balanced data.  
(b) Predicting user?s mouse click: As a measure 
of the smoothness of task manipulation, the net-
work predicted whether the user?s mouse click 
would be successfully performed within the next 5 
intervals (2.5sec). If a mouse click is predicted, the 
agent should just wait without annoying the user 
by unnecessary explanation. Since randomized 
data is not appropriate to test mouse click predic-
tion, we used 299 sequences of utterances that w-
ere not used for training. Our model predicted 84% 
of the user?s mouse clicks: 80% of them were pre-
dicted 3-5 intervals before the actual occurrence of 
the mouse click, and 20% were predicted 1 interval 
before. However, the model frequently generates 
wrong predictions. Improving precision rate is 
necessary.  
6 Discussion and Future Work 
We employed a Bayesian network technique to our 
goal of developing conversational agents that can 
generate fine-grained multimodal instruction dia-
logues, and we proposed a probabilistic model for 
predicting grounding judgment and user?s success-
ful mouse click. The results of preliminary evalua-
tion suggest that separate models of each modality 
for each conversational participant cannot properly 
describe the complex process of on-going multi-
modal interaction, but modeling the interaction as 
dyadic activities with multiple tracks of modalities 
is a promising approach. 
The advantage of employing the Bayesian net-
work technique is that, by considering the cost of 
misclassification and the benefit of correct classifi-
cation, the model can be easily adjusted according 
to the purpose of the system or the user?s skill level. 
For example, we can make the model more cau-
tious or incautious. Thus, our next step is to im-
plement the proposed model into a conversational 
agent, and evaluate our model not only in its accu-
racy, but also in its effectiveness by testing the 
model with various utility values. 
References 
Brennan, S. 2000. Processes that shape conversation and 
their implications for computational linguistics. In 
Proceedings of 38th Annual Meeting of the ACL. 
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H. 
and Yan, H. (2000). Human Conversation as a Sys-
tem Framework: Designing Embodied Conversa-
tional Agents. Embodied Conversational Agents. J. 
Cassell, J. Sullivan, S. Prevost and E. Churchill. 
Cambridge, MA, MIT Press: 29-63. 
Clark, H. H. and Schaefer, E. F. 1989. Contributing to 
discourse. Cognitive Science 13: 259-294. 
Clark, H. H. and Krych, M. A. 2004. Speaking while 
monitoring addressees for understanding. Journal of 
Memory and Language 50(1): 62-81. 
Dillenbourg, P., Traum, D. R. and Schneider, D. 1996. 
Grounding in Multi-modal Task Oriented Collabora-
tion. In Proceedings of EuroAI&Education Confer-
ence: 415-425. 
Kipp, M. 2004. Gesture Generation by Imitation - From 
Human Behavior to Computer Character Animation, 
Boca Raton, Florida: Dissertation.com. 
Nakano, Y. I., Reinstein, G., Stocky, T. and Cassell, J. 
2003. Towards a Model of Face-to-Face Grounding. 
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics: 553-561. 
Paek, T. and Horvitz, E. (1999). Uncertainty, Utility, 
and Misunderstanding: A Decision-Theoretic Per-
spective on Grounding in Conversational Systems. 
Working Papers of the AAAI Fall Symposium on 
Psychological Models of Communication in Collabo-
rative Systems. S. E. Brennan, A. Giboin and D. 
Traum: 85-92. 
Whittaker, S. (2003). Theories and Methods in Medi-
ated Communication. The Handbook of Discourse 
Processes. A. Graesser, MIT Press. 
 
Table 1: Preliminary evaluation results 
 Precision Recall F-measure
Content  
change  0.53 0.99 0.68 
Same  
content 1.00 0.81 0.90 
124
