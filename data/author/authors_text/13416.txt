Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 109?112,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Using Generation for Grammar Analysis and Error Detection
Michael Wayne Goodman?
University of Washington
Dept. of Linguistics
Box 354340 Seattle, WA 98195, USA
goodmami@u.washington.edu
Francis Bond
NICT Language Infrastructure Group
3-5 Hikaridai, Seika-cho, So?raku-gun,
Kyoto, 619-0289 Japan
bond@ieee.org
Abstract
We demonstrate that the bidirectionality
of deep grammars, allowing them to gen-
erate as well as parse sentences, can be
used to automatically and effectively iden-
tify errors in the grammars. The system is
tested on two implemented HPSG gram-
mars: Jacy for Japanese, and the ERG for
English. Using this system, we were able
to increase generation coverage in Jacy by
18% (45% to 63%) with only four weeks
of grammar development.
1 Introduction
Linguistically motivated analysis of text provides
much useful information for subsequent process-
ing. However, this is generally at the cost of re-
duced coverage, due both to the difficulty of pro-
viding analyses for all phenomena, and the com-
plexity of implementing these analyses. In this
paper we present a method of identifying prob-
lems in a deep grammar by exploiting the fact that
it can be used for both parsing (interpreting text
into semantics) and generation (realizing seman-
tics as text). Since both parsing and generation use
the same grammar, their performance is closely
related: in general improving the performance or
cover of one direction will also improve the other.
(Flickinger, 2008)
The central idea is that we test the grammar on
a full round trip: parsing text to its semantic repre-
sentation and then generating from it. In general,
any sentence where we cannot reproduce the orig-
inal, or where the generated sentence significantly
differs from the original, identifies a flaw in the
grammar, and with enough examples we can pin-
point the grammar rules causing these problems.
We call our system Egad, which stands for Erro-
neous Generation Analysis and Detection.
?This research was carried out while visiting NICT.
2 Background
This work was inspired by the error mining ap-
proach of van Noord (2004), who identified prob-
lematic input for a grammar by comparing sen-
tences that parsed and those that didn?t from a
large corpus. Our approach takes this idea and fur-
ther applies it to generation. We were also inspired
by the work of Dickinson and Lee (2008), whose
?variation n-gram method? models the likelihood
a particular argument structure (semantic annota-
tion) is accurate given the verb and some context.
We tested Egad on two grammars: Jacy (Siegel,
2000), a Japanese grammar and the English Re-
source Grammar (ERG) (Flickinger, 2000, 2008)
from the DELPH-IN1 group. Both grammars are
written in the Head-driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994) framework,
and use Minimal Recursion Semantics (MRS)
(Copestake et al, 2005) for their semantic rep-
resentations. The Tanaka Corpus (Tanaka, 2001)
provides us with English and Japanese sentences.
The specific motivation for this work was to in-
crease the quality and coverage of generated para-
phrases using Jacy and the ERG. Bond et al
(2008) showed they could improve the perfor-
mance of a statistical machine translation system
by training on a corpus that included paraphrased
variations of the English text. We want to do the
same with Japanese text, but Jacy was not able to
produce paraphrases as well (the ERG had 83%
generation coverage, while Jacy had 45%) Im-
proving generation would also greatly benefit X-
to-Japanese machine translation tasks using Jacy.
2.1 Concerning Grammar Performance
There is a difference between the theoretical and
practical power of the grammars. Sometimes the
1Deep Linguistic Processing with HPSG Initiative ? see
http://www.delph-in.net for background informa-
tion, including the list of current participants and pointers to
available resources and documentation
109
parser or generator can reach the memory (i.e.
edge) limit, resulting in a valid result not being
returned. Also, we only look at the top-ranked2
parse and the first five generations for each item.
This is usually not a problem, but it could cause
Egad to report false positives.
HPSG grammars are theoretically symmetric
between parsing and generation, but in practice
this is not always true. For example, to improve
performance, semantically empty lexemes are not
inserted into a generation unless a ?trigger-rule?
defines a context for them. These trigger-rules
may not cover all cases.
3 Grammar Analysis
When analyzing a grammar, Egad looks at all in-
put sentences, parses, and generations processed
by the grammar and uses the information therein
to determine characteristics of these items. These
characteristics are encoded in a vector that can be
used for labeling and searching items. Some char-
acteristics are useful for error mining, while others
are used for grammar analysis.
3.1 Characteristic Types
Egad determines both general characteristics of an
item (parsability and generability), and character-
istics comparing parses with generations.
General characteristics show whether each item
could: be parsed (?parsable?), generate from
parsed semantics (?generable?), generate the orig-
inal parsed sentence (?reproducible?), and gener-
ate other sentences (?paraphrasable?).
For comparative characteristics, Egad com-
pares every generated sentence to the parsed sen-
tence whence its semantics originated, and deter-
mines if the generated sentence uses the same set
of lexemes, derivation tree,3 set of rules, surface
form, and MRS as the original.
3.2 Characteristic Patterns
Having determined all applicable characteristics
for an item or a generated sentence, we encode the
values of those characteristics into a vector. We
call this vector a characteristic pattern, or CP.
An example CP showing general characteristics is:
0010 -----
2Jacy and the ERG both have parse-ranking models.
3In comparing the derivation trees, we only look at phrasal
nodes. Lexemes and surface forms are not compared.
The first four digits are read as: the item is
parsable, generable, not reproducible, and is para-
phrasable. The five following dashes are for com-
parative characteristics and are inapplicable except
for generations.
3.3 Utility of Characteristics
Not all characteristics are useful for all tasks. We
were interested in improving Jacy?s ability to gen-
erate sentences, so we primarily looked at items
that were parsable but ungenerable. In comparing
generated sentences with the original parsed sen-
tence, those with differing semantics often point to
errors, as do those with a different surface form but
the same derivation tree and lexemes (which usu-
ally means an inflectional rule was misapplied).
4 Problematic Rule Detection
Our method for detecting problematic rules is to
train a maximum entropy-based classifier4 with n-
gram paths of rules from a derivation tree as fea-
tures and characteristic patterns as labels. Once
trained, we do feature-selection to look at what
paths of rules are most predictive of certain labels.
4.1 Rule Paths
We extract n-grams over rule paths, or RPs,
which are downward paths along the derivation
tree. (Toutanova et al, 2005) By creating sepa-
rate RPs for each branch in the derivation tree, we
retain some information about the order of rule ap-
plication without overfitting to specific tree struc-
tures. For example, Figure 1 is the derivation tree
for (1). A couple of RPs extracted from the deriva-
tion tree are shown in Figure 2.
(1) ?????
shashin-utsuri-ga
picture-taking-NOM
??
ii
good
(X is) good at taking pictures.
4.2 Building a Model
We build a classification model by using a parsed
or generated sentence?s RPs as features and that
sentence?s CP as a label. The set of RPs includes
n-grams over all specified values of N. The labels
are, to be more accurate, regular expressions of
4We would like to look at using different classifiers here,
such as Decision Trees. We initially chose MaxEnt because
it was easy to implement, and have since had little motivation
to change it because it produced useful results.
110
utterance rule-decl-finite
head subj rule
hf-complement-rule
quantify-n-lrule
compounds-rule
shashin
??
utsuri 1
??
ga
?
unary-vstem-vend-rule
adj-i-lexeme-infl-rule
ii-adj
??
Figure 1: Derivation tree for (1)
quantify-n-lrule ? compounds-rule ? shashin
quantify-n-lrule ? compounds-rule ? utsuri 1
Figure 2: Example RPs extracted from Figure 1
CPs and may be fully specified to a unique CP or
generalize over several.5 The user can weight the
RPs by their N value (e.g. to target unigrams).
4.3 Finding Problematic Rules
After training the model, we have a classifier that
predicts CPs given a set of RPs. What we want,
however, is the RP most strongly associated with
a given CP. The classifier we use provides an easy
method to get the score a given feature has for
some label. We iterate over all RPs, get their score,
then sort them based on the score. To help elim-
inate redundant results, we exclude any RP that
either subsumes or is subsumed by a previous (i.e.
higher ranked) RP.
Given a CP, the RP with the highest score
should indeed be the one most closely associated
to that CP, but it might not lead to the greatest
number of items affected. Fixing the second high-
est ranked RP, for example, may improve more
items than fixing the top ranked one. To help the
grammar developer decide the priority of prob-
lems to fix, we also output the count of items ob-
served with the given CP and RP.
5 Results and Evaluation
We can look at two sets of results: how well
Egad was able to analyze a grammar and detect
errors, and how well a grammar developer could
use Egad to fix a problematic grammar. While the
latter is also influenced by the skill of the gram-
mar developer, we are interested in how well Egad
5For example, /0010 -----/ is fully specified.
/00.. -----/ marginalizes two general characteristics
points to the most significant errors, and how it can
help reduce development time.
5.1 Error Mining
Table 1 lists the ten highest ranked RPs associated
with items that could parse but could not generate
in Jacy. Some RPs appear several times in differ-
ent contexts. We made an effort to decrease the
redundancy, but clearly this could be improved.
From this list of ten problematic RPs, there
are four unique problems: quantify-n-lrule (noun
quantification), no-nspec (noun specification), to-
comp-quotarg (? to quotative particle), and te-
adjunct (verb conjugation). The extra rules listed
in each RP show the context in which each
problem occurs, and this can be informative as
well. For instance, quantify-n-lrule occurs in
two primary contexts (above compounds-rule and
nominal-numcl-rule). The symptoms of the prob-
lem occur in the interation of rules in each context,
but the source of the problem is quantify-n-lrule.
Further, the problems identified are not always
lexically marked. quantify-n-lrule occurs for all
bare noun phrases (ie. without determiners). This
kind of error cannot be accurately identified by us-
ing just word or POS n-grams, we need to use the
actual parse tree.
5.2 Error Correction
Egad greatly facilitated our efforts to find and fix
a wide variety of errors in Jacy. For example, we
restructured semantic predicate hierarchies, fixed
noun quantification, allowed some semantically
empty lexemes to generate in certain contexts,
added pragmatic information to distinguish be-
tween politeness levels in pronouns, allowed im-
peratives to generate, allowed more constructions
for numeral classifiers, and more.
Egad also identified some issues with the ERG:
both over-generation (an under-constrained inflec-
tional rule) and under-generation (sentences with
the construction take {care|charge|. . . } of were
not generating).
5.3 Updated Grammar Statistics
After fixing the most significant problems in Jacy
(outlined in Section 5.2) as reported by Egad,
we obtained new statistics about the grammar?s
coverage and characteristics. Table 2 shows the
original and updated general statistics for Jacy.
We increased generability by 18%, doubled repro-
ducibility, and increased paraphrasability by 17%.
111
Score Count Rule Path N-grams
1.42340952569648 109 hf-complement-rule? quantify-n-lrule? compounds-rule
0.960090299833317 54 hf-complement-rule? quantify-n-lrule? nominal-numcl-rule? head-specifier-rule
0.756227560530811 63 head-specifier-rule? hf-complement-rule? no-nspec? ???
0.739668926140179 62 hf-complement-rule? head-specifier-rule? hf-complement-rule? no-nspec
0.739090261637851 22 hf-complement-rule? hf-adj-i-rule? quantify-n-lrule? compounds-rule
0.694215264789286 36 hf-complement-rule? hf-complement-rule? to-comp-quotarg? ???
0.676244980660372 82 vstem-vend-rule? te-adjunct? ???
0.617621482523537 26 hf-complement-rule? hf-complement-rule? to-comp-varg? ???
0.592260546433334 36 hf-adj-i-rule? hf-complement-rule? quantify-n-lrule? nominal-numcl-rule
0.564790702894285 62 quantify-n-lrule? compounds-rule? vn2n-det-lrule
Table 1: Top 10 RPs for ungenerable items
Original Modified
Parsable 82% 83%
Generable 45% 63%
Reproducible 11% 22%
Paraphrasable 44% 61%
Table 2: Jacy?s improved general statistics
As an added bonus, our work focused on improv-
ing generation also improved parsability by 1%.
Work is now continuing on fixing the remainder
of the identified errors.
6 Future Work
In future iterations of Egad, we would like to ex-
pand our feature set (e.g. information from failed
parses), and make the system more robust, such
as replacing lexical-ids (specific to a lexeme) with
lexical-types, since all lexemes of the same type
should behave identically. A more long-term goal
would allow Egad to analyze the internals of the
grammar and point out specific features within the
grammar rules that are causing problems. Some
of the errors detected by Egad have simple fixes,
and we believe there is room to explore methods
of automatic error correction.
7 Conclusion
We have introduced a system that identifies er-
rors in implemented HPSG grammars, and further
finds and ranks the possible sources of those prob-
lems. This tool can greatly reduce the amount
of time a grammar developer would spend find-
ing bugs, and helps them make informed decisions
about which bugs are best to fix. In effect, we are
substituting cheap CPU time for expensive gram-
mar developer time. Using our system, we were
able to improve Jacy?s absolute generation cover-
age by 18% (45% to 63%) with only four weeks
of grammar development.
8 Acknowledgments
Thanks to NICT for their support, Takayuki Kurib-
ayashi for providing native judgments, and Mar-
cus Dickinson for comments on an early draft.
References
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine trans-
lation by paraphrasing the training data. In International
Workshop on Spoken Language Translation, pages 150?
157. Honolulu.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281?
332.
Markus Dickinson and Chong Min Lee. 2008. Detecting
errors in semantic annotation. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08). Marrakech, Morocco.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28. (Special Issue on Efficient Processing with
HPSG).
Dan Flickinger. 2008. The English resource grammar. Tech-
nical Report 2007-7, LOGON, http://www.emmtee.
net/reports/7.pdf. (Draft of 2008-11-30).
Carl Pollard and Ivan A. Sag. 1994. Head Driven
Phrase Structure Grammar. University of Chicago Press,
Chicago.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation, pages 265 ? 280. Springer, Berlin,
Germany.
Yasuhito Tanaka. 2001. Compilation of a multilingual paral-
lel corpus. In Proceedings of PACLING 2001, pages 265?
268. Kyushu. (http://www.colips.org/afnlp/
archives/pacling2001/pdf/tanaka.pdf).
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the redwoods corpus. Research on Lan-
guage and Computation, 3(1):83?105.
Gertjan van Noord. 2004. Error mining for wide-coverage
grammar engineering. In 42nd Annual Meeting of the
Association for Computational Linguistics: ACL-2004.
Barcelona.
112
Proceedings of the ACL 2010 System Demonstrations, pages 1?6,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Grammar Prototyping and Testing with the
LinGO Grammar Matrix Customization System
Emily M. Bender, Scott Drellishak, Antske Fokkens, Michael Wayne Goodman,
Daniel P. Mills, Laurie Poulson, and Safiyyah Saleem
University of Washington, Seattle, Washington, USA
{ebender,sfd,goodmami,dpmills,lpoulson,ssaleem}@uw.edu,
afokkens@coli.uni-saarland.de
Abstract
This demonstration presents the LinGO
Grammar Matrix grammar customization
system: a repository of distilled linguis-
tic knowledge and a web-based service
which elicits a typological description of
a language from the user and yields a cus-
tomized grammar fragment ready for sus-
tained development into a broad-coverage
grammar. We describe the implementation
of this repository with an emphasis on how
the information is made available to users,
including in-browser testing capabilities.
1 Introduction
This demonstration presents the LinGO Gram-
mar Matrix grammar customization system1 and
its functionality for rapidly prototyping grammars.
The LinGO Grammar Matrix project (Bender et
al., 2002) is situated within the DELPH-IN2 col-
laboration and is both a repository of reusable
linguistic knowledge and a method of delivering
this knowledge to a user in the form of an ex-
tensible precision implemented grammar. The
stored knowledge includes both a cross-linguistic
core grammar and a series of ?libraries? contain-
ing analyses of cross-linguistically variable phe-
nomena. The core grammar handles basic phrase
types, semantic compositionality, and general in-
frastructure such as the feature geometry, while
the current set of libraries includes analyses of
word order, person/number/gender, tense/aspect,
case, coordination, pro-drop, sentential negation,
yes/no questions, and direct-inverse marking, as
well as facilities for defining classes (types) of lex-
ical entries and lexical rules which apply to those
types. The grammars produced are compatible
with both the grammar development tools and the
1
http://www.delph-in.net/matrix/customize/
2
http://www.delph-in.net
grammar-based applications produced by DELPH-
IN. The grammar framework used is Head-driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) and the grammars map bidirectionally
between surface strings and semantic representa-
tions in the format of Minimal Recursion Seman-
tics (Copestake et al, 2005).
The Grammar Matrix project has three goals?
one engineering and two scientific. The engineer-
ing goal is to reduce the cost of creating gram-
mars by distilling the solutions developed in exist-
ing DELPH-IN grammars and making them easily
available for new projects. The first scientific goal
is to support grammar engineering for linguistic
hypothesis testing, allowing users to quickly cus-
tomize a basic grammar and use it as a medium in
which to develop and test analyses of more inter-
esting phenomena.3 The second scientific goal is
to use computational methods to combine the re-
sults of typological research and formal syntactic
analysis into a single resource that achieves both
typological breadth (handling the known range of
realizations of the phenomena analyzed) and ana-
lytical depth (producing analyses which work to-
gether to map surface strings to semantic represen-
tations) (Drellishak, 2009).
2 System Overview
Grammar customization with the LinGO Gram-
mar Matrix consists of three primary activities:
filling out the questionnaire, preliminary testing of
the grammar fragment, and grammar creation.
2.1 Questionnaire
Most of the linguistic phenomena supported by the
questionnaire vary across languages along multi-
ple dimensions. It is not enough, for example,
3Research of this type based on the Grammar Matrix
includes (Crysmann, 2009) (tone change in Hausa) and
(Fokkens et al, 2009) (Turkish suspended affixation).
1
simply to know that the target language has coor-
dination. It is also necessary to know, among other
things, what types of phrases can be coordinated,
how those phrases are marked, and what patterns
of marking appear in the language. Supporting a
linguistic phenomenon, therefore, requires elicit-
ing the answers to such questions from the user.
The customization system elicits these answers us-
ing a detailed, web-based, typological question-
naire, then interprets the answers without human
intervention and produces a grammar in the format
expected by the LKB (Copestake, 2002), namely
TDL (type description language).
The questionnaire is designed for linguists who
want to create computational grammars of natu-
ral languages, and therefore it freely uses techni-
cal linguistic terminology, but avoids, when possi-
ble, mentioning the internals of the grammar that
will be produced, although a user who intends to
extend the grammar will need to become familiar
with HPSG and TDL before doing so.
The questionnaire is presented to the user as a
series of connected web pages. The first page the
user sees (the ?main page?) contains some intro-
ductory text and hyperlinks to direct the user to
other sections of the questionnaire (?subpages?).
Each subpage contains a set of related questions
that (with some exceptions) covers the range of
a single Matrix library. The actual questions in
the questionnaire are represented by HTML form
fields, including: text fields, check boxes, ra-
dio buttons, drop-downs, and multi-select drop-
downs. The values of these form fields are stored
in a ?choices file?, which is the object passed on
to the grammar customization stage.
2.1.1 Unbounded Content
Early versions of the customization system (Ben-
der and Flickinger, 2005; Drellishak and Bender,
2005) only allowed a finite (and small) number
of entries for things like lexical types. For in-
stance, users were required to provide exactly one
transitive verb type and one intransitive verb type.
The current system has an iterator mechanism in
the questionnaire that allows for repeated sections,
and thus unlimited entries. These repeated sec-
tions can also be nested, which allows for much
more richly structured information.
The utility of the iterator mechanism is most
apparent when filling out the Lexicon subpage.
Users can create an arbitrary number of lexical
rule ?slots?, each with an arbitrary number of
morphemes which each in turn bear any num-
ber of feature constraints. For example, the
user could create a tense-agreement morpholog-
ical slot, which contains multiple portmanteau
morphemes each expressing some combination of
tense, subject person and subject number values
(e.g., French -ez expresses 2nd person plural sub-
ject agreement together with present tense).
The ability provided by the iterators to create
unbounded content facilitates the creation of sub-
stantial grammars through the customization sys-
tem. Furthermore, the system allows users to ex-
pand on some iterators while leaving others un-
specified, thus modeling complex rule interactions
even when it cannot cover features provided by
these rules. A user can correctly model the mor-
photactic framework of the language using ?skele-
tal? lexical rules?those that specify morphemes?
forms and their co-occurrence restrictions, but per-
haps not their morphosyntactic features. The user
can then, post-customization, augment these rules
with the missing information.
2.1.2 Dynamic Content
In earlier versions of the customization system, the
questionnaire was static. Not only was the num-
ber of form fields static, but the questions were
the same, regardless of user input. The current
questionnaire is more dynamic. When the user
loads the customization system?s main page or
subpages, appropriate HTML is created on the fly
on the basis of the information already collected
from the user as well as language-independent in-
formation provided by the system.
The questionnaire has two kinds of dynamic
content: expandable lists for unbounded entry
fields, and the population of drop-down selec-
tors. The lists in an iterated section can be ex-
panded or shortened with ?Add? and ?Delete? but-
tons near the items in question. Drop-down selec-
tors can be automatically populated in several dif-
ferent ways.4 These dynamic drop-downs greatly
lessen the amount of information the user must
remember while filling out the questionnaire and
can prevent the user from trying to enter an invalid
value. Both of these operations occur without re-
freshing the page, saving time for the user.
4These include: the names of currently-defined features,
the currently-defined values of a feature, or the values of vari-
ables that match a particular regular expression.
2
2.2 Validation
It makes no sense to attempt to create a consis-
tent grammar from an empty questionnaire, an in-
complete questionnaire, or a questionnaire con-
taining contradictory answers, so the customiza-
tion system first sends a user?s answers through
?form validation?. This component places a set
of arbitrarily complex constraints on the answers
provided. The system insists, for example, that
the user not state the language contains no deter-
miners but then provide one in the Lexicon sub-
page. When a question fails form validation, it
is marked with a red asterisk in the questionnaire,
and if the user hovers the mouse cursor over the as-
terisk, a pop-up message appears describing how
form validation failed. The validation component
can also produce warnings (marked with red ques-
tion marks) in cases where the system can gen-
erate a grammar from the user?s answers, but we
have reason to believe the grammar won?t behave
as expected. This occurs, for example, when there
are no verbal lexical entries provided, yielding a
grammar that cannot parse any sentences.
2.3 Creating a Grammar
After the questionnaire has passed validation, the
system enables two more buttons on the main
page: ?Test by Generation? and ?Create Gram-
mar?. ?Test by Generation? allows the user to test
the performance of the current state of the gram-
mar without leaving the browser, and is described
in ?3. ?Create Grammar? causes the customiza-
tion system to output an LKB-compatible grammar
that includes all the types in the core Matrix, along
with the types from each library, tailored appropri-
ately, according to the specific answers provided
for the language described in the questionnaire.
2.4 Summary
This section has briefly presented the structure
of the customization system. While we antici-
pate some future improvements (e.g., visualiza-
tion tools to assist with designing type hierarchies
and morphotactic dependencies), we believe that
this system is sufficiently general to support the
addition of analyses of many different linguistic
phenomena. The system has been used to create
starter grammars for more than 40 languages in the
context of a graduate grammar engineering course.
To give sense of the size of the grammars
produced by the customization system, Table 1
compares the English Resource Grammar (ERG)
(Flickinger, 2000), a broad-coverage precision
grammar in the same framework under develop-
ment since 1994, to 11 grammars produced with
the customization system by graduate students in
a grammar engineering class at the University of
Washington. The students developed these gram-
mars over three weeks using reference materials
and the customization system. We compare the
grammars in terms of the number types they de-
fine, as well as the number of lexical rule and
phrase structure rule instances.5 We separate
types defined in the Matrix core grammar from
language-specific types defined by the customiza-
tion system. Not all of the Matrix-provided types
are used in the definition of the language-specific
rules, but they are nonetheless an important part of
the grammar, serving as the foundation for further
hand-development. The Matrix core grammar in-
cludes a larger number of types whose function is
to provide disjunctions of parts of speech. These
are given in Table 1, as ?head types?. The final col-
umn in the table gives the number of ?choices? or
specifications that the users gave to the customiza-
tion system in order to derive these grammars.
3 Test-by-generation
The purpose of the test-by-generation feature is to
provide a quick method for testing the grammar
compiled from a choices file. It accomplishes this
by generating sentences the grammar deems gram-
matical. This is useful to the user in two main
ways: it quickly shows whether any ungrammat-
ical sentences are being licensed by the grammar
and, by providing an exhaustive list of licensed
sentences for an input template, allows users to see
if an expected sentence is not being produced.
It is worth emphasizing that this feature of the
customization system relies on the bidirectional-
ity of the grammars; that is, the fact that the same
grammar can be used for both parsing and genera-
tion. Our experience has shown that grammar de-
velopers quickly find generation provides a more
stringent test than parsing, especially for the abil-
ity of a grammar to model ungrammaticality.
3.1 Underspecified MRS
Testing by generation takes advantage of the gen-
eration algorithm include in the LKB (Carroll et al,
5Serious lexicon development is taken as a separate task
and thus lexicon size is not included in the table.
3
Language Family Lg-specific types Matrix types Head types Lex rules Phrasal rules Choices
ERG Germanic 3654 N/A N/A 71 226 N/A
Breton Celtic 220 413 510 57 49 1692
Cherokee Iroquoian 182 413 510 95 27 985
French Romance 137 413 510 29 22 740
Jamamad?? Arauan 188 413 510 87 11 1151
Lushootseed Salish 95 413 510 20 8 391
Nishnaabemwin Algonquian 289 413 510 124 50 1754
Pashto Iranian 234 413 510 86 19 1839
Pali Indo-Aryan 237 413 510 92 55 1310
Russian Slavic 190 413 510 56 35 993
Shona Bantu 136 413 510 51 9 591
Vietnamese Austro-Asiatic 105 413 510 2 26 362
Average 182.9 413 510 63.5 28.3 1073.5
Table 1: Grammar sizes in comparison to ERG
1999). This algorithm takes input in the form of
Minimal Recursion Semantics (MRS) (Copestake
et al, 2005): a bag of elementary predications,
each bearing features encoding a predicate string,
a label, and one or more argument positions that
can be filled with variables or with labels of other
elementary predications.6 Each variable can fur-
ther bear features encoding ?variable properties?
such as tense, aspect, mood, sentential force, per-
son, number or gender.
In order to test our starter grammars by gen-
eration, therefore, we must provide input MRSs.
The shared core grammar ensures that all of
the grammars produce and interpret valid MRSs,
but there are still language-specific properties in
these semantic representations. Most notably, the
predicate strings are user-defined (and language-
specific), as are the variable properties. In addi-
tion, some coarser-grained typological properties
(such as the presence or absence of determiners)
lead to differences in the semantic representations.
Therefore, we cannot simply store a set of MRSs
from one grammar to use as input to the generator.
Instead, we take a set of stored template MRSs
and generalize them by removing all variable
properties (allowing the generator to explore all
possible values), leaving only the predicate strings
and links between the elementary predications.
We then replace the stored predicate strings with
ones selected from among those provided by the
user. Figure 1a shows an MRS produced by a
grammar fragment for English. Figure 1b shows
the MRS with the variable properties removed
and the predicate strings replaced with generic
place-holders. One such template is needed for
every sentence type (e.g., intransitive, transitive,
6This latter type of argument encodes scopal dependen-
cies. We abstract away here from the MRS approach to scope
underspecification which is nonetheless critical for its com-
putational tractability.
a. ? h1,e2, {h7: cat n rel(x4:SG:THIRD),
h3:exist q rel(x4, h5, h6),
h1: sleep v rel(e2:PRES, x4)},
{h5 qeq h7} ?
b. ? h1,e2, {h7:#NOUN1#(x4),
h3:#DET1#(x4, h5, h6),
h1:#VERB#(e2, x4)},
{h5 qeq h7} ?
Figure 1: Original and underspecified MRS
negated-intransitive, etc.). In order to ensure that
the generated strings are maximally informative to
the user testing a grammar, we take advantage of
the lexical type system. Because words in lexical
types as defined by the customization system dif-
fer only in orthography and predicate string, and
not in syntactic behavior, we need only consider
one word of each type. This allows us to focus the
range of variation produced by the generator on
(a) the differences between lexical types and (b)
the variable properties.
3.2 Test by generation process
The first step of the test-by-generation process is
to compile the choices file into a grammar. Next,
a copy of the LKB is initialized on the web server
that is hosting the Matrix system, and the newly-
created grammar is loaded into this LKB session.
We then construct the underspecified MRSs in
order to generate from them. To do this, the pro-
cess needs to find the proper predicates to use for
verbs, nouns, determiners, and any other parts of
speech that a given MRS template may require. For
nouns and determiners, the choices file is searched
for the predicate for one noun of each lexical noun
type, all of the determiner predicates, and whether
or not each noun type needs a determiner or not.
For verbs, the process is more complicated, re-
quiring valence information as well as predicate
strings in order to select the correct MRS template.
In order to get this information, the process tra-
verses the type hierarchy above the verbal lexical
4
types until it finds a type that gives valence infor-
mation about the verb. Once the process has all
of this information, it matches verbs to MRS tem-
plates and fills in appropriate predicates.
The test-by-generation process then sends these
constructed MRSs to the LKB process and displays
the generation results, along with a brief explana-
tion of the input semantics that gave rise to them,
in HTML for the user.7
4 Related Work
As stated above, the engineering goal of the Gram-
mar Matrix is to facilitate the rapid development
of large-scale precision grammars. The starter
grammars output by the customization system are
compatible in format and semantic representations
with existing DELPH-IN tools, including software
for grammar development and for applications in-
cluding machine translation (Oepen et al, 2007)
and robust textual entailment (Bergmair, 2008).
More broadly, the Grammar Matrix is situated
in the field of multilingual grammar engineer-
ing, or the practice of developing linguistically-
motivated grammars for multiple languages within
a consistent framework. Other projects in this
field include ParGram (Butt et al, 2002; King
et al, 2005) (LFG), the CoreGram project8 (e.g.,
(Mu?ller, 2009)) (HPSG), and the MetaGrammar
project (de la Clergerie, 2005) (TAG).
To our knowledge, however, there is only one
other system that elicits typological information
about a language and outputs an appropriately cus-
tomized implemented grammar. The system, de-
scribed in (Black, 2004) and (Black and Black,
2009), is called PAWS (Parser And Writer for
Syntax) and is available for download online.9
PAWS is being developed by SIL in the context
of both descriptive (prose) grammar writing and
?computer-assisted related language adaptation?,
the practice of writing a text in a target language
by starting with a translation of that text in a
related source language and mapping the words
from target to source. Accordingly, the output of
PAWS consists of both a prose descriptive grammar
7This set-up scales well to multiple users, as the user?s in-
teraction with the LKB is done once per customized grammar,
providing output for the user to peruse as his or her leisure.
The LKB process does not persist, but can be started again
by reinvoking test-by-generation, such as when the user has
updated the grammar definition.
8
http://hpsg.fu-berlin.de/Projects/core.html
9
http://www.sil.org/computing/catalog/show_
software.asp?id=85
and an implemented grammar. The latter is in the
format required by PC-PATR (McConnel, 1995),
and is used primarily to disambiguate morpholog-
ical analyses of lexical items in the input string.
Other systems that attempt to elicit linguistic in-
formation from a user include the Expedition (Mc-
Shane and Nirenburg, 2003) and Avenue projects
(Monson et al, 2008), which are specifically tar-
geted at developing machine translation for low-
density languages. These projects differ from the
Grammar Matrix customization system in elic-
iting information from native speakers (such as
paradigms or translations of specifically tailored
corpora), rather than linguists. Further, unlike the
Grammar Matrix customization system, they do
not produce resources meant to sustain further de-
velopment by a linguist.
5 Demonstration Plan
Our demonstration illustrates how the customiza-
tion system can be used to create starter gram-
mars and test them by invoking test-by-generation.
We first walk through the questionnaire to illus-
trate the functionality of libraries and the way that
the user interacts with the system to enter infor-
mation. Then, using a sample grammar for En-
glish, we demonstrate how test-by-generation can
expose both overgeneration (ungrammatical gen-
erated strings) and undergeneration (gaps in gen-
erated paradigms). Finally, we return to the ques-
tionnaire to address the bugs in the sample gram-
mar and retest to show the result.
6 Conclusion
This paper has presented an overview of the
LinGO Grammar Matrix Customization System,
highlighting the ways in which it provides ac-
cess to its repository of linguistic knowledge. The
current customization system covers a sufficiently
wide range of phenomena that the grammars it
produces are non-trivial. In addition, it is not al-
ways apparent to a user what the implications will
be of selecting various options in the question-
naire, nor how analyses of different phenomena
will interact. The test-by-generation methodology
allows users to interactively explore the conse-
quences of different linguistic analyses within the
platform. We anticipate that it will, as a result, en-
courage users to develop more complex grammars
within the customization system (before moving
on to hand-editing) and thereby gain more benefit.
5
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
0644097. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Emily M. Bender and Dan Flickinger. 2005. Rapid
prototyping of scalable grammars: Towards modu-
larity in extensions to a language-independent core.
In Proc. of IJCNLP-05 (Posters/Demos).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proc. of the Workshop on Grammar Engineering
and Evaluation at COLING 2002, pages 8?14.
Richard Bergmair. 2008. Monte Carlo semantics:
McPIET at RTE4. In Text Analysis Conference (TAC
2008) Workshop-RTE-4 Track. National Institute of
Standards and Technology, pages 17?19.
Cheryl A. Black and H. Andrew Black. 2009. PAWS:
Parser and writer for syntax: Drafting syntactic
grammars in the third wave. In SIL Forum for Lan-
guage Fieldwork, volume 2.
Cheryl A. Black. 2004. Parser and writer for syn-
tax. Paper presented at the International Confer-
ence on Translation with Computer-Assisted Tech-
nology: Changes in Research, Teaching, Evaluation,
and Practice, University of Rome ?La Sapienza?,
April 2004.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proc. of the Workshop
on Grammar Engineering and Evaluation at COL-
ING 2002, pages 1?7.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proc. of the 7th
European workshop on natural language generation
(EWNLG99), pages 86?95.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI, Stanford.
Berthold Crysmann. 2009. Autosegmental representa-
tions in an HPSG for Hausa. In Proc. of the Work-
shop on Grammar Engineering Across Frameworks
2009.
E?ric Villemonte de la Clergerie. 2005. From meta-
grammars to factorized TAG/TIG parsers. In Proc.
of IWPT?05, pages 190?191.
Scott Drellishak and Emily M. Bender. 2005. A co-
ordination module for a crosslinguistic grammar re-
source. In Stefan Mu?ller, editor, Proc. of HPSG
2005, pages 108?128, Stanford. CSLI.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6:15 ? 28.
Antske Fokkens, Laurie Poulson, and Emily M. Ben-
der. 2009. Inflectional morphology in Turkish VP-
coordination. In Stefan Mu?ller, editor, Proc. of
HPSG 2009, pages 110?130, Stanford. CSLI.
Tracy Holloway King, Martin Forst, Jonas Kuhn, and
Miriam Butt. 2005. The feature space in parallel
grammar writing. Research on Language & Com-
putation, 3(2):139?163.
Stephen McConnel. 1995. PC-PATR Refer-
ence Manual. Summer Institute for Linguistics.
http://www.sil.org/pcpatr/manual/pcpatr.html.
Marjorie McShane and Sergei Nirenburg. 2003. Pa-
rameterizing and eliciting text elements across lan-
guages for use in natural language processing sys-
tems. Machine Translation, 18:129?165.
Christian Monson, Ariadna Font Llitjs, Vamshi Am-
bati, Lori Levin, Alon Lavie, Alison Alvarez,
Roberto Aranovich, Jaime Carbonell, Robert Fred-
erking, Erik Peterson, and Katharina Probst. 2008.
Linguistic structure and bilingual informants help
induce machine translation of lesser-resourced lan-
guages. In LREC?08.
Stefan Mu?ller. 2009. Towards an HPSG analysis of
Maltese. In Bernard Comrie, Ray Fabri, Beth Hume,
Manwel Mifsud, Thomas Stolz, and Martine Van-
hove, editors, Introducing Maltese linguistics. Pa-
pers from the 1st International Conference on Mal-
tese Linguistics, pages 83?112. Benjamins, Amster-
dam.
Stephan Oepen, Erik Velldal, Jan Tore Lnning, Paul
Meurer, Victoria Rosn, and Dan Flickinger. 2007.
Towards hybrid quality-oriented machine transla-
tion. On linguistics and probabilities in MT. In
11th International Conference on Theoretical and
Methodological Issues in Machine Translation.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago, IL.
6
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 74?83,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards Creating Precision Grammars from Interlinear Glossed Text:
Inferring Large-Scale Typological Properties
Emily M. Bender Michael Wayne Goodman Joshua Crowgey Fei Xia
Department of Linguistics
University of Washington
Seattle WA 98195-4340
{ebender,goodmami,jcrowgey,fxia}@uw.edu
Abstract
We propose to bring together two kinds of
linguistic resources?interlinear glossed
text (IGT) and a language-independent
precision grammar resource?to automat-
ically create precision grammars in the
context of language documentation. This
paper takes the first steps in that direction
by extracting major-constituent word or-
der and case system properties from IGT
for a diverse sample of languages.
1 Introduction
Hale et al (1992) predicted that more than 90%
of the world?s approximately 7,000 languages will
become extinct by the year 2100. This is a crisis
not only for the field of linguistics?on track to
lose the majority of its primary data?but also a
crisis for the social sciences more broadly as lan-
guages are a key piece of cultural heritage. The
field of linguistics has responded with increased
efforts to document endangered languages. Lan-
guage documentation not only captures key lin-
guistic data (both primary data and analytical
facts) but also supports language revitalization ef-
forts. It must include both primary data collec-
tion (as in Abney and Bird?s (2010) universal cor-
pus) and analytical work elucidating the linguistic
structures of each language. As such, the outputs
of documentary linguistics are dictionaries, de-
scriptive (prose) grammars as well as transcribed
and translated texts (Woodbury, 2003).
Traditionally, these outputs were printed ar-
tifacts, but the field of documentary linguistics
has increasingly realized the benefits of producing
digital artifacts as well (Nordhoff and Poggeman,
2012). Bender et al (2012a) argue that the docu-
mentary value of electronic descriptive grammars
can be significantly enhanced by pairing them with
implemented (machine-readable) precision gram-
mars and grammar-derived treebanks. However,
the creation of such precision grammars is time
consuming, and the cost of developing them must
be brought down if they are to be effectively inte-
grated into language documentation projects.
In this work, we are interested in leveraging
existing linguistic resources of two distinct types
in order to facilitate the development of precision
grammars for language documentation. The first
type of linguistic resource is collections of inter-
linear glossed text (IGT), a typical format for dis-
playing linguistic examples. A sample of IGT
from Shona is shown in (1).
(1) Ndakanga
ndi-aka-nga
SBJ.1SG-RP-AUX
ndakatenga
ndi-aka-teng-a
SBJ.1SG-RP-buy-FV
muchero
mu-chero
CL3-fruit
?I had bought fruit.? [sna] (Toews, 2009:34)
The annotations in IGT result from deep linguistic
analysis and represent much effort on the part of
field linguists. These rich annotations include the
segmentation of the source line into morphemes,
the glossing of those individual morphemes, and
the translation into a language of broader commu-
nication. The IGT format was developed to com-
pactly display this information to other linguists.
Here, we propose to repurpose such data in the au-
tomatic development of further resources.
The second resource we will be working with
is the LinGO Grammar Matrix (Bender et al,
2002; 2010), an open source repository of imple-
mented linguistic analyses. The Grammar Matrix
pairs a core grammar, shared across all grammars
it creates, with a series of libraries of analyses
of cross-linguistically variable phenomena. Users
access the system through a web-based question-
naire which elicits linguistic descriptions of lan-
guages and then outputs working HPSG (Pol-
lard and Sag, 1994) grammar fragments compat-
ible with DELPH-IN (www.delph-in.net) tools
based on those descriptions. For present purposes,
this system can be viewed as a function which
maps simple descriptions of languages to preci-
74
sion grammar fragments. These fragments are rel-
atively modest, yet they relate linguistic strings to
semantic representations (and vice versa) and are
ready to be built out to broad coverage.
Thus we ask whether the information encoded
by documentary linguists in IGT can be lever-
aged to answer the Grammar Matrix?s question-
naire and create a precision grammar fragment
automatically. The information required by the
Grammar Matrix questionnaire concerns five dif-
ferent aspects of linguistic systems: (i) constituent
ordering (including the presence/absence of con-
stituent types), (ii) morphosyntactic systems, (iii)
morphosyntactic features, (iv) lexical types and
their instances and (v) morphological rules. In this
initial work, we target examples of types (i) and
(ii): the major constituent word order and the gen-
eral type of case system in a language. The Gram-
mar Matrix and other related work are described
in further in ?2. In ?3 we present our test data and
experimental set-up. ??4?5 describe our method-
ology and results for the two tasks, respectively,
with further discussion and outlook in ??6?7.
2 Background and Related Work
2.1 The Grammar Matrix
The Grammar Matrix produces precision gram-
mars on the basis of description of languages
that include both high-level typological informa-
tion and more specific detail. Among the for-
mer are aspects (i)?(iii) listed in ?1. The third
of these (morphosyntactic features) concerns the
type and range of grammaticized information that
a language marks in its morphology and/or syn-
tax. This includes person/number systems (e.g.,
is there an inclusive/exclusive distinction in non-
singular first person forms?), the range of aspec-
tual distinctions a language marks, and the range
of cases (if any) in a language, inter alia. The an-
swers to these questions in turn cause the system
to provide relevant features that the user can ref-
erence in providing the more specific information
elicited by the questionnaire ((iv) and (v) above),
viz., the definition of both lexical types (e.g., first
person dual exclusive pronouns) and morphologi-
cal rules (e.g., nominative case marking on nouns).
The information input by the user to the Gram-
mar Matrix questionnaire is stored in a file called
a ?choices file?. The choices file is used both in
the dynamic definition of the html pages (so that
the features available for lexical definitions de-
pend on earlier choices) and as the input to the cus-
tomization script that actually produces the gram-
mar fragments to spec. The customization sys-
tem distinguishes between choices files which are
complete and consistent (and can be used to cre-
ate working grammar fragments) and those which
do not yet have answers to required questions or
give answers which are inconsistent according to
the underlying grammatical theory. The ultimate
goal of the present project is to be able to automat-
ically create complete and consistent choices files
on the basis of IGT, and in fact to create complete
and consistent choices files which take maximal
advantage of the analyses stored in the Grammar
Matrix customization system, answering not only
the minimal set of questions required but in fact all
which are relevant and possible to answer based on
the information in the IGT.
Creating such complete and consistent choices
files is a long-term project, with different ap-
proches required for the different types of ques-
tions outlined in ?1. Bender et al (2012b) take
some initial steps towards answering the questions
which define lexical rules. We envision answering
the questions regarding morphosyntactic features
through an analysis of the grams that appear on the
gloss line, with reference to the GOLD ontology
(Farrar and Langendoen, 2003). The implementa-
tion of such systems in such a way that they are
robust to potentially noisy data will undoubtedly
be non-trivial. The contribution of this paper is
the development of systems to handle one example
each of the questions of types (i) and (ii), namely
detecting major constituent word order and the un-
derlying case system. For the first, we build di-
rectly on the work of Lewis and Xia (2008) (see
?2.2). Our experiment can be viewed as an at-
tempt to reproduce their results in the context of
the specific view of word order possibilities devel-
oped in the Grammar Matrix. The second question
(that of case systems) is in some ways more sub-
tle, requiring not only analysis of IGT instances in
isolation and aggregation of the results, but also
identification of particular kinds of IGT instances
and comparison across them.
2.2 RiPLes
The RiPLes project has two intertwined goals.
The first goal is to create a framework that allows
the rapid development of resources for resource-
poor languages (RPLs), which is accomplished by
75
Figure 1: Welsh IGT with alignment and projected
syntactic structure
bootstrapping NLP tools with initial seeds created
by projecting syntactic information from resource-
rich languages to RPLs through IGT. Projecting
syntactic structures has two steps. First, the words
in the language line and the translation line are
aligned via the gloss line. Second, the transla-
tion line is parsed by a parser for the resource-rich
language and the parse tree is then projected to
the language line using word alignment and some
heuristics as illustrated in Figure 1 (adapted from
Xia and Lewis (2009)).1 Previous work has ap-
plied these projected trees to enhance the perfor-
mance of statistical parsers (Georgi et al, 2012).
Though the projected trees are noisy, they contain
enough information for those tasks.
The second goal of RiPLes is to use the au-
tomatically created resources to perform cross-
lingual study on a large number of languages
to discover linguistic knowledge. For instance,
Lewis and Xia (2008) showed that IGT data en-
riched with the projected syntactic structure could
be used to determine the word order property of a
language with a high accuracy (see ?4). Naseem
et al (2012) use this type of information (in their
case, drawn from the WALS database (Haspel-
math et al, 2008)) to improve multilingual depen-
dency parsing. Here, we build on this aspect of
RiPLes and begin to extend it towards the wider
range of linguistic phenomena and more detailed
classification within phenomena required by the
Grammar Matrix questionnaire.
2.3 Other Related Work
Our work is also situated with respect to attempts
to automatically characterize typological proper-
1The details of the algorithm and experimental results
were reported in (Xia and Lewis, 2007).
ties of languages, including Daume? III and Camp-
bell?s (2007) Bayesian approach to discovering ty-
pological implications and Georgi et al?s (2010)
work on predicting (unknown) typological proper-
ties by clustering languages based on known prop-
erties. Both projects use the typological database
WALS (Haspelmath et al, 2008), which has in-
formation about 192 different typological proper-
ties and about 2,678 different languages (though
the matrix is very sparse). This approach is com-
plementary to ours, and it remains an interesting
question whether our results could be improved
by bringing in information about other typological
properties of the language (either extracted from
the IGT or looked up in a typological database).
Another strand of related work concerns the col-
lection and curation of IGT, including the ODIN
project (Lewis, 2006; Xia and Lewis, 2008),
which harvests IGT from linguistics publications
available over the web and TypeCraft (Beermann
and Mihaylov, 2009), which facilitates the collab-
orative development of IGT annotations. TerraL-
ing/SSWL2 (Syntactic Structures of the World?s
Languages) has begun a database which combines
both typological properties and IGT illustrating
those properties, contributed by linguists.
Finally, Beerman and Hellan (2011) represents
another approach to inducing grammars from IGT,
by bringing the hand-built linguistic knowledge
sources closer together: On the one hand, their
cross-linguistic grammar resource (TypeGram) in-
cludes a mechanism for mapping from strings
specifying verb valence and valence-altering lex-
ical rules to sets of grammar constraints. On
the other hand, their IGT authoring environment
(TypeCraft) provides support for annotating exam-
ples with those strings. The approach advocated
here attempts to bridge the gap between IGT and
grammar specification algorithmically, instead.
3 Development and Test Data
Our long-term goal is to produce working gram-
mar fragments from IGT produced in documen-
tary linguistics projects. However, in order to
evaluate the performance of approaches to answer-
ing the high-level questions in the Grammar Ma-
trix questionnaire, we need both IGT and gold-
standard answers for a reasonably-sized sample of
languages. We have constructed development and
test data for this purpose on the basis of work done
2
http://sswl.railsplayground.net/, accessed 4/25/13
76
Sets of languages DEV1 (n=10) DEV2 (n=10) TEST (n=11)
Range of testsuite sizes 16?359 11?229 48?216
Median testsuite size 91 87 76
Language families Indo-European (4), Niger- Indo-European (3), Indo-European (2), Afro-Asiatic,
Congo (2), Afro-Asiatic, Dravidian (2), Algic, Austro-Asiatic, Austronesian,
Japanese, Nadahup, Creole, Niger-Congo, Arauan, Carib, Karvelian,
Sino-Tibetan Quechuan, Salishan N. Caucasian, Tai-Kadai, Isolate
Table 1: Language families and testsuites sizes (in number of grammatical examples)
by students in a class that uses the Grammar Ma-
trix (Bender, 2007). In this class, students work
with descriptive resources for languages they are
typically not familiar with to create testsuites (cu-
rated collections of grammatical and ungrammat-
ical examples) and Grammar Matrix choices files.
Later on in the class, the students extend the gram-
mar fragments output by the customization system
to handle a broader fragment of the language. Ac-
cordingly, the testsuites cover phenomena which
go beyond the customization system.
Testsuites for grammars, especially in their
early stages of development, require examples that
are simple (isolating the phenomena illustrated by
the examples to the extent possible), built out of
a small vocabulary, and include both grammati-
cal and ungrammatical examples (Lehmann et al,
1996). The examples included in descriptive re-
sources often don?t fit these requirements exactly.
As a result, the data we are working with include
examples invented by the students on the basis of
the descriptive statements in their resources.3
In total, we have testsuites and associated
choices files for 31 languages, spanning 17 lan-
guage families (plus one creole and one language
isolate). The most well-represented family is
Indo-European, with nine languages. We used 20
languages, in two dev sets, for algorithm develop-
ment (including manual error analysis), and saved
11 languages as a held-out test set to verify the
generalizability of our approach. Table 1 lists the
language families and the range of testsuite sizes
for each of these sets of languages.
4 Inferring Word Order
Lewis and Xia (2008) show how IGT from ODIN
(Lewis, 2006) can be used to determine, with high
accuracy, the word order properties of a language.
They identify 14 typological parameters related to
word order for which WALS (Haspelmath et al,
2008) or other typological resources provide in-
3Such examples are flagged in the testsuites? meta-data.
formation. The parameter most closely relevant to
the present work is Order of Words in a Sentence
(Dryer, 2011). For this parameter, Lewis and Xia
tested their method in 97 languages and found that
their system had 99% accuracy provided the IGT
collections had at least 40 instances per language.
The Grammar Matrix?s word order questions
differ somewhat from the typological classifi-
cation that Lewis and Xia (2008) were using.
Answering the Grammar Matrix questionnaire
amounts to more than making a descriptive state-
ment about a language. The Grammar Matrix cus-
tomization system translates collections of such
descriptive statements into working grammar frag-
ments. In the case of word order, this most di-
rectly effects the number and nature of phrase
structure rules included in the output grammar, but
can also interact with other aspects of the gram-
mar (e.g., the treatment of argument optionality).
More broadly, specifying the word order system
of a grammar determines both grammaticality (ac-
cepting some strings, ruling out others) and, for
the fixed word orders at least, aspects of the map-
ping of syntactic to semantic arguments.
Lewis and Xia (2008), like Dryer (2011), gave
the six fixed orders of S, O and V plus ?no dom-
inant order?. In contrast, the Grammar Matrix
distinguishes Free (pragmatically constrained), V-
final, V-initial, and V2 orders, in addition to the
six fixed orders. It is important to note that the
relationship between the word order type of a lan-
guage and the actual orders attested in sentences
can be somewhat indirect. For a fixed word order
language, we would expect the order declared as
its type to be the most common in running text,
but not the only type available. English, for exam-
ple, is an SVO language, but several constructions
allow for other orders, including subject-auxiliary
inversion, so-called topicalization, and others:
(2) Did Kim leave?
(3) The book, Kim forgot.
In a language with more word order flexibility in
general, there may still be a preferred word order
77
which is the most common due to pragmatic or
other constraints. Users of the Grammar Matrix
are advised to choose one of the fixed word orders
if the deviations from that order can generally be
accounted for by specific syntactic constructions,
and a freer word order otherwise.
The relationship between the correct word or-
der choice for the Grammar Matrix customization
system and the distribution of actual token word
orders in our development and test data is affected
by another factor, related to Lewis and Xia?s ?IGT
bias? which we dub ?testsuite bias?. The collec-
tions of IGT we are using were constructed as test-
suites for grammar engineering projects and thus
comprise examples selected or constructed to il-
lustrate specific grammatical properties in a test-
ing regime where one example is enough to repre-
sent each sentence type of interest. Therefore, they
do not represent a natural distribution of word or-
der types. For example, the testsuite authors may
show the full range of possible word orders in the
word order section of the testsuite and then default
to one particular choice for other portions (those
illustrating e.g., case systems or negation).
4.1 Methodology
Our first stpes mirror the RiPLes approach, pars-
ing parse the English translation of each sentence
and projecting the parsed structure onto the source
language line. Functional tags, such as SBJ and
OBJ, are added to the NP nodes on the English
side based on our knowledge of English word or-
der and then carried over to the source language
side during the projection of parse trees. The trees
are then searched for any of ten patterns: SOV,
SVO, OSV, OVS, VSO, VOS, SV, VS, OV, and
VO. The six ternary patterns match when both ver-
bal arguments are present in the same clause. The
four binary patterns are for intransitive sentences
or those with dropped arguments. These ten pat-
terns make up the observed word orders.
Given our relatively limited data set (each lan-
guage is one data point), we present an initial
approach to determining underlying word order
based on heuristics informed by general linguis-
tic knowledge. We compare the distribution of ob-
served word orders to distributions we expect to
see for canonical examples of underlying word or-
ders. We accomplish this by first deconstructing
the ternary observed-word-orders into binary pat-
terns (the four above plus SO and OS). This gives
us three axes: one for the tendency to exhibit VS
or SV order, another for VO or OV order, and an-
other for OS or SO order. By counting the ob-
served word orders in the IGT examples, we can
place the language in this three-dimensional space.
Figure 4.1 depicts this space with the positions of
canonical word orders.4 The canonical word order
positions are those found under homogeneous ob-
servations. For example, the canonical position for
SOV order is when 100% of the sentences exhibit
SO, OV, and SV orders; and the canonical position
for Free word order is when each observed order
occurs with equal frequency to its opposite order
(on the same axis; e.g. VO and OV). We select the
underlying word order by finding which canoni-
cal word order position has the shortest Euclidean
distance to the observed word order position.
When a language is selected as Free word or-
der, we employ a secondary heuristic to decide if
it is actually V2 word order. The V2 order cannot
be easily recognized only with the binary word or-
ders, so it is not given a unique point in the three-
dimensional space. Rather, we try to recognize it
by comparing the ternary orders. A Free-order lan-
guage is reclassified as V2 if SVO and OVS occur
more frequently than SOV and OSV.5
OVS
SOV
V-finalVS
OV
OSV
SVV-initial
VOS
OS
SO
SVO
VSO
VO
Free/V2
Figure 2: Three axes of basic word order and the
positions of canonical word orders.
4.2 Results
Table 2 shows the results we obtained for our dev
and test sets. For comparison, we use a most-
4Of the eight vertices of this cube, six represent canoni-
cal word orders the other two impossible combinations: The
vertex for (SV, VO, OS) (e.g.) has S both before and after O.
5The VOS and VSO patterns are excluded from this com-
parison, since they can go either way?there may be un-
aligned constituents (i.e. not a S, O, or V) before the verb
which are ignored by our system.
78
frequent-type baseline, selecting SOV for all lan-
guages, based on Dryer?s (2011) survey. We get
high accuracy for DEV1, low accuracy for DEV2,
and moderate accuracy for TEST, but all are sig-
nificantly higher than the baseline.
Dataset Inferred WO Baseline
DEV1 0.900 0.200
DEV2 0.500 0.100
TEST 0.727 0.091
Table 2: Accuracy of word-order inference
Hand analysis of the errors in the dev sets
show that some languages fall victim to the test-
suite bias, such as Russian, Quechua, and Tamil.
All of these languages have Free word order, but
our system infers SVO for Russian and SOV for
Quechua and Tamil, because the authors of the
testsuites used one order significantly more than
the others. Similarly, the Free word order lan-
guage Nishnaabemwin is inferred as V2 because
there are more SVO and OVS patterns given than
others. We also see errors due to misalignment
from RiPLes? syntactic projection. The VSO lan-
guage Welsh is inferred as SVO because the near-
ubiquitous sentence-initial auxiliary doesn?t align
to the main verb of the English translation.
5 Inferring Case Systems
Case refers to linguistic phenomena in which the
form of a noun phrase (NP) varies depending on
the function of the NP in a sentence (Blake, 2001).
The Grammar Matrix?s case library (Drellishak,
2009) focuses on case marking of core arguments
of verbs. Specifying a grammar for case involves
both choosing the high-level case system to be
modeled as well as associating verb types with
case frames and defining the lexical items or lex-
ical rules which mark the case on the NPs. Here,
we focus on the high-level case system question
as it is logically prior, and in some ways more in-
teresting than the lexical details: Answering this
question requires identifying case frames of verbs
in particular examples and then comparing across
those examples, as described below.
The high-level case system of a language con-
cerns the alignment of case marking between tran-
sitive and intransitive clauses. The three ele-
ments in question are the subjects of intransi-
tives (dubbed S), the subjects (or agent-like ar-
guments) of transitives (dubbed A) and the ob-
jects (or patient-like arguments) of intransitives
Case Case grams present
system NOM ? ACC ERG ? ABS
none
nom-acc X
erg-abs X
split-erg X X
(conditioned on V)
Table 3: GRAM case system assignment rules
(O). Among languages which make use of case,
the most common alignment type is a nominative-
accusative system (Comrie 2011a,b). In this
type, S takes the same kind of marking as A.6
The Grammar Matrix case library provides nine
options, including none, nominative-accusative,
ergative-absolutive (S marked like O), tripartite (S,
A and O all distinct) and several more intricate
types. For example, in a language with one type
of split case system the alignment is nominative-
accusative in non-past tense clauses, but ergative-
absolutive in past tense ones.
As with major constituent word order, the con-
straints implementing a case system in a grammar
serve to model both grammaticality and the map-
ping between syntactic and semantic arguments.
Here too, the distribution of tokens may be some-
thing other than a pure expression of the case
alignment type. Sources of noise in the distri-
bution include: argument optionality (e.g., tran-
sitives with one or more covert arguments), ar-
gument frames other than simple intransitives or
transitives, and quirky case (verbs that use a non-
standard case frame for their arguments, such as
the German verb helfen which selects a dative ar-
gument, though the language?s general system is
nominative-accusative (Drellishak, 2009)).
5.1 Methodology
We explore two possible methodologies for infer-
ring case systems, one relatively na??ve and one
more elaborate, and compare them to a most-
frequent-type baseline. Method 1, called GRAM,
considers only the gloss line of the IGT and as-
sumes that it complies with the Leipzig Glossing
Rules (Bickel et al, 2008). These rules not only
prescribe formatting aspects of IGT but also pro-
vide a set of licensed ?grams?, or tags for grammat-
ical properties that appear in the gloss line. GRAM
scans for the grams associated with case, and as-
signs case systems according to Table 3.
This methodology is simple to implement and
6English?s residual case system is of this type.
79
expected to work well given Leipzig-compliant
IGT. However, since it does not model the func-
tion of case, it is dependent on the IGT authors?
choice of gram symbols, and may be confused by
either alternative case names (e.g., SBJ and OBJ for
nominative and accusative or LOC for ergative in
languages where it is homophonous with the loca-
tive case) or by other grams which collide with the
case name-space (such as NOM for nominalizer).
It also only handles four of the nine case systems
(albeit the most frequent ones).
Method 2, called SAO, is more theoretically
motivated, builds on the RiPLes approach used
in inferring word order, and is designed to be
robust to idiosyncratic glossing conventions. In
this methodology, we first identify the S, A and
O arguments by projecting the information from
the parse of the English translation (including
the function tags) to the source sentence (and its
glosses). We discard all items which do not appear
to be simple transitive or intransitive clauses with
all arguments overt, and then collect all grams for
each argument type (from all words within in the
NP, including head nouns as well as determiners
and adjectives). While there are many grammati-
cal features that can be marked on NPs (such as
number, definiteness, honorifics, etc.), the only
ones that should correlate strongly with grammat-
ical function are case-marking grams. Further-
more, in any given NP, while case may be multi-
ply marked, we only expect one type of case gram
to appear. We thus assume that the most frequent
gram for each argument type is a case marker (if
there are any) and assign the case system accord-
ing to the following rules, where Sg, Og and Ag de-
note the most frequent grams associated with these
argument positions, respectively:
? Nominative-accusative: Sg=Ag, Sg 6=Og
? Ergative-absolutive: Sg=Og, Sg 6=Ag
? No case: Sg=Ag=Og, or Sg 6=Ag 6=Og and Sg,
Ag, Og also present on each of the other ar-
gument types
? Tripartite: Sg 6=Ag 6=Og, and Sg, Ag, Og (vir-
tually) absent from the other argument types
? Split-S: Sg 6=Ag 6=Og, and Ag and Og are both
present in the list for the S argument type
Here, we?re using Split-S to stand in for both
Split-S and Fluid-S. These are both systems where
some S arguments are marked like A, and some
like O. In Split-S, which is taken depends on the
verb. In Fluid-S, it depends on the interpretation of
the verb. These could be distinguished by looking
for intransitive verbs that appear more than once in
the data and checking whether their S arguments
all have consistently A or O marking.
This system is agnostic as to the spelling of the
case grams. By relying on more analysis of the
IGT than GRAM, it also introduces new kinds of
brittleness. Recognizing the difference between
grams being present and (virtually) absent makes
the system susceptible to noise.
5.2 Results
Table 4 shows the results for the inference of case-
marking systems. Currently GRAM performs best,
but both methods generally perform better than
the baseline. The better performance of GRAM
is expected, given the small size and generally
Leipzig-compliant glossing of our data sets. In
future work, we plan to incorporate data from
ODIN, which is likely less consistently annotated
but more voluminous, and we expect SAO to be
more robust than GRAM to this kind of data.
Dataset GRAM SAO Baseline
DEV1 0.900 0.700 0.400
DEV2 0.900 0.500 0.500
TEST 0.545 0.545 0.455
Table 4: Accuracy of case-marking inference
We find that GRAM is sometimes able to do well
when RiPLes gives alignment errors. For exam-
ple, Old Japanese is a NOM-ACC language, but the
case-marking grams (associated to postpositions)
are not aligned to the NP arguments, so SAO is not
able to judge their distribution. On the other hand,
SAO prevails when non-standard grams are used,
such as the NOM-ACC language Hupdeh, which is
annotated with SUBJ and OBJ grams. This comple-
mentarity suggests scope for system combination,
which we leave to future work.
6 Discussion and Future Work
Our initial results are promising, but also show
remaining room for improvement. Error analysis
suggests two main directions to pursue:
Overcoming testsuite bias In both the word or-
der and case system tasks, we see the effect of
testsuite bias on our system results. The testsuites
for freer word order languages can be artificially
dominated by a particular word order that the test-
suite author found convenient. Further, the re-
stricted vocabulary used in testsuites, combined
80
with a general preference for animates as subjects,
leads to stems and certain grams potentially being
misidentified as case markers.
We believe that these aspects of testsuite bias
are not typical of our true target input data, viz.,
the larger collections of IGT created by field
projects. On the other hand, there may be other as-
pects of testsuites which are simplifying the prob-
lem and to which our current methods are over-
fitted. To address these issues, we intend to look
to larger datasets in future work, both IGT collec-
tions from field projects and IGT from ODIN. For
the field projects, we will need to construct choices
files. For ODIN, we can search for data from the
languages we already have choices files for.
As we move from testsuites to test corpora
(e.g., narratives collected in documentary linguis-
tics projects), we expect to find different distribu-
tions of word order types. Our current methodol-
ogy for extracting word order is based on idealized
locations in our word order space for each strict
word order type. Working with naturally occurring
corpora it should be possible to gain a more em-
pirically based understanding of the relationship
between underlying word order and sentence type
distributions. It will be particularly interesting to
see how stable these relationships are across lan-
guages with the same underlying word order type
but from different language families and/or with
differences in other typological characteristics.
Better handling of unaligned words The other
main source of error is words that remain un-
aligned in the projected syntactic structure and
thus only loosely incorporated into the syntax
trees. This includes items like case marking adpo-
sitions in Japanese, which are unaligned because
there is no corresponding word in English, and
auxiliaries in Welsh, which are unaligned when
the English translation doesn?t happen to use an
auxiliary. In the former case, our SAO method
for case system extraction doesn?t include the case
grams in the set of grams for each NP. In the latter,
the word order inference system is unable to pick
up on the VSO order represented as Aux+S+[VP].
Simply fixing the attachment of the auxiliaries will
not be enough in this case, as the word order infer-
ence algorithm will need to be extended to han-
dle auxiliaries, but fixing the alignment is the first
step. Alignment problems are also the main reason
our initial attempts to extract information about
the order of determiners and nouns haven?t yet
been able to beat the most-frequent-type baseline.
Better handling of these unaligned words is
a non-trivial task, and will require bringing in
sources of knowledge other than the structure of
the English translation. The information we have
to leverage in this regard comes mainly from the
gloss line and from general linguistic/typological
knowledge which can be added to the algorithm.
That is, there are types of grams which are canon-
ically associated with verbal projections and types
of grams canonically associated with nominal pro-
jections. When these grams occur on unaligned
elements, we can hypothesize that the elements
are auxiliaries and case-marking adpositions re-
spectively. Further typological considerations will
motivate heuristics for modifying tree structures
based on these classifications.
Other directions for future work include extend-
ing this methodology to other aspects of grammat-
ical description, including additional high-level
systems (e.g., argument optionality), discovering
the range of morphosyntactic features active in a
language, and describing and populating lexical
types (e.g., common nouns with a particular gen-
der). Once we are able to answer enough of the
questionnaire that the customization system is able
to output a grammar, interesting options for de-
tailed evaluation will become available. In par-
ticular, we will be able to parse the IGT (includ-
ing held-out examples) with the resulting gram-
mar, and then compare the resulting semantic rep-
resentations to those produced by parsing the En-
glish translations with tools that produce compara-
ble semantic representations for English (using the
English Resource Grammar (Flickinger, 2000)).
7 Conclusions and Future Work
In this paper we have presented an approach to
combining two types of linguistic resources?IGT,
as produced by documentary linguists and a cross-
linguistic grammar resource supporting preci-
sion parsing and generation?to create language-
specific resources which can help enrich language
documentation and support language revitaliza-
tion efforts. In addition to presenting the broad vi-
sion of the project, we have reported initial results
in two case studies as a proof-of-concept. Though
there is still a ways to go, we find these initial re-
sults a promising indication of the approach?s abil-
ity to assist in the preservation of the key type of
cultural heritage that is linguistic systems.
81
Acknowledgments
We are grateful to the students in Ling 567 at the
University of Washington who created the test-
suites and choices files used as development and
test data in this work and to the three anonymous
reviewers for helpful comments and discussion.
This material is based upon work supported
by the National Science Foundation under Grant
No. BCS-1160274. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the National Science
Foundation.
References
Steven Abney and Steven Bird. 2010. The human
language project: building a universal corpus of the
world?s languages. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 88?97, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Dorothee Beerman and Lars Hellan. 2011. Induc-
ing grammar from IGT. In Proceedings of the 5th
Language and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics (LTC 2011).
Dorothee Beermann and Pavel Mihaylov. 2009. Type-
Craft: Linguistic data and knowledge sharing, open
access and linguistic methodology. Paper presented
at the Workshop on Small Tools in Cross-linguistic
Research, University of Utrecht. The Netherlands.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8?14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Com-
putation, pages 1?50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179?206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, David Wax, and Michael Wayne
Goodman. 2012b. From IGT to precision grammar:
French verbal morphology. In LSA Annual Meeting
Extended Abstracts 2012.
Emily M. Bender. 2007. Combining research and
pedagogy in the development of a crosslinguistic
grammar resource. In Tracy Holloway King and
Emily M. Bender, editors, Proceedings of the GEAF
2007 Workshop, Stanford, CA. CSLI Publications.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, second edition.
Bernard Comrie. 2011a. Alignment of case marking of
full noun phrases. In Matthew S. Dryer and Martin
Haspelmath, editors, The World Atlas of Language
Structures Online. Max Planck Digital Library, Mu-
nich.
Bernard Comrie. 2011b. Alignment of case marking of
pronouns. In Matthew S. Dryer and Martin Haspel-
math, editors, The World Atlas of Language Struc-
tures Online. Max Planck Digital Library, Munich.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 65?
72, Prague, Czech Republic, June. Association for
Computational Linguistics.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Matthew S. Dryer. 2011. Order of subject, object and
verb. In Matthew S. Dryer and Martin Haspelmath,
editors, The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich.
Scott Farrar and Terry Langendoen. 2003. A linguistic
ontology for the semantic web. Glot International,
7:97?100.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6 (1) (Special Issue on Efficient Pro-
cessing with HPSG):15 ? 28.
Ryan Georgi, Fei Xia, and William Lewis. 2010.
Comparing language similarity across genetic and
typologically-based groupings. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 385?393,
Beijing, China, August. Coling 2010 Organizing
Committee.
82
Ryan Georgi, Fei Xia, and William Lewis. 2012. Im-
proving dependency parsing with interlinear glossed
text and syntactic projection. In Proceedings of
COLING 2012: Posters, pages 371?380, Mumbai,
India, December.
Ken Hale, Michael Krauss, Lucille J. Wata-
homigie, Akira Y. Yamamoto, Colette Craig,
LaVerne Masayesva Jeanne, and Nora C. England.
1992. Endangered languages. Language, 68(1):pp.
1?42.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2008. The World Atlas
of Language Structures Online. Max Planck Digital
Library, Munich. http://wals.info.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-
Prost, Klaus Netter, Veronika Lux, Judith Klein,
Kirsten Falkedal, Frederik Fouvry, Dominique Esti-
val, Eva Dauphin, Herve` Compagnion, Judith Baur,
Lorna Balkan, and Doug Arnold. 1996. TSNLP:
Test suites for natural language processing. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 2, COLING ?96, pages 711?
716, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685?690, Hyderabad, India.
William D. Lewis. 2006. ODIN: A model for adapting
and enriching legacy infrastructure. In Proceedings
of the e-Humanities Workshop, Held in cooperation
with e-Science, Amsterdam.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 629?637, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Sebastian Nordhoff and Karl-Ludwig G. Poggeman,
editors. 2012. Electronic Grammaticography. Uni-
versity of Hawaii Press, Honolulu.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Carmela Toews. 2009. The expression of tense and as-
pect in Shona. Selected Proceedings of the 39th An-
nual Converence on African Linguistics, pages 32?
41.
Tony Woodbury. 2003. Defining documentary lin-
guistics. Language documentation and description,
1(1):35.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Fei Xia and William D. Lewis. 2008. Repurposing
theoretical linguistic data for tool development and
search. In Proceedings of the Third International
Joint Conference on Natural Language Processing,
pages 529?536, Hyderabad, India.
Fei Xia and William Lewis. 2009. Applying NLP
technologies to the collection and enrichment of lan-
guage data on the web to aid linguistic research. In
Proceedings of the EACL 2009 Workshop on Lan-
guage Technology and Resources for Cultural Her-
itage, Social Sciences, Humanities, and Education
(LaTeCH ? SHELT&R 2009), pages 51?59, Athens,
Greece, March. Association for Computational Lin-
guistics.
83
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 43?53,
Baltimore, Maryland, USA, 26 June 2014.
c
?2014 Association for Computational Linguistics
Learning Grammar Specifications from IGT: A Case Study of Chintang
Emily M. Bender Joshua Crowgey Michael Wayne Goodman Fei Xia
Department of Linguistics
University of Washington
Seattle, WA 98195-4340 USA
{ebender,jcrowgey,goodmami,fxia}@uw.edu
Abstract
We present a case study of the methodol-
ogy of using information extracted from
interlinear glossed text (IGT) to create of
actual working HPSG grammar fragments
using the Grammar Matrix focusing on
one language: Chintang. Though the re-
sults are barely measurable in terms of
coverage over running text, they nonethe-
less provide a proof of concept. Our expe-
rience report reflects on the ways in which
this task is non-trivial and on mismatches
between the assumptions of the methodol-
ogy and the realities of IGT as produced in
a large-scale field project.
1 Introduction
We explore the possibility of learning precision
grammar fragments from existing products of doc-
umentary linguistic work. A precision grammar is
a grammar which encodes a sharp notion of gram-
maticality and furthermore relates strings to elabo-
rate semantic representations. Such objects are of
interest in the context of documentary linguistics
because: (1) they are valuable tools in the explo-
ration of linguistic hypotheses (especially regard-
ing the interaction of various phenomena); (2) they
facilitate the search for examples in corpora which
are not yet understood; and (3) they can support
the development of treebanks (see Bender et al.,
2012a). However, they are expensive to build.
The present work is carried out in the context of
the AGGREGATION project,
1
which is exploring
whether such grammars can be learned on the ba-
sis of data already collected and enriched through
the work of descriptive linguists, specifically, col-
lections of IGT (interlinear glossed text).
The grammars themselves are not likely targets
for machine learning, especially in the absence of
1
http://depts.washington.edu/uwcl/aggregation/
treebanks, which are not generally available for
languages that are the focus of descriptive and
documentary linguistics. Instead, we take advan-
tage of the LinGO Grammar Matrix customiza-
tion system (Bender et al., 2002; Bender et al.,
2010) which maps from collections of statements
of linguistic properties (encoded in choices files)
to HPSG (Pollard and Sag, 1994) grammar frag-
ments which in turn can be used to parse strings
into semantic representations in the format of Min-
imal Recursion Semantics (MRS; Copestake et al.,
2005) and conversely, to generate strings from
MRS representations. The choices files are a
much simpler representation than the grammars
derived from them and therefore a more approach-
able learning target. Furthermore, using the Gram-
mar Matrix customization system to produce the
grammars results in much less noise in the auto-
matically derived grammar code than would arise
in a system learning grammars directly.
Here, we focus on a case study of Chintang, a
Kiranti language of Nepal, described by the Chin-
tang Language Research Project (CLRP) (Bickel
et al., 2009). Where Lewis and Xia (2008) and
Bender et al. (2013) apply similar methodologies
to extract large scale properties for many lan-
guages, we focus on a case study of a single lan-
guage, looking at both the large scale properties
and the lexical details. This is important for two
reasons: First, it gives us a chance to look in-
depth at the possible sources of difficulty in ex-
tracting the large scale properties. Second, while
large-scale properties are undoubtedly important,
the bulk of the information specified in a preci-
sion grammar is far more fine-grained. In this
case study we apply the methodology of Bender et
al. (2013) to extract general word order and case
properties and examine the sources of error affect-
ing those results. We also explore extensions of
those methodologies and that of Wax (2014) to ex-
tract lexical entries and specifications for morpho-
43
logical rules. Together with a few default spec-
ifications, this information is enough to allow us
to define grammars through the Grammar Matrix
customization system and thus evaluate the results
in terms of parsing coverage, accuracy and am-
biguity over running text. Chintang is particu-
larly well-suited for this case study because it is
an actual endangered language subject to active
descriptive research, making the evaluation of our
techniques realistic. Furthermore, the descriptive
research on Chintang is fairly advanced, having
produced both large corpora of high-quality IGT
and sophisticated linguistic descriptions, making
the evaluation and error analysis possible.
2 Related Work
This work can be understood as a task related to
both grammar induction and grammar extraction,
though it is distinct from both. It also connects
with and extends previous work using interlinear
glossed text to extract grammatical properties.
Grammar induction (Clark, 2001; Klein and
Manning, 2002; Klein and Manning, 2004;
Haghighi and Klein, 2006; Smith and Eisner,
2006; Snyder et al., 2009, inter alios) involves the
learning of grammars from unlabeled sentences.
Here, unlabeled means that the sentences are of-
ten POS tagged, but no syntactic structures for
the sentences are available. Most of those stud-
ies choose probabilistic context-free grammars
(PCFGs) or dependency grammars as the gram-
mar framework, and estimate the probability of
the context-free rules or dependency arcs from the
data. These studies improve parsing performance
significantly over some baselines such as the EM
algorithm, but the induced grammars are very dif-
ferent from precision grammars with respect to
content, quality, and grammar framework.
Grammar extraction, on the other hand, learns
grammars (sets of rules) from treebanks. Here the
idea is to use heuristics to convert the syntactic
structures in a treebank into derivation trees con-
forming to a particular framework, and then ex-
tract grammars from those trees. This has been
done in a wide range of grammar frameworks, in-
cluding PCFG (e.g. Krotov et al., 1998), LTAG
(e.g. Xia, 1999; Chen and Vijay-Shanker, 2000),
LFG (e.g. Cahill et al., 2004), CCG (e.g. Hock-
enmaier and Steedman, 2002, 2007), and HPSG
(e.g. Miyao et al., 2004; Cramer and Zhang, 2009).
However, this approach is not applicable to work
word-order=v-final
has-dets=yes
noun-det-order=det-noun
...
case-marking=erg-abs
erg-abs-erg-case-name=erg
erg-abs-abs-case-name=abs
...
verb4_valence=erg-abs
verb4_stem1_orth=sams-i-ne
verb4_stem1_pred=_sams-i-ne_v_re
...
verb-pc3_inputs=verb-pc2
verb-pc3_lrt1_name=2nd-person-subj
verb-pc3_lrt1_feat1_name=pernum
verb-pc3_lrt1_feat1_value=2nd
verb-pc3_lrt1_feat1_head=subj
verb-pc3_lrt1_lri1_inflecting=yes
verb-pc3_lrt1_lri1_orth=a-
Figure 1: Excerpts from a choices file
on endangered language documentation, as tree-
banks are not available for such languages.
A third line of research attempts to bootstrap
NLP tools for resource-poor languages by taking
advantage of IGT data and resources for resource-
rich languages. The canonical form of an IGT in-
stance includes a language line, a word-to-word
or morpheme-to-morpheme gloss line, and a trans-
lation line (typically in a resource-rich language).
The bootstrapping process starts with word align-
ment of the language line and translation line with
the help of the gloss line. Then the translation line
is parsed and the parse tree is projected to the lan-
guage line using the alignments (Xia and Lewis,
2007). The projected trees can be used to answer
linguistic questions such as word order (Lewis
and Xia, 2008) or bootstrap parsers (Georgi et al.,
2013). Our work extends this methodology to the
construction of precision grammars.
3 Methodology
Our goal in this work is to automatically create
choices files on the basis of IGT data. The choices
files encode both general properties about the lan-
guage we are trying to model as well as more spe-
cific information including lexical classes, lexical
items within lexical classes and definitions of lexi-
cal rules. Lexical rule definitions can include both
morphotactic information (ordering of affixes) as
well as morphosyntactic information, though here
our focus is on the former. Sample excerpts from
a choices file are given in Fig 1. These choices
files are then input into the Grammar Matrix cus-
tomization system
2
which produces HPSG gram-
2
SVN revision (for reproducibility): 27678.
44
mar fragments that meet the specifications in the
choices files. The Grammar Matrix customization
system provides analyses of a range of linguistic
phenomena. Here, we focus on a few that we con-
sider the most basic: major constituent word or-
der, the general case system, case frames for spe-
cific verbs, case marking on nouns, and morpho-
tactics for verbs. In ?3.1 we describe the dataset
we are working with. ?3.2 describes the different
approaches we take to building choices files on the
basis of this dataset. ?3.3 explains the metrics we
will use to evaluate the resulting grammars in ?4.
3.1 The Chintang Dataset
Chintang (ISO639-3: ctn) is a language spoken by
about 5000 people in Nepal and believed to be-
long to the Eastern subgroup of the Kiranti lan-
guages, which in turn are argued to belong to the
larger Tibeto-Burman family (Bickel et al., 2007;
Schikowski et al., in press). Here we briefly sum-
marize properties of the language that relate to
the information we are attempting to automatically
detect in the IGT, and in many cases make the
problem interestingly difficult.
Schikowski et al. (in press) describe Chintang
as exhibiting information-structurally constrained
word order: All permutations of the major senten-
tial constituents are expected to be valid, with the
different orders subject to different felicity condi-
tions. They state, however, that no detailed analy-
sis of word order has yet been carried out, and so
this description should be taken as preliminary.
In contrast, much detailed work has been done
on the marking of arguments, both via agree-
ment on the verb and via case marking of depen-
dents (Bickel et al., 2010; Stoll and Bickel, 2012;
Schikowski et al., in press). The case marking sys-
tem can be understood as following an ergative-
absolutive pattern, but with several variations from
that theme. In an ergative-absolutive pattern, the
sole argument of an intransitive verb (here called
S) is marked the same as the most patient-like ar-
gument of a transitive verb (here called O) and
differentiated from the most agent-like argument
of a transitive verb (here called A). Most A ar-
guments are marked with an overt case marker
called ergative, while S and O arguments appear
without a case marker. In most writing about the
language, this unmarked case is called nomina-
tive; here we will use the term absolutive. Simi-
larly, verbs agree with up to two arguments, and
the agreement markers for S and O are generally
shared and distinguished from those for A.
Divergences from the ergative-absolutive pat-
tern include variable marking of ergative case on
first and second person pronouns as well as va-
lence alternations such as one that licenses oc-
currences of transitive verbs with two absolutive
arguments (and S-style agreement with the A ar-
gument) when the O argument is of an indefinite
quantity (Schikowski et al., in press). Further-
more, the language allows dropping of arguments
(A, S, and O). Finally, there are of course valences
beyond simple intransitive and transitive, as well
as case frames even for two-argument verbs other
than { ERG, ABS }. As a result of the combination of
these facts, the actual occurrence of ergative-case-
marked arguments in speech is relatively low: Ex-
amining a corpus of speech spoken to and around
children, Stoll and Bickel (2012) find that only
11% of (semantically) transitive verb tokens have
an overt, ergative-marked NP A argument. As dis-
cussed below, these properties make it difficult for
automated methods to detect both the overall case
system of the language and accurate information
regarding the case frames of individual verbs.
The dataset we are using contains 9793 (8863
train, 930 test) IGT instances which come from
the corpus of narratives and other speech col-
lected, transcribed, translated and glossed by the
CLRP.
3
An example is shown in Fig. 2. As can
be seen in Fig. 2, the glossing in this dataset is ex-
tremely thorough. It is also supported by a detailed
Toolbox lexicon that encodes not only alternative
forms for each lemma as well as glosses in English
and Nepali, but also valence frames for most verb
entries which list the expected case marking on
the arguments. Finally, note that morphosyntactic
properties without a morphological reflex are sys-
tematically unglossed in the data, so that ABS never
appears (nor does SG for singular nouns, etc.).
In our experiments, we abstract away from the
problem of morphophonological analysis in order
to focus on morphosyntax and lexical acquisition.
Accordingly, our grammars target the second line
of the IGT, which represents each form as a se-
quence of phonologically regularized morphemes.
3.2 Grammars
In this section, we describe the different means we
use for extracting the different kinds of informa-
3
http://www.spw.uzh.ch/clrp
45
unisaNa
u-nisa-Na
3sPOSS-younger.brother-ERG.A
khatte
khatt-e
take-IND.PST
mo
mo
DEM.DOWN
kosi
kosi-i
river-LOC
moba
mo-pe
DEM.DOWN-LOC
?The younger brother took it to the river.? [ctn] (Bickel et al., 2013c)
Figure 2: Sample IGT
tion required to build the choices files (see Fig 1
above). We first describe our points of comparison
(oracle, ?3.2.1 and baseline, ?3.2.2), and then con-
sider different ways of detecting the large-scale
properties (word order, ?3.2.3; overall case sys-
tem, ?3.2.4). Next we turn to different ways of ex-
tracting two kinds of lexical information: the con-
straints on case (i.e. case frames of verbs and the
case marking on nouns, ?3.2.5) and verbal mor-
photactics (?3.2.6). Finally, we describe a small
set of hand-coded ?choices? which are added to all
choices files (except the oracle one) in order to cre-
ate working grammars (?3.2.7).
The alternative approaches to extracting the var-
ious kinds of information can be cross-classified
with each other, giving the set of choices files de-
scribed in Table 1. The first column gives iden-
tifiers for the choices files. The second specifies
how the lexicon was created, the third how the
value for major constituent word order was deter-
mined, and the fourth how the values for case were
determined, including the overall case system, the
case frames, and the case values for nouns. These
options are all described in more detail below.
3.2.1 Oracle choices file
As an upper-bound, we use the choices file de-
veloped in Bender et al., 2012b. This file in-
cludes hand-specified definitions of lexical rules
for nouns and verbs as well as lexical entries cre-
ated by importing lexical entries from the Tool-
box lexicon developed by the CLRP. This lex-
icon, as noted above, lists valence frames for
most verbal entries. As the Grammar Matrix
customization system currently only provides for
simple transitive and intransitive verbs, only two
verb classes were defined: intransitives with the
case frame { ABS } and transitives with the case
frame { ERG, ABS }. In addition, there is one class
of nouns. Finally, the choices file includes hand-
coded lexical entries for pronouns. As an upper-
bound, this choices file can be expected to repre-
sent high precision and moderate recall: verbs that
don?t fit the two classes defined aren?t imported.
Note that the Grammar Matrix customization
system does not currently support the definition of
adjectives, adverbs, or other parts of speech out-
side of verb, noun, determiner, (certain) adposi-
tions, conjunctions and auxiliaries. Thus while we
expect each grammar to be able to parse at least
some sentences in the corpus, to the extent that
sentences tend to include words outside the classes
noun, verb and determiner, we expect relatively
low coverage, even from our upper-bound.
3.2.2 Baseline choices file
Our baseline choices file is designed to create a
working grammar, without particular high-level
information about Chintang, that focuses on cov-
erage at the expense of precision. We hand-
specified the (counter-factual) assertion that there
is no case marking in Chintang, and in addi-
tion that Chintang allows free word order (on the
grounds that this is the least constrained word or-
der possibility). It also defines bare-bones classes
of nouns, determiners and transitive verbs, and
then populates the lexicon by using a variant of the
methodology in Xia and Lewis 2007. In particu-
lar, we parse the translation line using the Char-
niak parser (Charniak, 1997) and then use the cor-
respondences inherent in IGT to create a projected
tree structure for the language line, following Xia
and Lewis. An example of the result for Chintang
is shown in Fig 3. The projected trees include part
of speech tags for each word that can be aligned.
For each such word tagged as noun, verb, or deter-
miner, we create an instance in the corresponding
lexical type. In this baseline grammar, all verbs
are assumed to be transitive, but since all argu-
ments can (optionally) be dropped, the grammar is
expected to be able to cover intransitive sentences,
even if the semantic representation is wrong.
Since this baseline choices file models Chintang
as if it had no case marking, we expect it the re-
sulting grammar to have relatively high recall in
terms of the combination of nominal and verbal
constituents. On the other hand, since it is build-
ing a full-form lexicon and Chintang is a morpho-
logically complex language, we expect it to have
relatively low lexical coverage on held-out data.
46
Choices file Lexicon Word order Case
ORACLE Manual Manual Manual
BASELINE Fullform Default None
FF-AUTO-NONE Fullform Auto None
FF-DEFAULT-GRAM Fullform Default Auto (GRAM)
FF-AUTO-GRAM Fullform Auto Auto (GRAM)
FF-DEFAULT-SAO* Fullform Default Auto (SAO)
FF-AUTO-SAO* Fullform Auto Auto (SAO)
MOM-DEFAULT-NONE MOM Default None
MOM-AUTO-NONE MOM Auto None
MOM-DEFAULT-GRAM* MOM Default Auto (GRAM)
MOM-AUTO-GRAM* MOM Auto Auto (GRAM)
MOM-DEFAULT-SAO* MOM Default Auto (SAO)
MOM-AUTO-SAO* MOM Auto Auto (SAO)
Table 1: Choices files generated
s
vp
np-obj
pp
np
nn vbd
jutta khet-a-N-e
shoe buy-PST-1sS/P-IND.PST
I bought a pair of shoe .
prp vbd dt nn in nn
np-subj-prp np np
pp .
np-obj
vp
s
Figure 3: Projected tree structure (ex. from (Bickel
et al., 2013d))
3.2.3 Word order
We applied the methodology of Bender et al.
(2013) for determining major constituent order.
For our dataset, the algorithm chose ?v-final?,
which matches what is in the ORACLE choices file,
but is not necessarily correct. We created two ver-
sions of each of the other choices files, one with
the default (baseline) answer of ?free word order?
and one with this automatically supplied answer.
3.2.4 Case system
Similarly, we applied extended versions of the two
methods for automatically discovering case sys-
tems from Bender et al. 2013: GRAM which looks
for known case grams in glosses (not using pro-
jected trees) and SAO which extends the structure-
projection methodology of Xia and Lewis (2007)
to detect S, A and O arguments and then looks
for the most frequent gram associated with each
of these.
4
The GRAM method determines the
case system of Chintang to be ergative-absolutive,
while the SAO method indicates ?none? (no case).
Specifying a case system in a choices file has no
effect on the coverage or precision of the resulting
grammar if the lexical items don?t constrain case.
Thus the case system choices only make sense in
combination with the case frames choices (?3.2.5).
3.2.5 Case frames and case values
The HPSG analysis of case involves a feature CASE
which is constrained by both verbs and nouns:
Nouns constrain their own CASE value, while verbs
constrain the CASE value of the arguments they se-
lect for.
5
In order to constrain verbs and nouns
appropriately, we first need a range of possible
case values. For choices files built based on the
GRAM system, we consider case markers to be any
of those included in the set of grams defined by
the Leipzig Glossing Rules (Bickel et al., 2008):
ABL, ABS, ACC, ALL, COM, DAT, ERG, GEN, INS, LOC,
and OBL. For choices files built based on the SAO
system, we consider as case markers only those
grams (automatically) identified as marking S, A,
or O. In the present study, that should only be erga-
tive; as there is no marked case for absolutive, all
other nouns were treated as absolutive (regardless
of their actual case marking, since the SAO system
has no way to detect other case grams).
4
Our extensions involved making the system able to han-
dle the situation where one or more of S, A and O are morpho-
logically unmarked and therefore unreflected in the glosses.
5
For the details of the analyses of case systems provided
by the Grammar Matrix, see Drellishak 2009.
47
In choices files which specify case systems, we
constrain the case value for nouns by creating one
noun class for every case value, and then assigning
the lexical entries for nouns to those lexical classes
based on the grams in the gloss of the noun.
6
Similarly, we create lexical classes for each
case frame identified for transitive and intransitive
verbs: We look for case grams on each argument
of the verb, as determined by the function tags in
the projected tree (e.g. NP-SUBJ-PRP in Fig 3).
7
For
each case frame we identify, we create a lexical
class, and we create lexical entries for verbs based
on the case frames we extract for them. When
the system identifies both an overt subject and an
overt object, it considers the verb to be transitive
and constrains the case of its two arguments based
on the observed case values. If either argument
is overt but not marked for case, the verb is con-
strained to select for the default case on that argu-
ment, according to the detected case marking sys-
tem (i.e. ergative for transitive subjects and absolu-
tive for transitive objects, in this instance). When
there is an overt subject but no overt object, the
verb is treated as intransitive and is constrained to
select for a subject of the observed case (or the
default case, here absolutive, if the overt subject
bears no case marker). When there is an overt ob-
ject but no subject, the verb is assumed to be tran-
sitive and the object?s case assigned as with other
transitives but the subject?s case is constrained to
the default (i.e. ergative, in this instance). Verbs
with no overt arguments are not matched.
3.2.6 MOM choices file: Automatically
extracted lemmas and lexical rules
The final refinement we try on our baseline is
to apply the ?Matrix-ODIN Morphology? (MOM)
methodology of Wax 2014. This methodology at-
tempts to automatically identify affixes and cre-
ate appropriate descriptions of lexical rules in a
choices file to model those affixes. As a result,
it also identifies stems. Thus we use the same ba-
sic choices as in the baseline choices file, but now
populate the lexicon with stems rather than full-
forms. Compared to BASELINE, this one should re-
sult in a grammar with better lexical coverage on
held-out data, to the extent that the MOM system
6
In future work, we plan to extend the MOM approach
(?3.2.6) from verbs to nouns, but for now, the nouns are
treated as full-form lexical entries across all choices files.
7
While the GRAM method doesn?t require the projected
trees to determine the overall case system, we do need them
here to find case frames for particular verbs.
is able to correctly extract both stems and inflec-
tional rules. We note that while the MOM system
uses the same conceptual approach to alignment as
that in the BASELINE, GRAM and SAO approaches, the
implementation is separate, and so does not find
exactly the same set of verbs.
3.2.7 Shared choices
The ORACLE choices file ran as-is. For the re-
maining choices files, we also needed to answer
the questions about determiners (whether there are
any, position with respect to the noun). Based on
initial experiments, we chose ?yes? for the pres-
ence of determiners and ?det-noun? order. In an
attempt to boost coverage generally, we also coded
the choices that allow any argument to be dropped.
While the determiner-related choices are specific
to Chintang, the latter set of choices could be ex-
pected to boost coverage (at the cost of some pre-
cision) for any language.
3.2.8 Summary
Table 1 shows the 10 logical possibilities that arise
from combining the methods discussed in this sec-
tion, in addition to the ORACLE grammar and the
BASELINE grammar. However, we test only a subset
of these possibilities for the following reasons:
8
The SAO system chose no case as the case system
for Chintang. As a result, this makes FF-DEFAULT-
SAO and FF-AUTO-SAO the same as BASELINE and FF-
AUTO-NONE, respectively. In future work, we aim
to improve the SAO system but until it is effec-
tive enough to pick some case system for Chin-
tang, these options do not require further testing.
Secondly, while it is possible in principle to com-
bine the output of the MOM system (which classi-
fies verbs based on their morphological combina-
toric potential) with the output of the system be-
hind the GRAM choices files (which classifies verbs
based on their case frames), doing so is non-trivial
because these classifications are orthogonal, yet
each verb must inherit from each dimension. We
thus leave the exploration of MOM-DEFAULT-GRAM
and MOM-AUTO-GRAM (and likewise MOM-DEFAULT-
SAO and MOM-AUTO-SAO) for future work.
3.3 Evaluation
We evaluate the grammars generated by the
choices files over both the data used to develop
them (?training?; 8863 items) as well as data not
included in the development process (held-out
8
Untested choices files are marked with an * in the table.
48
?test? data; 930 items). We run both of these eval-
uations because we are actually testing two sepa-
rate questions. The first is whether the grammars
generated in this way can provide useful analyti-
cal tools to linguists. In this primary use-case, we
expect a linguist to provide the system with all of
their IGT and then use the generated grammars in
order to gain insights into that same data. This
does not amount to a case of testing on the train-
ing data because the annotations provided to the
system (IGT) are not the same as those produced
by the system (full parses, including semantic rep-
resentations). However, we are still interested in
also testing on held-out data in order to answer the
second question: whether grammars generated in
this way can also generalize to further texts.
We evaluate the grammars generated by the
choices files we create in terms of lexical cov-
erage, parse coverage, parse accuracy and am-
biguity. Lexical coverage measures how many
items consist only of word forms recognized by
the grammar. Any item with unknown lexical
items won?t parse.
9
Parse coverage is the num-
ber of items that receive any analysis at all, where
ambiguity is the number of different analyses each
item receives. To measure parse accuracy, we
examined the items that parse and determined
which parses had semantic representations whose
predicate-argument structures plausibly matched
what was indicated in the gloss.
4 Results
Table 2 compares the lexical information encoded
in each of the choices files in a quantitative fash-
ion. The first thing to note is that the grammars
vary widely in the size of their lexicons. The BASE-
LINE/FF lexicons are expected to be larger than the
others because they take each fully inflected form
encountered as a separate lexical entry. On the
other hand, the ORACLE choices file was built on the
basis of the Toolbox lexicon (dictionary) from the
CLRP and thus is effectively created on the basis
of a much larger dataset. The GRAM choices files
only contain verbs for which a case frame could
be identified. If the projected tree was not inter-
pretable by our extraction heuristics or if the ex-
ample had no overt arguments, then the verb will
not be extracted. The MOM choices files, on the
9
There are methods for handling unknown lexical items
(e.g. Adolphs et al., 2008) in more mature grammars of this
type, but these are not applicable at this stage.
other hand, only need to identify verbs in the string
to be able to extract them, and should be able to
generalize across different inflected forms of the
same verb. This gives a number of verb entries
intermediate between that for BASELINE/FF and the
GRAM files. For nouns, there is less variation: the
MOM files use the same data as the BASELINE, while
the GRAM method faces as simpler problem than
for verbs: it only needs to identify the case gram
(if any) in a noun?s gloss. The slightly larger num-
bers of nouns in the GRAM files v. the others can be
explained by the same form being glossed in two
different ways in the training data.
The remaining differences can be briefly ex-
plained as follows: The ORACLE choices file does
not contain any entries for determiners. The oth-
ers all contain the same 240 entries; one for any
word aligned by the algorithm to a determiner in
the English translation. Only the ORACLE and MOM
choices files attempt to handle morphology, and so
far MOM only does verbal morphology.
Table 3 presents the results of parsing training
and test data with the various grammars, in abso-
lute numbers and in percentages of the entire data
set. The ?lexical coverage? columns indicate for
how many items the grammars were able to rec-
ognize each constituent word form. The ?items
parsed? columns show the number of items that
received any analysis at all, while ?items correct?
show the number of items that were judged (by
one of the authors) to have a predicate-argument
structure that plausibly reflects the gloss given in
the IGT. The final column shows the average num-
ber of distinct analyses the grammars find for the
items they parse at all.
The results are in fact barely measurable with
these metrics (especially on the test data), but
nonetheless speak to the differences between the
grammars. Regarding lexical coverage, the ORA-
CLE grammar does best on the test data set. This
is because it is the only choices file not derived
from the training data. Not surprisingly, the BASE-
LINE grammar has the highest number of readings
per item parsed, followed closely by FF-AUTO-NONE
which adds only a minor constraint on word or-
der.
10
On the other hand, comparing the number
of items parsed to the number judged correct, ex-
cept for the MOM choices files, the ?survival rate?
was over 50% for all other tests.
11
This suggests
10
It is in this relative lack of constraint that BASELINE
mostly clearly forms a baseline to improve upon.
11
The vast majority of the incorrect parses for the MOM
49
Choices file # verb entries # noun entries # det entries # verb affixes # noun affixes
ORACLE 900 4751 0 160 24
BASELINE 3005 1719 240 0 0
FF-AUTO-NONE 3005 1719 240 0 0
FF-DEFAULT-GRAM 739 1724 240 0 0
FF-AUTO-GRAM 739 1724 240 0 0
MOM-DEFAULT-NONE 1177 1719 240 262 0
MOM-AUTO-NONE 1177 1719 240 262 0
Table 2: Amount of lexical information in each choices file
Training Data (N = 8863) Test Data (N = 930)
lexical items items average lexical items items average
choices file coverage (%) parsed (%) correct (%) readings coverage (%) parsed (%) correct (%) readings
ORACLE 1165 (13) 174 (3.5) 132 (1.5) 2.17 116 (12.5) 20 (2.2) 10 (1.1) 1.35
BASELINE 1276 (14) 398 (7.9) 216 (2.4) 8.30 41 (4.4) 15 (1.6) 8 (0.9) 28.87
FF-AUTO-NONE 1276 (14) 354 (4.0) 196 (2.2) 7.12 41 (4.4) 13 (1.4) 7 (0.8) 13.92
FF-DEFAULT-GRAM 911 (10) 126 (1.4) 84 (0.9) 4.08 18 (1.9) 4 (0.4) 2 (0.2) 5.00
FF-AUTO-GRAM 911 (10) 120 (1.4) 82 (0.9) 3.84 18 (1.9) 4 (0.4) 2 (0.2) 5.00
MOM-DEFAULT-NONE 1102 (12) 814 (9.2) 52 (0.6) 6.04 39 (4.2) 16 (1.7) 3 (0.3) 10.81
MOM-AUTO-NONE 1102 (12) 753 (8.5) 49 (0.6) 4.20 39 (4.2) 10 (1.1) 3 (0.3) 9.20
Table 3: Results
that, despite the noise introduced by the automatic
methods of lexical extraction, the precision gram-
mar backbone provided by the Grammar Matrix
can still provide high-quality parses.
For example, the BASELINE grammar produces
six parses of the string in (1):
(1) din
din
day
khiptukum
khipt-u-kV-m
count-3P-IND.NPST-1/2nsA
?(We) count days.? [ctn] (Bickel et al., 2013b)
Among these six is one which produces the se-
mantic representation in (2). While this grammar
does not yet capture any of the agreement mor-
phology that indicates that the subject is first per-
son plural, it does correctly link the ?day? to the
semantic ARG2 of ?count?.
(2)
? h
1
,
h
3
: din n day(x
4
),
h
5
: exist q rel(x
4
, h
6
, h
7
),
h
6
: khipt-u-kv-m v count(e
2
, x
9
, x
4
)
{ h
6
=
q
h
3
} ?
Finally, we note that the longest items we are
able to parse consist of one verb and two NPs, each
of which can have only up to two words (a deter-
miner and a noun). Most of the examples that do
parse consist of only one or two words, while the
full data set ranges from items of length 1 to items
of length 25 (average 4.5 words/item in training,
choices files involved analyses of words for ?yes?, ?well?,
?what? and the like as verbs. Note that one form of ?yes? is the
copula, and such examples were accepted. Another source of
incorrect parses for many grammars involves homophony be-
tween the focus particle and a verb meaning ?come?.
5 words/item in test). The Grammar Matrix al-
ready supports some longer sentences in the form
of coordination, so one avenue for future work is
to explore the automatic detection of coordination
strategies. Otherwise, branching out to longer sen-
tences will require additions to the Grammar Ma-
trix allowing the specification of modifiers and a
wider range of valence types for verbs.
5 Error Analysis
The opportunity to work closely with one lan-
guage has allowed us to observe several ways
in which the assumptions of the systems we are
building on do not match what we find in the data.
Here we briefly review some of those mismatches
and reflects on what could be done to handle them.
The first observation concerns the non-glossing
of zero-marked morphosyntactic features, such as
absolutive case in Chintang. From the point of
view of a consumer of IGT it is certainly desirable
to have as much information as possible made ex-
plicit in the glossing. From the point of view of
a project creating IGT in the context of on-going
fieldwork, however, it is likely often difficult to
reliably gloss zero morphemes and thus the de-
cision to leave them systematically unglossed is
quite sensible. Both the GRAM method and espe-
cially the SAO method for detecting case systems,
which we extended to extracting case frames for
particular verbs, are not yet fully robust to the
possibility that certain case values are unmarked
morphologically and thus not glossed in the data.
50
While we extended them to a certain extent in this
work, there is still more to be done on this front.
A second observation concerns the glossing of
proper names, as in (3):
(3) pailego
paile-ko
first-GEN
ubhiyauti
u-bhiya
3A-marriage
paphuma
paphu-ma
a.clan.of.Rai.people-F
?His first marriage was with a Phuphu woman.?
[ctn] (Bickel et al., 2013a)
We use statistical alignment between the trans-
lation line and the gloss line and between the
gloss line and the language line in order to project
information from the analysis of the translation
line onto the language line. Glosses such as
?a.clan.of.Rai.people? tend to confuse this align-
ment process, though they are very informative to
a human reader of the IGT. Error analysis of sen-
tences for which we were unable to extract subject
and object arguments at all suggested that many
of the errors were caused by misalignments likely
due to the aligner not being able to cope with this
kind of glossing. Future work will explore how to
train the aligner to function better in such cases.
In addition to properties of the glossing conven-
tions, there are also properties of the language that
proved challenging for our system. The first is the
intricate nature of the case-marking system as dis-
cussed in ?3.1. In particular, our system does not
model any distinction between 1st and 2nd per-
son pronouns and other nouns, such that when the
pronouns appear without a case marker, they are
taken to be in the unmarked case (i.e. absolutive),
though this is not necessarily so. The second prop-
erty of the language that our system found diffi-
cult is the optionality of arguments. We were able
to adapt our case frame extraction strategy to han-
dle dropped subjects, but dropped objects are more
confounding: our system is unable so far to distin-
guish such verbs from intransitives. One possible
way forward in this case is to draw more informa-
tion from the English translation in the IGT: En-
glish tends not to drop arguments, and so when we
find an object (especially a pronominal object) in
the English translation that is not aligned to any-
thing in the language line, we would have evidence
that the verb in question may be transitive.
Finally, we looked closely at the items in the test
data for which we had complete lexical analysis,
but which still failed to parse. We did this both for
the fullform and MOM-based lexicons. The goal
here was to evaluate whether (a) our assignment of
items to lexical categories was correct (and there
was some other issue standing in the way of an-
alyzing the item) or (b) we should have parsed a
given item, but our system had misidentified the
words in question in such a way that no syntactic
analysis could be found. For the baseline system,
we found that although some items had misidenti-
fied categories (specifically, pronouns and adverbs
were sometimes misidentified as determiners), the
two major obstacles to parsing came from multi-
verb constructions or sentential fragments. Of the
26 unparsed items with lexical coverage, 10 con-
tained multiple verbs and 12 were NP or interjec-
tory fragments (eg: ?Yes, yes, yes.?). We observed
a similar pattern among 23 unparsed items from
the MOM-based lexicon. We can take two lessons
from this assessment: (1) since much of our data
comes from naturally occurring speech, it may be
useful to rerun our tests with an NP fragment as
a valid root symbol in our grammars; (2) proper
identification of auxiliary verbs is an important
next step for improving our system.
6 Conclusion
In this paper we have taken the first steps towards
creating actual precision grammars by creating
Grammar Matrix customization system choices
files on the basis of automated analysis of IGT.
Measured in terms of coverage over held-out data,
the results are hardly impressive and might seem
discouraging. However, we see in these initial for-
ays rather a proof-of-concept. Moreover, the pro-
cess of digging into the details of getting an IGT-
to-grammar system working for one particular lan-
guage has been a very rich source of information
on the mismatches between the assumptions of
systems built to handle high-level properties and
the linguistic facts and glossing conventions of the
kind of data they are meant to handle.
7 Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
BCS-1160274. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the NSF.
We would like to thank David Wax for his as-
sistance in setting up the MOM system, Olga Za-
maraeva for general discussion, and especially the
CRLP for providing access to the Chintang data.
51
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Dan Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural
language parsing. Marrakech, Morocco, May.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8?14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Com-
putation, pages 1?50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179?206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, Robert Schikowski, and Balthasar
Bickel. 2012b. Deriving a lexicon for a precision
grammar from language documentation resources:
A case study of Chintang. In Proceedings of COL-
ING 2012, pages 247?262, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Emily M. Bender, Michael Wayne Goodman, Joshua
Crowgey, and Fei Xia. 2013. Towards creating pre-
cision grammars from interlinear glossed text: Infer-
ring large-scale typological properties. In Proceed-
ings of the 7th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 74?83, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Balthasar Bickel, Goma Banjade, Martin Gaenszle,
Elena Lieven, Netra Paudyal, Ichchha Rai, Manoj
Rai, Novel Kishor Rai, and Sabine Stoll. 2007. Free
prefix ordering in Chintang. Language, 83(1):43?
73.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai,
Elena Lieven, Goma Banjade, Toya Nath Bhatta,
Netra Paudyal, Judith Pettigrew, Ichchha P. Rai,
Manoj Rai, Robert Schikowski, and Sabine Stoll.
2009. Audiovisual corpus of the chintang lan-
guage, including a longitudinal corpus of language
acquisition by six children, plus a trilingual dic-
tionary, paradigm sets, grammar sketches, ethno-
graphic descriptions, and photographs. DOBES
Archive, http://www.mpi.nl/DOBES.
Balthasar Bickel, Manoj Rai, Netra P. Paudyal, Goma
Banjade, Toya N. Bhatta, Martin Gaenszle, Elena
Lieven, Ichchha Purna Rai, Novel Kishore Rai, and
Sabine Stoll. 2010. The syntax of three-argument
verbs in Chintang and Belhare (Southeastern Ki-
ranti). In Studies in Ditransitive Constructions: A
Comparative Handbook, pages 382?408. Mouton de
Gruyter, Berlin.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013a. Hatuwa. Accessed:
15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013b. Khadak?s daily life.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013c. Tale of a poor guy.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013d. Talk of kazi?s trip.
Accessed: 15 January 2013.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 319?326, Barcelona, Spain, July.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of AAAI-1997.
John Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proc. of the 6th International Workshop on Parsing
Technologies (IWPT-2000), Italy.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distri-
butional clustering. In Proc. of the 5th Confer-
ence on Computational Natural Language Learning
(CoNLL-2001).
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language & Compu-
tation, 3(4):281?332.
52
Bart Cramer and Yi Zhang. 2009. Construction of a
german hpsg grammar from a detailed treebank. In
Proceedings of the 2009 Workshop on Grammar En-
gineering Across Frameworks (GEAF 2009), pages
37?45, Suntec, Singapore.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Ryan Georgi, Fei Xia, and William D. Lewis. 2013.
Enhanced and portable dependency projection algo-
rithms using interlinear glossed text. In Proceedings
of ACL 2013 (Volume 2: Short Papers), pages 306?
311, Sofia, Bulgaria, August.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven grammar induction. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL
2006), pages 881?888, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proc. of LREC-2002, pages
1974?1981.
Julia Hockenmaier and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3):355?396.
Dan Klein and Christopher Manning. 2002. A gen-
eral constituent context model for improved gram-
mar induction. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2002), Philadelphia, PA.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2004), Barcelona, Spain.
Alexander Krotov, Mark Hepple, Robert Gaizauskas,
and Yorick Wilks. 1998. Compacting the Penn
Treebank Grammar. In Proc. of the 36th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-1998), Montreal, Quebec, Canada.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685?690, Hyderabad, India.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proc. of the First In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-2004), Hainan, China.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Robert Schikowski, Balthasar Bickel, and Netra
Paudyal. in press. Flexible valency in Chintang.
In B. Comrie and A. Malchukov, editors, Valency
Classes: A Comparative Handbook. Mouton de
Gruyter, Berlin.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL/COLING 2006), pages 569?
576, Sydney, Australia, July. Association for Com-
putational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 73?81, August.
Sabine Stoll and Balthasar Bickel. 2012. How to
measure frequency? Different ways of counting
ergatives in Chintang (Tibeto-Burman, Nepal) and
their implications. In Frank Seifart, Geoffrey Haig,
Nikolaus P. Himmelmann, Dagmar Jung, Anna Mar-
getts, and Paul Trilsbeek, editors, Potentials of Lan-
guage Documentation: Methods, Analyses, and Uti-
lization, pages 83?89. University of Hawai?i Press,
Manoa.
David Wax. 2014. Automated grammar engineering
for verbal morphology. Master?s thesis, University
of Washington.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Fei Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed Corpora. In Proc. of 5th Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS-1999), Beijing, China.
53
