Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 177?184
Manchester, August 2008
Pedagogically Useful Extractive Summaries for Science Education 
Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner 
Institute of Cognitive Science 
Department of Computer Science 
University of Colorado at Boulder 
sebastian.delachica, faisal.ahmad, james.martin, 
tamara.sumner@colorado.edu 
 
 
 Abstract 
This paper describes the design and 
evaluation of an extractive summarizer 
for educational science content called 
COGENT. COGENT extends MEAD 
based on strategies elicited from an em-
pirical study with science domain and in-
structional design experts. COGENT 
identifies sentences containing pedagogi-
cally relevant concepts for a specific sci-
ence domain. The algorithms pursue a 
hybrid approach integrating both domain 
independent bottom-up sentence scoring 
features and domain-aware top-down fea-
tures. Evaluation results indicate that 
COGENT outperforms existing summar-
izers and generates summaries that 
closely resemble those generated by hu-
man experts. COGENT concept invento-
ries appear to also support the computa-
tional identification of student miscon-
ceptions about earthquakes and plate tec-
tonics. 
1 Introduction 
Multidocument summarization (MDS) research 
efforts have resulted in significant advancements 
in algorithm and system design (Mani, 2001). 
Many of these efforts have focused on summariz-
ing news articles, but not significantly explored 
the research issues arising from processing edu-
cational content to support pedagogical applica-
tions. This paper describes our research into the 
application of MDS techniques to educational 
                                                 
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
science content to generate pedagogically useful 
summaries. 
Knowledge maps are graphical representations 
of domain information laid out as networks of 
nodes containing rich concept descriptions inter-
connected using a fixed set of relationship types 
(Holley and Dansereau, 1984). Knowledge maps 
are a variant of the concept maps used to capture, 
assess, and track student knowledge in education 
research (Novak and Gowin, 1984). Learning 
research indicates that knowledge maps may be 
useful cognitive scaffolds, helping users lacking 
domain expertise to understand the macro-level 
structure of an information space (O'Donnell et 
al., 2002). Knowledge maps have emerged as an 
effective representation to generate conceptual 
browsers that help students navigate educational 
digital libraries, such as the Digital Library for 
Earth System Education (DLESE.org) (Butcher 
et al, 2006). In addition, knowledge maps have 
proven useful for domain and instructional ex-
perts to capture domain knowledge from digital 
library resources and to analyze student under-
standing for the purposes of providing formative 
assessments (Ahmad et al, 2007).  
Knowledge maps have proven useful both as 
representations of knowledge for assessment 
purposes and as learning resources for presenta-
tion to students. However, domain knowledge 
map construction by experts is an expensive 
knowledge engineering activity. In this paper, we 
describe our progress towards the automated 
generation of pedagogically useful extractive 
summaries from educational texts about a sci-
ence domain. In the context of automated knowl-
edge map generation, summary sentences corre-
spond to concepts. While the detection of rela-
tionships between concepts is also part of our 
overall research agenda, this paper focuses solely 
on concept identification using MDS techniques. 
The remainder of this paper is organized as fol-
177
lows. First, we review related work in the areas 
of automated concept extraction from texts and 
extractive summarization. We then describe the 
empirical study we have conducted to understand 
how domain and instructional design experts 
identify pedagogically important science con-
cepts in educational digital library resources. 
Next, we provide a detailed description of the 
algorithms we have designed based on expert 
strategies elicited from our empirical study. We 
then present and discuss our evaluation results 
using automated summarization metrics and hu-
man judgments. Finally, we present our conclu-
sions and future work in this area. 
2 Related Work 
Our work is informed by efforts to automate the 
acquisition of ontology concepts from text. On-
toLearn (Navigli and Velardi, 2004) extracts do-
main terminology from a collection of texts using 
a syntactic parse to identify candidate terms that 
are filtered based on domain relevance and con-
nected using a semantic interpretation based on 
word sense disambiguation. The newly identified 
concepts and relationships are used to update an 
existing ontology. Knowledge Puzzle focuses on 
n-grams to produce candidate terms filtered 
based on term frequency in the input documents 
and on the number of relationships associated 
with a given term (Zouaq et al, 2007). This ap-
proach leverages pattern extraction techniques to 
identify concepts and relationships. While these 
approaches produce ontologies useful for compu-
tational purposes, the identified concepts are of a 
very fine granularity and therefore may yield 
graphs not suitable for identifying student mis-
conceptions or for presentation back to the stu-
dent. Clustering by committee has also been used 
to discover concepts from a text by grouping 
terms into conceptually related clusters (Lin and 
Pantel, 2002). The resulting clusters appear to be 
tightly related, but operate at a very fine level of 
granularity. Our approach focuses on sentences 
as units of knowledge to produce concise repre-
sentations that may be useful both as computa-
tional objects and as learning resources to present 
back to the student. Therefore, extractive sum-
marization research also informs our work. 
Topic representation and topic themes have 
been used to explore promising MDS techniques 
(Harabagiu and Lacatusu, 2005). Recent efforts 
in graph-based MDS have integrated sentence 
affinity, information richness and diversity pen-
alties to produce very promising results (Wan 
and Yang, 2006). Finally, MEAD is a widely 
used multi-document summarization and evalua-
tion platform (Radev et al, 2000). MEAD re-
search efforts have resulted in significant contri-
butions to support the development of summari-
zation applications (Radev et al, 2000). While 
all these systems have produced promising re-
sults in automated evaluations, none have di-
rectly targeted educational content as input or the 
generation of pedagogically useful summaries. 
We are directly building upon MEAD due its 
focus on sentence extraction and its high degree 
of modularization. 
3 Empirical Study 
We have conducted a study to capture how hu-
man experts construct and use knowledge maps. 
In this 10-month study, we examined how ex-
perts created knowledge maps from educational 
digital libraries and how they used the maps to 
assess student work and provide personalized 
feedback. 
In this paper, we are focusing on the knowl-
edge map construction aspects of the study. Four 
geology and instructional design experts collabo-
ratively selected 20 resources from DLESE to 
construct a domain knowledge map on earth-
quakes and plates tectonics for high school age 
learners. The experts independently created 
knowledge maps of individual resources which 
they collaboratively merged into the final domain 
knowledge map in a one-day workshop. The re-
sulting domain knowledge map consisted of 564 
nodes containing domain concepts and 578 rela-
tionships. The concepts consist of 7,846 words, 
or 5% of the total number of words in the origi-
nal resources. Figure 1 shows a fragment of the 
domain knowledge map created by our experts. 
 
 
Figure 1. Fragment of domain  knowledge map 
created by domain and instructional experts 
 
Experts created nodes containing concepts of 
varying granularity, including nouns, noun 
phrases, partial sentences, single sentences, and 
178
multiple sentences. Our analysis of this domain 
knowledge map indicates that experts relied on 
copying-and-pasting (58%) and paraphrasing 
(37%) to create most domain concepts. Only 5% 
of the nodes could not be traced directly to the 
original resources. 
Experts used relationship types in a Zipf-like 
distribution with the top 10 relationship types 
accounting for 64% of all relationships. The top 
2 relationship types each accounted for more 
than 10% of all relationships: elaborations (19% 
or 110 links) and examples (14% or 78 links). 
We have established the completeness of this 
domain knowledge map by asking a domain ex-
pert to assess its content coverage of nationally-
recognized educational goals on earthquakes and 
plate tectonics for high school age learners using 
the American Association for the Advancement 
of Science (AAAS) Benchmarks (Project 2061, 
1993). The results indicate adequate content cov-
erage of the relevant AAAS Benchmarks achieved 
through 82 of the concepts (15%) with the re-
maining 482 concepts (85%) providing very de-
tailed elaborations of the associated learning 
goals. 
Qualitative analysis of the verbal protocols 
captured during the study indicates that all ex-
perts used external sources to construct the do-
main knowledge map. Experts made references 
to their own knowledge (e.g., ?I know that??), 
to content learned or taught in geology courses, 
to other resources used in the study, and to the 
National Science Education Standards (NSES), a 
comprehensive collection of nationally-
recognized science learning goals for K-12 stu-
dents (National Research Council, 1996). 
We have examined sentence extraction agree-
ment between experts using a kappa measure that 
accounts for prevalence of judgments and con-
flicting biases amongst experts, called PABA-
kappa (Byrt et al, 1993). The average PABA-
kappa value of 0.62 indicates that our experts 
substantially agree on sentence extraction from 
digital library resources. While this study was 
not designed as an annotation project to support 
summarization evaluation, this level of agree-
ment indicates that the concepts selected by the 
experts may serve as the reference summary to 
evaluate the performance of our summarizer. 
4 Summarizer for Science Education 
Creating a knowledge map from a collection of 
input texts involves identifying sentences con-
taining important domain concepts, linking con-
cepts, and labeling those links. This paper fo-
cuses solely on identifying and extracting peda-
gogically relevant sentences as domain concepts. 
We have designed and implemented an extrac-
tive summarizer for educational science content, 
called COGENT, based on MEAD version 3.11 
(Radev et al, 2000). COGENT processes a col-
lection of educational digital library resources by 
first preprocessing each resource using Tidy 
(tidy.sourceforge.net) to fix improperly format-
ted HTML code. COGENT then merges multiple 
web pages into a single HTML document and 
extracts the contents of each resource into a plain 
text file. We have extended MEAD with sen-
tence scoring features based on domain content, 
document structure, and sentence length. 
4.1 Domain Content 
We have designed two sentence-scoring features 
that aim to capture the domain content relevance 
of each sentence: the educational standards 
feature and the gazetteer feature. 
We have developed a feature that models how 
human experts used external sources to identify 
and extract concepts. The educational standards 
feature uses the textual description of the 
relevant AAAS Benchmarks on earthquakes and 
plate tectonics for high-school age learners and 
the associated NSES. Each sentence receives a 
score based on its similarity to the text contents 
of the learning goals and educational standards 
computed using a TFIDF (Term Frequency-
Inverse Document Frequency) approach  (Salton 
and Buckley, 1988). We have used KinoSearch, 
a Perl implementation of the Lucene search 
engine (lucene.apache.org), to create an index 
that includes the AAAS Benchmarks learning 
goal description (boosted by 2), subject (boosted 
by 8), and keywords (boosted by 2), plus the text 
of the associated national standards (not 
boosted). Sentence scores are based on the 
similarity scores generated by KinoSearch in 
response to a query consisting of the sentence 
text. 
To account for the large number of examples 
used by the experts in the domain knowledge 
map (14% of all links), we have developed a 
feature that reflects the number and relevance of 
the geographical names in each sentence. Earth 
science examples often refer to names of 
geographical places, including geological 
formations on the planet. The gazetteer feature 
leverages the Alexandria Digital Library (ADL) 
Gazetteer service (Hill, 2000) to check whether 
named entities identified in each sentence match 
179
entries in the ADL Gazetteer. A gazetteer is a 
georeferencing resource containing information 
about locations and place-names, including 
latitude and longitude as well as type information 
about the corresponding geographical feature. 
Each sentence receives a score based on a TFIDF 
approach where the TF is the number of times a 
particular location name appears in the sentence 
and the IDF is the inverse of the count of 
gazetteer entries matching the location name. If 
the ADL Gazetteer returns a large number of 
results for a given place-name, it means there are 
many geographical locations identified by that 
name. Our assumption is that unique names may 
be more pedagogically relevant. For example, 
Ohio receives an IDF score of 0.0625 because 
the ADL Gazetteer contains 16 entries so named, 
while the Mid-Atlantic Ridge, the distinctive 
underwater mountain range dividing the Atlantic 
Ocean, receives a score of 1.0 as it appears only 
once. 
4.2 Document Structure 
Based on the intuition that the HTML structure 
of a web site reflects content relevancy, we have 
developed the hypertext feature. The hypertext 
feature assigns a higher score to sentences con-
tained under higher level HTML headings.  
  
Heading Bonus 
H1 1/1 = 1.00 
H2 1/2 = 0.50 
H3 1/3 = 0.33 
H4 1/4 = 0.25 
H5 1/5 = 0.20 
H6 1/6 = 0.17 
Table 1. Hypertext feature heading bonus 
 
Within a given heading level, the hypertext 
feature assigns a higher score to sentences that 
appear earlier within that level based on both 
relative paragraph order within the heading and 
relative sentence position within each paragraph. 
The equation used to compute the hypertext 
score for a sentence is  
44
_1 * _1 * _  _ nosentnoparbonusheadingscorehypertext =  
where heading_bonus is obtained from Table 1, 
par_no is the paragraph number within the head-
ing, and sent_no is the sentence number within 
the paragraph. We use the 4 1 x   function to at-
tenuate the contributions to the feature score of 
later paragraphs and sentences. Initially, we used 
the same function MEAD uses to modulate its 
position feature ( 2 1 x ), but initial experimenta-
tion indicated this function decayed too rapidly, 
resulting in later sentences being over-penalized. 
4.3 Sentence Length 
To promote the extraction of sentences contain-
ing scientific concepts, we have developed the 
content word density feature. This feature makes 
a cut-off decision based on the ratio of content 
words to function words in a sentence. The con-
tent word density feature uses a pre-populated 
list of function words (a stopword list) to calcu-
late the ratio of content to function words within 
each sentence, keeping sentences that meet or 
exceed the ratio of 50%. This cut-off value im-
plies that the extracted sentences contain rela-
tively more content words than function words. 
4.4 Sentence Scoring and Selection 
We compute the final score of each sentence by 
adding the scores obtained for the MEAD default 
configuration features (centroid and position) to 
the scores for the COGENT features (educational 
standards, gazetteer, and hypertext). After the 
sentences have been sorted according to their 
cumulative scores, we keep sentences that pass 
the cut-off constraints, including the MEAD 
length feature equal or greater than 9 and CO-
GENT content word density equal or greater than 
50%. We use the MEAD cosine re-ranker to 
eliminate redundant sentences based on a cutoff 
similarity value of 0.7. Since human experts used 
only 5% of the total word count in the resources, 
we have configured MEAD to use a 5% word 
compression rate. 
5 Evaluation 
We have evaluated COGENT by processing the 
20 digital library resources used in the empirical 
study and comparing its output against the con-
cepts identified by the experts. 
5.1 Quality 
To assess the quality of the generated summaries, 
we have examined three configurations: Random, 
Default, and COGENT. The Random configura-
tion extracts a random collection of sentences 
from the input texts. The Default configuration 
uses the MEAD default centroid, position and 
length (cut-off value of 9) sentence scoring fea-
tures. Finally, the COGENT configuration in-
cludes the MEAD default features and the CO-
GENT features. The Default and COGENT con-
figurations use the MEAD cosine function with a 
threshold of 0.7 to eliminate redundant sen-
180
tences. All three configurations use a word com-
pression factor of 5% resulting in summaries of 
very similar length. 
For this evaluation, we leverage ROUGE (Lin 
and Hovy, 2003) to address the relative quality of 
the generated summaries based on common n-
gram counts and longest common subsequence 
(LCS). We report on ROUGE-1 (unigrams), 
ROUGE-2 (bigrams), ROUGE W-1.2 (weighted 
LCS), and ROUGE-S* (skip bigrams) as they 
appear to correlate well with human judgments 
for longer multi-document summaries, particu-
larly ROUGE-1 (Lin, 2004). Table 2 shows the 
results of this ROUGE-based evaluation includ-
ing recall (R), precision (P), and balanced f-
measure (F). 
 
  Random Default COGENT 
R 0.4855 0.4976 0.6073 
P 0.5026 0.5688 0.6034 R-1 
F 0.4939 0.5308 0.6054 
R 0.0972 0.1321 0.1907 
P 0.1006 0.1510 0.1895 R-2 
F 0.0989 0.1409 0.1901 
R 0.0929 0.0951 0.1185 
P 0.1533 0.1733 0.1877 R-W-1.2 
F 0.1157 0.1228 0.1453 
R 0.2481 0.2620 0.3820 
P 0.2657 0.3424 0.3772 R-S* 
F 0.2566 0.2969 0.3796 
Table 2. Quality evaluation results (5% word 
compression) 
 
COGENT consistently outperforms the Ran-
dom and Default baselines based on all four re-
ported ROUGE measures. Given that much of 
the original research efforts on MEAD have cen-
tered on news articles, this result is not surpris-
ing. Pedagogical content, such as the educational 
digital library resources used in our work, differs 
in rhetorical intent, structure and terminology 
from the news articles leveraged by the MEAD 
researchers. However, the COGENT features 
described here are complementary to the default 
MEAD configuration. COGENT can best be 
characterized as a hybrid MDS, integrating bot-
tom-up (centroid, position, length, hypertext, and 
content word density) and top-down (educational 
standards and gazetteer) sentence scoring fea-
tures. This hybrid approach reflects our findings 
from observing expert behaviors for identifying 
concepts from educational digital library re-
sources. We believe the overall improvement in 
quality scores may be due to the COGENT fea-
tures targeting different dimensions of what con-
stitutes a pedagogically effective summary than 
the default MEAD features. 
To characterize the COGENT summary con-
tents, one of our research team members manu-
ally generated a summary corresponding to the 
best case for an extractive summarizer. This Best 
Case summary comprises the sentences from the 
digital library resources that align to the concepts 
selected by the human experts in our empirical 
study. Since the experts created concepts of vary-
ing granularity, this alignment produces the list 
of sentences that the experts would have pro-
duced if they had only selected single sentences 
to create concepts for their domain knowledge 
map. This summary comprises 621 sentences 
consisting of 13,116 words, or about a 9% word 
compression. 
For this aspect of the evaluation, we have used 
ROUGE-L, an LCS metric computed using 
ROUGE. The ROUGE-L computation examines 
the union LCS between each reference sentence 
and all the sentences in the candidate summary. 
We believe this metric may be well-suited to re-
flect the degree of linguistic surface structure 
similarity between summaries. We postulate that 
ROUGE-L may be able to account for the explic-
itly copy-pasted concepts and to detect the more 
subtle similarities with paraphrased concepts in 
the expert-generated domain knowledge map. 
We have also used the content-based evaluation 
capabilities of MEAD to report on a cosine 
measure to capture similarity between the candi-
date summaries and the reference. Table 3 shows 
the results of this aspect of the evaluation includ-
ing recall (R), precision (P), and balanced f-
measure (F). 
 
  Random 
(5%) 
Default 
(5%) 
COGENT 
(5%) 
Best Case 
(9%) 
R 0.4814 0.4919 0.6021 0.9669 
P 0.4982 0.5623 0.5982 0.6256 R-L 
F 0.4897 0.5248 0.6001 0.7597 
Cosine 0.5382 0.6748 0.8325 0.9323 
Table 3. Content-based evaluation results (word 
compression in parentheses) 
 
COGENT consistently outperforms the Ran-
dom and Default baselines on both the ROUGE-
L and cosine measures. Given the cosine value of 
0.8325, it appears COGENT extracts sentences 
containing similar terms  in very similar fre-
quency distribution as the experts. 
The ROUGE-L scores also consistently indi-
cate that the COGENT summary may be closer 
to the reference summary in relative word order-
181
ing than either the Random or Default configura-
tions. However, the scores for the Best Case 
summary reveal two interesting points. First, the 
ROUGE-L recall score for COGENT (R=0. 
6021) is lower than that obtained by the Best 
Case summary (R=0.9669), meaning our sum-
marizer appears to be extracting different sen-
tences than those selected by the experts. Given 
the high cosine similarity with the reference 
summary (0.8325), we hypothesize that CO-
GENT may be selecting sentences that cover 
very similar concepts to those selected by the 
experts only expressed differently. Second, we 
would have expected the ROUGE-L precision 
score for the Best Case configuration to be closer 
to 1.0. Instead, the Best Case precision score is 
0.6256, only a minor improvement over CO-
GENT (P=0.5982). Since the sentences in the 
Best Case summary come directly from the digi-
tal library resources, we hypothesize that experts 
may have used extensive linguistic transforma-
tions for paraphrased concepts, resulting in struc-
tures that ROUGE-L could not identify as simi-
lar. 
Given the difference in word compression for 
the Best Case summary, we have performed an 
incremental analysis using the ROUGE-L meas-
ure shown in Figure 2.  
ROUGE-L COGENT Evaluation
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30
MEAD Word Percent Compression
Recall Precision F-Measure
 
Figure 2. COGENT ROUGE-L results at differ-
ent word compression rates 
 
This graph shows improved COGENT per-
formance in ROUGE-L recall as the length of the 
summary increases, while both precision and f-
measure degrade. COGENT can match the recall 
scores of the Best Case summary (R=0.9669) by 
making the generated summary longer (30% 
word compression rate or 32,619 words), but the 
precision would suffer a sizeable decay 
(P=0.1558). For educational applications, more 
comprehensive concept inventories (longer 
summaries) may be better suited for computa-
tional purposes, such as pedagogical reasoning 
about student understanding, while more succinct 
inventories (shorter summaries) may be more 
appropriate for display to the student. 
5.2 Pedagogical Utility 
We have evaluated COGENT?s pedagogical util-
ity in the context of computationally identifying 
student scientific misconceptions. We have de-
veloped algorithms that reliably detect incorrect 
statements in student essays by comparing an 
expert-created domain knowledge map to an ex-
pert-created knowledge map of an essay. These 
algorithms use textual entailment techniques 
based on a shallow linguistic analysis of knowl-
edge map concepts to identify sentences that con-
tradict concepts in the domain knowledge map. 
Initial evaluation results indicate that these algo-
rithms identify incorrect statements nearly as 
adeptly as human experts. 
 
 Manual 
Expert  
Agreement 
Expert  
Knowledge 
Maps 
COGENT  
Concept  
Inventory 
Recall 0.69 0.87 0.93 
Precision 0.69 0.57 0.57 
F-Measure 0.69 0.68 0.69 
Table 4. Incorrect statement identification 
evaluation results 
 
As shown in Table 4, the algorithms detect 
87% of all incorrect statements identified by ex-
perts and 57% of the reported incorrect state-
ments agree with human judgments on the same 
task. By comparison, experts show 69% overlap 
on average along both dimensions. Introducing 
the COGENT concept inventory in place of the 
expert-created domain knowledge map improves 
recall performance, as the algorithms return 93% 
of all incorrect statements reported by the ex-
perts, while preserving 57% precision. These 
results indicate that the generated summary cov-
ers the necessary pedagogical concepts to com-
putationally identify student scientific miscon-
ceptions. 
Informal sampling of the sentences selected by 
COGENT shows the following three important 
science concepts receiving the highest scores:  
1. Earthquakes are the result of forces deep 
within the Earth's interior that continuously 
affect the surface of the Earth. 
2. Scientists believed that the movement of the 
Earth's plates bends and squeezes the rocks at 
the edges of the plates. 
3. In particular, four major scientific develop-
ments spurred the formulation of the plate-
182
tectonics theory: (1) demonstration of the 
ruggedness and youth of the ocean floor; (2) 
confirmation of repeated reversals of the 
Earth magnetic field in the geologic past; (3) 
emergence of the seafloor-spreading hypothe-
sis and associated recycling of oceanic crust; 
and (4) precise documentation that the world's 
earthquake and volcanic activity is concen-
trated along oceanic trenches and submarine 
mountain ranges. 
For a more rigorous analysis of the pedagogi-
cal utility of the COGENT concepts, we asked an 
instructional expert with domain expertise in ge-
ology to evaluate the 326 sentences returned by 
COGENT. The expert used a 5-point Likert scale 
to judge whether each concept would be peda-
gogically useful in the context of a concept in-
ventory on earthquakes and plate tectonics 
knowledge for high school age learners. The ex-
pert agreed or strongly agreed that 60% of the 
sentences would be pedagogically useful, with 
30% of the sentences being potentially useful and 
only 10% of the sentences being judged as not 
useful. These results indicate that COGENT ap-
pears to perform quite well at identifying sen-
tences that contain information relevant for 
learning about the domain. 
We have also completed an ablation study to 
identify the relative contribution of the COGENT 
features to the quality of the summary. We have 
focused on the cosine metric to capture the over-
all similarity between the COGENT concept in-
ventory and the concepts from the expert-created 
knowledge map. 
 
Features Cosine 
All Features 0.8325 
(Gazetteer) 0.5545 
(Hypertext) 0.5575 
(Educational Standards) 0.8083 
(Content Word Density) 0.8271 
 
Table 5. Feature ablation evaluation results for 
COGENT 
 
Table 5 shows the cosine similarity between 
the concept inventory generated after taking the 
feature shown in parentheses out of the summar-
izer. The results are ordered from low-to-high 
such that the feature contributing the most to the 
all-features cosine score appears at the top of the 
table. Removing either the gazetteer or the hy-
pertext feature causes the largest drops in simi-
larity indicating the importance of the use of ex-
amples and the relevance of document structure 
for the quality of the COGENT-generated sum-
mary. Meanwhile both the educational standards 
and content word density appear to provide mod-
est but useful improvements to the quality of the 
COGENT summary. 
Given that our algorithms have only been 
evaluated on the topic of earthquakes and plate 
tectonics for high school age learners, COGENT 
may be limited in its ability to transcend domains 
due to its reliance on two domain-aware sentence 
scoring features: educational standards and gaz-
etteer. However, the educational standards fea-
ture may be applicable across other science top-
ics because the AAAS Benchmarks and NSES 
provide very thorough and detailed coverage of a 
wide range of topics for the Science, Technol-
ogy, Engineering, and Math disciplines for 
grades K-12. Only the gazetteer feature would 
need to be replaced, especially given its signifi-
cant contribution to the quality of the generated 
summary as indicated by the results of the abla-
tion study. We believe these results highlight the 
need to generalize our approach, perhaps using a 
classifier for identifying examples in educational 
texts without resorting to overly domain-specific 
language resources, such as the ADL Gazetteer. 
Overall, the evaluation results indicate that our 
approach holds promise for effectively identify-
ing concepts for inclusion in the construction of a 
pedagogically useful domain knowledge map 
from educational science content. 
6 Conclusions and Future Work 
In this paper, we have presented a multi-
document summarization system, COGENT, that 
integrates bottom-up and top-down sentence 
scoring features to identify pedagogically rele-
vant concepts from educational digital library 
resources. Our results indicate that COGENT 
generates concept inventories that resemble those 
identified by experts and outperforms existing 
multi-document summarization systems. We 
have also used the COGENT concept inventory 
as input to our  misconception identification al-
gorithms and the evaluation results indicate the 
algorithms perform as well as when using an ex-
pert-created domain knowledge map. In the con-
text of generating domain knowledge maps, our 
next step is to explore how machine learning 
techniques may be employed to connect concepts 
with links. 
Automating the process of creating inventories 
of important pedagogical concepts represents an 
important step towards creating scalable intelli-
183
gent learning and tutoring systems. We hope our 
progress in this direction may contribute to in-
crease the interest within the computational lin-
guistics research community in novel educational 
technology research. 
Acknowledgments 
This research is funded in part by the National 
Science Foundation under NSF IIS/ALT Award 
0537194. Any opinions, findings, and conclu-
sions or recommendations expressed in this ma-
terial are those of the author(s) and do not neces-
sarily reflect the views of the NSF. 
References 
Ahmad, F., de la Chica, S., Butcher, K., Sumner, T. 
and Martin, J.H. (2007, June 17-23). Towards 
automatic conceptual personalization tools. In Pro-
ceedings of the 7th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries, (Vancouver, British Co-
lumbia, Canada, 2007), pages 452 - 461. 
Butcher, K.R., Bhushan, S. and Sumner, T. (2006). 
Multimedia displays for conceptual discovery: in-
formation seeking with strand maps. ACM Multi-
media Systems, 11 (3), pages 236-248. 
Byrt, T., Bishop, J. and Carlin, J.B. (1993). Bias, 
prevalence, and kappa. Journal of Clinical Epide-
miology, 46 (5), pages 423-429. 
Harabagiu, S. and Lacatusu, F. (2005, August 15-19). 
Topic themes for multi-document summarization. 
In Proceedings of the 28th Annual International 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, (Salvador, Brazil, 
2005), pages 202-209. 
Hardy, H., Shimizu, N., Strzalkowski, T., Ting, L., 
Wise, G.B. and Zhang, X. (2002). Summarizing 
large document sets using concept-based cluster-
ing. In Proceedings of the Human Language Tech-
nology Conference 2002, (San Diego, California, 
United States, 2002), pages 222-227. 
Hill, L.L. (2000, September 18-20). Core elements of 
digital gazetteers: placenames, categories, and 
footprints. In Proceedings of the 4th European 
Conference on Digital Libraries, (Lisbon, Portugal, 
2000), pages 280-290. 
Holley, C.D. and Dansereau, D.F. (1984). Spatial 
learning strategies: Techniques, applications, and 
related issues. Academic Press, Orlando, Florida. 
Lin, C.Y. (2004). ROUGE: A package for automatic 
evaluation of summaries. In Proceedings of the 
Workshop on Text Summarization Branches Out, 
(Barcelona, Spain, 2004). 
Lin, C.Y. and Hovy, E. (2003, May-June). Automatic 
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the Human 
Language Technology Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics, HLT-NAACL, (Edmonton, 
Canada, 2003), pages 71-78. 
Lin, D. and Pantel, P. (2002, August 24-September 1). 
Concept discovery from text. In Proceedings of the 
19th International Conference on Computational 
Linguistics, (Taipei, Taiwan, 2002), pages 1-7. 
Mani, I. (2001). Automatic Summarization. Mitkov, 
R. (Ed.) John Benjamins B.V., Amsterdam, The 
Netherlands. 
National Research Council. (1996). National Science 
Education Standards. National Academy Press, 
Washington, DC. 
Navigli, R. and Velardi, P. (2004). Learning domain 
ontologies from document warehouses and dedi-
cated websites. Computational Linguistics, 30 (2), 
pages 151-179. 
Novak, J.D. and Gowin, D.B. (1984). Learning how 
to learn. Cambridge University Press, New York, 
New York. 
O'Donnell, A.M., Dansereau, D.F. and Hall, R.H. 
(2002). Knowledge maps as scaffolds for cognitive 
processing. Educational Psychology Review, 14 
(1), pages 71-86. 
Project 2061. (1993). Benchmarks for science liter-
acy. Oxford University Press, New York, New 
York, United States. 
Radev, D.R., Jing, H. and Budzikowska, M. (2000). 
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evalua-
tion, and user studies. In Proceedings of the 
ANLP/NAACL 2000 Workshop on Summariza-
tion, (2000), pages 21-30. 
Salton, G. and Buckley, C. (1988). Term-weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24 (5), pages 513-
523. 
Wan, X. and Yang, J. (2006, June 5th-7th). Improved 
affinity graph based multi-document summariza-
tion. In Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the Association for Computational Lin-
guistics, (New York City, New York, 2006), pages 
181-184. 
Zouaq, A., Nkambou, R. and Frasson, C. (2007, July 
9-13). Learning a domain ontology in the Knowl-
edge Puzzle project. In Proceedings of the Fifth In-
ternational Workshop on Ontologies and Semantic 
Web for E-Learning, (Marina del Rey, California, 
2007). 
184
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 17?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extractive Summaries for Educational Science Content 
Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner 
Institute of Cognitive Science 
Department of Computer Science 
University of Colorado at Boulder 
sebastian.delachica, faisal.ahmad, james.martin, 
tamara.sumner@colorado.edu 
 
 
Abstract 
This paper describes an extractive summarizer 
for educational science content called 
COGENT. COGENT extends MEAD based 
on strategies elicited from an empirical study 
with domain and instructional experts. 
COGENT implements a hybrid approach inte-
grating both domain independent sentence 
scoring features and domain-aware features. 
Initial evaluation results indicate that 
COGENT outperforms existing summarizers 
and generates summaries that closely resem-
ble those generated by human experts. 
1 Introduction 
Knowledge maps consist of nodes containing rich 
concept descriptions interconnected using a limited 
set of relationship types (Holley and Dansereau, 
1984). Learning research indicates that knowledge 
maps may be useful for learners to understand the 
macro-level structure of an information space 
(O'Donnell et al, 2002). Knowledge maps have 
also emerged as an effective computational infra-
structure to support the automated generation of 
conceptual browsers. Such conceptual browsers 
appear to allow students to focus on the science 
content of large educational digital libraries (Sum-
ner et al, 2003), such as the Digital Library for 
Earth System Education (DLESE.org). Knowledge 
maps have also shown promise as domain and stu-
dent knowledge representations to support person-
alized learning interactions (de la Chica et al, 
2008). 
In this paper we describe our progress towards 
the generation of science concept inventories as 
summaries of digital library collections. Such in-
ventories provide the basis for the construction of 
knowledge maps useful both as computational 
knowledge representations and as learning re-
sources for presentation to the student. 
2 Related Work 
Our work is informed by efforts to automate the 
acquisition of ontology concepts from text. On-
toLearn extracts candidate domain terms from texts 
using a syntactic parse and updates an existing on-
tology with the identified concepts and relation-
ships (Navigli and Velardi, 2004). Knowledge 
Puzzle focuses on n-gram identification to produce 
a list of candidate terms pruned using information 
extraction techniques to derive the ontology 
(Zouaq et al, 2007). Lin and Pantel (2002) dis-
cover concepts using clustering by committee to 
group terms into conceptually related clusters. 
These approaches produce ontologies of very fine 
granularity and therefore graphs that may not be 
suitable for presentation to a student. 
Multi-document summarization (MDS) re-
search also informs our work. XDoX analyzes 
large document sets to extract important themes 
using n-gram scoring and clustering (Hardy et al, 
2002). Topic representation and topic themes have 
also served as the basis for the exploration of 
promising MDS techniques (Harabagiu and Laca-
tusu, 2005). Finally, MEAD is a widely used MDS 
and evaluation platform (Radev et al, 2000). 
While all these systems have produced promising 
results in automated evaluations, none have di-
rectly targeted educational content collections. 
17
3 Empirical Study 
We have conducted a study to capture how human 
experts processed digital library resources to create 
a domain knowledge map. Four geology and in-
structional design experts selected 20 resources 
from DLESE to construct a knowledge map on 
earthquakes and plates tectonics for high school 
age learners. The resulting knowledge map con-
sists of 564 concepts and 578 relationships. 
 
Figure 1. Expert knowledge map excerpt 
The concepts include 7,846  words, or 5% of 
the resources. Our experts relied on copying-and-
pasting (58%) and paraphrasing (37%) to create 
most concepts. Only 5% of the concepts could not 
be traced directly to the original resources. Rela-
tionship types were used in a Zipf-like distribution 
with the top 2 relationship types each accounting 
for more than 10% of all relationships: elabora-
tions (19%) and examples (14%). 
Analysis by an independent instructional expert 
indicates that this knowledge map provides ade-
quate coverage of nationally-recognized educa-
tional goals on earthquakes and plate tectonics for 
high school learners using the American Associa-
tion for the Advancement of Science (AAAS) 
Benchmarks (Project 2061, 1993). 
Verbal protocol analysis shows that all experts 
used external sources to create the knowledge map, 
including their own expertise, other digital library 
resources, and the National Science Education 
Standards (NSES), a comprehensive collection of 
nationally-recognized science learning goals for K-
12 students (National Research Council, 1996). 
We have examined sentence extraction agree-
ment between experts using the prevalence-
adjusted bias-adjusted (PABA) kappa to account 
for prevalence of judgments and conflicting biases 
amongst experts (Byrt et al, 1993). The average 
PABA-kappa value of 0.62 indicates that experts 
substantially agree on sentence extraction from 
digital library resources. This level of agreement 
suggests that these concepts may serve as the ref-
erence summary to evaluate our system. 
4 Summarizer for Science Education 
We have implemented an extractive summarizer 
for educational science content, COGENT, based 
on MEAD version 3.11 (Radev et al, 2000). 
COGENT complements the default MEAD sen-
tence scoring features with features based on find-
ings from the empirical study. COGENT 
represents a hybrid approach integrating bottom-up 
(hypertext and content word density) and top-down 
(educational standards and gazetteer) features.  
We model how human experts used external in-
formation sources with the educational standards 
feature. This feature leverages the text of the rele-
vant AAAS Benchmarks and associated NSES. 
Each sentence receives a score based on its TFIDF 
similarity to the textual contents of these learning 
goals and educational standards. 
We have developed a feature that reflects the 
large number of examples extracted by the experts. 
Earth science examples often refer to geographical 
locations and geological formations. The gazetteer 
feature checks named entities from each sentence 
against the Alexandria Digital Library (ADL) Gaz-
etteer (Hill, 2000). A gazetteer is a geo-referencing 
resource containing location and type information 
about place-names. Each sentence receives a 
TFIDF score based on place-name term frequency 
and overall uniqueness in the gazetteer. Our as-
sumption is that geographical locations with more 
unique names may be more pedagogically relevant. 
Based on the intuition that the HTML structure 
of a resource reflects relevancy, we have devel-
oped the hypertext feature. This feature computes a 
sentence score directly proportional to the HTML 
heading level and inversely proportional to the 
relative paragraph number within a heading and to 
the relative sentence position within a paragraph. 
18
To promote the extraction of sentences contain-
ing science concepts, we have developed the con-
tent word density feature. This feature computes 
the ratio of content to function words in a sentence. 
Function words are identified using a stopword list, 
and the feature only keeps sentences featuring 
more content words than function words. 
We compute the final sentence score by adding 
the MEAD default feature scores (centroid and 
position) to the COGENT feature scores (educa-
tional standards, gazetteer, and hypertext). 
COGENT keeps sentences that pass the cut-off 
constraints, including the MEAD sentence length 
of 9 and COGENT content word density of 50%. 
The default MEAD cosine re-ranker eliminates 
redundant sentences. Since the experts used 5% of 
the total word count in the resources, we produce 
summaries of that same length. 
5 Evaluation 
We have evaluated COGENT by processing the 20 
digital library resources used in the empirical study 
and comparing the output against the concepts 
identified by the experts. Three configurations are 
considered: Random, Default, and COGENT. The 
Random summary uses MEAD to extract random 
sentences. The Default summary uses the MEAD 
centroid, position and length default features. Fi-
nally, the COGENT summary extends MEAD with 
the COGENT features. 
We use ROUGE (Lin, 2004) to assess summary 
quality using common n-gram counts and longest 
common subsequence (LCS) measures. We report 
on ROUGE-1 (unigrams), ROUGE-2 (bigrams), 
ROUGE W-1.2 (weighted LCS), and ROUGE-S* 
(skip bigrams) as they have been shown to corre-
late well with human judgments for longer multi-
document summaries (Lin, 2004). Table 1 shows 
the results for recall (R), precision (P), and bal-
anced f-measure (F). 
  Random Default COGENT 
R 0.4855 0.4976 0.6073 
P 0.5026 0.5688 0.6034 R-1 
F 0.4939 0.5308 0.6054 
R 0.0972 0.1321 0.1907 
P 0.1006 0.1510 0.1895 R-2 
F 0.0989 0.1409 0.1901 
R 0.0929 0.0951 0.1185 
P 0.1533 0.1733 0.1877 R-W-1.2 
F 0.1157 0.1228 0.1453 
  Random Default COGENT 
R 0.2481 0.2620 0.3820 
P 0.2657 0.3424 0.3772 R-S* 
F 0.2566 0.2969 0.3796 
Table 1. Quality evaluation results 
Table 1 indicates that COGENT consistently 
outperforms the Random and Default summaries. 
These results indicate the promise of our approach 
to generate extractive summaries of educational 
science content. Given our interest in generating a 
pedagogically effective domain knowledge map, 
we have also conducted a content-centric evalua-
tion. 
To characterize the COGENT summary con-
tents, one of the authors manually constructed a 
summary corresponding to the best case output for 
an extractive summarizer. This Best Case summary 
comprises all the sentences from the resources that 
align to all the concepts selected by the experts. 
This summary comprises 621 sentences consisting 
of 13,116 words, or about a 9% word compression.  
We use ROUGE-L to examine the union LCS 
between the reference and candidate summaries, 
thus capturing their linguistic surface structure 
similarity. We also use MEAD to report on cosine 
similarity. Table 2 shows the results for recall (R), 
precision (P), and balanced f-measure (F). 
  Random 
(5%) 
Default 
(5%) 
COGENT 
(5%) 
Best Case  
(9%) 
R 0.4814 0.4919 0.6021 0.9669 
P 0.4982 0.5623 0.5982 0.6256 R-L 
F 0.4897 0.5248 0.6001 0.7597 
Cosine 0.5382 0.6748 0.8325 0.9323 
Table 2. Content evaluation results (word compression) 
The ROUGE-L scores consistently indicate that 
the COGENT summary may be closer to the refer-
ence in linguistic surface structure than either the 
Random or Default summaries. Since the 
COGENT ROUGE-L recall score (R=0. 6021) is 
lower than the Best Case (R=0.9669), it is likely 
that COGENT may be extracting different sen-
tences than those selected by the experts. Based on 
the high cosine similarity with the reference 
(0.8325), we hypothesize that COGENT may be 
selecting sentences that cover very similar con-
cepts to those selected by the experts, but ex-
pressed differently. 
Given the difference in word compression for 
the Best Case summary, we have performed an 
19
incremental analysis using the ROUGE-L measure 
shown in Figure 2. 
ROUGE-L COGENT Evaluation
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30
MEAD Word Percent Compression
Recall Precision F-Measure
 
Figure 2. Incremental COGENT ROUGE-L analysis 
Figure 2 indicates that COGENT can match the 
Best Case recall (R=0.9669) by generating a longer 
summary. For educational applications, lengthier 
summaries may be better suited for computational 
purposes, such as diagnosing student understand-
ing, while shorter summaries may be more appro-
priate for display to the student. 
6 Conclusions 
COGENT extends MEAD based on strategies elic-
ited from an empirical study with domain and in-
structional experts. Initial evaluation results 
indicate that COGENT holds promise for identify-
ing important domain pedagogical concepts. We 
are exploring portability to other science education 
domains and machine learning techniques to con-
nect concepts into a knowledge map. Automating 
the creation of inventories of pedagogically impor-
tant concepts may represent an important step to-
wards scalable intelligent tutoring systems. 
Acknowledgements 
This research is funded in part by the National Sci-
ence Foundation under NSF IIS/ALT Award 
0537194. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the author(s) and do not necessarily reflect 
the views of the NSF. 
References 
T. Byrt, J. Bishop and J. B. Carlin. Bias, prevalence, and 
kappa. Journal of Clinical Epidemiology, 46, 5 
(1993), 423-429. 
S. de la Chica, F. Ahmad, T. Sumner, J. H. Martin and 
K. Butcher. Computational foundations for personal-
izing instruction with digital libraries. International 
Journal of Digital Libraries, to appear in the Special 
Issue on Digital Libraries and Education. 
S. Harabagiu and F. Lacatusu. Topic themes for multi-
document summarization. In Proc. of the 28th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 
(Salvador, Brazil, 2005), 202-209. 
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. 
Wise and X. Zhang. Summarizing large document 
sets using concept-based clustering. In Proc. of the 
Human Language Technology Conference 2002, 
(San Diego, California, United States, 2002), 222-
227. 
L. L. Hill. Core elements of digital gazetteers: place-
names, categories, and footprints. In Proc. of the 4th 
European Conference on Digital Libraries, (Lisbon, 
Portugal, 2000), 280-290. 
C. D. Holley and D. F. Dansereau. Spatial learning 
strategies: Techniques, applications, and related is-
sues. Academic Press, Orlando, Florida, 1984. 
C. Y. Lin. ROUGE: A package for automatic evaluation 
of summaries. In Proc. of the Workshop on Text 
Summarization Branches Out, (Barcelona, Spain, 
2004). 
D. Lin and P. Pantel. Concept discovery from text. In 
Proc. of the 19th International Conference on Com-
putational Linguistics, (Taipei, Taiwan, 2002), 1-7. 
National Research Council. National Science Education 
Standards. National Academy Press, Washington, 
DC, 1996. 
R. Navigli and P. Velardi. Learning domain ontologies 
from document warehouses and dedicated websites. 
Computational Linguistics, 30, 2 (2004), 151-179. 
A. M. O'Donnell, D. F. Dansereau and R. H. Hall. 
Knowledge maps as scaffolds for cognitive process-
ing. Educational Psychology Review, 14, 1 (2002), 
71-86. 
Project 2061. Benchmarks for science literacy. Oxford 
University Press, New York, New York, United 
States, 1993. 
D. R. Radev, H. Jing and M. Budzikowska. Centroid-
based summarization of multiple documents: sen-
tence extraction, utility-based evaluation, and user 
studies. In Proc. of the ANLP/NAACL 2000 Work-
shop on Summarization, (2000), 21-30. 
T. Sumner, S. Bhushan, F. Ahmad and Q. Gu. Design-
ing a language for creating conceptual browsing in-
terfaces for digital libraries. In Proc. of the 3rd 
ACM/IEEE-CS Joint Conference on Digital Librar-
ies, (Houston, Texas, 2003), 258-260. 
A. Zouaq, R. Nkambou and C. Frasson. Learning a do-
main ontology in the Knowledge Puzzle project. In 
Proc. of the Fifth International Workshop on Ontolo-
gies and Semantic Web for E-Learning, (Marina del 
Rey, California, 2007). 
20
Back to Basics for Monolingual Alignment: Exploiting Word Similarity and
Contextual Evidence
Md Arafat Sultan?, Steven Bethard? and Tamara Sumner?
?Institute of Cognitive Science and Department of Computer Science
University of Colorado Boulder
?Department of Computer and Information Sciences
University of Alabama at Birmingham
arafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.edu
Abstract
We present a simple, easy-to-replicate monolin-
gual aligner that demonstrates state-of-the-art
performance while relying on almost no su-
pervision and a very small number of external
resources. Based on the hypothesis that words
with similar meanings represent potential pairs
for alignment if located in similar contexts, we
propose a system that operates by finding such
pairs. In two intrinsic evaluations on alignment
test data, our system achieves F1 scores of 88?
92%, demonstrating 1?3% absolute improve-
ment over the previous best system. Moreover,
in two extrinsic evaluations our aligner out-
performs existing aligners, and even a naive
application of the aligner approaches state-of-
the-art performance in each extrinsic task.
1 Introduction
Monolingual alignment is the task of discovering and
aligning similar semantic units in a pair of sentences
expressed in a natural language. Such alignments pro-
vide valuable information regarding how and to what
extent the two sentences are related. Consequently,
alignment is a central component of a number of
important tasks involving text comparison: textual
entailment recognition, textual similarity identifica-
tion, paraphrase detection, question answering and
text summarization, to name a few.
The high utility of monolingual alignment has
spawned significant research on the topic in the re-
cent past. Major efforts that have treated alignment
as a standalone problem (MacCartney et al., 2008;
Thadani and McKeown, 2011; Yao et al., 2013a) are
primarily supervised, thanks to the manually aligned
corpus with training and test sets from Microsoft Re-
search (Brockett, 2007). Primary concerns of such
work include both quality and speed, due to the fact
that alignment is frequently a component of larger
NLP tasks.
Driven by similar motivations, we seek to devise a
lightweight, easy-to-construct aligner that produces
high-quality output and is applicable to various end
tasks. Amid a variety of problem formulations and
ingenious approaches to alignment, we take a step
back and examine closely the effectiveness of two
frequently made assumptions: 1) Related semantic
units in two sentences must be similar or related
in their meaning, and 2) Commonalities in their se-
mantic contexts in the respective sentences provide
additional evidence of their relatedness (MacCartney
et al., 2008; Thadani and McKeown, 2011; Yao et al.,
2013a; Yao et al., 2013b). Alignment, based solely
on these two assumptions, reduces to finding the best
combination of pairs of similar semantic units in sim-
ilar contexts.
Exploiting existing resources to identify similarity
of semantic units, we search for robust techniques
to identify contextual commonalities. Dependency
trees are a commonly used structure for this purpose.
While they remain a central part of our aligner, we
expand the horizons of dependency-based alignment
beyond exact matching by systematically exploiting
the notion of ?type equivalence? with a small hand-
crafted set of equivalent dependency types. In addi-
tion, we augment dependency-based alignment with
surface-level text analysis.
While phrasal alignments are important and have
219
Transactions of the Association for Computational Linguistics, 2 (2014) 219?230. Action Editor: Alexander Koller.
Submitted 11/2013; Revised 1/2014; Published 5/2014. c?2014 Association for Computational Linguistics.
been investigated in multiple studies, we focus pri-
marily on word alignments (which have been shown
to form the vast majority of alignments (? 95%)
in multiple human-annotated corpora (Yao et al.,
2013b)), keeping the framework flexible enough to
allow incorporation of phrasal alignments in future.
Evaluation of our aligner on the benchmark dataset
reported in (Brockett, 2007) shows an F1 score of
91.7%: a 3.1% absolute improvement over the previ-
ous best system (Yao et al., 2013a), corresponding
to a 27.2% error reduction. It shows superior perfor-
mance also on the dataset reported in (Thadani et
al., 2012). Additionally, we present results of two
extrinsic evaluations, namely textual similarity iden-
tification and paraphrase detection. Our aligner not
only outperforms existing aligners in each task, but
also approaches top systems for the extrinsic tasks.
2 Background
Monolingual alignment has been applied to various
NLP tasks including textual entailment recognition
(Hickl et al., 2006; Hickl and Bensley, 2007), para-
phrase identification (Das and Smith, 2009; Madnani
et al., 2012), and textual similarity assessment (Ba?r
et al., 2012; Han et al., 2013) ? in some cases ex-
plicitly, i.e., as a separate module. But many such
systems resort to simplistic and/or ad-hoc strategies
for alignment and in most such work, the alignment
modules were not separately evaluated on alignment
benchmarks, making their direct assessment difficult.
With the introduction of the MSR alignment cor-
pus (Brockett, 2007) developed from the second
Recognizing Textual Entailment challenge data (Bar-
Haim et al., 2006), direct evaluation and comparison
of aligners became possible. The first aligner trained
and evaluated on the corpus was a phrasal aligner
called MANLI (MacCartney et al., 2008). It repre-
sents alignments as sets of different edit operations
(where a sequence of edits turns one input sentence
into the other) and finds an optimal set of edits via
a simulated annealing search. Weights of different
edit features are learned from the training set of the
MSR alignment corpus using a perceptron learning
algorithm. MANLI incorporates only shallow fea-
tures characterizing contextual similarity: relative
positions of the two phrases being aligned (or not) in
the two sentences and boolean features representing
whether or not the preceding and following tokens of
the two phrases are similar.
Thadani and McKeown (2011) substituted
MANLI?s simulated annealing-based decoding with
integer linear programming, and achieved a consider-
able speed-up. More importantly for our discussion,
they found contextual evidence in the form of syn-
tactic constraints useful in better aligning stop words.
Thadani et al. (2012) further extended the model by
adding features characterizing dependency arc edits,
effectively bringing stronger influence of contextual
similarity into alignment decisions. Again the perfor-
mance improved consequently.
The most successful aligner to date both in terms
of accuracy and speed, called JacanaAlign, was de-
veloped by Yao et al. (2013a). In contrast to the
earlier systems, JacanaAlign is a word aligner that
formulates alignment as a sequence labeling prob-
lem. Each word in the source sentence is labeled
with the corresponding target word index if an align-
ment is found. It employs a conditional random field
to assign the labels and uses a feature set similar to
MANLI?s in terms of the information they encode
(with some extensions). Contextual features include
only semantic match of the left and the right neigh-
bors of the two words and their POS tags. Even
though JacanaAlign outperformed the MANLI en-
hancements despite having less contextual features,
it is difficult to compare the role of context in the
two models because of the large paradigmatic dispar-
ity. An extension of JacanaAlign was proposed for
phrasal alignments in (Yao et al., 2013b), but the
contextual features remained largely unchanged.
Noticeable in all the above systems is the use of
contextual evidence as a feature for alignment, but
in our opinion, not to an extent sufficient to harness
its full potential. Even though deeper dependency-
based modeling of contextual commonalities can be
found in some other studies (Kouylekov and Magnini,
2005; Chambers et al., 2007; Chang et al., 2010; Yao
et al., 2013c), we believe there is further scope for
systematic exploitation of contextual evidence for
alignment, which we aim to do in this work.
On the contrary, word semantic similarity has been
a central component of most aligners; various mea-
sures of word similarity have been utilized, including
string similarity, resource-based similarity (derived
from one or more lexical resources like WordNet)
220
Align
identical
word
sequences
Align
named
entities
Align
content
words
Align
stop
words
Figure 1: System overview
and distributional similarity (computed from word
co-occurrence statistics in large corpora). An impor-
tant trade-off between precision, coverage and speed
exists here and aligners commonly rely on only a
subset of these measures (Thadani and McKeown,
2011; Yao et al., 2013a). We use the Paraphrase
Database (PPDB) (Ganitkevitch et al., 2013), which
is a large resource of lexical and phrasal paraphrases
constructed using bilingual pivoting (Bannard and
Callison-Burch, 2005) over large parallel corpora.
3 System
Our system operates as a pipeline of alignment mod-
ules that differ in the types of word pairs they align.
Figure 1 is a simplistic representation of the pipeline.
Each module makes use of contextual evidence to
make alignment decisions. In addition, the last two
modules are informed by a word semantic similarity
algorithm. Because of their phrasal nature, we treat
named entities separately from other content words.
The rationale behind the order in which the modules
are arranged is discussed later in this section (3.3.5).
Before discussing each alignment module in de-
tail, we describe the system components that identify
word and contextual similarity.
3.1 Word Similarity
The ability to correctly identify semantic similarity
between words is crucial to our aligner, since con-
textual evidence is important only for similar words.
Instead of treating word similarity as a continuous
variable, we define three levels of similarity.
The first level is an exact word or lemma match
which is represented by a similarity score of 1. The
second level represents semantic similarity between
two terms which are not identical. To identify such
word pairs, we employ the Paraphrase Database
(PPDB)1. We use the largest (XXXL) of the PPDB?s
lexical paraphrase packages and treat all pairs iden-
tically by ignoring the accompanying statistics. We
1http://paraphrase.org
customize the resource by removing pairs of identi-
cal words or lemmas and adding lemmatized forms
of the remaining pairs. For now, we use the term
ppdbSim to refer to the similarity of each word pair
in this modified version of PPDB (which is a value in
(0, 1)) and later explain how we determine it (Section
3.3.5). Finally, any pair of different words which is
absent in PPDB is assigned a zero similarity score.
3.2 Extracting Contextual Evidence
Our alignment modules collect contextual evidence
from two complementary sources: syntactic depen-
dencies and words occurring within a small textual
vicinity of the two words to be aligned. The applica-
tion of each kind assumes a common principle of min-
imal evidence. Formally, given two input sentences
S and T , we consider two words s ? S and t ? T to
form a candidate pair for alignment if ?rs ? S and
?rt ? T such that:
1. (s, t) ? <Sim and (rs, rt) ? <Sim, where
<Sim is a binary relation indicating sufficient
semantic relatedness between the members of
each pair (? ppdbSim in our case).
2. (s, rs) ? <C1 and (t, rt) ? <C2 , such that
<C1 ? <C2 ; where <C1 and <C2 are binary re-
lations representing specific types of contextual
relationships between two words in a sentence
(e.g., an nsubj dependency between a verb and
a noun). The symbol ? represents equivalence
between two relationships, including identical-
ity.
Note that the minimal-evidence assumption holds
a single piece of contextual evidence as sufficient
support for a potential alignment; but as we discuss
later in this section, an evidence for word pair (s, t)
(where s ? S and t ? S) may not lead to an align-
ment if there exists a competing pair (s?, t) or (s, t?)
with more evidence (where s? ? S and t? ? T ).
In the rest of this section, we elaborate the different
forms of contextual relationships we exploit along
with the notion of equivalence between relationships.
3.2.1 Syntactic Dependencies
Dependencies can be important sources of con-
textual evidence. Two nsubj children rs and rt of
two verbs s ? S and t ? T , for example, pro-
vide evidence for not only an (s, t) alignment, but
221
S: He wrote a book .
nsubj
dobj
det
T : I read the book he wrote .
nsubj
dobj
det
rcmod
nsubj
Figure 2: Equivalent dependency types: dobj and rcmod
also an (rs, rt) alignment if (s, t) ? <Sim and
(rs, rt) ? <Sim. (We adopt the Stanford typed de-
pendencies (de Marneffe and Manning, 2008).)
Moreover, dependency types can exhibit equiva-
lence; consider the two sentences in Figure 2. The
dobj dependency in S is equivalent to the rcmod
dependency in T (dobj ? rcmod, following our ear-
lier notation) since they represent the same semantic
relation between identical word pairs in the two sen-
tences. To be able to use such evidence for alignment,
we need to go beyond exact matching of dependen-
cies and develop a mapping among dependency types
that encodes such equivalence. Note also that the
parent-child roles are opposite for the two depen-
dency types in the above example, a scenario that
such a mapping must accommodate.
The four possible such scenarios regarding parent-
child orientations are shown in Figure 3. If (s, t) ?
<Sim and (rs, rt) ? <Sim (represented by bidirec-
tional arrows), then each orientation represents a set
of possible ways in which the S and T dependen-
cies (unidirectional arrows) can provide evidence of
similarity between the contexts of s in S and t in T .
Each such set comprises equivalent dependency type
pairs for that orientation. In the example of Figure 2,
(dobj, rcmod) is such a pair for orientation (c), given
s = t = ?wrote? and rs = rt = ?book?.
We apply the notion of dependency type equiva-
lence to intra-category alignment of content words
in four major lexical categories: verbs, nouns,
adjectives and adverbs (the Stanford POS tag-
ger (Toutanova et al., 2003) is used to identify the
categories). Table 1 shows dependency type equiva-
lences for each lexical category of s and t.
The ??? sign on column 5 of some rows repre-
sents a duplication of the column 4 content of the
s
rs
t
rt
rs
s
rt
t
s
rs
t
rt
s
rs
t
rt
(a) (b) (c) (d)
Figure 3: Parent-child orientations in dependencies
same row. For each row, columns 4 and 5 show two
sets of dependency types; each member of the first
is equivalent to each member of the second for the
current orientation (column 1) and lexical categories
of the associated words (columns 2 and 3). For exam-
ple, row 2 represents the fact that an agent relation
(between s and rs; s is the parent) is equivalent to an
nsubj relation (between t and rt; t is the parent).
Note that the equivalences are fundamentally re-
dundant across different orientations. For example,
row 2 (which is presented as an instance of ori-
entation (a)) can also be presented as an instance
of orientation (b) with POS(s)=POS(t)=noun and
POS(rs)=POS(rt)=verb. We avoid such redundancy
for compactness. For the same reason, the equiva-
lence of dobj and rcmod in Figure 2 is shown in the
table only as an instance of orientation (c) and not as
an instance of orientation (d) (in general, this is why
orientations (b) and (d) are absent in the table).
We present dependency-based contextual evidence
extraction in Algorithm 1. (The Stanford dependency
parser (de Marneffe et al., 2006) is used to extract the
dependencies.) Given a word pair (si, tj) from the in-
put sentences S and T , it collects contextual evidence
(as indexes of rsi and rtj with a positive similarity)
for each matching row in Table 1. An exact match
of the two dependencies is also considered a piece
of evidence. Note that Table 1 only considers con-
tent word pairs (si, tj) such that POS(si)=POS(tj),
but as 90% of all content word alignments in the
MSR alignment dev set are within the same lexical
category, this is a reasonable set to start with.
3.2.2 Textual Neighborhood
While equivalent dependencies can provide strong
contextual evidence, they can not ensure high recall
because, a) the ability to accurately extract depen-
222
Orientation POS(s, t) POS(rs, rt) S Dependency Types T Dependency Types
s
rs
t
rt
verb
verb {purpcl, xcomp} ??
noun
{agent, nsubj, xsubj} ??
{dobj, nsubjpass, rel} ??
{tmod, prep in, prep at, prep on} ??
{iobj, prep to} ??
noun
verb {infmod, partmod, rcmod} ??
(a) noun {pos, nn, prep of, prep in, prep at, prep for} ??adjective {amod, rcmod} ??
s
rs
t
rt verb verb
{conj and} ??
{conj or} ??
{conj nor} ??
noun {dobj, nsubjpass, rel} {infmod, partmod, rcmod}
adjective {acomp} {cop, csubj}
noun noun
{conj and} ??
{conj or} ??
{conj nor} ??
adjective {amod, rcmod} {nsubj}
adjective adjective
{conj and} ??
{conj or} ??
(c) {conj nor} ??
adverb adverb
{conj and} ??
{conj or} ??
{conj nor} ??
Table 1: Equivalent dependency structures
Algorithm 1: depContext(S, T, i, j, EQ)
Input:
1. S, T : Sentences to be aligned
2. i: Index of a word in S
3. j: Index of a word in T
4. EQ: Dependency type equivalences (Table 1)
Output: context = {(k, l)}: pairs of word indexes
context? {(k, l) : wordSim(sk, tl) > 01
? (i, k, ?s) ? dependencies(S)2
? (j, l, ?t) ? dependencies(T )3
? POS(si) = POS(tj) ? POS(sk) = POS(tl)4
? (?s = ?t5
? (POS(si), POS(sk), ?s, ?t) ? EQ))}6
dencies is limited by the accuracy of the parser, and
b) we investigate equivalence types for only inter-
lexical-category alignment in this study. Therefore
we apply an additional model of word context: the
textual neighborhood of s in S and t in T .
Extraction of contextual evidence for content
words from textual neighborhood is described in Al-
gorithm 2. Like the dependency-based module, it
accumulates evidence for each (si, tj) pair by in-
specting multiple pairs of neighboring words. But in-
stead of aligning only words within a lexical category,
Algorithm 2: textContext(S, T, i, j, STOP)
Input:
1. S, T : Sentences to be aligned
2. i: Index of a word in S
3. j: Index of a word in T
4. STOP: A set of stop words
Output: context = {(k, l)}: pairs of word indexes
Ci ? {k : k ? [i? 3, i+ 3] ? k 6= i ? sk 6? STOP}1
Cj ? {l : l ? [j ? 3, j + 3] ? l 6= j ? tl 6? STOP}2
context? Ci ? Cj3
this module also performs inter-category alignment,
considering content words within a (3, 3) window
of si and tj as neighbors. We implement relational
equivalence (?) here by holding any two positions
within the window equally contributive and mutually
comparable as sources of contextual evidence.
3.3 The Alignment Algorithm
We now describe each alignment module in the
pipeline and their sequence of operation.
3.3.1 Identical Word Sequences
The presence of a common word sequence in S
and T is indicative of an (a) identical, and (b) con-
223
textually similar word in the other sentence for each
word in the sequence. We observe the results of
aligning identical words in such sequences of length
n containing at least one content word. This simple
heuristic demonstrates a high precision (? 97%) on
the MSR alignment dev set for n ? 2, and therefore
we consider membership in such sequences as the
simplest form of contextual evidence in our system
and align all identical word sequence pairs in S and
T containing at least one content word. From this
point on, we will refer to this module as wsAlign.
3.3.2 Named Entities
We align named entities separately to enable the
alignment of full and partial mentions (and acronyms)
of the same entity. We use the Stanford Named Entity
Recognizer (Finkel et al., 2005) to identify named
entities in S and T . After aligning the exact term
matches, any unmatched term of a partial mention
is aligned to all terms in the full mention. The mod-
ule recognizes only first-letter acronyms and aligns
an acronym to all terms in the full mention of the
corresponding name.
Since named entities are instances of nouns, named
entity alignment is also informed by contextual ev-
idence (which we discuss in the next section), but
happens before alignment of other generic content
words. Parents (or children) of a named entity are
simply the parents (or children) of its head word. We
will refer to this module as a method named neAlign
from this point on.
3.3.3 Content Words
Extraction of contextual evidence for promising
content word pairs has already been discussed in
Section 3.2, covering both dependency-based context
and textual context.
Algorithm 3 (cwDepAlign) describes the
dependency-based alignment process. For each
potentially alignable pair (si, tj), the dependency-
based context is extracted as described in Algorithm
1, and context similarity is calculated as the sum
of the word similarities of the (sk, tl) context word
pairs (lines 2-7). (The wordSim method returns a
similarity score in {0, ppdbSim, 1}.) The alignment
score of the (si, tj) pair is then a weighted sum
of word and contextual similarity (lines 8-12).
(We discuss how the weights are set in Section
Algorithm 3: cwDepAlign(S, T,EQ,AE , w, STOP)
Input:
1. S, T : Sentences to be aligned
2. EQ: Dependency type equivalences (Table 1)
3. AE : Already aligned word pair indexes
4. w: Weight of word similarity relative to contex-
tual similarity
5. STOP: A set of stop words
Output: A = {(i, j)}: word index pairs of aligned
words {(si, tj)} where si ? S and tj ? T
?? ?; ?? ? ?; ?? ?1
for si ? S, tj ? T do2
if si 6? STOP ? ??tl : (i, l) ? AE3
? tj 6? STOP ? ??sk : (k, j) ? AE4
? wordSim(si, tj) > 0 then5
context? depContext(S, T, i, j, EQ)6
contextSim?
?
(k,l)?context
wordSim(sk, tl)
7
if contextSim > 0 then8
?? ? ? {(i, j)}9
??(i, j)? context10
?(i, j)? w ? wordSim(si, tj)11
+(1? w) ? contextSim12
Linearize and sort ? in decreasing order of ?(i, j)13
A? ?14
for (i, j) ? ? do15
if ??l : (i, l) ? A16
???k : (k, j) ? A then17
A? A ? {(i, j)}18
for (k, l) ? ??(i, j) do19
if ??q : (k, q) ? A ?AE20
???p : (p, l) ? A ?AE then21
A? A ? {(k, l)}22
3.3.5.) The module then aligns (si, tj) pairs with
non-zero evidence in decreasing order of this score
(lines 13-18). In addition, it aligns all the pairs
that contributed contextual evidence for the (si, tj)
alignment (lines 19-22). Note that we implement a
one-to-one alignment whereby a word gets aligned
at most once within the module.
Algorithm 4 (cwTextAlign) presents alignment
based on similarities in the textual neighborhood. For
each potentially alignable pair (si, tj), Algorithm 2
is used to extract the context, which is a set of neigh-
boring content word pairs (lines 2-7). The contextual
similarity is the sum of the similarities of these pairs
224
Algorithm 4: cwTextAlign(S, T,AE , w, STOP)
Input:
1. S, T : Sentences to be aligned
2. AE : Existing alignments by word indexes
3. w: Weight of word similarity relative to contex-
tual similarity
4. STOP: A set of stop words
Output: A = {(i, j)}: word index pairs of aligned
words {(si, tj)} where si ? S and tj ? T
?? ?; ?? ?1
for si ? S, tj ? T do2
if si 6? STOP ? ??tl : (i, l) ? AE3
? tj 6? STOP ? ??sk : (k, j) ? AE4
? wordSim(si, tj) > 0 then5
?? ? ? {(i, j)}6
context? textContext(S, T, i, j, STOP)7
contextSim?
?
(k,l)?context
wordSim(sk, tl)
8
?(i, j)? w ? wordSim(si, tj)9
+ (1? w) ? contextSim10
Linearize and sort ? in decreasing order of ?(i, j)11
A? ?12
for (i, j) ? ? do13
if ??l : (i, l) ? A14
???k : (k, j) ? A then15
A? A ? {(i, j)}16
(line 8), and the alignment score is a weighted sum of
word similarity and contextual similarity (lines 9, 10).
The alignment score is then used to make one-to-one
word alignment decisions (lines 11-16). Considering
textual neighbors as weaker sources of evidence, we
do not align the neighbors.
Note that in cwTextAlign we also align semanti-
cally similar content word pairs (si, tj) with no con-
textual similarities if no pairs (sk, tj) or (si, tl) exist
with a higher alignment score. This is a consequence
of our observation of the MSR alignment dev data,
where we find that more often than not content words
are inherently sufficiently meaningful to be aligned
even in the absence of contextual evidence when
there are no competing pairs.
The content word alignment module is thus itself
a pipeline of cwDepAlign and cwTextAlign.
3.3.4 Stop Words
We follow the contextual evidence-based approach
to align stop words as well, some of which get aligned
Algorithm 5: align(S, T,EQ,w, STOP)
Input:
1. S, T : Sentences to be aligned
2. EQ: Dependency type equivalences (Table 1)
3. w: Weight of word similarity relative to contex-
tual similarity
4. STOP: A set of stop words
Output: A = {(i, j)}: word index pairs of aligned
words {(si, tj)} where si ? S and tj ? T
A? wsAlign(S, T )1
A? A ? neAlign(S, T,EQ,A,w)2
A? A ? cwDepAlign(S, T,EQ,A,w, STOP)3
A? A ? cwTextAlign(S, T,A,w, STOP)4
A? A ? swDepAlign(S, T,A,w, STOP)5
A? A ? swTextAlign(S, T,A,w, STOP)6
as part of word sequence alignment as discussed in
Section 3.3.1, and neighbor alignment as discussed
in Section 3.3.3. For the rest we utilize dependen-
cies and textual neighborhoods as before, with three
adjustments.
Firstly, since stop word alignment is the last mod-
ule in our pipeline, rather than considering all se-
mantically similar word pairs for contextual similar-
ity, we consider only aligned pairs. Secondly, since
many stop words (e.g. determiners, modals) typi-
cally demonstrate little variation in the dependencies
they engage in, we ignore type equivalences for stop
words and implement only exact matching of depen-
dencies. (Stop words in general can participate in
equivalent dependencies, but we leave constructing
a corresponding mapping for future work.) Finally,
for textual neighborhood, we separately check align-
ments of the left and the right neighbors and aggre-
gate the evidences to determine alignment ? again
due to the primarily syntactic nature of interaction of
stop words with their neighbors.
Thus stop words are also aligned in a sequence of
dependency and textual neighborhood-based align-
ments. We assume two corresponding modules
named swDepAlign and swTextAlign, respectively.
3.3.5 The Algorithm
Our full alignment pipeline is shown as the method
align in Algorithm 5. Note that the strict order of the
alignment modules limits the scope of downstream
modules since each such module discards any word
that has already been aligned by an earlier module
225
(this is accomplished via the variable A; the corre-
sponding parameter in Algorithms 3 and 4 is AE).
The rationales behind the specific order of the mod-
ules can now be explained: (1) wsAlign is a module
with relatively higher precision, (2) it is convenient to
align named entities before other content words to en-
able alignment of entity mentions of different lengths,
(3) dependency-based evidence was observed to be
more reliable (i.e. of higher precision) than textual
evidence in the MSR alignment dev set, and (4) stop
word alignments are dependent on existing content
word alignments.
The aligner assumes two free parameters:
ppdbSim and w (in Algorithms 3 and 4). To
determine their values, we exhaustively search
through the two-dimensional space (ppdbSim,w)
for ppdbSim,w ? {0.1, ..., 0.9, 1} and the combina-
tion (0.9, 0.9) yields the best F1 score for the MSR
alignment dev set. We use these values for the aligner
in all of its subsequent applications.
4 Evaluation
We evaluate the performance of our aligner both in-
trinsically and extrinsically on multiple corpora.
4.1 Intrinsic Evaluation
The MSR alignment dataset2 (Brockett, 2007) was
designed to train and directly evaluate automated
aligners. Three annotators individually aligned words
and phrases in 1600 pairs of premise and hypothe-
sis sentences from the RTE2 challenge data (divided
into dev and test sets, each consisting of 800 sen-
tences). The dataset has subsequently been used to
assess several top performing aligners (MacCartney
et al., 2008; Thadani and McKeown, 2011; Yao et
al., 2013a; Yao et al., 2013b). We use the test set for
evaluation in the same manner as these studies: (a)
we apply majority rule to select from the three sets
of annotations for each sentence and discard three-
way disagreements, (b) we evaluate only on the sure
links (word pairs that annotators mentioned should
certainly be aligned, as opposed to possible links).
We test the generalizability of the aligner by eval-
uating it, unchanged (i.e. with identical parameter
values), on a second alignment corpus: the Edin-
2http://www.cs.biu.ac.il/ nlp/files/RTE 2006 Aligned.zip
System P% R% F1% E%
MS
R
MacCartney et al. (2008) 85.4 85.3 85.3 21.3
Thadani & McKeown (2011) 89.5 86.2 87.8 33.0
Yao et al. (2013a) 93.7 84.0 88.6 35.3
Yao et al. (2013b) 92.1 82.8 86.8 29.1
This Work 93.7 89.8 91.7 43.8
ED
B+
+ Yao et al. (2013a) 91.3 82.0 86.4 15.0
Yao et al. (2013b) 90.4 81.9 85.9 13.7
This Work 93.5 82.5 87.6 18.3
Table 2: Results of intrinsic evaluation on two datasets
burgh++3 (Thadani et al., 2012) corpus. The test set
consists of 306 pairs; each pair is aligned by at most
two annotators and we adopt the random selection
policy described in (Thadani et al., 2012) to resolve
disagreements.
Table 2 shows the results. For each corpus, it
shows precision (% alignments that matched with
gold annotations), recall (% gold alignments discov-
ered by the aligner), F1 score and the percentage
of sentences that received the exact gold alignments
(denoted by E) from the aligner.
On the MSR test set, our aligner shows a 3.1%
improvement in F1 score over the previous best sys-
tem (Yao et al., 2013a) with a 27.2% error reduction.
Importantly, it demonstrates a considerable increase
in recall without a loss of precision. TheE score also
increases as a consequence.
On the Edinburgh++ test set, our system achieves a
1.2% increase in F1 score (an error reduction of 8.8%)
over the previous best system (Yao et al., 2013a),
with improvements in both precision and recall. This
is a remarkable result that demonstrates the general
applicability of the aligner, as no parameter tuning
took place.
4.1.1 Ablation Test
We perform a set of ablation tests to assess the
importance of the aligner?s individual components.
Each row of Table 3 beginning with (-) shows a fea-
ture excluded from the aligner and two associated
sets of metrics, showing the performance of the re-
sulting algorithm on the two alignment corpora.
Without a word similarity module, recall drops
as would be expected. Without contextual evidence
(word sequences, dependencies and textual neigh-
bors) precision drops considerably and recall also
falls. Without dependencies, the aligner still gives
3http://www.ling.ohio-state.edu/?scott/#edinburgh-plusplus
226
MSR EDB++
Feature P% R% F1% P% R% F1%
Original 93.7 89.8 91.7 93.5 82.5 87.6
(-) Word Similarity 95.2 86.3 90.5 95.1 77.3 85.3
(-) Contextual Evidence 81.3 86.0 83.6 86.4 80.6 83.4
(-) Dependencies 94.2 88.8 91.4 93.8 81.3 87.1
(-) Text Neighborhood 85.5 90.4 87.9 90.4 84.3 87.2
(-) Stop Words 94.2 88.3 91.2 92.2 80.0 85.7
Table 3: Ablation test results
state-of-the-art results, which points to the possibility
of a very fast yet high-performance aligner. Without
evidence from textual neighbors, however, the preci-
sion of the aligner suffers badly. Textual neighbors
find alignments across different lexical categories,
a type of alignment that is currently not supported
by our dependency equivalences. Extending the set
of dependency equivalences might alleviate this is-
sue. Finally, without stop words (i.e. while aligning
content words only) recall drops just a little for each
corpus, which is expected as content words form the
vast majority of non-identical word alignments.
4.2 Extrinsic Evaluation
We extrinsically evaluate our system on textual simi-
larity identification and paraphrase detection. Here
we discuss each task and the results of evaluation.
4.2.1 Semantic Textual Similarity
Given two short input text fragments (commonly
sentences), the goal of this task is to identify the
degree to which the two fragments are semantically
similar. The *SEM 2013 STS task (Agirre et al.,
2013) assessed a number of STS systems on four test
datasets by comparing their output scores to human
annotations. Pearson correlation coefficient with hu-
man annotations was computed individually for each
test set and a weighted sum of the correlations was
used as the final evaluation metric (the weight for
each dataset was proportional to its size).
We apply our aligner to the task by aligning each
sentence pair and taking the proportion of content
words aligned in the two sentences (by normalizing
with the harmonic mean of their number of content
words) as a proxy of their semantic similarity. Only
three of the four STS 2013 datasets are freely avail-
able4 (headlines, OnWN, and FNWN), which we use
for our experiments (leaving out the SMT dataset).
4http://ixa2.si.ehu.es/sts/
System Correl.% Rank
Han et al. (2013) 73.7 1 (original)
JacanaAlign 46.2 66
This Work 67.2 7
Table 4: Extrinsic evaluation on STS 2013 data
These three sets contain 1500 annotated sentence
pairs in total.
Table 4 shows the results. The first row shows the
performance of the top system in the task. With a
direct application of our aligner (no parameter tun-
ing), our STS algorithm acheives a 67.15% weighted
correlation, which would earn it the 7th rank among
90 participating systems. Considering the fact that
alignment is one of many components of STS, this
result is truly promising.
For comparison, we also evaluate the previous best
aligner named JacanaAlign (Yao et al., 2013a) on
STS 2013 data (the JacanaAlign public release5 is
used, which is a version of the original aligner with
extra lexical resources). We apply three different val-
ues derived from its output as proxies of semantic
similarity: a) aligned content word proportion, b) the
Viterbi decoding score, and c) the normalized decod-
ing score. Of the three, (b) gives the best results,
which we show in row 2 of Table 4. Our aligner
outperforms JacanaAlign by a large margin.
4.2.2 Paraphrase Identification
The goal of paraphrase identification is to decide if
two sentences have the same meaning. The output is
a yes/no decision instead of a real-valued similarity
score as in STS. We use the MSR paraphrase cor-
pus6 (4076 dev pairs, 1725 test pairs) (Dolan et al.,
2004) to evaluate our aligner and compare with other
aligners. Following earlier work (MacCartney et al.,
2008; Yao et al., 2013b), we use a normalized align-
ment score of the two sentences to make a decision
based on a threshold which we set using the dev set.
Alignments with a higher-than-threshold score are
taken to be paraphrases and the rest non-paraphrases.
Again, this is an oversimplified application of the
aligner, even more so than in STS, since a small
change in linguistic properties of two sentences
(e.g. polarity or modality) can turn them into non-
5https://code.google.com/p/jacana/
6http://research.microsoft.com/en-us/downloads/607d14d9-
20cd-47e3-85bc-a2f65cd28042/
227
System Acc.% P% R% F1%
Madnani et al. (2012) 77.4 79.0 89.9 84.1
Yao et al. (2013a) 70.0 72.6 88.1 79.6
Yao et al. (2013b) 68.1 68.6 95.8 79.9
This Work 73.4 76.6 86.4 81.2
Table 5: Extrinsic evaluation on MSR paraphrase data
paraphrases despite having a high degree of align-
ment. So the aligner was not expected to demonstrate
state-of-the-art performance, but still it gets close as
shown in Table 5. The first column shows the accu-
racy of each system in classifying the input sentences
into one of two classes: true (paraphrases) and false
(non-paraphrases). The rest of the columns show the
performance of the system for the true class in terms
of precision, recall, and F1 score. Italicized numbers
represent scores that were not reported by the authors
of the corresponding papers, but have been recon-
structed from the reported data (and hence are likely
to have small precision errors).
The first row shows the best performance by any
system on the test set to the best of our knowledge.
The next two rows show the performances of two
state-of-the-art aligners (performances of both sys-
tems were reported in (Yao et al., 2013b)). The
last row shows the performance of our aligner. Al-
though it does worse than the best paraphrase system,
it outperforms the other aligners.
5 Discussion
Our experiments reveal that a word aligner based on
simple measures of lexical and contextual similar-
ity can demonstrate state-of-the-art accuracy. How-
ever, as alignment is frequently a component of larger
tasks, high accuracy alone is not always sufficient.
Other dimensions of an aligner?s usability include
speed, consumption of computing resources, replica-
bility, and generalizability to different applications.
Our design goals include achieving a balance among
such multifarious and conflicting goals.
A speed advantage of our aligner stems from for-
mulating the problem as one-to-one word alignment
and thus avoiding an expensive decoding phase. The
presence of multiple phases is offset by discarding
already aligned words in subsequent phases. The
use of PPDB as the only (hashable) word similarity
resource helps in reducing latency as well as space
requirements. As shown in Section 4.1.1, further
speedup could be achieved with only a small perfor-
mance degradation by considering only the textual
neighborhood as source of contextual evidence.
However, the two major goals that we believe the
aligner achieves to the greatest extent are replicabil-
ity and generalizability. The easy replicability of
the aligner stems from its use of only basic and fre-
quently used NLP modules (a lemmatizer, a POS
tagger, an NER module, and a dependency parser: all
available as part of the Stanford CoreNLP suite7; we
use a Python wrapper8) and a single word similarity
resource (PPDB).
We experimentally show that the aligner can be
successfully applied to different alignment datasets
as well as multiple end tasks. We believe a design
characteristic that enhances the generalizability of
the aligner is its minimal dependence on the MSR
alignment training data, which originates from a tex-
tual entailment corpus having unique properties such
as disparities in the lengths of the input sentences
and a directional nature of their relationship (i.e.,
the premise implying the hypothesis, but not vice
versa). A related potential reason is the symmetry
of the aligner?s output (caused by its assumption of
no directionality) ? the fact that it outputs the same
set of alignments regardless of the order of the input
sentences, in contrast to most existing aligners.
Major limitations of the aligner include the inabil-
ity to align phrases, including multiword expressions.
It is incapable of capturing and exploiting long dis-
tance dependencies among words (e.g. coreferences).
No word similarity resource is perfect and PPDB is
no exception, therefore certain word alignments also
remain undetected.
6 Conclusions
We show how contextual evidence can be used to
construct a monolingual word aligner with certain de-
sired properties, including state-of-the-art accuracy,
easy replicability, and high generalizability. Some
potential avenues for future work include: allow-
ing phrase-level alignment via phrasal similarity re-
sources (e.g. the phrasal paraphrases of PPDB), in-
cluding other sources of similarity such as vector
space models or WordNet relations, expanding the set
7http://nlp.stanford.edu/downloads/corenlp.shtml
8https://github.com/dasmith/stanford-corenlp-python
228
of dependency equivalences and/or using semantic
role equivalences, and formulating our alignment al-
gorithm as objective optimization rather than greedy
search.
The aligner is available for download at
https://github.com/ma-sultan/
monolingual-word-aligner.
Acknowledgments
This material is based in part upon work supported by
the National Science Foundation under Grant Num-
bers EHR/0835393 and EHR/0835381. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics. Association for Computational
Linguistics, 32-43.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics. Association for Computa-
tional Linguistics, 597-604.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics. Association for
Computational Linguistics, 435-440.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of The Second
PASCAL Recognising Textual Entailment Challenge.
Chris Brockett. 2007. Aligning the RTE 2006 Cor-
pus. Technical Report MSR-TR-2007-77, Microsoft
Research.
Nathanael Chambers, Daniel Cer, Trond Grenager, David
Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine
de Marneffe, Daniel Ramage, Eric Yeh, and Christopher
D Manning. 2007. Learning alignments and leverag-
ing natural logic. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing As-
sociation for Computational Linguistics, 165-170.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative Learning over Con-
strained Latent Representations. In Proceedings of the
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
Association for Computational Linguistics, 429-437.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase Iden-
tication as Probabilistic Quasi-Synchronous Recogni-
tion. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP. Association for Computational Linguistics,
468-476.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation. 449-454.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical Report, Stanford University.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised Construction of Large Paraphrase Corpora:
Exploiting Massively Parallel News Sources. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics. Association for Computational
Linguistics, 350-356.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating Non-local Information into
Information Extraction Systems by Gibbs Sampling. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, 363-370.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics. Association for Computational
Linguistics, 758-764.
Lushan Han, Abhay Kashyap, Tim Finin, James Mayeld,
and Jonathan Weese. 2013. UMBC EBIQUITY-CORE:
Semantic Textual Similarity Systems. In Proceedings
of the Second Joint Conference on Lexical and Compu-
tational Semantics, Volume 1. Association for Compu-
tational Linguistics, 44-52.
Andrew Hickl and Jeremy Bensley. 2007. A Discourse
Commitment-Based Framework for Recognizing Tex-
tual Entailment. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing. As-
sociation for Computational Linguistics, 171-176.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCCs GROUNDHOG
229
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the PASCAL Challenges
Workshop: Recognising Textual Entailment Challenge
17-20.
Bill MacCartney, Michel Galley, and Christopher D. Man-
ning. 2008. A Phrase-Based Alignment Model for Nat-
ural Language Inference. In Proceedings of the 2008
Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics,
802-811.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining Machine Translation Metrics for
Paraphrase Identification. In Proceedings of 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, 182-190.
Kapil Thadani and Kathleen McKeown. 2011. Optimal
and Syntactically-Informed Decoding for Monolingual
Phrase-Based Alignment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies. Associa-
tion for Computational Linguistics, 254-259.
Kapil Thadani, Scott Martin, and Michael White. 2012.
A Joint Phrasal and Dependency Model for Paraphrase
Alignment. In Proceedings of COLING 2012: Posters.
1229-1238.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003 Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics. Association for
Computational Linguistics, 173-180.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,
and Peter Clark. 2013a. A Lightweight and High Per-
formance Monolingual Word Aligner. In Proceedings
of the 51st Annual Meeting of the Association for Com-
putational Linguistics. Association for Computational
Linguistics, 702-707.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,
and Peter Clark. 2013b. Semi-Markov Phrase-based
Monolingual Alignment. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics,
590-600.
Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,
and Peter Clark. 2013c. Answer Extraction as Se-
quence Tagging with Tree Edit Distance. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics. Association for Computational Linguistics, 858-
867.
230
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 176?180, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DLS@CU-CORE: A Simple Machine Learning Model of Semantic    
Textual Similarity 
Md. Arafat Sultan, Steven Bethard, Tamara Sumner 
Institute of Cognitive Science and Department of Computer Science  
University of Colorado, Boulder, CO 80309 
{arafat.sultan, steven.bethard, sumner}@colorado.edu 
   
Abstract 
We present a system submitted in the Semantic 
Textual Similarity (STS) task at the Second 
Joint Conference on Lexical and Computa-
tional Semantics (*SEM 2013). Given two 
short text fragments, the goal of the system is 
to determine their semantic similarity. Our sys-
tem makes use of three different measures of 
text similarity: word n-gram overlap, character 
n-gram overlap and semantic overlap. Using 
these measures as features, it trains a support 
vector regression model on SemEval STS 2012 
data. This model is then applied on the STS 
2013 data to compute textual similarities. Two 
different selections of training data result in 
very different performance levels: while a cor-
relation of 0.4135 with gold standards was ob-
served in the official evaluation (ranked 63rd 
among all systems) for one selection, the other 
resulted in a correlation of 0.5352 (that would 
rank 21st).  
1 Introduction 
Automatically identifying the semantic similarity 
between two short text fragments (e.g. sentences) is 
an important research problem having many im-
portant applications in natural language processing, 
information retrieval, and digital education. Exam-
ples include automatic text summarization, question 
answering, essay grading, among others.  
   However, despite having important applications, 
semantic similarity identification at the level of 
short text fragments is a relatively recent area of in-
vestigation. The problem was formally brought to 
attention and the first solutions were proposed in 
2006 with the works reported in (Mihalcea et al, 
2006) and (Li et al, 2006). Work prior to these fo-
cused primarily on large documents (or individual 
words) (Mihalcea et al, 2006). But the sentence-
level granularity of the problem is characterized by 
factors like high specificity and low topicality of the 
expressed information, and potentially small lexical 
overlap even between very similar texts, asking for 
an approach different from those that were designed 
for larger texts. 
Since its inception, the problem has seen a large 
number of solutions in a relatively small amount of 
time. The central idea behind most solutions is the 
identification and alignment of semantically similar 
or related words across the two sentences, and the 
aggregation of these similarities to generate an over-
all similarity score (Mihalcea et al, 2006; Islam and 
Inkpen, 2008; ?ari? et al, 2012). 
The Semantic Textual Similarity task (STS) or-
ganized as part of the Semantic Evaluation Exer-
cises (see (Agirre et al, 2012) for a description of 
STS 2012) provides a common platform for evalua-
tion of such systems via comparison with human-
annotated similarity scores over a large dataset.  
In this paper, we present a system which was 
submitted in STS 2013. Our system is based on very 
simple measures of lexical and character-level over-
lap, semantic overlap between the two sentences 
based on word relatedness measures, and surface 
features like the sentences? lengths. These measures 
are used as features for a support vector regression 
model that we train with annotated data from 
SemEval STS 2012. Finally, the trained model is ap-
plied on the STS 2013 test pairs. 
Our approach is inspired by the success of simi-
lar systems in STS 2012: systems that combine mul-
tiple measures of similarity using a machine learn-
ing model to generate an overall score (B?r et al, 
2012; ?ari? et al, 2012). We wanted to investigate 
how a minimal system of this kind, making use of 
very few external resources, performs on a large da-
taset. Our experiments reveal that the performance 
of such a system depends highly on the training 
data. While training on one dataset yielded a best 
176
correlation (among our three runs, described later in 
this document) of only 0.4135 with the gold scores, 
training on another dataset showed a considerably 
higher correlation of 0.5352.  
2 Computation of Text Similarity: System 
Overview 
In this section, we present a high-level description 
of our system. More details on extraction of some of 
the measures of similarity are provided in Section 3. 
Given two input sentences ?1 and ?2, our algo-
rithm can be described as follows: 
1. Compute semantic overlap (8 features): 
a. Lemmatize ?1 and ?2 using a memory-
based lemmatizer1 and remove all stop 
words. 
b. Compute the degree to which the concepts 
in ?1 are covered by semantically similar 
concepts in ?2 and vice versa (see Section 3 
for details). The result of this step is two dif-
ferent ?degree of containment? values (?1 in 
?2 and vice versa). 
c. Compute the minimum, maximum, arith-
metic mean and harmonic mean of the two 
values to use as features in the machine 
learning model. 
d. Repeat steps 1a through 1c for a weighted 
version of semantic overlap where each 
word in the first sentence is assigned a 
weight which is proportional to its specific-
ity in a selected corpus (see Section 3). 
2. Compute word ?-gram overlap (16 features): 
a. Extract ?-grams (for ? = 1, 2, 3, 4) of all 
words in ?1 and ?2 for four different setups 
characterized by the four different value 
combinations of the two following varia-
bles: lemmatization (on and off), stop-
WordsRemoved (on and off). 
b. Compute the four measures (min, max, 
arithmetic and harmonic mean) for each 
value of n. 
3. Compute character ?-gram overlap (16 fea-
tures):  
a. Repeat   all steps in 2 above for character ?-
grams (? = 2, 3, 4, 5). 
                                                          
1 http://www.clips.ua.ac.be/pages/MBSP#lemmatizer 
2 http://conceptnet5.media.mit.edu/data/5.1/as-
soc/c/en/cat? filter=/c/en/dog&limit=1 
4. Compute sentence length features (2 features): 
a. Compute the lengths of ?1 and ?2; and the 
minimum and maximum of the two values. 
b. Include the ratio of the maximum to the min-
imum and the difference between the maxi-
mum and minimum in the feature set. 
5. Train a support vector regression model on the 
features extracted in steps 1 through 4 above us-
ing data from SemEval 2012 STS (see Section 
4 for specifics on the dataset). We used the 
LibSVM implementation of SVR in WEKA. 
6. Apply the model on STS 2013 test data. 
3 Semantic Overlap Measures 
In this section, we describe the computation of the 
two sets of semantic overlap measures mentioned in 
step 1 of the algorithm in Section 2. 
We compute semantic overlap between two sen-
tences by first computing the semantic relatedness 
among their constituent words. Automatically com-
puting the semantic relatedness between words is a 
well-studied problem and many solutions to the 
problem have been proposed. We compute word re-
latedness in two forms: semantic relatedness and 
string similarity. For semantic relatedness, we uti-
lize two web services. The first one concerns a re-
source named ConceptNet (Liu and Singh, 2004), 
which holds a large amount of common sense 
knowledge concerning relationships between real-
world entities. It provides a web service2 that gener-
ates word relatedness scores based on these relation-
ships. We will use the term ?????(?1, ?2) to de-
note the relatedness of the two words ?1 and ?2 as 
generated by ConceptNet. 
We also used the web service3 provided by an-
other resource named Wikipedia Miner (Milne and 
Witten, 2013). While ConceptNet successfully cap-
tures common sense knowledge about words and 
concepts, Wikipedia Miner specializes in identify-
ing relationships between scientific concepts pow-
ered by Wikipedia's vast repository of scientific in-
formation (for example, Einstein and relativity). We 
will use the term ?????(?1, ?2) to denote the re-
latedness of the two words ?1 and ?2 as generated 
by Wikipedia Miner. Using two systems enabled us 
3 http://wikipedia-miner.cms.waikato.ac.nz/ser-
vices/compare?  term1=cat&term2=dog 
177
to increase the coverage of our word similarity com-
putation algorithm.  
Each of these web services return a score in the 
range [0, 1] where 0 represents no relatedness and 1 
represents complete similarity. A manual inspection 
of both services indicates that in almost all cases 
where the services? word similarity scores deviate 
from what would be the human-perceived similar-
ity, they generate lower scores (i.e. lower than the 
human-perceived score). This is why we take the 
maximum of the two services? similarity scores for 
any given word pair as their semantic relatedness: 
??????(?1, ?2) 
= max?{?????(?1, ?2),?????(?1, ?2)} 
We also compute the string similarity between 
the two words by taking a weighted combination of 
the normalized lengths of their longest common 
substring, subsequence and prefix (normalization is 
done for each of the three by dividing its length with 
the length of the smaller word). We will refer to the 
string similarity between words ?1 and ?2 as 
?????????(??1, ?2). This idea is taken from (Islam 
and Inkpen, 2008); the rationale is to be able to find 
the similarity between (1) words that have the same 
lemma but the lemmatizer failed to lemmatize at 
least one of the two surface forms successfully, and 
(2) words at least one of which has been misspelled. 
We take the maximum of the string similarity and 
the semantic relatedness between two words as the 
final measure of their similarity: 
???(?1, ?2) 
= max?{??????(?1, ?2), ?????????(?1, ?2)} 
At the sentence level, our first set of semantic 
overlap measures (step 1b) is an unweighted meas-
ure that treats all content words equally. More spe-
cifically, after the preprocessing in step 1a of the al-
gorithm, we compute the degree of semantic cover-
age of concepts expressed by individual content 
words in ?1 by ?2 using the following equation: 
?????(?1, ?2) =
? [max
???2
{???(?, ?)}]???1
|?1|
 
                                                          
4 http://googleresearch.blogspot.com/2006/08/all-our-n-
gram-are-belong-to-you.html 
where ???(?, ?) is the similarity between the two 
lemmas ? and ?. 
We also compute a weighted version of semantic 
coverage (step 1d in the algorithm) by incorporating 
the specificity of each word (measured by its infor-
mation content) as shown in the equation below:  
????(?1, ?2) =
? [max
???2
{??(?). ???(?, ?)}]???1
|?1|
 
where ??(?) stands for the information content of 
the word ?. Less common words (across a selected 
corpus) have high information content: 
??(?) = ln
? ?(??)????
?(?)
 
where C is the set of all words in the chosen corpus 
and f(w) is the frequency of the word w in the cor-
pus. We have used the Google Unigram Corpus4 to 
assign the required frequencies to these words. 
4 Evaluation 
The STS 2013 test data consists of four datasets: 
two datasets consisting of gloss pairs (OnWN: 561 
pairs and FNWN: 189 pairs), a dataset of machine 
translation evaluation pairs (SMT: 750 pairs) and a 
dataset consisting of news headlines (headlines: 750 
pairs). For each dataset, the output of a system is 
evaluated via comparison with human-annotated 
similarity scores and measured using the Pearson 
Correlation Coefficient. Then a weighted sum of the 
correlations for all datasets are taken to be the final 
score, where each dataset?s weight is the proportion 
of sentence pairs in that dataset. 
We computed the similarity scores using three 
different feature sets (for our three runs) for the sup-
port vector regression model: 
1. All features mentioned in Section 2. This set of 
features were used in our run 1. 
2. All features except word ?-gram overlap (ex-
periments on STS 2012 test data revealed that 
using word n-grams actually lowers the perfor-
mance of our model, hence this decision). These 
are the features that were used in our run 2. 
3. Only character ?-gram and length features (just 
to test the performance of the model without 
178
any semantic features). Our run 3 was based on 
these features. 
We trained the support vector regression model 
on two different training datasets, both drawn from 
STS 2012 data: 
1. In the first setup, we chose the training datasets 
from STS 2012 that we considered the most 
similar to the test dataset. The only exception 
was the FNWN dataset, for which we selected 
the all the datasets from 2012 because no single 
dataset from STS 2012 seemed to have similar-
ity with this dataset. For the OnWN test dataset, 
we selected the OnWN dataset from STS 2012. 
For both headlines and SMT, we selected SMT-
news and SMTeuroparl from STS 2012. The ra-
tionale behind this selection was to train the ma-
chine learning model on a distribution similar to 
the test data. 
2. In the second setup, we aggregated all datasets 
(train and test) from STS 2012 and used this 
combined dataset to train the three models that 
were later applied on each STS 2013 test data. 
Here the rationale is to train on as much data as 
possible. 
Table 1 shows the results for the first setup. This 
is the performance of the set of scores which we ac-
tually submitted in STS 2013. The first four col-
umns show the correlations of our system with the 
gold standard for all runs. The rightmost column 
shows the overall weighted correlations. As we can 
see, run 1 with all the features demonstrated the best 
performance among the three runs. There was a con-
siderable drop in performance in run 3 which did not 
utilize any semantic similarity measure. 
Table 1. Results for manually selected training data 
Run headlines OnWN FNWN SMT Total 
1 .4921 .3769 .4647 .3492 .4135 
2 .4669 .4165 .3859 .3411 .4056 
3 .3867 .2386 .3726 .3337 .3309 
As evident from the table, evaluation results did 
not indicate a particularly promising system. Our 
best system ranked 63rd among the 90 systems eval-
uated in STS 2013. We further investigated to find 
out the reason: is the set of our features insufficient 
to capture text semantic similarity, or were the train-
ing data inappropriate for their corresponding test 
data? This is why we experimented with the second 
setup discussed above. Following are the results:  
Table 2. Results for combined training data 
Run headlines OnWN FNWN SMT Total 
1 .6854 .5981 .4647 .3518 .5339 
2 .7141 .5953 .3859 .349 .5352 
3 .6998 .4826 .3726 .3365 .4971 
As we can see in Table 2, the correlations for all 
feature sets improved by more than 10% for each 
run. In this case, the best system with correlation 
0.5352 would rank 21st among all systems in STS 
2013. These results indicate that the primary reason 
behind the system?s previous bad performance (Ta-
ble 1) was the selection of an inappropriate dataset. 
Although it was not clear in the beginning which of 
the two options would be the better, this second ex-
periment reveals that selecting the largest possible 
dataset to train is the better choice for this dataset. 
5 Conclusions 
In this paper, we have shown how simple measures 
of text similarity using minimal external resources 
can be used in a machine learning setup to compute 
semantic similarity between short text fragments. 
One important finding is that more training data, 
even when drawn from annotations on different 
sources of text and thus potentially having different 
feature value distributions, improve the accuracy of 
the model in the task. Possible future expansion in-
cludes use of more robust concept alignment strate-
gies using semantic role labeling, inclusion of struc-
tural similarities of the sentences (e.g. word order, 
syntax) in the feature set, incorporating word sense 
disambiguation and more robust strategies of con-
cept weighting into the process, among others. 
References 
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: a pilot on se-
mantic textual similarity. In Proceedings of the First 
Joint Conference on Lexical and Computational Se-
mantics. ACL, Stroudsburg, PA, USA, 385-393. 
Daniel B?r, Chris Biemann, Iryna Gurevych, and Torsten 
Zesch. 2012. UKP: computing semantic textual simi-
larity by combining multiple content similarity 
measures. In Proceedings of the First Joint Confer-
ence on Lexical and Computational Semantics. ACL, 
Stroudsburg, PA, USA, 435-440. 
Aminul Islam and Diana Inkpen. 2008. Semantic text 
similarity using corpus-based word similarity and 
string similarity.  ACM Trans. Knowl. Discov. Data 2, 
2, Article 10 (July 2008), 25 pages. 
179
Yuhua Li, David Mclean, Zuhair A. Bandar, James D. 
O?Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE 
Transactions on Knowledge and Data Engineering, 
vol.18, no.8, 1138-1150. 
Hugo Liu and Push Singh. 2004. ConceptNet ? a prac-
tical commonsense reasoning tool-kit. BT Technology 
Journal 22, 4 (October 2004), 211-226. 
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 
2006. Corpus-based and knowledge-based measures 
of text semantic similarity. In Proceedings of the 21st 
national conference on Artificial intelligence - Volume 
1 (AAAI'06), Anthony Cohn (Ed.), Vol. 1. AAAI 
Press 775-780. 
David Milne and Ian H. Witten. 2013. An open-source 
toolkit for mining Wikipedia. Artif. Intell. 194 (Janu-
ary 2013), 222-239. 
Frane ?ari?, Goran Glava?, Mladen Karan, Jan ?najder, 
and Bojana Dalbelo Ba?i?.?ari?. 2012. TakeLab: sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the First Joint Conference on Lexical and 
Computational Semantics. ACL, Stroudsburg, PA, 
USA, 441-448.  
180
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 603?607, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CU : Computational Assessment of Short Free Text Answers - A Tool for
Evaluating Students? Understanding
Ifeyinwa Okoye
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
okoye@colorado.edu
Steven Bethard
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
bethard@colorado.edu
Tamara Sumner
Institute of Cognitive Science
Dept. of Computer Science
University of Colorado
Boulder, CO 80309, USA
sumner@colorado.edu
Abstract
Assessing student understanding by evaluat-
ing their free text answers to posed questions
is a very important task. However, manually,
it is time-consuming and computationally, it is
difficult. This paper details our shallow NLP
approach to computationally assessing student
free text answers when a reference answer is
provided. For four out of the five test sets, our
system achieved an overall accuracy above the
median and mean.
1 Introduction
Assessing student understanding is one of the holy
grails of education (Redecker et al, 2012). If we
(teachers, tutors, intelligent tutors, potential employ-
ers, parents and school administrators) know what
and how much a student knows, then we know what
the student still needs to learn. And then, can ef-
ficiently and effectively educate the student. How-
ever, the task of assessing what exactly a student un-
derstands about a particular topic can be expensive,
difficult and subjective.
Using multiple choice questionnaires is one of
the most prevalent forms of assessing student under-
standing because it is easy and fast, both manually
and computationally. However there has been a lot
of pushback from educators about the validity of re-
sults gotten from multiple choice questionnaires.
Assessing student understanding by evaluating
student free text answers either written or spoken is
one of the preferred alternatives to multiple choice
questionnaires. As an assessment tool, free text an-
swers can illuminate what and how much a student
knows since the student is forced to recall terms and
make connections between those terms rather than
just picking one out of several options. However,
assessing free text answers manually is tedious, ex-
pensive and time-consuming, hence the search for a
computational option.
There are three main issues that can limit the com-
putational approach and corresponding performance
when assessing free text answers: (1) the unit of
assessment, (2) the reference and (3) the level of
assessment. The unit of assessment can be words,
facets, phrases, sentences, short answers or essays.
The reference is the correct answer and what is
being compared to the student answer. Most re-
searchers generate the reference manually (Noorbe-
hbahani and Kardan, 2011; Graesser et al, 2004) but
some have focused on automatically generating the
reference (Ahmad, 2009). The level of assessment
can be coarse with 2 categories such as correct and
incorrect or more finer-grained with up to 19 cate-
gories as in (Ahmad, 2009). In general, the finer-
grained assessments are more difficult to assess.
2 The Student Response Analysis Task
The student response analysis task was posed as fol-
lows: Given a question, a known correct/reference
answer and a 1 or 2 sentence student answer, classify
the student answer into two, three or five categories.
The two categories were correct and incorrect; the
three categories were correct, contradictory and in-
correct; while the five categories were correct, par-
tially correct but incomplete, contradictory, irrele-
vant and not in the domain (Dzikovska et al, 2013).
We chose to work on the 2-way response task only
603
because for our application, we need to simply know
if a student answer is correct or incorrect. Our ap-
plication is an interactive essay-based personalized
learning environment (Bethard et al, 2012).
The overarching goal of our application is to cre-
ate a scalable online service that recommends re-
sources to users based on the their conceptual under-
standing expressed in an essay or short answer form.
Our application automatically constructs a domain
knowledge base from digital library resources and
identifies the core concepts in the domain knowl-
edge base.It detects flaws and gaps in users? sci-
ence knowledge and recommends digital library re-
sources to address users? misconceptions and knowl-
edge gaps. The gaps are detected by identifying the
core concepts which the user has not discussed. The
flaws (incorrect understanding/misconceptions) are
currently being identified by a process of (1) seg-
menting a student essay into sentences, (2) align-
ing the student sentence to a sentence in the domain
knowledge base and (3) using the system we devel-
oped for the student response analysis task to deter-
mine if the student sentence is correct or incorrect.
The development of our misconception detection
algorithm has been limited by the alignment task.
However, with the data set from the student response
analysis task containing correct alignments, we hope
to be able to use it to make improvements to our
misconception detection algorithm. We discuss our
current misconception detection system below.
3 System Description
Our system mainly exploits shallow NLP tech-
niques, in particular text overlap, to see how much
we can gain from using a simple system and how
much more some more semantic features could add
to the simple system. Although we have access to
the question which a 1-2 sentence student answer
corresponds to, we chose not to use that in our sys-
tem because in our application we do not have ac-
cess to that information. We were trying to build a
system that would work in our current essay-based
application.
Some of the student answers in the dataset have a
particular reference answer which they match. How-
ever, we do not make use of this information in our
system either. We assume that for a particular ques-
tion, all the corresponding reference answers can be
used to determine the correctness of any of the stu-
dent answers.
3.1 Features
The features we use are:
1. CosineSimilarity : This is the average cosine
similarity (Jurafsky and James, 2000) between
a student answer vector and all the correspond-
ing reference answer vectors. The vectors are
based on word counts. The words were low-
ercased and included stopwords and punctua-
tions.
2. CosineSimilarityNormalized : This is the av-
erage cosine similarity between a student an-
swer vector and all the corresponding reference
answer vectors, with the word counts within
the vectors divided by the word counts in Gi-
gaword, a background corpus. We divided
the raw counts by the counts in Gigaword to
ensure that punctuations, stopwords and other
non-discriminatory words do not artificially in-
crease the cosine similarity.
3. UnigramRefStudent : This is the average un-
igram coverage of the reference answers by a
student answer. To calculate this, the student
answer and all the corresponding reference an-
swers are tokenized into unigrams. Next, for
each reference answer, we count the number of
unigrams in the reference answer that are con-
tained in the student answer and divide it by the
number of unigrams in the reference answer.
The value we get for this feature, is the aver-
age over all the reference answers.
4. UnigramStudentRef : This is the average uni-
gram coverage of the student answer by the ref-
erence answers. To calculate this, the student
answer and all the corresponding reference an-
swers are tokenized into unigrams. Next, for
each reference answer, we count the number
of unigrams in the student answer that are con-
tained in the reference answer and divide it by
the number of unigrams in the student answer.
The value we get for this feature, is the average
over all the reference answers.
604
5. BigramRefStudent : This is similar to the Un-
igramRefStudent feature, but using bigrams.
6. BigramStudentRef : This is similar to the Un-
igramStudentRef feature, but using bigrams.
7. LemmaRefStudent : This is similar to the Un-
igramRefStudent feature, but in this case, the
lemmas are used in place of words.
8. LemmaStudentRef : This is similar to the Un-
igramStudentRef feature, but in this case, the
lemmas are used in place of words.
9. UnigramPosRefStudent : This is similar to
the UnigramRefStudent feature, but we use
part-of-speech unigrams for this feature in
place of word unigrams.
10. UnigramPosStudentRef : This is similar to
the UnigramStudentRef feature, but we use
part-of-speech unigrams for this feature in
place of word unigrams.
11. BigramPosRefStudent : This is similar to the
BigramRefStudent feature, but we use part-of-
speech bigrams for this feature in place of word
unigrams.
12. BigramPosStudentRef : This is similar to the
BigramStudentRef feature, but we use part-of-
speech bigrams for this feature in place of word
unigrams.
3.2 Implementation
We used the ClearTK (Ogren et al, 2008) toolkit
within Eclipse to extract features from the student
and reference sentences. We trained a LibSVM
(Chang and Lin, 2011) binary classifier to classify a
feature vector into two classes, correct or incorrect.
We used the default parameters for LibSVM except
for the cost parameter, for which we tried different
values. However, the default value of 1 gave us the
best result on the training set. Our two runs/systems
are essentially the same system but with a cost pa-
rameter of 1 and 10.
4 Results
The Student Response Analysis Task overall re-
sult can be found in the Task description paper
(Dzikovska et al, 2013). The CU system achieved
a ranking of above the mean and median for four
of the five different test sets. We perfomed below
the mean and median on the sciEntsBank unseen an-
swers. The accuracy result for the test data is shown
in Table 4. The results on our training data and
a breakdown of the contribution of each feature is
shown in Table 5. In Table 5 ALL refers to all the
features while ALL-CosineSimilarity is all the fea-
tures excluding the CosineSimilarity feature.
Sys
tem
beetle
un-
seen
an-
swers
beetle
un-
seen
ques-
tions
sciEnts
Bank
un-
seen
an-
swers
sciEnts
Bank
un-
seen
ques-
tions
sciEnts
Bank
un-
seen
do-
mains
CU
run
1
0.786 0.718 0.656 0.674 0.693
CU
run
2
0.784 0.717 0.654 0.671 0.691
Table 1: Overall Accuracy results for CU system on the
test Data
5 Discussion
As can be seen from Table 4 and further elaborated
on in (Dzikovska et al, 2013), there were two main
datasets, Beetle and SciEntsBank. The Beetle data
set has multiple reference answer per question while
the SciEntsBank has one reference answer per ques-
tion. Our system did better on the beetle data set
than the SciEntsBank data set, both during devel-
opment and on the final test sets. This leads us to
believe that our system will do well when there are
multiple reference answers rather than just one.
We analyzed the training data to understand
where our system was failing and what we could do
to make it better. We tried removing stopwords be-
fore constructing the feature vectors but that made
the results worse. Here are two examples where re-
moving the stopwords will make it impossible to as-
certain the validity of the student answer:
? It was connected. becomes connected
605
? It will work because that is closing the switch.
becomes work closing switch
Because the student answers are free text and use
pronouns in place of the nouns that were in the ques-
tion, the stop words are important to provide context.
Feature Type Beetle
& sci-
Ents
Bank
1 ALL 0.703
2 ALL - CosineSimilarity 0.702
3 ALL - CosineSimilari-
tyNormalized
0.700
4 ALL - UnigramRefStudent 0.702
5 ALL - UnigramStudentRef 0.701
6 ALL - BigramRefStudent 0.702
7 ALL - BigramStudentRef 0.699
8 ALL - LemmaRefStudent 0.701
9 ALL - LemmaStudentRef 0.700
10 ALL - UnigramPosRefStu-
dent
0.703
11 ALL - UnigramPosStuden-
tRef
0.703
12 ALL - BigramPosRefStu-
dent
0.702
13 ALL - BigramPosStuden-
tRef
0.702
Table 2: Accuracy results for 5X cross validation on the
training data
Currently, we are working on extracting and
adding several features that we did not use for the
task due to time constraints, to see if they improve
our result. Some of the things we are working on
are:
1. Resolving Coreference
We will use the current state-of-art coreference
system and assume that the question precedes
the student answer in a paragraph when resolv-
ing coreference.
2. Compare main predicates
The question is how to assign a value to the se-
mantic similarity between the main predicates.
If the predicates are separate and connect, then
there should be a way to indicate that the men-
tion of one of them in the reference, precludes
the validity of the student answer being correct
if it mentions the other. However, we also have
to take negation into account here. not sepa-
rated and connected should be marked as very
similar if not equal. We plan to include the al-
gorithm from the best system in the semantic
similarity task to our current system.
3. Compare main subject and object from a
syntactic parse or the numbered arguments
in semantic role label arguments
We have to resolve coreference for this to work
well. And again, we run into the problem of
how to assign a semantic similarity value to two
words that might not share the same synset in
ontologies such as Wordnet.
4. Optimize parameters and explore other clas-
sifiers Throughout developing and testing our
system, we used only the LibSVM classifier
and only optimized the cost parameter. How-
ever, there might be a different classifier or
set of options that can model the data better.
We hope to run through most of the classifiers
available and see if using a different one, with
different options improves our accuracy.
6 Conclusion
We have shown that there is value in using shallow
NLP features to judge the validity of free answer text
when the reference answers are given. However,
looking at the sentences that our system labeled as
correct and the gold standard incorrect or vice versa,
it is clear that we have to delve into more seman-
tic features if we want our system to be more accu-
rate. We hope to keep working on this task in sub-
sequent years to ensure continuous improvements in
systems that can assess student knowledge by eval-
uating free answer texts. Such systems will be able
to give students the formative feedback they need
to help them learn better. In addition, such systems
will provide teachers, intelligent tutors and adminis-
trators with feedback about student knowledge, so as
to help them adapt their curriculum, teaching and tu-
toring methods to better serve students? knowledge
needs.
606
References
Faisal Ahmad. 2009. Generating conceptually personal-
ized interactions for educational digital libraries using
concept maps. Ph.D. thesis, University of Colorado at
Boulder.
Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H
Martin, Md Arafat Sultan, and Tamara Sumner. 2012.
Identifying science concepts and student misconcep-
tions in an interactive essay writing tutor. In Proceed-
ings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 12?21. Association for
Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Arthur Graesser, Shulan Lu, George Jackson, Heather
Mitchell, Mathew Ventura, Andrew Olney, and Max
Louwerse. 2004. AutoTutor: A tutor with dialogue
in natural language. Behavior Research Methods,
36:180?192.
Daniel Jurafsky and H James. 2000. Speech and lan-
guage processing an introduction to natural language
processing, computational linguistics, and speech.
F Noorbehbahani and AA Kardan. 2011. The automatic
assessment of free text answers using a modified bleu
algorithm. Computers & Education, 56(2):337?345.
Philip V Ogren, Philipp G Wetzler, and Steven J Bethard.
2008. Cleartk: A uima toolkit for statistical natu-
ral language processing. Towards Enhanced Inter-
operability for Large HLT Systems: UIMA for NLP,
page 32.
Christine Redecker, Yves Punie, and Anusca Ferrari.
2012. eassessment for 21st century learning and skills.
In 21st Century Learning for 21st Century Skills,
pages 292?305. Springer.
607
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241?246,
Dublin, Ireland, August 23-24, 2014.
DLS@CU: Sentence Similarity from Word Alignment
Md Arafat Sultan
?
, Steven Bethard
?
, Tamara Sumner
?
?
Institute of Cognitive Science and Department of Computer Science
University of Colorado Boulder
?
Department of Computer and Information Sciences
University of Alabama at Birmingham
arafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.edu
Abstract
We present an algorithm for computing
the semantic similarity between two sen-
tences. It adopts the hypothesis that se-
mantic similarity is a monotonically in-
creasing function of the degree to which
(1) the two sentences contain similar se-
mantic units, and (2) such units occur in
similar semantic contexts. With a simplis-
tic operationalization of the notion of se-
mantic units with individual words, we ex-
perimentally show that this hypothesis can
lead to state-of-the-art results for sentence-
level semantic similarity. At the Sem-
Eval 2014 STS task (task 10), our system
demonstrated the best performance (mea-
sured by correlation with human annota-
tions) among 38 system runs.
1 Introduction
Semantic textual similarity (STS), in the context
of short text fragments, has drawn considerable
attention in recent times. Its application spans a
multitude of areas, including natural language pro-
cessing, information retrieval and digital learning.
Examples of tasks that benefit from STS include
text summarization, machine translation, question
answering, short answer scoring, and so on.
The annual series of SemEval STS tasks (Agirre
et al., 2012; Agirre et al., 2013; Agirre et al., 2014)
is an important platform where STS systems are
evaluated on common data and evaluation criteria.
In this article, we describe an STS system which
participated and outperformed all other systems at
SemEval 2014.
The algorithm is a straightforward application
of the monolingual word aligner presented in (Sul-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tan et al., 2014). This aligner aligns related words
in two sentences based on the following properties
of the words:
1. They are semantically similar.
2. They occur in similar semantic contexts in
the respective sentences.
The output of the word aligner for a sentence
pair can be used to predict the pair?s semantic sim-
ilarity by taking the proportion of their aligned
content words. Intuitively, the more semantic
components in the sentences we can meaningfully
align, the higher their semantic similarity should
be. In experiments on STS 2013 data reported
by Sultan et al. (2014), this approach was found
highly effective. We also adopt this hypothesis of
semantic compositionality for STS 2014.
We implement an STS algorithm that is only
slightly different from the algorithm in (Sultan et
al., 2014). The approach remains equally success-
ful on STS 2014 data.
2 Background
We focus on two relevant topics in this section:
the state of the art of STS research, and the word
aligner presented in (Sultan et al., 2014).
2.1 Semantic Textual Similarity
Since the inception of textual similarity research
for short text, perhaps with the studies reported
by Mihalcea et al. (2006) and Li et al. (2006),
the topic has spawned significant research inter-
est. The majority of systems have been reported
as part of the SemEval 2012 and *SEM 2013 STS
tasks (Agirre et al., 2012; Agirre et al., 2013).
Here we confine our discussion to systems that
participated in these tasks.
With designated training data for several test
sets, supervised systems were the most successful
in STS 2012 (B?ar et al., 2012;
?
Sari?c et al., 2012;
241
Jimenez et al., 2012). Such systems typically ap-
ply a regression algorithm on a large number of
STS features (e.g., string similarity, syntactic sim-
ilarity and word or phrase-level semantic similar-
ity) to generate a final similarity score. This ap-
proach continued to do well in 2013 (Han et al.,
2013; Wu et al., 2013; Shareghi and Bergler, 2013)
even without domain-specific training data, but the
best results were demonstrated by an unsupervised
system (Han et al., 2013). This has important im-
plications for STS since extraction of each feature
adds to the latency of a supervised system. STS
systems are typically important in the context of
a larger system rather than on their own, so high
latency is an obvious drawback for such systems.
We present an STS system that has simplicity,
high accuracy and speed as its design goals, can
be deployed without any supervision, operates in
a linguistically principled manner with purely se-
mantic sentence properties, and was the top sys-
tem at SemEval STS 2014.
2.2 The Sultan et al. (2014) Aligner
The word aligner presented in (Sultan et al., 2014)
has been used unchanged in this work and plays a
central role in our STS algorithm. We give only an
overview here; for the full details, see the original
article.
We will denote the sentences being aligned (and
are subsequently input to the STS algorithm) as
S
(1)
and S
(2)
. We describe only content word
alignment here; stop words are not used in our STS
computation.
The aligner first identifies word pairs w
(1)
i
?
S
(1)
and w
(2)
j
? S
(2)
such that:
1. w
(1)
i
and w
(2)
j
have non-zero semantic simi-
larity, sim
Wij
. The calculation of sim
Wij
is
described in Section 2.2.1.
2. The semantic contexts of w
(1)
i
and w
(2)
j
have
some similarity, sim
Cij
. We define the se-
mantic context of a word w in a sentence
S as a set of words in S, and the seman-
tic context of the word pair (w
(1)
i
, w
(2)
j
), de-
noted by context
ij
, as the Cartesian product
of the context of w
(1)
i
in S
(1)
and the con-
text of w
(2)
j
in S
(2)
. We define several types
of context (i.e., several selections of words)
and describe the corresponding calculations
of sim
Cij
in Section 2.2.2.
3. There are no competing pairs scoring higher
Align
identical
word
sequences
Align
named
entities
Align
content
words
using
depen-
dencies
Align
content
words
using sur-
rounding
words
Figure 1: The alignment pipeline.
than (w
(1)
i
, w
(2)
j
) under f(sim
W
, sim
C
) =
0.9 ? sim
W
+ 0.1 ? sim
C
. That is,
there are no pairs (w
(1)
k
, w
(2)
j
) such that
f(sim
Wkj
, sim
Ckj
) > f(sim
Wij
, sim
Cij
),
and there are no pairs (w
(1)
i
, w
(2)
l
) such that
f(sim
Wil
, sim
Cil
) > f(sim
Wij
, sim
Cij
).
The weights 0.9 and 0.1 were derived empiri-
cally via a grid search in the range [0, 1] (with
a step size of 0.1) to maximize alignment per-
formance on the training set of the (Brockett,
2007) alignment corpus. This set contains
800 human-aligned sentence pairs collected
from a textual entailment corpus (Bar-Haim
et al., 2006).
The aligner then performs one-to-one word align-
ments in decreasing order of the f value.
This alignment process is applied in four steps
as shown in Figure 1; each step applies the above
process to a particular type of context: identi-
cal words, dependencies and surrounding content
words. Additionally, named entities are aligned in
a separate step (details in Section 2.2.2).
Words that are aligned by an earlier module of
the pipeline are not allowed to be re-aligned by
downstream modules.
2.2.1 Word Similarity
Word similarity (sim
W
) is computed as follows:
1. If the two words or their lemmas are identi-
cal, then sim
W
= 1.
2. If the two words are present as a pair
in the lexical XXXL corpus of the Para-
phrase Database
1
(PPDB) (Ganitkevitch et
al., 2013), then sim
W
= 0.9.
2
For this
step, PPDB was augmented with lemmatized
forms of the already existing word pairs.
3
1
PPDB is a large database of lexical, phrasal and syntactic
paraphrases.
2
Again, the value 0.9 was derived empirically via a grid
search in [0, 1] (step size = 0.1) to maximize alignment per-
formance on the (Brockett, 2007) training data.
3
The Python NLTK WordNetLemmatizer was used to
lemmatize the original PPDB words.
242
3. For any other word pair, sim
W
= 0.
2.2.2 Contextual Similarity
Contextual similarity (sim
C
) for a word pair
(w
(1)
i
, w
(2)
j
) is computed as the sum of the word
similarities for each pair of words in the context of
(w
(1)
i
, w
(2)
j
). That is:
sim
Cij
=
?
(w
(1)
k
,w
(2)
l
) ? context
ij
sim
Wkl
Each of the stages in Figure 1 employs a specific
type of context.
Identical Word Sequences. Contextual sim-
ilarity for identical word sequences (a word se-
quence W which is present in both S
(1)
and S
(2)
and contains at least one content word) defines the
context by pairing up each word in the instance of
W in S
(1)
with its occurrence in the instance of
W in S
(2)
. All such sequences with length ? 2
are aligned; longer sequences are aligned before
shorter ones. This simple step was found to be of
very high precision in (Sultan et al., 2014) and re-
duces the overall computational cost of alignment.
Named Entities. Named entities are a special
case in the alignment pipeline. Even though the
context for a named entity is defined in the same
way as it is defined for any other content word
(as described below), named entities are aligned
in a separate step before other content words be-
cause they have special properties such as corefer-
ring mentions of different lengths (e.g. Smith and
John Smith, BBC and British Broadcasting Cor-
poration). The head word of the named entity is
used in dependency calculations.
Dependencies. Dependency-based contex-
tual similarity defines the context for the pair
(w
(1)
i
, w
(2)
j
) using the syntactic dependencies of
w
(1)
i
and w
(2)
j
. The context is the set of all word
pairs (w
(1)
k
, w
(2)
l
) such that:
? w
(1)
k
is a dependency of w
(1)
i
,
? w
(2)
l
is a dependency of w
(2)
j
,
? w
(1)
i
and w
(2)
j
have the same lexical category,
? w
(1)
k
and w
(2)
l
have the same lexical category,
and,
? The two dependencies are either identical or
semantically ?equivalent? according to the
equivalence table provided by Sultan et al.
S
(1)
: He wrote a book .
nsubj
dobj
det
S
(2)
: I read the book he wrote .
nsubj
dobj
det
rcmod
nsubj
Figure 2: Example of dependency equivalence.
(2014). We explain semantic equivalence of
dependencies using an example below.
Equivalence of Dependency Structures. Con-
sider S
(1)
and S
(2)
in Figure 2. Note that w
(1)
2
=
w
(2)
6
= ?wrote? and w
(1)
4
= w
(2)
4
= ?book? in
this pair. Now, each of the two following typed
dependencies: dobj(w
(1)
2
, w
(1)
4
) in S
(1)
and rc-
mod(w
(2)
4
, w
(2)
6
) in S
(2)
, represents the relation
?thing that was written? between the verb ?wrote?
and its argument ?book?. Thus, to summarize,
an instance of contextual evidence for a possible
alignment between the pair (w
(1)
2
, w
(2)
6
) (?wrote?)
lies in the pair (w
(1)
4
, w
(2)
4
) (?book?) and the equiv-
alence of the two dependency types dobj and rc-
mod.
The equivalence table of Sultan et al. (2014) is
a list of all such possible equivalences among dif-
ferent dependency types (given that w
(1)
i
has the
same lexical category as w
(2)
j
and w
(1)
k
has the
same lexical category as w
(2)
l
).
If there are no word pairs with identical or
equivalent dependencies as defined above, i.e. if
sim
Cij
= 0, then w
(1)
i
and w
(2)
j
will not be
aligned by this module.
Surrounding Content Words. Surrounding-
word-based contextual similarity defines the con-
text of a word in a sentence as a fixed window of
3 words to its left and 3 words to its right. Only
content words in the window are considered. (As
explained in the beginning of this section, the con-
text of the pair (w
(1)
i
, w
(2)
j
) is then the Cartesian
product of the context of w
(1)
i
in S
(1)
and w
(2)
j
in
S
(2)
.) Note that w
(1)
i
and w
(2)
j
can be of different
lexical categories here.
A content word can often be surrounded by
stop words which provide almost no information
about its semantic context. The chosen window
size is assumed, on average, to effectively make
243
Data Set Source of Text # of Pairs
deft-forum discussion forums 450
deft-news news articles 300
headlines news headlines 750
images image descriptions 750
OnWN word sense definitions 750
tweet-news news articles and tweets 750
Table 1: Test sets for SemEval STS 2014.
sufficient contextual information available while
avoiding the inclusion of contextually unrelated
words. But further experiments are necessary to
determine the best span in the context of align-
ment.
Unlike dependency-based alignment, even if
there are no similar words in the context, i.e. if
sim
Cij
= 0, w
(1)
i
may still be aligned to w
(2)
j
if
sim
Wij
> 0 and no alignments for w
(1)
i
or w
(2)
j
have been found by earlier stages of the pipeline.
2.2.3 The Alignment Sequence
The rationale behind the specific sequence of
alignment steps (Figure 1) was explained in (Sul-
tan et al., 2014): (1) Identical word sequence
alignment was found to be the step with the
highest precision in experiments on the (Brock-
ett, 2007) training data, (2) It is convenient to
align named entities before other content words
to enable alignment of entity mentions of differ-
ent lengths, (3) Dependency-based evidence was
observed to be more reliable (i.e. of higher preci-
sion) than textual evidence on the (Brockett, 2007)
training data.
3 Method
Our STS score is a function of the proportions of
aligned content words in the two input sentences.
The proportion of content words in S
(1)
that are
aligned to some word in S
(2)
is:
prop
(1)
Al
=
|{i : [?j : (i, j) ? Al] and w
(1)
i
? C}|
|{i : w
(1)
i
? C}|
where C is the set of all content words in En-
glish and Al are the predicted word alignments. A
word alignment is a pair of indices (i, j) indicating
that word w
(1)
i
is aligned to w
(2)
j
. The proportion
of aligned content words in S
(2)
, prop
(2)
Al
, can be
computed in a similar way.
We posit that a simple yet sensible way to obtain
an STS estimate for S
(1)
and S
(2)
is to take a mean
Data Set Run 1 Run 2
deft-forum 0.4828 0.4828
deft-news 0.7657 0.7657
headlines 0.7646 0.7646
images 0.8214 0.8214
OnWN 0.7227 0.8589
tweet-news 0.7639 0.7639
Weighted Mean 0.7337 0.7610
Table 2: Results of evaluation on SemEval STS
2014 data. Each value on columns 2 and 3 is the
correlation between system output and human an-
notations for the corresponding data set. The last
row shows the value of the final evaluation metric.
of prop
(1)
Al
and prop
(2)
Al
. Our two submitted runs
use the harmonic mean:
sim(S
(1)
, S
(2)
) =
2? prop
(1)
Al
? prop
(2)
Al
prop
(1)
Al
+ prop
(2)
Al
It is a more conservative estimate than the arith-
metic mean, and penalizes sentence pairs with a
large disparity between the values of prop
(1)
Al
and
prop
(2)
Al
. Experiments on STS 2012 and 2013 data
revealed the harmonic mean of the two propor-
tions to be a better STS estimate than the arith-
metic mean.
4 Data
STS systems at SemEval 2014 were evaluated on
six data sets. Each test set consists of a number
of sentence pairs; each pair has a human-assigned
similarity score in the range [0, 5] which increases
with similarity. Every score is the mean of five
scores crowdsourced using the Amazon Mechan-
ical Turk. The sentences were collected from a
variety of sources. In Table 1, we provide a brief
description of each test set.
5 Evaluation
We submitted the results of two system runs at
SemEval 2014 based on the idea presented in Sec-
tion 3. The two runs were identical, except for the
fact that for the OnWN test set, we specified the
following words as additional stop words during
run 2 (but not during run 1): something, someone,
somebody, act, activity, some, state.
4
For both
4
OnWN has many sentence pairs where each sentence
is of the form ?the act/activity/state of verb+ing some-
thing/somebody?. The selected words act merely as fillers
in such pairs and consequently do not typically contribute to
the similarity scores.
244
Data Set Run 1 Run 2
FNWN 0.4686 0.4686
headlines 0.7797 0.7797
OnWN 0.6083 0.8197
SMT 0.3837 0.3837
Weighted Mean 0.5788 0.6315
Table 3: Results of evaluation on *SEM STS 2013
data.
runs, the tweet-news sentences were preprocessed
by separating the hashtag from the word for each
hashtagged word.
Table 2 shows the performance of each run.
Rows 1 through 6 show the Pearson correlation
coefficients between the system scores and human
annotations for all test sets. The last row shows
the value of the final evaluation metric, which is a
weighted sum of all correlations in rows 1?6. The
weight assigned to a data set is proportional to its
number of pairs. Our run 1 ranked 7th and run 2
ranked 1st among 38 submitted system runs.
An important implication of these results is the
fact that knowledge of domain-specific stop words
can be beneficial for an STS system. Even though
we imparted this knowledge to our system during
run 2 via a manually constructed set of additional
stop words, simple measures like TF-IDF can be
used to automate the process.
5.1 Performance on STS 2012 and 2013 Data
We applied our algorithm on the 2012 and 2013
STS test sets to examine its general utility. Note
that the STS 2013 setup was similar to STS 2014
with no domain-dependent training data, whereas
several of the 2012 test sets had designated train-
ing data.
Over all the 2013 test sets, our two runs demon-
strated weighted correlations of 0.5788 (rank: 4)
and 0.6315 (rank: 1), respectively. Table 3 shows
performances on individual test sets. (Descrip-
tions of the test sets can be found in (Agirre et
al., 2013).) Again, run 2 outperformed run 1 on
OnWN by a large margin.
On the 2012 test sets, however, the performance
was worse (relative to other systems), with respec-
tive weighted correlations of 0.6476 (rank: 8) and
0.6423 (rank: 9). Table 4 shows performances on
individual test sets. (Descriptions of the test sets
can be found in (Agirre et al., 2012).)
This performance drop seems to be an obvious
consequence of the fact that our algorithm was
not trained on domain-specific data: on STS 2013
Data Set Run 1 Run 2
MSRpar 0.6413 0.6413
MSRvid 0.8200 0.8200
OnWN 0.7227 0.7004
SMTeuroparl 0.4267 0.4267
SMTnews 0.4486 0.4486
Weighted Mean 0.6476 0.6423
Table 4: Results of evaluation on SemEval STS
2012 data.
data, the top two STS 2012 systems, with respec-
tive weighted correlations of 0.5652 and 0.5221
(Agirre et al., 2013), were outperformed by our
system by a large margin.
In contrast to the other two years, our run 1
outperformed run 2 on the 2012 OnWN test set
by a very small margin. A closer inspection
revealed that the previously mentioned sentence
structure ?the act/activity/state of verb+ing some-
thing/somebody? is much less common in this set,
and as a result, our additional stop words tend to
play more salient semantic roles in this set than in
the other two OnWN sets (i.e. they act relatively
more as content words than stop words). The drop
in correlation with human annotations is a con-
sequence of this role reversal. This result again
shows the importance of a proper selection of stop
words for STS and also points to the challenges
associated with making such a selection.
6 Conclusions and Future Work
We show that alignment of related words in two
sentences, if carried out in a principled and accu-
rate manner, can yield state-of-the-art results for
sentence-level semantic similarity. Our system has
the desired quality of being both accurate and fast.
Evaluation on test data from different STS years
demonstrates its general applicability as well.
The idea of STS from alignment is worth inves-
tigating with larger semantic units (i.e. phrases)
in the two sentences. Another possible research
direction is to investigate whether the alignment
proportions observed for the two sentences can be
used as features to improve performance in a su-
pervised setup (even though this scenario is ar-
guably less common in practice because of un-
availability of domain or situation-specific train-
ing data).
Acknowledgments
This material is based in part upon work supported
by the National Science Foundation under Grant
245
Numbers EHR/0835393 and EHR/0835381. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views
of the National Science Foundation.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, Volume 2: Proceedings
of the Sixth International Workshop on Semantic
Evaluation, SemEval ?12, pages 385-393, Montreal,
Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics, *SEM ?13, pages 32-43, Atlanta,
Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilin-
gual semantic textual similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The Second PASCAL Recognising
Textual Entailment Challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation, held in
conjunction with the 1st Joint Conference on Lexical
and Computational Semantics, SemEval ?12, pages
435-440, Montreal, Canada.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Technical Report MSR-TR-2007-77, Microsoft Re-
search.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL-HLT ?13,
pages 758-764, Atlanta, Georgia, USA.
Lushan Han, Abhay Kashyap, Tim Finin, James
Mayfield, and Jonathan Weese. 2013. UMBC
EBIQUITY-CORE: Semantic Textual Similarity
Systems. In Proceedings of the Second Joint Con-
ference on Lexical and Computational Semantics,
*SEM ?13, pages 44-52, Atlanta, Georgia, USA.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality: a parameterized sim-
ilarity function for text comparison. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation, held in conjunction with the 1st Joint Con-
ference on Lexical and Computational Semantics,
SemEval ?12, pages 449-453, Montreal, Canada.
Yuhua Li, David Mclean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engi-
neering, vol.18, no.8. 1138-1150.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceed-
ings of the 21st national conference on Artificial in-
telligence, AAAI ?06, pages 775-780, Boston, Mas-
sachusetts, USA.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. TakeLab: sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation, held in conjunction with the 1st
Joint Conference on Lexical and Computational Se-
mantics, SemEval ?12, pages 441-448, Montreal,
Canada.
Ehsan Shareghi and Sabine Bergler. 2013. CLaC-
CORE: Exhaustive Feature Combination for Mea-
suring Textual Similarity. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics, *SEM ?13, pages 202-206, At-
lanta, Georgia, USA.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to Basics for Monolingual Align-
ment: Exploiting Word Similarity and Contextual
Evidence. Transactions of the Association for Com-
putational Linguistics, 2 (May), pages 219-230.
Stephen Wu, Dongqing Zhu, Ben Carterette, and Hong-
fang Liu. 2013. MayoClinicNLP-CORE: Semantic
representations for textual similarity. In Proceed-
ings of the Second Joint Conference on Lexical and
Computational Semantics, *SEM ?13, pages 148-
154, Atlanta, Georgia, USA.
246
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 12?21,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying science concepts and student misconceptions
in an interactive essay writing tutor
Steven Bethard
University of Colorado
Boulder, Colorado, USA
steven.bethard@colorado.edu
Ifeyinwa Okoye
University of Colorado
Boulder, Colorado, USA
ifeyinwa.okoye@colorado.edu
Md. Arafat Sultan
University of Colorado
Boulder, Colorado, USA
arafat.sultan@colorado.edu
Haojie Hang
University of Colorado
Boulder, Colorado, USA
haojie.hang@colorado.edu
James H. Martin
University of Colorado
Boulder, Colorado, USA
james.martin@colorado.edu
Tamara Sumner
University of Colorado
Boulder, Colorado, USA
tamara.sumner@colorado.edu
Abstract
We present initial steps towards an interac-
tive essay writing tutor that improves science
knowledge by analyzing student essays for mis-
conceptions and recommending science web-
pages that help correct those misconceptions.
We describe the five components in this sys-
tem: identifying core science concepts, deter-
mining appropriate pedagogical sequences for
the science concepts, identifying student mis-
conceptions in essays, aligning student miscon-
ceptions to science concepts, and recommend-
ing webpages to address misconceptions. We
provide initial models and evaluations of the
models for each component.
1 Introduction
Students come to class with a variety of misconcep-
tions present in their science knowledge. For ex-
ample, science assessments developed by the Amer-
ican Association for the Advancement of Science
(AAAS)1 showed that 49% of American 6th-8th
graders believe that the Earth?s tectonic plates are
only feet thick (while in fact they are miles thick)
and that 48% of American 6th-8th graders believe
that atoms of a solid are not moving (while in fact
all atoms are in constant motion). A key challenge
for interactive tutoring systems is thus to identify and
correct such student misconceptions.
In this article, we develop an interactive essay writ-
ing tutor that tries to address these challenges. The
tutor first examines a set of science webpages to iden-
tify key concepts (Section 4) and attempts to order
1http://assessment.aaas.org/
the science concepts in a pedagogically appropriate
learning path (Section 5). Then the tutor examines a
student essay and identifies misconception sentences
(Section 6) and aligns these misconceptions to the
true science concepts (Section 7). Finally, the tutor
suggests science webpages that can help the student
address each of the misconceptions (Section 8).
The key contributions of this work are:
? Demonstrating that a summarization approach
can identify core science concepts
? Showing how a learning path model can be boot-
strapped from webpages with grade metadata
? Developing models for misconception identifi-
cation based on textual entailment techniques
? Presenting an information retrieval approach to
aligning misconceptions to science concepts
? Designing a system that recommends webpages
to address student misconceptions
2 Related work
Interactive tutoring systems have been designed for
a variety of domains and applications. Dialog-based
tutoring systems, such as Why2-Atlas (VanLehn et
al., 2002), AutoTutor (Graesser et al, 2004) and
MetaTutor (Azevedo et al, 2008), interact with stu-
dents via questions and answers. Student knowledge
is judged by comparing student responses to knowl-
edge bases of domain concepts and misconceptions.
These knowledge bases are typically manually cu-
rated, and a new knowledge base must be constructed
for each new domain where the tutor is to be used.
12
Essay-based tutoring systems, such as Summary
Street (Wade-Stein and Kintsch, 2004) or CLICK
(de la Chica et al, 2008b), interact with students who
are writing a summary or essay. They compare what
the student has written to domain knowledge in the
form of textbooks or webpages. They typically do not
require a knowledge base to be manually constructed,
instead using natural language processing techniques
to compare the student?s essay to the information in
the textbooks or webpages.
The current work is inspired by these essay-based
tutoring systems, where interaction revolves around
essay writing. However, where Summary Street re-
lies primarily upon measuring how much of a text-
book a student essay has ?covered?, we aim to give
more detailed assessments that pinpoint specific stu-
dent misconceptions. CLICK targets a similar goal
to ours, but assumes that accurate knowledge maps
can be generated for both the domain knowledge and
for each student essay. Our approach does not re-
quire the automatic generation of knowledge maps,
instead working directly with the sentences in the
student essays and the webpages of science domain
knowledge.
3 System overview
Our system is composed of five key components.
First, a core concept identifier examines domain
knowledge (webpages) and identifies key concepts
(sentences) that describe the most important pieces
of knowledge in the domain. Second, a concept se-
quencer assigns a pedagogically appropriate order in
which a student should learn the identified core con-
cepts. Third, a misconception identifier examines the
student essay and identifies sentences that describe
misconceptions the student has about the domain.
Fourth, a misconception-concept aligner finds a core
concept that can be used to correct each misconcep-
tion. Finally, a recommender takes all the informa-
tion about core concepts and student misconceptions,
decides what order to address the misconceptions in,
and identifies a set of resources (webpages) for the
student to read.
To assemble this system, we draw on a variety of
existing datasets (and some data collection of our
own). For example, we use data from an annotation
study of concept coreness to evaluate our model for
identifying domain concepts, and we use data from
science assessments of the American Association for
the Advancement of Science to train and evaluate our
model for identifying misconceptions. We use this
disparate data to establish baseline models for each of
the tutor?s components. In the near future, this base-
line tutoring system will be used to collect student
essays and other data that will allow us to develop
more sophisticated model for each component.
4 Identifying core concepts
This first module aims at automatically identifying a
set of core concepts in a given set of digital library
resources or webpages. Core concepts in a subject
domain are critical ideas necessary to support deep
science learning and transfer in that domain. From
a digital learning perspective, availability of such
concepts helps in providing pedagogical feedback
to learners to support robust learning and also in
prioritizing instructional intervention (e.g., deciding
the order in which to treat student misconceptions).
A concept can be materialized using different levels
of linguistic expressions (e.g. phrases, sentences or
paragraphs), but for this work, we focus only on
individual sentences as expressions of concepts.
We used COGENT (de la Chica et al, 2008a), a
multi-document summarization system to extract con-
cepts (i.e. sentences) from a given set of resources.
In the following two subsections, we describe the
COGENT system, discuss how we used it for core
concept extraction and report the results of its evalu-
ation of effectiveness.
4.1 Model
COGENT is a text summarizer that builds on MEAD
(Radev et al, 2004), a multidocument summarization
and evaluation platform . MEAD was originally de-
veloped to summarize news articles. COGENT aims
to generate pedagogically useful summaries from
educational resources.
COGENT extends MEAD by incorporating new
features in the summarization process. MEAD uses
a set of generic (i.e. domain-independent) features to
evaluate each sentence in the given set of documents.
These features include the length of the sentence, the
distance from the sentence to the beginning of the
document, etc. Individual scores of a sentence along
13
these dimensions are combined to assign a total score
to the sentence. After removing redundant sentences,
MEAD then generates a summary using the sentences
that had the highest scores. A user-specified parame-
ter determines the number of sentences included in
the summary.
COGENT extends this framework by incorporat-
ing new domain-general and domain-specific features
in the sentence scoring process. The domain-general
features include a document structure feature, which
takes into account a sentence?s level in terms of
HTML headings, and a content word density fea-
ture, which computes the ratio of content words to
function words. The domain-specific features include
an educational standards feature, which uses a TF-
IDF based textual similarity score between a sentence
and nationally recognized educational goals from the
American Association for the Advancement of Sci-
ence (AAAS) Benchmarks (Project2061., 1993) and
the associated National Science Education Standards
(NRC, 1996), and a gazetteer feature, which scores
sentences highly that mention many unique names
from a gazetteer of named entities.
While in the past, COGENT was used primarily
as a summarization system, in the current work, we
evaluate its utility as a means of identifying core
concepts. That is, are the top sentences selected
by COGENT also the sentences describing the key
science concepts in the domain?
4.2 Evaluation
We evaluate the core concept extraction module by
assessing the extracted concepts against human ex-
pert annotations. We ran an annotation study where
two human experts assigned ?coreness? ratings to
a selected set of sentences collected from digital
resources in three science domains: Plate Tecton-
ics, Weather and Climate, and Biological Evolution.
These experts had been recruited based on their train-
ing and expertise in the selected subject domains.
First, a set of digital resources was selected from
the Digital Library for Earth System Education
(DLESE) 2 across the three subject domains. Then
COGENT was used to extract the top 5% sentences
for each domain. The experts then annotated each
extracted sentence with its coreness rating on a scale
2http://www.dlese.org
Extraction %
0.5% 1.0% 2.5% 5.0%
Plate Tectonics 3.33 3.27 3.00 2.81
Weather and Climate 3.13 2.97 3.07 2.99
Biological Evolution 2.00 2.13 2.46 2.25
Table 1: Average coreness of sentences extracted at differ-
ent percentages in each domain
of 1 to 4, 4 being the highest. Human annotation is
a time-consuming process and this is why we had
to limit the number of extracted sentences to a mod-
erate 5% (which is still more than 400 sentences).
17% of the sentences were double annotated and the
inter-rater reliability, measured by Spearman?s rho,
was 0.38. These expert ratings of sentences form the
basis of our evaluation.
Table 1 shows the average coreness assigned by the
experts to sentences extracted by COGENT in each
domain, for different extraction percentages. For ex-
ample, if COGENT is used to extract the top 1% of
sentences from all the Plate Tectonics resources, then
the average of their coreness ratings (as assigned by
the experts) is 3.27, representing a high level of core-
ness. This is essentially a measure of the precision
of COGENT at 1% extraction. Note that we cannot
calculate a measure of recall without asking experts
to annotate all of the domain sentences, a time con-
suming task which was outside of the scope of this
study.
The performance of COGENT was the best in the
Plate Tectonics domain since the domain-aware fea-
tures (e.g. the gazetteer features) used to train CO-
GENT were selected from this domain. In the ?near
domain? of Weather and Climate, the performance is
still good, but performance falls in the ?far domain?
of Biological Evolution, because of the significant
differences between the training domain and the test
domain. In the two latter domains, the performance
of COGENT was also inconsistent in that with an
increase in the extraction percentage, the average
coreness increased in some cases and decreased in
others. This inconsistency and overall degradation
in performance in the two latter domains are indica-
tive of the importance of introducing domain-aware
features into COGENT.
It is evident from the values in Table 1 that the
core concepts extraction module does a decent job,
14
especially when trained with appropriate domain-
aware features.
5 Sequencing core concepts
The goal of this next component is to take a set of
core science concepts (sentences), as produced by
the preceding module, and predict an appropriate se-
quence in which those concepts should be learned by
the student. Some concepts serve as building blocks
for other concepts, and thus it is essential to learn the
basic concepts first (and address any misconceptions
associated with them) before moving on to other con-
cepts that depend on the basic concepts. For example,
a student must first understand the concept of tectonic
plates before they can understand the concept of a
convergent plate boundary. The sequence of core
concepts that results from this module will serve as
input for the later module that prioritizes a student?s
misconceptions.
There may exist several different but reasonable
concept sequences (also known as learning paths) ?
the goal of this component is to recommend at least
one of these. As a first step, we focus on generating
a single concept sequence that represents a general
path through the learning goals, much like textbooks
and curriculums do.
5.1 Models
Our model for concept sequencing is a pair-wise
ordering model, that takes two concepts c1 and c2,
and predicts whether c1 should come before or after
c2 in the recommended learning path. Formally,
SEQUENCE(c1, c2) =
{
0 if c1 < c2
1 if c1 ? c2
To generate a complete ordering of concepts, we
construct a precedence table from these pair-wise
judgments and generate a path that is consistent with
these judgments.
We learn the SEQUENCE model as a supervised
classifier, where a feature vector is extracted for each
of the two concepts and the two feature vectors, con-
catenated, serve as the input to the classifier. For each
word in each concept, we include the following two
features:
? local word count - the number of times the
word appeared in this concept
? global word count - the log of the ratio between
the number of times the word occurred in the
concept and the number of times it occurred in
a background corpus, Gigaword (Graff, 2002)
These features are motivated by the work of Tanaka-
ishii et al(2010) that showed that local and global
word count features were sufficient to build a pair-
wise readability classifier that achieved 90% accu-
racy.
For the supervised classifier, we consider naive
Bayes, decision trees, and support vector machines.
5.2 Evaluation
To evaluate our concept sequencing model, we gath-
ered learning paths from experts in high school earth
science. Using the model from Section 4, we selected
30 core concepts for the domain of plate tectonics.
We asked two earth science experts to each come up
with two learning paths for these core concepts, with
the first path following an evidence or research based
and second path following a traditional learning path.
An evidence or research based learning path, is
a pedagogy where students are encouraged to use
the scientific method to learn about a phenomena, i.e
they gather information by observing the phenomena,
form a hypothesis, perform experiment, collect and
analyze data and then interpret the data and draw
conclusions that hopefully align with the current un-
derstanding about the phenomena. A teacher that
uses this learning path acts as a guide on the side. A
traditional learning path on the other hand, is the ped-
agogy where teachers are simply trying to pass on the
correct information to students rather than letting the
students discover the information themselves. In a
classroom environment, a teacher using this learning
path would be seen as the classical sage on stage.
We used the learning paths collected from the ex-
perts to form two test sets, one for the evidence-based
pedagogy, and one for the traditional pedagogy. For
each pedagogy, we asked which of all the possible
pair-wise orderings our experts agreed upon. For ex-
ample, if the first expert said that A < B < C and
the second expert said that A < C < B, then both
experts agreed that A < B and A < C, while they
disagreed on whether B < C or C < B. Note that
we evaluate pair-wise orderings here, not a complete
ranking of the concepts, because the experts did not
15
Pedagogy Pairs (%) c1 < c2 c1 ? c2
Evidence 637 (68%) 48.5% 51.5%
Traditional 613 (70%) 48.5% 51.5%
Table 2: Test sets for sequencing concepts. The Pairs
column shows how many pairs the experts agreed upon
(out of a total of 30 ? 29 = 870 pairs).
produce a total ordering of the concepts, only a par-
tial tree-like ordering. The experts put the concepts
in levels, with concepts in the same level having no
precedence relationship, while a concept in a lower
level preceded a concept in a higher level.
For our test sets, we selected only the pairs on
which both experts agreed. Table 2 shows that experts
agreed on 68-70% of the pair-wise orderings. Table
2 also shows the percentage of each type of pair-wise
ordering (c1 < c2 vs. c1 ? c2) present in the data.
Note that even though all concepts are paired with all
other concepts, because the experts do not produce
complete orderings, the number of agreements for
each type of ordering may not be the same. Consider
the case where expert E1 says that concepts A and
B are on the same level (i.e., A = B) and expert E2
says that concept A is in a lower level than concept
B (i.e., A < B). Then for the pair (A,B), they
disagree on the relation (E1 says A ? B while E2
says A < B) but for the pair (B,A) they agree on
the relation (they both say B ? A). As a result, the
c1 ? c2 class is slightly larger than the c1 < c2 class.
Since these data sets were small, we reserved them
for testing, and trained our pair-wise classification
model using a proxy task: ordering sentences by
grade. In this task, the model is given two sentences
s1 and s2, one written for middle school and written
for high school, and asked to decide whether s1 < s2
(i.e. s1 is the middle school sentence) or s2 < s1
(i.e. s2 is the middle school sentence). We expect
that a model for ordering sentences by grade should
also be a reasonable model for ordering concepts
for a pedagogical learning path. And importantly,
getting grade ordering data automatically is easy: the
Digital Library for Earth System Education (DLESE)
contains a variety of earth science resources with
metadata about the grade level they were written for.
To construct the training data, we searched the
DLESE website for text resources that contained
the words earthquake or plate tectonics. We col-
Baseline NaiveBayes SVM
Evidence 51.5% 60.8% 53.3%
Traditional 51.5% 56.6% 49.7%
Table 3: Accuracy result from Naive Bayes and SVM for
classifying the core concepts
lected 10 such resources for each of the two grade
cohorts, middle school (we allowed anything K-8)
and high school (we allowed anything 9+). We down-
loaded the webpage for each resource, and used CO-
GENT to extract the 20 most important sentences
from each. This resulted in 200 sentences for each
of the two grade cohorts. To create pairs of grade-
ordered sentences, we paired up middle and high
school concepts both ways: middle school first (i.e.
SEQUENCE(cm, ch) = 0) and high school first (i.e.
SEQUENCE(ch, cm) = 1). This resulted in 40,000
grade-ordered sentence pairs for training.
We then used this proxy-task training data to
train our models. We extracted 1702 unique non-
stopwords from the training data, resulting in 3404
features per concept, and 6808 features per con-
cept pair (i.e. per classification instance). On the
grade-ordering task, we evaluated three models using
WEKA3, a naive Bayes model, a decision tree (J48)
model, and a support vector machine (SVM) model.
Using a stratified 50/50 split of the training data, we
found that the naive Bayes and SVM models both
achieved an accuracy of 80.2%, while the decision
tree achieved only 62%. So, we selected the naive
Bayes and SVM models for our real task, concept
sequencing.
Table 3 shows the performance of the two models
on the expert judgments of concept sequencing. We
find that the naive Bayes model produces more expert-
like concept sequences than would be generated by
chance and also outperforms the SVM model on the
concept sequencing task. For the final output of the
module, we combine the pair-wise judgments into a
complete concept sequence, breaking any ties in the
pair-wise judgments by preferring the order of the
concepts in the output of the core concept identifier.
3http://www.cs.waikato.ac.nz/ml/weka/
16
6 Identifying student misconceptions
The previous components have focused on analyzing
the background knowledge ? finding core concepts
in the domain and selecting an appropriate learning
sequence for these concepts. The current component
focuses on the student essay, using the collected back-
ground knowledge to help analyze the essay and give
feedback.
Given a student essay, the goal of this component
is to identify which sentences in the essay are most
likely to be misconceptions. The task of misconcep-
tion identification is closely related to the task of
textual entailment (Dagan et al, 2006), in which the
goal is to predict if a hypothesis sentence, H, can be
reasonably concluded given another sentence, T. In
misconception identification, the goal is to predict if
a student sentence can be concluded from any com-
bination of the sentences in the domain knowledge,
similar to a textual entailment task with a single H
but many Ts. A student sentence that can not be
concluded from the domain knowledge is likely a
misconception.
6.1 Models
We developed two models for identifying student
misconceptions, inspired by work in textual entail-
ment that showed that a model that simply counts the
words in H that appeared in T, after expanding the
words in T using WordNet, achieves state-of-the-art
performance (Shnarch et al, 2011)4.
The Coverage model scores a student sentence
by counting the number of its words that are also in
some domain sentence. Low-scoring sentences are
likely misconceptions. Formally:
SCORE(s) =
|s ? d|
|s|
d =
?
s??D
EXPAND(s?)
where s is a student sentence (a list of words), D is
the set of domain sentences, and EXPAND performs
lexical expansion on the words of a sentence.
The Retrieval model indexes the domain sen-
tences with an information retrieval system (we use
4The paper also proposes a more elaborate probabilistic
model, but shows that the ?lexical coverage? model we adopt
here is quite competitive both with their probabilistic model and
with the top-performing systems of RTE5 and RTE6.
Lucene5), and scores a student sentence by querying
the index and summing the scores. Formally:
SCORE(s) =
?
s??D
SCORElucene(s, EXPAND(s
?))
where s, D and EXPAND are defined as before, and
SCORElucene is a cosine over TF-IDF vectors6.
For both the Coverage and Retrieval models, we
consider the following lexical expansion techniques
for defining the EXPAND function:
? tokens ? words in the sentence (no expansion)
? tokens, synsets ? words in the sentence, plus
all lemmas of all WordNet synsets of each word
? tokens, synsetsexpanded ? words in the sentence,
plus all lemmas of all WordNet synsets of each
word, plus all lemmas of derived forms, hy-
ponyms or meroynms of the WordNet synsets
? tokens, synsetsexpanded?4 ? words in the sen-
tence, plus all lemmas of all WordNet synsets of
each word, plus all lemmas of WordNet synsets
reachable by a path of no more than 4 links
through derived forms, hyponyms or meroynms
6.2 Evaluation
We evaluate the quality of our misconception identi-
fication models using data collected from the Amer-
ican Association for the Advancement of Science?s
Project 2061 Science Assessment Website7. This
website identifies the main ideas in various topics
under Life Science, Physical Science and Earth Sci-
ence, and for each idea provides several sentences
of description along with its individual concepts and
common student misconceptions.
We used 3 topics (17 ideas, averaging 6.2 descrip-
tion sentences, 7.1 concept sentences and 9.9 miscon-
ception sentences each) as a development set:
CE Cells
AM Atoms, Molecules, and States of Matter
PT Plate Tectonics
We used 11 topics (64 ideas, averaging 5.9 descrip-
tion sentences, 9.4 concept sentences and 8.6 miscon-
ception sentences each) as the test set:
5http://lucene.apache.org
6See org.apache.lucene.search.Similarity javadoc for details.
7http://assessment.aaas.org/
17
Model MAP P@1
Randomly ordered 0.607 0.607
Coverage - tokens 0.647 0.471
Coverage - tokens, synsets 0.633 0.529
Coverage - tokens, synsetsexpanded 0.650 0.471
Coverage - tokens, synsetsexpanded?4 0.690 0.706
Retrieval - tokens 0.665 0.529
Retrieval - tokens, synsets 0.641 0.471
Retrieval - tokens, synsetsexpanded 0.650 0.529
Retrieval - tokens, synsetsexpanded?4 0.684 0.647
Table 4: Development set results for identifying miscon-
ceptions.
EN Evolution and Natural Selection
BF Human Body Systems
IE Interdependence in Ecosystems
ME Matter and Energy in Living Systems
RH Reproduction, Genes, and Heredity
EG Energy: Forms, Transformation, Transfer. . .
FM Force and Motion
SC Substances, Chemical Reactions. . .
WC Weather and Climate: Basic Elements
CL Weather and Climate: Seasonal Differences
WE Weathering, Erosion, and Deposition
For the evaluation, we provide all of the idea?s de-
scription sentences as the domain knowledge, and
combine all of an idea?s concepts and misconcep-
tions into a ?student essay?8. We then ask the system
to rank the sentences in the essay, placing miscon-
ceptions above true concepts. Accuracy at placing
misconceptions at the top of the ranked list is then
measured using mean average precision (MAP) and
precision at the first item (P@1).
The models were compared to a chance baseline:
the expected MAP and P@1 if the concept and mis-
conception sentences were ordered randomly. Table 4
shows that on the development set, while all models
outperformed the random ordering baseline?s MAP
(0.607), only models with lexical expansion from
4-link WordNet chains outperformed the baseline?s
P@1 (0.607). The Coverage and Retrieval models us-
ing this expansion technique had comparable MAPs
8These ?student essays? are a naive approximation of real
essays, but the sentences are at least drawn from real student er-
rors. In the future, we hope to create an evaluation corpus where
real student essays have been annotated for misconceptions.
Model MAP P@1
Randomly ordered 0.487 0.487
Coverage - tokens, synsetsexpanded?4 0.603 0.578
Retrieval - tokens, synsetsexpanded?4 0.644 0.625
Table 5: Test set results for identifying misconceptions.
(0.690 vs. 0.684), but the Coverage model had a
higher P@1 (0.706 vs. 0.647). These top two mis-
conception identification models were evaluated on
the test set. Table 5 shows that both models again
outperformed the random ordering baseline, and the
Retrieval model outperformed the Coverage model
(0.644 vs. 0.603 MAP, 0.625 vs. 0.578 P@1).
7 Aligning misconceptions to concepts
The goal of this component is to take the miscon-
ception sentences identified in a student essay and
align them to the core science concepts identified for
the domain. For example, a student misconception
like Earth?s plates cannot bend would be aligned to
a science concept like Mountains form when plate
material slowly bends over time.
7.1 Models
The model for misconception-concept alignment
takes a similar approach to that of the Retrieval
model for misconception identification. The align-
ment model applies lexical expansion to each word
in a core science concept, indexes the expanded con-
cepts with an information retrieval system, and scores
each concept for its relevance to a student misconcep-
tion by querying the index with the misconception
and returning the index?s score for that concept. For-
mally:
SCORE(c) = SCORElucene(m, EXPAND(c))
where m is the query misconception, c is the science
concept, and EXPAND and SCORElucene are defined
as in the Retrieval model for misconception identi-
fication. The concept with the highest score is the
concept that best aligns to the student misconception
according to the model.
For lexical expansion, we consider the same defini-
tions of EXPAND as for misconception identification:
tokens; tokens, synsets; tokens, synsetsexpanded;
and tokens, synsetsexpanded?4.
18
Model MAP P@1
Randomly ordered 0.276 0.276
Alignment - Tokens 0.731 0.639
Alignment - Tokens, synsets 0.813 0.734
Alignment - tokens, synsetsexpanded 0.790 0.698
Alignment - Tokens, synsetsexpanded?4 0.762 0.639
Table 6: Development set results for aligning concepts to
misconceptions.
7.2 Evaluation
We again leverage the AAAS Science Assessments to
evaluate the misconception-concept alignment mod-
els. In addition to identifying key science ideas, and
the concepts and common misconceptions within
each idea, the AAAS Science Assessments provide
links between the misconceptions and the concepts.
Usually there is a single concept to which each mis-
conception is aligned, but the AAAS data aligns as
many as 16 concepts to a misconception in some
cases.
For the evaluation, we give the system one miscon-
ception from an idea, and the list of all concepts from
that idea, and ask the system to rank the concepts9.
If the system performs well, the concepts that are
aligned to the misconception should be ranked above
the other concepts. Accuracy at placing the aligned
concepts at the top of the ranked list is then measured
using mean average precision (MAP) and precision
at the first item (P@1).
The models were compared to a chance baseline:
the expected MAP and P@1 if the concept and mis-
conception sentences were ordered randomly. Ta-
ble 6 shows that on the development set, all models
outperformed the random ordering baseline. Lexi-
cal expansion with tokens and synsets achieved the
highest performance, 0.813 MAP and 0.734 P@1.
This model was evaluated on the test set, and Table 7
shows that the model again outperformed the random
ordering baseline, achieving 0.704 MAP and 0.611
P@1. Overall, these are promising results ? given a
student misconception, the model?s first choice for a
concept to address the misconception is helpful more
than 60% of the time.
9As discussed in Section 6.2, there are on average 9.4 con-
cepts per item. This is not too far off from the 10-20 core con-
cepts we typically expect the tutor to extract for each domain.
Model MAP P@1
Randomly ordered 0.259 0.259
Alignment - Tokens, synsets 0.704 0.611
Table 7: Test set results for aligning concepts to miscon-
ceptions.
8 Recommending resources
The goal of this component is to take a set of student
misconceptions, the core science concepts to which
each misconception is aligned, and the pedagogical
ordering of the core science concepts, and recom-
mend digital resources (webpages) to address the
most important of the misconceptions. For example,
a student that believes that water evaporates into the
air only when the air is very warm might be directed
to websites about evaporation and condensation. The
recommended resources are intended to help the stu-
dent quickly locate the concept knowledge necessary
to correct each of their misconceptions.
8.1 Models
The intuition behind our model is simple: sentences
from recommended resources should contain the
same or lexically related terminology as both the
misconception sentences and their aligned concepts.
As a first approach to this problem, we focus on the
overlap between recommended sentences and the
misconception sentences, and use an information re-
trieval approach to build a resource recommender.
First, the user gives the model a set of domain
knowledge webpages, and we use an information re-
trieval system (Lucene) to index each sentence from
each of the webpages. (Note that we index all sen-
tences, not just core concept sentences.) Given a
student misconception, we query the index and iden-
tify the source URL for each sentence that is returned.
We then return the list of the recommended URLs,
keeping only the first instance of each URL if dupli-
cates exist. Formally:
SCORE(url) = max
s?url
SCORElucene(m, s)
where url is a domain resource, s is a sentence from a
domain resource and m is the student misconception.
URLs are ranked by score and the top k URLs are
returned as recommendations.
19
8.2 Evaluation
As a preliminary evaluation of the resource recom-
mendation model, we obtained student misconcep-
tion sentences that had been aligned to concepts in
a knowledge map of plate tectonics (Ahmad, 2009).
The concepts in the knowledge map were originally
drawn from 37 domain webpages, thus each concept
could serve as a link between a student misconcep-
tion and a recommended webpage. For evaluation,
we took all 11 misconceptions for a single student,
where each misconception had been aligned through
the concepts to on average 3.4 URLs. For each mis-
conception, we asked the recommender model to
rank the 37 domain URLs in order of their relevance
to the student misconception.
We expect the final interactive essay writing sys-
tem to return up to k = 5 resources for each mis-
conception, so we evaluated the performance of the
recommender model in terms of precision at five
(P@5). That is, of the top five URLs recommended
by the system, how many were also recommended
by the experts? Averaging over the 11 student mis-
conception queries, the current model achieves P@5
of 32%, an acceptable initial baseline as randomly
recommending resources would achieve only P@5
of 9%.
9 Discussion
In this article, we have presented our initial steps
towards an interactive essay writing system that can
help students identify and remedy misconceptions in
their science knowledge. The system relies on tech-
niques drawn from a variety of areas of natural lan-
guage processing research, including multi-document
summarization, textual entailment and information
retrieval. Each component has been evaluated inde-
pendently and demonstrated promising initial perfor-
mance.
A variety of challenges remain for this effort. The
core concept identification system performs well on
the plate tectonics domain that it was originally de-
veloped for, but poorer on more distant domains,
suggesting the need for more domain-independent
features. The model for sequencing science concepts
pedagogically uses only the most basic of word-based
features, and could potentially benefit from features
drawn from other research areas such as text readabil-
ity. The misconception identification and alignment
models perform well on the AAAS science assess-
ments but have not yet been evaluated on real student
essays, which may require moving from lexical cover-
age models to more sophisticated entailment models.
Finally, the recommender model considers only in-
formation about the misconception sentence (not the
aligned core concept nor the pedagogical ordering of
concepts) and recommends entire resources instead
of directing students to specifically relevant sentences
or paragraphs.
Perhaps the most important challenge for this work
will be moving from evaluating the components in-
dependently to a whole-system evaluation in the con-
text of a real essay writing task. We are currently
designing a study to gather data on students using the
system, from which we hope to derive information
about which components are most reliable or useful
to the students. This information will help guide our
research to focus on improving the components that
yield the greatest benefits to the students.
References
[Ahmad2009] Faisal Ahmad. 2009. Generating conceptu-
ally personalized interactions for educational digital
libraries using concept maps. Ph.D. thesis, University
of Colorado at Boulder.
[Azevedo et al2008] Roger Azevedo, Amy Witherspoon,
Arthur Graesser, Danielle McNamara, Vasile Rus,
Zhiqiang Cai, Mihai Lintean, and Emily Siler. 2008.
MetaTutor: An adaptive hypermedia system for train-
ing and fostering self-regulated learning about complex
science topics. In Meeting of Society for Computers in
Psychology, November.
[Dagan et al2006] Ido Dagan, Oren Glickman, and Ber-
nardo Magnini. 2006. The PASCAL recognising
textual entailment challenge. In Joaquin Quin?onero
Candela, Ido Dagan, Bernardo Magnini, and Florence
d?Alche? Buc, editors, Machine Learning Challenges.
Evaluating Predictive Uncertainty, Visual Object Clas-
sification, and Recognising Tectual Entailment, volume
3944 of Lecture Notes in Computer Science, pages 177?
190. Springer Berlin / Heidelberg.
[de la Chica et al2008a] Sebastian de la Chica, Faisal Ah-
mad, James H. Martin, and Tamara Sumner. 2008a.
Pedagogically useful extractive summaries for science
education. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 177?184, Stroudsburg, PA, USA.
Association for Computational Linguistics.
20
[de la Chica et al2008b] Sebastian de la Chica, Faisal Ah-
mad, Tamara Sumner, James H. Martin, and Kirsten
Butcher. 2008b. Computational foundations for person-
alizing instruction with digital libraries. International
Journal on Digital Libraries, 9(1):3?18, July.
[Graesser et al2004] Arthur Graesser, Shulan Lu, George
Jackson, Heather Mitchell, Mathew Ventura, Andrew
Olney, and Max Louwerse. 2004. AutoTutor: A tutor
with dialogue in natural language. Behavior Research
Methods, 36:180?192.
[Graff2002] David Graff. 2002. English Gigaword. Lin-
guistic Data Consortium.
[NRC1996] National Research Council NRC. 1996.
National Science Education Standards. National
Academy Press, Washington DC.
[Project2061.1993] Project2061. 1993. Benchmarks for
Science Literacy. Oxford University Press, New York,
United States.
[Radev et al2004] Dragomir R. Radev, Hongyan Jing,
Ma?gorzata Stys?, and Daniel Tam. 2004. Centroid-
based summarization of multiple documents. Inf. Pro-
cess. Manage., 40(6):919?938, November.
[Shnarch et al2011] Eyal Shnarch, Jacob Goldberger, and
Ido Dagan. 2011. A probabilistic modeling frame-
work for lexical entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 558?563, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
[Tanaka-Ishii et al2010] K. Tanaka-Ishii, S. Tezuka, and
H. Terada. 2010. Sorting texts by readability. Compu-
tational Linguistics, 36(2):203?227.
[VanLehn et al2002] Kurt VanLehn, Pamela Jordan, Car-
olyn Rose?, Dumisizwe Bhembe, Michael Bo?ttner, Andy
Gaydos, Maxim Makatchev, Umarani Pappuswamy,
Michael Ringenberg, Antonio Roque, Stephanie Siler,
and Ramesh Srivastava. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay writ-
ing. In Stefano Cerri, Guy Gouarde`res, and Fa?bio
Paraguac?u, editors, Intelligent Tutoring Systems, vol-
ume 2363 of Lecture Notes in Computer Science, pages
158?167. Springer Berlin / Heidelberg.
[Wade-Stein and Kintsch2004] David Wade-Stein and
Eileen Kintsch. 2004. Summary Street: Interactive
computer support for writing. Cognition and Instruc-
tion, 22(3):333?362.
21
