Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 245?253,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using a Dependency Parser to Improve SMT for Subject-Object-Verb
Languages
Peng Xu, Jaeho Kang, Michael Ringgaard and Franz Och
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{xp,jhkang,ringgaard,och}@google.com
Abstract
We introduce a novel precedence reordering
approach based on a dependency parser to sta-
tistical machine translation systems. Similar
to other preprocessing reordering approaches,
our method can efficiently incorporate linguis-
tic knowledge into SMT systems without in-
creasing the complexity of decoding. For a set
of five subject-object-verb (SOV) order lan-
guages, we show significant improvements in
BLEU scores when translating from English,
compared to other reordering approaches, in
state-of-the-art phrase-based SMT systems.
1 Introduction
Over the past ten years, statistical machine transla-
tion has seen many exciting developments. Phrase-
based systems (Och, 2002; Koehn et.al., 2003;
Och and Ney, 2004) advanced the machine transla-
tion field by allowing translations of word sequences
(a.k.a., phrases) instead of single words. This ap-
proach has since been the state-of-the-art because of
its robustness in modeling local word reordering and
the existence of an efficient dynamic programming
decoding algorithm.
However, when phrase-based systems are used
between languages with very different word or-
ders, such as between subject-verb-object (SVO)
and subject-object-verb (SOV) languages, long dis-
tance reordering becomes one of the key weak-
nesses. Many reordering methods have been pro-
posed in recent years to address this problem in dif-
ferent aspects.
The first class of approaches tries to explicitly
model phrase reordering distances. Distance based
distortion model (Och, 2002; Koehn et.al., 2003) is
a simple way of modeling phrase level reordering.
It penalizes non-monotonicity by applying a weight
to the number of words between two source phrases
corresponding to two consecutive target phrases.
Later on, this model was extended to lexicalized
phrase reordering (Tillmann, 2004; Koehn, et.al.,
2005; Al-Onaizan and Papineni, 2006) by applying
different weights to different phrases. Most recently,
a hierarchical phrase reordering model (Galley and
Manning, 2008) was proposed to dynamically deter-
mine phrase boundaries using efficient shift-reduce
parsing. Along this line of research, discrimina-
tive reordering models based on a maximum entropy
classifier (Zens and Ney, 2006; Xiong, et.al., 2006)
also showed improvements over the distance based
distortion model. None of these reordering models
changes the word alignment step in SMT systems,
therefore, they can not recover from the word align-
ment errors. These models are also limited by a
maximum allowed reordering distance often used in
decoding.
The second class of approaches puts syntactic
analysis of the target language into both modeling
and decoding. It has been shown that direct model-
ing of target language constituents movement in ei-
ther constituency trees (Yamada and Knight, 2001;
Galley et.al., 2006; Zollmann et.al., 2008) or depen-
dency trees (Quirk, et.al., 2005) can result in signifi-
cant improvements in translation quality for translat-
ing languages like Chinese and Arabic into English.
A simpler alternative, the hierarchical phrase-based
245
approach (Chiang, 2005; Wu, 1997) also showed
promising results for translating Chinese to English.
Similar to the distance based reordering models, the
syntactical or hierarchical approaches also rely on
other models to get word alignments. These mod-
els typically combine machine translation decoding
with chart parsing, therefore significantly increase
the decoding complexity. Even though some re-
cent work has shown great improvements in decod-
ing efficiency for syntactical and hierarchical ap-
proaches (Huang and Chiang, 2007), they are still
not as efficient as phrase-based systems, especially
when higher order language models are used.
Finally, researchers have also tried to put source
language syntax into reordering in machine trans-
lation. Syntactical analysis of source language
can be used to deterministically reorder input sen-
tences (Xia and McCord, 2004; Collins et.al., 2005;
Wang et.al., 2007; Habash, 2007), or to provide mul-
tiple orderings as weighted options (Zhang et.al.,
2007; Li et.al., 2007; Elming, 2008). In these
approaches, input source sentences are reordered
based on syntactic analysis and some reordering
rules at preprocessing step. The reordering rules
can be either manually written or automatically ex-
tracted from data. Deterministic reordering based on
syntactic analysis for the input sentences provides
a good way of resolving long distance reordering,
without introducing complexity to the decoding pro-
cess. Therefore, it can be efficiently incorporated
into phrase-based systems. Furthermore, when the
same preprocessing reordering is performed for the
training data, we can still apply other reordering ap-
proaches, such as distance based reordering and hi-
erarchical phrase reordering, to capture additional
local reordering phenomena that are not captured by
the preprocessing reordering. The work presented in
this paper is largely motivated by the preprocessing
reordering approaches.
In the rest of the paper, we first introduce our de-
pendency parser based reordering approach based on
the analysis of the key issues when translating SVO
languages to SOV languages. Then, we show exper-
imental results of applying this approach to phrase-
based SMT systems for translating from English to
five SOV languages (Korean, Japanese, Hindi, Urdu
and Turkish). After showing that this approach can
also be beneficial for hierarchical phrase-based sys-
John can hit ballthe
?? ? ??
? ????? 
.
.
Figure 1: Example Alignment Between an English and a
Korean Sentence
tems, we will conclude the paper with future re-
search directions.
2 Translation between SVO and SOV
Languages
In linguistics, it is possible to define a basic word
order in terms of the verb (V) and its arguments,
subject (S) and object (O). Among all six possible
permutations, SVO and SOV are the most common.
Therefore, translating between SVO and SOV lan-
guages is a very important area to study. We use
English as a representative of SVO languages and
Korean as a representative for SOV languages in our
discussion about the word orders.
Figure 1 gives an example sentence in English and
its corresponding translation in Korean, along with
the alignments between the words. Assume that we
split the sentences into four phrases: (John , t@),
(can hit , `  ????), (the ball , ? ?D)
and (. , .). Since a phrase-based decoder generates
the translation from left to right, the following steps
need to happen when we translate from English to
Korean:
? Starts from the beginning of the sentence,
translates ?John? to ?t@?;
? Jumps to the right by two words, translates ?the
ball? to ???D?;
? Jumps to the left by four words, translates ?can
hit? to ?`?????;
? Finally, jumps to the right by two words, trans-
lates ?.? to ?.?.
It is clear that in order for the phrase-based decoder
to successfully carry out all of the reordering steps, a
very strong reordering model is required. When the
sentence gets longer with more complex structure,
the number of words to move over during decod-
ing can be quite high. Imagine when we translate
246
Figure 2: Dependency Parse Tree of an Example English
Sentence
the sentence ?English is used as the first or second
language in many countries around the world .?.
The decoder needs to make a jump of 13 words in
order to put the translation of ?is used? at the end
of the translation. Normally in a phrase-based de-
coder, very long distance reordering is not allowed
because of efficiency considerations. Therefore, it
is very difficult in general to translate English into
Korean with proper word order.
However, knowing the dependency parse trees of
the English sentences may simplify the reordering
problem significantly. In the simple example in Fig-
ure 1, if we analyze the English sentence and know
that ?John? is the subject, ?can hit? is the verb and
?the ball? is the object, we can reorder the English
into SOV order. The resulting sentence ?John the
ball can hit .? will only need monotonic translation.
This motivates us to use a dependency parser for En-
glish to perform the reordering.
3 Precedence Reordering Based on a
Dependency Parser
Figure 2 shows the dependency tree for the example
sentence in the previous section. In this parse, the
verb ?hit? has four children: a subject noun ?John?,
an auxiliary verb ?can?, an object noun ?ball? and a
punctuation ?.?. When transforming the sentence to
SOV order, we need to move the object noun and the
subtree rooted at it to the front of the head verb, but
after the subject noun. We can have a simple rule to
achieve this.
However, in reality, there are many possible chil-
dren for a verb. These children have some relative
ordering that is typically fixed for SOV languages.
In order to describe this kind of ordering, we pro-
pose precedence reordering rules based on a depen-
dency parse tree. All rules here are based English
and Korean examples, but they also apply to other
SOV languages, as we will show later empirically.
A precedence reordering rule is a mapping from
T to a set of tuples {(L,W,O)}, where T is the
part-of-speech (POS) tag of the head in a depen-
dency parse tree node, L is a dependency label for
a child node, W is a weight indicating the order of
that child node and O is the type of order (either
NORMAL or REVERSE). The type of order is only
used when we have multiple children with the same
weight, while the weight is used to determine the
relative order of the children, going from largest to
smallest. The weight can be any real valued num-
ber. The order type NORMAL means we preserve
the original order of the children, while REVERSE
means we flip the order. We reserve a special label
self to refer to the head node itself so that we can
apply a weight to the head, too. We will call this
tuple a precedence tuple in later discussions. In this
study, we use manually created rules only.
Suppose we have a precedence rule: VB ?
(nsubj, 2, NORMAL), (dobj, 1, NORMAL), (self,
0, NORMAL). For the example shown in Figure 2,
we would apply it to the ROOT node and result in
?John the ball can hit .?.
Given a set of rules, we apply them in a depen-
dency tree recursively starting from the root node. If
the POS tag of a node matches the left-hand-side of
a rule, the rule is applied and the order of the sen-
tence is changed. We go through all children of the
node and get the precedence weights for them from
the set of precedence tuples. If we encounter a child
node that has a dependency label not listed in the set
of tuples, we give it a default weight of 0 and de-
fault order type of NORMAL. The children nodes
are sorted according to their weights from highest to
lowest, and nodes with the same weights are ordered
according to the type of order defined in the rule.
3.1 Verb Precedence Rules
Verb movement is the most important movement
when translating from English (SVO) to Korean
(SOV). In a dependency parse tree, a verb node can
potentially have many children. For example, aux-
iliary and passive auxiliary verbs are often grouped
together with the main verb and moved together with
it. The order, however, is reversed after the move-
ment. In the example of Figure 2, the correct Korean
247
     
??
? ??
? ????? 
   
??? ?
.
Figure 3: Dependency Parse Tree with Alignment for a
Sentence with Preposition Modifier
word order is ?` (hit)  ????(can) . Other
categories that are in the same group are phrasal verb
particle and negation.
If the verb in an English sentence has a preposi-
tional phrase as a child, the prepositional phrase is
often placed before the direct object in the Korean
counterpart. As shown in Figure 3, ?)?t \?
(?with a bat?) is actually between ?t@? (?John?)
and ???D? (?the ball?).
Another common reordering phenomenon is
when a verb has an adverbial clause modifier. In that
case, the whole adverbial clause is moved together to
be in front of the subject of the main sentence. Inside
the adverbial clause, the ordering follows the same
verb reordering rules, so we recursively reorder the
clause.
Our verb precedence rule, as in Table 1, can cover
all of the above reordering phenomena. One way
to interpret this rule set is as follows: for any node
whose POS tag is matches VB* (VB, VBZ, VBD,
VBP, VBN, VBG), we group the children node that
are phrasal verb particle (prt), auxiliary verb (aux),
passive auxiliary verb (auxpass), negation (neg) and
the verb itself (self) together and reverse them. This
verb group is moved to the end of the sentence. We
move adverbial clause modifier to the beginning of
the sentence, followed by a group of noun subject
(nsubj), preposition modifier and anything else not
listed in the table, in their original order. Right be-
fore the verb group, we put the direct object (dobj).
Note that all of the children are optional.
3.2 Adjective Precedence Rules
Similar to the verbs, adjectives can also take an aux-
iliary verb, a passive auxiliary verb and a negation
T (L, W, O)
VB*
(advcl, 1, NORMAL)
(nsubj, 0, NORMAL)
(prep, 0, NORMAL)
(dobj, -1, NORMAL)
(prt, -2, REVERSE)
(aux, -2, REVERSE)
(auxpass, -2, REVERSE)
(neg, -2, REVERSE)
(self, -2, REVERSE)
JJ or JJS or JJR
(advcl, 1, NORMAL)
(self, -1, NORMAL)
(aux, -2, REVERSE)
(auxpass, -2, REVERSE)
(neg, -2, REVERSE)
(cop, -2, REVERSE)
NN or NNS
(prep, 2, NORMAL)
(rcmod, 1, NORMAL)
(self, 0, NORMAL)
IN or TO (pobj, 1, NORMAL)(self, -1, NORMAL)
Table 1: Precedence Rules to Reorder English to SOV
Language Order (These rules were extracted manually by
a bilingual speaker after looking at some text book exam-
ples in English and Korean, and the dependency parse
trees of the English examples.)
as modifiers. In such cases, the change in order from
English to Korean is similar to the verb rule, except
that the head adjective itself should be in front of the
verbs. Therefore, in our adjective precedence rule in
the second panel of Table 1, we group the auxiliary
verb, the passive auxiliary verb and the negation and
move them together after reversing their order. They
are moved to right after the head adjective, which is
put after any other modifiers.
For both verb and adjective precedence rules,
we also apply some heuristics to prevent exces-
sive movements. In order to do this, we disallow
any movement across punctuation and conjunctions.
Therefore, for sentences like ?John hit the ball but
Sam threw the ball?, the reordering result would be
?John the ball hit but Sam the ball threw?, instead
of ?John the ball but Sam the ball threw hit?.
3.3 Noun and Preposition Precedence Rules
In Korean, when a noun is modified by a preposi-
tional phrase, such as in ?the way to happiness?,
the prepositional phrase is usually moved in front of
the noun, resulting in ??? (happiness)<\ ?
8 (to the way)? . Similarly for relative clause mod-
ifier, it is also reordered to the front of the head noun.
For preposition head node with an object modifier,
248
the order is the object first and the preposition last.
One example is ?with a bat? in Figure 3. It corre-
sponds to ?)?t (a bat) \(with)?. We handle
these types of reordering by the noun and preposi-
tion precedence rules in the third and fourth panel of
Table 1.
With the rules defined in Table 1, we now show a
more complex example in Figure 4. First, the ROOT
node matches an adjective rule, with four children
nodes labeled as (csubj, cop, advcl, p), and with
precedence weights of (0, -2, 1, 0). The ROOT node
itself has a weight of -1. After reordering, the sen-
tence becomes: ?because we do n?t know what the
future has Living exciting is .?. Note that the whole
adverbial phrase rooted at ?know? is moved to the
beginning of the sentence. After that, we see that
the child node rooted at ?know? matches a verb rule,
with five children nodes labeled as (mark, nsubj,
aux, neg, ccomp), with weights (0, 0, -2, -2, 0). In
this case, the verb itself also has weight -2. Now
we have two groups of nodes, with weight 0 and -2,
respectively. The first group has a NORMAL order
and the second group has a REVERSE order. Af-
ter reordering, the sentence becomes: ?because we
what the future has know n?t do Living exciting
is .?. Finally, we have another node rooted at ?has?
that matches the verb rule again. After the final re-
ordering, we end up with the sentence: ?because we
the future what has know n?t do Living exciting
is .?. We can see in Figure 4 that this sentence has an
almost monotonic alignment with a reasonable Ko-
rean translation shown in the figure1.
4 Related Work
As we mentioned in our introduction, there have
been several studies in applying source sentence re-
ordering using syntactical analysis for statistical ma-
chine translation. Our precedence reordering ap-
proach based on a dependency parser is motivated by
those previous works, but we also distinguish from
their studies in various ways.
Several approaches use syntactical analysis to
provide multiple source sentence reordering options
through word lattices (Zhang et.al., 2007; Li et.al.,
2007; Elming, 2008). A key difference between
1We could have improved the rules by using a weight of -3
for the label ?mark?, but it was not in our original set of rules.
their approaches and ours is that they do not perform
reordering during training. Therefore, they would
need to rely on reorder units that are likely not vio-
lating ?phrase? boundaries. However, since we re-
order both training and test data, our system oper-
ates in a matched condition. They also focus on ei-
ther Chinese to English (Zhang et.al., 2007; Li et.al.,
2007) or English to Danish (Elming, 2008), which
arguably have less long distance reordering than be-
tween English and SOV languages.
Studies most similar to ours are those preprocess-
ing reordering approaches (Xia and McCord, 2004;
Collins et.al., 2005; Wang et.al., 2007; Habash,
2007). They all perform reordering during prepro-
cessing based on either automatically extracted syn-
tactic rules (Xia and McCord, 2004; Habash, 2007)
or manually written rules (Collins et.al., 2005; Wang
et.al., 2007). Compared to these approaches, our
work has a few differences. First of all, we study
a wide range of SOV languages using manually ex-
tracted precedence rules, not just for one language
like in these studies. Second, as we will show in
the next section, we compare our approach to a
very strong baseline with more advanced distance
based reordering model, not just the simplest distor-
tion model. Third, our precedence reordering rules,
like those in Habash, 2007, are more flexible than
those other rules. Using just one verb rule, we can
perform the reordering of subject, object, preposi-
tion modifier, auxiliary verb, negation and the head
verb. Although we use manually written rules in
this study, it is possible to learn our rules automat-
ically from alignments, similarly to Habash, 2007.
However, unlike Habash, 2007, our manually writ-
ten rules handle unseen children and their order nat-
urally because we have a default precedence weight
and order type, and we do not need to match an often
too specific condition, but rather just treat all chil-
dren independently. Therefore, we do not need to
use any backoff scheme in order to have a broad cov-
erage. Fourth, we use dependency parse trees rather
than constituency trees.
There has been some work on syntactic word or-
der model for English to Japanese machine transla-
tion (Chang and Toutanova, 2007). In this work, a
global word order model is proposed based on fea-
tures including word bigram of the target sentence,
displacements and POS tags on both source and tar-
249
                  
??? ???
  ?????????? ? ?
?
? ??
??? ???
.
we the Livingwhatfuture knowhas n't do excitingbecause is .
csubj cop detmarkROOT auxnsubj neg advcl nsubjdobj ccomp p
Living is thebecauseexciting dowe n't know futurewhat has
.
VBG VBZ DTINJJ VBPPRP RB VB NNWP VBZ
.
Label
Token
POS
Figure 4: A Complex Reordering Example (Reordered English sentence and alignments are at the bottom.)
get sides. They build a log-linear model using these
features and apply the model to re-rank N -best lists
from a baseline decoder. Although we also study the
reordering problem in English to Japanese transla-
tion, our approach is to incorporate the linguistically
motivated reordering directly into modeling and de-
coding.
5 Experiments
We carried out all our experiments based on a state-
of-the-art phrase-based statistical machine transla-
tion system. When training a system for English
to any of the 5 SOV languages, the word alignment
step includes 3 iterations of IBM Model-1 training
and 2 iterations of HMM training. We do not use
Model-4 because it is slow and it does not add much
value to our systems in a pilot study. We use the
standard phrase extraction algorithm (Koehn et.al.,
2003) to get al phrases up to length 5. In addition
to the regular distance distortion model, we incor-
porate a maximum entropy based lexicalized phrase
reordering model (Zens and Ney, 2006) as a fea-
ture used in decoding. In this model, we use 4 re-
ordering classes (+1, > 1, ?1, < ?1) and words
from both source and target as features. For source
words, we use the current aligned word, the word
before the current aligned word and the next aligned
word; for target words, we use the previous two
words in the immediate history. Using this type of
features makes it possible to directly use the maxi-
mum entropy model in the decoding process (Zens
and Ney, 2006). The maximum entropy models are
trained on all events extracted from training data
word alignments using the LBFGS algorithm (Mal-
ouf, 2002). Overall for decoding, we use between 20
System Source Target
English?Korean 303M 267M
English?Japanese 316M 350M
English?Hindi 16M 17M
English?Urdu 17M 19M
English?Turkish 83M 76M
Table 2: Training Corpus Statistics (#words) of Systems
for 5 SOV Languages
to 30 features, whose weights are optimized using
MERT (Och, 2003), with an implementation based
on the lattice MERT (Macherey et.al., 2008).
For parallel training data, we use an in-house col-
lection of parallel documents. They come from var-
ious sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. Therefore, for some doc-
uments in the training data, we do not necessarily
have the exact clean translations. Table 2 shows the
actual statistics about the training data for all five
languages we study. For all 5 SOV languages, we
use the target side of the parallel data and some more
monolingual text from crawling the web to build 4-
gram language models.
We also collected about 10K English sentences
from the web randomly. Among them, 9.5K are used
as evaluation data. Those sentences were translated
by humans to all 5 SOV languages studied in this
paper. Each sentence has only one reference trans-
lation. We split them into 3 subsets: dev contains
3,500 sentences, test contains 1,000 sentences and
the rest of 5,000 sentences are used in a blindtest
set. The dev set is used to perform MERT training,
while the test set is used to select trained weights
due to some nondeterminism of MERT training. We
use IBM BLEU (Papineni et al, 2002) to evaluate
250
our translations and use character level BLEU for
Korean and Japanese.
5.1 Preprocessing Reordering and Reordering
Models
We first compare our precedence rules based prepro-
cessing reordering with the maximum entropy based
lexicalized reordering model. In Table 3, Baseline
is our system with both a distance distortion model
and the maximum entropy based lexicalized reorder-
ing model. For all results reported in this section,
we used a maximum allowed reordering distance of
10. In order to see how the lexicalized reordering
model performs, we also included systems with and
without it (-LR means without it). PR is our pro-
posed approach in this paper. Note that since we ap-
ply precedence reordering rules during preprocess-
ing, we can combine this approach with any other
reordering models used during decoding. The only
difference is that with the precedence reordering, we
would have a different phrase table and in the case
of LR, different maximum entropy models.
In order to implement the precedence rules, we
need a dependency parser. We choose to use a
deterministic inductive dependency parser (Nivre
and Scholz, 2004) for its efficiency and good ac-
curacy. Our implementation of the deterministic
dependency parser using maximum entropy models
as the underlying classifiers achieves 87.8% labeled
attachment score and 88.8% unlabeled attachment
score on standard Penn Treebank evaluation.
As our results in Table 3 show, for all 5 lan-
guages, by using the precedence reordering rules as
described in Table 1, we achieve significantly bet-
ter BLEU scores compared to the baseline system.
In the table, We use two stars (??) to mean that
the statistical significance test using the bootstrap
method (Koehn, 2004) gives an above 95% signif-
icance level when compared to the baselie. We mea-
sured the statistical significance level only for the
blindtest data.
Note that for Korean and Japanese, our prece-
dence reordering rules achieve better absolute
BLEU score improvements than for Hindi, Urdu and
Turkish. Since we only analyzed English and Ko-
rean sentences, it is possible that our rules are more
geared toward Korean. Japanese has almost exactly
the same word order as Korean, so we could assume
Language System dev test blind
Korean
BL 25.8 27.0 26.2
-LR 24.7 25.6 25.1
-LR+PR 27.3 28.3 27.5**
+PR 27.8 28.7 27.9**
Japanese
BL 29.5 29.3 29.3
-LR 29.2 29.0 29.0
-LR+PR 30.3 31.0 30.6**
+PR 30.7 31.2 31.1**
Hindi
BL 19.1 18.9 18.3
-LR 17.4 17.1 16.4
-LR+PR 19.6 18.8 18.7**
+PR 19.9 18.9 18.8**
Urdu
BL 9.7 9.5 8.9
-LR 9.1 8.6 8.2
-LR+PR 10.0 9.6 9.6**
+PR 10.0 9.8 9.6**
Turkish
BL 10.0 10.5 9.8
-LR 9.1 10.0 9.0
-LR+PR 10.5 11.0 10.3**
+PR 10.5 10.9 10.4**
Table 3: BLEU Scores on Dev, Test and Blindtest for En-
glish to 5 SOV Languages with Various Reordering Op-
tions (BL means baseline, LR means maximum entropy
based lexialized phrase reordering model, PR means
precedence rules based preprocessing reordering.)
the benefits can carry over to Japanese.
5.2 Reordering Constraints
One of our motivations of using the precedence re-
ordering rules is that English will look like SOV lan-
guages in word order after reordering. Therefore,
even monotone decoding should be able to produce
better translations. To see this, we carried out a con-
trolled experiment, using Korean as an example.
Clearly, after applying the precedence reordering
rules, our English to Korean system is not sensitive
to the maximum allowed reordering distance any-
more. As shown in Figure 5, without the rules, the
blindtest BLEU scores improve monotonically as
the allowed reordering distance increases. This indi-
cates that the order difference between English and
Korean is very significant. Since smaller allowed
reordering distance directly corresponds to decod-
ing time, we can see that with the same decoding
speed, our proposed approach can achieve almost
5% BLEU score improvements on blindtest set.
5.3 Preprocessing Reordering and
Hierarchical Model
The hierarchical phrase-based approach has been
successfully applied to several systems (Chiang,
251
1 2 4 6 8 10Maximum Allowed Reordering Distance
0.23
0.24
0.25
0.26
0.27
0.28
Blindt
est BL
EU Sc
ore No LexReorderBaselineNo LexReorder, with ParserReorderWith ParserReorder
Figure 5: Blindtest BLEU Score for Different Maximum
Allowed Reordering Distance for English to Korean Sys-
tems with Different Reordering Options
2005; Zollmann et.al., 2008). Since hierarchical
phrase-based systems can capture long distance re-
ordering by using a PSCFG model, we expect it to
perform well in English to SOV language systems.
We use the same training data as described in the
previous sections for building hierarchical systems.
The same 4-gram language models are also used for
the 5 SOV languages. We adopt the SAMT pack-
age (Zollmann and Venugopal, 2006) and follow
similar settings as Zollmann et.al., 2008. We allow
each rule to have at most 6 items on the source side,
including nonterminals and extract rules from initial
phrases of maximum length 12. During decoding,
we allow application of all rules of the grammar for
chart items spanning up to 12 source words.
Since our precedence reordering applies at pre-
processing step, we can train a hierarchical system
after applying the reordering rules. When doing so,
we use exactly the same settings as a regular hier-
archical system. The results for both hierarchical
systems and those combined with the precedence re-
ordering are shown in Table 4, together with the best
normal phrase-based systems we copy from Table 3.
Here again, we mark any blindtest BLEU score that
is better than the corresponding hierarchical system
with confidence level above 95%. Note that the hier-
archical systems can not use the maximum entropy
based lexicalized phrase reordering models.
Except for Hindi, applying the precedence re-
ordering rules in a hierarchical system can achieve
statistically significant improvements over a normal
hierarchical system. We conjecture that this may be
because of the simplicity of our reordering rules.
Language System dev test blind
Korean
PR 27.8 28.7 27.9
Hier 27.4 27.7 27.9
PR+Hier 28.5 29.1 28.8**
Japanese
PR 30.7 31.2 31.1**
Hier 30.5 30.6 30.5
PR+Hier 31.0 31.3 31.1**
Hindi
PR 19.9 18.9 18.8
Hier 20.3 20.3 19.3
PR+Hier 20.0 19.7 19.3
Urdu
PR 10.0 9.8 9.6
Hier 10.4 10.3 10.0
PR+Hier 11.2 10.7 10.7**
Turkish
PR 10.5 10.9 10.4
Hier 11.0 11.8 10.5
PR+Hier 11.1 11.6 10.9**
Table 4: BLEU Scores on Dev, Test and Blindtest for En-
glish to 5 SOV Languages in Hierarchical Phrase-based
Systems (PR is precedence rules based preprocessing re-
ordering, same as in Table 3, while Hier is the hierarchi-
cal system.)
Other than the reordering phenomena covered by
our rules in Table 1, there could be still some local or
long distance reordering. Therefore, using a hierar-
chical phrase-based system can improve those cases.
Another possible reason is that after the reordering
rules apply in preprocessing, English sentences in
the training data are very close to the SOV order. As
a result, EM training becomes much easier and word
alignment quality becomes better. Therefore, a hier-
archical phrase-based system can extract better rules
and hence achievesbetter translation quality.
We also point out that hierarchical phrase-based
systems require a chart parsing algorithm during de-
coding. Compared to the efficient dynamic pro-
gramming in phrase-based systems, it is much
slower. This makes our approach more appealing
in a realtime statistical machine translation system.
6 Conclusion
In this paper, we present a novel precedence re-
ordering approach based on a dependency parser.
We successfully applied this approach to systems
translating English to 5 SOV languages: Korean,
Japanese, Hindi, Urdu and Turkish. For all 5 lan-
guages, we achieve statistically significant improve-
ments in BLEU scores over a state-of-the-art phrase-
based baseline system. The amount of training data
for the 5 languages varies from around 17M to more
than 350M words, including some noisy data from
252
the web. Our proposed approach has shown to be
robust and versatile. For 4 out of the 5 languages,
our approach can even significantly improve over a
hierarchical phrase-based baseline system. As far as
we know, we are the first to show that such reorder-
ing rules benefit several SOV languages.
We believe our rules are flexible and can cover
many linguistic reordering phenomena. The format
of our rules also makes it possible to automatically
extract rules from word aligned corpora. In the fu-
ture, we plan to investigate along this direction and
extend the rules to languages other than SOV.
The preprocessing reordering like ours is known
to be sensitive to parser errors. Some preliminary
error analysis already show that indeed some sen-
tences suffer from parser errors. In the recent years,
several studies have tried to address this issue by us-
ing a word lattice instead of one reordering as in-
put (Zhang et.al., 2007; Li et.al., 2007; Elming,
2008). Although there is clearly room for improve-
ments, we also feel that using one reordering during
training may not be good enough either. It would be
very interesting to investigate ways to have efficient
procedure for training EM models and getting word
alignments using word lattices on the source side of
the parallel data. Along this line of research, we
think some kind of tree-to-string model (Liu et.al.,
2006) could be interesting directions to pursue.
References
Yaser Al-Onaizan and Kishore Papineni 2006. Distortion Models for
Statistical Machine Translation In Proceedings of ACL
Pi-Chuan Chang and Kristina Toutanova 2007. A Discriminative Syn-
tactic Word Order Model for Machine Translation In Proceedings
of ACL
David Chiang 2005. A Hierarchical Phrase-based Model for Statistical
Machine Translation In Proceedings of ACL
Michael Collins, Philipp Koehn and Ivona Kucerova 2005. Clause
Restructuring for Statistical Machine Translation In Proceedings of
ACL
Jakob Elming 2008. Syntactic Reordering Integrated with Phrase-
based SMT In Proceedings of COLING
Michel Galley and Christopher D. Manning 2008. A Simple and Ef-
fective Hierarchical Phrase Reordering Model In Proceedings of
EMNLP
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve
DeNeefe, Wei Wang and Ignacio Thayer 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation Models In Pro-
ceedings of COLING-ACL
Nizar Habash 2007. Syntactic Preprocessing for Statistical Machine
Translation In Proceedings of 11th MT Summit
Liang Huang and David Chiang 2007. Forest Rescoring: Faster De-
coding with Integrated Language Models, In Proceedings of ACL
Philipp Koehn 2004. Statistical Significance Tests for Machine Trans-
lation Evaluation In Proceedings of EMNLP
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot 2005. Edinborgh
System Description for the 2005 IWSLT Speech Translation Evalu-
ation In International Workshop on Spoken Language Translation
Philipp Koehn, Franz J. Och and Daniel Marcu 2003. Statistical
Phrase-based Translation, In Proceedings of HLT-NAACL
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li and Yi
Guan 2007. A Probabilistic Approach to Syntax-based Reordering
for Statistical Machine Translation, In Proceedings of ACL
Yang Liu, Qun Liu and Shouxun Lin 2006. Tree-to-string Alignment
Template for Statistical Machine Translation, In Proceedings of
COLING-ACL
Wolfgang Macherey, Franz J. Och, Ignacio Thayer and Jakob Uszkoreit
2008. Lattice-based Minimum Error Rate Training for Statistical
Machine Translation In Proceedings of EMNLP
Robert Malouf 2002. A comparison of algorithms for maximum en-
tropy parameter estimation In Proceedings of the Sixth Workshop
on Computational Language Learning (CoNLL-2002)
Joakim Nivre and Mario Scholz 2004. Deterministic Dependency Pars-
ing for English Text. In Proceedings of COLING
Franz J. Och 2002. Statistical Machine Translation: From Single Word
Models to Alignment Template Ph.D. Thesis, RWTH Aachen, Ger-
many
Franz J. Och. 2003. Minimum Error Rate Training in Statistical Ma-
chine Translation. In Proceedings of ACL
Franz J. Och and Hermann Ney 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computational Linguis-
tics, 30:417-449
Kishore Papineni, Roukos, Salim et al 2002. BLEU: A Method for
Automatic Evaluation of Machine Translation. In Proceedings of
ACL
Chris Quirk, Arul Menezes and Colin Cherry 2005. Dependency Tree
Translation: Syntactically Informed Phrasal SMT In Proceedings of
ACL
Christoph Tillmann 2004. A Block Orientation Model for Statistical
Machine Translation In Proceedings of HLT-NAACL
Chao Wang, Michael Collins and Philipp Koehn 2007. Chinese Syntac-
tic Reordering for Statistical Machine Translation In Proceedings of
EMNLP-CoNLL
Dekai Wu 1997. Stochastic Inversion Transduction Grammars and
Bilingual Parsing of Parallel Corpus In Computational Linguistics
23(3):377-403
Fei Xia and Michael McCord 2004. Improving a Statistical MT Sys-
tem with Automatically Learned Rewrite Patterns In Proceedings of
COLING
Deyi Xiong, Qun Liu and Shouxun Lin 2006. Maximum Entropy
Based Phrase Reordering Model for Statistical Machine Translation
In Proceedings of COLING-ACL
Kenji Yamada and Kevin Knight 2001. A Syntax-based Statistical
Translation Model In Proceedings of ACL
Yuqi Zhang, Richard Zens and Hermann Ney 2007. Improve Chunk-
level Reordering for Statistical Machine Translation In Proceedings
of IWSLT
Richard Zens and Hermann Ney 2006. Discriminative Reordering
Models for Statistical Machine Translation In Proceedings of the
Workshop on Statistical Machine Translation, HLT-NAACL pages
55-63
Andreas Zollmann and Ashish Venugopal 2006. Syntax Augmented
Machine Translation via Chart Parsing In Proceedings of NAACL
2006 - Workshop on Statistical Machine Translation
Andreas Zollmann, Ashish Venugopal, Franz Och and Jay Ponte
2008. A Systematic Comparison of Phrase-Based, Hierarchical and
Syntax-Augmented Statistical MT In Proceedings of COLING
253
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705?713,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Uptraining for Accurate Deterministic Question Parsing
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, Hiyan Alshawi
Google Research
{slav,pichuan,ringgaard,hiyan}@google.com
Abstract
It is well known that parsing accuracies drop
significantly on out-of-domain data. What is
less known is that some parsers suffer more
from domain shifts than others. We show
that dependency parsers have more difficulty
parsing questions than constituency parsers.
In particular, deterministic shift-reduce depen-
dency parsers, which are of highest interest
for practical applications because of their lin-
ear running time, drop to 60% labeled accu-
racy on a question test set. We propose an
uptraining procedure in which a deterministic
parser is trained on the output of a more ac-
curate, but slower, latent variable constituency
parser (converted to dependencies). Uptrain-
ing with 100K unlabeled questions achieves
results comparable to having 2K labeled ques-
tions for training. With 100K unlabeled and
2K labeled questions, uptraining is able to
improve parsing accuracy to 84%, closing
the gap between in-domain and out-of-domain
performance.
1 Introduction
Parsing accuracies on the popular Section 23 of the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank have been steadily improving over the past
decade. At this point, we have many different pars-
ing models that reach and even surpass 90% depen-
dency or constituency accuracy on this test set (Mc-
Donald et al, 2006; Nivre et al, 2007; Charniak and
Johnson, 2005; Petrov et al, 2006; Carreras et al,
2008; Koo and Collins, 2010). Quite impressively,
models based on deterministic shift-reduce parsing
algorithms are able to rival the other computation-
ally more expensive models (see Nivre (2008) and
references therein for more details). Their linear
running time makes them ideal candidates for large
scale text processing, and our model of choice for
this paper.
Unfortunately, the parsing accuracies of all mod-
els have been reported to drop significantly on out-
of-domain test sets, due to shifts in vocabulary and
grammar usage (Gildea, 2001; McClosky et al,
2006b; Foster, 2010). In this paper, we focus our
attention on the task of parsing questions. Questions
pose interesting challenges for WSJ-trained parsers
because they are heavily underrepresented in the
training data (there are only 334 questions among
the 39,832 training sentences). At the same time,
questions are of particular interest for user facing
applications like question answering or web search,
which necessitate parsers that can process questions
in a fast and accurate manner.
We start our investigation in Section 3 by train-
ing several state-of-the-art (dependency and con-
stituency) parsers on the standard WSJ training set.
When evaluated on a question corpus, we observe
dramatic accuracy drops exceeding 20% for the de-
terministic shift-reduce parsers. In general, depen-
dency parsers (McDonald et al, 2006; Nivre et al,
2007), seem to suffer more from this domain change
than constituency parsers (Charniak and Johnson,
2005; Petrov et al, 2006). Overall, the latent vari-
able approach of Petrov et al (2006) appears to gen-
eralize best to this new domain, losing only about
5%. Unfortunately, the parsers that generalize better
to this new domain have time complexities that are
cubic in the sentence length (or even higher), render-
ing them impractical for web-scale text processing.
705
SBARQ
WHNP
WP
What
SQ
VBZ
does
NP
DT
the
NNP
Peugeot
NN
company
VP
VB
manufacture
.
?
(a)
What does the Peugeot company manufacture ?ROOT
dobj aux det nn p   nsubjroot
(b)
Figure 1: Example constituency tree from the QuestionBank (a) converted to labeled Stanford dependencies (b).
We therefore propose an uptraining method, in
which a deterministic shift-reduce parser is trained
on the output of a more accurate, but slower parser
(Section 4). This type of domain adaptation is rem-
iniscent of self-training (McClosky et al, 2006a;
Huang and Harper, 2009) and co-training (Blum and
Mitchell, 1998; Sagae and Lavie, 2006), except that
the goal here is not to further improve the perfor-
mance of the very best model. Instead, our aim is
to train a computationally cheaper model (a linear
time dependency parser) to match the performance
of the best model (a cubic time constituency parser),
resulting in a computationally efficient, yet highly
accurate model.
In practice, we parse a large amount of unlabeled
data from the target domain with the constituency
parser of Petrov et al (2006) and then train a deter-
ministic dependency parser on this noisy, automat-
ically parsed data. The accuracy of the linear time
parser on a question test set goes up from 60.06%
(LAS) to 76.94% after uptraining, which is compa-
rable to adding 2,000 labeled questions to the train-
ing data. Combining uptraining with 2,000 labeled
questions further improves the accuracy to 84.14%,
fully recovering the drop between in-domain and
out-of-domain accuracy.
We also present a detailed error analysis in Sec-
tion 5, showing that the errors of the WSJ-trained
model are primarily caused by sharp changes in syn-
tactic configurations and only secondarily due to
lexical shifts. Uptraining leads to large improve-
ments across all error metrics and especially on im-
portant dependencies like subjects (nsubj).
2 Experimental Setup
We used the following experimental protocol
throughout the paper.
2.1 Data
Our main training set consists of Sections 02-21 of
the Wall Street Journal portion of the Penn Treebank
(Marcus et al, 1993), with Section 22 serving as de-
velopment set for source domain comparisons. For
our target domain experiments, we evaluate on the
QuestionBank (Judge et al, 2006), which includes
a set of manually annotated questions from a TREC
question answering task. The questions in the Ques-
tionBank are very different from our training data in
terms of grammatical constructions and vocabulary
usage, making this a rather extreme case of domain-
adaptation. We split the 4,000 questions contained
in this corpus in three parts: the first 2,000 ques-
tions are reserved as a small target-domain training
set; the remaining 2,000 questions are split in two
equal parts, the first serving as development set and
the second as our final test set. We report accuracies
on the developments sets throughout this paper, and
test only at the very end on the final test set.
We convert the trees in both treebanks from con-
stituencies to labeled dependencies (see Figure 1)
using the Stanford converter, which produces 46
types of labeled dependencies1 (de Marneffe et al,
2006). We evaluate on both unlabeled (UAS) and
labeled dependency accuracy (LAS).2
Additionally, we use a set of 2 million ques-
tions collected from Internet search queries as unla-
beled target domain data. All user information was
anonymized and only the search query string was re-
tained. The question sample is selected at random
after passing two filters that select queries that are
1We use the Stanford Lexicalized Parser v1.6.2.
2Because the QuestionBank does not contain function tags,
we decided to strip off the function tags from the WSJ be-
fore conversion. The Stanford conversion only uses the -ADV
and -TMP tags, and removing all function tags from the WSJ
changed less than 0.2% of the labels (primarily tmod labels).
706
Training on Evaluating on WSJ Section 22 Evaluating on QuestionBank
WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 88.42 84.89 95.00 ? 74.14 62.81 88.48
McDonald et al (2006) ? 89.47 86.43 95.00 ? 80.01 67.00 88.48
Charniak (2000) 90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49
Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59
Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57
Petrov (2010) 92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08
Our shift-reduce parser ? 88.24 84.69 95.00 ? 72.23 60.06 88.48
Our shift-reduce parser (gold POS) ? 90.51 88.53 100.00 ? 78.30 68.92 100.00
Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.
similar in style to the questions in the QuestionBank:
(i) the queries must start with an English function
word that can be used to start a question (what, who
when, how, why, can, does, etc.), and (ii) the queries
have a maximum length of 160 characters.
2.2 Parsers
We use multiple publicly available parsers, as well
as our own implementation of a deterministic shift-
reduce parser in our experiments. The depen-
dency parsers that we compare are the determinis-
tic shift-reduce MaltParser (Nivre et al, 2007) and
the second-order minimum spanning tree algorithm
based MstParser (McDonald et al, 2006). Our shift-
reduce parser is a re-implementation of the Malt-
Parser, using a standard set of features and a lin-
ear kernel SVM for classification. We also train and
evaluate the generative lexicalized parser of Char-
niak (2000) on its own, as well as in combination
with the discriminative reranker of Charniak and
Johnson (2005). Finally, we run the latent variable
parser (a.k.a. BerkeleyParser) of Petrov et al (2006),
as well as the recent product of latent variable gram-
mars version (Petrov, 2010). To facilitate compar-
isons between constituency and dependency parsers,
we convert the output of the constituency parsers to
labeled dependencies using the same procedure that
is applied to the treebanks. We also report their F1
scores for completeness.
While the constituency parsers used in our experi-
ments view part-of-speech (POS) tagging as an inte-
gral part of parsing, the dependency parsers require
the input to be tagged with a separate POS tagger.
We use the TnT tagger (Brants, 2000) in our experi-
ments, because of its efficiency and ease of use. Tag-
ger and parser are always trained on the same data.
3 Parsing Questions
We consider two domain adaptation scenarios in this
paper. In the first scenario (sometimes abbreviated
as WSJ), we assume that we do not have any labeled
training data from the target domain. In practice, this
will always be the case when the target domain is
unknown or very diverse. The second scenario (ab-
breviated as WSJ+QB) assumes a small amount of
labeled training data from the target domain. While
this might be expensive to obtain, it is certainly fea-
sible for narrow domains (e.g. questions), or when a
high parsing accuracy is really important.
3.1 No Labeled Target Domain Data
We first trained all parsers on the WSJ training set
and evaluated their performance on the two domain
specific evaluation sets (newswire and questions).
As can be seen in the left columns of Table 1, all
parsers perform very well on the WSJ development
set. While there are differences in the accuracies,
all scores fall within a close range. The table also
confirms the commonly known fact (Yamada and
Matsumoto, 2003; McDonald et al, 2005) that con-
stituency parsers are more accurate at producing de-
pendencies than dependency parsers (at least when
the dependencies were produced by a deterministic
transformation of a constituency treebank, as is the
case here).
This picture changes drastically when the per-
formance is measured on the QuestionBank devel-
opment set (right columns in Table 1). As one
707
Evaluating on Training on WSJ + QB Training on QuestionBank
QuestionBank F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80
McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80
Charniak (2000) 89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84
Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56
Petrov (2010) 92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79
Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80
Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00
Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
might have expected, the accuracies are significantly
lower, however, the drop for some of the parsers
is shocking. Most notably, the deterministic shift-
reduce parsers lose almost 25% (absolute) on la-
beled accuracies, while the latent variable parsers
lose around 12%.3 Note also that even with gold
POS tags, LAS is below 70% for our determinis-
tic shift-reduce parser, suggesting that the drop in
accuracy is primarily due to a syntactic shift rather
than a lexical shift. These low accuracies are espe-
cially disturbing when one considers that the aver-
age question in the evaluation set is only nine words
long and therefore potentially much less ambiguous
than WSJ sentences. We will examine the main error
types more carefully in Section 5.
Overall, the dependency parsers seem to suf-
fer more from the domain change than the con-
stituency parsers. One possible explanation is that
they lack the global constraints that are enforced by
the (context-free) grammars. Even though the Mst-
Parser finds the globally best spanning tree, all con-
straints are local. This means for example, that it
is not possible to require the final parse to contain
a verb (something that can be easily expressed by
a top-level production of the form S ? NP VP in a
context free grammar). This is not a limitation of de-
pendency parsers in general. For example, it would
be easy to enforce such constraints in the Eisner
(1996) algorithm or using Integer Linear Program-
ming approaches (Riedel and Clarke, 2006; Martins
et al, 2009). However, such richer modeling capac-
ity comes with a much higher computational cost.
Looking at the constituency parsers, we observe
3The difference between our shift-reduce parser and the
MaltParser are due to small differences in the feature sets.
that the lexicalized (reranking) parser of Charniak
and Johnson (2005) loses more than the latent vari-
able approach of Petrov et al (2006). This differ-
ence doesn?t seem to be a difference of generative
vs. discriminative estimation. We suspect that the
latent variable approach is better able to utilize the
little evidence in the training data. Intuitively speak-
ing, some of the latent variables seem to get alo-
cated for modeling the few questions present in the
training data, while the lexicalization contexts are
not able to distinguish between declarative sentences
and questions.
To verify this hypothesis, we conducted two addi-
tional experiments. In the first experiment, we col-
lapsed the question specific phrasal categories SQ
and SBARQ to their declarative sentence equivalents
S and SBAR. When the training and test data are
processed this way, the lexicalized parser loses 1.5%
F1, while the latent variable parser loses only 0.7%.
It is difficult to examine the grammars, but one can
speculate that some of the latent variables were used
to model the question specific constructions and the
model was able to re-learn the distinctions that we
purposefully collapsed. In the second experiment,
we removed all questions from the WSJ training set
and retrained both parsers. This did not make a
significant difference when evaluating on the WSJ
development set, but of course resulted in a large
performance drop when evaluating on the Question-
Bank. The lexicalized parser came out ahead in this
experiment,4 confirming our hypothesis that the la-
tent variable model is better able to pick up the small
amount of relevant evidence that is present in the
WSJ training data (rather than being systematically
4The F1 scores were 52.40% vs. 56.39% respectively.
708
better suited for modeling questions).
3.2 Some Labeled Target Domain Data
In the above experiments, we considered a situation
where we have no labeled training data from the tar-
get domain, as will typically be the case. We now
consider a situation where a small amount of labeled
data (2,000 manually parsed sentences) from the do-
main of interest is available for training.
We experimented with two different ways of uti-
lizing this additional training data. In a first experi-
ment, we trained models on the concatenation of the
WSJ and QuestionBank training sets (we did not at-
tempt to weight the different corpora). As Table 2
shows (left columns), even a modest amount of la-
beled data from the target domain can significantly
boost parsing performance, giving double-digit im-
provements in some cases. While not shown in the
table, the parsing accuracies on the WSJ develop-
ment set where largely unaffected by the additional
training data.
Alternatively, one can also train models exclu-
sively on the QuestionBank data, resulting in ques-
tion specific models. The parsing accuracies of
these domain-specific models are shown in the right
columns of Table 2, and are significantly lower than
those of models trained on the concatenated training
sets. They are often times even lower than the results
of parsers trained exclusively on the WSJ, indicating
that 2,000 sentences are not sufficient to train accu-
rate parsers, even for quite narrow domains.
4 Uptraining for Domain-Adaptation
The results in the previous section suggest that
parsers without global constraints have difficul-
ties dealing with the syntactic differences between
declarative sentences and questions. A possible ex-
planation is that similar word configurations can ap-
pear in both types of sentences, but with very differ-
ent syntactic interpretation. Local models without
global constraints are therefore mislead into dead-
end interpretations from which they cannot recover
(McDonald and Nivre, 2007). Our approach will
therefore be to use a large amount of unlabeled data
to bias the model towards the appropriate distribu-
tion for the target domain. Rather than looking
for feature correspondences between the domains
 70
 75
 80
 85
 90
1M100K10K1K100100
U
A
S
WSJ+QB
WSJ
 60
 65
 70
 75
 80
 85
1M100K10K1K100100
LA
S
Number of unlabeled questions
WSJ+QB
WSJ
Figure 2: Uptraining with large amounts of unlabeled
data gives significant improvements over two different
supervised baselines.
(Blitzer et al, 2006), we propose to use automati-
cally labeled target domain data to learn the target
domain distribution directly.
4.1 Uptraining vs. Self-training
The idea of training parsers on their own output has
been around for as long as there have been statis-
tical parsers, but typically does not work well at
all (Charniak, 1997). Steedman et al (2003) and
Clark et al (2003) present co-training procedures
for parsers and taggers respectively, which are ef-
fective when only very little labeled data is avail-
able. McClosky et al (2006a) were the first to im-
prove a state-of-the-art constituency parsing system
by utilizing unlabeled data for self-training. In sub-
sequent work, they show that the same idea can be
used for domain adaptation if the unlabeled data is
chosen accordingly (McClosky et al, 2006b). Sagae
and Tsujii (2007) co-train two dependency parsers
by adding automatically parsed sentences for which
the parsers agree to the training data. Finally, Suzuki
et al (2009) present a very effective semi-supervised
approach in which features from multiple generative
models estimated on unlabeled data are combined in
a discriminative system for structured prediction.
All of these approaches have in common that their
ultimate goal is to improve the final performance.
Our work differs in that instead of improving the
709
Uptraining with Using only WSJ data Using WSJ + QB data
different base parsers UAS LAS POS UAS LAS POS
Baseline 72.23 60.06 88.48 83.70 78.27 91.32
Self-training 73.62 61.63 89.60 84.26 79.15 92.09
Uptraining on Petrov et al (2006) 86.02 76.94 90.75 88.38 84.02 93.63
Uptraining on Petrov (2010) 85.21 76.19 90.74 88.63 84.14 93.53
Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.
performance of the best parser, we want to build
a more efficient parser that comes close to the ac-
curacy of the best parser. To do this, we parse
the unlabeled data with our most accurate parser
and generate noisy, but fairly accurate labels (parse
trees) for the unlabeled data. We refer to the parser
used for producing the automatic labels as the base
parser (unless otherwise noted, we used the latent
variable parser of Petrov et al (2006) as our base
parser). Because the most accurate base parsers are
constituency parsers, we need to convert the parse
trees to dependencies using the Stanford converter
(see Section 2). The automatically parsed sentences
are appended to the labeled training data, and the
shift-reduce parser (and the part-of-speech tagger)
are trained on this new training set. We did not
increase the weight of the WSJ training data, but
weighted the QuestionBank training data by a fac-
tor of ten in the WSJ+QB experiments.
4.2 Varying amounts of unlabeled data
Figure 2 shows the efficacy of uptraining as a func-
tion of the size of the unlabeled data. Both la-
beled (LAS) and unlabeled accuracies (UAS) im-
prove sharply when automatically parsed sentences
from the target domain are added to the training data,
and level off after 100,000 sentences. Comparing
the end-points of the dashed lines (models having
access only to labeled data from the WSJ) and the
starting points of the solid lines (models that have
access to both WSJ and QuestionBank), one can see
that roughly the same improvements (from 72% to
86% UAS and from 60% to 77% LAS) can be ob-
tained by having access to 2,000 labeled sentences
from the target domain or uptraining with a large
amount of unlabeled data from the target domain.
The benefits seem to be complementary and can be
combined to give the best results. The final accu-
racy of 88.63 / 84.14 (UAS / LAS) on the question
evaluation set is comparable to the in-domain per-
formance on newswire data (88.24 / 84.69).
4.3 Varying the base parser
Table 3 then compares uptraining on the output of
different base parsers to pure self-training. In these
experiments, the same set of 500,000 questions was
parsed by different base parsers. The automatic
parses were then added to the labeled training data
and the parser was retrained. As the results show,
self-training provides only modest improvements of
less than 2%, while uptraining gives double-digit
improvements in some cases. Interestingly, there
seems to be no substantial difference between up-
training on the output of a single latent variable
parser (Petrov et al, 2006) and a product of latent
variable grammars (Petrov, 2010). It appears that
the roughly 1% accuracy difference between the two
base parsers is not important for uptraining.
4.4 POS-less parsing
Our uptraining procedure improves parse quality on
out-of-domain data to the level of in-domain ac-
curacy. However, looking closer at Table 3, one
can see that the POS accuracy is still relatively low
(93.53%), potentially limiting the final accuracy.
To remove this limitation (and also the depen-
dence on a separate POS tagger), we experimented
with word cluster features. As shown in Koo et al
(2008), word cluster features can be used in con-
junction with POS tags to improve parsing accuracy.
Here, we use them instead of POS tags in order to
further reduce the domain-dependence of our model.
Similar to Koo et al (2008), we use the Brown clus-
tering algorithm (Brown et al, 1992) to produce a
deterministic hierarchical clustering of our input vo-
cabulary. We then extract features based on vary-
710
UAS LAS POS
Part-of-Speech Tags 88.35 84.05 93.53
Word Cluster Features 87.92 83.73 ?
Table 4: Parsing accuracies of uptrained parsers with and
without part-of-speech tags and word cluster features.
ing cluster granularities (6 and 10 bits in our experi-
ments). Table 4 shows that roughly the same level of
accuracy can be achieved with cluster based features
instead of POS tag features. This change makes our
parser completely deterministic and enables us to
process sentences in a single left-to-right pass.
5 Error Analysis
To provide a better understanding of the challenges
involved in parsing questions, we analyzed the er-
rors made by our WSJ-trained shift-reduce parser
and also compared them to the errors that are left
after uptraining.
5.1 POS errors
Many parsing errors can be traced back to POS tag-
ging errors, which are much more frequent on out-
of-domain data than on in-domain data (88.8% on
the question data compared to above 95.0% on WSJ
data). Part of the reason for the lower POS tagging
accuracy is the higher unknown word ratio (7.3% on
the question evaluation set, compared to 3.4% on the
WSJ evaluation set). Another reason is a change in
the lexical distribution.
For example, wh-determiners (WDT) are quite
rare in the WSJ training data (relative frequency
0.45%), but five times more common in the Ques-
tionBank training data (2.49%). In addition to this
frequency difference, 52.43% of the WDTs in the
WSJ are the word ?which? and 46.97% are?that?. In
the QuestionBank on the other hand, ?what? is by
far the most common WDT word (81.40%), while
?which? and ?that? account only for 13.65% and
4.94% respectively. Not surprisingly the most com-
mon POS error involves wh-determiners (typically
the word ?what?) being incorrectly labeled as Wh-
pronouns (WP), resulting in head and label errors
like the one shown in Figure 3(a).
To separate out POS tagging errors from parsing
errors, we also ran experiments with correct (gold)
Dep. Label Frequency WSJ Uptrained
nsubj 934 41.02 88.64
amod 556 78.21 86.00
dobj 555 70.10 83.12
attr 471 8.64 93.49
aux 467 77.31 82.56
Table 5: F1 scores for the most frequent labels in the
QuestionBank development set. Uptraining leads to huge
improvements compared to training only on the WSJ.
POS tags. The parsing accuracies of our shift-reduce
parser using gold POS tags are listed in the last rows
of Tables 1 and 2. Even with gold POS tags, the de-
terministic shift-reduce parser falls short of the ac-
curacies of the constituency parsers (with automatic
tags), presumably because the shift-reduce model is
making only local decisions and is lacking the global
constraints provided by the context-free grammar.
5.2 Dependency errors
To find the main error types, we looked at the most
frequent labels in the QuestionBank development
set, and analyzed the ones that benefited the most
from uptraining. Table 5 has the frequency and F-
scores of the dependency types that we are going to
discuss in the following. We also provide examples
which are illustrated in Figure 3.
nsubj: The WSJ-trained model is often producing
parses that are missing a subject (nsubj). Questions
like ?What is the oldest profession?? and ?When
was Ozzy Osbourne born?? should have ?profes-
sion? and ?Osbourne? as nsubjs, but in both cases
the WSJ-trained parser did not label any subj (see
Figures 3(b) and 3(c)). Another common error is to
mislabel nsubj. For example, the nsubj of ?What are
liver enzymes?? should be enzymes, but the WSJ-
trained parser labels ?What? as the nsubj, which
makes sense in a statement but not in a question.
amod: The model is overpredicting ?amod?, re-
sulting in low precision figures for this label. An
example is ?How many points make up a perfect
fivepin bowling score??. The Stanford dependency
uses ?How? as the head of ?many? in noun phrases
like ?How many points?, and the relation is a generic
?dep?. But in the WSJ model prediction, ?many?s?
head is ?points,? and the relation mislabeled as
amod. Since it?s an adjective preceding the noun,
711
What is the oldest profession ?
ROOT
det     amod proot attr nsubj
WP VBZ DT JJS NN .ROOT
det     amod proot   attrdep
WP VBZ DT JJS NN .
What is the oldest profession ?
When was Ozzy Osbourne born ?
ROOT WRB VBZ  NNP NNP VBN .
root padvmod aux nn nsubj
When was Ozzy Osbourne   born ?
ROOT WRB VBZ  NNP NNP   NNP .
   root    nnnn pcompl nsubj
What films featured the character ?
ROOT WDT NNS  VBD DT NN  NNP NNP .
Popeye Doyle
nsubj dep det    nn nn dobj
What films featured the character ?
ROOT WP NNS  VBD DT NN  NNP NNP .
Popeye Doyle
   nsubj    compl det    nn nn   root    ccomp(a)
(b)
(c)
(d)
How many people did Randy ?
ROOT WRB JJ  NNS VBD NNP  NNP VB .
Craft kill
  dobj dep aux    nn nsubj p
?
.
dep
How many people did Randy
ROOT WRB JJ  NNS VBD NNP  NNP VB
Craft kill
compl amod ccomp   nn nsubj pnsubjroot
Figure 3: Example questions from the QuestionBank development set and their correct parses (left), as well as the
predictions of a model trained on the WSJ (right).
the WSJ model often makes this mistake and there-
fore the precision is much lower when it doesn?t see
more questions in the training data.
dobj: The WSJ model doesn?t predict object ex-
traction well. For example, in ?How many people
did Randy Craft kill?? (Figure 3(d)), the direct ob-
ject of kill should be ?How many people.? In the
Stanford dependencies, the correct labels for this
noun phrase are ?dobj dep dep,? but the WSJ model
predicts ?compl amod nsubj.? This is a common
error caused by the different word order in ques-
tions. The uptrained model is much better at han-
dling these type of constructions.
attr: An attr (attributive) is a wh-noun phrase
(WHNP) complement of a copular verb. In the WSJ
training data, only 4,641 out of 950,028 dependen-
cies are attr (0.5%); in the QuestionBank training
data, 1,023 out of 17,069 (6.0%) are attr. As a con-
sequence, the WSJ model cannot predict this label
in questions very well.
aux: ?What does the abbreviation AIDS stand
for?? should have ?stand? as the main head of the
sentence, and ?does? as its aux. However, the WSJ
model labeled ?does? as the main head. Similar
patterns occur in many questions, and therefore the
WSJ has a very low recall rate.
In contrast, mostly local labels (that are not re-
lated to question/statement structure differences)
have a consistently high accuracy. For example: det
has an accuracy of 98.86% with the WSJ-trained
model, and 99.24% with the uptrained model.
6 Conclusions
We presented a method for domain adaptation of de-
terministic shift-reduce parsers. We evaluated mul-
tiple state-of-the-art parsers on a question corpus
and showed that parsing accuracies degrade substan-
tially on this out-of-domain task. Most notably, de-
terministic shift-reduce parsers have difficulty deal-
ing with the modified word order and lose more
than 20% in accuracy. We then proposed a simple,
yet very effective uptraining method for domain-
adaptation. In a nutshell, we trained a deterministic
shift-reduce parser on the output of a more accurate,
but slower parser. Uptraining with large amounts of
unlabeled data gives similar improvements as hav-
ing access to 2,000 labeled sentences from the target
domain. With 2,000 labeled questions and a large
amount of unlabeled questions, uptraining is able to
close the gap between in-domain and out-of-domain
accuracy.
712
Acknowledgements
We would like to thank Ryan McDonald for run-
ning the MstParser experiments and for many fruit-
ful discussions on this topic. We would also like to
thank Joakim Nivre for help with the MatlParser and
Marie-Catherine de Marneffe for help with the Stan-
ford Dependency Converter.
References
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT ?98.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In CoNLL ?08.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In AI ?97.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrap-
ping pos-taggers using unlabelled data. In CoNLL ?03.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In COLING ?96.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
Z. Huang and M. Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP ?09.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
bank: creating a corpus of parse-annotated questions.
In ACL ?06.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In ACL ?10.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL ?08.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In ACL ?09.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In NAACL ?06.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
ACL ?06.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
EMNLP ?07.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL ?06.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2).
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov. 2010. Products of random latent variable
grammars. In NAACL ?10.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP ?06.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In NAACL ?06.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In CoNLL ?07.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL ?03.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In EMNLP
?09.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In IWPT
?03.
713
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1489?1499,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training dependency parsers by jointly optimizing multiple objectives
Keith Hall Ryan McDonald Jason Katz-Brown Michael Ringgaard
Google Research
{kbhall|ryanmcd|jasonkb|ringgaard}@google.com
Abstract
We present an online learning algorithm for
training parsers which allows for the inclusion
of multiple objective functions. The primary
example is the extension of a standard su-
pervised parsing objective function with addi-
tional loss-functions, either based on intrinsic
parsing quality or task-specific extrinsic mea-
sures of quality. Our empirical results show
how this approach performs for two depen-
dency parsing algorithms (graph-based and
transition-based parsing) and how it achieves
increased performance on multiple target tasks
including reordering for machine translation
and parser adaptation.
1 Introduction
The accuracy and speed of state-of-the-art depen-
dency parsers has motivated a resumed interest in
utilizing the output of parsing as an input to many
downstream natural language processing tasks. This
includes work on question answering (Wang et al,
2007), sentiment analysis (Nakagawa et al, 2010),
MT reordering (Xu et al, 2009), and many other
tasks. In most cases, the accuracy of parsers de-
grades when run on out-of-domain data (Gildea,
2001; McClosky et al, 2006; Blitzer et al, 2006;
Petrov et al, 2010). But these accuracies are mea-
sured with respect to gold-standard out-of-domain
parse trees. There are few tasks that actually depend
on the complete parse tree. Furthermore, when eval-
uated on a downstream task, often the optimal parse
output has a model score lower than the best parse
as predicted by the parsing model. While this means
that we are not properly modeling the downstream
task in the parsers, it also means that there is some
information from small task or domain-specific data
sets which could help direct our search for optimal
parameters during parser training. The goal being
not necessarily to obtain better parse performance,
but to exploit the structure induced from human la-
beled treebank data while targeting specific extrinsic
metrics of quality, which can include task specific
metrics or external weak constraints on the parse
structure.
One obvious approach to this problem is to em-
ploy parser reranking (Collins, 2000). In such a
setting, an auxiliary reranker is added in a pipeline
following the parser. The standard setting involves
training the base parser and applying it to a devel-
opment set (this is often done in a cross-validated
jack-knife training framework). The reranker can
then be trained to optimize for the downstream or
extrinsic objective. While this will bias the reranker
towards the target task, it is limited by the oracle
performance of the original base parser.
In this paper, we propose a training algorithm for
statistical dependency parsers (Ku?bler et al, 2009)
in which a single model is jointly optimized for a
regular supervised training objective over the tree-
bank data as well as a task-specific objective ? or
more generally an extrinsic objective ? on an ad-
ditional data set. The case where there are both
gold-standard trees and a task-specific objective for
the entire training set is a specific instance of the
larger problem that we address here. Specifically,
the algorithm takes the form of an online learner
where a training instance is selected and the param-
1489
eters are optimized based on the objective function
associated with the instance (either intrinsic or ex-
trinsic), thus jointly optimizing multiple objectives.
An update schedule trades-off the relative impor-
tance of each objective function. We call our algo-
rithm augmented-loss training as it optimizes mul-
tiple losses to augment the traditional supervised
parser loss.
There have been a number of efforts to exploit
weak or external signals of quality to train better pre-
diction models. This includes work on generalized
expectation (Mann and McCallum, 2010), posterior
regularization (Ganchev et al, 2010) and constraint
driven learning (Chang et al, 2007; Chang et al,
2010). The work of Chang et al (2007) on constraint
driven learning is perhaps the closest to our frame-
work and we draw connections to it in Section 5.
In these studies the typical goal is to use the weak
signal to improve the structured prediction models
on the intrinsic evaluation metrics. For our setting
this would mean using weak application specific sig-
nals to improve dependency parsing. Though we
explore such ideas in our experiments, in particular
for semi-supervised domain adaptation, we are pri-
marily interested in the case where the weak signal
is precisely what we wish to optimize, but also de-
sire the benefit from using both data with annotated
parse structures and data specific to the task at hand
to guide parser training.
In Section 2 we outline the augmented-loss algo-
rithm and provide a convergence analysis. In Sec-
tion 3 and 4 we present a set of experiments defin-
ing diffent augmented losses covering a task-specific
extrinsic loss (MT reordering), a domain adapta-
tion loss, and an alternate intrinsic parser loss. In
all cases we show the augmented-loss framework
can lead to significant gains in performance. In
Section 5 we tie our augmented-loss algorithm to
other frameworks for encoding auxiliary informa-
tion and/or joint objective optimization.
2 Methodology
We present the augmented-loss algorithm in the con-
text of the structured perceptron. The structured
perceptron (Algorithm 1) is an on-line learning al-
gorithm which takes as input: 1) a set of training
examples di = (xi, yi) consisting of an input sen-
Algorithm 1 Structured Perceptron
{Input data sets: D = {d1 = (x1, y1) . . . dN = (xN , yN )}}
{Input 0/1 loss: L(F?(x), y) = [F?(x) 6= y ? 1 : 0]}
{Let: F?(x) = arg maxy?Y ? ? ?(y)}
{Initialize model parameters: ? = ~0}
repeat
for i = 1 . . . N do
{Compute structured loss}
y?i = F?(xi)
if L(y?i, yi) > 0 then
{Update model Parameters}
? = ? + ?(yi)? ?(y?i)
end if
end for
until converged
{Return model ?}
tence xi and an output yi; and 2) a loss-function,
L(y?, y), that measures the cost of predicting out-
put y? relative to the gold standard y and is usu-
ally the 0/1 loss (Collins, 2002). For dependency
parser training, this set-up consists of input sen-
tences x and the corresponding gold dependency
tree y ? Yx, where Yx is the space of possible
parse trees for sentence x. In the perceptron setting,
F?(x) = arg maxy?Yx ? ??(y) where ? is mappingfrom a parse tree y for sentence x to a high dimen-
sional feature space. Learning proceeds by predict-
ing a structured output given the current model, and
if that structure is incorrect, updating the model: re-
warding features that fire in the gold-standard ?(yi),
and discounting features that fire in the predicted
output, ?(y?i).
The structured perceptron, as given in Algo-
rithm 1, only updates when there is a positive loss,
meaning that there was a prediction mistake. For
the moment we will abstract away from details such
as the precise definition of F (x) and ?(y). We
will show in the next section that our augmented-
loss method is general and can be applied to any de-
pendency parsing framework that can be trained by
the perceptron algorithm, such as transition-based
parsers (Nivre, 2008; Zhang and Clark, 2008) and
graph-based parsers (McDonald et al, 2005).
2.1 Augmented-Loss Training
The augmented-loss training algorithm that we pro-
pose is based on the structured perceptron; however,
the augmented-loss training framework is a general
1490
mechanism to incorporate multiple loss functions in
online learner training. Algorithm 2 is the pseudo-
code for the augmented-loss structured perceptron
algorithm. The algorithm is an extension to Algo-
rithm 1 where there are 1) multiple loss functions
being evaluated L1, . . . , LM ; 2) there are multiple
datasets associated with each of these loss functions
D1, . . . ,DM ; and 3) there is a schedule for pro-
cessing examples from each of these datasets, where
Sched(j, i) is true if the jth loss function should be
updated on the ith iteration of training. Note that
for data point dji = (x, y), which is the ith training
instance of the jth data set, that y does not neces-
sarily have to be a dependency tree. It can either
be a task-specific output of interest, a partial tree, or
even null, in the case where learning will be guided
strictly by the loss Lj . The training algorithm is ef-
fectively the same as the perceptron, the primary dif-
ference is that if Lj is an extrinsic loss, we cannot
compute the standard updates since we do not nec-
essarily know the correct parse (the line indicated by
?). Section 2.2 shows one method for updating the
parser parameters for extrinsic losses.
In the experiments in this paper, we only consider
the case where there are two loss functions: a super-
vised dependency parsing labeled-attachment loss;
and an additional loss, examples of which are pre-
sented in Section 3.
2.2 Inline Ranker Training
In order to make Algorithm 2 more concrete, we
need a way of defining the loss and resulting pa-
rameter updates for the case when Lj is not a stan-
dard supervised parsing loss (? from Algorithm 2).
Assume that we have a cost function C(xi, y?, yi)
which, given a training example (xi, yi) will give a
score for a parse y? ? Yxi relative to some output
yi. While we can compute the score for any parse,
we are unable to determine the features associated
with the optimal parse, as yi need not be a parse
tree. For example, consider a machine translation re-
ordering system which uses the parse y? to reorder the
words of xi, the optimal reordering being yi. Then
C(xi, y?, yi) is a reordering cost which is large if the
predicted parse induces a poor reordering of xi.
We propose a general purpose loss function which
is based on parser k-best lists. The inline reranker
uses the currently trained parser model ? to parse
Algorithm 2 Augmented-Loss Perceptron
{Input data sets}:
D1 = {d11 = (x11, y11) . . . d1N1 = (x1N1 , y1N1)},
. . .
DM = {dM1 = (xM1 , yM1 ) . . . dMNM = (xMNM , yMNM )}
{Input loss functions: L1 . . . LM}
{Initialize indexes: c1 . . . cM = ~0}
{Initialize model parameters: ? = ~0}
i = 0
repeat
for j = 1 . . .M do
{Check whether to update Lj on iteration i}
if Sched(j, i) then
{Compute index of instance ? reset if cj ? N j}
cj = [(cj ? N j) ? 0 : cj + 1]
{Compute structured loss for instance}
if Lj is intrinsic loss then
y? = F?(xjcj )
if Lj(y?, yjcj ) > 0 then
? = ? + ?(yjcj )? ?(y?) {yjcj is a tree}end if
else if Lj is an extrinsic loss then
{See Section 2.2}?
end if
end if
end for
i = i+ 1
until converged
{Return model ?}
the external input, producing a k-best set of parses:
Fk-best? (xi) = {y?1, . . . , y?k}. We can compute the
cost function C(xi, y?, yi) for all y? ? Fk-best? (xi). If
the 1-best parse, y?1, has the lowest cost, then there is
no lower cost parse in this k-best list. Otherwise, the
lowest-cost parse in Fk-best? (xi) is taken to be the
correct output structure yi, and the 1-best parse is
taken to be an incorrect prediction. We can achieve
this by substituting the following into Algorithm 2
at line ?.
Algorithm 3 Reranker Loss
{y?1, . . . , y?k} = Fk-best? (xi)
? = min? C(xjcj , y?? , yjcj ) {? is min const index}
Lj(y?1, yjcj ) = C(xjcj , y?1, yjcj )? C(xjcj , y?? , yjcj )
if Lj(y?1, yjcj ) > 0 then
? = ? + ?(y?? )? ?(y?1)
end if
Again the algorithm only updates when there is
an error ? when the 1-best output has a higher cost
than any other output in the k-best list ? resulting
1491
in positive Lj . The intuition behind this method is
that in the presence of only a cost function and a
k-best list, the parameters will be updated towards
the parse structure that has the lowest cost, which
over time will move the parameters of the model to
a place with low extrinsic loss.
We exploit this formulation of the general-
purpose augmented-loss function as it allows one to
include any extrinsic cost function which is depen-
dent of parses. The scoring function used does not
need to be factored, requiring no internal knowledge
of the function itself. Furthermore, we can apply this
to any parsing algorithm which can generate k-best
lists. For each parse, we must retain the features
associated with the parse (e.g., for transition-based
parsing, the features associated with the transition
sequence resulting in the parse).
There are two significant differences from the in-
line reranker loss function and standard reranker
training. First, we are performing this decision per
example as each data item is processed (this is done
in the inner loop of the Algorithm 2). Second, the
feedback function for selecting a parse is based on
an external objective function. The second point is
actually true for many minimum-error-rate training
scenarios, but in those settings the model is updated
as a post-processing stage (after the base-model is
trained).
2.3 Convergence of Inline Ranker Training
A training setD is loss-separable with margin ? > 0
if there exists a vector u with ?u? = 1 such that
for all y?, y?? ? Yx and (x, y) ? D, if L(y?, y) <
L(y??, y), then u??(y?)?u??(y??) ? ?. Furthermore,
let R ? ||?(y)? ?(y?)||, for all y, y?.
Assumption 1. Assume training set D is loss-
separable with margin ?.
Theorem 1. Given Assumption 1. Letm be the num-
ber of mistakes made when training the perceptron
(Algorithm 2) with inline ranker loss (Algorithm 3)
on D, where a mistake occurs for (x, y) ? D with
parameter vector ? when ?y?j ? F k-best? (x) where
y?j 6= y?1 and L(y?j , y) < L(y?1, y). If training is run
indefinitely, then m ? R2?2 .
Proof. Identical to the standard perceptron proof,
e.g., Collins (2002), by inserting in loss-separability
for normal separability.
Like the original perceptron theorem, this implies
that the algorithm will converge. However, unlike
the original theorem, it does not imply that it will
converge to a parameter vector ? such that for all
(x, y) ? D, if y? = arg maxy? ? ??(y?) then L(y?, y) =
0. Even if we assume for every x there exists an out-
put with zero loss, Theorem 1 still makes no guar-
antees. Consider a training set with one instance
(x, y). Now, set k = 2 for the k-best output list and
let y?1, y?2, and y?3 be the top-3 scoring outputs and
let L(y?1, y) = 1, L(y?2, y) = 2 and L(y?3, y) = 0.
In this case, no updates will ever be made and y?1
will remain unchanged even though it doesn?t have
minimal loss. Consider the following assumption:
Assumption 2. For any parameter vector ? that ex-
ists during training, either 1) for all (x, y) ? D,
L(y?1, y) = 0 (or some optimal minimum loss),
or 2) there exists at least one (x, y) ? D where
?y?j ? F k-best? (x) such that L(y?j , y) < L(y?1, y).
Assumption 2 states that for any ? that exists
during training, but before convergence, there is at
least one example in the training data where k is
large enough to include one output with a lower loss
when y?1 does not have the optimal minimal loss. If
k = ?, then this is the standard perceptron as it
guarantees the optimal loss output to be in the k-best
list. But we are assuming something much weaker
here, i.e., not that the k-best list will include the min-
imal loss output, only a single output with a lower
loss than the current best guess. However, it is strong
enough to show the following:
Theorem 2. Given Assumption 1 and Assumption 2.
Training the perceptron (Algorithm 2) with inline
ranker loss (Algorithm 3) on D 1) converges in fi-
nite time, and 2) produces parameters ? such that
for all (x, y) ? D, if y? = arg maxy? ? ? ?(y?) then
L(y?, y) = 0 (or equivalent minimal loss).
Proof. It must be the case for all (x, y) ? D that
L(y?1, y) = 0 (and y?1 is the argmax) after a finite
amount of time. Otherwise, by Assumption 2, there
exists some x, such that when it is next processed,
there would exist an output in the k-best list that
had a lower loss, which will result in an additional
mistake. Theorem 1 guarantees that this can not
continue indefinitely as the number of mistakes is
bounded.
1492
Thus, the perceptron algorithm will converge to
optimal minimal loss under the assumption that k
is large enough so that the model can keep improv-
ing. Note that this does not mean k must be large
enough to include a zero or minimum loss output,
just large enough to include a better output than
the current best hypothesis. Theorem 2, when cou-
pled with Theorem 1, implies that augmented-loss
learning will make at most R2/?2 mistakes at train-
ing, but does not guarantee the rate at which these
mistakes will be made, only that convergence is fi-
nite, providing that the scheduling time (defined by
Sched()) between seeing the same instance is always
finite, which is always true in our experiments.
This analysis does not assume anything about the
loss L. Every instance (x, y) can use a different loss.
It is only required that the loss for a specific input-
output pair is fixed throughout training. Thus, the
above analysis covers the case where some training
instances use an extrinsic loss and others an intrin-
sic parsing loss. This also suggests more efficient
training methods when extracting the k-best list is
prohibitive. One can parse with k = 2, 4, 8, 16, . . .
until an k is reached that includes a lower loss parse.
It may be the case that for most instances a small
k is required, but the algorithm is doing more work
unnecessarily if k is large.
3 Experimental Set-up
3.1 Dependency Parsers
The augmented-loss framework we present is gen-
eral in the sense that it can be combined with any
loss function and any parser, provided the parser can
be parameterized as a linear classifier, trained with
the perceptron and is capable of producing a k-best
list of trees. For our experiments we focus on two
dependency parsers.
? Transition-based: An implementation of the
transition-based dependency parsing frame-
work (Nivre, 2008) using an arc-eager transi-
tion strategy and are trained using the percep-
tron algorithm as in Zhang and Clark (2008)
with a beam size of 8. Beams with varying
sizes can be used to produce k-best lists. The
features used by all models are: the part-of-
speech tags of the first four words on the buffer
and of the top two words on the stack; the word
identities of the first two words on the buffer
and of the top word on the stack; the word iden-
tity of the syntactic head of the top word on the
stack (if available); dependency arc label iden-
tities for the top word on the stack, the left and
rightmost modifier of the top word on the stack,
and the left most modifier of the first word in
the buffer (if available). All feature conjunc-
tions are included.
? Graph-based: An implementation of graph-
based parsing algorithms with an arc-factored
parameterization (McDonald et al, 2005). We
use the non-projective k-best MST algorithm to
generate k-best lists (Hall, 2007), where k = 8
for the experiments in this paper. The graph-
based parser features used in the experiments
in this paper are defined over a word, wi at po-
sition i; the head of this word w?(i) where ?(i)
provides the index of the head word; and part-
of-speech tags of these words ti. We use the
following set of features similar to McDonald
et al (2005):
isolated features: wi, ti, w?(i), t?(i)
word-tag pairs: (wi, ti); (w?(i), t?(i))
word-head pairs: (wi, w?(i)), (ti, t?(i))
word-head-tag triples: (t?(i), wi, ti)
(w?(i), wi, ti)
(w?(i), t?(i), ti)
(w?(i), t?(i), wi)
tag-neighbourhood: (t?(i), t?(i)+1, ti?1, ti)
(t?(i), t?(i)+1, ti+1, ti)
(t?(i), t?(i)?1, ti?1, ti)
(t?(i), t?(i)?1, ti+1, ti)
between features: ?j i < j < ?(i) || ?(i) < j < i
(t?(i), tj , ti)
arc-direction/length : (i? ?(i) > 0, |i? ?(i)|)
3.2 Data and Tasks
In the next section, we present a set of scoring func-
tions that can be used in the inline reranker loss
framework, resulting in a new augmented-loss for
each one. Augmented-loss learning is then applied
to target a downstream task using the loss functions
to measure gains. We show empirical results for two
extrinsic loss-functions (optimizing for the down-
stream task): machine translation and domain adap-
tation; and for one intrinsic loss-function: an arc-
length parsing score. For some experiments we also
1493
measure the standard intrinsic parser metrics unla-
beled attachment score (UAS) and labeled attach-
ment score (LAS) (Buchholz and Marsi, 2006).
In terms of treebank data, the primary training
corpus is the Penn Wall Street Journal Treebank
(PTB) (Marcus et al, 1993). We also make use
of the Brown corpus, and the Question Treebank
(QTB) (Judge et al, 2006). For PTB and Brown
we use standard training/development/testing splits
of the data. For the QTB we split the data into
three sections: 2000 training, 1000 development,
and 1000 test. All treebanks are converted to de-
pendency format using the Stanford converter v1.6
(de Marneffe et al, 2006).
4 Experiments
4.1 Machine Translation Reordering Score
As alluded to in Section 2.2, we use a reordering-
based loss function to improve word order in a ma-
chine translation system. In particular, we use a sys-
tem of source-side reordering rules which, given a
parse of the source sentence, will reorder the sen-
tence into a target-side order (Collins et al, 2005).
In our experiments we work with a set of English-
Japanese reordering rules1 and gold reorderings
based on human generated correct reordering of an
aligned target sentences. We use a reordering score
based on the reordering penalty from the METEOR
scoring metric. Though we could have used a fur-
ther downstream measure like BLEU, METEOR has
also been shown to directly correlate with translation
quality (Banerjee and Lavie, 2005) and is simpler to
measure.
reorder-score = 1? # chunks? 1# unigrams matched? 1
reorder-cost = 1? reorder-score
All reordering augmented-loss experiments are
run with the same treebank data as the baseline
(the training portions of PTB, Brown, and QTB).
The extrinsic reordering training data consists of
10930 examples of English sentences and their cor-
rect Japanese word-order. We evaluate our results on
an evaluation set of 6338 examples of similarly cre-
ated reordering data. The reordering cost, evaluation
1Our rules are similar to those from Xu et al (2009).
Exact Reorder
trans?PTB + Brown + QTB 35.29 76.49
trans?0.5?aug.-loss 38.71 78.19
trans?1.0?aug.-loss 39.02 78.39
trans?2.0?aug.-loss 39.58 78.67
graph?PTB + Brown + QTB 25.71 69.84
graph?0.5? aug.-loss 28.99 72.23
graph?1.0?aug.-loss 29.99 72.88
graph?2.0?aug.-loss 30.03 73.15
Table 1: Reordering scores for parser-based reordering
(English-to-Japanese). Exact is the number of correctly
reordered sentences. All models use the same treebank-
data (PTB, QTB, and the Brown corpus). Results for
three augmented-loss schedules are shown: 0.5 where for
every two treebank updates we make one augmented-loss
update, 1 is a 1-to-1 mix, and 2 is where we make twice
as many augmented-loss updates as treebank updates.
criteria and data used in our experiments are based
on the work of Talbot et al (2011).
Table 1 shows the results of using the reordering
cost as an augmented-loss to the standard treebank
objective function. Results are presented as mea-
sured by the reordering score as well as a coarse
exact-match score (the number of sentences which
would have correct word-order given the parse and
the fixed reordering rules). We see continued im-
provements as we adjust the schedule to process the
extrinsic loss more frequently, the best result being
when we make two augmented-loss updates for ev-
ery one treebank-based loss update.
4.2 Semi-supervised domain adaptation
Another application of the augmented-loss frame-
work is to improve parser domain portability in the
presence of partially labeled data. Consider, for ex-
ample, the case of questions. Petrov et al (2010)
observed that dependency parsers tend to do quite
poorly when parsing questions due to their lim-
ited exposure to them in the news corpora from
the PennTreebank. Table 2 shows the accuracy
of two parsers (LAS, UAS and the F1 of the root
dependency attachment) on the QuestionBank test
data. The first is a parser trained on the standard
training sections of the PennTreebank (PTB) and
the second is a parser trained on the training por-
tion of the QuestionBank (QTB). Results for both
1494
LAS UAS Root-F1
trans?PTB 67.97 73.52 47.60
trans?QTB 84.59 89.59 91.06
trans?aug.-loss 76.27 86.42 83.41
graph?PTB 65.27 72.72 43.10
graph?QTB 82.73 87.44 91.58
graph?aug.-loss 72.82 80.68 86.26
Table 2: Domain adaptation results. Table shows (for
both transition and graph-based parsers) the labeled ac-
curacy score (LAS), unlabeled accuracy score (UAS)
and Root-F1 for parsers trained on the PTB and QTB
and tested on the QTB. The augmented-loss parsers are
trained on the PTB but with a partial tree loss on QTB
that considers only root dependencies.
transition-based parsers and graph-based parsers are
given. Clearly there is significant drop in accu-
racy for a parser trained on the PTB. For example,
the transition-based PTB parser achieves a LAS of
67.97% relative to 84.59% for the parser trained on
the QTB.
We consider the situation where it is possible to
ask annotators a single question about the target do-
main that is relatively easy to answer. The question
should be posed so that the resulting answer pro-
duces a partially labeled dependency tree. Root-F1
scores from Table 2 suggest that one simple ques-
tion is ?what is the main verb of this sentence?? for
sentences that are questions. In most cases this task
is straight-forward and will result in a single depen-
dency, that from the root to the main verb of the sen-
tence. We feel this is a realistic partial labeled train-
ing setting where it would be possible to quickly col-
lect a significant amount of data.
To test whether such weak information can signif-
icantly improve the parsing of questions, we trained
an augmented-loss parser using the training set of
the QTB stripped of all dependencies except the de-
pendency from the root to the main verb of the sen-
tence. In other words, for each sentence, the parser
may only observe a single dependency at training
from the QTB ? the dependency to the main verb.
Our augmented-loss function in this case is a simple
binary function: 0 if a parse has the correct root de-
pendency and 1 if it does not. Thus, the algorithm
will select the first parse in the k-best list that has the
correct root as the proxy to a gold standard parse.2
The last row in each section of Table 2 shows the
results for this augmented-loss system when weight-
ing both losses equally during training. By simply
having the main verb annotated in each sentence ?
the sentences from the training portion of the QTB
? the parser can eliminate half of the errors of the
original parser. This is reflected by both the Root-
F1 as well as LAS/UAS. It is important to point out
that these improvements are not limited to simply
better root predictions. Due to the fact that parsing
algorithms make many parsing decisions jointly at
test time, all such decisions influence each other and
improvements are seen across the board. For exam-
ple, the transition-based PTB parser has an F1 score
of 41.22% for verb subjects (nsubj), whereas the
augmented-loss parser has an F1 of 73.52%. Clearly
improving just a single (and simple to annotate) de-
pendency leads to general parser improvements.
4.3 Average Arc Length Score
The augmented-loss framework can be used to in-
corporate multiple treebank-based loss functions as
well. Labeled attachment score is used as our base
model loss function. In this set of experiments we
consider adding an additional loss function which
weights the lengths of correct and incorrect arcs, the
average (labeled) arc-length score:
ALS =
?
i ?(??i, ?i)(i? ?i)?
i(i? ?i)
For each word of the sentence we compute the dis-
tance between the word?s position i and the posi-
tion of the words head ?i. The arc-length score is
the summed length of all those with correct head as-
signments (?(??i, ?i) is 1 if the predicted head and
the correct head match, 0 otherwise). The score is
normalized by the summed arc lengths for the sen-
tence. The labeled version of this score requires that
the labels of the arc are also correct. Optimizing
for dependency arc length is particularly important
as parsers tend to do worse on longer dependencies
(McDonald and Nivre, 2007) and these dependen-
cies are typically the most meaningful for down-
stream tasks, e.g., main verb dependencies for tasks
2For the graph-based parser one can also find the higest scor-
ing tree with correct root by setting the score of all competing
arcs to ??.
1495
LAS UAS ALS
trans?PTB 88.64 91.64 82.96
trans?unlabeled aug.-loss 88.74 91.91 83.65
trans?labeled aug.-loss 88.84 91.91 83.46
graph?PTB 85.75 88.70 73.88
graph?unlabeled aug.-loss 85.80 88.81 74.26
graph?labeled aug.-loss 85.85 88.93 74.40
Table 3: Results for both parsers on the development set
of the PTB. When training with ALS (labeled and unla-
beled), we see an improvement in UAS, LAS, and ALS.
Furthermore, if we use a labeled-ALS as the metric for
augmented-loss training, we also see a considerable in-
crease in LAS.
like information extraction (Yates and Etzioni, 2009)
and textual entailment (Berant et al, 2010).
In Table 3 we show results for parsing with the
ALS augmented-loss objective. For each parser, we
consider two different ALS objective functions; one
based on unlabeled-ALS and the other on labeled-
ALS. The arc-length score penalizes incorrect long-
distance dependencies more than local dependen-
cies; long-distance dependencies are often more de-
structive in preserving sentence meaning and can be
more difficult to predict correctly due to the larger
context on which they depend. Combining this with
the standard attachment scores biases training to fo-
cus on the difficult head dependencies.
For both experiments we see that by adding the
ALS augmented-loss we achieve an improvement in
LAS and UAS in addition to ALS. The augmented-
loss not only helps us improve on the longer depen-
dencies (as reflected in the increased ALS), but also
in the main parser objective function of LAS and
UAS. Using the labeled loss function provides better
reinforcement as can be seen in the improvements
over the unlabeled loss-function. As with all experi-
ments in this paper, the graph-based parser baselines
are much lower than the transition-based parser due
to the use of arc-factored features. In these experi-
ments we used an inline-ranker loss with 8 parses.
We experimented with larger sizes (16 and 64) and
found very similar improvements: for example, the
transition parser?s LAS for the labeled loss is 88.68
and 88.84, respectively).
We note that ALS can be decomposed locally and
could be used as the primary objective function for
parsing. A parse with perfect scores under ALS
and LAS will match the gold-standard training tree.
However, if we were to order incorrect parses of a
sentence, ALS and LAS will suggest different order-
ings. Our results show that by optimizing for losses
based on a combination of these metrics we train a
more robust parsing model.
5 Related Work
A recent study by Katz-Brown et al (2011) also in-
vestigates the task of training parsers to improve MT
reordering. In that work, a parser is used to first
parse a set of manually reordered sentences to pro-
duce k-best lists. The parse with the best reordering
score is then fixed and added back to the training set
and a new parser is trained on resulting data. The
method is called targeted self-training as it is simi-
lar in vein to self-training (McClosky et al, 2006),
with the exception that the new parse data is targeted
to produce accurate word reorderings. Our method
differs as it does not statically fix a new parse, but
dynamically updates the parameters and parse selec-
tion by incorporating the additional loss in the inner
loop of online learning. This allows us to give guar-
antees of convergence. Furthermore, we also evalu-
ate the method on alternate extrinsic loss functions.
Liang et al (2006) presented a perceptron-based
algorithm for learning the phrase-translation param-
eters in a statistical machine translation system.
Similar to the inline-ranker loss function presented
here, they use a k-best lists of hypotheses in order to
identify parameters which can improve a global ob-
jective function: BLEU score. In their work, they
are interested in learning a parameterization over
translation phrases (including the underlying word-
alignment) which optimizes the BLEU score. Their
goal is considerably different; they want to incor-
porate additional features into their model and de-
fine an objective function which allows them to do
so; whereas, we are interested in allowing for mul-
tiple objective functions in order to adapt the parser
model parameters to downstream tasks or alternative
intrinsic (parsing) objectives.
The work that is most similar to ours is that
of Chang et al (2007), who introduced the Con-
straint Driven Learning algorithm (CODL). Their al-
gorithm specifically optimizes a loss function with
1496
the addition of constraints based on unlabeled data
(what we call extrinsic datasets). For each unla-
beled example, they use the current model along
with their set of constraints to select a set of k au-
tomatically labeled examples which best meet the
constraints. These induced examples are then added
to their training set and, after processing each unla-
beled dataset, they perform full model optimization
with the concatenation of training data and newly
generated training items. The augmented-loss al-
gorithm can be viewed as an online version of this
algorithm which performs model updates based on
the augmented-loss functions directly (rather than
adding a set of examples to the training set). Un-
like the CODL approach, we do not perform com-
plete optimization on each iteration over the unla-
beled dataset; rather, we incorporate the updates in
our online learning algorithm. As mentioned earlier,
CODL is one example of learning algorithms that
use weak supervision, others include Mann and Mc-
Callum (2010) and Ganchev et al (2010). Again,
these works are typically interested in using the ex-
trinsic metric ? or, in general, extrinsic information
? to optimize the intrinsic metric in the absence of
any labeled intrinsic data. Our goal is to optimize
both simultaneously.
The idea of jointly training parsers to optimize
multiple objectives is related to joint learning and in-
ference for tasks like information extraction (Finkel
and Manning, 2009) and machine translation (Bur-
kett et al, 2010). In such works, a large search space
that covers both the space of parse structures and
the space of task-specific structures is defined and
parameterized so that standard learning and infer-
ence algorithms can be applied. What sets our work
apart is that there is still just a single parameter set
that is being optimized ? the parser parameters. Our
method only uses feedback from task specific objec-
tives in order to update the parser parameters, guid-
ing it towards better downstream performance. This
is advantageous for two reasons. First, it decouples
the tasks, making inference and learning more effi-
cient. Second, it does not force arbitrary paraemter
factorizations in order to define a joint search space
that can be searched efficiently.
Finally, augmented-loss training can be viewed
as multi-task learning (Caruana, 1997) as the model
optimizes multiple objectives over multiple data sets
with a shared underlying parameter space.
6 Discussion
The empirical results show that incorporating an
augmented-loss using the inline-ranker loss frame-
work achieves better performance under metrics as-
sociated with the external loss function. For the in-
trinsic loss, we see that the augmented-loss frame-
work can also result in an improvement in parsing
performance; however, in the case of ALS, this is
due to the fact that the loss function is very closely
related to the standard evaluation metrics of UAS
and LAS.
Although our analysis suggests that this algorithm
is guaranteed to converge only for the separable
case, it makes a further assumption that if there is
a better parse under the augmented-loss, then there
must be a lower cost parse in the k-best list. The em-
pirical evaluation presented here is based on a very
conservative approximation by choosing lists with
at most 8 parses. However, in our experiments, we
found that increasing the size of the lists did not sig-
nificantly increase our accuracy under the external
metrics. If we do have at least one improvement
in our k-best lists, the analysis suggests that this is
enough to move in the correct direction for updating
the model. The assumption that there will always
be an improvement in the k-best list if there is some
better parse breaks down as training continues. We
suspect that an increasing k, as suggested in Sec-
tion 2.3, will allow for continued improvements.
Dependency parsing, as presented in this pa-
per, is performed over (k-best) part-of-speech tags
and is therefore dependent on the quality of the
tagger. The experiments presented in this paper
made use of a tagger trained on the source treebank
data which severely limits the variation in parses.
The augmented-loss perceptron algorithm presented
here can be applied to any online learning prob-
lem, including part-of-speech tagger training. To
build a dependency parser which is better adapted
to a downstream task, one would want to perform
augmented-loss training on the tagger as well.
7 Conclusion
We introduced the augmented-loss training algo-
rithm and show that the algorithm can incorporate
1497
additional loss functions to adapt the model towards
extrinsic evaluation metrics. Analytical results are
presented that show that the algorithm can opti-
mize multiple objective functions simultaneously.
We present an empirical analysis for training depen-
dency parsers for multiple parsing algorithms and
multiple loss functions.
The augmented-loss framework supports both in-
trinsic and extrinsic losses, allowing for both com-
binations of objectives as well as multiple sources
of data for which the results of a parser can be eval-
uated. This flexibility makes it possible to tune a
model for a downstream task. The only requirement
is a metric which can be defined over parses of the
downstream data. Our dependency parsing results
show that we are not limited to increasing parser
performance via more data or external domain adap-
tation techniques, but that we can incorporate the
downstream task into parser training.
Acknowledgements: We would like to thank Kuz-
man Ganchev for feedback on an earlier draft of this
paper as well as Slav Petrov for frequent discussions
on this topic.
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Berant, I. Dagan, and J. Goldberger. 2010. Global
learning of focused entailment graphs. In Proc. of
ACL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, J. Blitzer, and D. Klein. 2010. Joint parsing
and alignment with weakly synchronized grammars.
In Proc. of NAACL.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, Genoa,
Italy.
J.R. Finkel and C.D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
K. Hall. 2007. k-best spanning tree parsing. In Proc. of
ACL, June.
J. Judge, A. Cahill, and J. Van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proc. of ACL, pages 497?504.
J. Katz-Brown, S. Petrov, R. McDonald, D. Talbot,
F. Och, H. Ichikawa, M. Seno, and H. Kazawa. 2011.
Training a parser for machine translation reordering.
In Proc. of EMNLP.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. 2006.
An end-to-end discriminative approach to machine
translation. In Proc. of COLING/ACL.
G.S. Mann and A. McCallum. 2010. Generalized Ex-
pectation Criteria for Semi-Supervised Learning with
Weakly Labeled Data. The Journal of Machine Learn-
ing Research, 11:955?984.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19:313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
1498
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. De-
pendency tree-based sentiment classification using crfs
with hidden variables. In Proc. of NAACL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, P.C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In Proc. of EMNLP, pages 705?713.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalu-
ation framework for machine translation reordering.
In Proc. of the Sixth Workshop on Statistical Machine
Translation.
M. Wang, N.A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Us-
ing a dependency parser to improve SMT for Subject-
Object-Verb languages. In Proc. of NAACL.
A. Yates and O. Etzioni. 2009. Unsupervised meth-
ods for determining object and relation synonyms on
the web. Journal of Artificial Intelligence Research,
34(1):255?296.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP, pages 562?571.
1499
Deterministic Statistical Mapping of
Sentences to Underspecified Semantics
Hiyan Alshawi
Google, Inc.
(hiyan@google.com)
Pi-Chuan Chang
Google, Inc.
(pichuan@google.com)
Michael Ringgaard
Google, Inc.
(ringgaard@google.com)
Abstract
We present a method for training a statistical model for mapping natural language sentences to
semantic expressions. The semantics are expressions of an underspecified logical form that has prop-
erties making it particularly suitable for statistical mapping from text. An encoding of the semantic
expressions into dependency trees with automatically generated labels allows application of exist-
ing methods for statistical dependency parsing to the mapping task (without the need for separate
traditional dependency labels or parts of speech). The encoding also results in a natural per-word
semantic-mapping accuracy measure. We report on the results of training and testing statistical mod-
els for mapping sentences of the Penn Treebank into the semantic expressions, for which per-word
semantic mapping accuracy ranges between 79% and 86% depending on the experimental condi-
tions. The particular choice of algorithms used also means that our trained mapping is deterministic
(in the sense of deterministic parsing), paving the way for large-scale text-to-semantic mapping.
1 Introduction
Producing semantic representations of text is motivated not only by theoretical considerations but also
by the hypothesis that semantics can be used to improve automatic systems for tasks that are intrinsically
semantic in nature such as question answering, textual entailment, machine translation, and more gen-
erally any natural language task that might benefit from inference in order to more closely approximate
human performance. Since formal logics have formal denotational semantics, and are good candidates
for supporting inference, they have often been taken to be the targets for mapping text to semantic
representations, with frameworks emphasizing (more) tractable inference choosing first order predicate
logic (Stickel, 1985) while those emphasizing representational power favoring one of the many available
higher order logics (van Benthem, 1995).
It was later recognized that in order to support some tasks, fully specifying certain aspects of a logic
representation, such as quantifier scope, or reference resolution, is often not necessary. For example, for
semantic translation, most ambiguities of quantifier scope can be carried over from the source language
to the target language without being resolved. This led to the development of underspecified semantic
representations (e.g. QLF, Alshawi and Crouch (1992) and MRS, Copestake et al(2005)) which are
easier to produce from text without contextual inference but which can be further specified as necessary
for the task being performed.
While traditionally mapping text to formal representations was predominantly rule-based, for both
the syntactic and semantic components (Montague (1973), Pereira and Shieber (1987), Alshawi (1992)),
good progress in statistical syntactic parsing (e.g. Collins (1999), Charniak (2000)) led to systems that
applied rules for semantic interpretation to the output of a statistical syntactic parser (e.g. Bos et al
(2004)). More recently researchers have looked at statistical methods to provide robust and trainable
methods for mapping text to formal representations of meaning (Zettlemoyer and Collins, 2005).
In this paper we further develop the two strands of work mentioned above, i.e. mapping text to
underspecified semantic representations and using statistical parsing methods to perform the analysis.
15
Here we take a more direct route, starting from scratch by designing an underspecified semantic repre-
sentation (Natural Logical Form, or NLF) that is purpose-built for statistical text-to-semantics mapping.
An underspecified logic whose constructs are motivated by natural language and that is amenable to
trainable direct semantic mapping from text without an intervening layer of syntactic representation. In
contrast, the approach taken by (Zettlemoyer and Collins, 2005), for example, maps into traditional logic
via lambda expressions, and the approach taken by (Poon and Domingos, 2009) depends on an initial
step of syntactic parsing.
In this paper, we describe a supervised training method for mapping text to NLF, that is, producing
a statistical model for this mapping starting from training pairs consisting of sentences and their corre-
sponding NLF expressions. This method makes use of an encoding of NLF expressions into dependency
trees in which the set of labels is automatically generated from the encoding process (rather than being
pre-supplied by a linguistically motivated dependency grammar). This encoding allows us to perform the
text-to-NLF mapping using any existing statistical methods for labeled dependency parsing (e.g. Eisner
(1996), Yamada and Matsumoto (2003), McDonald, Crammer, Pereira (2005)). A side benefit of the
encoding is that it leads to a natural per-word measure for semantic mapping accuracy which we use for
evaluation purposes. By combing our method with deterministic statistical dependency models together
with deterministic (hard) clusters instead of parts of speech, we obtain a deterministic statistical text-to-
semantics mapper, opening the way to feasible mapping of text-to-semantics at a large scale, for example
the entire web.
This paper concentrates on the text-to-semantics mapping which depends, in part, on some properties
of NLF. We will not attempt to defend the semantic representation choices for specific constructions il-
lustrated here. NLF is akin to a variable-free variant of QLF or an MRS in which some handle constraints
are determined during parsing. For the purposes of this paper it is sufficient to note that NLF has roughly
the same granularity of semantic representation as these earlier underspecified representations.
We outline the steps of our text-to-semantics mapping method in Section 2, introduce NLF in Sec-
tion 3, explain the encoding of NLF expressions as formal dependency trees in Section 4, and report on
experiments for training and testing statistical models for mapping text to NLF expressions in Section 5.
2 Direct Semantic Mapping
Our method for mapping text to natural semantics expressions proceeds as follows:
1. Create a corpus of pairs consisting of text sentences and their corresponding NLF semantic ex-
pressions.
2. For each of the sentence-semantics pairs in the corpus, align the words of the sentence to the tokens
of the NLF expressions.
3. ?Encode? each alignment pair as an ordered dependency tree in which the labels are generated by
the encoding process.
4. Train a statistical dependency parsing model with the set of dependency trees.
5. For a new input sentence S, apply the statistical parsing model to S, producing a labeled depen-
dency tree DS .
6. ?Decode? DS into a semantic expression for S.
For step 1, the experiments in this paper (Section 5) obtain the corpus by converting an existing
constituency treebank into semantic expressions. However, direct annotation of a corpus with semantic
expressions is a viable alternative, and indeed we are separately exploring that possibility for a different,
open domain, text corpus.
16
For steps 4 and 5, any method for training and applying a dependency model from a corpus of labeled
dependency trees may be used. As described in Section 5, for the experiments reported here we use an
algorithm similar to that of Nivre (2003).
For steps 2, 3 and 6, the encoding of NLF semantic expressions as dependency trees with automati-
cally constructed labels is described in Section 4.
3 Semantic Expressions
NLF expressions are by design amenable to facilitating training of text-to-semantics mappings. For this
purpose, NLF has a number of desirable properties:
1. Apart from a few built-in logical connectives, all the symbols appearing in NLF expressions are
natural language words.
2. For an NLF semantic expression corresponding to a sentence, the word tokens of the sentence
appear exactly once in the NLF expression.
3. The NLF notation is variable-free.
Technically, NLF expressions are expression of an underspecified logic, i.e. a semantic representation
that leaves open the interpretation of certain constructs (for example the scope of quantifiers and some
operators and the referents of terms such as anaphora, and certain implicit relations such as those for
compound nominals). NLF is similar in some ways to Quasi Logical Form, or QLF (Alshawi, 1992), but
the properties listed above keep NLF closer to natural language than QLF, hence natural logical form. 1
There is no explicit formal connection between NLF and Natural Logic (van Benthem, 1986), though it
may turn out that NLF is a convenient starting point for some Natural Logic inferences.
In contrast to statements of a fully specified logic in which denotations are typically taken to be
functions from possible worlds to truth values (Montague, 1973), denotations of a statement in an under-
specified logic are typically taken to be relations between possible worlds and truth values (Alshawi and
Crouch (1992), Alshawi (1996)). Formal denotations for NLF expressions are beyond the scope of this
paper and will be described elsewhere.
3.1 Connectives and Examples
A NLF expression for the sentence
In 2002, Chirpy Systems stealthily acquired two profitable companies producing pet acces-
sories.
is shown in Figure 1.
The NLF constructs and connectives are explained in Table 1. For variable-free abstraction, an NLF
expression [p, ?, a] corresponds to ?x.p(x, a). Note that some common logical operators are not
built-in since they will appear directly as words such as not.2 We currently use the unknown/unspecified
operator, %, mainly for linguistic constructions that are beyond the coverage of a particular semantic
mapping model. A simple example that includes % in our converted WSJ corpus is Other analysts are
nearly as pessimistic for which the NLF expression is
[are, analysts.other, pessimistic%nearly%as]
In Section 5 we give some statistics on the number of semantic expressions containing % in the data used
for our experiments and explain how it affects our accruracy results.
1The term QLF is now sometimes used informally (e.g. Liakata and Pulman (2002), Poon and Domingos (2009)) for any
logic-like semantic representation without explicit quantifier scope.
2NLF does include Horn clauses, which implictly encode negation, but since Horn clauses are not part of the experiments
reported in this paper, we will not discuss them further here.
17
[acquired
/stealthily
:[in, ?, 2002],
Chirpy+Systems,
companies.two
:profitable
:[producing,
?,
pet+accessories]]
Figure 1: Example of an NLF semantic expression.
Operator Example Denotation Language Constructs
[...] [sold, Chirpy, Growler] predication tuple clauses, prepositions, ...
: company:profitable intersection adjectives, relative clauses, ...
. companies.two (unscoped) quantification determiners, measure terms
? [in, ?, 2005] variable-free abstract prepositions, relatives, ...
_ [eating, _, apples] unspecified argument missing verb arguments, ...
{...} and{Chirpy, Growler} collection noun phrase coordination, ...
/ acquired/stealthily type-preserving operator adverbs, modals, ...
+ Chirpy+Systems implicit relation compound nominals, ...
@ meeting@yesterday temporal restriction bare temporal modifiers, ...
& [...] & [...] conjunction sentences, ...
|...| |Dublin, Paris, Bonn| sequence paragraphs, fragments, lists, ...
% met%as uncovered op constructs not covered
Table 1: NLF constructs and connectives.
4 Encoding Semantics as Dependencies
We encode NLF semantic expressions as labeled dependency trees in which the label set is generated
automatically by the encoding process. This is in contrast to conventional dependency trees for which
the label sets are presupplied (e.g. by a linguistic theory of dependency grammar). The purpose of
the encoding is to enable training of a statistical dependency parser and converting the output of that
parser for a new sentence into a semantic expression. The encoding involves three aspects: Alignment,
headedness, and label construction.
4.1 Alignment
Since, by design, each word token corresponds to a symbol token (the same word type) in the NLF ex-
pression, the only substantive issue in determining the alignment is the occurrence of multiple tokens
of the same word type in the sentence. Depending on the source of the sentence-NLF pairs used for
training, a particular word in the sentence may or may not already be associated with its corresponding
word position in the sentence. For example, in some of the experiments reported in this paper, this corre-
spondence is provided by the semantic expressions obtained by converting a constituency treebank (the
well-known Penn WSJ treebank). For situations in which the pairs are provided without this informa-
tion, as is the case for direct annotation of sentences with NLF expressions, we currently use a heuristic
greedy algorithm for deciding the alignment. This algorithm tries to ensure that dependents are near their
heads, with a preference for projective dependency trees. To guage the importance of including correct
alignments in the input pairs (as opposed to training with inferred alignments), we will present accuracy
results for semantic mapping for both correct and automatically infererred alignments.
18
4.2 Headedness
The encoding requires a definition of headedness for words in an NLF expression, i.e., a head-function
h from dependent words to head words. We define h in terms of a head-function g from an NLF
(sub)expression e to a word w appearing in that (sub)expression, so that, recursively:
g(w) = w
g([e1, ..., en]) = g(e1)
g(e1 : e2) = g(e1)
g(e1.e2) = g(e1)
g(e1/e2) = g(e1)
g(e1@e2) = g(e1)
g(e1&e2) = g(e1)
g(|e1, ..., en|) = g(e1)
g(e1{e2, ..., en}) = g(e1)
g(e1 + ...+ en) = g(en)
g(e1%e2) = g(e1)
Then a head word h(w) for a dependent w is defined in terms of the smallest (sub)expression e
containing w for which
h(w) = g(e) 6= w
For example, for the NLF expression in Figure 1, this yields the heads shown in Table 3. (The labels
shown in that table will be explained in the following section.)
This definition of headedness is not the only possible one, and other variations could be argued for.
The specific definition for NLF heads turns out to be fairly close to the notion of head in traditional
dependency grammars. This is perhaps not surprising since traditional dependency grammars are often
partly motivated by semantic considerations, if only informally.
4.3 Label Construction
As mentioned, the labels used during the encoding of a semantic expression into a dependency tree are
derived so as to enable reconstruction of the expression from a labeled dependency tree. In a general
sense, the labels may be regarded as a kind of formal semantic label, though more specifically, a label is
interpretable as a sequence of instructions for constructing the part of a semantic expression that links a
dependent to its head, given that part of the semantic expression, including that derived from the head,
has already been constructed. The string for a label thus consists of a sequence of atomic instructions,
where the decoder keeps track of a current expression and the parent of that expression in the expression
tree being constructed. When a new expression is created it becomes the current expression whose parent
is the old current expression. The atomic instructions (each expressed by a single character) are shown
in Table 2.
A sequence of instructions in a label can typically (but not always) be paraphrased informally as
?starting from head word wh, move to a suitable node (at or above wh) in the expression tree, add speci-
fied NLF constructs (connectives, tuples, abstracted arguments) and then add wd as a tuple or connective
argument.?
Continuing with our running example, the labels for each of the words are shown in Table 3.
Algorithmically, we find it convenient to transform semantic expressions into dependency trees and
vice versa via a derivation tree for the semantic expression in which the atomic instruction symbols listed
above are associated with individual nodes in the derivation tree.
The output of the statistical parser may contain inconsistent trees with formal labels, in particular
trees in which two different arguments are predicated to fill the same position in a semantic expression
tuple. For such cases, the decoder that produces the semantic expression applies the simple heuristic
19
Instruction Decoding action
[, {, | Set the current expression to a
newly created tuple, collection,
or sequence.
:, /, ., +, &, @, % Attach the current subexpression
to its parent with the specified
connective.
* Set the current expression to a
newly created symbol from the
dependent word.
0, 1, ... Add the current expression at the
specified parent tuple position.
?, _ Set the current subexpression to
a newly created abstracted-over or
unspecfied argument.
- Set the current subexpression to be
the parent of the current expression.
Table 2: Atomic instructions in formal label sequences.
Dependent Head Label
In acquired [:?1-*0
2002 in -*2
Chirpy Systems *+
Systems acquired -*1
stealthily acquired */
acquired [*0
two companies *.
profitable companies *:
companies acquired -*2
producing companies [:?1-*0
pet accessories *+
accessories producing -*2
Table 3: Formal labels for an example sentence.
20
Dataset Null Labels? Auto Align? WSJ sections Sentences
Train+Null-AAlign yes no 2-21 39213
Train-Null-AAlign no no 2-21 24110
Train+Null+AAlign yes yes 2-21 35778
Train-Null+AAlign no yes 2-21 22611
Test+Null-AAlign yes no 23 2416
Test-Null-AAlign no no 23 1479
Table 4: Datasets used in experiments.
of using the next available tuple position when such a conflicting configuration is predicated. In our
experiments, we are measuring per-word semantic head-and-label accuracy, so this heuristic does not
play a part in that evaluation measure.
5 Experiments
5.1 Data Preparation
In the experiments reported here, we derive our sentence-semantics pairs for training and testing from
the Penn WSJ Treebank. This choice reflects the lack, to our knowledge, of a set of such pairs for a
reasonably sized publicly available corpus, at least for NLF expressions. Our first step in preparing the
data was to convert the WSJ phrase structure trees into semantic expressions. This conversion is done
by programming the Stanford treebank toolkit to produce NLF trees bottom-up from the phrase structure
trees. This conversion process is not particularly noteworthy in itself (being a traditional rule-based
syntax-to-semantics translation process) except perhaps to the extent that the closeness of NLF to natural
language perhaps makes the conversion somewhat easier than, say, conversion to a fully resolved logical
form.
Since our main goal is to investigate trainable mappings from text strings to semantic expressions,
we only use the WSJ phrase structure trees in data preparation: the phrase structure trees are not used as
inputs when training a semantic mapping model, or when applying such a model. For the same reason,
in these experiments, we do not use the part-of-speech information associated with the phrase structure
trees in training or applying a semantic mapping model. Instead of parts-of-speech we use word cluster
features from a hierarchical clustering produced with the unsupervised Brown clustering method (Brown
et al 1992); specifically we use the publicly available clusters reported in Koo et al (2008).
Constructions in the WSJ that are beyond the explicit coverage of the conversion rules used for data
preparation result in expressions that include the unknown/unspecified (or ?Null?) operator %. We report
on different experimental settings in which we vary how we treat training or testing expressions with
%. This gives rise to the data sets in Table 4 which have +Null (i.e., including %), and -Null (i.e., not
including %) in the data set names.
Another attribute we vary in the experiments is whether to align the words in the semantic expressions
to the words in the sentence automatically, or whether to use the correct alignment (in this case preserved
from the conversion process, but could equally be provided as part of a manual semantic annotation
scheme, for example). In our current experiments, we discard non-projective dependency trees from
training sets. Automatic alignment results in additional non-projective trees, giving rise to different
effective training sets when auto-alignment is used: these sets are marked with +AAlign, otherwise -
AAlign. The training set numbers shown in Table 4 are the resulting sets after removal of non-projective
trees.
21
Training Test Accuracy(%)
+Null-AAlign +Null-AAlign 81.2
-Null-AAlign +Null-AAlign 78.9
-Null-AAlign -Null-AAlign 86.1
+Null-AAlign -Null-AAlign 86.5
Table 5: Per-word semantic accuracy when training with the correct alignment.
Training Test Accuracy(%)
+Null+AAlign +Null-AAlign 80.4
-Null+AAlign +Null-AAlign 78.0
-Null+AAlign -Null-AAlign 85.5
+Null+AAlign -Null-AAlign 85.8
Table 6: Per-word semantic accuracy when training with an auto-alignment.
5.2 Parser
As mentioned earlier, our method can make use of any trainable statistical dependency parsing algorithm.
The parser is trained on a set of dependency trees with formal labels as explained in Sections 2 and 4.
The specific parsing algorithm we use in these experiments is a deterministic shift reduce algorithm
(Nivre, 2003), and the specific implementation of the algorithm uses a linear SVM classifier for predict-
ing parsing actions (Chang et al, 2010). As noted above, hierarchical cluster features are used instead
of parts-of-speech; some of the features use coarse (6-bit) or finer (12-bit) clusters from the hierarchy.
More specifically, the full set of features is:
? The words for the current and next input tokens, for the top of the stack, and for the head of the
top of the stack.
? The formal labels for the top-of-stack token and its leftmost and rightmost children, and for the
leftmost child of the current token.
? The cluster for the current and next three input tokens and for the top of the stack and the token
below the top of the stack.
? Pairs of features combining 6-bit clusters for these tokens together with 12-bit clusters for the top
of stack and next input token.
5.3 Results
Tables 5 and 6 show the per-word semantic accuracy for different training and test sets. This measure is
simply the percentage of words in the test set for which both the predicted formal label and the head word
are correct. In syntactic dependency evaluation terminology, this corresponds to the labeled attachment
score.
All tests are with respect to the correct alignment; we vary whether the correct alignment (Table 5)
or auto-alignment (Table 6) is used for training to give an idea of how much our heuristic alignment
is hurting the semantic mapping model. As shown by comparing the two tables, the loss in accuracy
due to using the automatic alignment is only about 1%, so while the automatic alignment algorithm can
probably be improved, the resulting increase in accuracy would be relatively small.
As shown in the Tables 5 and 6, two versions of the test set are used: one that includes the ?Null?
operator %, and a smaller test set with which we are testing only the subset of sentences for which the
semantic expressions do not include this label. The highest accuracies (mid 80?s) shown are for the
22
# Labels # Train Sents Accuracy(%)
151 (all) 22611 85.5
100 22499 85.5
50 21945 85.5
25 17669 83.8
12 7008 73.4
Table 7: Per-word semantic accuracy after pruning label sets in Train-Null+AAlign (and testing with
Test-Null-AAlign).
(easier) test set which excludes examples in which the test semantic expressions contain Null operators.
The strictest settings, in which semantic expressions with Null are not included in training but included
in the test set effectively treat prediction of Null operators as errors. The lower accuracy (high 70?s) for
such stricter settings thus incorporates a penalty for our incomplete coverage of semantics for the WSJ
sentences. The less strict Test+Null settings in which % is treated as a valid output may be relevant to
applications that can tolerate some unknown operators between subexpressions in the output semantics.
Next we look at the effect of limiting the size of the automatically generated formal label set prior
to training. For this we take the configuration using the TrainWSJ-Null+AAlign training set and the
TestWSJ-Null-AAlign test set (the third row in Table refPerWordSemanticAccuracyAAlign for which
auto-alignment is used and only labels without the NULL operator % are included). For this training
set there are 151 formal labels. We then limit the training set to instances that only include the most
frequent k labels, for k = 100, 50, 25, 12, while keeping the test set the same. As can be seen in Table 7,
the accuracy is unaffected when the training set is limited to the 100 most frequent or 50 most frequent
labels. There is a slight loss when training is limited to 25 labels and a large loss if it is limited to 12
labels. This appears to show that, for this corpus, the core label set needed to construct the majority
of semantic expressions has a size somewhere between 25 and 50. It is perhaps interesting that this is
roughly the size of hand-produced traditional dependency label sets. On the other hand, it needs to be
emphasized that since Table 7 ignores beyond-coverage constructions that presently include Null labels,
it is likely that a larger label set would be needed for more complete semantic coverage.
6 Conclusion and Further Work
We?ve shown that by designing an underspecified logical form that is motivated by, and closely related to,
natural language constructions, it is possible to train a direct statistical mapping from pairs of sentences
and their corresponding semantic expressions, with per-word accuracies ranging from 79% to 86% de-
pending on the strictness of the experimental setup. The input to training does not require any traditional
syntactic categories or parts of speech. We also showed, more specifically, that we can train a model that
can be applied deterministically at runtime (using a deterministic shift reduce algorithm combined with
deterministic clusters), making large-scale text-to-semantics mapping feasible.
In traditional formal semantic mapping methods (Montague (1973), Bos et al (2004)), and even
some recent statistical mapping methods (Zettlemoyer and Collins, 2005), the semantic representation is
overloaded to performs two functions: (i) representing the final meaning, and (ii) composing meanings
from the meanings of subconstituents (e.g. through application of higher order lambda functions). In our
view, this leads to what are perhaps overly complex semantic representations of some basic linguistic
constructions. In contrast, in the method we presented, these two concerns (meaning representation and
semantic construction) are separated, enabling us to keep the semantics of constituents simple, while
turning the construction of semantic expressions into a separate structured learning problem (with its
own internal prediction and decoding mechanisms).
Although, in the experiments we reported here we do prepare the training data from a traditional
treebank, we are encouraged by the results and believe that annotation of a corpus with only semantic
23
expressions is sufficient for building an efficient and reasonably accurate text-to-semantics mapper. In-
deed, we have started building such a corpus for a question answering application, and hope to report
results for that corpus in the future. Other further work includes a formal denotational semantics of the
underspecified logical form and elaboration of practical inference operations with the semantic expres-
sions. This work may also be seen as a step towards viewing semantic interpretation of language as the
interaction between a pattern recognition process (described here) and an inference process.
References
Hiyan Alshawi and Richard Crouch. 1992. Monotonic Semantic Interpretation. Proceedings of the 30th Annual
Meeting of the Association for Computational Linguistics. Newark, Delaware, 32?39.
Hiyan Alshawi, ed. 1992. The Core Language Engine. MIT Press, Cambridge, Massachusetts.
Hiyan Alshawi. 1996. Underspecified First Order Logics. In Semantic Ambiguity and Underspecification, edited
by Kees van Deemter and Stanley Peters, CSLI Publications, Stanford, California.
Johan van Benthem. 1986. Essays in Logical Semantics. Reidel, Dordrecht.
Johan van Benthem. 1995. Language in Action: Categories, Lambdas, and Dynamic Logic. MIT Press, Cam-
bridge, Massachusetts.
Bos, Johan, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. Proceedings of the 20th International Conference on Computa-
tional Linguistics. Geneva, Switzerland, 1240?1246.
P. Brown, V. Pietra, P. Souza, J. Lai, and R. Mercer. 1992. Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Eugene Charniak. 2000. A maximum entropy inspired parser. Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational Linguistics, Seattle, Washington.
Michael Collins. 1999. Head Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
A. Copestake, D. Flickinger, I. Sag, C. Pollard. 2005. Minimal Recursion Semantics, An Introduction. Research
on Language and Computation, 3(23):281-332.
D. Davidson. 1967. The Logical Form of Action Sentences. In The Logic of Decision and Action, edited by
N. Rescher, University of Pittsburgh Press, Pittsburgh, Pennsylvania.
Jason Eisner. 1996. Three New Probabilistic Models for Dependency Parsing: An Exploration. Proceedings of
the 16th International Conference on Computational Linguistics (COLING-96, 340?345.
T. Koo, X. Carreras, and M. Collins. 2008. Simple Semisupervised Dependency Parsing. Proceedings of the
Annual Meeting of the Association for Computational Linguistics.
Maria Liakata and Stephen Pulman. 2002. From trees to predicate-argument structures. Proceedings of the 19th
International Conference on Computational Linguistics. Taipei, Taiwan, 563?569.
Chang, Y.-W., C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin. 2010. Training and Testing Low-degree
Polynomial Data Mappings via Linear SVM. Journal of Machine Learning Research, 11, 1471?1490.
Ryan McDonald, Koby Crammer and Fernando Pereira 2005. Online Large-Margin Training of Dependency
Parsers. Proceedomgs of the 43rd Annual Meeting of the Association for Computational Linguistics..
R. Montague. 1973. The Proper Treatment of Quantification in Ordinary English. In Formal Philosophy, edited
by R. Thomason, Yale University Press, New Haven.
Fernando Pereira and Stuart Shieber. 1987. Prolog and Natural Language Analysis. Center for the Study of
Language and Information, Stanford, California.
Joakim Nivre 2003 An Efficient Algorithm for Projective Dependency Parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies, 149?160.
H. Poon and P. Domingos 2009. Unsupervised semantic parsing. Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, Singapore, 2009.
Mark Stickel. 1985. Automated deduction by theory resolution. Journal of Automated Reasoning, 1, 4.
Hiroyasu Yamada and Yuji Matsumoto 2003. Statistical dependency analysis with support vector machines.
Proceedings of the 8th International Workshop on Parsing Technologies, 195?206.
Zettlemoyer, Luke S. and Michael Collins. 2005. Learning to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. Proceedings of the 21st Conference on Uncertainty in Artificial
Intelligence. Edinburgh, Scotland, 658?666.
24
