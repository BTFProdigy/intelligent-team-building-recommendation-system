Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 945?955,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Two Is Bigger (and Better) Than One: the Wikipedia Bitaxonomy Project
Tiziano Flati, Daniele Vannella, Tommaso Pasini and Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
{flati,vannella,navigli}@di.uniroma1.it
p.tommaso@gmail.com
Abstract
We present WiBi, an approach to the
automatic creation of a bitaxonomy for
Wikipedia, that is, an integrated taxon-
omy of Wikipage pages and categories.
We leverage the information available in
either one of the taxonomies to reinforce
the creation of the other taxonomy. Our
experiments show higher quality and cov-
erage than state-of-the-art resources like
DBpedia, YAGO, MENTA, WikiNet and
WikiTaxonomy. WiBi is available at
http://wibitaxonomy.org.
1 Introduction
Knowledge has unquestionably become a key
component of current intelligent systems in many
fields of Artificial Intelligence. The creation and
use of machine-readable knowledge has not only
entailed researchers (Mitchell, 2005; Mirkin et al,
2009; Poon et al, 2010) developing huge, broad-
coverage knowledge bases (Hovy et al, 2013;
Suchanek and Weikum, 2013), but it has also
hit big industry players such as Google (Singhal,
2012) and IBM (Ferrucci, 2012), which are mov-
ing fast towards large-scale knowledge-oriented
systems.
The creation of very large knowledge bases
has been made possible by the availability of
collaboratively-curated online resources such as
Wikipedia and Wiktionary. These resources are
increasingly becoming enriched with new con-
tent in many languages and, although they are
only partially structured, they provide a great deal
of valuable knowledge which can be harvested
and transformed into structured form (Medelyan
et al, 2009; Hovy et al, 2013). Prominent
examples include DBpedia (Bizer et al, 2009),
BabelNet (Navigli and Ponzetto, 2012), YAGO
(Hoffart et al, 2013) and WikiNet (Nastase and
Strube, 2013). The types of semantic relation
in these resources range from domain-specific, as
in Freebase (Bollacker et al, 2008), to unspec-
ified relations, as in BabelNet. However, un-
like the case with smaller manually-curated re-
sources such as WordNet (Fellbaum, 1998), in
many large automatically-created resources the
taxonomical information is either missing, mixed
across resources, e.g., linking Wikipedia cate-
gories to WordNet synsets as in YAGO, or coarse-
grained, as in DBpedia whose hypernyms link to a
small upper taxonomy.
Current approaches in the literature have mostly
focused on the extraction of taxonomies from the
network of Wikipedia categories. WikiTaxonomy
(Ponzetto and Strube, 2007), the first approach
of this kind, is based on the use of heuristics
to determine whether is-a relations hold between
a category and its subcategories. Subsequent ap-
proaches have also exploited heuristics, but have
extended them to any kind of semantic relation
expressed in the category names (Nastase and
Strube, 2013). But while the aforementioned at-
tempts provide structure for categories that sup-
ply meta-information for Wikipedia pages, sur-
prisingly little attention has been paid to the ac-
quisition of a full-fledged taxonomy for Wikipedia
pages themselves. For instance, Ruiz-Casado et
al. (2005) provide a general vector-based method
which, however, is incapable of linking pages
which do not have a WordNet counterpart. Higher
coverage is provided by de Melo and Weikum
(2010) thanks to the use of a set of effective heuris-
tics, however, the approach also draws on Word-
Net and sense frequency information.
In this paper we address the task of taxono-
mizing Wikipedia in a way that is fully indepen-
dent of other existing resources such as WordNet.
We present WiBi, a novel approach to the cre-
ation of a Wikipedia bitaxonomy, that is, a tax-
onomy of Wikipedia pages aligned to a taxonomy
of categories. At the core of our approach lies the
idea that the information at the page and category
945
level are mutually beneficial for inducing a wide-
coverage and fine-grained integrated taxonomy.
2 WiBi: A Wikipedia Bitaxonomy
We induce a Wikipedia bitaxonomy, i.e., a taxon-
omy of pages and categories, in 3 phases:
1. Creation of the initial page taxonomy: we
first create a taxonomy for the Wikipedia
pages by parsing textual definitions, ex-
tracting the hypernym(s) and disambiguating
them according to the page inventory.
2. Creation of the bitaxonomy: we leverage
the hypernyms in the page taxonomy, to-
gether with their links to the corresponding
categories, so as to induce a taxonomy over
Wikipedia categories in an iterative way. At
each iteration, the links in the page taxonomy
are used to identify category hypernyms and,
conversely, the new category hypernyms are
used to identify more page hypernyms.
3. Refinement of the category taxonomy: fi-
nally we employ structural heuristics to over-
come inherent problems affecting categories.
The output of our three-phase approach is a bitax-
onomy of millions of pages and hundreds of thou-
sands of categories for the English Wikipedia.
3 Phase 1: Inducing the Page Taxonomy
The goal of the first phase is to induce a taxonomy
of Wikipedia pages. Let P be the set of all the
pages and let T
P
= (P,E) be the page taxonomy
directed graph whose nodes are pages and whose
edge set E is initially empty (E := ?). For each
p ? P our aim is to identify the most suitable gen-
eralization p
h
? P so that we can create the edge
(p, p
h
) and add it to E. For instance, given the
page APPLE, which represents the fruit meaning
of apple, we want to determine that its hypernym
is FRUIT and add the hypernym edge connecting
the two pages (i.e., E := E?{(APPLE, FRUIT)}).
To do this, we perform a syntactic step, in which
the hypernyms are extracted from the page?s tex-
tual definition, and a semantic step, in which the
extracted hypernyms are disambiguated according
to the Wikipedia inventory.
3.1 Syntactic step: hypernym extraction
In the syntactic step, for each page p ? P , we
extract zero, one or more hypernym lemmas, that
is, we output potentially ambiguous hypernyms
for the page. The first assumption, which follows
Julia Fiona Roberts is
an
American actress
NNP NNP NNP VBZ DT JJ
NN
nn
nn
nsubj
cop
det
amod
Figure 1: A dependency tree example with copula.
the Wikipedia guidelines and is validated in the
literature (Navigli and Velardi, 2010; Navigli and
Ponzetto, 2012), is that the first sentence of each
Wikipedia page p provides a textual definition for
the concept represented by p. The second assump-
tion we build upon is the idea that a lexical tax-
onomy can be obtained by extracting hypernyms
from textual definitions. This idea dates back to
the early 1970s (Calzolari et al, 1973), with later
developments in the 1980s (Amsler, 1981; Calzo-
lari, 1982) and the 1990s (Ide and V?eronis, 1993).
To extract hypernym lemmas, we draw on the
notion of copula, that is, the relation between the
complement of a copular verb and the copular verb
itself. Therefore, we apply the Stanford parser
(Klein and Manning, 2003) to the definition of a
page in order to extract all the dependency rela-
tions of the sentence. For example, given the def-
inition of the page JULIA ROBERTS, i.e., ?Julia
Fiona Roberts is an American actress.?, the Stan-
ford parser outputs the set of dependencies shown
in Figure 1. The noun involved in the copula re-
lation is actress and thus it is taken as the page?s
hypernym lemma. However, the extracted hyper-
nym is sometimes overgeneral (one, kind, type,
etc.). For instance, given the definition of the
page APOLLO, ?Apollo is one of the most impor-
tant and complex of the Olympian deities in an-
cient Greek and Roman religion [...].?, the only
copula relation extracted is between is and one.
To cope with this problem we use a list of stop-
words.
1
When such a term is extracted as hyper-
nym, we replace it with the rightmost noun of the
first following noun sequence (e.g., deity in the
above example). If the resulting lemma is again a
stopword we repeat the procedure, until a valid hy-
pernym or no appropriate hypernym can be found.
Finally, to capture multiple hypernyms, we iter-
atively follow the conj and and conj or relations
starting from the initially extracted hypernym. For
example, consider the definition of ARISTOTLE:
?Aristotle was a Greek philosopher and polymath,
a student of Plato and teacher of Alexander the
Great.? Initially, the philosopher hypernym is
selected thanks to the copula relation, then, fol-
1
E.g., species, genus, one, etc. Full list available online.
946
lowing the conjunction relations, also polymath,
student and teacher are extracted as hypernyms.
While more sophisticated approaches like Word-
Class Lattices could be applied (Navigli and Ve-
lardi, 2010), we found that, in practice, our hy-
pernym extraction approach provides higher cov-
erage, which is critical in our case.
3.2 Semantic step: hypernym disambiguation
Since our aim is to connect pairs of pages via
hypernym relations, our second step consists of
disambiguating the obtained hypernym lemmas of
page p by associating the most suitable page with
each hypernym. Following previous work (Ruiz-
Casado et al, 2005; Navigli and Ponzetto, 2012),
as the inventory for a given lemma we consider the
set of pages whose main title is the lemma itself,
except for the sense specification in parenthesis.
For instance, given fruit as the hypernym for AP-
PLE we would like to link APPLE to FRUIT as op-
posed to, e.g., FRUIT (BAND) or FRUIT (ALBUM).
3.2.1 Hypernym linkers
To disambiguate hypernym lemmas, we exploit
the structural features of Wikipedia through a
pipeline of hypernym linkers L = {L
i
}, applied
in cascade order (cf. Section 3.3.1). We start with
the set of page-hypernym pairs H = {(p, h)} as
obtained from the syntactic step. The successful
application of a linker to a pair (p, h) ? H yields
a page p
h
as the most suitable sense of h, result-
ing in setting isa(p, h) = p
h
. At step i, the i-
th linker L
i
? L is applied to H and all the hy-
pernyms which the linker could disambiguate are
removed from H . This prevents lower-precision
linkers from overriding decisions taken by more
accurate ones.
We now describe the hypernym linkers. In what
follows we denote with p
h
? p
h
the fact that the
definition of a Wikipedia page p contains an oc-
currence of h linked to page p
h
. Note that p
h
is
not necessarily a sense of h.
Crowdsourced linker If p
h
? p
h
, i.e., the hyper-
nym h is found to have been manually linked to p
h
in p by Wikipedians, we assign isa(p, h) = p
h
.
For example, because capital was linked in the
BRUSSELS page definition to CAPITAL CITY, we
set isa(BRUSSELS, capital) = CAPITAL CITY.
Category linker Given the set W ? P of
Wikipedia pages which have at least one category
in common with p, we select the majority sense
of h, if there is one, as hyperlinked across all the
definitions of pages in W :
isa(p, h) = argmax
p
h
?
p
?
?W
1(p
?
h
? p
h
)
where 1(p
?
h
? p
h
) is the characteristic function
which equals 1 if h is linked to p
h
in page
p
?
, 0 otherwise. For example, the linker sets
isa(EGGPLANT, plant) = PLANT because most of
the pages associated with TROPICAL FRUIT, a cat-
egory of EGGPLANT, contain in their definitions
the term plant linked to the PLANT page.
Multiword linker If p
m
? p
h
and m is a
multiword expression containing the lemma h
as one of its words, set isa(p, h) = p
h
. For
example, we set isa(PROTEIN, compound) =
CHEMICAL COMPOUND, as chemical compound
is linked to CHEMICAL COMPOUND in the defini-
tion of PROTEIN.
Monosemous linker If h is monosemous in
Wikipedia (i.e., there is only a single page p
h
for
that lemma), link it to its only sense by setting
isa(p, h) = p
h
. For example, we extract the
hypernym businessperson from the definition of
MERCHANT and, as it is unambiguous, we link
it to BUSINESSPERSON.
Distributional linker Finally, we provide a dis-
tributional approach to hypernym disambiguation.
We represent the textual definition of page p as a
distributional vector ~v
p
whose components are all
the English lemmas in Wikipedia. The value of
each component is the occurrence count of the cor-
responding content word in the definition of p.
The goal of this approach is to find the best
link for hypernym h of p among the pages h is
linked to, across the whole set of definitions in
Wikipedia. Formally, for each p
h
such that h
is linked to p
h
in some definition, we define the
set of pages P (p
h
) whose definitions contain a
link to p
h
, i.e., P (p
h
) = {p
?
? P |p
?
h
? p
h
}.
We then build a distributional vector ~v
p
?
for each
p
?
? P (p
h
) as explained above and create an ag-
gregate vector ~v
p
h
=
?
p
?
~v
p
?
. Finally, we de-
termine the similarity of p to each p
h
by calcu-
lating the dot product between the two vectors
sim(p, p
h
) = ~v
p
? ~v
p
h
. If sim(p, p
h
) > 0 for any
p
h
we perform the following association:
isa(p, h) = argmax
p
h
sim(p, p
h
)
For example, thanks to this linker we set
isa(VACUUM CLEANER, device) = MACHINE.
947
Figure 2: Distribution of linked hypernyms.
3.3 Page Taxonomy Evaluation
Statistics We applied the above linkers to the
October 2012 English Wikipedia dump. Out of
the 3,829,058 total pages, 4,270,232 hypernym
lemmas were extracted in the syntactic step for
3,697,113 pages (covering more than 96% of the
total). Due to illformed definitions, though, it
was not always possible to extract the hypernym
lemma: for example, 6 APRIL 2010 BAGHDAD
BOMBINGS is defined as ?A series of bomb ex-
plosions destroyed several buildings in Baghdad?,
which only implicitly provides the hypernym.
The semantic step disambiguated 3,718,612 hy-
pernyms for 3,294,562 Wikipedia pages, i.e., cov-
ering more than 86% of the English pages with at
least one disambiguated hypernym. Figure 2 plots
the number and distribution of hypernyms disam-
biguated by our hypernym linkers.
Taxonomy quality To evaluate the quality of
our page taxonomy we randomly sampled 1,000
Wikipedia pages. For each page we provided: i)
a list of suitable hypernym lemmas for the page,
mainly selected from its definition; ii) for each
lemma the correct hypernym page(s). We calcu-
lated precision as the average ratio of correct hy-
pernym lemmas (senses) to the total number of
lemmas (senses) returned for all the pages in the
dataset, recall as the number of correct lemmas
(senses) over the total number of lemmas (senses)
in the dataset, and coverage as the fraction of
pages for which at least one lemma (sense) was
returned, independently of its correctness. Results,
both at lemma- and sense-level, are reported in Ta-
ble 1. Not only does our taxonomy show high pre-
cision and recall in extracting ambiguous hyper-
nyms, it also disambiguates more than 3/4 of the
hypernyms with high precision.
3.3.1 Hypernym linker order
The optimal order of application of the above
linkers is the same as that presented in Section
3.2.1. It was established by selecting the combina-
tion, among all possible permutations, which max-
imized precision on a tuning set of 100 randomly
sampled pages, disjoint from our page dataset.
Prec. Rec. Cov.
Lemma 94.83 90.20 98.50
Sense 82.77 75.10 89.20
Table 1: Page taxonomy performance.
4 Phase 2: Inducing the Bitaxonomy
The page taxonomy built in Section 3 will serve
as a stable, pivotal input to the second phase, the
aim of which is to build our bitaxonomy, that is, a
taxonomy of pages and categories. Our key idea
is that the generalization-specialization informa-
tion available in each of the two taxonomies is
mutually beneficial. We implement this idea by
exploiting one taxonomy to update the other, and
vice versa, in an iterative way, until a fixed point
is reached. The final output of this phase is, on the
one hand, a page taxonomy augmented with addi-
tional hypernymy relations and, on the other hand,
a category taxonomy which is built from scratch.
4.1 Initialization
Our bitaxonomy B = {T
P
, T
C
} is a pair consist-
ing of the page taxonomy T
P
= (P,E), as ob-
tained in Section 3, and the category taxonomy
T
C
= (C, ?), which initially contains all the cate-
gories as nodes but does not include any hypernym
edge between category nodes. In the following
we describe the core algorithm of our approach,
which iteratively and mutually populates and re-
fines the edge sets E(T
P
) and E(T
C
).
4.2 The Bitaxonomy Algorithm
Preliminaries Before proceeding, we define
some basic concepts that will turn out to be use-
ful for presenting our algorithm. We denote by
super
T
(t) the set of all ancestors of a node t in the
taxonomy T (be it T
P
or T
C
). We further define a
verification function t;
T
t
?
which, in the case of
T
C
, returns true if there is a path in the Wikipedia
category network between t and t
?
, false other-
wise, and, in the case of T
P
, returns true if t
?
is
a sense, i.e., a page, of a hypernym h of t (that
is, (t, h) ? H , cf. Section 3.2.1). For instance,
SPORTSMEN ;
T
C
MEN BY OCCUPATION holds
for categories because the former is a sub-category
of the latter in Wikipedia, and RADIOHEAD ;
T
P
BAND (MUSIC) for pages, because band is a hy-
pernym extracted from the textual definition of
RADIOHEAD and BAND (MUSIC) is a sense of
band in Wikipedia. Note that, while the super
function returns information that we have already
learned, i.e., it is in T
P
and T
C
, the ; operator
948
holds just for candidate is-a relations, as it uses
knowledge from Wikipedia itself which is poten-
tially incorrect. For instance, SPORTSMEN ;
T
C
MEN?S SPORTS in the Wikipedia category net-
work, and RADIOHEAD ;
T
P
BAND (RADIO) be-
tween the two Wikipedia pages, both hold accord-
ing to our definition of ;, while connecting the
wrong hypernym candidates. At the core of our
algorithm, explained below, is the mutual lever-
aging of the super function from one of the two
taxonomies (pages or categories) to decide about
which candidates (for which a ; relation holds)
in the other taxonomy are real hypernyms.
Finally, we define the projection operator pi,
such that pi(c), c ? C, is the set of pages
categorized with c, and pi(p), p ? P , is the
set of categories associated with page p in
Wikipedia. For instance, the pages which belong
to the category OLYMPIC SPORTS are given by
pi(OLYMPIC SPORTS) = {BASEBALL, BOXING,
. . . , TRIATHLON}. Vice versa, pi(TRIATHLON) =
{MULTISPORTS, OLYMPIC SPORTS, . . . , OPEN
WATER SWIMMING}. The projection operator pi
enables us to jump from one taxonomy to the other
and expresses the mutual membership relation be-
tween pages and categories.
Algorithm We now describe in detail the bitax-
onomy algorithm, whose pseudocode is given in
Algorithm 1. The algorithm takes as input the two
taxonomies, initialized as described in Section 4.1.
Starting from the category taxonomy (line 1), the
algorithm updates the two taxonomies in turn, un-
til convergence is reached, i.e., no more edges can
be added to any side of the bitaxonomy. Let T be
the current taxonomy considered at a given mo-
ment and T
?
its dual taxonomy. The algorithm
proceeds by selecting a node t ? V (T ) for which
no hypernym edge (t, t
h
) could be found up until
that moment (line 3), and then tries to infer such
a relation by drawing on the dual taxonomy T
?
(lines 5-12). This is the core of the bitaxonomy al-
gorithm, in which hypernymy knowledge is trans-
ferred from one taxonomy to the other. By apply-
ing the projection operator pi to t, the algorithm
considers those nodes t
?
aligned to t in the dual
taxonomy (line 5) and obtains their hypernyms t
?
h
using the super
T
?
function (line 6). The nodes
reached in T
?
act as a clue for discovering the suit-
able hypernyms for the starting node t ? V (T ).
To perform the discovery, the algorithm projects
each such hypernym node t
?
h
? S and increments
the count of each projection t
h
? pi(t
?
h
) (line
Algorithm 1 The Bitaxonomy Algorithm
Input: T
P
, T
C
1: T := T
C
, T
?
:= T
P
2: repeat
3: for all t ? V (T ) s.t. @(t, t
h
) ? E(T ) do
4: reset count
5: for all t
?
? pi(t) do
6: S := super
T
?
(t
?
)
7: for all t
?
h
? S do
8: for all t
h
? pi(t
?
h
) do count(t
h
)++ end for
9: end for
10: end for
11:
?
t
h
:= argmax
t
h
: t;
T
t
h
count(t
h
)
12: if count(
?
t
h
) > 0 thenE(T ) := E(T )?{(t,
?
t
h
)}
13: end for
14: swap T and T
?
15: until convergence
16: return {T, T
?
}
8). Finally, the node
?
t
h
? V (T ) with maximum
count, and such that t ;
T
?
t
h
holds, if one exists,
is promoted as hypernym of t and a new hypernym
edge (t,
?
t
h
) is added toE(T ) (line 12). Finally, the
role of the two taxonomies is swapped and the pro-
cess is repeated until no more change is possible.
Example Let us illustrate the algorithm by way
of an example. Assume we are in the first iteration
(T = T
C
) and consider the Wikipedia category
t = OLYMPICS (line 3) and its super-categories
{MULTI-SPORT EVENTS, SPORT AND POLITICS,
INTERNATIONAL SPORTS COMPETITIONS}. This
category has 27 pages associated with it (line
5), 23 of which provide a hypernym page in T
P
(line 6): e.g., PARALYMPIC GAMES, associated
with the category OLYMPICS, is a MULTI-SPORT
EVENT and is therefore contained in S. By con-
sidering and counting the categories of each page
in S (lines 7-8), we end up counting the category
MULTI-SPORT EVENTS four times and other
categories, such as AWARDS and SWIMSUITS,
once. As MULTI-SPORT EVENTS has the highest
count and is connected to OLYMPICS by a path
in the Wikipedia category network (line 11),
the hypernym edge (OLYMPICS, MULTI-SPORT
EVENTS) is added to T
C
(line 12).
5 Phase 3: Category taxonomy
refinement
As the final phase, we refine and enrich the cate-
gory taxonomy. The goal of this phase is to pro-
vide broader coverage to the category taxonomy
T
C
created as explained in Section 4. We apply
three enrichment heuristics which add hypernyms
to those categories c for which no hypernym could
be found in phase 2, i.e., @c
?
s.t. (c, c
?
) ? E(T
C
).
949
Single super-category As a first structural re-
finement, we automatically link an uncovered cat-
egory c to c
?
if c
?
is the only direct super-category
of c in Wikipedia.
Sub-categories We increase the hypernym cov-
erage by exploiting the sub-categories of each un-
covered category c (see Figure 3a). In detail,
for each uncovered category c we consider the
set sub(c) of all the Wikipedia subcategories of
c (nodes c
1
, c
2
, . . . , c
n
in Figure 3a) and then let
each category vote, according to its direct hyper-
nym categories in T
C
(the vote is as in Algo-
rithm 1). Then we proceed in decreasing order
of vote and select the highest-ranking category c
?
which is connected to c via a path in T
C
, i.e.,
c ;
T
C
c
?
. We then pick up the direct ancestor
c
??
of c which lies in the path from c to c
?
and
add the hypernym edge (c, c
??
) to E(T
C
). For ex-
ample, consider the category FRENCH TELEVI-
SION PEOPLE; since this category has no asso-
ciated pages, in phase 2 no hypernym could be
found. However, by applying the sub-categories
heuristic, we discover that TELEVISION PEOPLE
BY COUNTRY is the hypernym most voted by our
target category?s descendants, such as FRENCH
TELEVISION ACTORS and FRENCH TELEVISION
DIRECTORS. Since TELEVISION PEOPLE BY
COUNTRY is at distance 1 in the Wikipedia
category network from FRENCH TELEVISION
PEOPLE, we add (FRENCH TELEVISION PEOPLE,
TELEVISION PEOPLE BY COUNTRY) to E(T
C
).
Super-categories We then apply a similar
heuristic involving super-categories (see Figure
3b). Given an uncovered category c, we consider
its direct Wikipedia super-categories and let them
vote, according to their hypernym categories in
T
C
. Then we proceed in decreasing order of vote
and select the highest-ranking category c
?
which is
connected to c in T
C
, i.e., c;
T
C
c
?
. We then pick
up the direct ancestor c
??
of c which lies in the path
from c to c
?
and add the edge (c, c
??
) to E(T
C
).
5.1 Bitaxonomy Evaluation
Category taxonomy statistics We applied
phases 2 and 3 to the output of phase 1, which
was evaluated in Section 3.3. In Figure 4a we
show the increase in category coverage at each
iteration throughout the execution of the two
phases (1SUP, SUB and SUPER correspond to
the three above heuristics of phase 3). The final
outcome is a category taxonomy which includes
594,917 hypernymy links between categories,
c
?
d
e
c
??
c
c
1
c
2
. . .
c
n
(a) Sub categ. heuristic.
hypernym in T
C
Wikipedia super-category
c
?
c
???
c
1
c
??
c
m
. . .
c
(b) Super categ. heuristic.
Figure 3: Heuristic patterns for the coverage re-
finement of the category taxonomy.
covering more than 96% of the 618,641 categories
in the October 2012 English Wikipedia dump.
The graph shows the steepest slope in the first
iterations of phase 2, which converges around
400k categories at iteration 30, and a significant
boost due to phase 3 producing another 175k
hypernymy edges, with the super-category heuris-
tic contributing most. 78.90% of the nodes in
T
C
belong to the same connected component.
The average height of the biggest component of
T
C
is 23.26 edges and the maximum height is
49. We note that the average height of T
C
is
much greater than that of T
P
, which reflects the
category taxonomy distinguishing between very
subtle classes, such as ALBUMS BY ARTISTS,
ALBUMS BY RECORDING LOCATION, etc.
Category taxonomy quality To estimate the
quality of the category taxonomy, we ran-
domly sampled 1,000 categories and, for each of
them, we manually associated the super-categories
which were deemed to be appropriate hypernyms.
Figure 4b shows the performance trend as the al-
gorithm iteratively covers more and more cate-
gories. Phase 2 is particularly robust across it-
erations, as it leads to increased recall while re-
taining very high precision. As regards phase 3,
the super-categories heuristic leads to only a slight
precision decrease, while improving recall consid-
erably. Overall, the final taxonomy T
C
achieves
85.80% precision, 83.40% recall and 97.20% cov-
erage on our dataset.
Page taxonomy improvement As a result of
phase 2, 141,105 additional hypernymy links were
also added to the page taxonomy, resulting in
an overall 82.99% precision, 77.90% recall and
92.10% coverage, with a non-negligible 3% boost
from phase 1 to phase 2 in terms of recall and cov-
erage on our Wikipedia page dataset.
We also calculated some statistics for the result-
ing taxonomy obtained by aggregating the 3.8M
950
Figure 4: Category taxonomy evaluation.
hypernym links in a single directed graph. Over-
all, 99% of nodes belong to the same connected
component, with a maximum height of 29 and an
average height on the biggest component of 6.98.
6 Related Work
Although the extraction of taxonomies from
machine-readable dictionaries was already being
studied in the early 1970s (Calzolari et al, 1973),
pioneering work on large amounts of data only
appeared in the 1990s (Hearst, 1992; Ide and
V?eronis, 1993). Approaches based on hand-
crafted patterns and pattern matching techniques
have been developed to provide a supertype for
the extracted terms (Etzioni et al, 2004; Blohm,
2007; Kozareva and Hovy, 2010; Navigli and Ve-
lardi, 2010; Velardi et al, 2013, inter alia). How-
ever, these methods do not link terms to existing
knowledge resources such as WordNet, whereas
those that explicitly link do so by adding new
leaves to the existing taxonomy instead of acquir-
ing wide-coverage taxonomies from scratch (Pan-
tel and Ravichandran, 2004; Snow et al, 2006).
The recent upsurge of interest in collabo-
rative knowledge curation has enabled several
approaches to large-scale taxonomy acquisition
(Hovy et al, 2013). Most approaches initially
focused on the Wikipedia category network, an
entangled set of generalization-containment rela-
tions between Wikipedia categories, to extract the
hypernymy taxonomy as a subset of the network.
The first approach of this kind was WikiTaxonomy
(Ponzetto and Strube, 2007; Ponzetto and Strube,
2011), based on simple, yet effective lightweight
heuristics, totaling more than 100k is-a relations.
Other approaches, such as YAGO (Suchanek et
al., 2008; Hoffart et al, 2013), yield a taxonom-
ical backbone by linking Wikipedia categories to
WordNet. However, the categories are linked to
the first, i.e., most frequent, sense of the category
head in WordNet, involving only leaf categories in
the linking.
Interest in taxonomizing Wikipedia pages, in-
stead, developed with DBpedia (Auer et al, 2007),
which pioneered the current stream of work aimed
at extracting semi-structured information from
Wikipedia templates and infoboxes. In DBpedia,
entities are mapped to a coarse-grained ontology
which is collaboratively maintained and contains
only about 270 classes corresponding to popular
named entity types, in contrast to our goal of struc-
turing the full set of Wikipedia articles in a larger
and finer-grained taxonomy.
A few notable efforts to reconcile the two sides
of Wikipedia, i.e., pages and categories, have
been put forward very recently: WikiNet (Nas-
tase et al, 2010; Nastase and Strube, 2013) is a
project which heuristically exploits different as-
pects of Wikipedia to obtain a multilingual con-
cept network by deriving not only is-a relations,
but also other types of relations. A second project,
MENTA (de Melo and Weikum, 2010), creates
one of the largest multilingual lexical knowledge
bases by interconnecting more than 13M articles
in 271 languages. In contrast to our work, hy-
pernym extraction is supervised in that decisions
are made on the basis of labelled training exam-
ples and requires a reconciliation step owing to
the heterogeneous nature of the hypernyms, some-
thing that we only do for categories, due to their
noisy network. While WikiNet and MENTA bring
together the knowledge available both at the page
and category level, like we do, they either achieve
low precision and coverage of the taxonomical
structure or exhibit overly general hypernyms, as
we show in our experiments in the next section.
Our work differs from the others in at least three
respects: first, in marked contrast to most other re-
sources, but similarly to WikiNet and WikiTaxon-
omy, our resource is self-contained and does not
depend on other resources such as WordNet; sec-
ond, we address the taxonomization task on both
sides, i.e., pages and categories, by providing an
algorithm which mutually and iteratively transfers
knowledge from one side of the bitaxonomy to the
other; third, we provide a wide coverage bitaxon-
omy closer in structure and granularity to a manual
WordNet-like taxonomy, in contrast, for example,
to DBpedia?s flat entity-focused hierarchy.
2
2
Note that all the competitors on categories have average
height between 1 and 3.69 on their biggest component, while
we have 23.26, while on pages their height is between 1.9 and
4.22, while ours is 6.98. Since WordNet?s average height is
8.07 we deem WiBi to be the resource structurally closest to
WordNet.
951
Dataset System Prec. Rec. Cov.
Pages
WiBi 84.11 79.40 92.57
WikiNet 57.29
??
71.45
??
82.01
DBpedia 87.06 51.50
??
55.93
MENTA 81.52 72.49
?
88.92
Categories
WiBi 85.18 82.88 97.31
WikiTax 88.50 54.83
??
59.43
YAGO 94.13 53.41
??
56.74
MENTA 87.11 84.63 97.15
MENTA
?ENT
85.18 71.95
??
84.47
Table 2: Page and category taxonomy evaluation.
?
(
??
) denotes statistically significant difference,
using ?
2
test, p < 0.02 (p < 0.01) between WiBi
and the daggered resource.
7 Comparative Evaluation
7.1 Experimental Setup
We compared our resource (WiBi) against the
Wikipedia taxonomies of the major knowledge re-
sources in the literature providing hypernym links,
namely DBpedia, WikiNet, MENTA, WikiTax-
onomy and YAGO (see Section 6). As datasets,
we used our gold standards of 1,000 randomly-
sampled pages (see Section 3.3) and categories
(see Section 5.1). In order to ensure a level playing
field, we detected those pages (categories) which
do not exist in any of the above resources and re-
moved them to ensure full coverage of the dataset
across all resources. For each resource we cal-
culated precision, by manually marking each hy-
pernym returned for each page (category) as cor-
rect or not. As regards recall, we note that in
two cases (i.e., DBpedia returning page super-
types from its upper taxonomy, YAGO linking cat-
egories to WordNet synsets) the generalizations
are neither pages nor categories and that MENTA
returns heterogeneous hypernyms as mixed sets of
WordNet synsets, Wikipedia pages and categories.
Given this heterogeneity, standard recall across re-
sources could not be calculated. For this reason we
calculated recall as described in Section 3.3.
7.2 Results
Wikipedia pages We first report the results of
the knowledge resources which provide page hy-
pernyms, i.e., we compare against WikiNet, DB-
pedia and MENTA. We use the original outputs
from the three resources: the first two are based
on dumps which are from the same year as the one
used in WiBi (cf. Section 3.3), while MENTA is
based on a dump dating back to 2010 (consisting
of 3.25M pages and 565k categories). We decided
to include the latter for comparison purposes, as it
uses knowledge from 271 Wikipedias to build the
final taxonomy. However, we recognize its perfor-
mance might be relatively higher on a 2012 dump.
We show the results on our page hypernym
dataset in Table 2 (top). As can be seen, WikiNet
obtains the lowest precision, due to the high num-
ber of hypernyms provided, many of which are
incorrect, with a recall between that of DBpe-
dia and MENTA. WiBi outperforms all other re-
sources with 84.11% precision, 79.40% recall and
92.57% coverage. MENTA seems to be the clos-
est resource to ours, however, we remark that the
hypernyms output by MENTA are very heteroge-
neous: 48% of answers are represented by a Word-
Net synset, 37% by Wikipedia categories and 15%
are Wikipedia pages. In contrast to all other re-
sources, WiBi outputs page hypernyms only.
Wikipedia categories We then compared all the
knowledge resources which deal with categories,
i.e., WikiTaxonomy, YAGO and MENTA. For the
latter two, the above considerations about the 2012
dump hold, whereas we reimplemented WikiTax-
onomy, which was based on a 2009 dump, to run it
on the same dump as WiBi. We excluded WikiNet
from our comparison because it turned out to have
low coverage of categories (i.e., less than 1%).
We show the results on our category dataset
in Table 2 (bottom). Despite other systems ex-
hibiting higher precision, WiBi generally achieves
higher recall, thanks also to its higher category
coverage. YAGO obtains the lowest recall and
coverage, because only leaf categories are consid-
ered. MENTA is the closest system to ours, ob-
taining slightly higher precision and recall. No-
tably, however, MENTA outputs the first WordNet
sense of entity for 13.17% of all the given answers,
which, despite being correct and accounted in pre-
cision and recall, is uninformative. Since a system
which always outputs entity would maximise all
the three measures, we also calculated the perfor-
mance for MENTA when discarding entity as an
answer; as Table 2 shows (bottom, MENTA
?ENT
),
recall drops to 71.95%. Further analysis, pre-
sented below, shows that the specificity of its hy-
pernyms is considerably lower than that of WiBi.
7.3 Analysis of the results
To get further insight into our results we per-
formed two additional analyses of the data. First,
we estimated the level of specialization of the
hypernyms in the different resources on our two
datasets. The idea is that a hypernym should be
952
Dataset System (X) WiBi=X WiBi>X WiBi<X
Pages
WikiNet 33.38 34.94 31.68
DBpedia 31.68 56.71 11.60
MENTA 19.04 50.85 30.12
Categories
WikiTax 43.11 38.51 18.38
YAGO 12.36 81.14 6.50
MENTA 12.36 73.69 13.95
Table 3: Specificity comparison.
valid while at the same time being as specific as
possible (e.g., SINGER should be preferred over
PERSON). We therefore calculated a measure,
which we called specificity, that computes the per-
centage of times a system outputs a more specific
answer than another system. To do this, we anno-
tated each hypernym returned by a system as fol-
lows: ?1 if the answer was wrong, 0 if missing, >
0 if correct; more specific answers were assigned
higher scores. When comparing two systems, we
select the respective most specific answers a
1
, a
2
and say the first system is more specific than the
latter whenever score(a
1
) > score(a
2
). Table 3
shows the results for all the resources and for both
the page and category taxonomies: WiBi consis-
tently provides considerably more specific hyper-
nyms than any other resource (middle column).
A second important aspect that we analyzed was
the granularity of each taxonomy, determined by
drawing each resource on a bidimensional plane
with the number of distinct hypernyms on the
x axis and the total number of hypernyms (i.e.,
edges) in the taxonomy on the y axis. Figures 5a
and 5b show the position of each resource for the
page and the category taxonomies, respectively.
As can be seen, WiBi, as well as the page tax-
onomy of MENTA, is the resource with the best
granularity, as not only does it attain high cover-
age, but it also provides a larger variety of classes
as generalizations of pages and categories. Specif-
ically, WiBi provides over 3M hypernym pages
chosen from a range of 94k distinct hypernyms,
while others exhibit a considerably smaller range
of distinct hypernyms (e.g., DBpedia by design,
but also WikiNet, with around 11k distinct page
hypernyms). The large variety of classes provided
by MENTA, however, is due to including more
than 100k Wikipedia categories (among which,
categories about deaths and births alone repre-
sent about 2% of the distinct hypernyms). As re-
gards categories, while the number of distinct hy-
pernyms of WiBi and WikiTaxonomy is approxi-
mately the same (around 130k), the total number
of hypernyms (around 580k for both taxonomies)
is distributed over half of the categories in Wiki-
(a) Page taxonomies (b) Category taxonomies
Figure 5: Hypernym granularity for the resources.
Taxonomy compared to WiBi, resulting in a dou-
ble number of hypernyms per category, but lower
coverage (cf. Table 2).
8 Conclusions
In this paper we have presented WiBi, an auto-
matic 3-phase approach to the construction of a
bitaxonomy for the English Wikipedia, i.e., a full-
fledged, integrated page and category taxonomy:
first, using a set of high-precision linkers, the page
taxonomy is populated; next, a fixed point algo-
rithm populates the category taxonomy while en-
riching the page taxonomy iteratively; finally, the
category taxonomy undergoes structural refine-
ments. Coverage, quality and granularity of the
bitaxonomy are considerably higher than the tax-
onomy of state-of-the-art resources like DBpedia,
YAGO, MENTA, WikiNet and WikiTaxonomy.
Our contributions are three-fold: i) we propose
a unified, effective approach to the construction of
a Wikipedia bitaxonomy, a richer structure than
those produced in the literature; ii) our method for
building the bitaxonomy is self-contained, thanks
to its independence from external resources (like
WordNet) and the virtual absence of supervision,
making WiBi replicable on any new version of
Wikipedia; iii) the taxonomy provides nearly full
coverage of pages and categories, encompassing
the entire encyclopedic knowledge in Wikipedia.
We will apply our video games with a purpose
(Vannella et al, 2014) to validate WiBi. We also
plan to integrate WiBi into BabelNet (Navigli and
Ponzetto, 2012), so as to fully taxonomize it, and
exploit its high quality for improving semantic
predicates (Flati and Navigli, 2013).
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Luca Telesca for his implementation of
WikiTaxonomy and Jim McManus for his com-
ments on the manuscript.
953
References
Robert A. Amsler. 1981. A Taxonomy for English
Nouns and Verbs. In Proceedings of Association for
Computational Linguistics (ACL ?81), pages 133?
138, Stanford, California, USA.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ive.
2007. DBpedia: A nucleus for a web of open data.
In Proceedings of 6th International Semantic Web
Conference joint with 2nd Asian Semantic Web Con-
ference (ISWC+ASWC 2007), pages 722?735, Bu-
san, Korea.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154?165.
Sebastian Blohm. 2007. Using the web to reduce data
sparseness in pattern-based information extraction.
In Proceedings of the 11th European Conference on
Principles and Practice of Knowledge Discovery in
Databases (PKDD), pages 18?29, Warsaw, Poland.
Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the International
Conference on Management of Data (SIGMOD ?08),
SIGMOD ?08, pages 1247?1250, New York, NY,
USA.
Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-
polli. 1973. Working on the Italian Machine Dictio-
nary: a Semantic Approach. In Proceedings of the
5th Conference on Computational Linguistics (COL-
ING ?73), pages 49?70, Pisa, Italy.
Nicoletta Calzolari. 1982. Towards the organization of
lexical definitions on a database structure. In Proc.
of the 9th Conference on Computational Linguistics
(COLING ?82), pages 61?64, Prague, Czechoslo-
vakia.
Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proceedings of Conference on Information and
Knowledge Management (CIKM ?10), pages 1099?
1108, New York, NY, USA.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
ItAll: (preliminary results). In Proceedings of the
13th International Conference on World Wide Web
(WWW ?04), pages 100?110, New York, NY, USA.
ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
David A. Ferrucci. 2012. Introduction to ?This is Wat-
son?. IBM Journal of Research and Development,
56(3):1.
Tiziano Flati and Roberto Navigli. 2013. SPred:
Large-scale Harvesting of Semantic Predicates. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1222?1232, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the International Conference on Computational
Linguistics (COLING ?92), pages 539?545, Nantes,
France.
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from Wikipedia. Artificial Intelligence, 194:28?61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Nancy Ide and Jean V?eronis. 1993. Extracting
knowledge bases from machine-readable dictionar-
ies: Have we wasted our time? In Proceedings of
the Workshop on Knowledge Bases and Knowledge
Structures, pages 257?266, Tokyo, Japan.
Dan Klein and Christopher D. Manning. 2003. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), pages 3?10,
Vancouver, British Columbia, Canada.
Zornitsa Kozareva and Eduard H. Hovy. 2010. A
Semi-Supervised Method to Learn and Construct
Taxonomies Using the Web. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?10), pages 1110?1118,
Seattle, WA, USA.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 558?566,
Athens, Greece.
Tom Mitchell. 2005. Reading the Web: A Break-
through Goal for AI. AI Magazine.
Vivi Nastase and Michael Strube. 2013. Transform-
ing Wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62?85.
954
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Caecilia Zirn, and Anas Elghafari. 2010. WikiNet:
A Very Large Scale Multi-Lingual Concept Net-
work. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010), pages 1318?1327, Uppsala, Sweden,
July. Association for Computational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL HLT
2013), Boston, Massachusetts, 2?7 May 2004, pages
321?328.
Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from Wikipedia.
In Proceedings of the 22nd Conference on the Ad-
vancement of Artificial Intelligence (AAAI ?07), Van-
couver, B.C., Canada, 22?26 July 2007, pages
1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737?1756.
Hoifung Poon, Janara Christensen, Pedro Domingos,
Oren Etzioni, Raphael Hoffmann, Chloe Kiddon,
Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Ste-
fan Schoenmackers, Stephen Soderland, Dan Weld,
Fei Wu, and Congle Zhang. 2010. Machine Read-
ing at the University of Washington. In Proceedings
of the 1st International Workshop on Formalisms
and Methodology for Learning by Reading in con-
junction with NAACL-HLT 2010, pages 87?95, Los
Angeles, California, USA.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of Wikipedia
encyclopedic entries to WordNet synsets. In Ad-
vances in Web Intelligence, volume 3528 of Lec-
ture Notes in Computer Science, pages 380?386.
Springer Verlag.
Amit Singhal. 2012. Introducing the Knowledge
Graph: Things, Not Strings. Technical report, Of-
ficial Blog (of Google). Retrieved May 18, 2012.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 801?
808.
Fabian Suchanek and Gerhard Weikum. 2013. Knowl-
edge harvesting from text and Web sources. In IEEE
29th International Conference on Data Engineer-
ing (ICDE 2013), pages 1250?1253, Brisbane, Aus-
tralia. IEEE Computer Society.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
Wikipedia and WordNet. Journal of Web Semantics,
6(3):203?217.
Daniele Vannella, David Jurgens, Daniele Scarfini,
Domenico Toscani, and Roberto Navigli. 2014.
Validating and Extending Semantic Knowledge
Bases using Video Games with a Purpose. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2014),
Baltimore, USA.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3):665?707.
955
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1294?1304,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Validating and Extending Semantic Knowledge Bases
using Video Games with a Purpose
Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani and Roberto Navigli
Department of Computer Science
Sapienza University of Rome
surname@di.uniroma1.it
Abstract
Large-scale knowledge bases are impor-
tant assets in NLP. Frequently, such re-
sources are constructed through automatic
mergers of complementary resources, such
as WordNet and Wikipedia. However,
manually validating these resources is pro-
hibitively expensive, even when using
methods such as crowdsourcing. We pro-
pose a cost-effective method of validat-
ing and extending knowledge bases using
video games with a purpose. Two video
games were created to validate concept-
concept and concept-image relations. In
experiments comparing with crowdsourc-
ing, we show that video game-based vali-
dation consistently leads to higher-quality
annotations, even when players are not
compensated.
1 Introduction
Large-scale knowledge bases are an essential
component of many approaches in Natural Lan-
guage Processing (NLP). Semantic knowledge
bases such as WordNet (Fellbaum, 1998), YAGO
(Suchanek et al, 2007), and BabelNet (Navigli
and Ponzetto, 2010) provide ontological struc-
ture that enables a wide range of tasks, such as
measuring semantic relatedness (Budanitsky and
Hirst, 2006) and similarity (Pilehvar et al, 2013),
paraphrasing (Kauchak and Barzilay, 2006), and
word sense disambiguation (Navigli and Ponzetto,
2012; Moro et al, 2014). Furthermore, such
knowledge bases are essential for building unsu-
pervised algorithms when training data is sparse
or unavailable. However, constructing and updat-
ing semantic knowledge bases is often limited by
the significant time and human resources required.
Recent approaches have attempted to build or
extend these knowledge bases automatically. For
example, Snow et al (2006) and Navigli (2005)
extend WordNet using distributional or structural
features to identify novel semantic connections
between concepts. The recent advent of large
semi-structured resources has enabled the creation
of new semantic knowledge bases (Medelyan et
al., 2009; Hovy et al, 2013) through automati-
cally merging WordNet and Wikipedia (Suchanek
et al, 2007; Navigli and Ponzetto, 2010; Nie-
mann and Gurevych, 2011). While these auto-
matic approaches offer the scale needed for open-
domain applications, the automatic processes of-
ten introduce errors, which can prove detrimental
to downstream applications. To overcome issues
from fully-automatic construction methods, sev-
eral works have proposed validating or extending
knowledge bases using crowdsourcing (Biemann
and Nygaard, 2010; Eom et al, 2012; Sarasua et
al., 2012). However, these methods, too, are lim-
ited by the resources required for acquiring large
numbers of responses.
In this paper, we propose validating and extend-
ing semantic knowledge bases using video games
with a purpose. Here, the annotation tasks are
transformed into elements of a video game where
players accomplish their jobs by virtue of playing
the game, rather than by performing a more tradi-
tional annotation task. While prior efforts in NLP
have incorporated games for performing annota-
tion and validation (Siorpaes and Hepp, 2008b;
Herda?gdelen and Baroni, 2012; Poesio et al,
2013), these games have largely been text-based,
adding game-like features such as high-scores on
top of an existing annotation task. In contrast,
we introduce two video games with graphical 2D
gameplay that is similar to what game players are
familiar with. The fun nature of the games pro-
vides an intrinsic motivation for players to keep
playing, which can increase the quality of their
work and lower the cost per annotation.
Our work provides the following three contribu-
tions. First, we demonstrate effective video game-
based methods for both validating and extending
1294
semantic networks, using two games that operate
on complementary sources of information: seman-
tic relations and sense-image mappings. In con-
trast to previous work, the annotation quality is
determined in a fully automatic way. Second, we
demonstrate that converting games with a purpose
into more traditional video games creates an in-
creased player incentive such that players annotate
for free, thereby significantly lowering annotation
costs below that of crowdsourcing. Third, for both
games, we show that games produce better quality
annotations than crowdsourcing.
2 Related Work
Multiple works have proposed linguistic
annotation-based games with a purpose for
tasks such as anaphora resolution (Hladk?a et
al., 2009; Poesio et al, 2013), paraphrasing
(Chklovski and Gil, 2005), term associations
(Artignan et al, 2009; Lafourcade and Joubert,
2010), query expansion (Simko et al, 2011), and
word sense disambiguation (Chklovski and Mi-
halcea, 2002; Seemakurty et al, 2010; Venhuizen
et al, 2013). Notably, all of these linguistic games
focus on users interacting with text, in contrast
to other highly successful games with a purpose
in other domains, such as Foldit (Cooper et al,
2010), in which players fold protein sequences,
and the ESP game (von Ahn and Dabbish, 2004),
where players label images with words.
Most similar to our work are games that create
or validate common sense knowledge. Two games
with a purpose have incorporated video game-
like mechanics for annotation. First, Herda?gdelen
and Baroni (2012) validate automatically acquired
common sense relations using a slot machine
game where players must identify valid relations
and arguments from randomly aligned data within
a time limit. Although the validation is embedded
in a game-like setting, players are limited to one
action (pulling the lever) unlike our games, which
feature a variety of actions and rich gameplay ex-
perience to keep players interested longer. Sec-
ond, Kuo et al (2009) describe a pet-raising game
where players must answer common sense ques-
tions in order to obtain pet food. While their game
is among the most video game-like, the annotation
task is a chore the player must perform in order to
return to the game, rather than an integrated, fun
part of the game?s objectives, which potentially
decreases motivation for answering correctly.
Several works have proposed adapting existing
word-based board game designs to create or val-
idate common sense knowledge. von Ahn et al
(2006) generate common sense facts by using a
game similar to Taboo
TM
, where one player must
list facts about a computer-selected lemma and a
second player must guess the original lemma hav-
ing seen only the facts. Similarly, Vickrey et al
(2008) gather free associations to a target word
with the constraint, similar to Taboo
TM
, where
players cannot enter a small set of banned words.
Vickrey et al (2008) also present two games simi-
lar to the Scattergories
TM
, where players are given
a category and then must list things in that cate-
gory. The two variants differ in the constraints im-
posed on the players, such as beginning all items
with a specific letter. For all three games, two
players play the same game under time limits and
then are rewarded if their answers match.
Last, three two-player games have focused
on validating and extending knowledge bases.
Rzeniewicz and Szyma?nski (2013) extend Word-
Net with common-sense knowledge using a 20
Questions-like game. In a rapid-play style game,
OntoPronto attempts to classify Wikipedia pages
as either categories or individuals (Siorpaes and
Hepp, 2008a). SpotTheLink uses a similar rapid
question format to have players align the DBpedia
and PROTON ontologies by agreeing on the dis-
tinctions between classes (Thaler et al, 2011).
Unlike dynamic gaming elements common in
our video games, the above games are all focused
on interacting with textual items. Another major
limitation is their need for always having two play-
ers, which requires them to sustain enough inter-
est to always maintain an active pool of players.
While the computer can potentially act as a second
player, such a simulated player is often limited to
using preexisting knowledge or responses, which
makes it difficult to validate new types of entities
or create novel answers. In contrast, we drop this
requirement thanks to a new strategy for assign-
ing confidence scores to the annotations based on
negative associations.
3 Video Game with a Purpose Design
To create video games, our development process
focused on a common design philosophy and a
common data set.
3.1 Design Objectives
Three design objectives were used to develop the
video games. First, the annotation task should be
a central and natural action with familiar video
game mechanics. That is, the annotation should
1295
be supplied by common actions such as collecting
items, puzzles, or destroying objects, rather than
through extrinsic tasks that players must complete
in order to return to the game. This design has
the benefits of (1) growing the annotator pool with
video games players, and (2) potentially increas-
ing annotator enjoyment.
Second, the game should be playable by a single
player, with reinforcement for correct game play
coming from gold standard examples.
1
We note
that gold standard examples may come from both
true positive and true negative items.
Third, the game design should be sufficiently
general to annotate a variety of linguistic phenom-
ena, such that only the game data need be changed
to accomplish a different annotation task. While
some complex linguistic annotation tasks such as
preposition attachment may be difficult to inte-
grate directly into gameplay, many simpler but still
necessary annotation tasks such as word and im-
age associations can be easily modeled with tradi-
tional video game mechanics.
3.2 Annotation Setup
Tasks We focused on two annotation tasks: (1)
validating associations between two concepts, and
(2) validating associations between a concept and
an image. For each task we developed a video
game with a purpose that integrates the task within
the game, as illustrated in Sections 4 and 5.
Knowledge base As the reference knowledge
base, we chose BabelNet
2
(Navigli and Ponzetto,
2010), a large-scale multilingual semantic ontol-
ogy created by automatically merging WordNet
with other collaboratively-constructed resources
such as Wikipedia and OmegaWiki. BabelNet
data offers two necessary features for generat-
ing the games? datasets. First, by connecting
WordNet synsets to Wikipedia pages, most synsets
are associated with a set of pictures; while often
noisy, these pictures sometimes illustrate the tar-
get concept and are an ideal case for validation.
Second, BabelNet contains the semantic relations
from both WordNet and hyperlinks in Wikipedia;
these relations are again an ideal case of valida-
tion, as not all hyperlinks connect semantically-
related pages in Wikipedia. Last, we stress that
while our games use BabelNet data, they could
easily validate or extend other knowledge bases
such as YAGO (Suchanek et al, 2007) as well.
1
This design is in contrast to two-player games where mu-
tual agreement reinforces correct behavior.
2
http://babelnet.org
Data We created a common set of concepts, C,
used in both games, containing sixty synsets se-
lected from all BabelNet synsets with at least fifty
associated images. Using the same set of synsets,
separate datasets were created for the two valida-
tion tasks. In each dataset, a concept c ? C is
associated with two sets: a set V
c
containing items
to validate, and a setN
c
with examples of true neg-
ative items (i.e., items where the relation to c does
not hold). We use the notation V and N when re-
ferring to the to-validate and true negative sets for
all concepts in a dataset, respectively.
For the concept-concept dataset, V
c
is the union
of V
B
c
, which contains the lemmas of all synsets
incident to c in BabelNet, and V
n
c
, which con-
tains novel lemmas derived from statistical asso-
ciations. Specifically, novel lemmas were selected
by computing the ?
2
statistic for co-occurrences
between the lemmas of c and all other part of
speech-tagged lemmas in Wikipedia. The 30 lem-
mas with the highest ?
2
are included in V
c
. To
enable concept-to-concept annotations, we disam-
biguate novel lemmas using a simple heuristic
based on link co-occurrence count (Navigli and
Ponzetto, 2012). Each set V
c
contains 77.6 lem-
mas on average.
For the concept-image data, V
c
is the union of
V
B
c
, which contains all images associated with c in
BabelNet, and V
n
c
, which contains web-gathered
images using a lemma of c as the query. Web-
gathered images were retrieved using Yahoo! Boss
image search and the first result set (35 images)
was added to V
c
. Each set V
c
contains 77.0 images
on average.
For both datasets, each negative set N
c
is con-
structed as ?
c
?
?C\{c}
V
B
c
?
, i.e., from the items re-
lated in BabelNet to all other concepts in C. By
constructingN
c
directly from the knowledge base,
play actions may be validated based on recogni-
tion of true negatives, removing the heavy burden
for ever manually creating a gold standard test set.
Annotation Aggregation In each game, an item
is annotated when players make a binary choice as
to whether the item?s relation is true (e.g., whether
an image is related to a concept). To produce a
final annotation, a rating of p ? n is computed,
where p and n denote the number of times players
have marked the item?s relation as true or false, re-
spectively. Items with a positive rating after aggre-
gating are marked as true examples of the relation
and false otherwise.
1296
(a) The passphrase shown at the start (b) Main gameplay screen with a close-up of a player?s interaction with two humans
Figure 1: Screenshots of the key elements of Infection
4 Game 1: Infection
The first game, Infection, validates the concept-
concept relation dataset.
Design Infection is designed as a top-down
shooter game in the style of Commando. Infection
features the classic game premise that a virus has
partially infected humanity, turning people into
zombies. The player?s responsibility is to stop
zombies from reaching the city and rescue humans
that are fleeing to the city. Both zombies and hu-
mans appear at the top of the screen, advance to
the bottom and, upon reaching it, enter the city.
In the game, some humans are infected, but
have not yet become zombies; these infected hu-
mans must be stopped before reaching the city.
Because infected and uninfected humans look
identical, the player uses a passphrase call-and-
response mechanism to distinguish between the
two. Each level features a randomly-chosen
passphrase that the player?s character shouts. Un-
infected humans are expected to respond with a
word or phrase related to the passphrase; in con-
trast, infected humans have become confused due
to the infection and will say something completely
unrelated in an attempt to sneak past. When an in-
fected human reaches the city, the city?s total in-
fection level increases; should the infection level
increase beyond a certain threshold, the player
fails the stage and must replay it to advance the
game. Furthermore, if any time after ten humans
have been seen, the player has killed more than
80% of the uninfected humans, the player?s gun is
taken by the survivors and she loses the stage.
Figure 1a shows instructions for the passphrase
?medicine.? In the corresponding gameplay,
shown in the close up of Figure 1b, a hu-
man shouts a valid response, ?radiology? for the
level?s passphrase, while the nearby infected hu-
man shouts an incorrect response ?longitude.?
Gameplay is divided into eight stages, each with
increasing difficulty. Each stage has a goal of
saving a specific number of uninfected humans.
Infection incorporates common game mechanics,
such as unlockable weapons, power-ups that re-
store health, and achievements. Scoring is based
on both the number of zombies killed and the per-
centage of uninfected humans saved, motivating
players to kill infected humans in order to increase
their score. Importantly, Infection also includes a
leaderboard where players compete for top posi-
tions based on their total scores.
Annotation Each human is assigned a response
selected uniformly from V or N . Humans with
responses from N are treated as infected. Players
annotate by selecting which humans are infected:
Allowing a human with a response from V to enter
the city is treated as a positive annotation; killing
that human is treated as a negative annotation.
The design of Infection enables annotating mul-
tiple types of conceptual relations such as syn-
onymy or antonymy by changing only the descrip-
tion of the passphrase and how uninfected humans
are expected to respond.
Quality Enforcement Mechanisms Infection in-
cludes two game mechanics to limit adversarial
players from creating many low quality annota-
tions. Specifically, the game prevents players
from both (1) allowing all humans to live, via the
city infection level and (2) killing all humans, via
survivors taking the player?s gun; these actions
would both generate many false positives and false
negatives, respectively. These mechanics ensure
the game naturally produces better quality anno-
tations; in contrast, common crowdsourcing plat-
forms do not support analogous mechanics for en-
forcing this type of correctness at annotation time.
5 Game 2: The Knowledge Towers
The second game, The Knowledge Towers (TKT),
validates the concept-image dataset.
Design TKT is designed as a single-player role
playing game (RPG) where the player explores a
1297
(a) An example tower?s concept (b) Image selection screen (c) Gameplay
Figure 2: Screenshots of the key elements of The Knowledge Towers.
series of towers to unlock long-forgotten knowl-
edge. At the start of each tower, a target con-
cept is shown, e.g., the tower of ?tango,? along
with a description of the concept (Figure 2a). The
player must then recover the knowledge of the tar-
get concept by acquiring pictures of it. Pictures are
obtained through defeating monsters and opening
treasure chests, such as those shown in Figure 2c.
However, players must distinguish pictures of the
tower?s concept from unrelated pictures. When an
image is picked up, the player may keep or discard
it, as shown in Figure 2b. A player?s inventory is
limited to eight pictures to encourage them to se-
lect the most relevant pictures only.
Once the player has collected enough pictures,
the door to the boss room is unlocked and the
player may enter to defeat the boss and complete
the tower. Pictures may also be deposited in spe-
cial reward chests that grant experience bonuses if
the deposited pictures are from V . Gathering un-
related pictures has adverse effects on the player.
If the player finishes the level with a majority of
unrelated pictures, the player?s journey is unsuc-
cessful and she must replay the tower.
TKT includes RPG game elements commonly
found in game series such as Diablo and the Leg-
end of Zelda: players begin with a specific charac-
ter class that has class-specific skills, such as War-
rior or Thief, but will unlock the ability to play as
other classes by successfully completing the tow-
ers. Last, TKT includes a leaderboard where play-
ers can compete for positions; a player?s score is
based on increasing her character?s abilities and
her accuracy at discarding images from N .
Annotation Players annotate by deciding which
images to keep in their inventory. Images receive
positive rating annotations from: (1) depositing
the image in a reward chest, and (2) ending the
level with the image still in the inventory. Con-
versely, images receive a negative rating when a
player (1) views the image but intentionally avoids
picking it up or (2) drops the image from her in-
ventory.
TKT is designed to assist in the validation and
extension of automatically-created image libraries
that link to semantic concepts, such as ImageNet
(Deng et al, 2009) and that of Torralba et al
(2008). However, its general design allows for
other types of annotations, such as image labeling,
by changing the tower?s instructions and pictures.
Quality Enforcement Mechanisms Similar to
Infection, TKT includes analogous mechanisms
for limiting adversarial player annotations. Play-
ers who collect no images are prevented from en-
tering the boss room, limiting their ability to gen-
erate false negative annotations. Similarly, players
who collect all images are likely to have half of
their images from N and therefore fail the tower?s
quality-check after defeating the boss.
6 Experiments
Two experiments were performed with Infection
and TKT: (1) an evaluation of players? ability to
play accurately and to validate semantic relations
and image associations and (2) a comprehensive
cost comparison. Each experiment compared (a)
free and financially-incentivized versions of each
game, (b) crowdsourcing, and (c) a non-video
game with a purpose.
6.1 Experimental Setup
Gold Standard Data To compare the quality of
annotation from games and crowdsourcing, a gold
standard annotation was produced for a 10% sam-
ple of each dataset (cf. Section 3.2). Two annota-
tors independently rated the items and, in cases of
disagreement, a third expert annotator adjudicated.
Unlike in the game setting, annotators were free to
consult additional resources such as Wikipedia.
To measure inter-annotator agreement (IAA) on
the gold standard annotations, we calculated Krip-
1298
pendorff?s ? (Krippendorff, 2004; Artstein and
Poesio, 2008); ? ranges between [-1,1] where 1
indicates complete agreement, -1 indicates sys-
tematic disagreement, and values near 0 indicate
agreement at chance levels. Gold standard an-
notators had high agreement, 0.774, for concept-
concept relations. However, image-concept agree-
ment was only moderate, 0.549. A further analy-
sis revealed differences in the annotators? thresh-
olds for determining association, with one anno-
tator permitting more abstract relations. However,
the adjudication process resolved these disputes,
resulting in substantial agreement by all annota-
tors on the final gold annotations.
Incentives At the start of each game, players were
shown brief descriptions of the game and a de-
scription of a contest where the top-ranked players
would win either (1) monetary prizes in the form
of gift cards, or (2) a mention and thanks in this
paper. We refer to these as the paid and free ver-
sions of the game, respectively. In the paid setting,
the five top-ranking players were offered gift cards
valued at 25, 15, 15, 10, and 10 USD, starting from
first place (a total of 75 USD per game). To in-
crease competition among players and to perform
a fairer time comparison with crowdsourcing, the
contest period was limited to two weeks.
6.2 Comparison Methods
To compare with the video games, items were
annotated using two additional methods: crowd-
sourcing and a non-video game with a purpose.
Crowdsourcing Setup Crowdsourcing was per-
formed using the CrowdFlower platform. Anno-
tation tasks were designed to closely match each
game?s annotation process. A task begins with a
description of a target synset and its textual def-
inition; following, ten annotation questions are
shown. Separate tasks were used for validat-
ing concept-concept and concept-image relations.
Each tasks? questions were shown as a binary
choice of whether the item is related to the task?s
concept. Workers were paid 0.05 USD per task.
Each question was answered by three workers.
Following common practices for guarding
against adversarial workers (Mason and Suri,
2012), the tasks for concept c include quality
check questions using items from N
c
. Workers
who rate too many relations from N
c
as valid are
removed by CrowdFlower and prevented from par-
ticipating further. One of the ten questions in a
task used an item fromN
c
, resulting in a task mix-
ture of 90% annotation questions and 10% quality-
check questions. However, we note that both of
our video games use data that is 50% annotation,
50% quality-check. While the crowdsourcing task
could be adjusted to use an increased number of
quality-check options, such a design is uncommon
and artificially inflates the cost of the crowdsourc-
ing comparison beyond what would be expected.
Therefore, although the crowdsourcing and game-
based annotation tasks differ slightly, we chose to
use the common setup in order to create a fair cost-
comparison between the two.
Non-video Game with a Purpose To measure
the impact of the video game itself on the anno-
tation process, we developed a non-video game
with a purpose, referred to as SuchGame. Players
perform a single action in SuchGame: after be-
ing shown a concept c and its textual definition, a
player answers whether an item is related to the
concept. Items are drawn equally from V
c
and N
c
,
with players scoring a point each time they select
that an item from N is not related. A round of
gameplay contains ten questions. After the round
ends, players see their score for that round and the
current leaderboard. Two versions of SuchGame
were released, one for each dataset. SuchGame
was promoted with same free recognition incen-
tive as Infection and TKT.
6.3 Game Release
Both video games were released to multiple on-
line forums, social media sites, and Facebook
groups. SuchGame was released to separate Face-
book groups promoting free webgames and groups
for indie games. For each release, we estimated
an upper-bound of the audience sizes using avail-
able statistics such as Facebook group sites, web-
site analytics, and view counts. The free and paid
versions had sizes of 21,546 and 14,842 people,
respectively; SuchGame had an upper bound of
569,131 people. Notices promoting the game were
separated so that audiences saw promotions for
one of either the paid or free incentive version.
Games were also released in such a way as to pre-
serve the anonymity of the study, which limited
our ability to advertise to public venues where the
anonymity might be compromised.
7 Results and Discussion
7.1 Gameplay Analysis
In this section we analyze the games in terms of
participation and player?s ability to correctly play.
Players completed over 1388 games during the
1299
 0 50 100
 150 200 250
 300 350 400
 450
Numb
er of I
tems
Player
CorrectIncorrect
(a) Infection (free)
 0 50
 100 150
 200 250
 300 350
 400
Numb
er of I
tems
Player
CorrectIncorrect
(b) Infection (paid)
 0 100
 200 300
 400 500
 600 700
Numb
er of I
tems
Player
CorrectIncorrect
(c) TKT (free)
 0 200
 400 600
 800 1000
 1200 1400
 1600
Numb
er of I
tems
Player
CorrectIncorrect
(d) TKT (paid)
Figure 3: Accuracy of the top-40 players in rejecting true negative items during gameplay.
G.S. Agreement
# Players # Anno. N -Acc. Krip.?s ? True Pos. True Neg. All Cost per Ann.
TKT free 100 3005 97.0 0.333 82.5 82.5 82.5 $0.000
TKT paid 97 3318 95.4 0.304 69.0 92.1 74.0 $0.023
Crowdflower 290 13854 - 0.478 59.5 93.7 66.2 $0.008
Infection free 89 3150 71.0 0.445 67.8 68.4 68.1 $0.000
Infection paid 163 3355 65.9 0.330 69.1 54.8 61.1 $0.022
Crowdflower 1097 13764 - 0.167 16.9 96.4 59.6 $0.008
Table 1: Annotation statistics from all sources. N -Accuracy denotes accuracy at rejecting items fromN ;
G.S. Agreement denotes percentage agreement of the aggregated annotations with the gold standard.
study period. The paid and free versions of TKT
had similar numbers of players, while the paid ver-
sion of Infection attracted nearly twice the play-
ers compared to the free version, shown in Ta-
ble 1, Column 1. However, both versions created
approximately the same number of annotations,
shown in Column 2. Surprisingly, SuchGame re-
ceived little attention, with only a few players
completing a full round of game play. We believe
this emphasizes the strength of video game-based
annotation; adding incentives and game-like fea-
tures to an annotation task will not necessarily in-
crease its appeal. Given SuchGame?s minimal in-
terest, we omit it from further analysis.
Second, the type of incentive did not change the
percentage of items from N that players correctly
reject, shown for all players as N -accuracy in Ta-
ble 1 Column 3 and per-player in Figure 3. How-
ever, players were much more accurate at reject-
ing items from N in TKT than in Infection. We
attribute this difference to the nature of the items
and the format of the games. The images used
by TKT provide concrete examples of a concept,
which can be easily compared with the game?s cur-
rent concept; in addition, TKT allows players to
inspect items as long as a player prefers. In con-
trast, concept-concept associations require more
background knowledge to determine if a relation
exists; furthermore, Infection gives players limited
time to decide (due to board length) and also con-
tains cognitive distractors (zombies). Neverthe-
less, player accuracy remains high for both games
(Table 1, Col. 3) indicating the games represent a
viable medium for making annotation decisions.
Last, the distribution of player annotation fre-
quencies (Figure 3) suggests that the leaderboard
and incentives motivated players. Especially in the
paid condition, a clear group appears in the top
five positions, which were advertised as receiving
prizes. The close proximity of players in the paid
positions is a result of continued competition as
players jostled for higher-paying prizes.
7.2 Annotation Quality
This section assesses the annotation quality of
both games and of CrowdFlower in terms of (1)
the IAA of the participants, measured using Krip-
pendorff?s ?, and (2) the percentage agreement of
the resulting annotations with the gold standard.
Players in both free and paid games had similar
IAA, though the free version is consistently higher
(Table 1, Col. 4).
3
For images, crowdsourcing
workers have a higher IAA than game players;
however, this increased agreement is due to ad-
versarial workers consistently selecting the same,
incorrect answer. In contrast, both video games
contain mechanisms for limiting such behavior.
The strength of both crowdsourcing and games
with a purpose comes from aggregating multiple
annotations of a single item; i.e., while IAA may
3
In conversations with players after the contest ended,
several mentioned that being aware their play was contribut-
ing to research motivated them to play more accurately.
1300
Lemma Abbreviated Definition Most-selected Items
atom
The smallest possible
particle of a chemical
element
spectrum, nonparticulate radiation, molecule, hydrogen, electron
? ? ?
chord
A combination of three
or more notes
voicing, triad, tonality,? strum, note, harmony
?
color
An attribute from re-
flected or emitted light
orange, brown,? video, sadness, RGB, pigment
? ? ? ?
fire
The state of combustion
in which inflammable
material burns
sprinkler, machine gun, chemical reduction, volcano, organic chemistry
? ? ?
religion
The expression of
man?s belief in and
reverence for a super-
human power
polytheistic,? monotheistic, Jainism, Christianity,? Freedom of religion
? ? ?
Table 2: Examples of the most-selected words and images from the free version of both games. Bolded
words and images with a dashed border denote items not in BabelNet. Only the items marked with a ?
were rated as valid in the aggregated CrowdFlower annotations.
be low, the majority annotation of an item may be
correct. Therefore, in Table 1, we calculate the
percentage agreement of the aggregated annota-
tions with the gold standard annotations for ap-
proving valid relations (true positives; Col. 5), re-
jecting invalid relations (true negatives; Col. 6),
and for both combined (Col. 7). On average, both
video games in all settings produce more accurate
annotations than crowdsourcing. Indeed, despite
having lower IAA for images, the free version of
TKT provides an absolute 16.3% improvement in
gold standard agreement over crowdsourcing.
Examining the difference in annotation quality
for true positives and negatives, we see a strong
bias with crowdsourcing towards rejecting all
items. This bias leads to annotations with few false
positives, but as Column 5 shows, crowdflower
workers consistently performed much worse than
game players at identifying valid relations, pro-
ducing many false negative annotations. Indeed,
for concept-concept relations, workers identified
only 16.9% of the valid relations.
In contrast to crowdsourcing, both games were
effective at identifying valid relations. Table
2 shows examples of the most frequently cho-
sen items from V for the free versions of both
games. For both games, players were equally
likely to select novel items, suggesting the games
can serve a useful purpose of adding these miss-
ing relations in automatically constructed knowl-
edge bases. Highlighting one example, the five
most selected concept-concept relations for chord
were all novel; BabelNet included many relations
to highly-specific concepts (e.g., ?Circle of fifths?)
but did not include relations to more commonly-
associated concepts, like note and harmony.
7.3 Cost Analysis
This section provides a cost-comparison between
the video games and crowdsourcing. The free
versions of both games proved highly success-
ful, yielding high-quality annotations at no direct
cost. Both free and paid conditions produced sim-
ilar volumes of annotations, suggesting that play-
ers do not need financial incentives provided that
the games are fun to play. It could be argued that
the recognition incentive was motivating players
in the free condition and thus some incentive was
required. However, player behavior indicates oth-
erwise: After the contest period ended, no players
in the free setting registered for being acknowl-
edged by name, which strongly suggests the in-
centive was not contributing to their motivation for
playing. Furthermore, a minority of players con-
tinued to play even after the contest period ended,
suggesting that enjoyment was a driving factor.
1301
Last, while crowdsourcing has seen different qual-
ity and volume from workers in paid and unpaid
settings (Rogstadius et al, 2011), in contrast, our
games produced approximately-equivalent results
from players in both settings.
Crowdsourcing was slightly more cost-effective
than both games in the paid condition, as shown
in Table 1, Column 8. However, three additional
factors need to be considered. First, both games
intentionally uniformly sample between V and N
to increase player engagement,
4
which generates a
larger number of annotations for items in N than
are produced by crowdsourcing. When annota-
tions on items in N are included for both games
and crowdsourcing, the costs per annotation drop
to comparable levels: $0.007 for CrowdFlower
tasks, $0.008 for TKT, and $0.011 for Infection.
Second, for both annotation tasks, crowdsourc-
ing produced lower quality annotations, especially
for valid relations. Based on agreement with the
gold standard (Table 1, Col. 5), the estimated cost
for crowdsourcing a correct true positive annota-
tion increases to $0.014 for a concept-image and
a $0.048 for concepts-concept annotation. In con-
trast, the cost when using video games increases
only to $0.033 for concept-image and $0.031 for
concept-concept. These cost increases suggest
that crowdsourcing is not always cheaper with re-
spect to quality.
Third, we note that both video games in the paid
setting incur a fixed cost (for the prizes) and there-
fore additional games played can only further de-
crease the cost per annotation. Indeed, the present
study divided the audience pool into two separate
groups which effectively halved the potential num-
ber of annotations per game. Assuming combining
the audiences would produce the same number of
annotations, both our games? costs per annotation
drop to $0.012.
Last, video games can potentially come with
indirect costs due to software development and
maintenance. Indeed, Poesio et al (2013) report
spending 60,000? in developing their Phrase De-
tectives game with a purpose over a two-year pe-
riod. In contrast, both games here were developed
as a part of student projects using open source soft-
ware and assets and thus incurred no cost; fur-
thermore, games were created in a few months,
rather than years. Given that few online games
attain significant sustained interest, we argue that
4
Earlier versions that used mostly items from V proved
less engaging due to players frequently performing the same
action, e.g., saving most humans or collecting most pictures.
our lightweight model is preferable for producing
video games with a purpose. While using students
is not always possible, the development process
is fast enough to sufficiently reduce costs below
those reported for Phrase Detectives.
8 Conclusion
Two video games have been presented for vali-
dating and extending knowledge bases. The first
game, Infection, validates concept-concept rela-
tions, and the second, The Knowledge Towers,
validates image-concept relations. In experiments
involving online players, we demonstrate three
contributions. First, games were released in two
conditions whereby players either saw financial
incentives for playing or a personal satisfaction
incentive where they were thanked by us. We
demonstrated that both conditions produced nearly
identical numbers of annotations and, moreover,
that players were disinterested in the satisfaction
incentive, suggesting they played out of interest
in the game itself. Furthermore, we demonstrated
the effectiveness of a novel design for games with
a purpose which does not require two players for
validation and instead reinforces behavior only
using true negative items that required no man-
ual annotation. Second, in a comparison with
crowdsourcing, we demonstrate that video game-
based annotations consistently generated higher-
quality annotations. Last, we demonstrate that
video game-based annotation can be more cost-
effective than crowdsourcing or annotation tasks
with game-like features: The significant number
of annotations generated by the satisfaction incen-
tive condition shows that a fun game can generate
high-quality annotations at virtually no cost. All
annotated resources, demos of the games, and a
live version of the top-ranking items for each con-
cept are currently available online.
5
In the future we will apply our video games
to the validation of more data, such as the new
Wikipedia bitaxonomy (Flati et al, 2014).
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
We thank Francesco Cecconi for his support
with the websites and the many video game play-
ers without whose enjoyment this work would not
be possible.
5
http://lcl.uniroma1.it/games/
1302
References
Guillaume Artignan, Mountaz Hasco?et, and Math-
ieu Lafourcade. 2009. Multiscale visual analysis
of lexical networks. In Proceedings of the Inter-
national Conference on Information Visualisation,
pages 685?690.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing wordnet. In Proceedings of the 5th Global
WordNet conference.
Alexander Budanitsky and Graeme Hirst. 2006.
Evaluating WordNet-based measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Timothy Chklovski and Yolanda Gil. 2005. Improv-
ing the design of intelligent acquisition interfaces for
collecting world knowledge from web contributors.
In Proceedings of the International Conference on
Knowledge Capture, pages 35?42. ACM.
Tim Chklovski and Rada Mihalcea. 2002. Building a
Sense Tagged Corpus with Open Mind Word Expert.
In Proceedings of ACL 2002 Workshop on WSD: Re-
cent Successes and Future Directions, Philadelphia,
PA, USA.
Seth Cooper, Firas Khatib, Adrien Treuille, Janos
Barbero, Jeehyung Lee, Michael Beenen, Andrew
Leaver-Fay, David Baker, Zoran Popovi?c, and Foldit
players. 2010. Predicting protein structures with a
multiplayer online game. Nature, 466(7307):756?
760.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In Proceedings of the
Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 248?255.
Soojeong Eom, Markus Dickinson, and Graham Katz.
2012. Using semi-experts to derive judgments on
word sense alignment: a pilot study. In Proceed-
ings of the Conference on Language Resources and
Evaluation (LREC), pages 605?611.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and better)
than one: the Wikipedia Bitaxonomy Project. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Bal-
timore, Maryland.
Amac? Herda?gdelen and Marco Baroni. 2012. Boot-
strapping a game with a purpose for common sense
collection. ACM Transactions on Intelligent Sys-
tems and Technology, 3(4):1?24.
Barbora Hladk?a, Ji?r?? M??rovsk`y, and Pavel Schlesinger.
2009. Play the language: Play coreference. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics and Inter-
national Joint Conference of the Asian Federation
of Natural Language Processing (ACL-IJCNLP),
pages 209?212. Association for Computational Lin-
guistics.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Conference of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 455?462.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, second edition.
Yen-ling Kuo, Jong-Chuan Lee, Kai-yang Chiang, Rex
Wang, Edward Shen, Cheng-wei Chan, and Jane
Yung-jen Hsu. 2009. Community-based game de-
sign: experiments on social games for common-
sense data collection. In Proceedings of the ACM
SIGKDD Workshop on Human Computation, pages
15?22.
Mathieu Lafourcade and Alain Joubert. 2010. Com-
puting trees of named word usages from a crowd-
sourced lexical network. In Proceedings of the In-
ternational Multiconference on Computer Science
and Information Technology (IMCSIT), pages 439?
446, Wisla, Poland.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on amazons mechanical turk.
Behavior Research Methods, 44(1):1?23.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716?754.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: A Unified Approach. Transactions of
the Association for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual se-
mantic network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, pages 216?225.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Joining forces pays off: Multilingual Joint Word
Sense Disambiguation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1399?
1410, Jeju, Korea.
1303
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research
Symposium Conference, Clearwater Beach, Florida,
15?17 May 2005, pages 548?553.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people?s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the International Conference on
Computational Semantics (IWCS), pages 205?214.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1341?1351, Sofia, Bulgaria.
Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,
Livio Robaldo, and Luca Ducceschi. 2013. Phrase
detectives: Utilizing collective intelligence for
internet-scale language resource creation. ACM
Transactions on Interactive Intelligent Systems,
3(1):3:1?3:44, April.
Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur,
Boris Smus, Jim Laredo, and Maja Vukovic. 2011.
An assessment of intrinsic and extrinsic motivation
on task performance in crowdsourcing markets. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jacek Rzeniewicz and Julian Szyma?nski. 2013.
Bringing Common Sense to WordNet with a Word
Game. In Computational Collective Intelligence.
Technologies and Applications, volume 8083 of Lec-
ture Notes in Computer Science, pages 296?305.
Springer.
Cristina Sarasua, Elena Simperl, and Natalya F Noy.
2012. CrowdMap: Crowdsourcing ontology align-
ment with microtasks. In Proceedings of the Inter-
national Semantic Web Conference (ISWC), pages
525?541.
Nitin Seemakurty, Jonathan Chu, Luis Von Ahn, and
Anthony Tomasic. 2010. Word sense disambigua-
tion via human computation. In Proceedings of the
ACM SIGKDD Workshop on Human Computation,
pages 60?63. ACM.
Jakub Simko, Michal Tvarozek, and Maria Bielikova.
2011. Little search game: term network acquisition
via a human computation game. In Proceedings of
the ACM conference on Hypertext and Hypermedia,
pages 57?62.
Katharina Siorpaes and Martin Hepp. 2008a. Games
with a purpose for the semantic web. IEEE Intelli-
gent Systems, 23(3):50?60.
Katharina Siorpaes and Martin Hepp. 2008b. On-
togame: Weaving the semantic web by online
games. In Sean Bechhofer, Manfred Hauswirth, Jrg
Hoffmann, and Manolis Koubarakis, editors, The
Semantic Web: Research and Applications, volume
5021 of Lecture Notes in Computer Science, pages
751?766. Springer Berlin Heidelberg.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogeneous
evidence. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL), Sydney, Aus-
tralia, pages 801?808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. unifying WordNet and Wikipedia. In Proceed-
ings of the 16th World Wide Web Conference, Banff,
Canada, 8?12 May 2007, pages 697?706.
Stefan Thaler, Elena Paslaru Bontas Simperl, and
Katharina Siorpaes. 2011. SpotTheLink: A Game
for Ontology Alignment. In Proceedings of the
6th Conference on Professional Knowledge Man-
agement: From Knowledge to Action, pages 246?
253.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 30(11):1958?1970.
Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense la-
beling. In Proceedings of the International Confer-
ence on Computational Semantics (IWCS).
David Vickrey, Aaron Bronzan, William Choi, Aman
Kumar, Jason Turner-Maier, Arthur Wang, and
Daphne Koller. 2008. Online word games for se-
mantic data collection. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 533?542.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
Conference on Human Factors in Computing Sys-
tems (CHI), pages 319?326.
Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006.
Verbosity: a game for collecting common-sense
facts. In Proceedings of the Conference on Human
Factors in Computing Systems (CHI), pages 75?78.
1304
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 67?72,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
WoSIT: A Word Sense Induction Toolkit
for Search Result Clustering and Diversification
Daniele Vannella, Tiziano Flati and Roberto Navigli
Dipartimento di Informatica
Sapienza Universit`a di Roma
{vannella,flati,navigli}@di.uniroma1.it
Abstract
In this demonstration we present WoSIT,
an API for Word Sense Induction (WSI)
algorithms. The toolkit provides imple-
mentations of existing graph-based WSI
algorithms, but can also be extended with
new algorithms. The main mission of
WoSIT is to provide a framework for the
extrinsic evaluation of WSI algorithms,
also within end-user applications such as
Web search result clustering and diversifi-
cation.
1 Introduction
The Web is by far the world?s largest information
archive, whose content ? made up of billions of
Web pages ? is growing exponentially. Unfortu-
nately the retrieval of any given piece of infor-
mation is an arduous task which challenges even
prominent search engines such as those developed
by Google, Yahoo! and Microsoft. Even today,
such systems still find themselves up against the
lexical ambiguity issue, that is, the linguistic prop-
erty due to which a single word may convey dif-
ferent meanings.
It has been estimated that around 4% of Web
queries and 16% of the most frequent queries are
ambiguous (Sanderson, 2008). A major issue as-
sociated with the lexical ambiguity phenomenon
on the Web is the low number of query words sub-
mitted by Web users to search engines. A pos-
sible solution to this issue is the diversification of
search results obtained by maximizing the dissimi-
larity of the top-ranking Web pages returned to the
user (Agrawal et al., 2009; Ashwin Swaminathan
and Kirovski, 2009). Another solution consists of
clustering Web search results by way of clustering
engines such as Carrot
1
and Yippy
2
and presenting
them to the user grouped by topic.
1
http://search.carrot2.org
2
http://yippy.com
Diversification and Web clustering algorithms,
however, do not perform any semantic analysis of
search results, clustering them solely on the basis
of their lexical similarity. Recently, it has been
shown that the automatic acquisition of the mean-
ings of a word of interest, a task referred to as
Word Sense Induction, can be successfully inte-
grated into search result clustering and diversifica-
tion (Navigli and Crisafulli, 2010; Di Marco and
Navigli, 2013) so as to outperform non-semantic
state-of-the-art Web clustering systems.
In this demonstration we describe a new toolkit
for Word Sense Induction, called WoSIT, which
i) provides ready implementations of existing
WSI algorithms; ii) can be extended with addi-
tional WSI algorithms; iii) enables the integration
of WSI algorithms into search result clustering
and diversification, thereby providing an extrinsic
evaluation tool. As a result the toolkit enables the
objective comparison of WSI algorithms within an
end-user application in terms of the degree of di-
versification of the search results of a given am-
biguous query.
2 WoSIT
In Figure 1 we show the workflow of the WoSIT
toolkit, composed of three main phases: WSI;
semantically-enhanced search result clustering
and diversification; evaluation. Given a target
query q whose meanings we want to automati-
cally acquire, the toolkit first builds a graph for q,
obtained either from a co-occurrence database, or
constructed programmatically by using any user-
provided input. The co-occurrence graph is then
input to a WSI algorithm, chosen from among
those available in the toolkit or implemented by
the user. As a result, a set of word clusters
is produced. This concludes the first phase of
the WoSIT workflow. Then, the word clusters
produced are used for assigning meanings to the
search results returned by a search engine for the
query q, i.e. search result disambiguation. The
67
  
+ MANUAL ANNOTATIONS
DBDB
Search resultdisambiguation
w1w3w2w4w5w6 s4s5s1 s3
s2
Eval ResultsWord ClustersWSIAlgorithm WSI EvaluatorAssignment of results to clustersCo-occurrence graph
Co-occurrence Information Web search engine
WSI Semantically EnhancedSearch Result Clustering
Dataset
Evaluation
Figure 1: The WoSIT workflow.
outcome is that we obtain a clustering of search
results. Finally, during the third phase, we apply
the evaluation module which performs an evalua-
tion of the search result clustering quality and the
diversification performance.
We now describe in detail the three main phases
of WoSIT.
2.1 Word Sense Induction
The first phase of WoSIT consists of the automatic
identification of the senses of a query of inter-
est, i.e. the task of Word Sense Induction. Al-
though WoSIT enables the integration of custom
implementations which can potentially work with
any WSI paradigm, the toolkit provides ready-to-
use implementations of several graph-based algo-
rithms that work with word co-occurrences. All
these algorithms carry out WSI in two steps: co-
occurrence graph construction (Section 2.1.1) and
discovery of word senses (Section 2.1.2).
2.1.1 Co-occurrence graph construction
Given a target query q, we build a co-occurrence
graph G
q
= (V,E) such that V is the set of
words co-occurring with q and E is the set of undi-
rected edges, each denoting a co-occurrence be-
tween pairs of words in V . In Figure 2 we show
an example of a co-occurrence graph for the target
word excalibur.
WoSIT enables the creation of the co-
occurrence graph either programmatically, by
adding edges and vertices according to any user-
specific algorithm, or starting from the statis-
tics for co-occurring words obtained from a co-
occurrence database (created, e.g., from a text cor-
pus, as was done by Di Marco and Navigli (2013)).
In either case, weights for edges have to be pro-
vided in terms of the correlation strength between
pairs of words (e.g. using Dice, Jaccard or other
co-occurrence measures).
The information about the co-occurrence
database, e.g. a MySQL database, is provided
programmatically or via parameters in the prop-
erties configuration file (db.properties).
The co-occurrence database has to follow a
given schema provided in the toolkit docu-
mentation. An additional configuration file
(wosit.properties) also allows the user
to specify additional constraints, e.g. the
minimum weight value of co-occurrence (the
wordGraph.minWeight parameter) to be
added as edges to the graph.
The graphs produced can also be saved to binary
(i.e. serialized) or text file:
g.saveToSer(fileName);
g = WordGraph.loadFromSer(fileName);
g.saveToTxt(fileName);
g = WordGraph.loadFromTxt(fileName);
We are now ready to provide our co-occurrence
graph, created with just a few lines of code, as in-
put to a WSI algorithm, as will be explained in the
next section.
2.1.2 Discovery of Word Senses
Once the co-occurrence graph for the query q is
built, it can be input to any WSI algorithm which
extends the GraphClusteringAlgorithm
class in the toolkit. WoSIT comes with a number
of ready-to-use such algorithms, among which:
68
Car
Limousine
King Arthur
Excalibur
Film
Fantasy
Book
0.02
0.015
0.025
0.005
0.04
0.006
0.007
0.01
0.013
0.012
0.02
Figure 2: Example of a co-occurrence graph for
the word excalibur.
? Balanced Maximum Spanning Tree (B-
MST) (Di Marco and Navigli, 2013), an ex-
tension of a WSI algorithm based on the
calculation of a Maximum Spanning Tree
(Di Marco and Navigli, 2011) aimed at bal-
ancing the number of co-occurrences in each
sense cluster.
? HyperLex (V?eronis, 2004), an algorithm
which identifies hubs in co-occurrence
graphs, thereby identifying basic meanings
for the input query.
? Chinese Whispers (Biemann, 2006), a ran-
domized algorithm which partitions nodes by
means of the iterative transfer of word sense
information across the co-occurrence graph
(Biemann, 2006).
? Squares, Triangles and Diamonds
(SquaT++) (Di Marco and Navigli, 2013),
an extension of the SquaT algorithm (Navigli
and Crisafulli, 2010) which exploits three
cyclic graph patterns to determine and
discard those vertices (or edges) with weak
degree of connectivity in the graph.
We also provide an implementation of a word
clustering algorithm, i.e. Lin98 (Lin, 1998),
which does not rely on co-occurrence graphs, but
just on the word co-occurrence information to it-
eratively refine word clusters on the basis of their
?semantic? relationships.
A programmatic example of use of the B-MST
WSI algorithm is as follows:
BMST mst = new BMST(g);
mst.makeClustering();
Clustering wordClusters =
mst.getClustering();
where g is a co-occurrence graph created as ex-
plained in Section 2.1.1, provided as input to
the constructor of the algorithm?s class. The
makeClustering method implements the in-
duction algorithm and creates the word clus-
ters, which can then be retrieved calling the
getClustering method. As a result an in-
stance of the Clustering class is provided.
As mentioned above, WoSIT also enables
the creation of custom WSI implementa-
tions. This can be done by extending the
GraphClusteringAlgorihm abstract class.
The new algorithm just has to implement two
methods:
public void makeClustering();
public Clustering getClustering();
As a result, the new algorithm is readily inte-
grated into the WoSIT toolkit.
2.2 Semantically-enhanced Search Result
Clustering and Diversification
We now move to the use of the induced senses of
our target query q within an application, i.e. search
result clustering and diversification.
Search result clustering. The next step (cf. Fig-
ure 1) is the association of the search results re-
turned by a search engine for query q with the most
suitable word cluster (i.e. meaning of q). This can
be done in two lines:
SnippetAssociator associator =
SnippetAssociator.getInstance();
SnippetClustering clustering =
associator.associateSnippet(
targetWord,
searchResults,
wordClusters,
AssociationMetric.DEGREE_OVERLAP);
The first line obtains an instance of the class
which performs the association between search re-
sult snippets and the word clusters obtained from
the WSI algorithm. The second line calls the asso-
ciation method associateSnippet which in-
puts the target word, the search results obtained
from the search engine, the word clusters and, fi-
nally, the kind of metric to use for the associa-
tion. Three different association metrics are im-
plemented in the toolkit:
? WORD OVERLAP performs the association by
maximizing the size of the intersection be-
tween the word sets in each snippet and the
word clusters;
? DEGREE OVERLAP performs the association
by calculating for each word cluster the sum
69
of the vertex degrees in the co-occurrence
graph of the words occurring in each snippet;
? TOKEN OVERLAP is similar in spirit to
WORD OVERLAP, but takes into account each
token occurrence in the snippet bag of words.
Search result diversification. The above two
lines of code return a set of snippet clusters and, as
a result, semantically-enhanced search result clus-
tering is performed. At the end, the resulting clus-
tering can be used to provide a diversified rerank-
ing of the results:
List<Snippet> snippets =
clustering.diversify(sorter);
The diversify method returns a flat list of
snippet results obtained according to the Sorter
object provided in input. The Sorter abstract
class is designed to rerank the snippet clusters ac-
cording to some predefined rule. For instance, the
CardinalitySorter class, included in the
toolkit, sorts the clusters according to the size of
each cluster. Once a sorting order has been es-
tablished, an element from each snippet cluster is
added to an initially-empty list; next, a second el-
ement from each cluster is added, and so on, until
all snippets are added to the list.
The sorting rules implemented in the toolkit are:
? CardinalitySorter: sorts the clusters
according to their size, i.e. the number of ver-
tices in the cluster;
? MeanSimilaritySorter: sorts the clus-
ters according to the average association
score between the snippets in the cluster and
the backing word cluster (defined by the se-
lected association metrics).
Notably, the end user can then implement his or
her own custom sorting procedure by simply ex-
tending the Sorter class.
2.2.1 Search Result Datasets
The framework comes with two search result
datasets of ambiguous queries: the AMBI-
ENT+MORESQUE dataset made available by
Bernardini et al. (2009) and Navigli and Crisa-
fulli (2010), respectively, and the SemEval-2013-
Task11 dataset.
3
New result datasets can be pro-
vided by users complying with the dataset format
described below.
3
For details visit http://lcl.uniroma1.it/
wosit/.
A search result dataset in WoSIT is made up of
at least two files:
? topics.txt, which contains the queries
(topics) of interest together with their nu-
meric ids. For instance:
id description
1 polaroid
2 kangaroo
3 shakira
... ...
? results.txt, which lists the search re-
sults for each given query, in terms of URL,
page title and page snippet:
ID url title snippet
1.1 http://www.polaroid.com/ Polaroid | Home ...
1.2 http://www.polaroid.com/products products...
1.3 http://en.wikipedia.org/wiki/Polaroid_Cor...
... ...
Therefore, the two files provide the queries and the
corresponding search results returned by a search
engine. In order to enable an automatic evaluation
of the search result clustering and diversification
output, two additional files have to be provided:
? subTopics.txt, which for each query
provides the list of meanings for that query,
e.g.:
ID description
1.1 Polaroid Corporation, a multinational con...
1.2 Instant film photographs are sometimes kn...
1.3 Instant camera (or Land camera), sometime...
... ...
? STRel.txt, which provides the manual as-
sociations between each search result and the
most suitable meaning as provided in the
subTopics.txt file. For instance:
subTopicID resultID
1.1 1.1
1.1 1.2
1.1 1.3
... ...
2.3 WSI Evaluator
As shown in Figure 1 the final component of our
workflow is the evaluation of WSI when integrated
into search result clustering and diversification (al-
ready used by Navigli and Vannella (2013)). This
component, called the WSI Evaluator, takes as
input the snippet clusters obtained for a given
query together with the fully annotated search re-
sult dataset, as described in the previous section.
Two kinds of evaluations are carried out, described
in what follows.
70
1 Dataset searchResults = Dataset.getInstance();
2 DBConfiguration db = DBConfiguration.getInstance();
3 for(String targetWord : dataset.getQueries())
4 {
5 WordGraph g = WordGraph.createWordGraph(targetWord, searchResults, db);
6 BMST mst = new BMST(g);
7 mst.makeClustering();
8 SnippetAssociator snippetAssociator = SnippetAssociator.getInstance();
9 SnippetClustering snippetClustering = snippetAssociator.associateSnippet(
10 targetWord, searchResults, mst.getClustering(), AssociationMetric.WORD_OVERLAP);
11 snippetClustering.export("output/outputMST.txt", true);
12 }
13 WSIEvaluator.evaluate(searchResults, "output/outputMST.txt");
Figure 3: An example of evaluation code for the B-MST clustering algorithm.
2.3.1 Evaluation of the clustering quality
The quality of the output produced by
semantically-enhanced search result cluster-
ing is evaluated in terms of Rand Index (Rand,
1971, RI), Adjusted Rand Index (Hubert and
Arabie, 1985, ARI), Jaccard Index (JI) and,
finally, precision and recall as done by Crabtree et
al. (2005), together with their F1 harmonic mean.
2.3.2 Evaluation of the clustering diversity
To evaluate the snippet clustering diversity the
measures of S-recall@K and S-precision@r (Zhai
et al., 2003) are calculated. These measures de-
termine how many different meanings of a query
are covered in the top-ranking results shown to the
user. We calculate these measures on the output of
the three different association metrics illustrated in
Section 2.2.
3 A Full Example
We now show a full example of usage of the
WoSIT API. The code shown in Figure 3 initially
obtains a search result dataset (line 1), selects a
database (line 2) and iterates over its queries (line
3). Next, a co-occurrence graph for the current
query is created from a co-occurrence database
(line 5) and an instance of the B-MST WSI algo-
rithm is created with the graph as input (line 6).
After executing the algorithm (line 7), the snippets
for the given query are clustered (lines 8-10). The
resulting snippet clustering is appended to an out-
put file (line 11). Finally, the WSI evaluator is run
on the resulting snippet clustering using the given
dataset (line 13).
3.1 Experiments
We applied the WoSIT API to the AMBI-
ENT+MORESQUE dataset using 4 induction al-
Algorithm
Assoc. Web1T
metr. ARI JI F1 # cl.
SquaT++
WO 69.65 75.69 59.19 2.1
DO 69.21 75.45 59.19 2.1
TO 69.67 75.69 59.19 2.1
B-MST
WO 60.76 71.51 64.56 5.0
DO 66.48 69.37 64.84 5.0
TO 63.17 71.21 64.04 5.0
HyperLex
WO 60.86 72.05 65.41 13.0
DO 66.27 68.00 71.91 13.0
TO 62.82 70.87 65.08 13.0
Chinese Whispers
WO 67.75 75.37 60.25 12.5
DO 65.95 69.49 70.33 12.5
TO 67.57 74.69 60.50 12.5
Table 1: Results of WSI algorithms with a Web1T
co-occurrence database and the three association
metrics (Word Overlap, Degree Overlap and To-
ken Overlap). The reported measures are Ad-
justed Rand Index (ARI), Jaccard Index (JI) and
F1. We also show the average number of clusters
per query produced by each algorithm.
gorithms among those available in the toolkit,
where co-occurrences were obtained from the
Google Web1T corpus (Brants and Franz, 2006).
In Table 1 we show the clustering quality results
output by the WoSIT evaluator, whereas in Fig-
ure 4 we show the diversification performance in
terms of S-recall@K.
3.2 Conclusions
In this demonstration we presented WoSIT, a full-
fledged toolkit for Word Sense Induction algo-
rithms and their integration into search result clus-
tering and diversification. The main contributions
are as follows: first, we release a Java API for
performing Word Sense Induction which includes
several ready-to-use implementations of existing
algorithms; second, the API enables the use of the
acquired senses for a given query for enhancing
71
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
2.0 4.0 6.0 8.0 10.0 12.0 14.0 16.0 18.0 20.0
S-reca
ll-at-K
K
HyperLexBMSTChineseWSquaT++
Figure 4: S-recall@K performance.
search result clustering and diversification; third,
we provide an evaluation component which, given
an annotated dataset of search results, carries out
different kinds of evaluation of the snippet cluster-
ing quality and diversity.
WoSIT is the first available toolkit which pro-
vides an end-to-end approach to the integration of
WSI into a real-world application. The toolkit en-
ables an objective comparison of WSI algorithms
as well as an evaluation of the impact of apply-
ing WSI to clustering and diversifying search re-
sults. As shown by Di Marco and Navigli (2013),
this integration is beneficial and allows outperfor-
mance of non-semantic state-of-the-art Web clus-
tering systems.
The toolkit, licensed under a Creative Com-
mons Attribution-Non Commercial-Share Alike
3.0 License, is available at http://lcl.
uniroma1.it/wosit/.
References
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halver-
son, and Samuel Ieong. 2009. Diversifying search
results. In Proc. of the Second International Confer-
ence on Web Search and Web Data Mining (WSDM
2009), pages 5?14, Barcelona, Spain.
Cherian V. Mathew Ashwin Swaminathan and Darko
Kirovski. 2009. Essential Pages. In Proc. of the
2009 IEEE/WIC/ACM International Joint Confer-
ence on Web Intelligence and Intelligent Agent Tech-
nology, volume 1, pages 173?182.
Andrea Bernardini, Claudio Carpineto, and Massim-
iliano D?Amico. 2009. Full-Subtopic Retrieval
with Keyphrase-Based Search Results Clustering.
In Proc. of Web Intelligence 2009, volume 1, pages
206?213, Los Alamitos, CA, USA.
Chris Biemann. 2006. Chinese Whispers - an Effi-
cient Graph Clustering Algorithm and its Applica-
tion to Natural Language Processing Problems. In
Proc. of TextGraphs: the First Workshop on Graph
Based Methods for Natural Language Processing,
pages 73?80, New York City.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, ver. 1, LDC2006T13. In Linguistic Data Con-
sortium, Philadelphia, USA.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selec-
tion. In Proc. of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172?
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2011. Clus-
tering Web Search Results with Maximum Spanning
Trees. In Proc. of the XIIth International Confer-
ence of the Italian Association for Artificial Intelli-
gence (AI*IA), pages 201?212, Palermo, Italy.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and Diversifying Web Search Results with
Graph-Based Word Sense Induction. Computa-
tional Linguistics, 39(3):709?754.
Lawrence Hubert and Phipps Arabie. 1985. Compar-
ing Partitions. Journal of Classification, 2(1):193?
218.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proc. of the 17
th
Inter-
national Conference on Computational linguistics
(COLING), pages 768?774, Montreal, Canada.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing Word Senses to Improve Web Search Result
Clustering. In Proc. of the 2010 Conference on Em-
pirical Methods in Natural Language Processing,
pages 116?126, Boston, USA.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 Task 11: Evaluating Word Sense In-
duction & Disambiguation within An End-User Ap-
plication. In Proc. of the 7
th
International Work-
shop on Semantic Evaluation (SemEval 2013), in
conjunction with the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 193?201, Atlanta, USA.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical association, 66(336):846?850.
Mark Sanderson. 2008. Ambiguous queries: test col-
lections need more sense. In Proc. of the 31st an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 499?506, Singapore.
Jean V?eronis. 2004. HyperLex: lexical cartography
for information retrieval. Computer, Speech and
Language, 18(3):223?252.
ChengXiang Zhai, William W. Cohen, and John Laf-
ferty. 2003. Beyond independent relevance: Meth-
ods and evaluation metrics for subtopic retrieval. In
Proc. of the 26th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 10?17, Toronto, Canada.
72
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 193?201, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 11: Word Sense Induction & Disambiguation
within an End-User Application
Roberto Navigli and Daniele Vannella
Dipartimento di Informatica
Sapienza Universita` di Roma
Viale Regina Elena, 295 ? 00161 Roma Italy
{navigli,vannella}@di.uniroma1.it
Abstract
In this paper we describe our Semeval-2013
task on Word Sense Induction and Dis-
ambiguation within an end-user application,
namely Web search result clustering and diver-
sification. Given a target query, induction and
disambiguation systems are requested to clus-
ter and diversify the search results returned by
a search engine for that query. The task en-
ables the end-to-end evaluation and compari-
son of systems.
1 Introduction
Word ambiguity is a pervasive issue in Natural Lan-
guage Processing. Two main techniques in compu-
tational lexical semantics, i.e., Word Sense Disam-
biguation (WSD) and Word Sense Induction (WSI)
address this issue from different perspectives: the
former is aimed at assigning word senses from a pre-
defined sense inventory to words in context, whereas
the latter automatically identifies the meanings of a
word of interest by clustering the contexts in which
it occurs (see (Navigli, 2009; Navigli, 2012) for a
survey).
Unfortunately, the paradigms of both WSD and
WSI suffer from significant issues which hamper
their success in real-world applications. In fact, the
performance of WSD systems depends heavily on
which sense inventory is chosen. For instance, the
most popular computational lexicon of English, i.e.,
WordNet (Fellbaum, 1998), provides fine-grained
distinctions which make the disambiguation task
quite difficult even for humans (Edmonds and Kil-
garriff, 2002; Snyder and Palmer, 2004), although
disagreements can be solved to some extent with
graph-based methods (Navigli, 2008). On the other
hand, although WSI overcomes this issue by allow-
ing unrestrained sets of senses, its evaluation is par-
ticularly arduous because there is no easy way of
comparing and ranking different representations of
senses. In fact, all the proposed measures in the lit-
erature tend to favour specific cluster shapes (e.g.,
singletons or all-in-one clusters) of the senses pro-
duced as output. Indeed, WSI evaluation is actually
an instance of the more general and difficult problem
of evaluating clustering algorithms.
Nonetheless, many everyday tasks carried out by
online users would benefit from intelligent systems
able to address the lexical ambiguity issue effec-
tively. A case in point is Web information retrieval, a
task which is becoming increasingly difficult given
the continuously growing pool of Web text of the
most wildly disparate kinds. Recent work has ad-
dressed this issue by proposing a general evaluation
framework for injecting WSI into Web search result
clustering and diversification (Navigli and Crisa-
fulli, 2010; Di Marco and Navigli, 2013). In this
task the search results returned by a search engine
for an input query are grouped into clusters, and di-
versified by providing a reranking which maximizes
the meaning heterogeneity of the top ranking results.
The Semeval-2013 task described in this paper1
adopts the evaluation framework of Di Marco and
Navigli (2013), and extends it to both WSD and WSI
systems. The task is aimed at overcoming the well-
known limitations of in vitro evaluations, such as
those of previous SemEval tasks on the topic (Agirre
1http://www.cs.york.ac.uk/semeval-2013/task11/
193
and Soroa, 2007; Manandhar et al, 2010), and en-
abling a fair comparison between the two disam-
biguation paradigms. Key to our framework is the
assumption that search results grouped into a given
cluster are semantically related to each other and
that each cluster is expected to represent a specific
meaning of the input query (even though it is possi-
ble for more than one cluster to represent the same
meaning). For instance, consider the target query
apple and the following 3 search result snippets:
1. Apple Inc., formerly Apple Computer, Inc., is...
2. The science of apple growing is called pomology...
3. Apple designs and creates iPod and iTunes...
Participating systems were requested to produce a
clustering that groups snippets conveying the same
meaning of the input query apple, i.e., ideally {1, 3}
and {2} in the above example.
2 Task setup
For each ambiguous query the task required partic-
ipating systems to cluster the top ranking snippets
returned by a search engine (we used the Google
Search API). WSI systems were required to iden-
tify the meanings of the input query and cluster the
snippets into semantically-related groups according
to their meanings. Instead, WSD systems were re-
quested to sense-tag the given snippets with the ap-
propriate senses of the input query, thereby implic-
itly determining a clustering of snippets (i.e., one
cluster per sense).
2.1 Dataset
We created a dataset of 100 ambiguous queries.
The queries were randomly sampled from the AOL
search logs so as to ensure that they had been used in
real search sessions. Following previous work on the
topic (Bernardini et al, 2009; Di Marco and Navigli,
2013) we selected those queries for which a sense
inventory exists as a disambiguation page in the En-
glish Wikipedia2. This guaranteed that the selected
queries consisted of either a single word or a multi-
word expression for which we had a collaboratively-
edited list of meanings, including lexicographic and
encyclopedic ones. We discarded all queries made
2http://en.wikipedia.org/wiki/Disambiguation page
Figure 1: An example of search result for the apple query,
including: page title, URL and snippet.
query length 1 2 3 4
AOL logs 45.89 40.98 10.98 2.32
our dataset 40.00 40.00 15.00 5.00
Table 1: Percentage distribution of AOL query lengths
(first row) vs. the queries sampled for our task (second
row).
up of > 4 words, since the length of the great ma-
jority of queries lay in the range [1, 4]. In Table
1 we compare the percentage distribution of 1- to
4-word queries in the AOL query logs against our
dataset of queries. Note that we increased the per-
centage of 3- and 4-word queries in order to have
a significant coverage of those lengths. Anyhow,
in both cases most queries contained from 1 to 2
words. Note that the reported percentage distribu-
tions of query length is different from recent statis-
tics for two reasons: first, over the years users have
increased the average number of words per query in
order to refine their searches; second, we selected
only queries which were either single words (e.g.,
apple) or multi-word expressions (e.g., mortal kom-
bat), thereby discarding several long queries com-
posed of different words (such as angelina jolie ac-
tress).
Finally, we submitted each query to Google
search and retrieved the 64 top-ranking results re-
turned for each query. Therefore, overall the dataset
consists of 100 queries and 6,400 results. Each
search result includes the following information:
page title, URL of the page and snippet of the page
text. We show an example of search result for the
apple query in Figure 1.
2.2 Dataset Annotation
For each query q we used Amazon Mechani-
cal Turk3 to annotate each query result with the
3https://www.mturk.com
194
most suitable sense. The sense inventory for q
was obtained by listing the senses available in the
Wikipedia disambiguation page of q augmented
with additional options from the classes obtained
from the section headings of the disambiguation
page plus the OTHER catch-all meaning. For in-
stance, consider the apple query. We show its disam-
biguation page in Figure 2. The sense inventory for
apple was made up of the senses listed in that page
(e.g., MALUS, APPLE INC., APPLE BANK, etc.)
plus the set of generic classes OTHER PLANTS AND
PLANT PARTS, OTHER COMPANIES, OTHER FILMS,
plus OTHER.
For each query we ensured that three annotators
tagged each of the 64 results for that query with
the most suitable sense among those in the sense
inventory (selecting OTHER if no sense was appro-
priate). Specifically, each Turker was provided with
the following instructions: ?The goal is annotating
the search result snippets returned by Google for a
given query with the appropriate meaning among
those available (obtained from the Wikipedia disam-
biguation page for the query). You have to select
the meaning that you consider most appropriate?.
No constraint on the age, gender and citizenship of
the annotators was imposed. However, in order to
avoid random tagging of search results, we provided
3 gold-standard result annotations per query, which
could be shown to the Turker more than once during
the annotation process. In the case (s)he failed to
annotate the gold items, the annotator was automat-
ically excluded.
2.3 Inter-Annotator Agreement and
Adjudication
In order to determine the reliability of the Turkers?
annotations, we calculated the individual values of
Fleiss? kappa ? (Fleiss, 1971) for each query q and
then averaged them:
? =
?
q?Q ?q
|Q|
, (1)
where ?q is the Fleiss? kappa agreement of the three
annotators who tagged the 64 snippets returned by
the Google search engine for the query q ? Q, and
Q is our set of 100 queries. We obtained an average
value of ? = 0.66, which according to Landis and
Figure 2: The Wikipedia disambiguation page of Apple.
Koch (1977) can be seen as substantial agreement,
with a standard deviation ? = 0.185.
In Table 2 we show the agreement distribution
of our 6400 snippets, distinguishing between full
agreement (3 out of 3), majority agreement (2 out of
3), and no agreement. Most of the items were anno-
tated with full or majority agreement, indicating that
the manual annotation task was generally doable for
the layman. We manually checked all the cases of
majority agreement, correcting only 7.92% of the
majority adjudications, and manually adjudicated
all the snippets for which there was no agreement.
We observed during adjudication that in many cases
the disagreement was due to the existence of sub-
tle sense distinctions, like between MORTAL KOM-
BAT (VIDEO GAME) and MORTAL KOMBAT (2011
VIDEO GAME), or between THE DA VINCI CODE
and INACCURACIES IN THE DA VINCI CODE.
The average number of senses associated with
the search results of each query was 7.69
(higher than in previous datasets, such as AMBI-
ENT4+MORESQUE5, which associates 5.07 senses
4http://credo.fub.it/ambient
5http://lcl.uniroma1.it/moresque
195
Full agr. Majority Disagr.
% snippets 66.70 25.85 7.45
Table 2: Percentage of snippets with full agreement, ma-
jority agreement and full disagreement.
per query on average).
3 Scoring
Following Di Marco and Navigli (2013), we eval-
uated the systems? outputs in terms of the snippet
clustering quality (Section 3.1) and the snippet di-
versification quality (Section 3.2). Given a query
q ? Q and the corresponding set of 64 snippet re-
sults, let C be the clustering output by a given system
and let G be the gold-standard clustering for those
results. Each measure M(C,G) presented below is
calculated for the query q using these two cluster-
ings. The overall results on the entire set of queries
Q in the dataset is calculated by averaging the val-
ues of M(C,G) obtained for each single test query
q ? Q.
3.1 Clustering Quality
The first evaluation concerned the quality of the
clusters produced by the participating systems.
Since clustering evaluation is a difficult issue, we
calculated four distinct measures available in the lit-
erature, namely:
? Rand Index (Rand, 1971);
? Adjusted Rand Index (Hubert and Arabie,
1985);
? Jaccard Index (Jaccard, 1901);
? F1 measure (van Rijsbergen, 1979).
The Rand Index (RI) of a clustering C is a mea-
sure of clustering agreement which determines the
percentage of correctly bucketed snippet pairs across
the two clusterings C and G. RI is calculated as fol-
lows:
RI(C,G) =
TP + TN
TP + FP + FN + TN
, (2)
where TP is the number of true positives, i.e., snip-
pet pairs which are in the same cluster both in C and
HHHHHHG
C
C1 C2 ? ? ? Cm Sums
G1 n11 n12 ? ? ? n1m a1
G2 n21 n22 ? ? ? n2m a2
...
...
...
. . .
...
...
Gg ng1 ng2 ? ? ? ngm ag
Sums b1 b2 ? ? ? bm N
Table 3: Contingency table for the clusterings G and C.
G, TN is the number of true negatives, i.e., pairs
which are in different clusters in both clusterings,
and FP and FN are, respectively, the number of false
positives and false negatives. RI ranges between 0
and 1, where 1 indicates perfect correspondence.
Adjusted Rand Index (ARI) is a development of
Rand Index which corrects the RI for chance agree-
ment and makes it vary according to expectaction:
ARI(C,G) =
RI(C,G)? E(RI(C,G))
maxRI(C,G)? E(RI(C,G))
.
(3)
where E(RI(C,G)) is the expected value of the RI.
Using the contingency table reported in Table 3 we
can quantify the degree of overlap between C and G,
where nij denotes the number of snippets in com-
mon between Gi and Cj (namely, nij = |Gi ? Cj |),
ai and bj represent, respectively, the number of snip-
pets inGi and Cj , andN is the total number of snip-
pets, i.e., N = 64. Now, the above equation can be
reformulated as:
ARI(C,G)=
?
ij (nij2 )?[
?
i (ai2 )
?
j (bj2 )]/(
N
2 )
1
2 [
?
i (ai2 )+
?
j (bj2 )]?[
?
i (ai2 )
?
j (bj2 )]/(
N
2 )
.
(4)
The ARI ranges between ?1 and +1 and is 0
when the index equals its expected value.
Jaccard Index (JI) is a measure which takes into
account only the snippet pairs which are in the same
cluster both in C and G, i.e., the true positives (TP),
while neglecting true negatives (TN), which are the
vast majority of cases. JI is calculated as follows:
JI(C,G) =
TP
TP + FP + FN
. (5)
Finally, the F1 measure calculates the harmonic
mean of precision (P) and recall (R). Precision de-
termines how accurately the clusters of C represent
196
the query meanings in the gold standard G, whereas
recall measures how accurately the different mean-
ings in G are covered by the clusters in C. We follow
Crabtree et al (2005) and define the precision of a
cluster Cj ? C as follows:
P (Cj) =
|Csj |
|Cj |
, (6)
whereCsj is the intersection betweenCj ? C and the
gold cluster Gs ? G which maximizes the cardinal-
ity of the intersection. The recall of a query sense s
is instead calculated as:
R(s) =
|
?
Cj?Cs C
s
j |
ns
, (7)
where Cs is the subset of clusters of C whose ma-
jority sense is s, and ns is the number of snippets
tagged with query sense s in the gold standard. The
total precision and recall of the clustering C are then
calculated as:
P =
?
Cj?C P (Cj)|Cj |?
Cj?C |Cj |
; R =
?
s?S R(s)ns?
s?S ns
(8)
where S is the set of senses in the gold standard G
for the given query (i.e., |S| = |G|). The two values
of P and R are then combined into their harmonic
mean, namely the F1 measure:
F1(C,G) =
2PR
P +R
. (9)
3.2 Clustering Diversity
Our second evaluation is aimed at determining the
impact of the output clustering on the diversifica-
tion of the top results shown to a Web user. To
this end, we applied an automatic procedure for flat-
tening the clusterings produced by the participating
systems to a list of search results. Given a clus-
tering C = (C1, C2, . . . , Cm), we add to the ini-
tially empty list the first element of each cluster Cj
(j = 1, . . . ,m); then we iterate the process by se-
lecting the second element of each cluster Cj such
that |Cj | ? 2, and so on. The remaining elements re-
turned by the search engine, but not included in any
cluster of C, are appended to the bottom of the list
in their original order. Note that systems were asked
to sort snippets within clusters, as well as clusters
themselves, by relevance.
Since our goal is to determine how many differ-
ent meanings are covered by the top-ranking search
results according to the output clustering, we used
the measures of S-recall@K (Subtopic recall at rank
K) and S-precision@r (Subtopic precision at recall
r) (Zhai et al, 2003).
S-recall@K determines the ratio of different
meanings for a given query q in the top-K results
returned:
S-recall@K =
|{sense(ri) : i ? {1, . . . ,K}}|
g
,
(10)
where sense(ri) is the gold-standard sense associ-
ated with the i-th snippet returned by the system,
and g is the total number of distinct senses for the
query q in our gold standard.
S-precision@r instead determines the ratio of dif-
ferent senses retrieved for query q in the first Kr
snippets, where Kr is the minimum number of top
results for which the system achieves recall r. The
measure is defined as follows:
S-precision@r =
| ?Kri=1 sense(ri)|
Kr
. (11)
3.3 Baselines
We compared the participating systems with two
simple baselines:
? SINGLETONS: each snippet is clustered as a
separate singleton cluster (i.e., |C| = 64).
? ALL-IN-ONE: all snippets are clustered into a
single cluster (i.e., |C| = 1).
These baselines are important in that they make
explicit the preference of certain quality measures
towards clusterings made up with a small or large
number of clusters.
4 Systems
5 teams submitted 10 systems, out of which 9 were
WSI systems, while 1 was a WSD system, i.e., us-
ing the Wikipedia sense inventory for performing
the disambiguation task. All systems could exploit
the information provided for each search result, i.e.,
URL, page title and result snippet. WSI systems
were requested to use unannotated corpora only.
197
System URLs Snippets Wikipedia YAGO Hierarchy Distr. Thesaurus Other
W
S
I
HDP-CLUSTERS-LEMMA X X
HDP-CLUSTERS-NOLEMMA X X
DULUTH.SYS1.PK2 X
DULUTH.SYS7.PK2 X
DULUTH.SYS9.PK2 Gigaword
UKP-WSI-WP-LLR2 X X X WaCky
UKP-WSI-WP-PMI X X X WaCky
UKP-WSI-WACKY-LLR X X X WaCky
SATTY-APPROACH1 X
WSD RAKESH X DBPedia
Table 4: Resources used for WSI/WSD.
We asked each team to provide information about
their systems. In Table 4 we report the resources
used by each system. The HDP and UKP systems
use Wikipedia as raw text for sampling word counts;
DULUTH-SYS9-PK2 uses the first 10,000 paragraphs
of the Associated Press wire service data from the
English Gigaword Corpus (Graff, 2003, 1st edition),
whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-
PK2 both use the snippets for inducing the query
senses. Finally, the UKP systems were the only ones
to retrieve the Web pages from the corresponding
URLs and exploit them for WSI purposes. They
also use WaCky (Baroni et al, 2009) and a distri-
butional thesaurus obtained from the Leipzig Cor-
pora Collection6 (Biemann et al, 2007). SATTY-
APPROACH1 just uses snippets.
The only participating WSD system, RAKESH,
uses the YAGO hierarchy (Suchanek et al, 2008) to-
gether with DBPedia abstracts (Bizer et al, 2009).
5 Results
We show the results of RI and ARI in Table 5. The
best performing systems are those from the HDP
team, with considerably higher RI and ARI. The
next best systems are SATTY-APPROACH1, which
uses only the words in the snippets, and the only
WSD system, i.e., RAKESH. SINGLETONS perform
well with RI, but badly when chance agreement is
taken into account.
As for F1 and JI, whose values are shown in Table
6, the two HDP systems again perform best in terms
of F1, and are on par with UKP-WSI-WACKY-LLR in
terms of JI. The third best approach in terms of F1
is again SATTY-APPROACH1, which however per-
6http://corpora.uni-leipzig.de/
System RI ARI
W
SI
HDP-CLUSTERS-LEMMA 65.22 21.31
HDP-CLUSTERS-NOLEMMA 64.86 21.49
SATTY-APPROACH1 59.55 7.19
DULUTH.SYS9.PK2 54.63 2.59
DULUTH.SYS1.PK2 52.18 5.74
DULUTH.SYS7.PK2 52.04 6.78
UKP-WSI-WP-LLR2 51.09 3.77
UKP-WSI-WP-PMI 50.50 3.64
UKP-WSI-WACKY-LLR 50.02 2.53
WSD RAKESH 58.76 8.11
B
L SINGLETONS 60.09 0.00
ALL-IN-ONE 39.90 0.00
Table 5: Results for Rand Index (RI) and Adjusted Rand
Index (ARI), sorted by RI.
forms badly in terms of JI. The SINGLETONS base-
line clearly obtains the best F1 performance, but the
worst JI results. The ALL-IN-ONE baseline outper-
forms all other systems with the JI measure, because
TN are not considered, which favours large clusters.
To get more insights into the performance of the
various systems, we calculated the average number
of clusters per clustering produced by each system
and compared it with the gold standard average. We
also computed the average cluster size, i.e., the aver-
age number of snippets per cluster. The statistics are
shown in Table 7. Interestingly, the best performing
systems are those with the cluster number and aver-
age number of clusters closest to the gold standard
ones. This finding is also confirmed by Figure 3,
where we draw each system according to its average
values regarding cluster number and size: again the
distance from the gold standard is meaningful.
We now move to the diversification perfor-
198
System JI F1
W
SI
UKP-WSI-WACKY-LLR 33.94 58.26
HDP-CLUSTERS-NOLEMMA 33.75 68.03
HDP-CLUSTERS-LEMMA 33.02 68.30
DULUTH.SYS1.PK2 31.79 56.83
UKP-WSI-WP-LLR2 31.77 58.64
DULUTH.SYS7.PK2 31.03 58.78
UKP-WSI-WP-PMI 29.32 60.48
DULUTH.SYS9.PK2 22.24 57.02
SATTY-APPROACH1 15.05 67.09
WSD RAKESH 30.52 39.49
B
L SINGLETONS 0.00 100.00
ALL-IN-ONE 39.90 54.42
Table 6: Results for Jaccard Index (JI) and F1 measure.
System # cl. ACS
GOLD STANDARD 7.69 11.56
W
SI
HDP-CLUSTERS-LEMMA 6.63 11.07
HDP-CLUSTERS-NOLEMMA 6.54 11.68
SATTY-APPROACH1 9.90 6.46
UKP-WSI-WP-PMI 5.86 30.30
DULUTH.SYS7.PK2 3.01 25.15
UKP-WSI-WP-LLR2 4.17 21.87
UKP-WSI-WACKY-LLR 3.64 32.34
DULUTH.SYS9.PK2 3.32 19.84
DULUTH.SYS1.PK2 2.53 26.45
WSD RAKESH 9.07 2.94
Table 7: Average number of clusters (# cl.) and average
cluster size (ACS).
5
10
15
20
25
30
35
40
2 4 6 8 10 12
aver
age n
umbe
r of cl
usters
average cluster size (ACS)
gold-standardhdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2rakeshsatty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmi
Figure 3: Average cluster size (ACS) vs. average number
of clusters.
mance, calculated in terms of S-recall@K and S-
precision@r, whose results are shown in Tables 8
System K5 10 20 40
W
SI
HDP-CL.-NOLEMMA 50.80 63.21 79.26 92.48
HDP-CL.-LEMMA 48.13 65.51 78.86 91.68
UKP-WACKY-LLR 41.19 55.41 68.61 83.90
UKP-WP-LLR2 41.07 53.76 68.87 85.87
UKP-WP-PMI 40.45 56.25 68.70 84.92
SATTY-APPROACH1 38.97 48.90 62.72 82.14
DULUTH.SYS7.PK2 38.88 53.79 70.38 86.23
DULUTH.SYS9.PK2 37.15 49.90 68.91 83.65
DULUTH.SYS1.PK2 37.11 53.29 71.24 88.48
WSD RAKESH 46.48 62.36 78.66 90.72
Table 8: S-recall@K.
System r50 60 70 80
W
SI
HDP-CL.-LEMMA 48.85 42.93 35.19 27.62
HDP-CL.-NOLEMMA 48.18 43.88 34.85 29.30
UKP-WP-PMI 42.83 33.40 26.63 22.92
UKP-WACKY-LLR 42.47 31.73 25.39 22.71
UKP-WP-LLR2 42.06 32.04 26.57 22.41
DULUTH.SYS1.PK2 40.08 31.31 26.73 24.51
DULUTH.SYS7.PK2 39.11 30.42 26.54 23.43
DULUTH.SYS9.PK2 35.90 29.72 25.26 21.26
SATTY-APPROACH1 34.94 26.88 23.55 20.40
WSD RAKESH 48.00 39.04 32.72 27.92
Table 9: S-precision@r.
and 9, respectively. Here we find that, again, the
HDP team obtains the best performance, followed by
RAKESH. We note however that not all systems op-
timized the order of clusters and cluster snippets by
relevance.
We also graph the diversification performance
trend of S-recall@K and S-precision@r in Fig-
ures 4 and 5 for K = 1, . . . , 25 and r ?
{40, 50, . . . , 100}.
6 Conclusions and Future Directions
One of the aims of the SemEval-2013 task on Word
Sense Induction & Disambiguation within an End
User Application was to enable an objective compar-
ison of WSI and WSD systems when integrated into
Web search result clustering and diversification. The
task is a hard one, in that it involves clustering, but
provides clear-cut evidence that our end-to-end ap-
plication framework overcomes the limits of previ-
ous in-vitro evaluations. Indeed, the systems which
create good clusters and better diversify search re-
sults, i.e., those from the HDP team, achieve good
performance across all the proposed measures, with
no contradictory evidence.
199
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
5 10 15 20 25
S-re
call-
at-K
K
hdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2satty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmirakesh
Figure 4: S-recall@K.
0.0
0.1
0.2
0.3
0.4
0.5
0.6
40 50 60 70 80 90 100
S-pr
ecis
ion-a
t-r
r
hdp-lemmahdp-nolemmasys1.pk2sys7.pk2sys9.pk2satty-approach1ukp-wsi-wacky-llrukp-wsi-wp-llr2ukp-wsi-wp-pmirakesh
Figure 5: S-precision@r.
Our annotation experience showed that the
Wikipedia sense inventory, augmented with our
generic classes, is a good choice for semantically
tagging search results, in that it covers most of the
meanings a Web user might be interested in. In fact,
only 20% of the snippets was annotated with the
OTHER class.
Future work might consider large-scale multilin-
gual lexical resources, such as BabelNet (Navigli
and Ponzetto, 2012), both as sense inventory and for
performing the search result clustering and diversi-
fication task.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Antonio Di Marco and David A. Jur-
gens for their help.
200
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
pages 7?12, Prague, Czech Republic.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Andrea Bernardini, Claudio Carpineto, and Massimil-
iano D?Amico. 2009. Full-subtopic retrieval with
keyphrase-based search results clustering. In Proceed-
ings of Web Intelligence 2009, volume 1, pages 206?
213, Los Alamitos, CA, USA.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig corpora collec-
tion - monolingual corpora of standard size. In Pro-
ceedings of Corpus Linguistic 2007, Birmingham, UK.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009. Dbpedia - a crystallization point
for the web of data. J. Web Sem., 7(3):154?165.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selection.
In Proceedings of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172?
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and diversifying web search results with graph-
based word sense induction. Computational Linguis-
tics, 39(4).
Philip Edmonds and Adam Kilgarriff. 2002. Introduc-
tion to the special issue on evaluating word sense dis-
ambiguation systems. Journal of Natural Language
Engineering, 8(4):279?291.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. In Psychological Bulletin,
volume 76, page 378?382.
David Graff. 2003. English Gigaword. In Technical
Report, LDC2003T05, Linguistic Data Consortium,
Philadelphia, PA, USA.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
Partitions. Journal of Classification, 2(1):193?218.
Paul Jaccard. 1901. E?tude comparative de la distribution
florale dans une portion des alpes et des jura. In Bul-
letin de la Socie?te? Vaudoise des Sciences Naturelles,
volume 37, page 547?579.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159?174.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68, Uppsala, Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116?
126, Boston, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli. 2008. A structural approach to the
automatic adjudication of word sense disagreements.
Journal of Natural Language Engineering, 14(4):293?
310.
Roberto Navigli. 2009. Word Sense Disambiguation: a
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of word sense
disambiguation, induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115?129.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical association, 66(336):846?850.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the 3rd Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text (Senseval-3), pages 41?
43, Barcelona, Spain.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
wikipedia and wordnet. Journal of Web Semantics,
6(3):203?217.
Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Butterworths, second edition.
ChengXiang Zhai, William W. Cohen, and John Lafferty.
2003. Beyond independent relevance: Methods and
evaluation metrics for subtopic retrieval. In Proceed-
ings of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, pages 10?17, Toronto, Canada.
201
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 222?231, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 12: Multilingual Word Sense Disambiguation
Roberto Navigli, David Jurgens and Daniele Vannella
Dipartimento di Informatica
Sapienza Universita` di Roma
Viale Regina Elena, 295 ? 00161 Roma Italy
{navigli,jurgens,vannella}@di.uniroma1.it
Abstract
This paper presents the SemEval-2013 task on
multilingual Word Sense Disambiguation. We
describe our experience in producing a mul-
tilingual sense-annotated corpus for the task.
The corpus is tagged with BabelNet 1.1.1,
a freely-available multilingual encyclopedic
dictionary and, as a byproduct, WordNet 3.0
and the Wikipedia sense inventory. We present
and analyze the results of participating sys-
tems, and discuss future directions.
1 Introduction
Word Sense Disambiguation (WSD), the task of au-
tomatically assigning predefined meanings to words
occurring in context, is a fundamental task in com-
putational lexical semantics (Navigli, 2009; Navigli,
2012). Several Senseval and SemEval tasks have
been organized in the past to study the performance
and limits of disambiguation systems and, even
more importantly, disambiguation settings. While
an ad-hoc sense inventory was originally chosen for
the first Senseval edition (Kilgarriff, 1998; Kilgarriff
and Palmer, 2000), later tasks (Edmonds and Cot-
ton, 2001; Snyder and Palmer, 2004; Mihalcea et
al., 2004) focused on WordNet (Miller et al, 1990;
Fellbaum, 1998) as a sense inventory. In 2007 the
issue of the fine sense granularity of WordNet was
addressed in two different SemEval disambiguation
tasks, leading to the beneficial creation of coarser-
grained sense inventories from WordNet itself (Nav-
igli et al, 2007) and from OntoNotes (Pradhan et al,
2007).
In recent years, with the exponential growth of
the Web and, consequently, the increase of non-
English speaking surfers, we have witnessed an up-
surge of interest in multilinguality. SemEval-2010
tasks on cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2010) and cross-lingual lexi-
cal substitution (Mihalcea et al, 2010) were orga-
nized. While these tasks addressed the multilin-
gual aspect of sense-level text understanding, they
departed from the traditional WSD paradigm, i.e.,
the automatic assignment of senses from an existing
inventory, and instead focused on lexical substitu-
tion (McCarthy and Navigli, 2009). The main factor
hampering traditional WSD from going multilingual
was the lack of a freely-available large-scale multi-
lingual dictionary.
The recent availability of huge collaboratively-
built repositories of knowledge such as Wikipedia
has enabled the automated creation of large-scale
lexical knowledge resources (Hovy et al, 2013).
Over the past few years, a wide-coverage multi-
lingual ?encyclopedic? dictionary, called BabelNet,
has been developed (Navigli and Ponzetto, 2012a).
BabelNet1 brings together WordNet and Wikipedia
and provides a multilingual sense inventory that cur-
rently covers 6 languages. We therefore decided to
put the BabelNet 1.1.1 sense inventory to the test
and organize a traditional Word Sense Disambigua-
tion task on a given English test set translated into 4
other languages (namely, French, German, Spanish
and Italian). Not only does BabelNet enable mul-
tilinguality, but it also provides coverage for both
lexicographic (e.g., apple as fruit) and encyclopedic
1http://babelnet.org
222
meanings (e.g., Apple Inc. as company). In this pa-
per we describe our task and disambiguation dataset
and report on the system results.
2 Task Setup
The task required participating systems to annotate
nouns in a test corpus with the most appropriate
sense from the BabelNet sense inventory or, alter-
natively, from two main subsets of it, namely the
WordNet or Wikipedia sense inventories. In contrast
to previous all-words WSD tasks we did not focus
on the other three open classes (i.e., verbs, adjec-
tives and adverbs) since BabelNet does not currently
provide non-English coverage for them.
2.1 Test Corpus
The test set consisted of 13 articles obtained from
the datasets available from the 2010, 2011 and 2012
editions of the workshop on Statistical Machine
Translation (WSMT).2 The articles cover different
domains, ranging from sports to financial news.
The same article was available in 4 different lan-
guages (English, French, German and Spanish). In
order to cover Italian, an Italian native speaker man-
ually translated each article from English into Ital-
ian, with the support of an English mother tongue
advisor. In Table 1 we show for each language the
number of words of running text, together with the
number of multiword expressions and named enti-
ties annotated, from the 13 articles.
2.2 Sense Inventories
2.2.1 BabelNet inventory
To semantically annotate all the single- and multi-
word expressions, as well as the named entities, oc-
curring in our test corpus we used BabelNet 1.1.1
(Navigli and Ponzetto, 2012a). BabelNet is a mul-
tilingual ?encyclopedic dictionary? and a semantic
network currently covering 6 languages, namely:
English, Catalan, French, German, Italian and Span-
ish. BabelNet is obtained as a result of a novel inte-
gration and enrichment methodology. This resource
is created by linking the largest multilingual Web en-
cyclopedia ? i.e., Wikipedia ? to the most popular
computational lexicon ? i.e., WordNet 3.0. The inte-
gration is performed via an automatic mapping and
2http://www.statmt.org/wmt12/
by filling in lexical gaps in resource-poor languages
with the aid of Machine Translation (MT).
Its lexicon includes lemmas which denote both
lexicographic meanings (e.g., balloon) and ency-
clopedic ones (e.g., Montgolfier brothers). The
basic meaning unit in BabelNet is the Babel
synset, modeled after the WordNet synset (Miller
et al, 1990; Fellbaum, 1998). A Babel synset
is a set of synonyms which express a concept
in different languages. For instance, { Globus
aerosta`ticCA, BalloonEN, Ae?rostationFR, BallonDE,
Pallone aerostaticoIT, . . . , Globo aerosta?ticoES } is
the Babel synset for the balloon aerostat, where the
language of each synonym is provided as a subscript
label. Thanks to their multilingual nature, we were
able to use Babel synsets as interlingual concept tags
for nouns occurring within text written in any of the
covered languages.
2.2.2 WordNet and Wikipedia inventories
Since BabelNet 1.1.1 is a superset of the Word-
Net 3.0 and Wikipedia sense inventories,3 once text
is annotated with Babel synsets, it turns out to
be annotated also according to either WordNet or
Wikipedia, or both. In fact, in order to induce the
WordNet annotations, one can restrict to those lex-
ical items annotated with Babel synsets which con-
tain WordNet senses for the target lemma; similarly,
for Wikipedia, we restrict to those items tagged with
Babel synsets which contain Wikipedia pages for the
target lemma.
2.3 BabelNet sense inventory validation
Because BabelNet is an automatic integration of
WordNet and Wikipedia, the resulting Babel synsets
may contain WordNet and Wikipedia entries about
different meanings of the same lemma. The under-
lying cause is a wrong mapping between the two
original resources. For instance, in BabelNet 1.1
the WordNet synset { arsenic, As, atomic number
33 } was mapped to the Wikipedia page AS (RO-
MAN COIN), and therefore the same Babel synset
mixed the two meanings.
In order to avoid an inconsistent semantic tag-
ging of text, we decided to manually check all the
mappings in BabelNet 1.1 between Wikipedia pages
3For version 1.1.1 we used the English Wikipedia database
dump from October 1, 2012.
223
Language Instances Single- Multiword Named Mean senses Mean senses
words expressions Entities per instance per lemma
BabelNet
English 1931 1604 127 200 1.02 1.09
French 1656 1389 89 176 1.05 1.15
German 1467 1267 21 176 1.00 1.05
Italian 1706 1454 211 41 1.22 1.27
Spanish 1481 1103 129 249 1.15 1.19
Wikipedia
English 1242 945 102 195 1.15 1.16
French 1039 790 72 175 1.18 1.14
German 1156 957 21 176 1.07 1.08
Italian 1977 869 85 41 1.20 1.18
Spanish 1103 758 107 248 1.11 1.10
WordNet
English 1644 1502 85 57 1.01 1.10
Table 1: Statistics for the sense annotations of the test set.
and WordNet senses involving lemmas in our En-
glish test set for the task. Overall, we identified 8306
synsets for 978 lemmas to be manually checked. We
recruited 8 annotators in our research group and as-
signed each lemma to two annotators. Each anno-
tator was instructed to check each Babel synset and
determine whether any of the following three opera-
tions was needed:
? Delete a mapping and separate the WordNet
sense from the Wikipedia page (like in the ar-
senic vs. AS (ROMAN COIN) example above);
? Add a mapping between a WordNet sense and a
Wikipedia page (formerly available as two sep-
arate Babel synsets);
? Merge two Babel synsets which express the
same concept.
After disagreement adjudication carried out by
the first author, the number of delete, add and merge
operations was 493, 203 and 43, respectively, for a
total of 739 operations (i.e., 8.8% of synsets cor-
rected). As a result of our validation of BabelNet
1.1, we obtained version 1.1.1, which is currently
available online.
2.4 Sense Annotation
To ensure high quality annotations, the annotation
process was completed in three phases. Because
BabelNet is a superset of both the WordNet and
Wikipedia sense inventories, all annotators used the
BabelNet 1.1.1 sense inventory for their respective
language. These BabelNet annotations were then
projected into WordNet and Wikipedia senses. An-
notation was performed by one native speaker each
for English, French, German and Spanish and, for
Italian, by two native speakers who annotated dif-
ferent subsets of the corpus.
In the first phase, each annotator was instructed
to inspect each instance to check that (1) the lemma
was tagged with the correct part of speech, (2) lem-
mas were correctly annotated as named entity or
multiword expressions, and (3) the meaning of the
instance?s lemma had an associated sense in Ba-
belNet. Based on these criteria, annotators removed
dozens of instances from the original data.
In the second phase, each instance in the En-
glish dataset was annotated using BabelNet senses.
To reduce the time required for annotation in the
other languages, the sense annotations for the En-
glish dataset were then projected onto the other four
224
Language Projected Valid Invalid
instances projections projections
French 1016 791 225
German 592 373 219
Italian 1029 774 255
Spanish 911 669 242
Table 2: Statistics when using the English sense an-
notations to project the correct sense of a lemma in
another language of the sentence-aligned test data.
languages using the sense translation API of Babel-
Net (Navigli and Ponzetto, 2012d). The projection
operated as follows, using the aligned sentences in
the English and non-English texts. For an instance
in the non-English text, all of the senses for that in-
stance?s lemma were compared with the sense an-
notations in the English sentence. If any of that
lemma?s senses was used in the English sentence,
then that sense was selected for the non-English
instance. The matching procedure operates at the
sentence-aligned level because the instances them-
selves are not aligned; i.e., different languages have
different numbers of instances per sentence, which
are potentially ordered differently due to language-
specific construction. Ultimately, this projection la-
beled approximately 50-70% of the instances in the
other four languages. Given the projected senses,
the annotators for the other four languages were then
asked to (1) correct the projected sense labels and
(2) annotate those still without senses.4 These anno-
tations were recorded in text in a stand-off file; no
further annotation tools were used.
The resulting sense projection proved highly use-
ful for selecting the correct sense. Table 2 shows
the number of corrections made by the annotators
to the projected senses, who changed only 22-37%
of the labels. While simple, the projection method
offers significant potential for generating good qual-
ity sense-annotated data from sentence-aligned mul-
tilingual text.
In the third phase, an independent annotator re-
viewed the labels for the high-frequency lemmas for
4During the second phase, annotators were also allowed
to add and remove instances that were missed during the first
phase, which resulted in small number of changes.
all languages to check for systematic errors and dis-
cuss possible changes to the labeling. This review
resulted in only a small number of changes to less
than 5% of the total instances, except for German
which had a slightly higher percentage of changes.
Table 1 summarizes the sense annotation statis-
tics for the test set. Annotators were allowed to use
multiple senses in the case of ambiguity, but en-
couraged to use a single sense whenever possible.
In rare cases, a lemma was annotated with senses
from a different lemma. For example, WordNet does
not contain a sense for ?card? that corresponds to
the penalty card meaning (as used in sports such
as football). In contrast, BabelNet has a sense for
?penalty card? from Wikipedia which, however, is
not mapped to the lemma ?card?. In such cases,
we add both the closest meaning from the original
lemma (e.g., the rectangual piece of paper sense in
WordNet) and the most suitable sense that may have
a different lemma form (e.g., PENALTY CARD).
Previous annotation studies have shown that,
when a fine-grained sense inventory is used, annota-
tors will often label ambiguous instances with multi-
ple senses if allowed (Erk and McCarthy, 2009; Jur-
gens and Klapaftis, 2013). Since BabelNet is a com-
bination of a fine-grained inventory (WordNet) and
contains additional senses from Wikipedia, we ana-
lyzed the average number of BabelNet sense anno-
tations per instance, shown in column six of Table 1.
Surprisingly, Table 1 suggests that the rate of mul-
tiple sense annotation varies significantly between
languages.
BabelNet may combine multiple Wikipedia pages
into a single BabelNet synset. As a result, when
Wikipedia is used as a sense inventory, instances are
annotated with all of the Wikipedia pages associated
with each BabelNet synset. Indeed, Table 1 shows a
markedly increased multi-sense annotation rate for
three languages when using Wikipedia.
As a second analysis, we considered the observed
level of polysemy for each of the unique lemmas.
The last column of Table 1 shows the average num-
ber of different senses seen for each lemma across
the test sets. In all languages, often only a single
sense of a lemma was used. Because the test set is
constructed based on topical documents, infrequent
lemmas mostly occurred within a single document
where they were used with a consistent interpreta-
225
tion. However, we note that in the case of lem-
mas that were only seen with a single sense, this
sense does not always correspond to the most fre-
quent sense as seen in SemCor.
3 Evaluation
Task 12 uses the standard definitions of precision
and recall for WSD evaluation (see, e.g., (Navigli,
2009)). Precision measures the percentage of the
sense assignments provided by the system that are
identical to the gold standard; Recall measures the
percentage of instances that are correctly labeled by
the system. When a system provides sense labels
for all instances, precision and recall are equivalent.
Systems using BabelNet and WordNet senses are
compared against the Most Frequent Sense (MFS)
baseline obtained by using the WordNet most fre-
quent sense. For the Wikipedia sense inventory, we
constructed a pseudo-MFS baseline by selecting (1)
the Wikipedia page associated with the highest rank-
ing WordNet sense, as ranked by SemCor frequency,
or (2) when no synset for a lemma was associ-
ated with a WordNet sense, the first Wikipedia page
sorted using BabelNet?s ordering criteria, i.e., lexi-
cographic sorting. We note that, in the second case,
this procedure frequently selected the page with the
same name as the lemma itself. For instance, the
first sense of Dragon Ball is the cartoon with title
DRAGON BALL, followed by two films (DRAGON
BALL (1990 FILM) and DRAGON BALL EVOLU-
TION).
Systems were scored separately for each sense in-
ventory. We note that because the instances in each
test set are filtered to include only those that can
be labeled with the respective inventory, both the
Wikipedia and WordNet test sets are subsets of the
instances in the BabelNet test set.
4 Participating Systems
Three teams submitted a total of seven systems for
the task, with at least one participant attempting
all of the sense inventory and language combina-
tions. Six systems participated in the WSD task
with BabelNet senses; two teams submitted four sys-
tems using WordNet senses; and one team submitted
three systems for Wikipedia-based senses. Notably,
all systems used graph-based approaches for sense
disambiguation, either using WordNet or BabelNet?s
synset graphs. We summarize the teams? systems as
follows.
DAEBAK! DAEBAK! submitted one system
called PD (Peripheral Diversity) based on BabelNet
path indices from the BabelNet synset graph. Us-
ing a ?5 sentence window around the target word,
a graph is constructed for all senses of co-occurring
lemmas following the procedure proposed by Nav-
igli and Lapata (2010). The final sense is selected
based on measuring connectivity to the synsets of
neighboring lemmas. The MFS is used as a backoff
strategy when no appropriate sense can be picked
out.
GETALP GETALP submitted three systems, two
for BabelNet and one for WordNet, all based on
the ant-colony algorithm of (Schwab et al, 2012),
which uses the sense inventory network structure
to identify paths connecting synsets of the target
lemma to the synsets of other lemmas in context.
The algorithm requires setting several parameters
for the weighting of the structure of the context-
based graph, which vary across the three systems.
The BN1 system optimizes its parameters from the
trial data, while the BN2 and WN1 systems are
completely unsupervised and optimize their param-
eters directly from the structure of the BabelNet and
WordNet graphs.
UMCC-DLSI UMCC-DLSI submitted three
systems based on the ISR-WN resource (Gutie?rrez
et al, 2011), which enriches the WordNet se-
mantic network using edges from multiple lexical
resources, such as WordNet Domains and the
eXtended WordNet. WSD was then performed
using the ISR-WN network in combination with
the algorithm of Gutie?rrez (2012), which is an
extension of the Personalized PageRank algorithm
for WSD (Agirre and Soroa, 2009) which includes
senses frequency. The algorithm requires initial-
izing the PageRank algorithm with a set of seed
synsets (vertices) in the network; this initialization
represents the key variation among UMCC?s three
approaches. The RUN-1 system performs WSD
using all noun instances from the sentence context.
In contrast, the RUN-2 works at the discourse level
and initializes the PageRank using the synsets of all
226
Team System English French German Italian Spanish
DAEBAK! PD 0.604 0.538 0.591 0.613 0.600
GETALP BN-1 0.263 0.261 0.404 0.324 -
GETALP BN-2 0.266 0.257 0.400 0.324 0.371
UMCC-DLSI RUN-1 0.677 0.605 0.618 0.657 0.705
UMCC-DLSI RUN-2 0.685 0.605 0.621 0.658 0.710
UMCC-DLSI RUN-3 0.680 - - - -
MFS 0.665 0.453 0.674 0.575 0.645
Table 3: System performance, reported as F1, for all five languages in the test set when using BabelNet
senses. Top performing systems are marked in bold.
nouns in the document. Finally, the RUN-3 system
initializes using all words in the sentence.
5 Results and Discussion
All teams submitted at least one system using the
BabelNet inventory, shown in Table 3. The UMCC-
DLSI systems were consistently able to outperform
the MFS baseline (a notoriously hard-to-beat heuris-
tic) in all languages except German. Additionally,
the DAEBAK! system outperformed the MFS base-
line on French and Italian. The UMCC-DLSI RUN-
2 system performed the best for all languages. No-
tably, this system leverages the single-sense per dis-
course heuristic (Yarowsky, 1995), which uses the
same sense label for all occurrences of a lemma in a
document.
UMCC-DLSI submitted the only three sys-
tems to use Wikipedia-based senses. Table 4 shows
their performance. Of the three sense inventories,
Wikipedia had the most competitive MFS baseline,
scoring at least 0.694 on all languages. Notably,
the Wikipedia-based system has the lowest recall of
all systems. Despite having superior precision to the
MFS baseline, the low recall brought the resulting
F1 measure below the MFS.
Two teams submitted four total systems for Word-
Net, shown in Table 5. The UMCC-DLSI RUN-2
system was again the top-performing system, under-
scoring the benefit of using discourse information in
selecting senses. The other two UMCC-DLSI sys-
tems also surpassed the MFS baseline. Though still
performing worse than the MFS baseline, when us-
ing the WordNet sense graph, the GETALP system
sees a noticeable improvement of 0.14 over its per-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45
WS
D F
1
Number of senses for the instance
DAEBAK! PDGETALP BN-2 UMCC-DLSI Run-2
Figure 1: F1 measure according to the degree of
instance polysemy, reported when at least ten in-
stances have the specified polysemy.
formance on English data when using the WordNet
sense graph.
The disambiguation task encompasses multiple
types of entities. Therefore, we partitioned the Ba-
belNet test data according to the type of instance be-
ing disambiguated; Table 6 highlights the results per
instance type, averaged across all languages.5 Both
multiword expressions and named entities are less
polysemous, resulting in a substantially higher MFS
baseline that no system was able to outperform on
the two classes. However, for instances made of a
single term, both of the UMCC-DLSI systems were
able to outperform the MFS baseline.
BabelNet adds many Wikipedia senses to the ex-
isting WordNet senses, which increases the poly-
5We omit the UMCC-DLSI Run-3 system from analysis, as
it participated in only a single language.
227
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
UMCC-DLSI RUN-1 0.619 0.484 0.543 0.817 0.480 0.605 0.758 0.460 0.572 0.785 0.458 0.578 0.773 0.493 0.602
UMCC-DLSI RUN-2 0.620 0.487 0.546 0.815 0.478 0.603 0.769 0.467 0.581 0.787 0.463 0.583 0.778 0.502 0.610
UMCC-DLSI RUN-3 0.622 0.489 0.548 - - - - - - - - - - - -
MFS 0.860 0.753 0.803 0.698 0.691 0.694 0.836 0.827 0.831 0.833 0.813 0.823 0.830 0.819 0.824
Table 4: The F1 measure for each system across all five languages in the test set when using Wikipedia-based
senses.
Team System Precision Recall F1
GETALP WN-1 0.406 0.406 0.406
UMCC-DLSI RUN-1 0.639 0.635 0.637
UMCC-DLSI RUN-2 0.649 0.645 0.647
UMCC-DLSI RUN-3 0.642 0.639 0.640
MFS 0.630 0.630 0.630
Table 5: System performance when using WordNet senses. Top performing systems are marked in bold.
Team System Single term Multiword expression Named Entity
DAEBAK! PD 0.502 0.801 0.910
GETALP BN-1 0.232 0.724 0.677
GETALP BN-2 0.235 0.740 0.656
UMCC-DLSI RUN-1 0.582 0.806 0.865
UMCC-DLSI RUN-2 0.584 0.809 0.864
MFS 0.511 0.853 0.920
Table 6: System F1 per instance type, averaged across all submitted languages, with the highest system
scores in bold.
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
DAEBAK PD 0.769 0.364 0.494 0.747 0.387 0.510 0.762 0.307 0.438 0.778 0.425 0.550 0.778 0.450 0.570
GETALP BN-2 0.793 0.111 0.195 0.623 0.130 0.215 0.679 0.124 0.210 0.647 0.141 0.231 0.688 0.177 0.282
UMCC-DLSI RUN-1 0.787 0.421 0.549 0.754 0.441 0.557 0.741 0.330 0.457 0.796 0.461 0.584 0.830 0.525 0.643
UMCC-DLSI RUN-2 0.791 0.419 0.548 0.760 0.436 0.554 0.746 0.332 0.460 0.799 0.453 0.578 0.837 0.530 0.649
Table 7: System performance when the system?s annotations are restricted to only those senses that it also
uses in the aligned sentences of at least two other languages.
semy of most instances. As a further analysis, we
consider the relationship between the polysemy of
an instance?s target and system performance. In-
stances were grouped according to the number of
BabelNet senses that their lemma had; following,
systems were scored on each grouping. Figure 1
shows the performance of the best system from each
team on each polysemy-based instance grouping,
with a general trend of performance decay as the
number of senses increases. Indeed, all systems?
performances are negatively correlated with the de-
gree of polysemy, ranging from -0.401 (UMCC-
DLSI RUN-1) to -0.654 (GETALP BN-1) when
measured using Pearson?s correlation. All systems?
228
correlations are significant at p < 0.05.
Last, we note that all systems operated by sense-
annotating each language individually without tak-
ing advantage of either the multilingual structure of
BabelNet or the sentence alignment of the test data.
For example, the sense projection method used to
create the initial set of multilingual annotations on
our test data (cf. Table 2) suggests that the sense
translation API could be used as a reliable source for
estimating the correctness of an annotation; specifi-
cally, given the sense annotations for each language,
the translation API could be used to test whether the
sense is also present in the aligned sentence in the
other languages.
Therefore, we performed a post-hoc analysis of
the benefit of multilingual sense alignment using the
results of the four systems that submitted for all lan-
guages in BabelNet. For each language, we filter
the sense annotations such that an annotation for an
instance is retained only if the system assigned the
same sense to some word in the aligned sentence
from at least two other languages.
Table 7 shows the resulting performance for the
four systems. As expected, the systems exhibit sig-
nificantly lower recall due to omitting all language-
specific instances. However, the resulting precision
is significantly higher than the original performance,
shown in Table 3. Additionally, we analyzed the set
of instances reported for each system and confirmed
that the improvement is not due to selecting only
monosemous lemmas. Despite the GETALP system
having the lower performance of the four systems
when all instances are considered, the system ob-
tains the highest precision for the English dataset.
Furthermore, the UMCC-DLSI systems still obtain
moderate recall, while enjoying 0.106-0.155 abso-
lute improvements in precision across all languages.
While the resulting F1 is lower due to a loss of recall,
we view this result as a solid starting point for other
methods to sense-tag the remaining instances. Over-
all, these results corroborate previous studies sug-
gesting that highly precise sense annotations can be
obtained by leveraging multiple languages (Navigli
and Ponzetto, 2012b; Navigli and Ponzetto, 2012c).
6 Conclusion and Future Directions
Following recent SemEval efforts with word senses
in multilingual settings, we have introduced a new
task on multilingual WSD that uses the recently
released BabelNet 1.1.1 sense inventory. Using a
data set of 13 articles in five languages, all nomi-
nal instances were annotated with BabelNet senses.
Because BabelNet is a superset of WordNet and
Wikipedia, the task also facilitates analysis in those
sense inventories.
Three teams submitted seven systems, with all
systems leveraging the graph-based structure of
WordNet and BabelNet. Several systems were able
to outperform the competitive MFS baseline, except
in the case of Wikipedia, but current performance
leaves significant room for future improvement. In
addition, we believe that future research could lever-
age sense parallelism available in sentence-aligned
multilingual corpora, together with enriched infor-
mation available in future versions of BabelNet. All
of the resources for this task, including the newest
1.1.1 version of BabelNet, were released on the task
website.6
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
A large group of people assisted with SemEval-
2013 Task 12, and without whose help this
task would not have been possible. In particular,
we would like to thank Philipp Cimiano, Maud
Erhmann, Sascha Hinte, Jesu?s Roque Campan?a
Go?mez, and Andreas Soos for their assistance
in sense annotation; our fellow LCL team mem-
bers: Moreno De Vincenzi, Stefano Faralli, Tiziano
Flati, Marc Franco Salvador, Andrea Moro, Silvia
Necs?ulescu, and Taher Pilehvar for their invaluable
assistance in creating BabelNet 1.1.1, preparing and
validating sense annotations, and sense-tagging the
Italian corpus; last, we thank Jim McManus for his
help in producing the Italian test data.
6http://www.cs.york.ac.uk/semeval-2013/
task12/
229
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, Athens, Greece, pages 33?41.
Philip Edmonds and Scott Cotton. 2001. Senseval-2:
Overview. In Proceedings of The Second International
Workshop on Evaluating Word Sense Disambiguation
Systems, pages 1?6, Toulouse, France.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing, pages 440?449,
Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Yoan Gutie?rrez, Antonio Ferna?ndez Orqu??n, Sonia
Va?zquez, and Andre?s Montoyo. 2011. Enriching the
integration of semantic resources based on wordnet.
Procesamiento del Lenguaje Natural, 47:249?257.
Yoan Gutie?rrez. 2012. Ana?lisis sema?ntico multidimen-
sional aplicado a la desambiguacio?n del lenguaje nat-
ural. Ph.D. thesis, Universidad de Alicante.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-structured
content and artificial intelligence: The story so far. Ar-
tificial Intelligence, 194:2?27.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Adam Kilgarriff and Martha Palmer. 2000. Introduction
to the special issue on senseval. Computers and the
Humanities, 34(1-2):1?13.
Adam Kilgarriff. 1998. Senseval: An exercise in eval-
uating word sense disambiguation programs. In Pro-
ceedings of the First International Conference on Lan-
guage Resources and Evaluation, pages 1255?1258,
Granada, Spain.
Els Lefever and Veronique Hoste. 2010. Semeval-2010
task 3: Cross-lingual word sense disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 15?20, Uppsala, Sweden.
Association for Computational Linguistics.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of the 3rd International Work-
shop on the Evaluation of Systems for the Semantic
Analysis of Text (SENSEVAL-3) at ACL-04, Barcelona,
Spain, 25?26 July 2004, pages 25?28.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the 5th international workshop
on semantic evaluation, pages 9?14, Uppsala, Sweden.
Association for Computational Linguistics.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235?244.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 32(4):678?
692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence (AAAI), Toronto, Ontario, Canada.
Roberto Navigli and Simone Paolo Ponzetto. 2012c.
Joining forces pays off: Multilingual Joint Word Sense
Disambiguation. In Proceedings of EMNLP-CoNLL,
pages 1399?1410, Jeju Island, Korea.
Roberto Navigli and Simone Paolo Ponzetto. 2012d.
Multilingual WSD with just a few lines of code: the
BabelNet API. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2012), Jeju, Korea.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
grained English all-words task. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, pages 30?
35.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Roberto Navigli. 2012. A quick tour of Word Sense
Disambiguation, Induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115?129.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Repub-
lic, pages 87?92.
Didier Schwab, Je?ro?me Goulian, Andon Tchechmedjiev,
and Herve? Blanchon. 2012. Ant colony algorithm for
230
the unsupervised word sense disambiguation of texts:
Comparison and evaluation. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING), pages 8?15, Mumbai, India.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Proceedings of ACL 2004
SENSEVAL-3 Workshop, pages 41?43, Barcelona,
Spain.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA, USA.
231
