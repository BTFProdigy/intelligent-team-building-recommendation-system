Proceedings of ACL-08: HLT, pages 728?736,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Lexicon-Based Resolution of Unknown Words for Full
Morphological Analysis
Meni Adler and Yoav Goldberg and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science?
POB 653 Be?er Sheva, 84105, Israel
{adlerm,goldberg,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Morphological disambiguation proceeds in 2
stages: (1) an analyzer provides all possible
analyses for a given token and (2) a stochastic
disambiguation module picks the most likely
analysis in context. When the analyzer does
not recognize a given token, we hit the prob-
lem of unknowns. In large scale corpora, un-
knowns appear at a rate of 5 to 10% (depend-
ing on the genre and the maturity of the lexi-
con).
We address the task of computing the distribu-
tion p(t|w) for unknown words for full mor-
phological disambiguation in Hebrew. We in-
troduce a novel algorithm that is language in-
dependent: it exploits a maximum entropy let-
ters model trained over the known words ob-
served in the corpus and the distribution of
the unknown words in known tag contexts,
through iterative approximation. The algo-
rithm achieves 30% error reduction on dis-
ambiguation of unknown words over a com-
petitive baseline (to a level of 70% accurate
full disambiguation of unknown words). We
have also verified that taking advantage of a
strong language-specific model of morpholog-
ical patterns provides the same level of disam-
biguation. The algorithm we have developed
exploits distributional information latent in a
wide-coverage lexicon and large quantities of
unlabeled data.
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
1 Introduction
The term unknowns denotes tokens in a text that can-
not be resolved in a given lexicon. For the task of
full morphological analysis, the lexicon must pro-
vide all possible morphological analyses for any
given token. In this case, unknown tokens can be
categorized into two classes of missing informa-
tion: unknown tokens are not recognized at all by
the lexicon, and unknown analyses, where the set
of analyses for a lexeme does not contain the cor-
rect analysis for a given token. Despite efforts on
improving the underlying lexicon, unknowns typi-
cally represent 5% to 10% of the number of tokens
in large-scale corpora. The alternative to continu-
ously investing manual effort in improving the lex-
icon is to design methods to learn possible analy-
ses for unknowns from observable features: their
letter structure and their context. In this paper, we
investigate the characteristics of Hebrew unknowns
for full morphological analysis, and propose a new
method for handling such unavoidable lack of in-
formation. Our method generates a distribution of
possible analyses for unknowns. In our evaluation,
these learned distributions include the correct anal-
ysis for unknown words in 85% of the cases, con-
tributing an error reduction of over 30% over a com-
petitive baseline for the overall task of full morpho-
logical analysis in Hebrew.
The task of a morphological analyzer is to pro-
duce all possible analyses for a given token. In
Hebrew, the analysis for each token is of the form
lexeme-and-features1: lemma, affixes, lexical cate-
1In contrast to the prefix-stem-suffix analysis format of
728
gory (POS), and a set of inflection properties (ac-
cording to the POS) ? gender, number, person, sta-
tus and tense. In this work, we refer to the mor-
phological analyzer of MILA ? the Knowledge Cen-
ter for Processing Hebrew2 (hereafter KC analyzer).
It is a synthetic analyzer, composed of two data re-
sources ? a lexicon of about 2,400 lexemes, and a
set of generation rules (see (Adler, 2007, Section
4.2)). In addition, we use an unlabeled text cor-
pus, composed of stories taken from three Hebrew
daily news papers (Aruts 7, Haaretz, The Marker),
of 42M tokens. We observed 3,561 different com-
posite tags (e.g., noun-sing-fem-prepPrefix:be) over
this corpus. These 3,561 tags form the large tagset
over which we train our learner. On the one hand,
this tagset is much larger than the largest tagset used
in English (from 17 tags in most unsupervised POS
tagging experiments, to the 46 tags of the WSJ cor-
pus and the about 150 tags of the LOB corpus). On
the other hand, our tagset is intrinsically factored as
a set of dependent sub-features, which we explicitly
represent.
The task we address in this paper is morphologi-
cal disambiguation: given a sentence, obtain the list
of all possible analyses for each word from the an-
alyzer, and disambiguate each word in context. On
average, each token in the 42M corpus is given 2.7
possible analyses by the analyzer (much higher than
the average 1.41 POS tag ambiguity reported in En-
glish (Dermatas and Kokkinakis, 1995)). In previ-
ous work, we report disambiguation rates of 89%
for full morphological disambiguation (using an un-
supervised EM-HMM model) and 92.5% for part of
speech and segmentation (without assigning all the
inflectional features of the words).
In order to estimate the importance of unknowns
in Hebrew, we analyze tokens in several aspects: (1)
the number of unknown tokens, as observed on the
corpus of 42M tokens; (2) a manual classification
of a sample of 10K unknown token types out of the
200K unknown types identified in the corpus; (3) the
number of unknown analyses, based on an annotated
corpus of 200K tokens, and their classification.
About 4.5% of the 42M token instances in the
Buckwalter?s Arabic analyzer (2004), which looks for any le-
gal combination of prefix-stem-suffix, but does not provide full
morphological features such as gender, number, case etc.
2http://mila.cs.technion.ac.il.html
training corpus were unknown tokens (45% of the
450K token types). For less edited text, such as ran-
dom text sampled from the Web, the percentage is
much higher ? about 7.5%. In order to classify these
unknown tokens, we sampled 10K unknown token
types and examined them manually. The classifica-
tion of these tokens with their distribution is shown
in Table 13. As can be seen, there are two main
classes of unknown token types: Neologisms (32%)
and Proper nouns (48%), which cover about 80%
of the unknown token instances. The POS distribu-
tion of the unknown tokens of our annotated corpus
is shown in Table 2. As expected, most unknowns
are open class words: proper names, nouns or adjec-
tives.
Regarding unknown analyses, in our annotated
corpus, we found 3% of the 100K token instances
were missing the correct analysis in the lexicon
(3.65% of the token types). The POS distribution of
the unknown analyses is listed in Table 2. The high
rate of unknown analyses for prepositions at about
3% is a specific phenomenon in Hebrew, where
prepositions are often prefixes agglutinated to the
first word of the noun phrase they head. We observe
the very low rate of unknown verbs (2%) ? which are
well marked morphologically in Hebrew, and where
the rate of neologism introduction seems quite low.
This evidence illustrates the need for resolution
of unknowns: The naive policy of selecting ?proper
name? for all unknowns will cover only half of the
errors caused by unknown tokens, i.e., 30% of the
whole unknown tokens and analyses. The other 70%
of the unknowns ( 5.3% of the words in the text in
our experiments) will be assigned a wrong tag.
As a result of this observation, our strategy is to
focus on full morphological analysis for unknown
tokens and apply a proper name classifier for un-
known analyses and unknown tokens. In this paper,
we investigate various methods for achieving full
morphological analysis distribution for unknown to-
kens. The methods are not based on an annotated
corpus, nor on hand-crafted rules, but instead ex-
ploit the distribution of words in an available lexicon
and the letter similarity of the unknown words with
known words.
3Transcription according to Ornan (2002)
729
Category Examples DistributionTypes Instances
Proper names ?asulin (family name) oileq`
?a?udi (Audi) ice`` 40% 48%
Neologisms ?agabi (incidental) iab`
tizmur (orchestration) xenfz 30% 32%
Abbreviation mz?p (DIFS) t"fnkb?t (security officer) h"aw 2.4% 7.8%
Foreign
presentacyah (presentation) divhpfxt
?a?ut (out) he``
right
3.8% 5.8%
Wrong spelling
?abibba?ah
.
ronah (springatlast) dpexg`aaia`
?idiqacyot (idication) zeivwici`
ryus?alaim (Rejusalem) milyeix
1.2% 4%
Alternative spelling ?opyynim (typical) mipiite`priwwilegyah (privilege ) diblieeixt 3.5% 3%
Tokenization ha?sap (the?threshold) sq"d
?al/17 (on/17) 71/lr 8% 2%
Table 1: Unknown Hebrew token categories and distribution.
Part of Speech Unknown Tokens Unknown Analyses Total
Proper name 31.8% 24.4% 56.2%
Noun 12.6% 1.6% 14.2%
Adjective 7.1% 1.7% 8.8%
Junk 3.0% 1.3% 4.3%
Numeral 1.1% 2.3% 3.4%
Preposition 0.3% 2.8% 3.1%
Verb 1.8% 0.4% 2.2%
Adverb 0.9% 0.9% 1.8%
Participle 0.4% 0.8% 1.2%
Copula / 0.8% 0.8%
Quantifier 0.3% 0.4% 0.7%
Modal 0.3% 0.4% 0.7%
Conjunction 0.1% 0.5% 0.6%
Negation / 0.6% 0.6%
Foreign 0.2% 0.4% 0.6%
Interrogative 0.1% 0.4% 0.5%
Prefix 0.3% 0.2% 0.5%
Pronoun / 0.5% 0.5%
Total 60% 40% 100%
Table 2: Unknowns Hebrew POS Distribution.
730
2 Previous Work
Most of the work that dealt with unknowns in the last
decade focused on unknown tokens (OOV). A naive
approach would assign all possible analyses for each
unknown token with uniform distribution, and con-
tinue disambiguation on the basis of a learned model
with this initial distribution. The performance of a
tagger with such a policy is actually poor: there are
dozens of tags in the tagset (3,561 in the case of He-
brew full morphological disambiguation) and only
a few of them may match a given token. Several
heuristics were developed to reduce the possibility
space and to assign a distribution for the remaining
analyses.
Weischedel et al (1993) combine several heuris-
tics in order to estimate the token generation prob-
ability according to various types of information ?
such as the characteristics of particular tags with
respect to unknown tokens (basically the distribu-
tion shown in Table 2), and simple spelling fea-
tures: capitalization, presence of hyphens and spe-
cific suffixes. An accuracy of 85% in resolving un-
known tokens was reported. Dermatas and Kokki-
nakis (1995) suggested a method for guessing un-
known tokens based on the distribution of the ha-
pax legomenon, and reported an accuracy of 66% for
English. Mikheev (1997) suggested a guessing-rule
technique, based on prefix morphological rules, suf-
fix morphological rules, and ending-guessing rules.
These rules are learned automatically from raw text.
They reported a tagging accuracy of about 88%.
Thede and Harper (1999) extended a second-order
HMM model with a C = ck,i matrix, in order to en-
code the probability of a token with a suffix sk to
be generated by a tag ti. An accuracy of about 85%
was reported.
Nakagawa (2004) combine word-level and
character-level information for Chinese and
Japanese word segmentation. At the word level, a
segmented word is attached to a POS, where the
character model is based on the observed characters
and their classification: Begin of word, In the
middle of a word, End of word, the character is a
word itself S. They apply Baum-Welch training over
a segmented corpus, where the segmentation of each
word and its character classification is observed, and
the POS tagging is ambiguous. The segmentation
(of all words in a given sentence) and the POS
tagging (of the known words) is based on a Viterbi
search over a lattice composed of all possible word
segmentations and the possible classifications of
all observed characters. Their experimental results
show that the method achieves high accuracy over
state-of-the-art methods for Chinese and Japanese
word segmentation. Hebrew also suffers from
ambiguous segmentation of agglutinated tokens into
significant words, but word formation rules seem to
be quite different from Chinese and Japanese. We
also could not rely on the existence of an annotated
corpus of segmented word forms.
Habash and Rambow (2006) used the
root+pattern+features representation of Arabic
tokens for morphological analysis and generation
of Arabic dialects, which have no lexicon. They
report high recall (95%?98%) but low precision
(37%?63%) for token types and token instances,
against gold-standard morphological analysis. We
also exploit the morphological patterns characteris-
tic of semitic morphology, but extend the guessing
of morphological features by using contextual
features. We also propose a method that relies
exclusively on learned character-level features and
contextual features, and eventually reaches the same
performance as the patterns-based approach.
Mansour et al (2007) combine a lexicon-based
tagger (such as MorphTagger (Bar-Haim et al,
2005)), and a character-based tagger (such as the
data-driven ArabicSVM (Diab et al, 2004)), which
includes character features as part of its classifica-
tion model, in order to extend the set of analyses
suggested by the analyzer. For a given sentence, the
lexicon-based tagger is applied, selecting one tag for
a token. In case the ranking of the tagged sentence is
lower than a threshold, the character-based tagger is
applied, in order to produce new possible analyses.
They report a very slight improvement on Hebrew
and Arabic supervised POS taggers.
Resolution of Hebrew unknown tokens, over a
large number of tags in the tagset (3,561) requires
a much richer model than the the heuristics used
for English (for example, the capitalization feature
which is dominant in English does not exist in He-
brew). Unlike Nakagawa, our model does not use
any segmented text, and, on the other hand, it aims
to select full morphological analysis for each token,
731
including unknowns.
3 Method
Our objective is: given an unknown word, provide
a distribution of possible tags that can serve as the
analysis of the unknown word. This unknown anal-
ysis step is performed at training and testing time.
We do not attempt to disambiguate the word ? but
only to provide a distribution of tags that will be dis-
ambiguated by the regular EM-HMM mechanism.
We examined three models to construct the distri-
bution of tags for unknown words, that is, whenever
the KC analyzer does not return any candidate anal-
ysis, we apply these models to produce possible tags
for the token p(t|w):
Letters A maximum entropy model is built for
all unknown tokens in order to estimate their tag
distribution. The model is trained on the known
tokens that appear in the corpus. For each anal-
ysis of a known token, the following features are
extracted: (1) unigram, bigram, and trigram letters
of the base-word (for each analysis, the base-word
is the token without prefixes), together with their
index relative to the start and end of the word. For
example, the n-gram features extracted for the word
abc are { a:1 b:2 c:3 a:-3 b:-2 c:-1
ab:1 bc:2 ab:-2 bc:-1 abc:1 abc:-1
} ; (2) the prefixes of the base-word (as a single
feature); (3) the length of the base-word. The class
assigned to this set of features, is the analysis of the
base-word. The model is trained on all the known
tokens of the corpus, each token is observed with its
possible POS-tags once for each of its occurrences.
When an unknown token is found, the model
is applied as follows: all the possible linguistic
prefixes are extracted from the token (one of the 76
prefix sequences that can occur in Hebrew); if more
than one such prefix is found, the token is analyzed
for each possible prefix. For each possible such
segmentation, the full feature vector is constructed,
and submitted to the Maximum Entropy model.
We hypothesize a uniform distribution among the
possible segmentations and aggregate a distribution
of possible tags for the analysis. If the proposed
tag of the base-word is never found in the corpus
preceded by the identified prefix, we remove this
possible analysis. The eventual outcome of the
model application is a set of possible full morpho-
logical analyses for the token ? in exactly the same
format as the morphological analyzer provides.
Patterns Word formation in Hebrew is based on
root+pattern and affixation. Patterns can be used to
identify the lexical category of unknowns, as well
as other inflectional properties. Nir (1993) investi-
gated word-formation in Modern Hebrew with a spe-
cial focus on neologisms; the most common word-
formation patterns he identified are summarized in
Table 3. A naive approach for unknown resolution
would add all analyses that fit any of these patterns,
for any given unknown token. As recently shown by
Habash and Rambow (2006), the precision of such
a strategy can be pretty low. To address this lack of
precision, we learn a maximum entropy model on
the basis of the following binary features: one fea-
ture for each pattern listed in column Formation of
Table 3 (40 distinct patterns) and one feature for ?no
pattern?.
Pattern-Letters This maximum entropy model is
learned by combining the features of the letters
model and the patterns model.
Linear-Context-based p(t|c) approximation
The three models above are context free. The
linear-context model exploits information about the
lexical context of the unknown words: to estimate
the probability for a tag t given a context c ? p(t|c)
? based on all the words in which a context occurs,
the algorithm works on the known words in the
corpus, by starting with an initial tag-word estimate
p(t|w) (such as the morpho-lexical approximation,
suggested by Levinger et al (1995)), and iteratively
re-estimating:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?C p(t|c)p(c|w)allow(t, w)
Z
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of contexts.
allow(t, w) is a binary function indicating whether t
is a valid tag for w. p(c|w) and p(w|c) are estimated
via raw corpus counts.
Loosely speaking, the probability of a tag given a
context is the average probability of a tag given any
732
Category Formation Example
Verb Template
?iCCeC ?ibh
.
en (diagnosed) oga`
miCCeC mih
.
zer (recycled) xfgn
CiCCen timren (manipulated) oxnz
CiCCet tiknet (programmed) zpkz
tiCCeC ti?arek (dated) jx`z
Participle Template
meCuCaca ms?wh
.
zar (reconstructed) xfgeyn
muCCaC muqlat
.
(recorded) hlwen
maCCiC malbin (whitening) oialn
Noun
Suffixation
ut h
.
aluciyut (pioneership) zeivelg
ay yomanay (duty officer) i`pnei
an ?egropan (boxer) otexb`
on pah
.
on (shack) oegt
iya marakiyah (soup tureen) diiwxn
it t
.
iyulit (open touring vehicle) zileih
a lomdah (courseware) dcnel
Template
maCCeC mas?neq (choke) wpyn
maCCeCa madgera (incubator) dxbcn
miCCaC mis?ap (branching) srqn
miCCaCa mignana (defensive fighting) dppbn
CeCeCa pelet
.
(output) hlt
tiCCoCet tiproset (distribution) zqextz
taCCiC tah
.
rit
.
(engraving) hixgz
taCCuCa tabru?ah (sanitation) d`exaz
miCCeCet micrepet (leotard) ztxvn
CCiC crir (dissonance) xixv
CaCCan bals?an (linguist) oyla
CaCeCet s?ah
.
emet (cirrhosis) zngy
CiCul t
.
ibu? (ringing) reaih
haCCaCa hanpas?a (animation) dytpd
heCCeC het?em (agreement) m`zd
Adjective
Suffixationb
i nora?i (awful) i`xep
ani yeh
.
idani (individual) ipcigi
oni t
.
elewizyonic (televisional) ipeifieelh
a?i yed
.
ida?i (unique) i`cigi
ali st
.
udentiali (student) il`ihpcehq
Template C1C2aC3C2aC3
d metaqtaq (sweetish) wzwzn
CaCuC rapus (flaccid ) qetx
Adverb Suffixation
ot qcarot (briefly) zexvw
it miyadit (immediately) zicin
Prefixation b bekeip (with fun) sika
aCoCeC variation: wzer ?wyeq (a copy).
bThe feminine form is made by the t and iya suffixes: ipcigi yeh
.
idanit (individual), dixvep nwcriya (Christian).
cIn the feminine form, the last h of the original noun is omitted.
dC1C2aC3C2oC3 variation: oehphw qt.ant.wn (tiny).
Table 3: Common Hebrew Neologism Formations.
733
Model Analysis Set MorphologicalDisambiguationCoverage Ambiguity Probability
Baseline 50.8% 1.5 0.48 57.3%
Pattern 82.8% 20.4 0.10 66.8%
Letter 76.7% 5.9 0.32 69.1%
Pattern-Letter 84.1% 10.4 0.25 69.8%
WordContext-Pattern 84.4% 21.7 0.12 66.5%
TagContext-Pattern 85.3% 23.5 0.19 64.9%
WordContext-Letter 80.7% 7.94 0.30 69.7%
TagContext-Letter 83.1% 7.8 0.22 66.9%
WordContext-Pattern-Letter 85.2% 12.0 0.24 68.8%
TagContext-Pattern-Letter 86.1% 14.3 0.18 62.1%
Table 4: Evaluation of unknown token full morphological analysis.
of the words appearing in that context, and similarly
the probability of a tag given a word is the averaged
probability of that tag in all the (reliable) contexts
in which the word appears. We use the function
allow(t, w) to control the tags (ambiguity class) al-
lowed for each word, as given by the lexicon.
For a given word wi in a sentence, we examine
two types of contexts: word context wi?1, wi+1,
and tag context ti?1, ti+1. For the case of word con-
text, the estimation of p(w|c) and p(c|w) is simply
the relative frequency over all the events w1, w2, w3
occurring at least 10 times in the corpus. Since the
corpus is not tagged, the relative frequency of the
tag contexts is not observed, instead, we use the
context-free approximation of each word-tag, in or-
der to determine the frequency weight of each tag
context event. For example, given the sequence
icnl ziznerl daebz tgubah l?umatit lmadai (a quite
oppositional response), and the analyses set pro-
duced by the context-free approximation: tgubah
[NN 1.0] l?umatit [] lmadai [RB 0.8, P1-NN 0.2].
The frequency weight of the context {NN RB} is
1 ? 0.8 = 0.8 and the frequency weight of the con-
text {NN P1-NN} is 1 ? 0.2 = 0.2.
4 Evaluation
For testing, we manually tagged the text which is
used in the Hebrew Treebank (consisting of about
90K tokens), according to our tagging guideline (?).
We measured the effectiveness of the three mod-
els with respect to the tags that were assigned to the
unknown tokens in our test corpus (the ?correct tag?),
according to three parameters: (1) The coverage of
the model, i.e., we count cases where p(t|w) con-
tains the correct tag with a probability larger than
0.01; (2) the ambiguity level of the model, i.e., the
average number of analyses suggested for each to-
ken; (3) the average probability of the ?correct tag?,
according to the predicted p(t|w). In addition, for
each experiment, we run the full morphology dis-
ambiguation system where unknowns are analyzed
according by the model.
Our baseline proposes the most frequent tag
(proper name) for all possible segmentations of the
token, in a uniform distribution. We compare the
following models: the 3 context free models (pat-
terns, letters and the combined patterns and letters)
and the same models combined with the word and
tag context models. Note that the context models
have low coverage (about 40% for the word context
and 80% for the tag context models), and therefore,
the context models cannot be used on their own. The
highest coverage is obtained for the combined model
(tag context, pattern, letter) at 86.1%.
We first show the results for full morphological
disambiguation, over 3,561 distinct tags in Table 4.
The highest coverage is obtained for the model com-
bining the tag context, patterns and letters models.
The tag context model is more effective because
it covers 80% of the unknown words, whereas the
word context model only covers 40%. As expected,
our simple baseline has the highest precision, since
the most frequent proper name tag covers over 50%
of the unknown words. The eventual effectiveness of
734
Model Analysis Set POS TaggingCoverage Ambiguity Probability
Baseline 52.9% 1.5 0.52 60.6%
Pattern 87.4% 8.7 0.19 76.0%
Letter 80% 4.0 0.39 77.6%
Pattern-Letter 86.7% 6.2 0.32 78.5%
WordContext-Pattern 88.7% 8.8 0.21 75.8%
TagContext-Pattern 89.5% 9.1 0.14 73.8%
WordContext-Letter 83.8% 4.5 0.37 78.2%
TagContext-Letter 87.1% 5.7 0.28 75.2%
WordContext-Pattern-Letter 87.8 6.5 0.32 77.5%
TagContext-Pattern-Letter 89.0% 7.2 0.25 74%
Table 5: Evaluation of unknown token POS tagging.
the method is measured by its impact on the eventual
disambiguation of the unknown words. For full mor-
phological disambiguation, our method achieves an
error reduction of 30% (57% to 70%). Overall, with
the level of 4.5% of unknown words observed in our
corpus, the algorithm we have developed contributes
to an error reduction of 5.5% for full morphological
disambiguation.
The best result is obtained for the model com-
bining pattern and letter features. However, the
model combining the word context and letter fea-
tures achieves almost identical results. This is an
interesting result, as the pattern features encapsulate
significant linguistic knowledge, which apparently
can be approximated by a purely distributional ap-
proximation.
While the disambiguation level of 70% is lower
than the rate of 85% achieved in English, it must
be noted that the task of full morphological disam-
biguation in Hebrew is much harder ? we manage
to select one tag out of 3,561 for unknown words as
opposed to one out of 46 in English. Table 5 shows
the result of the disambiguation when we only take
into account the POS tag of the unknown tokens.
The same models reach the best results in this case
as well (Pattern+Letters and WordContext+Letters).
The best disambiguation result is 78.5% ? still much
lower than the 85% achieved in English. The main
reason for this lower level is that the task in He-
brew includes segmentation of prefixes and suffixes
in addition to POS classification. We are currently
investigating models that will take into account the
specific nature of prefixes in Hebrew (which encode
conjunctions, definite articles and prepositions) to
better predict the segmentation of unknown words.
5 Conclusion
We have addressed the task of computing the distri-
bution p(t|w) for unknown words for full morpho-
logical disambiguation in Hebrew. The algorithm
we have proposed is language independent: it ex-
ploits a maximum entropy letters model trained over
the known words observed in the corpus and the dis-
tribution of the unknown words in known tag con-
texts, through iterative approximation. The algo-
rithm achieves 30% error reduction on disambigua-
tion of unknown words over a competitive baseline
(to a level of 70% accurate full disambiguation of
unknown words). We have also verified that tak-
ing advantage of a strong language-specific model
of morphological patterns provides the same level
of disambiguation. The algorithm we have devel-
oped exploits distributional information latent in a
wide-coverage lexicon and large quantities of unla-
beled data.
We observe that the task of analyzing unknown to-
kens for POS in Hebrew remains challenging when
compared with English (78% vs. 85%). We hy-
pothesize this is due to the highly ambiguous pattern
of prefixation that occurs widely in Hebrew and are
currently investigating syntagmatic models that ex-
ploit the specific nature of agglutinated prefixes in
Hebrew.
735
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos-tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer, version 2.0.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proceeding of HLT-NAACL-
04.
Michael Elhadad, Yael Netzer, David Gabay, and Meni
Adler. 2005. Hebrew morphological tagging guide-
lines. Technical report, Ben-Gurion University, Dept.
of Computer Science.
Nizar Habash and Owen Rambow. 2006. Magead: A
morphological analyzer and generator for the arabic
dialects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 681?688, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In ACL07 Workshop on Computational Ap-
proaches to Semitic Languages, Prague, Czech Repub-
lic.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405?423.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th international
conference on Computational Linguistics, Geneva.
Raphael Nir. 1993. Word-Formation in Modern Hebrew.
The Open University of Israel, Tel-Aviv, Israel.
Uzi Ornan. 2002. Hebrew in Latin script. Le?s?one?nu,
LXIV:137?151. (in Hebrew).
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
736
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 57?64,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Can You Tag the Modal? You Should.
Yael Netzer and Meni Adler and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yaeln,adlerm,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Computational linguistics methods are typ-
ically first developed and tested in English.
When applied to other languages, assump-
tions from English data are often applied
to the target language. One of the most
common such assumptions is that a ?stan-
dard? part-of-speech (POS) tagset can be
used across languages with only slight vari-
ations. We discuss in this paper a specific is-
sue related to the definition of a POS tagset
for Modern Hebrew, as an example to clar-
ify the method through which such varia-
tions can be defined. It is widely assumed
that Hebrew has no syntactic category of
modals. There is, however, an identified
class of words which are modal-like in their
semantics, and can be characterized through
distinct syntactic and morphologic criteria.
We have found wide disagreement among
traditional dictionaries on the POS tag at-
tributed to such words. We describe three
main approaches when deciding how to tag
such words in Hebrew. We illustrate the im-
pact of selecting each of these approaches
on agreement among human taggers, and on
the accuracy of automatic POS taggers in-
duced for each method. We finally recom-
mend the use of a ?modal? tag in Hebrew
and provide detailed guidelines for this tag.
Our overall conclusion is that tagset defini-
tion is a complex task which deserves appro-
priate methodology.
1 Introduction
In this paper we address one linguistic issue that was
raised while tagging a Hebrew corpus for part of
speech (POS) and morphological information. Our
corpus is comprised of short news stories. It in-
cludes roughly 1,000,000 tokens, in articles of typ-
ical length between 200 to 1000 tokens. The arti-
cles are written in a relatively simple style, with a
high token/word ratio. Of the full corpus, a sam-
ple of articles comprising altogether 100,000 tokens
was assembled at random and manually tagged for
part of speech. We employed four students as tag-
gers. An initial set of guidelines was first composed,
relying on the categories found in several dictionar-
ies and on the Penn treebank POS guidelines (San-
torini, 1995). Tagging was done using an automatic
tool1. We relied on existing computational lexicons
(Segal, 2000; Yona, 2004) to generate candidate tags
for each word. As many words from the corpus were
either missing or tagged in a non uniform manner in
the lexicons, we recommended looking up missing
words in traditional dictionaries. Disagreement was
also found among copyrighted dictionaries, both for
open and closed set categories. Given the lack of
a reliable lexicon, the taggers were not given a list
of options to choose from, but were free to tag with
whatever tag they found suitable. The process, al-
though slower and bound to produce unintentional
mistakes, was used for building a lexicon, and to
refine the guidelines and on occasion modify the
POS tagset. When constructing and then amending
the guidelines we sought the best trade-off between
1http://wordfreak.sourceforge.net
57
accuracy and meaningfulness of the categorization,
and simplicity of the guidelines, which is important
for consistent tagging.
Initially, each text was tagged by four different
people, and the guidelines were revised according
to questions or disagreements that were raised. As
the guidelines became more stable, the disagreement
rate decreased, each text was tagged by three peo-
ple only and eventually two taggers and a referee
that reviewed disagreements between the two. The
disagreement rate between any two taggers was ini-
tially as high as 20%, and dropped to 3% after a few
rounds of tagging and revising the guidelines.
Major sources of disagreements that were identi-
fied, include:
Prepositional phrases vs. prepositions In Hebrew,
formative letters ?      b,c,l,m2 ? can be attached
to a noun to create a short prepositional phrase. In
some cases, such phrases function as a preposition
and the original meaning of the noun is not clearly
felt. Some taggers would tag the word as a prepo-
sitional prefix + noun, while others tagged it as a
preposition, e.g., 
	 
 b?iqbot (following), that
can be tagged as 	 
 b-iqbot (in the footsteps
of).
Adverbial phrases vs. Adverbs the problem is simi-
lar to the one above, e.g.,  	  bdiyuq (exactly), can
be tagged as b-diyuq (with accuracy).
Participles vs. Adjectives as both categories can
modify nouns, it is hard to distinguish between
them, e.g,   Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 32?39,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Gaiku : Generating Haiku with Word Associations Norms
Yael Netzer? and David Gabay and Yoav Goldberg? and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yaeln,gabayd,yoavg,elhadad}@cs.bgu.ac.il
Abstract
creativeness / a pleasing field / of bloom
Word associations are an important element
of linguistic creativity. Traditional lexical
knowledge bases such as WordNet formalize
a limited set of systematic relations among
words, such as synonymy, polysemy and hy-
pernymy. Such relations maintain their sys-
tematicity when composed into lexical chains.
We claim that such relations cannot explain
the type of lexical associations common in
poetic text. We explore in this paper the
usage of Word Association Norms (WANs)
as an alternative lexical knowledge source
to analyze linguistic computational creativity.
We specifically investigate the Haiku poetic
genre, which is characterized by heavy re-
liance on lexical associations. We first com-
pare the density of WAN-based word asso-
ciations in a corpus of English Haiku po-
ems to that of WordNet-based associations as
well as in other non-poetic genres. These
experiments confirm our hypothesis that the
non-systematic lexical associations captured
in WANs play an important role in poetic text.
We then present Gaiku, a system to automat-
ically generate Haikus from a seed word and
using WAN-associations. Human evaluation
indicate that generated Haikus are of lesser
quality than human Haikus, but a high propor-
tion of generated Haikus can confuse human
readers, and a few of them trigger intriguing
reactions.
? Supported by Deutsche Telekom Laboratories at Ben-
Gurion University of the Negev.
? Supported by the Lynn and William Frankel Center for
Computer Sciences.
1 Introduction
Traditional lexical knowledge bases such as Word-
Net formalize a limited set of systematic relations
that exist between words, such as synonymy, pol-
ysemy, hypernymy. When such relations are com-
posed, they maintain their systematicity, and do not
create surprising, unexpected word associations.
The human mind is not limited to such system-
atic relations, and people tend to associate words to
each other with a rich set of relations, such as non
systematic paradigmatic (doctor-nurse) and syntag-
matic relations (mash-potato) as identified by Saus-
sure (1949). Such associations rely on cultural
(mash-television), emotional (math - yuck) and per-
sonal experience (autumn - Canada).
In linguistic creativity, such as prose or poetry
writing, word associations play an important role
and the ability to connect words into new, unex-
pected relations is one of the key mechanisms that
triggers the reader involvement.
We explore in this paper the usage of Word As-
sociation Norms (WANs) as an alternative lexical
knowledge source to analyze linguistic computa-
tional creativity. WANs have been developed in psy-
chological research in the past 40 years. They record
typical word associations evoked by people when
they are submitted a trigger word. Such associations
(e.g., table to chair or cloth) are non-systematic, yet
highly stable across people, time (over a period of 30
years) and languages. WANs have been compiled in
various languages, and provide an interesting source
to analyze word associations in creative writing.
We specifically investigate the Haiku poetic
32
genre, which is characterized by heavy reliance on
lexical associations. The hypothesis we investigate
is that WANs play a role in computational creativ-
ity, and better explain the type of word associations
observed in creative writing than the systematic re-
lations found in thesauri such as WordNet.
In the rest of the paper, we refine our hypothe-
sis and present observations on a dataset of English
Haikus we collected. We find that the density of
WAN-based word associations in Haikus is much
higher than in other genres, and also much higher
than the density of WordNet-based associations. We
then present Gaiku, a system we developed to auto-
matically generate Haikus from a seed word using
word association norms. Evaluation we performed
with a group of 60 human readers indicates that the
generated Haikus exhibit interesting creative charac-
teristics and sometimes receive intriguing acclaim.
2 Background and Previous Work
2.1 Computational Creativity
Computational creativity in general and linguistic in
particular, is a fascinating task. On the one hand, lin-
guistic creativity goes beyond the general NLP tasks
and requires understanding and modelling knowl-
edge which, almost by definition, cannot be formal-
ized (i.e., terms like beautiful, touching, funny or in-
triguing). On the other hand, this vagueness itself
may enable a less restrictive formalization and allow
a variety of quality judgments. Such vague formal-
izations are naturally more useful when a computa-
tional creativity system does not attempt to model
the creativity process itself, but instead focuses on
?creative products? such as poetry (see Section 2.3),
prose and narrative (Montfort, 2006), cryptic cross-
word clues (Hardcastle, 2007) and many others.
Some research focus on the creative process itself
(see (Ritchie, 2006) for a comprehensive review of
the field). We discuss in this paper what Boden
(1998) calls P-Creativity (Psychological Creativity)
which is defined relative to the initial state of knowl-
edge, and H-Creativity (Historical Creativity) which
is relative to a specific reference culture. Boden
claims that, while hard to reproduce, exploratory
creativity is most successful in computer models of
creativity. This is because the other kinds of creativ-
ity are even more elusive due to the difficulty of ap-
proaching the richness of human associative mem-
ory, and the difficulty of identifying our values and
of expressing them in computational form.
We investigate in our work one way of addressing
this difficulty: we propose to use associative data as
a knowledge source as a first approximation of hu-
man associative capabilities. While we do not ex-
plain such associations, we attempt to use them in
a constructive manner as part of a simple combina-
tional model of creativity in poetry.
2.2 Word Associations and Creativity
Associations and creativity are long known to be
strongly connected. Mendick (Mendick, 1969) de-
fines creative thinking as ?the forming of associative
elements into new combinations which either meet
specified requirements or are in some way useful.?
The usefulness criterion distinguishes original think-
ing from creative thinking. A creative solution is
reached through three main paths: serendipity (ran-
dom stimuli evoke associative elements), similar-
ity (stimuli and solution are found similar through
an association) and mediation (both ?problem? and
?solution? can be associated to similar elements).
In our work, we hypothesize that interesting Haiku
poems exhibit creative word associations. We rely
on this hypothesis to first generate candidate word
associations starting from a seed word and follow-
ing random walks through WANs, but also to rank
candidate Haiku poems by measuring the density of
WAN-based associations they exhibit.
2.3 Poetry Generation
Although several automatic and semi-automatic po-
etry generation systems were developed over the
years, most of them did not rise above the level of
?party tricks? (Manurung et al, 2000). In his the-
sis, (Manurung, 2003), defined a poem to be a text
that meets three properties: meaningfulness, gram-
maticality and poeticness. Two of the few systems
that attempt to explicitly represent all three prop-
erties are reported in (Gervas, 2001) and (D??az-
Agudo et al, 2002). Both systems take as input a
prose message provided by the user, and translate it
into formal Spanish poetry. The system proposed
in (Manurung et al, 2000) is similar in that it fo-
cuses on the syntactic and phonetic patterns of the
poem, putting less stress on the semantics. The sys-
33
tem starts with a simple seed and gradually devel-
ops a poem, by making small syntactic and semantic
changes at every step.
Specifically in the subfield of Haiku generation,
the Haiku generator presented in (Wong and Chun,
2008) produces candidate poems by combining lines
taken from blogs. The system then ranks the can-
didates according to semantic similarity, which is
computed using the results returned by a search en-
gine when querying for words in each line. Hitch-
Haiku (Tosa et al, 2008), another Haiku generation
system, starts from two seed words given by the user.
It retrieves two phrases containing these words from
a corpus, and then adds a third phrase that connects
both input words, using lexical resources.
In our work, we induce a statistical language
model of the structure of Haikus from an analysis
of a corpus of English Haikus, and explore ways to
combine chains of lexical associations into the ex-
pected Haiku syntactic structure. The key issues we
investigate are the importance of WAN-based asso-
ciations in the Haiku generation process, and how a
chain of words, linked through WAN-based associa-
tions, can be composed into a Haiku-like structure.
2.4 Haiku
Haiku is a form of poetry originated in Japan in
the sixteenth century. The genre was adopted in
Western languages in the 20th Century. The origi-
nal form of a poem is of three lines of five, seven
and five syllables (although this constraint is loos-
ened in non-Japanese versions of Haiku (Gilbert and
Yoneoka, 2000)). Haiku, by its nature, aims to re-
flect or evoke emotion using an extremely economi-
cal linguistic form; most Haiku use present tense and
use no judgmental words; in addition, functional or
syntactic words may be dropped. Traditional Haiku
involve reference to nature and seasons, but modern
and western Haiku are not restricted to this theme1.
We adopt the less ?constraining? definition of the
author Jack Kerouac (2004) for a Haiku ?I propose
that the ?Western Haiku? simply say a lot in three
short lines in any Western language. Above all, a
Haiku must be very simple and free of all poetic
1Senryu poetry, similar in form to Haiku, is the Japanese
genre of poems that relate to human and relationships, and may
be humorous. Hereafter, we use Haiku for both the original
definition and the Senryu as well.
trickery and make a little picture and yet be as airy
and graceful as a Vivaldi Pastorella.? (pp. x-xi). In
addition, we are guided by the saying ? The best
haiku should leave the reader wondering ? (Quoted
in (Blasko and Merski, 1998))
2.5 Word Association Norms
The interest in word associations is common to
many fields. Idiosyncrasy of associations was used
as a diagnostic tool at the beginning of the 20th cen-
tury, but nowadays the majority of approaches deal
less with particular associations and more with gen-
eral patterns in order to study the structure of the
mental lexicon and of semantic memory (Rubinsten
et al, 2005).
Word Association Norms (WAN) are a collection
of cue words and the set of free associations that
were given as responses to the cue, accompanied
with quantitative and statistical measures. Subjects
are given a word and asked to respond immediately
with the first word that comes to their mind. The
largest WAN we know for English is the University
of South Florida Free Association Norms (Nelson et
al., 1998).
Word Association Norms and Thesauri in NLP
Sinopalnikova and Smrz (2004) have shown that
when building and extending semantic networks,
WANs have advantages over corpus-based meth-
ods. They found that WANs cover semantic rela-
tions that are difficult to acquire from a corpus: 42%
of the non-idiosyncratic cue-target pairs in an En-
glish WAN never co-appeared in a 10 words win-
dow in a large balanced text corpus. From the point
of view of computational creativity, this is encourag-
ing, since it suggests that association-based content
generation can lead to texts that are both sensible
and novel. (Duch and Pilichowski, 2007)?s work,
from a neuro-cognitive perspective, generates neol-
ogisms based, among other data, on word associa-
tion. (Duch and Pilichowski, 2007) sums ?creativity
requires prior knowledge, imagination and filtering
of the results.?
3 WordNet vs. Associations
Word association norms add an insight on language
that is not found in WordNet or are hard to acquire
from corpora, and therefore can be used as an ad-
ditional tool in NLP applications and computational
34
creativity.
We choose the Haiku generation task using word
associations, since this genre of poetry encapsulates
meaning in a special way. Haiku tend to use words
which are connected through associative or phono-
logical connections (very often ambiguous).
We hypothesize that word-associations are good
catalyzers for creativity, and use them as a building
block in the creative process of Haiku generation.
We first test this hypothesis by analyzing a corpus of
existing Haiku poems.
3.1 Analyzing existing text
Can the creativity of text as reflected in word as-
sociations be quantified? Are Haiku poems indeed
more associative than newswire text or prose? If
this is the case, we expect Haiku to have more asso-
ciative relations, which cannot be easily recovered
by WordNet than other type of text. We view the
WAN as an undirected graph in which the nodes
are stemmed words, and two nodes are connected
iff one of them is a cue for the other. We take the
associative distance between two words to be the
number of edges in the shortest path between the
words in the associations-graph. Interestingly, al-
most any word pair in the association graph is con-
nected with a path of at most 3 edges. Thus, we
take two words to be associatively related if their
associative distance is 1 or 2. Similarly, we define
the WordNet distance between two stemmed words
to be the number of edges in the shortest path be-
tween any synset of one word to any synset of the
other word2. Two words are WordNet-related if their
WordNet distance is less than 4 (this is consistent
with works on lexical-cohesion, (Morris and Hirst,
1991)).
We take the associativity of a piece of text to be
the number of associated word pairs in the text, nor-
malized by the number of word pairs in the text of
which both words are in the WAN.3 We take the
WordNet-relations level of a piece of text to be the
number of WordNet-related word pairs in the text.
2This is the inverse of the path-similarity measure of (Ped-
ersen et al, 2004).
3This normalization is performed to account for the limited
lexical coverage of the WAN. We don?t want words that appear
in a text, but are not covered by the WAN, to affect the associa-
tivity level of the text.
SOURCE AVG. ASSOC AVG. WORDNETRELATIONS (<3) RELATIONS (<4)
News 0.26 2.02
Prose 0.22 1.4
Haiku 0.32 1.38
Table 1: Associative and WordNet relations in various
text genres
We measure the average associativity and Word-
Net levels of 200 of the Haiku in our Haiku Cor-
pus (Section 4.1), as well as of random 12-word
sequences from Project Gutenberg and from the
NANC newswire corpus.
The results are presented in Table 1.
Perhaps surprisingly, the numbers for the Guten-
berg texts are lower on all measures. This is at-
tributed to the fact that Gutenberg texts have many
more pronouns and non-content words than the
Haiku and newswire text. Haiku text appears to
be more associative than newswire text. Moreover,
newswire documents have many more WordNet-
relations than the Haiku poems ? whenever words
are related in Haiku, this relatedness tends to be cap-
tured via the association network rather than via the
WordNet relations. The same trend is apparent also
when considering the Gutenberg numbers: they have
about 15% less associations than newswire text, but
about 30% less WordNet-relations. This supports
the claim that associative information which is not
readily available in WordNet is a good indicator of
creative content.
3.2 Generating creative content
We now investigate how word-associations can help
in the process of generating Haikus. We define
a 5 stage generative process: theme selection in
which the general theme of the Haiku is decided,
syntactic planning, which sets the Haiku form and
syntactic constraints, content selection / semantic
planning which combines syntactic and aesthetic
constraints with the theme selected in the previous
stages to form good building blocks, filtered over-
generation of many Haiku based on these selected
building blocks, and finally re-ranking of the gen-
erated Haiku based on external criteria.
The details of the generation algorithm are pre-
sented in Section 4.2. Here we focus on the creative
aspect of this process ? theme selection. Our main
claim is that WANs are a good source for interest-
35
ing themes. Specifically, interesting themes can be
obtained by performing a short random walk on the
association graph induced by the WAN network.
Table 2 presents the results of several random
walks of 3 steps starting from the seed words ?Dog?,
?Winter?, ?Nature? and ?Obsession?. For compar-
ison, we also present the results of random walks
over WordNet glosses for the same seeds.
We observe that the association network is bet-
ter for our needs than WordNet. Random walks in
WordNet are more likely to stay too close to the seed
word, limiting the poetic options, or to get too far
and produce almost random connections.
4 Algorithm for generating Haiku
4.1 Dataset
We used the Word Association Norms (WAN) of the
University of South Florida 4 (Nelson et al, 1998)
for discovering associations of words. The dataset
(Appendix A, there) includes 5,019 cue words and
10,469 additional target that were collected with
more than 6,000 participants since 1973.
We have compiled a Haiku Corpus, which in-
cludes approximately 3,577 Haiku in English of var-
ious sources (amateurish sites, children?s writings,
translations of classic Japanese Haiku of Bashu and
others, and ?official? sites of Haiku Associations
(e.g., Haiku Path - Haiku Society of America).
For the content selection part of the algorithms,
we experimented with two data sources: a corpus of
1TB web-based N-grams supplied by Google, and
the complete text of Project Gutenberg. The Guten-
berg data has the advantage of being easier to POS-
tag and contains less restricted-content, while the
Google Web data is somewhat more diverse.
4.2 Algorithm Details
Our Haiku generation algorithm includes 5 stages:
theme selection, syntactic planning, content selec-
tion, filtered over generation, and ranking.
The Theme Selection stage is in charge of dictat-
ing the overall theme of our Haiku. We start with
a user-supplied seed word (e.g. WINTER). We then
consult the Association database in order to enrich
the seed word with various associations. Ideally, we
would like these associations to be close enough to
4http://w3.usf.edu/FreeAssociation/
the seed word to be understandable, yet far enough
away from it as to be interesting. After some ex-
perimenting, we came up with the following heuris-
tic, which we found to provide adequate results. We
start with the seed word, and conduct a short random
walk on the associations graph. Each random step
is comprised of choosing a random direction (either
?Cue? or ?Target?) using a uniform distribution, and
then a random neighbor according to its relative fre-
quency. We conduct several (8) such walks, each
with 3 steps, and keep all the resulting words. This
gives us mostly close, probable associations, as well
as some less probable, further away from the seed.
The syntactic planning stage determines the
form of the generated Haiku, setting syntactic and
aesthetic constraints for the generative process. This
is done in a data-driven way by considering common
line patterns from our Haiku corpus. In a training
stage, we POS-tagged each of the Haiku, and then
extracted a pattern from each of the Haiku lines. A
line-pattern is a sequence of POS-tags, in which the
most common words are lexicalized to include the
word-form in addition to the POS-tag. An example
for such a line pattern might be DT the JJ NN.
We kept the top-40 frequent patterns for each of the
Haiku lines, overall 120 patterns. When generating a
new Haiku, we choose a random pattern for the first
line, then choose the second line pattern conditioned
on the first, and the third line pattern conditioned
on the second. The line patterns are chosen with a
probability proportional to their relative frequencies
in the training corpus. For the second and third lines
we use the conditional probabilities of a pattern ap-
pearing after the previous line pattern. The result
of this stage is a 3-line Haiku skeleton, dictating the
number of words on each line, their POS-tags, and
the placement of specific function words.
In the Content Selection stage, we look for pos-
sible Haiku lines, based on our selected theme and
syntactic structure. We go over our candidate lines5,
and extract lines which match the syntactic patterns
and contain a stemmed appearance of one of the
stemmed theme words. In our current implemen-
tation, we require the first line to contain the seed
word, and the second and third line to contain any of
5These are POS-tagged n-grams extracted from a large text
corpora: the Google T1 dataset or Project Gutenberg
36
SEED WAN WORDNET
Dog puppy adorable cute heel villain villainess
Dog cat curious george hound scoundrel villainess
Winter summer heat microwave wintertime solstice equinox
Winter chill cold alergy midwinter wintertime season
Nature animals instinct animals world body crotch
Nature natural environment surrounding complexion archaism octoroon
Obsession cologne perfume smell fixation preoccupation thought
Obsession compulsion feeling symptom compulsion onomatomania compulsion
Table 2: Some random walks on the WordNet and WAN induced graphs
the theme words. Other variations, such as choos-
ing a different word set for each line, are of course
possible.
The over generation stage involves creating
many possible Haiku candidates by randomly
matching lines collected in the content selection
stage. We filter away Haiku candidates which have
an undesired properties, such as repeating the same
content-word in two different lines.
All of the generated Haiku obey the syntactic and
semantic constraints, but not all of them are interest-
ing. Thus, we rank the Haiku in order to weed out
the better ones. The top-ranking Haiku is the output
of our system. Our current heuristic prefers highly
associative Haikus. This is done by counting the
number of 1st and 2nd degree associations in each
Haiku, while giving more weight to 2nd degree as-
sociations in order to encourage ?surprises?. While
all the candidate Haiku were generated based on a
common theme of intended associative connections,
the content selection and adherence to syntactic con-
straints introduce additional content words and with
them some new, unintended associative connections.
Our re-ranking approach tries to maximize the num-
ber of such connections.6
5 Evaluation
The ultimate goal of a poetry generation system is to
produce poems that will be considered good if writ-
ten by a human poet. It is difficult to evaluate to what
extent a poetry generation system can meet this goal
(Ritchie, 2001; Manurung et al, 2000). Difficulties
arise from two major sources: first, since a creative
6While this heuristic works well, it leaves a lot to be desired.
It considers only the quantity of the associations, and not their
quality. Indeed, when looking at the Haiku candidates produced
in the generation stage, one can find many interesting pieces,
where some of the lower ranking ones are far better than the top
ranking.
work should be novel, it cannot be directly evaluated
by comparison to some gold standard. Second, it is
hard for people to objectively evaluate the quality of
poetry. Even determining whether a text is a poem
or not is not an easy task, as readers expect poetry
to require creative reading, and tolerate, to some ex-
tent, ungrammatical structures or cryptic meaning.
5.1 ?Turing Test? Experiment
To evaluate the quality of Gaiku, we asked a group
of volunteers to read a set of Haiku, indicate how
much they liked each one (on a scale of 1-5), and
classify each Haiku as written by a human or by a
computer.
We compiled two sets of Haiku. The first set
(AUTO) contained 25 Haiku. 10 Haiku chosen at
random from our Haiku corpus, and 15 computer
generated ones. The computer generated Haiku
were created by identifying the main word in the first
line of each human-written Haiku, and passing it as
a seed word to the Haiku generation algorithm (in
case a first line in human-written Haiku contained
two main words, two Haiku were generated). We in-
cluded the top-ranking Haiku returning from a single
run of the system for each seed word. The only hu-
man judgement in compiling this set was in the iden-
tification of the main words of the human Haiku.
The second set (SEL) was compiled of 9 haiku po-
ems that won awards7, and 17 computer Haiku that
were selected by us, after several runs of the auto-
matic process. (Again, each poem in the automatic
poems set shared at least one word with some poem
in the human Haiku set).
The subjects were not given any information
about the number of computer-generated poems in
the sets.
7Gerald Brady Memorial Award Collection http://www.hsa-
haiku.org/bradyawards/brady.htm 2006-2007
37
The AUTO questionnaire was answered by 40
subjects and the SEL one by 22. (Altogether, 52 dif-
ferent people took part in the experiment, as some
subjects answered both versions). The subjects were
all adults (age 18 to 74), some were native English
speakers and others were fully fluent in English. Ex-
cept a few, they did not have academic background
in literature.
5.2 Results and Discussion
Results are presented in Table 3 and Figure 1.
Overall, subjects were correct in 66.7% of their
judgements in AUTO and 61.4% in SEL. The aver-
age grade that a poem - human or machine-made -
received correlates with the percentage of subjects
who classified it as human. The average grade and
rate of acceptance as written by human were signifi-
cantly higher for the Haiku written by people. How-
ever, some computer Haiku rivaled the average hu-
man poem in both measures. This is true even for
AUTO, in which both the generation and the selec-
tion processes were completely automatic. The best
computer Haiku of SEL scored better than most hu-
man Haiku in both measures.
The best computer poem in SEL was:
early dew / the water contains / teaspoons of honey
which got an average grade of 3.09 and was classi-
fied as human by 77.2% of the subjects.
At the other extreme, the computer poem (SEL):
space journey / musical instruments mythology /
of similar drugs
was classified as human by only 9% of the subjects,
and got an average grade of 2.04.
The best Haiku in the AUTO set was:
cherry tree / poisonous flowers lie / blooming
which was classified as human by 72.2% of the sub-
jects and got an average grade of 2.75.
The second human-like computer generated
Haiku in each set were:
spring bloom / showing / the sun?s pyre
(AUTO, 63.8% human) and:
blind snakes / on the wet grass / tombstoned terror
(SEL, 77.2% human).
There were, expectedly, lots of disagreements.
Poetry reading and evaluation is subjective and by
Human Poems Gaiku
AUTO avg. % classified as Human 72.5% 37.2%
avg. grade 2.86 2.11
SEL avg. % classified as Human 71.7% 44.1%
avg. grade 2.84 2.32
Table 3: Turing-test experiment results
itself (in particular for Haiku) a creative task. In ad-
dition, people have very different ideas in mind as to
a computer?s ability to do things. (One subject said,
for example, that the computer generated
holy cow / a carton of milk / seeking a church
is too stupid to be written by a computer; how-
ever, content is very strongly connected and does
not seem random). On the other end, subjects often
remarked that some of the human-authored Haiku
contained metaphors which were too obvious to be
written by a human.
Every subject was wrong at least 3 times (at least
once in every direction); every poem was wrongly-
classified at least once. Some really bad auto-poems
got a good grade here and there, while even the most
popular human poems got a low grade sometimes.
6 Discussion and Future Work
Word association norms were shown to be a useful
tool for a computational creativity task, aiding in the
creation of an automatic Haiku-generation software,
which is able to produce ?human-like? Haiku. How-
ever, associations can be used for many other tasks.
In the last decade, lexical chains are often used in
various NLP tasks such as text summarization or text
categorization; WordNet is the main resource for
detecting the cohesive relationships between words
and their relevance to a given chain (Morris and
Hirst, 1991). We believe that using word association
norms can enrich the information found in WordNet
and enable the detection of more relevant words.
Another possible application is for assisting
word-finding problem of children with specific lan-
guage impairments (SLI). A useful tactic practiced
as an assistance to retrieve a forgotten word is by
saying all words that come to mind. The NLP task,
therefore, is for a set of a given associations, recon-
struct the targeted word.
38
0 20 40 60 80 100
1.5
2
2.5
3
3.5
4
% of subjects who classified the poem as written by a human
Av
ar
eg
e 
gr
ad
e
 
 
Gaiku poems
Human poems
0 20 40 60 80 100
1.5
2
2.5
3
3.5
4
% of subjects who classified the poem as written by a human
Av
ar
eg
e 
gr
ad
e
 
 
Gaiku poems
Human poems
Figure 1: Average grades and percentages of subjects who classified poems as written by humans, for AUTO (left)
and SEL. Circles represent Haiku written by people, and stars represent machine-made Haiku
References
D.G. Blasko and D.W. Merski. 1998. Haiku poetry
and metaphorical thought: An invention to interdisci-
plinary study. Creativity Research Journal, 11.
M.A. Boden. 1998. Creativity and artificial intelligence.
Artificial Intelligence, 103(1?2).
F. de Saussure, C. Bally, A. Riedlinger, and
A. Sechehaye. 1949. Cours de linguistique gen-
erale. Payot, Paris.
B. D??az-Agudo, P. Gerva?s, and P. A. Gonza?lez-Calero.
2002. Poetry generation in COLIBRI. In Proc. of EC-
CBR.
W. Duch and M. Pilichowski. 2007. Experiments with
computational creativity. Neural Information Process-
ing, Letters and Reviews, 11(3).
P. Gervas. 2001. An expert system for the composition of
formal Spanish poetry. Journal of Knowledge-Based
Systems, 14.
R. Gilbert and J. Yoneoka. 2000. From 5-7-5 to 8-8-8:
An investigation of Japanese Haiku metrics and impli-
cations for English Haiku. Language Issues: Journal
of the Foreign Language Education Center.
D. Hardcastle. 2007. Cryptic crossword clues: Generat-
ing text with a hidden meaning BBKCS-07-04. Tech-
nical report, Birkbeck College, London.
J. Kerouac. 2004. Book of Haikus. Enitharmon Press.
H.M. Manurung, G. Ritchie, and H. Thompson. 2000.
Towards a computational model of poetry generation.
In Proc. of the AISB?00.
H.M. Manurung. 2003. An evolutionary algorithm ap-
proach to poetry generation. Ph.D. thesis, University
of Edinburgh.
S.A. Mendick. 1969. The associative basis of the cre-
ative process. Psychological Review.
N. Montfort. 2006. Natural language generation and nar-
rative variation in interactive fiction. In Proc. of Com-
putational Aesthetics Workshop at AAAI 2006, Boston.
J. Morris and G. Hirst. 1991. Lexical cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17.
D.L. Nelson, C.L. Mcevoy, and T.A. Schreiber.
1998. The University of South Florida Word
Association, Rhyme, and Word Fragment Norms.
http://www.usf.edu/FreeAssociation/.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In HLT-NAACL 2004: Demonstrations.
G. Ritchie. 2001. Assessing creativity. In Proc. of
AISB?01 Symposium.
G. Ritchie. 2006. The transformational creativity hy-
pothesis. New Generation Computing, 24.
O. Rubinsten, D. Anaki, A. Henik, S. Drori, and Y. Faran.
2005. Free association norms in the Hebrew language.
Word Norms in Hebrew. (In Hebrew).
A. Sinopalnikova and P. Smrz. 2004. Word association
thesaurus as a resource for extending semantic net-
works. In Communications in Computing.
N. Tosa, H. Obara, and M. Minoh. 2008. Hitch haiku:
An interactive supporting system for composing haiku
poem. In Proc. of the 7th International Conference on
Entertainment Computing.
M. Tsan Wong and A. Hon Wai Chun. 2008. Automatic
Haiku generation using vsm. In Proc. of ACACOS?08,
April.
39
