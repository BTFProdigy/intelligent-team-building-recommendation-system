Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 245?256, Dublin, Ireland, August 23-29 2014.
High Performance Word Sense Alignment by Joint Modeling of Sense
Distance and Gloss Similarity
Michael Matuschek
?
and Iryna Gurevych
??
? Ubiquitous Knowledge Processing Lab (UKP-DIPF),
German Institute for Educational Research and Educational Information
Schlo?str. 29, 60486 Frankfurt, Germany
?
Ubiquitous Knowledge Processing Lab (UKP-TUDA),
Department of Computer Science, Technische Universit?at Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we present a machine learning approach for word sense alignment (WSA) which
combines distances between senses in the graph representations of lexical-semantic resources
with gloss similarities. In this way, we significantly outperform the state of the art on each of the
four datasets we consider. Moreover, we present two novel datasets for WSA between Wiktionary
and Wikipedia in English and German. The latter dataset in not only of unprecedented size, but
also created by the large community of Wiktionary editors instead of expert annotators, making
it an interesting subject of study in its own right as the first crowdsourced WSA dataset. We will
make both datasets freely available along with our computed alignments.
1 Introduction
Lexical-semantic resources (LSRs) are an important foundation for numerous natural language process-
ing (NLP) tasks such as word sense disambiguation (WSD) or information extraction (IE). However,
large-scale LSRs are only available for a few languages. The Princeton WordNet (Fellbaum, 1998)
is commonly used for English, but for most languages such resources are small or missing altogether.
Another problem is that, even for English, there is no single LSR which is suitable for all different
application scenarios, because the resources contain different words, senses or even information types.
Recently, it has been argued that collaboratively constructed resources (e.g. Wiktionary (Meyer and
Gurevych, 2012))) are a viable alternative, especially for smaller languages (Matuschek et al., 2013), but
there are still considerable drawbacks in coverage which make their usage challenging.
These observations have led to the insight that word sense alignment (WSA), i.e. linking at the level
of word senses, is key for the efficient exploitation of LSRs, and it was shown that the usage of linked
resources can indeed yield performance improvements. Examples include WSD using aligned Word-
Net and Wikipedia (Navigli and Ponzetto, 2012a), semantic role labeling using PropBank, VerbNet and
FrameNet (Palmer, 2009), the construction of a semantic parser using FrameNet, WordNet, and VerbNet
(Shi and Mihalcea, 2005) and IE using WordNet and Wikipedia (Moro et al., 2013). Cholakov et al.
(2014) address the special task of verb sense disambiguation. They use the large-scale resource UBY
(Gurevych et al., 2012) which contains nine resources in two languages, mapped to a uniform represen-
tation using the LMF standard for interoperability (Eckle-Kohler et al., 2012), and also (among others)
sense alignments between WordNet, FrameNet, VerbNet and Wiktionary which are exploited in their
approach.
However, WSA is challenging because of word ambiguities, different sense granularities and informa-
tion types (Navigli, 2006), so that past efforts mostly focused on specific resources or applications, where
expert-built resources such as WordNet played a central role in most cases. Approaches which aim at
being more generic (i.e. applicable to a wider range of LSRs) usually focused on only one information
source for the alignment (e.g. glosses or graph structures) without combining them in an elaborate way.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
245
In this paper, we want to go beyond this previous work in two ways: i) For the first time, we present
an alignment between the large-scale collaboratively constructed resources Wiktionary and Wikipedia.
While both LSRs have been extensively used in NLP and especially WSA (see Section 2), no attempt has
been made to combine them, although Wiktionary was explicitly designed to complement the encyclope-
dic knowledge in Wikipedia with linguistic knowledge. Apart from already established tasks like WSD,
the strong multilingual focus of both resources makes their combination especially promising for appli-
cations such as knowledge-based machine translation or computer-assisted translation where additional
background knowledge and translation options can be crucial (Matuschek et al., 2013). To fill this gap in
the body of research, we present two new evaluation datasets for English and German, where the latter
is not only of remarkable size, but also directly extracted from Wiktionary in a novel approach, making
it the first crowdsourced WSA dataset. ii) Also for the first time, we jointly model different aspects of
sense similarity by applying machine learning techniques to WSA. However, unlike previous approaches,
we do not engineer our features towards a specific resource pair, rendering the approach powerful but
proprietary. Instead, we aim to combine generic features which are applicable to a variety of resources,
and we show that combining them leads to state-of-the-art WSA performance. In particular, we employ
distances calculated with Dijkstra-WSA (Matuschek and Gurevych, 2013), an algorithm which works
on graph representations of resources, as well as gloss similarity values. This lets us take advantage
of both (orthogonal) ways of identifying equivalent senses and yields a very robust and flexible WSA
framework.
The rest of this paper is structured as follows: In Section 2 we discuss related work, in Section 3 we
describe our approach and introduce the resources and datasets we use in our experiments, in Section 4
we evaluate our results, and we conclude in Section 5 with some directions for future work.
2 Related Work
There are two main approaches to WSA which have been applied: Similarity-based and graph-based
ones. To our knowledge, there exists no previous work which effectively combines both approaches in a
unified framework, and only few works which combine both kinds of features for different purposes.
2.1 Similarity-based Approaches
WordNet was aligned to Wikipedia (Niemann and Gurevych, 2011) and Wiktionary (Meyer and
Gurevych, 2011) using a framework based on gloss similarity, in spirit of the earliest work in WSD
presented by Lesk (1986). In both cases, cosine and personalized PageRank (PPR) similarity (Agirre
and Soroa, 2009) were calculated, and a simple machine learning approach was used to classify each
pair of senses (see Section 3.3). This idea was also applied to cross-lingual alignment between WordNet
and the German part of OmegaWiki (Gurevych et al., 2012), using machine translation as an intermediate
component. Henrich et al. (2011) use a similar approach for aligning GermaNet and Wiktionary, but with
word overlap as the similarity measure. De Melo and Weikum (2010) report an alignment of WordNet
synsets to Wikipedia articles which is also based on word overlap. We later report results based on gloss
similarity as one of our baselines (Tables 2 and 3).
2.2 Graph-based Approaches
In one of the earliest structure-based works, Daud?e et al. (2003) map different versions of WordNet based
on the synset hierarchy. Navigli (2009) disambiguates WordNet glosses, i.e. sense markers are assigned
to all non-stopwords in each WordNet gloss. The approach is based on finding circles in the WordNet
relation graph to identify disambiguations. In later work, this idea was applied to the disambiguation
of translations in a bilingual dictionary (Flati and Navigli, 2012). While this ?alignment? of dictionary
entries is related to our problem, it was not discussed how this idea could be applied to word sense
alignment of two resources. Laparra et al. (2010) use a shortest path algorithm (SSI-Dijkstra+) to align
FrameNet lexical units (LUs) with WordNet synsets. They align monosemous LUs first and then search
for the closest synset in WordNet for the other LUs in the same frame. The LUs are, however, considered
as mere texts to be disambiguated; there is no attempt made to exploit the graph structure of FrameNet.
246
Ponzetto and Navigli (2009) use a graph-based method for aligning WordNet synsets and Wikipedia
categories. Using semantic relations, they build subgraphs of WordNet for each category and then align
senses to categories based on the structural features. In our own previous work, we presented Dijkstra-
WSA, a graph-based approach working with shortest paths (Matuschek and Gurevych, 2013). It achieves
state-of-the-art precision, but recall is an issue if the graphs are sparse (i.e. in case of only few semantic
relations). As Dijkstra-WSA distances are one of the features we use for our machine learning approach,
we will present this approach in more detail in section 3.2.2 and also report results for Dijkstra-WSA on
our evaluation datasets for comparison.
2.3 Hybrid Approaches
In later work, Navigli and Ponzetto (2012a) also align WordNet with the full Wikipedia. Besides using
bag-of-words overlap to compute gloss similarity, they also build a graph structure for the senses in both
resources by using WordNet semantic relations. The goal is to determine which WordNet sense is closest
to the Wikipedia sense to be aligned. However, the graph structure of Wikipedia is disregarded, as is the
global structure of WordNet, as just a locally restricted subset of WordNet relations is used. In the same
context of BabelNet, Navigli and Ponzetto (2012b) also present BabelRelate, an approach which relies on
translations to compute cross-lingual semantic similarity; however, they do not apply it to WSA. Dijkstra-
WSA was enhanced by using a backoff, by means of performing a graph-based alignment first, and in
cases where no alignment target sense can be found, a decision is made based on the similarity of glosses
(Matuschek and Gurevych, 2013). While this simple two-step approach increases recall substantially,
it comes at the expense of lower precision. However, the overall F-measure achieved state-of-the-art
performance on every considered dataset (0.65?0.87). We also report the results for this hybrid approach
as a baseline (Tables 2 and 3). De Melo and Weikum (2008) use a machine learning approach with a
combination of structural and content-based features of WordNet, but for building new wordnets in other
languages, not aligning existing ones.
In summary, the different approaches to compute similarity have mostly been used in isolation, or
combined in a shallow or restricted way. More complex approaches usually require resource-specific
feature engineering, which makes their transferability to other resources or languages difficult. Thus,
we present a framework which combines different similarity measures in a generic and flexible way and
enables state-of-the-art WSA performance on a variety of resources with modest effort.
3 The Alignment Procedure
The basic steps of our alignment algorithm are:
1. For each sense in one resource, all possible candidates in the other resource are retrieved. Can-
didates are senses which have the same attached lemma and part of speech. For instance, for the
programming sense of Java in one resource, their might exist senses for programming, island or
coffee in the other one which are all possible alignment targets.
2. For each candidate pair, we calculate a set of features describing their similarity in different ways.
3. For a set of word senses (the gold standard), the alignment decision is made by human annotators.
4. A machine learning classifier is trained on this gold standard, and an alignment decision is made for
the remainder of the candidate pairs to produce a complete alignment of the resources. In our setup,
we use 10-fold cross validation to train the classifier.
The different datasets and steps of the algorithm are explained in more detail in the following sections.
3.1 Resources and Datasets
We use four different WSA evaluation datasets, two of which are presented for the first time. To ensure
compatibility with previous work, we use the same versions of the resources as reported in (Gurevych et
al., 2012) and (Matuschek and Gurevych, 2013).
247
Pair Pos. Neg. Polysemy One cand. F
1
A
0
Composition
WordNet-OmegaWiki 210 473 1.50 75.2% 0.84 0.85 random
WordNet-Wiktionary 313 2 110 4.76 18.6% 0.78 0.93 manual
Wiktionary-Wikipedia (En) 75 292 1.27 87.6% 0.79 0.95 automatic
Wiktionary-Wikipedia (De) 21 855 9 953 1.47 77.6% 0.85 0.89 crowd
Table 1: Characteristics of the gold standards used in the evaluation. The degree of polysemy (i.e.
the number of possible alignment targets per sense) hints towards the difficulty of the task, as does
the number of senses with only one alignment candidate. WordNet-Wiktionary stands out as it was
manually composed and is not representative of the full alignment (Meyer and Gurevych, 2011). The
inter-annotator agreements A
0
and F
1
can be considered as upper bounds for automatic alignment accu-
racy and F-measure. Note that for the Wiktionary-Wikipedia datasets, due to the nature of their creation,
the agreement was originally not available; we estimated it by manually re-annotating a sample of 100
examples with two annotators.
3.1.1 Resources
WordNet (Fellbaum, 1998) is a computational lexicon for English created at Princeton University. It
is organized in sets of synonyms (synsets), each expressing a distinct concept. Synsets are represented
by textual definitions (so-called glosses). A hierarchical organization is encoded via semantic relations
such as hyponymy.
Wikipedia is a collaboratively created online encyclopedia available in almost 300 languages. The
current English version contains around 4 400 000 articles, and the German one around 1 700 000 articles,
each usually describing a particular concept. Due to its encyclopedic nature, Wikipedia mostly covers
nouns, while the other LSRs discussed also cover verbs, adjectives, etc. Articles are connected via
hyperlinks in the article text (implying a graph structure), and the first paragraph usually gives a short
summary of the topic, serving as a gloss for our purposes. Articles are also linked to the equivalent
articles in other languages.
Wiktionary is a dictionary ?side project? of Wikipedia, available in over 500 languages. Currently,
the English Wiktionary contains over 500 000 lexical entry pages, while the German one contains around
350 000 ones. For a word, multiple senses can be encoded, and these are usually represented by glosses.
Wiktionary also contains hyperlinks to synonyms, hypernyms, etc. and translations into other languages.
OmegaWiki is a freely editable online dictionary like Wiktionary. However, instead of distinct lan-
guage editions, OmegaWiki contains language-independent concepts (?Defined Meanings?) which carry
lexicalizations in different languages. These concepts are connected via semantic relations. OmegaWiki
contains over 46 000 concepts and lexicalizations in almost 500 languages.
3.1.2 Datasets
WordNet?OmegaWiki: The first alignment between these LSRs based on the German part of
OmegaWiki was reported in (Gurevych et al., 2012). As OmegaWiki Defined Meanings are multilin-
gual, we used the same dataset for monolingual WSA in later work (Matuschek and Gurevych, 2013).
Table 1 presents details about this and the other evaluation datasets.
WordNet?Wiktionary: Meyer and Gurevych (2011) originally used this dataset for similarity-based
alignment. While we could not improve upon this using Dijkstra-WSA on its own (Matuschek and
Gurevych, 2013), the backoff approach yielded a significant improvement. This dataset was manually
composed according to specific criteria, hence it differs from the others and is not fully representative of
the full alignment.
Wiktionary?Wikipedia (English): No evaluation dataset (let alone a full alignment) has been
reported for this resource pair yet. However, as the datasets for WordNet-Wiktionary (Meyer and
Gurevych, 2011) and WordNet-Wikipedia (Niemann and Gurevych, 2011) are lexically overlapping,
we were able to automatically create a gold standard for Wiktionary-Wikipedia by exploiting the transi-
tivity of the alignment relation, i.e. by using WordNet as a pivot. Note that, unlike Wiktionary, Word-
248
Net synsets have multiple lexicalizations for the same meaning, introducing alignment candidates from
Wikipedia which might not be applicable to a particular Wiktionary sense. Hence, we decided to filter
the examples where the lexeme of the Wiktionary sense and the Wikipedia article title did not match. An
effect of this process was that words not contained in all three resources were filtered out, and many ex-
amples were left with few or only one candidate, leading to a low polysemy. We also manually checked
the derived gold standard and corrected a small number of wrong annotations introduced through the
automatic process. The resulting dataset is thus considerably smaller than the others, but it still turned
out to be sufficient for machine learning experiments.
Wiktionary?Wikipedia (German): Same as for the English editions, neither a gold standard nor an
alignment was previously reported for this pair. We were able to create a gold standard in a novel way by
exploiting the fact that many German Wiktionary senses contain links to the corresponding Wikipedia
articles, inducing a sense alignment between the two LSRs manually validated by the Wiktionary com-
munity. However, we were unable to extract such an alignment for English, as Wikipedia articles are
attached to the lexical entry page in this version and not to a specific sense.
In the German Wiktionary, a large portion of the senses is linked in this way, and even after aggres-
sively filtering out invalid link targets (e.g. disambiguation pages or pages with a non-matching title),
we retained over 20 000 alignments between Wiktionary senses and Wikipedia pages, a sample of which
we manually confirmed to be correct. Of course, this only yields positive examples; to also include cases
of non-alignment, we extracted the other candidate (i.e. lexically matching) Wikipedia articles for each
aligned Wiktionary sense, assuming that Wiktionary editors also considered and discarded them before
eventually creating a link. Interestingly, the number of negative examples derived in this way is rela-
tively low in comparison to the other datasets. An analysis revealed that a large fraction of the linked
Wiktionary senses are either scientific terms (e.g. from biology) or named entities such as cities. Both
types of senses tend to have few alternative candidates in Wikipedia due to their specificity, and it seems
logical that Wiktionary users predominantly link these senses to the explanatory Wikipedia articles which
are not familiar to the majority of users.
In the end, this process yielded a WSA dataset with unprecedented characteristics: It was not only
created and validated by a crowd of editors rather than a handful of annotators, but it is also an order of
magnitude larger than previously reported datasets (Table 1). This enables us to assess the performance
of our WSA approach in a scenario which is close in size to a full alignment task, allowing a more
well-grounded statement about its effectiveness.
3.2 Feature Engineering
The selection of features for our machine learning approach was driven by the premise to keep the
framework as generic and resource-agnostic as possible, in order to ensure applicability to many different
LSRs without additional engineering effort. Thorough analysis of existing resources and approaches
revealed that two types of information are available for the vast majority of LSRs: i) Glosses, or more
general, textual descriptions of concepts, and ii) Relationships between concepts inducing a graph, given
through semantic relations, links, or other means. We also evaluated some features which are specific to
a smaller subset of resources (see Section 3.2.3).
3.2.1 Gloss Similarity
Cosine similarity (COS) calculates the cosine of the angle between a vector representation of two
senses s
1
and s
2
. For the vector representation of a sense, we use a bag-of-words approach, i.e., a vector
BoW(s) contains the term frequencies of all words in the description of s. In this work, we only rely on
the textual definition of a sense to keep the approach as generic as possible, while the usage of example
sentences, related words, synonyms etc. would also be possible.
Personalized PageRank similarity (PPR) (Agirre and Soroa, 2009) measures the semantic related-
ness between two word senses s
1
and s
2
by comparing semantic vectors which can be derived in different
ways; we utilize the variant introduced by Niemann and Gurevych (2011). The idea is to identify senses
of words in a sense?s gloss which are central for describing its meaning. These senses (represented in a
graph derived from an LSR such as WordNet) should have a high PageRank score (i.e. a high centrality).
249
3.2.2 Dijkstra-WSA Distance
Dijkstra-WSA (Matuschek and Gurevych, 2013) is the graph-based WSA algorithm we use to calculate
a distance-based similarity measure between word senses. We will briefly explain its two steps.
Graph construction: The resource graph is comprised of a set of nodes V which represents the senses
of an LSR and a set of edges E ? V ? V which expresses semantic relatedness between them. One
can use semantic relations, hyperlinks, or other relatedness indicators. For sparse LSRs, it is advisable
to add edges between senses s
1
and s
2
if a monosemous term t with sense s
2
is included in the gloss of
s
1
. For example, one can link a sense of Java to programming language if the latter term is included in
the former?s definition text. This monosemous linking enhances the graph density (and hence, the recall)
significantly.
Computing sense alignments: First, trivial alignments between the two resource graphs A and B are
created. Alignments are trivial if two senses have the same attached lexeme in A and B and this lexeme
is also unique in either resource. Intuitively, these alignments serve as ?bridges? between highly related
regions of A and B. Next, for each remaining sense s ? A, the set of possible target senses T ? B
is retrieved in a similar fashion as for our approach, and for each of them the shortest path is computed
using Dijkstra?s algorithm (Dijkstra, 1959). While Dijkstra-WSA then goes on to directly align the sense
which is closest to the source sense, we save the distance for each candidate sense and directly use it as
a feature, expressing semantic relatedness based on the structure of both underlying resources. When no
distance can be computed (in case of a disconnected graph), we assume infinite distance.
3.2.3 Other Features
We also experimented with other features which were accessible directly from the resources, i.e. with-
out the need for external knowledge or extensive computational effort; these were usually not available
for every resource pair. Features we tried were the part of speech (Wiktionary, OmegaWiki, Word-
Net), the sense index, i.e. the position in the sense list for a lexeme (WordNet, Wiktionary), similarity
of example sentences (WordNet, Wiktionary), overlap of translations into other languages (Wikipedia,
Wiktionary, cf. (Bond and Foster, 2013)) and overlap of domain labels (Wikipedia, Wiktionary, Word-
Net, OmegaWiki). However, for none of these features we could observe any significant
1
impact on
the results, mostly due to sparsity of the respective features. Thus, we do not report them, but on the
other hand we consider this an indicator that gloss similarity and distance in the resource graph already
sufficiently capture the similarity between senses.
3.3 Machine Learning Classifiers
We experimented with different machine learning classifiers using WEKA (Hall et al., 2009). While a
detailed discussion of these classifiers is beyond the scope of this work, we will at least give a short
description of the ones we eventually used. For more details, please refer to textbooks such as (Murphy,
2012). We used WEKA?s standard configuration in every case.
Threshold-based classifiers work by simply trying to learn a numeric boundary value which separates
positive examples from negative ones. Although this approach is rather naive, it has been successfully
used in previous WSA efforts (Meyer and Gurevych, 2011; Niemann and Gurevych, 2011).
A Naive Bayes classifier assumes that features are independent (i.e. the value of one feature is unre-
lated to any other feature), and is thus able to learn reliable classification probabilities on relatively small
training sets. While the independence assumption can be considered an oversimplification, the algorithm
is widely used due to its efficiency and good precision.
Bayesian Networks (or belief networks) also classify based on probabilities learned from training
data, however, they offer the advantage of modeling dependencies between features, hence allowing a
more accurate representation of the data. Technically, such a network is a directed acyclic graph modeling
the conditional dependencies between variables.
A Perceptron is a classifier which maps a real-valued input vector to a binary output, by means of an
artificial neural network. It is commonly used for pattern recognition, also in NLP (Collins, 2002).
1
All significance claims in this paper are based on McNemar?s test at a confidence level of 1%.
250
Support Vector Machines (SVMs) construct a hyperplane in a multi-dimensional space which yields
a good separation between positive and negative training examples, represented as data points.
Decision Trees are built from training input by iteratively splitting the set of samples based on attribute
values so that the resulting subset is as homogeneous as possible with regard to the class label. Unseen
examples can be classified by testing the attribute values and following different branches of the tree.
One of the main advantages (e.g. in comparison to SVMs) is that this approach is easily interpretable.
4 Experimental Results and Analysis
Baselines For reference, we report six different baselines: i) Random: A random sense from the set of
candidates is chosen in each case, ii) 1:1: An alignment is always made if and only if there is exactly one
candidate, iii) 1st: The first of the candidate senses is always selected
2
, iv) SIM: A similarity threshold
is learned for gloss similarity values as suggested by Meyer and Gurevych (2011), cf. Section 3.2.1,
v) DWSA: The closest candidate sense in the resource graph is aligned as we suggested in (Matuschek
and Gurevych, 2013), cf. Section 3.2.2, vi) HYB: A hybrid approach of using DWSA first and then SIM
as a backoff, also suggested by us (Matuschek and Gurevych, 2013). The latter approach represents
state-of-the-art performance for WSA. Note that for the two Wiktionary-Wikipedia datasets, no previous
results were available, so we created similarity-based and Dijkstra-WSA alignments ourselves, based on
the same versions of the resources as in the previous work. For the other datasets, we used the numbers
reported in the original papers (Matuschek and Gurevych, 2013; Meyer and Gurevych, 2011).
Overview Tables 2 and 3 present the results for all setups. Although the best classifiers for each
dataset always outperform the previous state of the art and the baselines by a significant margin, there
is no consistent pattern in the results across different LSRs and classifiers. One reason for this is that
the range of feature values varies substantially between different datasets. For instance, Dijkstra-WSA
distances tend to be greater when Wikipedia is involved simply by its virtue of being larger than the other
LSRs, and gloss similarities also differ depending on the average length of the glosses and the language.
Another factor are the gold standards, which are quite different in terms of size and composition (see
Table 1). Thus, no classifier is the undisputed ?winner?, but Bayesian Networks proved most robust
in our experiments, showing competitive results in every case. As training them is also computationally
cheap (compared to SVMs, for instance), we would generally recommend this kind of classifier for WSA
tasks. In the following, we also provide a more detailed discussion of the results for each individual
dataset.
WordNet-OmegaWiki In this case, the precision of the alignment is satisfactory for every classifier,
while both previously reported approaches struggle for different reasons (Gurevych et al., 2012; Ma-
tuschek and Gurevych, 2013). The strength of the machine learning becomes apparent especially in
comparison with the HYB approach: While the latter merely combines independent alignment decisions,
hence achieving better recall but failing to improve precision (cf. Section 2.3), the joint usage of features
leads to a massive improvement. Analysis of the decision tree classifier shows that, as we suspected, the
?edge cases? are explicitly reflected in the learned model, i.e. examples with high gloss similarity but
also a high Dijkstra-WSA distance (or vice versa) are ruled out with higher confidence. This observation
generally also holds for the other datasets. As an example, the two senses of genome in biology (?The
non-redundant genetic information stored in DNA sequences that defines an individual organism?) and
algorithmics (?In the context of a genetic algorithm, the information that defines an individual entity?)
have similar glosses; they are, however, quite far apart in the graph and thus not aligned. The Bayesian
Network achieves the best results as it comprehensively models this interdependence of features. The
SVM achieves the best precision, but the distribution of feature values does not lend itself well to linear
separation in this case, leading to unsatisfactory recall.
WordNet-Wiktionary For this dataset, the results look similar to WordNet-OmegaWiki as far as the
improvement of precision is concerned, as the joint usage of features helps to make a correct decision on
2
While this corresponds to the most frequent sense baseline in other setups, note that no explicit frequency information is
available for OmegaWiki, Wiktionary and Wikipedia, so that the first sense baseline is only a rough approximation.
251
WordNet-OmegaWiki WordNet-Wiktionary
P R F
1
A P R F
1
A
Random 0.46 0.35 0.40 0.51 0.21 0.59 0.31 0.67
1:1 0.36 0.64 0.46 0.55 0.68 0.19 0.30 0.88
1st 0.34 0.80 0.48 0.47 0.33 0.51 0.40 0.80
SIM 0.55 0.53 0.54 0.73 0.67 0.65 0.66 0.91
DWSA 0.56 0.69 0.62 0.74 0.68 0.27 0.39 0.89
HYB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92
SVM 0.95 0.32 0.48 0.79 0.82 0.61 0.70 0.93
Naive Bayes 0.73 0.62 0.67 0.82 0.71 0.79 0.75 0.92
Bayesian Network 0.75 0.72 0.74 0.84 0.70 0.84 0.77 0.94
Perceptron 0.73 0.58 0.65 0.81 0.74 0.72 0.73 0.92
Decision Tree 0.68 0.63 0.66 0.80 0.78 0.66 0.72 0.93
Agreement - - 0.84 0.85 - - 0.78 0.93
Table 2: Alignment results for WordNet-OmegaWiki and WordNet-Wiktionary: Using baselines (top),
approaches from previous work (middle) and different machine learning classifiers (bottom). We report
precision, recall, F-measure (the harmonic mean of both) and accuracy. Best results for each value and
dataset are marked in bold. The inter-annotator agreements A
0
and F
1
are given as upper bounds.
borderline examples. However, in this case the recall is also substantially improved, especially for the
Bayesian classifiers. This was an issue in the original Dijkstra-WSA results (Matuschek and Gurevych,
2013) due to the low connectivity of the English Wiktionary graph. The combination of distances and
gloss similarities is able to alleviate this shortcoming of Wiktionary to some extent, as examples with
missing Dijkstra-WSA distance can still be aligned in case of sufficient gloss similarity. SVMs also show
the best precision here, but are challenged by the suboptimal separability of the feature space.
Wiktionary-Wikipedia (English) The low connectivity of Wiktionary is not as much an issue here as
for WordNet-Wiktionary, mostly due to the different composition of the gold standard ? higher-frequency
words tended to be retained (see Section 3.1.2), which in turn are better connected within Wiktionary.
This leads to reasonable results for Dijkstra-WSA alone. The hybrid approach reaches the best recall, but
due to the relatively low precision of the SIM alignment, the overall result leaves room for improvement.
This improvement is again achieved via joint modeling of features. As for the datasets discussed above,
the precision is improved significantly; this is especially true for the Bayesian Network classifier. Preci-
sion and recall for the SVM classifier are also satisfactory in this case (due to the better linear separability
of the feature space), making it the best overall classifier along with the Perceptron.
Wiktionary-Wikipedia (German) On this dataset, the naive baselines are very strong, due to the dis-
proportionately large number of positive examples ? this is especially true for the 1:1 setup which reaches
perfect precision. In other words, whenever there is only one alignment candidate, it is already the cor-
rect one. The HYB approach also yields good results thanks to the high precision of its two components,
but recall is an issue for gloss similarity due to the richer morphology and different formation of com-
pounds in German. We did not use a compound splitter (an obvious extension for future work), so that,
for instance ?Kinderspiel? and ?Spiel f?ur Kinder? (both meaning ?a game for children?) could not be
lexically matched. However, when machine learning is applied, the recall can again be significantly im-
proved at only a negligible expense of precision. Here, as for the WordNet-Wiktionary dataset, the joint
modeling of distance and gloss similarity allows to correctly align more borderline examples. While the
strong bias towards positive examples might make this dataset not fully representative of a full alignment
task (which is the eventual goal of WSA), the results still beat the strong baselines in terms of F-measure
and thus indicate that WSA, and especially our approach, works well on such a large-scale dataset.
252
Wiktionary-Wikipedia (En) Wiktionary-Wikipedia (De)
P R F
1
A P R F
1
A
Random 0.41 0.49 0.45 0.48 0.68 0.40 0.51 0.46
1:1 0.17 0.56 0.26 0.33 1.0 0.63 0.77 0.75
1st 0.23 0.88 0.36 0.37 0.93 0.66 0.78 0.74
SIM 0.60 0.67 0.63 0.84 0.85 0.46 0.60 0.57
DWSA 0.78 0.55 0.65 0.87 0.85 0.61 0.71 0.66
HYB 0.62 0.79 0.70 0.86 0.90 0.72 0.80 0.75
SVM 0.82 0.70 0.76 0.92 0.76 0.84 0.80 0.71
Naive Bayes 0.79 0.69 0.73 0.92 0.85 0.54 0.66 0.62
Bayesian Network 0.91 0.63 0.74 0.93 0.86 0.81 0.83 0.77
Perceptron 0.82 0.70 0.76 0.92 0.75 0.92 0.82 0.73
Decision Tree 0.79 0.69 0.73 0.92 0.87 0.81 0.84 0.78
Agreement - - 0.79 0.95 - - 0.85 0.89
Table 3: Results for Wiktionary-Wikipedia alignment in English and German: Using baselines (top),
approaches from previous work (middle) and different machine learning classifiers (bottom). We report
precision, recall, F-measure (the harmonic mean of both) and accuracy. Best results for each value and
dataset are marked in bold. The inter-annotator agreements A
0
and F
1
are given as upper bounds.
Error analysis Error sources for our system are mostly the same as for the previously reported ap-
proaches ? if equivalent concepts are described very differently (known as the ?lexical gap?, e.g. the
senses ?divulge confidential information? and ?to confess under interrogation? of the verb to sing) and
happen to be not very close in the resource graph, i.e. both similarity measures fail at once, they are likely
not aligned (false negatives). On the other hand, false positives occur for examples such as Brand, which
is the name of districts in two different German cities (Aachen and Zwickau). The sense descriptions
are very much alike, and the senses are also located in similar regions of the resource graphs (roughly
speaking, German geography), which makes the distinction hard. Addressing these issues might be pos-
sible by computing more sophisticated gloss similarity measures (e.g. using lexical expansion (Iida et
al., 2008)) or enhancing the graph construction process. In general, however, there are no discernible
systematic errors made by our system.
5 Conclusions and future work
We have shown that through joint modeling of different similarity measures for WSA the overall align-
ment quality in terms of F-measure can be significantly improved over the state of the art for each and
every of the considered four datasets. This proves that such a joint usage of global structure as well as
the content of the LSRs is indeed preferable over using either of them in isolation or combining them in
a simple backoff approach, since it effectively utilizes both ways of calculating similarity.
Apart from substantially improving WSA performance, we also present two new datasets for
Wiktionary-Wikipedia alignment in English and German which fill a considerable gap in the previous
work on WSA. One of Wiktionary?s explicit purposes is to complement the knowledge in Wikipedia,
so that an alignment between these widely used resources seems a natural and important extension to
the body of work in this field. Especially for (semi-) automatic translation tasks, this resource combina-
tion seems extremely promising due to the abundant multilingual content in both resources (see Section
3.1.1). We suggested a comparable combination of Wiktionary and OmegaWiki in the past (Matuschek
et al., 2013), but the much larger Wikipedia is bound to hold even more potential. Moreover, the Ger-
man dataset is of unprecedented size, allowing more credible statements about the performance of WSA
algorithms in a full alignment scenario. Another interesting aspect is that this dataset was derived from
links created by the crowd of Wiktionary editors, not by expert annotators; thus, it can be considered the
first crowdsourced WSA dataset. This type of dataset creation is also one aspect of future work. We want
to investigate in more detail to what extent these alignments are trustworthy, what steps are necessary
253
to improve the dataset?s size and quality, and how negative examples (i.e. non-alignments) can be more
reliably derived. We also plan to find out if such datasets could be created for other Wiktionary language
editions.
The fact that the achieved results are close to the human agreement suggests that, for the datasets
considered, there is not much room for improvement. Thus, we plan to apply and adapt the algorithm to
LSRs with different properties than the ones considered here, such as the more syntax-focused FrameNet
(Ruppenhofer et al., 2010) which only recently has received research attention in automatic WSA (Hart-
mann and Gurevych, 2013). The usage of syntactic features to express sense similarity has not been
thoroughly explored yet, and it seems a promising direction to make further progress in WSA. Usage of
more elaborate textual similarity features (e.g. covering semantic similarity or using lexical expansion)
as it was suggested for text reuse detection (B?ar et al., 2012) would be another direction worth exploring.
Inspired by the semi-automatic construction of the Wiktionary-Wikipedia gold standard for English
from existing datasets, we also want to investigate whether an alignment of more than two resources
at once (n-way alignment) is feasible, using joint knowledge from all LSRs involved. For instance,
the information that two senses in resources A and B share a strong resemblance to a sense in another
resource C could be expressed by an additional feature.
Acknowledgements
This work has been supported by the Volkswagen Foundation as part of the Lichtenberg Professorship
Program under grant No. I/82806 and by the Hessian research excellence program ?Landes-Offensive
zur Entwicklung Wissenschaftlich-?okonomischer Exzellenz (LOEWE)? as part of the research center
?Digital Humanities?. We would also like to thank the anonymous reviewers for their helpful remarks.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of
the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?41,
Athens, Greece.
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2012. Text Reuse Detection Using a Composition of Text Simi-
larity Measures. In Proceedings of the 24th International Conference on Computational Linguistics (COLING
2012), pages 167?184, Mumbai, India, December.
Francis Bond and Ryan Foster. 2013. Linking and Extending an Open Multilingual Wordnet. In Proceedings
of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1352?1362, Sofia, Bulgaria, August.
Kostadin Cholakov, Judith Eckle-Kohler, and Iryna Gurevych. 2014. Automated verb sense labelling based on
linked lexical resources. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics (EACL 2014), pages 68?77, Gothenburg, Sweden, April.
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language
Processing - Volume 10, pages 1?8, Philadelphia, USA.
Jordi Daud?e, Llu??s Padr?o, and German Rigau. 2003. Validation and tuning of wordnet mapping techniques. In
Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP?03),
Borovets, Bulgaria.
Gerard De Melo and Gerhard Weikum. 2008. A Machine Learning Approach to Building Aligned Wordnets. In
Proceedings of the First International Conference on Global Interoperability for Language Resources, pages
163?170, Hong Kong.
Gerard De Melo and Gerhard Weikum. 2010. Providing Multilingual, Multimodal Answers to Lexical Database
Queries. In Proceedings of the 7th Language Resources and Evaluation Conference (LREC 2010), pages 348?
355, Valetta, Malta.
Edsger W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269?
271.
254
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hartmann, Michael Matuschek, and Christian M. Meyer. 2012.
UBY-LMF - A Uniform Model for Standardizing Heterogeneous Lexical-Semantic Resources in ISO-LMF. In
Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC?12), pages
275?282, Istanbul, Turkey.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, USA.
Tiziano Flati and Roberto Navigli. 2012. The CQC algorithm: Cycling in graphs to semantically enrich and
enhance a bilingual dictionary. Journal of Artificial Intelligence Research (JAIR), 43:135?171.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian
Wirth. 2012. UBY - A Large-Scale Unified Lexical-Semantic Resource Based on LMF. In Proceedings of the
13th Conference of the European Chapter of the Association for Computational Linguistics (EACL?12), pages
580?590, Avignon, France.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The
WEKA Data Mining Software: An Update. volume 11, pages 10?18.
Silvana Hartmann and Iryna Gurevych. 2013. FrameNet on the Way to Babel: Creating a Bilingual FrameNet
Using Wiktionary as Interlingual Connection. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL 2013), volume 1, pages 1363?1373, August.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodolazova. 2011. Semi-Automatic Extension of GermaNet with
Sense Definitions from Wiktionary. In Proceedings of the 5th Language and Technology Conference (LTC
2011), pages 126?130, Poznan, Poland.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008. Gloss-based semantic similarity metrics for predominant
sense acquisition. In Proceedings of the Third International Joint Conference on Natural Language Processing
(IJCNLP ?08), pages 561?568.
Egoitz Laparra, German Rigau, and Montse Cuadros. 2010. Exploring the integration of WordNet and FrameNet.
In Proceedings of the 5th Global WordNet Conference (GWC?10), Mumbai, India.
Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation
(SIGDOC ?86), pages 24?26, Toronto, Canada.
Michael Matuschek and Iryna Gurevych. 2013. Dijkstra-WSA: A Graph-Based Approach to Word Sense Align-
ment. Transactions of the Association for Computational Linguistics (TACL), 1:151?164, May.
Michael Matuschek, Christian Meyer, and Iryna Gurevych. 2013. Multilingual Knowledge in Aligned Wiktionary
and OmegaWiki for Translation Applications. Translation: Computation, Corpora, Cognition, 3(1):87?118.
Christian M. Meyer and Iryna Gurevych. 2011. What psycholinguists know about chemistry: Aligning Wiktionary
and WordNet for increased domain coverage. In Proceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages 883?892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012. Wiktionary: A new rival for expert-built lexicons? Exploring
the possibilities of collaborative lexicography. In Sylviane Granger and Magali Paquot, editors, Electronic
Lexicography, chapter 13, pages 259?291. Oxford University Press.
Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu, Roberto Navigli, and Hans Uszkoreit. 2013. Semantic rule
filtering for web-scale relation extraction. In Proceedings of the 12
th
International Semantic Web Conference
(ISWC 2013), pages 347?362, Sydney, Australia.
Kevin P. Murphy. 2012. Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine
Learning series). The MIT Press, August.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. BabelNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b. BabelRelate! A Joint Multilingual Approach to Computing
Semantic Relatedness. In Proceedings of the 26th AAAI Conference on Artificial Intelligence, Toronto, Canada,
July.
Roberto Navigli. 2006. Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics, pages 105?112, Sydney, Australia.
255
Roberto Navigli. 2009. Using Cycles and Quasi-Cycles to Disambiguate Dictionary Glosses. In Proceedings
of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL?09),
pages 594?602, Athens, Greece.
Elisabeth Niemann and Iryna Gurevych. 2011. The People?s Web meets Linguistic Knowledge: Automatic Sense
Alignment of Wikipedia and WordNet. In Proceedings of the 9th International Conference on Computational
Semantics (IWCS), pages 205?214, Oxford, UK.
Martha Palmer. 2009. SemLink: Linking PropBank, VerbNet and FrameNet. In Proceedings of the Generative
Lexicon Conference (GenLex-09), pages 9?15, Pisa, Italy.
Simone Paolo Ponzetto and Roberto Navigli. 2009. Large-scale taxonomy mapping for restructuring and inte-
grating Wikipedia. In Proceedings of the 21
st
International Joint Conference on Artificial Intelligence, pages
2083?2088, Pasadena, CA, USA.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2010.
FrameNet II: Extended Theory and Practice. International Computer Science Institute, Berkeley, CA, Septem-
ber.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces Together: Combining FrameNet, VerbNet and WordNet for
Robust Semantic Parsing. In Computational Linguistics and Intelligent Text Processing: 6th International
Conference, volume 3406 of Lecture Notes in Computer Science, pages 100?111. Berlin/Heidelberg: Springer.
256
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 580?590,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
UBY ? A Large-Scale Unified Lexical-Semantic Resource
Based on LMF
Iryna Gurevych??, Judith Eckle-Kohler?, Silvana Hartmann?, Michael Matuschek?,
Christian M. Meyer? and Christian Wirth?
? Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
? Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
We present UBY, a large-scale lexical-
semantic resource combining a wide range
of information from expert-constructed
and collaboratively constructed resources
for English and German. It currently
contains nine resources in two lan-
guages: English WordNet, Wiktionary,
Wikipedia, FrameNet and VerbNet,
German Wikipedia, Wiktionary and
GermaNet, and multilingual OmegaWiki
modeled according to the LMF standard.
For FrameNet, VerbNet and all collabora-
tively constructed resources, this is done
for the first time. Our LMF model captures
lexical information at a fine-grained level
by employing a large number of Data
Categories from ISOCat and is designed
to be directly extensible by new languages
and resources. All resources in UBY can
be accessed with an easy to use publicly
available API.
1 Introduction
Lexical-semantic resources (LSRs) are the foun-
dation of many NLP tasks such as word sense
disambiguation, semantic role labeling, question
answering and information extraction. They are
needed on a large scale in different languages.
The growing demand for resources is met nei-
ther by the largest single expert-constructed re-
sources (ECRs), such as WordNet and FrameNet,
whose coverage is limited, nor by collaboratively
constructed resources (CCRs), such as Wikipedia
and Wiktionary, which encode lexical-semantic
knowledge in a less systematic form than ECRs,
because they are lacking expert supervision.
Previously, there have been several indepen-
dent efforts of combining existing LSRs to en-
hance their coverage w.r.t. their breadth and depth,
i.e. (i) the number of lexical items, and (ii) the
types of lexical-semantic information contained
(Shi and Mihalcea, 2005; Johansson and Nugues,
2007; Navigli and Ponzetto, 2010b; Meyer and
Gurevych, 2011). As these efforts often targeted
particular applications, they focused on aligning
selected, specialized information types. To our
knowledge, no single work focused on modeling
a wide range of ECRs and CCRs in multiple lan-
guages and a large variety of information types in
a standardized format. Frequently, the presented
model is not easily scalable to accommodate an
open set of LSRs in multiple languages and the in-
formation mined automatically from corpora. The
previous work also lacked the aspects of lexicon
format standardization and API access. We be-
lieve that easy access to information in LSRs is
crucial in terms of their acceptance and broad ap-
plicability in NLP.
In this paper, we propose a solution to this. We
define a standardized format for modeling LSRs.
This is a prerequisite for resource interoperabil-
ity and the smooth integration of resources. We
employ the ISO standard Lexical Markup Frame-
work (LMF: ISO 24613:2008), a metamodel for
LSRs (Francopoulo et al 2006), and Data Cate-
gories (DCs) selected from ISOCat.1 One of the
main challenges of our work is to develop a model
that is standard-compliant, yet able to express the
information contained in diverse LSRs, and that in
the long term supports the integration of the vari-
ous resources.
The main contributions of this paper can be
1http://www.isocat.org/
580
summarized as follows: (1) We present an LMF-
based model for large-scale multilingual LSRs
called UBY-LMF. We model the lexical-semantic
information down to a fine-grained level of in-
formation (e.g. syntactic frames) and employ
standardized definitions of linguistic information
types from ISOCat. (2) We present UBY, a large-
scale LSR implementing the UBY-LMF model.
UBY currently contains nine resources in two
languages: English WordNet (WN, Fellbaum
(1998), Wiktionary2 (WKT-en), Wikipedia3 (WP-
en), FrameNet (FN, Baker et al(1998)), and
VerbNet (VN, Kipper et al(2008)); German Wik-
tionary (WKT-de), Wikipedia (WP-de), and Ger-
maNet (GN, Kunze and Lemnitzer (2002)), and
the English and German entries of OmegaWiki4
(OW), referred to as OW-en and OW-de. OW,
a novel CCR, is inherently multilingual ? its ba-
sic structure are multilingual synsets, which are a
valuable addition to our multilingual UBY. Essen-
tial to UBY are the nine pairwise sense alignments
between resources, which we provide to enable
resource interoperability on the sense level, e.g.
by providing access to the often complementary
information for a sense in different resources. (3)
We present a Java-API which offers unified access
to the information contained in UBY.
We will make the UBY-LMF model, the re-
source UBY and the API freely available to the
research community.5 This will make it easy for
the NLP community to utilize UBY in a variety of
tasks in the future.
2 Related Work
The work presented in this paper concerns
standardization of LSRs, large-scale integration
thereof at the representational level, and the uni-
fied access to lexical-semantic information in the
integrated resources.
Standardization of resources. Previous work
includes models for representing lexical informa-
tion relative to ontologies (Buitelaar et al 2009;
McCrae et al 2011), and standardized single
wordnets (English, German and Italian wordnets)
in the ISO standard LMF (Soria et al 2009; Hen-
rich and Hinrichs, 2010; Toral et al 2010).
2http://www.wiktionary.org/
3http://www.wikipedia.org/
4http://www.omegawiki.org/
5http://www.ukp.tu-darmstadt.de/data/uby
McCrae et al(2011) propose LEMON, a con-
ceptual model for lexicalizing ontologies as an
extension of the LexInfo model (Buitelaar et al
2009). LEMON provides an LMF-implementation
in the Web Ontology Language (OWL), which
is similar to UBY-LMF, as it also uses DCs
from ISOCat, but diverges further from the stan-
dard (e.g. by removing structural elements such
as the predicative representation class). While
we focus on modeling lexical-semantic informa-
tion comprehensively and at a fine-grained level,
the goal of LEMON is to support the linking be-
tween ontologies and lexicons. This goal entails
a task-targeted application: domain-specific lex-
icons are extracted from ontology specifications
and merged with existing LSRs on demand. As a
consequence, there is no available large-scale in-
stance of the LEMON model.
Soria et al(2009) define WordNet-LMF, an
LMF model for representing wordnets used in
the KYOTO project, and Henrich and Hinrichs
(2010) do this for GN, the German wordnet.
These models are similar, but they still present
different implementations of the LMF meta-
model, which hampers interoperability between
the resources. We build upon this work, but ex-
tend it significantly: UBY goes beyond model-
ing a single ECR and represents a large number
of both ECRs and CCRs with very heterogeneous
content in the same format. Also, UBY-LMF
features deeper modeling of lexical-semantic in-
formation. Henrich and Hinrichs (2010), for
instance, do not explicitly model the argument
structure of subcategorization frames, since each
frame is represented as a string. In UBY-LMF,
we represent them at a fine-grained level neces-
sary for the transparent modeling of the syntax-
semantics interface.
Large-scale integration of resources. Most
previous research efforts on the integration of re-
sources targeted at world knowledge rather than
lexical-semantic knowledge. Well known exam-
ples are YAGO (Suchanek et al 2007), or DBPe-
dia (Bizer et al 2009).
Atserias et al(2004) present the Meaning Mul-
tilingual Central Repository (MCR). MCR inte-
grates five local wordnets based on the Interlin-
gual Index of EuroWordNet (Vossen, 1998). The
overall goal of the work is to improve word sense
disambiguation. This work is similar to ours, as it
581
aims at a large-scale multilingual resource and in-
cludes several resources. It is however restricted
to a single type of resource (wordnets) and fea-
tures a single type of lexical information (seman-
tic relations) specified upon synsets. Similarly,
de Melo and Weikum (2009) create a multilin-
gual wordnet by integrating wordnets, bilingual
dictionaries and information from parallel cor-
pora. None of these resources integrate lexical-
semantic information, such as syntactic subcate-
gorization or semantic roles.
McFate and Forbus (2011) present NULEX,
a syntactic lexicon automatically compiled from
WN, WKT-en and VN. As their goal is to cre-
ate an open-license resource to enhance syntactic
parsing, they enrich verbs and nouns in WN with
inflection information from WKT-en and syntac-
tic frames from VN. Thus, they only use a small
part of the lexical information present in WKT-en.
Padro? et al(2011) present their work on lex-
icon merging within the Panacea Project. One
goal of Panacea is to create a lexical resource de-
velopment platform that supports large-scale lex-
ical acquisition and can be used to combine exist-
ing lexicons with automatically acquired ones. To
this end, Padro? et al(2011) explore the automatic
integration of subcategorization lexicons. Their
current work only covers Spanish, and though
they mention the LMF standard as a potential data
model, they do not make use of it.
Shi and Mihalcea (2005) integrate FN, VN and
WN, and Palmer (2009) presents a combination of
Propbank, VN and FN in a resource called SEM-
LINK in order to enhance semantic role labeling.
Similar to our work, multiple resources are in-
tegrated, but their work is restricted to a single
language and does not cover CCRs, whose pop-
ularity and importance has grown tremendously
over the past years. In fact, with the excep-
tion of NULEX, CCRs have only been consid-
ered in the sense alignment of individual resource
pairs (Navigli and Ponzetto, 2010a; Meyer and
Gurevych, 2011).
API access for resources. An important factor
to the success of a large, integrated resource is a
single public API, which facilitates the access to
the information contained in the resource. The
most important LSRs so far can be accessed us-
ing various APIs, for instance the Java WordNet
API,6 or the Java-based Wikipedia API.7
With a stronger focus of the NLP community
on sharing data and reproducing experimental re-
sults these tools are becoming important as never
before. Therefore, a major design objective of
UBY is a single API. This is similar in spirit to the
motivation of Pradhan et al(2007), who present
integrated access to corpus annotations as a main
goal of their work on standardizing and integrat-
ing corpus annotations in the OntoNotes project.
To summarize, related work focuses either on
the standardization of single resources (or a single
type of resource), which leads to several slightly
different formats constrained to these resources,
or on the integration of several resources in an
idiosyncratic format. CCRs have not been con-
sidered at all in previous work on resource stan-
dardization, and the level of detail of the model-
ing is insufficient to fully accommodate different
types of lexical-semantic information. API ac-
cess is rarely provided. This makes it hard for
the community to exploit their results on a large
scale. Thus, it diminishes the impact that these
projects might achieve upon NLP beyond their
original specific purpose, if their results were rep-
resented in a unified resource and could easily be
accessed by the community through a single pub-
lic API.
3 UBY ? Data model
LMF defines a metamodel of LSRs in the Uni-
fied Modeling Language (UML). It provides a
number of UML packages and classes for model-
ing many different types of resources, e.g. word-
nets and multilingual lexicons. The design of
a standard-compliant lexicon model in LMF in-
volves two steps: in the first step, the structure
of the lexicon model has to be defined by choos-
ing a combination of the LMF core package and
zero to many extensions (i.e. UML packages). In
the second step, these UML classes are enriched
by attributes. To contribute to semantic interop-
erability, it is essential for the lexicon model that
the attributes and their values refer to Data Cat-
egories (DCs) taken from a reference repository.
DCs are standardized specifications of the terms
that are used for attributes and their values, or in
other words, the linguistic vocabulary occurring
6http://sourceforge.net/projects/jwordnet/
7http://code.google.com/p/jwpl/
582
in a lexicon model. Consider, for instance, the
term lexeme that is defined differently in WN and
FN: in FN, a lexeme refers to a word form, not
including the sense aspect. In WN, on the con-
trary, a lexeme is an abstract pairing of mean-
ing and form. According to LMF, the DCs are
to be selected from ISOCat, the implementation
of the ISO 12620 Data Category Registry (DCR,
Broeder et al(2010)), resulting in a Data Cate-
gory Selection (DCS).
Design of UBY-LMF. We have designed UBY-
LMF8 as a model of the union of various hetero-
geneous resources, namely WN, GN, FN, and VN
on the one hand and CCRs on the other hand.
Two design principles guided our development
of UBY-LMF: first, to preserve the information
available in the original resources and to uni-
formly represent it in UBY-LMF. Second, to be
able to extend UBY in the future by further lan-
guages, resources, and types of linguistic infor-
mation, in particular, alignments between differ-
ent LSRs.
Wordnets, FN and VN are largely complemen-
tary regarding the information types they provide,
see, e.g. Baker and Fellbaum (2009). Accord-
ingly, they use different organizational units to
represent this information. Wordnets, such as
WN and GN, primarily contain information on
lexical-semantic relations, such as synonymy, and
use synsets (groups of lexemes that are synony-
mous) as organizational units. FN focuses on
groups of lexemes that evoke the same prototypi-
cal situation (so-called semantic frames, Fillmore
(1982)) involving semantic roles (so-called frame
elements). VN, a large-scale verb lexicon, is or-
ganized in Levin-style verb classes (Levin, 1993)
(groups of verbs that share the same syntactic al-
ternations and semantic roles) and provides rich
subcategorization frames including semantic roles
and a specification of semantic predicates.
UBY-LMF employs several direct subclasses
of Lexicon in order to account for the various or-
ganization types found in the different LSRs con-
sidered. While the LexicalEntry class reflects
the traditional headword-based lexicon organiza-
tion, Synset represents synsets from wordnets,
SemanticPredicate models FN semantic
frames, and SubcategorizationFrameSet
corresponds to VN alternation classes.
8See www.ukp.tu-darmstadt.de/data/uby
SubcategorizationFrame is com-
posed of syntactic arguments, while
SemanticPredicate is composed of se-
mantic arguments. The linking between syntactic
and semantic arguments is represented by the
SynSemCorrespondence class.
The SenseAxis class is very important in
UBY-LMF, as it connects the different source
LSRs. Its role is twofold: first, it links the cor-
responding word senses from different languages,
e.g. English and German. Second, it represents
monolingual sense alignments, i.e. sense align-
ments between different lexicons in the same lan-
guage. The latter is a novel interpretation of
SenseAxis introduced by UBY-LMF.
The organization of lexical-semantic knowl-
edge found in WP, WKT, and OW can be mod-
eled with the classes in UBY-LMF as well. WP
primarily provides encyclopedic information on
nouns. It mainly consists of article pages which
are modeled as Senses in UBY-LMF.
WKT is in many ways similar to tradi-
tional dictionaries, because it enumerates senses
under a given headword on an entry page.
Thus, WKT entry pages can be represented by
LexicalEntries and WKT senses by Senses.
OW is different from WKT and WP, as it is or-
ganized in multilingual synsets. To model OW
in UBY-LMF, we split the synsets per language
and included them as monolingual Synsets in
the corresponding Lexicon (e.g., OW-en or OW-
de). The original multilingual information is pre-
served by adding a SenseAxis between corre-
sponding synsets in OW-en and OW-de.
The LMF standard itself contains only few lin-
guistic terms and does neither specify attributes
nor their values. Therefore, an important task in
developing UBY-LMF has been the specification
of attributes and their values along with the proper
attachment of attributes to LMF classes. In partic-
ular, this task involved selecting DCs from ISO-
Cat and, if necessary, adding new DCs to ISOCat.
Extensions in UBY-LMF. Although UBY-
LMF is largely compliant with LMF, the task of
building a homogeneous lexicon model for many
highly heterogeneous LSRs led us to extend LMF
in several ways: we added two new classes and
several new relationships between classes.
First, we were facing a huge variety of lexical-
semantic labels for many different dimensions of
583
semantic classification. Examples of such dimen-
sions include ontological type (e.g. selectional re-
strictions in VN and FN), domain (e.g. Biology in
WN), style and register (e.g. labels in WKT, OW),
or sentiment (e.g. sentiment of lexical units in
FN). Since we aim at an extensible LMF-model,
capable of representing further dimensions of se-
mantic classification, we did not squeeze the in-
formation on semantic classes present in the con-
sidered LSRs into existing LMF classes. Instead,
we addressed this issue by introducing a more
general class, SemanticLabel, which is an op-
tional subclass of Sense, SemanticPredicate,
and SemanticArgument. This new class has
three attributes, encoding the name of the label,
its type (e.g. ontological, register, sentiment), and
a numeric quantification (e.g. sentiment strength).
Second, we attached the subclass Frequency
to most of the classes in UBY-LMF, in order to
encode frequency information. This is of partic-
ular importance when using the resource in ma-
chine learning applications. This extension of the
standard has already been made in WordNet-LMF
(Soria et al 2009). Currently, the Frequency
class is used to keep corpus frequencies for lex-
ical units in FN, but we plan to use it for en-
riching many other classes with frequency in-
formation in future work, such as Senses or
SubcategorizationFrames.
Third, the representation of FN in LMF re-
quired adding two new relationships between
LMF classes: we added a relationship between
SemanticArgument and Definition, in or-
der to represent the definitions available for frame
elements in FN. In addition, we added a re-
lationship between the Context class and the
MonoLingualExternalRef, to represent the
links to annotated corpus sentences in FN.
Finally, WKT turned out to be hard to tackle,
because it contains a special kind of ambiguity in
the semantic relations and translation links listed
for senses: the targets of both relations and trans-
lation links are ambiguous, as they refer to lem-
mas (word forms), rather than to senses (Meyer
and Gurevych, 2010). These ambiguous rela-
tion targets could not directly be represented in
LMF, since sense and translation relations are
defined between senses. To resolve this, we
added a relationship between SenseRelation
and FormRepresentation, in order to encode
the ambiguous WKT relation target as a word
form. Disambiguating the WKT relation targets
to infer the target sense is left to future work.
A related issue occurred, when we mapped WN
to LMF. WN encodes morphologically related
forms as sense relations. UBY-LMF represents
these related forms not only as sense relations (as
in WordNet-LMF), but also at the morphologi-
cal level using the RelatedForm class from the
LMF Morphology extension. In LMF, however,
the RelatedForm class for morphologically re-
lated lexemes is not associated with the corre-
sponding sense in any way. Discarding the WN
information on the senses involved in a particular
morphological relation would lead to information
loss in some cases. Consider as an example the
WN verb buy (purchase) which is derivationally
related to the noun buy, while on the other hand
buy (accept as true, e.g. I can?t buy this story) is
not derivationally related to the noun buy. We ad-
dressed this issue by adding a sense attribute to
the RelatedForm class. Thus, in extension of
LMF, UBY-LMF allows sense relations to refer to
a form relation target and morphological relations
to refer to a sense relation target.
Data Categories in UBY-LMF. We encoun-
tered large differences in the availability of DCs
in ISOCat for the morpho-syntactic, lexical-
syntactic, and lexical-semantic parts of UBY-
LMF. Many DCs were missing in ISOCat and we
had to enter them ourselves. While this was feasi-
ble at the morpho-syntactic and lexical-syntactic
level, due to a large body of standardization re-
sults available, it was much harder at the lexical-
semantic level where standardization is still on-
going. At the lexical-semantic level, UBY-LMF
currently allows string values for a number of at-
tribute values, e.g. for semantic roles. We can eas-
ily integrate the results of the ongoing standard-
ization efforts into UBY-LMF in the future.
4 UBY ? Population with information
4.1 Representing LSRs in UBY-LMF
UBY-LMF is represented by a DTD (as suggested
by the standard) which can be used to automat-
ically convert any given resource into the corre-
sponding XML format.9 This conversion requires
a detailed analysis of the resource to be converted,
followed by the definition of a mapping of the
9Therefore, UBY-LMF can be considered as a serializa-
tion of LMF.
584
concepts and terms used in the original resource
to the UBY-LMF model. There are two major
tasks involved in the development of an automatic
conversion routine: first, the basic organizational
unit in the source LSR has to be identified and
mapped, e.g. synset in WN or semantic frame in
FN, and second, it has to be determined, how a
(LMF) sense is defined in the source LSR.
A notable aspect of converting resources into
UBY-LMF is the harmonization of linguistic ter-
minology used in the LSRs. For instance, a
WN Word and a GN Lexical Unit are mapped to
Sense in UBY-LMF.
We developed reusable conversion routines for
the future import of updated versions of the source
LSRs into UBY, provided the structure of the
source LSR remains stable. These conversion
routines extract lexical data from the source LSRs
by calling their native APIs (rather than process-
ing the underlying XML data). Thus, all lexical
information which can be accessed via the APIs
is converted into UBY-LMF.
Converting the LSRs introduced in the previ-
ous section yielded an instantiation of UBY-LMF
named UBY. The LexicalResource instance
UBY currently comprises 10 Lexicon instances,
one each for OW-de and OW-en, and one lexicon
each for the remaining eight LSRs.
4.2 Adding Sense Alignments
Besides the uniform and standardized representa-
tion of the single LSRs, one major asset of UBY
is the semantic interoperability of resources at the
sense level. In the following, we (i) describe how
we converted already existing sense alignments of
resources into LMF, and (ii) present a framework
to infer alignments automatically for any pair of
resources.
Existing Alignments. Previous work on sense
alignment yielded several alignments, such as
WN?WP-en (Niemann and Gurevych, 2011),
WN?WKT-en (Meyer and Gurevych, 2011) and
VN?FN (Palmer, 2009).
We converted these alignments into UBY-LMF
by creating a SenseAxis instance for each pair of
aligned senses. This involved mapping the sense
IDs from the proprietary alignment files to the
corresponding sense IDs in UBY.
In addition, we integrated the sense alignments
already present in OW and WP. Some OW en-
tries provide links to the corresponding WP page.
Also, the German and English language editions
of WP and OW are connected by inter-language
links between articles (Senses in UBY). We can
expect that these links have high quality, as they
were entered manually by users and are subject
to community control. Therefore, we straightfor-
wardly imported them into UBY.
Alignment Framework. Automatically creat-
ing new alignments is difficult because of word
ambiguities, different granularities of senses,
or language specific conceptualizations (Navigli,
2006). To support this task for a large number
of resources across languages, we have designed
a flexible alignment framework based on the
state-of-the-art method of Niemann and Gurevych
(2011). The framework is generic in order to al-
low alignments between different kinds of entities
as found in different resources, e.g. WN synsets,
FN frames or WP articles. The only requirement
is that the individual entities are distinguishable
by a unique identifier in each resource.
The alignment consists of the following steps:
First, we extract the alignment candidates for a
given resource pair, e.g. WN sense candidates for
a WKT-en entry. Second, we create a gold stan-
dard by manually annotating a subset of candi-
date pairs as ?valid? or ?non-valid?. Then, we
extract the sense representations (e.g. lemmatized
bag-of-words based on glosses) to compute the
similarity of word senses (e.g. by cosine similar-
ity). The gold standard with corresponding sim-
ilarity values is fed into Weka (Hall et al 2009)
to train a machine learning classifier, and in the
final step this classifier is used to automatically
classify the candidate sense pairs as (non-)valid
alignment. Our framework also allows us to train
on a combination of different similarity measures.
Using our framework, we were able to re-
produce the results reported by Niemann and
Gurevych (2011) and Meyer and Gurevych
(2011) based on the publicly available evaluation
datasets10 and the configuration details reported
in the corresponding papers.
Cross-Lingual Alignment. In order to align
word senses across languages, we extended the
monolingual sense alignment described above to
the cross-lingual setting. Our approach utilizes
10http://www.ukp.tu-darmstadt.de/data/sense-alignment/
585
Moses,11 trained on the Europarl corpus. The
lemma of one of the two senses to be aligned
as well as its representations (e.g. the gloss) is
translated into the language of the other resource,
yielding a monolingual setting. E.g., the WN
synset {vessel, watercraft} with its gloss ?a craft
designed for water transportation? is translated
into {Schiff, Wasserfahrzeug} and ?Ein Fahrzeug
fu?r Wassertransport?, and then the candidate ex-
traction and all downstream steps can take place
in German. An inherent problem with this ap-
proach is that incorrect translations also lead to
invalid alignment candidates. However, these are
most probably filtered out by the machine learn-
ing classifier as the calculated similarity between
the sense representations (e.g. glosses) should be
low if the candidates do not match.
We evaluated our approach by creating a cross-
lingual alignment between WN and OW-de, i.e.
the concepts in OW with a German lexicaliza-
tion.12 To our knowledge, this is the first study on
aligning OW with another LSR. OW is especially
interesting for this task due to its multilingual con-
cepts, as described by Matuschek and Gurevych
(2011). The created gold standard could, for in-
stance, be re-used to evaluate alignments for other
languages in OW.
To compute the similarity of word senses, we
followed the approach by Niemann and Gurevych
(2011) while covering both translation directions.
We used the cosine similarity for comparing the
German OW glosses with the German translations
of WN glosses and cosine and personalized page
rank (PPR) similarity for comparison of the Ger-
man OW glosses translated into English with the
original English WN glosses. Note that PPR sim-
ilarity is not available for German as it is based
on WN. Thereby, we filtered out the OW con-
cepts without a German gloss which left us with
11,806 unique candidate pairs. We randomly se-
lected 500 WN synsets for analysis yielding 703
candidate pairs. These were manually annotated
as being (non-)alignments. For the subsequent
machine learning task we used a simple threshold-
based classifier and ten-fold cross validation.
Table 1 summarizes the results of different sys-
tem configurations. We observe that translation
11http://www.statmt.org/moses/
12OmegaWiki consists of interlinked language-
independent concepts to which lexicalizations in several
languages are attached.
Translation Similarity
direction measure P R F1
EN > DE Cosine (Cos) 0.666 0.575 0.594
DE > EN Cos 0.674 0.658 0.665
DE > EN PPR 0.721 0.712 0.716
DE > EN PPR + Cos 0.723 0.712 0.717
Table 1: Cross-lingual alignment results
into English works significantly better than into
German. Also, the more elaborate similarity mea-
sure PPR yields better results than cosine similar-
ity, while the best result is achieved by a combina-
tion of both. Niemann and Gurevych (2011) make
a similar observation for the monolingual setting.
Our F-measure of 0.717 in the best configuration
lies between the results of Meyer and Gurevych
(2011) (0.66) and Niemann and Gurevych (2011)
(0.78), and thus verifies the validity of the ma-
chine translation approach. Therefore, the best
alignment was subsequently integrated into UBY.
5 Evaluating UBY
We performed an intrinsic evaluation of UBY by
computing a number of resource statistics. Our
evaluation covers two aspects: first, it addresses
the question if our automatic conversion routines
work correctly. Second, it provides indicators for
assessing UBY in terms of the gain in coverage
compared to the single LSRs.
Correctness of conversion. Since we aim to
preserve the maximal amount of information from
the original LSRs, we should be able to replace
any of the original LSRs and APIs by UBY and
the UBY-API without losing information. As
the conversion is largely performed automatically,
systematic errors and information loss could be
introduced by a faulty conversion routine. In or-
der to detect such errors and to prove the correct-
ness of the automatic conversion and the result-
ing representation, we have compared the orig-
inal resource statistics of the classes and infor-
mation types in the source LSRs to the cor-
responding classes in their UBY counterparts.
For instance, the number of lexical relations in
WordNet has been compared to the number of
SenseRelations in the UBY WordNet lexi-
con.13
13For detailed analysis results see the UBY website.
586
Lexical Sense
Lexicon Entry Sense Relation
FN 9,704 11,942 ?
GN 83,091 93,407 329,213
OW-de 30,967 34,691 60,054
OW-en 51,715 57,921 85,952
WP-de 790,430 838,428 571,286
WP-en 2,712,117 2,921,455 3,364,083
WKT-de 85,575 72,752 434,358
WKT-en 335,749 421,848 716,595
WN 156,584 206,978 8,559
VN 3,962 31,891 ?
UBY 4,259,894 4,691,313 5,300,941
Table 2: UBY resource statistics (selected classes).
Lexicon pair Languages SenseAxis
WN?WP-en EN?EN 50,351
WN?WKT-en EN?EN 99,662
WN?VN EN?EN 40,716
FN?VN EN?EN 17,529
WP-en?OW-en EN?EN 3,960
WP-de?OW-de DE?DE 1,097
WN?OW-de EN?DE 23,024
WP-en?WP-de EN?DE 463,311
OW-en?OW-de EN?DE 58,785
UBY All 758,435
Table 3: UBY alignment statistics.
Gain in coverage. UBY offers an increased
coverage compared to the single LSRs as reflected
in the resource statistics. Tables 2 and 3 show the
statistics on central classes in UBY. As UBY is
organized in several Lexicons, the number of
UBY lexical entries is the sum of the lexical en-
tries in all 10 Lexicons. Thus, UBY contains
more than 4.2 million lexical entries, 4.6 million
senses, 5.3 million semantic relations between
senses and more than 750,000 alignments. These
statistics represent the total numbers of lexical en-
tries, senses and sense relations in UBY without
filtering of identical (i.e. corresponding) lexical
entries, senses and relations. Listing the num-
ber of unique senses would require a full align-
ment between all integrated resources, which is
currently not available.
We can, however, show that UBY contains over
3.08 million unique lemma-POS combinations for
English and over 860,000 for German, over 3.94
million in total, see Table 4. Therefore, we as-
sessed the coverage on lemma level. Table 4 also
shows the number of lemmas with entries in one
or more than one lexicon, additionally split by
POS and language. Lemmas occurring only once
in UBY increase the coverage at lemma level. For
lemmas with parallel entries in several UBY lex-
icons, new information becomes available in the
form of additional sense definitions and comple-
mentary information types attached to lemmas.
Finally, the increase in coverage at sense level
can be estimated for senses that are aligned across
at least two UBY-lexicons. We gain access to
all available, partly complementary information
types attached to these aligned senses, e.g. seman-
tic relations, subcategorization frames, encyclo-
pedic or multilingual information. The number
of pairwise sense alignments provided by UBY is
given in Table 3. In addition, we computed how
many senses simultaneously take part in at least
two pairwise sense alignments. For English, this
applies to 31,786 senses, for which information
from 3 UBY lexicons is available.
EN Lexicons noun verb adjective
5 1 699 -
4 1,630 1,888 430
3 8,439 1,948 2,271
2 53,856 4,727 12,290
1 2,900,652 50,209 41,731
? (unique EN) 3,080,771
DE Lexicons noun verb adjective
4 1,546 - -
3 10,374 372 342
2 26,813 3,174 2,643
1 803,770 6,108 7,737
? (unique DE) 862,879
Table 4: Number of lemmas (split by POS and lan-
guage) with entries in i UBY lexicons, i = 1, . . . , 5.
6 Using UBY
UBY API. For convenient access to UBY, we
implemented a Java-API which is built around
the Hibernate14 framework. Hibernate allows to
easily store the XML data which results from
converting resources into Uby-LMF into a corre-
sponding SQL database.
Our main design principle was to keep the ac-
cess to the resource as simple as possible, despite
the rich and complex structure of UBY. Another
14http://www.hibernate.org/
587
important design aspect was to ensure that the
functionality of the individual, resource-specific
APIs or user interfaces is mirrored in the UBY
API. This enables porting legacy applications to
our new resource. To facilitate the transition to
UBY, we plan to provide reference tables which
list the corresponding UBY-API operations for the
most important operations in the WN API, some
of which are shown in Table 5.
WN function UBY function
Dictionary UBY
getIndexWord(pos,
lemma)
getLexicalEntries(
pos, lemma)
IndexWord LexicalEntry
getLemma() getLemmaForm()
Synset Synset
getGloss() getDefinitionText()
getWords() getSenses()
Pointer SynsetRelation
getType() getRelName()
Word Sense
getPointers() getSenseRelations()
Table 5: Some equivalent operations in WN API and
UBY API.
While it is possible to limit access to single re-
sources by a parameter and thus mimic the behav-
ior of the legacy APIs (e.g. only retrieve Synsets
and their relations from WN), the true power of
UBY API becomes visible when no such con-
straints are applied. In this case, all imported re-
sources are queried to get one combined result,
while retaining the source of the respective in-
formation. On top of this, the information about
existing sense alignments across resources can be
accessed via SenseAxis relations, so that the re-
turned combined result covers not only the lexi-
cal, but also the sense level.
Community issues. One of the most important
reasons for UBY is creating an easy-to-use pow-
erful LSR to advance NLP research and develop-
ment. Therefore, community building around the
resource is one of our major concerns. To this end,
we will offer free downloads of the lexical data
and software presented in this paper under open li-
censes, namely: The UBY-LMF DTD, mappings
and conversion tools for existing resources and
sense alignments, the Java API, and, as far as li-
censing allows,15 already converted resources. If
resources cannot be made available for download,
the conversion tools will still allow users with ac-
cess to these resources to import them into UBY
easily. In this way, it will be possible for users to
build their ?custom UBY? containing selected re-
sources. As the underlying resources are subject
to continuous change, updates of the correspond-
ing components will be made available on a regu-
lar basis.
7 Conclusions
We presented UBY, a large-scale, standardized
LSR containing nine widely used resources in two
languages: English WN, WKT-en, WP-en, FN
and VN, German WP-de, WKT-de, and GN, and
OW in English and German. As all resources
are modeled in UBY-LMF, UBY enables struc-
tural interoperability across resources and lan-
guages down to a fine-grained level of informa-
tion. For FN, VN and all of the CCRs in En-
glish and German, this is done for the first time.
Besides, by integrating sense alignments we also
enable the lexical-semantic interoperability of re-
sources. We presented a unified framework for
aligning any LSRs pairwise and reported on ex-
periments which align OW-de and WN. We will
release the UBY-LMF model, the resource and the
UBY-API at the time of publication.16 Due to the
added value and the large scale of UBY, as well as
its ease of use, we believe UBY will boost the per-
formance of NLP making use of lexical-semantic
knowledge.
Acknowledgments
This work has been supported by the Emmy
Noether Program of the German Research Foun-
dation (DFG) under grant No. GU 798/3-1 and
by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant
No. I/82806. We thank Richard Eckart de
Castilho, Yevgen Chebotar, Zijad Maksuti and Tri
Duc Nghiem for their contributions to this project.
References
Jordi Atserias, Lu??s Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
15Only GermaNet is subject to a restricted license and can-
not be redistributed in UBY format.
16http://www.ukp.tu-darmstadt.de/data/uby
588
Vossen. 2004. The Meaning Multilingual Central
Repository. In Proceedings of the second interna-
tional WordNet Conference (GWC 2004), pages 23?
30, Brno, Czech Republic.
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguis-
tic Annotation Workshop, ACL-IJCNLP ?09, pages
125?129, Suntec, Singapore.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th
International Conference on Computational Lin-
guistics (COLING-ACL?98, pages 86?90, Montreal,
Canada.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
So?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia A Crys-
tallization Point for the Web of Data. Journal of
Web Semantics: Science, Services and Agents on the
World Wide Web, (7):154?165.
Daan Broeder, Marc Kemps-Snijders, Dieter Van Uyt-
vanck, Menzo Windhouwer, Peter Withers, Peter
Wittenburg, and Claus Zinn. 2010. A Data Cat-
egory Registry- and Component-based Metadata
Framework. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), pages 43?47, Valletta, Malta.
Paul Buitelaar, Philipp Cimiano, Peter Haase, and
Michael Sintek. 2009. Towards Linguistically
Grounded Ontologies. In Lora Aroyo, Paolo
Traverso, Fabio Ciravegna, Philipp Cimiano, Tom
Heath, Eero Hyvo?nen, Riichiro Mizoguchi, Eyal
Oren, Marta Sabou, and Elena Simperl, editors, The
Semantic Web: Research and Applications, pages
111?125, Berlin/Heidelberg, Germany. Springer.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined ev-
idence. In Proceedings of the 18th ACM conference
on Information and knowledge management (CIKM
?09), CIKM ?09, pages 513?522, New York, NY,
USA. ACM.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Charles J. Fillmore. 1982. Frame Semantics. In The
Linguistic Society of Korea, editor, Linguistics in
the Morning Calm, pages 111?137. Hanshin Pub-
lishing Company, Seoul, Korea.
Gil Francopoulo, Nuria Bel, Monte George, Nico-
letta Calzolari, Monica Monachini, Mandy Pet, and
Claudia Soria. 2006. Lexical Markup Framework
(LMF). In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC), pages 233?236, Genoa, Italy.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Verena Henrich and Erhard Hinrichs. 2010. Standard-
izing wordnets in the ISO standard LMF: Wordnet-
LMF for GermaNet. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING), pages 456?464, Beijing, China.
Richard Johansson and Pierre Nugues. 2007. Us-
ing WordNet to extend FrameNet coverage. In
Proceedings of the Workshop on Building Frame-
semantic Resources for Scandinavian and Baltic
Languages, at NODALIDA, pages 27?30, Tartu, Es-
tonia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. Language Resources and Evalu-
ation, 42:21?40.
Claudia Kunze and Lothar Lemnitzer. 2002. Ger-
maNet ? representation, visualization, application.
In Proceedings of the Third International Con-
ference on Language Resources and Evaluation
(LREC), pages 1485?1491, Las Palmas, Canary Is-
lands, Spain.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press, Chicago,
IL, USA.
Michael Matuschek and Iryna Gurevych. 2011.
Where the journey is headed: Collaboratively con-
structed multilingual Wiki-based resources. In
SFB 538: Mehrsprachigkeit, editor, Hamburger Ar-
beiten zur Mehrsprachigkeit, Hamburg, Germany.
John McCrae, Dennis Spohr, and Philipp Cimiano.
2011. Linking Lexical Resources and Ontologies
on the Semantic Web with Lemon. In The Seman-
tic Web: Research and Applications, volume 6643
of Lecture Notes in Computer Science, pages 245?
259. Springer, Berlin/Heidelberg, Germany.
Clifton J. McFate and Kenneth D. Forbus. 2011.
NULEX: an open-license broad coverage lexicon.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 363?367, Portland, OR, USA.
Christian M. Meyer and Iryna Gurevych. 2010. Worth
its Weight in Gold or Yet Another Resource ?
A Comparative Study of Wiktionary, OpenThe-
saurus and GermaNet. In Alexander Gelbukh, ed-
itor, Computational Linguistics and Intelligent Text
Processing: 11th International Conference, volume
6008 of Lecture Notes in Computer Science, pages
38?49. Berlin/Heidelberg: Springer, Ias?i, Romania.
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Align-
ing Wiktionary and WordNet for Increased Domain
589
Coverage. In Proceedings of the 5th International
Joint Conference on Natural Language Processing
(IJCNLP), pages 883?892, Chiang Mai, Thailand.
Roberto Navigli and Simone Paolo Ponzetto. 2010a.
BabelNet: Building a Very Large Multilingual Se-
mantic Network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 216?225, Uppsala, Sweden, July.
Roberto Navigli and Simone Paolo Ponzetto. 2010b.
Knowledge-rich Word Sense Disambiguation Ri-
valing Supervised Systems. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1522?1531, Uppsala,
Sweden.
Roberto Navigli. 2006. Meaningful Clustering of
Senses Helps Boost Word Sense Disambiguation
Performance. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages
105?112, Sydney, Australia.
Elisabeth Niemann and Iryna Gurevych. 2011. The
People?s Web meets Linguistic Knowledge: Auto-
matic Sense Alignment of Wikipedia and WordNet.
In Proceedings of the 9th International Conference
on Computational Semantics (IWCS), pages 205?
214, Oxford, UK.
Muntsa Padro?, Nu?ria Bel, and Silvia Necsulescu.
2011. Towards the Automatic Merging of Lexical
Resources: Automatic Mapping. In Proceedings of
the International Conference on Recent Advances
in Natural Language Processing (RANLP), pages
296?301, Hissar, Bulgaria.
Martha Palmer. 2009. Semlink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Gen-
erative Lexicon Conference (GenLex-09), pages 9?
15, Pisa, Italy.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. In Proceedings of
the International Conference on Semantic Comput-
ing, pages 517?526, Washington, DC, USA.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings
of the Sixth International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLing), pages 100?111, Mexico City, Mexico.
Claudia Soria, Monica Monachini, and Piek Vossen.
2009. Wordnet-LMF: fleshing out a standardized
format for Wordnet interoperability. In Proceed-
ings of the 2009 International Workshop on Inter-
cultural Collaboration, pages 139?146, Palo Alto,
CA, USA.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, pages 697?706, Banff,
Canada.
Antonio Toral, Stefania Bracale, Monica Monachini,
and Claudia Soria. 2010. Rejuvenating the Italian
WordNet: Upgrading, Standarising, Extending. In
Proceedings of the 5th Global WordNet Conference
(GWC), Bombay, India.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht, Nether-
lands.
590
Transactions of the Association for Computational Linguistics, 1 (2013) 151?164. Action Editor: Patrick Pantel.
Submitted 12/2012; Revised 2/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Dijkstra-WSA: A Graph-Based Approach to Word Sense Alignment
Michael Matuschek ? and Iryna Gurevych ??
? Ubiquitous Knowledge Processing Lab (UKP-DIPF),
German Institute for Educational Research and Educational Information
Schlo?str. 29, 60486 Frankfurt, Germany
?Ubiquitous Knowledge Processing Lab (UKP-TUDA),
Department of Computer Science, Technische Universita?t Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we present Dijkstra-WSA, a
novel graph-based algorithm for word sense
alignment. We evaluate it on four different
pairs of lexical-semantic resources with dif-
ferent characteristics (WordNet-OmegaWiki,
WordNet-Wiktionary, GermaNet-Wiktionary
and WordNet-Wikipedia) and show that it
achieves competitive performance on 3 out
of 4 datasets. Dijkstra-WSA outperforms the
state of the art on every dataset if it is com-
bined with a back-off based on gloss similar-
ity. We also demonstrate that Dijkstra-WSA
is not only flexibly applicable to different re-
sources but also highly parameterizable to op-
timize for precision or recall.
1 Introduction
Lexical-semantic resources (LSRs) are a corner-
stone for many Natural Language Processing (NLP)
applications such as word sense disambiguation
(WSD) and information extraction. However, the
growing demand for large-scale resources in dif-
ferent languages is hard to meet. The Princeton
WordNet (WN) (Fellbaum, 1998) is widely used for
English, but for most languages corresponding re-
sources are considerably smaller or missing. Col-
laboratively constructed resources like Wiktionary
(WKT) and OmegaWiki (OW) provide a viable op-
tion for such cases and seem especially suitable
for smaller languages (Matuschek et al, 2013), but
there are still considerable gaps in coverage which
need to be filled. A related problem is that there
usually does not exist a single resource which works
best for all purposes, as different LSRs cover differ-
ent words, senses and information types.
These considerations have sparked increasing re-
search efforts in the area of word sense alignment
(WSA). It has been shown that aligned resources
can indeed lead to better performance than using the
resources individually. Examples include seman-
tic parsing using FrameNet (FN), WN, and VerbNet
(VN) (Shi and Mihalcea, 2005), word sense disam-
biguation using an alignment of WN and Wikipedia
(WP) (Navigli and Ponzetto, 2012) and semantic
role labeling using a combination of PropBank, VN
and FN in the SemLink project (Palmer, 2009).
Some of these approaches to WSA either rely heav-
ily on manual labor (e.g. Shi and Mihalcea (2005))
or on information which is only present in few
resources such as the most frequent sense (MFS)
(Suchanek et al, 2008). This makes it difficult to
apply them to a larger set of resources.
In earlier work, we presented the large-scale re-
source UBY (Gurevych et al, 2012). It contains
nine resources in two languages which are mapped
to a uniform representation using the LMF standard
(Eckle-Kohler et al, 2012). They are thus struc-
turally interoperable. UBY contains pairwise sense
alignments between a subset of these resources, and
this work also presented a framework for creat-
ing alignments based on the similarity of glosses
(Meyer and Gurevych, 2011). However, it is not
clear to what extent this approach can be applied to
resources which lack this kind of information (see
Section 3).
In summary, aligning senses is a key requirement
for semantic interoperability of LSRs to increase the
151
coverage and effectiveness in NLP tasks. Still, exist-
ing efforts are mostly focused on specific types of re-
sources (most often requiring glosses) or application
scenarios. In this paper, we propose an approach to
alleviate this and present Dijkstra-WSA, a novel, ro-
bust algorithm for word sense alignment which is
applicable to a wide variety of resource pairs and
languages. For the first time, we apply a graph-based
algorithm which works on full graph representations
of both resources to word sense alignment. This en-
ables us to take a more abstract perspective and re-
duce the problem of identifying equivalent senses to
the problem of matching nodes in these graphs. Also
for the first time, we comparatively evaluate a WSA
algorithm on a variety of different datasets with dif-
ferent characteristics.
The key properties of Dijkstra-WSA are:
Robustness The entities within the LSRs which
are to be aligned (usually senses or synsets) are mod-
eled as nodes in the graph. These nodes are con-
nected by an edge if they are semantically related.
While, for instance, semantic relations lend them-
selves very well to deriving edges, different possi-
bilities for graph construction are equally valid as
the algorithm is agnostic to the origin of the edges.
Language-independence No external resources
such as corpora or other dictionaries are needed; the
graph construction and alignment only rely on the
information from the considered LSRs.
Flexibility The graph construction as well as the
actual alignment are highly parameterizable to ac-
commodate different requirements regarding preci-
sion or recall.
The rest of this paper is structured as follows:
In Section 2 we give a precise problem description
and introduce the resources covered in our experi-
ments, in Section 3 we discuss some related work,
while our graph-based algorithm Dijkstra-WSA is
presented in Section 4. We describe an evaluation
on four datasets with different properties, including
an error analysis, in Section 5 and conclude in Sec-
tion 6, pointing out directions for future work.
2 Notation and Resources
2.1 Problem Description
A word sense alignment, or alignment for short, is
formally defined as a list of pairs of senses from
two LSRs. A pair of aligned senses denote the same
meaning. E.g., the two senses of letter ?The conven-
tional characters of the alphabet used to represent
speech? and ?A symbol in an alphabet, bookstave?
(taken from WN and WKT, respectively) are clearly
equivalent and should be aligned.
2.2 Evaluation Resources
For the evaluation of Dijkstra-WSA, we align four
pairs of LSRs used in previous work, namely WN-
OW (Gurevych et al, 2012), WN-WKT (Meyer and
Gurevych, 2011), GN-WKT (Henrich et al, 2011)
and WN-WP (Niemann and Gurevych, 2011). Our
goal is to cover resources with different character-
istics: Expert-built (WN, GN) and collaboratively
constructed LSRs (WP, WKT, OW), resources in
different languages (English and German) and also
resources with few sense descriptions (GN) or se-
mantic relations (WKT). We contrastively discuss
the results of the Dijkstra-WSA algorithm on these
different datasets and relate the results to the prop-
erties of the LSRs involved. Moreover, using exist-
ing datasets ensures comparability to previous work
which discusses only one dataset at a time.
WordNet (WN) (Fellbaum, 1998) is a lexical re-
source for the English language created at Princeton
University. The resource is organized in sets of syn-
onymous words (synsets) which are represented by
glosses (sometimes accompanied by example sen-
tences) and organized in a hierarchy. The latest ver-
sion 3.0 contains 117,659 synsets.
Wikipedia (WP) is a freely available, multilin-
gual online encyclopedia. WP can be edited by ev-
ery Web user, which causes rapid growth: By Febru-
ary 2013 the English WP contained over 4,000,000
article pages. Each article usually describes a dis-
tinct concept, and articles are connected by hyper-
links within the article texts.
Wiktionary (WKT) is the dictionary pendant to
WP. By February 2013 the English WKT contained
over 3,200,000 article pages, while the German edi-
tion contained over 200,000 ones. For each word,
multiple senses can be encoded. Similar to WN,
they are represented by a gloss and usage exam-
ples. There also exist hyperlinks to synonyms, hy-
pernyms, meronyms etc. The targets of these rela-
tions are not senses, however, but merely lexemes
(i.e. the relations are not disambiguated).
152
LSRs P /R/F1/Acc. Approach
Meyer and Gurevych (2011) WN-WKT 0.67/0.65/0.66/0.91 Gloss similarity + Machine learning
Niemann and Gurevych (2011) WN-WP 0.78/0.78/0.78/0.95 Gloss similarity + Machine learning
Henrich et al (2011) GN-WKT 0.84/0.85/0.84/0.94 Pseudo-gloss overlap
de Melo and Weikum (2010) WN-WP 0.86/NA/NA/NA Gloss/article overlap
Laparra et al (2010) FN-WN 0.79/0.79/0.79/NA Dijkstra-SSI+ (WSD algorithm)
Navigli (2009) WN 0.64/0.64/0.64/NA Graph-based WSD of WN glosses
Ponzetto and Navigli (2009) WN-WP NA/NA/NA/0.81 Graph-based, only for WP categories
Navigli and Ponzetto (2012) WN-WP 0.81/0.75/0.78/0.83 Graph-based WSA using WN relations
Table 1: Summary of various approaches to WSA. ?NA? stands for ?Not Available?.
OmegaWiki (OW) is a freely editable online
dictionary like WKT. However, there do not ex-
ist distinct language editions as OW is organized
in language-independent concepts (?Defined Mean-
ings?) to which lexicalizations in various languages
are attached. These can be considered as multilin-
gual synsets, and they are interconnected by unam-
biguous relations just like WN. As of February 2013,
OW contains over 46,000 of these concepts and lex-
icalizations in over 400 languages.
GermaNet (GN) is the German counterpart to
WN (Hamp and Feldweg, 1997). It is also organized
in synsets (around 70,000 in the latest version 7.0)
which are connected via semantic relations.
3 Related Work
The are two strands of closely related work:
Similarity-based and graph-based approaches to
word sense alignment. To our knowledge, there ex-
ists no previous work which fully represents both
LSRs involved in an alignment as graphs. We give a
summary of different approaches in Table 1.
3.1 Similarity-based Approaches
Niemann and Gurevych (2011) and Meyer and
Gurevych (2011) created WN-WP and WN-WKT
alignments using a framework which first calcu-
lates the similarity of glosses (or glosses and ar-
ticles in the case of WN-WP) using either cosine
or personalized page rank (PPR) similarity (Agirre
and Soroa, 2009) and then learns a threshold on the
gold standard to classify each pair of senses as a
(non-)valid alignment. This approach was later ex-
tended to cross-lingual alignment between the Ger-
man OW and WN (Gurevych et al, 2012) using a
machine translation component. However, its appli-
cability depends on the availability and quality of the
glosses, which are not present in every case (e.g. for
VN). Moreover, as it involves supervised machine
learning, it requires the initial effort of manually an-
notating a sufficient amount of training data. Hen-
rich et al (2011) use a similar approach for align-
ing GN and WKT. However, they use word over-
lap as a similarity measure and do not require a ma-
chine learning component as they align to the can-
didate sense with the highest similarity regardless of
the absolute value. The alignment of WP articles
and WN synsets reported by de Melo and Weikum
(2010) also relies on word overlap.
Although these approaches give reasonable re-
sults (with precision in the range of 0.67-0.84), they
all depend on the lexical knowledge contained in the
glosses, yielding low recall if there is insufficient
lexical overlap (known as the ?lexical gap?, see for
instance (Meyer and Gurevych, 2011)). Consider
these two senses of Thessalonian in WKT and WN:
?A native or inhabitant of Thessalonica? and ?Some-
one or something from, or pertaining to, Thessa-
loniki?. These are (mostly) identical and should be
aligned, but there is no word overlap due to the in-
terchangeable usage of the synonyms Thessalonica
and Thessaloniki.
3.2 Graph-based Approaches
Laparra et al (2010) utilize the SSI-Dijkstra+ al-
gorithm to align FN lexical units (LUs) with WN
synsets. The basic idea is to align monosemous LUs
first and, based on this, find the closest synset in WN
for the other LUs in the same frame. However, as
SSI-Dijkstra+ is a word sense disambiguation (not
alignment) algorithm, the LUs are merely consid-
ered as texts which are to be disambiguated; there
is no attempt made to build a global graph structure
for FN. Moreover, the algorithm solely relies on the
153
semantic relations found in WN and eXtended WN
(Mihalcea and Moldovan, 2001). Thus, it is not ap-
plicable to other resources which have no or only
few relations such as WKT.
Navigli (2009) aims at disambiguating WN
glosses, i.e. assigning the correct senses to all non-
stopwords in each WN gloss. His approach is to
find the shortest possible circles in the WN relation
graph to identify the correct disambiguation. In later
work, this idea was extended to the disambiguation
of translations in a bilingual dictionary (Flati and
Navigli, 2012). However, there is no discussion of
how this idea could be applied to word sense align-
ment of two or more resources. We build upon this
idea of finding shortest paths (circles are a special
kind of path) and extend it to multiple resources and
edges other than semantic relations, in particular WP
links and links to senses of monosemous lexemes
appearing in glosses.
Ponzetto and Navigli (2009) propose a graph-
based method to tackle the related, but slightly dif-
ferent problem of aligning WN synsets and WP cat-
egories (not articles). Using semantic relations, they
build WN subgraphs for each WP category and then
align those synsets which best match the category
structure. In later work, Navigli and Ponzetto (2012)
also align WN with the full WP. They build ?disam-
biguation contexts? for the senses in both resources
by using, for instance, WP redirects or WN glosses
and then compute the similarity between these con-
texts. Again, a graph structure is built from WN se-
mantic relations covering all possible senses in these
contexts. The goal is to determine which WN sense
is closest to the WP sense to be aligned. While these
approaches are in some respects similar to Dijkstra-
WSA, they do not take the global structure of both
resources into account. Instead, they merely rely
on a (locally restricted) subset of WN relations for
creating the alignment. Thus, applying these ap-
proaches to resources in different languages might
be difficult if WN relations are not applicable.
4 Dijkstra-WSA
In this section, we discuss our approach to aligning
lexical-semantic resources based on the graph struc-
ture. This includes two steps: (i) the initial construc-
tion of the graphs using appropriate parameters, and
(ii) the alignment itself.
4.1 Graph Construction
We represent the set of senses (or synsets, if appli-
cable) of an LSR L as a set of nodes V where the set
of edges E,E ? V ? V between these nodes rep-
resents semantic relatedness between them. We call
this a resource graph. A WP article is considered a
sense as it represents a distinct concept.
There are multiple options for deriving the edges
from the resource. The most straightforward ap-
proach is to directly use the existing semantic rela-
tions (such as hyponymy), as it has been reported in
previous work (Laparra et al, 2010; Navigli, 2009).
For WP, we can directly use the given hyperlinks be-
tween articles as they also express a certain degree
of relatedness (Milne and Witten, 2008). However,
for many LSRs no or only few semantic relations
exist. Consider WKT: Its relations are not sense dis-
ambiguated (Meyer and Gurevych, 2012). We thus
cannot determine the correct target sense if a relation
is pointing to an ambiguous word.
Our solution to this is twofold: First, for each
sense s, we create an edge (s, t) for those seman-
tic relations which have a monosemous target t, as
in this case the target sense is unambiguous. This
approach, however, only recovers a subset of the re-
lations, and it is not applicable to resources where
no sense relations exist at all, e.g. FN. For this case,
we propose to use the glosses of senses in the LSR
to derive additional edges in the following way: For
each monosemous, non-stopword lexeme l (a com-
bination of lemma and part of speech) in the gloss
of a sense s1 with a sense sl, we introduce an edge
(s1, sl). Moreover, if there is another sense s2 with l
in its gloss, we also introduce an edge (s1, s2). This
technique will be called linking of monosemous lex-
emes or monosemous linking throughout the rest of
this paper. The intuition behind this is that monose-
mous lexemes usually have a rather specific mean-
ing, and thus it can be expected that the senses in
whose description they appear have at least a certain
degree of semantic relationship. This directly re-
lates to the notion of ?information content? (Resnik,
1995), stating that senses in an LSR which are more
specific (and hence more likely to be monosemous)
are more useful for evaluating semantic similarity.
Note that this step requires part of speech tagging
154
of the glosses, which we perform as a preprocess-
ing step. Thereby we filter out stopwords and words
tagged as ?unknown? by the POS tagger.
As an example, consider the gloss of Java: ?An
object-oriented programming language?. Even in
the absence of any semantic relations, we could
unambiguously derive an edge between this sense
of Java and the multiword noun programming lan-
guage if the latter is monosemous, i.e. if there exists
exactly one sense for this lexeme in the LSR. Also,
if programming language appears in the gloss of one
of the senses of Python, we can derive an edge be-
tween these senses of Java and Python, expressing
that they are semantically related.
An important factor to keep in mind, however, is
the density of the resulting graph. In preliminary ex-
periments, we discovered that linking every monose-
mous lexeme yielded very dense graphs with short
paths between most senses. In turn, we decided to
exclude ?common? lexemes and focus on more spe-
cific ones in order to increase the graph?s meaning-
fulness. The indicator for this is the frequency of a
lexeme in the LSR, i.e. how often it occurs in the
glosses. Our experiments on small development sets
(100 random samples of each gold standard) indeed
show that a strict filter leads to discriminative edges
resulting in high precision, while at the same time
graph sparsity decreases recall. Independently of
the resource pair, we discovered that setting this fre-
quency limit value ? to about 1/100 of the graph size
(e.g. 1,000 for a graph containing 100,000 senses)
gives the best balance between precision and recall;
larger values of ? usually led to no significant im-
provement1 in recall while the precision was contin-
uously degrading. Note that WP was excluded from
these experiments as the identification and linking of
monosemous lexemes in all WP articles proved too
time-consuming; instead, we decided to use only the
already given links (see Section 5.3).
4.2 Computing Sense Alignments
Initialization After resource graphs for both LSRs
A and B are created, the trivial alignments are re-
trieved and introduced as edges between them. Triv-
ial alignments are those between senses which have
1All significance statements throughout the paper are based
on McNemar?s test and the confidence level of 1%.
Dijkstra-WSA(A,B)
1 ASenseSet = A.senses
2 BSenseSet = B.senses
3 UnalignableSenses = ?
4
5 foreach sense s ? ASenseSet
6 if(s.isMonosemous)
7 t = findTrivialMatch(s, BSenseSet)
8 if(t != null)
9 ASenseSet.remove(s)
10 BSenseSet.remove(t)
11 createEdge(s,t)
12
13 foreach sense s? ? ASenseSet
14 ASenseSet.remove(s?)
15 T=findCandidatesWithSameLexeme(s?, B)
16 if(T!= ?)
17 t?=findShortestPathToCandidates(s?, T)
18 if(t? != null)
19 createEdge(s?, t?)
20 else
21 UnalignableSenses.put(s?)
22 else
23 UnalignableSenses.put(s?)
Table 2: Pseudocode of the Dijkstra-WSA algorithm.
the same attached lexeme inA andB and where this
lexeme is also monosemous within either resource.
E.g., if the noun phrase programming language is
contained in either resource and has exactly one
sense in each one, we can directly infer the align-
ment. For WP, a lexeme was considered monose-
mous if there was exactly one article with this title,
also counting titles with a bracketed disambiguation
(e.g., Java (programming language) and Java (is-
land) are two distinct senses of Java). While this
method does not work perfectly, we observed a pre-
cision> 0.95 for monosemous gold standard senses,
which is in line with the observations by Henrich et
al. (2011).
Alignment We consider each sense s ? A which
has not been aligned in the initialization step. For
this, we first retrieve the set of possible target senses
T ? B (those with matching lemma and part of
speech) and compute the shortest path to each of
them with Dijkstra?s shortest path algorithm (Dijk-
155
stra, 1959). The candidate t ? T with the shortest
distance is then assigned as the alignment target, and
the algorithm continues with the next still unaligned
sense in A until either all senses are aligned or no
path can be found for the remaining senses. The in-
tuition behind this is that the trivial alignments from
the initialization serve as ?bridges? between A and
B, such that a path starting from a sense s1 in A tra-
verses edges to find a nearby already aligned sense
s2, ?jumps? to B using a cross-resource edge lead-
ing to t2 and then ideally finds an appropriate target
sense t1 in the vicinity of t2. Note that with each
successful alignment, edges are added to the graph
so that, in theory, a different ordering of the consid-
ered senses would lead to different results. While
we observed slight differences for repeated runs us-
ing the same configuration, these were in no case
statistically significant. The pseudo code of this al-
gorithm can be found in Table 2, while an example
can be found in Figure 1.
Figure 1: An example of how Dijkstra-WSA works.
While there exist 2 candidates for aligning a sense s1 ? A
(dashed lines) (a), the correct one t1 ? B can be deter-
mined by finding the shortest path using an already es-
tablished edge between two monosemous senses s2 ? A
and t2 ? B (solid line) (b).
Parameter Influence Apart from the already
mentioned parameter ? for limiting the number of
edges in the graph, another important variable is the
maximum allowed path length ? of Dijkstra?s algo-
rithm. In general, allowing an unbounded search for
the candidate senses is undesirable as long paths,
while increasing recall, usually also lead to a de-
crease in precision, as the nodes which can be
reached in many steps are usually also semantically
distant from the source sense. In this respect, we
found significant differences between the optimal
configuration for individual resource pairs. How-
ever, the general observation is that short paths (? ?
3) lead to a very high precision, while paths longer
than 10 do not increase recall significantly any more.
A modification of the algorithm is to not only
align the closest target sense, but all senses which
can be reached with a certain number of steps. This
caters to the fact that, due to different sense granu-
larities, one coarser sense inA can be represented by
several senses in B and vice versa (see Table 3 for
the fraction of 1:n alignments in the datasets). Re-
garding this modification, we made the observation
that the recall improved (sometimes considerably),
but at the same time the precision decreased, some-
times to an extent where the overall F-Measure (the
harmonic mean of precision and recall) got worse.
In the evaluation section, we state which setting is
used for which datasets and configurations.
5 Experimental Work
5.1 Datasets and their Characteristics
WN 3.0-English OW The previous alignment be-
tween these two resources reported in Gurevych et
al. (2012) is based on the German OW (database
dump from 2010/01/03) and WN 3.0 and utilizes
gloss similarities using machine translation as an in-
termediate component. This does not pose a prob-
lem since for each synset in the German part of OW,
there is a translation in the English part. This makes
the German-English gold standard directly usable
for our purposes.2 Table 3 presents the details about
this as well as the other evaluation datasets, includ-
ing the observed inter-rater agreement A0 (where
available) which can be considered as an upper
bound for automatic alignment accuracy and the de-
gree of polysemy (i.e. the number of possible align-
ment targets per sense) which is a hint towards the
difficulty of the alignment task.
WN 3.0-English WKT We use the gold stan-
dard dataset from Meyer and Gurevych (2011) with-
out any modification, thus for comparability to this
2Cross-lingual alignment is left to future work.
156
work, we use the same WKT dump version (from
2010/02/01) which contains around 421,000 senses.
GN 7.0-German WKT Henrich et al (2011)
aligned the German WKT (dump from 2011/04/02,
72,000 senses) and GN 7.0. This is the only existing
alignment between these two resources so far, and
we use their freely available dataset3 to test Dijkstra-
WSA on a language other than English. As this
alignment is fairly large (see Table 3), we created a
random sample as a gold standard to keep the com-
putation time at bay. However, the datasets are still
similar enough to allow direct comparison of the
results. Note that no inter-annotator agreement is
available for this study.
WN 3.0-English WP We use the gold standard
from Niemann and Gurevych (2011). For compa-
rability, we use the same Wikipeda dump version
(from 2009/08/22) with around 2,921,000 articles.
5.2 Baselines
WN-OW We used the same configuration as in
Gurevych et al (2012) to calculate a similarity-
based alignment for the monolingual case (i.e. with-
out the translation step) as a baseline and achieved
comparable results.
WN-WKT As stated above, the alignment4 pre-
sented in Meyer and Gurevych (2011) was created
by calculating the similarity of glosses and training
a machine-learning classifier on the gold standard to
classify each pair of senses.
GN-WKT The automatic alignment results (i.e.
the outcome of the algorithm without manual post-
correction) reported by Henrich et al (2011) were
unavailable for us as a baseline. Thus, we utilize
the alignment approach by Meyer and Gurevych
(2011) to create a similarity-based baseline, with mi-
nor modifications. Unlike the original approach, we
directly align senses regardless of their similarity if
the decision is trivial (see Section 4.2). We also do
not train a machine learning component on a gold
standard. Instead, we adapt the idea of Henrich et al
(2011) to align the most similar candidate regardless
of the absolute value.
WN-WP The alignment reported in Niemann and
Gurevych (2011) was created in the same way as
3http://www.sfs.uni-tuebingen.de/GermaNet/wiktionary.shtml
4Available at http://www.ukp.tu-darmstadt.de/data/lexical-
resources/wordnet-wiktionary-alignment/
the WN-WKT alignment described in Meyer and
Gurevych (2011). Note that while the full alignment
results5 proved incomplete, the correct alignment re-
sults on the gold standard were available and thus
used in our experiments.
We will henceforth mark these similarity-based
results with SB.
5.3 System Configurations
For the construction of the resource graphs we ex-
perimented with three options:
Semantic relations only (SR) OW, WN and GN
all feature disambiguated sense relations which can
be directly used as edges between senses. Note that
in the expert-built resources, the majority of nodes
are connected by sense relations, while this is not
the case for OW. For WKT, only the unambiguous
semantic relations can be used (see Section 4.1), re-
sulting in graphs less dense and with many isolated
nodes. However, as we reported in Matuschek et
al. (2013), the English WKT is almost 6 times as
large as the German one for the versions we used in
our experiments (421,000 senses vs. 72,000 senses),
while it contains not even twice as many relations
(720,000 vs. 430,000). This is directly reflected in
the fewer isolated nodes for the German WKT. WP
links are also unambiguous as they lead to a distinct
article. However, intuitively not all links in an arti-
cle are equally meaningful. Thus, for the SR config-
uration, we decided to retain only the category links
and the links within the first paragraph of the article.
We assume that the targets of these links are most
closely related to the sense an article represents as
the first paragraph usually includes a concise defini-
tion of a concept, and the category links allow deter-
mining the topic an article belongs to.
Linking of monosemous lexemes only (LM) For
this configuration, the limiting parameter ? was set
to 1/100 of the graph size for every resource ex-
cept WP as described in section 4.1. As our ex-
periments show, linking the monosemous lexemes in
the glosses while disregarding semantic relations re-
sults in well-connected graphs for all resources but
GN and WKT. Only about 10% of the GN senses
have a gloss, thus this option was completely disre-
5Available at http://www.ukp.tu-darmstadt.de/data/lexical-
resources/wordnet-wikipedia-alignment/
157
Pair Aligned Not Aligned Sum 1:n Alignments % Polysemy Sampling A0
WN-OW 210 473 683 10.7% 1.50 Random 0.85
WN-WKT 313 2,110 2,423 2.7% 4.76 Balanced 0.93
GN-WKT (full) 27,127 18,509 45,636 5.6% 1.78 All N/A
GN-WKT (sample) 1,000 751 1,751 4.8% 1.84 Random N/A
WN-WP 227 1,588 1,815 5.2% 5.7 Balanced 0.97
Table 3: Characteristics of the gold standards used in the evaluation. A0 is the observed inter-rater agreement which
can be considered as an upper bound for alignment accuracy. The degree of polysemy (i.e. the number of possible
alignment targets per sense) hints towards the difficulty of the alignment task.
garded in this case. For both WKTs, an analysis of
the graphs revealed that the reason for the relatively
high number of isolated nodes are very short glosses,
containing many polysemous lexemes. For WP, we
refrained from monosemous linking due to the pro-
hibitive computation time. Instead, we decided to
use the fully linked WP (excluding the links used
for the SR configuration) in this case. The rationale
is that in the majority of articles many meaningful
terms link to the corresponding articles anyway, so
that the resulting graph is comparable with those for
the other LSRs.
Combining both (SR+LM) This configuration
yields the maximum number of available edges. We
report the results for GN only for this configuration
and omit the SR results for the sake of brevity as
the influence on the F-Measure for the GN-WKT
alignment (see Section 5.4) is not statistically sig-
nificant. For WKT, this configuration only increases
the number of connected nodes slightly (as insuffi-
cient glosses often coincide with missing semantic
relations), while for OW an almost connected graph
can be constructed.
Table 4 gives an overview of the fraction of iso-
lated nodes for each resource in every configuration.
Note again that for each alignment task (i.e. each
pair of resources), we tuned the parameters on 100
random samples from each gold standard for a result
balancing precision and recall as discussed above.
Individual tuning of parameters was necessary for
each pair due to the greatly varying properties of the
LSRs (e.g. the number of senses). While it would
have been ideal to train and test on disjoint sets, we
calculated the overall results on the full gold stan-
dards including the development sets to ensure com-
parability with the previous work.
Hybrid Approach Manual inspection of the re-
sults revealed that the alignments found by Dijkstra-
Resource SR LM SR+LM
WN 0.25 0.07 0.02
GN 0.0 0.92 0.0
WKT-en 0.98 0.32 0.30
WKT-de 0.69 0.18 0.15
OW 0.41 0.33 0.04
WP 0.06 0.05 0.04
Table 4: This table describes what percentage of nodes
remains isolated (i.e. with 0 attached edges) in differ-
ent graph configurations using semantic relations (SR),
monosemous linking (LM) (? = 1/100) or both
(SR+LM). Note that this number is highest for the En-
glish WKT as the few semantic relations and short
glosses do not offer many possibilities for connecting
nodes, while the German WKT and OW do not suffer
from this problem as much. GN is fully linked via re-
lations, but has only few glosses which makes monose-
mous linking ineffective. WN and WP are relatively well-
linked in all configurations. Also note that for WP, SR
means that we used category links and links from the first
paragraph, while links from the rest of the article were
used for the LM configuration.
WSA are usually different from those based on the
gloss similarity. While the latter precisely recog-
nizes alignments with similar wording of glosses,
Dijkstra-WSA is advantageous if the glosses are dif-
ferent but the senses are still semantically close in
the graph. Section 5.5 will analyze this in greater
detail. Exploiting this fact, we experimented with
a hybrid approach: We perform an alignment us-
ing Dijkstra-WSA, tuned for high precision (i.e. us-
ing shorter path lengths) and fall back to using the
results of the similarity-based approaches for those
cases where no alignment target could be found in
the graph. These results are marked with +SB in the
result overview (Table 5).
158
WordNet-OmegaWiki WordNet-Wiktionary GermaNet-Wiktionary WordNet-Wikipedia
P R F1 Acc. P R F1 Acc. P R F1 Acc. P R F1 Acc.
Random 0.46 0.35 0.40 0.51 0.21 0.59 0.31 0.67 0.44 0.51 0.47 0.54 0.49 0.62 0.53 0.86
SB 0.55 0.53 0.54 0.73 0.67 0.65 0.66 0.91 0.93 0.74 0.83 0.83 0.78 0.78 0.78 0.95
SR 0.66 0.45 0.53 0.76 0.95 0.13 0.23 0.89 0.94 0.65 0.77 0.78 0.82 0.63 0.71 0.93
LM 0.62 0.54 0.58 0.77 0.72 0.24 0.36 0.89 0.89 0.75 0.81 0.80 0.65 0.66 0.65 0.91
SR+LM 0.56 0.69 0.62 0.74 0.68 0.27 0.39 0.89 0.90 0.78 0.83 0.82 0.75 0.67 0.71 0.93
SR+SB 0.60 0.65 0.63 0.76 0.68 0.67 0.68 0.92 0.90 0.82 0.86 0.84 0.75 0.87 0.81 0.95
LM+SB 0.60 0.70 0.64 0.76 0.68 0.70 0.69 0.92 0.86 0.87 0.87 0.85 0.70 0.87 0.78 0.94
SR+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95
A0 - - - 0.85 - - - 0.93 - - - N/A - - - 0.97
Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links
(LM) or both (SR+LM). The similarity-based (SB) baselines, also used as a back-off for the hybrid approaches (+SB),
were created using the approach reported in Gurevych et al (2012). Note that for GN, the SR+LM configuration was
always used. The different configurations given for this alignment thus only apply to WKT. For WP, SR means that
only category links and links within the first paragraph were used, while LM uses links from the full article. A random
baseline and the inter-annotator agreement A0 of the gold standard annotation (if available) are given for reference.
5.4 Experimental Results
WN-OW When using only semantic relations (SR),
we achieved an F-Measure of 0.53 which is com-
parable with the 0.54 from Gurevych et al (2012).
Notably, our approach has a high precision, while
the recall is considerably worse due to the relative
sparsity of the resulting OW resource graph. When
adding more edges to the graph by linking monose-
mous lexemes (SR+LM), we can drastically improve
the recall, leading to an overall F-Measure of 0.62,
which is a significant improvement over our previ-
ous results (Gurevych et al, 2012). Using monose-
mous links only (LM), the result of 0.58 still out-
performs Gurevych et al (2012) due to the higher
precision. Building a graph from glosses alone is
thus a viable approach if no or only few semantic
relations are available. Regarding the path lengths,
? = 10 works best when semantic relations are in-
cluded in the graph, while for the LM configura-
tion shorter paths (? ? 5) were more appropriate.
The intuition behind this is that for semantic rela-
tions, unlike monosemous links, even longer paths
still express a high degree of semantic relatedness.
Also, when semantic relations are involved allow-
ing multiple alignments increases the overall results
(which is in line with the relatively high number of
1:n alignments in the gold standard), while this is not
the case for the LM configuration; here, the edges
again do not sufficiently express relatedness.
Using the hybrid approach (+SB), we can increase
the F-Measure up to 0.65 if semantic relations and
monosemous linking are combined (SR+LM) and
the parameters are tuned for high precision (? ? 3,
1:1 alignments). This is significantly better than
Dijkstra-WSA alone in any configuration. In this
scenario, we also observe the best overall recall.
WN-WKT Experiments using only the semantic
relations (SR) yield a very low recall - the small
number of sense relations with monosemous targets
in WKT leaves the graph very sparse. Nevertheless,
the alignment targets which Dijkstra-WSA finds are
mostly correct, with a precision greater than 0.95
even when allowing 1:n alignments. Using only
monosemous links (LM) improves the recall consid-
erably, but unlike the WN-OW alignment, it stays
fairly low. Consequently, even when using seman-
tic relations and monosemous links in conjunction
(SR+LM), the recall can only be increased slightly,
leading to an overall F-Measure of 0.39. As men-
tioned above, this is due to the WKT glosses. In
many cases, they are very short, often consisting
of only 3-5 words, many of which are polysemous.
This leads to many isolated nodes in the graph with
no or only very few connecting edges. The ideal,
rather short path length ? of 2-3 stems from the rela-
tively high polysemy of the gold standard (see Table
3). We experimented with ? ? 4, achieving rea-
sonable recall, but in this case the precision was so
low that this configuration, in conclusion, does not
increase the F-Measure. However, 1:n alignments
work well with these short paths as the correct align-
ments are mostly in the close vicinity of a sense,
159
hence we achieve an increase in recall in this case
without too much loss of precision.
For the hybrid approach, we achieve an
F-Measure of 0.69 when using all edges
(SR+LM+SB), setting the path length to 2, and
also allowing 1:n alignments. This is a statistically
significant improvement over Meyer and Gurevych
(2011) which again confirms the effectiveness of
the hybrid approach.
GN-WKT As stated above, we used the SR+LM
configuration for GN in every case. For the German
WKT, the much greater number of relations com-
pared to its English counterpart is directly reflected
in the results, as using the semantic relations only
(SR) not only yields the best precision of 0.94 but
also a good recall of 0.65. Using the semantic re-
lations together with monosemous links (SR+LM)
yields the F-Measure of 0.83, which is on par with
the similarity-based (SB) approach.
In the hybrid configuration, we can increase
the performance to an F-Measure of up to
0.87 (SR+LM+SB), significantly outperforming all
graph-based and similarity-based configurations.
In general, results for this pair of LSRs are higher
in comparison with the others. We attribute this to
the fact that the German WKT and GN both are
densely linked with semantic relations which is es-
pecially beneficial for the recall of Dijkstra-WSA.
This is also reflected in the ideal ? of 10-12: Many
high-confidence edges allow long paths which still
express a considerable degree of relatedness. How-
ever, while the results for 1:n alignments are al-
ready good, restricting oneself to 1:1 alignments
gives the best overall results as the precision can
then be pushed towards 0.90 without hurting recall
too much. An important factor in this respect is that
the GN-WKT dataset has a relatively low degree of
polysemy (compared to WN-WKT) and only few
1:n alignments (compared to WN-OW), two facts
which make the task significantly easier.
WN-WP The SR configuration (WN relations +
WP category/first paragraph links) yields the best
precision (0.82), even outperforming the SB ap-
proach, and an F-Measure of 0.71. This again shows
that using an appropriate parametrization (? ? 4
in this case) Dijkstra-WSA can detect alignments
with high confidence. The relatively low recall of
0.63 could be increased by allowing longer paths,
however, as hyperlinks do not express relatedness as
reliably as semantic relations, this introduces many
false positives and thus hurts precision considerably.
This issue of ?misleading? WP links becomes even
more prominent when the links from the full articles
are used as edges (LM); while the increase in recall
is relatively small the precision drops substantially.
However, using all possible links (SR+LM) allows
us to balance out precision and recall to some extent,
while yielding the same F-Measure as the SR config-
uration. Note that 1:1 alignments were enforced in
any case, as the high polysemy of the dataset in con-
junction with the dense WP link structure rendered
1:n alignments very imprecise.
Using the hybrid approach, we can increase the F-
Measure up to 0.81 (SR+SB), outperforming the re-
sults reported in Niemann and Gurevych (2011) by
a significant margin. The F-Measure for LM+SB is
slightly worse due to the lower precision. Combin-
ing all edges (SR+LM+SB) does not influence the
results any more, but in any case the hybrid configu-
ration achieves the best overall recall (0.87).
In conclusion, our experiments on all four
datasets consistently demonstrate that combining
Dijkstra-WSA with a similarity-based approach as
a back-off yields the strongest performance. The re-
sults of these best alignments will be made freely
available to the research community on our website
(http://www.ukp.tu-darmstadt.de).
5.5 Error Analyis
The by far most significant error source, reflected in
the relatively low recall for different configurations,
is the high number of false negatives, i.e. sense pairs
which were not aligned although they should have
been. This is especially striking for the WN-WKT
alignment. As discussed earlier, WKT contains a
significant number of short glosses, which in many
cases also contain few or no monosemous terms. A
prototypical example is the first sense of seedling:
?A young plant grown from seed?. This gloss has
no monosemous words which could be linked, and
as there are also no semantic relations attached to
this sense which could be exploited, the node is iso-
lated in the graph. Our experiments show that for
the English WKT, even when optimizing the param-
eters for recall, around 30% of the senses remain
isolated, i.e. without edges. This is by far the high-
160
est value across all resources (see Table 4). Solv-
ing this problem would require making the graph
more dense, and especially finding ways to include
isolated nodes as well. However, this example also
shows why the hybrid approach works so well: The
correct WN sense ?young plant or tree grown from
a seed? was recognized by the similarity-based ap-
proach with high confidence.
With regard to false positives, Dijkstra-WSA and
the similarity-based approaches display very similar
performance. This is because senses with very sim-
ilar wording are likely to share the same monose-
mous words, leading to a close vicinity in the graph
and the false alignment. As an example, consider
two senses of bowdlerization in WN (?written mate-
rial that has been bowdlerized?) and WKT (?The ac-
tion or instance of bowdlerizing; the omission or re-
moval of material considered vulgar or indecent.?).
While these senses are clearly related, they are not
identical and should not be aligned, nevertheless the
similar wording (and especially the use of the highly
specific verb ?bowdlerize?) results in an alignment.
Similarly to the similarity-based approaches, it is an
open question how this kind of error can be effec-
tively avoided (Meyer and Gurevych, 2011).
There is a considerable number of exam-
ples where Dijkstra-WSA recognizes an alignment
which similarity-based approaches do not. The two
senses of Thessalonian from the introductory exam-
ple (Section 3) contain the terms Thessalonica and
Thessaloniki in their glosses which are both monose-
mous in WN as well as in WKT, sharing the also
monosemous noun Greece in their glosses. This
yields the bridge between the resources to find a path
and correctly derive the alignment.
6 Conclusions and Future Work
In this work, we present Dijkstra-WSA, a graph-
based algorithm for word sense alignment. We
show that this algorithm performs competitively on
3 out of 4 evaluation datasets. A hybrid approach
leads to a statistically significant improvement over
similarity-based state of the art results on every
dataset. Dijkstra-WSA can operate on glosses or
semantic relations alone (although it is beneficial if
both are combined), and it does not require any ex-
ternal knowledge in the form of annotated training
data or corpora. Additionally, it is flexibly config-
urable for different pairs of LSRs in order to opti-
mize for precision or recall.
An important task for future work is to evalu-
ate Dijkstra-WSA on LSRs which structurally dif-
fer from the ones discussed here. It is important
to determine how resources like FN or VN can be
meaningfully transformed into a graph representa-
tion. Another idea is to extend the approach to
cross-lingual resource alignment, which would re-
quire a machine translation component to identify
sense alignment candidates with the correct lexeme.
Regarding the algorithm itself, the main direction
for future work is to increase recall while keeping
high precision. One possible way would be to not
only link monosemous lexemes, but also to create
edges for polysemous ones. Laparra et al (2010)
discuss a possibility to do this with high precision.
The main idea is to focus on lexemes with a low de-
gree of polysemy and align if one of the possible
senses is clearly more similar to the source sense
than the other(s). If recall is still low, more poly-
semous lexemes can be examined.
A weighting of edges (e.g. based on gloss simi-
larities) has not been considered at all, but would be
easily applicable to the existing framework.
A more elaborate idea would be to investigate
entirely different graph-based algorithms, e.g. for
matching nodes in bipartite graphs. Also, we plan to
investigate if and how our approach can be extended
to align more than two resources at once using the
graph representations. This might improve align-
ment results as more information about the overall
alignment topology becomes available.
Acknowledgements
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg Professorship
Program under grant No. I/82806 and by the
Hessian research excellence program ?Landes-
Offensive zur Entwicklung Wissenschaftlich-
o?konomischer Exzellenz (LOEWE)? as part of the
research center ?Digital Humanities?. We would
like to thank Christian M. Meyer, Wolfgang Stille,
Karsten Weihe and Tristan Miller for insightful
discussions and comments. We also thank the
anonymous reviewers for their helpful remarks.
161
References
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 33?41, Athens, Greece.
Gerard de Melo and Gerhard Weikum. 2010. Pro-
viding Multilingual, Multimodal Answers to Lexical
Database Queries. In Proceedings of the 7th Language
Resources and Evaluation Conference (LREC 2010),
pages 348?355, Valetta, Malta.
Edsger. W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1:269?271. 10.1007/BF01386390.
Judith Eckle-Kohler, Iryna Gurevych, Silvana Hartmann,
Michael Matuschek, and Christian M. Meyer. 2012.
UBY-LMF - A Uniform Model for Standardizing Het-
erogeneous Lexical-Semantic Resources in ISO-LMF.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC?12),
pages 275?282, Istanbul, Turkey.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tiziano Flati and Roberto Navigli. 2012. The CQC al-
gorithm: Cycling in graphs to semantically enrich and
enhance a bilingual dictionary. Journal of Artificial
Intelligence Research (JAIR), 43:135?171.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann,
Michael Matuschek, Christian M. Meyer, and Chris-
tian Wirth. 2012. UBY - A Large-Scale Unified
Lexical-Semantic Resource Based on LMF. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL?12), pages 580?590, Avignon, France.
Birgit Hamp and Helmut Feldweg. 1997. Germanet - a
lexical-semantic net for german. In In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9?15.
Verena Henrich, Erhard Hinrichs, and Tatiana Vodola-
zova. 2011. Semi-Automatic Extension of GermaNet
with Sense Definitions from Wiktionary. In Proceed-
ings of the 5th Language and Technology Conference
(LTC 2011), pages 126?130, Poznan, Poland.
Egoitz Laparra, German Rigau, and Montse Cuadros.
2010. Exploring the integration of WordNet and
FrameNet. In Proceedings of the 5th Global WordNet
Conference (GWC?10), Mumbai, India.
Michael Matuschek, Christian M. Meyer, and Iryna
Gurevych. 2013. Multilingual Knowledge in
Aligned Wiktionary and OmegaWiki for Computer-
Aided Translation. Translation: Computation, Cor-
pora, Cognition. Special Issue on ?Language Technol-
ogy for a Multilingual Europe?, to appear.
Christian M. Meyer and Iryna Gurevych. 2011. What
Psycholinguists Know About Chemistry: Aligning
Wiktionary and WordNet for Increased Domain Cov-
erage. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 883?892, Chiang Mai, Thailand.
Christian M. Meyer and Iryna Gurevych. 2012. Wik-
tionary: A new rival for expert-built lexicons? Ex-
ploring the possibilities of collaborative lexicography.
In Sylviane Granger and Magali Paquot, editors, Elec-
tronic Lexicography, chapter 13, pages 259?291. Ox-
ford University Press.
Rada Mihalcea and Dan I. Moldovan. 2001. eXtended
WordNet: progress report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
pages 95?100, Pittsburgh, PA, USA.
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the AAAI
Workshop on Wikipedia and Artificial Intelligence: an
Evolving Synergy, pages 25?30, Chicago, IL, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli. 2009. Using Cycles and Quasi-Cycles
to Disambiguate Dictionary Glosses. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?09),
pages 594?602, Athens, Greece.
Elisabeth Niemann and Iryna Gurevych. 2011. The Peo-
ple?s Web meets Linguistic Knowledge: Automatic
Sense Alignment of Wikipedia and WordNet. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics (IWCS), pages 205?214, Oxford,
UK.
Martha Palmer. 2009. SemLink: Linking PropBank,
VerbNet and FrameNet. In Proceedings of the Genera-
tive Lexicon ConferenceGenLex-09, pages 9?15, Pisa,
Italy.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring and
integrating Wikipedia. In Proceedings of the 21st In-
ternational Joint Conference on Artificial Intelligence,
pages 2083?2088, Pasadena, CA, USA.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In In-
ternational Joint Conference for Artificial Intelligence
(IJCAI-95), pages 448?453, Montreal, Canada.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and WordNet
for Robust Semantic Parsing. In Computational Lin-
guistics and Intelligent Text Processing: 6th Interna-
tional Conference, volume 3406 of Lecture Notes in
162
Computer Science, pages 100?111. Berlin/Heidelberg:
Springer.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A Large Ontology from
Wikipedia and WordNet. Web Semantics, 6(3):203?
217.
163
164
