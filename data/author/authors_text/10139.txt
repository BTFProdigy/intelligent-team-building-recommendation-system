Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1097?1104
Manchester, August 2008
Grammar Comparison Study for Translational Equivalence 
Modeling and Statistical Machine Translation 
Min Zhang1,  Hongfei Jiang2,  Haizhou Li1,  Aiti Aw1  and  Sheng Li2 
1Institute for Infocomm Research, Singapore 
2Harbin Institute of Technology, China 
{mzhang, hli, aaiti}@i2r.a-star.edu.sg 
{hfjiang, lisheng}@mtlab.hit.edu.cn 
 
Abstract 
This paper presents a general platform, 
namely synchronous tree sequence sub-
stitution grammar (STSSG), for the 
grammar comparison study in Transla-
tional Equivalence Modeling (TEM) and 
Statistical Machine Translation (SMT). 
Under the STSSG platform, we compare 
the expressive abilities of various gram-
mars through synchronous parsing and a 
real translation platform on a variety of 
Chinese-English bilingual corpora. Ex-
perimental results show that the STSSG 
is able to better explain the data in paral-
lel corpora than other grammars. Our 
study further finds that the complexity of 
structure divergence is much higher than 
suggested in literature, which imposes a 
big challenge to syntactic transformation-
based SMT. 
1 Introduction 
Translational equivalence is a mathematical rela-
tion that holds between linguistic expressions 
with the same meaning (Wellington et al, 2006).  
The common explicit representations of this rela-
tion are word alignments, phrase alignments and 
structure alignments between bilingual sentences. 
Translational Equivalence Modeling (TEM) is a 
process to describe and build these alignments 
using mathematical models. Thus, the study of 
TEM is highly relevant to Statistical Machine 
Translation (SMT). 
Grammar is the most important infrastructure 
for TEM and SMT since translation models? ex-
pressive and generative abilities are mainly de-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
termined by the grammar. Many grammars, such 
as finite-state grammars (FSG), bracket/inversion 
transduction grammars (BTG/ITG) (Wu, 1997), 
context-free grammar (CFG), tree substitution 
grammar (TSG) (Comon et al, 2007) and their 
synchronous versions, have been explored in 
SMT. Based on these grammars, a great number 
of SMT models have been recently proposed, 
including string-to-string model (Synchronous 
FSG) (Brown et al, 1993; Koehn et al, 2003), 
tree-to-string model (TSG-string) (Huang et al, 
2006; Liu et al, 2006; Liu et al, 2007), string-to-
tree model (string-CFG/TSG) (Yamada and 
Knight, 2001; Galley et al, 2006; Marcu et al, 
2006), tree-to-tree model (Synchronous 
CFG/TSG, Data-Oriented Translation) (Chiang, 
2005; Cowan et al, 2006; Eisner, 2003; Ding and 
Palmer, 2005; Zhang et al, 2007; Bod, 2007; 
Quirk wt al., 2005; Poutsma, 2000; Hearne and 
Way, 2003) and so on. 
Although many achievements have been ob-
tained by these advances, it is still unclear which 
of these important pursuits is able to best explain 
human translation data, as each has its advan-
tages and disadvantages. Therefore, it has great 
meaning in both theory and practice to do com-
parison studies among these grammars and SMT 
models to see which of them are capable of better 
describing parallel translation data. This is a fun-
damental issue worth exploring in multilingual 
information processing. However, little effort in 
previous work has been put in this point. To ad-
dress this issue, in this paper we define a general 
platform, namely synchronous tree sequence 
substitution grammar (STSSG), for the compari-
son studies. The STSSG can be seen as a gener-
alization of Synchronous TSG (STSG) by replac-
ing elementary tree (a single subtree used in 
STSG) with contiguous tree sequence as the ba-
sic translation unit. As a result, most of previous 
grammars used in SMT can be interpreted as the 
reduced versions of the STSSG. Under the 
STSSG platform, we compare the expressive 
1097
abilities of various grammars and translation 
models through linguistically-based synchronous 
parsing and a real translation platform. By syn-
chronous parsing, we aim to study which gram-
mar can well explain translation data (i.e. transla-
tional equivalence alignment) while by the real 
translation platform, we expect to investigate 
which model can achieve better translation per-
formance. In addition, we also measure the im-
pact of various factors in this study, including the 
genera of corpora (newspaper domain via spoken 
domain), the accuracy of word alignments and 
syntax parsing (automatically vs. manually).  
We report our experimental settings, experi-
mental results and our findings in detail in the 
rest of the paper, which is organized as follows: 
Section 2 reviews previous work. Section 3 
elaborates the general framework while Section 4 
reports the experimental results. Finally, we con-
clude our work in Section 5. 
2 Previous Work 
There are only a few of previous work related to 
the study of translation grammar comparison. 
Fox (2002) is the first to look at how well pro-
posed translation models fit actual translation 
data empirically. She examined the issue of 
phrasal cohesion between English and French 
and discovered that while there is less cohesion 
than one might desire, there is still a large 
amount of regularity in the constructions where 
breakdowns occur. This suggests that reordering 
words by phrasal movement is a reasonable strat-
egy (Fox, 2002). She has also examined the dif-
ferences in cohesion between Treebank-style 
parse trees, trees with flattened verb phrases, and 
dependency structures. Their experimental re-
sults indicate that the highest degree of cohesion 
is present in dependency structures. 
Motivated by the same problem raised by Fox 
(2002), Galley et al (2004) study what rule can 
better explain human translation data. They first 
propose a theory that gives formal semantics to 
word-level alignments defined over parallel cor-
pora, and then use the theory to introduce a linear 
algorithm that is used to derive from word-
aligned, parallel corpora the minimal set of syn-
tactically motivated transformation rules to ex-
plain human translation data. Their basic idea is 
to create transformation rules that condition on 
larger fragments of tree structure. Their experi-
mental results suggest that their proposed rules 
provide a good, realistic indicator of the com-
plexities inherent in translation than SCFG. 
Wellington et al (2006) describes their study 
of the patterns of translational equivalence exhib-
ited by a variety of bilingual/monolingual bitexts. 
They empirically measure the lower bounds on 
alignment failure rates with and without gaps 
under the constraints of word alignment alone or 
with one or both side parse trees. Their study 
finds surprisingly many examples of translational 
equivalence that could not be analyzed using bi-
nary-branching structures without discontinuities. 
Thus, they claim that the complexity of these 
patterns in every bitext is higher than suggested 
in the literature. In addition, they suggest that the 
low coverage rates without gaps under the con-
straints of independently generated monolingual 
parse trees might be the main reason why ?syn-
tactic? constraints have not yet increased the ac-
curacy of SMT systems. However, they find that 
simply allowing a single gap in bilingual phrases 
or other types of constituent can improve cover-
age dramatically. 
DeNeefe et al (2007) compares the strengths 
and weaknesses of a syntax-based MT model 
with a phrase-based MT model from the view-
points of translational equivalence extraction 
methods and coverage. They find that there are 
surprising differences in phrasal coverage ? nei-
ther is merely a superset of the other. They also 
investigate the reason why some phrase pairs are 
not learned by the syntax-based model. They fur-
ther propose several solutions and evaluate on 
the syntax-based extraction techniques in light of 
phrase pairs captured and translation accuracy. 
Finally, significant performance improvement is 
reported using their solutions. 
Different from previous work discussed above, 
this paper mainly focuses on the expressive abil-
ity comparison studies among different gram-
mars and models through synchronous parsing 
and a real SMT platform. Fox (2002), Galley et 
al (2004) and Wellington et al (2006) examine 
TEM only. DeNeefe et al (2007) only compares 
the strengths and weaknesses of a syntax-based 
MT model with a phrase-based MT model. 
3 The General Platform: the STSSG 
In this section, we first define the STSSG plat-
form in Subsection 3.1, and then explain why it 
is a general framework that can cover most of 
previous syntax-based translation grammars and 
models in Subsection 3.2. In Subsection 3.3 and 
3.4, we discuss the STSSG-based SMT and syn-
chronous parsing, which are used to compare 
different grammars and translation models. 
1098
1( )
IT e
1( )
JT f
A
 
 
Figure 1.  A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation 
 
 
 
Figure 2. Two examples of translation rules 
3.1 Definition of the STSSG 
The STSSG is an extension of the STSG by us-
ing tree sequences (rather than elementary trees) 
as the basic translation unit. A STSSG is a septet 
, , , , ,,t t ts s sG N N S S P? ?=< > , where: 
z s?  and t?  are source and target terminal 
alphabets (POSs or lexical words), respec-
tively, and 
z sN  and tN are source and target non-
terminal alphabets (linguistic phrase tag, i.e. 
NP/VP?), respectively, and 
z s sS N?  and t tS N?  are the source and tar-
get start symbols (roots of source and target 
parse trees), and 
z P is a production rule set. 
A grammar rule ir  in the STSSG is an aligned 
tree sequence pair, < s? , t? , A  >, where s? and 
t?  are tree sequences of source side and target 
sides, respectively, and A is the alignments be-
tween leaf nodes of two tree sequences. Here, the 
key concept of ?tree sequence? refers to an or-
dered subtree sequence covering a consecutive 
tree fragment in a complete parse tree. The leaf 
nodes of a subtree in a tree sequence can be ei-
ther non-terminal symbols or terminal symbols. 
Fig. 2 shows two STSSG rules extracted from 
the aligned tree pair shown in Fig. 1, where 1r is 
also a STSG rule.  
In the STSSG, a translational equivalence is 
modeled as a tree sequence pair while MT is 
viewed as a tree sequence substitution process. 
From the definition of ?tree sequence?, we can 
see that a subtree in a tree sequence is a so-called 
elementary tree used in TSG. This suggests that 
SCFG and STSG are only a subset of STSSG 
and SCFG is a subset of STSG. The next subsec-
tion discusses how to configure the STSSG to 
implement the other two simplified grammars. 
This is the reason why we call the STSSG a gen-
eral framework for synchronous grammar-based 
translation modeling. 
It is worth noting that, from rule rewriting 
viewpoint, STSSG can be thought of as a re-
stricted version of synchronous multi-component 
TAGs (Schuler et al, 2000) although TAG is 
more powerful than TSG due to the additional 
operation ?adjunctions?. The synchronous multi-
component TAG can also rewrite several non-
terminals in one step of derivation. The differ-
ence between them is that the rewriting sites (i.e. 
the substitution nodes) must be contiguous in 
STSSG. In addition, STSSG is also related to 
tree automata (Comon et al, 2007). However, the 
discussion on the theoretical relation and com-
parison between them is out of the scope of the 
paper. In this paper, we focus on the comparison 
study of SMT grammars using the STSSG plat-
form. 
3.2 Rule Extraction and Grammar Con-
figuration 
All the STSSG mapping rules are extracted from 
bi-parsed trees. Our rule extraction algorithm is 
an extension of that presented at (Chiang, 2005; 
Liu et al, 2006; Zhang et al, 2007). We modify 
their tree-to-tree/string rule extraction algorithms 
to extract tree-sequence-to-tree-sequence rules. 
Our rules2 are extracted in two steps: 
                                                 
2  We classify the rules into two categories: initial 
rules, whose leaf nodes must be terminals, and ab-
1099
1) Extracting initial rules from bi-parsed trees. 
This is rather straightforward. We first generate 
all fully lexicalized source and target tree se-
quences (whose leaf nodes must be lexical words) 
using a DP algorithm and then iterate over all 
generated source and target sequence pairs. If 
their word alignments are all within the scope of 
the current tree sequence pair, then the current 
tree sequence pair is an initial rule. 
2) Extracting abstract rules from the extracted 
initial rules. The idea behind is that we generate 
an abstract rule from a ?big? initial rule by re-
moving one or more ?small? initial rules from 
the ?big? one, where the ?small? ones must be a 
sub-graph of the ?big? one. Please refer to 
(Chiang, 2005; Liu et al, 2006; Zhang et al, 
2007) for the implementation details. 
As indicated before (Chiang, 2005; Zhang et 
al., 2007), the above scheme generates a very 
large number of rules, which not only makes the 
system too complicated but also introduces too 
many undesirable ambiguities. To control the 
overall model complexity, we introduce the fol-
lowing parameters: 
1) The maximal numbers of trees in the source 
and target tree sequences: s? and t? . 
2) The maximal tree heights in the source and 
target tree sequences: s? and t? . 
3) The maximal numbers of non-terminal leaf 
nodes in the source and target tree sequences: 
s? and t? . 
Now let us see how to implement other mod-
els in relation to STSSG based the STSSG 
through configuring the above parameters. 
1) STSG-based tree-to-tree model (Zhang et 
al., 2007; Bod, 2007) when s? = t? =1. 
2) SCFG-based tree-to-tree model when s? = 
t? =1 and s? = t? =2. 
3) Phrase-based translation model only (no re-
ordering model) when s? = t? =0 and s? = t? =1. 
4) TSG-CFG-based tree-to-string model (Liu 
et al, 2006) when s? = t? =1, t? =2 and ignore 
phrase tags in target side.  
5) CFG-TSG-based string-to-tree model (Gal-
ley et al, 2006) when s? = t? =1and s? =2. 
6) TSSG-CFG-based tree-sequence-to-string 
model (Liu et al, 2007) when t? =2 and ignore 
phrase tags in target side. 
                                                                          
stract rule that having at least one non-terminal leaf 
node. 
From the above definitions, we can see that all 
of previous related models/grammars can be can 
be interpreted as the reduced versions of the 
STSSG. This is the reason why we use the 
STSSG as a general platform for our model and 
grammar comparison studies. 
3.3 Model Training and Decoder for SMT 
We use the tree sequence mapping rules to model 
the translation process. Given the source parse 
tree 1( )
JT f , there are multiple derivations3 that 
could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is ob-
tained by summing over the probabilities of all 
derivations. The probability of each derivation?  
is given by the product of the probabilities of all 
the rules ( )ip r  used in the derivation (here we 
assume that a rule is applied independently in a 
derivation). 
1 1 1 1( | ) ( ( ) | ( ))
                  = ( )
i
I J I J
i
r
r rP e f P T e T f
p r
? ??
=
??           (1) 
The model is implemented under log-linear 
framework. We use seven basic features that are 
analogous to the commonly used features in 
phrase-based systems (Koehn, 2004): 1) bidirec-
tional rule mapping probabilities; 2) bidirectional 
lexical translation probabilities; 3) the target lan-
guage model; 4) the number of rules used and 5) 
the number of target words. Besides, we define 
two new features: 1) the number of lexical words 
in a rule to control the model?s preference for 
lexicalized rules over un-lexicalized rules and 2) 
the average tree height in a rule to balance the 
usage of hierarchical rules and more flat rules. 
The overall training process is similar to the 
process in the phrase-based system (koehn et al, 
2007): word alignment, rule extraction, feature 
extraction and probability calculation and feature 
weight tuning. 
Given 1( )
JT f , the decoder is to find the best 
derivation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (2) 
By default, same as other SMT decoder, here 
we use Viterbi derivation in Eq (2) instead of the 
                                                 
3 A derivation is a sequence of tree sequence rules that 
maps a source parse tree to its target one. 
1100
summing probabilities in Eq (3). This is to make 
the decoder speed not too slow. The decoder is a 
standard span-based chart parser together with a 
function for mapping the source derivations to 
the target ones. To speed up the decoder, we util-
ize several thresholds to limit the search beams 
for each span, such as the number of rules used 
and the number of hypotheses generated. 
3.4 Synchronous Parsing   
A synchronous parser is an algorithm that can 
infer the syntactic structure of each component 
text in a multitext and simultaneously infer the 
correspondence relation between these structures. 
When a parser?s input can have fewer dimen-
sions than the parser?s grammar, we call it a 
translator. When a parser?s grammar can have 
fewer dimensions than the parser?s input, we call 
it a synchronizer (Melamed, 2004). Therefore, 
synchronous parsing and MT are closed to each 
other. In this paper, we use synchronous parsing 
to compare the ability of different grammars in 
translational equivalence modeling.  
Given a bilingual sentence pair 1
Jf and 1
Ie , the 
synchronous parser is to find a derivation ?  that 
generates < 1( )
JT f , 1( )
IT e >. Our synchronous 
parser is similar to the synchronous CKY parser 
presented at (Melamed, 2004). The difference is 
that we implement it based on our STSSG de-
coder. Therefore, in nature the parser is a stan-
dard synchronous chart parser but constrained by 
the rules of the STSSG grammar. In our imple-
mentation, we simply use our decoder to simu-
late the bilingual parser: 1) for each sentence pair, 
we extract one model; 2) we use the model and 
the decoder to translate the source sentence of 
the given sentence pair; 3) if the target sentence 
is successfully generated by the decoder, then we 
say the symphonious parsing is successful. 
Please note that the synchronous parsing is con-
sidered as successful once the last words in the 
source and target sentences are covered by the 
decoder even if there is no a complete target 
parse tree generated (it may be a tree sequence). 
This is because our study only concerns whether 
all translational equivalences are linked together 
by the synchronous parser correctly. 
4 Experiments 
4.1 Experimental Settings 
Synchronous parsing settings: Our experiments 
of synchronous parsing are carried on three Chi-
nese-to-English bilingual corpora: the FBIS cor-
pus, the IWSLT 2007 training set and the HIT 
Corpus. The FBIS data is a collection of trans-
lated newswire documents published by major 
news agencies from three representative loca-
tions: Beijing, Taipei and Hongkong. The 
IWSLT data is a multilingual speech corpus on 
travel domain while the HIT corpus consists of 
example sentences of a Chinese-English diction-
ary. The first two corpora are sentence-aligned 
while the HIT corpus is a manually bi-parsed 
corpus with manually annotated word alignments. 
We use the three corpora to study whether the 
models? expressive abilities are domain depend-
ent and how the performance of word alignment 
and parsing affect the ability of translation mod-
els. We selected 2000 sentence pairs from each 
individual corpus for the comparison study of 
translational equivalence modeling. Table 1 
gives descriptive statistics of the tree data set. 
 
 Chinese English 
FBIS 48,331 59,788 
IWSLT  17,667 18,427 
HIT 18,215 20,266 
 
Table 1. # of words of experimental data 
for synchronous parsing (there are 2k sen-
tence pairs in each individual corpus) 
 
In the synchronous parsing experiments, we 
compared three synchronous grammars: SCFG, 
STSG and STSSG using the STSSG platform. 
We use the same settings except the following 
parameters (please refer to Subsection 3.2 for 
their definitions): s? = t? =1, s? = t? =2 for 
SCFG ; s? = t? =1 and s? = t? =6 for STSG; 
s? = t? = 4 and s? = t? =6 for STSSG. We iter-
ate over each sentence pair in the three corpora 
with the following process: 
1) to used Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences separately,  
this means that our study is based on the Penn 
Treebank style grammar.  
2) to extract SCFG, STSG and STSSG rules 
form each sentence pair, respectively; 
3) to do synchronous parsing using the exacted 
rules.  
Finally, we can calculate the successful rate of 
the synchronous parsing on each corpus. 
SMT evaluation settings: For the SMT ex-
periments, we trained the translation model on 
the FBIS corpus (7.2M (Chinese)+9.2M(English) 
words) and trained a 4-gram language model on 
1101
the Xinhua portion of the English Gigaword cor-
pus (181M words) using the SRILM Toolkits 
(Stolcke, 2002) with modified Kneser-Ney 
smoothing (Chen and Goodman, 1998). We used 
these sentences with less than 50 characters from 
the NIST MT-2002 test set as our development 
set and the NIST MT-2005 test set as our test set. 
We used the Stanford parser (Klein and Manning, 
2003) to parse bilingual sentences on the training 
set and Chinese sentences on the development 
and test sets. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002). We 
used GIZA++ and the heuristics ?grow-diag-
final? to generate m-to-n word alignments. For 
the MER training, we modified Koehn?s MER 
trainer (Koehn, 2004) for our STSSG-based sys-
tem. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). We com-
pared four SMT systems: Moses (Koehn et al, 
2007), SCFG-based, STSG-based and STSSG-
based tree-to-tree translation models. For Moses, 
we used its default settings. For the others, we 
implemented them on the STSSG platform by 
adopting the same settings as used in the syn-
chronous parsing. We optimized the decoding 
parameters on the development sets empirically. 
4.2 Experimental Results  
 
 SCFG STSG STSSG 
FBIS 7 (0.35%) 143 (7.15%) 388 (19.4%) 
IWSLT 171 (8.6%) 1179 (58.9%) 1708 (85.4%)
HIT 65 (3.23%) 1133 (56.6%) 1532 (76.6%)
 
Table 2. Successful rates (numbers inside 
bracket) of synchronous parsing over 2,000 
sentence pairs, where the integers outside 
bracket are the numbers of successfully-
parsed sentence pairs 
 
Table 2 reports the experimental results of syn-
chronous parsing. It shows that: 
1) As an extension of STSG/SCFG, STSSG 
outperforms STSG and SCFG consistently in the 
three data sets. The significant difference sug-
gests that the STSSG is much more effective in 
modeling translational equivalences and structure 
divergences. The reason is simply because the 
STSSG uses tree sequences as the basic transla-
tion unit so that it can model non-syntactic 
phrase equivalence with structure information 
and handle structure reordering in a large span.  
2) STSG shows much better performance than 
SCFG. It is mainly due to that STSG allow mul-
tiple level tree nodes operation and reordering in 
a larger span than SCFG. It reconfirms that only 
allowing sibling nodes reordering as done in 
SCFG may be inadequate for translational equiva-
lence modeling (Galley et al, 2004)4.  
3) All the three models on the FBIS corpus 
show much lower performance than that on the 
other two corpora. The main reason, as shown in 
Table 1, is that the sentences in the FBIS corpus 
are much longer than that in the other corpus, so 
their syntactic structures are significantly more 
complicated than the other two. In addition, al-
though tree sequences are utilized, STSSG show 
much lower performance in the FBIS corpus. 
This implies that the complexity of structure di-
vergence between two languages is higher than 
suggested in literature (Fox, 2002; Galley et al, 
2004). Therefore, structure divergence is still a 
big challenge to translational equivalence model-
ing when using syntactic structure mapping. 
4) The HIT corpus does not show better per-
formance than the IWSLT corpus although the 
HIT corpus is manually annotated with parse 
trees and word alignments. In order to study 
whether high performance word alignment and 
parsing results can help synchronous parsing, we 
do several cross validations and report the ex-
perimental results in Table 3. 
 
 Gold Word Alignment 
Automatic 
Word Align-
ment 
 Gold Parse 3.2/56.6/76.6 2.9/57.7/80.9
 Automatic  
Parse 3.2/55.6/76.0 2.9/54.2/78.8
 
Table 3. Successful rates (SCFG/STSG/ 
STSSG)(%) with regards to different word 
alignments and parse trees  on the HIT corpus 
 
Table 3 compares the performance of syn-
chronous parsing on the HIT corpus when using 
gold and automatic parser and word alignment. It 
is surprised that gold word alignments and parse 
trees do not help and even decrease the perform-
ance slightly. Our analysis further finds that 
                                                 
4 This claim is mainly hold for linguistically-informed 
SCFG since formal SCFG and BTG already showed 
much better performance in the formally syntax-based 
translation framework (Chiang, 2005). This is because 
the formal syntax is learned from phrase translational 
equivalences directly without relying on any linguistic 
theory (Chiang, 2005). Thus, it may not suffer from 
the issues of non-isomorphic structure alignment and 
non-syntactic phrase usage heavily (Wellington et al, 
2006). 
1102
more than 90% sentence pairs out of all the sen-
tence pairs that can be successfully bi-parsed are 
in common in the four experiments. This sug-
gests that the STSSG/STSG (SCFG achieves too 
much lower performance) and our rule extraction 
algorithm are robust in dealing with the errors 
introduced by the word alignment and parsing 
programs. If a parser, for example, makes a sys-
tematic error, we expect to learn a rule that can 
nevertheless be systematically used to model cor-
rect translational equivalence. Our error analysis 
on the three corpora shows that most of the fail-
ures of synchronous parsing are due to the struc-
ture divergence (i.e. the nature of non-
isomorphic structure mapping) and the long dis-
tance dependence in the syntactic structures.  
 
 SCFG Moses STSG STSSG
BLEU(%) 22.72 23.86 24.71 26.07 
 
     Table 3. Performance comparison of dif-
ferent grammars on FBIS corpus 
 
Table 3 compares different grammars in terms 
of translation performance. It shows that: 
1) The same as synchronous parsing, the 
STSSG-based model statistically significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empiri-
cally verifies the effect of the tree-sequence-based 
grammar for statistical machine translation.  
2) Both STSSG and STSG outperform Moses 
significantly and STSSG clearly outperforms 
STSG, which suggest that: 
z The linguistically motivated structure fea-
tures are still useful for SMT, which can be cap-
tured by the two syntax-based grammars through 
tree node operations. 
z STSSG is much more effective in utiliz-
ing linguistic structures than STSG since it uses 
tree sequence as the basic translation unit. This 
enables STSSG not only to handle structure reor-
derings by tree node operations in a larger span, 
but also to capture non-syntactic phrases with syn-
tactic information, and hence giving the grammar 
more expressive power. 
3) The linguistic-based SCFG shows much 
lower performance. This is largely because SCFG 
only allows sibling nodes reordering and fails to 
utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure trans-
fer.  
The above two experimental results show that 
STSSG achieves significant improvements over 
the other two grammars in terms of synchronous 
parsing?s successful rate and translation Bleu 
score. 
5 Conclusions 
Grammar is the fundamental infrastructure in 
translational equivalence modeling and statistical 
machine translation since grammar formalizes 
what kind of rule to be learned from a parallel 
text. In this paper, we first present a general plat-
form STSSG and demonstrate that a number of 
synchronous grammars and SMT models can be 
easily implemented based on the platform. We 
then compare the expressive abilities of different 
grammars on the platform using synchronous 
parsing and statistical machine translation. Our 
experimental results show that STSSG can better 
explain the data in parallel corpora than the other 
two synchronous grammars. We further finds 
that, although syntactic structure features are 
helpful in modeling translational equivalence, the 
complexity of structure divergence is much 
higher than suggested in literature, which im-
poses a big challenge to syntactic transformation-
based SMT. This may explain why traditional 
syntactic constraints in SMT do not yield much 
performance improvement over robust phrase-
substitution models. 
The fundamental assumption underlying much 
recent work on syntax-based modeling, which is 
considered to be one of next technology break-
throughs in SMT, is that translational equiva-
lence can be well modeled by structural trans-
formation. However, as discussed in prior arts 
(Galley et al, 2004) and this paper, linguisti-
cally-informed SCFG is an inadequate model for 
parallel corpora due to its nature that only allow-
ing child-node reorderings. Although STSG 
shows much better performance than SCFG, its 
two major limitations are that it only allows 
structure distortion operated on a single sub-tree 
and cannot model non-syntactic phrases. STSSG 
extends STSG by using tree sequence as the ba-
sic translation unit. This gives the grammar much 
more expressive power.  
There are many open issues in the syntactic 
transformation-based SMT due to the divergence 
nature between bilingual structure mappings. We 
find that structural divergences are more serious 
than suggested in the literature (Fox, 2002; Gal-
lery et al, 2004) or what we expected when sen-
tences are longer. We will continue to investigate 
1103
whether and how parallel corpora can be well 
modeled by syntactic structure mappings.   
References 
Rens Bod. 2007. Unsupervised Syntax-Based Ma-
chine Translation: The Contribution of Discon-
tinuous Phrases. MT-Summmit-07. 51-56.  
Peter F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311. 
S. F. Chen and J. Goodman. 1998. An empirical study 
of smoothing techniques for language modeling. 
Technical Report TR-10-98, Harvard University 
Center for Research in Computing Technology. 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05. 263-270. 
H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard, 
D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree 
automata techniques and applications. Available at: 
http://tata.gforge.inria.fr/. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. EMNLP-06. 232-241. 
S. DeNeefe, K. Knight, W. Wang and D. Marcu. 2007. 
What Can Syntax-based MT Learn from Phrase-
based MT? EMNLP-CoNLL-07. 755-763 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. ACL-05. 541-548. 
Bonnie J. Dorr (1994). Machine Translation Diver-
gences: A formal description and proposed solu-
tion. Computational Linguistics, 20(4): 597-633 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for MT. ACL-03 (companion volume). 
Heidi J. Fox. 2002. Phrasal Cohesion and Statistical 
Machine Translation. EMNLP-2002. 304-311  
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. 
DeNeefe, W. Wang and I. Thayer. 2006. Scalable 
Inference and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
M. Galley, M. Hopkins, K. Knight and D. Marcu. 
2004. What?s in a translation rule? HLT-NAACL. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Mary Hearne and Andy Way. 2003. Seeing the wood 
for the trees: data-oriented translation. MT Sum-
mit IX, 165-172. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical phrase-based translation. HLT-NAACL-03. 
127-133. 
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. AMTA-04, 115-124. 
Philipp Koehn, H. Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. 
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. 
Constantin and E. Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
K. Papineni, Salim Roukos, ToddWard and Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
William Schuler, David Chiang and Mark Dras. 2000. 
Multi-Component TAG and Notions of Formal 
Power. ACL-2000. 448-455 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COL-
ING-ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377-403. 
K. Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
M. Zhang, H. Jiang, A. Aw, J. Sun, S. Li and C. Tan. 
2007. A Tree-to-Tree Alignment-based Model for 
SMT. MT-Summit-07. 535-542. 
Y. Zhang, S. Vogel and A. Waibel. 2004. Interpreting 
BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04.  
1104
Proceedings of ACL-08: HLT, pages 559?567,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Tree Sequence Alignment-based Tree-to-Tree Translation Model 
 
 
Min Zhang1  Hongfei Jiang2  Aiti Aw1  Haizhou Li1  Chew Lim Tan3 and Sheng Li2
1Institute for Infocomm Research 2Harbin Institute of Technology 3National University of Singapore
mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg 
aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn  
hli@i2r.a-star.edu.sg   
 
  
Abstract 
This paper presents a translation model that is 
based on tree sequence alignment, where a tree 
sequence refers to a single sequence of sub-
trees that covers a phrase. The model leverages 
on the strengths of both phrase-based and lin-
guistically syntax-based method. It automati-
cally learns aligned tree sequence pairs with 
mapping probabilities from word-aligned bi-
parsed parallel texts. Compared with previous 
models, it not only captures non-syntactic 
phrases and discontinuous phrases with lin-
guistically structured features, but also sup-
ports multi-level structure reordering of tree 
typology with larger span. This gives our 
model stronger expressive power than other re-
ported models. Experimental results on the 
NIST MT-2005 Chinese-English translation 
task show that our method statistically signifi-
cantly outperforms the baseline systems.  
1 Introduction 
Phrase-based modeling method (Koehn et al, 
2003; Och and Ney, 2004a) is a simple, but power-
ful mechanism to machine translation since it can 
model local reorderings and translations of multi-
word expressions well. However, it cannot handle 
long-distance reorderings properly and does not 
exploit discontinuous phrases and linguistically 
syntactic structure features (Quirk and Menezes, 
2006). Recently, many syntax-based models have 
been proposed to address the above deficiencies 
(Wu, 1997; Chiang, 2005; Eisner, 2003; Ding and 
Palmer, 2005; Quirk et al 2005; Cowan et al, 
2006; Zhang et al, 2007; Bod, 2007; Yamada and 
Knight, 2001; Liu et al, 2006; Liu et al, 2007; 
Gildea, 2003; Poutsma, 2000; Hearne and Way, 
2003). Although good progress has been reported, 
the fundamental issues in applying linguistic syn-
tax to SMT, such as non-isomorphic tree align-
ment, structure reordering and non-syntactic phrase 
modeling, are still worth well studying. 
In this paper, we propose a tree-to-tree transla-
tion model that is based on tree sequence align-
ment. It is designed to combine the strengths of 
phrase-based and syntax-based methods. The pro-
posed model adopts tree sequence 1  as the basic 
translation unit and utilizes tree sequence align-
ments to model the translation process. Therefore, 
it not only describes non-syntactic phrases with 
syntactic structure information, but also supports 
multi-level tree structure reordering in larger span. 
These give our model much more expressive 
power and flexibility than those previous models. 
Experiment results on the NIST MT-2005 Chinese-
English translation task show that our method sig-
nificantly outperforms Moses (Koehn et al, 2007), 
a state-of-the-art phrase-based SMT system, and 
other linguistically syntax-based methods, such as 
SCFG-based and STSG-based methods (Zhang et 
al., 2007). In addition, our study further demon-
strates that 1) structure reordering rules in our 
model are very useful for performance improve-
ment while discontinuous phrase rules have less 
contribution and 2) tree sequence rules are able to 
model non-syntactic phrases with syntactic struc-
ture information, and thus contribute much to the 
performance improvement, but those rules consist-
ing of more than three sub-trees have almost no 
contribution.  
The rest of this paper is organized as follows: 
Section 2 reviews previous work. Section 3 elabo-
                                                          
1 A tree sequence refers to an ordered sub-tree sequence that 
covers a phrase or a consecutive tree fragment in a parse tree. 
It is the same as the concept ?forest? used in Liu et al(2007).  
559
rates the modelling process while Sections 4 and 5 
discuss the training and decoding algorithms. The 
experimental results are reported in Section 6. Fi-
nally, we conclude our work in Section 7. 
2 Related Work 
Many techniques on linguistically syntax-based 
SMT have been proposed in literature. Yamada 
and Knight (2001) use noisy-channel model to 
transfer a target parse tree into a source sentence. 
Eisner (2003) studies how to learn non-isomorphic 
tree-to-tree/string mappings using a STSG. Ding 
and Palmer (2005) propose a syntax-based transla-
tion model based on a probabilistic synchronous 
dependency insertion grammar. Quirk et al (2005) 
propose a dependency treelet-based translation 
model. Cowan et al (2006) propose a feature-
based discriminative model for target language 
syntactic structures prediction, given a source 
parse tree. Huang et al (2006) study a TSG-based 
tree-to-string alignment model. Liu et al (2006) 
propose a tree-to-string model. Zhang et al 
(2007b) present a STSG-based tree-to-tree transla-
tion model. Bod (2007) reports that the unsuper-
vised STSG-based translation model performs 
much better than the supervised one. The motiva-
tion behind all these work is to exploit linguistical-
ly syntactic structure features to model the 
translation process. However, most of them fail to 
utilize non-syntactic phrases well that are proven 
useful in the phrase-based methods (Koehn et al, 
2003). 
The formally syntax-based model for SMT was 
first advocated by Wu (1997). Xiong et al (2006) 
propose a MaxEnt-based reordering model for 
BTG (Wu, 1997) while Setiawan et al (2007) pro-
pose a function word-based reordering model for 
BTG. Chiang (2005)?s hierarchal phrase-based 
model achieves significant performance improve-
ment. However, no further significant improve-
ment is achieved when the model is made sensitive 
to syntactic structures by adding a constituent fea-
ture (Chiang, 2005). 
In the last two years, many research efforts were 
devoted to integrating the strengths of phrase-
based and syntax-based methods. In the following, 
we review four representatives of them.   
1) Hassan et al (2007) integrate supertags (a 
kind of lexicalized syntactic description) into the 
target side of translation model and language mod-
el under the phrase-based translation framework, 
resulting in good performance improvement. How-
ever, neither source side syntactic knowledge nor 
reordering model is further explored.  
2) Galley et al (2006) handle non-syntactic 
phrasal translations by traversing the tree upwards 
until a node that subsumes the phrase is reached. 
This solution requires larger applicability contexts 
(Marcu et al, 2006). However, phrases are utilized 
independently in the phrase-based method without 
depending on any contexts.  
3) Addressing the issues in Galley et al (2006), 
Marcu et al (2006) create an xRS rule headed by a 
pseudo, non-syntactic non-terminal symbol that 
subsumes the phrase and its corresponding multi-
headed syntactic structure; and one sibling xRS 
rule that explains how the pseudo symbol can be 
combined with other genuine non-terminals for 
acquiring the genuine parse trees. The name of the 
pseudo non-terminal is designed to reflect the full 
realization of the corresponding rule. The problem 
in this method is that it neglects alignment consis-
tency in creating sibling rules and the naming me-
chanism faces challenges in describing more 
complicated phenomena (Liu et al, 2007).  
4) Liu et al (2006) treat all bilingual phrases as 
lexicalized tree-to-string rules, including those 
non-syntactic phrases in training corpus. Although 
the solution shows effective empirically, it only 
utilizes the source side syntactic phrases of the in-
put parse tree during decoding. Furthermore, the 
translation probabilities of the bilingual phrases 
and other tree-to-string rules are not compatible 
since they are estimated independently, thus hav-
ing different parameter spaces. To address the 
above problems, Liu et al (2007) propose to use 
forest-to-string rules to enhance the expressive 
power of their tree-to-string model. As is inherent 
in a tree-to-string framework, Liu et al?s method 
defines a kind of auxiliary rules to integrate forest-
to-string rules into tree-to-string models. One prob-
lem of this method is that the auxiliary rules are 
not described by probabilities since they are con-
structed during decoding, rather than learned from 
the training corpus. So, to balance the usage of dif-
ferent kinds of rules, they use a very simple feature 
counting the number of auxiliary rules used in a 
derivation for penalizing the use of forest-to-string 
and auxiliary rules. 
In this paper, an alternative solution is presented 
to combine the strengths of phrase-based and syn-
560
1( )
IT e
1( )
JT f
A
 
 
Figure 1: A word-aligned parse tree pairs of a Chi-
nese sentence and its English translation  
 
 
 
Figure 2: Two Examples of tree sequences 
 
 
 
Figure 3: Two examples of translation rules 
tax-based methods. Unlike previous work, our so-
lution neither requires larger applicability contexts 
(Galley et al, 2006), nor depends on pseudo nodes 
(Marcu et al, 2006) or auxiliary rules (Liu et al, 
2007). We go beyond the single sub-tree mapping 
model to propose a tree sequence alignment-based 
translation model. To the best of our knowledge, 
this is the first attempt to empirically explore the 
tree sequence alignment based model in SMT.  
3 Tree Sequence Alignment Model 
3.1 Tree Sequence Translation Rule   
The leaf nodes of a sub-tree in a tree sequence can 
be either non-terminal symbols (grammar tags) or 
terminal symbols (lexical words). Given a pair of 
source and target parse trees 1( )
JT f and 1( )
IT e  in 
Fig. 1, Fig. 2 illustrates two examples of tree se-
quences derived from the two parse trees. A tree 
sequence translation rule r  is a pair of aligned tree 
sequences r =< 2
1
( )jjTS f , 21( )
i
iTS e , A%  >, where: 
z 2
1
( )jjTS f is a source tree sequence, covering 
the span [ 1 2,j j ] in 1( )
JT f , and 
z 2
1
( )iiTS e is a target one, covering the span 
[ 1 2,i i ] in 1( )
IT e , and 
z A% are the alignments between leaf nodes of 
two tree sequences, satisfying the following 
condition: 1 2 1 2( , ) :i j A i i i j j j? ? ? ? ? ? ?% . 
Fig. 3 shows two rules extracted from the tree pair 
shown in Fig. 1, where r1 is a tree-to-tree rule and 
r2 is a tree sequence-to-tree sequence rule. Ob-
viously, tree sequence rules are more powerful 
than phrases or tree rules as they can capture all 
phrases (including both syntactic and non-syntactic 
phrases) with syntactic structure information and 
allow any tree node operations in a longer span. 
We expect that these properties can well address 
the issues of non-isomorphic structure alignments, 
structure reordering, non-syntactic phrases and 
discontinuous phrases translations. 
3.2 Tree Sequence Translation Model 
Given the source and target sentences 1
Jf and 1
Ie  
and their parse trees 1( )
JT f and 1( )
IT e , the tree 
sequence-to-tree sequence translation model is 
formulated as: 
1 1
1 1
1 1 1 1 1 1
( ), ( )
1 1
( ), ( )
1 1 1
1 1 1 1
( | ) ( , ( ), ( ) | )
( ( ( ) | )
( ( ) | ( ), )
( | ( ), ( ), ))
                
                      
                      
J I
J I
I J I I J J
T f T e
J J
T f T e
I J J
I I J J
r r
r
r
r
P e f P e T e T f f
P T f f
P T e T f f
P e T e T f f
=
=
?
?
?
? (1) 
In our implementation, we have: 
561
1) 1 1( ( ) | ) 1
J JrP T f f ? since we only use the best 
source and target parse tree pairs in training. 
2) 1 1 1 1( | ( ), ( ), ) 1
I I J JrP e T e T f f ? since we just 
output the leaf nodes of 1( )
IT e to generate 1
Ie  
regardless of source side information. 
Since 1( )
JT f contains the information of 1
Jf , 
now we have: 
1 1 1 1 1
1 1
( | ) ( ( ) | ( ), )
                 ( ( ) | ( ))
I J I J J
I J
r r
r
P e f P T e T f f
P T e T f
=
=
           (2) 
By Eq. (2), translation becomes a tree structure 
mapping issue. We model it using our tree se-
quence-based translation rules. Given the source 
parse tree 1( )
JT f , there are multiple derivations 
that could lead to the same target tree 1( )
IT e , the 
mapping probability 1 1( ( ) | ( ))
I JrP T e T f is obtained 
by summing over the probabilities of all deriva-
tions. The probability of each derivation? is given 
as the product of the probabilities of all the rules 
( )ip r  used in the derivation (here we assume that 
a rule is applied independently in a derivation). 
2 2
1 1
1 1 1 1( | ) ( ( ) | ( ))
     = ( : ( ), ( ), )
i
I J I J
i j
i i j
r
r rP e f P T e T f
p r TS e TS f A
? ??
=
< >?? %    (3) 
Eq. (3) formulates the tree sequence alignment-
based translation model. Figs. 1 and 3 show how 
the proposed model works. First, the source sen-
tence is parsed into a source parse tree. Next, the 
source parse tree is detached into two source tree 
sequences (the left hand side of rules in Fig. 3). 
Then the two rules in Fig. 3 are used to map the 
two source tree sequences to two target tree se-
quences, which are then combined to generate a 
target parse tree. Finally, a target translation is 
yielded from the target tree.  
Our model is implemented under log-linear 
framework (Och and Ney, 2002). We use seven 
basic features that are analogous to the commonly 
used features in phrase-based systems (Koehn, 
2004): 1) bidirectional rule mapping probabilities; 
2) bidirectional lexical rule translation probabilities; 
3) the target language model; 4) the number of 
rules used and 5) the number of target words. In 
addition, we define two new features: 1) the num-
ber of lexical words in a rule to control the model?s 
preference for lexicalized rules over un-lexicalized 
rules and 2) the average tree depth in a rule to bal-
ance the usage of hierarchical rules and flat rules. 
Note that we do not distinguish between larger (tal-
ler) and shorter source side tree sequences, i.e. we 
let these rules compete directly with each other. 
4 Rule Extraction 
Rules are extracted from word-aligned, bi-parsed 
sentence pairs 1 1( ), ( ),
J IT f T e A< > , which are 
classified into two categories: 
z initial rule, if all leaf nodes of the rule are 
terminals (i.e. lexical word), and 
z abstract rule, otherwise, i.e. at least one leaf 
node is a non-terminal (POS or phrase tag). 
Given an initial rule 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% , 
its sub initial rule is defined as a triple 
4 4
3 3
?( ), ( ),j ij iTS f TS e A< >  if and only if: 
z 4 4
3 3
?( ), ( ),j ij iTS f TS e A< > is an initial rule. 
z 3 4 3 4( , ) :i j A i i i j j j? ? ? ? ? ? ?% , i.e. 
A? A? %  
z 4
3
( )jjTS f is a sub-graph of 21( )
j
jTS f while  
4
3
( )iiTS e  is a sub-graph of 21( )
i
iTS e . 
Rules are extracted in two steps: 
1) Extracting initial rules first. 
2) Extracting abstract rules from extracted ini-
tial rules with the help of sub initial rules. 
It is straightforward to extract initial rules. We 
first generate all fully lexicalized source and target 
tree sequences using a dynamic programming algo-
rithm and then iterate over all generated source and 
target tree sequence pairs 2 2
1 1
( ), ( )j ij iTS f TS e< > . If 
the condition ? ( , )i j? 1 2 1 2:A i i i j j j? ? ? ? ? ? ? 
is satisfied, the triple 2 2
1 1
( ), ( ),j ij iTS f TS e A< >% is 
an initial rule, where A%  are alignments between 
leaf nodes of 2
1
( )jjTS f  and 21( )
i
iTS e . We then de-
rive abstract rules from initial rules by removing 
one or more of its sub initial rules. The abstract 
rule extraction algorithm presented next is imple-
mented using dynamic programming. Due to space 
limitation, we skip the details here. In order to con-
trol the number of rules, we set three constraints 
for both finally extracted initial and abstract rules:  
1) The depth of a tree in a rule is not greater 
562
than h . 
2) The number of non-terminals as leaf nodes is 
not greater than c . 
3) The tree number in a rule is not greater than d. 
In addition, we limit initial rules to have at most 
seven lexical words as leaf nodes on either side. 
However, in order to extract long-distance reorder-
ing rules, we also generate those initial rules with 
more than seven lexical words for abstract rules 
extraction only (not used in decoding). This makes 
our abstract rules more powerful in handling 
global structure reordering. Moreover, by configur-
ing these parameters we can implement other 
translation models easily: 1) STSG-based model  
when 1d = ; 2) SCFG-based model when 1d =  
and 2h = ; 3) phrase-based translation model only 
(no reordering model) when 0c =  and 1h = . 
 
Algorithm 1: abstract rules extraction 
Input: initial rule set inir  
Output: abstract rule set absr  
1: for each i inir r? , do 
2:    put all sub initial rules of ir  into a set subiniir
3:    for each subset subiniir? ? do 
4:          if there are spans overlapping between 
any two rules in the subset ?  then 
5:                    continue   //go to line 3 
6:           end if  
7:           generate an abstract rule by removing 
the portions covered by ?  from ir  and 
co-indexing the pairs of non-terminals 
that rooting the removed source and 
target parts 
8:           add them into the abstract rule set absr  
9:     end do 
10: end do  
 
5 Decoding 
Given 1( )
JT f , the decoder is to find the best deri-
vation ?  that generates < 1( )JT f , 1( )IT e >.  
1
1
1 1
,
? arg max ( ( ) | ( ))
  arg max ( )
I
I
i
I J
e
i
e r
re P T e T f
p r
? ??
=
? ?              (4) 
Algorithm 2: Tree Sequence-based Decoder 
 Input: 1( )
JT f   Output: 1( )
IT e  
 Data structures: 
1 2[ , ]h j j    To store translations to a span 1 2[ , ]j j  
1: for s = 0 to J -1 do      // s: span length 
2:     for 1j = 1 to J - s , 2j = 1j + s  do  
3:          for each rule r spanning 1 2[ , ]j j  do  
4:               if r  is an initial rule then 
5:                    insert r into 1 2[ , ]h j j  
6:               else 
7:      generate new translations from 
r by replacing non-terminal leaf 
nodes of r with their correspond-
ing spans? translations that are al-
ready translated in previous steps 
8:      insert them into 1 2[ , ]h j j  
9:  end if 
10: end for 
11: end for 
12: end for 
13: output the hypothesis with the highest score  
in [1, ]h J  as the final best translation 
 
The decoder is a span-based beam search to-
gether with a function for mapping the source deri-
vations to the target ones. Algorithm 2 illustrates 
the decoding algorithm. It translates each span ite-
ratively from small one to large one (lines 1-2).  
This strategy can guarantee that when translating 
the current span, all spans smaller than the current 
one have already been translated before if they are 
translatable (line 7). When translating a span, if the 
usable rule is an initial rule, then the tree sequence 
on the target side of the rule is a candidate transla-
tion (lines 4-5). Otherwise, we replace the non-
terminal leaf nodes of the current abstract rule 
with their corresponding spans? translations that 
are already translated in previous steps (line 7). To 
speed up the decoder, we use several thresholds to 
limit search beams for each span:  
1) ? , the maximal number of rules used 
2) ? , the minimal log probability of rules 
3) ? , the maximal number of translations yield  
It is worth noting that the decoder does not force 
a complete target parse tree to be generated. If no 
rules can be used to generate a complete target 
parse tree, the decoder just outputs whatever have 
563
been translated so far monotonically as one hy-
pothesis. 
6 Experiments 
6.1 Experimental Settings 
We conducted Chinese-to-English translation ex-
periments. We trained the translation model on the 
FBIS corpus (7.2M+9.2M words) and trained a 4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM Toolkits (Stolcke, 2002) with modified 
Kneser-Ney smoothing. We used sentences with 
less than 50 characters from the NIST MT-2002 
test set as our development set and the NIST MT-
2005 test set as our test set. We used the Stanford 
parser (Klein and Manning, 2003) to parse bilin-
gual sentences on the training set and Chinese sen-
tences on the development and test sets. The 
evaluation metric is case-sensitive BLEU-4 (Papi-
neni et al, 2002). We used GIZA++ (Och and Ney, 
2004) and the heuristics ?grow-diag-final? to gen-
erate m-to-n word alignments. For the MER train-
ing (Och, 2003), we modified Koehn?s MER 
trainer (Koehn, 2004) for our tree sequence-based 
system. For significance test, we used Zhang et als 
implementation (Zhang et al 2004). 
We set three baseline systems: Moses (Koehn et 
al., 2007), and SCFG-based and STSG-based tree-
to-tree translation models (Zhang et al, 2007). For 
Moses, we used its default settings. For the 
SCFG/STSG and our proposed model, we used the 
same settings except for the parameters d and h  
( 1d = and 2h = for the SCFG; 1d = and 6h = for 
the STSG; 4d =  and 6h = for our model). We 
optimized these parameters on the training and de-
velopment sets: c =3, ? =20, ? =-100 and ? =100. 
6.2 Experimental Results   
We carried out a number of experiments to ex-
amine the proposed tree sequence alignment-based 
translation model. In this subsection, we first re-
port the rule distributions and compare our model 
with the three baseline systems. Then we study the 
model?s expressive ability by comparing the con-
tributions made by different kinds of rules, includ-
ing strict tree sequence rules, non-syntactic phrase 
rules, structure reordering rules and discontinuous 
phrase rules2. Finally, we investigate the impact of 
maximal sub-tree number and sub-tree depth in our 
model. All of the following discussions are held on 
the training and test data. 
 
 
Rule 
 Initial Rules  Abstract Rules  
L P U Total 
BP 322,965 0 0  322,965
TR 443,010 144,459 24,871  612,340
TSR 225,570 103,932 714  330,216
 
Table 1: # of rules used in the testing ( 4d = , h =  6) 
(BP: bilingual phrase (used in Moses), TR: tree rule (on-
ly 1 tree), TSR: tree sequence rule (> 1 tree), L: fully 
lexicalized, P: partially lexicalized, U: unlexicalized) 
 
Table 1 reports the statistics of rules used in the 
experiments. It shows that:  
1) We verify that the BPs are fully covered by 
the initial rules (i.e. lexicalized rules), in which the 
lexicalized TSRs model all non-syntactic phrase 
pairs with rich syntactic information. In addition, 
we find that the number of initial rules is greater 
than that of bilingual phrases. This is because one 
bilingual phrase can be covered by more than one 
initial rule which having different sub-tree struc-
tures. 
2) Abstract rules generalize initial rules to un-
seen data and with structure reordering ability. The 
number of the abstract rule is far less than that of 
the initial rules. This is because leaf nodes of an 
abstract rule can be non-terminals that can 
represent any sub-trees using the non-terminals as 
roots.   
Fig. 4 compares the performance of different 
models. It illustrates that: 
1) Our tree sequence-based model significantly 
outperforms (p < 0.01) previous phrase-based and 
linguistically syntax-based methods. This empirical-
ly verifies the effect of the proposed method. 
2) Both our method and STSG outperform Mos-
es significantly. Our method also clearly outper-
forms STSG. These results suggest that: 
z The linguistically motivated structure features 
are very useful for SMT, which can be cap-
                                                          
2 To be precise, we examine the contributions of strict tree 
sequence rules and single tree rules separately in this section. 
Therefore, unless specified, the term ?tree sequence rules? 
used in this section only refers to the strict tree sequence rules, 
which must contain at least two sub-trees on the source side. 
564
tured by the two syntax-based models through 
tree node operations. 
z Our model is much more effective in utilizing 
linguistic structures than STSG since it uses 
tree sequence as basic translation unit. This 
allows our model not only to handle structure 
reordering by tree node operations in a larger 
span, but also to capture non-syntactic phras-
es, which circumvents previous syntactic 
constraints, thus giving our model more ex-
pressive power. 
3) The linguistically motivated SCFG shows 
much lower performance. This is largely because 
SCFG only allows sibling nodes reordering and fails 
to utilize both non-syntactic phrases and those syn-
tactic phrases that cannot be covered by a single 
CFG rule. It thereby suggests that SCFG is less 
effective in modelling parse tree structure transfer 
between Chinese and English when using Penn 
Treebank style linguistic grammar and under word-
alignment constraints. However, formal SCFG 
show much better performance in the formally syn-
tax-based translation framework (Chiang, 2005). 
This is because the formal syntax is learned from 
phrases directly without relying on any linguistic 
theory (Chiang, 2005). As a result, it is more ro-
bust to the issue of non-syntactic phrase usage and 
non-isomorphic structure alignment.  
24.71
26.07
23.86
22.72
21.5
22.5
23.5
24.5
25.5
26.5
SCFG Moses STSG Ours
BL
EU
(%
)
 
Figure 4: Performance comparison of different methods 
 
Rule  
Type 
TR 
(STSG) 
TR 
+TSR_L 
TR+TSR_L
+TSR_P 
TR 
+TSR 
BLEU(%) 24.71 25.72 25.93 26.07 
 
Table 2: Contributions of TSRs (see Table 1 for the de-
finitions of the abbreviations used in this table) 
 
Table 2 measures the contributions of different 
kinds of tree sequence rules. It suggests that: 
1) All the three kinds of TSRs contribute to the 
performance improvement and their combination 
further improves the performance. It suggests that 
they are complementary to each other since the 
lexicalized TSRs are used to model non-syntactic 
phrases while the other two kinds of TSRs can ge-
neralize the lexicalized rules to unseen phrases. 
2)  The lexicalized TSRs make the major con-
tribution since they can capture non-syntactic 
phrases with syntactic structure features. 
 
Rule Type BLEU (%) 
TR+TSR 26.07 
(TR+TSR) w/o SRR 24.62 
(TR+TSR) w/o DPR 25.78 
 
Table 3: Effect of Structure Reordering Rules (SRR: 
refers to the structure reordering rules that have at least 
two non-terminal leaf nodes with inverted order in the 
source and target sides, which are usually not captured 
by phrase-based models. Note that the reordering be-
tween lexical words and non-terminal leaf nodes is not 
considered here) and Discontinuous Phrase Rules (DPR: 
refers to these rules having at least one non-terminal 
leaf node between two lexicalized leaf nodes) in our 
tree sequence-based model ( 4d =  and 6h = ) 
 
Rule Type # of rules # of rules overlapped 
(Intersection) 
SRR 68,217 18,379 (26.9%) 
DPR 57,244 18,379 (32.1%) 
 
Table 4: numbers of SRR and DPR rules 
 
Table 3 shows the contributions of SRR and 
DPR. It clearly indicates that SRRs are very effec-
tive in reordering structures, which improve per-
formance by 1.45 (26.07-24.62) BLEU score. 
However, DPRs have less impact on performance 
in our tree sequence-based model. This seems in 
contradiction to the previous observations3 in lite-
rature. However, it is not surprising simply be-
cause we use tree sequences as the basic translation 
units. Thereby, our model can capture all phrases. 
In this sense, our model behaves like a phrase-
based model, less sensitive to discontinuous phras-
                                                          
3 Wellington et al (2006) reports that discontinuities are very 
useful for translational equivalence analysis using binary-
branching structures under word alignment and parse tree 
constraints while they are almost of no use if under word 
alignment constraints only. Bod (2007) finds that discontinues 
phrase rules make significant performance improvement in 
linguistically STSG-based SMT models. 
565
es (Wellington et al, 2006). Our additional expe-
riments also verify that discontinuous phrase rules 
are complementary to syntactic phrase rules (Bod, 
2007) while non-syntactic phrase rules may com-
promise the contribution of discontinuous phrase 
rules. Table 4 reports the numbers of these two 
kinds of rules. It shows that around 30% rules are 
shared by the two kinds of rule sets. These over-
lapped rules contain at least two non-terminal leaf 
nodes plus two terminal leaf nodes, which implies 
that longer rules do not affect performance too 
much. 
 
22.07
25.28
26.1425.94 26.02 26.07
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5 6
BL
EU
(%
)
 
Figure 5: Accuracy changing with different max-
imal tree depths ( h = 1 to 6 when 4d = ) 
 
22.72
24.71
26.0526.03 26.07
25.74
25.2925.2825.2624.78
21.5
22.5
23.5
24.5
25.5
26.5
1 2 3 4 5
B
LE
U
(%
)
 
Figure 6: Accuracy changing with the different maximal 
number of trees in a tree sequence ( d =1 to 5), the upper 
line is for 6h =  while the lower line is for 2h = .  
 
Fig. 5 studies the impact when setting different 
maximal tree depth ( h ) in a rule on the perfor-
mance. It demonstrates that:  
1) Significant performance improvement is 
achieved when the value of h  is increased from 1 
to 2. This can be easily explained by the fact that 
when h = 1, only monotonic search is conducted, 
while h = 2 allows non-terminals to be leaf nodes, 
thus introducing preliminary structure features to 
the search and allowing non-monotonic search. 
2) Internal structures and large span (due to h  
increasing) are also useful as attested by the gain 
of 0.86 (26.14-25.28) Blue score when the value of 
h  increases from 2 to 4. 
Fig. 6 studies the impact on performance by set-
ting different maximal tree number (d) in a rule. It 
further indicates that: 
1) Tree sequence rules (d >1) are useful and 
even more helpful if we limit the tree depth to no 
more than two (lower line, h=2). However, tree 
sequence rules consisting of more than three sub-
trees have almost no contribution to the perform-
ance improvement. This is mainly due to data 
sparseness issue when d >3. 
2) Even if only two-layer sub-trees (lower line) 
are allowed, our method still outperforms STSG 
and Moses when d>1. This further validates the 
effectiveness of our design philosophy of using 
multi-sub-trees as basic translation unit in SMT. 
7 Conclusions and Future Work 
In this paper, we present a tree sequence align-
ment-based translation model to combine the 
strengths of phrase-based and syntax-based me-
thods. The experimental results on the NIST MT-
2005 Chinese-English translation task demonstrate 
the effectiveness of the proposed model. Our study 
also finds that in our model the tree sequence rules 
are very useful since they can model non-syntactic 
phrases and reorderings with rich linguistic struc-
ture features while discontinuous phrases and tree 
sequence rules with more than three sub-trees have 
less impact on performance. 
There are many interesting research topics on 
the tree sequence-based translation model worth 
exploring in the future. The current method ex-
tracts large amount of rules. Many of them are re-
dundant, which make decoding very slow. Thus, 
effective rule optimization and pruning algorithms 
are highly desirable. Ideally, a linguistically and 
empirically motivated theory can be worked out, 
suggesting what kinds of rules should be extracted 
given an input phrase pair. For example, most 
function words and headwords can be kept in ab-
stract rules as features. In addition, word align-
ment is a hard constraint in our rule extraction. We 
will study direct structure alignments to reduce the 
impact of word alignment errors. We are also in-
terested in comparing our method with the forest-
to-string model (Liu et al, 2007). Finally, we 
would also like to study unsupervised learning-
based bilingual parsing for SMT.  
566
 References  
Rens Bod. 2007. Unsupervised Syntax-Based Machine 
Translation: The Contribution of Discontinuous 
Phrases. MT-Summmit-07. 51-56. 
David Chiang. 2005. A hierarchical phrase-based mod-
el for SMT. ACL-05. 263-270. 
Brooke Cowan, Ivona Kucerova and Michael Collins. 
2006. A discriminative model for tree-to-tree transla-
tion. EMNLP-06. 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sertion grammars. ACL-05. 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? HLT-
NAACL-04. 
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and Training of Context-Rich Syntactic 
Translation Models. COLING-ACL-06. 961-968 
Daniel Gildea. 2003. Loosely Tree-Based Alignment for 
Machine Translation. ACL-03. 80-87. 
Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers. HLT-NAACL-2004. 105-112. 
Mary Hearne and Andy Way. 2003. Seeing the wood for 
the trees: data-oriented translation. MT Summit IX, 
165-172. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical Syntax-Directed Translation with Ex-
tended Domain of Locality. AMTA-06 (poster). 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. ACL-03. 423-430. 
Philipp Koehn, F. J. Och and D. Marcu. 2003. Statistic-
al phrase-based translation. HLT-NAACL-03. 127-
133. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. AMTA-04, 115-124 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
ACL-07 (poster) 77-180. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-String Statistical Translation Rules. 
ACL-07. 704-711. 
Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 
2006. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-06. 
44-52. 
I. Dan Melamed. 2004. Statistical machine translation 
by parsing. ACL-04. 653-660. 
Franz J. Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statistical 
machine translation. ACL-02. 295-302. 
Franz J. Och. 2003. Minimum error rate training in 
statistical machine translation. ACL-03. 160-167. 
Franz J. Och and Hermann Ney. 2004a. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30(4):417-449. 
Kishore Papineni, Salim Roukos, ToddWard and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. ACL-02. 311-318. 
Arjen Poutsma. 2000. Data-oriented translation. 
COLING-2000. 635-641 
Chris Quirk and Arul Menezes. 2006. Do we need 
phrases? Challenging the conventional wisdom in 
SMT. COLING-ACL-06. 9-16. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. ACL-05. 271-279. 
Stefan Riezler and John T. Maxwell III. 2006. Gram-
matical Machine Translation. HLT-NAACL-06. 
248-255. 
Hendra Setiawan, Min-Yen Kan and Haizhou Li. 2007. 
Ordering Phrases with Function Words. ACL-7. 
712-719. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Benjamin Wellington, Sonjia Waxmonsky and I. Dan 
Melamed. 2006. Empirical Lower Bounds on the 
Complexity of Translational Equivalence. COLING-
ACL-06. 977-984. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377-403. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
SMT. COLING-ACL-06. 521? 528. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation model. ACL-01. 523-530. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng 
Li and Chew Lim Tan. 2007. A Tree-to-Tree Align-
ment-based Model for Statistical Machine Transla-
tion. MT-Summit-07. 535-542. 
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement 
do we need to have a better system? LREC-04. 2051-
2054. 
567
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125?128,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Statistical Machine Translation Model Based on a Synthetic
Synchronous Grammar
Hongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo Wang
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cn
Abstract
Recently, various synchronous grammars
are proposed for syntax-based machine
translation, e.g. synchronous context-free
grammar and synchronous tree (sequence)
substitution grammar, either purely for-
mal or linguistically motivated. Aim-
ing at combining the strengths of differ-
ent grammars, we describes a synthetic
synchronous grammar (SSG), which ten-
tatively in this paper, integrates a syn-
chronous context-free grammar (SCFG)
and a synchronous tree sequence substitu-
tion grammar (STSSG) for statistical ma-
chine translation. The experimental re-
sults on NIST MT05 Chinese-to-English
test set show that the SSG based transla-
tion system achieves significant improve-
ment over three baseline systems.
1 Introduction
The use of various synchronous grammar based
formalisms has been a trend for statistical ma-
chine translation (SMT) (Wu, 1997; Eisner, 2003;
Galley et al, 2006; Chiang, 2007; Zhang et al,
2008). The grammar formalism determines the in-
trinsic capacities and computational efficiency of
the SMT systems.
To evaluate the capacity of a grammar formal-
ism, two factors, i.e. generative power and expres-
sive power are usually considered (Su and Chang,
1990). The generative power refers to the abil-
ity to generate the strings of the language, and
the expressive power to the ability to describe the
same language with fewer or no extra ambigui-
ties. For the current synchronous grammars based
SMT, to some extent, the generalization ability of
the grammar rules (the usability of the rules for the
new sentences) can be considered as a kind of the
generative power of the grammar and the disam-
biguition ability to the rule candidates can be con-
sidered as an embodiment of expressive power.
However, the generalization ability and the dis-
ambiguition ability often contradict each other in
practice such that various grammar formalisms
in SMT are actually different trade-off be-
tween them. For instance, in our investiga-
tions for SMT (Section 3.1), the Formally SCFG
based hierarchical phrase-based model (here-
inafter FSCFG) (Chiang, 2007) has a better gen-
eralization capability than a Linguistically moti-
vated STSSG based model (hereinafter LSTSSG)
(Zhang et al, 2008), with 5% rules of the former
matched by NIST05 test set while only 3.5% rules
of the latter matched by the same test set. How-
ever, from expressiveness point of view, the for-
mer usually results in more ambiguities than the
latter.
To combine the strengths of different syn-
chronous grammars, this paper proposes a statisti-
cal machine translation model based on a synthetic
synchronous grammar (SSG) which syncretizes
FSCFG and LSTSSG. Moreover, it is noteworthy
that, from the combination point of view, our pro-
posed scheme can be considered as a novel system
combination method which goes beyond the ex-
isting post-decoding style combination of N -best
hypotheses from different systems.
2 The Translation Model Based on the
Synthetic Synchronous Grammar
2.1 The Synthetic Synchronous Grammar
Formally, the proposed Synthetic Synchronous
Grammar (SSG) is a tuple
G = ??
s
,?
t
, N
s
, N
t
, X,P?
where ?
s
(?
t
) is the alphabet set of source (target)
terminals, namely the vocabulary; N
s
(N
t
) is the
alphabet set of source (target) non-terminals, such
125
? ? ???
Figure 1: A syntax tree pair example. Dotted lines
stands for the word alignments.
as the POS tags and the syntax labels; X repre-
sents the special nonterminal label in FSCFG; and
P is the grammar rule set which is the core part of
a grammar. Every rule r in P is as:
r = ??, ?,A
NT
, A
T
, ???
where ? ? [{X}, N
s
,?
s
]
+
is a sequence of one or
more source words in ?
s
and nonterminals sym-
bols in [{X}, N
s
];? ? [{X}, N
t
,?
t
]
+
is a se-
quence of one or more target words in ?
t
and non-
terminals symbols in [{X}, N
t
]; A
T
is a many-to-
many corresponding set which includes the align-
ments between the terminal leaf nodes from source
and target side, and A
NT
is a one-to-one corre-
sponding set which includes the synchronizing re-
lations between the non-terminal leaf nodes from
source and target side; ?? contains feature values
associated with each rule.
Through this formalization, we can see that
FSCFG rules and LSTSSG rules are both in-
cluded. However, we should point out that the
rules with mixture of X non-terminals and syn-
tactic non-terminals are not included in our cur-
rent implementation despite that they are legal
under the proposed formalism. The rule extrac-
tion in current implementation can be considered
as a combination of the ones in (Chiang, 2007)
and (Zhang et al, 2008). Given the sentence pair
in Figure 1, some SSG rules can be extracted as
illustrated in Figure 2.
2.2 The SSG-based Translation Model
The translation in our SSG-based translation
model can be treated as a SSG derivation. A
derivation consists of a sequence of grammar rule
applications. To model the derivations as a latent
variable, we define the conditional probability dis-
tribution over the target translation e and the cor-
Input: A source parse tree T (f
J
1
)
Output: A target translation e?
for u := 0 to J ? 1 do
for v := 1 to J ? u do
foreach rule r = ??, ?,A
NT
, A
T
, ??? spanning
[v, v + u] do
if A
NT
of r is empty then
Add r into H[v, v + u];
end
else
Substitute the non-terminal leaf node pair
(N
src
, N
tgt
) with the hypotheses in the
hypotheses stack corresponding with N
src
?s
span iteratively.
end
end
end
end
Output the 1-best hypothesis in H[1, J] as the final translation.
Figure 3: The pseudocode for the decoding.
responding derivation d of a given source sentence
f as
(1) p
?
(d, e|f) =
exp
?
k
?
k
H
k
(d, e, f)
?
?
(f)
where H
k
is a feature function ,?
k
is the corre-
sponding feature weight and ?
?
(f) is a normal-
ization factor for each derivation of f. The main
challenge of SSG-based model is how to distin-
guish and weight the different kinds of derivations
. For a simple illustration, using the rules listed in
Figure 2, three derivations can be produced for the
sentence pair in Figure 1 by the proposed model:
d
1
= (R
4
, R
1
, R
2
)
d
2
= (R
6
, R
7
, R
8
)
d
3
= (R
4
, R
7
, R
2
)
All of them are SSG derivations while d
1
is also a
FSCFG derivation, d
2
is also a LSTSSG deriva-
tion. Ideally, the model is supposed to be able
to weight them differently and to prefer the better
derivation, which deserves intensive study. Some
sophisticated features can be designed for this is-
sue. For example, some features related with
structure richness and grammar consistency
1
of a
derivation should be designed to distinguish the
derivations involved various heterogeneous rule
applications. For the page limit and the fair com-
parison, we only adopt the conventional features
as in (Zhang et al, 2008) in our current implemen-
tation.
1
This relates with reviewers? questions: ?can a rule ex-
pecting an NN accept an X?? and ?. . . the interaction between
the two typed of rules . . . ?. In our study in progress, we
would design some features to distinguish the derivation steps
which fulfill the expectation or not, to measure how much
heterogeneous rules are applied in a derivation and so on.
126
R6
1?
BA
VV[2]NN[1]
1
VB[2] NP[1]?
PN
to me
TO PRP
PP
1
R7
penthe
DT NN
NP
??
NN
1
R4 Give 1? 1 X[1] X[2]X[2]? X[1] R5 X[1]X[1] ? 2 the pen 1 to 2me1??
R1 penthe 1?? 1 R3 theGive 2 pen 1? 2?? 1R2 to me 1? 1
R8
?
VV
Give
VB
11
Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure
1. R
1
-R
3
are bilingual phrase rules, R
4
-R
5
are FSCFG rules and R
6
-R
8
are LSTSSG rules.
2.3 Decoding
For efficiency, our model approximately search for
the single ?best? derivation using beam search as
(2) (
?
e,
?
d) = argmax
e,d
{
?
k
?
k
h
k
(d, e, f)
}
.
The major challenge for such a SSG-based de-
coder is how to apply the heterogeneous rules in a
derivation. For example, (Chiang, 2007) adopts a
CKY style span-based decoding while (Liu et al,
2006) applies a linguistically syntax node based
bottom-up decoding, which are difficult to inte-
grate. Fortunately, our current SSG syncretizes
FSCFG and LSTSSG. And the conventional de-
codings of both FSCFG and LSTSSG are span-
based expansion. Thus, it would be a natural way
for our SSG-based decoder to conduct a span-
based beam search. The search procedure is given
by the pseudocode in Figure 3. A hypotheses
stack H[i, j] (similar to the ?chart cell? in CKY
parsing) is arranged for each span [i, j] for stor-
ing the translation hypotheses. The hypotheses
stacks are ordered such that every span is trans-
lated after its possible antecedents: smaller spans
before larger spans. For translating each span
[i, j], the decoder traverses each usable rule r =
??, ?,A
NT
, A
T
, ???. If there is no nonterminal
leaf node in r, the target side ? will be added into
H[i, j] as the candidate hypothesis. Otherwise, the
nonterminal leaf nodes in r should be substituted
iteratively by the corresponding hypotheses until
all nonterminal leaf nodes are processed. The key
feature of our decoder is that the derivations are
based on synthetic grammar, so that one derivation
may consist of applications of heterogeneous rules
(Please see d
3
in Section 2.2 as a simple demon-
stration).
3 Experiments and Discussions
Our system, named HITREE, is implemented in
standard C++ and STL. In this section we report
Extracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)
BP 11,137 4,613(41.4%) 323(0.5%)
LSTSSG 45,580 28,497(62.5%) 984(3.5%)
FSCFG 59,339 25,520(43.0%) 1,266(5.0%)
HITREE 93,782 49,404(52.7%) 1,927(3.9%)
Table 1: The statistics of the counts of the rules in
different phases. ?k? means one thousand.
on experiments with Chinese-to-English transla-
tion base on it. We used FBIS Chinese-to-English
parallel corpora (7.2M+9.2M words) as the train-
ing data. We also used SRI Language Model-
ing Toolkit to train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus(181M words). NIST MT2002 test set is used
as the development set. The NIST MT2005 test
set is used as the test set. The evaluation met-
ric is case-sensitive BLEU4. For significant test,
we used Zhang?s implementation (Zhang et al,
2004)(confidence level of 95%). For comparisons,
we used the following three baseline systems:
LSTSSG An in-house implementation of linguis-
tically motivated STSSG based model similar
to (Zhang et al, 2008).
FSCFG An in-house implementation of purely
formally SCFG based model similar to (Chiang,
2007).
MBR We use an in-house combination system
which is an implementation of a classic sentence
level combination method based on the Minimum
Bayes Risk (MBR) decoding (Kumar and Byrne,
2004).
3.1 Statistics of Rule Numbers in Different
Phases
Table 1 summarizes the statistics of the rules for
different models in three phases: after extrac-
tion (Extracted), after scoring(Scored), and af-
ter filtering (Filtered) (filtered by NIST05 test
set just, similar to the filtering step in phrase-
based SMT system). In Extracted phase, FSCFG
127
ID System BLEU4 #of used rules(k)
1 LSTSSG 0.2659?0.0043 984
2 FSCFG 0.2613?0.0045 1,266
3 HITREE 0.2730?0.0045 1,927
4 MBR(1,2) 0.2685?0.0044 ?
Table 2: The Comparison of LSTSSG, FSCFG
,HITREE and the MBR.
has obvious more rules than LSTSSG. However,
in Scored phase, this situation reverses. Inter-
estingly, the situation reverses again in Filtered
phase. The reasons for these phenomenons are
that FSCFG abstract rules involves high-degree
generalization. Each FSCFG abstract rule aver-
agely have several duplicates
2
in the extracted rule
set. Then, the duplicates will be discarded dur-
ing scoring. However, due to the high-degree gen-
eralization , the FSCFG abstract rules are more
likely to be matched by the test sentences. Con-
trastively, LSTSSG rules have more diversified
structures and thus weaker generalization capabil-
ity than FSCFG rules. From the ratios of two tran-
sition states, Table 1 indicates that HITREE can
be considered as compromise of FSCFG between
LSTSSG.
3.2 Overall Performances
The performance comparison results are presented
in Table 2. The experimental results show that
the SSG-based model (HITREE) achieves signifi-
cant improvements over the models based on the
two isolated grammars: FSCFG and LSTSSG
(both p < 0.001). From combination point of
view, the newly proposed model can be consid-
ered as a novel method going beyond the con-
ventional post-decoding style combination meth-
ods. The baseline Minimum Bayes Risk com-
bination of LSTSSG based model and FSCFG
based model (MBR(1, 2)) obtains significant im-
provements over both candidate models (both p <
0.001). Meanwhile, the experimental results show
that the proposed model outperforms MBR(1, 2)
significantly (p < 0.001). These preliminary re-
sults indicate that the proposed SSG-based model
is rather promising and it may serve as an alterna-
tive, if not superior, to current combination meth-
ods.
4 Conclusions
To combine the strengths of different gram-
mars, this paper proposes a statistical machine
2
Rules with identical source side and target side are du-
plicated.
translation model based on a synthetic syn-
chronous grammar (SSG) which syncretizes a
purely formal synchronous context-free gram-
mar (FSCFG) and a linguistically motivated syn-
chronous tree sequence substitution grammar
(LSTSSG). Experimental results show that SSG-
based model achieves significant improvements
over the FSCFG-based model and LSTSSG-based
model.
In the future work, we would like to verify
the effectiveness of the proposed model on vari-
ous datasets and to design more sophisticated fea-
tures. Furthermore, the integrations of more dif-
ferent kinds of synchronous grammars for statisti-
cal machine translation will be investigated.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In computational linguistics, 33(2).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL 2003.
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-
rich syntactic translation models In Proceedings of
ACL-COLING.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
04.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. In Proceedings of ACL-COLING.
Keh-Yin Su and Jing-Shin Chang. 1990. Some key
Issues in Designing Machine Translation Systems.
Machine Translation, 5(4):265-300.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of LREC 2004, pages 2051-2054.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT.
128
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 45?50,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study of Translation Rule Classification for Syntax-based Statistical
Machine Translation
Hongfei Jiang, Sheng Li, Muyun Yang and Tiejun Zhao
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,lisheng,ymy,tjzhao}@mtlab.hit.edu.cn
Abstract
Recently, numerous statistical machine trans-
lation models which can utilize various kinds
of translation rules are proposed. In these
models, not only the conventional syntactic
rules but also the non-syntactic rules can be
applied. Even the pure phrase rules are in-
cludes in some of these models. Although the
better performances are reported over the con-
ventional phrase model and syntax model, the
mixture of diversified rules still leaves much
room for study. In this paper, we present a
refined rule classification system. Based on
this classification system, the rules are classi-
fied according to different standards, such as
lexicalization level and generalization. Espe-
cially, we refresh the concepts of the structure
reordering rules and the discontiguous phrase
rules. This novel classification system may
supports the SMT research community with
some helpful references.
1 Introduction
Phrase-based statistical machine translation mod-
els (Marcu and Wong, 2002; Koehn et al, 2003; Och
and Ney, 2004; Koehn, 2004; Koehn et al, 2007)
have achieved significant improvements in trans-
lation accuracy over the original IBM word-based
model. However, there are still many limitations in
phrase based models. The most frequently pointed
limitation is its inefficacy to modeling the struc-
ture reordering and the discontiguous correspond-
ing. To overcome these limitations, many syntax-
based SMT models have been proposed (Wu, 1997;
Chiang, 2007; Ding et al, 2005; Eisner, 2003; Quirk
et al, 2005; Liu et al, 2007; Zhang et al, 2007;
Zhang et al, 2008a; Zhang et al, 2008b; Gildea,
2003; Galley et al, 2004; Marcu et al, 2006; Bod,
2007). The basic motivation behind syntax-based
model is that the syntax information has the poten-
tial to model the structure reordering and discontigu-
ous corresponding by the intrinsic structural gener-
alization ability. Although remarkable progresses
have been reported, the strict syntactic constraint
(the both sides of the rules should strictly be a sub-
tree of the whole syntax parse) greatly hinders the
utilization of the non-syntactic translation equiva-
lents. To alleviate this constraint, a few works have
attempted to make full use of the non-syntactic rules
by extending their syntax-based models to more
general frameworks. For example, forest-to-string
transformation rules have been integrated into the
tree-to-string translation framework by (Liu et al,
2006; Liu et al, 2007). Zhang et al (2008a) made
it possible to utilize the non-syntactic rules and even
the phrases which are used in phrase based model
by advancing a general tree sequence to tree se-
quence framework based on the tree-to-tree model
presented in (Zhang et al, 2007). In these mod-
els, various kinds of rules can be employed. For
example, as shown in Figure 1 and Figure 2, Fig-
ure 1 shows a Chinese-to-English sentence pair with
syntax parses on both sides and the word alignments
(dotted lines). Figure 2 lists some of the rules which
can be extracted from the sentence pair in Figure 1
by the system used in (Zhang et al, 2008a). These
rules includes not only conventional syntax rules but
also the tree sequence rules (the multi-headed syn-
tax rules ). Even the phrase rules are adopted by
45
the system. Although the better performances are
reported over the conventional phrase-based model
and syntax-based model, the mixture of diversified
rules still leaves much room for study. Given such a
hybrid rule set, we must want to know what kinds of
rules can make more important contributions to the
overall system performance and what kinds of rules
are redundant compared with the others. From en-
gineering point of view, the developers may concern
about which kinds of rules should be preferred and
which kinds of rules could be discard without too
much decline in translation quality. However, one of
the precondition for the investigations of these issues
is what are the ?rule categories?? In other words,
some comprehensive rule classifications are neces-
sary to make the rule analyses feasible. The motiva-
tion of this paper is to present such a rule classifica-
tion.
2 Related Works
A few researches have made some exploratory in-
vestigations towards the effects of different rules by
classifying the translation rules into different sub-
categories (Liu et al, 2007; Zhang et al, 2008a;
DeNeefe et al, 2007). Liu et al (2007) differenti-
ated the rules in their tree-to-string model which in-
tegrated with forest1-to-string into fully lexicalized
rules, non-lexicalized rules and partial lexicalized
rules according to the lexicalization levels. As an
extension, Zhang et al (2008a) proposed two more
categories: Structure Reordering Rules (SRR) and
Discontiguous Phrase Rules (DPR). The SRR stands
for the rules which have at least two non-terminal
leaf nodes with inverted order in the source and tar-
get side. And DPR refers to the rules having at
least one non-terminal leaf node between two termi-
nal leaf nodes. (DeNeefe et al, 2007) made an illu-
minating breakdown of the different kinds of rules.
Firstly, they classify all the GHKM2 rules (Galley et
al., 2004; Galley et al, 2006) into two categories:
lexical rules and non-lexical rules. The former are
the rules whose source side has no source words.
In other words, a non-lexical rule is a purely ab-
1A ?forest? means a sub-tree sequence derived from a given
parse tree
2One reviewer asked about the acronym GHKM. We guess
it is an acronym for the authors of (Galley et al, 2004): Michel
Galley, Mark Hopkins, Kevin Knight and Daniel Marcu.
? ? ???
Figure 1: A syntax tree pair example. Dotted lines stands
for the word alignments.
stract rule. The latter is the complementary set of
the former. And then lexical rules are classified fur-
ther into phrasal rules and non-phrasal rules. The
phrasal rules refer to the rules whose source side
and the yield of the target side contain exactly one
contiguous phrase each. And the one or more non-
terminals can be placed on either side of the phrase.
In other words, each phrasal rule can be simulated
by the conjunction of two more phrase rules. (De-
Neefe et al, 2007) classifies non-phrasal rules fur-
ther into structural rules, re-ordering rules, and non-
contiguous phrase rules. However, these categories
are not explicitly defined in (DeNeefe et al, 2007)
since out of its focus. Our proposed rule classifica-
tion is inspired by these works.
3 Rules Classifications
Currently, there have been several classifications
in SMT research community. Generally, the rules
can be classified into two main groups according to
whether syntax information is involved: bilingual
phrases (Phrase) and syntax rules (Syntax). Fur-
ther, the syntax rules can be divided into three cat-
egories according to the lexicalization levels (Liu et
al., 2007; Zhang et al, 2008a):
1) Fully lexicalized (FLex): all leaf nodes in both
the source and target sides are lexicons (termi-
nals)
2) Unlexicalized (ULex): all leaf nodes in both the
46
??
? ?
??? ?
???
? ?
Figure 2: Some rules can be extracted by the system used in (Zhang et al, 2008a) from the sentence pair in Figure 1.
source and target sides are non-lexicons (non-
terminals)
3) Partially lexicalized (PLex): otherwise.
In Figure 2, R1-R3 are FLex rules, and R5-R8 are
PLex rules.
Following (Zhang et al, 2008b), a syntax rule r
can be formalized into a tuple
< ?s, ?t, AT , ANT >
, where ?s and ?t are tree sequences of source side
and target side respectively, AT is a many-to-many
correspondence set which includes the alignments
between the terminal leaf nodes from source and tar-
get side, and ANT is a one-to-one correspondence
set which includes the synchronizing relations be-
tween the non-terminal leaf nodes from source and
target side.
Then, the syntax rules can also fall into two cat-
egories according to whether equipping with gen-
eralization capability (Chiang, 2007; Zhang et al,
2008a):
1) Initial rules (Initial): all leaf nodes of this rule are
terminals.
2) Abstract rules (Abstract): otherwise, i.e. at least
one leaf node is a non-terminal.
A non-terminal leaf node in a rule is named an ab-
stract node since it has the generalization capabil-
ity. Comparing these two classifications for syntax
rules, we can find that a FLex rule is a initial rule
when ULex rules and PLex rules belong to abstract
rules.
These classifications are clear and easy for un-
derstanding. However, we argue that they need
further refinement for in-depth study. Specially,
more refined differentiations are needed for the ab-
stract rules (ULex rules and PLex rules) since they
play important roles for the characteristic capabil-
ities which are deemed to be the advantages over
the phrase-based model. For instance, the potentials
to model the structure reordering and the discon-
tiguous correspondence. The Structure Reordering
Rules (SRR) and Discontiguous Phrase Rules (DPR)
mentioned by (Zhang et al, 2008a) can be regarded
as more in-depth classification of the syntax rules.
In (Zhang et al, 2008a), they are described as fol-
lows:
Definition 1: The Structure Reordering Rule
(SRR) refers to the structure reordering rule that has
at least two non-terminal leaf nodes with inverted
order in the source and target side.
Definition 2: The Discontiguous Phrase Rule
(DPR) refers to the rule having at least one non-
terminal leaf node between two lexicalized leaf
nodes.
47
Based on these descriptions, R7, R8 in Figure 2
belong to the category of SRR and R6, R7 fall into
the category of DPR. Although these two definitions
are easy implemented in practice, we argue that the
definition of SRR is not complete. The reordering
rules involving the reordering between content word
terminals and non-terminal (such as R5 in Figure
2) also can model the useful structure reorderings.
Moreover, it is not uncommon that a rule demon-
strates the reorderings between two non-terminals
as well as the reorderings between one non-terminal
and one content word terminal. The reason for our
emphasis of content word terminal is that the re-
orderings between the non-terminals and function
word are less meaningful.
One of the theoretical problems with phrase based
SMT models is that they can not effectively model
the discontiguous translations and numerous at-
tempts have been made on this issue (Simard et al,
2005; Quirk and Menezes, 2006; Wellington et al,
2006; Bod, 2007; Zhang et al, 2007). What seems
to be lacking, however, is a explicit definition to the
discontiguous translation. The definition of DPR
in (Zhang et al, 2008a) is explicit but somewhat
rough and not very accurate. For example, in Fig-
ure 3(a), non-terminal node pair ([0,???], [0,?love?]
) is surrounded by lexical terminals. According to
Definition 2, it is a DPR. However, obviously it is
not a discontiguous phrase actually. This rule can be
simulated by conjunctions of three phrases (???, ?I?;
???, ?love?; ???,?you?). In contrast, the translation
rule in Figure 3(b) is an actual discontiguous phrase
rule. The English correspondences of the Chinese
word ??? is dispersed in the English side in which
the correspondence of Chinese word ??? is inserted.
This rule can not be simulated by any conjunctions
of the sub phrases. It must be noted that the dis-
contiguous phrase (???-?switch . . . off?) can not
be abstracted under the existing synchronous gram-
mar frameworks. The fundamental reason is that
the corresponding parts should be abstracted in the
same time and lexicalized in the same time. In other
words, the discontiguous phrase can not be modeled
by the permutation between non-terminals (abstract
nodes). Another point to notice is that our focus in
this paper is the ability demonstrated by the abstract
rules. Thus, we do not pay much attentions to the re-
orderings and discontiguous phrases involved in the
? ?? ? ?
Figure 3: Examples for demonstrating the actual discon-
tiguous phrase. (a) is a negative example for the definition
of DPR in (Zhang et al, 2008a), (b) is a actual discon-
tiguous phrase rule.
2
Figure 4: The rule classifications used in this paper. (a)
shows that the rules can be divided into phrase rules and
syntax rules according to whether a rule includes the syn-
tactic information. (b) illustrates that the syntax rules can
be classified into three kinds according to the lexicaliza-
tion level. (c) shows that the abstract rules can be classi-
fied into more refined sub-categories.
phrase rules (e.g. ?? ??-?switch the light off?)
since they lack the generalization capability. There-
fore, the discontiguous phrase is limited to the rela-
tion between non-terminals and terminals.
On the basis of the above analyses, we present
a novel classification system for the abstract rules
based on the crossings between the leaf node
alignment links. Given an abstract rule r =<
?s, ?t, AT , ANT >, it is
1) a Structure Reordering Rule (SRR), if ? a link
l ? ANT is crossed with a link l? ? {AT ?ANT }
a) a SRR NT2 rule, if the link l? ? ANT
b) a SRR NT-T rule, if the link l? ? AT
2) not a Structure Reordering Rule (N-SRR), other-
wise.
48
??
?
Figure 5: The patterns to show the characteristics of dis-
contiguous phrase rules.
Note that the intersection of SRR NT2 and SRR NT-
T is not necessary an empty set, i.e. a rule can be
both SRR NT2 and SRR NT-T rule.
The basic characteristic of the discontiguous
translation is that the correspondence of one non-
terminal NT is inserted among the correspondences
of one phrase X . Figure 5 (a) illustrates this sit-
uation. However, this characteristic can not sup-
port necessary and sufficient condition. For exam-
ple, if the phrase X can be divided like Figure 5
(b), then the rule in Figure 5 (a) is actually a re-
ordering rule rather than a discontiguous phrase rule.
For sufficient condition, we constrain that the phrase
X = wi . . . wj need to satisfy the requirement: wi
should be connected with wj through word align-
ment links (A word is connected with itself). In Fig-
ure 5(c), f1 is connected with f2 when NT ? is in-
serted between e1 and e2. Thus, the rule in Figure
5(c) is a discontiguous phrase rule.
Definition 3: Given an abstract rule r =<
?s, ?t, AT , ANT >, it is a Discontiguous Phrase iff
? two links lt1, lt2 from AT and a link lnt from ANT ,
satisfy: lt1, lt2 are emitted from the same word and
lt1 is crossed with lnt when lt2 is not crossed with
lnt.
Through Definition 3, we know that the DPR is a
sub-set of the SRR NT-T.
4 Conclusions and Future Works
In this paper, we present a refined rule classifica-
tion system. Based on this classification system, the
rules are classified according to different standards,
such as lexicalization level and generalization. Es-
pecially, we refresh the concepts of the structure re-
ordering rules and the discontiguous phrase rules.
This novel classification system may supports the
SMT research community with some helpful refer-
ences.
In the future works, aiming to analyze the rule
contributions and the redundances issues using the
presented rule classification based on some real
translation systems, we plan to implement some syn-
chronous grammar based syntax translation models
such as the one presented in (Liu et al, 2007) or
in (Zhang et al, 2008a). Taking such a system as
the experimental platform, we can perform compre-
hensive statistics about distributions of different rule
categories. What is more important, the contribu-
tion of each rule category can be evaluated seriatim.
Furthermore, which kinds of rules are preferentially
applied in the 1-best decoding can be studied. All
these investigations could reveal very useful infor-
mation for the optimization of rule extraction and the
improvement of the computational models for syn-
chronous grammar based machine translation.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
Rens Bod. 2007. Unsupervised syntax-based machine
translation: The contribution of discontiguous phrases.
In Proceedings of Machine Translation Summit XI
2007,Copenhagen, Denmark.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. In computational linguistics, 33(2).
Ding, Y. and Palmer, M. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars In Proceedings of ACL.
DeNeefe, S. and Knight, K. and Wang, W. and Marcu, D.
2007. What can syntax-based MT learn from phrase-
based MT? In Proceedings of EMNLP/CONLL.
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-HLT 2004, pages 273-280.
49
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-rich
syntactic translation models In Proceedings of ACL-
COLING
Daniel Gildea 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of ACL 2003,
pages 80-87.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL 2003, pages 127-133, Edmonton,
Canada, May.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115-124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. ACL 2007,
demonstration session, Prague, Czech Republic, June
2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation.
In Proceedings of ACL-COLING.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL 2007, pages 704-711.
Daniel Marcu and William Wong. 2002. A phrase based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language Phrases. In
Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440-447.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417-449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages 271-
279, Ann Arbor, Michigan, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proceedings of
HLT/NAACL
Simard, M. and Cancedda, N. and Cavestro, B. and
Dymetman, M. and Gaussier, E. and Goutte, C. and
Yamada, K. and Langlais, P. and Mauser, A. 2005.
Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP, volume 2, pages 901-904.
Benjamin Wellington, Sonjia Waxmonsky and I. Dan
Melamed. 2006. Empirical Lower Bounds on the
Complexity of Translational Equivalence. In Proceed-
ings of ACL-COLING 2006, pages 977-984.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. In
Proceedings of ACL 1997. Computational Linguistics,
23(3):377-403.
Min Zhang, Hongfei Jiang, Ai Ti AW, Jun Sun, Sheng
Li, and Chew Lim Tan. 2007. A tree-to-tree
alignment-based model for statistical machine trans-
lation. In Proceedings of Machine Translation Summit
XI 2007,Copenhagen, Denmark.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008a. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT
Min Zhang, Hongfei Jiang, Haizhou Li, Ai Ti AW,
and Sheng Li. 2008b. Grammar Comparison Study
for Translational Equivalence Modeling and Statistical
Machine Translation. In Proceedings of Coling
50
