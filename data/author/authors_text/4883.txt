Utterance Segmentation Using Combined Approach  
Based on Bi-directional N-gram and Maximum Entropy 
 
Ding Liu 
National Laboratory of Pattern Recognition
Institute of Automation 
Chinese Academy of Sciences 
Beijing 100080, China. 
dliu@nlpr.ia.ac.cn 
Chengqing Zong 
National Laboratory of Pattern Recognition
Institute of Automation 
Chinese Academy of Sciences 
Beijing 100080, China. 
cqzong@nlpr.ia.ac.cn  
 
 
 
Abstract 
This paper proposes a new approach to 
segmentation of utterances into sentences 
using a new linguistic model based upon 
Maximum-entropy-weighted Bi-
directional N-grams. The usual N-gram 
algorithm searches for sentence bounda-
ries in a text from left to right only. Thus 
a candidate sentence boundary in the text 
is evaluated mainly with respect to its left 
context, without fully considering its right 
context. Using this approach, utterances 
are often divided into incomplete sen-
tences or fragments. In order to make use 
of both the right and left contexts of can-
didate sentence boundaries, we propose a 
new linguistic modeling approach based 
on Maximum-entropy-weighted Bi-
directional N-grams. Experimental results 
indicate that the new approach signifi-
cantly outperforms the usual N-gram al-
gorithm for segmenting both Chinese and 
English utterances. 
1 Introduction 
Due to the improvement of speech recognition 
technology, spoken language user interfaces, spo-
ken dialogue systems, and speech translation sys-
tems are no longer only laboratory dreams. 
Roughly speaking, such systems have the structure 
shown in Figure 1. 
 
 
Figure 1. System with speech input. 
 
In these systems, the language analysis module 
takes the output of speech recognition as its input, 
representing the current utterance exactly as pro-
nounced, without any punctuation symbols mark-
ing the boundaries of sentences. Here is an 
example: ???????? 9 ???????
???????? 913??? . (this way please 
please take this elevator to the ninth floor the floor 
attendant will meet you at your elevator entrance 
there and show you to room 913.) As the example 
shows, it will be difficult for a text analysis module 
to parse the input if the utterance is not segmented. 
Further, the output utterance from the speech rec-
ognizer usually contains wrongly recognized 
words or noise words. Thus it is crucial to segment 
the utterance before further language processing. 
We believe that accurate segmentation can greatly 
improve the performance of language analysis 
modules. 
Stevenson et al have demonstrated the difficul-
ties of text segmentation through an experiment in 
which six people, educated to at least the Bache-
lor?s degree level, were required to segment into 
sentences broadcast transcripts from which all 
punctuation symbols had been removed. The ex-
perimental results show that humans do not always 
agree on the insertion of punctuation symbols, and 
that their segmentation performance is not very 
good (Stevenson and Gaizauskas, 2000). Thus it is 
a great challenge for computers to perform the task 
Output (text 
or speech) Language 
analysis and 
generation 
Speech  
 
recognition
Input speech
automatically. To solve this problem, many meth-
ods have been proposed, which can be roughly 
classified into two categories. One approach is 
based on simple acoustic criteria, such as non-
speech intervals (e.g. pauses), pitch and energy. 
We can call this approach acoustic segmentation. 
The other approach, which can be called linguistic 
segmentation, is based on linguistic clues, includ-
ing lexical knowledge, syntactic structure, seman-
tic information etc. Acoustic segmentation can not 
always work well, because utterance boundaries do 
not always correspond to acoustic criteria. For ex-
ample: ??<pause>??<pause>??????
???<pause>??<pause>?????. Since 
the simple acoustic criteria are inadequate, linguis-
tic clues play an indispensable role in utterance 
segmentation, and many methods relying on them 
have been proposed. 
This paper proposes a new approach to linguis-
tic segmentation using a Maximum-entropy-
weighted Bi-directional N-gram-based algorithm 
(MEBN). To evaluate the performance of MEBN, 
we conducted experiments in both Chinese and 
English. All the results show that MEBN outper-
forms the normal N-gram algorithm. The remain-
der of this paper will focus on description of our 
new approach for linguistic segmentation. In Sec-
tion 2, some related work on utterance segmenta-
tion is briefly reviewed, and our motivations are 
described. Section 3 describes MEBN in detail. 
The experimental results are presented in Section 4. 
Finally, Section 5 gives our conclusion. 
2 Related Work and Our Motivations  
2.1 Related Work 
Stolcke et al (1998, 1996) proposed an approach 
to detection of sentence boundaries and disfluency 
locations in speech transcribed by an automatic 
recognizer, based on a combination of prosodic 
cues modeled by decision trees and N-gram lan-
guage models. Their N-gram language model is 
mainly based on part of speech, and retains some 
words which are particularly relevant to segmenta-
tion. Of course, most part-of-speech taggers re-
quire sentence boundaries to be pre-determined; so 
to require the use of part-of-speech information in 
utterance segmentation would risk circularity. Cet-
tolo et al?s (1998) approach to sentence boundary 
detection is somewhat similar to Stolcke et al?s. 
They applied word-based N-gram language models 
to utterance segmentation, and then combined 
them with prosodic models. Compared with N-
gram language models, their combined models 
achieved an improvement of 0.5% and 2.3% in 
precision and recall respectively. 
Beeferman et al (1998) used the CYBERPUNC 
system to add intra-sentence punctuation (espe-
cially commas) to the output of an automatic 
speech recognition (ASR) system. They claim that, 
since commas are the most frequently used punc-
tuation symbols, their correct insertion is by far the 
most helpful addition for making texts legible. 
CYBERPUNC augmented a standard trigram 
speech recognition model with lexical information 
concerning commas, and achieved a precision of 
75.6% and a recall of 65.6% when testing on 2,317 
sentences from the Wall Street Journal. 
Gotoh et al (1998) applied a simple non-speech 
interval model to detect sentence boundaries in 
English broadcast speech transcripts. They com-
pared their results with those of N-gram language 
models and found theirs far superior. However, 
broadcast speech transcripts are not really spoken 
language, but something more like spoken written 
language. Further, radio broadcasters speak for-
mally, so that their reading pauses match sentence 
boundaries quite well. It is thus understandable that 
the simple non-speech interval model outperforms 
the N-gram language model under these conditions; 
but segmentation of natural utterances is quite dif-
ferent. 
Zong et al (2003) proposed an approach to ut-
terance segmentation aiming at improving the per-
formance of spoken language translation (SLT) 
systems. Their method is based on rules which are 
oriented toward key word detection, template 
matching, and syntactic analysis. Since this ap-
proach is intended to facilitate translation of Chi-
nese-to-English SLT systems, it rewrites long 
sentences as several simple units. Once again, 
these results cannot be regarded as general-purpose 
utterance segmentation. Furuse et al (1998) simi-
larly propose an input-splitting method for translat-
ing spoken language which includes many long or 
ill-formed expressions. The method splits an input 
into well-balanced translation units, using a seman-
tic dictionary. 
Ramaswamy et al (1998) applied a maximum 
entropy approach to the detection of command 
boundaries in a conversational natural language 
user interface. They considered as their features 
words and their distances to potential boundaries. 
They posited 400 feature functions, and trained 
their weights using 3000 commands. The system 
then achieved a precision of 98.2% in a test set of 
1900 commands. However, command sentences 
for conversational natural language user interfaces 
contain much smaller vocabularies and simpler 
structures than the sentences of natural spoken lan-
guage. In any case, this method has been very 
helpful to us in designing our own approach to ut-
terance segmentation. 
There are several additional approaches which are 
not designed for utterance segmentation but which 
can nevertheless provide useful ideas. For example, 
Reynar et al (1997) proposed an approach to the 
disambiguation of punctuation marks. They con-
sidered only the first word to the left and right of 
any potential sentence boundary, and claimed that 
examining wider context was not beneficial. The 
features they considered included the candidate?s 
prefix and suffix; the presence of particular charac-
ters in the prefix or suffix; whether the candidate 
was honorific (e.g. Mr., Dr.); and whether the can-
didate was a corporate designator (e.g. Corp.). The 
system was tested on the Brown Corpus, and 
achieved a precision of 98.8%. Elsewhere, Nakano 
et al (1999) proposed a method for incrementally 
understanding user utterances whose semantic 
boundaries were unknown. The method operated 
by incrementally finding plausible sequences of 
utterances that play crucial roles in the task execu-
tion of dialogues, and by utilizing beam search to 
deal with the ambiguity of boundaries and with 
syntactic and semantic ambiguities. Though the 
method does not require utterance segmentation 
before discourse processing, it employs special 
rule tables for discontinuation of significant utter-
ance boundaries. Such rule tables are not easy to 
maintain, and experimental results have demon-
strated only that the method outperformed the 
method assuming pauses to be semantic boundaries.  
2.2 Our motivations 
Though numerous methods for utterance segmen-
tation have been proposed, many problems remain 
unsolved.  
One remaining problem relates to the language 
model. The N-gram model evaluates candidate 
sentence boundaries mainly according to their left 
context, and has achieved reasonably good results, 
but it can?t take into account the distant right con-
text to the candidate. This is the reason that N-
gram methods often wrongly divide some long 
sentences into halves or multiple segments. For 
example:????????. The N-gram method 
is likely to insert a boundary mark between ??? 
and ???, which corresponds to our everyday im-
pression that, if reading from the left and not 
considering several more words to the right of the 
current word, we will probably consider ????
?? as a whole sentence. However, we find that, if 
we search the sentence boundaries from right to 
left, such errors can be effectively avoided. In the 
present example, we won?t consider ?????? 
as a whole sentence, and the search will be contin-
ued until the word ??? is encountered. Accord-
ingly, in order to avoid segmentation errors made 
by the normal N-gram method, we propose a re-
verse N-gram segmentation method (RN) which 
does seek sentence boundaries from right to left. 
Further, we simply integrate the two N-gram 
methods and propose a bi-directional N-gram 
method (BN), which takes into account both the 
left and the right context of a candidate segmenta-
tion site. Since the relative usefulness or signifi-
cance of the two N-gram methods varies 
depending on the context, we propose a method of 
weighting them appropriately, using parameters 
generated by a maximum entropy method which 
takes as its features information about words in the 
context. This is our Maximum-Entropy-Weighted 
Bi-directional N-gram-based segmentation method. 
We hope MEBN can retain the correct segments 
discovered by the usual N-gram algorithm, yet ef-
fectively skip the wrong segments. 
3 Maximum-Entropy-Weighted Bi-
directional N-gram-based Segmentation 
Method 
3.1 Normal N-gram Algorithm (NN) for Ut-
terance Segmentation 
Assuming that mWWW ...21 (where m is a natural 
number) is a word sequence, we consider it as an n 
order Markov chain, in which the word 
)1( miWi ??  is predicted by the n-1 words to its 
left. Here is the corresponding formula: 
)...|()...|( 11121 ?+?? = iniiii WWWPWWWWP  
From this conditional probability formula for a 
word, we can derive the probability of a word se-
quence iWWW ...21 :  
)...|()...()...( 12112121 ?? ?= iiii WWWWPWWWPWWWP  
Integrating the two formulas above, we get: 
)...|()...()...( 1112121 ?+?? ?= iniiii WWWPWWWPWWWP  
Let us use SB to indicate a sentence boundary 
and add it to the word sequence. The value of 
)...( 121 +ii SBWWWWP  and )...( 121 +iiWWWWP will 
determine whether a specific word 
)1( miWi ?? is the final word of a sentence. We 
say iW  is the final word of a sentence if and only 
if )...( 121 +ii SBWWWWP > )...( 121 +iiWWWWP .  
Taking the trigram as our example and consid-
ering the two cases where Wi-1 is and is not the 
final word of a sentence, )...( 121 +ii SBWWWWP  
and )...( 121 +iiWWWWP  is computed respectively 
by the following two formulas: 
)|()...(
)|()...()...(
)|()|()...(
)|()|()...()...(
11121
121121
11121
121121
iiiii
iiiii
iiiiii
iiiiii
WWWPWWWWP
SBWWPSBWWWPWWWWP
SBWWPWWSBPWWWWP
SBWWPSBWSBPSBWWWPSBWWWWP
?+?
++
+??
++
?+
?=
??+
??=
 
In the normal N-gram method, the above iterative 
formulas are computed to search the sentence 
boundaries from 1W  to mW . 
3.2 Reverse N-gram Algorithm (RN) for Ut-
terance Segmentation 
In the reverse N-gram segmentation method, we 
take the word sequence mWWW ...21  as a reverse 
Markov chain in which )1( miWi ??  is predicted 
by the n-1 words to its right. That is: 
 )...|()...|( 1111 +?++? = iniiimmi WWWPWWWWP  
As in the N-gram algorithm, we compute the 
occurring probability of word sequence 
mWWW ...21  using the formula: 
)...|()...()...( 11111 +?+?? ?= immiimmimm WWWWPWWWPWWWP  
Then the iterative computation formula is: 
)...|()...()...( 11111 +?++?? ?= iniiimmimm WWWPWWWPWWWP  
By adding SB to the word sequence, we say iW  
is the final word of a sentence if and only if 
)...( 11 iimm SBWWWWP +? > )...( 11 iimm WWWWP +? . 
Similar to NN, )...( 11 iimm SBWWWWP +?  and 
)...( 11 iimm WWWWP +?  are computed as follows in 
the trigram: 
)|()...(
)|()...()...(
)|()|()...(
)|()|()...()...(
12121
11111
112121
111111
++++?
++?+?
+++++?
+++?+?
?+
?=
??+
??=
iiiiimm
iiimmiimm
iiiiiimm
iiiimmiimm
WWWPWWWWP
SBWWPSBWWWPWWWWP
SBWWPWWSBPWWWWP
SBWWPSBWSBPSBWWWPSBWWWWP
  
In contrast to the normal N-gram segmentation 
method, we compute the above iterative formulas 
to seek sentence boundaries from mW  to 1W . 
3.3 Bi-directional N-gram Algorithm for Ut-
terance Segmentation 
From the iterative formulas of the normal N-gram 
algorithm and the reverse N-gram algorithm, we 
can see that the normal N-gram method recognizes 
a candidate sentence boundary location mainly 
according to its left context, while the reverse N-
gram method mainly depends on its right context. 
Theoretically at least, it is reasonable to suppose 
that, if we synthetically consider both the left and 
the right context by integrating the NN and the RN, 
the overall segmentation accuracy will be im-
proved. 
Considering the word sequence mWWW ...21 , the 
candidate sites for sentence boundaries may be 
found between 1W  and 2W , between 2W  and 
3W , ?, or between 1?mW and mW . The number of 
candidate sites is thus m-1. We number those m-1 
candidate sites 1, 2 ? m-1 in succession, and we 
use )(iPis )11( ??? mi  and 
)(iPno )11( ??? mi  respectively to indicate the 
probability that the current site i really is, or is not, 
a sentence boundary. Thus, to compute the word 
sequence segmentation, we must compute )(iPis  
and )(iPno  for each of the m-1 candidate sites. In 
the bi-directional BN, we compute )(iPis  and 
)(iPno  by combining the NN results and RN re-
sults. The combination is described by the follow-
ing formulas: 
)()()(
)()()(
___
___
iPiPiP
iPiPiP
RNnoNNnoBNno
RNisNNisBNis
?=
?=
 
where )(_ iP NNis , )(_ iP NNno  denote the probabili-
ties calculated by NN which correspond to 
)...( 121 +ii SBWWWWP  and )...( 121 +iiWWWWP in 
section 3.1 respectively and )(_ iP RNis , )(_ iP RNno  
denote the probabilities calculated by RN which 
correspond to )...( 11 iimm SBWWWWP +?  and 
)...( 11 iimm WWWWP +?  in section 3.2 respectively. 
We say there exits a sentence boundary at site i 
)11( ??? mi if and only if )()( __ iPiP BNnoBNis > . 
3.4 Maximum Entropy Approach for Utter-
ance Segmentation 
In this section, we explain our maximum-entropy-
based model for utterance segmentation. That is, 
we estimate the joint probability distribution of the 
candidate sites and their surrounding words. Since 
we consider information concerning the lexical 
context to be useful, we define the feature func-
tions for our maximum method as follows: 
??
? ===
else
bScefixincludeif
cbf jj 0
)0&&)),((Pr(1
),(10
 
??
? ===
else
bScefixincludeif
cbf jj 0
)1&&)),((Pr(1
),(11
 
??
? ===
else
bScSuffixincludeif
cbf jj 0
)0&&)),(((1
),(20
 
??
? ===
else
bScSuffixincludeif
cbf jj 0
)1&&)),(((1
),(21
 
Sj denotes a sequence of one or more words 
which we can call the Matching String. (Note that 
Sj may contain the sentence boundary mark ?SB?.) 
The candidate c?s state is denoted by b, where b=1 
indicates that c is a sentence boundary and b=0 
indicates that it is not a boundary. Prefix(c) de-
notes all the word sequences ending with c (that is, 
c's left context plus c) and Suffix(c) denotes all the 
word sequences beginning with c (in other words, 
c plus its right context). For example: in the utter-
ance: ?<c1>?<c2>?<c3>?<c4>?<c5>?, 
???, ????,  and ????? are c3?s Prefix, while 
??? , ????and ????? are c3?s Suffix. The 
value of function )),((Pr jScefixinclude  is true 
when word sequence Sj is one of c?s Prefixes, and 
the value of function )),(( jScSuffixinclude  is 
true when Sj is one of c?s Suffixes. 
Corresponding to the four feature functions 
),(10 cbf j , ),(11 cbf j , ),(20 cbf j , ),(21 cbf j  are the 
four parameters 10j? , 11j? , 20j? , 21j? . Thus the 
joint probability distribution of the candidate sites 
and their surrounding contexts is given by: 
)(),(
1
),(
21
),(
20
),(
11
),(
10
21201110? = ???= kj cbfjcbfjcbfjcbfj jjjjbcP ?????
where k is the total number of the Matching Strings 
and ? is a parameter set to make P(c,1) and P(c,0) 
sum to 1. The unknown parameters 
10j? , 11j? , 20j? , 21j?  are chosen to maximize the 
likelihood of the training data using the General-
ized Iterative Scaling (Darroch and Ratcliff, 1972) 
algorithm. In the maximum entropy approach, we 
say that a candidate site is a sentence boundary if 
and only if P(c, 1) > P(c, 0). (At this point, we can 
anticipate a technical problem with the maximum 
approach to utterance segmentation. When a 
Matching String contains SB, we cannot know 
whether it belongs to the Prefixes or Suffixes of 
the candidate site until the left and right contexts of 
the candidate site have been segmented. Thus if the 
segmentation proceeds from left to right, the lexi-
cal information in the right context of the current 
candidate site will always remain uncertain. Like-
wise, if it proceeds from right to left, the informa-
tion in the left context of the current candidate site 
remains uncertain. The next subsection will de-
scribe a pragmatic solution to this problem.) 
3.5 Maximum-Entropy-Weighted Bi-
directional N-gram Algorithm for Utter-
ance Segmentation 
In the bi-directional N-gram based algorithm, we 
have considered the left-to-right N-gram algorithm 
and the right-to-left algorithm as having the same 
significance. Actually, however, they should be 
assigned differing weights, depending on the lexi-
cal contexts. The combination formulas are as fol-
lows: 
)()()()()(
)()()()()(
____
____
iPCWiPCWiP
iPCWiPCWiP
RNnoinorNNnoinonno
RNisiisrNNisiisnis
???=
???=  
)(_ iisn CW , )(_ inon CW , )(_ iisr CW , )(_ inor CW  
are the functions of the context surrounding candi-
date site i which denotes the weights of 
)(_ iP NNis , )(_ iP NNno , )(_ iP RNis  and )(_ iP RNno  re-
spectively. Assuming that the weights of )(_ iP NNis  
and )(_ iP NNno  depend upon the context to the left 
of the candidate site, and that the weights of 
)(_ iP RNis  and )(_ iP RNno  depend on the context to 
the right of the candidate site, the weight functions 
can be rewritten as: 
)(_ iisn LeftCW , )(_ inon LeftCW , )(_ iisr RightCW ,
)(_ inor RightCW . It is reasonable to assume that as 
the joint probability ),( SBiLeftCP i =  rises, 
)(_ iP NNis  will increase in significance. (The joint 
probability in question is the probability of the cur-
rent candidate?s left context, taken together with 
the probability that the candidate is a sentence 
boundary.) Therefore the value of )(_ iisn LeftCW  
is given by ),()(_ SBiLeftCPLeftCW iiisn == . 
Similarly we can give the formulas for comput-
ing )(_ inon LeftCW , )(_ iisr RightCW , and 
)(_ inor RightCW  as follows: 
)!,()(_ SBiLeftCPLeftCW iinon ==  
),()(_ SBiRightCPRightCW iiisr ==  
)!,()(_ SBiRightCPRightCW iinor ==  
We can easily get the values of 
),( SBiLeftCP i = , )!,( SBiLeftCP i = ,
),( SBiRightCP i = , and )!,( SBiRightCP i =  
using the method described in the maximum en-
tropy approach section. For example: 
? === kj ifji jSBiLeftCP 1 ),1(1111),( ??  
? === kj ifji jSBiLeftCP 1 ),0(1010)!,( ??  
As mentioned in last subsection, we need seg-
mented contexts for maximum entropy approach. 
Since the maximum entropy parameters for MEBN 
algorithm are used as modifying NN and RN, we 
just estimate the joint probability of the candidate 
and its surrounding contexts based upon the seg-
ments by NN and RN. Using NLeftCi indicate the 
left context to the candidate i which has been seg-
mented by NN algorithm and RRightCi indicate the 
right context to i which has been segmented by RN, 
the combination probability computing formulas 
for MEBN are as follows: 
)()!,(
)()!,()(
)(),(
)(),()(
_
__
_
__
iPSBiRRightCP
iPSBiNLeftCPiP
iPSBiRRightCP
iPSBiNLeftCPiP
RNnoi
NNnoiMEBNno
RNisi
NNisiMEBNis
?=?
?==
?=?
?==
 
We evaluate site i as a sentence boundary if and 
only if )()( __ iPiP MEBNnoMEBNis > . 
4 Experiment 
4.1 Model Training 
Our models are trained on both Chinese and Eng-
lish corpora, which cover the domains of hotel res-
ervation, flight booking, traffic information, 
sightseeing, daily life and so on. We replaced the 
full stops with ?SB? and removed all other punc-
tuation marks in the training corpora. Since in most 
actual systems part of speech information cannot 
be accessed before determining the sentence 
boundaries, we use Chinese characters and English 
words without POS tags as the units of our N-gram 
models. Trigram and reverse trigram probabilities 
are estimated based on the processed training cor-
pus by using Modified Kneser-Ney Smoothing 
(Chen and Goodman, 1998). As to the maximum 
entropy model, the Matching Strings are chosen as 
all the word sequences occurring in the training 
corpus whose length is no more than 3 words. The 
unknown parameters corresponding to the feature 
functions are generated based on the training cor-
pus using the Generalized Iterative Scaling algo-
rithm. Table 1 gives an overview of the training 
corpus. 
Corpus SIZE SB Num-
ber 
Average Length 
of Sentence 
Chinese 4.02MB 148967 8 Chinese charac-
ters 
English 4.49MB 149311 6 words 
Table 1. Overview of the Training Corpus. 
4.2 Testing Results 
We test our methods using open corpora which are 
also limited to the domains mentioned above. All 
punctuation marks are removed from the test cor-
pora. An overview of the test corpus appears in 
table 2. 
Corpus SIZE SB 
Number 
Average Length 
of Sentence 
Chinese 412KB 12032 10 Chinese char-
acters 
English 391KB 10518 7 words 
Table 2. Overview of the Testing Corpus. 
We have implemented four segmentation algo-
rithms using NN, RN, BN and MEBN respectively. 
If we use ?RightNum? to denote the number of 
right segmentations, ?WrongNum? denote the 
number of wrong segmentations, and ?TotalNum? 
to denote the number of segmentations in the 
original testing corpus, the precision (P) can be 
computed using the formula 
P=RightNum/(RightNum+WrongNum), the recall 
(R) is computed as R=RightNum/TotalNum, and 
the F-Score is computed as F-Score = 
RP
RP
+
??2
. 
The testing results are described in Table 3 and 
Table 4. 
Methods Total Num 
Right 
Num 
Wrong 
Num 
Preci-
sion  Recall F-Score 
NN 12032 10167 2638 79.4% 84.5% 81.9% 
RN 12032 10396 2615 79.9% 86.4% 83.0% 
BN 12032 10528 2249 82.4% 87.5% 84.9% 
MEBN 12032 10348 1587 86.7% 86.0% 86.3% 
Table 3. Experimental Results for Chinese Utter-
ance Segmentation. 
Methods Total Num 
Right 
Num 
Wrong
Num 
Preci-
sion  Recall F-Score 
NN 10518 8730 3164 73.4% 83.0% 77.9% 
RN 10518 9014 3351 72.9% 85.7% 78.8% 
BN 10518 9056 3019 75.0% 86.1% 80.2% 
MEBN 10518 8929 2403 78.8% 84.9% 81.7% 
Table 4. Experimental Results for English Utter-
ance Segmentation. 
From the result tables it is clear that RN, BN, and 
MEBN all outperforms the normal N-gram algo-
rithm in the F-score for both Chinese and English 
utterance segmentation. MEBN achieved the best 
performance which improves the precision by 
7.3% and the recall by 1.5% in the Chinese ex-
periment, and improves the precision by 5.4% and 
the recall by 1.9% in the English experiment. 
4.3 Result analysis 
MEBN was proposed in order to maintain the cor-
rect segments of the normal N-gram algorithm 
while skipping the wrong segments. In order to see 
whether our original intention has been realized, 
we compared the segments as determined by RN 
with those determined by NN, compare the seg-
ments found by BN with those of NN and then 
compare the segments found by MEBN with those 
of NN. For RN, BN and MEBN, suppose TN de-
notes the number of total segmentations, CON de-
notes the number of correct segmentations 
overlapping with those found by NN; SWN de-
notes the number of wrong NN segmentations 
which were skipped; WNON denotes the number 
of wrong segmentations not overlapping with those 
of NN; and CNON denotes the number of segmen-
tations which were correct but did not overlap with 
those of NN. The statistical results are listed in 
Table 5 and Table 6. 
Methods TN CON SWN WNON CNON
RN 13011 9525 1098 1077 870 
BN 12777 9906 753 355 622 
MEBN 11935 9646 1274 223 678 
Table 5. Chinese Utterance Segmentation Results 
Comparison. 
Methods TN CON SWN WNON CNON 
RN 12365 8223 1077 1271 792 
BN 12075 8565 640 488 491 
MEBN 11332 8370 1247 486 559 
Table 6. English Utterance Segmentation Results 
Comparison. 
Focusing upon the Chinese results, we can see that 
RN skips 1098 incorrect segments found by NN, 
and has 9525 correct segments in common with 
those of NN. It verifies our supposition that RN 
can effectively avoid some errors made by NN. 
But because at the same time RN brings in 1077 
new errors, RN doesn?t improve much in precision. 
BN skips 753 incorrect segments and brings in 355 
new segmentation errors; has 9906 correct seg-
ments in common with those of NN and brings in 
622 new correct segments. So by equally integrat-
ing NN and RN, BN on one hand finds more cor-
rect segments, on the other hand brings in less 
wrong segments than NN. But in skipping incor-
rect segments by NN, BN still performs worse than 
RN, showing that it only exerts the error skipping 
ability of RN to some extent. As for MEBN, it 
skips 1274 incorrect segments and at the same time 
brings in only 223 new incorrect segments. Addi-
tionally it maintains 9646 correct segments in com-
mon with those of NN and brings in 678 new 
correct segments. In recall MEBN performs a little 
worse than BN, but in precision it achieves a much 
better performance than BN, showing that modi-
fied by the maximum entropy weights, MEBN 
makes use of the error skipping ability of RN more 
effectively. Further, in skipping wrong segments 
by NN, MEBN even outperforms RN, which indi-
cates the weights we set on NN and RN not only 
act as modifying parameters, but also have direct 
beneficial affection on utterance segmentation. 
5 Conclusion 
This paper proposes a reverse N-gram algorithm, a 
bi-directional N-gram algorithm and a Maximum-
entropy-weighted Bi-directional N-gram algorithm 
for utterance segmentation. The experimental re-
sults for both Chinese and English utterance seg-
mentation show that MEBN significantly 
outperforms the usual N-gram algorithm. This is 
because MEBN takes into account both the left and 
right contexts of candidate sites: it integrates the 
left-to-right N-gram algorithm and the right-to-left 
N-gram algorithm with appropriate weights, using 
clues on the sites? lexical context, as modeled by 
maximum entropy. 
Acknowledgements 
This work is sponsored by the Natural Sciences 
Foundation of China under grant No.60175012, as 
well as supported by the National Key Fundamen-
tal Research Program (the 973 Program) of China 
under the grant G1998030504. 
The authors are very grateful to Dr. Mark Selig-
man for his very useful suggestions and his very 
careful proofreading.  
References 
Beeferman D., A. Berger, and J. Lafferty. 1998. 
CYBERPUNC: A lightweight punctuation annotation 
system for speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal 
Processing, Seattle, WA. pp. 689-692. 
Beeferman D., A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning 
34, pp 177-210.  
Berger A., S. Della Pietra, and V. Della Pietra. 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, 22(1), pp. 39-
71.  
Cettolo M. and D. Falavigna. 1998. Automatic 
Detection of Semantic Boundaries Based on Acoustic 
and Lexical Knowledge. ICSLP 1998, pp. 1551-1554. 
Chen S. F. and J. Goodman. 1998. An empirical study 
of smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Center for Research in Com-
puting Technology, Harvard University. pp.243-255. 
Darroch J. N. and D. Ratcliff. 1972. Generalized Itera-
tive Scaling for Log-Linear Models. The Annals of 
Mathematical Statistics, 43(5), pp. 1470-1480. 
Furuse O., S. Yamada, and K. Yamamoto. 1998. Split-
ting Long or Ill-formed Input for Robust Spoken-
language Translation. COLING-ACL 1998, pp. 421-
427. 
Gotoh Y. and S. Renals. 2000. Sentence Boundary De-
tection in Broadcast Speech Transcripts. In Proc. In-
ternational Workshop on Automatic Speech 
Recognition, pp. 228-235. 
Nakano M., N. Miyazaki, J. Hirasawa, K. Dohsaka, and 
T. Kawabata. 1999. Understanding Unsegmented User 
Utterances in Real-Time Spoken Dialogue Systems. 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-99), Col-
lege Park, MD, USA, pp. 200-207. 
Ramaswamy N. G. and J. Kleindienst. 1998. Automatic 
Identification of Command Boundaries in a Conversa-
tional Natural Language User Interface. ICSLP 1998. 
pp. 401-404. 
Reynar J. and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In 
Proceedings of the 5th Conference on Applications of 
Natural Language Processing (ANLP), Washington 
DC, pp. 16-19.  
Seligman M. 2000. Nine Issues in Speech Translation. 
In Machine Translation, 15, pp. 149-185. 
Stevenson M. and R. Gaizauskas. 2000. Experiments on 
sentence boundary detection. In Proceedings of the 
Sixth Conference on Applied Natural Language Proc-
essing and the First Conference of the North American 
Chapter of the Association for Computational Linguis-
tics, pp. 24-30. 
Stolcke A. and E. Shriberg. 1996. Automatic linguistic 
segmentation of conversational speech. Proc. Intl. 
Conf. on Spoken Language Processing, Philadelphia, 
PA, vol. 2, pp. 1005-1008.  
Stolcke A., E. Shriberg, R. Bates, M. Ostendorf, D. 
Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Auto-
matic Detection of Sentence Boundaries and Disfluen-
cies based on Recognized Words. Proc. Intl. Conf. on 
Spoken Language Processing, Sydney, Australia, vol. 
5, pp. 2247-2250.  
Zong, C. and F. Ren. 2003. Chinese Utterance Segmen-
tation in Spoken Language translation. In Proceedings 
of the 4th international conference on intelligent text 
processing and Computational Linguistics (CICLing), 
Mexico, Feb 16-22. pp. 516-525. 
Zhou Y. 2001. Utterance Segmentation Based on Deci-
sion Tree. Proceedings of the 6th National joint Con-
ference on Computational Linguistics, Taiyuan, China, 
pp. 246-252.  
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1308?1317,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Bayesian Learning of Phrasal Tree-to-String Templates
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
{dliu, gildea}@cs.rochester.edu
Abstract
We examine the problem of overcoming
noisy word-level alignments when learn-
ing tree-to-string translation rules. Our
approach introduces new rules, and re-
estimates rule probabilities using EM. The
major obstacles to this approach are the
very reasons that word-alignments are
used for rule extraction: the huge space
of possible rules, as well as controlling
overfitting. By carefully controlling which
portions of the original alignments are re-
analyzed, and by using Bayesian infer-
ence during re-analysis, we show signifi-
cant improvement over the baseline rules
extracted from word-level alignments.
1 Introduction
Non-parametric Bayesian methods have been suc-
cessfully applied to directly learn phrase pairs
from a bilingual corpus with little or no depen-
dence on word alignments (Blunsom et al, 2008;
DeNero et al, 2008). Because such approaches di-
rectly learn a generative model over phrase pairs,
they are theoretically preferable to the standard
heuristics for extracting the phrase pairs from the
many-to-one word-level alignments produced by
the IBM series models (Brown et al, 1993) or
the Hidden Markov Model (HMM) (Vogel et al,
1996). We wish to apply this direct, Bayesian ap-
proach to learn better translation rules for syntax-
based statistical MT (SSMT), by which we specif-
ically refer to MT systems using Tree-to-String
(TTS) translation templates derived from syntax
trees (Liu et al, 2006; Huang et al, 2006; Gal-
ley et al, 2006; May and Knight, 2007), as op-
posed to formally syntactic systems such as Hi-
ero (Chiang, 2007). The stumbling block pre-
venting us from taking this approach is the ex-
tremely large space of possible TTS templates
when no word alignments are given. Given a sen-
tence pair and syntax tree over one side, there
are an exponential number of potential TTS tem-
plates and a polynomial number of phrase pairs.
In this paper, we explore methods for restricting
the space of possible TTS templates under con-
sideration, while still allowing good templates to
emerge directly from the data as much as possible.
We find an improvement in translation accuracy
through, first, using constraints to limit the number
of new templates, second, using Bayesian methods
to limit which of these new templates are favored
when re-analyzing the training data with EM, and,
third, experimenting with different renormaliza-
tion techniques for the EM re-analysis.
We introduce two constraints to limit the num-
ber of TTS templates that we extract directly from
tree/string pairs without using word alignments.
The first constraint is to limit direct TTS tem-
plate extraction to the part of the corpus where
word alignment tools such as GIZA++ do poorly.
There is no reason not to re-use the good align-
ments from GIZA++, which holds a very compet-
itive baseline performance. As already mentioned,
the noisy alignments from GIZA++ are likely
to cross the boundaries of the tree constituents,
which leads to comparatively big TTS templates.
We use this fact as a heuristic to roughly distin-
guish noisy from good word alignments.
1
Here
we define big templates as those with more than
8 symbols in their right hand sides (RHSs). The
word alignments in big templates are considered
to be noisy and will be recomposed by extracting
smaller TTS templates. Another reason to do ex-
traction on big templates is that the applicability
of big templates to new sentences is very limited
due to their size, and the portion of the training
data from which they are extracted is effectively
wasted. The second constraint, after choosing the
1
Precisely differentiating the noisy/good word alignments
is as hard as correctly aligning the words.
1308
extraction site, is to extract the TTS templates all
the way down to the leaves of the hosting tem-
plates. This constraint limits the number of possi-
ble left hand sides (LHSs) to be equal to the num-
ber of tree nodes in the hosting templates. The
entire extraction process can be summarized in 3
steps:
1. Compute word alignments using GIZA++,
and generate the basic TTS templates.
2. Select big templates from the basic TTS tem-
plates in step 1, and extract smaller TTS tem-
plates all the way down to the bottom from
big templates, without considering the pre-
computed word alignments.
3. Combine TTS templates from step 1 and step
2 and estimate their probabilities using Vari-
ational Bayes with a Dirichlet Process prior.
In step 2, since there are no constraints from the
pre-computed word alignments, we have complete
freedom in generating all possible TTS templates
to overcome noisy word alignments. We use vari-
ational EM to approximate the inference of our
Bayesian model and explore different normaliza-
tion methods for the TTS templates. A two-stage
normalization is proposed by combining LHS-
based normalization with normalization based on
the root of the LHS, and is shown to be the best
model when used with variational EM.
Galley et al (2006) recompose the TTS tem-
plates by inserting unaligned target words and
combining small templates into bigger ones. The
recomposed templates are then re-estimated using
the EM algorithm described in Graehl and Knight
(2004). This approach also generates TTS tem-
plates beyond the precomputed word alignments,
but the freedom is only granted over unaligned tar-
get words, and most of the pre-computed word
alignments remain unchanged. Other prior ap-
proaches towards improving TTS templates fo-
cus on improving the word alignment performance
over the classic models such as IBM series mod-
els and Hidden Markov Model (HMM), which do
not consider the syntactic structure of the align-
ing languages and produce syntax-violating align-
ments. DeNero and Klein (2007) use a syntax-
based distance in an HMM word alignment model
to favor syntax-friendly alignments. Fossum et al
(2008) start from the GIZA++ alignment and in-
crementally delete bad links based on a discrim-
S
NP
VP
?
NN
?
AUX
issu
e
has
??
??
?
?
S
NP
VP
?
NN
?
AUX
issu
e
has
??
??
?
?
Figure 1: 5 small TTS templates are extracted based on the
correct word alignments (left), but only 1 big TTS template
(right) can be extracted when the cross-boundary noisy align-
ments are added in.
inative model with syntactic features. This ap-
proach can only find a better subset of the GIZA++
alignment and requires a parallel corpus with gold-
standard word alignment for training the discrim-
inative model. May and Knight (2007) factorize
the word alignment into a set of re-orderings rep-
resented by the TTS templates and build a hierar-
chical syntax-based word alignment model. The
problem is that the TTS templates are generated
by the word alignments from GIZA++, which lim-
its the potential of the syntactic re-alignment. As
shown by these prior approaches, directly improv-
ing the word alignment either falls into the frame-
work of many-to-one alignment, or is substantially
confined by the word alignment it builds upon.
The remainder of the paper focuses on the
Bayesian approach to learning TTS templates and
is organized as follows: Section 2 describes the
procedure for generating the candidate TTS tem-
plates; Section 3 describes the inference methods
used to learn the TTS templates; Section 4 gives
the empirical results, Section 5 discusses the char-
acteristics of the learned TTS templates, and Sec-
tion 6 presents the conclusion.
2 Extracting Phrasal TTS Templates
The Tree-to-String (TTS) template, the most im-
portant component of a SSMT system, usually
contains three parts: a fragment of a syntax tree
in its left hand side (LHS), a sequence of words
and variables in its right hand side (RHS), and
a probability indicating how likely the template
is to be used in translation. The RHS of a TTS
template shows one possible translation and re-
ordering of its LHS. The variables in a TTS tem-
plate are further transformed using other TTS tem-
plates, and the recursive process continues until
there are no variables left. There are two ways
1309
SNP VP
NP1 PP ADJAUX
in isNP3
of
NP3 ? NP1 ??
beautiful
2
1
3 4
Figure 2: Examples of valid and invalid templates extracted
from a big Template. Template 1, invalid, doesn?t go all the
way down to the bottom. Template 2 is valid. Template 3, in-
valid, doesn?t have the same set of variables in its LHS/RHS.
Template 4, invalid, is not a phrasal TTS template.
that TTS templates are commonly used in ma-
chine translation. The first is synchronous pars-
ing (Galley et al, 2006; May and Knight, 2007),
where TTS templates are used to construct syn-
chronous parse trees for an input sentence, and
the translations will be generated once the syn-
chronous trees are built up. The other way is
the TTS transducer (Liu et al, 2006; Huang et
al., 2006), where TTS templates are used just as
their name indicates: to transform a source parse
tree (or forest) into the proper target string. Since
synchronous parsing considers all possible syn-
chronous parse trees of the source sentence, it is
less constrained than TTS transducers and hence
requires more computational power. In this paper,
we use a TTS transducer to test the performance of
different TTS templates, but our techniques could
also be applied to SSMT systems based on syn-
chronous parsing.
2.1 Baseline Approach: TTS Templates
Obeying Word Alignment
TTS templates are commonly generated by de-
composing a pair of aligned source syntax tree
and target string into smaller pairs of tree frag-
ments and target string (i.e., the TTS templates).
To keep the number of TTS templates to a manage-
able scale, only the non-decomposable TTS tem-
plates are generated. This algorithm is referred to
as GHKM (Galley et al, 2004) and is widely used
in SSMT systems (Galley et al, 2006; Liu et al,
2006; Huang et al, 2006). The word alignment
used in GHKM is usually computed independent
of the syntactic structure, and as DeNero and Klein
(2007) and May and Knight (2007) have noted,
Ch-En En-Ch Union Heuristic
28.6% 33.0% 45.9% 20.1%
Table 1: Percentage of corpus used to generate big templates,
based on different word alignments
9-12 13-20 ?21
Ch-En 18.2% 17.4% 64.4%
En-Ch 15.9% 20.7% 63.4%
Union 9.8% 15.1% 75.1%
Heuristic 24.6% 27.9% 47.5%
Table 2: In the selected big templates, the distribution of
words in the templates of different sizes, which are measured
based on the number of symbols in their RHSs
is not the best for SSMT systems. In fact, noisy
word alignments cause more damage to a SSMT
system than to a phrase based SMT system, be-
cause the TTS templates can only be derived from
tree constituents. If some noisy alignments happen
to cross over the boundaries of two constituents,
as shown in Figure 2, a much bigger tree frag-
ment will be extracted as a TTS template. Even
though the big TTS templates still carry the orig-
inal alignment information, they have much less
chance of getting matched beyond the syntax tree
where they were extracted, as we show in Sec-
tion 4. In other words, a few cross-boundary noisy
alignments could disable a big portion of a training
syntax tree, while for a phrase-based SMT system,
their effect is limited to the phrases they align. As
a rough measure of how the training corpus is af-
fected by the big templates, we calculated the dis-
tribution of target words in big and non-big TTS
templates. The word alignment is computed using
GIZA++
2
for the selected 73,597 sentence pairs in
the FBIS corpus in both directions and then com-
bined using union and heuristic diagonal growing
(Koehn et al, 2003). Table 1 shows that big
templates consume 20.1% to 45.9% of the training
corpus depending on different types of word align-
ments. The statistics indicate that a significant
portion of the training corpus is simply wasted,
if the TTS templates are extracted based on word
alignments from GIZA++. On the other hand, it
shows the potential for improving an SSMT sys-
tem if we can efficiently re-use the wasted train-
ing corpus. By further examining the selected big
templates, we find that the most common form of
big templates is a big skeleton template starting
2
GIZA++ is available at
http://www.fjoch.com/GIZA++.html
1310
from the root of the source syntax tree, and hav-
ing many terminals (words) misaligned in the bot-
tom. Table 2 shows, in the selected big templates,
the distribution of words in the templates of differ-
ent sizes (measured based on the number of sym-
bols in their RHS). We can see that based on ei-
ther type of word alignment, the most common
big templates are the TTS templates with more
than 20 symbols in their RHSs, which are gen-
erally the big skeleton templates. The advantage
of such big skeleton templates is that they usually
have good marginal accuracy
3
and allow accurate
smaller TTS templates to emerge.
2.2 Liberating Phrasal TTS Templates From
Noisy Word Alignments
To generate better TTS templates, we use a more
direct way than modifying the underlying word
alignment: extract smaller phrasal TTS tem-
plates from the big templates without looking at
their pre-computed word alignments. We define
phrasal TTS templates as those with more than
one symbol (word or non-terminal) in their LHS.
The reason to consider only phrasal TTS tem-
plates is that they are more robust than the word-
level TTS templates in addressing the complicated
word alignments involved in big templates, which
are usually not the simple type of one-to-many or
many-to-one. Abandoning the pre-computed word
alignments in big templates, an extracted smaller
TTS template can have many possible RHSs, as
long as the two sides have the same set of vari-
ables. Note that the freedom is only given to the
alignments of the words; for the variables in the
big templates, we respect the pre-computed word
alignments. To keep the extracted smaller TTS
templates to a manageable scale, the following two
constraints are applied:
1. The LHS of extracted TTS templates should
go all the way down to the bottom of the LHS
of the big templates. This constraint ensures
that at most N LHSs can be extracted from
one big Template, where N is the number of
tree nodes in the big Template?s LHS.
2. The number of leaves (including both words
and variables) in an extracted TTS template?s
LHS should not exceed 6. This constraint
limits the size of the extracted TTS templates.
3
Here, marginal accuracy means the correctness of the
TTS template?s RHS corresponding to its LHS.
( VP ( AUX is ) ( ADJ  beautiful ) )         ??( PP ( IN of )  NP3 )          NP3  ?( NP NP1 ( PP ( IN of )  NP3 ) )          NP3  ? NP1( NP NP1 ( PP ( IN of )  NP3 ) )          NP3  ? NP1  ??
Figure 3: All valid templates that can be extracted from the
example in Figure 2.1
for all template t do
if size(t.rhs) > 8 then
for all tree node s in t.lhs do
subt = subtree(s, t.lhs);
if leaf num(subt) ? 6 then
for i=1:size(t.rhs) do
for j=i:size(t.rhs) do
if valid(subt, i, j) then
create template(subt, i, j);
Figure 4: Algorithm that liberates smaller TTS Templates
from big templates
As we show in Section 4, use of bigger TTS
templates brings very limited performance
gain.
Figure 2.2 describes the template liberating algo-
rithm running in O(NM
2
), where N denotes the
number of tree nodes in the LHS of the input big
Template andM denotes the length of the RHS. In
the algorithm, function valid returns true if there
are the same set of variables in the left/right hand
side of an extracted TTS template; subtree(x, y)
denotes the sub-tree in y which is rooted at x and
goes all the way down to y?s bottom. Figure 2.1
shows valid and invalid TTS templates which can
be extracted from an example hosting TTS tem-
plate. Note that, in order to keep the example
simple, the hosting TTS template only has 4 sym-
bols in its RHS, which does not qualify as a big
template according to our definition. Figure 2.2
shows the complete set of valid TTS templates
which can be extracted from the example TTS
template. The subscripts of the non-terminals are
used to differentiate identical non-terminals in dif-
ferent positions. The extraction process blindly
releases smaller TTS templates from the big tem-
plates, among which only a small fraction are cor-
rect TTS templates. Therefore, we need an infer-
ence method to raise the weight of the correct tem-
plates and decrease the weight of the noisy tem-
plates.
1311
3 Estimating TTS Template Probability
The Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) can be used to estimate
the TTS templates? probabilities, given a genera-
tive model addressing how a pair of source syn-
tax tree and target string is generated. There are
two commonly used generative models for syntax-
based MT systems, each of which corresponds to
a normalization method for the TTS templates.
The LHS-based normalization (LHSN) (Liu et al,
2006; Huang et al, 2006), corresponds to the
generative process where the source syntax sub-
tree is first generated, and then the target string
is generated given the source syntax subtree. The
other one is normalization based on the root of
the LHS (ROOTN) (Galley et al, 2006), corre-
sponding to the generative process where, given
the root of the syntax subtree, the LHS syntax sub-
tree and the RHS string are generated simultane-
ously. By omitting the decomposition probability
in the LHS-based generative model, the two gen-
erative models share the same formula for comput-
ing the probability of a training instance:
Pr(T, S) =
?
R
Pr(T, S,R) =
?
R
(
?
t?R
Pr(t)
)
where T and S denote the source syntax tree and
target string respectively, R denotes the decompo-
sition of (T, S), and t denotes the TTS template.
The expected counts of the TTS templates can then
be efficiently computed using an inside-outside-
like dynamic programming algorithm (May and
Knight, 2007).
LHSN, as shown by Galley et al (2006), cannot
accurately restore the true conditional probabili-
ties of the target sentences given the source sen-
tences in the training corpus. This indicates that
LHSN is not good at predicting unseen sentences
or at translating new sentences. But this deficiency
does not affect its ability to estimate the expected
counts of the TTS templates, because the posteri-
ors of the TTS templates only depend on the com-
parative probabilities of the different derivations
of a training instance (a pair of tree and string).
In fact, as we show in Section 4, LHSN is bet-
ter than ROOTN in liberating smaller TTS tem-
plates out of the big templates, since it is less bi-
ased to the big templates in the EM training.
4
Be-
cause the two normalization methods have their
4
Based on LHSN, the difference between the probabil-
ity of a big Template and the product of the probabilities of
E-step:
for all pair of syntax tree T and target string S do
for all TTS Template t do
EC(t)+ =
P
R:t?R
Pr(T,S,R)
?
P
R
?
Pr(T,S,R
?
)
?
;
Increase ?;
M-step:
for all TTS Template t do
if it is the last iteration then
Pr(t) =
EC(t)
P
t
?
:t
?
.root=t.root
EC(t
?
)
;
else
Pr(t) =
EC(t)
P
t
?
:t
?
.lhs=t.lhs
EC(t
?
)
;
Figure 5: EM Algorithm For Estimating TTS Templates
own strength and weakness, both of them are used
in our EM algorithm: LHSN is used in all EM
iterations except the last one to compute the ex-
pected counts of the TTS templates, and ROOTN
is used in the last EM iteration to compute the final
probabilities of the TTS templates. This two-stage
normalization method is denoted as MIXN in this
paper.
Deterministic Annealing (Rose et al, 1992) is
is used in our system to speed up the training
process, similar to Goldwater et al (2006). We
start from a high temperature and gradually de-
crease the temperature to 1; we find that the ini-
tial high temperature can also help small templates
to survive the initial iterations. The complete EM
framework is sketched in Figure 3, where ? is the
inverse of the specified temperature, and EC de-
notes the expected count.
3.1 Bayesian Inference with the Dirichlet
Process Prior
Bayesian inference plus the Dirichlet Process (DP)
have been shown to effectively prevent MT mod-
els from overfitting the training data (DeNero et
al., 2008; Blunsom et al, 2008). A similar ap-
proach can be applied here for SSMT by consider-
ing each TTS template as a cluster, and using DP
to adjust the number of TTS templates according
to the training data. Note that even though there
is a size limitation on the liberated phrasal TTS
templates, standard EM will still tend to overfit
the training data by pushing up the probabilities of
the big templates from the noisy word alignments.
The complete generative process, integrating the
DP prior and the generative models described in
its decomposing TTS templates is much less than the one
based on ROOTN, thus LHSN gives comparably more ex-
pected counts to the smaller TTS templates than ROOTN.
1312
for all TTS Template t do
if it is the last iteration then
Pr(t) =
exp(?(EC(t)+?G
0
(t)))
exp
(
?
((
P
t
?
:t
?
.root=t.root
EC(t
?
)
)
+?
))
;
else
Pr(t) =
exp(?(EC(t)+?G
0
(t)))
exp
(
?
((
P
t
?
:t
?
.lhs=t.lhs
EC(t
?
)
)
+?
))
;
Figure 6: M-step of the Variational EM
Section 3.1, is given below:
?
r
| {?
r
, G
r
0
} ? DP (?
r
, G
r
0
)
t | ?
t.root
? ?
t.root
(T, S) | {SG, {t}, ?} ? SG({t}, ?)
where G
0
is a base distribution of the TTS tem-
plates, t denotes a TTS template, ?
t.root
denotes
the multinomial distribution over TTS templates
with the same root as t, SG denotes the generative
model for a pair of tree and string in Section 3.1,
and ? is a free parameter which adjusts the rate at
which new TTS templates are generated.
It is intractable to do exact inference under the
Bayesian framework, even with a conjugate prior
such as DP. Two methods are commonly used
for approximate inference: Markov chain Monte
Carlo (MCMC) (DeNero et al, 2008), and Vari-
ational Bayesian (VB) inference (Blunsom et al,
2008). In this paper, the latter approach is used be-
cause it requires less running time. The E-step of
VB is exactly the same as standard EM, and in the
M-step the digamma function ? and the base dis-
tributionG
0
are used to increase the uncertainty of
the model. Similar to standard EM, both LHS- and
root-based normalizations are used in the M-step,
as shown in Figure 3.1. For the TTS templates,
which are also pairs of subtrees and strings, a natu-
ral choice ofG
0
is the generative models described
in Section 3.1. BecauseG
0
estimates the probabil-
ity of the new TTS templates, the root-based gen-
erative model is superior to the LHS-based gener-
ative model and used in our approach.
3.2 Initialization
Since the EM algorithm only converges to a lo-
cal minimum, proper initializations are needed to
achieve good performance for both standard EM
and variational EM. For the baseline templates
derived from word alignments, the initial counts
are set to the raw counts in the training corpus.
For the templates blindly extracted from big tem-
plates, the raw count of a LHS tree fragment is
distributed among their RHSs based on the like-
lihood of the template, computed by combining
for all big template t do
for all template g extracted from t do
g.count = g.lhs.count = 0;
for all template g extracted from t do
g.count += w in(g)?w out(g, t);
g.lhs.count += w in(g)?w out(g, t);
for all template g extracted from t do
g.init +=
g.count
g.lhs.count
;
Figure 7: Compute the initial counts of the liberated TTS
templates
the word-based inside/outside scores. The algo-
rithm is sketched in Figure 3.2, where the inside
score w in(g) is the product of the IBM Model 1
scores in both directions, computed based on the
words in g?s LHS and RHS. The outside score
w out(g, t) is computed similarly, except that the
IBM Model 1 scores are computed based on the
words in the hosting template t?s LHS/RHS ex-
cluding the words in g?s LHS/RHS. The initial
probabilities of the TTS templates are then com-
puted by normalizing their initial counts using
LHSN or ROOTN.
4 Experiments
We train an English-to-Chinese translation sys-
tem using the FBIS corpus, where 73,597 sentence
pairs are selected as the training data, and 500 sen-
tence pairs with no more than 25 words on the Chi-
nese side are selected for both the development
and test data.
5
Charniak (2000)?s parser, trained
on the Penn Treebank, is used to generate the En-
glish syntax trees. Modified Kneser-Ney trigram
models are trained using SRILM (Stolcke, 2002)
upon the Chinese portion of the training data. The
trigram language model, as well as the TTS tem-
plates generated based on different methods, are
used in the TTS transducer. The model weights
of the transducer are tuned based on the develop-
ment set using a grid-based line search, and the
translation results are evaluated based on a single
Chinese reference
6
using BLEU-4 (Papineni et al,
2002). Huang et al (2006) used character-based
BLEU as a way of normalizing inconsistent Chi-
nese word segmentation, but we avoid this prob-
lem as the training, development, and test data are
from the same source.
5
The total 74,597 sentence pairs used in experiments are
those in the FBIS corpus whose English part can be parsed
using Charniak (2000)?s parser.
6
BLEU-4 scores based on a single reference are much
lower than the ones based on multiple references.
1313
E2C C2E Union Heuristic
w/ Big 13.37 12.66 14.55 14.28
w/o Big 13.20 12.62 14.53 14.21
Table 3: BLEU-4 scores (test set) of systems based on
GIZA++ word alignments
? 5 ? 6 ? 7 ? 8 ? ?
BLEU-4 14.27 14.42 14.43 14.45 14.55
Table 4: BLEU-4 scores (test set) of the union alignment, us-
ing TTS templates up to a certain size, in terms of the number
of leaves in their LHSs
4.1 Baseline Systems
GHKM (Galley et al, 2004) is used to generate
the baseline TTS templates based on the word
alignments computed using GIZA++ and different
combination methods, including union and the di-
agonal growing heuristic (Koehn et al, 2003). We
also tried combining alignments from GIZA++
based on intersection, but it is worse than both
single-direction alignments, due to its low cover-
age of training corpus and the incomplete transla-
tions it generates. The baseline translation results
based on ROOTN are shown in Table 4.1. The first
two columns in the table show the results of the
two single direction alignments. e2c and c2e de-
note the many English words to one Chinese word
alignment and the many Chinese words to one En-
glish word alignment, respectively. The two rows
show the results with and without the big tem-
plates, from which we can see that removing the
big templates does not affect performance much;
this verifies our postulate that the big templates
have very little chance of being used in the trans-
lation. Table 4.1, using the union alignments as
the representative and measuring a template?s size
by the number of leaves in its LHS, also demon-
strates that using big TTS templates brings very
limited performance gain.
The result that the union-based combination
outperforms either single direction alignments and
even the heuristic-based combination, combined
with the statistics of the disabled corpus in Sec-
tion 2.2, shows that more disabled training cor-
pus actually leads to better performance. This can
be explained by the fact that the union alignments
have the largest number of noisy alignments gath-
ered together in the big templates, and thus have
the least amount of noisy alignments which lead
to small and low-quality TTS templates.
 17
 17.5
 18
 18.5
 19
 19.5
 20
 20.5
 21
 1  2  3  4  5  6  7  8  9  10
1.01.01.01.00.90.80.70.50.30.1
iteration
temperature parameter ?
MIXN-EM
LHSN-VB
LHSN-EM
ROOTN-EM
ROOTN-VB
MIXN-VB
Figure 8: BLEU-4 scores (development set) of annealing EM
and annealing VB in each iteration.
4.2 Learning Phrasal TTS Templates
To test our learning methods, we start with the
TTS templates generated based on e2c, c2e, and
union alignments using GHKM. This gives us
0.98M baseline templates. We use the big tem-
plates from the union alignments as the basis
and extract 10.92M new phrasal TTS templates,
which, for convenience, are denoted by NEW-
PHR. Because based on Table 1 and Table 2
the union alignment has the greatest number of
alignment links and therefore produces the largest
rules, this gives us the greatest flexibility in re-
aligning the input sentences. The baseline TTS
templates as well as NEW-PHR are initialized us-
ing the method in Section 3.3 for both annealing
EM and annealing VB. To simplify the experi-
ments, the same Dirichlet Process prior is used for
all multinomial distributions of the TTS templates
with different roots. G
0
in the Dirichlet prior is
computed based on the 1-level TTS templates se-
lected from the baseline TTS templates, so that the
big templates are efficiently penalized. The train-
ing algorithms follow the same annealing sched-
ule, where the temperature parameter ? is initial-
ized to 0.1, and gradually increased to 1.
We experiment with the two training algo-
rithms, annealing EM and annealing VB, with dif-
ferent normalization methods. The experimental
results based on the development data are shown
in Figure 4.2, where the free parameter ? of an-
nealing VB is set to 1, 100, and 100 respec-
tively for ROOTN, LHSN, and MIXN. The re-
sults verify that LHSN is worse than ROOTN in
predicting the translations, since MIXN outper-
forms LHSN with both annealing EM and VB.
ROOTN is on par with MIXN and much better
1314
Max Likelihood Annealing EM Annealing VB
w/o new-phr with new-phr w/o new-phr with new-phr w/o new-phr with new-phr
LHSN 14.05 13.16 14.31 15.33 14.82 16.15
ROOTN 14.50 13.49 14.90 16.06 14.76 16.12
MIXN NA NA 14.82 16.37 14.93 16.84
Table 5: BLEU-4 scores (test set) of different systems.
Initial Template Final Template
number new-phr% number new-phr%
ROOTN 11.9M 91.8% 408.0K 21.9%
LHSN 11.9M 91.8% 557.2K 29.8%
MIXN 11.9M 91.8% 500.5K 27.6%
Table 6: The total number of templates and the percentage of
NEW-PHR, in the beginning and end of annealing VB
than LHSN when annealing EM is used; but with
annealing VB, it is outperformed by MIXN by
a large margin and is even slightly worse than
LHSN. This indicates that ROOTN is not giv-
ing large expected counts to NEW-PHR and leaves
very little space for VB to further improve the re-
sults. For all the normalization methods, anneal-
ing VB outperforms annealing EM and maintains
a longer ascending path, showing better control of
overfitting for the Bayesian models. Figure 4.2
shows the optimized results of the development
set based on annealing VB with different ?. The
best performance is achieved as ? approaches 1,
100, and 100 for ROOTN, LHSN and MIXN re-
spectively. The ? parameter can be viewed as a
weight used to balance the expected counts and
the probabilities from G
0
. Thus it is reasonable
for LHSN and MIXN to have bigger optimal ?
than ROOTN, since ROOTN gives lower expected
counts to NEW-PHR than LHSN and MIXN do.
To see the contribution of the phrasal template
extraction in the performance gain, MT experi-
ments are conducted by turning this component
on and off. Results on the test set, obtained by
using parameters optimized on the development
set, are shown in Table 4.2. The template counts
used in the Max-Likelihood training are the same
as the ones used in the initialization of anneal-
ing EM and VB. Results show that for annealing
EM and VB, use of NEW-PHR greatly improves
performance, while for the Max-Likelihood train-
ing, use of NEW-PHR hurts performance. This
is not surprising, because Max-Likelihood train-
ing cannot efficiently filter out the noisy phrasal
templates introduced in the initial NEW-PHR. An-
other observation is that annealing VB does not al-
ways outperform annealing EM. With NEW-PHR
 19.8
 20
 20.2
 20.4
 20.6
 20.8
 21
 0.1  1  10  100  1000
?
MIXN
ROOTN
LHSN
Figure 9: BLEU-4 scores (development set) of annealing VB
with different ?.
turned on, annealing VB shows consistent supe-
riority over annealing EM; while without NEW-
PHR, it only outperforms annealing EM based on
LHSN and MIXN, and the improvement is not as
big as when NEW-PHR is turned on. This indi-
cates that without NEW-PHR, there is less need
to use VB to shrink down the size of the tem-
plate set. Table 4.2 shows the statistics of the ini-
tial template set including NEW-PHR and the final
TTS template set after annealing VB is conducted,
where we can see annealing VB efficiently re-
duces NEW-PHR to a relatively small size and re-
sults in much more compact systems than the sys-
tem based on the baseline templates from GIZA++
alignments. Comparing with the best GIZA++-
based system union, our best system, utilizing
NEW-PHR and the two-stage template normaliza-
tion, demonstrates the strength of annealing VB
by an absolute improvement of 2.29% in BLEU-
4 score, from 14.55 to 16.84. This improvement
is significant at p < 0.005 based on 2000 itera-
tions of paired bootstrap re-sampling of the test
set (Koehn, 2004).
5 Discussion
Our experimental results are obtained based on
a relatively small training corpus, the improved
performance may be questionable when a larger
training corpus is used. Someone may wonder if
the performance gain primarily comes from the
1315
Many-to-one Alignment
( VP ( VB make ) ( NP ( DT a ) ( JJ complete ) ( NN statement ) ) ) ????
( S ( VP VBG ( NP ( DT the ) ( NN mass ) ( NN line ) ) PP ) ) PP VBG ????
( PP ( TO to ) ( NP ( DT the ) ( JJS greatest ) ( NN extent ) ) ) ???? ?
( PP ( IN of ) ( NP ( JJ peaceful ) ( NNP coexistence ) ) ) ????
Many-to-many Alignment
( VP ( VBN based ) ( PP ( IN on ) ( NP ( JJ actual ) ( NNS needs ) ) ) ) ? ?? ??
( PP ( IN into ) ( NP ( NP ( DT the ) ( NNS hands ) ) PP ) ) ?? ? PP ??
( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ?
( SBAR ( S ( NP ( DT the ) ( VBG aging ) NN ) ( VP ( aux is ) NP ) ) ) NN ? ? ? ? NP
( NP NP1 PP ( , , ) ( VP ( VBN centered ) ( PP ( IN around ) NP2 ) ) ) ? NP2 ? ?? ? NP1 PP
Allowance of Bad Word Segmentation
( NP ( NP ( NNP japan ) ( POS 's ) ) ( NNP sdf ) ( NNP navy ) ) ??? ? ???
( NP ( PDT all ) ( NP ( NNS people ) ( POS 's ) ) ( NNS organizations ) ) ?? ?? ?
Figure 10: Examples of the learned TTS templates
reduced out of vocabulary (OOV) ratio. We ex-
amined the OOV ratio of the test set with/without
the learned TTS templates, and found the differ-
ence was very small. In fact, our method is de-
signed to learn the phrasal TTS templates, and ex-
plictly avoids lexical pairs. To further understand
the characteristics of the learned TTS templates,
we list some representative templates in Figure 4.2
classified in 3 groups. The group Many-to-one
Alignment and Many-to-many Alignment show the
TTS templates based on complicated word align-
ments, which are difficult to compute based on the
existing word alignment models. These templates
do not have rare English words, whose translation
cannot be found outside the big templates. The
difficulty lies in the non-literal translation of the
source words, which are unlikely to learnt by soly
increasing the size of the training corpus. One
other interesting observation is that our learning
method is tolerant to noisy Chinese word segmen-
tation, as shown in group Allowance of Bad Word
Segmentation.
6 Conclusion
This paper proposes a Bayesian model for extract-
ing the Tree-to-String templates directly from the
data. By limiting the extraction to the big tem-
plates from the pre-computed word alignments
and applying a set of constraints, we restrict the
space of possible TTS templates under consider-
ation, while still allowing new and more accurate
templates to emerge from the training data. The
empirical results demonstrate the strength of our
approach, which outperforms the GIZA++-based
systems by a large margin. This encourages a
move from word-alignment-based systems to sys-
tems based on consistent, end-to-end probabilistic
modeling. Because our Bayesian model employs
a very simple prior, more sophisticated generative
models provide a possible direction for further ex-
perimentation.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Neu-
ral Information Processing Systems (NIPS).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-01, pages
132?139.
1316
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?21.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL-07, pages 17?24.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In EMNLP08.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improveword alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, Columbus, Ohio. ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-04, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL-06, pages 961?968, July.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the Human Language Technology Conference/North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL-06,
Sydney, Australia, July.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of
EMNLP.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL-02.
K. Rose, E. Gurewitz, and G. C. Fox. 1992. Vec-
tor quantization by deterministic annealing. IEEE
Transactions on Information Theory, 38(4):1249?
1257, July.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on
Spoken Language Processing, volume 2, pages 901?
904.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In COLING-96, pages 836?841.
1317
Proceedings of NAACL HLT 2007, pages 41?48,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Source-Language Features and Maximum Correlation Training
for Machine Translation Evaluation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose three new features for MT
evaluation: source-sentence constrained
n-gram precision, source-sentence re-
ordering metrics, and discriminative un-
igram precision, as well as a method of
learning linear feature weights to directly
maximize correlation with human judg-
ments. By aligning both the hypothe-
sis and the reference with the source-
language sentence, we achieve better cor-
relation with human judgments than pre-
viously proposed metrics. We further
improve performance by combining indi-
vidual evaluation metrics using maximum
correlation training, which is shown to be
better than the classification-based frame-
work.
1 Introduction
Evaluation has long been a stumbling block in the
development of machine translation systems, due to
the simple fact that there are many correct trans-
lations for a given sentence. The most commonly
used metric, BLEU, correlates well over large test
sets with human judgments (Papineni et al, 2002),
but does not perform as well on sentence-level eval-
uation (Blatz et al, 2003). Later approaches to im-
prove sentence-level evaluation performance can be
summarized as falling into four types:
? Metrics based on common loose sequences of
MT outputs and references (Lin and Och, 2004;
Liu and Gildea, 2006). Such metrics were
shown to have better fluency evaluation per-
formance than metrics based on n-grams such
BLEU and NIST (Doddington, 2002).
? Metrics based on syntactic similarities such as
the head-word chain metric (HWCM) (Liu and
Gildea, 2005). Such metrics try to improve flu-
ency evaluation performance for MT, but they
heavily depend on automatic parsers, which are
designed for well-formed sentences and cannot
generate robust parse trees for MT outputs.
? Metrics based on word alignment between MT
outputs and the references (Banerjee and Lavie,
2005). Such metrics do well in adequacy evalu-
ation, but are not as good in fluency evaluation,
because of their unigram basis (Liu and Gildea,
2006).
? Combination of metrics based on machine
learning. Kulesza and Shieber (2004) used
SVMs to combine several metrics. Their
method is based on the assumption that
higher classification accuracy in discriminat-
ing human- from machine-generated transla-
tions will yield closer correlation with human
judgment. This assumption may not always
hold, particularly when classification is diffi-
cult. Lita et al (2005) proposed a log-linear
model to combine features, but they only did
preliminary experiments based on 2 features.
Following the track of previous work, to improve
evaluation performance, one could either propose
new metrics, or find more effective ways to combine
the metrics. We explore both approaches. Much
work has been done on computing MT scores based
41
on the pair of MT output/reference, and we aim to
investigate whether some other information could
be used in the MT evaluation, such as source sen-
tences. We propose two types of source-sentence
related features as well as a feature based on part of
speech. The three new types of feature can be sum-
marized as follows:
? Source-sentence constrained n-gram precision.
Overlapping n-grams between an MT hypothe-
sis and its references do not necessarily indicate
correct translation segments, since they could
correspond to different parts of the source sen-
tence. Thus our constrained n-gram precision
counts only overlapping n-grams in MT hy-
pothesis and reference which are aligned to the
same words in the source sentences.
? Source-sentence reordering agreement. With
the alignment information, we can compare the
reorderings of the source sentence in the MT
hypothesis and in its references. Such compar-
ison only considers the aligned positions of the
source words in MT hypothesis and references,
and thus is oriented towards evaluating the sen-
tence structure.
? Discriminative unigram precision. We divide
the normal n-gram precision into many sub-
precisions according to their part of speech
(POS). The division gives us flexibility to train
the weights of each sub-precision in frame-
works such as SVM and Maximum Correla-
tion Training, which will be introduced later.
The motivation behind such differentiation is
that different sub-precisions should have dif-
ferent importance in MT evaluation, e.g., sub-
precision of nouns, verbs, and adjectives should
be important for evaluating adequacy, and
sub-precision in determiners and conjunctions
should mean more in evaluating fluency.
Along the direction of feature combination, since
indirect weight training using SVMs, based on re-
ducing classification error, cannot always yield good
performance, we train the weights by directly opti-
mizing the evaluation performance, i.e., maximizing
the correlation with the human judgment. This type
of direct optimization is known as Minimum Error
Rate Training (Och, 2003) in the MT community,
and is an essential component in building the state-
of-art MT systems. It would seem logical to apply
similar methods to MT evaluation. What is more,
Maximum Correlation Training (MCT) enables us
to train the weights based on human fluency judg-
ments and adequacy judgments respectively, and
thus makes it possible to make a fluency-oriented or
adequacy-oriented metric. It surpasses previous MT
metrics? approach, where a a single metric evaluates
both fluency and adequacy. The rest of the paper is
organized as follows: Section 2 gives a brief recap of
n-gram precision-based metrics and introduces our
three extensions to them; Section 3 introduces MCT
for MT evaluation; Section 4 describes the experi-
mental results, and Section 5 gives our conclusion.
2 Three New Features for MT Evaluation
Since our source-sentence constrained n-gram preci-
sion and discriminative unigram precision are both
derived from the normal n-gram precision, it is
worth describing the original n-gram precision met-
ric, BLEU (Papineni et al, 2002). For every MT
hypothesis, BLEU computes the fraction of n-grams
which also appear in the reference sentences, as well
as a brevity penalty. The formula for computing
BLEU is shown below:
BLEU = BPN
N
X
n=1
P
C
P
ngram?C Countclip(ngram)
P
C
P
ngram??C Count(ngram?)
where C denotes the set of MT hypotheses.
Countclip(ngram) denotes the clipped number of
n-grams in the candidates which also appear in the
references. BP in the above formula denotes the
brevity penalty, which is set to 1 if the accumulated
length of the MT outputs is longer than the arith-
metic mean of the accumulated length of the refer-
ences, and otherwise is set to the ratio of the two.
For sentence-level evaluation with BLEU, we com-
pute the score based on each pair of MT hypothe-
sis/reference. Later approaches, as described in Sec-
tion 1, use different ways to manipulate the morpho-
logical similarity between the MT hypothesis and its
references. Most of them, except NIST, consider the
words in MT hypothesis as the same, i.e., as long as
the words in MT hypothesis appear in the references,
42
they make no difference to the metrics.1 NIST com-
putes the n-grams weights as the logarithm of the ra-
tio of the n-gram frequency and its one word lower
n-gram frequency. From our experiments, NIST is
not generally better than BLEU, and the reason, we
conjecture, is that it differentiates the n-grams too
much and the frequency estimated upon the evalua-
tion corpus is not always reliable. In this section we
will describe two other strategies for differentiating
the n-grams, one of which uses the alignments with
the source sentence as a further constraint, while the
other differentiates the n-gram precisions according
to POS.
2.1 Source-sentence Constrained N-gram
Precision
The quality of an MT sentence should be indepen-
dent of the source sentence given the reference trans-
lation, but considering that current metrics are all
based on shallow morphological similarity of the
MT outputs and the reference, without really under-
standing the meaning in both sides, the source sen-
tences could have some useful information in dif-
ferentiating the MT outputs. Consider the Chinese-
English translation example below:
Source: wo bu neng zhe me zuo
Hypothesis: I must hardly not do this
Reference: I must not do this
It is clear that the word not in the MT output can-
not co-exist with the word hardly while maintain-
ing the meaning of the source sentence. None of
the metrics mentioned above can prevent not from
being counted in the evaluation, due to the simple
reason that they only compute shallow morphologi-
cal similarity. Then how could the source sentence
help in the example? If we reveal the alignment
of the source sentence with both the reference and
the MT output, the Chinese word bu neng would
be aligned to must not in the reference and must
hardly in the MT output respectively, leaving the
word not in the MT output not aligned to any word in
the source sentence. Therefore, if we can somehow
find the alignments between the source sentence and
the reference/MT output, we could be smarter in se-
lecting the overlapping words to be counted in the
1In metrics such as METEOR, ROUGE, SIA (Liu and
Gildea, 2006), the positions of words do make difference, but
it has nothing to do with the word itself.
for all n-grams wi, ..., wi+n?1 in MT hypothesis
do
max val = 0;
for all reference sentences do
for all n-grams rj , ..., rj+n?1 in current ref-
erence sentence do
val=0;
for k=0; k ? n-1; k ++ do
if wi+k equals rj+k AND MTaligni
equals REFalignj then
val += 1n ;
if val ? max val then
max val = val;
hit count += max val;
return hit countMThypothesislength ? length penalty;
Figure 1: Algorithm for Computing Source-
sentence Constrained n-gram Precision
metric: only select the words which are aligned to
the same source words. Now the question comes
to how to find the alignment of source sentence and
MT hypothesis/references, since the evaluation data
set usually does not contain alignment information.
Our approach uses GIZA++2 to construct the many-
to-one alignments between source sentences and the
MT hypothesis/references respectively.3 GIZA++
could generate many-to-one alignments either from
source sentence to the MT hypothesis, in which case
every word in MT hypothesis is aligned to a set
of (or none) words in the source sentence, or from
the reverse direction, in which case every word in
MT hypothesis is aligned to exactly one word (or
none) word in the source sentence. In either case,
using MTaligni and REFaligni to denote the po-
sitions of the words in the source sentences which
are aligned to a word in the MT hypothesis and a
word in the reference respectively, the algorithm for
computing source-sentence constrained n-gram pre-
cision of length n is described in Figure 1.
Since source-sentence constrained n-gram preci-
sion (SSCN) is a precision-based metric, the vari-
2GIZA++ is available at
http://www.fjoch.com/GIZA++.html
3More refined alignments could be got for source-hypothesis
from the MT system, and for source-references by using manual
proof-reading after the automatic alignment. Doing so, how-
ever, requires the MT system?s cooperation and some costly hu-
man labor.
43
able length penalty is used to avoid assigning a
short MT hypothesis a high score, and is computed
in the same way as BLEU. Note that in the algo-
rithm for computing the precision of n-grams longer
than one word, not all words in the n-grams should
satisfy the source-sentence constraint. The reason is
that the high order n-grams are already very sparse
in the sentence-level evaluation. To differentiate the
SSCNs based on the source-to-MT/Ref (many-to-
one) alignments and the MT/Ref-to-source (many-
to-one) alignments, we use SSCN1 and SSCN2 to
denote them respectively. Naturally, we could com-
bine the constraint in SSCN1 and SSCN2 by either
taking their union (the combined constrained is sat-
isfied if either one is satisfied) or intersecting them
(the combined constrained is satisfied if both con-
straints are satisfied). We use SSCN u and SSCN i
to denote the SSCN based on unioned constraints
and intersected constraints respectively. We could
also apply the stochastic word mapping proposed in
SIA (Liu and Gildea, 2006) to replace the hard word
matching in Figure 1, and the corresponding met-
rics are denoted as pSSCN1, pSSCN2, pSSCN u,
pSSCN i, with the suffixed number denoting differ-
ent constraints.
2.2 Metrics Based on Source Word Reordering
Most previous MT metrics concentrate on the co-
occurrence of the MT hypothesis words in the ref-
erences. Our metrics based on source sentence re-
orderings, on the contrary, do not take words identi-
ties into account, but rather compute how similarly
the source words are reordered in the MT output and
the references. For simplicity, we only consider the
pairwise reordering similarity. That is, for the source
word pair wi and wj , if their aligned positions in the
MT hypothesis and a reference are in the same order,
we call it a consistent word pair. Our pairwise re-
ordering similarity (PRS) metric computes the frac-
tion of the consistent word pairs in the source sen-
tence. Figure 2 gives the formal description of PRS.
SrcMTi and SrcRefk,i denote the aligned position
of source word wi in the MT hypothesis and the kth
reference respectively, and N denotes the length of
the source sentence.
Another criterion for evaluating the reordering of
the source sentence in the MT hypothesis is how
well it maintains the original word order in the
for all word pair wi, wj in the source sentence
such that i < j do
for all reference sentences rk do
if (SrcMTi == SrcMTj AND
SrcRefk,i == SrcRefk,j) OR
((SrcMTi ? SrcMTj) ? (SrcRefk,i ?
SrcRefk,j) > 0) then
count + +; break;
return 2?countN?(N?1) ;
Figure 2: Compute Pairwise Reordering Similarity
for all word pair wi, wj in the source sentence,
such that i < j do
if SrcMTi ? SrcMTj < 0 then
count + +;
return 2?countN?(N?1) ;
Figure 3: Compute Source Sentence Monotonic Re-
ordering Ratio
source sentence. We know that most of the time,
the alignment of the source sentence and the MT hy-
pothesis is monotonic. This idea leads to the metric
of monotonic pairwise ratio (MPR), which computes
the fraction of the source word pairs whose aligned
positions in the MT hypothesis are of the same order.
It is described in Figure 3.
2.3 Discriminative Unigram Precision Based
on POS
The Discriminative Unigram Precision Based on
POS (DUPP) decomposes the normal unigram pre-
cision into many sub-precisions according to their
POS. The algorithm is described in Figure 4.
These sub-precisions by themselves carry the
same information as standard unigram precision, but
they provide us the opportunity to make a better
combined metric than the normal unigram precision
with MCT, which will be introduced in next section.
for all unigram s in the MT hypothesis do
if s is found in any of the references then
countPOS(s) += 1
precisionx = countxmt hypothesis length
?x ? POS
Figure 4: Compute DUPP for N-gram with length n
44
Such division could in theory be generalized to work
with higher order n-grams, but doing so would make
the n-grams in each POS set much more sparse. The
preprocessing step for the metric is tagging both
the MT hypothesis and the references with POS. It
might elicit some worries about the robustness of the
POS tagger on the noise-containing MT hypothesis.
This should not be a problem for two reasons. First,
compared with other preprocessing steps like pars-
ing, POS tagging is easier and has higher accuracy.
Second, because the counts for each POS are accu-
mulated, the correctness of a single word?s POS will
not affect the result very much.
3 Maximum Correlation Training for
Machine Translation Evaluation
Maximum Correlation Training (MCT) is an in-
stance of the general approach of directly optimiz-
ing the objective function by which a model will
ultimately be evaluated. In our case, the model is
the linear combination of the component metrics, the
parameters are the weights for each component met-
ric, and the objective function is the Pearson?s corre-
lation of the combined metric and the human judg-
ments. The reason to use the linear combination of
the metrics is that the component metrics are usu-
ally of the same or similar order of magnitude, and it
makes the optimization problem easy to solve. Us-
ing w to denote the weights, and m to denote the
component metrics, the combined metric x is com-
puted as:
x(w) =
?
j
wjmj (1)
Using hi and x(w)i denote the human judgment
and combined metric for a sentence respectively, and
N denote the number of sentences in the evaluation
set, the objective function is then computed as:
Pearson(X(w), H) =
PN
i=1 x(w)ihi ?
PN
i=1 x(w)i
PN
i=1 hi
N
q
(
PN
i=1 x(w)2i ?
(PNi=1 x(w)i)2
N )(
PN
i=1 h2i ?
(PNi=1 hi)2
N )
Now our task is to find the weights for each compo-
nent metric so that the correlation of the combined
metric with the human judgment is maximized. It
can be formulated as:
w = argmax
w
Pearson(X(w), H) (2)
The function Pearson(X(w), H) is differentiable
with respect to the vector w, and we compute this
derivative analytically and perform gradient ascent.
Our objective function not always convex (one can
easily create a non-convex function by setting the
human judgments and individual metrics to some
particular value). Thus there is no guarantee that,
starting from a random w, we will get the glob-
ally optimal w using optimization techniques such
as gradient ascent. The easiest way to avoid ending
up with a bad local optimum to run gradient ascent
by starting from different random points. In our ex-
periments, the difference in each run is very small,
i.e., by starting from different random initial values
of w, we end up with, not the same, but very similar
values for Pearson?s correlation.
4 Experiments
Experiments were conducted to evaluate the perfor-
mance of the new metrics proposed in this paper,
as well as the MCT combination framework. The
data for the experiments are from the MT evalua-
tion workshop at ACL05. There are seven sets of
MT outputs (E09 E11 E12 E14 E15 E17 E22), each
of which contains 919 English sentences translated
from the same set of Chinese sentences. There are
four references (E01, E02, E03, E04) and two sets
of human scores for each MT hypothesis. Each hu-
man score set contains a fluency and an adequacy
score, both of which range from 1 to 5. We create a
set of overall human scores by averaging the human
fluency and adequacy scores. For evaluating the au-
tomatic metrics, we compute the Pearson?s correla-
tion of the automatic scores and the averaged human
scores (over the two sets of available human scores),
for overall score, fluency, and adequacy. The align-
ment between the source sentences and the MT hy-
pothesis/references is computed by GIZA++, which
is trained on the combined corpus of the evalua-
tion data and a parallel corpus of Chinese-English
newswire text. The parallel newswire corpus con-
tains around 75,000 sentence pairs, 2,600,000 En-
glish words and 2,200,000 Chinese words. The
45
stochastic word mapping is trained on a French-
English parallel corpus containing 700,000 sentence
pairs, and, following Liu and Gildea (2005), we only
keep the top 100 most similar words for each En-
glish word.
4.1 Performance of the Individual Metrics
To evaluate our source-sentence based metrics, they
are used to evaluate the 7 MT outputs, with the 4 sets
of human references. The sentence-level Pearson?s
correlation with human judgment is computed for
each MT output, and the averaged results are shown
in Table 1. As a comparison, we also show the re-
sults of BLEU, NIST, METEOR, ROUGE, WER,
and HWCM. For METEOR and ROUGE, WORD-
NET and PORTER-STEMMER are enabled, and for
SIA, the decay factor is set to 0.6. The number
in brackets, for BLEU, shows the n-gram length it
counts up to, and for SSCN, shows the length of the
n-gram it uses. In the table, the top 3 results in each
column are marked bold and the best result is also
underlined. The results show that the SSCN2 met-
rics are better than the SSCN1 metrics in adequacy
and overall score. This is understandable since what
SSCN metrics need is which words in the source
sentence are aligned to an n-gram in the MT hy-
pothesis/references. This is directly modeled in the
alignment used in SSCN2. Though we could also
get such information from the reverse alignment, as
in SSCN1, it is rather an indirect way and could con-
tain more noise. It is interesting that SSCN1 gets
better fluency evaluation results than SSCN2. The
SSCN metrics with the unioned constraint, SSCN u,
by combining the strength of SSCN1 and SSCN2,
get even better results in all three aspects. We can
see that SSCN metrics, even without stochastic word
mapping, get significantly better results than their
relatives, BLEU, which indicates the source sen-
tence constraints do make a difference. SSCN2 and
SSCN u are also competitive to the state-of-art MT
metrics such as METEOR and SIA. The best SSCN
metric, pSSCN u(2), achieves the best performance
among all the testing metrics in overall and ade-
quacy, and the second best performance in fluency,
which is just a little bit worse than the best fluency
metric SIA.
The two reordering based metrics, PRS and MPR,
are not as good as the other testing metrics, in terms
Fluency Adequacy Overall
ROUGE W 24.8 27.8 29.0
ROUGE S 19.7 30.9 28.5
METEOR 24.4 34.8 33.1
SIA 26.8 32.1 32.6
NIST 1 09.6 22.6 18.5
WER 22.5 27.5 27.7
PRS 14.2 19.4 18.7
MPR 11.0 18.2 16.5
BLEU(1) 18.4 29.6 27.0
BLEU(2) 20.4 31.1 28.9
BLEU(3) 20.7 30.4 28.6
HWCM(2) 22.1 30.3 29.2
SSCN1(1) 24.2 29.6 29.8
SSCN2(1) 22.9 33.0 31.3
SSCN u(1) 23.8 34.2 32.5
SSCN i(1) 23.4 28.0 28.5
pSSCN1(1) 24.9 30.2 30.6
pSSCN2(1) 23.8 34.0 32.4
pSSCN u(1) 24.5 34.6 33.1
pSSCN i(1) 24.1 28.8 29.3
SSCN1(2) 24.0 29.6 29.7
SSCN2(2) 23.3 31.5 31.8
SSCN u(2) 24.1 34.5 32.8
SSCN i(2) 23.1 27.8 28.2
pSSCN1(2) 24.9 30.2 30.6
pSSCN2(2) 24.3 34.4 32.8
pSSCN u(2) 25.2 35.4 33.9
pSSCN i(2) 23.9 28.7 29.1
Table 1: Performance of Component Metrics
of the individual performance. It should not be sur-
prising since they are totally different kind of met-
rics, which do not count the overlapping n-grams,
but the consistent/monotonic word pair reorderings.
As long as they capture some property of the MT
hypothesis, they might be able to boost the per-
formance of the combined metric under the MCT
framework.
4.2 Performance of the Combined Metrics
To test how well MCT works, the following scheme
is used: each set of MT outputs is evaluated by MCT,
which is trained on the other 6 sets of MT outputs
and their corresponding human judgment; the aver-
aged correlation of the 7 sets of MT outputs with the
human judgment is taken as the final result.
4.2.1 Discriminative Unigram Precision based
on POS
We first use MCT to combine the discriminative
unigram precisions. To reduce the sparseness of the
unigrams of each POS, we do not use the original
POS set, but use a generalized one by combining
46
all POS tags with the same first letter (e.g., the dif-
ferent verb forms such as VBN, VBD, and VBZ are
transformed to V). The unified POS set contains 23
POS tags. To give a fair comparison of DUPP with
BLEU, the length penalty is also added into it as a
component. Results are shown in Table 2. DUPP f,
DUPP a and DUPP o denote DUPP trained on hu-
man fluency, adequacy and overall judgment respec-
tively. This shows that DUPP achieves obvious im-
provement over BLEU, with only the unigrams and
length penalty, and DUPP f/ a/ o gets the best re-
sult in fluency/adequacy/overall evaluation, showing
that MCT is able to make a fluency- or adequacy-
oriented metric.
4.2.2 Putting It All Together
The most interesting question in this paper is, with
all these metrics, how well we can do in the MT
evaluation. To answer the question, we put all the
metrics described into the MCT framework and use
the combined metric to evaluate the 7 MT outputs.
Note that to speed up the training process, we do
not directly use 24 DUPP components, instead, we
use the 3 combined DUPP metrics. With the met-
rics shown in Table 1, we then have in total 31 met-
rics. Table 2 shows the results of the final combined
metric. We can see that MCT trained on fluency,
adequacy and overall human judgment get the best
results among all the testing metrics in fluency, ade-
quacy and overall evaluation respectively. We did a
t-test with Fisher?s z transform for the combined re-
sults and the individual results to see how significant
the difference is. The combined results in adequacy
and overall are significantly better at 99.5% confi-
dence than the best results of the individual metrics
(pSSCN u(2)), and the combined result in fluency
is significantly better at 96.9% confidence than the
best individual metric (SIA). We also give the upper
bound for each evaluation aspect by training MCT
on the testing MT outputs, e.g., we train MCT on
E09 and then use it to evaluate E09. The upper-
bound is the best we can do with the MCT based
on linear combination. Another linear framework,
Classification SVM (CSVM),4 is also used to com-
bine the testing metrics except DUPP. Since DUPP
is based on MCT, to make a neat comparison, we
rule out DUPP in the experiments with CSVM. The
4http://svmlight.joachims.org/
Fluency Adequacy Overall
DUPP f 23.6 30.1 30.1
DUPP a 22.1 32.9 30.9
DUPP o 23.2 32.8 31.3
MCT f(4) 30.3 36.7 37.2
MCT a(4) 28.0 38.9 37.4
MCT o(4) 29.4 38.8 38.0
Upper bound 35.3 43.4 42.2
MCT f(3) 29.2 34.7 35.3
MCT a(3) 27.4 38.4 36.8
MCT o(3) 28.8 38.0 37.2
CSVM(3) 27.3 36.9 35.5
Table 2: Combination of the Testing Metrics
testing scheme is the same as MCT, except that we
only use 3 references for each MT hypothesis, and
the positive samples for training CSVM are com-
puted as the scores of one of the 4 references based
on the other 3 references. The slack parameter of
CSVM is chosen so as to maximize the classifica-
tion accuracy of a heldout set of 800 negative and
800 positive samples, which are randomly selected
from the training set. The results are shown in Ta-
ble 2. We can see that MCT, with the same number
of reference sentences, is better than CSVM. Note
that the resources required by MCT and CSVM are
different. MCT uses human judgments to adjust the
weights, while CSVM needs extra human references
to produce positive training samples.
To have a rough idea of how the component met-
rics contribute to the final performance of MCT, we
incrementally add metrics into the MCT in descend-
ing order of their overall evaluation performance,
with the results shown in Figure 5. We can see that
the performance improves as the number of metrics
increases, in a rough sense. The major improvement
happens in the 3rd, 4th, 9th, 14th, and 30th metrics,
which are METEOR, SIA, DUPP a, pSSCN1(1),
and PRS. It is interesting to note that these are not
the metrics with the highest individual performance.
Another interesting observation is that there are no
two metrics belonging to the same series in the most
beneficial metrics, indicating that to get better com-
bined metrics, individual metrics showing different
sentence properties are preferred.
5 Conclusion
This paper first describes two types of new ap-
proaches to MT evaluation, which includes making
47
0 5 10 15 20 25 30 35
0.24
0.26
0.28
0.3
0.32
0.34
0.36
0.38
0.4
the number of metrics (o: adequacy, x: fluency, +: overall)
co
rr
e
la
tio
n 
w
ith
 h
um
an
 fl
ue
nc
y/
ov
er
al
l/a
de
qu
ac
y 
jud
ge
me
nts
Figure 5: Performance as a Function of the Number
of Interpolated Metrics
use of source sentences, and discriminating unigram
precisions based on POS. Among all the testing met-
rics including BLEU, NIST, METEOR, ROUGE,
and SIA, our new metric, pSSCN u(2), based on
source-sentence constrained bigrams, achieves the
best adequacy and overall evaluation results, and the
second best result in fluency evaluation. We fur-
ther improve the performance by combining the in-
dividual metrics under the MCT framework, which
is shown to be better than a classification based
framework such as SVM. By examining the contri-
bution of each component metric, we find that met-
rics showing different properties of a sentence are
more likely to make a good combined metric.
Acknowledgments This work was supported by
NSF grants IIS-0546554, IIS-0428020, and IIS-
0325646.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judegments. In Proceedings of
the ACL-04 workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Univer-
sity, Baltimore. Summer Workshop Final Report.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In In HLT 2002, Human Language Technology
Conference, San Diego, CA.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), Baltimore, MD, October.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42th Annual Conference of the
Association for Computational Linguistics (ACL-04),
Barcelona, Spain.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
Blanc: Learning evaluation metrics for mt. Vancouver.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization.
Ding Liu and Daniel Gildea. 2006. Stochastic iterative
alignment for machine translation evaluation. Sydney.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL-
03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL-
02, Philadelphia, PA.
48
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 539?546,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Stochastic Iterative Alignment for Machine Translation Evaluation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
A number of metrics for automatic eval-
uation of machine translation have been
proposed in recent years, with some met-
rics focusing on measuring the adequacy
of MT output, and other metrics focus-
ing on fluency. Adequacy-oriented met-
rics such as BLEU measure n-gram over-
lap of MT outputs and their references, but
do not represent sentence-level informa-
tion. In contrast, fluency-oriented metrics
such as ROUGE-W compute longest com-
mon subsequences, but ignore words not
aligned by the LCS. We propose a metric
based on stochastic iterative string align-
ment (SIA), which aims to combine the
strengths of both approaches. We com-
pare SIA with existing metrics, and find
that it outperforms them in overall evalu-
ation, and works specially well in fluency
evaluation.
1 Introduction
Evaluation has long been a stumbling block in
the development of machine translation systems,
due to the simple fact that there are many correct
translations for a given sentence. Human evalu-
ation of system output is costly in both time and
money, leading to the rise of automatic evalua-
tion metrics in recent years. In the 2003 Johns
Hopkins Workshop on Speech and Language En-
gineering, experiments on MT evaluation showed
that BLEU and NIST do not correlate well with
human judgments at the sentence level, even when
they correlate well over large test sets (Blatz et
al., 2003). Liu and Gildea (2005) also pointed
out that due to the limited references for every
MT output, using the overlapping ratio of n-grams
longer than 2 did not improve sentence level eval-
uation performance of BLEU. The problem leads
to an even worse result in BLEU?S fluency eval-
uation, which is supposed to rely on the long n-
grams. In order to improve sentence-level evalu-
ation performance, several metrics have been pro-
posed, including ROUGE-W, ROUGE-S (Lin and
Och, 2004) and METEOR (Banerjee and Lavie,
2005). ROUGE-W differs from BLEU and NIST
in that it doesn?t require the common sequence be-
tween MT output and the references to be consec-
utive, and thus longer common sequences can be
found. There is a problem with loose-sequence-
based metrics: the words outside the longest com-
mon sequence are not considered in the metric,
even if they appear both in MT output and the
reference. ROUGE-S is meant to alleviate this
problem by computing the common skipped bi-
grams instead of the LCS. But the price ROUGE-
S pays is falling back to the shorter sequences and
losing the advantage of long common sequences.
METEOR is essentially a unigram based metric,
which prefers the monotonic word alignment be-
tween MT output and the references by penalizing
crossing word alignments. There are two prob-
lems with METEOR. First, it doesn?t consider
gaps in the aligned words, which is an important
feature for evaluating the sentence fluency; sec-
ond, it cannot use multiple references simultane-
ously.1 ROUGE and METEOR both use WordNet
and Porter Stemmer to increase the chance of the
MT output words matching the reference words.
Such morphological processing and synonym ex-
traction tools are available for English, but are not
always available for other languages. In order to
take advantage of loose-sequence-based metrics
and avoid the problems in ROUGE and METEOR,
we propose a new metric SIA, which is based on
loose sequence alignment but enhanced with the
following features:
1METEOR and ROUGE both compute the score based on
the best reference
539
? Computing the string alignment score based
on the gaps in the common sequence. Though
ROUGE-W also takes into consider the gaps
in the common sequence between the MT
output and the reference by giving more cred-
its to the n-grams in the common sequence,
our method is more flexible in that not only
do the strict n-grams get more credits, but
also the tighter sequences.
? Stochastic word matching. For the purpose
of increasing hitting chance of MT outputs in
references, we use a stochastic word match-
ing in the string alignment instead of WORD-
STEM and WORD-NET used in METEOR
and ROUGE. Instead of using exact match-
ing, we use a soft matching based on the sim-
ilarity between two words, which is trained
in a bilingual corpus. The corpus is aligned
in the word level using IBM Model4 (Brown
et al, 1993). Stochastic word matching is a
uniform replacement for both morphological
processing and synonym matching. More im-
portantly, it can be easily adapted for differ-
ent kinds of languages, as long as there are
bilingual parallel corpora available (which is
always true for statistical machine transla-
tion).
? Iterative alignment scheme. In this scheme,
the string alignment will be continued until
there are no more co-occuring words to be
found between the MT output and any one of
the references. In this way, every co-occuring
word between the MT output and the refer-
ences can be considered and contribute to the
final score, and multiple references can be
used simultaneously.
The remainder of the paper is organized as fol-
lows: section 2 gives a recap of BLEU, ROUGE-
W and METEOR; section 3 describes the three
components of SIA; section 4 compares the per-
formance of different metrics based on experimen-
tal results; section 5 presents our conclusion.
2 Recap of BLEU, ROUGE-W and
METEOR
The most commonly used automatic evaluation
metrics, BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002), are based on the assumption
that ?The closer a machine translation is to a pro-
mt1: Life is like one nice chocolate in box
ref: Life is just like a box of tasty chocolate
ref: Life is just like a box of tasty chocolate
mt2: Life is of one nice chocolate in box 
Figure 1: Alignment Example for ROUGE-W
fessional human translation, the better it is? (Pa-
pineni et al, 2002). For every hypothesis, BLEU
computes the fraction of n-grams which also ap-
pear in the reference sentences, as well as a brevity
penalty. NIST uses a similar strategy to BLEU but
further considers that n-grams with different fre-
quency should be treated differently in the evalu-
ation (Doddington, 2002). BLEU and NIST have
been shown to correlate closely with human judg-
ments in ranking MT systems with different qual-
ities (Papineni et al, 2002; Doddington, 2002).
ROUGE-W is based on the weighted longest
common subsequence (LCS) between the MT out-
put and the reference. The common subsequences
in ROUGE-W are not necessarily strict n-grams,
and gaps are allowed in both the MT output and
the reference. Because of the flexibility, long
common subsequences are feasible in ROUGE-
W and can help to reflect the sentence-wide sim-
ilarity of MT output and references. ROUGE-W
uses a weighting strategy where the LCS contain-
ing strict n-grams is favored. Figure 1 gives two
examples that show how ROUGE-W searches for
the LCS. For mt1, ROUGE-W will choose either
life is like chocolate or life is like box as the LCS,
since neither of the sequences ?like box? and ?like
chocolate? are strict n-grams and thus make no dif-
ference in ROUGE-W (the only strict n-grams in
the two candidate LCS is life is). For mt2, there
is only one choice of the LCS: life is of chocolate.
The LCS of mt1 and mt2 have the same length and
the same number of strict n-grams, thus they get
the same score in ROUGE-W. But it is clear to us
that mt1 is better than mt2. It is easy to verify that
mt1 and mt2 have the same number of common 1-
grams, 2-grams, and skipped 2-grams with the ref-
erence (they don?t have common n-grams longer
than 2 words), thus BLEU and ROUGE-S are also
not able to differentiate them.
METEOR is a metric sitting in the middle
of the n-gram based metrics and the loose se-
540
mt1: Life is like one nice chocolate in box
ref: Life is just like a box of tasty chocolate
ref: Life is just like a box of tasty chocolate
mt2: Life is of one nice chocolate in box 
Figure 2: Alignment Example for METEOR
quence based metrics. It has several phases and
in each phase different matching techniques (EX-
ACT, PORTER-STEM, WORD-NET) are used to
make an alignment for the MT output and the ref-
erence. METEOR doesn?t require the alignment to
be monotonic, which means crossing word map-
pings (e.g. a b is mapped to b a) are allowed,
though doing so will get a penalty. Figure 2 shows
the alignments of METEOR based on the same
example as ROUGE. Though the two alignments
have the same number of word mappings, mt2 gets
more crossed word mappings than mt1, thus it will
get less credits in METEOR. Both ROUGE and
METEOR normalize their evaluation result based
on the MT output length (precision) and the ref-
erence length (recall), and the final score is com-
puted as the F-mean of them.
3 Stochastic Iterative Alignment (SIA)
for Machine Translation Evaluation
We introduce three techniques to allow more sen-
sitive scores to be computed.
3.1 Modified String Alignment
This section introduces how to compute the string
alignment based on the word gaps. Given a pair
of strings, the task of string alignment is to obtain
the longest monotonic common sequence (where
gaps are allowed). SIA uses a different weighting
strategy from ROUGE-W, which is more flexible.
In SIA, the alignments are evaluated based on the
geometric mean of the gaps in the reference side
and the MT output side. Thus in the dynamic pro-
gramming, the state not only includes the current
covering length of the MT output and the refer-
ence, but also includes the last aligned positions in
them. The algorithm for computing the alignment
score in SIA is described in Figure 3. The sub-
routine COMPUTE SCORE, which computes the
score gained from the current aligned positions, is
shown in Figure 4. From the algorithm, we can
function GET ALIGN SCORE(mt, M, ref, N)
. Compute the alignment score of the MT output mt
with length M and the reference ref with length N
for i = 1; i ? M; i = i +1 do
for j = 1; j ? N; j = j +1 do
for k = 1; k ? i; k = k +1 do
for m = 1; m ? j; m = m +1 do
scorei,j,k,m
= max{scorei?1,j,k,m,scorei,j?1,k,m } ;
end for
end for
scorei,j,i,j =
max
n=1,M ;p=1,N
{scorei,j,i,j , scorei?1,j?1,n,p
+ COMPUTE SCORE(mt,ref, i, j, n, p)};
end for
end for
return scoreM,N,M,NM ;
end function
Figure 3: Alignment Algorithm Based on Gaps
function COMPUTE SCORE(mt, ref, i, j, n, p)
if mt[i] == ref [j] then
return 1/
p
(i ? n) ? (j ? p);
else
return 0;
end if
end function
Figure 4: Compute Word Matching Score Based
on Gaps
see that not only will strict n-grams get higher
scores than non-consecutive sequences, but also
the non-consecutive sequences with smaller gaps
will get higher scores than those with larger gaps.
This weighting method can help SIA capture more
subtle difference of MT outputs than ROUGE-W
does. For example, if SIA is used to align mt1
and ref in Figure 1, it will choose life is like box
instead of life is like chocolate, because the aver-
age distance of ?box-box? to its previous mapping
?like-like? is less than ?chocolate-chocolate?. Then
the score SIA assigns to mt1 is:
( 1
1 ? 1 +
1
1 ? 1 +
1?
1 ? 2
+ 1?
2 ? 5
)
?18 = 0.399(1)
For mt2, there is only one possible alignment,
its score in SIA is computed as:
( 1
1 ? 1 +
1
1 ? 1 +
1?
1 ? 5
+ 1?
2 ? 3
)
?18 = 0.357(2)
Thus, mt1 will be considered better than mt2 in
SIA, which is reasonable. As mentioned in sec-
tion 1, though loose-sequence-based metrics give
a better reflection of the sentence-wide similarity
of the MT output and the reference, they cannot
541
make full use of word-level information. This de-
fect could potentially lead to a poor performance
in adequacy evaluation, considering the case that
the ignored words are crucial to the evaluation. In
the later part of this section, we will describe an it-
erative alignment scheme which is meant to com-
pensate for this defect.
3.2 Stochastic Word Mapping
In ROUGE and METEOR, PORTER-STEM and
WORD-NET are used to increase the chance of
the MT output words matching the references.
We use a different stochastic approach in SIA to
achieve the same purpose. The string alignment
has a good dynamic framework which allows the
stochastic word matching to be easily incorporated
into it. The stochastic string alignment can be im-
plemented by simply replacing the function COM-
PUTE SCORE with the function of Figure 5. The
function similarity(word1, word2) returns a ratio
which reflects how similar the two words are. Now
we consider how to compute the similarity ratio of
two words. Our method is motivated by the phrase
extraction method of Bannard and Callison-Burch
(2005), which computes the similarity ratio of two
words by looking at their relationship with words
in another language. Given a bilingual parallel
corpus with aligned sentences, say English and
French, the probability of an English word given
a French word can be computed by training word
alignment models such as IBM Model4. Then for
every English word e, we have a set of conditional
probabilities given each French word: p(e|f1),
p(e|f2), ... , p(e|fN ). If we consider these proba-bilities as a vector, the similarities of two English
words can be obtained by computing the dot prod-
uct of their corresponding vectors.2 The formula
is described below:
similarity(ei, ej) =
N
?
k=1
p(ei|fk)p(ej |fk) (3)
Paraphrasing methods based on monolingual par-
allel corpora such as (Pang et al, 2003; Barzilay
and Lee, 2003) can also be used to compute the
similarity ratio of two words, but they don?t have
as rich training resources as the bilingual methods
do.
2Although the marginalized probability (over all French
words) of an English word given the other English word
(PNk=1 p(ei|fk)p(fk|ej)) is a more intuitive way of measur-ing the similarity, the dot product of the vectors p(e|f) de-
scribed above performed slightly better in our experiments.
function STO COMPUTE SCORE(mt, ref, i, j, n, p)
if mt[i] == ref [j] then
return 1/
p
(i ? n) ? (j ? p);
else
return similarity(mt[i],ref [i])?
(i?n)?(j?p)
;
end if
end function
Figure 5: Compute Stochastic Word Matching
Score
3.3 Iterative Alignment Scheme
ROUGE-W, METEOR, and WER all score MT
output by first computing a score based on each
available reference, and then taking the highest
score as the final score for the MT output. This
scheme has the problem of not being able to use
multiple references simultaneously. The itera-
tive alignment scheme proposed here is meant to
alleviate this problem, by doing alignment be-
tween the MT output and one of the available ref-
erences until no more words in the MT output
can be found in the references. In each align-
ment round, the score based on each reference
is computed and the highest one is taken as the
score for the round. Then the words which have
been aligned in best alignment will not be con-
sidered in the next round. With the same num-
ber of aligned words, the MT output with fewer
alignment rounds should be considered better than
those requiring more rounds. For this reason, a
decay factor ? is multiplied with the scores of
each round. The final score of the MT output is
then computed by summing the weighted scores
of each alignment round. The scheme is described
in Figure 6.
The function GET ALIGN SCORE 1 used
in GET ALIGN SCORE IN MULTIPLE REFS
is slightly different from GET ALIGN SCORE
described in the prior subsection. The dynamic
programming algorithm for getting the best
alignment is the same, except that it has two more
tables as input, which record the unavailable po-
sitions in the MT output and the reference. These
positions have already been used in the prior best
alignments and should not be considered in the
ongoing alignment. It also returns the aligned
positions of the best alignment. The pseudocode
for GET ALIGN SCORE 1 is shown in Figure 7.
The computation of the length penalty is similar
to BLEU: it is set to 1 if length of the MT output
is longer than the arithmetic mean of length of the
542
function GET ALIGN SCORE IN MULTIPLE REFS(mt,
ref 1, ..., ref N , ?)
. Iteratively Compute the Alignment Score Based on
Multiple References and the Decay Factor ?
final score = 0;
while max score != 0 do
for i = 1, ..., N do
(score, align) =
GET ALIGN SCORE 1(mt, ref i, mt table, ref tablei);
if score > max score then
max score = score;
max align = align;
max ref = i;
end if
end for
final score += max score ??;
? ? = ?;
Add the words in align to mt table and
ref tablemax ref ;
end while
return final score? length penalty;
end function
Figure 6: Iterative Alignment Scheme
references, and otherwise is set to the ratio of the
two. Figure 8 shows how the iterative alignment
scheme works with an evaluation set containing
one MT output and two references. The selected
alignment in each round is shown, as well as the
unavailable positions in MT output and refer-
ences. With the iterative scheme, every common
word between the MT output and the reference
set can make a contribution to the metric, and
by such means SIA is able to make full use of
the word-level information. Furthermore, the
order (alignment round) in which the words are
aligned provides a way to weight them. In BLEU,
multiple references can be used simultaneously,
but the common n-grams are treated equally.
4 Experiments
Evaluation experiments were conducted to com-
pare the performance of different metrics includ-
ing BLEU, ROUGE, METEOR and SIA.3 The test
data for the experiments are from the MT evalu-
ation workshop at ACL05. There are seven sets
of MT outputs (E09 E11 E12 E14 E15 E17 E22),
all of which contain 919 English sentences. These
sentences are the translation of the same Chinese
input generated by seven different MT systems.
The fluency and adequacy of each sentence are
manually ranked from 1 to 5. For each MT output,
there are two sets of human scores available, and
3METEOR and ROUGE can be downloaded at
http://www.cs.cmu.edu/?alavie/METEOR and
http://www.isi.edu/licensed-sw/see/rouge
function GET ALIGN SCORE1(mt, ref, mttable, reftable)
. Compute the alignment score of the MT output mt
with length M and the reference ref with length N, without
considering the positions in mttable and reftable
M = |mt|; N = |ref |;
for i = 1; i ? M; i = i +1 do
for j = 1; j ? N; j = j +1 do
for k = 1; k ? i; k = k +1 do
for m = 1; m ? j; m = m +1 do
scorei,j,k,m
= max{scorei?1,j,k,m, scorei,j?1,k,m};
end for
end for
if i is not in mttable and j is not in reftable then
scorei,j,i,j = max
n=1,M ;p=1,N
{scorei,j,i,j ,
scorei?1,j?1,n,p + COMPUTE SCORE(mt, ref, i, j, n, p)};
end if
end for
end for
return scoreM,N,M,NM and the corresponding alignment;
end function
Figure 7: Alignment Algorithm Based on Gaps
Without Considering Aligned Positions
m: England with France discussed this crisis in London
r1: Britain and France consulted about this crisis in London with each other
r2: England and France discussed the crisis in London
m: England with France discussed this crisis in London
r2: England and France discussed the crisis in London
r1: Britain and France consulted about this crisis in London with each other
m: England with France discussed this crisis in London
r1: Britain and France consulted about this crisis in London with each other
r2: England and France discussed the crisis in London
Figure 8: Alignment Example for SIA
543
we randomly choose one as the score used in the
experiments. The human overall scores are calcu-
lated as the arithmetic means of the human fluency
scores and adequacy scores. There are four sets
of human translations (E01, E02, E03, E04) serv-
ing as references for those MT outputs. The MT
outputs and reference sentences are transformed to
lower case. Our experiments are carried out as fol-
lows: automatic metrics are used to evaluate the
MT outputs based on the four sets of references,
and the Pearson?s correlation coefficient of the au-
tomatic scores and the human scores is computed
to see how well they agree.
4.1 N -gram vs. Loose Sequence
One of the problems addressed in this paper is
the different performance of n-gram based metrics
and loose-sequence-based metrics in sentence-
level evaluation. To see how they really differ
in experiments, we choose BLEU and ROUGE-
W as the representative metrics for the two types,
and used them to evaluate the 6433 sentences in
the 7 MT outputs. The Pearson correlation coeffi-
cients are then computed based on the 6433 sam-
ples. The experimental results are shown in Ta-
ble 1. BLEU-n denotes the BLEU metric with
the longest n-gram of length n. F denotes flu-
ency, A denotes adequacy, and O denotes overall.
We see that with the increase of n-gram length,
BLEU?s performance does not increase monoton-
ically. The best result in adequacy evaluation is
achieved at 2-gram and the best result in fluency is
achieved at 4-gram. Using n-grams longer than 2
doesn?t buy much improvement for BLEU in flu-
ency evaluation, and does not compensate for the
loss in adequacy evaluation. This confirms Liu and
Gildea (2005)?s finding that in sentence level eval-
uation, long n-grams in BLEU are not beneficial.
The loose-sequence-based ROUGE-W does much
better than BLEU in fluency evaluation, but it does
poorly in adequacy evaluation and doesn?t achieve
a significant improvement in overall evaluation.
We speculate that the reason is that ROUGE-W
doesn?t make full use of the available word-level
information.
4.2 METEOR vs. SIA
SIA is designed to take the advantage of loose-
sequence-based metrics without losing word-level
information. To see how well it works, we choose
E09 as the development set and the sentences in
the other 6 sets as the test data. The decay fac-
B-3 R 1 R 2 M S
F 0.167 0.152 0.192 0.167 0.202
A 0.306 0.304 0.287 0.332 0.322
O 0.265 0.256 0.266 0.280 0.292
Table 2: Sentence level evaluation results of
BLEU, ROUGE, METEOR and SIA
tor in SIA is determined by optimizing the over-
all evaluation for E09, and then used with SIA
to evaluate the other 5514 sentences based on the
four sets of references. The similarity of English
words is computed by training IBM Model 4 in
an English-French parallel corpus which contains
seven hundred thousand sentence pairs. For every
English word, only the entries of the top 100 most
similar English words are kept and the similarity
ratios of them are then re-normalized. The words
outside the training corpus will be considered as
only having itself as its similar word. To com-
pare the performance of SIA with BLEU, ROUGE
and METEOR, the evaluation results based on
the same testing data is given in Table 2. B-
3 denotes BLEU-3; R 1 denotes the skipped bi-
gram based ROUGE metric which considers all
skip distances and uses PORTER-STEM; R 2 de-
notes ROUGE-W with PORTER-STEM; M de-
notes the METEOR metric using PORTER-STEM
and WORD-NET synonym; S denotes SIA.
We see that METEOR, as the other metric
sitting in the middle of n-gram based metrics
and loose sequence metrics, achieves improve-
ment over BLEU in both adequacy and fluency
evaluation. Though METEOR gets the best re-
sults in adequacy evaluation, in fluency evaluation,
it is worse than the loose-sequence-based metric
ROUGE-W-STEM. SIA is the only one among
the 5 metrics which does well in both fluency and
adequacy evaluation. It achieves the best results in
fluency evaluation and comparable results to ME-
TEOR in adequacy evaluation, and the balanced
performance leads to the best overall evaluation
results in the experiment. To estimate the signif-
icance of the correlations, bootstrap resampling
(Koehn, 2004) is used to randomly select 5514
sentences with replacement out of the whole test
set of 5514 sentences, and then the correlation co-
efficients are computed based on the selected sen-
tence set. The resampling is repeated 5000 times,
and the 95% confidence intervals are shown in Ta-
bles 3, 4, and 5. We can see that it is very diffi-
544
BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-5 BLEU-6 ROUGE-W
F 0.147 0.162 0.166 0.168 0.165 0.164 0.191
A 0.288 0.296 0.291 0.285 0.279 0.274 0.268
O 0.243 0.256 0.255 0.251 0.247 0.244 0.254
Table 1: Sentence level evaluation results of BLEU and ROUGE-W
low mean high
B-3 (-16.6%) 0.138 0.165 0.192 (+16.4%)
R 1 (-17.8%) 0.124 0.151 0.177 (+17.3%)
R 2 (-14.3%) 0.164 0.191 0.218 (+14.2%)
M (-15.8%) 0.139 0.166 0.191 (+15.5%)
S (-13.3%) 0.174 0.201 0.227 (+13.3%)
Table 3: 95% significance intervals for sentence-
level fluency evaluation
low mean high
B-3 (-08.2%) 0.280 0.306 0.330 (+08.1%)
R 1 (-08.5%) 0.278 0.304 0.329 (+08.4%)
R 2 (-09.2%) 0.259 0.285 0.312 (+09.5%)
M (-07.3%) 0.307 0.332 0.355 (+07.0%)
S (-07.9%) 0.295 0.321 0.346 (+07.8%)
Table 4: 95% significance intervals for sentence-
level adequacy evaluation
cult for one metric to significantly outperform an-
other metric in sentence-level evaluation. The re-
sults show that the mean of the correlation factors
converges right to the value we computed based on
the whole testing set, and the confidence intervals
correlate with the means.
While sentence-level evaluation is useful if we
are interested in a confidence measure on MT out-
puts, syste-x level evaluation is more useful for
comparing MT systems and guiding their develop-
ment. Thus we also present the evaluation results
based on the 7 MT output sets in Table 6. SIA uses
the same decay factor as in the sentence-level eval-
uation. Its system-level score is computed as the
arithmetic mean of the sentence level scores, and
low mean high
B-3 (-09.8%) 0.238 0.264 0.290 (+09.9%)
R 1 (-10.2%) 0.229 0.255 0.281 (+10.0%)
R 2 (-10.0%) 0.238 0.265 0.293 (+10.4%)
M (-09.0%) 0.254 0.279 0.304 (+08.8%)
S (-08.7%) 0.265 0.291 0.316 (+08.8%)
Table 5: 95% significance intervals for sentence-
level overall evaluation
WLS WLS WLS WLS
PROB INCS PROB
INCS
F 0.189 0.202 0.188 0.202
A 0.295 0.310 0.311 0.322
O 0.270 0.285 0.278 0.292
Table 7: Results of different components in SIA
WLS WLS WLS WLS
INCS INCS INCS INCS
STEM WN STEM
WN
F 0.188 0.188 0.187 0.191
A 0.311 0.313 0.310 0.317
O 0.278 0.280 0.277 0.284
Table 8: Results of SIA working with Porter-Stem
and WordNet
so are ROUGE, METEOR and the human judg-
ments. We can see that SIA achieves the best per-
formance in both fluency and adequacy evaluation
of the 7 systems. Though the 7-sample based re-
sults are not reliable, we can get a sense of how
well SIA works in the system-level evaluation.
4.3 Components in SIA
To see how the three components in SIA con-
tribute to the final performance, we conduct exper-
iments where one or two components are removed
in SIA, shown in Table 7. The three components
are denoted as WLS (weighted loose sequence
alignment), PROB (stochastic word matching),
and INCS (iterative alignment scheme) respec-
tively. WLS without INCS does only one round
of alignment and chooses the best alignment score
as the final score. This scheme is similar to
ROUGE-W and METEOR. We can see that INCS,
as expected, improves the adequacy evaluation
without hurting the fluency evaluation. PROB
improves both adequacy and fluency evaluation
performance. The result that SIA works with
PORTER-STEM and WordNet is also shown in
Table 8. When PORTER-STEM and WordNet are
545
B-6 R 1 R 2 M S
F 0.514 0.466 0.458 0.378 0.532
A 0.876 0.900 0.906 0.875 0.928
O 0.794 0.790 0.792 0.741 0.835
Table 6: Results of BLEU, ROUGE, METEOR and SIA in system level evaluation
both used, PORTER-STEM is used first. We can
see that they are not as good as using the stochastic
word matching. Since INCS and PROB are inde-
pendent of WLS, we believe they can also be used
to improve other metrics such as ROUGE-W and
METEOR.
5 Conclusion
This paper describes a new metric SIA for MT
evaluation, which achieves good performance by
combining the advantages of n-gram-based met-
rics and loose-sequence-based metrics. SIA uses
stochastic word mapping to allow soft or partial
matches between the MT hypotheses and the ref-
erences. This stochastic component is shown to
be better than PORTER-STEM and WordNet in
our experiments. We also analyzed the effect of
other components in SIA and speculate that they
can also be used in other metrics to improve their
performance.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judegments. In Proceed-
ings of the ACL-04 workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Conference of the As-
sociation for Computational Linguistics (ACL-05).
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 16?23.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for machine translation. Technical report,
Center for Language and Speech Processing, Johns
Hopkins University, Baltimore. Summer Workshop
Final Report.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In In HLT 2002, Human Lan-
guage Technology Conference, San Diego, CA.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 388?395, Barcelona,
Spain, July.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-04), Barcelona, Spain.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In ACL
2005 Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of the 2003 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.
546
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Syntactic Features for Evaluation of Machine Translation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
Automatic evaluation of machine transla-
tion, based on computing n-gram similar-
ity between system output and human ref-
erence translations, has revolutionized the
development of MT systems. We explore
the use of syntactic information, includ-
ing constituent labels and head-modifier
dependencies, in computing similarity be-
tween output and reference. Our results
show that adding syntactic information
to the evaluation metric improves both
sentence-level and corpus-level correla-
tion with human judgments.
1 Introduction
Evaluation has long been a stumbling block in the
development of machine translation systems, due to
the simple fact that there are many correct transla-
tions for a given sentence. Human evaluation of sys-
tem output is costly in both time and money, leading
to the rise of automatic evaluation metrics in recent
years. The most commonly used automatic evalua-
tion metrics, BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002), are based on the assumption
that ?The closer a machine translation is to a profes-
sional human translation, the better it is? (Papineni
et al, 2002). For every hypothesis, BLEU computes
the fraction of n-grams which also appear in the ref-
erence sentences, as well as a brevity penalty. NIST
uses a similar strategy to BLEU but further consid-
ers that n-grams with different frequency should be
treated differently in the evaluation. It introduces the
notion of information weights, which indicate that
rarely occurring n-grams count more than those fre-
quently occurring ones in the evaluation (Dodding-
ton, 2002). BLEU and NIST have been shown to
correlate closely with human judgments in ranking
MT systems with different qualities (Papineni et al,
2002; Doddington, 2002).
In the 2003 Johns Hopkins Workshop on Speech
and Language Engineering, experiments on MT
evaluation showed that BLEU and NIST do not cor-
relate well with human judgments at the sentence
level, even when they correlate well over large test
sets (Blatz et al, 2003). Kulesza and Shieber (2004)
use a machine learning approach to improve the cor-
relation at the sentence level. Their method, based
on the assumption that higher classification accuracy
in discriminating human- from machine-generated
translations will yield closer correlation with hu-
man judgments, uses support vector machine (SVM)
based learning to weight multiple metrics such as
BLEU, NIST, and WER (minimal word error rate).
The SVM is trained for differentiating the MT hy-
pothesis and the professional human translations,
and then the distance from the hypothesis?s metric
vector to the hyper-plane of the trained SVM is taken
as the final score for the hypothesis.
While the machine learning approach improves
correlation with human judgments, all the metrics
discussed are based on the same type of information:
n-gram subsequences of the hypothesis translations.
This type of feature cannot capture the grammatical-
ity of the sentence, in part because they do not take
into account sentence-level information. For exam-
ple, a sentence can achieve an excellent BLEU score
without containing a verb. As MT systems improve,
the shortcomings of n-gram based evaluation are be-
coming more apparent. State-of-the-art MT output
25
reference: S
NP
PRON
VP
V NP
ART N
hypothesis 1: S
NP
PRON
VP
V NP
ART N
hypothesis 2: S
NP
ART N
NP
PRON
VP
V
Figure 1: Syntax Trees of the Examples
often contains roughly the correct words and con-
cepts, but does not form a coherent sentence. Often
the intended meaning can be inferred; often it can-
not. Evidence that we are reaching the limits of n-
gram based evaluation was provided by Charniak et
al. (2003), who found that a syntax-based language
model improved the fluency and semantic accuracy
of their system, but lowered their BLEU score.
With the progress of MT research in recent years,
we are not satisfied with the getting correct words
in the translations; we also expect them to be well-
formed and more readable. This presents new chal-
lenges to MT evaluation. As discussed above, the
existing word-based metrics can not give a clear
evaluation for the hypothesis? fluency. For exam-
ple, in the BLEU metric, the overlapping fractions
of n-grams with more than one word are considered
as a kind of metric for the fluency of the hypothesis.
Consider the following simple example:
Reference: I had a dog.
Hypothesis 1: I have the dog.
Hypothesis 2: A dog I had.
If we use BLEU to evaluate the two sentences, hy-
pothesis 2 has two bigrams a dog and I had which
are also found in the reference, and hypothesis 1 has
no bigrams in common with the reference. Thus hy-
pothesis 2 will get a higher score than hypothesis 1.
The result is obviously incorrect. However, if we
evaluate their fluency based on the syntactic simi-
larity with the reference, we will get our desired re-
sults. Figure 1 shows syntactic trees for the example
sentences, from which we can see that hypothesis 1
has exactly the same syntactic structure with the ref-
erence, while hypothesis 2 has a very different one.
Thus the evaluation of fluency can be transformed as
computing the syntactic similarity of the hypothesis
and the references.
This paper develops a number of syntactically
motivated evaluation metrics computed by automat-
ically parsing both reference and hypothesis sen-
tences. Our experiments measure how well these
metrics correlate with human judgments, both for in-
dividual sentences and over a large test set translated
by MT systems of varying quality.
2 Evaluating Machine Translation with
Syntactic Features
In order to give a clear and direct evaluation for the
fluency of a sentence, syntax trees are used to gen-
erate metrics based on the similarity of the MT hy-
pothesis?s tree and those of the references. We can?t
expect that the whole syntax tree of the hypothesis
can always be found in the references, thus our ap-
proach is to be based on the fractions of the subtrees
26
which also appear in the reference syntax trees. This
idea is intuitively derived from BLEU, but with the
consideration of the sparse subtrees which lead to
zero fractions, we average the fractions in the arith-
metic mean, instead of the geometric mean used
in BLEU. Then for each hypothesis, the fractions
of subtrees with different depths are calculated and
their arithmetic mean is computed as the syntax tree
based metric, which we denote as ?subtree metric?
STM:
STM = 1
D
D
?
n=1
?
t?subtreesn(hyp) countclip(t)
?
t?subtreesn(hyp) count(t)
where D is the maximum depth of subtrees con-
sidered, count(t) denotes the number of times sub-
tree t appears in the candidate?s syntax tree, and
countclip(t) denotes the clipped number of times
t appears in the references? syntax trees. Clipped
here means that, for a given subtree, the count com-
puted from the hypothesis syntax tree can not exceed
the maximum number of times the subtree occurs in
any single reference?s syntax tree. A simple exam-
ple with one hypothesis and one reference is shown
in Figure 2. Setting the maximum depth to 3, we
go through the hypothesis syntax tree and compute
the fraction of subtrees with different depths. For
the 1-depth subtrees, we get S, NP, VP, PRON, V,
NP which also appear in the reference syntax tree.
Since PRON only occurs once in the reference, its
clipped count should be 1 rather than 2. Then we
get 6 out of 7 for the 1-depth subtrees. For the 2-
depth subtrees, we get S?NP VP, NP?PRON, and
VP?V NP which also appear in the reference syntax
tree. For the same reason, the subtree NP?PRON
can only be counted once. Then we get 3 out of 4
for the 2-depth subtree. Similarly, the fraction of
3-depth subtrees is 1 out of 2. Therefore, the final
score of STM is (6/7+3/4+1/2)/3=0.702.
While the subtree overlap metric defined above
considers only subtrees of a fixed depth, subtrees of
other configurations may be important for discrimi-
nating good hypotheses. For example, we may want
to look for the subtree:
S
NP VP
V NP
to find sentences with transitive verbs, while ignor-
ing the internal structure of the subject noun phrase.
In order to include subtrees of all configurations in
our metric, we turn to convolution kernels on our
trees. Using H(x) to denote the vector of counts of
all subtrees found in tree x, for two trees T1 and T2,the inner product H(T1) ?H(T2) counts the numberof matching pairs of subtrees of T1 and T2. Collinsand Duffy (2001) describe a method for efficiently
computing this dot product without explicitly com-
puting the vectors H , which have dimensionality ex-
ponential in the size of the original tree. In order to
derive a similarity measure ranging from zero to one,
we use the cosine of the vectors H:
cos(T1, T2) =
H(T1) ?H(T2)
|H(T1)||H(T2)|
Using the identity
|H(T1)| =
?
H(T1) ?H(T1)
we can compute the cosine similarity using the ker-
nel method, without ever computing the entire of
vector of counts H . Our kernel-based subtree metric
TKM is then defined as the maximum of the cosine
measure over the references:
TKM = max
t?ref
cos(hyp, t)
The advantage of using the tree kernel is that it
can capture the similarity of subtrees of different
shapes; the weak point is that it can only use the
reference trees one by one, while STM can use them
simultaneously. The dot product also weights indi-
vidual features differently than our other measures,
which compute overlap in the same way as does
BLEU. For example, if the same subtree occurs 10
times in both the hypothesis and the reference, this
contributes a term of 100 to the dot product, rather
than 10 in the clipped count used by BLEU and by
our subtree metric STM.
2.1 Dependency-Based Metrics
Dependency trees consist of trees of head-modifier
relations with a word at each node, rather than just
at the leaves. Dependency trees were found to corre-
spond better across translation pairs than constituent
trees by Fox (2002), and form the basis of the ma-
chine translation systems of Alshawi et al (2000)
27
reference: S
NP
PRON
VP
V NP
ART N
hypothesis: S
NP
PRON
VP
V NP
PRON
Figure 2: Examples for the Computation of STM
and Lin (2004). We derived dependency trees from
the constituent trees by applying the determinis-
tic headword extraction rules used by the parser of
Collins (1999). For the example of the reference
syntax tree in Figure 2, the whole tree with the root
S represents a sentence; and the subtree NP?ART N
represents a noun phrase. Then for every node in the
syntax tree, we can determine its headword by its
syntactic structure; from the subtree NP?ART N,
for example, the headword selection rules chose the
headword of NP to be word corresponding to the
POS N in the subtree, and the other child, which cor-
responds to ART, is the modifier for the headword.
The dependency tree then is a kind of structure con-
stituted by headwords and every subtree represents
the modifier information for its root headword. For
example, the dependency tree of the sentence I have
a red pen is shown as below.
have
I pen
a red
The dependency tree contains both the lexical and
syntactic information, which inspires us to use it for
the MT evaluation.
Noticing that in a dependent tree the child
nodes are the modifier of its parent, we propose
a dependency-tree based metric by extracting the
headwords chains from both the hypothesis and the
reference dependency trees. A headword chain is
a sequence of words which corresponds to a path
in the dependency tree. Take the dependency tree
in Figure 2 as the example, the 2-word headword
chains include have I, have pen, pen a, and pen
red. Before using the headword chains, we need
to extract them out of the dependency trees. Fig-
ure 3 gives an algorithm which recursively extracts
the headword chains in a dependency tree from short
to long. Having the headword chains, the headword
chain based metric is computed in a manner similar
to BLEU, but using n-grams of dependency chains
rather than n-grams in the linear order of the sen-
tence. For every hypothesis, the fractions of head-
word chains which also appear in the reference de-
pendency trees are averaged as the final score. Using
HWCM to denote the headword chain based metric,
it is computed as follows:
HWCM = 1
D
D
?
n=1
?
g?chainn(hyp) countclip(g)
?
g?chainn(hyp) count(g)
where D is chosen as the maximum length chain
considered.
We may also wish to consider dependency rela-
tions over more than two words that are contigu-
ous but not in a single ancestor chain in the depen-
dency tree. For this reason, the two methods de-
scribed in section 3.1 are used to compute the simi-
larity of dependency trees between the MT hypothe-
sis and its references, and the corresponding metrics
are denoted DSTM for dependency subtree metric
and DTKM for dependency tree kernel metric.
3 Experiments
Our testing data contains two parts. One part is a set
of 665 English sentences generated by a Chinese-
English MT system. And for each MT hypothesis,
three reference translations are associated with it.
28
Input: dependency tree T, maximum length N of the headword chain
Output: headword chains from length 1 to N
for i = 1 to N
for every node n in T
if i == 1
add n?s word to n?s 1 word headword chains;
else
for every direct child c of n
for every i-1 words headword chain hc of c
newchain = joint(n?s word, hc);
add newchain to the i words headword chains of n;
endfor
endfor
endif
endfor
endfor
Figure 3: Algorithm for Extracting the Headword Chains
The human judgments, on a scale of 1 to 5, were col-
lected at the 2003 Johns Hopkins Speech and Lan-
guage Summer Workshop, which tells the overall
quality of the MT hypotheses. The translations were
generated by the alignment template system of Och
(2003). This testing set is called JHU testing set
in this paper. The other set of testing data is from
MT evaluation workshop at ACL05. Three sets of
human translations (E01, E03, E04) are selected as
the references, and the outputs of seven MT systems
(E9 E11 E12 E14 E15 E17 E22) are used for testing
the performance of our syntactic metrics. Each set
of MT translations contains 929 English sentences,
each of which is associated with human judgments
for its fluency and adequacy. The fluency and ade-
quacy scores both range from 1 to 5.
3.1 Sentence-level Evaluation
Our syntactic metrics are motivated by a desire to
better capture grammaticality in MT evaluation, and
thus we are most interested in how well they cor-
relate with human judgments of sentences? fluency,
rather than the adequacy of the translation. To
do this, the syntactic metrics (computed with the
Collins (1999) parser) as well as BLEU were used
to evaluate hypotheses in the test set from ACL05
MT workshop, which provides both fluency and ad-
equacy scores for each sentence, and their Pearson
coefficients of correlation with the human fluency
scores were computed. For BLEU and HWCM, in
order to avoid assigning zero scores to individual
MaxLength/Depth BLEU HWCM STM DSTM
1 0.126 0.130 ?? ??
2 0.132 0.142 0.142 0.159
3 0.117 0.157 0.147 0.150
4 0.093 0.153 0.136 0.121
kernel 0.065 0.090
Table 1: Correlation with Human Fluency Judg-
ments for E14
sentences, when precision for n-grams of a particu-
lar length is zero we replace it with an epsilon value
of 10?3. We choose E14 and E15 as two repre-
sentative MT systems in the ACL05 MT workshop
data set, which have relatively high human scores
and low human scores respectively. The results are
shown in Table 1 and Table 2, with every metric
indexed by the maximum n-gram length or subtree
depth. The last row of the each table shows the tree-
kernel-based measures, which have no depth param-
eter to adjust, but implicitly consider all depths.
The results show that in both systems our syntac-
tic metrics all achieve a better performance in the
correlation with human judgments of fluency. We
also notice that with the increasing of the maximum
length of n-grams, the correlation of BLEU with hu-
man judgments does not necessarily increase, but
decreases in most cases. This is contrary to the argu-
ment in BLEU which says that longer n-grams bet-
ter represent the sentences? fluency than the shorter
29
MaxLength/Depth BLEU HWCM STM DSTM
1 0.122 0.128 ?? ??
2 0.094 0.120 0.134 0.137
3 0.073 0.119 0.144 0.124
4 0.048 0.113 0.143 0.121
kernel 0.089 0.066
Table 2: Correlation with Human Fluency Judg-
ments for E15
ones. The problem can be explained by the limi-
tation of the reference translations. In our exper-
iments, every hypothesis is evaluated by referring
to three human translations. Since the three human
translations can only cover a small set of possible
translations, with the increasing of n-gram length,
more and more correct n-grams might not be found
in the references, so that the fraction of longer n-
grams turns to be less reliable than the short ones
and hurts the final scores. In the the corpus-level
evaluation of a MT system, the sparse data problem
will be less serious than in the sentence-level evalu-
ation, since the overlapping n-grams of all the sen-
tences and their references will be summed up. So
in the traditional BLEU algorithm used for corpus-
level evaluation, a maximum n-gram of length 4 or 5
is usually used. A similar trend can be found in syn-
tax tree and dependency tree based metrics, but the
decreasing ratios are much lower than BLEU, which
indicates that the syntactic metrics are less affected
by the sparse data problem. The poor performance
of tree-kernel based metrics also confirms our argu-
ments on the sparse data problem, since the kernel
measures implicitly consider the overlapping ratios
of the sub-trees of all shapes, and thus will be very
much affected by the sparse data problem.
Though our syntactic metrics are proposed for
evaluating the sentences? fluency, we are curious
how well they do in the overall evaluation of sen-
tences. Thus we also computed each metric?s cor-
relation with human overall judgments in E14, E15
and JHU testing set. The overall human score for
each sentence in E14 and E15 is computed by sum-
ming up its fluency score and adequacy score. The
results are shown in Table 3, Table 4, and Table
5. We can see that the syntactic metrics achieve
MaxLength/Depth BLEU HWCM STM DSTM
1 0.176 0.191 ?? ??
2 0.185 0.195 0.171 0.193
3 0.169 0.202 0.168 0.175
4 0.137 0.199 0.158 0.143
kernel 0.093 0.127
Table 3: Correlation with Human Overall Judgments
for E14
MaxLength/Depth BLEU HWCM STM DSTM
1 0.146 0.152 ?? ??
2 0.124 0.142 0.148 0.152
3 0.095 0.144 0.151 0.139
4 0.067 0.137 0.144 0.137
kernel 0.098 0.084
Table 4: Correlation with Human Overall Judgments
for E15
competitive correlations in the test, among which
HWCM, based on headword chains, gives better
performances in evaluation of E14 and E15, and a
slightly worse performance in JHU testing set than
BLEU. Just as with the fluency evaluation, HWCM
and other syntactic metrics present more stable per-
formance as the n-gram?s length (subtree?s depth)
increases.
3.2 Corpus-level Evaluation
While sentence-level evaluation is useful if we are
interested in a confidence measure on MT outputs,
corpus level evaluation is more useful for comparing
MaxLength/Depth BLEU HWCM STM DSTM
1 0.536 0.502 ?? ??
2 0.562 0.555 0.515 0.513
3 0.513 0.538 0.529 0.477
4 0.453 0.510 0.497 0.450
kernel 0.461 0.413
Table 5: Correlation with Human Overall Judgments
for JHU Testing Set
30
MaxLength/Depth BLEU HWCM STM DSTM
1 0.629 0.723 ?? ??
2 0.683 0.757 0.538 0.780
3 0.724 0.774 0.597 0.780
4 0.753 0.778 0.612 0.788
5 0.781 0.780 0.618 0.778
6 0.763 0.778 0.618 0.782
kernel 0.539 0.875
Table 6: Corpus-level Correlation with Human
Overall Judgments (E9 E11 E12 E14 E15 E17 E22)
MT systems and guiding their development. Does
higher sentence-level correlation necessarily indi-
cate higher correlation in corpus-level evaluation?
To answer this question, we used our syntactic met-
rics and BLEU to evaluate all the human-scored MT
systems (E9 E11 E12 E14 E15 E17 E22) in the
ACL05 MT workshop test set, and computed the
correlation with human overall judgments. The hu-
man judgments for an MT system are estimated by
summing up each sentence?s human overall score.
Table 6 shows the results indexed by different n-
grams and tree depths.
We can see that the corpus-level correlation and
the sentence-level correlation don?t always corre-
spond. For example, the kernel dependency subtree
metric achieves a very good performance in corpus-
level evaluation, but it has a poor performance in
sentence-level evaluation. Sentence-level correla-
tion reflects the relative qualities of different hy-
potheses in a MT system, which does not indicate
any information for the relative qualities of differ-
ent systems. If we uniformly decrease or increase
every hypothesis?s automatic score in a MT sys-
tem, the sentence-level correlation with human judg-
ments will remain the same, but the corpus-level cor-
relation will be changed. So we might possibly get
inconsistent corpus-level and sentence-level correla-
tions.
From the results, we can see that with the increase
of n-grams length, the performance of BLEU and
HWCM will first increase up to length 5, and then
starts decreasing, where the optimal n-gram length
of 5 corresponds to our usual setting for BLEU algo-
rithm. This shows that corpus-level evaluation, com-
pared with the sentence-level evaluation, is much
less sensitive to the sparse data problem and thus
leaves more space for making use of comprehen-
sive evaluation metrics. We speculate this is why the
kernel dependency subtree metric achieves the best
performance among all the metrics. We can also see
that HWCM and DSTM beat BLEU in most cases
and exhibit more stable performance.
An example hypothesis which was assigned a
high score by HWCM but a low score by BLEU is
shown in Table 7. In this particular sentence, the
common head-modifier relations ?aboard? plane?
and ?plane ? the? caused a high headword chain
overlap, but did not appear as common n-grams
counted by BLEU. The hypothesis is missing the
word ?fifth?, but was nonetheless assigned a high
score by human judges. This is probably due to its
fluency, which HWCM seems to capture better than
BLEU.
4 Conclusion
This paper introduces several syntax-based metrics
for the evaluation of MT, which we find to be par-
ticularly useful for predicting a hypothesis?s fluency.
The syntactic metrics, except the kernel based ones,
all outperform BLEU in sentence-level fluency eval-
uation. For the overall evaluation of sentences for
fluency and adequacy, the metric based on headword
chain performs better than BLEU in both sentence-
level and corpus-level correlation with human judg-
ments. The kernel based metrics, though poor in
sentence-level evaluation, achieve the best results in
corpus-level evaluation, where sparse data are less
of a barrier.
Our syntax-based measures require the existence
of a parser for the language in question, however it
is worth noting that a parser is required for the tar-
get language only, as all our measures of similarity
are defined across hypotheses and references in the
same language.
Our results, in particular for the primarily struc-
tural STM, may be surprising in light of the fact
that the parser is not designed to handle ill-formed
or ungrammatical sentences such as those produced
by machine translation systems. Modern statistical
parsers have been tuned to discriminate good struc-
tures from bad rather than good sentences from bad.
31
hyp Diplomats will be aboard the plane to return home .
ref1 Diplomats are to come back home aboard the fifth plane .
ref2 Diplomatic staff would go home in a fifth plane .
ref3 Diplomatic staff will take the fifth plane home .
Table 7: An example hypothesis in the ACL05-MTE workshop which was assigned a high score by HWCM
(0.511) but a low score by BLEU (0.084). Both human judges assigned a high score (4).
Indeed, in some recent work on re-ranking machine
translation hypotheses (Och et al, 2004), parser-
produced structures were not found to provide help-
ful information, as a parser is likely to assign a good-
looking structure to even a lousy input hypothesis.
However, there is an important distinction be-
tween the use of parsers in re-ranking and evaluation
? in the present work we are looking for similarities
between pairs of parse trees rather than at features
of a single tree. This means that the syntax-based
evaluation measures can succeed even when the tree
structure for a poor hypothesis looks reasonable on
its own, as long as it is sufficiently distinct from the
structures used in the references.
We speculate that by discriminatively training
weights for the individual subtrees and headword
chains used by the syntax-based metrics, further im-
provements in evaluation accuracy are possible.
Acknowledgments We are very grateful to Alex
Kulesza for assistance with the JHU data. This work
was partially supported by NSF ITR IIS-09325646
and NSF ITR IIS-0428020.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Univer-
sity, Baltimore. Summer Workshop Final Report.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for machine
translation. In Proc. MT Summit IX.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In In HLT 2002, Human Language Technology
Conference, San Diego, CA.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2002), pages 304?311.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), Baltimore, MD, October.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), pages 625?630, Geneva, Switzerland.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the 2004 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-04), Boston.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Conference of the Association for Com-
putational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.
32
Proceedings of the Third Workshop on Statistical Machine Translation, pages 62?69,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improved Tree-to-string Transducer for Machine Translation
Ding Liu and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We propose three enhancements to the tree-
to-string (TTS) transducer for machine trans-
lation: first-level expansion-based normaliza-
tion for TTS templates, a syntactic align-
ment framework integrating the insertion of
unaligned target words, and subtree-based n-
gram model addressing the tree decomposi-
tion probability. Empirical results show that
these methods improve the performance of a
TTS transducer based on the standard BLEU-
4 metric. We also experiment with semantic
labels in a TTS transducer, and achieve im-
provement over our baseline system.
1 Introduction
Syntax-based statistical machine translation
(SSMT) has achieved significant progress during
recent years, with two threads developing simul-
taneously: the synchronous parsing-based SSMT
(Galley et al, 2006; May and Knight, 2007) and
the tree-to-string (TTS) transducer (Liu et al,
2006; Huang et al, 2006). Synchronous SSMT
here denotes the systems which accept a source
sentence as the input and generate the translation
and the syntactic structure for both the source and
the translation simultaneously. Such systems are
sometimes also called TTS transducers, but in this
paper, TTS transducer refers to the system which
starts with the syntax tree of a source sentence and
recursively transforms the tree to the target language
based on TTS templates.
In synchronous SSMT, TTS templates are used
similar to the context free grammar used in the stan-
dard CYK parser, thus the syntax is part of the output
and can be thought of as a constraint on the transla-
tion process. In the TTS transducer, since the parse
tree is given, syntax can be thought of as an addi-
tional feature of the input to be used in the transla-
tion. The idea of synchronous SSMT can be traced
back to Wu (1997)?s Stochastic Inversion Transduc-
tion Grammars. A systematic method for extract-
ing TTS templates from parallel corpora was pro-
posed by Galley et al (2004), and later binarized
by Zhang et al (2006) for high efficiency and ac-
curacy. In the other track, the TTS transducer orig-
inated from the tree transducer proposed by Rounds
(1970) and Thatcher (1970) independently. Graehl
and Knight (2004) generalized the tree transducer
to the TTS transducer and introduced an EM al-
gorithm to estimate the probability of TTS tem-
plates based on a bilingual corpus with one side
parsed. Liu et al (2006) and Huang et al (2006)
then used the TTS transducer on the task of Chinese-
to-English and English-to-Chinese translation, re-
spectively, and achieved decent performance.
Despite the progress SSMT has achieved, it is
still a developing field with many problems un-
solved. For example, the word alignment com-
puted by GIZA++ and used as a basis to extract
the TTS templates in most SSMT systems has been
observed to be a problem for SSMT (DeNero and
Klein, 2007; May and Knight, 2007), due to the
fact that the word-based alignment models are not
aware of the syntactic structure of the sentences and
could produce many syntax-violating word align-
ments. Approaches have been proposed recently to-
wards getting better word alignment and thus bet-
ter TTS templates, such as encoding syntactic struc-
ture information into the HMM-based word align-
ment model DeNero and Klein (2007), and build-
62
ing a syntax-based word alignment model May
and Knight (2007) with TTS templates. Unfortu-
nately, neither approach reports end-to-end MT per-
formance based on the syntactic alignment. DeN-
ero and Klein (2007) focus on alignment and do not
present MT results, while May and Knight (2007)
takes the syntactic re-alignment as an input to an EM
algorithm where the unaligned target words are in-
serted into the templates and minimum templates are
combined into bigger templates (Galley et al, 2006).
Thus the improvement they reported is rather indi-
rect, leading us to wonder how much improvement
the syntactic alignment model can directly bring to a
SSMT system. Some other issues of SSMT not fully
addressed before are highlighted below:
1. Normalization of TTS templates. Galley et
al. (2006) mentioned that with only the mini-
mum templates extracted from GHKM (Galley
et al, 2004), normalizing the template proba-
bility based on its tree pattern ?can become ex-
tremely biased?, due to the fact that bigger tem-
plates easily get high probabilities. They in-
stead use a joint model where the templates are
normalized based on the root of their tree pat-
terns and show empirical results for that. There
is no systematic comparison of different nor-
malization methods.
2. Decomposition model of a TTS transducer
(or syntactic language model in synchronous
SSMT). There is no explicit modeling for the
decomposition of a syntax tree in the TTS
transducer (or the probability of the syntactic
tree in a synchronous SSMT). Most systems
simply use a uniform model (Liu et al, 2006;
Huang et al, 2006) or implicitly consider it
with a joint model producing both syntax trees
and the translations (Galley et al, 2006).
3. Use of semantics. Using semantic features in
a SSMT is a natural step along the way to-
wards generating more refined models across
languages. The statistical approach to semantic
role labeling has been well studied (Xue and
Palmer, 2004; Ward et al, 2004; Toutanova et
al., 2005), but there is no work attempting to
use such information in SSMT, to our limited
knowledge.
This paper proposes novel methods towards solv-
ing these problems. Specifically, we compare three
ways of normalizing the TTS templates based on the
tree pattern, the root of the tree pattern, and the first-
level expansion of the tree pattern respectively, in
the context of hard counting and EM estimation; we
present a syntactic alignment framework integrating
both the template re-estimation and insertion of un-
aligned target words; we use a subtree-based n-gram
model to address the decomposition of the syntax
trees in TTS transducer (or the syntactic language
model for synchronous SSMT); we use a statistical
classifier to label the semantic roles defined by Prop-
Bank (Palmer et al, 2005) and try different ways of
using the semantic features in a TTS transducer.
We chose the TTS transducer instead of syn-
chronous SSMT for two reasons. First, the decoding
algorithm for the TTS transducer has lower compu-
tational complexity, which makes it easier to inte-
grate a complex decomposition model. Second, the
TTS Transducer can be easily integrated with se-
mantic role features since the syntax tree is present,
and it?s not clear how to do this in a synchronous
SSMT system. The remainder of the paper will
focus on introducing the improved TTS transducer
and is organized as follows: Section 2 describes the
implementation of a basic TTS transducer; Section
3 describes the components of the improved TTS
transducer; Section 4 presents the empirical results
and Section 5 gives the conclusion.
2 A Basic Tree-to-string Transducer for
Machine Translation
The TTS transducer, as a generalization to the finite
state transducer, receives a tree structure as its input
and recursively applies TTS templates to generate
the target string. For simplicity, usually only one
state is used in the TTS transducer, i.e., a TTS tem-
plate will always lead to the same outcome wher-
ever it is used. A TTS template is composed of a
left-hand side (LHS) and a right-hand side (RHS),
where LHS is a subtree pattern and RHS is a se-
quence of the variables and translated words. The
variables in the RHS of a template correspond to the
bottom level non-terminals in the LHS?s subtree pat-
tern, and their relative order indicates the permuta-
tion desired at the point where the template is ap-
63
SQ
AUX  NP1 RB  VP2  ?3 
  Is            not
NP1 ?? VP2  ?3
Figure 1: A TTS Template Example
SQ
AUX  NP  1R  BP   V 
2?
3SQ 3AUX I?s NPn 31R ot ? s BP ?   V? s ? ?  NPn  ?? BP ?  V?
NN
? t ?
? ?
V? ? ?
BRN
ot ? ? IoI?? ? ?
3NP 3? ?  ? ? ? s 3NN ? t ? ss ? ?  ??3BP BRNns ? ?  BRNn3BRN ? IoI?? ? ? s ? ?  ??3V Vs ? ?  ?
    ??     ?? ?? ?
Figure 2: Derivation Example
plied to translate one language to another. The vari-
ables are further transformed and the recursive pro-
cess goes on until there are no variables left. The
formal description of a TTS transducer is described
in Graehl and Knight (2004), and our baseline ap-
proach follows the Extended Tree-to-String Trans-
ducer defined in (Huang et al, 2006). Figure 1 gives
an example of the English-to-Chinese TTS template,
which shows how to translate a skeleton YES/NO
question from English to Chinese. NP 1 and V P 2
are the variables whose relative position in the trans-
lation are determined by the template while their ac-
tual translations are still unknown and dependent on
the subtrees rooted at them; and the English words Is
and not are translated into the Chinese word MeiYou
in the context of the template. The superscripts at-
tached on the variables are used to distinguish the
non-terminals with identical names (if there is any).
Figure 2 shows the steps of transforming the English
sentence ?Is the job not finished ?? to the corre-
sponding Chinese.
For a given derivation (decomposition) of a syn-
tax tree, the translation probability is computed as
the product of the templates which generate both
the source syntax trees and the target translations.
In theory, the translation model should sum over
all possible derivations generating the target transla-
tion, but in practice, usually only the best derivation
is considered:
Pr(S|T,D?) =
?
t?D?
Weight(t)
Here, S denotes the target translation, T denotes the
source syntax tree, and D? denotes the best deriva-
tion of T . The implementation of a TTS trans-
ducer can be done either top down with memoiza-
tion to the visited subtrees (Huang et al, 2006), or
with a bottom-up dynamic programming (DP) algo-
rithm (Liu et al, 2006). This paper uses the lat-
ter approach, and the algorithm is sketched in Fig-
ure 3. For the baseline approach, only the translation
model and n-gram model for the target language are
used:
S? = argmax
S
Pr(T |S) = argmax
S
Pr(S)Pr(S|T )
Since the n-gram model tends to favor short transla-
tions, a penalty is added to the translation templates
with fewer RHS symbols than LHS leaf symbols:
Penalty(t) = exp(|t.RHS| ? |t.LHSLeaf |)
where |t.RHS| denotes the number of symbols in
the RHS of t, and |t.LHSLeaf | denotes the num-
ber of leaves in the LHS of t. The length penalty is
analogous to the length feature widely used in log-
linear models for MT (Huang et al, 2006; Liu et al,
2006; Och and Ney, 2004). Here we distribute the
penalty into TTS templates for the convenience of
DP, so that we don?t have to generate the N -best list
and do re-ranking. To speed up the decoding, stan-
dard beam search is used.
In Figure 3, BinaryCombine denotes the target-
size binarization (Huang et al, 2006) combination.
The translation candidates of the template?s vari-
ables, as well as its terminals, are combined pair-
wise in the order they appear in the RHS of the
template. fi denotes a combined translation, whose
probability is equal to the product of the probabili-
ties of the component translations, the probability of
the rule, the n-gram probability of connecting the
component translations, and the length penalty of
64
Match(v, t): the descendant tree nodes of v, which match the variables in template t
v.sk: the stack associated with tree node v
In(cj , fi): the translation candidate of cj which is chosen to combine fi
???????????????????????????????????
for all tree node v in bottom-up order do
for all template t applicable at v do
{c1, c2, ..., cl}=Match(v, t);
{f1, f2, ..., fm} = BinaryCombine(c1.sk, c2.sk, ..., cn.sk, t);
for i=1:m do
Pr(fi) =
?l
j=1Pr(In(cj , fi)) ? Weight(t)
? ? Lang(v, t, fi)? ? Penalty(t)?;
Add (fi, P r(fi)) to v.sk;
Prune v.sk;
Figure 3: Decoding Algorithm
the template. ?, ? and ? are the weights of the length
penalty, the translation model, and the n-gram lan-
guage model, respectively. Each state in the DP
chart denotes the best translation of a tree node with
a certain prefix and suffix. The length of the pre-
fix and the suffix is equal to the length of the n-gram
model minus one. Without the beam pruning, the de-
coding algorithm runs in O(N4(n?1)RPQ), where
N is the vocabulary size of the target language, n is
the length of the n-gram model, R is the maximum
number of templates applicable to one tree node, P
is the maximum number of variables in a template,
and Q is the number of tree nodes in the syntax tree.
The DP algorithm works for most systems in the pa-
per, and only needs to be slightly modified to en-
code the subtree-based n-gram model described in
Section 3.3.
3 Improved Tree-to-string Transducer for
Machine Translation
3.1 Normalization of TTS Templates
Given the story that translations are generated based
on the source syntax trees, the weight of the template
is computed as the probability of the target strings
given the source subtree:
Weight(t) =
#(t)
#(t? : LHS(t?) = LHS(t))
Such normalization, denoted here as TREE, is used
in most tree-to-string template-based MT systems
(Liu et al, 2007; Liu et al, 2006; Huang et al,
2006). Galley et al (2006) proposed an alteration
in synchronous SSMT which addresses the proba-
bility of both the source subtree and the target string
given the root of the source subtree:
Weight(t) =
#(t)
#(t? : root(t?) = root(t))
This method is denoted as ROOT. Here, we propose
another modification:
Weight(t) =
#(t)
#(t? : cfg(t?) = cfg(t))
(1)
cfg in Equation 1 denotes the first level expansion
of the source subtree and the method is denoted as
CFG. CFG can be thought of as generating both the
source subtree and the target string given the first
level expansion of the source subtree. TREE focuses
on the conditional probability of the target string
given the source subtree, ROOT focuses on the joint
probability of both the source subtree and the target
string, while CFG, as something of a compromise
between TREE and ROOT, hopefully can achieve a
combined effect of both of them. Compared with
TREE, CFG favors the one-level context-free gram-
mar like templates and gives penalty to the templates
bigger (in terms of the depth of the source subtree)
than that. It makes sense considering that the big
templates, due to their sparseness in the corpus, are
often assigned unduly large probabilities by TREE.
3.2 Syntactic Word Alignment
The idea of building a syntax-based word alignment
model has been explored byMay and Knight (2007),
with an algorithm working from the root tree node
down to the leaves, recursively replacing the vari-
ables in the matched tree-to-string templates until
there are no such variables left. The TTS tem-
plates they use are initially gathered using GHKM
65
1. Run GIZA++ to get the initial word alignment, use
GHKM to gather translation templates, and com-
pute the initial probability as their normalized fre-
quency.
2. Collect all the one-level subtrees in the training cor-
pus containing only non-terminals and create TTS
templates addressing all the permutations of the
subtrees? leaves if its spanning factor is not greater
than four, or only the monotonic translation tem-
plate if its spanning factor is greater than four. Col-
lect all the terminal rules in the form of A ? B
where A is one source word, B is the consecutive
target word sequence up to three words long, and
A, B occurs in some sentence pairs. These extra
templates are assigned a small probability 10?6.
3. Run the EM algorithm described in (Graehl and
Knight, 2004) with templates obtained in step 1 and
step 2 to re-estimate their probabilities.
4. Use the templates from step 3 to compute the viterbi
word alignment.
5. The templates not occurring in the viterbi deriva-
tion are ignored and the probability of the remain-
ing ones are re-normalized based on their frequency
in the viterbi derivation.
Figure 4: Steps generating the refined TTS templates
(Galley et al, 2004) with the word alignment com-
puted by GIZA++ and re-estimated using EM, ig-
noring the alignment from Giza++. The refined
word alignment is then fed to the expanded GHKM
(Galley et al, 2006), where the TTS templates will
be combined with the unaligned target words and
re-estimated in another EM framework. The syn-
tactic alignment proposed here shares the essence of
May and Knight?s approach, but combines the re-
estimation of the TTS templates and insertion of the
unaligned target words into a single EM framework.
The process is described in Figure 4. The inser-
tion of the unaligned target words is done implicitly
as we include the extra terminal templates in Fig-
ure 4, and the extra non-terminal templates ensure
that we can get a complete derivation forest in the
EM training. The last viterbi alignment step may
seem unnecessary given that we already have the
EM-estimated templates, but in experiments we find
that it produces better result by cutting off the noisy
(usually very big) templates resulting from the poor
alignments of GIZA++.
3.3 Tree Decomposition Model
A deficiency of the translation model for tree-to-
string transducer is that it cannot fully address
the decomposition probability of the source syntax
trees. Though we can say that ROOT/CFG implic-
itly includes the decomposition model, a more di-
rect and explicit modeling of the decomposition is
still desired. Here we propose a novel n-gram-like
model to solve this problem. The probability of a
decomposition (derivation) of a syntax tree is com-
puted as the product of the n-gram probability of
the decomposed subtrees conditioned on their ascen-
dant subtrees. The formal description of the model
is in Equation 2, whereD denotes the derivation and
PT (st) denotes the direct parent subtree of st.
Pr(D|T ) =
?
subtrees
st?D
Pr(st|PT (st), PT (PT (st)), ...)
(2)
Now, with the decomposition model added in, the
probability of the target string given the source syn-
tax tree is computed as:
Pr(S|T ) = Pr(D?|T )? Pr(S|T,D?)
To encode this n-gram probability of the subtrees
in the decoding process, we need to expand the
state space of the dynamic programming algorithm
in Figure 3, so that each state represents not only
the prefix/suffix of the partial translation, but also
the decomposition history of a tree node. For ex-
ample, with a bigram tree model, the states should
include the different subtrees in the LHS of the tem-
plates used to translate a tree node. With bigger n-
grams, more complex history information should be
encoded in the states, and this leads to higher com-
putational complexity. In this paper, we only con-
sider the tree n-gram up to size 2. It is not practi-
cal to search the full state space; instead, we mod-
ify the beam search algorithm in Figure 3 to encode
the decomposition history information. The mod-
ified algorithm for the tree bigram creates a stack
for each tree pattern occurring in the templates ap-
plicable to a tree node. This ensures that for each
tree node, the decompositions headed with differ-
ent subtrees have equal number of translation can-
didates surviving to the upper phase. The function
66
SQAUXXS NP1URBV21URU??V?3I sRnoB
t V? n?U? VB
? ? ? RU3t V? n?U? VB ? P ? Q? VR? Q
2V? s2VR
?U? ? oU? V3? s2V?? RU? B?U? Qs? 3? 2V?
2V? s? nsBQ? s? 3? s2V?
? ? ? ? P
t V? n?U? VB
? sR? U?QAV
? o? ? ? RVV? Q? RU?
Figure 5: Flow graph of the system with all components
integrated
BinaryCombine is almost the same as in Figure 3,
except that the translation candidates (states) of each
tree node are grouped according to their associated
subtrees. The bigram probabilities of the subtrees
can be easily computed with the viterbi derivation in
last subsection. Also, a weight should be assigned
to this component. This tree n-gram model can be
easily adapted and used in synchronous SSMT sys-
tems such as May and Knight (2007), Galley et al
(2006). The flow graph of the final system with all
the components integrated is shown in Figure 5.
3.4 Use of Semantic Roles
Statistical approaches to MT have gone through
word-based systems, phrase-based systems, and
syntax-based systems. The next generation would
seem to be semantic-based systems. We use Prop-
Bank (Palmer et al, 2005) as the semantic driver in
our TTS transducer because it is built upon the same
corpus (the Penn Treebank) used to train the statisti-
cal parser, and its shallow semantic roles are more
easily integrated into a TTS transducer. A Max-
Entropy classifier, with features following Xue and
Palmer (2004) andWard et al (2004), is used to gen-
erate the semantic roles for each verb in the syntax
trees. We then replace the syntactic labels with the
semantic roles so that we have more general tree la-
bels, or combine the semantic roles with the syntac-
tic labels to generate more refined tree node labels.
Though semantic roles are associated with the verbs,
it is not feasible to differentiate the roles of different
NP VP VP NP
(S NP-agent VP) 0.983 0.017
(S NP-patient VP) 0.857 0.143
Table 1: The TREE-based weights of the skeleton tem-
plates with NP in different roles
verbs due to the data sparseness problem. If some
tree nodes are labeled different roles for different
verbs, those semantic roles will be ignored.
A simple example demonstrating the need for se-
mantics in the TTS transducer is that in English-
Chinese translation, the NP VP skeleton phrase is
more likely to be inverted when NP is in a patient
role than when it is in an agent role. Table 1 shows
the TREE-based weights of the 4 translation tem-
plates, computed based on our training corpus. This
shows that the difference caused by the roles of NP
is significant.
4 Experiment
We used 74,597 pairs of English and Chinese sen-
tences in the FBIS data set as our experimental
data, which are further divided into 500 test sen-
tence pairs, 500 development sentence pairs and
73597 training sentence pairs. The test set and de-
velopment set are selected as those sentences hav-
ing fewer than 25 words on the Chinese side. The
translation is from English to Chinese, and Char-
niak (2000)?s parser, trained on the Penn Treebank,
is used to generate the syntax trees for the English
side. The weights of the MT components are op-
timized based on the development set using a grid-
based line search. The Chinese sentence from the se-
lected pair is used as the single reference to tune and
evaluate the MT system with word-based BLEU-4
(Papineni et al, 2002). Huang et al (2006) used
character-based BLEU as a way of normalizing in-
consistent Chinese word segmentation, but we avoid
this problem as the training, development, and test
data are from the same source.
4.1 Syntax-Based System
The decoding algorithm described in Figure 3 is
used with the different normalization methods de-
scribed in Section 3.1 and the results are summa-
rized in Table 2. The TTS templates are extracted
using GHKM based on the many-to-one alignment
67
Baseline Syntactic Alignment Subtree bigram
dev test dev test dev test
TREE 12.29 8.90 13.25 9.65 14.84 10.61
ROOT 12.41 9.66 13.72 10.16 14.24 10.66
CFG 13.27 9.69 14.32 10.29 15.30 10.99
PHARAOH 9.04 7.84
Table 2: BLEU-4 scores of various systems with the syntactic alignment and subtree bigram improvements added
incrementally.
from Chinese to English obtained from GIZA++.
We have tried using alignment in the reverse direc-
tion and the union of both directions, but neither
of them is better than the Chinese-to-English align-
ment. The reason, based on the empirical result,
is simply that the Chinese-to-English alignments
lead to the maximum number of templates using
GHKM. A modified Kneser-Ney bigram model of
the Chinese sentence is trained using SRILM (Stol-
cke, 2002) using the training set. For comparison,
results for Pharaoh (Koehn, 2004), trained and tuned
under the same condition, are also shown in Table 2.
The phrases used in Pharaoh are extracted as the pair
of longest continuous spans in English and Chinese
based on the union of the alignments in both direc-
tion. We tried using alignments of different direc-
tions with Pharaoh, and find that the union gives
the maximum number of phrase pairs and the best
BLEU scores. The results show that the TTS trans-
ducers all outperform Pharaoh, and among them, the
one with CFG normalization works better than the
other two.
We tried the three normalization methods in the
syntactic alignment process in Figure 4, and found
that the initialization (step 1) and viterbi alignment
(step 3 and 4) based on the least biased model
ROOT gave the best performance. Table 2 shows
the results with the final template probability re-
normalized (step 5) using TREE, ROOT and CFG
respectively. We can see that the syntactic align-
ment brings a reasonable improvement for the TTS
transducer no matter what normalization method is
used. To test the effect of the subtree-based n-
gram model, SRILM is used to compute a modi-
fied Kneser-Ney bigram model for the subtree pat-
terns used in the viterbi alignment. The last 3 lines
in Table 2 show the improved results by further in-
corporating the subtree-based bigram model. We
can see that the difference of the three normaliza-
tion methods is lessened and TREE, the weakest nor-
malization in terms of addressing the decomposition
probability, gets the biggest improvement with the
subtree-based bigram model added in.
4.2 Semantic-Based System
Following the standard division, our max-entropy
based SRL classifier is trained and tuned using sec-
tions 2-21 and section 24 of PropBank, respectively.
The F-score we achieved on section 23 is 88.70%.
We repeated the experiments in last section with
the semantic labels generated by the SRL classi-
fier. Table 3 shows the results, comparing the non-
semantic-based systems with similar systems us-
ing the refined and general semantic labels, respec-
tively. Unfortunately, semantic based systems do
not always outperform the syntactic based systems.
We can see that for the baseline systems based on
TREE and ROOT, semantic labels improve the re-
sults, while for the other systems, they are not re-
ally better than the syntactic labels. Our approach
to semantic roles is preliminary; possible improve-
ments include associating role labels with verbs and
backing off to the syntactic-label based models from
semantic-label based TTS templates. In light of our
results, we are optimistic that more sophisticated
use of semantic features can further improve a TTS
transducer?s performance.
5 Conclusion
This paper first proposes three enhancements to the
TTS transducer: first-level expansion-based normal-
ization for TTS templates, a syntactic alignment
framework integrating the insertion of unaligned tar-
get words, and a subtree-based n-gram model ad-
dressing the tree decomposition probability. The ex-
periments show that the first-level expansion-based
68
No Semantic Labels Refined Labels General Labels
Syntactic Subtree Syntactic Subtree Syntactic Subtree
Baseline Alignment Bigram Baseline Alignment Bigram Baseline Alignment Bigram
TREE 8.90 9.65 10.61 9.40 10.25 10.42 9.40 10.02 10.47
ROOT 9.66 10.16 10.66 9.89 10.32 10.43 9.82 10.17 10.42
CFG 9.69 10.29 10.99 9.66 10.16 10.33 9.58 10.25 10.59
Table 3: BLEU-4 scores of semantic-based systems on test data. As in Table 2, the syntactic alignment and subtree
bigram improvements are added incrementally within each condition.
normalization for TTS templates is better than the
root-based one and the tree-based one; the syntactic
alignment framework and the n-gram based tree de-
composition model both improve a TTS transducer?s
performance. Our experiments using PropBank se-
mantic roles in the TTS transducer show that the ap-
proach has potential, improving on our baseline sys-
tem. However, adding semantic roles does not im-
prove our best TTS system.
References
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL-07, pages 17?24.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL-06, pages 961?968, July.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In The Sixth Conference of the Association for
Machine Translation in the Americas, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL-06, Sydney,
Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL-07, Prague.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of
EMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL-
02.
William C. Rounds. 1970. Mappings and grammars on
trees. Mathematical Systems Theory, 4(3):257?287.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, volume 2, pages 901?904.
J. W. Thatcher. 1970. Generalized2 sequential machine
maps. J. Comput. System Sci., 4:339?367.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of ACL-05, pages 589?596.
Wayne Ward, Kadri Hacioglu, James Martin, , and Dan
Jurafsky. 2004. Shallow semantic parsing using sup-
port vector machines. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of NAACL-06, pages 256?
263.
69
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 716?724,
Beijing, August 2010
Semantic Role Features for Machine Translation
Ding Liu
Department of Computer Science
University of Rochester
Daniel Gildea
Department of Computer Science
University of Rochester
Abstract
We propose semantic role features for a
Tree-to-String transducer to model the re-
ordering/deletion of source-side semantic
roles. These semantic features, as well as
the Tree-to-String templates, are trained
based on a conditional log-linear model
and are shown to significantly outperform
systems trained based on Max-Likelihood
and EM. We also show significant im-
provement in sentence fluency by using
the semantic role features in the log-linear
model, based on manual evaluation.
1 Introduction
Syntax-based statistical machine translation
(SSMT) has achieved significant progress during
recent years (Galley et al, 2006; May and
Knight, 2007; Liu et al, 2006; Huang et al,
2006), showing that deep linguistic knowledge,
if used properly, can improve MT performance.
Semantics-based SMT, as a natural extension
to SSMT, has begun to receive more attention
from researchers (Liu and Gildea, 2008; Wu
and Fung, 2009). Semantic structures have two
major advantages over syntactic structures in
terms of helping machine translation. First of all,
semantic roles tend to agree better between two
languages than syntactic constituents (Fung et al,
2006). This property motivates the approach of
using the consistency of semantic roles to select
MT outputs (Wu and Fung, 2009). Secondly,
the set of semantic roles of a predicate models
the skeleton of a sentence, which is crucial to
the readability of MT output. By skeleton, we
mean the main structure of a sentence including
the verbs and their arguments. In spite of the
theoretical potential of the semantic roles, there
has not been much success in using them to
improve SMT systems.
Liu and Gildea (2008) proposed a semantic role
based Tree-to-String (TTS) transducer by adding
semantic roles to the TTS templates. Their ap-
proach did not differentiate the semantic roles of
different predicates, and did not always improve
the TTS transducer?s performance. Wu and Fung
(2009) took the output of a phrase-based SMT sys-
tem Moses (Koehn et al, 2007), and kept permut-
ing the semantic roles of the MT output until they
best matched the semantic roles in the source sen-
tence. This approach shows the positive effect of
applying semantic role constraints, but it requires
re-tagging semantic roles for every permuted MT
output and does not scale well to longer sentences.
This paper explores ways of tightly integrating
semantic role features (SRFs) into an MT system,
rather than using them in post-processing or n-
best re-ranking. Semantic role labeling (SRL) sys-
tems usually use sentence-wide features (Xue and
Palmer, 2004; Pradhan et al, 2004; Toutanova et
al., 2005); thus it is difficult to compute target-
side semantic roles incrementally during decoding.
Noticing that the source side semantic roles are
easy to compute, we apply a compromise approach,
where the target side semantic roles are generated
by projecting the source side semantic roles us-
ing the word alignments between the source and
target sentences. Since this approach does not per-
form true SRL on the target string, it cannot fully
evaluate whether the source and target semantic
structures are consistent. However, the approach
does capture the semantic-level re-ordering of the
sentences. We assume here that the MT system is
capable of providing word alignment (or equiva-
lent) information during decoding, which is gener-
ally true for current statistical MT systems.
Specifically, two types of semantic role features
are proposed in this paper: a semantic role re-
ordering feature designed to capture the skeleton-
level permutation, and a semantic role deletion fea-
716
ture designed to penalize missing semantic roles in
the target sentence. To use these features during de-
coding, we need to keep track of the semantic role
sequences (SRS) for partial translations, which can
be generated based on the source-side semantic
role sequence and the corresponding word align-
ments. Since the SRL system and the MT sys-
tem are separate, a translation rule (e.g., a phrase
pair in phrase-based SMT) could cover two partial
source-side semantic roles. In such cases partial
SRSs must be recorded in such a way that they can
be combined later with other partial SRSs. Deal-
ing with this problem will increase the complexity
of the decoding algorithm. Fortunately, Tree-to-
String transducer based MT systems (Liu et al,
2006; Huang et al, 2006) can avoid this problem
by using the same syntax tree for both SRL and
MT. Such an arrangement guarantees that a TTS
template either covers parts of one source-side se-
mantic role, or a few complete semantic roles. This
advantage motivates us to use a TTS transducer as
the MT system with which to demonstrate the use
of the proposed semantic role features. Since it is
hard to design a generative model to combine both
the semantic role features and the TTS templates,
we use a log-linear model to estimate the feature
weights, by maximizing the conditional probabil-
ities of the target strings given the source syntax
trees. The log-linear model with latent variables
has been discussed by Blunsom et al (2008); we
apply this technique to combine the TTS templates
and the semantic role features.
The remainder of the paper is organized as fol-
lows: Section 2 describes the semantic role fea-
tures proposed for machine translation; Section 3
describes how semantic role features are used and
trained in a TTS transducer; Section 4 presents
the experimental results; and Section 5 gives the
conclusion.
2 Semantic Role Features for Machine
Translation
2.1 Defining Semantic Roles
There are two semantic standards with publicly
available training data: PropBank (Palmer et al,
2005) and FrameNet (Johnson et al, 2002). Prop-
Bank defines a set of semantic roles for the verbs
in the Penn TreeBank using numbered roles. These
roles are defined individually for each verb. For
example, for the verb disappoint, the role name
arg1 means experiencer, but for the verb wonder,
role name arg1 means cause. FrameNet is moti-
vated by the idea that a certain type of verbs can
be gathered together to form a frame, and in the
same frame, a set of semantic roles is defined and
shared among the verbs. For example, the verbs
boil, bake, and steam will be in frame apply heat,
and they have the semantic roles of cook, food, and
heating instrument. Of these two semantic stan-
dards, we choose PropBank over FrameNet for the
following reasons:
1. PropBank has a simpler semantic definition
than FrameNet and thus is easier for auto-
matic labeling.
2. PropBank is built upon the Penn TreeBank
and is more consistent with statistical parsers,
most of which are trained on the Penn Tree-
Bank.
3. PropBank is a larger corpus than FrameNet.
Note that the semantic standard/corpus is not cru-
cial in this paper. Any training corpus that can be
used to automatically obtain the set of semantic
roles of a verb could be used in our approach.
2.2 Semantic Role Features
Ideally, we want to use features based on the true
semantic roles of the MT candidates. Consider-
ing there is no efficient way of integrating SRL
and MT, accurate target-side semantic roles can
only be used in post-processing and re-ranking
the MT outputs, where a limited number of MT
candidates are considered. On the other hand, it
is much easier to obtain reliable semantic roles
for the source sentences. This paper uses a com-
promise approach, where the target-side semantic
roles are projected from the source-side semantic
roles using the word alignment derived from the
translation process. More specifically, we define
two types of semantic role features:
1. Semantic Role Re-ordering (SRR) This fea-
ture describes re-ordering of the source-side
717
semantic roles (including the predicate) in the
target side. It takes the following form:
SrcPred : SrcRole1, ..., SrcRolen
? TarRole1, ..., TarRolen
where SrcPred and SrcRole denotes the
central verb and semantic roles in the source
side, and TarRole denotes the target-side
roles. The source/target SRSs do not need be
continuous, but there should be a one-to-one
alignment between the roles in the two sides.
Compared to the general re-ordering models
used in statistical MT systems, this type of
feature is capable of modeling skeleton-level
re-ordering, which is crucial to the fluency
of MT output. Because a predicate can have
different semantic role sequences in different
voices, passive/active are tagged for each oc-
currence of the verbs based on their POS and
preceding words. Figure 1 shows examples
of the feature SRR.
2. Deleted Roles (DR) are the individual source-
side semantic roles which are deleted in the
MT outputs, taking the form of:
SrcPred : SrcRole ? deleted
DR is meant to penalize the deletion of the
semantic roles. Though most statistical MT
systems have penalties for word deletion, it
is still useful to make separate features for
the deletion of semantic roles, which is con-
sidered more harmful than the deletion of
non-core components (e.g., modifiers) and
deserves more serious penalty. Examples of
the deletion features can be found in Figure 1.
Both types of features can be made non-lexicalized
by removing the actual verb but retaining its voice
information in the features. Non-lexicalized fea-
tures are used in the system to alleviate the problem
of sparse verbs.
3 Using Semantic Role Features in
Machine Translation
This section describes how to use the proposed se-
mantic role features in a Tree-to-String transducer,
I??di
d??n
ot??
see
??th
e??b
??
??
arg
0
arg
?ne
g
arg
1
SRR
: see?
acti
ve:?
arg?
neg
verb
bor
row
ed?
acti
ve:?
arg1
?a
bor
row
ed?
acti
ve:?
arg1
?v e
bor
row
ed?
acti
ve:?
arg0
?ve
bor
row
ed?
acti
ve:?
arg1
?a
DR:
see
?act
ive:
?arg
0??
dele
tboo
k??y
ou??
bor
row
ed
??
??arg1
arg
0
??a
rg?n
egv
erb
rg0
??a
rg0
?arg
1
erb
??v
erb
?arg
1
erb
??a
rg0
?ver
b
rg0
?ver
b??
arg0
?ver
b?ar
g1
ted
?
Figure 1: Examples of the semantic role features
assuming that the semantic roles have been tagged
for the source sentences. We first briefly describe
the basic Tree-to-String translation model used in
our experiments, and then describe how to modify
it to incorporate the semantic role features.
3.1 Basic Tree-to-String Transducer
A Tree-to-String transducer receives a syntax tree
as its input and, by recursively applying TTS tem-
plates, generates the target string. A TTS tem-
plate is composed of a left-hand side (LHS) and
a right-hand side (RHS), where the LHS is a sub-
tree pattern and the RHS is a sequence of variables
and translated words. The variables in the RHS
of a template correspond to the bottom level non-
terminals in the LHS?s subtree pattern, and their
relative order indicates the permutation desired at
the point where the template is applied to translate
one language to another. The variables are further
transformed, and the recursive process goes on un-
til there are no variables left. The formal descrip-
tion of a TTS transducer is given by Graehl and
Knight (2004), and our baseline approach follows
the Extended Tree-to-String Transducer defined by
Huang et al (2006). For a given derivation (de-
composition into templates) of a syntax tree, the
translation probability is computed as the product
of the templates which generate both the source
syntax trees and the target translations.
Pr(S | T,D?) =
?
t?D?
Pr(t)
Here, S denotes the target sentence, T denotes the
source syntax tree, and D? denotes the derivation
of T . In addition to the translation model, the
718
function DECODE(T )
for tree node v of T in bottom-up order do
for template t applicable at v do
{c1, c2}=match(v, t);
s.leftw = c1.leftw;
s.rightw = c2.rightw;
s.val = c1.val ? c2.val;
s.val ?= Pr(t);
s.val ?= Pr(c2.leftw|c1.rightw);
add s to v?s beam;
Figure 2: Decoding algorithm for the standard Tree-to-String
transducer. leftw/rightw denote the left/right boundary
word of s. c1, c2 denote the descendants of v, ordered based
on RHS of t.
TTS system includes a trigram language model,
a deletion penalty, and an insertion bonus. The
bottom-up decoding algorithm for the TTS trans-
ducer is sketched in Figure 2. To incorporate the
n-gram language model, states in the algorithm
denote a tree node?s best translations with different
left and right boundary words. We use standard
beam-pruning to narrow the search space. To sim-
plify the description, we assume in Figure 2 that
a bigram language model is used and all the TTS
templates are binarized. It is straightforward to
generalize the algorithm for larger n-gram models
and TTS templates with any number of children in
the bottom using target-side binarized combination
(Huang et al, 2006).
3.2 Modified Tree-to-String Transducer with
Semantic Role Features
Semantic role features can be used as an auxiliary
translation model in the TTS transducer, which
focuses more on the skeleton-level permutation.
The model score, depending on not only the in-
put source tree and the derivation of the tree, but
also the semantic roles of the source tree, can be
formulated as:
Pr(S | T,D?) =
?
f?F (S,T.role,D?)
Pr(f)
where T denotes the source syntax tree with
semantic roles, T.role denotes the seman-
tic role sequence in the source side and
F (S.role, T.role,D?) denotes the set of defined
semantic role features over T.role and the target
side semantic role sequence S.role. Note that
given T.role and the derivation D?, S.role can
VP NP
[gi
vin
g:?
VB
G
[gi
vin
g:?v
erb
]
giv
ing
VP
[gi
vin
g:?a
r g
TTS
?te
mp
lat
e:
(VP
?(V
BG
?giv
in
Tri
gge
red
??SR
R:?
?giv
ing
?ac
tive
:?a
Tri
gge
red
?DR
:????
?giv
ing
?ac
tive
:?vNP
[gi
vin
g:?
VB
G
[gi
vin
g:?v
erb
]
giv
ing
arg
2]
NP
[gi
vin
g:?a
rg1
]
g2?
arg
1]
g?)?
?NP
#1?
NP
#2?
)???
NP
#1?
NP
#2
arg
2?a
rg1
??
arg
2?a
rg1
ver
b??
de
let
ed
arg
2]
NP
[gi
vin
g:?a
rg1
]
Figure 3: An example showing the combination of the se-
mantic role sequences of the states. Above/middle is the state
information before/after applying the TTS template, and bot-
tom is the used TTS template and the triggered SRFs during
the combination.
be easily derived. Now we show how to in-
corporate the two types of semantic role features
into a TTS transducer. To use the semantic role
re-ordering feature SRR, the states in the decod-
ing algorithm need to be expanded to encode the
target-side SRSs. The SRSs are initially attached
to the translation states of the source tree con-
? PP VBZ ? ??
VP
VBZ
[bring: verb]
NP
[bring: arg1]
PP
[bring: arg3]
NNP NN
new test
0 3 4
^
Combined SRS arg3 verb arg1
Median = 3 arg1
Figure 4: An example showing how to compute the target side
position of a semantic role by using the median of its aligning
points.
719
stituents which are labeled as semantic roles for
some predicate. These semantic roles are then
accumulated with re-ordering and deletion oper-
ations specified by the TTS templates as the de-
coding process goes bottom-up. Figure 5 shows
the decoding algorithm incorporating the SRR fea-
tures. The model component corresponding to the
feature SRR is computed when combining two
translation states. I.e., the probabilities of the SRR
features composed based on the semantic roles of
the two combining states will be added into the
combined state. See Figure 3 for examples. The
theoretical upper bound of the decoding complex-
ity is O(NM4(n?1)R(?Ci=0 C!i! )V ), where N isthe number of nodes in the source syntax tree, M
is the vocabulary size of the target language, n is
the order of the n-gram language model, R is the
maximum number of TTS templates which can be
matched at a tree node, C is the maximum number
of roles of a verb, and V is the maximum number
of verbs in a sentence. In this formula, ?Ci=0 C!i!is the number of role sequences obtained by first
choosing i out of C possible roles and then per-
muting the i roles. This theoretical upper bound
is not reached in practice, because the number of
possible TTS templates applicable at a tree node
is very limited. Furthermore, since we apply beam
pruning at each tree node, the running time is con-
trolled by the beam size, and is linear in the size of
the tree.
The re-ordering of the semantic roles from
source to target is computed for each TTS template
as part of the template extraction process, using
the word-level alignments between the LHS/RHS
of the TTS template (e.g., Figure 3). This is usu-
ally straightforward, with the exception of the case
where the words that are aligned to a particular
role?s span in the source side are not continuous
in the target side, as shown in Figure 4. Since
we are primarily interested in the relative order of
the semantic roles, we approximate each seman-
tic role?s target side position by the median of the
word positions that is aligned to. If more than one
semantic role is mapped to the same position in
the target side, their source side order will be used
as their target side order, i.e., monotonic transla-
tion is assumed for those semantic roles. Figure 4
shows an example of calculating the target side
function DECODE(T )
for tree node v of T in bottom-up order do
for template t applicable at v do
{c1, c2}=match(v, t);
s.leftw = c1.leftw;
s.rightw = c2.rightw;
s.role = concatenate(c1.role, c2.role);
if v is a semantic role then
set s.role to v.role;
s.val = c1.val ? c2.val;
s.val ?= Pr(t);
s.val ?= Pr(c2.leftw|c1.rightw);
. Compute the probabilities associated with semantic roles
s.val ?= Qf?Sema(c1.role,c2.role,t) Pr(f);add s to v?s beam;
Figure 5: Decoding algorithm using semantic role features.
Sema(c1.role, c2.role, t) denotes the triggered semantic
role features when combining two children states, and ex-
amples can be found in Figure 3.
SRS based on a complicated TTS template. The
word alignments in the TTS templates are also used
to compute the deletion feature DR. Whenever a
semantic role is deleted in a TTS template?s RHS,
the corresponding deletion penalty will be applied.
3.3 Training
We describe two alternative methods for training
the weights for the model?s features, including both
the individual TTS templates and the semantic
role features. The first method maximizes data
likelihood as is standard in EM, while the second
method maximizes conditional likelihood for a log-
linear model following Blunsom et al (2008).
3.3.1 Maximizing Data Likelihood
The standard way to train a TTS translation
model is to extract the minimum TTS templates us-
ing GHKM (Galley et al, 2004), and then normal-
ize the frequency of the extracted TTS templates
(Galley et al, 2004; Galley et al, 2006; Liu et al,
2006; Huang et al, 2006). The probability of the
semantic features SRR and DR can be computed
similarly, given that SRR and DR can be derived
from the paired source/target sentences and the
word alignments between them. We refer to this
model as max-likelihood training and normalize
the counts of TTS templates and semantic features
based on their roots and predicates respectively.
We wish to overcome noisy alignments from
GIZA++ and learn better TTS rule probabilities
by re-aligning the data using EM within the TTS
720
E-step:
for all pair of syntax tree T and target string S do
for all TTS Template t, semantic features f do
EC(t) +=
P
D:t?D Pr(S,T,D)P
D? Pr(S,T,D?)
;
EC(f) +=
P
D:f?D Pr(S,T,D)P
D? Pr(S,T,D?)
;
M-step:
for all TTS Template t, semantic features f do
Pr(t) = EC(t)P
t?:t?.root=t.root EC(t?)
;
Pr(f) = EC(f)P
f?:f?.predicate=t.predicate EC(f ?)
;
Figure 6: EM Algorithm For Estimating TTS Templates and
Semantic Features
framework (May and Knight, 2007). We can es-
timate the expected counts of the TTS templates
and the semantic features by formulating the prob-
ability of a pair of source tree and target string
as:
X
D
Pr(S, T,D) =
X
D
0
@
Y
t?D
Pr(t)
Y
f?F (S,T.role,D)
Pr(f)
1
A
Though the above formulation, which makes the
total probability of all the pairs of trees and strings
less than 1, is not a strict generative model, we can
still use the EM algorithm (Dempster et al, 1977)
to estimate the probability of the TTS templates
and the semantic features, as shown in Figure 6.
The difficult part of the EM algorithm is the E-
step, which computes the expected counts of the
TTS templates and the semantic features by sum-
ming over all possible derivations of the source
trees and target strings. The standard inside-
outside algorithm (Graehl and Knight, 2004) can
be used to compute the expected counts of the TTS
templates. Similar to the modification made in the
TTS decoder, we can add the target-side semantic
role sequence to the dynamic programming states
of the inside-outside algorithm to compute the ex-
pected counts of the semantic features. This way
each state (associated with a source tree node) rep-
resents a target side span and the partial SRSs. To
speed up the training, a beam is created for each
target span and only the top rated SRSs in the beam
are kept.
3.3.2 Maximizing Conditional Likelihood
A log-linear model is another way to combine
the TTS templates and the semantic features to-
gether. Considering that the way the semantic
function COMPUTE PARTITION(T )
for tree node v of T in bottom-up order do
for template t applicable at v do
for {s1, s2}=Match(v, t) do
s.sum += s1.sum? s2.sum?
exp(?t +
P
f?Sema(s1,s2,t) ?f );
s.role = concatenate(s1.role, s2.role);
add s to v;
for state s in root do res += s.sum;
return res;
Figure 7: Computing the partition function of the conditional
probability Pr(S|T ). Sema(s1, s2, t) denotes all the seman-
tic role features generated by combining s1 and s2 using t.
role features are defined makes it impossible to
design a sound generative model to incorporate
these features, a log-linear model is also a theoreti-
cally better choice than the EM algorithm. If we
directly translate the EM algorithm into the log-
linear model, the problem becomes maximizing
the data likelihood represented by feature weights
instead of feature probabilities:
Pr(S, T ) =
P
D exp
P
i ?ifi(S, T,D)P
S?,T ?
P
D? exp
P
i ?ifi(S?, T ?, D?)
where the features f include both the TTS tem-
plates and the semantic role features. The numer-
ator in the formula above can be computed using
the same dynamic programming algorithm used to
compute the expected counts in the EM algorithm.
However, the partition function (denominator) re-
quires summing over all possible source trees and
target strings, and is infeasible to compute. In-
stead of approximating the partition function using
methods such as sampling, we change the objective
function from the data likelihood to the conditional
likelihood:
Pr(S | T ) =
P
D exp
P
i ?ifi(S, T,D)P
S??all(T )
P
D? exp
P
i ?ifi(S?, T,D?)
where all(T ) denotes all the possible target strings
which can be generated from the source tree T .
Given a set of TTS templates, the new partition
function can be efficiently computed using the dy-
namic programming algorithm shown in Figure 7.
Again, to simplify the illustration, only binary TTS
templates are used. Using the conditional proba-
bility as the objective function not only reduces
the computational cost, but also corresponds better
to the TTS decoder, where the best MT output is
721
selected only among the possible candidates which
can be generated from the input source tree using
TTS templates.
The derivative of the logarithm of the objective
function (over the entire training corpus) w.r.t. a
feature weight can be computed as:
? log
Q
S,T Pr(S | T )
??i
=
X
S,T
{ECD|S,T (fi)? ECS?|T (fi)}
where ECD|S,T (fi), the expected count of a fea-
ture over all derivations given a pair of tree and
string, can be computed using the modified inside-
outside algorithm described in Section 3.2, and
ECS?|T (fi), the expected count of a feature over
all possible target strings given the source tree,
can be computed in a similar way to the partition
function described in Figure 7. With the objective
function and its derivatives, a variety of optimiza-
tion methods can be used to obtain the best feature
weights; we use LBFGS (Zhu et al, 1994) in our
experiments. To prevent the model from overfitting
the training data, a weighted Gaussian prior is used
with the objective function. The variance of the
Gaussian prior is tuned based on the development
set.
4 Experiments
We train an English-to-Chinese translation system
using the FBIS corpus, where 73,597 sentence
pairs are selected as the training data, and 500
sentence pairs with no more than 25 words on the
Chinese side are selected for both the development
and test data.1 Charniak (2000)?s parser, trained on
the Penn Treebank, is used to generate the English
syntax trees. To compute the semantic roles for the
source trees, we use an in-house max-ent classifier
with features following Xue and Palmer (2004) and
Pradhan et al (2004). The semantic role labeler
is trained and tuned based on sections 2?21 and
section 24 of PropBank respectively. The standard
role-based F-score of our semantic role labeler is
88.70%. Modified Kneser-Ney trigram models
are trained using SRILM (Stolcke, 2002) on the
Chinese portion of the training data. The model
1The total 74,597 sentence pairs used in experiments are
those in the FBIS corpus whose English part can be parsed
using Charniak (2000)?s parser.
(n-gram language model, TTS templates, SRR,
DR) weights of the transducer are tuned based on
the development set using a grid-based line search,
and the translation results are evaluated based on a
single Chinese reference using BLEU-4 (Papineni
et al, 2002). Huang et al (2006) used character-
based BLEU as a way of normalizing inconsistent
Chinese word segmentation, but we avoid this prob-
lem as the training, development, and test data are
from the same source.
The baseline system in our experiments uses
the TTS templates generated by using GHKM
and the union of the two single-direction align-
ments generated by GIZA++. Unioning the two
single-direction alignments yields better perfor-
mance for the SSMT systems using TTS templates
(Fossum et al, 2008) than the two single-direction
alignments and the heuristic diagonal combination
(Koehn et al, 2003). The two single-direction
word alignments as well as the union are used to
generate the initial TTS template set for both the
EM algorithm and the log-linear model. The ini-
tial TTS templates? probabilities/weights are set to
their normalized counts based on the root of the
TTS template (Galley et al, 2006). To test seman-
tic role features, their initial weights are set to their
normalized counts for the EM algorithm and to 0
for the log-linear model. The performance of these
systems is shown in Table 1. We can see that the
EM algorithm, based only on TTS templates, is
slightly better than the baseline system. Adding
semantic role features to the EM algorithm actu-
ally hurts the performance, which is not surprising
since the combination of the TTS templates and
semantic role features does not yield a sound gen-
erative model. The log-linear model based on TTS
templates achieves significantly better results than
both the baseline system and the EM algorithm.
Both improvements are significant at p < 0.05
based on 2000 iterations of paired bootstrap re-
sampling of the test set (Koehn, 2004).
Adding semantic role features to the log-linear
model further improves the BLEU score. One prob-
lem in our approach is the sparseness of the verbs,
which makes it difficult for the log-linear model
to tune the lexicalized semantic role features. One
way to alleviate this problem is to make features
based on verb classes. We first tried using the verb
722
TTS Templates + SRF + Verb Class
Union 15.6 ? ?
EM 15.9 15.5 15.6
Log-linear 17.1 17.4 17.6
Table 1: BLEU-4 scores of different systems
equal better worse
With SRF vs. W/O SRF 72% 20.2% 7.8%
Table 2: Distribution of the sentences where the semantic
role features give no/positive/negative impact to the sentence
fluency in terms of the completeness and ordering of the
semantic roles.
classes in VerbNet (Dang et al, 1998). Unfortu-
nately, VerbNet only covers about 34% of the verb
tokens in our training corpus, and does not im-
prove the system?s performance. We then resorted
to automatic clustering based on the aspect model
(Hofmann, 1999; Rooth et al, 1999). The training
corpus used in clustering is the English portion of
the selected FBIS corpus. Though automatically
obtained verb clusters lead to further improvement
in BLEU score, the total improvement from the se-
mantic role features is not statistically significant.
Because BLEU-4 is biased towards the adequacy
of the MT outputs and may not effectively evaluate
their fluency, it is desirable to give a more accurate
evaluation of the sentence?s fluency, which is the
property that semantic role features are supposed
to improve. To do this, we manually compare
the outputs of the two log-linear models with and
without the semantic role features. Our evaluation
focuses on the completeness and ordering of the
semantic roles, and better, equal, worse are tagged
for each pair of MT outputs indicating the impact
of the semantic role features. Table 2 shows the
manual evaluation results based on the entire test
set, and the improvement from SRF is significant
at p < 0.005 based on a t-test. To illustrate how
SRF impacts the translation results, Figure 8 gives
3 examples of the MT outputs with and without
the SRFs.
5 Conclusion
This paper proposes two types of semantic role
features for a Tree-to-String transducer: one mod-
els the reordering of the source-side semantic role
sequence, and the other penalizes the deletion of a
source-side semantic role. These semantic features
Sou
rce
Lau
nch
ing
1N
ew
2D
ip
SRF
?On
??
1?
? 2
??
3?
? 4
SRF
?Of
f
??
2?
? 3
??
4
Sou
rce
It 1
is 2
the
ref
ore
3
ne
tra
nsf
orm
ati
on
9o
f 10
hig
h 14
tec
hn
olo
gie
s 15
SRF
?On
??
12
3?
4?
? 6,
7?
?
SRF
?Of
f
??
12
3?
4?
??
14
,15
Sou
rce
A 1
gra
tify
ing
2
cha
n
str
uct
ure
8o
f 9e
thn
ic
SRF
?On
??
??
10
,11
??
8?
4
SRF
?Of
f
??
1?
??
2?
? 3
,?pl
om
ati
c 3O
ffe
nsi
ve 4
4 ece
ssa
ry 4
to 5
spe
ed
6
up
7
the
8
tra
dit
ion
al 1
1in
du
str
ies
12
wit
h 13
5 ??
? 14
,15
??
9?
??
? 11
,12
,?
? 6,
7?
??
? 11
,12
??
9
nge
3
als
o 4
occ
urr
ed
5
in 6
the
7
10
mi
no
rity
11
cad
res
12
??
5?
??
2?
? 3
??
4?
??
? 10
,11
??
?
??
8
Figure 8: Examples of the MT outputs with and without SRFs.
The first and second example shows that SRFs improve the
completeness and the ordering of the MT outputs respectively,
the third example shows that SRFs improve both properties.
The subscripts of each Chinese phrase show their aligned
words in English.
and the Tree-to-String templates, trained based on
a conditional log-linear model, are shown to sig-
nificantly improve a basic TTS transducer?s per-
formance in terms of BLEU-4. To avoid BLEU?s
bias towards the adequacy of the MT outputs, man-
ual evaluation is conducted for sentence fluency
and significant improvement is shown by using
the semantic role features in the log-linear model.
Considering our semantic features are the most ba-
sic ones, using more sophisticated features (e.g.,
the head words and their translations of the source-
side semantic roles) provides a possible direction
for further experimentation.
Acknowledgments This work was funded by
NSF IIS-0546554 and IIS-0910611.
References
Blunsom, Phil, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL-08), Columbus, Ohio.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-01, pages
132?139.
Dang, Hoa Trang, Karin Kipper, Martha Palmer, and
723
Joseph Rosenzweig. 1998. Investigating regu-
lar sense extensions based on intersective Levin
classes. In COLING/ACL-98, pages 293?299, Mon-
treal. ACL.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?21.
Fossum, Victoria, Kevin Knight, and Steven Abney.
2008. Using syntax to improveword alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, Columbus, Ohio. ACL.
Fung, Pascale, Zhaojun Wu, Yongsheng Yang, and
Dekai Wu. 2006. Learning of Chinese/English se-
mantic structure mapping. In IEEE/ACL 2006 Work-
shop on Spoken Language Technology, Aruba.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-04, pages 273?280.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL-06, pages 961?968, July.
Graehl, Jonathan and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-04.
Hofmann, Thomas. 1999. Probabilistic latent semantic
analysis. In Uncertainity in Artificial Intelligence,
UAI?99, Stockholm.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, MA.
Johnson, Christopher R., Charles J. Fillmore, Miriam
R. L. Petruck, Collin F. Baker, Michael Ellsworth,
Josef Ruppenhofer, and Esther J. Wood. 2002.
FrameNet: Theory and practice. Version 1.0,
http://www.icsi.berkeley.edu/framenet/.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, Edmonton, Alberta.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177?180.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395, Barcelona, Spain, July.
Liu, Ding and Daniel Gildea. 2008. Improved tree-
to-string transducers for machine translation. In
ACL Workshop on Statistical Machine Translation
(ACL08-SMT), pages 62?69, Columbus, Ohio.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL-06,
Sydney, Australia, July.
May, Jonathan and Kevin Knight. 2007. Syntactic
re-alignment models for machine translation. In Pro-
ceedings of EMNLP.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
ACL-02.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James
Martin, , and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of NAACL-04.
Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th Annual Meeting of the ACL,
pages 104?111, College Park, Maryland.
Stolcke, Andreas. 2002. SRILM - an extensible lan-
guage modeling toolkit. In International Conference
on Spoken Language Processing, volume 2, pages
901?904.
Toutanova, Kristina, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-05, pages 589?
596.
Wu, Dekai and Pascale Fung. 2009. Semantic roles
for smt: A hybrid two-pass model. In Proceedings
of the HLT-NAACL 2009: Short Papers, Boulder,
Colorado.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP.
Zhu, Ciyou, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1994. L-BFGS-B: Fortran subroutines for
large-scale bound constrained optimization. Techni-
cal report, ACM Trans. Math. Software.
724
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 484?488,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Novel Text Classifier Based on Quantum Computation 
 
 
Ding Liu,  Xiaofang Yang,  Minghu Jiang 
Laboratory of Computational Linguistics, School of Humanities, 
Tsinghua University, Beijing , China 
Dingliu_thu@126.com  xfyang.thu@gmail.com   
jiang.mh@mail.tsinghua.edu.cn 
 
 
Abstract 
In this article, we propose a novel classifier 
based on quantum computation theory. Differ-
ent from existing methods, we consider the 
classification as an evolutionary process of a 
physical system and build the classifier by us-
ing the basic quantum mechanics equation. 
The performance of the experiments on two 
datasets indicates feasibility and potentiality of 
the quantum classifier.  
1 Introduction 
Taking modern natural science into account, the 
quantum mechanics theory (QM) is one of the 
most famous and profound theory which brings a 
world-shaking revolution for physics. Since QM 
was born, it has been considered as a significant 
part of theoretic physics and has shown its power 
in explaining experimental results. Furthermore, 
some scientists believe that QM is the final prin-
ciple of physics even the whole natural science. 
Thus, more and more researchers have expanded 
the study of QM in other fields of science, and it 
has affected almost every aspect of natural sci-
ence and technology deeply, such as quantum 
computation.   
The principle of quantum computation has al-
so affected a lot of scientific researches in com-
puter science, specifically in computational mod-
eling, cryptography theory as well as information 
theory. Some researchers have employed the 
principle and technology of quantum computa-
tion to improve the studies on Machine Learning 
(ML) (A?meur et al, 2006; A?meur et al, 2007; 
Chen et al, 2008; Gambs, 2008; Horn and 
Gottlieb, 2001; Nasios and Bors, 2007), a field 
which studies theories and constructions of sys-
tems that can learn from data, among which clas-
sification is a typical task. Thus, we attempted to  
 
build a computational model based on quantum 
computation theory to handle classification tasks 
in order to prove the feasibility of applying the 
QM model to machine learning. 
In this article, we present a method that con-
siders the classifier as a physical system amena-
ble to QM and treat the entire process of classifi-
cation as the evolutionary process of a closed 
quantum system. According to QM, the evolu-
tion of quantum system can be described by a 
unitary operator. Therefore, the primary problem 
of building a quantum classifier (QC) is to find 
the correct or optimal unitary operator. We ap-
plied classical optimization algorithms to deal 
with the problem, and the experimental results 
have confirmed our theory. 
The outline of this paper is as follows. First, 
the basic principle and structure of QC is intro-
duced in section 2. Then, two different experi-
ments are described in section 3. Finally, section 
4 concludes with a discussion. 
2  Basic principle of quantum classifier  
As we mentioned in the introduction, the major 
principle of quantum classifier (QC) is to consid-
er the classifier as a physical system and the 
whole process of classification as the evolution-
ary process of a closed quantum system. Thus, 
the evolution of the quantum system can be de-
scribed by a unitary operator (unitary matrix), 
and the remaining job is to find the correct or 
optimal unitary operator. 
2.1 Architecture of quantum classifier 
The architecture and the whole procedure of data 
processing of QC are illustrated in Figure 1. As 
is shown, the key aspect of QC is the optimiza-
tion part where we employ the optimization algo-
rithm to find an optimal unitary operator ??.  
484
  
Figure 1. Architecture of quantum classifier 
 
The detailed information about each phase of the 
process will be explained thoroughly in the fol-
lowing sections.  
2.2 Encode input state and target state 
In quantum mechanics theory, the state of a 
physical system can be described as a superposi-
tion of the so called eigenstates which are or-
thogonal. Any state, including the eigenstate, can 
be represented by a complex number vector. We 
use Dirac?s braket notation to formalize the data 
as equation 1: 
|?? =???|???
?
																					(1) 
 
where |?? denotes a state and ?? ? ? is a com-
plex number with ?? = ??|??? being the projec-
tion of |?? on the eigenstate |???. According to 
quantum theory, ?? denotes the probability am-
plitude. Furthermore, the probability of |?? col-
lapsing on |??? is P(??) =
|??|
?
? |??|??
 . 
Based on the hypothesis that QC can be con-
sidered as a quantum system, the input data 
should be transformed to an available format in 
quantum theory ? the complex number vector. 
According to Euler?s formula, a complex number 
z can be denoted as ? = ???? with r? ?, ? ? ?. 
Equation 1, thus, can be written as: 
 
																													|?? =???|???????
?
												(2) 
 
where ??  and ??  denote the module and the 
phase of the complex coefficient respectively.  
 
 
For different applications, we employ different 
approaches to determine the value of ?? and ??. 
Specifically, in our experiment, we assigned the 
term frequency, a feature frequently used in text 
classification to ?? , and treated the phase ?? as 
a constant, since we found the phase makes little 
contribution to the classification.  
For each data sample ???????, we calculate 
the corresponding input complex number vector 
by equation 3, which is illustrated in detail in 
Figure 2. 
 
																|??? =???? ? ??|????
?
???
																					(3)	 
 
 
Figure 2. Process of calculating the input state 
 
Each eigenstate |???  denotes the correspond-
ing ????????, resulting in m eigenstates for  all 
the samples.  
As is mentioned above, the evolutionary pro-
cess of a closed physical system can be described 
by a unitary operator, depicted by a matrix as in 
equation 4: 
 
																																|??? = ??|?																											4)) 
 
where |??? and |?? denote the final state and the 
initial state respectively. The approach to deter-
mine the unitary operator will be discussed in 
485
section 2.3. We encode the target state in the 
similar way. Like the Vector Space Model(VSM), 
we use a label matrix to represent each class as in 
Figure 3. 
 
 
Figure 3.  Label matrix 
 
For each input sample ???????, we generate 
the corresponding target complex number vector 
according to equation 5: 
																						|??? =???? ? ??|????
?
???
															(5) 
 
where each eigenstate |??? represents the corre-
sponding ???? ?? , resulting in w eigenstates for 
all the labels. Totally, we need ?+?  eigen-
states, including features and labels. 
 
2.3 Finding the Hamiltonian matrix and the 
Unitary operator 
As is mentioned in the first section, finding a 
unitary operator to describe the evolutionary pro-
cess is the vital step in building a QC. As a basic 
quantum mechanics theory, a unitary operator 
can be represented by a unitary matrix with the 
property ?? = ??? , and a unitary operator can 
also be written as equation 6: 
																																	? = ?
???
? ?																												6)) 
 
where H is the Hamiltonian matrix and ? is the 
reduced Planck constant. Moreover, the Hamil-
tonian H is a Hermitian matrix with the property 
?? = (??)? = ?. The remaining job, therefore, 
is to find an optimal Hamiltonian matrix. 
Since H is a Hermitian matrix, we only need 
to determine (? +?)?  free real parameters, 
provided that the dimension of H is (m+w). Thus, 
the problem of determining H can be regarded as 
a classical optimization problem, which can be 
resolved by various optimization algorithms 
(Chen and Kudlek, 2001). An error function is 
defined as equation 7: 
 
														?)???) =
?
? ????
???????(??(??,??
											(7) 
where T is a set of training pairs with ?? ,
?? , ???	??  denoting the target, input, and output 
state respectively, and ??  is determined by ?? as 
equation 8: 
 
                    |??? = ?
???
?
8)                      ???|?)          
 
In the optimization phase, we employed sever-
al optimization algorithm, including BFGS, Ge-
neric Algorithm, and a multi-objective optimiza-
tion algorithm SQP (sequential quadratic pro-
gramming) to optimize the error function. In our 
experiment, the SQP method performed best out-
performed the others.  
 
3 Experiment 
We tested the performance of QC on two differ-
ent datasets. In section 3.1, the Reuters-21578 
dataset was used to train a binary QC. We com-
pared the performance of QC with several classi-
cal classification methods, including Support 
Vector Machine (SVM) and K-nearest neighbor 
(KNN). In section 3.2, we evaluated the perfor-
mance on multi-class classification using an oral 
conversation datasets and analyzed the results. 
3.1 Reuters-21578 
The Reuters dataset we tested contains 3,964 
texts belonging to ?earnings? category and 8,938 
texts belonging to ?others? categories. In this 
classification task, we selected the features by 
calculating the ??  score of each term from the 
?earnings? category (Manning and Sch?tze, 
2002).  
For the convenience of counting, we adopted 
3,900 ?earnings? documents and 8,900 ?others? 
documents and divided them into two groups: the 
training pool and the testing sets. Since we fo-
cused on the performance of QC trained by 
small-scale training sets in our experiment, we 
each selected 1,000 samples from the ?earnings? 
and the ?others? category as our training pool 
and took the rest of the samples (2,900 ?earnings? 
and 7,900 ?others? documents) as our testing sets.  
We randomly selected training samples from the 
training pool ten times to train QC, SVM, and 
KNN classifier respectively and then verified the 
three trained classifiers on the testing sets, the 
results of which are illustrated in Figure 4. We 
noted that the QC performed better than both 
KNN and SVM on small-scale training sets, 
when the number of training samples is less than 
50. 
486
 
Figure 4.  Classification accuracy for Reuters-
21578 datasets 
 
Generally speaking, the QC trained by a large 
training set may not always has an ideal perfor-
mance. Whereas some single training sample 
pair led to a favorable result when we used only 
one sample from each category to train the QC. 
Actually, some single samples could lead to an 
accuracy of more than 90%, while some others 
may produce an accuracy lower than 30%. 
Therefore, the most significant factor for QC is 
the quality of the training samples rather than the 
quantity. 
3.2 Oral conversation datasets 
Besides the binary QC, we also built a multi-
class version and tested its performance on an 
oral conversation dataset which was collected by 
the Laboratory of Computational Linguistics of 
Tsinghua university. The dataset consisted of 
1,000 texts and were categorized into 5 classes, 
each containing 200 texts. We still took the term 
frequency as the feature, the dimension of which 
exceeded 1,000. We, therefore, utilized the pri-
mary component analysis (PCA) to reduce the 
high dimension of the features in order to de-
crease the computational complexity. In this ex-
periment, we chose the top 10 primary compo-
nents of the outcome of PCA, which contained 
nearly 60% information of the original data. 
Again, we focused on the performance of QC 
trained by small-scale training sets. We selected 
100 samples from each class to construct the 
training pool and took the rest of the data as the 
testing sets. Same to the experiment in section 
3.1, we randomly selected the training samples 
from the training pool ten times to train QC, 
SVM, and KNN classifier respectively and veri 
fied the models on the testing sets, the results of 
which are shown in Figure 5. 
 
 
Figure 5.  Classification accuracy for oral 
conversation datasets 
 
4 Discussion 
We present here our model of text classification 
and compare it with SVM and KNN on two da-
tasets. We find that it is feasible to build a super-
vised learning model based on quantum mechan-
ics theory. Previous studies focus on combining 
quantum method with existing classification 
models such as neural network (Chen et al, 2008) 
and kernel function (Nasios and Bors, 2007) aim-
ing to improve existing models to work faster 
and more efficiently. Our work, however, focus-
es on developing a novel method which explores 
the relationship between machine learning model 
with physical world, in order to investigate these 
models by physical rule which describe our uni-
verse. Moreover, the QC performs well in text 
classification compared with SVM and KNN and 
outperforms them on small-scale training sets. 
Additionally, the time complexity of QC depends 
on the optimization algorithm and the amounts of 
features we adopt. Generally speaking, simulat-
ing quantum computing on classical computer 
always requires more computation resources, and 
we believe that quantum computer will tackle the 
difficulty in the forthcoming future. Actually, 
Google and NASA have launched a quantum 
computing AI lab this year, and we regard the 
project as an exciting beginning. 
Future studies include: We hope to find a 
more suitable optimization algorithm for QC and  
a more reasonable physical explanation towards 
the ?quantum nature? of the QC. We hope our 
attempt will shed some light upon the application 
of quantum theory into the field of machine 
learning. 
 
 
 
487
Acknowledgments 
 
This work was supported by the National Natural 
Science Foundation in China (61171114), State 
Key Lab of Pattern Recognition open foundation, 
CAS. Tsinghua University Self-determination 
Research Project (20111081023 & 20111081010) 
and Human & liberal arts development founda-
tion (2010WKHQ009) 
 
References  
Esma A?meur, Gilles Brassard, and S?bastien Gambs. 
2006. Machine Learning in a Quantum World. Ca-
nadian AI 2006 
Esma A?meur, Gilles Brassard and S?bastien Gambs. 
2007.   Quantum Clustering Algorithms. Proceed-
ings of the 24 th International Conference on Ma-
chine Learning 
Joseph C.H. Chen and Manfred Kudlek. 2001. Duality 
of Syntex and Semantics ? From the View Point of 
Brain as a Quantum Computer. Proceedings of Re-
cent Advances in NLP 
Joseph C.H. Chen. 2001. Quantum Computation and 
Natural Language Processing. University of Ham-
burg, Germany. Ph.D. thesis 
Joseph C.H. Chen. 2001. A Quantum Mechanical 
Approach to Cognition and Representation. Con-
sciousness and its Place in Nature,Toward a Sci-
ence of Consciousness. 
Cheng-Hung Chen, Cheng-Jian Lin and Chin-Teng 
Lin. 2008. An efficient quantum neuro-fuzzy clas-
sifier based on fuzzy entropy and compensatory 
operation. Soft Comput, 12:567?583. 
Fumiyo Fukumoto and Yoshimi Suzuki. 2002. Ma-
nipulating Large Corpora for Text Classification. 
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing 
S?bastien Gambs. 2008. Quantum classification, 
arXiv:0809.0444 
Lov K. Grover. 1997. Quantum Mechanics Helps in 
Searching for a Needle in a Haystack. Physical Re 
view Letters, 79,325?328 
David Horn and Assaf Gottlieb. 2001. The Method of 
Quantum Clustering. Proceedings of Advances in 
Neural Information Processing Systems . 
Christopher D. Manning and Hinrich Sch?tze. 2002. 
Foundations of Statistical Natural Language Pro-
cessing. MIT Press. Cambridge, Massachu-
setts,USA. 
Nikolaos Nasios and Adrian G. Bors. 2007. Kernel-
based classification using quantum mechanics. Pat-
tern Recognition, 40:875?889 
Hartmut Neven and Vasil S. Denchev. 2009. Training 
a Large Scale Classifier with the Quantum Adia-
batic Algorithm. arXiv:0912.0779v1 
Michael A. Nielsen and Isasc L. Chuang. 2000. Quan-
tum Computation and Quantum Information, Cam-
bridge University Press, Cambridge, UK. 
Masahide Sasaki and and Alberto Carlini. 2002. 
Quantum learning and universal quantum matching 
machine. Physical Review, A 66, 022303 
Dan Ventura. 2002. Pattern classification using a 
quantum system. Proceedings of the Joint Confer-
ence on Information Sciences.  
 
488
