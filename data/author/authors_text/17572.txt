INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 146?149,
Utica, May 2012. c?2012 Association for Computational Linguistics
Content Selection From Semantic Web Data
Nadjet Bouayad-Agha1
Gerard Casamayor1
Leo Wanner1,2
1DTIC, University Pompeu Fabra
2Institucio? Catalana de Recerca i Estudis Avanc?ats
Barcelona, Spain
firstname.lastname@upf.edu
Chris Mellish
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
So far, there has been little success in Natural
Language Generation in coming up with gen-
eral models of the content selection process.
Nonetheless, there has been some work on
content selection that employ Machine learn-
ing or heuristic search. On the other side, there
is a clear tendency in NLG towards the use of
resources encoded in standard Semantic Web
representation formats. For these reasons, we
believe that time has come to propose an initial
challenge on content selection from Semantic
Web data. In this paper, we briefly outline the
idea and plan for the execution of this task.
1 Motivation
So far, there has been little success in Natural Lan-
guage Generation in coming up with general mod-
els of the content selection process. Most of the
researchers in the field agree that this lack of suc-
cess is because the knowledge and context (commu-
nicative goals, user profile, discourse history, query,
etc) needed for this task depend on the application
domain. This often led in the past to template-
or graph-based combined content selection and dis-
course structuring approaches operating on idiosyn-
cratically encoded small sets of input data. Fur-
thermore, in many NLG-applications, target texts
and sometimes even empirical data are not avail-
able, which makes it difficult to employ empirical
approaches to knowledge elicitation. Nonetheless,
during the last decade, there has been a steady flow
of new work on content selection that employed Ma-
chine learning (Barzilay and Lapata, 2005; Duboue
and McKeown, 2003; Jordan and Walker, 2005;
Kelly et al, 2009), heuristic search (O?Donnell et
al., 2001; Demir et al, 2010; Mellish and Pan,
2008), or a combination thereof (Bouayad-Agha et
al., 2011). All of these strategies can deal with large
volumes of data.
On the other side, there is a clear tendency in NLG
towards the use of resources encoded in terms of
standard Semantic Web representation formats such
as OWL and RDF, e.g., (Wilcock and Jokinen, 2003;
Bontcheva and Wilks, 2004; Mellish and Pan, 2008;
Power and Third, 2010; Bouayad-Agha et al, 2011;
Dannells et al, 2012), to name but a few. However,
although most of these works make a good attempt
at realisation, the problem of content determination
from Semantic Web data is relatively untouched.
For these reasons, we believe that the time has
come to bring together researchers working on (or
interested in working on) content selection to par-
ticipate in a challenge for this task using standard
freely available web data as input. The availability
of open modular multi-domain multi-billion triple
data and of open ontological resources (Bizer et al,
2009) presented in a standard knowledge represen-
tation formalism make semantic web data a natural
choice for such a challenge.
As will be presented below, this initial challenge
presents a relatively simple content selection task
with no user model and a straightforward commu-
nicative goal so that people are encouraged to take
part and motivated to stay on for later challenges, in
which the task will be successively enhanced from
gained experience.
A content determination challenge would be a
chance to (i) directly compare the performance of
146
different types of content selection strategies; (ii)
contribute towards developing a standard ?off-the-
shelf? content selection module; and (iii) contribute
towards a standard interface between text planning
and linguistic generation.
To get the widest reception possible, the challenge
will be open to any approach, be it template-, rule-
or heuristic-based, or empirical. Furthermore, it will
be advertised in the Semantic Web Community to
get contributors from other horizons, see, e.g., (Dai
et al, 2010).
In what follows, we briefly outline the idea and
plan for the execution of the challenge. In Section 2,
we outline a description of the task. In Section 3,
the data and domain that will be used are presented.
Section 4 describes how this data is to be prepared
for the task, and Section 5 how it will be released to
the participants. In Section 6, we sketch the eval-
uation including the preparation of the evaluation
dataset. Section 7 gives a proposed schedule for
each of the tasks involved in organizing the chal-
lenge. Finally, in Section 8, we provide short bi-
ographies of the members of the organization team,
focusing on their experience in the proposed task.
2 Task Description
The core of the task to be addressed can be formu-
lated as follows:
Build a system which, given a set of RDF
triples containing facts about a celebrity
and a target text (for instance, a wikipedia-
style article about that person), selects
those triples that are reflected in the target
text.
The participants are also free to consider the se-
mantics defined by the data sources in their ap-
proach, rely on additional resources like ontologies
from other sources, or disregard the semantics com-
pletely.
The implemented system should output its results
in a predefined standard format that can be used for
automatic evaluation.
It could be that the RDF data does not contain ev-
erything that would ideally be included in such an
article, but that is ignored here. The task consists in
selecting content that is communicated in the target
text.
3 The data
The domain will be constituted by short biographies
of famous people. This is an interesting domain for
the challenge because Semantic Web data and corre-
sponding texts for this domain are available in large
quantities (e.g., DBPedia or Freebase for the data
and many other sources for biography texts, among
them Wikipedia).
The data will consist, for each famous person, of
a pair of RDF-triple set and associated text(s). For
each pair, the RDF data will include both informa-
tion communicated and excluded from the text. The
text may convey information not present in the RDF-
triples, but this will be kept to a minimum, always
subject to using naturally-occurring texts. All pairs
should contain enough RDF-triples and text to make
the pair interesting for the content selection task.
When choosing data for the challenge, we will
prefer semantic contents classified under consistent
ontologies over plain Linked Data with no explicit
semantics. The semantics of the RDF data (vocab-
ularies, ontologies) will be provided, preferably en-
coded in Semantic Web standards (e.g., in RDFS or
OWL).
4 Data Preparation
The task of data preparation consists in 1) data gath-
ering and preparation, which is to be carried out by
the organizers, and 2) working dataset selection and
annotation, which is to be carried out by both the
organizers and participants.
4.1 Data gathering and preparation
This preparatory stage consists in choosing the
repository sources, downloading the relevant on-
tologies (to the extent those will be provided), and
downloading and pairing the data and associated
texts (= the paired corpus).
4.2 Working Dataset selection and annotation
The participants will be asked to participate in a pre-
liminary task consisting in marking which triples are
included in the text given a subset of the paired cor-
pus (the size of the subset still has to be decided).
This task could be supported by some automatic
anchoring techniques such as used in (Duboue and
McKeown, 2003; Barzilay and Lapata, 2005). The
147
objectives of the task are threefold: (1) to provide all
participants with a common set of ?correct answers?
to be exploited in their approach, (2) to familiarize
the participants with the nature of the contents, their
semantics and the texts, and (3) to provide the task
with a ceiling for the evaluation, i.e. inter-annotator
agreement.
Annotation guidelines will be needed to ensure
that all participants follow the same procedure when
annotating texts. For this purpose, an early docu-
ment will be produced detailing the procedure to-
gether with examples and descriptions of relevant
problems such as ambiguities in the annotation. The
guidelines will be improved in multiple stages of an-
notation and revision with the goal of maximizing
inter-annotator agreement.
5 Data release
The participants in the challenge will be given ac-
cess to the set of all correct answers and a large
portion of the non-marked paired corpus, as well as
their semantics (i.e., ontologies and the like). The
remaining unseen, non-marked set will be kept for
evaluation.
6 Evaluation
The evaluation consists of 1) a preparatory stage for
selecting and annotating the evaluation dataset, and
2) an evaluation stage.
6.1 Evaluation dataset selection and annotation
Once all participants have submitted their exe-
cutable to solve the task, the evaluation set will be
processed. If timing is tight, however, this could be
done whilst the participants are still working on the
task or extra effort (for instance, from the organiz-
ers) could be brought in. A subset of the data is
randomly selected and annotated with the selected
triples by the participants. This two-stage approach
to triple selection annotation is proposed in order to
avoid any bias on the evaluation data.
6.2 Evaluation
Each executable is run against the test corpus and the
selected triples evaluated against the gold triple se-
lection set. Since this is formally a relatively simple
task of selecting a subset of a given set, we will use
for evaluation standard precision, recall and F mea-
sures. In addition, other appropriate metrics will be
explored?for instance, certain metrics for extrac-
tive summarisation (which is to some extent a simi-
lar task).
The organizers will explore whether it will be fea-
sible to select and annotate some test examples from
a different corpus and have the systems evaluated on
these as a separate task.
7 Schedule
Table 1 presents the different tasks, protagonists and
the schedule involved in the organization of the chal-
lenge. The challenge proper will take place between
November 2012 and May/June 2013.
8 Organizers
Nadjet Bouayad-Agha has been a lecturer and re-
searcher at DTIC, UPF, since 2002. She obtained
her PhD on Text Planning in 2001 from the Univer-
sity of Brighton and has been working ever since her
postgraduate studies at the University of Paris VII in
NLG, more specifically on Text Planning. In recent
years her focus has been on how to exploit semantic
web representations and technologies for Text Plan-
ning in general and content selection in particular.
Gerard Casamayor is a PhD student at DTIC,
UPF, working on text planning from general-
purpose semantic data. His main interests are ma-
chine learning and interactive, collaborative text
planning. As part of his thesis, he is developing a
text planning approach that can be trained directly
by domain experts, minimizing the need of encoding
or annotating prior knowledge about how to solve
the task.
Chris Mellish has been a professor at the Univer-
sity of Aberdeen since 2003, when he moved from a
similar position at the University of Edinburgh. He
has been doing research in NLG since 1984 and or-
ganised the second European NLG workshop. His
work on content selection includes the opportunis-
tic planning approach used by the ILEX system and
a rule-based approach to content selection from se-
mantic web data presented in ENLG 2011.
Leo Wanner has been ICREA Research Profes-
sor at DTIC, UPF, since 2005. Before, he was
148
What? Who? When?
Data gathering and preparation Organizers Summer 2012
Working dataset selection and annotation Organizers and Participants Sept/Oct 2012
Data Release Organizers November 2012
Evaluation dataset selection and annotation Organizers and Participants May 2013
Evaluation Organizers June 2013
Publication@INLG Organizers August 2013
Table 1: Content Selection Challenge Organization Schedule
affiliated as Assistant Professor with the Univer-
sity of Stuttgart. Wanner is involved in research
on multilingual text generation since the late 80ies.
Among his research foci are user-oriented con-
tent selection and the interface between language-
independent ontology-based and linguistic represen-
tations in text generation.
References
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive Content Selection for Concept-to-Text Genera-
tion. Proceedings of the Joint Human Language Tech-
nology and Empirical Methods in Natural Language
Processing Conferences (HLT/EMNLP-2005) Van-
couver, Canada.
Christian Bizer, Tom Heath and Tim Berners-Lee 2009.
Linked Data - The Story So Far. International Journal
on Semantic Web and Information Systems 5(3) 1?22
Kalina Bontcheva and Yorick Wilks 2004. Automatic
Report Generation from Ontologies: the MIAKT ap-
proach Nineth International Conference on Appli-
cations of Natural Language to Information Systems
(NLDB?2004) 324?335.
Nadjet Bouayad-Agha, Gerard Casamayor and Leo Wan-
ner. 2011. Content selection from an ontology-based
knowledge base for the generation of football sum-
maries Proceedings of the 13th European Workshop
on Natural Language Generation (ENLG?2011) 72?
81 Nancy, France.
Yintang Dai, Shiyong Zhang, Jidong Chen, Tianyuan
Chen and Wei Zhang. 2010 Semantic Network Lan-
guage Generation based on a Semantic Networks Seri-
alization Grammar World Wide Web 13:307341
Dana Danne?lls, Mariana Damova, Ramona Enache and
Milen Chechev. 2012 Multilingual Online Genera-
tion from Semantic Web Ontologies Proceedings of
the 21st International Conference on World Wide Web
(WWW?12) 239?242
Seniz Demir, Sandra Carberry and Kathleen F. McCoy.
2010. A Discourse-Aware Graph-Based Content-
Selection Framework. Proceedings of the Interna-
tional Language Generation Conference. Sweden.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Sta-
tistical Acquisition of Content Selection Rules for Nat-
ural Language Generation. Proceedings of the 2003
conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Sapporo, Japan.
Dimitrios Galanis and Ion Androutsopoulos. 2007. Gen-
erating Multilingual Personalized Descriptions from
OWL Ontologies on the Semantic Web: the Natu-
ralOWL System. Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation
(ENLG07)
Pamela W. Jordan and Marilyn A. Walker 2005 Learning
content selection rules for generating object descrip-
tions in dialogue Journal of Artificial Intelligence Re-
search 24, 157?194.
Colin Kelly, Ann Copestake, and Nikiforos Karamanis.
2009 Investigating content selection for language gen-
eration using machine learning. Proceedings of the
12th European Workshop on Natural Language Gen-
eration.. 130?137.
Chris Mellish and Jeff Z. Pan. 2008 Language Di-
rected Inference from Ontologies. Artificial Intelli-
gence. 172(10):1285-1315.
Mick ODonnell, Chris Mellish, Jon Oberlander, and Alis-
tair Knott. 2001. ILEX: an architecture for a dynamic
hypertext generation system. Natural Language Engi-
neering. 7(3):225?250.
Richard Power and Allan Third 2010 Expressing OWL
axioms by English sentences: dubious in theory, fea-
sible in practice Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (CI-
CLING?01). 1006?1013.
Graham Wilcok and Kristiina Jokinen 2003 Generat-
ing Responses and Explanations from RDF/XML and
DAML+OIL. IJCAI03 Workshop on Knowledge and
Reasoning in Practical Dialogue Systems. 58?63.
149
Proceedings of the 14th European Workshop on Natural Language Generation, pages 98?102,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Overview of the First Content Selection Challenge from Open Semantic
Web Data
Nadjet Bouayad-Agha1
Gerard Casamayor1
Leo Wanner1,2
1DTIC, University Pompeu Fabra
2Institucio? Catalana de Recerca i Estudis Avanc?ats
Barcelona, Spain
firstname.lastname@upf.edu
Chris Mellish
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
In this overview paper we present the out-
come of the first content selection chal-
lenge from open semantic web data, fo-
cusing mainly on the preparatory stages
for defining the task and annotating the
data. The task to perform was described
in the challenge?s call as follows: given a
set of RDF triples containing facts about
a celebrity, select those triples that are re-
flected in the target text (i.e., a short bi-
ography about that celebrity). From the
initial nine expressions of interest, finally
two participants submitted their systems
for evaluation.
1 Introduction
In (Bouayad-Agha et al, 2012), we presented the
NLG challenge of content selection from seman-
tic web data. The task to perform was described
as follows: given a set of RDF triples contain-
ing facts about a celebrity, select those triples that
are reflected in the target text (i.e., a short biogra-
phy about that celebrity). The task first required
a data preparation stage that involved the follow-
ing two subtasks: 1) data gathering and prepara-
tion, that is, deciding which data and texts to use,
then downloading and pairing them, and 2) work-
ing dataset selection and annotation, that is, defin-
ing the criteria/guidelines for determining when a
triple is marked as selected in the target text, and
producing a corpus of triples annotated for selec-
tion.
There were initially nine interested participants
(including the two organizing parties). Five of
which participated in the (voluntary) triple anno-
tation rounds.1 In the end, only two participants
submitted their systems:
1We would like to thank Angelos Georgaras and Stasinos
Konstantopoulos from NCSR (Greece) for their participation
in the annotation rounds.
UA: Roman Kutlak, Chris Mellish and Kees van
Deemter. Department of Computing Science
, University of Aberdeen, Scotland (UK).
UIC: Hareen Venigalla and Barbara Di iEugenio.
Department of Computer Science, University
of Illinois at Chicago (USA).
Before the presentation of the baseline evalua-
tion of the submitted systems and the discussion
of the results (Section 4), we outline the two data
preparation subtasks (Sections 2 and 3). In Sec-
tion 5, we then sketch some conclusions with re-
gard to the achievements and future of the con-
tent selection task challenge. More details about
the data, annotation and resources described in this
overview, as well as links for downloading the data
and other materials (e.g., evaluation results, code,
etc.) are available on the challenge?s website.2
2 Data gathering and preparation
We chose Freebase as our triple datastore.3,4 We
obtained the triple set for each person in the Turtle
format (ttl) by grepping the official Freebase RDF
dump released on the 30th of December 2012 for
all triples whose subject is the person?s URI; cer-
tain meta-data and irrelevant triples (i.e., triples
with specific namespaces such as ?base? or ?com-
mon?) have been filtered out.
Each triple set is paired with the person?s sum-
mary biography typically available in Wikipedia,
which consists of the first paragraph(s) preceding
the page?s table of contents5
Our final corpus consists of 60000+ pairs, all of
which follow two restrictions that are supposed to
2http://www.taln.upf.edu/cschallenge2013/
3http://www.freebase.com
4For a comparison between Freebase and DBPedia, see
http://wiki.freebase.com/wiki/DBPedia.
5For example, the first four paragraphs in the follow-
ing page constitute the summary biography of that person:
http://en.wikipedia.org/wiki/George Clooney.
98
maximize the chances of having interesting pairs
with sufficient original and selected input triples
for the challenge. Firstly, the number of unique
predicates in the input ttl must be greater than
10. The number 10 is estimated based on the
fact that a person?s nationality, date and place of
birth, profession, type and gender are almost al-
ways available and selected, such that we need a
somewhat large set to select content from in or-
der to make the task minimally challenging. Sec-
ondly, the Wikipedia-extracted summary biogra-
phy must contain more than 5 anchors and at least
20% of the available anchors, where an anchor is
a URI in the text (i.e., external href attribute value
in the html) pointing to another Wikipedia article
which is directly related to that person. Given that
most Freebase topics have a corresponding DBPe-
dia entity with a Wikipedia article, anchors found
in the introductory text are an indicator of potential
relevant facts available in Freebase and are com-
municated in the text. In other words, the anchor
threshold restriction is useful to discard pairs with
very few triples to annotate. We found this crite-
rion more reliable than the absolute length of the
text which is not necessarily proportional with the
number of triples available for that person.
3 Working Dataset selection and
annotation
The manual annotation task consisted in emulat-
ing the content selection task of a Natural Lan-
guage Generation system, by marking in the triple
dataset associated with a person the triples predi-
cated in the summary biography of that person ac-
cording to a set of guidelines. We performed two
rounds of annotations. In the first round, partic-
ipants were asked to select content for the same
three celebrities. The objectives of this annota-
tion, in which five individuals belonging to four
distinct institutions participated, were 1) for par-
ticipants to get acquainted with the content selec-
tion task envisaged, the domain and guidelines,
2) to validate the guidelines, and 3) to formally
evaluate the complexity of the task by calculat-
ing inter-annotator agreement. For the latter we
used free-marginal multi-rater Kappa, as it seemed
suited for the annotation task (i.e. independent rat-
ings, discrete categories, multiple raters, annota-
tors are not restricted in how they distribute cat-
egories across cases) (Justus, 2005). We obtained
an average Kappa of 0.92 across the three pairs for
the 5 annotators and 2 categories (selected, not se-
lected), which indicates a high level of agreement
and therefore validates our annotation guidelines.
Our objective for the second round of annota-
tions was to obtain a dataset for participants to
work with. In the end, we gathered 344 pairs from
5 individuals of 5 distinct institutions. It should be
noted that although both rounds of annotations fol-
low the anchor restriction presented in Section 2,
the idea to set a minimum number of predicates
for the larger corpus of 60000+ pairs came forth
after analysing the results of the second round and
noting the data sparsity in some pairs. In what fol-
lows, we detail how the triples were presented to
human annotators and what were the annotation
criteria set forth in the guidelines.
3.1 Data presentation
A machine-readable triple consists of a subject
which is a Freebase machine id (mid), a predicate
and an object which can either be a Freebase mid
or a literal, as shown in the following two triples:
ns:m.0dvld
ns:people.person.spouse_s
ns:m.02kknf3 .
ns:m.0dvld
ns:people.person.date_of_birth
"1975-10-05"??xsd:datetime .
Triples were transformed into a human-readable
form. In particular, each mid in object position
(e.g., 02kknf3) was automatically mapped onto
an abbreviated description of the Freebase topic
it refers to. Thus, the triples above have been
mapped onto a tabular form consisting of (1) pred-
icate, (2) object description, (3) object id, and (4)
object types (for literals):
(1) /people/person/spouse_s
(2) "1998-11-22 - Jim Threapleton -
2001-12-13 - Marriage -
Freebase Data Team - Marriage"
(3) /m/02kknf3
(1) /people/person/date_of_birth
(2) value
(3) "1975-10-05"
(4) "datetime"
For each triple thus presented, annotators were
asked to mark 1) whether it was selected, 2) in
which sentence(s) of the text did it appear, and
3) which triples, if any, are its coreferents. Two
triples are coreferent if their overlap in meaning is
such that either of them can be selected to repre-
sent the content communicated by the same text
99
fragment and as such should not count as two sep-
arate triples in the evaluation. Thus, the same text
might say He is probably best known for his stint
with heavy metal band Godsmack and He has also
toured and recorded with a number of other bands
including Detroit based metal band Halloween
?The Heavy Metal Horror Show? . . . , thus refer-
ring in two different sentences to near-equivalent
triples /music/artist/genre ??Heavy
metal" and /music/artist/genre
??Hard rock".
3.2 Annotation criteria
Annotators were asked to first read the text care-
fully, trying to identify propositional units (i.e.,
potential triples) and then to associate each iden-
tified propositional unit with zero, one or more
(coreferent) triples according to the following
rules:
Rule 1. One cannot annotate facts that are not
predicated and cannot be inferred from predicates
in the text. In other words, all facts must be
grounded in the text. For example, in the sentence
He starred in Annie Hall, the following is pred-
icated: W.H.has profession actor and
W.H. acted in film Annie Hall. The
former fact can be inferred from the latter. How-
ever, the following is not predicated: (1) Person
has name W.H., (2) W.H. is Male, and (3)
W.H. is Person.
Rule 2. In general, one can annotate more
generic facts if they can be inferred from more
specific propositions in the text, but one cannot
annotate specific facts just because a more gen-
eral proposition is found in the text. In the exam-
ple He was a navigator, we can mark the triples
Person has profession Sailor as well
as Person has profession Navigator
(we would also mark them as coreferent). How-
ever, given the sentence He was a sailor, we can-
not mark the triple Person has profession
Navigator, unless we can infer it from the text
or world knowledge.
Rule 3. One can annotate specific facts from a
text where the predicate is too vague or general if
the facts can be inferred from the textual context,
from the available data, or using world knowledge.
This rule subsumes four sub-cases:
Rule 3.1. The predicate in the proposition is too
vague or general and can be associated with mul-
tiple, more specific triples. In this case, do not
select any triple. In the example Film A was a
great commercial success, we have several triples
associating the celebrity with Film A, as direc-
tor, actor, writer, producer and composer and none
of them with a predicate resembling ?commercial
success?. In this case there are no triples that can
be associated with the text.
Rule 3.2. The predicate in the proposition is
too vague or general, but according to the data
there is just one specific triple it can be associated
with. In this case, select that triple. In the ex-
ample Paris released Confessions of an Heiress,
the term released could be associated with au-
thored, wrote or published. However, there is only
one triple associating that subject with that object,
which matches one of the interpretations (i.e., au-
thoring) of the predicate. Therefore that triple can
be selected.
Rule 3.3. The predicate in the proposition is
too vague or general, but one or more specific
triples can be inferred using world knowl-
edge. In this case, select all. The sentence
He is also a jazz clarinetist who performs
regularly at small venues in Manhattan, can
be associated with the available triples W.H.
profession Clarinetist and W.H.
music/group member/instruments played
Clarinet, even though for this latter triple
the person being in a group is not mentioned
explicitly. However, this can be inferred from
basic world knowledge.
Rule 3.4. The predicate in the proposition is
too vague or general, but one or more specific
triples can be inferred using the textual context.
In this case, select all. In the example By the
mid-1960s Allen was writing and directing films
. . . Allen often stars in his own films . . . Some of
the best-known of his over 40 films are Annie Hall
(1977) . . . , the relations of the person with the
film Annie Hall are that of writer, director and
actor, as supported by the previous text. There-
fore we would annotate facts stating that the per-
son wrote, directed and starred in Annie Hall.
However, we wouldn?t annotate composer or pro-
ducer triples if they existed.
Rule 4. A proposition can be associated
with multiple facts with identical or over-
lapping meanings. In the example, Woody
Allen is a musician, we have the triples
W.H occupation musician and W.H
profession musician, which have near
100
identical meanings. Therefore, we mark both
triples and indicate that they co-refer. The
sentence Woody Allen won prize as best director
for film Manhattan, on the other hand, can be
associated with non-coreferring triples W.H won
prize and W.H. directed Manhattan.
Rule 5. If the text makes reference to a set of
facts but it does not enumerate them explicitly, and
there is no reason to believe it makes reference to
any of them in particular, then do not annotate in-
dividual facts. Thus, sentence Clint Eastwood has
seven children does not warrant marking each of
the seven children triples as selected, given that
they are not enumerated explicitly.
Rule 6. If the text makes a clear and unam-
biguous reference to a fact, do not annotate any
other facts, even though they can be inferred from
it. In other words, as explained in Rule 1, all an-
notated triples must be grounded in the text. In
the sentence For his work in the films Unforgiven
(1992) and Million Dollar Baby (2004), Eastwood
won Academy Awards for Best Director and Pro-
ducer of the Best Picture, we can infer from world
knowledge that the celebrity was nominated prior
to winning the award in those categories. How-
ever, the text makes a clear reference only to the
fact that he won the award and there is no reason
to believe that it is also predicating the fact that the
celebrity was nominated.
4 Baseline evaluation
Briefly speaking, the UA system uses a general
heuristic based on the cognitive notion of com-
munal common ground regarding each celebrity,
which is approximated by scoring each lexicalized
triple (or property) associated with a celebrity ac-
cording to the number of hits of the Google search
API. Only the top-ranked triples are selected (Kut-
lak et al 2013). The UIC system uses a small
set of rules for the conditional inclusion of pred-
icates that was derived offline from the statistical
analysis of the co-occurrence between predicates
that are about the same topic or that share some
shared arguments; only the best performing rules
tested against a subset of the development set are
included (Venigalla and Di Eugenio, 2013).
For the baseline evaluation, we used the devel-
opment set obtained in the second round annota-
tion (see Section 3). However, we only consider
pairs obtained during the second round annotation
that 1) follow both restrictions presented in Sec-
Baseline UIC UA
Precision 49 64 47
Recall 67 50 39
F1 51 51 42
Table 1: Baseline evaluation results (%)
tion 2, and 2) have no coreferring triples. This
last restriction was added to minimize errors be-
cause we observed that annotators were not al-
ways consistent in their annotation of triple coref-
erence.6 We therefore considered 188 annotations
from the 344 annotations of the development set.
Of these, we used 40 randomly selected annota-
tions for evaluating the systems and 144 for es-
timating a baseline that only considers the top 5
predicates (i.e., the predicates most often selected)
and the type-predicate.7.
The evaluation results of the three systems
(baseline, UIC and UA) are presented in Table 1.
The figures in the table were obtained by compar-
ing the triples selected and rejected by each system
against the manual annotation. The performance
of the baseline is quite high. The UA system based
on a general heuristic scores lower than the base-
line, whilst the UIC system has a better precision
than the baseline, albeit a lower recall. This might
be due, as the UA authors observe in their sum-
mary (Venigalla and Di Eugenio, 2013), to ?the
large number of predicates that are present only
in a few files . . . [which] makes it harder to de-
cide whether we have to include these predicates
or not.?
5 Conclusions
We have given an overview of the first content se-
lection challenge from open semantic web data,
focusing on the rather extensive and challenging
technological and methodological work involved
in defining the task and preparing the data. Unfor-
tunately, despite agile participation in these early
6Type-predicate triples were filtered out of the annotated
files in the development set whilst they were included in the
large corpus made available to the candidates. Therefore,
we added type-predicate triples in the development set a
posteriori for this evaluation. These type-predicate triples
might be coreferring with other triples, say ns:m.08rd51
ns:type.object.type ns:film.actor and
ns:m.08rd5 people/person/profession
"Actor" /m/02hrh1q . Nonetheless, this was not
taken into account in the evaluation.
7The top 5 predicates were (in descending order of fre-
quency): music track, film actor, profession, date of birth and
nationality
101
preparatory stages, the number of submitted sys-
tems was limited. Both of the presented systems
were data-intensive in that they usedeither a pool
of textual knowledge or the corpus of triple data
provided by the challenge in order to select the
most relevant data.
Unlike several previous challenges that involve
more traditional NLG tasks (e.g., surface realiza-
tion, referring expression generation), content se-
lection from large input semantic data is a rela-
tively new research endeavour in the NLG com-
munity that coincides with the rising interest in
statistical approaches to NLG and dates back, to
the best of our knowledge, to (Duboue and McK-
eown, 2003). Furthermore, although we had ini-
tially planned to produce a training set for the
task, the cost of manual annotation turned out
to be prohibitive and the resulting corpus was
only fit for development and baseline evaluation.
Despite these setbacks, we believe that open se-
mantic web data is a promising test-bed and ap-
plication field for NLG-oriented content selec-
tion (Bouayad-Agha et al, 2013) and trust that
this first challenge has prepared the ground for
follow up challenges with a larger participation.
We would also like to encourage researchers from
NLG and Semantic Web research fields to exploit
the framework and materials developed during the
course of this challenge to advance research in
content selection.
References
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2013. Natural Language Generation in the
Context of the Semantic Web. Submitted to the Se-
mantic Web Journal.
Nadjet Bouayad-Agha, Gerard Casamayor, Chris Mel-
lish, and Leo Wanner. 2012. Content Selection from
Semantic Web Data. INLG ?12 Proceedings of the
Seventh International Natural Language Generation
Conference. Pages 146-149.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical Acquisition of Content Selection Rules
for Natural Language Generation Proceedings of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP). Pages 121?128.
Randolph, Justus J. 2005. Free-marginal multirater
kappa (multirater Kfree): An alternative to fleiss
fixed-marginal multirater kappa. Presented as the
Joensuu University Learning and Instruction Sym-
posium.
Roman Kutlak, Chris Mellish and Kees van Deemter
2013. Content Selection Challenge University of
Aberdeen entry Proceedings of the 14th European
Natural Language Generation (ENLG) Workshop.
Hareen Venigalla and Barbara Di Eugenio. 2013. UIC-
CSC: The Content Selection Challenge Entry from
the University of Illinois at Chicago Proceedings
of the 14th European Natural Language Generation
(ENLG) Workshop.
102
