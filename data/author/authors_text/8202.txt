c? 2003 Association for Computational Linguistics
Word Reordering and a Dynamic
Programming Beam Search Algorithm for
Statistical Machine Translation
Christoph Tillmann? Hermann Ney?
IBM T. J. Watson Research Center RWTH Aachen
In this article, we describe an efficient beam search algorithm for statistical machine translation
based on dynamic programming (DP). The search algorithm uses the translation model presented
in Brown et al (1993). Starting from a DP-based solution to the traveling-salesman problem,
we present a novel technique to restrict the possible word reorderings between source and target
language in order to achieve an efficient search algorithm. Word reordering restrictions especially
useful for the translation direction German to English are presented. The restrictions are gener-
alized, and a set of four parameters to control the word reordering is introduced, which then can
easily be adopted to new translation directions. The beam search procedure has been successfully
tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian
Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil
task, a sentence can be translated in a few seconds, only a small number of search errors occur,
and there is no performance degradation as measured by the word error criterion used in this
article.
1. Introduction
This article is about a search procedure for statistical machine translation (MT). The
task of the search procedure is to find the most likely translation given a source sen-
tence and a set of model parameters. Here, we will use a trigram language model and
the translation model presented in Brown et al (1993). Since the number of possible
translations of a given source sentence is enormous, we must find the best output
without actually generating the set of all possible translations; instead we would like
to focus on the most likely translation hypotheses during the search process. For this
purpose, we present a data-driven beam search algorithm similar to the one used in
speech recognition search algorithms (Ney et al 1992). The major difference between
the search problem in speech recognition and statistical MT is that MT must take into
account the different word order for the source and the target language, which does
not enter into speech recognition. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes
a dynamic programming (DP)?based search algorithm for statistical MT that mono-
tonically translates the input sentence from left to right. The word order difference is
dealt with using a suitable preprocessing step. Although the resulting search proce-
dure is very fast, the preprocessing is language specific and requires a lot of manual
? IBM T. J. Watson Research Center, Yorktown Heights, NY 10598. E-mail: ctill@us.ibm.com. The research
reported here was carried out while the author was with Lehrstuhl fu?r Informatik VI, Computer
Science Department, RWTH Aachen.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen, D-52056 Aachen,
Germany. E-mail: ney@informatik.rwth-aachen.de.
98
Computational Linguistics Volume 29, Number 1
work. Currently, most search algorithms for statistical MT proposed in the literature
are based on the A? concept (Nilsson 1971). Here, the word reordering can be easily
included in the search procedure, since the input sentence positions can be processed
in any order. The work presented in Berger et al (1996) that is based on the A? concept,
however, introduces word reordering restrictions in order to reduce the overall search
space.
The search procedure presented in this article is based on a DP algorithm to solve
the traveling-salesman problem (TSP). A data-driven beam search approach is pre-
sented on the basis of this DP-based algorithm. The cities in the TSP correspond to
source positions of the input sentence. By imposing constraints on the possible word
reorderings similar to that described in Berger et al (1996), the DP-based approach
becomes more effective: when the constraints are applied, the number of word re-
orderings is greatly reduced. The original reordering constraint in Berger et al (1996)
is shown to be a special case of a more general restriction scheme in which the word
reordering constraints are expressed in terms of simple combinatorical restrictions on
the processed sets of source sentence positions.1 A set of four parameters is given to
control the word reordering. Additionally, a set of four states is introduced to deal
with grammatical reordering restrictions (e.g., for the translation direction German to
English, the word order difference between the two languages is mainly due to the
German verb group. In combination with the reordering restrictions, a data-driven
beam search organization for the search procedure is proposed. A beam search prun-
ing technique is conceived that jointly processes partial hypotheses according to two
criteria: (1) The partial hypotheses cover the same set of source sentence positions,
and (2) the partial hypotheses cover sets C of source sentence positions of equal car-
dinality. A partial hypothesis is said to cover a set of source sentence positions when
exactly the positions in the set have already been processed in the search process. To
verify the effectiveness of the proposed techniques, we report and analyze results for
two translation tasks: the German to English Verbmobil task and French to English
Canadian Hansards task.
The article is structured as follows. Section 2 gives a short introduction to the trans-
lation model used and reports on other approaches to the search problem in statistical
MT. In Section 3, a DP-based search approach is presented, along with appropriate
pruning techniques that yield an efficient beam search algorithm. Section 4 reports
and analyzes translation results for the different translation directions. In Section 5,
we conclude with a discussion of the achieved results.
2. Previous Work
2.1 IBM Translation Approach
In this article, we use the translation model presented in Brown et al (1993), and the
mathematical notation we use here is taken from that paper as well: a source string
f J1 = f1 ? ? ? fj ? ? ? fJ is to be translated into a target string eI1 = e1 ? ? ? ei ? ? ? eI. Here, I is the
length of the target string, and J is the length of the source string. Among all possible
target strings, we will choose the string with the highest probability as given by Bayes?
1 The word reordering restriction used in the search procedure described in Berger et al (1996) is not
mentioned in Brown et al (1993), although exactly the translation model described there is used.
Equivalently, we use exactly the translation model described in Brown et al (1993) but try different
reordering restrictions for the DP-based search procedure.
99
Tillmann and Ney DP Beam Search for Statistical MT
Figure 1
Architecture of the statistical translation approach based on Bayes? decision rule.
decision rule:
e?I1 = arg max
eI1
{Pr(eI1 | f
J
1)}
= arg max
eI1
{Pr(eI1) ? Pr(f
J
1 | eI1)} (1)
Pr(eI1) is the language model of the target language, whereas Pr(f
J
1 | eI1) is the string
translation model. The language model probability is computed using a trigram lan-
guage model. The string translation probability Pr(f J1 | eI1) is modeled using a series of
five models of increasing complexity in training. Here, the model used for the trans-
lation experiments is the IBM-4 model. This model uses the same parameter set as
the IBM-5 model, which in preliminary experiments did not yield better translation
results. The actual implementation used during the experiments is described in Al-
Onaizan et al (1999) and in Och and Ney (2000). The argmax operation denotes the
search problem (i.e., the generation of the output sentence in the target language). The
overall architecture of the statistical translation approach is summarized in Figure 1.
In general, as shown in this figure, there may be additional transformations to make
the translation task simpler for the algorithm. The transformations may range from
simple word categorization to more complex preprocessing steps that require some
parsing of the source string. In this article, however, we will use only word catego-
100
Computational Linguistics Volume 29, Number 1
rization as an explicit transformation step. In the search procedure both the language
and the translation model are applied after the text transformation steps. The following
?types? of parameters are used for the IBM-4 translation model:
Lexicon probabilities: We use the lexicon probability p(f | e) for translating the
single target word e as the single source word f . A source word f may
be translated by the ?null? word e0 (i.e., it does not produce any target
word e). A translation probability p(f | e0) is trained along with the regular
translation probabilities.
Fertilities: A single target word e may be aligned to n = 0, 1 or more source words.
This is explicitly modeled by the fertility parameter ?(n | e): the probability
that the target word e is translated by n source words is ?(n | e). The
fertility for the ?null? word is treated specially (for details see Brown et al
[1993]). Berger et al (1996) describes the extension of a partial hypothesis
by a pair of target words (e?, e), where e? is not connected to any source
word f . In this case, the so-called spontaneous target word e? is accounted
for with the fertility. Here, the translation probability ?(0 | e?) and no-
translation probability p(f | e?).
Class-based distortion probabilities: When covering a source sentence position
j, we use distortion probabilities that depend on the previously covered
source sentence positions (we say that a source sentence position j is cov-
ered for a partial hypothesis when it is taken account of in the translation
process by generating a target word or the ?null? word e0 ). In Brown et
al. (1993), two types of distortion probabilities are distinguished: (1) the
leftmost word of a set of source words f aligned to the same target word
e (which is called the ?head?) is placed, and (2) the remaining source
words are placed. Two separate distributions are used for these two cases.
For placing the ?head? the center function center(i) (Brown et al [1993]
uses the notation i) is used: the average position of the source words
with which the target word ei?1 is aligned. The distortion probabilities
are class-based: They depend on the word class F(f ) of a covered source
word f as well as on the word class E(e) of the previously generated target
word e. The classes are automatically trained (Brown et al 1992).
When the IBM-4 model parameters are used during search, an input sentence can be
processed one source position at a time in a certain order primarily determined by the
distortion probabilities. We will use the following simplified set of translation model
parameters: lexicon probabilities p(f | e) and distortion probabilities p(j | j?, J). Here, j
is the currently covered input sentence position and j? is the previously covered input
sentence position. The input sentence length J is included, since we would like to think
of the distortion probability as normalized according to J. No fertility probabilities or
?null? word probabilities are used; thus each source word f is translated as exactly one
target word e and each target word e is translated as exactly one source word f . The
simplified notation will help us to focus on the most relevant details of the DP-based
search procedure. The simplified set of parameters leads to an unrealistic assumption
about the length of the source and target sentence, namely, I = J. During the translation
experiments we will, of course, not make this assumption. The implementation details
for using the full set of IBM-4 model parameters are given in Section 3.9.2.
101
Tillmann and Ney DP Beam Search for Statistical MT
2.2 Search Algorithms for Statistical Machine Translation
In this section, we give a short overview of search procedures used in statistical MT:
Brown et al (1990) and Brown et al (1993) describe a statistical MT system that is based
on the same statistical principles as those used in most speech recognition systems
(Jelinek 1976). Berger et al (1994) describes the French-to-English Candide translation
system, which uses the translation model proposed in Brown et al (1993). A detailed
description of the decoder used in that system is given in Berger et al (1996) but has
never been published in a paper: Throughout the search process, partial hypotheses
are maintained in a set of priority queues. There is a single priority queue for each
subset of covered positions in the source string. In practice, the priority queues are
initialized only on demand; far fewer than the full number of queues possible are actu-
ally used. The priority queues are limited in size, and only the 1,000 hypotheses with
the highest probability are maintained. Each priority queue is assigned a threshold
to select the hypotheses that are going to be extended, and the process of assigning
these thresholds is rather complicated. A restriction on the possible word reorderings,
which is described in Section 3.6, is applied.
Wang and Waibel (1997) presents a search algorithm for the IBM-2 translation
model based on the A? concept and multiple stacks. An extension of this algorithm
is demonstrated in Wang and Waibel (1998). Here, a reshuffling step on top of the
original decoder is used to handle more complex translation models (e.g., the IBM-3
model is added). Translation approaches that use the IBM-2 model parameters but are
based on DP are presented in Garc??a-Varea, Casacuberta, and Ney (1998) and Niessen
et al (1998). An approach based on the hidden Markov model alignments as used
in speech recognition is presented in Tillmann, Vogel, Ney, and Zubiaga (1997) and
Tillmann, Vogel, Ney, Zubiaga, and Sawaf (1997). This approach assumes that source
and target language have the same word order, and word order differences are dealt
with in a preprocessing stage. The work by Wu (1996) also uses the original IBM model
parameters and obtains an efficient search algorithm by restricting the possible word
reorderings using the so-called stochastic bracketing transduction grammar.
Three different decoders for the IBM-4 translation model are compared in Germann
et al (2001). The first is a reimplementation of the stack-based decoder described in
Berger et al (1996). The second is a greedy decoder that starts with an approximate
solution and then iteratively improves this first rough solution. The third converts
the decoding problem into an integer program (IP), and a standard software package
for solving IP is used. Although the last approach is guaranteed to find the optimal
solution, it is tested only for input sentences of length eight or shorter.
This article will present a DP-based beam search decoder for the IBM-4 translation
model. The decoder is designed to carry out an almost full search with a small number
of search errors and with little performance degradation as measured by the word error
criterion. A preliminary version of the work presented here was published in Tillmann
and Ney (2000).
3. Beam Search in Statistical Machine Translation
3.1 Inverted Alignment Concept
To explicitly describe the word order difference between source and target language,
Brown et al (1993) introduced an alignment concept, in which a source position j is
mapped to exactly one target position i:
regular alignment: j ? i = aj
102
Computational Linguistics Volume 29, Number 1
e
.
In
this
case
my
colleague
can
k
not
visit
on
the
fourth
of
May
m K S
you
a v M n
c
h
t
bF
e
s
u
c
h
e
n
.
n
d
e
a
n
n
o
e
g
e
e e
n
i a
i
I
i
ii
m
e
s
i
r
t
e
l
ll
l
a m
n
Figure 2
Regular alignment example for the translation direction German to English. For each German
source word there is exactly one English target word on the alignment path.
An example for this kind of alignment is given in Figure 2, in which each German
source position j is mapped to an English target position i. In Brown et al (1993), this
alignment concept is used for model IBM-1 through model IBM-5. For search purposes,
we use the inverted alignment concept as introduced in Niessen et al (1998) and Ney
et al (2000). An inverted alignment is defined as follows:
inverted alignment: i ? j = bi
Here, a target position i is mapped to a source position j. The coverage constraint for
an inverted alignment is not expressed by the notation: Each source position j should
be ?hit? exactly once by the path of the inverted alignment bI1 = b1 ? ? ? bi ? ? ? bI. The
advantage of the inverted alignment concept is that we can construct target sentence
hypotheses from bottom to top along the positions of the target sentence. Using the
inverted alignments in the maximum approximation, we rewrite equation (1) to obtain
the following search criterion, in which we are looking for the most likely target
103
Tillmann and Ney DP Beam Search for Statistical MT
Figure 3
Illustration of the transitions in the regular and in the inverted alignment model. The regular
alignment model (left figure) is used to generate the sentence from left to right; the inverted
alignment model (right figure) is used to generate the sentence from bottom to top.
sentence eI1 of length I = J for an observed source sentence f
J
1 of length J:
max
I
{
p(J | I) ? max
eI1
{p(eI1) ? p(f
J
1 | eI1)}
}
(2)
?= max
I
{
p(J | I) ? max
eI1
{
I
?
i=1
p(ei | ei?1, ei?2) ? max
bI1
I
?
i=1
[p(bi | bi?1, J) ? p(fbi | ei)]
}}
= max
I
{
p(J | I) ? max
eI1,b
I
1
{
I
?
i=1
[p(ei | ei?1, ei?2) ? p(bi | bi?1, J) ? p(fbi | ei)]
}}
The following notation is used: ei?1, ei?2 are the immediate predecessor target words,
ei is the word to be hypothesized, p(ei | ei?1, ei?2) denotes the trigram language model
probability, p(fbi | ei) denotes the lexicon probability for translating the target word ei
as source word fbi , and p(bi | bi?1, J) is the distortion probability for covering source
position bi after source position bi?1. Note that in equation (2) two products over i are
merged into a single product over i. The translation probability p(f J1 | eI1) is computed in
the maximum approximation using the distortion and the lexicon probabilities. Finally,
p(J | I) is the sentence length model, which will be dropped in the following (it is not
used in the IBM-4 translation model). For each source sentence f J1 to be translated, we
are searching for the unknown mapping that optimizes equation (2):
i ? (bi, ei)
In Section 3.3, we will introduce an auxiliary quantity that can be evaluated recursively
using DP to find this unknown mapping. We will explicitly take care of the coverage
constraint by introducing a coverage set C of source sentence positions that have
already been processed. Figure 3 illustrates the concept of the search algorithm using
inverted alignments: Partial hypotheses are constructed from bottom to top along the
positions of the target sentence. Partial hypotheses of length i?1 are extended to obtain
partial hypotheses of the length i. Extending a partial hypothesis means covering a
source sentence position j that has not yet been covered. For a given grid point in the
104
Computational Linguistics Volume 29, Number 1
Table 1
DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The
outermost loop is over the cardinality of subsets of already visited cities.
input: cities j = 1, . . . , J with distance matrix djj ?
initialization: D({k}, k) := d1k
for each path length c = 2, . . . , J do
for each pair (C, j), where C ? {2, . . . , J} and j ? C and |C| = c do
D(C, j) = min
j??C\{j}
{djj ? + D(C\{j}, j?)}
traceback:
? find shortest tour: D? = min
k?{2,...,J}
[D({2, . . . , J}, k) + dk1]
? recover optimal sequence of cities
translation lattice, the unknown target word sequence can be obtained by tracing back
the translation decisions to the partial hypothesis at stage i = 1. The grid points are
defined in Section 3.3. In the left part of the figure the regular alignment concept is
shown for comparison purposes.
3.2 Held and Karp Algorithm for Traveling-Salesman Problem
Held and Karp (1962) presents a DP approach to solve the TSP, an optimization prob-
lem that is defined as follows: Given are a set of cities {1, . . . , J} and for each pair
of cities j, j? the cost djj ? > 0 for traveling from city j to city j?. We are looking for
the shortest tour, starting and ending in city 1, that visits all cities in the set of cities
exactly once. We are using the notation C for the set of cities, since it corresponds to
a coverage set of processed source positions in MT. A straightforward way to find
the shortest tour is by trying all possible permutations of the J cities. The resulting
algorithm has a complexity of O(J!). DP can be used, however, to find the shortest tour
in O(J2 ? 2J), which is a much smaller complexity for larger values of J. The approach
recursively evaluates the quantity D(C, j):
D(C, j) := costs of the partial tour starting in city 1, ending
in city j, and visiting all cities in C
Subsets of cities C of increasing cardinality c are processed. The algorithm, shown in
Table 1, works because not all permutations of cities have to be considered explicitly.
During the computation, for a pair (C, j), the order in which the cities in C have been
visited can be ignored (except j); only the costs for the best path reaching j has to be
stored. For the initialization the costs for starting from city 1 are set: D({k}, k) = d1k for
each k ? {2, . . . , |C|}. Then, subsets C of increasing cardinality are processed. Finally,
the cost for the optimal tour is obtained in the second-to-last line of the algorithm.
The optimal tour itself can be found using a back-pointer array in which the optimal
decision for each grid point (C, j) is stored.
Figure 4 illustrates the use of the algorithm by showing the ?supergraph? that is
searched in the Held and Karp algorithm for a TSP with J = 5 cities. When traversing
the lattice from left to right following the different possibilities, a partial path to a node
j corresponds to the subset C of all cities on that path together with the last visited
105
Tillmann and Ney DP Beam Search for Statistical MT
Figure 4
Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5
cities. Not all permutations of cities have to be evaluated explicitly. For a given subset of cities
the order in which the cities have been visited can be ignored.
city j. Of all the different paths merging into the node j, only the partial path with the
smallest cost has to be retained for further computation.
3.3 DP-Based Algorithm for Statistical Machine Translation
In this section, the Held and Karp algorithm is applied to statistical MT. Using the
concept of inverted alignments as introduced in Section 3.1, we explicitly take care of
the coverage constraint by introducing a coverage set C of source sentence positions
that have already been processed. Here, the correspondence is according to the fact that
each source sentence position has to be covered exactly once, fulfilling the coverage
constraint. The cities of the more complex translation TSP correspond roughly to triples
(e?, e, j), the notation for which is given below. The final path output by the translation
algorithm will contain exactly one triple (e?, e, j) for each source position j.
The algorithm processes subsets of partial hypotheses with coverage sets C of
increasing cardinality c. For a trigram language model, the partial hypotheses are of
the form (e?, e, C, j), where e?, e are the last two target words, C is a coverage set for
the already covered source positions, and j is the last covered position. The target
word sequence that ends in e?, e is stored as a back pointer to the predecessor partial
hypothesis (and recursively to its predecessor hypotheses) and is not shown in the
notation. Each distance in the TSP now corresponds to the negative logarithm of the
product of the translation, distortion, and language model probabilities. The following
106
Computational Linguistics Volume 29, Number 1
Table 2
DP-based algorithm for statistical MT that consecutively processes subsets C of source
sentence positions of increasing cardinality.
input: source language string f1 ? ? ? fj ? ? ? fJ
initialization
for each cardinality c = 1, 2, . . . , J do
for each pair (C, j), where C ? {1, . . . , J} and j ? C and |C| = c do
for each pair of target words e?, e ? E
Qe?(e, C, j) = p(fj | e) max
e??
j??C\{j}
{p(j | j?, J) ? p(e | e?, e??) ? Qe??(e?, C\{j}, j?)}
traceback:
? find best end hypothesis: max
e,e? ,j
{p($ | e, e?) ? Qe?(e, {1, . . . , J}, j)}
? recover optimal word sequence
auxiliary quantity is defined:
Qe?(e, C, j) := probability of the best partial hypothesis (ei1, bi1), where
C = {bk | k = 1, . . . , i}, bi = j, ei = e, and ei?1 = e?
The above auxiliary quantity satisfies the following recursive DP equation:
Qe?(e, C, j) = p(fj | e) ? max
e??
j??C\{j}
{
p(j | j?, J) ? p(e | e?, e??) ? Qe??(e?, C\{j}, j
?
)
}
Here, j? is the previously covered source sentence position and e?, e?? are the predecessor
words. The DP equation is evaluated recursively for each hypothesis (e?, e, C, j). The
resulting algorithm is depicted in Table 2. Some details concerning the initialization
and the finding of the best target language string are presented in Section 3.4. p($ | e, e?)
is the trigram language probability for predicting the sentence boundary symbol $. The
complexity of the algorithm is O(E3 ? J2 ? 2J), where E is the size of the target language
vocabulary.
3.4 Verb Group Reordering: German to English
The above search space is still too large to translate even a medium-length input
sentence. On the other hand, only very restricted reorderings are necessary; for ex-
ample, for the translation direction German to English, the word order difference is
mostly restricted to the German verb group. The approach presented here assumes a
mostly monotonic traversal of the source sentence positions from left to right.2 A small
number of positions may be processed sooner than they would be in that monotonic
traversal. Each source position then generates a certain number of target words. The
restrictions are fully formalized in Section 3.5.
A typical situation is shown in Figure 5. When translating the sentence monotoni-
cally from left to right, the translation of the German finite verb kann, which is the left
verbal brace in this case, is skipped until the German noun phrase mein Kollege, which
is the subject of the sentence, is translated. Then, the right verbal brace is translated:
2 Also, this assumption is necessary for the beam search pruning techniques to work efficiently.
107
Tillmann and Ney DP Beam Search for Statistical MT
i
.
fourth
the
of
May
this
case
colleague
can
not
visit
In
e
e
F
a
k
n
n
m
i
dI
n
s
l
l
a
m K
o
S .
you
on
my
n
ie
i
n
i
e
m
l
l
e
g
e
b
e
s
u
c
h
e
n
c
h
t
i
e
r
t
e
n
a v
a
M
Figure 5
Word reordering for the translation direction German to English: The reordering is restricted
to the German verb group.
The infinitive besuchen and the negation particle nicht. The following restrictions are
used: One position in the source sentence may be skipped for a distance of up to L = 4
source positions, and up to two source positions may be moved for a distance of at
most R = 10 source positions (the notation L and R shows the relation to the handling
of the left and right verbal brace). To formalize the approach, we introduce four verb
group states S:
? Initial : A contiguous initial block of source positions is covered.
? Skip: One word may be skipped, leaving a ?hole? in the monotonic
traversal.
? Move: Up to two words may be ?moved? from later in the sentence.
? Cover : The sentence is traversed monotonically until the state Initial is
reached.
108
Computational Linguistics Volume 29, Number 1
Initial
Skip
Move Cover
11. vierten
5. Kollege
4. mein
1. In
3. Fall
2. diesem 12. Mai
6. kann
13. .
8. besuchen
7. nicht
10. am
9. Sie
Figure 6
Order in which the German source positions are covered for the German-to-English reordering
example given in Figure 5.
The states Move and Skip both allow a set of upcoming words to be processed sooner
than would be the case in the monotonic traversal. The state Initial is entered whenever
there are no uncovered positions to the left of the rightmost covered position. The
sequence of states needed to carry out the word reordering example in Figure 5 is
given in Figure 6. The 13 source sentence words are processed in the order shown.
A formal specification of the state transitions is given in Section 3.5. Any number of
consecutive German verb phrases in a sentence can be processed by the algorithm. The
finite-state control presented here is obtained from a simple analysis of the German-
to-English word reordering problem and is not estimated from the training data. It
can be viewed as an extension of the IBM-4 model distortion probabilities.
Using the above states, we define partial hypothesis extensions of the following
type:
(S ?, C\{j}, j?) ? (S, C, j)
Not only the coverage set C and the positions j, j?, but also the verb group states S,S ?,
are taken into account. For the sake of brevity, we have omitted the target language
words e, e? in the notation of the partial hypothesis extension. For each extension an
uncovered position is added to the coverage set C of the partial hypothesis, and the
verb group state S may change. A more detailed description of the partial hypoth-
esis extension for a certain state S is given in the next section in a more general
context. Covering the first uncovered position in the source sentence, we use the lan-
109
Tillmann and Ney DP Beam Search for Statistical MT
guage model probability p(e | $, $). Here, $ is the sentence boundary symbol, which
is thought to be at position 0 in the target sentence. The search starts in the hypoth-
esis (Initial , {?}, 0). {?} denotes the empty set, where no source sentence position is
covered. The following recursive equation is evaluated:
Qe?(e,S, C, j) (3)
= p(fj | e) max
e?? ,S? ,j?
(S? ,C\{j},j?)?(S,C,j)
j??C\{j}
{p(j | j?, J) ? p(e | e?, e??) ? Qe??(e?,S ?, C\{j}, j?)}
The search ends in the hypotheses (Initial , {1, . . . , J}, j); the last covered position may
be in the range j ? {J?L, . . . , J}, because some source positions may have been skipped
at the end of the input sentence. {1, . . . , J} denotes a coverage set including all positions
from position 1 to position J. The final translation probability QF is
QF = max
e,e?
j?{J?L,...,J}
p($ | e, e?) ? Qe?(e, Initial , {1, . . . , J}, j) (4)
where p($ | e, e?) denotes the trigram language model, which predicts the sentence
boundary $ at the end of the target sentence. QF can be obtained using an algorithm
very similar to the one given in Table 2. The complexity of the verb group reordering
for the translation direction German to English is O(E3 ? J ? (R2 ? L ? R)), as shown in
Tillmann (2001).
3.5 Word Reordering: Generalization
For the translation direction English to German, the word reordering can be restricted
in a similar way as for the translation direction German to English. Again, the word
order difference between the two languages is mainly due to the German verb group.
During the translation process, the English verb group is decomposed as shown in
Figure 7. When the sentence is translated monotonically from left to right, the trans-
lation of the English finite verb can is moved, and it is translated as the German left
verbal brace before the English noun phrase my colleague, which is the subject of the
sentence. The translations of the infinitive visit and of the negation particle not are
skipped until later in the translation process. For this translation direction, the trans-
lation of one source sentence position may be moved for a distance of up to L = 4
source positions, and the translation of up to two source positions may be skipped
for a distance of up to R = 10 source positions (we take over the L and R notation
from the previous section). Thus, the role of the skipping and the moving are simply
reversed with respect to their roles in German-to-English translation. For the example
translation in Figure 7, the order in which the source sentence positions are covered
is given in Figure 8.
We generalize the two approaches for the different translation directions as fol-
lows: In both approaches, we assume that the source sentence is mainly processed
monotonically. A small number of upcoming source sentence positions may be pro-
cessed earlier than they would be in the monotonic traversal: The states Skip and Move
are used as explained in the preceding section. The positions to be processed outside
the monotonic traversal are restricted as follows:
? The number of positions dealt with in the states Move and Skip is
restricted.
? There are distance restrictions on the source positions processed in those
states.
110
Computational Linguistics Volume 29, Number 1
.
 
In
diesem
Fall
kann
mein
Kollege
Sie
am
vierten
Mai
nicht
besuchen
.
n h
c
o
n
o
v y
o
u 
o
n o
u
u
o MI t
i
s
c
a
s
e
m
y
l
l
e
a
g
e
c
a
n t
i
s
i
t
t
h
e
f
t
h
r
f a
y
Figure 7
Word reordering for the translation direction English to German: The reordering is restricted
to the English verb group.
These restrictions will be fully formalized later in this section. In the state Move, some
source sentence positions are ?moved? from later in the sentence to earlier. After source
sentence positions are moved, they are marked, and the translation of the sentence is
continued monotonically, keeping track of the positions already covered. To formalize
the approach, we introduce four reordering states S:
? Initial : A contiguous initial block of source positions is covered.
? Skip: A restricted number of source positions may be skipped, leaving
?holes? in the monotonic traversal.
? Move: A restricted number of words may be ?moved? from later in the
sentence.
? Cover : The sentence is traversed monotonically until the state Initial is
reached.
To formalize the approach, the following notation is introduced:
rmax(C) = max
c?C
c
111
Tillmann and Ney DP Beam Search for Statistical MT
InitialSkip
Cover
Move
1. In
13. not
2. this
3. case
6. colleague
14. visit
15. . 4. can
5. my
7. you
8. on
9. the
10. fourth
11. of
12. May
Figure 8
Order in which the English source positions are covered for the English-to-German reordering
example given in Figure 7.
lmin(C) = min
c/?C
c
u(C) = card({c | c /? C and c < rmax(C)})
m(C) = card({c | c ? C and c > lmin(C)})
w(C) = rmax(C)? lmin(C)
rmax(C) is the rightmost covered and lmin(C) is the leftmost uncovered source position.
u(C) is the number of ?skipped? positions, and m(C) is the number of ?moved? po-
sitions. The function card(?) returns the cardinality of a set of source positions. The
function w(C) describes the ?window? size in which the word reordering takes place.
A procedural description for the computation of the set of successor hypotheses for
a given partial hypothesis (S, C, j) is given in Table 3. There are restrictions on the
possible successor states: A partial hypothesis in state Skip cannot be expanded into
a partial hypothesis in state Move and vice versa. If the coverage set for the newly
generated hypothesis covers a contiguous initial block of source positions, the state
Initial is entered. No other state S is considered as a successor state in this case (hence
the use of the continue statement in the procedural description). The set of successor
hypotheses Succ by which to extend the partial hypothesis (S, C, j) is computed using
the constraints defined by the values for numskip, widthskip, nummove, and widthmove ,
as explained in the Appendix. In particular, a source position k is discarded for ex-
tension if the ?window? restrictions are violated. Within the restrictions all possible
successors are computed. It can be observed that the set of successors, as computed
in Table 3, is never empty.
112
Computational Linguistics Volume 29, Number 1
Table 3
Procedural description to compute the set Succ of successor hypotheses by which to extend a
partial hypothesis (S, C, j).
input: partial hypothesis (S, C, j)
Succ := {?}
for each k /? C do
Set C? = C ? {k}
if u(C?) = 0
Succ := Succ ? (Initial, C?, k)
continue
if (S = Initial) or (S = Skip)
if w(C?) ? widthskip and u(C?) ? numskip
Succ := Succ ? (Skip, C?, k)
if (S = Initial) or (S = Move)
if k = lmin(C?) and w(C?) ? widthmove and m(C?) ? nummove
Succ := Succ ? (Move, C?, k)
if (S = Move) or (S = Cover)
if (lmin(C?) = k)
Succ := Succ ? (Cover, C?, k)
output: set Succ of successor hypotheses
There is an asymmetry between the two reordering states Move and Skip: While in
state Move, the algorithm is not allowed to cover the position lmin(C). It must first enter
the state Cover to do so. In contrast, for the state Skip, the newly generated hypothesis
always remains in the state Skip (until the state Initial is entered.) This is motivated
by the word reordering for the German verb group. After the right verbal brace has
been processed, no source words may be moved into the verbal brace from later in
the sentence. There is a redundancy in the reorderings: The same reordering might be
carried out using either the state Skip or Move, especially if widthskip and widthmove
are about the same. The additional computational burden is alleviated somewhat by
the fact that the pruning, as introduced in Section 3.8, does not distinguish hypotheses
according to the states. A complexity analysis for different reordering constraints is
given in Tillmann (2001).
3.6 Word Reordering: IBM-Style Restrictions
We now compare the new word reordering approach with the approach used in Berger
et al (1996). In the approach presented in this article, source sentence words are aligned
with hypothesized target sentence words.3 When a source sentence word is aligned, we
say its position is covered. During the search process, a partial hypothesis is extended
by choosing an uncovered source sentence position, and this choice is restricted. Only
one of the first n uncovered positions in a coverage set may be chosen, where n is
set to 4. This choice is illustrated in Figure 9. In the figure, covered positions are
marked by a filled circle, and uncovered positions are marked by an unfilled circle.
Positions that may be covered next are marked by an unfilled square. The restrictions
for a coverage set C can be expressed in terms of the expression u(C) defined in the
previous section: The number of uncovered source sentence positions to the left of
the rightmost covered position. Demanding u(C) ? 3, we obtain the S3 restriction
3 In Berger et al (1996), a morphological analysis is carried out and word morphemes are processed
during the search. Here, we process only full-form words.
113
Tillmann and Ney DP Beam Search for Statistical MT
uncovered position for extension
covered position
uncovered position
J1 j
Figure 9
Illustration of the IBM-style reordering constraint.
introduced in the Appendix. An upper bound of O(E3 ? J4) for the word reordering
complexity is given in Tillmann (2001).
3.7 Empirical Complexity Calculations
In order to demonstrate the complexity of the proposed reordering constraints, we
have modified our translation algorithm to show, for the different reordering con-
straints, the overall number of successor states generated by the algorithm given in
Table 3. The number of successors shown in Figure 10 is counted for a pseudotransla-
tion task in which a pseudo?source word x is translated into the identically pseudo?
target word x. No actual optimization is carried out; the total number of successors
is simply counted as the algorithm proceeds through subsets of increasing cardinality.
The complexity differences for the different reordering constraints result from the dif-
ferent number of coverage subsets C and corresponding reordering states S allowed.
For the different reordering constraints we obtain the following results (the abbrevia-
tions MON, GE, EG, and S3 are taken from the Appendix):
? MON: For this reordering restriction, a partial hypothesis is always
extended by the position lmin(C), hence the number of processed arcs is J.
? GE, EG: These two reordering constraints are very similar in terms of
complexity: The number of word reorderings is heavily restricted in
each. Actually, since the distance restrictions (expressed by the variables
widthskip and widthmove) apply, the complexity is linear in the length of
the input sentence J.
? S3: The S3 reordering constraint has a complexity close to J4. Since no
distance restrictions for the skipped positions apply, the overall search
space is significantly larger than for the GE or EG restriction.
114
Computational Linguistics Volume 29, Number 1
1
10
100
1000
10000
100000
1e+06
1e+07
0 5 10 15 20 25 30 35 40 45 50
"J4"
"S3"
"EG"
"GE"
"MON"
Figure 10
Number of processed arcs for the pseudotranslation task as a function of the input sentence
length J (y-axis is given in log scale). The complexity for the four different reordering
constraints MON, GE, EG, and S3 is given. The complexity of the S3 constraint is close to J4.
3.8 Beam Search Pruning Techniques
To speed up the search, a beam search strategy is used. There is a direct analogy to
the data-driven search organization used in continuous-speech recognition (Ney et al
1992). The full DP search algorithm proceeds cardinality-synchronously over subsets
of source sentence positions of increasing cardinality. Using the beam search concept,
the search can be focused on the most likely hypotheses. The hypotheses Qe?(e, C, j)
are distinguished according to the coverage set C, with two kinds of pruning based
on this coverage set:
1. The coverage pruning is carried out separately for each coverage set C.
2. The cardinality pruning is carried out jointly for all coverage sets C with
the same cardinality c = c(C).
After the pruning is carried out, we retain for further consideration only hypothe-
ses with a probability close to the maximum probability. The number of surviving
hypotheses is controlled by four kinds of thresholds:
? the coverage pruning threshold tC
? the coverage histogram threshold nC
? the cardinality pruning threshold tc
? the cardinality histogram threshold nc
For the coverage and the cardinality pruning, the probability Qe?(e, C, j) is adjusted to
take into account the uncovered source sentence positions C? = {1, . . . , J}\C. To make
115
Tillmann and Ney DP Beam Search for Statistical MT
this adjustment, for a source word f at an uncovered source position, we precompute
an upper bound p?(f ) for the product of language model and lexicon probability:
p?(f ) = max
e??,e?,e
{p(e | e?, e??) ? p(f | e)}
The above optimization is carried out only over the word trigrams (e, e?, e??) that have
actually been seen in the training data. Additionally, the observation pruning described
below is applied to the possible translations e of a source word f . The upper bound
is used in the beam search concept to increase the comparability between hypotheses
covering different coverage sets. Even more benefit from the upper bound p?(f ) can be
expected if the distortion and the fertility probabilities are taken into account (Tillmann
2001). Using the definition of p?(f ), the following modified probability Q?e?(e, C, j) is used
to replace the original probability Qe?(e, C, j), and all pruning is applied to the new
probability:
Q?e?(e, C, j) = Qe?(e, C, j) ?
?
j?C?
p?(fj)
For the translation experiments, equation (3) is recursively evaluated over subsets of
source positions of equal cardinality. For reasons of brevity, we omit the state descrip-
tion S in equation (3), since no separate pruning according to the states S is carried out.
The set of surviving hypotheses for each cardinality c is referred to as the beam. The
size of the beam for cardinality c depends on the ambiguity of the translation task for
that cardinality. To fully exploit the speedup of the DP beam search, the search space
is dynamically constructed as described in Tillmann, Vogel, Ney, Zubiaga, and Sawaf
(1997), rather than using a static search space. To carry out the pruning, the maximum
probabilities with respect to each coverage set C and cardinality c are computed:
? Coverage pruning: Hypotheses are distinguished according to the subset
of covered positions C. The probability Q?(C) is defined:
Q?(C) = max
e,e?,j
Q?e?(e, C, j)
? Cardinality pruning: Hypotheses are distinguished according to the
cardinality c(C) of subsets C of covered positions. The probability Q?(c) is
defined for all hypotheses with c(C) = c:
Q?(c) = max
C
c(C)=c
Q?(C)
The coverage pruning threshold tC and the cardinality pruning threshold tc are used
to prune active hypotheses. We call this pruning translation pruning. Hypotheses are
pruned according to their translation probability:
Q?e?(e, C, j) < tC ? Q?(C)
Q?e?(e, C, j) < tc ? Q?(c)
For the translation experiments presented in Section 4, the negative logarithms of the
actual pruning thresholds tc and tC are reported. A hypothesis (e?, e, C, j) is discarded if
its probability is below the corresponding threshold. For the current experiments, the
116
Computational Linguistics Volume 29, Number 1
coverage and the cardinality threshold are constant for different coverage sets C and
cardinalities c. Together with the translation pruning, histogram pruning is carried
out: The overall number N(C) of active hypotheses for the coverage set C and the
overall number N(c) of active hypotheses for all subsets of a given cardinality may
not exceed a given number; again, different numbers are used for coverage and cardi-
nality pruning. The coverage histogram pruning is denoted by nC , and the cardinality
histogram pruning is denoted by nc:
N(C) > nC
N(c) > nc
If the numbers of active hypotheses for each coverage set C and cardinality c, N(C)
and N(c), exceed the above thresholds, only the partial hypotheses with the highest
translation probabilities are retained (e.g., we may use nC = 1,000 for the coverage
histogram pruning).
The third type of pruning conducted observation pruning: The number of words
that may be produced by a source word f is limited. For each source language word
f the list of its possible translations e is sorted according to
p(f | e) ? puni(e)
where puni(e) is the unigram probability of the target language word e. Only the best no
target words e are hypothesized during the search process (e.g., during the experiments
to hypothesize, the best no = 50 words was sufficient.
3.9 Beam Search Implementation
In this section, we describe the implementation of the beam search algorithm presented
in the previous sections and show how it is applied to the full set of IBM-4 model
parameters.
3.9.1 Baseline DP Implementation. The implementation described here is similar to
that used in beam search speech recognition systems, as presented in Ney et al (1992).
The similarities are given mainly in the following:
? The implementation is data driven. Both its time and memory
requirements are strictly linear in the number of path hypotheses
(disregarding the sorting steps explained in this section).
? The search procedure is developed to work most efficiently when the
input sentences are processed mainly monotonically from left to right.
The algorithm works cardinality-synchronously, meaning that all the
hypotheses that are processed cover subsets of source sentence positions
of equal cardinality c.
? Since full search is prohibitive, we use a beam search concept, as in
speech recognition. We use appropriate pruning techniques in connection
with our cardinality-synchronous search procedure.
Table 4 shows a two-list implementation of the search algorithm given in Table 2 in
which the beam pruning is included. The two lists are referred to as S and Snew: S
is the list of hypotheses that are currently expanded, and Snew is the list of newly
117
Tillmann and Ney DP Beam Search for Statistical MT
Table 4
Two-list implementation of a DP-based search algorithm for statistical MT.
input: source string f1 ? ? ? fj ? ? ? fJ
initial hypothesis lists: S = {($, $, {?}, 0)}
for each cardinality c = 1, 2, . . . , J do
Snew = {?}
for each hypothesis (e?, e, C, j?) ? S, where j? ? C and |C| = c do
Expand (e?, e, C, j?) using probabilities p(fj | e) ? p(j | j?, J) ? p(e | e?, e??)
Look up and add or update expanded hypothesis in Snew
Sort hypotheses in Snew according to translation score
Carry out cardinality pruning
Sort hypotheses in Snew according to coverage set C and translation score
Carry out coverage pruning
Bookkeeping of surviving hypotheses in Snew
S := Snew
output: get best target word sequence eI1 from bookkeeping array
generated hypotheses. The search procedure processes subsets of covered source sen-
tence positions of increasing cardinality. The search starts with S = {($, $, {?}, 0)},
where $ denotes the sentence start symbol for the immediate two predecessor words
and {?} denotes the empty coverage set, in which no source position is covered yet.
For the initial search state, the position last covered is set to 0. A set S of active
hypotheses is expanded for each cardinality c using lexicon model, language model,
and distortion model probabilities. The newly generated hypotheses are added to the
hypothesis set Snew; for hypotheses that are not distinguished according to our DP
approach, only the best partial hypothesis is retained for further consideration. This
so-called recombination is implemented as a set of simple lookup and update opera-
tions on the set Snew of partial hypotheses. During the partial hypothesis extensions,
an anticipated pruning is carried out: Hypotheses are discarded before they are con-
sidered for recombination and are never added to Snew. (The anticipated pruning is not
shown in Table 4. It is based on the pruning thresholds described in Section 3.8.) After
the extension of all partial hypotheses in S, a pruning step is carried out for the hy-
potheses in the newly generated set Snew. The pruning is based on two simple sorting
steps on the list of partial hypotheses Snew. (Instead of sorting the partial hypothe-
ses, we might have used hashing.) First, the partial hypotheses are sorted according
to their translation scores (within the implementation, all probabilities are converted
into translation scores by taking the negative logarithm ? log()). Cardinality prun-
ing can then be carried out simply by running down the list of hypotheses, starting
with the maximum-probability hypothesis, and applying the cardinality thresholds.
Then, the partial hypotheses are sorted a second time according to their coverage set
C and their translation score. After this sorting step, all partial hypotheses that cover
the same subset of source sentence positions are located in consecutive fragments in
the overall list of partial hypotheses. Coverage pruning is carried out in a single run
over the list of partial hypotheses: For each fragment corresponding to the same cov-
erage set C, the coverage pruning threshold is applied. The partial hypotheses that
survive the two pruning stages are then written into the so-called bookkeeping array
(Ney et al 1992). For the next expansion step, the set S is set to the newly generated
list of hypotheses. Finally, the target translation is constructed from the bookkeeping
array.
118
Computational Linguistics Volume 29, Number 1
3.9.2 Details for IBM-4 Model. In this section, we outline how the DP-based beam
search approach can be carried out using the full set of IBM-4 parameters. (More
details can be found in Tillmann [2001] or in the cited papers.) First, the full set of
IBM-4 parameters does not make the simplifying assumption given in Section 3.1,
namely, that source and target sentences are of equal length: Either a target word e
may be aligned with several source words (its fertility is greater than one) or a single
source word may produce zero, one, or two target words, as described in Berger et
al. (1996), or both. Zero target words are generated if f is aligned to the ?null? word
e0. Generating a single target word e is the regular case. Two target words (e?, e??)
may be generated. The costs for generating the target word e? are given by its fertility
?(0 | e?) and the language model probability; no lexicon probability is used. During the
experiments, we restrict ourselves to triples of target words (e, e?, e??) actually seen in the
training data. This approach is used for the French-to-English translation experiments
presented in this article.
Another approach for mapping a single source language word to several target
language words involves preprocessing by the word-joining algorithm given in Till-
mann (2001), which is similar to the approach presented in Och, Tillmann, and Ney
(1999). Target words are joined during a training phase, and several joined target lan-
guage words are dealt with as a new lexicon entry. This approach is used for the
German-to-English translation experiments presented in this article.
In order to deal with the IBM-4 fertility parameters within the DP-based concept,
we adopt the distinction between open and closed hypotheses given in Berger et al
(1996). A hypothesis is said to be open if it is to be aligned with more source positions
than it currently is (i.e., at least two). Otherwise it is called closed. The difference
between open and closed is used to process the input sentence one position a time
(for details see Tillmann 2001). The word reordering restrictions and the beam search
pruning techniques are directly carried over to the full set of IBM-4 parameters, since
they are based on restrictions on the coverage vectors C only.
To ensure its correctness, the implementation was tested by carrying out forced
alignments on 500 German-to-English training sentence pairs. In a forced alignment,
the source sentence f J1 and the target sentence e
I
1 are kept fixed, and a full search with-
out re-ordering restrictions is carried out only over the unknown alignment aJ1. The
language model probability is divided out, and the resulting probability is compared to
the Viterbi probability as obtained by the training procedure. For 499 training sentences
the Viterbi alignment probability as obtained by the forced-alignment search was ex-
actly the same as the one produced by the training procedure. In one case the forced-
alignment search did obtain a better Viterbi probability than the training procedure.
4. Experimental Results
Translation experiments are carried out for the translation directions German to En-
glish and English to German (Verbmobil task) and for the translation directions French
to English and English to French (Canadian Hansards task). Section 4.1 reports on the
performance measures used. Section 4.2 shows translation results for the Verbmobil
task. Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied.
In Sections 4.2.3 through 4.2.5, the efficiency of the beam search pruning techniques is
shown for German-to-English translation, as the most detailed experiments are con-
ducted for that direction. Section 4.2.6 gives translation results for the translation direc-
tion English to German. In Section 4.3, translation results for the Canadian Hansards
task are reported.
119
Tillmann and Ney DP Beam Search for Statistical MT
4.1 Performance Measures for Translation Experiments
To measure the performance of the translation methods, we use three types of au-
tomatic and easy-to-use measures of the translation errors. Additionally, a subjective
evaluation involving human judges is carried out (Niessen et al 2000). The following
evaluation criteria are employed:
? WER (word error rate): The WER is computed as the minimum number of
substitution, insertion, and deletion operations that have to be
performed to convert the generated string into the reference target string.
This performance criterion is widely used in speech recognition. The
minimum is computed using a DP algorithm and is typically referred to
as edit or Levenshtein distance.
? mWER (multireference WER): We use the Levenshtein distance between
the automatic translation and several reference translations as a measure
of the translation errors. For example, on the Verbmobil TEST-331 test
set, an average of six reference translations per automatic translation are
available. The Levenshtein distance between the automatic translation
and each of the reference translations is computed, and the minimum
Levenshtein distance is taken. The resulting measure, the mWER, is
more robust than the WER, which takes into account only a single
reference translation.
? PER (position-independent word error rate): In the case in which only a
single reference translation per sentence is available, we introduce as an
additional measure the position-independent word error rate (PER). This
measure compares the words in the two sentences without taking the
word order into account. Words in the reference translation that have no
counterpart in the translated sentence are counted as substitution errors.
Depending on whether the translated sentence is longer or shorter than
the reference translation, the remaining words result in either insertion
(if the translated sentence is longer) or deletion (if the translated
sentence is shorter) errors. The PER is guaranteed to be less than or
equal to the WER. The PER is more robust than the WER since it ignores
translation errors due to different word order in the translated and
reference sentences.
? SSER (subjective sentence error rate): For a more fine-grained evaluation of
the translation results and to check the validity of the automatic
evaluation measures subjective judgments by test persons are carried out
(Niessen et al 2000). The following scale for the error count per sentence
is used in these subjective evaluations:
0.0 : semantically correct and syntactically correct
? ? ? : ? ? ?
0.5 : semantically correct and syntactically wrong
? ? ? : ? ? ?
1.0 : semantically wrong (independent of syntax)
Each translated sentence is judged by a human examiner according to
the above error scale; several human judges may be involved in judging
the same translated sentence. Subjective evaluation is carried out only
for the Verbmobil TEST-147 test set.
120
Computational Linguistics Volume 29, Number 1
Table 5
Training and test conditions for the German-to-English Verbmobil corpus (*number of words
without punctuation).
German English
Training: Sentences 58,073
Words 519,523 549,921
Words* 418,979 453,632
Vocabulary: Size 7,911 4,648
Singletons 3,453 1,699
TEST-331: Sentences 331
Words 5,591 6,279
Bigram/Trigram Perplexity 84.0/68.2 49.3/38.3
TEST-147: Sentences 147
Words 1,968 2,173
Bigram/Trigram Perplexity ? 34.6/28.1
4.2 Verbmobil Translation Experiments
4.2.1 The Task and the Corpus. The translation system is tested on the Verbmobil task
(Wahlster 2000). In that task, the goal is the translation of spontaneous speech in face-
to-face situations for an appointment scheduling domain. We carry out experiments for
both translation directions: German to English and English to German. Although the
Verbmobil task is still a limited-domain task, it is rather difficult in terms of vocabulary
size, namely, about 5,000 words or more for each of the two languages; second, the
syntactic structures of the sentences are rather unrestricted. Although the ultimate goal
of the Verbmobil project is the translation of spoken language, the input used for the
translation experiments reported on in this article is mainly the (more or less) correct
orthographic transcription of the spoken sentences. Thus, the effects of spontaneous
speech are present in the corpus; the effect of speech recognition errors, however, is
not covered. The corpus consists of 58,073 training pairs; its characteristics are given in
Table 5. For the translation experiments, a trigram language model with a perplexity of
28.1 is used. The following two test corpora are used for the translation experiments:
TEST-331: This test set consists of 331 test sentences. Only automatic evaluation is
carried out on this test corpus: The WER and the mWER are computed. For
each test sentence in the source language there is a range of acceptable
reference translations (six on average) provided by a human translator,
who is asked to produce word-to-word translations wherever it is possi-
ble. Part of the reference sentences are obtained by correcting automatic
translations of the test sentences that are produced using the approach pre-
sented in this article with different reordering constraints. The other part
is produced from the source sentences without looking at any of their
translations. The TEST-331 test set is used as held-out data for parameter
optimization (for the language mode scaling factor and for the distortion
model scaling factor). Furthermore, the beam search experiments in which
the effect of the different pruning thresholds is demonstrated are carried
out on the TEST-331 test set.
TEST-147: The second, separate test set consists of 147 test sentences. Translation
results are given in terms of mWER and SSER. No parameter optimization
121
Tillmann and Ney DP Beam Search for Statistical MT
is carried out on the TEST-147 test set; the parameter values as obtained
from the experiments on the TEST-331 test set are used.
4.2.2 Preprocessing Steps. To improve the translation performance the following
preprocessing steps are carried out:
Categorization: We use some categorization, which consists of replacing a single
word by a category. The only words that are replaced by a category label
are proper nouns denoting German cities. Using the new labeled corpus,
all probability models are trained anew. To produce translations in the
?normal? language, the categories are translated by rule and are inserted
into the target sentence.
Word joining: Target language words are joined using a method similar to the one
described in Och, Tillmann, and Ney (1999). Words are joined to handle
cases like the German compound noun ?Zahnarzttermin? for the English
?dentist?s appointment,? because a single word has to be mapped to two
or more target words. The word joining is applied only to the target lan-
guage words; the source language sentences remain unchanged. During
the search process several joined target language words may be generated
by a single source language word.
Manual lexicon: To account for unseen words in the test sentences and to obtain a
greater number of focused translation probabilities p(f | e), we use a bilin-
gual German-English dictionary. For each word e in the target vocabulary,
we create a list of source translations f according to this dictionary. The
translation probability pdic(f | e) for the dictionary entry (f , e) is defined as
pdic(f | e) =
?
?
?
1
Ne
if (f , e) is in dictionary
0 otherwise
where Ne is the number of source words listed as translations of the tar-
get word e. The dictionary probability pdic(f | e) is linearly combined
with the automatically trained translation probabilities paut(f | e) to ob-
tain smoothed probabilities p(f | e):
p(f | e) = (1 ? ?) ? pdic(f | e) + ? ? paut(f | e)
For the translation experiments, the value of the interpolation parameter
is fixed at ? = 0.5.
4.2.3 Effect of the Scaling Factors. In speech recognition, in which Bayes? decision rule
is applied, a language model scaling factor ?LM is used; a typical value is ?LM ? 15.
This scaling factor is employed because the language model probabilities are more
reliably estimated than the acoustic probabilities. Following this use of a language
model scaling factor in speech recognition, such a factor is introduced into statistical
MT, too. The optimization criterion in equation (1) is modified as follows:
e?I1 = arg max
eI1
{p(eI1)?LM ? p(f
J
1 | eI1)}
where p(eI1) is the language model probability of the target language sentence. In the
experiments presented here, a trigram language model is used to compute p(eI1). The
122
Computational Linguistics Volume 29, Number 1
Table 6
Computing time, mWER, and SSER for three different reordering constraints on the TEST-147
test set. During the translation experiments, reordered words are not allowed to cross
punctuation marks.
Reordering CPU time mWER SSER
constraint [sec] [%] [%]
MON 0.2 40.6 28.6
GE 5.2 33.3 21.0
S3 13.7 34.4 19.9
effect of the language model scaling factor ?LM is studied on the TEST-331 test set. A
minimum mWER is obtained for ?LM = 0.8, as reported in Tillmann (2001). Unlike in
speech recognition, the translation model probabilities seem to be estimated as reliably
as the language model probabilities in statistical MT.
A second scaling factor ?D is introduced for the distortion model probabilities
p(j | j?, J). A minimum mWER is obtained for ?D = 0.4, as reported in Tillmann
(2001). The WER and mWER on the TEST-331 test set increase significantly, if no
distortion probability is used, for the case ?D = 0.0. The benefit of a distortion prob-
ability scaling factor of ?D = 0.4 comes from the fact that otherwise, a low distor-
tion probability might suppress long-distant word reordering that is important for
German-to-English verb group reordering. The setting ?LM = 0.8 and ?D = 0.4 is used
for all subsequent translation results (including the translation direction English to
German).
4.2.4 Effect of the Word Reordering Constraints. Table 6 shows the computing time,
mWER, and SSER on the TEST-147 test set as a function of three reordering constraints:
MON, GE, and S3 (as discussed in the Appendix). The computing time is given in
terms of central processing unit (CPU) time per sentence (on a 450 MHz Pentium
III personal computer). For the SSER, it turns out that restricting the word reorder-
ing such that it may not cross punctuation marks improves translation performance
significantly. The average length of the sentence fragments that are separated by punc-
tuation marks is rather small: 4.5 words per fragment. A coverage pruning threshold
of tC = 5.0 and an observation pruning of no = 50 are applied during the experiments.4
No other type of pruning is used.5
The MON constraint performs worst in terms of both mWER and SSER. The
computing time is small, since no reordering is carried out. Constraints GE and S3
perform nearly identically in terms of both mWER and SSER. The GE constraint,
however, works about three times as fast as the S3 constraint.
Table 7 shows example translations obtained under the three different reordering
constraints. Again, the MON reordering constraint performs worst. In the second and
third translation examples, the S3 word reordering constraint performs worse than the
GE reordering constraint, since it cannot take the word reordering due to the German
verb group properly into account. The German finite verbs bin (second example) and
ko?nnten (third example) are too far away from the personal pronouns ich and Sie (six
4 For the translation experiments, the negative logarithm of the actual pruning thresholds tc and tC is
reported; for simplicity reasons we do not change the notation.
5 In a speech-to-speech demo system, we would use the GE reordering restriction and a slightly sharper
pruning in order to achieve translation times of about one second per sentence.
123
Tillmann and Ney DP Beam Search for Statistical MT
Table 7
Example translations for the translation direction German to English using three different
reordering constraints: MON, GE, and S3.
Input: Ja, wunderbar. Ko?nnen wir machen.
MON: Yes, wonderful. Can we do.
GE: Yes, wonderful. We can do that.
S3: Yes, wonderful. We can do that.
Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin.
Genaugenommen nur am dritten.
Wie wa?re es denn am a?hm Samstag, dem zehnten Februar?
MON: That is too tight , because I from the third in Kaiserslautern.
In fact only on the third. How about a?hm Saturday , the tenth
of February?
GE: That is too tight, because I am from the third in Kaiserslautern.
In fact only on the third. A?hm how about Saturday, February
the tenth?
S3: That is too tight, from the third because I will be in
Kaiserslautern. In fact only on the third. A?hm how
about Saturday, February the tenth?
Input: Wenn Sie dann noch den siebzehnten ko?nnten, wa?re das toll, ja.
MON: If you then also the seventeenth could, would be the great, yes.
GE: If you could then also the seventeenth, that would be great, yes.
S3: Then if you could even take seventeenth, that would be great, yes.
Input: Ja, das kommt mir sehr gelegen. Machen wir es dann
am besten so.
MON: Yes, that suits me perfectly. Do we should best like that.
GE: Yes, that suits me fine. We do it like that then best.
S3: Yes, that suits me fine. We should best do it like that.
and five source sentence positions, respectively) to be reordered properly. In the last
example, the less restrictive S3 reordering constraint leads to a better translation; the
GE translation is still acceptable, though.
4.2.5 Effect of the Beam Search Pruning Thresholds. In this section, the effect of the
beam search pruning is demonstrated. Translation results on the TEST-331 test set are
presented to evaluate the effectiveness of the pruning techniques.6 The quality of the
search algorithm with respect to the GE and S3 reordering constraints is evaluated
using two criteria:
1. The number of search errors for a certain combination of pruning
thresholds is counted. A search error occurs for a test sentence if the
final translation probability QF for a candidate translation eI1 as given in
equation (4) is smaller than a reference probability for that test sentence.
We will compute reference probabilities two ways, as explained below.
2. The mWER performance measure is computed as a function of the
pruning thresholds used. Generally, decreasing the pruning threshold
6 The CPU times on the TEST-331 set are higher, since the average fragment length is greater than for the
TEST-147 set.
124
Computational Linguistics Volume 29, Number 1
Table 8
Effect of the coverage pruning threshold tC on the number of search errors and mWER on the
TEST-331 test set (no cardinality pruning carried out: tc = ?). A cardinality histogram pruning
of 200,000 is applied to restrict the maximum overall size of the search space. The negative
logarithm of tC is reported.
Reordering tC CPU time Search errors mWER
constraint [sec] Qref > QF QF? > QF [%]
GE 0.01 0.21 318 323 73.5
0.1 0.43 231 301 53.1
1.0 1.43 10 226 30.3
2.5 4.75 5 142 25.8
5.0 29.6 ? 35 24.6
7.5 156 ? 2 24.9
10.0 630 ? ? 24.9
12.5 1300 ? ? 24.9
S3 0.01 5.48 314 324 70.0
0.1 9.21 225 303 50.9
1.0 46.2 4 223 31.6
2.5 190 ? 129 28.4
5.0 830 ? ? 28.3
leads to a higher word error rate, since the optimal path through the
translation lattice is missed, resulting in translation errors.
Two automatically generated reference probabilities are used. These probabilities are
computed separately for the reordering constraints GE and S3 (the difference is not
shown in the notation, but will be clear from the context):
Qref: A forced alignment is carried out between each of the test sentences and
its corresponding reference translation; only a single reference translation
for each test sentence is used. The probability obtained for the reference
translation is denoted by Qref.
QF? : A translation is carried out with conservatively large pruning thresholds,
yielding a translation close to the one with the maximum translation prob-
ability. The translation probability for that translation is denoted by QF? .
First, in a series of experiments we study the effect of the coverage and cardinality
pruning for the reordering constraints GE and S3. (When we report on the different
pruning thresholds, we will show the negative logarithm of those pruning thresholds.)
The experiments are carried out on two different pruning ?dimensions?:
1. In Table 8, only coverage pruning using threshold tC is carried out; no
cardinality pruning is applied: tc = ?.
2. In Table 9, only cardinality pruning using threshold tc is carried out; no
coverage pruning is applied: tC = ?.
Both tables use an observation pruning of no = 50. The effect of the coverage prun-
ing threshold tC is demonstrated in Table 8. For the translation experiments reported
in this table, the cardinality pruning threshold is set to tc = ?; thus, no compari-
son between partial hypotheses that do not cover the same set C of source sentence
125
Tillmann and Ney DP Beam Search for Statistical MT
Table 9
Effect of the cardinality pruning threshold tc on the number of search errors and mWER on
the TEST-331 test set (no coverage pruning is carried out: tC = ?). A coverage histogram
pruning of 1,000 is applied to restrict the overall size of the search space. The negative
logarithm of tc is shown.
Reordering tc CPU time Search errors mWER
constraint [sec] Qref > QF QF? > QF [%]
GE 1.0 0.03 45 287 48.5
2.0 0.06 20 277 41.9
3.0 0.13 16 266 37.7
4.0 0.30 6 239 34.1
5.0 0.55 2 212 30.5
7.5 3.2 ? 106 26.6
10.0 14.2 ? 32 25.1
12.5 42.2 ? 5 24.9
15.0 93.9 ? ? 24.9
17.5 176.7 ? ? 24.9
S3 1.0 0.02 10 331 51.4
2.0 0.05 1 283 46.2
3.0 0.10 1 274 43.3
4.0 0.22 ? 251 40.2
5.0 0.50 ? 227 37.5
7.5 4.3 ? 171 32.9
10.0 26.8 ? 99 30.8
12.5 123.3 ? 49 28.9
15.0 430 ? ? 28.2
positions is carried out. To restrict the overall size of the search space in terms of
CPU time and memory requirements, a cardinality pruning of nc = 200,000 is ap-
plied. As can be seen from Table 8, mWER and the number of search errors decrease
significantly as the coverage pruning threshold tC increases. For the GE reordering
constraint, mWER decreases from 73.5% to 24.9%. For a coverage pruning threshold
tC ? 5.0, mWER remains nearly constant at 25.0%, although search errors still occur.
For the S3 reordering constraint, mWER decreases from 70.0% to 28.3%. The largest
coverage threshold tested for the S3 constraint is tC = 5.0, since for larger threshold
values tC , the search procedure cannot be carried out because of memory and time
restrictions. The number of search errors is reduced as the coverage pruning thresh-
old is increased. It turns out to be difficult to verify search errors by looking at the
reference translation probabilities Qref alone. The translation with the maximum trans-
lation probability seems to be quite narrowly defined. The coverage pruning is more
effective for the GE constraint than for the S3 constraint, since the overall search space
for the GE reordering is smaller.
Table 9 shows the effect of the cardinality pruning threshold tc on mWER when
no coverage pruning is carried out (a histogram coverage pruning of 1,000 is applied
to restrict the overall size of the search space). The cardinality threshold tc has a
strong effect on mWER, which decreases significantly as the cardinality threshold tc
increases. For the GE reordering constraint, mWER decreases from 48.5% to 24.9%; for
the S3 reordering constraint, mWER decreases from 51.4% to 28.2%. For the coverage
threshold t = 15.0, the GE constraint works about four times as fast as the S3 constraint,
since the overall search space for the S3 constraint is much larger. Although the overall
search space is much larger for the S3 constraint, for smaller values of the coverage
126
Computational Linguistics Volume 29, Number 1
Table 10
Effect of observation pruning on the number of search errors and mWER on the TEST-331 test
set (parameter setting: tc = ?, tC = 10.0 ). No histogram pruning is applied. The results are
reported for the GE constraint.
Observation CPU time Search errors mWER
pruning no [sec] Qref > QF QF? > QF [%]
1 2.0 13 284 29.3
2 5.9 6 239 26.9
3 10.8 2 196 25.7
5 23.6 2 140 25.3
10 62.9 ? 99 24.8
25 238 ? 44 24.5
50 630 ? ? 24.9
threshold tC ? 5.0, the S3 constraint works as fast as the GE constraint or even faster,
because only a very small portion of the overall search space is searched for small
values of the cardinality pruning threshold tc. There is some computational overhead
in expanding a partial hypothesis for the GE constraint because the finite-state control
has to be handled. No results are obtained for the S3 constraint and the coverage
threshold tc = 17.5 because of memory restrictions. The number of search errors is
reduced as the cardinality pruning threshold is increased. Again, it is difficult to verify
search errors by looking at the reference translation probabilities alone.
Both coverage and cardinality pruning are more efficient for the GE reordering
constraint than for the S3 reordering constraint. For the S3 constraint, no translation
results are obtained for a coverage threshold tc > 5.0 without cardinality pruning
applied because of memory and computing time restrictions. For the GE constraint
virtually a full search can be carried out where only observation pruning is applied:
Identical target translations and translation probabilities are produced for the hypoth-
esis files for the two cases (1) tC = 10.0, tc = ?, and (2) tC = ?, tc = 15.0. (Actually,
for one test sentence in the TEST-331 test set, the translations are different, although
the translation probabilities are exactly the same.) Since the pruning is carried out
independently on two different pruning dimensions, no search errors will occur if the
thresholds are further increased.
Table 10 shows the effect of the observation pruning parameter no on mWER for
the reordering constraint GE. mWER is significantly reduced by hypothesizing up to
the best 50 target words e for a source language word f . mWER increases from 24.9%
to 29.3% when the number of hypothesized words is decreased to only a single word.
Table 11 demonstrates the effect of the combination of the coverage pruning thresh-
old tC = 5.0 and the cardinality pruning threshold tc = 12.5, where the actual values
are found in informal experiments: In a typical setting of the two parameters tc should
be at least twice as big as tC . For the GE reordering constraint, the average computing
time is about seven seconds per sentence without any loss in translation performance
as measured in terms of mWER. For the S3 reordering constraint, the average comput-
ing time per sentence is 27 seconds. Again, the combination of coverage and cardinality
pruning works more efficiently for the GE constraint. The memory requirement for
the algorithm is about 100 MB.
4.2.6 English-to-German Translation Experiments. A series of translation experiments
for the translation direction English to German are also carried out. The results, given
127
Tillmann and Ney DP Beam Search for Statistical MT
Table 11
Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to
speed up the search process for the two reordering constraints GE and S3 (no = 50). The
translation performance is shown in terms of mWER on the TEST-331 test set.
Reordering tC tc CPU time Search errors mWER
constraint [sec] Qref > QF QF? > QF [%]
GE 5.0 12.5 6.9 0 38 24.7
S3 5.0 12.5 26.9 0 65 29.2
Table 12
Translation results for the translation direction English to German on the TEST-331 test set.
The results are given in terms of computing time, WER, and PER for three different reordering
constraints: MON, EG, and S3.
Reordering CPU time WER PER
constraint [sec] [%] [%]
MON 0.5 70.6 57.0
EG 10.1 70.1 55.9
S3 53.2 70.1 55.8
in terms of WER and PER, are shown in Table 12. For the English-to-German translation
direction, a single reference translation for each test sentence is used to carry out
the automatic evaluation. The translation task for the translation direction English
to German is more difficult than for the translation direction German to English; the
trigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set,
as can be seen in Table 5. No parameter optimization is carried out for this translation
direction; the parameter settings are carried over from the results obtained in Table 11.
The word error rates for the translation direction English to German are signif-
icantly higher than those for the translation direction German to English. There are
several reasons for this: German vocabulary and perplexity are significantly larger
than those for English, and only a single reference translation per test sentence is
available for English-to-German translation. There is only a very small difference in
terms of word error rates for the reordering constraints EG and S3; in particular, WER
is 70.1% for both. The reordering constraint MON performs slightly worse: WER in-
creases to 70.6%, and PER increases to 57.0%. Table 13 shows translation examples
for the translation direction English to German. The MON constraint performs worst;
there is no significant difference in quality of translations produced under the EG and
the S3 constraints.
4.3 Canadian Hansards Translation Experiments
4.3.1 The Task and the Corpus. The second corpus on which we perform translation
experiments is the Hansard corpus. By law, the proceedings of the Canadian parlia-
ment are recorded in both French and English. (For historical reasons, these proceed-
ings are called ?Hansards.?) The remarks of the parliament members are written down
in whichever of the two languages they use. They are then translated into the other
language to produce complete sets of the proceedings, one in French and the other
in English. The resulting bilingual data have been sentence-aligned using statistical
methods (Brown et al 1990). Originally, about three million sentences were selected.
Here, we use a subset of the original training data; the details regarding this subset
128
Computational Linguistics Volume 29, Number 1
Table 13
Example translations for the translation direction English to German using three different
reordering constraints: MON, EG, and S3.
Input: Yeah , that wouldn?t be bad. Do you have any ideas where
I could stay?
MON: Ja, das wa?re schade. Haben Sie irgendwelche Ideen wo ich
ko?nnten u?bernachten?
EG: Ja, das wa?re nicht schlecht. Haben Sie irgendwelche Ideen wo
wir wohnen ko?nnten?
S3: Ja, das wa?re nicht schlecht. Haben Sie irgendwelche Ideen
wo wir wohnen ko?nnten?
Input: Oh, that sounds great . Could you arrange a suite for me?
MON: Oh, das klingt gut. Ko?nnten Sie unbedingt ein Suite bei mir?
EG: Oh, das klingt gut. Ko?nnten Sie einen Suite ausmachen fu?r mich?
S3: Oh, das klingt gut. Ko?nnten Sie mir einen Suite ausmachen?
Input: Well, I still need your signature here and then I will check
with your company.
MON: Also, ich konnte Arbeitskraft Unterschrift hier und ich werde
nachsehen mit Ihrer Firma.
EG: Also, ich bra?uchte noch Ihre Unterschrift und dann gucke ich hier
mit Ihrer Firma.
S3: Also, ich brauche hier noch Ihre Unterschrift und dann werde ich
veranlassen mit Ihrer Firma.
Table 14
Training and test conditions for the Hansards task (*number of words without punctuation).
French English
Train: Sentences 1,470,473
Words 24,338,195 22,163,092
Words* 22,175,069 20,063,378
Vocabulary: Size 100,269 78,332
Singletons 40,199 31,319
Test: Sentences 5,432
Words 97,646 80,559
Bigr./Tri. Perplexity 196.9/121.8 269.9/179.8
are given in Table 14. The Hansards corpus presents by far a more difficult task than
the Verbmobil corpus in terms of vocabulary size and number of training sentences.
The training and test sentences are less restrictive than for the Verbmobil task. For the
translation experiments on the Hansards corpus, no word joining is carried out. Two
target words can be produced by a single source word, as described in Section 3.9.2.
4.3.2 Translation Results. As can be seen in Table 15 for the translation direction
French to English and in Table 16 for the translation direction English to French, the
word error rates are rather high compared to those for the Verbmobil task. The reason
for the higher error rates is that, as noted in the previous section, the Hansards task
is by far less restrictive than the Verbmobil task, and the vocabulary size is much
129
Tillmann and Ney DP Beam Search for Statistical MT
Table 15
Computing time, WER, and PER for the translation direction French to English using the two
reordering constraints MON and S3. An almost ?full? search is carried out.
Reordering CPU time WER PER
constraint [sec] [%] [%]
MON 2.5 65.5 53.0
S3 580.0 64.9 51.4
Table 16
Computing time, WER, and PER for the translation direction English to French using the two
reordering constraints MON and S3. An almost ?full? search is carried out.
Reordering CPU time WER PER
constraint [sec] [%] [%]
MON 2.2 66.6 56.3
S3 189.1 66.0 54.4
larger. There is only a slight difference in performance between the MON and the
S3 reordering constraints on the Hansards task. The computation time is also rather
high compared to the Verbmobil task: For the S3 constraint, the average translation
time is about 3 minutes per sentence for the translation direction English to French
and about 10 minutes per sentence for the translation direction French to English.
The following parameter setting is used for the experiment conducted here: tC = 5.0,
tc = 10.0, nC = 250, and to = 12. (The actual parameters are chosen in informal
experiments to obtain reasonable CPU times while permitting only a small number of
search errors.) No cardinality histogram pruning is carried out. As for the German-
to-English translation experiments, word reordering is restricted so that it may not
cross punctuation boundaries. The resulting fragment lengths are much larger for
the translation direction English to French, and still larger for the translation direction
French to English, when compared to the fragment lengths for the translation direction
German to English, hence the high CPU times. In an additional experiment for the
translation direction French to English and the reordering constraint S3, we find we can
speed up the translation time to about 18 seconds per sentence by using the following
parameter setting: tC = 3.0, tc = 7.5, nC = 20, nc = 400, and no = 5. For the resulting
hypotheses file, PER increases only slightly, from 51.4% to 51.6%.
Translation examples for the translation direction French to English under the S3
reordering constraint are given in Table 17. The French input sentences show some
preprocessing that is carried out beforehand to simplify the translation task (e.g., des
is transformed into de les and l?est is transformed into le est). The translations pro-
duced are rather approximative in some cases, although the general meaning is often
preserved.
5. Conclusions
We have presented a DP-based beam search algorithm for the IBM-4 translation model.
The approach is based on a DP solution to the TSP, and it gains efficiency by imposing
constraints on the allowed word reorderings between source and target language. A
data-driven search organization in conjunction with appropriate pruning techniques
130
Computational Linguistics Volume 29, Number 1
Table 17
Example translations for the translation direction French to English using the S3 reordering
constraint.
Input Je crois que cela donne une bonne ide?e de les principes a`
retenir et de ce que devraient e?tre nos responsabilite?s.
S3 I think it is a good idea of the principles and to what
should be our responsibility.
Input Je pense que, inde?pendamment de notre parti, nous trouvons
tous cela inacceptable.
S3 I think, regardless of our party, we find that unacceptable.
Input Je ai le intention de parler surtout aujourd? hui de les nombreuses
ame?liorations apporte?es a` les programmes de pensions de tous les
Canadiens.
S3 I have the intention of speaking today about the many improvements
in pensions for all Canadians especially those programs.
Input Chacun en lui - me?me est tre`s complexe et le lien entre les deux le
est encore davantage de sorte que pour beaucoup la situation
pre?sente est confuse.
S3 Each in itself is very complex and the relationship between the two is more
so much for the present situation is confused.
is proposed. For the medium-sized Verbmobil task, a sentence can be translated in a
few seconds on average, with a small number of search errors and no performance
degradation as measured by the word error criterion used.
Word reordering is parameterized using a set of four parameters, in such a way
that it can easily be adopted to new translation directions. A finite-state control is
added, and its usefulness is demonstrated for the translation direction German to
English, in which the word order difference between the two languages is mainly due
to the German verb group. Future work might aim at a tighter integration of the IBM-4
model distortion probabilities and the finite-state control; the finite-state control itself
may be learned from training data.
The applicability of the algorithm applied in the experiments in this article is
not restricted to the IBM translation models or to the simplified translation model
used in the description of the algorithm in Section 3. Since the efficiency of the beam
search approach is based on restrictions on the allowed coverage vectors C alone,
the approach may be used for different types of translation models as well (e.g., for
the multiword-based translation model proposed in Och, Tillmann, and Ney [1999]).
On the other hand, since the decoding problem for the IBM-4 translation model is
provably NP-complete, as shown in Knight (1999) and Germann et al (2001), word
reordering restrictions as introduced in this article are essential for obtaining an effi-
cient search algorithm that guarantees that a solution close to the optimal one will be
found.
Appendix: Quantification of Reordering Restrictions
To quantify the reordering restrictions in Section 3.5, the four non-negative num-
bers numskip, widthskip, nummove, and widthmove are used (widthskip corresponds
to L, widthmove corresponds to R in Section 3.4; here, we use a more intuitive nota-
tion). Within the implementation of the DP search, the restrictions are provided to the
131
Tillmann and Ney DP Beam Search for Statistical MT
algorithm as an input parameter of the following type:
S numskip widthskip M nummove widthmove
The meaning of the reordering string is as follows: The two numbers following S that
are separated by an underscore describe the way words may be skipped; the two
numbers following M that are separated by an underscore describe the way words
may be moved during word reordering. The first number after S and M denotes
the number of positions that may be skipped or moved, respectively (e.g., for the
translation direction German to English [GE in the chart below], one position may
be skipped and two positions may be moved). The second number after S and M
restricts the distance a word may be skipped or moved, respectively. These ?width?
parameters restrict the word reordering to take place within a ?window? of a certain
size, established by the distance between the positions lmin(C) and rmax(C) as defined
in Section 3.5. In the notation, either the substring headed by S or that headed by M
(or both) may be omitted altogether to indicate that the corresponding reordering is
not allowed. Any numerical value in the string may be set to INF, denoting that an
arbitrary number of positions may be skipped/moved or that the moving or skipping
distance may be arbitrarily large. The following reordering strings are used in this
article:
Word reordering Description
string
 The empty string denotes the reordering restriction in which
(short: MON) no reordering is allowed.
S 01 04 M 02 10 This string describes the German-to-English word reordering.
(short: GE) Up to one word may be skipped for at most 4 positions,
and up to two words may be moved up to 10 positions.
S 02 10 M 01 04 This string describes the English-to-German word reordering.
(short: EG) Up to two words may be skipped for at most 10 positions
and up to one word may be moved for up to 4 positions.
S 03 INF This string describes the IBM-style word reordering
(short: S3) given in Section 3.6. Up to three words may be skipped for
an unrestricted number of positions. No words may be moved.
S INF INF or These strings denote the word re-ordering without
M INF INF restrictions.
(short: NO)
The word reordering strings can be directly used as input parameters to the DP-based
search procedure to test different reordering restrictions within a single implementa-
tion.
Acknowledgments
This work has been supported as part of the
Verbmobil project (contract number
01 IV 601 A) by the German Federal
Ministry of Education, Science, Research
and Technology and as part of the Eutrans
132
Computational Linguistics Volume 29, Number 1
project (ESPRIT project number 30268) by
the European Community. Some of the
experiments on the Canadian Hansards task
have been carried out by Nicola Ueffing
using the existing implementation of the
search algorithm (Och, Ueffing, and Ney
[2001]). We would like to thank the
anonymous reviewers for their detailed
comments on an earlier version of this
article. Also, we would like to thank Niyu
Ge, Scott McCarley, Salim Roukos, Nicola
Ueffing, and Todd Ward for their valuable
remarks.
References
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy,
Noah Smith, and David Yarowsky. 1999.
Statistical machine translation. Final
Report, Johns Hopkins University
Summer Workshop (WS 99) on Language
Engineering, Center for Language and
Speech Processing, Baltimore.
Berger, Adam L., Peter F. Brown, Stephen
A. Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Robert L.
Mercer, Harry Printz, and Lubos Ures.
1994. The Candide system for machine
translation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 152?157, San Mateo, California,
March.
Berger, Adam L., Peter F. Brown, Stephen
A. Della Pietra, Vincent J. Della Pietra,
Andrew S. Kehler, and Robert L. Mercer.
1996. Language translation apparatus and
method of using context-based translation
models. U.S. Patent 5510981.
Brown, Peter F., John Cocke, Vincent J. Della
Pietra, Stephen A. Della Pietra, Fred
Jelinek, John Lafferty, Robert L. Mercer,
and Paul S. Roosin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter F., Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Brown, Peter F., Peter V. deSouza, Vincent
J. Della Pietra, and Robert L. Mercer.
1992. Class-based n-gram models of
natural language. Computational
Linguistics, 18(4):467?479.
Garc??a-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative
DP-based search algorithm for statistical
machine translation. In Proceedings of the
Fifth International Conference on Spoken
Language Processing (ICSLP 98),
pages 1135?1139, Sydney, Australia,
November.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 38th Annual Conference of the Association
for Computational Linguistics (ACL 2001),
pages 228?235, Toulouse, France, July.
Held, Michael and Richard M. Karp. 1962.
A dynamic programming approach to
sequencing problems. SIAM,
10(1):196?210.
Jelinek, Fred. 1976. Speech recognition by
statistical methods. Proceedings of the IEEE,
64:532?556.
Knight, Kevin. 1999. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607?615.
Ney, Hermann, Dieter Mergel, Andreas
Noll, and Annedore Paeseler. 1992. Data
driven search organization for continuous
speech recognition in the SPICOS system.
IEEE Transactions on Signal Processing,
40(2):272?281.
Ney, Hermann, Sonja Niessen, Franz-Josef
Och, Hassan Sawaf, Christoph Tillmann,
and Stefan Vogel. 2000. Algorithms for
statistical translation of spoken language.
IEEE Transactions on Speech and Audio
Processing, 8(1):24?36.
Niessen, Sonja, Franz-Josef Och, Gregor
Leusch, and Hermann Ney. 2000. An
evaluation tool for machine translation:
Fast evaluation for MT research. In
Proceedings of the Second International
Conference on Language Resources and
Evaluation, pages 39?45, Athens, Greece,
May.
Niessen, Sonja, Stefan Vogel, Hermann Ney,
and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In Proceedings of the
36th Annual Conference of the Association for
Computational Linguistics and the 17th
International Conference on Computational
Linguistics (ACL/COLING 98),
pages 960?967, Montreal, Canada, August.
Nilsson, Nils J. 1971. Problem Solving Methods
in Artificial Intelligence. McGraw Hill, New
York.
Och, Franz-Josef and Hermann Ney. 2000. A
comparison of alignment models for
statistical machine translation. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 1086?1090,
Saarbru?cken, Germany, July?August.
133
Tillmann and Ney DP Beam Search for Statistical MT
Och, Franz-Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora
(EMNLP/VLC 99), pages 20?28, College
Park, Maryland, June.
Och, Franz-Josef, Nicola Ueffing, and
Hermann Ney. 2001. An efficient (A)*
search algorithm for statistical machine
translation. In Proceedings of the
Data-Driven Machine Translation Workshop,
39th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 55?62, Toulouse, France, July.
Tillmann, Christoph. 2001. Word Re-ordering
and Dynamic Programming Based Search
Algorithm for Statistical Machine Translation.
Ph.D. thesis, University of Technology,
Aachen, Germany.
Tillmann, Christoph and Hermann Ney.
2000. Word re-ordering and DP-based
search in statistical machine translation.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 850?856,
Saarbru?cken, Germany, July?August.
Tillmann, Christoph, Stefan Vogel, Hermann
Ney, and Alex Zubiaga. 1997. A DP-based
search using monotone alignments in
statistical translation. In Proceedings of the
35th Annual Conference of the Association for
Computational Linguistics (ACL 97),
pages 289?296, Madrid, July.
Tillmann, Christoph, Stefan Vogel, Hermann
Ney, Alex Zubiaga, and Hassan Sawaf.
1997. Accelerated DP-based search for
statistical translation. In Proceedings of the
Fifth European Conference on Speech
Communication and Technology (Eurospeech
97), pages 2667?2670, Rhodos, Greece,
September.
Wahlster, Wolfgang. 2000. Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer Verlag, Berlin.
Wang, Ye-Yi and Alex Waibel. 1997.
Decoding algorithm in statistical
translation. In Proceedings of the 35th
Annual Conference of the Association for
Computational Linguistics (ACL 97),
pages 366?372, Madrid, July.
Wang, Ye-Yi and Alex Waibel. 1998. Fast
decoding for statistical machine
translation. In Proceedings of the Fifth
International Conference on Spoken Language
Processing (ICSLP 98), pages 2775?2778,
Sydney, Australia, December.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Conference of the Association for
Computational Linguistics (ACL 96),
pages 152?158, Santa Cruz, California,
June.
A Phrase-Based Unigram Model for Statistical Machine Translation
Christoph Tillmann and Fei Xia
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
{ctill,feixia}@us.ibm.com
Abstract
In this paper, we describe a phrase-based un-
igram model for statistical machine transla-
tion that uses a much simpler set of model
parameters than similar phrase-based models.
The units of translation are blocks - pairs of
phrases. During decoding, we use a block un-
igram model and a word-based trigram lan-
guage model. During training, the blocks are
learned from source interval projections using
an underlying word alignment. We show exper-
imental results on block selection criteria based
on unigram counts and phrase length.
1 Phrase-based Unigram Model
Various papers use phrase-based translation systems (Och
et al, 1999; Marcu and Wong, 2002; Yamada and Knight,
2002) that have shown to improve translation quality
over single-word based translation systems introduced in
(Brown et al, 1993). In this paper, we present a simi-
lar system with a much simpler set of model parameters.
Specifically, we compute the probability of a block se-
quence bn1 . The block sequence probability Pr(bn1 ) is de-
composed into conditional probabilities using the chain
rule:
Pr(bn1 ) ?
n
?
i=1
Pr(bi|bi?1) (1)
=
n
?
i=1
p?(bi|bi?1) ? p(1??)(bi|bi?1)
?
n
?
i=1
p?(bi) ? p(1??)(bi|bi?1)
We try to find the block sequence that maximizes Pr(bn1 ):
bn1 = argmaxbn1 Pr(b
n
1 ). The model proposed is a joint
1
1
T
S
4
S S S
2 3 4
T
T
T
2
3
Figure 1: A block sequence that jointly generates 4 target
and source phrases.
model as in (Marcu and Wong, 2002), since target and
source phrases are generated jointly. The approach is il-
lustrated in Figure 1. The source phrases are given on the
x-axis and the target phrases are given on the y-axis.
The two types of parameters in Eq 1 are defined as:
? Block unigram model p(bi): we compute unigram
probabilities for the blocks. The blocks are simpler
than the alignment templates in (Och et al, 1999) in
that they do not have any internal structure.
? Trigram language model: the probability
p(bi|bi?1) between adjacent blocks is computed as
the probability of the first target word in the target
clump of bi given the final two words of the target
clump of bi?1.
The exponent ? is set in informal experiments to be 0.5.
No other parameters such as distortion probabilities are
used.
To select blocks b from training data, we compute uni-
gram block co-occurrence counts N(b). N(b) cannot be
Source
Target
Source
Target
Figure 2: The left picture shows three blocks that are
learned from projecting three source intervals. The right
picture shows three blocks that cannot be obtain from
source interval projections .
computed for all blocks in the training data: we would
obtain hundreds of millions of blocks. The blocks are
restricted by an underlying word alignment. The word
alignment is obtained from an HMM Viterbi training (Vo-
gel et al, 1996). The HMM Viterbi training is carried
out twice with English as target language and Chinese as
source language and vice versa. We take the intersection
of the two alignments as described in (Och et al, 1999).
To generate blocks from the intersection, we proceed as
follows: for each source interval [j, j ?], we compute the
minimum target index i and maximum target index i? of
the intersection alignment points that fall into the interval
[j, j?]. The approach is illustrated in Figure 2. In the left
picture, for example, the source interval [1, 3] is projected
into the target interval [1, 3] . The pair ([j, j ?], [i, i?])
together with the words at the corresponding positions
yields a block learned from this training sentence pair.
For source intervals without alignment points in them, no
blocks are produced. We also extend a block correspond-
ing to the interval pair ([j, j ?], [i, i?]) by elements on the
union of the two Viterbi HMM alignments. A similar
block selection scheme has been presented in (Och et al,
1999). Finally, the target and source phrases are restricted
to be equal or less than 8 words long. This way we obtain
23 millions blocks on our training data including blocks
that occur only once. This baseline set is further filtered
using the unigram count N(b): Nk denotes the set of
blocks b for which N(b) ? k. Blocks where the target
and the source clump are of length 1 are kept regardless
of their count.1 We compute the unigram probability p(b)
as relative frequency over all selected blocks.
We also tried a more restrictive projection scheme: source
intervals are projected into target intervals and the reverse
projection of the target interval has to be included in the
original source interval. The results for this symmet-
rical projection are currently worse, since some blocks
with longer target intervals are excluded. An example
of 4 blocks obtained from the training data is shown in
1To apply the restrictions exhaustively, we have imple-
mented tree-based data structures to store the 23 million blocks
with phrases of up to length 8 in about 1.6 gigabyte of RAM.
Figure 3: An example of 4 recursively nested blocks
b1, b2, b3, b4.
Figure 3. ?$DATE? is a placeholder for a date expres-
sion. Block b4 contains the blocks b1 to b3. All 4 blocks
are selected in training: the unigram decoder prefers
b4 even if b1,b2, and b3 are much more frequent. The
solid alignment points are elements from the intersec-
tion, the striped alignment points are elements from the
union. Using the union points, we can learn one-to-many
block translations; for example, the pair (c1,?Xinhua news
agency?) is learned from the training data.
We use a DP-based beam search procedure similar to the
one presented in (Tillmann, 2001). We maximize over
all block segmentations bn1 for which the source phrases
yield a segmentation of the input source sentence, gen-
erating the target sentence simultaneously. In the current
experiments, decoding without block re-ordering yields
the best translation results. The decoder translates about
180 words per second.
2 Experimental Results
The translation system is tested on a Chinese-to-English
translation task. The training data come from several
news sources. For testing, we use the DARPA/NIST MT
2001 dry-run testing data, which consists of 793 sen-
tences with 20, 333 words arranged in 80 documents.2
The training data is provided by the LDC and labeled by
NIST as the Large Data condition for the MT 2002 eval-
uation. The Chinese sentences are segmented into words.
The training data contains 23.7 million Chinese and 25.3
million English words.
Experimental results are presented in Table 1 and Ta-
ble 2. Table 1 shows the effect of the unigram threshold.
The second column shows the number of blocks selected.
The third column reports the BLEU score (Papineni et al,
2002) along with 95% confidence interval. We use IBM
2We did not use the first 25 documents of the 105-document
dry-run test set because they were used as a development test set
before the dry-run and were subsequently added to our training
data.
Table 1: Effect of the unigram threshold on the BLEU
score. The maximum phrase length is 8.
Selection # blocks BLEUr4n4
Restriction selected
IBM1 baseline 1.23M 0.11 ? 0.01
N2 4.23 M 0.18 ? 0.02
N3 1.22 M 0.18 ? 0.01
N4 0.84 M 0.17 ? 0.01
N5 0.65 M 0.17 ? 0.01
Table 2: Effect of the maximum phrase length on the
BLEU score. The unigram threshold is N(b) ? 2.
maximum # blocks BLEUr4n4
phrase length selected
8 4.23 M 0.18 ? 0.02
7 3.76 M 0.17 ? 0.02
6 3.26 M 0.17 ? 0.01
5 2.73 M 0.17 ? 0.01
4 2.16 M 0.17 ? 0.01
3 1.51 M 0.16 ? 0.01
2 0.77 M 0.14 ? 0.01
1 0.16 M 0.12 ? 0.01
Model 1 as a baseline model which is similar to our block
model: neither model uses distortion or alignment proba-
bilities. The best results are obtained for the N2 and the
N3 sets.
The N3 set uses only 1.22 million blocks in contrast to
N2 which has 4.23 million blocks. This indicates that the
number of blocks can be reduced drastically without af-
fecting the translation performance significantly. Table 2
shows the effect of the maximum phrase length on the
BLEU score for the N2 block set. Including blocks with
longer phrases actually helps to improve performance, al-
though length 4 already obtains good results.
We also ran the N2 on the June 2002 DARPA TIDES
Large Data evaluation test set. Six research sites and
four commercial off-the-shelf systems were evaluated in
Large Data track. A majority of the systems were phrase-
based translation systems. For comparison with other
sites, we quote the NIST score (Doddington, 2002) on
this test set: N2 system scores 7.44 whereas the official
top two systems scored 7.65 and 7.34 respectively.
3 Conclusion
In this paper, we described a phrase-based unigram model
for statistical machine translation. The model is much
simpler than other phrase-based statistical models. We
experimented with different restrictions on the phrases
selected from the training data. Longer phrases which
occur less frequently do not help much.
Acknowledgment
This work was partially supported by DARPA and mon-
itored by SPAWAR under contract No. N66001-99-2-
8916.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. of the Second International Confer-
ence of Human Language Technology Research, pages
138?145, March.
Daniel Marcu and William Wong. 2002. A Phrased-
Based, Joint Probability Model for Statistical Machine
Translation. In Proc. of the Conf. on Empirical Meth-
ods in Natural Language Processing (EMNLP 02),
pages 133?139, Philadelphia, PA, July.
Franz-Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC 99), pages 20?28,
College Park, MD, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of machine translation. In Proc. of the
40th Annual Conf. of the Association for Computa-
tional Linguistics (ACL 02), pages 311?318, Philadel-
phia, PA, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, University
of Technology, Aachen, Germany.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM BasedWord Alignment in Statistical Ma-
chine Translation. In Proc. of the 16th Int. Conf.
on Computational Linguistics (COLING 1996), pages
836?841, Copenhagen, Denmark, August.
Kenji Yamada and Kevin Knight. 2002. A Decoder for
Syntax-based Statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pages 303?310, Philadelphia, PA,
July.
TIPS: A Translingual Information Processing System
Y. Al-Onaizan, R. Florian, M. Franz, H. Hassan, Y. S. Lee, S. McCarley, K.
Papineni, S. Roukos, J. Sorensen, C. Tillmann, T. Ward, F. Xia
IBM T. J. Watson Research Center
Yorktown Heights
Abstract
Searching online information is
increasingly a daily activity for many
people. The multilinguality of online
content is also increasing (e.g. the
proportion of English web users, which
has been decreasing as a fraction the
increasing population of web users, dipped
below 50% in the summer of 2001). To
improve the ability of an English speaker
to search mutlilingual content, we built a
system that supports cross-lingual search
of an Arabic newswire collection and
provides on demand translation of Arabic
web pages into English. The cross-lingual
search engine supports a fast search
capability (sub-second response for typical
queries) and achieves state-of-the-art
performance in the high precision region
of the result list. The on demand statistical
machine translation uses the Direct
Translation model along with a novel
statistical Arabic Morphological Analyzer
to yield state-of-the-art translation quality.
The on demand SMT uses an efficient
dynamic programming decoder that
achieves reasonable speed for translating
web documents.
Overview
Morphologically rich languages like Arabic
(Beesley, K. 1996) present significant challenges
to many natural language processing applications
as the one described above because a word often
conveys complex meanings decomposable into
several morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown et
al. 1993) and information retrieval (Franz, M.
and McCarley, S. 2002). In this paper, we
present a cross-lingual English-Arabic search
engine combined with an on demand Arabic-
English statistical machine translation system
that relies on source language analysis for both
improved search and translation. We developed
novel statistical learning algorithms for
performing Arabic word segmentation (Lee, Y.
et al2003) into morphemes and morphological
source language (Arabic) analysis (Lee, Y. et al
2003b). These components improve both mono-
lingual (Arabic) search and cross-lingual
(English-Arabic) search and machine
translation. In addition, the system supports
either document translation or convolutional
models for cross-lingual search (Franz, M. and
McCarley, S. 2002).
The overall demonstration has the following
major components:
1. Mono-lingual search: uses Arabic word
segmentation and an okapi-like search
engine for document ranking.
2. Cross-lingual search: uses Arabic word
segmentation and morphological
analysis along with a statistical
morpheme translation matrix in a
convolutional model for document
ranking. The search can also use
document translation into English to
rank the Arabic documents. Both
approaches achieve similar precision in
the high precision region of retrieval.
The English query is also
morphologically analyzed to improve
performance.
3. OnDemand statistical machine
translation: this component uses both
analysis components along with a direct
channel translation model with a fast
dynamic programming decoder
(Tillmann, C. 2003). This system
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 1-2
                                                         Proceedings of HLT-NAACL 2003
achieves state-of-the-art Arabic-English
translation quality.
4. Arabic named entity detection and
translation: we have 31 categories of
Named Entities (Person, Organization,
etc.) that we detect and highlight in
Arabic text and provide the translation
of these entities into English. The
highlighted named entities help the user
to quickly assess the relevance of a
document.
All of the above functionality is available
through a web browser. We indexed the Arabic
AFP corpus about 330k documents for the
demonstration. The resulting search engine
supports sub-second query response. We also
provide an html detagging capability that allows
the translation of Arabic web pages while trying
to preserve the original layout as much as
possible in the on demand SMT component. The
Arabic Name Entity Tagger is currently run as an
offline process but we expect to have it online by
the demonstration time. We aslo include two
screen shots of the demonstration system.
Acknowledgments
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position of
policy of the Government and no official
endorsement should be inferred.
References
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89? 94.
Brown, P., Della Pietra, S., Della Pietra, V., and
Mercer, R. 1993. The mathematics of statistical
machine translation: Parameter Estimation.
Computational Linguistics, 19(2): 263?311.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402?405.
Lee, Y., Papineni, K., Roukos, S.,
Emam, O., and Hassan, H. 2003. Language
Model Based Arabic Word Segmentation.
Submitted for publication.
Lee, Y., Papineni, K., Roukos, S., Emam,
O., and Hassan, H. 2003b. Automatic
Induction of Morphological Analysis for
Statistical Machine Translation. Manuscript in
preparation.
Tillmann, C., 2003. Word Reordering and a
DP Beam Search Algorithm for Statistical
Machine Translation. Computational
Linguistics, 29(1): 97-133.
A Unigram Orientation Model for Statistical Machine Translation
Christoph Tillmann
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
ctill@us.ibm.com
Abstract
In this paper, we present a unigram segmen-
tation model for statistical machine transla-
tion where the segmentation units are blocks:
pairs of phrases without internal structure. The
segmentation model uses a novel orientation
component to handle swapping of neighbor
blocks. During training, we collect block un-
igram counts with orientation: we count how
often a block occurs to the left or to the right of
some predecessor block. The orientation model
is shown to improve translation performance
over two models: 1) no block re-ordering is
used, and 2) the block swapping is controlled
only by a language model. We show exper-
imental results on a standard Arabic-English
translation task.
1 Introduction
In recent years, phrase-based systems for statistical ma-
chine translation (Och et al, 1999; Koehn et al, 2003;
Venugopal et al, 2003) have delivered state-of-the-art
performance on standard translation tasks. In this pa-
per, we present a phrase-based unigram system similar
to the one in (Tillmann and Xia, 2003), which is ex-
tended by an unigram orientation model. The units of
translation are blocks, pairs of phrases without internal
structure. Fig. 1 shows an example block translation us-
ing five Arabic-English blocks
  
. The unigram
orientation model is trained from word-aligned training
data. During decoding, we view translation as a block
segmentation process, where the input sentence is seg-
mented from left to right and the target sentence is gener-
ated from bottom to top, one block at a time. A monotone
block sequence is generated except for the possibility to
swap a pair of neighbor blocks. The novel orientation
model is used to assist the block swapping: as shown in
b1
Lebanese
violate
warplanes
Israeli
A
l
T
A
}
r 
A
t
}
A
l
H
r
b
y
P
A
l
A
s
r
A
y
l
y
P
t
n
t
h
k
airspace
l
l
b
n
A
n
y
A
l
m
j
A
l
A
l
j
w
y
A
b2
b3
b4
b5
Figure 1: An Arabic-English block translation example
taken from the devtest set. The Arabic words are roman-
ized.
section 3, block swapping where only a trigram language
model is used to compute probabilities between neighbor
blocks fails to improve translation performance. (Wu,
1996; Zens and Ney, 2003) present re-ordering models
that make use of a straight/inverted orientation model that
is related to our work. Here, we investigate in detail
the effect of restricting the word re-ordering to neighbor
block swapping only.
In this paper, we assume a block generation process that
generates block sequences from bottom to top, one block
at a time. The score of a successor block
 
depends on its
predecessor block
 
	
and on its orientation relative to the
block
 	
. In Fig. 1 for example, block
 
is the predeces-
sor of block
 

, and block
 

is the predecessor of block
 
. The target clump of a predecessor block
 	
is adja-
cent to the target clump of a successor block
 
. A right
adjacent predecessor block  
	 is a block where addition-
ally the source clumps are adjacent and the source clump
of
 	
occurs to the right of the source clump of
 
. A left
adjacent predecessor block is defined accordingly.
During decoding, we compute the score  
 


	
of a
block sequence
 


with orientation


as a product of
block bigram scores:
 
 







 


 



 

 



 (1)
where
 
 is a block and

ffProceedings of NAACL HLT 2009: Short Papers, pages 93?96,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Simple Sentence-Level Extraction Algorithm for Comparable Data
Christoph Tillmann and Jian-ming Xu
IBM T.J. Watson Research Center
Yorktown Heights, N.Y. 10598
{ctill,jianxu}@us.ibm.com
Abstract
The paper presents a novel sentence pair ex-
traction algorithm for comparable data, where
a large set of candidate sentence pairs is scored
directly at the sentence-level. The sentence-
level extraction relies on a very efficient im-
plementation of a simple symmetric scoring
function: a computation speed-up by a fac-
tor of 30 is reported. On Spanish-English
data, the extraction algorithm finds the highest
scoring sentence pairs from close to 1 trillion
candidate pairs without search errors. Sig-
nificant improvements in BLEU are reported
by including the extracted sentence pairs into
the training of a phrase-based SMT (Statistical
Machine Translation) system.
1 Introduction
The paper presents a simple sentence-level trans-
lation pair extraction algorithm from comparable
monolingual news data. It differs from similar
algorithms that select translation correspondences
explicitly at the document level (Fung and Che-
ung, 2004; Resnik and Smith, 2003; Snover et
al., 2008; Munteanu and Marcu, 2005; Quirk et
al., 2007; Utiyama and Isahara, 2003). In these
publications, the authors use Information-Retrieval
(IR) techniques to match document pairs that are
likely translations of each other. More complex
sentence-level models are then used to extract par-
allel sentence pairs (or fragments). From a com-
putational perspective, the document-level filtering
steps are needed to reduce the number of candidate
sentence pairs. While IR techniques might be use-
ful to improve the selection accuracy, the current pa-
per demonstrates that they are not necessary to ob-
tain parallel sentence pairs. For some data, e.g. the
Portuguese-English Reuters data used in the experi-
ments in Section 3, document-level information may
not even be available.
In this paper, sentence pairs are extracted by a sim-
ple model that is based on the so-called IBM Model-
1 (Brown et al, 1993). The Model-1 is trained
on some parallel data available for a language pair,
i.e. the data used to train the baseline systems in
Section 3. The scoring function used in this pa-
per is inspired by phrase-based SMT. Typically, a
phrase-based SMT system includes a feature that
scores phrase pairs using lexical weights (Koehn et
al., 2003) which are computed for two directions:
source to target and target to source. Here, a sen-
tence pair is scored as a phrase pair that covers all
the source and target words. The scoring function
?(S, T ) is defined as follows:
?(S, T ) = (1)
=
J?
j=1
1
J ? log(
p(sj |T )? ?? ?
1
I ?
I?
i=1
p(sj|ti) )
? ?? ?
?(sj ,T )
+
I?
i=1
1
I ? log(
p(ti|S)? ?? ?
1
J ?
J?
j=1
p(ti|sj) )
? ?? ?
?(ti,S)
93
Here, S = sJ1 is the source sentence of length J and
T = tI1 is the target sentence of length I . p(s|T )
is the Model-1 probability assigned to the source
word s given the target sentence T , p(t|S) is defined
accordingly. p(s|t) and p(t|s) are word translation
probabilities obtained by two parallel Model-1 train-
ing steps on the same data, but swapping the role
of source and target language. They are smoothed
to avoid 0.0 entries; there is no special NULL-word
model and stop words are kept. The log(?) is applied
to turn the sentence-level probabilities into scores.
These log-probabilities are normalized with respect
to the source and target sentence length: this way
the score ?(S, T ) can be used across all sentence
pairs considered, and a single manually set thresh-
old ? is used to select all those sentence pairs whose
score is above it. For computational reasons, the
sum ?(S, T ) is computed over the following terms:
?(ti, S) where 1 ? i ? I and ?(sj, T ), where
1? j? J . The ? ?s and ??s represent partial score
contributions for a given source or target position.
Note that ?(S, T ) ? 0 since the terms ?(?, S) ? 0
and ?(?, T ) ? 0.
Section 2 presents an efficient implementation of
the scoring function in Eq. 1. Its effectiveness is
demonstrated in Section 3. Finally, Section 4 dis-
cusses future work and extensions of the current al-
gorithm.
2 Sentence-Level Processing
We process the comparable data at the sentence-
level: for each language and all the documents in
the comparable data, we distribute sentences over a
list of files : one file for each news feed f (for the
Spanish Gigaword data, there are 3 news feeds) and
publication date d . The Gigaword data comes anno-
tated with sentence-level boundaries, and all docu-
ment boundaries are discarded. This way, the Span-
ish data consists of about 24 thousand files and the
English data consists of about 53 thousand files (for
details, see Table 2). For a given source sentence S,
the search algorithm computes the highest scoring
sentence pair ?(S, T ) over a set of candidate trans-
lations T ? ?, where |?| can be in the hundreds
of thousands of sentences . ? consists of all target
sentences that have been published from the same
news feed f within a 7 day window from the pub-
lication date of the current source sentence S. The
extraction algorithm is guaranteed to find the highest
scoring sentence pairs (S, T ) among all T ? ?. In
order to make this processing pipeline feasible, the
scoring function in Eq. 1 needs to be computed very
efficiently. That efficiency is based on the decompo-
sition of the scoring functions into I + J terms ( ? ?s
and ??s) where source and target terms are treated
differently. While the scoring function computation
is symmetric, the processing is organized according
the source language files: all the source sentences
are processed one-by-one with respect to their indi-
vidual candidate sets ?:
? Caching for target term ?(t, S): For each tar-
get word t that occurs in a candidate translation
T , the Model-1 based probability p(t|S) can be
cached: its value is independent of the other
words in T . The same word t in different tar-
get sentences is processed with respect to the
same source sentence S and p(t|S) has to be
computed only once.
? Array access for source terms ?(s, T ): For a
given source sentence S, we compute the scor-
ing function ?(S, T ) over a set of target sen-
tences T ? ?. The computation of the source
term ?(s, T ) is based on translation probabil-
ities p(s|t) . For each source word s, we can
retrieve all target words t for which p(s|t) > 0
just once. We store those words t along with
their probabilities in an array the size of the tar-
get vocabulary. Words t that do not have an
entry in the lexicon have a 0 entry in that ar-
ray. We keep a separate array for each source
position. This way, we reduce the probability
access to a simple array look-up. Generating
the full array presentation requires less than 50
milliseconds per source sentence on average.
? Early-Stopping: Two loops compute the scor-
ing function ?(S, T ) exhaustively for each sen-
tence pair (S, T ): 1) a loop over all the target
position terms ?(ti, S), and 2) a loop over all
source position terms ?(sj , T ) . Once the cur-
rent partial sum is lower than the best score
?(S, Tbest) computed so far, the computation
can be safely discarded as ?(ti, S), ?(sj , T ) ?
94
Table 1: Effect of the implementation techniques on a
full search that computes ?(S, T ) exhaustively for all sen-
tence pairs (S, T ) for a given S.
Implementation Technique Speed
[secs/sent]
Baseline 33.95
+ Array access source terms 19.66
+ Cache for target terms 3.83
+ Early stopping 1.53
+ Frequency sorting 1.23
0 and adding additional terms can only lower
that partial sum further.
? Frequency-Sorting: Here, we aim at making
the early pruning step more efficient. Source
and target words are sorted according to the
source and target vocabulary frequency: less
frequent words occur at the beginning of a sen-
tence. These words are likely to contribute
terms with high partial scores. As a result, the
early-stopping step fires earlier and becomes
more effective.
? Sentence-level filter: The word-overlap filter
in (Munteanu and Marcu, 2005) has been im-
plemented: for a sentence pair (S, T ) to be con-
sidered parallel the ratio of the lengths of the
two sentences has to be smaller than two. Ad-
ditionally, at least half of the words in each sen-
tence have to have a translation in the other sen-
tence based on the word-based lexicon. Here,
the implementation of the coverage restriction
is tightly integrated into the above implemen-
tation: the decision whether a target word is
covered can be cached. Likewise, source word
coverage can be decided by a simple array
look-up.
3 Experiments
The parallel sentence extraction algorithm presented
in this paper is tested in detail on the large-
scale Spanish-English Gigaword data (Graff, 2006;
Graff, 2007). The Spanish data comes from 3
news feeds: Agence France-Presse (AFP), Associ-
ated Press Worldstream (APW), and Xinhua News
Table 2: Corpus statistics for comparable data. Any
document-level information is ignored.
Spanish English
Date-Feed Files 24, 005 53, 373
Sentences 19.4 million 47.9 million
Words 601.5 million 1.36 billion
Portuguese English
Date-Feed Files 351 355
Sentences 366.0 thousand 5.3 million
Words 11.6 million 171.1 million
Agency (XIN). We do not use the additional news
feed present in the English data. Table 1 demon-
strates the effectiveness of the implementation tech-
niques in Section 2. Here, the average extraction
time per source sentence is reported for one of the
24, 000 source language files. This file contains 913
sentences. Here, the size of the target candidate set
? is 61 736 sentences. All the techniques presented
result in some improvement. The baseline uses only
the length-based filtering and the coverage filtering
without caching the coverage decisions (Munteanu
and Marcu, 2005). Caching the target word proba-
bilities results in the biggest reduction. The results
are representative: finding the highest scoring target
sentence T for a given source sentence S takes about
1 second on average. Since 20 million source sen-
tences are processed, and the workload is distributed
over roughly 120 processors, overall processing time
sums to less than 3 days. Here, the total number of
translation pairs considered is close to 1 trillion.
The effect of including additional sentence pairs
along with selection statistics is presented in Ta-
ble 3. Translation results are presented for a standard
phrase-based SMT system. Here, both languages
use a test set with a single reference. Including about
1.4 million sentence pairs extracted from the Giga-
word data, we obtain a statistically significant im-
provement from 42.3 to 45.6 in BLEU (Papineni et
al., 2002). The baseline system has been trained
on about 1.8 million sentence pairs from Europarl
and FBIS parallel data. We also present results for
a Portuguese-English system: the baseline has been
trained on Europarl and JRC data. Parallel sentence
pairs are extracted from comparable Reuters news
data published in 2006. The corpus statistics for
95
Table 3: Spanish-English and Portuguese-English extrac-
tion results.
Data Source # candidates #train pairs Bleu
Spanish-English: ? = ?4.1
Baseline - 1, 825, 709 42.3
+ Gigaword 955.5 ? 109 1, 372, 124 45.6
Portuguese-English: ? = ?5.0
Baseline - 2, 221, 891 45.3
+ Reuters 06 32.8 ? 109 48, 500 48.5
the Portuguese-English data are given in Table 2.
The selection threshold ? is determined with the
help of bilingual annotators (it typically takes a few
hours). Sentence pairs are selected with a conserva-
tive threshold ?? first. Then, all the sentence pairs are
sorted by descending score. The annotator descends
this list to determine a score threshold cut-off. Here,
translation pairs are considered to be parallel if 75
% of source and target words have a corresponding
translation in the other sentence. Using a threshold
? = ?4.1 for the Spanish-English data, results in a
selection precision of around 80 % (most of the mis-
qualified pairs are partial translations with less than
75 % coverage or short sequences of high frequency
words). This simple selection criterion proved suf-
ficient to obtain the results presented in this paper.
As can be seen from Table 3, the optimal threshold
is language specific.
4 Future Work and Discussion
In this paper, we have presented a novel sentence-
level pair extraction algorithm for comparable data.
We use a simple symmetrized scoring function
based on the Model-1 translation probability. With
the help of an efficient implementation, it avoids
any translation candidate selection at the docu-
ment level (Resnik and Smith, 2003; Smith, 2002;
Snover et al, 2008; Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005; Fung and Cheung,
2004). In particular, the extraction algorithm works
when no document-level information is available.
Its usefulness for extracting parallel sentences is
demonstrated on news data for two language pairs.
Currently, we are working on a feature-rich ap-
proach (Munteanu and Marcu, 2005) to improve
the sentence-pair selection accuracy. Feature func-
tions will be ?light-weight? such that they can be
computed efficiently in an incremental way at the
sentence-level. This way, we will be able to main-
tain our search-driven extraction approach. We are
also re-implementing IR-based techniques to pre-
select translation pairs at the document-level, to
gauge the effect of this additional filtering step. We
hope that a purely sentence-level processing might
result in a more productive pair extraction in future.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. CL, 19(2):263?311.
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and Lexicon
Extraction via Bootstrapping and EM. In Proc, of
EMNLP 2004, pages 57?63, Barcelona, Spain, July.
Dave Graff. 2006. LDC2006T12: Spanish Gigaword
Corpus First Edition. LDC.
Dave Graff. 2007. LDC2007T07: English Gigaword
Corpus Third Edition. LDC.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of
HLT-NAACL?03, pages 127?133, Edmonton, Alberta,
Canada, May 27 - June 1.
Dragos S. Munteanu and Daniel Marcu. 2005. Improv-
ing Machine Translation Performance by Exploiting
Non-Parallel Corpora. CL, 31(4):477?504.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In In Proc. of
ACL?02, pages 311?318, Philadelphia, PA, July.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative Models of Noisy Translations with
Applications to Parallel Fragment Extraction. In
Proc. of the MT Summit XI, pages 321?327, Copen-
hagen,Demark, September.
Philip Resnik and Noah Smith. 2003. The Web as Paral-
lel Corpus. CL, 29(3):349?380.
Noah A. Smith. 2002. From Words to Corpora: Rec-
ognizing Translation. In Proc. of EMNLP02, pages
95?102, Philadelphia, July.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proc. of EMNLP08,
pages 856?865, Honolulu, Hawaii, October.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences. In Proc. of ACL03, pages 72?79,
Sapporo, Japan, July.
96
Proceedings of the 43rd Annual Meeting of the ACL, pages 557?564,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Localized Prediction Model for Statistical Machine Translation
Christoph Tillmann and Tong Zhang
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598 USA
 
ctill,tzhang  @us.ibm.com
Abstract
In this paper, we present a novel training
method for a localized phrase-based predic-
tion model for statistical machine translation
(SMT). The model predicts blocks with orien-
tation to handle local phrase re-ordering. We
use a maximum likelihood criterion to train a
log-linear block bigram model which uses real-
valued features (e.g. a language model score)
as well as binary features based on the block
identities themselves, e.g. block bigram fea-
tures. Our training algorithm can easily handle
millions of features. The best system obtains
a   % improvement over the baseline on a
standard Arabic-English translation task.
1 Introduction
In this paper, we present a block-based model for statis-
tical machine translation. A block is a pair of phrases
which are translations of each other. For example, Fig. 1
shows an Arabic-English translation example that uses 
blocks. During decoding, we view translation as a block
segmentation process, where the input sentence is seg-
mented from left to right and the target sentence is gener-
ated from bottom to top, one block at a time. A monotone
block sequence is generated except for the possibility to
swap a pair of neighbor blocks. We use an orientation
model similar to the lexicalized block re-ordering model
in (Tillmann, 2004; Och et al, 2004): to generate a block
	
with orientation 
 relative to its predecessor block 	 .
During decoding, we compute the probability  	  
  
of a block sequence 	  with orientation 
   as a product
of block bigram probabilities:

	









Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 721?728,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Discriminative Global Training Algorithm for Statistical MT
Christoph Tillmann
IBM T.J. Watson Research Center
Yorktown Heights, N.Y. 10598
ctill@us.ibm.com
Tong Zhang
Yahoo! Research
New York City, N.Y. 10011
tzhang@yahoo-inc.com
Abstract
This paper presents a novel training al-
gorithm for a linearly-scored block se-
quence translation model. The key com-
ponent is a new procedure to directly op-
timize the global scoring function used by
a SMT decoder. No translation, language,
or distortion model probabilities are used
as in earlier work on SMT. Therefore
our method, which employs less domain
specific knowledge, is both simpler and
more extensible than previous approaches.
Moreover, the training procedure treats the
decoder as a black-box, and thus can be
used to optimize any decoding scheme.
The training algorithm is evaluated on a
standard Arabic-English translation task.
1 Introduction
This paper presents a view of phrase-based SMT
as a sequential process that generates block ori-
entation sequences. A block is a pair of phrases
which are translations of each other. For example,
Figure 1 shows an Arabic-English translation ex-
ample that uses four blocks. During decoding, we
view translation as a block segmentation process,
where the input sentence is segmented from left
to right and the target sentence is generated from
bottom to top, one block at a time. A monotone
block sequence is generated except for the possi-
bility to handle some local phrase re-ordering. In
this local re-ordering model (Tillmann and Zhang,
2005; Kumar and Byrne, 2005) a block   with
orientation  is generated relative to its predeces-
sor block
 
. During decoding, we maximize the
score 
 
	 

	 
of a block orientation sequence

Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 225?228,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Beam-Search Extraction Algorithm for Comparable Data
Christoph Tillmann
IBM T.J. Watson Research Center
Yorktown Heights, N.Y. 10598
ctill@us.ibm.com
Abstract
This paper extends previous work on ex-
tracting parallel sentence pairs from com-
parable data (Munteanu and Marcu, 2005).
For a given source sentence S, a max-
imum entropy (ME) classifier is applied
to a large set of candidate target transla-
tions . A beam-search algorithm is used
to abandon target sentences as non-parallel
early on during classification if they fall
outside the beam. This way, our novel
algorithm avoids any document-level pre-
filtering step. The algorithm increases the
number of extracted parallel sentence pairs
significantly, which leads to a BLEU im-
provement of about 1 % on our Spanish-
English data.
1 Introduction
The paper presents a novel algorithm for ex-
tracting parallel sentence pairs from comparable
monolingual news data. We select source-target
sentence pairs (S, T ) based on a ME classifier
(Munteanu and Marcu, 2005). Because the set of
target sentences T considered can be huge, pre-
vious work (Fung and Cheung, 2004; Resnik and
Smith, 2003; Snover et al, 2008; Munteanu and
Marcu, 2005) pre-selects target sentences T at the
document level . We have re-implemented a par-
ticular filtering scheme based on BM25 (Quirk et
al., 2007; Utiyama and Isahara, 2003; Robertson
et al, 1995). In this paper, we demonstrate a dif-
ferent strategy . We compute the ME score in-
crementally at the word level and apply a beam-
search algorithm to a large number of sentences.
We abandon target sentences early on during clas-
sification if they fall outside the beam. For com-
parison purposes, we run our novel extraction al-
gorithm with and without the document-level pre-
filtering step. The results in Section 4 show that
the number of extracted sentence pairs is more
than doubled which also leads to an increase in
BLEU by about 1 % on the Spanish-English data.
The classification probability is defined as fol-
lows:
p(c|S, T ) =
exp( w
T
? f(c, S, T ) )
Z(S, T )
, (1)
where S = sJ
1
is a source sentence of length J and
T = t
I
1
is a target sentence of length I . c ? {0, 1}
is a binary variable . p(c|S, T ) ? [0, 1] is a proba-
bility where a value p(c = 1|S, T ) close to 1.0 in-
dicates that S and T are translations of each other.
w ? R
n is a weight vector obtained during train-
ing. f(c, S, T ) is a feature vector where the fea-
tures are co-indexed with respect to the alignment
variable c. Finally, Z(S, T ) is an appropriately
chosen normalization constant.
Section 2 summarizes the use of the binary clas-
sifier. Section 3 presents the beam-search algo-
rithm. In Section 4, we show experimental results.
Finally, Section 5 discusses the novel algorithm.
2 Classifier Training
The classifier in Eq. 1 is based on several real-
valued feature functions f
i
. Their computation
is based on the so-called IBM Model-1 (Brown et
al., 1993). The Model-1 is trained on some paral-
lel data available for a language pair, i.e. the data
used to train the baseline systems in Section 4.
p(s|T ) is the Model-1 probability assigned to a
source word s given the target sentence T , p(t|S)
is defined accordingly. p(s|t) and p(t|s) are word
translation probabilities obtained by two parallel
Model-1 training steps on the same data, but swap-
ping the role of source and target language. To
compute these values efficiently, the implementa-
tion techniques in (Tillmann and Xu, 2009) are
used. Coverage and fertility features are defined
based on the Model-1 Viterbi alignment: a source
225
word s is said to be covered if there is a target
word t ? T such that its probability is above a
threshold ?: p(s|t) > ? . We define the fertility
of a source word s as the number of target words
t ? T for which p(s|t) > ?. Target word cover-
age and fertility are defined accordingly. A large
number of ?uncovered? source and target positions
as well as a large number of high fertility words
indicate non-parallelism. We use the following
N = 7 features: 1,2) lexical Model-1 weight-
ing:
?
s
?log( p(s|T ) ) and
?
t
?log( p(t|S) ),
3,4) number of uncovered source and target po-
sitions, 5,6) sum of source and target fertilities,
7) number of covered source and target positions
. These features are defined in a way that they
can be computed incrementally at the word level.
Some thresholding is applied, e.g. a sequence of
uncovered positions has to be at least 3 positions
long to generate a non-zero feature value . In the
feature vector f(c, S, T ), each feature f
i
occurs
potentially twice, once for each class c ? {0, 1}.
For the feature vector f(c = 1, S, T ), all the fea-
ture values corresponding to class c = 0 are set
to 0, and vice versa. This particular way of defin-
ing the feature vector is needed for the search in
Section 3: the contribution of the ?negative? fea-
tures for c = 0 is only computed when Eq. 1 is
evaluated for the highest scoring final hypothesis
in the beam. To train the classifier, we have manu-
ally annotated a collection of 524 sentence pairs .
A sentence pair is considered parallel if at least
75 % of source and target words have a corre-
sponding translation in the other sentence, other-
wise it is labeled as non-parallel. A weight vector
w ? R
2?N is trained with respect to classification
accuracy using the on-line maxent training algo-
rithm in (Tillmann and Zhang, 2007).
3 Beam Search Algorithm
We process the comparable data at the sentence
level: sentences are indexed based on their publi-
cation date. For each source sentence S, a match-
ing score is computed over all the target sentences
T
m
? ? that have a publication date which differs
less than 7 days from the publication date of the
source sentence 1. We are aiming at finding the ?T
with the highest probability p(c = 1|S, ?T ), but we
cannot compute that probability for all sentence
1In addition, the sentence length filter in (Munteanu and
Marcu, 2005) is used: the length ratio max(J, I)/min(J, I)
of source and target sentence has to be smaller than 2.
pairs (S, T
m
) since |?| can be in tens of thousands
of sentences . Instead, we use a beam-search algo-
rithm to search for the sentence pair (S, ?T ) with
the highest matching score wT ? f(1, S, ?T ) 2. The
?light-weight? features defined in Section 2 are
such that the matching score can be computed in-
crementally while processing the source and target
sentence positions in some order. To that end, we
maintain a stack of matching hypotheses for each
source position j. Each hypothesis is assigned a
partial matching score based on the source and tar-
get positions processed so far. Whenever a partial
matching score is low compared to partial match-
ing scores of other target sentence candidates, that
translation pair can be discarded by carrying out
a beam-search pruning step. The search is orga-
nized in a single left-to-right run over the source
positions 1? j ? J and all active partial hypothe-
ses match the same portion of that source sentence.
There is at most a single active hypothesis for each
different target sentence T
i
, and search states are
defined as follows:
[ m , j , u
j
, u
i
; d ] .
Here, m ? {1, ? ? ? , |?|} is a target sentence in-
dex. j is a position in the source sentence, u
j
and
u
i
are the number of uncovered source and target
positions to the left of source position j and tar-
get position i (coverage computation is explained
above), and d is the partial matching score . The
target position i corresponding to the source posi-
tion j is computed deterministically as follows:
i = ?I ?
j
J
? , (2)
where the sentence lengths I and J are known
for a sentence pair (S, T ). Covering an additional
source position leads to covering additional target
positions as well, and source and target features
are computed accordingly. The search is initial-
ized by adding a single hypothesis for each target
sentence T
m
? ? to the stack for j = 1:
[ m , j = 1 , u
j
= 0 , u
i
= 0 ; 0 ] .
During the left-to-right search , state transitions of
the following type occur:
[ m , j , u
j
, u
i
; d ] ?
[ m , j + 1 , u
?
j
, u
?
i
; d
?
] ,
2This is similar to standard phrase-based SMT decoding,
where a set of real-valued features is used and any sentence-
level normalization is ignored during decoding. We assume
the effect of this approximation to be small.
226
where the partial score is updated as: d? = d +
w
T
? f(1, j, i) . Here, f(1, j, i) is a partial fea-
ture vector computed for all the additional source
and target positions processed in the last extension
step. The number of uncovered source and target
positions u? is updated as well. The beam-search
algorithm is carried out until all source positions j
have been processed. We extract the highest scor-
ing partial hypothesis from the final stack j = J
. For that hypothesis, we compute a global feature
vector f(1, S, T ) by adding all the local f(1, j, i)?s
component-wise. The ?negative? feature vector
f(0, S, T ) is computed from f(1, S, T ) by copy-
ing its feature values. We then use Eq. 1 to com-
pute the probability p(1|S, T ) and apply a thresh-
old of ? = 0.75 to extract parallel sentence pairs.
We have adjusted beam-search pruning techniques
taken from regular SMT decoding (Tillmann et al,
1997; Koehn, 2004) to reduce the number of hy-
potheses after each extension step. Currently, only
histogram pruning is employed to reduce the num-
ber of hypotheses in each stack.
The resulting beam-search algorithm is similar
to a monotone decoder for SMT: rather then in-
crementally generating a target translation, the de-
coder is used to select entire target sentences out of
a pre-defined list. That way, our beam search algo-
rithm is similar to algorithms in large-scale speech
recognition (Ney, 1984; Vintsyuk, 1971), where
an acoustic signal is matched to a pre-assigned list
of words in the recognizer vocabulary.
4 Experiments
The parallel sentence extraction algorithm pre-
sented in this paper is tested in detail on all of the
large-scale Spanish-English Gigaword data (Graff,
2006; Graff, 2007) as well as on some smaller
Portuguese-English news data . For the Spanish-
English data , matching sentence pairs come from
the same news feed. Table 1 shows the size of
the comparable data, and Table 2 shows the ef-
fect of including the additional sentence pairs into
the training of a phrase-based SMT system. Here,
both languages use a test set with a single ref-
erence. The test data comes from Spanish and
Portuguese news web pages that have been trans-
lated into English. Including about 1.35 million
sentence pairs extracted from the Gigaword data,
we obtain a statistically significant improvement
from 42.3 to 45.7 in BLEU. The baseline system
has been trained on about 1.8 million sentence
Table 1: Corpus statistics for comparable data.
Spanish English
Sentences 19.4 million 47.9 million
Words 601.5 million 1.36 billion
Portuguese English
Sentences 366.0 thousand 5.3 million
Words 11.6 million 171.1 million
pairs from Europarl and FBIS parallel data. We
also present results for a Portuguese-English sys-
tem: the baseline has been trained on Europarl and
JRC data. Parallel sentence pairs are extracted
from comparable news data published in 2006.
For this data, no document-level information was
available. To gauge the effect of the document-
level pre-filtering step, we have re-implemented
an IR technique based on BM25 (Robertson et al,
1995). This type of pre-filtering has also been used
in (Quirk et al, 2007; Utiyama and Isahara, 2003).
We split the Spanish data into documents. Each
Spanish document is translated into a bag of En-
glish words using Model-1 lexicon probabilities
trained on the baseline data. Each of these English
bag-of-words is then issued as a query against all
the English documents that have been published
within a 7 day window of the source document.
We select the 20 highest scoring English docu-
ments for each source document . These 20 docu-
ments provide a restricted set of target sentence
candidates. The sentence-level beam-search al-
gorithm without the document-level filtering step
searches through close to 1 trillion sentence pairs.
For the data obtained by the BM25-based filtering
step, we still use the same beam-search algorithm
but on a much smaller candidate set of only 25.4
billion sentence pairs. The probability selection
threshold ? is determined on some development
set in terms of precision and recall (based on the
definitions in (Munteanu and Marcu, 2005)). The
classifier obtains an F-measure classifications per-
formance of about 85 %. The BM25 filtering step
leads to a significantly more complex processing
pipeline since sentences have to be indexed with
respect to document boundaries and publication
date. The document-level pre-filtering reduces the
overall processing time by about 40 % (from 4 to
2.5 days on a 100-CPU cluster). However, the ex-
haustive sentence-level search improves the BLEU
score by about 1 % on the Spanish-English data.
227
Table 2: Spanish-English and Portuguese-English
extraction results. Extraction threshold is ? =
0.75 for both language pairs. # cands reports the
size of the overall search space in terms of sen-
tence pairs processed .
Data Source # cands # pairs Bleu
Baseline - 1.826 M 42.3
+ Giga 999.3 B 1.357 M 45.7
+ Giga (BM25) 25.4 B 0.609 M 44.8
Baseline - 2.222 M 45.3
+ News Data 2006 77.8 B 56 K 47.2
5 Future Work and Discussion
In this paper, we have presented a novel beam-
search algorithm to extract sentence pairs from
comparable data . It can avoid any pre-filtering
at the document level (Resnik and Smith, 2003;
Snover et al, 2008; Utiyama and Isahara, 2003;
Munteanu and Marcu, 2005; Fung and Cheung,
2004). The novel algorithm is successfully eval-
uated on news data for two language pairs. A
related approach that also avoids any document-
level pre-filtering has been presented in (Tillmann
and Xu, 2009). The efficient implementation tech-
niques in that paper are extended for the ME clas-
sifier and beam search algorithm in the current pa-
per, i.e. feature function values are cached along
with Model-1 probabilities.
The search-driven extraction algorithm presented
in this paper might also be applicable to other
NLP extraction task, e.g. named entity extraction.
Rather then employing a cascade of filtering steps,
a one-stage search with a specially adopted feature
set and search space organization might be carried
out . Such a search-driven approach makes less
assumptions about the data and may increase the
number of extracted entities, i.e. increase recall.
Acknowledgments
We would like to thanks the anonymous reviewers
for their valuable remarks.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. CL, 19(2):263?311.
Pascale Fung and Percy Cheung. 2004. Mining Very-
Non-Parallel Corpora: Parallel Sentence and Lexi-
con Extraction via Bootstrapping and EM. In Proc,
of EMNLP 2004, pages 57?63, Barcelona, Spain,
July.
Dave Graff. 2006. LDC2006T12: Spanish Gigaword
Corpus First Edition. LDC.
Dave Graff. 2007. LDC2007T07: English Gigaword
Corpus Third Edition. LDC.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA?04, Washing-
ton DC, September-October.
Dragos S. Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. CL, 31(4):477?504.
H. Ney. 1984. The Use of a One-stage Dynamic Pro-
gramming Algorithm for Connected Word Recogni-
tion. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 32(2):263?271.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative Models of Noisy Translations
with Applications to Parallel Fragment Extraction.
In Proc. of the MT Summit XI, pages 321?327,
Copenhagen,Demark, September.
Philip Resnik and Noah Smith. 2003. The Web as
Parallel Corpus. CL, 29(3):349?380.
S E Robertson, S Walker, M M Beaulieu, and M Gat-
ford. 1995. Okapi at TREC-4. In Proc. of the 4th
Text Retrieval Conference (TREC-4), pages 73?96.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proc. of EMNLP08,
pages 856?865, Honolulu, Hawaii, October.
Christoph Tillmann and Jian-Ming Xu. 2009. A Sim-
ple Sentence-Level Extraction Algorithm for Com-
parable Data. In Companion Vol. of NAACL HLT
09, pages 93?96, Boulder, Colorado, June.
Christoph Tillmann and Tong Zhang. 2007. A Block
Bigram Prediction Model for Statistical Machine
Translation. ACM-TSLP, 4(6):1?31, July.
Christoph Tillmann, Stefan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based Search Using
Monotone Alignments in Statistical Translation. In
Proc. of ACL 97, pages 289?296, Madrid,Spain,
July.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
Measures for Aligning Japanese-English News Arti-
cles and Sentences. In Proc. of ACL03, pages 72?79,
Sapporo, Japan, July.
T.K. Vintsyuk. 1971. Element-Wise Recognition of
Continuous Speech Consisting of Words From a
Specified Vocabulary. Cybernetics (Kibernetica),
(2):133?143, March-April.
228
A Projection Extension Algorithm for Statistical Machine Translation
Christoph Tillmann
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
ctill@us.ibm.com
Abstract
In this paper, we describe a phrase-based
unigram model for statistical machine
translation that uses a much simpler set
of model parameters than similar phrase-
based models. The units of translation are
blocks ? pairs of phrases. During decod-
ing, we use a block unigram model and a
word-based trigram language model. Dur-
ing training, the blocks are learned from
source interval projections using an un-
derlying high-precision word alignment.
The system performance is significantly
increased by applying a novel block exten-
sion algorithm using an additional high-
recall word alignment. The blocks are fur-
ther filtered using unigram-count selection
criteria. The system has been successfully
test on a Chinese-English and an Arabic-
English translation task.
1 Introduction
Various papers use phrase-based translation systems
(Och et al, 1999; Marcu and Wong, 2002; Ya-
mada and Knight, 2002) that have shown to improve
translation quality over single-word based transla-
tion systems introduced in (Brown et al, 1993). In
this paper, we present a similar system with a much
simpler set of model parameters. Specifically, we
compute the probability of a block sequence
  
. A
block
 
is a pair consisting of a contiguous source
and a contiguous target phrase. The block sequence
Figure 1: A block sequence that jointly generates

target and source phrases. The example is actual
decoder output and the English translation is slightly
incorrect.
probability 	
 
 
is decomposed into conditional
probabilities using the chain rule:

 






	
 

 


 (1)
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 9?16,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Efficient Dynamic Programming Search Algorithms for Phrase-Based SMT
Christoph Tillmann
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
ctill@us.ibm.com
Abstract
This paper presents a series of efficient
dynamic-programming (DP) based algorithms
for phrase-based decoding and alignment
computation in statistical machine translation
(SMT). The DP-based decoding algorithms are
analyzed in terms of shortest path-finding al-
gorithms, where the similarity to DP-based
decoding algorithms in speech recognition is
demonstrated. The paper contains the follow-
ing original contributions: 1) the DP-based de-
coding algorithm in (Tillmann and Ney, 2003)
is extended in a formal way to handle phrases
and a novel pruning strategy with increased
translation speed is presented 2) a novel align-
ment algorithm is presented that computes a
phrase alignment efficiently in the case that it
is consistent with an underlying word align-
ment. Under certain restrictions, both algo-
rithms handle MT-related problems efficiently
that are generally NP complete (Knight, 1999).
1 Introduction
This paper deals with dynamic programming based de-
coding and alignment algorithms for phrase-based SMT.
Dynamic Programming based search algorithms are be-
ing used in speech recognition (Jelinek, 1998; Ney et
al., 1992) as well as in statistical machine translation
(Tillmann et al, 1997; Niessen et al, 1998; Tillmann
and Ney, 2003). Here, the decoding algorithms are de-
scribed as shortest path finding algorithms in regularly
structured search graphs or search grids. Under certain
restrictions, e.g. start and end point restrictions for the
path, the shortest path computed corresponds to a rec-
ognized word sequence or a generated target language
translation. In these algorithms, a shortest-path search
({1},1)
({1,3},3)
({1,2},2)
({1,4},4)
({1,5},5)
({1,2,4},4)
({1,2,3},3)
({1,2,5},5)
({1,3,4},4)
({1,2,3},2)
({1,3,5},5)
({1,3,4},3)
({1,2,4},2)
({1,4,5},5)
({1,3,5},3)
({1,2,5},2)
({1,4,5},4)
({1,2,3,5},5)
({1,2,4,5},5)
({1,3,4,5},5)
({1,2,3,4},4)
({1,2,4,5},4)
({1,3,4,5},4)
({1,2,3,4},3)
({1,2,3,5},3)
({1,3,4,5},3)
({1,2,3,4},2)
({1,2,3,5},2)
({1,2,4,5},2)
({1,2,3,4,5},2)
({1,2,3,4,5},3)
({1,2,3,4,5},4)
({1,2,3,4,5},5)
Final
Figure 1: Illustration of a DP-based algorithm to solve
a traveling salesman problem with   cities. The visited
cities correspond to processed source positions.
is carried out in one pass over some input along a spe-
cific ?direction?: in speech recognition the search is time-
synchronous, the single-word based search algorithm in
(Tillmann et al, 1997) is (source) position-synchronous
or left-to-right, the search algorithm in (Niessen et al,
1998) is (target) position-synchronous or bottom-to-top,
and the search algorithm in (Tillmann and Ney, 2003) is
so-called cardinality-synchronous.
Taking into account the different word order between
source and target language sentences, it becomes less ob-
vious that a SMT search algorithm can be described as a
shortest path finding algorithm. But this has been shown
by linking decoding to a dynamic-programming solution
for the traveling salesman problem. This algorithm due
to (Held and Karp, 1962) is a special case of a shortest
path finding algorithm (Dreyfus and Law, 1977). The
regularly structured search graph for this problem is il-
lustrated in Fig. 1: all paths from the left-most to the
right-most vertex correspond to a translation of the in-
9
put sentence, where each source position is processed ex-
actly once. In this paper, the DP-based search algorithm
in (Tillmann and Ney, 2003) is extended in a formal way
to handle phrase-based translation. Two versions of a
phrase-based decoder for SMT that search slightly dif-
ferent search graphs are presented: a multi-beam decoder
reported in the literature and a single-beam decoder with
increased translation speed 1. A common analysis of all
the search algorithms above in terms of a shortest-path
finding algorithm for a directed acyclic graph (dag) is
presented. This analysis provides a simple way of ana-
lyzing the complexity of DP-based search algorithm.
Generally, the regular search space can only be fully
searched for small search grids under appropriate restric-
tions, i.e. the monotonicity restrictions in (Tillmann et
al., 1997) or the inverted search graph in (Niessen et al,
1998). For larger search spaces as are required for con-
tinuous speech recognition (Ney et al, 1992) 2 or phrase-
based decoding in SMT, the search space cannot be fully
searched: suitably defined lists of path hypothesis are
maintained that partially explore the search space. The
number of hypotheses depends locally on the number hy-
potheses whose score is close to the top scoring hypothe-
sis: this set of hypotheses is called the beam.
The translation model used in this paper is a phrase-
based model, where the translation units are so-called
blocks: a block is a pair of phrases which are transla-
tions of each other. For example, Fig. 2 shows an Arabic-
English translation example that uses   blocks. During
decoding, we view translation as a block segmentation
process, where the input sentence is segmented from left
to right and the target sentence is generated from bottom
to top, one block at a time. In practice, a largely mono-
tone block sequence is generated except for the possibil-
ity to swap some neighbor blocks. During decoding, we
try to minimize the score 	 
 of a block sequence 	 

under the restriction that the concatenated source phrases
of the blocks  yield a segmentation of the input sen-
tence:




















Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 37?45,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Rule-Driven Dynamic Programming Decoder for Statistical MT
Christoph Tillmann
IBM T.J. Watson Research Center
Yorktown Heights, N.Y. 10598
ctill@us.ibm.com
Abstract
The paper presents an extension of a dynamic
programming (DP) decoder for phrase-based
SMT (Koehn, 2004; Och and Ney, 2004) that
tightly integrates POS-based re-order rules
(Crego and Marino, 2006) into a left-to-right
beam-search algorithm, rather than handling
them in a pre-processing or re-order graph
generation step. The novel decoding algo-
rithm can handle tens of thousands of rules
efficiently. An improvement over a standard
phrase-based decoder is shown on an Arabic-
English translation task with respect to trans-
lation accuracy and speed for large re-order
window sizes.
1 Introduction
The paper presents an extension of a dynamic
programming (DP) decoder for phrase-based SMT
(Koehn, 2004; Och and Ney, 2004) where POS-
based re-order rules (Crego and Marino, 2006) are
tightly integrated into a left-to-right run over the
input sentence. In the literature, re-order rules are
applied to the source and/or target sentence as a
pre-processing step (Xia and McCord, 2004; Collins
et al, 2005; Wang et al, 2007) where the rules can
be applied on both training and test data. Another
way of incorporating re-order rules is via extended
monotone search graphs (Crego and Marino, 2006)
or lattices (Zhang et al, 2007; Paulik et al, 2007).
This paper presents a way of handling POS-based
re-order rules as an edge generation process: the
POS-based re-order rules are tightly integrated into
a left to right beam search decoder in a way that
29 000 rules which may overlap in an arbitrary
way (but not recursively) are handled efficiently.
Example rules which are used to control the novel
DP-based decoder are shown in Table 1, where each
POS sequence is associated with possibly several
permutations pi. In order to apply the rules, the input
sentences are POS-tagged. If a POS sequence of a
rule matches some identical POS sequence in the in-
put sentence the corresponding words are re-ordered
according to pi. The contributions of this paper are
as follows: 1) The novel DP decoder can handle
tens of thousands of POS-based rules efficiently
rather than a few dozen rules as is typically reported
in the SMT literature by tightly integrating them
into a beam search algorithm. As a result phrase
re-ordering with a large distortion window can be
carried out efficiently and reliably. 2) The current
rule-driven decoder is a first step towards including
more complex rules, i.e. syntax-based rules as in
(Wang et al, 2007) or chunk rules as in (Zhang et
al., 2007) using a decoding algorithm that is con-
ceptually similar to an Earley-style parser (Earley,
1970). More generally, ?rule-driven? decoding is
tightly linked to standard phrase-based decoding. In
future, the edge generation technique presented in
this paper might be extended to handle hierarchical
rules (Chiang, 2007) in a simple left-to-right beam
search decoder.
In the next section, we briefly summarize the
baseline decoder. Section 3 shows the novel rule-
driven DP decoder. Section 4 shows how the current
decoder is related to both DP-based decoding algo-
rithms in speech recognition and parsing. Finally,
37
Table 1: A list of 28 878 reorder rules sorted according to the rule occurrence count N(r) is used in this paper.
For each POS sequence the corresponding permutation pi is shown. Rule ID is the ordinal number of a rule in
the sorted list. The maximum rule length that can be handled efficiently is surprisingly long: about 20 words.
Rule ID r POS sequence pi N(r)
1 DET NOUN DET ADJ ? 2 3 0 1 4 421
2 DET NOUN NSUFF-FEM-SG DET ADJ NSUFF-FEM-SG ? 3 4 5 0 1 2 2 257
.
.
.
.
.
.
.
.
.
3 000 NOUN CASE-INDEF-ACC ADJ NSUFF-FEM-SG CONJ ADJ NSUFF-FEM-SG ? 2 3 4 5 6 0 1 6
.
.
.
.
.
.
.
.
.
28 878 PREP DET NOUN DET ADJ PREP NOUN-PROP ADJ ? 0 1 2 7 8 3 4
NSUFF-MASC-SG-ACC-INDEF CONJ IV3MS IV IVSUFF-DO:3FS 9 10 11 12 5 6 2
Section 5 shows experimental results.
2 Baseline DP Decoder
The translation model used in this paper is a phrase-
based model (Koehn et al, 2003), where the trans-
lation units are so-called blocks: a block b is a pair
consisting of a source phrase s and a target phrase
t which are translations of each other. The ex-
pression block is used here to emphasize that pairs
of phrases (especially longer phrases) tend to form
closely linked units in such a way that the transla-
tion process can be formalized as a block segmen-
tation process (Nagata et al, 2006; Tillmann and
Zhang, 2007). Here, the input sentence is segmented
from left to right while simultaneously generating
the target sentence, one block at a time. In prac-
tice, phrase-based or block-based translation mod-
els which largely monotone decoding algorithms ob-
tain close to state-of-the-art performance by using
skip and window-based restrictions to reduce the
search space (Berger et al, 1996). During decod-
ing, we maximize the score sw(bn1 ) of a phrase-pair
sequence bn1 = (si, ti)n1 :
sw(bn1 ) =
n
?
i=1
wT ? f(bi, bi?1), (1)
where bi is a block, bi?1 is its predecessor block,
and f(bi, bi?1) is a 8-dimensional feature vector
where the features are derived from some probabilis-
tic models: language model, translation model, and
distortion model probabilities. n is the number of
blocks in the translation and the weight vector w is
trained in a way as to maximize the decoder BLEU
score on some training data using an on-line algo-
rithm (Tillmann and Zhang, 2008). The decoder that
carries out the optimization in Eq. 1 is similar to a
standard phrase-based decoder (Koehn, 2004; Och
and Ney, 2004), where states are tuples of the fol-
lowing type:
[ C ; [i, j] ], (2)
where C is the so-called coverage vector that keeps
track of the already processed source position, [i, j]
is the source interval covered by the last source
phrase match. In comparison, (Koehn, 2004) uses
only the position of the final word of the last source
phrase translated. Since we are using the distortion
model in (Al-Onaizan and Papineni, 2006) the entire
last source phrase interval needs to be stored. Hy-
pothesis score and language model history are omit-
ted for brevity reasons. The states are stored in lists
or stacks and DP recombination is used to reduce the
size of the search space while extending states.
The algorithm described in this paper uses an in-
termediate data structure called an edge that repre-
sents a source phrase together with a target phrase
that is one of its possible translation. Formally, we
define:
[ [i, j] , tN1 ], (3)
where tN1 is the target phrase linked to the source
phrase si, ? ? ? , sj . The edges are stored in a so-called
chart. For each input interval that is matched by
some source phrase in the block set, a list of pos-
sible target phrase translations is stored in the chart.
Here, simple edges as in Eq. 3 are used to gener-
ate so-called rule edges that are defined later in the
paper. A similar data structure corresponding to an
edge is called translation option in (Koehn, 2004).
While the edge generation potentially slows down
the overall decoding process, for the baseline de-
38
e = 1 e = 2
e = 3
a
2
a
1
a
0
e = 4
e = 5
a
3
a
4
  ?S imp le ?
? E d g e s ?
1.  
2 .
1    0
1    2   0
? S imp le ?
E d g e s ?
A d d i t i o n a l
? R u l e ?  E d g e
C o p i e s
e = 2 ,  r = 1
p = B E G
O R I G = [ 1 , 2 ]
e = 1 ,  r = 1
p = E N D
O R I G = [ 0 , 1 ]
e = 3 ,  r = 2
p = B E G
O R I G = [ 1 , 3 ]
e = 1 ,  r = 2
p = E N D
O R I G = [ 0 , 1 ]
e = 1 e = 2
e = 3
a
2
a
1
e = 4
e = 5
a
3
a
4
e = 1 ,  r = 3
p = B E G I N
O R I G = [ 0 , 1 ]
e = 4 ,  r = 3
p = I N T E R
O R I G = [ 3 , 4 ]
e = 3 ,  r = 3
p = E N D
O R I G = [ 1 , 2 ]
0    3   4   1   2
a
0
p
0
p
0
p
0
p
1
p
1
p
1
p
2
p
2
p
3
p
4
3 .
  ?Or ig ina l ?
C h a r t
  ?Ex tended ?
C h a r t
Figure 1: Addition of rule edges to a chart containing 5 simple edges (some rule edges are not shown). The simple
edges remain in the chart after the rule edges have been added: they are used to carry out monotone translations.
coder generating all the simple edges takes less than
0.3 % of the overall decoding time.
3 DP-Search with Rules
This section explains the handling of the re-order
rules as an edge generation process. Assuming a
monotone translation, for the baseline DP decoder
(Koehn, 2004) each edge ending at position j can be
continued by any edge starting at position j + 1, i.e.
the simple edges are fully connected with respect to
their start and ending positions. For the rule-driven
decoder, all the re-ordering is handled by generat-
ing additional edges which are ?copies? of the sim-
ple edges in each rule context in which they occur.
Here, a rule edge copy ending at position j is not
fully connected with all other edges starting at po-
sition j + 1. Once a rule edge copy for a particular
rule id r has been processed that edge can be con-
tinued only by an edge copy for the same rule until
the end of the rule has been reached. To formalize
the approach, the search state definition in Eq. 2 is
modified as follows:
[ s ; [i, j] , r , sr , e ? {false, true} ] (4)
Here, the coverage vector C is replaced by a single
number s: a monotone search is carried out and all
the source positions up to position s (including s)
are covered. [i, j] is the coverage interval for the last
source phrase translated (the same as in Eq. 2). r is
the rule identifier, i.e. a rule position in the list in
Table 1. sr is the starting position for the rule match
of rule r in the input sentence, and e is a flag that
indicates whether the hypothesis h has covered the
entire span of rule r yet. The search starts with the
following initial state:
[ ?1 ; [?1,?1] ,?1 , ?1 , e = true ] , (5)
where the starting positions s, sr, and the coverage
interval [i, j] are all initialized with ?1, a virtual
source position to the left of the uncovered source
input. Throughout the search, a rule id of ?1 in-
dicates that no rule is currently applied for that hy-
pothesis, i.e. a contiguous source interval to the left
of s is covered.
39
States are extended by finding matching edges
and the generation of these edges is illustrated in
Fig. 1 for the use of 3 overlapping rules on a source
segment of 5 words a0, ? ? ? , a4 1. Edges are shown
as rectangles where the number on the left inside the
box corresponds to the enumeration of the simple
edges. In the top half of the picture the simple edges
which correspond to 5 phrase-to-phrase translations
are shown. In the bottom half all the edges after the
rule edge extension are shown (including simple and
rule edges). A rule edge contains additional compo-
nents: the rule id r, a relative edge position p (ex-
plained below), and the original source interval of a
rule edge before it has been re-ordered. A rule edge
is generated from a simple edge via a re-order rule
application: the newly generated edges are added
into the chart as shown in the lower half of Figure 1.
Here, rule 1 and 2 generate two new edges and rule
3 generates three new edges that are added into the
chart at their new re-ordered positions, e.g. copies
of edge 1 are added for the rule id r = 1 at start
position 2, for rule r = 2 at start position 3, and for
rule r = 3 at start position 0. Even if an edge copy
is added at the same position as the original edge a
new copy is needed. The three rules correspond to
matching POS sequences, i.e. the Arabic input sen-
tence has been POS tagged and a POS pj has been
assigned to each Arabic word aj . The same POS
sequence might generate several different permuta-
tions which is not shown here.
More formally, the edge generation process is car-
ried out as follows. First, for each source interval
[k, l] all the matching phrase pairs are found and
added into the chart as simple edges. In a second
run over the input sentence for each source inter-
val [k, l] all matching POS sequences are computed
and the corresponding source words ak, ? ? ? , al are
re-ordered according to the rule permutation. On
the re-ordered word sequence phrase matches are
computed only for those source phrases that already
occurred in the original (un-reordered) source sen-
tence. Both edge generation steps together still take
less than 1 % of the overall decoding time as shown
in Section 5: most of the decoding time is needed to
access the translation and the language model prob-
1Rule edges and simple edges may overlap arbitrarily, but
the final translation constitutes a non-overlapping boundary se-
quence.
abilities when extending partial decoder hypotheses
2
. Typically rule matches are much longer than edge
matches where several simple edges are needed to
cover the entire rule interval, i.e. three edges for rule
r = 3 in Fig. 1. As the edge copies corresponding
to the same rule must be processed in sequence they
are assigned one out of three possible positions p:
? BEG: Edge copy matches at the begin of rule
match.
? INTER: Edge copy lies within rule match inter-
val.
? END: Edge copy matches at the end of rule
match.
Formally, the rule edges in Fig. 1 are defined as fol-
lows, where a rule edge includes all the components
of a simple edge:
[
[i, j] , tN1 , r, p, [pi(i), pi(j)]
]
, (6)
where r is the rule id and p is the relative edge po-
sition. [pi(i), pi(j)] is the original coverage inter-
val where the edge matched before being re-ordered.
The original interval is not a necessary component of
the rule-driven algorithm but it makes a direct com-
parison with the window-based decoder straight-
forward as explained below. The rule edge defi-
nition for a rule r that matches at position sr is
slightly simplified: the processing interval is ac-
tually [sr + i, sr + j] and the original interval is
[sr+pi(i), sr+pi(j)]. For simplicity reasons, the off-
set sr is omitted in Fig 1. Using the original interval
has the following advantage: as the edges are pro-
cessed from left-to-right and the re-ordering is con-
trolled by the rules the translation score computation
is based on the original source interval [pi(i), pi(j)]
and the monotone processing is based on the match-
ing interval [i, j]. For the rule-driven decoder it
looks like the re-ordering is carried out like in a reg-
ular decoder with a window-based re-ordering re-
striction, but the rule-induced window can be large,
i.e. up to 15 source word positions. In particular
a distortion model can be applied when using the
2Strictly speaking, the edge generation constitutes two addi-
tional runs over the input sentence. In future, the rule edges can
be computed ?on demand? for each input position j resulting in
an even stricter implementation of the beam search concept.
40
e = 1 e = 2 e = 3
e = 4 e = 5 e = 6 e = 7 e = 8
2 , B E G I N
e = 9
6 ,  E N D
e = 1 1
e = 1 0
5 , I N T E R6 , B E G I N 8 , I N T E R 7 , E N D
1 , E N D7 , B E G I N
2 0 1
1 0 3 22 .  R U L E :
3 .  R U L E :
a
1
a
2
a
3
a
4
a
5
a
6
1 2 3 01 .  R U L E :
a
0
7 , B E G I N
3 ,  I N T E R
8 ,  I N T E R
h
1
h
2 h 3 h 4 h 5 h 6
h 7
h
8
h 9 h 1 0
h
1 1
h
1 2
h
1 3
h
1 4
Figure 2: Search lattice for the rule-driven decoder. The gray circles indicated partial hypotheses. An hypothesis is
expanded by applying an edge. DP recombination is used to restrict the search space throughout the rule lattice.
re-order rules. Additionally, rule-based probabilities
can be used as well. This concept allows to directly
compare a window-based decoder and the current
rule based decoder in Section 5.
The search space for the rule-driven decoder is il-
lustrated in Fig. 2. The gray shaded circles represent
translation hypotheses according to Eq. 4. A trans-
lation hypothesis h1 is extended by an edge which
covers some uncovered portion of the input sen-
tence to produce a new hypothesis h2. The decoder
searches monotonically through the entire chart of
edges, and word re-ordering is possible only through
the use of rule edges. The top half of the picture
shows the way simple edges contribute to the search
process: they are used to carry out a monotone trans-
lation. The dashed arrows indicate that hypotheses
can be recombined: when extending hypothesis h3
by edge e = 2 and hypothesis h4 by edge e = 8
only a single hypothesis h5 is kept as the history of
edge extensions can be ignored for future decoder
decisions with respect to the uncovered source posi-
tions. Here, the distortion model and the language
model history are ignored for illustration purposes.
As it can be seen in Fig. 2, the rule edge generation
step has created 3 copies of the simple edge e = 7,
which are marked by a dashed borderline. Hypothe-
ses covering the same input may not be merged, i.e.
hypotheses h9 and h13 for rules r = 1 and r = 2
have to be kept separate from the hypothesis h4. But
state merging may occur for states generated by rule
edges for the same rule r, i.e. rule r = 1 and state
h9.
Since rule edges have to be processed in a sequen-
tial order, looking up those that can extend a given
hypothesis h is more complicated than a phrase
translation look-up in a regular decoder. Given the
search state definition in Eq. 4, for a given rule id r
and coverage position s we have to be able to look-
up all possible edge extensions efficiently. This is
implemented by storing two lists:
1. For each source position j a list of possible
?starting? edges: these are all the simple edges
plus all rule edges with relative edge position
p = BEG. This list is used to expand hypotheses
according to the definition in Eq. 4 where the
rule flag e = true, i.e. the search has finished
covering an entire rule interval.
2. The second list is for continuing edges (p =
INTER or p = END). For each rule id r, rule
41
start position sr and source position j a list of
rule edges has to be stored that can continue an
already started rule coverage. This list is used
to expand hypotheses for which the rule flag e
is e = false, i.e. the hypothesis has not yet
finished covering the current rule interval, e.g.
the hypotheses h9 and h11 in Fig. 2.
The two lists are computed by a single run over
the chart after all chart edges have been generated
and before the search is carried out (the CPU time
to generate these lists is included in the edge gener-
ation CPU time reported in Section 5). The two lists
are used to find the successor edges for each hypoth-
esis h that corresponds to a rule r efficiently: only
a small fraction of the chart edges starting at posi-
tion j needs to be retrieved for an extension. The
rule start position sr has to be included for the sec-
ond list: it is possible that the same rule r matches
the input sentences for two intervals [i, j] and [i?, j?]
which overlap. This results in an invalid search state
configuration. Based on the two lists a monotone
search is carried out over the extended rule edge set
which implicitly generates a reordering lattice as in
similar approaches (Crego and Marino, 2006; Zhang
et al, 2007). But because the handling of the edges
is tightly integrated into the beam search algorithm
by applying the same beam thresholds it potentially
handles 10?s of thousands of rules efficiently.
4 DP Search
The DP decoder described in the previous section
bears some resemblance with search algorithms for
large vocabulary speech recognition. For exam-
ple, (Jelinek, 1998) presents a Viterbi decoder that
searches a composite trellis consisting of smaller
HMM acoustic trellises that are combined with lan-
guage model states in the case a trigram language
model. Multiple ?copies? of the same acoustic sub
models are incorporated into the overall trellis. The
highest probability word sequences is obtained us-
ing a Viterbi shortest path finding algorithm in a
possibly huge composite HMM (cf. Fig. 5.3 of
(Jelinek, 1998)). In comparison, in this paper the
edge ?copies? are used to generate hypotheses that
are hypotheses ?copies? of the same phrase match,
e.g. in Fig. 2 the states h4, h8, and h14 all result
from covering the same simple edge e7 as the most
recent phrase match. The states form a potentially
huge lattice as shown in Fig. 2. Similarly, (Ort-
manns and Ney, 2000) presents a DP search algo-
rithm where the interdependent decisions between
non-linear time alignment, word boundary detec-
tion, and word identification (the pronunciation lex-
icon is organized efficiently as a lexical tree) are all
carried out by searching a shortest path trough a pos-
sibly huge composite trellis or HMM. The similar-
ity between those speech recognition algorithms and
the current rule decoder derives from the following
observation: the use of a language model in speech
recognition introduces a coupling between adjacent
acoustic word models. Similarly, a rule match which
typically spans several source phrase matches intro-
duces a coupling between adjacent simple edges.
Viewed in this way, the handling of copies is a
technique of incorporating higher-level knowledge
sources into a simple one-step search process: ei-
ther by processing acoustic models in the context of
a language model or by processing simple edges in
the context of bigger re-ordering units, which exploit
a richer linguistic context.
The Earley parser in the presentation (Jurafsky
and Martin, 2000) also uses the notion of edges
which represent partial constituents derived in the
parsing process. These constituents are interpreted
as edges in a directed acyclic graph (DAG) which
represents the set of all sub parse trees considered.
This paper uses the notion of edges as well fol-
lowing (Tillmann, 2006) where phrase-based decod-
ing is also linked to a DAG path finding problem.
Since the re-order rules are not applied recursively,
the rule-driven algorithm can be linked to an Earley
parser where parsing is done with a linear grammar
(for a definition of linear grammar see (Harrison,
1978)). A formal analysis of the rule-driven decoder
might be important because of the following consid-
eration: in phrase-based machine translation the tar-
get sentence is generated from left-to-right by con-
catenating target phrases linked to source phrases
that cover some source positions. Here, a coverage
vector is typically used to ensure that each source
position is covered a limited number of times (typi-
cally once). Including a coverage vector C into the
search state definition results in an inherently expo-
nential complexity: for an input sentence of length
J there are 2J coverage vectors (Koehn, 2004). On
42
Table 2: Translation results on the MT06 data. w is the distortion limit.
words / sec generation [%] BLEU PREC TER
Baseline decoder w = 0 171.6 1.90 34.6 35.2 65.3
w = 2 25.4 0.29 36.6 37.7 63.5
w = 5 8.2 0.10 35.0 36.1 65.1
Rule decoder N(r) ? 2 9.1 0.75 37.1 38.2 63.5
(w = 15) N(r) ? 5 10.5 0.43 37.2 38.2 63.5
the contrary, the search state definition in Eq. 4 ex-
plicitly avoids the use of a coverage vector result-
ing in an essentially linear time decoding algorithm
(Section 5 reports the size of the the extended search
graph in terms of number of edges and shows that
the number of permutations per POS sequence is
less than 2 on average). The rule-driven algorithm
might be formally correct in the following sense. A
phrase-based decoder has to generate a phrase align-
ment where each source position needs to be cov-
ered by exactly one source phrase. The rule-based
decoder achieves this by local computation only: 1)
no coverage vector is used, 2) the rule edge genera-
tion is local to each individual rule, i.e. looking only
at the span of that rule, and 3) rules whose appli-
cation spans overlap arbitrarily (but not recursively)
are handled correctly. In future, a formal correctness
proof might be given.
5 Experimental Results
We test the novel edge generation algorithm on
a standard Arabic-to-English translation tasks: the
MT06 Arabic-English DARPA evaluation set con-
sisting of 1 529 sentences with 58 331 Arabic words
and 4 English reference translations . The transla-
tion model is defined in Eq. 1 where 8 probabilis-
tic features (language, translation,distortion model)
are used. The distortion model is similar to (Al-
Onaizan and Papineni, 2006). An on-line algorithm
similar to (Tillmann and Zhang, 2008) is used to
train the weight vector w. The decoder uses a 5-
gram language model , and the phrase table consists
of about 3.2 million phrase pairs. The phrase table
as well as the probabilistic features are trained on a
much larger training data consisting of 3.8 million
sentences. Translation results are given in terms of
the automatic BLEU evaluation metric (Papineni et
al., 2002) as well as the TER metric (Snover et al,
2006).
Our baseline decoder is similar to (Koehn, 2004;
Moore and Quirk, 2007). The goal of the current
paper is not to demonstrate an improvement in de-
coding speed but show the validity of the rule edge
generation algorithm. While the baseline and the
rule-driven decoder are compared with respect to
speed, they are both run with conservatively large
beam thresholds, e.g. a beam limit of 500 hypothe-
ses and a beam threshold of 7.5 (logarithmic scale)
per source position j. The baseline decoder and the
rule decoder use only 2 stacks to carry out the search
(rather than a stack for each source position) (Till-
mann, 2006). No rest-cost estimation is employed.
For the results in line 2 the number of phrase ?holes?
n in the coverage vector for a left to right traver-
sal of the input sentence is restricted using a typi-
cal skip-based decoder (Berger et al, 1996). Up to
2 phrases can be skipped. Additionally, the phrase
re-ordering is restricted to take place within a given
window size w. The 28, 878 rules used in this paper
are obtained from 14 989 manually aligned Arabic-
English sentences where the Arabic sentences have
been segmented and POS tagged . The rule selec-
tion procedure is similar to the one used in (Crego
and Marino, 2006) and rules are extracted that oc-
cur at least twice. The rule-based re-ordering uses
an additional probabilistic feature which is derived
from the rule unigram count N(r) shown in Table. 1:
p(r) = N(r)?
r?
N(r?) . The average number of POS se-
quence matches per input sentence is 34.9 where the
average number of permutations that generate edges
is 57.7. The average number of simple edges i.e.
phrase pairs per input sentence is 751.1. For the
rule-based decoder the average number of edges is
3187.8 which includes the simple edges.
Table 2 presents results that compare the base-
line decoder with the rule-driven decoder in terms
43
of translation performance and decoding speed. The
second column shows the distortion limit used by
the two decoders. For the rule-based decoder a max-
imum distortion limit w is implemented by filter-
ing out all the rule matches where the size of the
rule in terms of number of POS symbols is greater
than w, i.e. the rule edges are processed mono-
tonically but a monotone rule edge sequence for
the same rule id may not span more than w source
positions. The third column shows the translation
speed in terms of words per second. The fourth
column shows the percentage of CPU time needed
for the edge generation (including both simple and
rule edges). The final three columns report transla-
tion results in terms of BLEU , BLEU precision
score (PREC), and TER. The rule-based reorder-
ing restriction obtains the best translation scores on
the MT06 data: a BLEU score of 37.2 compared
to a BLEU score of 36.6 for the baseline decoder.
The statistical significance interval is rather large:
2.9 % on this test set as text from various gen-
res is included. Additional visual evaluation on the
dev set data shows that some successful phrase re-
ordering is carried out by the rule decoder which is
not handled correctly by the baseline decoder. As
can be seen from the results reducing the number
of rules by filtering all rules that occur at least 5
times (about 10 000 rules) slightly improves trans-
lation performance from 37.1 to 37.2. The edge
generation accounts for only a small fraction of the
overall decoding time. Fig. 3 and Fig. 4 demonstrate
additional advantages when using the rule-based de-
coder. Fig. 3 shows the translation BLEU score as
a function of the distortion limit window w. The
BLEU score actually decreases for the baseline de-
coder as the size w is increased. The optimal win-
dow size is surprisingly small: w = 2. A simi-
lar behavior is also reported in (Moore and Quirk,
2007) where w = 5 is used . For the rule-driven de-
coder however the BLEU score does not decrease
for large w: the rules restrict the local re-ordering in
the context of potentially very long POS sequences
which makes the re-ordering more reliable. Fig. 4
which shows the decoding speed as a function of the
window size w demonstrates that the rule-based de-
coder actually runs faster than the baseline decoder
for window sizes w ? 5.
 0.33
 0.34
 0.35
 0.36
 0.37
 0.38
 0.39
 0  2  4  6  8  10  12  14  16
BL
EU
maximum window size
rule-driven decoder N(r)>=5
BL
EU
rule-driven decoder N(r)>=2
BL
EU
distortion-limited phrase decoder
Figure 3: BLEU score as a function of window size w.
 10
 100
 0  2  4  6  8  10  12  14  16
CP
U 
[w
ord
s /
 se
c]
maximum window size
rule-driven decoder N(r)>=5
CP
U 
[w
ord
s /
 se
c]
rule-driven decoder N(r)>=2
CP
U 
[w
ord
s /
 se
c]
distortion-limited phrase decoder
Figure 4: Decoding speed as a function of window size
w.
6 Discussion and Future Work
The handling of the re-order rules is most similar to
work in (Crego and Marino, 2006) where the rules
are used to create re-order lattices. To make this
feasible, the rules have been vigorously filtered in
(Crego and Marino, 2006): only about 30 rules are
used in their experiments. On the contrary, the cur-
rent approach tightly integrates the re-order rules
into a phrase-based decoder such that 29 000 rules
can be handled efficiently. In future work our novel
approach might allow to make use of lexicalized re-
order rules as in (Xia and McCord, 2004) or syntac-
tic rules as in (Wang et al, 2007).
7 Acknowledgment
This work was supported by the DARPA GALE
project under the contract number HR0011-06-2-
00001. The authors would like to thank the anony-
mous reviewers for their detailed criticism.
44
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion Models for Statistical Machine Translation.
In Proceedings of ACL-COLING?06, pages 529?536,
Sydney, Australia, July.
Adam L. Berger, Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and
Robert L. Mercer. 1996. Language Translation Ap-
paratus and Method of Using Context-Based Trans-
lation Models. United States Patent, Patent Number
5510981, April.
David Chiang. 2007. Hierarchical Machine Translation.
Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL?05, pages 531?540, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
J.M. Crego and Jose? B. Marino. 2006. Integration of
POStag-based Source Reordering into SMT Decoding
by an Extended Search Graph. In Proc. of AMTA06,
pages 29?36, Cambridge, MA, August.
Jay Earley. 1970. An Efficient Context-Free Parsing Al-
gorithm. Communications of the ACM, 13(2):94?102.
Michael A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison Wesley.
Fred Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, MA.
Daniel Jurafsky and James H. Martin. 2000. Speech and
Language Processing. Prentice Hall.
Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL?03: Main Proceedings, pages 127?133, Ed-
monton, Alberta, Canada, May 27 - June 1.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA?04, Washington
DC, September-October.
Robet C. Moore and Chris Quirk. 2007. Faster Beam-
search Decoding for Phrasal SMT. Proc. of the MT
Summit XI, pages 321?327, September.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A Clustered Global
Phrase Reordering Model for Statistical Machine
Translation. In Proceedings of ACL-COLING?06,
pages 713?720, Sydney, Australia, July.
Franz-Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417?450.
Stefan Ortmanns and Hermann Ney. 2000. Progress in
Dynamic Programming Search for LVCSR. Proc. of
the IEEE, 88(8):1224?1240.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Proc. of
ACL?02, pages 311?318, Philadelphia, PA, July.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
Phrase-Based MT System for the 2007 ACL Workshop
on SMT. In Proc. of the ACL 2007 Second Workshop
on SMT, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA 2006, Boston,MA.
Christoph Tillmann and Tong Zhang. 2007. A Block Bi-
gram Prediction Model for Statistical Machine Trans-
lation. ACM-TSLP, 4(6):1?31, July.
Christoph Tillmann and Tong Zhang. 2008. An Online
Relevant Set Algorithm for Statistical Machine Trans-
lation. Accepted for publication in IEEE Transaction
on Audio, Speech, and Language Processing.
Christoph Tillmann. 2006. Efficient Dynamic Program-
ming Search Algorithms for Phrase-based SMT. In
Proceedings of the Workshop CHPSLP at HLT?06,
pages 9?16, New York City, NY, June.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese Syntactic reordering for statistical machine
translation. In Proc. of EMNLP-CoNLL?07, pages
737?745, Prague, Czech Republic, July.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proc. of Coling 2004, pages 508?514,
Geneva, Switzerland, Aug 23?Aug 27. COLING.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level Reordering of Source Language Sen-
tences with Automatically Learned Rules for Statis-
tical Machine Translation. In Proc. of SSST, NAACL-
HLT?07 / AMTA Workshop, pages 1?8, Rochester, NY,
April.
45
Word Re-order ing  and  DP-based  Search  in S ta t i s t i ca l  Mach ine  
Trans la t ion  
Christoph Til lmann and Hermann Ney 
Lehrstuhl fiir Informatik VI, Computer Science Department 
RWTH Aachen - University of Technology 
D-52056 Aachen, Germany 
{t illmann, ney}@inf ormat ik. rwth-aachen, de 
Abstract 
In this paper, we describe a search procedure for sta- 
tistical machine translation (MT) based on dynmnic 
programming (DP). Starting from a DP-based solu- 
tion to the traveling salesman problem, we present 
a novel technique to restrict the possible word re- 
ordering between source and target language in or- 
der to achieve an efficient search algorithm. A search 
restriction especially useful for tile translation di- 
rection from German to English is presented. The 
experimental tests are carried out on the Verbmo- 
bil task (Germm>English, 8000-word vocabulary), 
which is a limited-domain spoken-language task. 
1 Introduction 
The goal of machine translation is tile translation 
of a text given in some source language into a tar- 
gel: language. We are given a source string f J  = 
fl ...fj.-.f.l of length J, wlfich is to be translated into 
a target string c\[ = cl ...ei...el of length I. Among 
all possible target strings, we will choose the string 
with the highest probability: 
dI = argmax {Pr(e{lff)} q 
-- argmax {Pr (e l ) .P r ( f f le l )  } (1) 
The argmax operation denotes the search problem, 
i.e. the generation of the output sentence in the tar- 
get language. Pr(c~) is the language model of tim 
target language, whereas Pr(fi'le*l) is the transla- 
tion model. Our approach uses word-to-word epen- 
dencies between source and target words. The model 
is often further restricted so that each source word 
is assigned to exactly one target word (Brown et al, 
1993; Ney et al, 2000). These alignment models 
are similar to the concept of hidden Markov models 
(HMM) in speech recognition. The alignment map- 
ping is j --+ i = aj from source position j to target 
position i = aj. The use of this alignment model 
raises major problems if a source word has to be 
aligned to several target words, e.g. when translat- 
ing German compound nouns. A siuLple extension 
will be used to handle this problem. 
In Section 2, we briefly review our approach to sta- 
tistical machine translation. In Section 3, we in- 
troduce our novel concept to word re-ordering and 
a DP-based search, which is especially suitable for 
the translation direction fl'om German to English. 
This approach is compared to another re-ordering 
scheme presented in (Berger et al, 1996). In Sec- 
tion 4, we present the t)erformance measures used 
and give translation results on the Verbmobil task. 
2 Basic Approach 
In this section, we briefly review our translation ap- 
proach. In Eq. (1), Pr(C~l) is the language model, 
which is a trigrain language model in this case. For 
the translation model Pr(fil le{), we go on the as- 
sunlption that each source word is aligned to ex- 
actly one target word. The alignment model uses 
two kinds of parameters: alignment probabilities 
p(ajlaj_l, I, J), where the probability of alignment 
aj for position j deI)ends on tile previous alignment 
position aj-i  (Ney et al, 2000) and lexicon proba.- 
bilities p(\]~le~j). When aligning the words in par- 
allel texts (tbr language pairs like Spanish-English, 
French-Englisll, Italian-German,...), we typically ob- 
serve a strong localization effect. In many cases, 
there is an even stronger estriction: over large por- 
tions of tile source string, the alignment is monotone. 
2.1 Inverted Alignments 
To explicitly handle the word re-ordering between 
words in source and target language, we use the con- 
cept of the so-called inverted aligmnents as given in 
(Ney et al, 2000). An inverted alignment is defined 
as follows: 
inverted alignment: i -+ j = bi. 
Target positions i are mapped to source positions bi. 
What is important and is not expressed by the nota- 
tion is the so-called coverage constraint: each source 
position j should be 'hit' exactly once by the path 
of the inverted aligmnent b~ = bL...bi...bi. Using the 
inverted alignments in the maximum approximation, 
850 
we obtain as search criterion: 
/ f l  i - I  sn} . v ( ,Z l5  ? 
ell k i= l  
' }} ? max  I I  z,.5. v(f,,,le*)\] --- 
hi  i=1 
= max V( J I I ) -max 
I i I el 'hi i=1 
"p(bi\[bi-l,-\[,'\])'P(fbilCi)\] }}, 
where the two products over i have l)een merged into 
) i -1  a single product ()vet i. I (cilei_~) is tim trigram 
language model probability. The inverted alignment 
probability p(bi\[bi-l, I, .1) and the lexicon probabil- 
ity p(J'~,~ led are obtained by relative fl'equency es- 
timal;es frosll the Viterbi alignment path after the 
final training iteration. The details are given in 
(Och art(1 Ney, 2000). The sentence length prob- 
ability p(J\[1) is omitted without any loss in per- 
tbrmance. For the inverted alignment probability 
p(bi\[bi-~, I J), we drop the dependence on the tar- 
get sentence length I. 
2.2  Word Jo in ing  
The baseline alignment model does not pernfit hat a 
source word is aligned to two or more target words, 
e.g. %r the translation direction from German to 
\]?,nglish, the German (:Oml)ound I IOlStl  'Zahnarztter- 
rain' causes ira>blares, because it must be translated 
by the two target words dcntist'.s appoi'ntmcnt. We 
use a solution to this 1)roblenl similar to the one 
presented in (()ell el; al., 1999), where target words 
are joined during training. The word joining is (lotto 
on the basis of a likelihood criterion. An extended 
lexicon model is defined, and its likelihood is com- 
pared to a baseline lexicon model, which takes only 
single-word ependencies into aecollnt. E.g. when 
'Zahnarzttermin' is aligned to dentist'.s, the extended 
lexicon model might learn that 'Zahnarzttcrmin' ac- 
tually has to be aligned to both dentist's and ap- 
pointment. In the following, we assmne that this 
word joining has been carried out. 
"I DP  Algor i thm for Statistical 
Machine Translation 
in order to handle the necessary word re-ordering as 
&n optimization problem within our dynmnic pro- 
gramming approach, we describe a solution to the 
traveling salesnmn problem (TSP) which is based 
on dynamic programming (Held, Karp, 1962). The 
traveling salesman problem is an oi)timization prob- 
lem which is defined as follows: given are a set of 
May 
of 
fourth 
the 
oll 
you 
visit 
not 
can 
colleague 
my 
case 
this 
In 
O O O O O O O O O O ~_ . . . .O  
O O O O O O O O O /O~ O O O 
O O O O O O O O O/O O O O 
O O O O O O O O ? O O O O 
O O O O O O O O// O O O O O 
O O O O O O O/ /O  O O O O O 
o o o o o o 
oo Oo Oo Oo oO oO oO o o o ?
?_... o.... o ...o._...o.....? 
O O O O O/~ O O O O O O O 
O O O9/7 .O  O O O O O O O O 
O O f  O O O O O O O O O O 
 oO oOoOOoOOoOO 
I I I I 1 I I I I I I 1 I 
I d F k rn K S a v M n b 
. i a a e o i m i ~ i e 
e / n i / e e i C s 
s I n n I r h u 
e e t t C 
m g e h 
e I/ a 
n 
Figm'e 1: Re-ordering for the Gerlnan verbgroul). 
cities S = Sl ,--- ,  s ,  and tbr each pair of cities si, sj 
the cost dij > 0 for traveling flom city s: to city 
.sj. We arc.' looking for the shortest tour visiting 
all cities exactly once while starting and ending in 
city sl. A straightforward way to find the short- 
est tour is by trying all possible permutations of the 
n cities. The resulting algorithm has a complexity 
of O(n!). Itowever, dynamic progrmnming can be 
used to find tile shortest our in exponential time, 
namely in O(n 2-2'~), using the algorithm by Ileld and 
Karp. The approach recursively evahlates a quantity 
Q(C,j), where C is the set; of already visited cities 
and sj is the last visited city. Subsets C of increas- 
ing cardinality c are processed. The algorithm works 
due to the fact that not all permutations of cities 
have to be considered explicitly. For a given partial 
hypothesis (C, j), the order in which the cities in (2 
have beast visited cast be ignored (except j) ,  only the 
score for the best path reaching j has to be stored. 
This algorithm can be applied to statistical machine 
translation. Using the concept of inverted align- 
ments, we explicitly take care of the coverage con- 
straint by introducing a coverage set C of source sen- 
tence positions that have been already processed. 
The advantage is that we can recombine search hy- 
potheses by dynmnic programming. The cities of 
the traveling salesman probleIn correspond to source 
851 
Table 1: DP algorithm tbr statistical machine translation. 
input: source string fl..-f j . ..f. l  
initialization 
for each cardinality c = 1, 2,. - ?, J do 
tbr each pair (C,j), where j C C and ICl = c do 
tbr each tat'get word e E E 
max 
5,e  t !  
Q~,(e,C,j) =p(fjle) {p(jlj',J).p(5) .pa(ele',e" ) -O~,,(e',C\ {j},j')} 
j lEC\{j} 
words f j  in the input string of length J. For the 
final translation each source position is considered 
exactly once. Subsets of partial hypotheses with 
coverage sets C of increasing cardinality c are pro- 
cessed. For a trigrmn language model, the partial 
hyl)otheses are of tile form (c', c,(2, j) ,  e', c are the 
last two target words, C is a coverage set for tile al- 
ready covered source positions and j is the last posi- 
tion visited. Each distance in the traveling salesman 
problem now corresponds to the negative logarithm 
of tile product of the translation, alignment and lan- 
guage model probabilities. The following auxiliary 
quantity is defined: 
Qc~((?,~ C~j)  :.~. probability of tile best partial 
hypothesis (c~, b~), where 
C = {bt:\]k = 1, - - . , i} ,  bi = j ,  
(2 i ~- C a ILd  C i _  1 ~ e I. 
The type of alignment we have considered so far re- 
quires the stone length tbr source and target sen- 
tence, i.e. I = J.  Evidently, this is an unrealistic 
assumption, therefore we extend the concept of in- 
verted alignments as follows: Wtmn adding a new 
position to the coverage set C, we might generate i- 
ther 5 = 0 or a = 1 new target words. For 5 = 1, a 
new target language word is generated using the tri- 
gram language model p(e\[e', e"). For 5 = 0, no new 
target word is generated, while an additional source 
sentence position is covered. A modified language 
inodel probability pa(e\[c', c") is defined as follows: 
1.0 i f a=0 
Pa(ele"e") = p(e\]e',e") i fa  = 1 
We associate a distribution p(5) with the two cases 
5 = 0 and 5 = 1 and set p(5 = 1) = 0.7. 
The above auxiliary quantity satisfies tile following 
recursive DP equation: 
Qe, (c ,C , j )  = 
4. mein 5. Kollege 
~ ~  ,Q ~Verb_ .~  F in: l~ 
--_ J J  
1. In 7.nicht 9. Sie 
2. diesem 8. besuehen 10. am 
3. Fall 11. vierten 
6. kann 12. Mai 13.. 
Figure 2: Order in which source positions are visited 
tbr the example given in Fig.1. 
P(fJIe) "max {P(JlJ','/)'P(5)" ~,c I! 
i' ec\ {j} 
? pa( le', e")- Qe,, c \ {j}, j ') }. 
The DP equation is evaluated recursively for each 
hypothesis (e ' ,e,C, j ) .  TILe resulting algorithm is 
depicted in Table 1. The complexity of the algorithm 
is O(E  a ? j2 .2 .1) ,  where E is the size of the target 
language vocabulary. 
3.1 Word  Re-Order ing  w i th  Verbgroup 
Rest r i c t ions :  Quas i -monotone  Search  
The above search space is still too large to allow 
the translation of a inedium length input sentence. 
On the other hand, only very restricted re-orderings 
are necessary, e.g. for the translation direction fi'om 
852 
Table 2: Coverage set hyllothesis extensions for the IBM re-ordering. 
Predecessor eovera~e, set \[ I Successor coverage set 
({1,...,,, ,} \ ,l') ({1,... , , , ,} ,0 
({ \ ] , - -  . , , , t} \ {l,/1 } , l ' )  -- ({1 , ' ' ' ,  'L~, } \ {/l} ,/) 
\ ,l') ({1,.--,,,,,} \ ,0 
({ I , . . . , , , -  J} \ ,l') ({1,--.,,,,} \ ,m) 
Oerlnan to English the monotonicity constraint is 
violated mainly with respect; to the German verb- 
group. In German, the verbgroui) usually consists 
of a left and a right verbal brace., whereas in En- 
glish the words of the verbgroul) usually tbrm a se- 
quence of consecutive words. Our new al)t)roach, 
which is ('alle.d quas i -monotone  search, proce.sses 
the source sentence monotonically, while explicitly 
taking into account the positions of the (-lel'lnan 
verbgroup. 
A typical situation is shown in Figure \]. When 
translating the sentence monotonically fl'om left to 
right, the translation of the German finite verb 
'kmm', which is the left verbal brace in this case, 
is postponed mttil the German noun phrase 'mein 
t(ollege' is translated, which is the subject of the 
sentence. The.n, the. German infinitive 'besuclmn' 
and the negation particle 'nicht;' are translated. The 
trai ls\]at\oi l  of erie posit ion in the source sentence 
nmy be postponed ti)r up to L = 3 source positions, 
and the translation of u I) to two source positions 
l ink be anticittated for at most 1~ = l0 source l)osi- 
tions. To formalize the attl)roach, we introduce four 
verbgroup stat;es S: 
? hfitial (Z): A contiguous, initial block <)f s<mrce 
l)ositions is covered. 
? Skitlped (K;): The translation of up to one word 
may be l)OStl)oned . 
? Verl> (V): The translation of Ul> (;o two words 
may be antMl)ated. 
, Final (Y:): The rest of the sentence is pro- 
cessed monotonically taking accoullt of tit(; al: 
ready covered positions. 
\Vhile processing the source sentence monotonically, 
the iifitial state Z is entere.d whenever there are no 
mmovered positions to the left of the rightmost cov- 
ered position. The sequence of states needed to 
carry out the. word re-ordering example in Fig. 1 
is given in Fig. 2. The 13 positions of the source 
sentence are processed in the order shown. A posi- 
tion is presented by the word at that position. Using 
these states, we define partial hypothesis extensions, 
which are of the following type: 
(S',C \ { j} , j ' )  --9 (S,C, j ) ,  
Not only the cove.rage set C and the posit\oilS j , j ' ,  
but also the verbgroup states S, S' are taken into ac- 
count. ~1~) be short, we omit the target words c, e' in 
the tbrinulation of the search hypotheses. There are 
13 types of extensions needed to describe the verb- 
group re-ordering. The details are given in (Till- 
mann, 2000). For each extension a new position is 
added to the coverage set. Covering the first lul- 
covered position in the source sentence, we use the 
language model probatfility p(e\[$, $). IIere, $ is the 
sentence boundary symbol, which is thought o be at; 
position 0 in the target sentence. Tile search starts 
in the hyl)othesis (Z, {~}, 0). {~} denotes the empty 
set, where, no source sentence t)osition is covered. 
The following recursive quation is evaluated: 
= (2) 
{gj\[j', 3). p(5). ( ld, e"). p(fj lc) ? ntax ~,c II 
? max Oc, (c ' ,S ' ,C  \ {j},j')??. (st,/) ) 
(S t ,C \ t j} , f f )~(? , ,C , j )  
j ' cc \u}  
The search ends in the hypotheses (Z, {1, . - . ,  d}, j). 
{1, . . . ,  d} de.notes a coverage se.t including all posi- 
tions from the starting 1)osition I to position J and 
j C {d-  L , - . - ,  .\]}. The final score is obtaiiled from: 
,nax P($l", c'). G '  (c, Z, { 1, . . - ,  d}, .it, 
c ,c  I 
j c{a  :.,.. . ,a} 
where p($lc, c/) denotes the trigram language model, 
which predicts the sentence boundary $ at tim end 
of the target sentence. The complexity of the quasi- 
monotone search is O(\]'J a-,l- (\[~2-t-L-1~)). The proof 
is given ill (Tilhnann, 2000). 
3.2 Re-order ing  w i th  IBM Style 
Rest r i c t ions  
We compare our new api)roach with tim word re- 
ordering used in the IBM translation approach 
(Berger et al, 1996). A detailed descrit)tion of tile 
search procedure used is given in this patent. Source 
sentence words are aligned with hypothesized target 
sentence words, where the choice of a new source 
word, which has not been aligned with a target word 
yet, is restricted I . A procedural definitioll to restrict 
l In the approach described in (Berger et al, 1996), a mor- 
phological analysis is carried out and word morphenles rather 
thin, full-form words are used during the search, ltere, we 
process only flfll-forin words within the trmmlation proce.durc. 
853 
the number of pernmtations carried out for the word 
re-ordering is given. During the search process, a 
partial hyt)othesis i  extended by choosing a source 
sentence position, which has not been aligned with a 
target sentence t)osition yet. Only one of the first n 
positions which are not already aligned in a partial 
hyt)othesis may be chosen, where n is set to 4. Tile 
restriction can be expressed in terms of the nmn- 
ber of uncovered source sentence positions to the 
left of the rightmost position m in the coverage set. 
This munber must be less than or equal to n - 1. 
Otherwise for the predecessor search hyt)othesis, we 
wonld have chosen a position that would not have 
been among the first n uncovered t)ositions. 
Ignoring the identity of the target language words 
e and c', the possible partial hypothesis extensions 
due to the IBM restrictions are shown in Table 2. 
In general, m, l, l' ~k {/1,12,/3} and in line umber 3 
and 4, l' must be chosen not to violate the above 
re-ordering restriction. Note that in line 4 the last; 
visited position for tile successor hypothesis must 
be m. Otherwise, there will be four uncovered po- 
sitions tbr the t)redecessor hypothesis violating the 
restriction. A dynamic programming recursion sin> 
ilar to the one in Eq. 2 is evaluated. In this case, we 
have no finite-state restrictions for the search space. 
Tile search stm'ts in hyi)othesis ({0}, 0) and ends in 
the hyt)otheses ({1 , . . . , J} , j ) ,  with j C {1 , . . - ,d} .  
This approach leads to a search procedure with com- 
plexity O(E  a . j4). The proof is given in (Tilhnann, 
2000). 
4 Exper imenta l  Resu l t s  
4.1 The  Task and the Corpus  
We have tested tim translation system Oil the Verb- 
mobil task (Wahlster 1993). The Verbmobil task is 
an appointment scheduling task. Two subjects are 
each given a calendar and they are asked to schedule 
a meeting. The translation direction is from Ger- 
man to English. A summary of the corpus used in 
the experiments i given in Table 3. The perplexity 
for the trigrmn language model used is 26.5. Al- 
though the ultimate goal of tile Verbmobil project 
is the translation of spoken language, the input used 
for the translation experinmnts reported on in this 
paper is the (more or less) correct orthographic tran- 
scription of the spoken sentences. Thus, the effects 
of spontaneous speech are t)resent in the corpus, e.g. 
the syntactic structure of tile sentence is rather less 
restricted, however the effect of sl)eech recognition 
errors is not covered. 
For the experiments, we use a simt)le pret)rocessing 
step. German city names are replaced by category 
markers. The translation search is carried out with 
tlm category markers and tlm city names are resub- 
stituted into the target sentence as a postt)rocessing 
step. 
Table 3: Training and test; conditions for the Verb- 
mobil task (*number of words without punctuation 
marks). 
\ [German English 
Training: Selltences 
Words 
Words* 
Vocabulary Size 
Singletons 
Test-147: Sentences 
Words 
Perplexity 
58073 
519523 549921 
418979 453632 
7939 4648 
3454 1699 
147 
1968 2173 
- 26.5 
Table 4: Multi-reference word error rate (roWER) 
and subjective sentence rror rate (SSER) for three 
different search t)rocedures. 
Search CPU time 
Method \[sec\] 
MonS 0.9 
QmS 10.6 
IbnlS 28.6 
roWER SSER 
\[yo\] \[y0\] 
42.0 30.5 
34.4 23.8 
38.2 26.2 
4.2 Per formance Measures  
The following two error criteria are used ill our ex- 
t)erinmnts: 
? roWER: multi-reference WER: 
We use the Levenshtein distance between tile 
automatic translation and several reference 
translations as a measure of tile translation er- 
rors. On average, 6 reference translations per 
automatic translation are availal)le. Tile Lev- 
enshtein distance between the automatic trans- 
lation and each of tile reference translations is 
comt)uted, and the minimum Levenshtein dis- 
tance is taken. This measure has the advantage 
of being completely automatic. 
? SSER: subjective sentence rror rate: 
For a more detailed analysis, tile translations 
are judged i)y a tminan test 1)erson. For the er-- 
ror counts, a range from 0.0 to 1.0 is used. An 
error count of 0.0 is assigned to a perfect rans- 
lation, and an error count of 1.0 is assigned to 
a semantically and syntactically wrong transb> 
tion. 
4.3 Translat ion Exper iments  
For tile translation experiments, Eq. 2 is recursively 
evahlated. We apply a beam search concet)t as in 
st)eech recognition. However there is no global prun- 
ing. Search hypotheses are i)rocessed separately ac- 
cording to their coverage set d. The best scored 
854 
hyi)othesis tbr each coverage set is comlmted: 
Om, , , , , , ( c )  = c,c',6",j 
The hyl)othesis (d, e, $, C, j) is t)1'1151(;(l if:
Q~,(e,S,C, j )  < to.O,~cam(C), 
where to is a threshold to control the mmlber of sur- 
viving hypotheses. Additionally, for a given coverage 
set, at most 250 different hypotheses are kept dur- 
ing the search process, and the number of difl'erent 
words to |)e hyl)othesized by a source word is lim- 
ited. For each source word f ,  the list of its possible 
translations c is sorte(1 according to p(.flc) ? p.,,.,,i(c), 
where Puui(e) is the unigrmn probability of the En- 
glish word c. It is sufficient o consi(ter only the best 
50 words. 
We show translation results for three at)l)roaches: 
tile monotone search (MonS):  where no word re- 
ordering is allowed (Tillmann, 1997), the quasi- 
monotone search (QmS) as 1)resented in this palser 
amt the IBM style ( IbmS) search as described in 
Section 3.2. 
TMsle 4: shows translation results tbr the three ap- 
I)roaches. The eomlsuting time is given in terms of 
CPU time per sentence (on a 450-MIlz l?entimn-III- 
PC). Itere, the printing threshold to = 10.0 is used. 
q_5:anslation errors are reported in terms of multi- 
reference word error rate (roWER) and subjective 
s(mtenee rror rate (SSER). The monotone search 
tserforms worst in terms of both elTror rates 5IsWI~;I~. 
mid SSEIL The (;OSlll)lstislg time is low, sitlce 51o 5e- 
ordering is (:arried o551,. '\]'he quasi-inonotone s arch 
i)e1foI'551s t)est, in ter551s of l)oth error rates roWER 
and SSh;R. Additionally, it; works about 3 times as 
fast as the II3M style sem:eh. For our demonstra- 
tion system, we typically use the pruning threshold 
to = 5.0 to speed Ul) the search by a factor 5 while 
allowing for a sm?fll degradation i  translation accu- 
racy. 
The effect of the pruning threshold to is shown in 
Table 5. The coml)uting time, the number of search 
errors, and the mull;i-reference WEll, (roWER) are 
shown as a flmction of to. The negative logarithm 
of to is reporte(t. The translation scores for the hy- 
i)otheses generated with different threshohl values 
to are compared to the translation scores obtained 
with a conservatively large threshold to = 10.0. For 
each test series, we count tile mlml)er of sentences 
whose score is worse than the corresponding score of 
the tent; series with the conserw~tively large thresh- 
old to = 10.0, and this mm:ber is reported as the 
number of search errors, l)epending on the thresh- 
old to, the search algorithm may miss the globally 
of)timal path which typically results in additional 
translation errors. Decreasing the threshold results 
in higher mWEl l  due to additional search errors. 
Table 5: Effect 
of sere'e\ errors (147 sentences). 
Search to I CPU time #search 
Method \[ \[sec\] error 
QmS 0.0 0.07 108 
1.0 0.13 85 
2.5 0.35 44 
5.0 1.92 4 
10.0 10.6 0 
lbmS 0.0 0.14 108 
1.0 0.3 84 
2.5 0.8 45 
5.0 4.99 7 
10.0 28.52 0 
of the beam threshold on the nmnber 
roWER 
\[%1 
42.6 
37.8 
36.6 
34.6 
34.5 
43.4 
39.5 
39.1 
38.3 
38.2 
Table 6 shows example translations obtained by the 
three, difli;rent appro~mhes. Again, the monotone 
search performs worst. In the second and third 
translation examples, the 1bins word re-ordering 
performs worse than the QmS word re-ordering, 
since it; can not take l)roperly into at:count he word 
re-ordering (115(; to the, German verbgroul). Tile 
German finite verbs 'l)in' (second exmnple) and 
'kSnnten' (third exmnt)le) are too far away from the 
t)ersonal pronouns 'ich' and 'Sic' (6 respectively 5
source sentence positions). In the last example, the 
less restrictive IbmS word re-ordering leads to a bet- 
ter translation, although the QmS translation is still 
aceeptabh'. 
5 Conclusion 
in this pal)er, we have presented a new, efficient 
DP-based search procedure for statistical machine 
translation. The approach assumes that, the word re- 
ordering is restricted to a few positions in the source 
sentence. The approach as been successfiflly tested 
on the 8000-word Verbmobil task. l'hture exten- 
sions of the system might include: 1) An e?tended 
translation model, where we use. more context o pre- 
(lict a source word. 2) An trot)roved language model, 
which takes into at:count syntactic structure, e.g. to 
ensure that a l)roper English verbgrout) in generated. 
3) A tight coupling with the speech recognizer out- 
lint. 
Acknowledgements 
This work has been supported as part of tile Verb- 
mobil project (contract number 01 IV 601 A) by 
the Certain5 Federal Ministry of Education, Science, 
Research and Technology and as part of the Eutrans 
project (ESPRIT project number 30268) by the Eu- 
rot)can Comimmity. 
855 
Table 6: Example Translations for the Verbmobil task. 
Input: Ja ,  wunderbar. KSnnen wir macllen. 
MonS: Yes, wonderflfl. Can we do . 
QmS: Yes, wonderflfl. We can do that . 
IbInS: Yes, wonderflfl. We can do that . 
Input: Das ist zu knapt) , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen imr atn dritten . 
Wie wSre es denn mn ghln Samstag , dem zehnten Februar ? 
MonS: That is too t ight,  becmlse I froln the third in Kaiserslautern. hi fact only on the third . 
How about 5hm Saturday, the tenth of February "? 
QmS: That is too t ight, because Imn froln tim third in Kaiserslautern. In fact only on the third . 
Ahm how about Saturday, February the tenth ? 
IbmS: That is too tight , froln the third because I will be in Kaiserslautern. In fact only on the third. 
Atlln how about Saturday, February the tenth ? 
Input: Wenn Sie dann noch den siel)zehnten kSnnten , wSre das toll , j a .  
MonS: If you then also the seventeenth could, would be the great , yes . 
QmS: If you could then also the seventeenth , tlmt would be great,  yes . 
1binS: Then if you could even take seventeenth , that would be great, yes . 
Illtmt: .Ja, das kommt mir sehr gelegen. Machen wir es dann am besten so. 
MonS: Yes, that suits me perfectly . Do we should best like that . 
QmS: Yes , that suits me fine . We do it like that then best; . 
IbmS: Yes , that suits me fine . We should best do it like tlmt . 
References  
A. L. Berger, P. F. Brown, S. A. Della Pietra, 
V. J. Della Pietra, J. R. Gillett, A. S. Kehler, 
R. L. Mercer. 1996. Language Translation appa- 
ratus and method of using context-based transla- 
tion models. United States Patent, Patent Num- 
ber 5510981, April. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, 
and R. L. Mercer. 1993. The Mathematics of Sta- 
tistical Machine Translation: Parameter Estima- 
tion. Computational Linguistics, vol. 19, no. 2, 
pp. 263-311. 
M. Held, R. M. Karp. 1962. A Dynmnic Progrmn- 
ruing Approach to Sequencing Problems. &SIAM, 
vol. 10, no. 1, pp. 196-210. 
H. Ney, S. Niessen, F. J. Och, H. Sawaf, C. Tilhnmm, 
S. Vogel. 2000. Algorittuns for Statistical %'ansla- 
tion of Spoken Language. IEEE Transactions on 
Speech and Audio Processing, vol. 8, no. 1, pp. 24- 
36. 
F. J. Och, C. Tilhnann, H. Ney. 1999. hnprovcd 
Alignment Models for Statistical Machine ~IYans- 
lation. In Proc. of the ,loint SIGDAT Conference 
on Empirical Methods in Natural Language Pro- 
ccssing and Very Large Corpora (EMNLP99), pp. 
20-28, University of Maryland, College Park, MD, 
USA, June. 
F. J. Och and Ney, H. 2000. A comparison of align- 
ment models for statistical machine translation, hi 
Proceedings of COLING 2000: The 18th Interna- 
tional Conference on Computational Linguistics, 
Saarbr/ieken, Germany, July-August. 
C. Tilltnann. 2000. Complexity of the Dif  
ferent Word Re-ordering Approaches. The 
document can be found under the URL 
http ://www-i6. Informat ik. RWTg-Aachen. de/C 
olleagues/tilli/. Aachen University of' Tech- 
nology, Aachen, Germany, June. 
C. Tilhnann, S. Vogel, H. Ney and A. Zubiaga. 1997. 
A DP based Search Using Monotone Alignments 
in Statistical Translation. In Proc. of the 35th An- 
nual Conf. of the Association for" Computational 
Linguistics, pp. 289 296, Madrid, Spain, July. 
W. Wahlster. 1993. Verbmol)ih Translation of Face- 
to-Face Dialogs. MT Summit IV, pp. 127-135, 
Kobe, Japan. 
856 
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 110?119,
Dublin, Ireland, August 23 2014.
Improved Sentence-Level Arabic Dialect Classification
Christoph Tillmann and Yaser Al-Onaizan
IBM T.J. Watson Research Center
Yorktown Heights, NY, USA
{ctill,onaizan}@us.ibm.com
Saab Mansour
?
Aachen University
Aachen, Germany
mansour@cs.rwth-aachen.de
Abstract
The paper presents work on improved sentence-level dialect classification of Egyptian Arabic
(ARZ) vs. Modern Standard Arabic (MSA). Our approach is based on binary feature functions
that can be implemented with a minimal amount of task-specific knowledge. We train a feature-
rich linear classifier based on a linear support-vector machine (linear SVM) approach. Our best
system achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidan
and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy
improvement over the results published by (Zaidan and Callison-Burch, 2014). We also evaluate
the classifier on dialect data from an additional data source. Here, we find that features which
measure the informalness of a sentence actually decrease classification accuracy significantly.
1 Introduction
The standard form of written Arabic is Modern Standard Arabic (MSA) . It differs significantly from
various spoken varieties of Arabic (Zaidan and Callison-Burch, 2011; Zaidan and Callison-Burch, 2014;
Elfardy and Diab, 2013). Even though these dialects do not originally exist in written form, they are
present in social media texts. Recently a dataset of dialectal Arabic has been made available in the form
of the Arabic Online Commentary (AOC) set (Zaidan and Callison-Burch, 2011; Zaidan and Callison-
Burch, 2014). The data consists of reader commentary from the online versions of Arabic newspapers,
which have a high degree of dialect content. Data for the following dialects has been collected: Levan-
tine, Gulf, and Egyptian. The data had been obtained by a crowd-sourcing effort. In the current paper, we
present results for a binary classification task only, where we predict the dialect of Egyptian Arabic ARZ
vs. MSA sentences from the Al-Youm Al-Sabe? newspaper online commentaries
1
. Our ultimate goal
is to use the dialect classifier for building a dialect-aware Arabic-English statistical machine translation
(SMT) system. Our Arabic-English training data contains a significant amount of Egyptian dialect data
only, and we would like to adapt the components of our hierarchical phrase-based SMT system (Zhao
and Al-Onaizan, 2008) to that data.
Similar to (Elfardy and Diab, 2013), we present a sentence-level classifier that is trained in a supervised
manner. Our approach is based on an Arabic tokenizer, but we do not use a range of specialized tokenizers
or orthography normalizers. In contrast to the language-model (LM) based classifier used by (Zaidan and
Callison-Burch, 2014), we present a linear classifier approach that works best without the use of LM-
based features. Some improvements in terms of classification accuracy and 10-fold cross validation under
the same data conditions as (Zaidan and Callison-Burch, 2011; Elfardy and Diab, 2013) are presented.
In general, we aim at a smaller amount of domain specific feature engineering than previous related
approaches.
The paper is structured as follows. In Section 2, we present related work on language and dialect
identification. In Section 3, we discuss the linear classification model used in this paper. In Section 4, we
evaluate the classifier performance in terms of classification accuracy on two data sets and present some
?
Part of the work was done while the author was a student intern at the IBM T.J. Watson Research Center.
1
We use the ISO 639-3 code ARZ for denoting Egyptian Arabic.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
110
error analysis. Finally, in Section 5, we discuss future work on improved dialect-level classification and
its application to system adaptation for machine translation.
2 Related Work
From a computational perpective, we can view dialect identification as a more fine-grained form of lan-
guage identification (ID). Previous work on language ID examined the use of character histograms (Cav-
nar and Trenkle, 1994; Dunning, 1994), and high accuracy prediction results have been reported even
for languages with a common character set. (Baldwin and Lui, 2010) present a range of document-level
language identification techniques on three different data sets. They use n-gram counting techniques and
different tokenization schemes that are adopted to those data sets. Their classification task deals with
several languages, and it becomes more difficult as the number of languages increases. They present an
SVM-based multiclass classification approach similar to the one presented in this paper which performs
well on one of their data sets. (Trieschnigg et al., 2012) generates n-gram features based on character or
word sequences to classify dialectal documents in a dutch-language fairy-tale collection. Their baseline
model uses N -gram based text classification techniques as popularised in the TextCat tool (Cavnar and
Trenkle, 1994). Following (Baldwin and Lui, 2010), the authors extend the usage of n-gram features with
nearest neighbour and nearest-prototype models together with appropriately chosen similarity metrics.
(Zampieri and Gebre, 2012) classify two varieties of the same language: European and Brazilian Por-
tuguese. They use word and character-based language model classification techniques similar to (Zaidan
and Callison-Burch, 2014). (Huang and Lee, 2008) present simple bag-of-word techniques to classify
varieties of Chinese from the Chinese Gigaword corpus. (Kruengkrai et al., 2005) extend the use of n-
gram features to using string kernels: they may take into account all possible sub-strings for comparison
purposes. The resulting kernel-based classifier is compared against the method in (Cavnar and Trenkle,
1994). (Lui and Cook, 2013) present a dialect classification approach to identify Australian, British, and
Canadian English. They present results where they draw training and test data from different sources.
The successful transfer of models from one text source to another is evidence that their classifier indeed
captures dialectal rather than stylistic or formal differences. Language identification of related languages
is also addressed in the DSL (Discriminating Similar Languages) task of the present Vardial workshop
at COLING 14 (Tan et al., 2014).
While most of the above work focuses on document-level language classification, recent work on
handling Arabic dialect data addresses the problem of sentence-level classification (Zaidan and Callison-
Burch, 2011; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013; Zaidan and Callison-Burch,
2014). The work is based on the data collection effort by (Zaidan and Callison-Burch, 2014) which
crowdsources the annotation task to workers on Amazons Mechanical Turk. The classification results
by (Zaidan and Callison-Burch, 2014) are based on n-gram language-models, where the n-grams are
defined both on words and characters. The authors find that unigram word-based models perform best.
The word-based models are obtained after a minimal amount of preprocessing such as proper handling
of HTML entities and Arabic numbers. Classification accuracy is significantly reduced for shorter sen-
tences. (Elfardy and Diab, 2013) presents classifcation result based on various tokinization and ortho-
graphic normalization techniques as well as so-called meta features that estimate the informalness of the
data. Like our work, the authors focus on a binary dialect classification based on the ARZ-MSA portion
of the dataset in (Zaidan and Callison-Burch, 2011).
3 Classification Model
We use a linear model and compute a score s(t
n
1
) for a tokenized input sentence consisting of n tokens
t
i
:
s(t
n
1
) =
d
?
s=1
w
s
?
n
?
i=1
?
s
(c
i
, t
i
) (1)
where ?
s
(c
i
, t
i
) is a binary feature function which takes into account the context c
i
of token t
i
. w ? R
d
is a high-dimensional weight vector obtained during training. In our experiments, we classify a tokenized
111
Description MSA ARZ
# sentences # words # sentences # words
ARZ-MSA portion of AOC 13, 512 334K 12, 527 327K
DEV12 tune set 585 8.4K 634 9.3K
Table 1: We used the following dialect data: 1) the ARZ-MSA portion of the AOC data from commen-
taries of the Egyptian newspaper Al-Youm Al-Sabe?, and 2) the DEV12 tune set (1219 sentences) which
is the LDC2012E30 corpus BOLT Phase 1 dev-tune set. The DEV12 tune set was annotated by a native
speaker of Arabic.
sentence as being Egyptian dialect (ARZ) if s(t
n
1
) > 0. To train the weights w in Eq. 1, we use a linear
SVM approach (Hsieh et al., 2008; Fan et al., 2008). The trainer can easily handle a huge number of
instances and features. The training data is given as instance-label pairs (x
i
, y
i
) where i ? {1, ? ? ? , l} and
l is the number of training sentences. The x
i
are d-dimensional vectors of integer-valued features that
count how often a binary feature fired for a tokenized sentence t
n
1
. y
i
? {+1,?1} are the class labels
where a label of ?+1? represents Egyptian dialect. During training, we solve the following optimization
problem:
min
w
||w||
1
+ C
l
?
i=1
max(0, 1? y
i
w
T
x
i
) , (2)
i.e. we use L1 regularized L2-loss support vector classification. We set the penalty term C = 0.5. For
our experiments, we use the data set provided in (Zaidan and Callison-Burch, 2011) which also has been
used in the experiments in (Elfardy and Diab, 2013; Zaidan and Callison-Burch, 2014). We focus on the
binary classification between MSA and ARZ. Details on the data sources can be found in Table 1. We
present accuracy results in terms of 10-fold stratified cross-validation which are comparable to previously
published work.
3.1 Tokenization and Dictionaries
The Arabic tokenizer used in the current paper is based on (Lee et al., 2003). It is a general purpose
tokenizer which has been optimized towards improving machine translation quality of SMT systems
rather than dialect classification. Together with the tokenized text, a maximum-entropy based tagger
provides the part-of-speech (PoS) tags for each token. In addition, we have explored a range of features
that are based on the output of the AIDA software package (Elfardy and Diab, 2012; Mona Diab et
al., 2009 2011). The AIDA software has been made available to the participants of the DARPA-funded
Broad Operational Language Translation (BOLT) project. AIDA is a system for dialect identification,
classification and glossing on the token and sentence level for written Arabic. AIDA aggregates several
components including dictionaries and language models in order to perform named entity recognition,
dialect identification classification, and MSA English linearized glossing of the input text. We created
a dictionary from AIDA resources that includes about 41 000 ARZ tokens. In addition, we obtained a
second small dictionary of about 70 ARZ dialect tokens with the help of a native speaker of Arabic. The
list was created by training two IBM Model 1 lexicons, one on Egyptian Arabic data and another on
MSA data. We then inspected the ARZ lexicon entries with the highest cosine distance to their MSA
counterparts and kept the ones that are strong ARZ words. The tokens in both dictionaries are not ARZ
exclusive, but could occur in MSA as well.
3.2 Feature Set
In our work, we employ a simple set of binary feature functions based on the tokenized Arabic sentence.
For example, we define a token bigram feature as follows:
?
Bi
(t
k
, t
k?1
) =
{
1 t
k
= ??


?

?? and t
k?1
= ???g?
0 otherwise
. (3)
112
Token unigram and trigram features are defined accordingly. We also define unigram, bigram, and tri-
gram features based on PoS tags. Currently, just PoS unigrams are used in the experiments. We define
dictionary-based features as follows:
?
Dict
l
(t
k
) =
{
1 t
k
= ?

I

???X? and t
k
? Dict
l
0 otherwise
, (4)
where we use the two dictionaries Dict
1
and Dict
2
as described in Section 3.1. The dictionaries are
handled as token sets and we generate separate features for each of them. We generate some features
based on the AIDA tool output. AIDA provides a dialect label for each input token t
k
as well as a single
dialect label at the sentence level. A sentence-level binary feature based on the AIDA sentence level
classification is defined as follows:
?
AIDA
(t
n
1
) =
{
1 AIDA(t
n
1
) is ARZ
0 otherwise
(5)
where AIDA(t
n
1
) is the sentence-level classification of the AIDA tool. A word-level feature ?
AIDA
(t
k
) is
defined accordingly. These features improve the classification accuracy of our best system significantly.
We have also experimented with some real-valued feature. For example, we derived a feature from
dialect-specific language model probabilities:
?
LM
(t
n
1
) = 1/n ? [ log(p
MSA
(t
n
1
)) ? log(p
ARZ
(t
n
1
))] ,
where log(p
ARZ
(t
n
1
)) is the language-model log probability for the dialect class ARZ . We used a trigram
language model. p
MSA
(?) is defined accordingly. In addition, we have implemented a range of so-called
?meta? features similar to the ones defined in (Elfardy and Diab, 2013). For example, we define a feature
?
Excl
(t
n
1
) which is equal to the length of the longest consecutive sequence of exclamation marks in
the tokenized sentence t
n
1
. Similarly, we define features that count the longest sequence of punctuation
marks, the number of tokens, the averaged character-length of a token in the sentence, and the percentage
of words with word-lengthening effects. These features do not directly model dialectalness of the data
but rather try to capture the degree of in-formalness. Contrary to (Elfardy and Diab, 2013) we find that
those features do not improve accuracy of our best model in the cross-validation experiments. On the
DEV12 set, the use of the meta features results in a significant drop in accuracy.
4 Experiments
In this section, we present experimental results. Firstly, Section 4.1 demonstrates that our data is anno-
tated consistently. In Section 4.2, we present dialect prediction results in terms of accuracy and F-score
on our two data sets. In Section 4.3, we perform some qualitative error analysis for our classifier. In
Section 4.4, we present some preliminary effects on training a SMT system.
4.1 Annotator Agreement
To confirm the consistent annotation of our data, we have measured some inter-annotator and intra-
annotator agreement on it. A native speaker of Arabic was asked to classify the ARZ-MSA portion
of the dialect data using the following three labels: ARZ, MSA, Other. We randomly sampled 250
sentences from the ARZ-MSA portion of the Zaidan data maintaining the original dialect distribution.
The confusion matrix is shown in Table 2. It corresponds to a kappa value of 0.84 (using the definition of
(Fleiss, 1971)), which indicates a very high agreement. In addition, we did re-annotate a sub-set of 200
sentences from the DEV12 set over a time period of three months using our own annotator. The kappa
value of the corresponding confusion matrix is 0.93, indicating very high agreement as well.
4.2 Classification Experiments
Following previous work, we present dialect prediction results in terms of accuracy:
ACC =
# sent correctly tagged
# sent
, (6)
113
Predicted Class (IBM)
ARZ MSA Other
Actual ARZ 125 4 1
Class MSA 14 105 1
(AOC) Other 0 0 0
Table 2: Inter annotator agreement on 250 randomly selected AOC sentences from the data in Table 1.
An in-lab annotator?s dialect prediction is compared against the AOC data gold-standard dialect labels.
where ?# sent? is the number of sentences. In addition, we present dialect prediction results in terms of
precision, recall, and F-score. They are defined as follows:
Prec =
# sent correctly tagged as ARZ
# sent tagged as ARZ
(7)
Recall =
# sent correctly tagged as ARZ
# ref sent tagged as ARZ
F =
2 ? Prec ?Recall
(Prec+Recall)
.
MSA prediction F-score is defined analogously. Experimental results are presented in Table 3, where we
present results for different sets of feature types and the two test sets in Table 1. In the top half of the
table, results are presented in terms of 10-fold cross validation on the ARZ-MSA portion of the AOC
data. In the bottom half, we present results on DEV12 tune set, where we use the entire dialect data in
Table 1 for training (about 26K sentences).
As our baseline we have re-implemented the language-model-perplexity based approach reported in
(Zaidan and Callison-Burch, 2011). We train language models on the dialect-labeled commentary train-
ing data for each of the dialect classes c ? {MSA,ARZ}. During testing, we compute the language
model probability of a sentence s for each of the classes c. We assign a sentence to the class c with the
highest probability (or the lowest perplexity) . For the 10-fold cross validation experiments, 10 language
models are built and perplexities are computed on 10 different test sets. The resulting (averaged) ac-
curacy is 83.3 % for cross-validation and 82.2 % on the DEV12 tune set. In comparison, (Elfardy and
Diab, 2013) reports an accuracy of 80.4 % as perplexity-based baseline. We have carried out additional
experiments with a simple feature set that consists of only unigram token and bigram token features as
defined in Eq. 3. Such a system performs surprisingly well under both testing conditions: we achieved an
accuracy of 87.7 % on the AOC data and an accuracy of 83.4 % on the DEV12 test set. On the AOC set
using 10-fold cross validation, we achieve only a small improvement from using the dictionary features
defined in Eq. 4. The accuracy is improved from 87.7 % to 88.0 %. On the DEV12 set, we obtain a
much larger improvement from using these features. Furthermore, we have investigated the usefulness
of the AIDA-based features. The stand-alone sentence-level classification of the AIDA tool performs
quite poorly. On the DEV12 set, it achieves an accuracy of just 77.9 %. But using the AIDA assigned
sentence-level and token-level dialect labels based on the binary features defined in Eq. 5 improves ac-
curacy significantly, e.g. from 85.3 % to 87.8 % on the DEV12 set. In the current experiments, the
so-called meta features which are computed at the sentence level do not improve classification accuracy.
The meta features are only useful in classifying dialect data based on the in-formalness of the data, i.e.
the ARZ news commentaries tend to exhibit more in-formalness than the MSA commentaries. Finally,
the sentence-level perplexity feature defined in Eq. 6 did not improve accuracy as well (no results for this
feature are presented in Table 3).
4.3 Classifier Analysis
In this section, we perform a simple error analysis of the classifier performance on some dialect data for
which the degree of dialectalness is known. The data comes from news sources that differ from the data
used to train the classifier. The classifier is evaluated on data from the DARPA-funded BOLT project.
114
Feature Types MSA ARZ
ACC [%] PREC REC F PREC REC F
10-fold language-model 83.3 86.7 90.2 88.4 89.0 85.0 86.9
AOC aida-sentence label 81.0 84.2 78.0 81.0 78.0 84.3 81.0
uni,bi 87.7 86.6 90.2 88.4 89.0 85.0 86.9
uni,bi,dict,pos 88.0 86.9 90.4 88.6 89.2 85.3 87.2
uni,bi,dict,pos,aida 89.1 87.5 92.2 89.8 91.1 85.7 88.3
uni,bi,dict,pos,aida,meta 88.8 87.4 91.7 89.5 90.6 85.7 88.1
DEV12 language-model 82.2 85.1 76.2 80.4 80.0 87.7 83.7
aida-sentence label 77.9 80.9 70.8 75.5 75.8 84.5 79.9
uni,bi 83.4 81.1 85.1 83.1 85.6 81.7 83.6
uni,bi,dict,pos 85.3 83.5 87.5 85.5 88.0 84.1 86.0
uni,bi,dict,pos,aida 87.8 83.4 93.0 88.0 92.8 83.0 87.6
uni,bi,dict,pos,aida,meta 68.3 61.8 90.8 73.5 85.0 48.3 61.6
Table 3: Arabic Dialect Classification Results: predicting MSA vs. (ARZ) dialect in terms of 10-fold
cross-validation on the AOC data and on the DEV12 set using all the AOC data for training.
Corpus #Sent #Sent [ARZ] %[ARZ]
ARZ web forum 299K 183K 61%
Broadcast 169K 18K 11%
Newswire 885K 29K 3%
Table 4: Sub-corpora together with total number as well as percentage of sentences that are classified as
ARZ.
The BOLT data consists of several corpora collected from various resources. These resources include
newswire, web-logs, ARZ web forum data and others. Classification statistics are presented in Table 4,
where we report the number of sentences along with the percentage of those sentences classified as ARZ.
The distribution of the dialect labels in the classifier output appears to correspond to the expected origin
of the data. For example, the ARZ web forum data contains a majority of ARZ sentences, but quite a
few sentences are MSA such as greetings and quotations from Islamic resources (Quran, Hadith ...). The
broadcast conversation data is mainly MSA, but sometimes the speaker switches to dialectal usage for
a short phrase and then switches back to MSA. Lastly, the newswire data has a vast majority of MSA
sentences. Examining a small portion of newswire sentences classified as ARZ, the sentences labeled as
ARZ are mostly classification errors.
Example sentence classifications from the BOLT data are shown in Table 5. The first two text frag-
ments are taken from the Egyptian Arabic (ARZ) web forum data. In the first document fragment, the
user starts with MSA sentences, then switches to Egyptian (ARZ) dialect marked by the ARZ indicator
?


?
?@ and using the prefix # H
.
before a verb which is not allowed in MSA. The user then switches back
to MSA. The classifier is able to classify the Egyptian Arabic (ARZ) sentence correctly. In the second
document fragment, the user uses several Egyptian Arabic (ARZ) words. In the forth sentence no ARZ
words exist, and the classifier correctly classifies the sentence as MSA. The third text fragment shows
115
Predicted Arabic English
Dialect
MSA . X?XQ?@ ? ??
	
???
?
@

H@Q

? A
	
K @ i read the topic and the replies .
MSA .

???g

?Q?
	
? ??
	
???
?
@ the topic is great !
ARZ ??

?K


# H
.
?


?
? @ pB@ ?? A
	
K @ # ? i agree with the brother who said
MSA

?k
.
Ag ?? ?


	
? ???
	
?K


Y?@ Islam is significant in all
ARZ ZCJ
.
? @
?


?
?

H
Q

.
? ?


X ?A
	
J? @
	
?A

??? because they accept affliction with patience
ARZ PA?

J
	
K @
Q

.
? @

?X ?A?
g
?+

I??
?
?


?
?@ ? what Hamas did was a victory
ARZ ?C

Jk@

?? ?


	
? @?
	
?

?? ?A?
g
?


	
P who encountered the occupation
MSA PA?k
?


?
? @?
Q

.
? ? and they were patient despite the siege
ARZ ??+

?
	
?A? A
	
K+ H
.
P ?Y?
	
?A

??? that ?s why Allah rewarded them
ARZ* ?? ?


X ?



G

HXA

? Y

? # ? tdk ... led
ARZ*

??

?
C?@# H
.
?

?
	
J? @ Z @
Q

.
	
g ?j
	
JK


# ? transport experts blame
ARZ* . ?


+ # ? ?+ ?A

? A? Q?
	
Y

K ?J


?

?@ B i cannot remember what he told me
Table 5: Automatic classification examples for the dialect classes ARZ and MSA. Arabic source and
English target sentences are given. Dialectal words are in bold. Incorrect predictions are marked by an
asterisk (*).
some sentences from the newswire corpus that are mis-classified. The first sentence contains the word
?


X which corresponds to the letter ?d? in the abbreviation ?tdk?. The word is contained in one of our ARZ
dictionaries such that the binary AIDA-based feature in Eq. 5 fires and triggers a mis-classification. In
this context, the word is part of an abbreviation which is split in the Arabic text. In the other examples,
only a few of the binary features defined in Section 3.2 apply and features that correspond to Arabic
prefixes tend to support a classification as ARZ dialect.
4.4 Preliminary Application for SMT
The dialect classification of Arabic data for SMT can be used in various ways. Examples include domain-
specific tuning, mixture modeling, and the use of so-called provenance features (Chiang et al., 2011)
among others . As a motivation for the future use of the dialect classifier in SMT, we classify the BOLT
bilingual training data into ARZ and MSA parts and examine the effect on the phrase table scores. Phrase
translation pairs demonstrating the use of the classified training data are shown in Table 6. The ARZ web
forum data is split into an ARZ part and an MSA part and two separate phrase probability tables are
trained on these two splits. The ARZ web forum data is highly ambiguous with respect to dialect and it
is difficult to obtain good dialect-dependent splits of the data. In the first example in the table, the word

?J


K
.
Q??@ could mean ?Arab? in MSA, but in ARZ it could also mean ?car?. The phrase table scores obtained
from the classifier-split training data correctly reflect this ambiguity. The phrase pair with ?car? has the
lowest translation score for the BOLT.ARZ phrase table, while it has a higher cost in the BOLT.MSA
phrase table. In the full phrase table (BOLT), ?car? is the fifth translation candidate with a score of 2.09.
116
BOLT.ARZ BOLT.MSA
f e cost e cost

?J


K
.
Q??@
the car 1.20 arab 0.80
arab 1.25 the arab 1.32
the arab 1.70 Arabic 1.52
?


??Q?
merci 1.53 marsa 1.99
marsa 1.63 thanks 2.01
mursi 1.91 morcy 2.13
Table 6: Phrase tables based on classified training data. BOLT.ARZ is trained on the ARZ portion of
the ARZ web forums data, while BOLT.MSA is trained on the MSA part. The table includes Arabic
words and the top three phrase translation candidates, sorted (first is best) by the phrase model cost
(cost= ?log(p(f |e)) ).
In the second example, the word ?


??Q ? could function as a proper noun with its English translation
?mursi? or ?marsa?, but only in ARZ it could also be translated as ?thanks? (?merci?). In this case, the
classifier is unable to distinguish between the ARZ dialect and the MSA usage. We found out that the
word token ?merci? appears only 4 times in the training data, rendering its binary features unreliable
reliable. In general we note that the phrase tables build on the classified data become more domain-
specific, and it is left to future work to check whether improvements could carry over to the translation
quality.
5 Discussion and Future Work
The ultimate goal is to use the ARZ vs. MSA dialect classifier for training an adapted SMT system.
We split the training data at the sentence level using our classifier and train dialect-specific systems
on each of these splits along with a general dialect-independent system. We will be using techniques
similar to (Koehn and Schroeder, 2007; Chiang et al., 2011; Sennrich, 2012; Chen et al., 2013) to adapt
the general SMT system to a target domain with a predominant dialect. Or, we will be adopting an
SMT system to a development or test set where we use the classifier to predict the dialect for each
sentence and use a dialect-specific SMT system on each of them individually. Our approach of using
just binary feature functions in connection with a sentence-level global linear model can be related to
work on PoS-tagging (Collins, 2002). (Collins, 2002) trains a linear model based on Viterbi decoding
and the perceptron algorithm. The gold-standard PoS tags are given at the word-level, but the training
uses a global representation at the sentence level. Similarly, we use linear SVMs (Hsieh et al., 2008)
to train a classification model at the sentence level without access to sentence length statistics, i.e. our
best performing classifier does not compute features like the percentage of punctuation, numbers, or
averaged word length as has been proposed previously (Elfardy and Diab, 2013). All of our features are
actually computed at the token level (with the exception of a single sentence-level AIDA-based feature).
An interesting direction for future work could be to train the dialect classifier at the sentence level, but
use it to compute token-level predictions for a more fine-grained analysis. Even though the token-level
prediction task corresponds to a word-level tag set of just size 2, Viterbi decoding techniques could be
used to introduce novel context-dependent features, e.g. dialect tag n-gram features. Such a token-level
predictions might be used for weighting each phrase pair in an SMT system using methods like the
instance-based adaptation approach in (Foster et al., 2010).
Acknowledgement
The current work has been funded through the Broad Operational Language Translation (BOLT) program
under the project number DARPA HR0011-12-C-0015.
117
References
Timothy Baldwin and Marco Lui. 2010. Language Identification: The Long and the Short of the Matter. In Proc.
of HLT?10, pages 229?237, Los Angeles, California, June.
William Cavnar and John M. Trenkle. 1994. N-gram-based Text Categorization. In In Proceedings of SDAIR-94,
3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161?175.
Boxing Chen, George Foster, and Roland Kuhn. 2013. Adaptation of reordering models for statistical machine
translation. In Proc. of HLT?13, pages 938?946, Atlanta, Georgia, June.
David Chiang, Steve DeNeefe, and Michael Pust. 2011. Two Easy Improvements to Lexical Weighting. In Proc.
of HLT?11, pages 455?460, Portland, Oregon, USA, June.
Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of EMNLP?02, pages 1?8, Philadelphia,PA, July.
Ted Dunning. 1994. Statistical Identification of Language. technical report mccs 94-273. Technical report, New
Mexico State University.
Heba Elfardy and Mona Diab. 2012. Aida: Automatic Identification and Glossing of Dialectal Arabic. In Pro-
ceedings of the 16th EAMT Conference (Project Papers), pages 83?83, Trento, Italy, May.
Heba Elfardy and Mona Diab. 2013. Sentence level dialect Identification in arabic. In Proc. of the ACL 2013
(Volume 2: Short Papers), pages 456?461, Sofia, Bulgaria, August.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: a Library
for Large Linear Classification. Machine Learning Journal, 9:1871?1874.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin,
76(5):378.
George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation
in Statistical Machine Translation. In Proc. of EMNLP?10, pages 451?459.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S.Sundararajan. 2008. A Dual Coordinate Descent Method
for Large-scale linear SVM. In ICML, pages 919?926, Helsinki,Finland.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive Approach towards Text Source Classication based on
top-bag-of-word Similarity. In PACLIC 2008, pages 404?410, Cebu City, Philippines.
Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation.
In Proceedings of the Second Workshop on Statistical Machine Translation (WMT07), pages 224?227.
Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isahara. 2005. Language
Identification based on string kernels. In In Proceedings of the 5th International Symposium on Communications
and Information Technologies (ISCIT-2005, pages 896?899.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, and Hany Hassan. 2003. Language Model
Based Arabic Word Segmentation. In Proc. of the 41st Annual Conf. of the Association for Computational
Linguistics (ACL 03), pages 399?406, Sapporo, Japan, July.
Marco Lui and Paul Cook. 2013. Classifying English Documents by National Dialect. In Proc. Australasian
Language Technology Workshop, pages 5?15.
Mona Mona Diab, Heba Elfardy, and Yassine Benajiba. 2009?2011. AIDA Automatic Identification of Arabic Di-
alectal Text. a Tool for Dialect Identification & Classification, Named Entity Recognition, English and Modern
Standard Arabic Glossing and Normalization.
Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proc. of EACL?12, pages 539?549.
Liling Tan, Marcos Zampieri, Nicola Ljube?si?c, and J?org Tiedemann. 2014. Merging Comparable Data Sources
for the Discrimination of Similar Languages: The DSL Corpus Collection. In 7th Workshop on Building and
Using Comparable Corpora at LREC?14, Reykjavik, Iceland, September.
D. Trieschnigg, D. Hiemstra, M. Theune F. Jong, and T. Meder. 2012. An Exploration of Language Identication
Techniques for the Dutch Folktale Database. In Adaptation of Language Resources and Tools for Processing
Cultural Heritage Workshop (LREC 2012), Istanbul, Turkey, May.
118
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from non-
professionals. In Proc. of ACL / HLT 11, pages 1220?1229, Portland, Oregon, USA, June.
Omar F. Zaidan and Chris Callison-Burch. 2014. Arabic Dialect Classification. CL, 40(1):171?202.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic Identication of Language Varieties: The case
of Portuguese. In Konvens 12, pages 233?237, Vienna, Austria.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local word-reordering patterns for syntax-
based machine translation. In Proc. of EMNLP?08, pages 572?581, Honolulu, Hawaii, October.
119
