Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 666?675,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Online Learning in Tensor Space
Yuan Cao Sanjeev Khudanpur
Center for Language & Speech Processing and Human Language Technology Center of Excellence
The Johns Hopkins University
Baltimore, MD, USA, 21218
{yuan.cao, khudanpur}@jhu.edu
Abstract
We propose an online learning algorithm
based on tensor-space models. A tensor-
space model represents data in a compact
way, and via rank-1 approximation the
weight tensor can be made highly struc-
tured, resulting in a significantly smaller
number of free parameters to be estimated
than in comparable vector-space models.
This regularizes the model complexity and
makes the tensor model highly effective in
situations where a large feature set is de-
fined but very limited resources are avail-
able for training. We apply with the pro-
posed algorithm to a parsing task, and
show that even with very little training
data the learning algorithm based on a ten-
sor model performs well, and gives signif-
icantly better results than standard learn-
ing algorithms based on traditional vector-
space models.
1 Introduction
Many NLP applications use models that try to in-
corporate a large number of linguistic features so
that as much human knowledge of language can
be brought to bear on the (prediction) task as pos-
sible. This also makes training the model param-
eters a challenging problem, since the amount of
labeled training data is usually small compared to
the size of feature sets: the feature weights cannot
be estimated reliably.
Most traditional models are linear models, in
the sense that both the features of the data and
model parameters are represented as vectors in a
vector space. Many learning algorithms applied
to NLP problems, such as the Perceptron (Collins,
2002), MIRA (Crammer et al, 2006; McDonald
et al, 2005; Chiang et al, 2008), PRO (Hop-
kins and May, 2011), RAMPION (Gimpel and
Smith, 2012) etc., are based on vector-space mod-
els. Such models require learning individual fea-
ture weights directly, so that the number of param-
eters to be estimated is identical to the size of the
feature set. When millions of features are used but
the amount of labeled data is limited, it can be dif-
ficult to precisely estimate each feature weight.
In this paper, we shift the model from vector-
space to tensor-space. Data can be represented
in a compact and structured way using tensors as
containers. Tensor representations have been ap-
plied to computer vision problems (Hazan et al,
2005; Shashua and Hazan, 2005) and information
retrieval (Cai et al, 2006a) a long time ago. More
recently, it has also been applied to parsing (Cohen
and Collins, 2012; Cohen and Satta, 2013) and se-
mantic analysis (Van de Cruys et al, 2013). A
linear tensor model represents both features and
weights in tensor-space, hence the weight tensor
can be factorized and approximated by a linear
sum of rank-1 tensors. This low-rank approxi-
mation imposes structural constraints on the fea-
ture weights and can be regarded as a form of
regularization. With this representation, we no
longer need to estimate individual feature weights
directly but only a small number of ?bases? in-
stead. This property makes the the tensor model
very effective when training a large number of fea-
ture weights in a low-resource environment. On
the other hand, tensor models have many more de-
grees of ?design freedom? than vector space mod-
els. While this makes them very flexible, it also
creates much difficulty in designing an optimal
tensor structure for a given training set.
We give detailed description of the tensor space
666
model in Section 2. Several issues that come
with the tensor model construction are addressed
in Section 3. A tensor weight learning algorithm
is then proposed in 4. Finally we give our exper-
imental results on a parsing task and analysis in
Section 5.
2 Tensor Space Representation
Most of the learning algorithms for NLP problems
are based on vector space models, which represent
data as vectors ? ? R
n
, and try to learn feature
weight vectors w ? R
n
such that a linear model
y = w ? ? is able to discriminate between, say,
good and bad hypotheses. While this is a natural
way of representing data, it is not the only choice.
Below, we reformulate the model from vector to
tensor space.
2.1 Tensor Space Model
A tensor is a multidimensional array, and is a gen-
eralization of commonly used algebraic objects
such as vectors and matrices. Specifically, a vec-
tor is a 1
st
order tensor, a matrix is a 2
nd
order
tensor, and data organized as a rectangular cuboid
is a 3
rd
order tensor etc. In general, a D
th
order
tensor is represented as T ? R
n
1
?n
2
?...n
D
, and an
entry in T is denoted by T
i
1
,i
2
,...,i
D
. Different di-
mensions of a tensor 1, 2, . . . , D are named modes
of the tensor.
Using a D
th
order tensor as container, we can
assign each feature of the task a D-dimensional
index in the tensor and represent the data as ten-
sors. Of course, shifting from a vector to a tensor
representation entails several additional degrees of
freedom, e.g., the order D of the tensor and the
sizes {n
d
}
D
d=1
of the modes, which must be ad-
dressed when selecting a tensor model. This will
be done in Section 3.
2.2 Tensor Decomposition
Just as a matrix can be decomposed as a lin-
ear combination of several rank-1 matrices via
SVD, tensors also admit decompositions
1
into lin-
ear combinations of ?rank-1? tensors. A D
th
or-
der tensor A ? R
n
1
?n
2
?...n
D
is rank-1 if it can be
1
The form of tensor decomposition defined here is named
as CANDECOMP/PARAFAC(CP) decomposition (Kolda
and Bader, 2009). Another popular form of tensor decom-
position is called Tucker decomposition, which decomposes
a tensor into a core tensor multiplied by a matrix along each
mode. We focus only on the CP decomposition in this paper.
written as the outer product of D vectors, i.e.
A = a
1
? a
2
?, . . . ,?a
D
,
where a
i
? R
n
d
, 1 ? d ? D. A D
th
order tensor
T ? R
n
1
?n
2
?...n
D
can be factorized into a sum of
component rank-1 tensors as
T =
R
?
r=1
A
r
=
R
?
r=1
a
1
r
? a
2
r
?, . . . ,?a
D
r
where R, called the rank of the tensor, is the mini-
mum number of rank-1 tensors whose sum equals
T . Via decomposition, one may approximate a
tensor by the sum of H major rank-1 tensors with
H ? R.
2.3 Linear Tensor Model
In tensor space, a linear model may be written (ig-
noring a bias term) as
f(W ) = W ??,
where ? ? R
n
1
?n
2
?...n
D
is the feature tensor, W
is the corresponding weight tensor, and ? denotes
the Hadamard product. If W is further decom-
posed as the sum of H major component rank-1
tensors, i.e. W ?
?
H
h=1
w
1
h
?w
2
h
?, . . . ,?w
D
h
,
then
f(w
1
1
, . . . ,w
D
1
, . . . ,w
1
h
, . . . ,w
D
h
)
=
H
?
h=1
??
1
w
1
h
?
2
w
2
h
. . .?
D
w
D
h
, (1)
where ?
l
is the l-mode product operator between
a D
th
order tensor T and a vector a of dimension
n
d
, yielding a (D ? 1)
th
order tensor such that
(T ?
l
a)
i
1
,...,i
l?1
,i
l+1
,...,i
D
=
n
d
?
i
l
=1
T
i
1
,...,i
l?1
,i
l
,i
l+1
,...,i
D
? a
i
l
.
The linear tensor model is illustrated in Figure 1.
2.4 Why Learning in Tensor Space?
So what is the advantage of learning with a ten-
sor model instead of a vector model? Consider the
case where we have defined 1,000,000 features for
our task. A vector space linear model requires es-
timating 1,000,000 free parameters. However if
we use a 2
nd
order tensor model, organize the fea-
tures into a 1000 ? 1000 matrix ?, and use just
667
Figure 1: A 3
rd
order linear tensor model. The
feature weight tensor W can be decomposed as
the sum of a sequence of rank-1 component ten-
sors.
one rank-1 matrix to approximate the weight ten-
sor, then the linear model becomes
f(w
1
,w
2
) = w
T
1
?w
2
,
where w
1
,w
2
? R
1000
. That is to say, now we
only need to estimate 2000 parameters!
In general, if V features are defined for a learn-
ing problem, and we (i) organize the feature set
as a tensor ? ? R
n
1
?n
2
?...n
D
and (ii) use H
component rank-1 tensors to approximate the cor-
responding target weight tensor. Then the total
number of parameters to be learned for this ten-
sor model is H
?
D
d=1
n
d
, which is usually much
smaller than V =
?
D
d=1
n
d
for a traditional vec-
tor space model. Therefore we expect the tensor
model to be more effective in a low-resource train-
ing environment.
Specifically, a vector space model assumes each
feature weight to be a ?free? parameter, and es-
timating them reliably could therefore be hard
when training data are not sufficient or the fea-
ture set is huge. By contrast, a linear tensor model
only needs to learn H
?
D
d=1
n
d
?bases? of the m
feature weights instead of individual weights di-
rectly. The weight corresponding to the feature
?
i
1
,i
2
,...,i
D
in the tensor model is expressed as
w
i
1
,i
2
,...,i
D
=
H
?
h=1
w
1
h,i
1
w
2
h,i
2
. . . w
D
h,i
D
, (2)
where w
j
h,i
j
is the i
th
j
element in the vector w
j
h
.
In other words, a true feature weight is now ap-
proximated by a set of bases. This reminds us
of the well-known low-rank matrix approximation
of images via SVD, and we are applying similar
techniques to approximate target feature weights,
which is made possible only after we shift from
vector to tensor space models.
This approximation can be treated as a form of
model regularization, since the weight tensor is
represented in a constrained form and made highly
structured via the rank-1 tensor approximation. Of
course, as we reduce the model complexity, e.g. by
choosing a smaller and smaller H , the model?s ex-
pressive ability is weakened at the same time. We
will elaborate on this point in Section 3.1.
3 Tensor Model Construction
To apply a tensor model, we first need to con-
vert the feature vector into a tensor ?. Once the
structure of ? is determined, the structure of W
is fixed as well. As mentioned in Section 2.1, a
tensor model has many more degrees of ?design
freedom? than a vector model, which makes the
problem of finding a good tensor structure a non-
trivial one.
3.1 Tensor Order
The order of a tensor affects the model in two
ways: the expressiveness of the model and the
number of parameters to be estimated. We assume
H = 1 in the analysis below, noting that one can
always add as many rank-1 component tensors as
needed to approximate a tensor with arbitrary pre-
cision.
Obviously, the 1
st
order tensor (vector) model
is the most expressive, since it is structureless and
any arbitrary set of numbers can always be repre-
sented exactly as a vector. The 2
nd
order rank-1
tensor (rank-1 matrix) is less expressive because
not every set of numbers can be organized into
a rank-1 matrix. In general, a D
th
order rank-1
tensor is more expressive than a (D + 1)
th
order
rank-1 tensor, as a lower-order tensor imposes less
structural constraints on the set of numbers it can
express. We formally state this fact as follows:
Theorem 1. A set of real numbers that can be rep-
resented by a (D + 1)
th
order tensor Q can also
be represented by a D
th
order tensor P , provided
P andQ have the same volume. But the reverse is
not true.
Proof. See appendix.
On the other hand, tensor order also affects the
number of parameters to be trained. Assuming
that a D
th
order has equal size on each mode (we
will elaborate on this point in Section 3.2) and
the volume (number of entries) of the tensor is
fixed as V , then the total number of parameters
668
of the model is DV
1
D
. This is a convex func-
tion of D, and the minimum
2
is reached at either
D
?
= blnV c or D
?
= dlnV e.
Therefore, as D increases from 1 to D
?
, we
lose more and more of the expressive power of the
model but reduce the number of parameters to be
trained. However it would be a bad idea to choose
aD beyondD
?
. The optimal tensor order depends
on the nature of the actual problem, and we tune
this hyper-parameter on a held-out set.
3.2 Mode Size
The size n
d
of each tensor mode, d = 1, . . . , D,
determines the structure of feature weights a ten-
sor model can precisely represent, as well as the
number of parameters to estimate (we also as-
sume H = 1 in the analysis below). For exam-
ple, if the tensor order is 2 and the volume V is
12, then we can either choose n
1
= 3, n
2
= 4
or n
1
= 2, n
2
= 6. For n
1
= 3, n
2
= 4, the
numbers that can be precisely represented are di-
vided into 3 groups, each having 4 numbers, that
are scaled versions of one another. Similarly for
n
1
= 2, n
2
= 6, the numbers can be divided into
2 groups with different scales. Obviously, the two
possible choices of (n
1
, n
2
) also lead to different
numbers of free parameters (7 vs. 8).
GivenD and V , there are many possible combi-
nations of n
d
, d = 1, . . . , D, and the optimal com-
bination should indeed be determined by the struc-
ture of target features weights. However it is hard
to know the structure of target feature weights be-
fore learning, and it would be impractical to try ev-
ery possible combination of mode sizes, therefore
we choose the criterion of determining the mode
sizes as minimization of the total number of pa-
rameters, namely we solve the problem:
min
n
1
,...,n
D
D
?
d=1
n
d
s.t
D
?
d=1
n
d
= V
The optimal solution is reached when n
1
= n
2
=
. . . = n
D
= V
1
D
. Of course it is not guaran-
teed that V
1
D
is an integer, therefore we choose
n
d
= bV
1
D
c or dV
1
D
e, d = 1, . . . , D such that
?
D
d=1
n
d
? V and
[
?
D
d=1
n
d
]
? V is minimized.
The
[
?
D
d=1
n
d
]
? V extra entries of the tensor
correspond to no features and are used just for
2
The optimal integer solution can be determined simply
by comparing the two function values.
padding. Since for each n
d
there are only two
possible values to choose, we can simply enumer-
ate all the possible 2
D
(which is usually a small
number) combinations of values and pick the one
that matches the conditions given above. This way
n
1
, . . . , n
D
are fully determined.
Here we are only following the principle of min-
imizing the parameter number. While this strat-
egy might work well with small amount of train-
ing data, it is not guaranteed to be the best strategy
in all cases, especially when more data is avail-
able we might want to increase the number of pa-
rameters, making the model more complex so that
the data can be more precisely modeled. Ideally
the mode size needs to be adaptive to the amount
of training data as well as the property of target
weights. A theoretically guaranteed optimal ap-
proach to determining the mode sizes remains an
open problem, and will be explored in our future
work.
3.3 Number of Rank-1 Tensors
The impact of using H > 1 rank-1 tensors is ob-
vious: a larger H increases the model complexity
and makes the model more expressive, since we
are able to approximate target weight tensor with
smaller error. As a trade-off, the number of param-
eters and training complexity will be increased. To
find out the optimal value of H for a given prob-
lem, we tune this hyper-parameter too on a held-
out set.
3.4 Vector to Tensor Mapping
Finally, we need to find a way to map the orig-
inal feature vector to a tensor, i.e. to associate
each feature with an index in the tensor. Assum-
ing the tensor volume V is the same as the number
of features, then there are in all V ! ways of map-
ping, which is an intractable number of possibili-
ties even for modest sized feature sets, making it
impractical to carry out a brute force search. How-
ever while we are doing the mapping, we hope to
arrange the features in a way such that the corre-
sponding target weight tensor has approximately a
low-rank structure, this way it can be well approx-
imated by very few component rank-1 tensors.
Unfortunately we have no knowledge about the
target weights in advance, since that is what we
need to learn after all. As a way out, we first run
a simple vector-model based learning algorithm
(say the Perceptron) on the training data and es-
timate a weight vector, which serves as a ?surro-
669
gate? weight vector. We then use this surrogate
vector to guide the design of the mapping. Ide-
ally we hope to find a permutation of the surro-
gate weights to map to a tensor in such a way that
the tensor has a rank as low as possible. How-
ever matrix rank minimization is in general a hard
problem (Fazel, 2002). Therefore, we follow an
approximate algorithm given in Figure 2a, whose
main idea is illustrated via an example in Figure
2b.
Basically, what the algorithm does is to di-
vide the surrogate weights into hierarchical groups
such that groups on the same level are approx-
imately proportional to each other. Using these
groups as units we are able to ?fill? the tensor in a
hierarchical way. The resulting tensor will have an
approximate low-rank structure, provided that the
sorted feature weights have roughly group-wise
proportional relations.
For comparison, we also experimented a trivial
solution which maps each entry of the feature ten-
sor to the tensor just in sequential order, namely
?
0
is mapped to ?
0,0,...,0
, ?
1
is mapped to ?
0,0,...,1
etc. This of course ignores correlation between
features since the original feature order in the vec-
tor could be totally meaningless, and this strategy
is not expected to be a good solution for vector to
tensor mapping.
4 Online Learning Algorithm
We now turn to the problem of learning the feature
weight tensor. Here we propose an online learning
algorithm similar to MIRA but modified to accom-
modate tensor models.
Let the model be f(T ) = T ? ?(x, y), where
T =
?
H
h=1
w
1
h
? w
2
h
?, . . . ,?w
D
h
is the weight
tensor, ?(x, y) is the feature tensor for an input-
output pair (x, y). Training samples (x
i
, y
i
), i =
1, . . . ,m, where x
i
is the input and y
i
is the ref-
erence or oracle hypothesis, are fed to the weight
learning algorithm in sequential order. A predic-
tion z
t
is made by the model T
t
at time t from a
set of candidatesZ(x
t
), and the model updates the
weight tensor by solving the following problem:
min
T?Rn1?n2?...nD
1
2
?T ? T
t
?
2
+ C? (3)
s.t.
L
t
? ?, ? ? 0
where T is a decomposed weight tensor and
L
t
= T ??(x
t
, z
t
)? T ??(x
t
, y
t
) + ?(y
t
, z
t
)
Input:
Tensor order D, tensor volume V , mode size
n
d
, d = 1, . . . , D, surrogate weight vector v
Let
v
+
= [v
+
1
, . . . , v
+
p
] be the non-negative part of
v
v
?
= [v
?
1
, . . . , v
?
q
] be the negative part of v
Algorithm:
?
v
+
= sort(v
+
) in descending order
?
v
?
= sort(v
?
) in ascending order
u = V/n
D
e = p?mod(p, u), f = q ?mod(q, u)
Construct vector
X = [v?
+
1
, . . . , v?
+
e
, v?
?
1
, . . . , v?
?
f
,
v?
+
e+1
, . . . , v?
+
p
, v?
?
f+1
, . . . , v?
?
q
]
Map X
a
, a = 1, . . . , p + q to the tensor entry
T
i
1
,...,i
D
, such that
a =
D
?
d=1
(i
d
? 1)l
d?1
+ 1
where l
d
= l
d?1
n
d
, and l
0
= 1
(a) Mapping a surrogate weight vector to a tensor
(b) Illustration of the algorithm
Figure 2: Algorithm for mapping a surrogate
weight vector X to a tensor. (2a) provides the al-
gorithm; (2b) illustrates it by mapping a vector of
length V = 12 to a (n
1
, n
2
, n
3
) = (2, 2, 3) ten-
sor. The bars X
i
represent the surrogate weights
? after separately sorting the positive and nega-
tive parts ? and the labels along a path of the tree
correspond to the tensor-index of the weight rep-
resented by the leaf resulting from the mapping.
670
is the structured hinge loss.
This problem setting follows the same ?passive-
aggressive? strategy as in the original MIRA. To
optimize the vectors w
d
h
, h = 1, . . . ,H, d =
1, . . . , D, we use a similar iterative strategy as pro-
posed in (Cai et al, 2006b). Basically, the idea is
that instead of optimizing w
d
h
all together, we op-
timize w
1
1
,w
2
1
, . . . ,w
D
H
in turn. While we are up-
dating one vector, the rest are fixed. For the prob-
lem setting given above, each of the sub-problems
that need to be solved is convex, and according
to (Cai et al, 2006b) the objective function value
will decrease after each individual weight update
and eventually this procedure will converge.
We now give this procedure in more detail.
Denote the weight vector of the d
th
mode of
the h
th
tensor at time t as w
d
h,t
. We will up-
date the vectors in turn in the following order:
w
1
1,t
, . . . ,w
D
1,t
,w
1
2,t
, . . . ,w
D
2,t
, . . . ,w
1
H,t
, . . . ,w
D
H,t
.
Once a vector has been updated, it is fixed for
future updates.
By way of notation, define
W
d
h,t
= w
1
h,t+1
?, . . . ,?w
d?1
h,t+1
?w
d
h,t
?, . . . ,?w
D
h,t
(and letW
D+1
h,t
, w
1
h,t+1
?, . . . ,?w
D
h,t+1
),
?
W
d
h,t
= w
1
h,t+1
?, . . . ,?w
d?1
h,t+1
?w
d
?, . . . ,?w
D
h,t
(where w
d
? R
n
d
),
T
d
h,t
=
h?1
?
h
?
=1
W
D+1
h
?
,t
+W
d
h,t
+
H
?
h
?
=h+1
W
1
h
?
,t
(4)
?
T
d
h,t
=
h?1
?
h
?
=1
W
D+1
h
?
,t
+
?
W
d
h,t
+
H
?
h
?
=h+1
W
1
h
?
,t
?
d
h,t
(x, y)
= ?(x, y)?
2
w
2
h,t+1
. . .?
d?1
w
d?1
h,t+1
?
d+1
w
d+1
h,t
. . .?
D
w
D
h,t
(5)
In order to update from w
d
h,t
to get w
d
h,t+1
, the
sub-problem to solve is:
min
wd?Rnd
1
2
?
?
T
d
h,t
? T
d
h,t
?
2
+ C?
= min
wd?Rnd
1
2
?
?
W
d
h,t
?W
d
h,t
?
2
+ C?
= min
wd?Rnd
1
2
?
1
h,t+1
. . . ?
d?1
h,t+1
?
d+1
h,t
. . . ?
D
h,t
?w
d
?w
d
h,t
?
2
+ C?
s.t. L
d
h,t
? ?, ? ? 0.
where
?
d
h,t
= ?w
d
h,t
?
2
L
d
h,t
=
?
T
d
h,t
??(x
t
, z
t
)?
?
T
d
h,t
??(x
t
, y
t
)
+?(y
t
, z
t
)
= w
d
?
(
?
d
h,t
(x
t
, z
t
)? ?
d
h,t
(x
t
, y
t
)
)
?
(
h?1
?
h
?
=1
W
D+1
h
?
,t
+
H
?
h
?
=h+1
W
1
h
?
,t
)
?
(?(x
t
, y
t
)??(x
t
, z
t
))
+?(y
t
, z
t
)
Letting
??
d
h,t
, ?
d
h,t
(x
t
, y
t
)? ?
d
h,t
(x
t
, z
t
)
and
s
d
h,t
,
(
h?1
?
h
?
=1
W
D+1
h
?
,t
+
H
?
h
?
=h+1
W
1
h
?
,t
)
?
(?(x
t
, y
t
)??(x
t
, z
t
))
we may compactly write
L
d
h,t
= ?(y
t
, z
t
)? s
d
h,t
?w
d
???
d
h,t
.
This convex optimization problem is just like the
original MIRA and may be solved in a similar way.
The updating strategy for w
d
h,t
is derived as
w
d
h,t+1
= w
d
h,t
+ ???
d
h,t
? = (6)
min
{
C,
?(y
t
, z
t
)? T
d
h,t
? (?(x
t
, y
t
)??(x
t
, z
t
))
???
d
h,t
?
2
}
The initial vectors w
i
h,1
cannot be made all zero,
since otherwise the l-mode product in Equation
(5) would yield all zero ?
d
h,t
(x, y) and the model
would never get a chance to be updated. There-
fore, we initialize the entries of w
i
h,1
uniformly
such that the Frobenius-norm of the weight tensor
W is unity.
We call the algorithm above ?Tensor-MIRA?
and abbreviate it as T-MIRA.
671
5 Experiments
In this section we shows empirical results of the
training algorithm on a parsing task. We used the
Charniak parser (Charniak et al, 2005) for our ex-
periment, and we used the proposed algorithm to
train the reranking feature weights. For compari-
son, we also investigated training the reranker with
Perceptron and MIRA.
5.1 Experimental Settings
To simulate a low-resource training environment,
our training sets were selected from sections 2-9
of the Penn WSJ treebank, section 24 was used as
the held-out set and section 23 as the evaluation
set. We applied the default settings of the parser.
There are around V = 1.33 million features in
all defined for reranking, and the n-best size for
reranking is set to 50. We selected the parse with
the highest f -score from the 50-best list as the or-
acle.
We would like to observe from the experiments
how the amount of training data as well as dif-
ferent settings of the tensor degrees of freedom
affects the algorithm performance. Therefore we
tried all combinations of the following experimen-
tal parameters:
Parameters Settings
Training data (m) Sec. 2, 2-3, 2-5, 2-9
Tensor order (D) 2, 3, 4
# rank-1 tensors (H) 1, 2, 3
Vec. to tensor mapping approximate, sequential
Here ?approximate? and ?sequential? means us-
ing, respectively, the algorithm given in Figure 2
and the sequential mapping mentioned in Section
3.4. According to the strategy given in 3.2, once
the tensor order and number of features are fixed,
the sizes of modes and total number of parameters
to estimate are fixed as well, as shown in the tables
below:
D Size of modes Number of parameters
2 1155? 1155 2310
3 110? 110? 111 331
4 34? 34? 34? 34 136
5.2 Results and Analysis
The f -scores of the held-out and evaluation set
given by T-MIRA as well as the Perceptron and
MIRA baseline are given in Table 1. From the re-
sults, we have the following observations:
1. When very few labeled data are available for
training (compared with the number of fea-
tures), T-MIRA performs much better than
the vector-based models MIRA and Percep-
tron. However as the amount of training data
increases, the advantage of T-MIRA fades
away, and vector-based models catch up.
This is because the weight tensors learned
by T-MIRA are highly structured, which sig-
nificantly reduces model/training complex-
ity and makes the learning process very ef-
fective in a low-resource environment, but
as the amount of data increases, the more
complex and expressive vector-based models
adapt to the data better, whereas further im-
provements from the tensor model is impeded
by its structural constraints, making it insen-
sitive to the increase of training data.
2. To further contrast the behavior of T-MIRA,
MIRA and Perceptron, we plot the f -scores
on both the training and held-out sets given
by these algorithms after each training epoch
in Figure 3. The plots are for the exper-
imental setting with mapping=surrogate, #
rank-1 tensors=2, tensor order=2, training
data=sections 2-3. It is clearly seen that both
MIRA and Perceptron do much better than T-
MIRA on the training set. Nevertheless, with
a huge number of parameters to fit a limited
amount of data, they tend to over-fit and give
much worse results on the held-out set than
T-MIRA does.
As an aside, observe that MIRA consistently
outperformed Perceptron, as expected.
3. Properties of linear tensor model: The heuris-
tic vector-to-tensor mapping strategy given
by Figure 2 gives consistently better results
than the sequential mapping strategy, as ex-
pected.
To make further comparison of the two strate-
gies, in Figure 4 we plot the 20 largest sin-
gular values of the matrices which the surro-
gate weights (given by the Perceptron after
running for 1 epoch) are mapped to by both
strategies (from the experiment with training
data sections 2-5). From the contrast between
the largest and the 2
nd
-largest singular val-
ues, it can be seen that the matrix generated
672
by the first strategy approximates a low-rank
structure much better than the second strat-
egy. Therefore, the performance of T-MIRA
is influenced significantly by the way features
are mapped to the tensor. If the correspond-
ing target weight tensor has internal struc-
ture that makes it approximately low-rank,
the learning procedure becomes more effec-
tive.
The best results are consistently given by 2
nd
order tensor models, and the differences be-
tween the 3
rd
and 4
th
order tensors are not
significant. As discussed in Section 3.1, al-
though 3
rd
and 4
th
order tensors have less pa-
rameters, the benefit of reduced training com-
plexity does not compensate for the loss of
expressiveness. A 2
nd
order tensor has al-
ready reduced the number of parameters from
the original 1.33 million to only 2310, and it
does not help to further reduce the number of
parameters using higher order tensors.
4. As the amount of training data increases,
there is a trend that the best results come from
models with more rank-1 component tensors.
Adding more rank-1 tensors increases the
model?s complexity and ability of expression,
making the model more adaptive to larger
data sets.
6 Conclusion and Future Work
In this paper, we reformulated the traditional lin-
ear vector-space models as tensor-space models,
and proposed an online learning algorithm named
Tensor-MIRA. A tensor-space model is a com-
pact representation of data, and via rank-1 ten-
sor approximation, the weight tensor can be made
highly structured hence the number of parame-
ters to be trained is significantly reduced. This
can be regarded as a form of model regular-
ization.Therefore, compared with the traditional
vector-space models, learning in the tensor space
is very effective when a large feature set is defined,
but only small amount of training data is available.
Our experimental results corroborated this argu-
ment.
As mentioned in Section 3.2, one interesting
problem that merits further investigation is how
to determine optimal mode sizes. The challenge
of applying a tensor model comes from finding a
proper tensor structure for a given problem, and
 
95.5 96
 
96.5 97
 
97.5 98
 
98.5 99  1
 
2 3
 
4 5
 
6 7
 
8 9
 
10
f-score
Iteratio
ns
Trainin
g set f-
score T-MI
RA MIRA Percep
tron
(a) Training set
 
87
 
87.5 88
 
88.5 89
 
89.5 90  1
 
2 3
 
4 5
 
6 7
 
8 9
 
10
f-score
Iteratio
ns
Trainin
g set f-
score T-MI
RA MIRA Percep
tron
(b) Held-out set
Figure 3: f -scores given by three algorithms on
training and held-out set (see text for the setting).
the key to solving this problem is to find a bal-
ance between the model complexity (indicated by
the order and sizes of modes) and the number of
parameters. Developing a theoretically guaran-
teed approach of finding the optimal structure for
a given task will make the tensor model not only
perform well in low-resource environments, but
adaptive to larger data sets.
7 Acknowledgements
This work was partially supported by IBM via
DARPA/BOLT contract number HR0011-12-C-
0015 and by the National Science Foundation via
award number IIS-0963898.
References
Deng Cai , Xiaofei He , and Jiawei Han. 2006. Tensor
Space Model for Document Analysis Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval(SIGIR), 625?626.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Learn-
ing with Tensor Representation Technical Report,
Department of Computer Science, University of Illi-
nois at Urbana-Champaign.
673
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.43 89.16 89.22 89.16 89.21 89.24 89.27 89.14 89.24 89.21 88.90 88.89 89.13 88.88 88.88 89.15 88.87 88.99
Evaluation score 89.83 89.69
MIRA 88.57
Percep 88.23
(a) Training data: Section 2 only
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.26 89.06 89.12 89.33 89.11 89.19 89.18 89.14 89.15 89.2 89.01 88.82 89.24 88.94 88.95 89.19 88.91 88.98
Evaluation score 90.02 89.82
MIRA 89.00
Percep 88.59
(b) Training data: Section 2-3
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.40 89.44 89.17 89.5 89.37 89.18 89.47 89.32 89.18 89.23 89.03 88.93 89.24 88.98 88.94 89.16 89.01 88.85
Evaluation score 89.96 89.78
MIRA 89.49
Percep 89.10
(c) Training data: Section 2-5
Mapping Approximate Sequential
Rank-1 tensors 2 3 4 2 3 4
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.43 89.23 89.06 89.37 89.23 89.1 89.44 89.22 89.06 89.21 88.92 88.94 89.23 88.94 88.93 89.23 88.95 88.93
Evaluation score 89.95 89.84
MIRA 89.95
Percep 89.77
(d) Training data: Section 2-9
Table 1: Parsing f -scores. Tables (a) to (d) correspond to training data with increasing size. The upper-part of
each table shows the T-MIRA results with different settings, the lower-part shows the MIRA and Perceptron
baselines. The evaluation scores come from the settings indicated by the best held-out scores. The best results
on the held-out and evaluation data are marked in bold.
 
0
 
100
 
200
 
300
 
400
 
500
 
2 4
 
6 8
 
10 1
2 14
 
16 1
8 20
Singular value
Approx
imate Seque
ntial
Figure 4: The top 20 singular values of the surro-
gate weight matrices given by two mapping algo-
rithms.
Eugene Charniak, and Mark Johnson 2005. Coarse-
to-fine n-Best Parsing and MaxEnt Discriminative
Reranking Proceedings of the 43th Annual Meeting
on Association for Computational Linguistics(ACL)
173?180.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online Large-Margin Training of Syntactic
and Structural Translation Features Proceedings of
Empirical Methods in Natural Language Process-
ing(EMNLP), 224?233.
Shay Cohen and Michael Collins. 2012. Tensor De-
composition for Fast Parsing with Latent-Variable
PCFGs Proceedings of Advances in Neural Infor-
mation Processing Systems(NIPS).
Shay Cohen and Giorgio Satta. 2013. Approximate
PCFG Parsing Using Tensor Decomposition Pro-
ceedings of NAACL-HLT, 487?496.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov Models: Theory and Exper-
iments with Perceptron. Algorithms Proceedings of
Empirical Methods in Natural Language Process-
ing(EMNLP), 10:1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms Journal of Machine
Learning Research(JMLR), 7:551?585.
Maryam Fazel. 2002. Matrix Rank Minimization with
Applications PhD thesis, Stanford University.
Kevin Gimpel, and Noah A. Smith 2012. Structured
Ramp Loss Minimization for Machine Translation
Proceedings of North American Chapter of the As-
sociation for Computational Linguistics(NAACL),
221-231.
674
Tamir Hazan, Simon Polak, and Amnon Shashua 2005.
Sparse Image Coding using a 3D Non-negative Ten-
sor Factorization Proceedings of the International
Conference on Computer Vision (ICCV).
Mark Hopkins and Jonathan May. 2011. Tuning
as Reranking Proceedings of Empirical Methods
in Natural Language Processing(EMNLP), 1352-
1362.
Tamara Kolda and Brett Bader. 2009. Tensor Decom-
positions and Applications SIAM Review, 51:455-
550.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers Proceedings of the 43rd An-
nual Meeting of the ACL, 91?98.
Amnon Shashua, and Tamir Hazan. 2005. Non-
Negative Tensor Factorization with Applications to
Statistics and Computer Vision Proceedings of
the International Conference on Machine Learning
(ICML).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality Proceedings of NAACL-
HLT, 1142?1151.
A Proof of Theorem 1
Proof. For D = 1, it is obvious that if a set of
real numbers {x
1
, . . . , x
n
} can be represented by
a rank-1 matrix, it can always be represented by a
vector, but the reverse is not true.
For D > 1, if {x
1
, . . . , x
n
} can be repre-
sented by P = p
1
? p
2
? . . . ? p
D
, namely
x
i
= P
i
1
,...,i
D
=
?
D
d=1
p
d
i
d
, then for any compo-
nent vector in mode d,
[p
d
1
, p
d
2
, . . . , p
d
n
d
] = [s
d
1
p
d
1
, s
d
2
p
d
1
, . . . , s
d
n
p
d
p
d
1
]
where n
p
d
is the size of mode d of P , s
d
j
is a con-
stant and s
d
j
=
p
i1,...,i
d?1
,j,i
d+1
,...,i
D
p
i1,...,i
d?1
,1,i
d+1
,...,i
D
Therefore
x
i
= P
i
1
,...,i
D
= x
1,...,1
D
?
d=1
s
d
i
d
(7)
and this representation is unique for a given D(up
to the ordering of p
j
and s
d
j
in p
j
, which simply
assigns {x
1
, . . . , x
n
} with different indices in the
tensor), due to the pairwise proportional constraint
imposed by x
i
/x
j
, i, j = 1, . . . , n.
If x
i
can also be represented by Q, then x
i
=
Q
i
1
,...,i
D+1
= x
1,...,1
?
D+1
d=1
t
d
i
d
, where t
d
j
has a
similar definition as s
d
j
. Then it must be the case
that
?d
1
, d
2
? {1, . . . , D + 1}, d ? {1, . . . , D}, d
1
6= d
2
s.t.
t
d
1
i
d
1
t
d
2
i
d
2
= s
d
i
d
, (8)
t
d
a
i
d
a
= s
d
b
i
d
b
, d
a
6= d
1
, d
2
, d
b
6= d
since otherwise {x
1
, . . . , x
n
} would be repre-
sented by a different set of factors than those given
in Equation (7).
Therefore, in order for tensor Q to represent
the same set of real numbers that P represents,
there needs to exist a vector [s
d
1
, . . . , s
d
n
d
] that can
be represented by a rank-1 matrix as indicated by
Equation (8), which is in general not guaranteed.
On the other hand, if {x
1
, . . . , x
n
} can be rep-
resented by Q, namely
x
i
= Q
i
1
,...,i
D+1
=
D+1
?
d=1
q
d
i
d
then we can just pick d
1
? {1, . . . , D}, d
2
= d
1
+
1 and let
q
?
= [q
d
1
1
q
d
2
1
, q
d
1
1
q
d
2
2
, . . . , q
d
1
n
q
d
2
q
d
2
n
q
d
1
]
and
Q
?
= q
1
? . . .?q
d
1
?1
?q
?
?q
d
2
+1
? . . .?q
D+1
Hence {x
1
, . . . , x
n
} can also be represented by a
D
th
order tensor Q
?
.
675
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 171?176,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Description of the JHU System Combination Scheme for WMT 2011
Daguang Xu
Johns Hopkins University
Baltimore, USA
dxu5@jhu.edu
Yuan Cao
Johns Hopkins University
Baltimore, USA
yuan.cao@jhu.edu
Damianos Karakos
Johns Hopkins University
Baltimore, USA
damianos@jhu.edu
Abstract
This paper describes the JHU system combi-
nation scheme used in WMT-11. The JHU
system combination is based on confusion
network alignment, and inherited the frame-
work developed by (Karakos et al, 2008).
We improved our core system combination al-
gorithm by making use of TER-plus, which
was originally designed for string alignment,
for alignment of confusion networks. Exper-
imental results on French-English, German-
English, Czech-English and Spanish-English
combination tasks show significant improve-
ments on BLEU and TER by up to 2 points on
average, compared to the best individual sys-
tem output, and improvements compared with
the results produced by ITG which we used in
WMT-10.
1 Introduction
System combination aims to improve the translation
quality by combining the outputs from multiple in-
dividual MT systems. The state-of-the-art system
combination methodologies can be roughly catego-
rized as follows (Karakos et al, 2010):
1. Confusion network based: confusion network
is a form of lattice with the constraint that all
paths need to pass through all nodes. An exam-
ple of a confusion network is shown in Figure
1.
Here, the set of arcs between two consecutive
nodes represents a bin, the number following a
word is the count of this word in its bin, and
0 1this/10 2is/7was/3 3
a/8
one/2 4
dog/9
cat/1 5/0
./10
Figure 1: Example confusion network. The total count in
each bin is 10.
each bin has the same size. The basic method-
ology of system combination based on confu-
sion network includes the following steps: (a)
Choose one system output as the ?skeleton?,
which roughly decides the word order. (b)
Align further system outputs to the skeleton,
thus forming a confusion network. (c) Rescore
the final confusion network using a language
model, then pick the best path as the output of
combination.
A textual representation (where each line con-
tains the words and counts of each bin) is usu-
ally the most convenient for machine process-
ing.
2. Joint optimization based: unlike building con-
fusion network, this method considers all sys-
tem outputs at once instead of incrementally.
Then a log-linear model is used to derive costs,
followed by a search algorithm to explore the
combination space (Jayaraman et al, 2005;
Heafield et al, 2009; He et al, 2009).
3. Hypothesis selection based: this method only
includes algorithms that output one of the input
translations, and no word selection from mul-
tiple systems is performed. Typical algorithms
can be found in (Rosti et al, 2007).
171
This paper describes the JHU system com-
bination submitted to the Sixth Workshop
on Statistical Machine Translation (WMT-11)
(http://statmt.org/wmt11/index.html ). The JHU
system combination is confusion network based
as described above, following the basic system
combination framework described in (Karakos et
al., 2008). However, instead of ITG alignments
that were used in (Karakos et al, 2008), alignments
based on TER-plus (Snover et al, 2009) were used
now as the core system alignment algorithm.
The rest of the paper is organized as follows:
Section 2 introduces the application of TER-plus in
system combination. Section 3 introduces the JHU
system combination pipeline. Section 4 presents the
combination results and concluding remarks appear
in Section 5.
2 Word Reordering for Hypothesis
Alignment
Given the outputs of multiple MT systems, we
would like to reorder and align the words of different
hypothesis in a way such that an objective function is
optimized, thus reaching better translations by mak-
ing use of more information. In our system combi-
nation scheme, the objective function was based on
Translation-Edit-Rate Plus (TER-plus).
2.1 Introduction to TER-plus
TER-plus is an extension of Translation Error Rate
(TER) (Snover et al, 2006). TER is an evaluation
metric for machine translation; it generalizes Word
Error Rate (WER) by allowing block shifts in addi-
tion to the edit distance operations. However, one
problem with TER is that only exact match of word
blocks are allowed for shifting; this constraint might
be too strict as it sometimes prevents reasonable
shifts if two blocks have similar meanings.
TER-plus remedies this problem by introducing
new flexible matches between words, thus allowing
word substitutions and block shifts with costs much
lower than that of TER. Specifically, substitution
costs are now dependent on whether the words have
the same stem (stem matches) or are synonyms (syn-
onym matches). These operations relax the shift-
ing constraints of TER; shifts are now allowed if the
words of one string are synonyms or share the same
stem as the words of the string they are compared to
(Snover et al, 2009).
TER-plus identifies words with the same stem us-
ing the Porter stemming algorithm (Porter et al,
1980), and identifies synonyms using the WordNet
database (Miller et al, 1995).
2.2 TER-plus for system combination
Originally, TER-plus was designed for aligning to-
gether word strings. However, similar to the work
of (Karakos et al, 2010), who extended ITG to al-
low bilingual parsing of two confusion networks (by
treating each confusion network bin as a multi-word
entity), we converted the basic TER-plus code to
take into account multiple words present in confu-
sion network bins. Specifically, we define the cost
of aligning two confusion network bins as (Karakos
et al, 2010)
cost(b1, b2) =
1
|b1||b2|
?
w1?b1
?
w2?b2
C(w1, w2)
in which b1,b2 are the confusion network bins which
are candidates for alignment, | ? | is the size of a
bin, w1, w2 are words in b1 and b2 respectively, and
C(w1, w2) is defined as follows:
C(w1, w2) =
?
???????
???????
0 w1 matches w2
0.5 w2 is deleted
0.6 w2 is inserted
0.2 w1 and w2 are synonyms
0.2 w1 and w2 share stems
1 none of the above
Furthermore, the bin shift cost is set to 1.5. These
numbers are empirically determined based on exper-
imental results.
Similar to (Karakos et al, 2010), when a bin gets
?deleted?, it gets replaced with a NULL arc, which
simply encodes the empty string, and is otherwise
treated as a regular token in the alignments.
3 The JHU System Combination Pipeline
We now describe the JHU system combination
pipeline in which TER-plus is used as the core con-
fusion network alignment algorithm as introduced in
the previous section.
172
3.1 Combination procedure overview
The JHU system combination scheme is based on
confusion network as introduced in section 1. The
confusion networks are built in two stages:
1. Within-system combination: (optional, only
applicable in the case where per-system n-best
lists are available.) the within-system combi-
nation generates system-specific confusion net-
works based on the alignment of the n-best
translations.
2. Between-system combination: incremental
alignment of the confusion networks of differ-
ent systems generated in step 1, starting from
2-system combination up to the combination of
all systems. The order with which the systems
are selected is based on the individual BLEU
scores (i.e., the best two systems are first com-
bined, then the 3rd best is aligned to the result-
ing confusion network, etc.)
For the between-system combination we made
use of TER-plus as described in section 2.2.
3.2 Language model Rescoring with
Finite-State Transducer Operations
Once the between-system confusion networks are
ready (one confusion network per sentence), a path
through each of them has to be selected as the com-
bination output. In order to pick out the the most flu-
ent word sequence as the final translation, we need
to rescore the confusion networks using a language
model. This task can be performed efficiently via fi-
nite state transducer (FST) operations (Allauzen et
al., 2002). First, we build an FST for each confu-
sion network, called CN-FST. Since the confusion
network is just a sequence of bins and each bin is a
superposition of single words, the CN-FST can be
built as a linear FST in a straightforward way (see
Figure 1).
A 5-gram language model FST (LM-FST) is then
built for each sentence. To build the LM-FST, we
refer to the methodology described in (Allauzen et
al., 2003). In brief, the LM-FST is constructed in
the following way:
1. Extract the vocabulary of each segment.
2. Each state of the FST encodes an n-gram his-
tory (n ? 1 words). Each (non-null) arc that
originates from that state corresponds uniquely
to a word type (i.e., word that follows that his-
tory in the training data).
3. The cost of each word arc is the corre-
sponding language model score (negative log-
probability, based on the modified Kneser-Ney
formula (Kneser, 1995) for that n-gram).
4. Extra arcs are added for backing-off to lower-
order histories, thus allowing all possible word
strings to receive a non-zero probability.
In order to deal with the situation where a word
in the confusion network is not in the vocabulary of
the language model, we need to build another sim-
ple transducer, namely, the ?unknown word? FST
(UNK-FST), to map this word to the symbol <unk>
that encodes the out-of-vocabulary (OOV) words.
Note that this is useful only if one builds open-
vocabulary language models which always give a
non-zero probability to OOV words; e.g., check
out the option -unk of the SRILM toolkit (Stolcke,
2002). (Obviously, the UNK-FST leaves all other
words unmodified.)
After all these three transducers have been built,
they are composed in the following manner (for each
sentence):
CN-FST .o. UNK-FST .o. LM-FST
Note that a possible re-weighting of the arc costs
of the CN-FST can be done in order to better account
for the different dynamic ranges between the CN
costs and the LM-FST costs. Furthermore, to avoid
too many word deletions (especially in regions of the
confusion network where the words disagree most)
an additive word deletion penalty can be added to all
NULL arcs. The best (minimum-cost) path from this
resulting FST is selected as the output translation of
the system combination for that sentence.
3.3 System combination pipeline summary
We now summarize the JHU system combination
end-to-end pipeline as follows(since BLEU score is
a key metric in the WMT11 translation evaluation,
we use BLEU score as the system ranking criteria.
The BLEU score we computed for the experiments
below are all case-insensitive):
173
1. Process and re-format (lowercase, tokenize,
romanize, etc.) all individual system out-
puts. Note that we compute the case-insensitive
BLEU score in our experiments.
2. Build LM-FST and UNK-FST for each sen-
tence.
3. Decide the between-system combination order
according to the 1-best output BLEU score of
individual systems.
4. Do between-system combination based on the
order decided in step 3 using TER-plus.
5. Rescore the confusion network and start tuning
on the parameters: convert the between-system
confusion network into FST, compose it with
the UNK-FST and with the LM-FST. When
composing with LM-FST, try different CN arc
coefficients (we tried the range {5, . . . , 21}),
and unknown word insertion penalties (we tried
the values {0.3, 0.5, 0.7, 1}).
6. Compute the BLEU score for all m-syst x y
outputs, where m is the number of systems for
combination, x is the weight and y is the inser-
tion penalty.
7. Among all the scores computed in step 6, find
the best BLEU score, and keep the correspond-
ing parameter setting(m, x, y).
8. Apply the best parameter setting to the test
dataset for evaluation.
Obviously, if n-best outputs from systems are avail-
able, an extra step of producing within-system com-
binations (and searching for the best n-best size) will
also be executed.
4 Results
In WMT11, we participated in French-English,
German-English, Czech-English and Spanish-
English system combination tasks. Although we
followed the general system combination pipeline
introduced in 3.3, we did not do the within-system
combination since we received only 1-best outputs
from all systems.
We built both primary and contrastive systems,
and they differ in the way the 5-gram language mod-
els were trained. The language model for the pri-
mary system was trained with the monolingual Eu-
roparl, news commentary and news crawl corpus
provided by WMT11. The language model for the
contrastive system was trained using only the 1-
best outputs from all individual systems (sentence-
specific language model).
The number of systems used for combination
tuning in each language pair was: 24 for French-
English, 26 for German-English, 12 for Czech-
English, and 16 for Spanish-English. The best re-
sults for the combination in the primary system
made use of 23 systems for French-English, 5 sys-
tems for German-English, 10 systems for Czech-
English, 10 systems for Spanish-English. In the con-
trastive system, the number of systems were 20, 5,
6, 10 respectively.
The TER and BLEU scores on the development
set for the best individual system, the primary and
contrastive combinations are given in Table 1, and
the scores for test set are given in Table 2. From the
results we see that, compared with the best individ-
ual system outputs, system combination results in
significantly improved BLEU scores and remarkable
reductions on TER, for all language pairs. More-
over, we observe that the primary system performs
slightly better than the contrastive system in most
cases.
We also did the experiment of xx-English which
made combinations of all English outputs available
across different source languages. We used 35 sys-
tems in this experiment for both primary and con-
trastive combination, and best result made use of 15
and 16 systems respectively. The development and
test set results are shown in the ?xx-en? column in
table 1 and 2 respectively. From the results we see
the improvements on TER and BLEU scores of both
development and test sets almost doubled compared
with the best results of single language pairs.
To make a comparison with the old technique
we used in WMT10 system combination task, we
ran the WMT11 system combination task using ITG
with surface matching. The detailed implementation
is described in (Narsale, 2010). Table 3 and 4 show
the WMT11 results using ITG for alignment respec-
tively. It can be seen that TER-plus outperforms ITG
174
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 56.2 28.1 60.1 23.6 54.9 27.9 51.8 30.2 51.8 30.2
Primary combination 49.2 32.6 58.1 25.7 55.1 28.7 48.3 33.7 44.9 35.5
Contrastive combination 49.8 32.3 58.2 25.6 54.9 28.9 49.1 33.3 45.0 37.2
Table 1: Results for all language pairs on development set. The best number in each column is shown in bold.
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 58.2 30.5 65.1 23.5 59.7 29.1 60.0 28.9 58.2 30.5
Primary combination 55.9 31.9 64.4 25.0 60.1 29.6 55.4 33.5 51.7 36.3
Contrastive combination 56.5 31.6 65.7 24.4 59.9 29.8 56.5 33.4 52.5 36.5
Table 2: Results for all language pairs on test set. The best number in each column is shown in bold.
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 56.2 28.1 60.1 23.6 54.9 27.9 51.8 30.2 51.8 30.2
Primary combination 49.0 32.5 57.6 25.0 54.6 28.1 48.8 33.1 45.3 35.7
Contrastive combination 56.1 31.7 58.0 24.9 55.0 28.0 49.4 33.0 45.6 35.9
Table 3: Results for all language pairs on development set using ITG. The best number in each column is shown in
bold.
System
fr-en de-en cz-en es-en xx-en
TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
Best single system 58.2 30.5 65.1 23.5 59.7 29.1 60.0 28.9 58.2 30.5
Primary combination 55.9 31.9 64.5 24.7 60.1 29.4 55.8 33.0 52.2 35.0
Contrastive combination 56.6 31.4 64.7 24.4 60.7 29.6 56.6 33.0 52.9 35.3
Table 4: Results for all language pairs on test set using ITG. The best number in each column is shown in bold.
175
almost in all results. We will experiment with ITG
and flexible match costs and will report results in a
subsequent publication.
5 Conclusion
We described the JHU system combination scheme
that was used in WMT-11. The JHU system com-
bination system is confusion network based, and
we demonstrated the successful application of TER-
plus (which was originally designed for string align-
ment) to confusion network alignment. The WMT-
11 submission results show that significant improve-
ments on the TER and BLEU scores (over the best
individual system) were achieved.
Acknowledgments
This work was supported by the DARPA GALE pro-
gram Grant No HR0022-06-2-0001. We would also
like to thank the IBM Rosetta team for their strong
support in the system combination evaluation tasks.
References
D. Karakos, J. Smith, and S. Khudanpur. 2010. Hypoth-
esis ranking and two-pass approaches for machine
translation system combination. Acoustics Speech
and Signal Processing (ICASSP), IEEE International
Conference on.
S. Jayaraman and A. Lavie. 2005. Multi-engine machine
translation guided by explicit word matching. Proc.
EAMT:143?152.
K. Heafield, G. Hanneman, and A. Lavie. 2009.
Machinetranslation system combination with flexible
word ordering. Proc. EACL 2009, WSMT.
X. He and K. Toutanova. 2009. Joint optimization
for machine translation system combination. Proc.
EMNLP.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. Proceedings of Association for Computa-
tional Linguistics(ACL)
D. Karakos, J. Eisner, S. Khudanpur, M. Dreyer. 2008.
Machine translation system combination using ITG-
based alignments. Proceedings of Association for
Computational Linguistics(ACL) HLT, Short Papers
(Companion Volume):81-84.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, J.
Makhoul. 2006 A Study of Translation Edit Rate with
Targeted Human Annotation. Proceedings of Associa-
tion for Machine Translation in the Americas.
G.Miller. 1995 WordNet: A Lexical Database for En-
glish. . Communications of the ACM Vol. 38, No. 11.
M. Snover, N. Madnani, B. Dorr, R. Schwartz. 2009
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation at the 12th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2009), Athens, Greece.
M.F.Porter. 1980 An algorithm for suffix stripping. Pro-
gram 14(3):130-137
C. Allauzen, M. Mohri, B. Roark 1980 Generalized Al-
gorithms for Constructing Statistical Language Mod-
els. Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, July 2003,
pp. 40-47.
Sushant Narsale. 2010 JHU system combination scheme
for WMT 2010. Proceedings of Fifth Workshop on
Machine Translation, ACL.
R. Kneser, Ney. H. 2010 Improved backing-off for m-
gram language modeling. Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP.
A. Stolcke 2002 SRILM - An Extensible Language Mod-
eling Toolkit. Proceedings of International Conference
on Spoken Language Processing.
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, Mehryar
Mohri. 2002 OpenFst: A General and Efficient
Weighted Finite-State Transducer Library Proceed-
ings of the Ninth International Conference on Im-
plementation and Application of Automata, (CIAA
2007), vol. 4783, Lecture Notes in Computer Science,
pages 11-23, 2007
WMT11 official webpage. http://statmt.org/wmt11 /in-
dex.html
176
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 283?291,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joshua 4.0: Packing, PRO, and Paraphrases
Juri Ganitkevitch1, Yuan Cao1, Jonathan Weese1, Matt Post2, and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present Joshua 4.0, the newest version
of our open-source decoder for parsing-based
statistical machine translation. The main con-
tributions in this release are the introduction
of a compact grammar representation based
on packed tries, and the integration of our
implementation of pairwise ranking optimiza-
tion, J-PRO. We further present the exten-
sion of the Thrax SCFG grammar extractor
to pivot-based extraction of syntactically in-
formed sentential paraphrases.
1 Introduction
Joshua is an open-source toolkit1 for parsing-based
statistical machine translation of human languages.
The original version of Joshua (Li et al, 2009) was
a reimplementation of the Python-based Hiero ma-
chine translation system (Chiang, 2007). It was later
extended to support grammars with rich syntactic
labels (Li et al, 2010a). More recent efforts in-
troduced the Thrax module, an extensible Hadoop-
based extraction toolkit for synchronous context-
free grammars (Weese et al, 2011).
In this paper we describe a set of recent exten-
sions to the Joshua system. We present a new com-
pact grammar representation format that leverages
sparse features, quantization, and data redundancies
to store grammars in a dense binary format. This al-
lows for both near-instantaneous start-up times and
decoding with extremely large grammars. In Sec-
tion 2 we outline our packed grammar format and
1joshua-decoder.org
present experimental results regarding its impact on
decoding speed, memory use and translation quality.
Additionally, we present Joshua?s implementation
of the pairwise ranking optimization (Hopkins and
May, 2011) approach to translation model tuning.
J-PRO, like Z-MERT, makes it easy to implement
new metrics and comes with both a built-in percep-
tron classifier and out-of-the-box support for widely
used binary classifiers such as MegaM and Max-
Ent (Daume? III and Marcu, 2006; Manning and
Klein, 2003). We describe our implementation in
Section 3, presenting experimental results on perfor-
mance, classifier convergence, and tuning speed.
Finally, we introduce the inclusion of bilingual
pivoting-based paraphrase extraction into Thrax,
Joshua?s grammar extractor. Thrax?s paraphrase ex-
traction mode is simple to use, and yields state-of-
the-art syntactically informed sentential paraphrases
(Ganitkevitch et al, 2011). The full feature set of
Thrax (Weese et al, 2011) is supported for para-
phrase grammars. An easily configured feature-level
pruning mechanism allows to keep the paraphrase
grammar size manageable. Section 4 presents de-
tails on our paraphrase extraction module.
2 Compact Grammar Representation
Statistical machine translation systems tend to per-
form better when trained on larger amounts of bilin-
gual parallel data. Using tools such as Thrax, trans-
lation models and their parameters are extracted
and estimated from the data. In Joshua, translation
models are represented as synchronous context-free
grammars (SCFGs). An SCFG is a collection of
283
rules {ri} that take the form:
ri = Ci ? ??i, ?i,?i, ~?i?, (1)
where left-hand side Ci is a nonterminal symbol, the
source side ?i and the target side ?i are sequences
of both nonterminal and terminal symbols. Further,
?i is a one-to-one correspondence between the non-
terminal symbols of ?i and ?i, and ~?i is a vector of
features quantifying the probability of ?i translat-
ing to ?i, as well as other characteristics of the rule
(Weese et al, 2011). At decoding time, Joshua loads
the grammar rules into memory in their entirety, and
stores them in a trie data structure indexed by the
rules? source side. This allows the decoder to effi-
ciently look up rules that are applicable to a particu-
lar span of the (partially translated) input.
As the size of the training corpus grows, so does
the resulting translation grammar. Using more di-
verse sets of nonterminal labels ? which can signifi-
cantly improve translation performance ? further ag-
gravates this problem. As a consequence, the space
requirements for storing the grammar in memory
during decoding quickly grow impractical. In some
cases grammars may become too large to fit into the
memory on a single machine.
As an alternative to the commonly used trie struc-
tures based on hash maps, we propose a packed trie
representation for SCFGs. The approach we take is
similar to work on efficiently storing large phrase
tables by Zens and Ney (2007) and language mod-
els by Heafield (2011) and Pauls and Klein (2011) ?
both language model implementations are now inte-
grated with Joshua.
2.1 Packed Synchronous Tries
For our grammar representation, we break the SCFG
up into three distinct structures. As Figure 1 in-
dicates, we store the grammar rules? source sides
{?i}, target sides {?i}, and feature data {~?i} in sep-
arate formats of their own. Each of the structures
is packed into a flat array, and can thus be quickly
read into memory. All terminal and nonterminal
symbols in the grammar are mapped to integer sym-
bol id?s using a globally accessible vocabulary map.
We will now describe the implementation details for
each representation and their interactions in turn.
2.1.1 Source-Side Trie
The source-side trie (or source trie) is designed
to facilitate efficient lookup of grammar rules by
source side, and to allow us to completely specify a
matching set of rule with a single integer index into
the trie. We store the source sides {?i} of a grammar
in a downward-linking trie, i.e. each trie node main-
tains a record of its children. The trie is packed into
an array of 32-bit integers. Figure 1 illustrates the
composition of a node in the source-side trie. All
information regarding the node is stored in a con-
tiguous block of integers, and decomposes into two
parts: a linking block and a rule block.
The linking block stores the links to the child trie
nodes. It consists of an integer n, the number of chil-
dren, and n blocks of two integers each, containing
the symbol id aj leading to the child and the child
node?s address sj (as an index into the source-side
array). The children in the link block are sorted by
symbol id, allowing for a lookup via binary or inter-
polation search.
The rule block stores all information necessary to
reconstruct the rules that share the source side that
led to the current source trie node. It stores the num-
ber of rules, m, and then a tuple of three integers
for each of the m rules: we store the symbol id of
the left-hand side, an index into the target-side trie
and a data block id. The rules in the data block are
initially in an arbitrary order, but are sorted by ap-
plication cost upon loading.
2.1.2 Target-Side Trie
The target-side trie (or target trie) is designed to
enable us to uniquely identify a target side ?i with a
single pointer into the trie, as well as to exploit re-
dundancies in the target side string. Like the source
trie, it is stored as an array of integers. However,
the target trie is a reversed, or upward-linking trie:
a trie node retains a link to its parent, as well as the
symbol id labeling said link.
As illustrated in Figure 1, the target trie is ac-
cessed by reading an array index from the source
trie, pointing to a trie node at depth d. We then fol-
low the parent links to the trie root, accumulating
target side symbols gj into a target side string gd1 as
we go along. In order to match this traversal, the tar-
get strings are entered into the trie in reverse order,
i.e. last word first. In order to determine d from a
284
# children
# rules
child symbol
child address
rule left-hand side
target address
data block id
n ?
m ?
a
j
s
j
+
1
C
j
t
j
b
j
.
.
.
.
.
.
n
m
.
.
.
.
.
.
parent symbol
parent address
g
j
t
j-1
.
.
.
.
.
.
# features
feature id
feature value
n ?
f
j
v
j
.
.
.
n
.
.
.
.
.
Feature block 
index
Feature byte
buffer
Target trie
array
Source trie
array
f
j
.
.
.
.
.
.
Quantization
b
j
.
.
.
.
.
.
f
j
q
j
Figure 1: An illustration of our packed grammar data structures. The source sides of the grammar rules are
stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries
for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full
target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain
a data block id, which identifies feature data attached to the rule. The features are encoded according to a
type/quantization specification and stored as variable-length blocks of data in a byte buffer.
pointer into the target trie, we maintain an offset ta-
ble in which we keep track of where each new trie
level begins in the array. By first searching the offset
table, we can determine d, and thus know how much
space to allocate for the complete target side string.
To further benefit from the overlap there may be
among the target sides in the grammar, we drop the
nonterminal labels from the target string prior to in-
serting them into the trie. For richly labeled gram-
mars, this collapses all lexically identical target sides
that share the same nonterminal reordering behavior,
but vary in nonterminal labels into a single path in
the trie. Since the nonterminal labels are retained in
the rules? source sides, we do not lose any informa-
tion by doing this.
2.1.3 Features and Other Data
We designed the data format for the grammar
rules? feature values to be easily extended to include
other information that we may want to attach to a
rule, such as word alignments, or locations of occur-
rences in the training data. In order to that, each rule
ri has a unique block id bi associated with it. This
block id identifies the information associated with
the rule in every attached data store. All data stores
are implemented as memory-mapped byte buffers
that are only loaded into memory when actually re-
quested by the decoder. The format for the feature
data is detailed in the following.
The rules? feature values are stored as sparse fea-
tures in contiguous blocks of variable length in a
byte buffer. As shown in Figure 1, a lookup table
is used to map the bi to the index of the block in the
buffer. Each block is structured as follows: a sin-
gle integer, n, for the number of features, followed
by n feature entries. Each feature entry is led by an
integer for the feature id fj , and followed by a field
of variable length for the feature value vj . The size
of the value is determined by the type of the feature.
Joshua maintains a quantization configuration which
maps each feature id to a type handler or quantizer.
After reading a feature id from the byte buffer, we
retrieve the responsible quantizer and use it to read
the value from the byte buffer.
Joshua?s packed grammar format supports Java?s
standard primitive types, as well as an 8-bit quan-
tizer. We chose 8 bit as a compromise between
compression, value decoding speed and transla-
285
Grammar Format Memory
Hiero (43M rules)
Baseline 13.6G
Packed 1.8G
Syntax (200M rules)
Baseline 99.5G
Packed 9.8G
Packed 8-bit 5.8G
Table 1: Decoding-time memory use for the packed
grammar versus the standard grammar format. Even
without lossy quantization the packed grammar rep-
resentation yields significant savings in memory
consumption. Adding 8-bit quantization for the real-
valued features in the grammar reduces even large
syntactic grammars to a manageable size.
tion performance (Federico and Bertoldi, 2006).
Our quantization approach follows Federico and
Bertoldi (2006) and Heafield (2011) in partitioning
the value histogram into 256 equal-sized buckets.
We quantize by mapping each feature value onto the
weighted average of its bucket. Joshua allows for an
easily per-feature specification of type. Quantizers
can be share statistics across multiple features with
similar value distributions.
2.2 Experiments
We assess the packed grammar representation?s
memory efficiency and impact on the decoding
speed on the WMT12 French-English task. Ta-
ble 1 shows a comparison of the memory needed
to store our WMT12 French-English grammars at
runtime. We can observe a substantial decrease in
memory consumption for both Hiero-style gram-
mars and the much larger syntactically annotated
grammars. Even without any feature value quantiza-
tion, the packed format achieves an 80% reduction
in space requirements. Adding 8-bit quantization
for the log-probability features yields even smaller
grammar sizes, in this case a reduction of over 94%.
In order to avoid costly repeated retrievals of indi-
vidual feature values of rules, we compute and cache
the stateless application cost for each grammar rule
at grammar loading time. This, alongside with a lazy
approach to rule lookup allows us to largely avoid
losses in decoding speed.
Figure shows a translation progress graph for the
WMT12 French-English development set. Both sys-
 0 500 1000 1500 2000 2500
 0  500  1000  1500  2000  2500Sentences Translated Seconds PassedStandardPacked
Figure 2: A visualization of the loading and decod-
ing speed on the WMT12 French-English develop-
ment set contrasting the packed grammar represen-
tation with the standard format. Grammar loading
for the packed grammar representation is substan-
tially faster than that for the baseline setup. Even
with a slightly slower decoding speed (note the dif-
ference in the slopes) the packed grammar finishes
in less than half the time, compared to the standard
format.
tems load a Hiero-style grammar with 43 million
rules, and use 16 threads for parallel decoding. The
initial loading time for the packed grammar repre-
sentation is dramatically shorter than that for the
baseline setup (a total of 176 seconds for loading and
sorting the grammar, versus 1897 for the standard
format). Even though decoding speed is slightly
slower with the packed grammars (an average of 5.3
seconds per sentence versus 4.2 for the baseline), the
effective translation speed is more than twice that of
the baseline (1004 seconds to complete decoding the
2489 sentences, versus 2551 seconds with the stan-
dard setup).
3 J-PRO: Pairwise Ranking Optimization
in Joshua
Pairwise ranking optimization (PRO) proposed by
(Hopkins and May, 2011) is a new method for dis-
criminative parameter tuning in statistical machine
translation. It is reported to be more stable than the
popular MERT algorithm (Och, 2003) and is more
scalable with regard to the number of features. PRO
treats parameter tuning as an n-best list reranking
problem, and the idea is similar to other pairwise
ranking techniques like ranking SVM and IR SVMs
286
(Li, 2011). The algorithm can be described thusly:
Let h(c) = ?w,?(c)? be the linear model score
of a candidate translation c, in which ?(c) is the
feature vector of c and w is the parameter vector.
Also let g(c) be the metric score of c (without loss
of generality, we assume a higher score indicates a
better translation). We aim to find a parameter vector
w such that for a pair of candidates {ci, cj} in an n-
best list,
(h(ci)? h(cj))(g(ci)? g(cj)) =
?w,?(ci)??(cj)?(g(ci)? g(cj)) > 0,
namely the order of the model score is consistent
with that of the metric score. This can be turned into
a binary classification problem, by adding instance
??ij = ?(ci)??(cj)
with class label sign(g(ci) ? g(cj)) to the training
data (and symmetrically add instance
??ji = ?(cj)??(ci)
with class label sign(g(cj) ? g(ci)) at the same
time), then using any binary classifier to find the w
which determines a hyperplane separating the two
classes (therefore the performance of PRO depends
on the choice of classifier to a large extent). Given
a training set with T sentences, there are O(Tn2)
pairs of candidates that can be added to the training
set, this number is usually much too large for effi-
cient training. To make the task more tractable, PRO
samples a subset of the candidate pairs so that only
those pairs whose metric score difference is large
enough are qualified as training instances. This fol-
lows the intuition that high score differential makes
it easier to separate good translations from bad ones.
3.1 Implementation
PRO is implemented in Joshua 4.0 named J-PRO.
In order to ensure compatibility with the decoder
and the parameter tuning module Z-MERT (Zaidan,
2009) included in all versions of Joshua, J-PRO is
built upon the architecture of Z-MERT with sim-
ilar usage and configuration files(with a few extra
lines specifying PRO-related parameters). J-PRO in-
herits Z-MERT?s ability to easily plug in new met-
rics. Since PRO allows using any off-the-shelf bi-
nary classifiers, J-PRO provides a Java interface that
enables easy plug-in of any classifier. Currently, J-
PRO supports three classifiers:
? Perceptron (Rosenblatt, 1958): the percep-
tron is self-contained in J-PRO, no external re-
sources required.
? MegaM (Daume? III and Marcu, 2006): the clas-
sifier used by Hopkins and May (2011).2
? Maximum entropy classifier (Manning and
Klein, 2003): the Stanford toolkit for maxi-
mum entropy classification.3
The user may specify which classifier he wants to
use and the classifier-specific parameters in the J-
PRO configuration file.
The PRO approach is capable of handling a large
number of features, allowing the use of sparse dis-
criminative features for machine translation. How-
ever, Hollingshead and Roark (2008) demonstrated
that naively tuning weights for a heterogeneous fea-
ture set composed of both dense and sparse features
can yield subpar results. Thus, to better handle the
relation between dense and sparse features and pro-
vide a flexible selection of training schemes, J-PRO
supports the following four training modes. We as-
sume M dense features and N sparse features are
used:
1. Tune the dense feature parameters only, just
like Z-MERT (M parameters to tune).
2. Tune the dense + sparse feature parameters to-
gether (M +N parameters to tune).
3. Tune the sparse feature parameters only with
the dense feature parameters fixed, and sparse
feature parameters scaled by a manually speci-
fied constant (N parameters to tune).
4. Tune the dense feature parameters and the scal-
ing factor for sparse features, with the sparse
feature parameters fixed (M+1 parameters to
tune).
J-PRO supports n-best list input with a sparse fea-
ture format which enumerates only the firing fea-
tures together with their values. This enables a more
compact feature representation when numerous fea-
tures are involved in training.
2hal3.name/megam
3nlp.stanford.edu/software
287
0 10 20 300
10
20
30
40
Iteration
BLE
U
Dev set MT03 (10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT04(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT05(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Dev set MT03 (1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT04(1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT05(1026 features)
 
 
PercepMegaMMax?Ent
Figure 3: Experimental results on the development and test sets. The x-axis is the number of iterations (up to
30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper
row: results trained using only dense features (10 features); Lower row: results trained using dense+sparse
features (1026 features). Left column: development set (MT03); Middle column: test set (MT04); Right
column: test set (MT05).
Datasets Z-MERT
J-PRO
Percep MegaM Max-Ent
Dev (MT03) 32.2 31.9 32.0 32.0
Test (MT04) 32.6 32.7 32.7 32.6
Test (MT05) 30.7 30.9 31.0 30.9
Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
3.2 Experiments
We did our experiments using J-PRO on the NIST
Chinese-English data, and BLEU score was used as
the quality metric for experiments reported in this
section.4 The experimental settings are as the fol-
lowing:
Datasets: MT03 dataset (998 sentences) as devel-
opment set for parameter tuning, MT04 (1788 sen-
tences) and MT05 (1082 sentences) as test sets.
Features: Dense feature set include the 10 regular
features used in the Hiero system; Sparse feature set
4We also experimented with other metrics including TER,
METEOR and TER-BLEU. Similar trends as reported in this
section were observed. These results are omitted here due to
limited space.
includes 1016 target-side rule POS bi-gram features
as used in (Li et al, 2010b).
Classifiers: Perceptron, MegaM and Maximum
entropy.
PRO parameters: ? = 8000 (number of candidate
pairs sampled uniformly from the n-best list), ? = 1
(sample acceptance probability), ? = 50 (number of
top candidates to be added to the training set).
Figure 3 shows the BLEU score curves on the
development and test sets as a function of itera-
tions. The upper and lower rows correspond to
the results trained with 10 dense features and 1026
dense+sparse features respectively. We intentionally
selected very bad initial parameter vectors to verify
the robustness of the algorithm. It can be seen that
288
with each iteration, the BLEU score increases mono-
tonically on both development and test sets, and be-
gins to converge after a few iterations. When only 10
features are involved, all classifiers give almost the
same performance. However, when scaled to over a
thousand features, the maximum entropy classifier
becomes unstable and the curve fluctuates signifi-
cantly. In this situation MegaM behaves well, but
the J-PRO built-in perceptron gives the most robust
performance.
Table 2 compares the results of running Z-MERT
and J-PRO. Since MERT is not able to handle nu-
merous sparse features, we only report results for
the 10-feature setup. The scores for both setups
are quite close to each other, with Z-MERT doing
slightly better on the development set but J-PRO
yielding slightly better performance on the test set.
4 Thrax: Grammar Extraction at Scale
4.1 Translation Grammars
In previous years, our grammar extraction methods
were limited by either memory-bounded extractors.
Moving towards a parallelized grammar extraction
process, we switched from Joshua?s formerly built-
in extraction module to Thrax for WMT11. How-
ever, we were limited to a simple pseudo-distributed
Hadoop setup. In a pseudo-distributed cluster, all
tasks run on separate cores on the same machine
and access the local file system simultaneously, in-
stead of being distributed over different physical ma-
chines and harddrives. This setup proved unreliable
for larger extractions, and we were forced to reduce
the amount of data that we used to train our transla-
tion models.
For this year, however, we had a permanent clus-
ter at our disposal, which made it easy to extract
grammars from all of the available WMT12 data.
We found that on a properly distributed Hadoop
setup Thrax was able to extract both Hiero gram-
mars and the much larger SAMT grammars on the
complete WMT12 training data for all tested lan-
guage pairs. The runtimes and resulting (unfiltered)
grammar sizes for each language pair are shown in
Table 3 (for Hiero) and Table 4 (for SAMT).
Language Pair Time Rules
Cs ? En 4h41m 133M
De ? En 5h20m 219M
Fr ? En 16h47m 374M
Es ? En 16h22m 413M
Table 3: Extraction times and grammar sizes for Hi-
ero grammars using the Europarl and News Com-
mentary training data for each listed language pair.
Language Pair Time Rules
Cs ? En 7h59m 223M
De ? En 9h18m 328M
Fr ? En 25h46m 654M
Es ? En 28h10m 716M
Table 4: Extraction times and grammar sizes for
the SAMT grammars using the Europarl and News
Commentary training data for each listed language
pair.
4.2 Paraphrase Extraction
Recently English-to-English text generation tasks
have seen renewed interest in the NLP commu-
nity. Paraphrases are a key component in large-
scale state-of-the-art text-to-text generation systems.
We present an extended version of Thrax that im-
plements distributed, Hadoop-based paraphrase ex-
traction via the pivoting approach (Bannard and
Callison-Burch, 2005). Our toolkit is capable of
extracting syntactically informed paraphrase gram-
mars at scale. The paraphrase grammars obtained
with Thrax have been shown to achieve state-of-the-
art results on text-to-text generation tasks (Ganitke-
vitch et al, 2011).
For every supported translation feature, Thrax im-
plements a corresponding pivoted feature for para-
phrases. The pivoted features are set up to be aware
of the prerequisite translation features they are de-
rived from. This allows Thrax to automatically de-
tect the needed translation features and spawn the
corresponding map-reduce passes before the pivot-
ing stage takes place. In addition to features use-
ful for translation, Thrax also offers a number of
features geared towards text-to-text generation tasks
such as sentence compression or text simplification.
Due to the long tail of translations in unpruned
289
Source Bitext Sentences Words Pruning Rules
Fr ? En 1.6M 45M p(e1|e2), p(e2|e1) > 0.001 49M
{Da + Sv + Cs + De + Es + Fr} ? En 9.5M 100M
p(e1|e2), p(e2|e1) > 0.02 31M
p(e1|e2), p(e2|e1) > 0.001 91M
Table 5: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word
counts refer to the English side of the bitexts used.
translation grammars and the combinatorial effect
of pivoting, paraphrase grammars can easily grow
very large. We implement a simple feature-level
pruning approach that allows the user to specify up-
per or lower bounds for any pivoted feature. If a
paraphrase rule is not within these bounds, it is dis-
carded. Additionally, pivoted features are aware of
the bounding relationship between their value and
the value of their prerequisite translation features
(i.e. whether the pivoted feature?s value can be guar-
anteed to never be larger than the value of the trans-
lation feature). Thrax uses this knowledge to dis-
card overly weak translation rules before the pivot-
ing stage, leading to a substantial speedup in the ex-
traction process.
Table 5 gives a few examples of large paraphrase
grammars extracted from WMT training data. With
appropriate pruning settings, we are able to obtain
paraphrase grammars estimated over bitexts with
more than 100 million words.
5 Additional New Features
? With the help of the respective original au-
thors, the language model implementations by
Heafield (2011) and Pauls and Klein (2011)
have been integrated with Joshua, dropping
support for the slower and more difficult to
compile SRILM toolkit (Stolcke, 2002).
? We modified Joshua so that it can be used as
a parser to analyze pairs of sentences using a
synchronous context-free grammar. We imple-
mented the two-pass parsing algorithm of Dyer
(2010).
6 Conclusion
We present a new iteration of the Joshua machine
translation toolkit. Our system has been extended to-
wards efficiently supporting large-scale experiments
in parsing-based machine translation and text-to-text
generation: Joshua 4.0 supports compactly repre-
sented large grammars with its packed grammars,
as well as large language models via KenLM and
BerkeleyLM.We include an implementation of PRO,
allowing for stable and fast tuning of large feature
sets, and extend our toolkit beyond pure translation
applications by extending Thrax with a large-scale
paraphrase extraction module.
Acknowledgements This research was supported
by in part by the EuroMatrixPlus project funded
by the European Commission (7th Framework Pro-
gramme), and by the NSF under grant IIS-0713448.
Opinions, interpretations, and conclusions are the
authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Compu-
tational Linguistics.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of WMT06, pages
94?101. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguistics.
290
Kristy Hollingshead and Brian Roark. 2008. Rerank-
ing with baseline system scores and ranks as features.
Technical report, Center for Spoken Language Under-
standing, Oregon Health & Science University.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010a. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Zhifei Li, Ziyuan Wang, and Sanjeev Khudanpur. 2010b.
Unsupervised discriminative language model training
for machine translation using simulated confusion sets.
In Proceedings of COLING, Beijing, China, August.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Morgan &
Claypool Publishers.
Chris Manning and Dan Klein. 2003. Optimization,
maxent models, and conditional estimation without
magic. In Proceedings of HLT/NAACL, pages 8?8. As-
sociation for Computational Linguistics.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41rd
Annual Meeting of the Association for Computational
Linguistics (ACL-2003), Sapporo, Japan.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of WMT11.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In Pro-
ceedings of HLT/NAACL, pages 492?499, Rochester,
New York, April. Association for Computational Lin-
guistics.
291
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 206?212,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joshua 5.0: Sparser, better, faster, server
Matt Post1 and Juri Ganitkevitch2 and Luke Orland1 and Jonathan Weese2 and Yuan Cao2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Sciences Department
University of Pennsylvania
Abstract
We describe improvements made over the
past year to Joshua, an open-source trans-
lation system for parsing-based machine
translation. The main contributions this
past year are significant improvements in
both speed and usability of the grammar
extraction and decoding steps. We have
also rewritten the decoder to use a sparse
feature representation, enabling training of
large numbers of features with discrimina-
tive training methods.
1 Introduction
Joshua is an open-source toolkit1 for hierarchical
and syntax-based statistical machine translation
of human languages with synchronous context-
free grammars (SCFGs). The original version of
Joshua (Li et al, 2009) was a port (from Python to
Java) of the Hiero machine translation system in-
troduced by Chiang (2007). It was later extended
to support grammars with rich syntactic labels (Li
et al, 2010). Subsequent efforts produced Thrax,
the extensible Hadoop-based extraction tool for
synchronous context-free grammars (Weese et al,
2011), later extended to support pivoting-based
paraphrase extraction (Ganitkevitch et al, 2012).
Joshua 5.0 continues our yearly update cycle.
The major components of Joshua 5.0 are:
?3.1 Sparse features. Joshua now supports an
easily-extensible sparse feature implementa-
tion, along with tuning methods (PRO and
kbMIRA) for efficiently setting the weights
on large feature vectors.
1joshua-decoder.org
?3.2 Significant speed increases. Joshua 5.0 is up
to six times faster than Joshua 4.0, and also
does well against hierarchical Moses, where
end-to-end decoding (including model load-
ing) of WMT test sets is as much as three
times faster.
?3.3 Thrax 2.0. Our reengineered Hadoop-based
grammar extractor, Thrax, is up to 300%
faster while using significantly less interme-
diate disk space.
?3.4 Many other features. Joshua now includes a
server mode with fair round-robin scheduling
among and within requests, a bundler for dis-
tributing trained models, improvements to the
Joshua pipeline (for managing end-to-end ex-
periments), and better documentation.
2 Overview
Joshua is an end-to-end statistical machine trans-
lation toolkit. In addition to the decoder com-
ponent (which performs the actual translation), it
includes the infrastructure needed to prepare and
align training data, build translation and language
models, and tune and evaluate them.
This section provides a brief overview of the
contents and abilities of this toolkit. More infor-
mation can be found in the online documentation
(joshua-decoder.org/5.0/).
2.1 The Pipeline: Gluing it all together
The Joshua pipeline ties together all the infrastruc-
ture needed to train and evaluate machine transla-
tion systems for research or industrial purposes.
Once data has been segmented into parallel train-
ing, development, and test sets, a single invocation
of the pipeline script is enough to invoke this entire
infrastructure from beginning to end. Each step is
206
broken down into smaller steps (e.g., tokenizing a
file) whose dependencies are cached with SHA1
sums. This allows a reinvoked pipeline to reliably
skip earlier steps that do not need to be recom-
puted, solving a common headache in the research
and development cycle.
The Joshua pipeline is similar to other ?ex-
periment management systems? such as Moses?
Experiment Management System (EMS), a much
more general, highly-customizable tool that al-
lows the specification and parallel execution of
steps in arbitrary acyclic dependency graphs
(much like the UNIX make tool, but written with
machine translation in mind). Joshua?s pipeline
is more limited in that the basic pipeline skeleton
is hard-coded, but reduced versatility covers many
standard use cases and is arguably easier to use.
The pipeline is parameterized in many ways,
and all the options below are selectable with
command-line switches. Pipeline documentation
is available online.
2.2 Data preparation, alignment, and model
building
Data preparation involves data normalization (e.g.,
collapsing certain punctuation symbols) and tok-
enization (with the Penn treebank or user-specified
tokenizer). Alignment with GIZA++ (Och and
Ney, 2000) and the Berkeley aligner (Liang et al,
2006b) are supported.
Joshua?s builtin grammar extractor, Thrax, is
a Hadoop-based extraction implementation that
scales easily to large datasets (Ganitkevitch et al,
2013). It supports extraction of both Hiero (Chi-
ang, 2005) and SAMT grammars (Zollmann and
Venugopal, 2006) with extraction heuristics eas-
ily specified via a flexible configuration file. The
pipeline also supports GHKM grammar extraction
(Galley et al, 2006) using the extractors available
from Michel Galley2 or Moses.
SAMT and GHKM grammar extraction require
a parse tree, which are produced using the Berke-
ley parser (Petrov et al, 2006), or can be done out-
side the pipeline and supplied as an argument.
2.3 Decoding
The Joshua decoder is an implementation of the
CKY+ algorithm (Chappelier et al, 1998), which
generalizes CKY by removing the requirement
2nlp.stanford.edu/?mgalley/software/
stanford-ghkm-latest.tar.gz
that the grammar first be converted to Chom-
sky Normal Form, thereby avoiding the complex-
ities of explicit binarization schemes (Zhang et
al., 2006; DeNero et al, 2009). CKY+ main-
tains cubic-time parsing complexity (in the sen-
tence length) with Earley-style implicit binariza-
tion of rules. Joshua permits arbitrary SCFGs, im-
posing no limitation on the rank or form of gram-
mar rules.
Parsing complexity is still exponential in the
scope of the grammar,3 so grammar filtering re-
mains important. The default Thrax settings ex-
tract only grammars with rank 2, and the pipeline
implements scope-3 filtering (Hopkins and Lang-
mead, 2010) when filtering grammars to test sets
(for GHKM).
Joshua uses cube pruning (Chiang, 2007) with
a default pop limit of 100 to efficiently explore the
search space. Other decoder options are too nu-
merous to mention here, but are documented on-
line.
2.4 Tuning and testing
The pipeline allows the specification (and optional
linear interpolation) of an arbitrary number of lan-
guage models. In addition, it builds an interpo-
lated Kneser-Ney language model on the target
side of the training data using KenLM (Heafield,
2011; Heafield et al, 2013), BerkeleyLM (Pauls
and Klein, 2011) or SRILM (Stolcke, 2002).
Joshua ships with MERT (Och, 2003) and PRO
implementations. Tuning with k-best batch MIRA
(Cherry and Foster, 2012) is also supported via
callouts to Moses.
3 What?s New in Joshua 5.0
3.1 Sparse features
Until a few years ago, machine translation systems
were for the most part limited in the number of fea-
tures they could employ, since the line-based op-
timization method, MERT (Och, 2003), was not
able to efficiently search over more than tens of
feature weights. The introduction of discrimina-
tive tuning methods for machine translation (Liang
et al, 2006a; Tillmann and Zhang, 2006; Chiang
et al, 2008; Hopkins and May, 2011) has made
it possible to tune large numbers of features in
statistical machine translation systems, and open-
3Roughly, the number of consecutive nonterminals in a
rule (Hopkins and Langmead, 2010).
207
source implementations such as Cherry and Foster
(2012) have made it easy.
Joshua 5.0 has moved to a sparse feature rep-
resentation internally. First, to clarify terminol-
ogy, a feature as implemented in the decoder is
actually a template that can introduce any number
of actual features (in the standard machine learn-
ing sense). We will use the term feature function
for these templates and feature for the individual,
traditional features that are induced by these tem-
plates. For example, the (typically dense) features
stored with the grammar on disk are each separate
features contributed by the PHRASEMODEL fea-
ture function template. The LANGUAGEMODEL
template contributes a single feature value for each
language model that was loaded.
For efficiency, Joshua does not store the en-
tire feature vector during decoding. Instead, hy-
pergraph nodes maintain only the best cumulative
score of each incoming hyperedge, and the edges
themselves retain only the hyperedge delta (the in-
ner product of the weight vector and features in-
curred by that edge). After decoding, the feature
vector for each edge can be recomputed and ex-
plicitly represented if that information is required
by the decoder (for example, during tuning).
This functionality is implemented via the fol-
lowing feature function interface, presented here
in simplified pseudocode:
interface FeatureFunction:
apply(context, accumulator)
The context comprises fixed pieces of the input
sentence and hypergraph:
? the hypergraph edge (which represents the
SCFG rule and sequence of tail nodes)
? the complete source sentence
? the input span
The accumulator object?s job is to accumulate
feature (name,value) pairs fired by a feature func-
tion during the application of a rule, via another
interface:
interface Accumulator:
add(feature_name, value)
The accumulator generalization4 permits the use
of a single feature-gathering function for two ac-
cumulator objects: the first, used during decoding,
maintains only a weighted sum, and the second,
4Due to Kenneth Heafield.
used (if needed) during k-best extraction, holds
onto the entire sparse feature vector.
For tuning large sets of features, Joshua sup-
ports both PRO (Hopkins and May, 2011), an in-
house version introduced with Joshua 4.0, and k-
best batch MIRA (Cherry and Foster, 2012), im-
plemented via calls to code provided by Moses.
3.2 Performance improvements
We introduced many performance improvements,
replacing code designed to get the job done under
research timeline constraints with more efficient
alternatives, including smarter handling of locking
among threads, more efficient (non string-based)
computation of dynamic programming state, and
replacement of fixed class-based array structures
with fixed-size literals.
We used the following experimental setup to
compare Joshua 4.0 and 5.0: We extracted a large
German-English grammar from all sentences with
no more than 50 words per side from Europarl v.7
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpora using Thrax default settings.
After filtering against our test set (newstest2012),
this grammar contained 70 million rules. We then
trained three language models on (1) the target
side of our grammar training data, (2) English
Gigaword, and (3) the monolingual English data
released for WMT13. We tuned a system using
kbMIRA and decoded using KenLM (Heafield,
2011). Decoding was performed on 64-core 2.1
GHz AMD Opteron processors with 256 GB of
available memory.
Figure 1 plots the end-to-end runtime5 as a
function of the number of threads. Each point in
the graph is the minimum of at least fifteen runs
computed at different times over a period of a few
days. The main point of comparison, between
Joshua 4.0 and 5.0, shows that the current version
is up to 500% faster than it was last year, espe-
cially in multithreaded situations.
For further comparison, we took these models,
converted them to hierarchical Moses format, and
then decoded with the latest version.6 We com-
piled Moses with the recommended optimization
settings7 and used the in-memory (SCFG) gram-
5i.e., including model loading time and grammar sorting
6The latest version available on Github as of June 7, 2013
7With tcmalloc and the following compile flags:
--max-factors=1 --kenlm-max-order=5
debug-symbols=off
208
 500 1000 2000 3000 4000
 5000 10000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 4.0 (in-memory)Moses (in-memory)Joshua 4.0 (packed)Joshua 5.0 (packed)
Figure 1: End-to-end runtime as a function of the
number of threads. Each data point is the mini-
mum of at least fifteen different runs.
 200 300 400 500 1000 2000
 3000 4000 5000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 5.0Moses
Figure 2: Decoding time alone.
mar format. BLEU scores were similar.8 In this
end-to-end setting, Joshua is about 200% faster
than Moses at high thread counts (Figure 1).
Figure 2 furthers the Moses and Joshua com-
parison by plotting only decoding time (subtract-
ing out model loading and sorting times). Moses?
decoding speed is 2?3 times faster than Joshua?s,
suggesting that the end-to-end gains in Figure 1
are due to more efficient grammar loading.
3.3 Thrax 2.0
The Thrax module of our toolkit has undergone
a similar overhaul. The rule extraction code was
822.88 (Moses), 22.99 (Joshua 4), and 23.23 (Joshua 5).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 3: Here, position-aware lexical and part-of-
speech n-gram features, labeled dependency links,
and features reflecting the phrase?s CCG-style la-
bel NP/NN are included in the context vector.
rewritten to be easier to understand and extend, al-
lowing, for instance, for easy inclusion of alterna-
tive nonterminal labeling strategies.
We optimized the data representation used for
the underlying map-reduce framework towards
greater compactness and speed, resulting in a
300% increase in extraction speed and an equiv-
alent reduction in disk I/O (Table 1). These
gains enable us to extract a syntactically labeled
German-English SAMT-style translation grammar
from a bitext of over 4 million sentence pairs in
just over three hours. Furthermore, Thrax 2.0 is
capable of scaling to very large data sets, like
the composite bitext used in the extraction of the
paraphrase collection PPDB (Ganitkevitch et al,
2013), which counted 100 million sentence pairs
and over 2 billion words on the English side.
Furthermore, Thrax 2.0 contains a module fo-
cused on the extraction of compact distributional
signatures over large datasets. This distribu-
tional mode collects contextual features for n-
gram phrases, such as words occurring in a win-
dow around the phrase, as well as dependency-
based and syntactic features. Figure 3 illustrates
the feature space. We then compute a bit signature
from the resulting feature vector via a randomized
locality-sensitive hashing projection. This yields a
compact representation of a phrase?s typical con-
text. To perform this projection Thrax relies on
the Jerboa toolkit (Van Durme, 2012). As part of
the PPDB effort, Thrax has been used to extract
rich distributional signatures for 175 million 1-
to-4-gram phrases from the Annotated Gigaword
corpus (Napoles et al, 2012), a parsed and pro-
209
Cs-En Fr-En De-En Es-En
Rules 112M 357M 202M 380M
Space Time Space Time Space Time Space Time
Joshua 4.0 120GB 112 min 364GB 369 min 211GB 203 min 413GB 397 min
Joshua 5.0 31GB 25 min 101GB 81 min 56GB 44 min 108GB 84 min
Difference -74.1% -77.7% -72.3% -78.0% -73.5% -78.3% -73.8% -78.8%
Table 1: Comparing Hadoop?s intermediate disk space use and extraction time on a selection of Europarl
v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax?s final
grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers
and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
cessed version of the English Gigaword (Graff et
al., 2003).
Thrax is distributed with Joshua and is also
available as a separate download.9
3.4 Other features
Joshua 5.0 also includes many features designed
to increase its usability. These include:
? A TCP/IP server architecture, designed to
handle multiple sets of translation requests
while ensuring fairness in thread assignment
both across and within these connections.
? Intelligent selection of translation and lan-
guage model training data using cross-
entropy difference to rank training candidates
(Moore and Lewis, 2010; Axelrod et al,
2011) (described in detail in Orland (2013)).
? A bundler for easy packaging of trained mod-
els with all of its dependencies.
? A year?s worth of improvements to the
Joshua pipeline, including many new features
and supported options, and increased robust-
ness to error.
? Extended documentation.
4 WMT Submissions
We submitted a constrained entry for all tracks ex-
cept English-Czech (nine in total). Our systems
were constructed in a straightforward fashion and
without any language-specific adaptations using
the Joshua pipeline. For each language pair, we
trained a Hiero system on all sentences with no
more than fifty words per side in the Europarl,
News Commentary, and Common Crawl corpora.
9github.com/joshua-decoder/thrax
We built two interpolated Kneser-Ney language
models: one from the monolingual News Crawl
corpora (2007?2012), and another from the tar-
get side of the training data. For systems translat-
ing into English, we added a third language model
built on Gigaword. Language models were com-
bined linearly into a single language model using
interpolation weights from the tuning data (new-
stest2011). We tuned our systems with kbMIRA.
For truecasing, we used a monolingual translation
system built on the training data, and finally deto-
kenized with simple heuristics.
5 Summary
The 5.0 release of Joshua is the result of a signif-
icant year-long research, engineering, and usabil-
ity effort that we hope will be of service to the
research community. User-friendly packages of
Joshua are available from joshua-decoder.
org, while developers are encouraged to partic-
ipate via github.com/joshua-decoder/
joshua. Mailing lists, linked from the main
Joshua page, are available for both.
Acknowledgments Joshua?s sparse feature rep-
resentation owes much to discussions with Colin
Cherry, Barry Haddow, Chris Dyer, and Kenneth
Heafield at MT Marathon 2012 in Edinburgh.
This material is based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
210
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland, UK., July.
J.C. Chappelier, M. Rajman, et al 1998. A generalized
CYK algorithm for parsing stochastic CFG. In First
Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL-HLT, pages 427?436, Montre?al,
Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, Waikiki, Hawaii, USA, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, Adam Pauls, and Dan Klein. 2009.
Asynchronous binarization for synchronous gram-
mars. In Proceedings of ACL, Suntec, Singapore,
August.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL/COLING, Sydney, Australia, July.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, PRO, and paraphrases. In Proceedings of
the Workshop on Statistical Machine Translation.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database.
In Proceedings of HLT/NAACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2003.
English gigaword. Linguistic Data Consortium,
Philadelphia.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of ACL, Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguis-
tics.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proceedings of
EMNLP, pages 646?655.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, Athens,
Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proceedings of the Workshop on
Statistical Machine Translation.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of ACL/COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of
NAACL, pages 104?111, New York City, USA, June.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of ACL (short papers), pages 220?224.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL, pages 440?
447, Hong Kong, China, October.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
Sapporo, Japan.
Luke Orland. 2013. Intelligent selection of trans-
lation model training data for machine translation
with TAUS domain data: A summary. Master?s the-
sis, Johns Hopkins University, Baltimore, Maryland,
June.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
pages 258?267, Portland, Oregon, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL,
Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Seventh International
Conference on Spoken Language Processing.
211
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia, July.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Ex-
cellence, Johns Hopkins University.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the
Thrax grammar extractor. In Proceedings of the
Workshop on Statistical Machine Translation.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, New York, New York.
212
