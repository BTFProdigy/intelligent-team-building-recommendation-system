Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834?842,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Application-driven Statistical Paraphrase Generation
Shiqi Zhao, Xiang Lan, Ting Liu, Sheng Li
Information Retrieval Lab, Harbin Institute of Technology
6F Aoxiao Building, No.27 Jiaohua Street, Nangang District
Harbin, 150001, China
{zhaosq,xlan,tliu,lisheng}@ir.hit.edu.cn
Abstract
Paraphrase generation (PG) is important
in plenty of NLP applications. However,
the research of PG is far from enough. In
this paper, we propose a novel method for
statistical paraphrase generation (SPG),
which can (1) achieve various applications
based on a uniform statistical model, and
(2) naturally combine multiple resources
to enhance the PG performance. In our
experiments, we use the proposed method
to generate paraphrases for three differ-
ent applications. The results show that
the method can be easily transformed from
one application to another and generate
valuable and interesting paraphrases.
1 Introduction
Paraphrases are alternative ways that convey the
same meaning. There are two main threads in the
research of paraphrasing, i.e., paraphrase recogni-
tion and paraphrase generation (PG). Paraphrase
generation aims to generate a paraphrase for a
source sentence in a certain application. PG shows
its importance in many areas, such as question
expansion in question answering (QA) (Duboue
and Chu-Carroll, 2006), text polishing in natu-
ral language generation (NLG) (Iordanskaja et al,
1991), text simplification in computer-aided read-
ing (Carroll et al, 1999), and sentence similarity
computation in the automatic evaluation of ma-
chine translation (MT) (Kauchak and Barzilay,
2006) and summarization (Zhou et al, 2006).
This paper presents a method for statistical
paraphrase generation (SPG). As far as we know,
this is the first statistical model specially designed
for paraphrase generation. It?s distinguishing fea-
ture is that it achieves various applications with a
uniform model. In addition, it exploits multiple
resources, including paraphrase phrases, patterns,
and collocations, to resolve the data shortage prob-
lem and generate more varied paraphrases.
We consider three paraphrase applications in
our experiments, including sentence compression,
sentence simplification, and sentence similarity
computation. The proposed method generates
paraphrases for the input sentences in each appli-
cation. The generated paraphrases are then man-
ually scored based on adequacy, fluency, and us-
ability. The results show that the proposed method
is promising, which generates useful paraphrases
for the given applications. In addition, comparison
experiments show that our method outperforms a
conventional SMT-based PG method.
2 Related Work
Conventional methods for paraphrase generation
can be classified as follows:
Rule-based methods: Rule-based PG methods
build on a set of paraphrase rules or patterns,
which are either hand crafted or automatically
collected. In the early rule-based PG research,
the paraphrase rules are generally manually writ-
ten (McKeown, 1979; Zong et al, 2001), which
is expensive and arduous. Some researchers then
tried to automatically extract paraphrase rules (Lin
and Pantel, 2001; Barzilay and Lee, 2003; Zhao
et al, 2008b), which facilitates the rule-based PG
methods. However, it has been shown that the
coverage of the paraphrase patterns is not high
enough, especially when the used paraphrase pat-
terns are long or complicated (Quirk et al, 2004).
Thesaurus-based methods: The thesaurus-based
methods generate a paraphrase t for a source sen-
tence s by substituting some words in s with
their synonyms (Bolshakov and Gelbukh, 2004;
834
Kauchak and Barzilay, 2006). This kind of method
usually involves two phases, i.e., candidate extrac-
tion and paraphrase validation. In the first phase,
it extracts all synonyms from a thesaurus, such as
WordNet, for the words to be substituted. In the
second phase, it selects an optimal substitute for
each given word from the synonyms according to
the context in s. This kind of method is simple,
since the thesaurus synonyms are easy to access.
However, it cannot generate other types of para-
phrases but only synonym substitution.
NLG-based methods: NLG-based methods (Ko-
zlowski et al, 2003; Power and Scott, 2005) gen-
erally involve two stages. In the first one, the
source sentence s is transformed into its semantic
representation r by undertaking a series of NLP
processing, including morphology analyzing, syn-
tactic parsing, semantic role labeling, etc. In the
second stage, a NLG system is employed to gen-
erate a sentence t from r. s and t are paraphrases
as they are both derived from r. The NLG-based
methods simulate human paraphrasing behavior,
i.e., understanding a sentence and presenting the
meaning in another way. However, deep analysis
of sentences is a big challenge. Moreover, devel-
oping a NLG system is also not trivial.
SMT-based methods: SMT-based methods
viewed PG as monolingual MT, i.e., translating s
into t that are in the same language. Researchers
employ the existing SMT models for PG (Quirk
et al, 2004). Similar to typical SMT, a large
parallel corpus is needed as training data in the
SMT-based PG. However, such data are difficult
to acquire compared with the SMT data. There-
fore, data shortage becomes the major limitation
of the method. To address this problem, we have
tried combining multiple resources to improve the
SMT-based PG model (Zhao et al, 2008a).
There have been researchers trying to propose
uniform PG methods for multiple applications.
But they are either rule-based (Murata and Isa-
hara, 2001; Takahashi et al, 2001) or thesaurus-
based (Bolshakov and Gelbukh, 2004), thus they
have some limitations as stated above. Further-
more, few of them conducted formal experiments
to evaluate the proposed methods.
3 Statistical Paraphrase Generation
3.1 Differences between SPG and SMT
Despite the similarity between PG and MT, the
statistical model used in SMT cannot be directly
applied in SPG, since there are some clear differ-
ences between them:
1. SMT has a unique purpose, i.e., producing
high-quality translations for the inputs. On
the contrary, SPG has distinct purposes in
different applications, such as sentence com-
pression, sentence simplification, etc. The
usability of the paraphrases have to be as-
sessed in each application.
2. In SMT, words of an input sentence should
be totally translated, whereas in SPG, not all
words of an input sentence need to be para-
phrased. Therefore, a SPG model should be
able to decide which part of a sentence needs
to be paraphrased.
3. The bilingual parallel data for SMT are easy
to collect. In contrast, the monolingual paral-
lel data for SPG are not so common (Quirk
et al, 2004). Thus the SPG model should
be able to easily combine different resources
and thereby solve the data shortage problem
(Zhao et al, 2008a).
4. Methods have been proposed for automatic
evaluation in MT (e.g., BLEU (Papineni et
al., 2002)). The basic idea is that a translation
should be scored based on their similarity to
the human references. However, they cannot
be adopted in SPG. The main reason is that it
is more difficult to provide human references
in SPG. Lin and Pantel (2001) have demon-
strated that the overlapping between the au-
tomatically acquired paraphrases and hand-
crafted ones is very small. Thus the human
references cannot properly assess the quality
of the generated paraphrases.
3.2 Method Overview
The SPG method proposed in this work contains
three components, i.e., sentence preprocessing,
paraphrase planning, and paraphrase generation
(Figure 1). Sentence preprocessing mainly in-
cludes POS tagging and dependency parsing for
the input sentences, as POS tags and dependency
information are necessary for matching the para-
phrase pattern and collocation resources in the
following stages. Paraphrase planning (Section
3.3) aims to select the units to be paraphrased
(called source units henceforth) in an input sen-
tence and the candidate paraphrases for the source
835
Multiple Paraphrase Tables
PT1 ??
Paraphrase 
Planning
Paraphrase 
Generation t
Sentence 
Preprocessings
A
PT2 PTn
Figure 1: Overview of the proposed SPG method.
units (called target units) from multiple resources
according to the given application A. Paraphrase
generation (Section 3.4) is designed to generate
paraphrases for the input sentences by selecting
the optimal target units with a statistical model.
3.3 Paraphrase Planning
In this work, the multiple paraphrase resources are
stored in paraphrase tables (PTs). A paraphrase ta-
ble is similar to a phrase table in SMT, which con-
tains fine-grained paraphrases, such as paraphrase
phrases, patterns, or collocations. The PTs used in
this work are constructed using different corpora
and different score functions (Section 3.5).
If the applications are not considered, all units
of an input sentence that can be paraphrased us-
ing the PTs will be extracted as source units. Ac-
cordingly, all paraphrases for the source units will
be extracted as target units. However, when a cer-
tain application is given, only the source and target
units that can achieve the application will be kept.
We call this process paraphrase planning, which is
formally defined as in Figure 2.
An example is depicted in Figure 3. The ap-
plication in this example is sentence compression.
All source and target units are listed below the in-
put sentence, in which the first two source units
are phrases, while the third and fourth are a pattern
and a collocation, respectively. As can be seen, the
first and fourth source units are filtered in para-
phrase planning, since none of their paraphrases
achieve the application (i.e., shorter in bytes than
the source). The second and third source units are
kept, but some of their paraphrases are filtered.
3.4 Paraphrase Generation
Our SPG model contains three sub-models: a
paraphrase model, a language model, and a usabil-
ity model, which control the adequacy, fluency,
Input: source sentence s
Input: paraphrase application A
Input: paraphrase tables PTs
Output: set of source units SU
Output: set of target units TU
Extract source units of s from PTs: SU={su1, ?, sun}
For each source unit sui
Extract its target units TUi={tui1, ?, tuim}
For each target unit tuij
If tuij cannot achieve the application A
Delete tuij from TUi
End If
End For
If TUi is empty
Delete sui from SU
End If
End for
Figure 2: The paraphrase planning algorithm.
and usability of the paraphrases, respectively1.
Paraphrase Model: Paraphrase generation is a
decoding process. The input sentence s is first
segmented into a sequence of I units s?I1, which
are then paraphrased to a sequence of units t?I1.
Let (s?i, t?i) be a pair of paraphrase units, their
paraphrase likelihood is computed using a score
function ?pm(s?i, t?i). Thus the paraphrase score
ppm(s?I1, t?I1) between s and t is decomposed into:
ppm(s?I1, t?I1) =
I?
i=1
?pm(s?i, t?i)?pm (1)
where ?pm is the weight of the paraphrase model.
Actually, it is defined similarly to the translation
model in SMT (Koehn et al, 2003).
In practice, the units of a sentence may be para-
phrased using different PTs. Suppose we have K
PTs, (s?ki , t?ki) is a pair of paraphrase units from
the k-th PT with the score function ?k(s?ki , t?ki),
then Equation (1) can be rewritten as:
ppm(s?I1, t?I1) =
K?
k=1
(
?
ki
?k(s?ki , t?ki)?k) (2)
where ?k is the weight for ?k(s?ki , t?ki).
Equation (2) assumes that a pair of paraphrase
units is from only one paraphrase table. However,
1The SPG model applies monotone decoding, which does
not contain a reordering sub-model that is often used in SMT.
Instead, we use the paraphrase patterns to achieve word re-
ordering in paraphrase generation.
836
The US government should take the overall situation into consideration and actively promote bilateral high-tech trades.
The US government
The US administration
The US government on
overall situation 
overall interest
overall picture
overview
situation as a whole
whole situation
take [NN_1] into consideration  
consider [NN_1]
take into account [NN_1]
take account of [NN_1]
take [NN_1] into account
take into consideration [NN_1]
<promote, OBJ, trades>  
<sanction, OBJ, trades>
<stimulate, OBJ, trades>
<strengthen, OBJ, trades>
<support, OBJ, trades>
<sustain, OBJ, trades>
Paraphrase application: sentence compression
Figure 3: An example of paraphrase planning.
we find that about 2% of the paraphrase units ap-
pear in two or more PTs. In this case, we only
count the PT that provides the largest paraphrase
score, i.e., k? = argmaxk{?k(s?i, t?i)?k}.
In addition, note that there may be some units
that cannot be paraphrased or prefer to keep un-
changed during paraphrasing. Therefore, we have
a self-paraphrase table in the K PTs, which para-
phrases any separate word w into itself with a con-
stant score c: ?self (w,w) = c (we set c = e?1).
Language Model: We use a tri-gram language
model in this work. The language model based
score for the paraphrase t is computed as:
plm(t) =
J?
j=1
p(tj |tj?2tj?1)?lm (3)
where J is the length of t, tj is the j-th word of t,
and ?lm is the weight for the language model.
Usability Model: The usability model prefers
paraphrase units that can better achieve the ap-
plication. The usability of t depends on para-
phrase units it contains. Hence the usability model
pum(s?I1, t?I1) is decomposed into:
pum(s?I1, t?I1) =
I?
i=1
pum(s?i, t?i)?um (4)
where ?um is the weight for the usability model
and pum(s?i, t?i) is defined as follows:
pum(s?i, t?i) = e?(s?i,t?i) (5)
We consider three applications, including sentence
compression, simplification, and similarity com-
putation. ?(s?i, t?i) is defined separately for each:
? Sentence compression: Sentence compres-
sion2 is important for summarization, subti-
tle generation, and displaying texts in small
screens such as cell phones. In this appli-
cation, only the target units shorter than the
sources are kept in paraphrase planning. We
define ?(s?i, t?i) = len(s?i) ? len(t?i), where
len(?) denotes the length of a unit in bytes.
? Sentence simplification: Sentence simplifi-
cation requires using common expressions in
sentences so that readers can easily under-
stand the meaning. Therefore, only the target
units more frequent than the sources are kept
in paraphrase planning. Here, the frequency
of a unit is measured using the language
model mentioned above3. Specifically, the
langauge model assigns a score scorelm(?)
for each unit and the unit with larger score
is viewed as more frequent. We define
?(s?i, t?i) = 1 iff scorelm(t?i) > scorelm(s?i).
? Sentence similarity computation: Given a
reference sentence s?, this application aims to
paraphrase s into t, so that t is more similar
(closer in wording) with s? than s. This ap-
plication is important for the automatic eval-
uation of machine translation and summa-
rization, since we can paraphrase the human
translations/summaries to make them more
similar to the system outputs, which can re-
fine the accuracy of the evaluation (Kauchak
and Barzilay, 2006). For this application,
2This work defines compression as the shortening of sen-
tence length in bytes rather than in words.
3To compute the language model based score, the
matched patterns are instantiated and the matched colloca-
tions are connected with words between them.
837
only the target units that can enhance the sim-
ilarity to the reference sentence are kept in
planning. We define ?(s?i, t?i) = sim(t?i, s?)?
sim(s?i, s?), where sim(?, ?) is simply com-
puted as the count of overlapping words.
We combine the three sub-models based on a
log-linear framework and get the SPG model:
p(t|s) =
K?
k=1
(?k
?
ki
log ?k(s?ki , t?ki))
+ ?lm
J?
j=1
log p(tj |tj?2tj?1)
+ ?um
I?
i=1
?(s?i, t?i) (6)
3.5 Paraphrase Resources
We use five PTs in this work (except the self-
paraphrase table), in which each pair of paraphrase
units has a score assigned by the score function of
the corresponding method.
Paraphrase phrases (PT-1 to PT-3): Para-
phrase phrases are extracted from three corpora:
(1) Corp-1: bilingual parallel corpus, (2) Corp-
2: monolingual comparable corpus (comparable
news articles reporting on the same event), and
(3) Corp-3: monolingual parallel corpus (paral-
lel translations of the same foreign novel). The
details of the corpora, methods, and score func-
tions are presented in (Zhao et al, 2008a). In
our experiments, PT-1 is the largest, which con-
tains 3,041,822 pairs of paraphrase phrases. PT-2
and PT-3 contain 92,358, and 17,668 pairs of para-
phrase phrases, respectively.
Paraphrase patterns (PT-4): Paraphrase patterns
are also extracted from Corp-1. We applied the ap-
proach proposed in (Zhao et al, 2008b). Its basic
assumption is that if two English patterns e1 and e2
are aligned with the same foreign pattern f , then
e1 and e2 are possible paraphrases. One can refer
to (Zhao et al, 2008b) for the details. PT-4 con-
tains 1,018,371 pairs of paraphrase patterns.
Paraphrase collocations (PT-5): Collocations4
can cover long distance dependencies in sen-
tences. Thus paraphrase collocations are useful for
SPG. We extract collocations from a monolingual
4A collocation is a lexically restricted word pair with a
certain syntactic relation. This work only considers verb-
object collocations, e.g., <promote, OBJ, trades>.
corpus and use a binary classifier to recognize if
any two collocations are paraphrases. Due to the
space limit, we cannot introduce the detail of the
approach. We assign the score ?1? for any pair
of paraphrase collocations. PT-5 contains 238,882
pairs of paraphrase collocations.
3.6 Parameter Estimation
To estimate parameters ?k(1 ? k ? K), ?lm,
and ?um, we adopt the approach of minimum error
rate training (MERT) that is popular in SMT (Och,
2003). In SMT, however, the optimization objec-
tive function in MERT is the MT evaluation cri-
teria, such as BLEU. As we analyzed above, the
BLEU-style criteria cannot be adapted in SPG. We
therefore introduce a new optimization objective
function in this paper. The basic assumption is that
a paraphrase should contain as many correct unit
replacements as possible. Accordingly, we design
the following criteria:
Replacement precision (rp): rp assesses the pre-
cision of the unit replacements, which is defined
as rp = cdev(+r)/cdev(r), where cdev(r) is the
total number of unit replacements in the generated
paraphrases on the development set. cdev(+r) is
the number of the correct replacements.
Replacement rate (rr): rr measures the para-
phrase degree on the development set, i.e., the per-
centage of words that are paraphrased. We define
rr as: rr = wdev(r)/wdev(s), where wdev(r) is
the total number of words in the replaced units on
the development set, and wdev(s) is the number of
words of all sentences on the development set.
Replacement f-measure (rf): We use rf as the
optimization objective function in MERT, which
is similar to the conventional f-measure and lever-
ages rp and rr: rf = (2? rp? rr)/(rp+ rr).
We estimate parameters for each paraphrase ap-
plication separately. For each application, we first
ask two raters to manually label all possible unit
replacements on the development set as correct or
incorrect, so that rp, rr, and rf can be automati-
cally computed under each set of parameters. The
parameters that result in the highest rf on the de-
velopment set are finally selected.
4 Experimental Setup
Our SPG decoder is developed by remodeling
Moses that is widely used in SMT (Hoang and
Koehn, 2008). The POS tagger and depen-
dency parser for sentence preprocessing are SVM-
838
Tool (Gimenez and Marquez, 2004) and MST-
Parser (McDonald et al, 2006). The language
model is trained using a 9 GB English corpus.
4.1 Experimental Data
Our method is not restricted in domain or sentence
style. Thus any sentence can be used in develop-
ment and test. However, for the sentence similarity
computation purpose in our experiments, we want
to evaluate if the method can enhance the string-
level similarity between two paraphrase sentences.
Therefore, for each input sentence s, we need a
reference sentence s? for similarity computation.
Based on the above consideration, we acquire
experiment data from the human references of
the MT evaluation, which provide several human
translations for each foreign sentence. In detail,
we use the first translation of a foreign sentence
as the source s and the second translation as the
reference s? for similarity computation. In our ex-
periments, the development set contains 200 sen-
tences and the test set contains 500 sentences, both
of which are randomly selected from the human
translations of 2008 NIST Open Machine Transla-
tion Evaluation: Chinese to English Task.
4.2 Evaluation Metrics
The evaluation metrics for SPG are similar to the
human evaluation for MT (Callison-Burch et al,
2007). The generated paraphrases are manually
evaluated based on three criteria, i.e., adequacy,
fluency, and usability, each of which has three
scales from 1 to 3. Here is a brief description of
the different scales for the criteria:
Adequacy 1: The meaning is evidently changed.
2: The meaning is generally preserved.
3: The meaning is completely preserved.
Fluency 1: The paraphrase t is incomprehensible.
2: t is comprehensible.
3: t is a flawless sentence.
Usability 1: t is opposite to the application purpose.
2: t does not achieve the application.
3: t achieves the application.
5 Results and Analysis
We use our method to generate paraphrases for the
three applications. Results show that the percent-
ages of test sentences that can be paraphrased are
97.2%, 95.4%, and 56.8% for the applications of
sentence compression, simplification, and similar-
ity computation, respectively. The reason why the
last percentage is much lower than the first two
is that, for sentence similarity computation, many
sentences cannot find unit replacements from the
PTs that improve the similarity to the reference
sentences. For the other applications, only some
very short sentences cannot be paraphrased.
Further results show that the average number of
unit replacements in each sentence is 5.36, 4.47,
and 1.87 for sentence compression, simplification,
and similarity computation. It also indicates that
sentence similarity computation is more difficult
than the other two applications.
5.1 Evaluation of the Proposed Method
We ask two raters to label the paraphrases based
on the criteria defined in Section 4.2. The labeling
results are shown in the upper part of Table 1. We
can see that for adequacy and fluency, the para-
phrases in sentence similarity computation get the
highest scores. About 70% of the paraphrases are
labeled ?3?. This is because in sentence similar-
ity computation, only the target units appearing
in the reference sentences are kept in paraphrase
planning. This constraint filters most of the noise.
The adequacy and fluency scores of the other two
applications are not high. The percentages of la-
bel ?3? are around 30%. The main reason is that
the average numbers of unit replacements for these
two applications are much larger than sentence
similarity computation. It is thus more likely to
bring in incorrect unit replacements, which influ-
ence the quality of the generated paraphrases.
The usability is needed to be manually labeled
only for sentence simplification, since it can be
automatically labeled in the other two applica-
tions. As shown in Table 1, for sentence simplifi-
cation, most paraphrases are labeled ?2? in usabil-
ity, while merely less than 20% are labeled ?3?.
We conjecture that it is because the raters are not
sensitive to the slight change of the simplification
degree. Thus they labeled ?2? in most cases.
We compute the kappa statistic between the
raters. Kappa is defined as K = P (A)?P (E)1?P (E) (Car-
letta, 1996), where P (A) is the proportion of times
that the labels agree, and P (E) is the proportion
of times that they may agree by chance. We define
P (E) = 13 , as the labeling is based on three point
scales. The results show that the kappa statistics
for adequacy and fluency are 0.6560 and 0.6500,
which indicates a substantial agreement (K: 0.61-
0.8) according to (Landis and Koch, 1977). The
839
Adequacy (%) Fluency (%) Usability (%)
1 2 3 1 2 3 1 2 3
Sentence rater1 32.92 44.44 22.63 21.60 47.53 30.86 0 0 100
compression rater2 40.54 34.98 24.49 25.51 43.83 30.66 0 0 100
Sentence rater1 29.77 44.03 26.21 22.01 42.77 35.22 25.37 61.84 12.79
simplification rater2 33.33 35.43 31.24 24.32 39.83 35.85 30.19 51.99 17.82
Sentence rater1 7.75 24.30 67.96 7.75 22.54 69.72 0 0 100
similarity rater2 7.75 19.01 73.24 6.69 21.48 71.83 0 0 100
Baseline-1 rater1 47.31 30.75 21.94 43.01 33.12 23.87 - - -
rater2 47.10 30.11 22.80 34.41 41.51 24.09 - - -
Baseline-2 rater1 29.45 52.76 17.79 25.15 52.76 22.09 - - -
rater2 33.95 46.01 20.04 27.61 48.06 24.34 - - -
Table 1: The evaluation results of the proposed method and two baseline methods.
kappa statistic for usability is 0.5849, which is
only moderate (K: 0.41-0.6).
Table 2 shows an example of the generated para-
phrases. A source sentence s is paraphrased in
each application and we can see that: (1) for sen-
tence compression, the paraphrase t is 8 bytes
shorter than s; (2) for sentence simplification, the
words wealth and part in t are easier than their
sources asset and proportion, especially for the
non-native speakers; (3) for sentence similarity
computation, the reference sentence s? is listed be-
low t, in which the words appearing in t but not in
s are highlighted in blue.
5.2 Comparison with Baseline Methods
In our experiments, we implement two baseline
methods for comparison:
Baseline-1: Baseline-1 follows the method pro-
posed in (Quirk et al, 2004), which generates
paraphrases using typical SMT tools. Similar to
Quirk et al?s method, we extract a paraphrase ta-
ble for the SMT model from a monolingual com-
parable corpus (PT-2 described above). The SMT
decoder used in Baseline-1 is Moses.
Baseline-2: Baseline-2 extends Baseline-1 by
combining multiple resources. It exploits all PTs
introduced above in the same way as our pro-
posed method. The difference from our method is
that Baseline-2 does not take different applications
into consideration. Thus it contains no paraphrase
planning stage or the usability sub-model.
We tune the parameters for the two baselines
using the development data as described in Sec-
tion 3.6 and evaluate them with the test data. Since
paraphrase applications are not considered by the
baselines, each baseline method outputs a single
best paraphrase for each test sentence. The gener-
ation results show that 93% and 97.8% of the test
sentences can be paraphrased by Baseline-1 and
Baseline-2. The average number of unit replace-
ments per sentence is 4.23 and 5.95, respectively.
This result suggests that Baseline-1 is less capa-
ble than Baseline-2, which is mainly because its
paraphrase resource is limited.
The generated paraphrases are also labeled by
our two raters and the labeling results can be found
in the lower part of Table 1. As can be seen,
Baseline-1 performs poorly compared with our
method and Baseline-2, as the percentage of la-
bel ?1? is the highest for both adequacy and flu-
ency. This result demonstrates that it is necessary
to combine multiple paraphrase resources to im-
prove the paraphrase generation performance.
Table 1 also shows that Baseline-2 performs
comparably with our method except that it does
not consider paraphrase applications. However,
we are interested how many paraphrases gener-
ated by Baseline-2 can achieve the given applica-
tions by chance. After analyzing the results, we
find that 24.95%, 8.79%, and 7.16% of the para-
phrases achieve sentence compression, simplifi-
cation, and similarity computation, respectively,
which are much lower than our method.
5.3 Informal Comparison with Application
Specific Methods
Previous research regarded sentence compression,
simplification, and similarity computation as to-
tally different problems and proposed distinct
method for each one. Therefore, it is interesting
to compare our method to the application-specific
methods. However, it is really difficult for us to
840
Source
sentence
Liu Lefei says that in the long term, in terms of asset alocation, overseas investment should occupy a
certain proportion of an insurance company?s overall allocation.
Sentence
compression
Liu Lefei says that in [the long run]phr , [in area of [asset alocation][NN 1]]pat, overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
simplification
Liu Lefei says that in [the long run]phr , in terms of [wealth]phr [distribution]phr , overseas investment
should occupy [a [certain][JJ 1] part of [an insurance company?s overall allocation][NN 1]]pat.
Sentence
similarity
Liu Lefei says that in [the long run]phr , in terms [of capital]phr allocation, overseas investment should
occupy [the [certain][JJ 1] ratio of [an insurance company?s overall allocation][NN 1]]pat.
(reference sentence: Liu Lefei said that in terms of capital allocation, outbound investment should make
up a certain ratio of overall allocations for insurance companies in the long run .)
Table 2: The generated paraphrases of a source sentence for different applications. The target units after
replacement are shown in blue and the pattern slot fillers are in cyan. [?]phr denotes that the unit is a
phrase, while [?]pat denotes that the unit is a pattern. There is no collocation replacement in this example.
reimplement the methods purposely designed for
these applications. Thus here we just conduct an
informal comparison with these methods.
Sentence compression: Sentence compression
is widely studied, which is mostly reviewed as a
word deletion task. Different from prior research,
Cohn and Lapata (2008) achieved sentence com-
pression using a combination of several opera-
tions including word deletion, substitution, inser-
tion, and reordering based on a statistical model,
which is similar to our paraphrase generation pro-
cess. Besides, they also used paraphrase patterns
extracted from bilingual parallel corpora (like our
PT-4) as a kind of rewriting resource. However,
as most other sentence compression methods, their
method allows information loss after compression,
which means that the generated sentences are not
necessarily paraphrases of the source sentences.
Sentence Simplification: Carroll et al (1999)
has proposed an automatic text simplification
method for language-impaired readers. Their
method contains two main parts, namely the lex-
ical simplifier and syntactic simplifier. The for-
mer one focuses on replacing words with simpler
synonyms, while the latter is designed to transfer
complex syntactic structures into easy ones (e.g.,
replacing passive sentences with active forms).
Our method is, to some extent, simpler than Car-
roll et al?s, since our method does not contain syn-
tactic simplification strategies. We will try to ad-
dress sentence restructuring in our future work.
Sentence Similarity computation: Kauchak
and Barzilay (2006) have tried paraphrasing-based
sentence similarity computation. They paraphrase
a sentence s by replacing its words with Word-
Net synonyms, so that s can be more similar in
wording to another sentence s?. A similar method
has also been proposed in (Zhou et al, 2006),
which uses paraphrase phrases like our PT-1 in-
stead of WordNet synonyms. These methods can
be roughly viewed as special cases of ours, which
only focus on the sentence similarity computation
application and only use one kind of paraphrase
resource.
6 Conclusions and Future Work
This paper proposes a method for statistical para-
phrase generation. The contributions are as fol-
lows. (1) It is the first statistical model spe-
cially designed for paraphrase generation, which
is based on the analysis of the differences between
paraphrase generation and other researches, espe-
cially machine translation. (2) It generates para-
phrases for different applications with a uniform
model, rather than presenting distinct methods for
each application. (3) It uses multiple resources,
including paraphrase phrases, patterns, and collo-
cations, to relieve data shortage and generate more
varied and interesting paraphrases.
Our future work will be carried out along two
directions. First, we will improve the components
of the method, especially the paraphrase planning
algorithm. The algorithm currently used is sim-
ple but greedy, which may miss some useful para-
phrase units. Second, we will extend the method to
other applications, We hope it can serve as a uni-
versal framework for most if not all applications.
Acknowledgements
The research was supported by NSFC (60803093,
60675034) and 863 Program (2008AA01Z144).
Special thanks to Wanxiang Che, Ruifang He,
Yanyan Zhao, Yuhang Guo and the anonymous re-
viewers for insightful comments and suggestions.
841
References
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation, pages 136-158.
Jean Carletta. 1996. Assessing Agreement on Clas-
sification Tasks: The Kappa Statistic. In Computa-
tional Linguistics, 22(2): 249-254.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, John Tait. 1999. Simpli-
fying Text for Language-Impaired Readers. In Pro-
ceedings of EACL, pages 269-270.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion In Proceed-
ings of COLING, pages 137-144.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had Asked:
The impact of paraphrasing for Question Answer-
ing. In Proceedings of HLT-NAACL, pages 33-36.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, pages
43-46.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL, pages 455-462.
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure us-
ing lexico-grammatical resources. In Proceedings
of IWP, pages 1-8.
J. R. Landis and G. G. Koch. 1977. The Measure-
ment of Observer Agreement for Categorical Data.
In Biometrics 33(1): 159-174.
De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natural
Language Engineering 7(4): 343-360.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual Dependency Parsing with a
Two-Stage Discriminative Parser. In Proceedings of
CoNLL.
Kathleen R. McKeown. 1979. Paraphrasing Using
Given and New Information in a Question-Answer
System. In Proceedings of ACL, pages 67-72.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal Model for Paraphrasing - Using Transformation
Based on a Defined Criteria. In Proceedings of NL-
PRS, pages 47-54.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic gen-
eration of large-scale paraphrases. In Proceedings of
IWP, pages 73-79.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Tetsuro Takahashi, Tomoyam Iwakura, Ryu Iida, At-
sushi Fujita, Kentaro Inui. 2001. KURA: A
Transfer-based Lexico-structural Paraphrasing En-
gine. In Proceedings of NLPRS, pages 37-46.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. Combining Multiple Resources
to Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. ParaEval: Using Para-
phrases to Evaluate Summaries Automatically. In
Proceedings of HLT-NAACL, pages 447-454.
Chengqing Zong, Yujie Zhang, Kazuhide Yamamoto,
Masashi Sakamoto, Satoshi Shirai. 2001. Approach
to Spoken Chinese Paraphrasing Based on Feature
Extraction. In Proceedings of NLPRS, pages 551-
556.
842
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326?1334,
Beijing, August 2010
Leveraging Multiple MT Engines for Paraphrase Generation
Shiqi Zhao??, Haifeng Wang?, Xiang Lan?, and Ting Liu?
?Baidu Inc.
?HIT Center for Information Retrieval, Harbin Institute of Technology
{zhaoshiqi, wanghaifeng}@baidu.com,
{xlan, tliu}@ir.hit.edu.cn
Abstract
This paper proposes a method that lever-
ages multiple machine translation (MT)
engines for paraphrase generation (PG).
The method includes two stages. Firstly,
we use a multi-pivot approach to acquire
a set of candidate paraphrases for a source
sentence S. Then, we employ two kinds
of techniques, namely the selection-based
technique and the decoding-based tech-
nique, to produce a best paraphrase T for
S using the candidates acquired in the first
stage. Experimental results show that:
(1) The multi-pivot approach is effective
for obtaining plenty of valuable candi-
date paraphrases. (2) Both the selection-
based and decoding-based techniques can
make good use of the candidates and pro-
duce high-quality paraphrases. Moreover,
these two techniques are complementary.
(3) The proposed method outperforms a
state-of-the-art paraphrase generation ap-
proach.
1 Introduction
This paper addresses the problem of paraphrase
generation (PG), which seeks to generate para-
phrases for sentences. PG is important in many
natural language processing (NLP) applications.
For example, in machine translation (MT), a
sentence can be paraphrased so as to make it
more translatable (Zhang and Yamamoto, 2002;
Callison-Burch et al, 2006). In question answer-
ing (QA), a question can be paraphrased to im-
prove the coverage of answer extraction (Duboue
and Chu-Carroll, 2006; Riezler et al, 2007). In
natural language generation (NLG), paraphrasing
can help to increase the expressive power of the
NLG systems (Iordanskaja et al, 1991).
In this paper, we propose a novel PG method.
For an English sentence S, the method first ac-
quires a set of candidate paraphrases with a multi-
pivot approach, which uses MT engines to auto-
matically translate S into multiple pivot languages
and then translate them back into English. Fur-
thermore, the method employs two kinds of tech-
niques to produce a best paraphrase T for S us-
ing the candidates, i.e., the selection-based and
decoding-based techniques. The former selects
a best paraphrase from the candidates based on
Minimum Bayes Risk (MBR), while the latter
trains a MT model using the candidates and gen-
erates paraphrases with a MT decoder.
We evaluate our method on a set of 1182 En-
glish sentences. The results show that: (1) al-
though the candidate paraphrases acquired by MT
engines are noisy, they provide good raw ma-
terials for further paraphrase generation; (2) the
selection-based technique is effective, which re-
sults in the best performance; (3) the decoding-
based technique is promising, which can generate
paraphrases that are different from the candidates;
(4) both the selection-based and decoding-based
techniques outperform a state-of-the-art approach
SPG (Zhao et al, 2009).
2 Related Work
2.1 Methods for Paraphrase Generation
MT-based method is the mainstream method on
PG. It regards PG as a monolingual machine trans-
lation problem, i.e., ?translating? a sentence S
into another sentence T in the same language.
1326
Quirk et al (2004) first presented the MT-based
method. They trained a statistical MT (SMT)
model on a monolingual parallel corpus extracted
from comparable news articles and applied the
model to generate paraphrases. Their work shows
that SMT techniques can be extended to PG. How-
ever, its usefulness is limited by the scarcity of
monolingual parallel data.
To overcome the data sparseness problem, Zhao
et al (2008a) improved the MT-based PG method
by training the paraphrase model using multi-
ple resources, including monolingual parallel cor-
pora, monolingual comparable corpora, bilingual
parallel corpora, etc. Their results show that bilin-
gual parallel corpora are the most useful among
the exploited resources. Zhao et al (2009) further
improved the method by introducing a usability
sub-model into the paraphrase model so as to gen-
erate varied paraphrases for different applications.
The main disadvantage of the MT-based
method is that its performance heavily depends on
the fine-grained paraphrases, such as paraphrase
phrases and patterns, which provide paraphrase
options in decoding. Hence one has to first ex-
tract fine-grained paraphrases from various cor-
pora with different methods (Zhao et al, 2008a;
Zhao et al, 2009), which is difficult and time-
consuming.
In addition to the MT-based method, re-
searchers have also investigated other methods for
paraphrase generation, such as the pattern-based
methods (Barzilay and Lee, 2003; Pang et al,
2003), thesaurus-based methods (Bolshakov and
Gelbukh, 2004; Kauchak and Barzilay, 2006),
and NLG-based methods (Kozlowski et al, 2003;
Power and Scott, 2005).
2.2 Pivot Approach for Paraphrasing
Bannard and Callison-Burch (2005) introduced
the pivot approach to extracting paraphrase
phrases from bilingual parallel corpora. Their ba-
sic assumption is that two English phrases aligned
with the same phrase in a foreign language (also
called a pivot language) are potential paraphrases.
Zhao et al (2008b) extended the approach and
used it to extract paraphrase patterns. Both of the
above works have proved the effectiveness of the
pivot approach in paraphrase extraction.
Pivot approach can also be used in paraphrase
generation. It generates paraphrases by translating
sentences from a source language to one (single-
pivot) or more (multi-pivot) pivot languages and
then translating them back to the source language.
Duboue et al (2006) first proposed the multi-
pivot approach for paraphrase generation, which
was specially designed for question expansion in
QA. In addition, Max (2009) presented a single-
pivot approach for generating sub-sentential para-
phrases. A clear difference between our method
and the above works is that we propose selection-
based and decoding-based techniques to gener-
ate high-quality paraphrases using the candidates
yielded from the pivot approach.
3 Multi-pivot Approach for Acquiring
Candidate Paraphrases
A single-pivot PG approach paraphrases a sen-
tence S by translating it into a pivot language
PL with a MT engine MT1 and then translat-
ing it back into the source language with MT2.
In this paper, a single-pivot PG system is repre-
sented as a triple (MT1, PL, MT2). A multi-
pivot PG system is made up of a set of single-pivot
systems with various pivot languages and MT en-
gines. Given m pivot languages and n MT en-
gines, we can build a multi-pivot PG system con-
sisting of N (N ? n ? m ? n) single-pivot ones,
where N = n ? m ? n iff all the n MT engines
can perform bidirectional translation between the
source and each pivot language.
In this work, we experiment with 6 pivot lan-
guages (Table 1) and 3 MT engines (Table 2) in
the multi-pivot approach. All the 3 MT engines
are off-the-shelf systems, in which Google and
Microsoft translators are SMT engines, while Sys-
tran translator is a rule-based MT engine. Each
MT engine can translate English to all the 6 pivot
languages and back to English. We thereby con-
struct a multi-pivot PG system consisting of 54
(3*6*3) single-pivot systems.
The advantages of the multi-pivot PG approach
lie in two aspects. First, it effectively makes use
of the vast bilingual data and translation rules un-
derlying the MT engines. Second, the approach is
simple, which just sends sentences to the online
MT engines and gets the translations back.
1327
Source Sentence he said there will be major cuts in the salaries of high-level civil servants .
(GG, G, MS) he said there are significant cuts in the salaries of high-level officials .
(GG, F , GG) he said there will be significant cuts in the salaries of top civil level .
(MS, C, MS) he said that there will be a major senior civil service pay cut .
(MS, F , ST ) he said there will be great cuts in the wages of the high level civils servant .
(ST , G, GG) he said that there are major cuts in the salaries of senior government officials .
Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.
1 French (F) 4 Italian (I)
2 German (G) 5 Portuguese (P)
3 Spanish (S) 6 Chinese (C)
Table 1: Pivot languages used in the approach.
1 Google Translate (GG)
(translate.google.com)
2 Microsoft Translator (MS)
(www.microsofttranslator.com)
3 Systran Online Translation (ST)
(www.systransoft.com)
Table 2: MT engines utilized in the approach.
4 Producing High-quality Paraphrases
using the Candidates
Table 3 shows some examples of candidate para-
phrases for a sentence. As can be seen, the can-
didates do provide some correct and useful para-
phrase substitutes (in bold) for the source sen-
tence. However, they also contain quite a few er-
rors (in italic) due to the limited translation qual-
ity of the MT engines. The problem is even
worse when the source sentences get longer and
more complicated. Therefore, we need to com-
bine the outputs of the multiple single-pivot PG
systems and produce high-quality paraphrases out
of them. To this end, we investigate two tech-
niques, namely, the selection-based and decoding-
based techniques.
4.1 Selection-based Technique
Given a source sentence S along with a set D of
candidate paraphrases {T1, T2, ..., Ti, ...TN}, the
goal of the selection-based technique is to select
the best paraphrase T?i for S from D. The para-
phrase selection technique we propose is based on
Minimum Bayes Risk (MBR). In detail, the MBR
based technique first measures the quality of each
candidate paraphrase Ti ? D in terms of Bayes
risk (BR), and then selects the one with the min-
imum BR as the best paraphrase. In detail, given
S, a candidate Ti ? D, a reference paraphrase
T 1, and a loss function L(T, Ti) that measures the
quality of Ti relative to T , we define the Bayes
risk as follows:
BR(Ti) = EP (T,S)[L(T, Ti)], (1)
where the expectation is taken under the true dis-
tribution P (T, S) of the paraphrases. According
to (Bickel and Doksum, 1977), the candidate para-
phrase that minimizes the Bayes risk can be found
as follows:
T?i = arg minTi?D
?
T?T
L(T, Ti)P (T |S), (2)
where T represents the space of reference para-
phrases. In practice, however, the collection of
reference paraphrases is not available. We thus
construct a set D? = D?{S} to approximate T 2.
In addition, we cannot estimate P (T |S) in Equa-
tion (2), either. Therefore, we make a simplifica-
tion by assigning a constant c to P (T |S) for each
T ? D?, which can then be removed:
T?i = arg minTi?D
?
T?D?
L(T, Ti). (3)
Equation (3) can be further rewritten using a gain
function G(T, Ti) instead of the loss function:
1Here we assume that we have the collection of all possi-
ble paraphrases of S, which are used as references.
2The source sentence S is included in D? based on the
consideration that a sentence is allowed to keep unchanged
during paraphrasing.
1328
T?i = arg maxTi?D
?
T?D?
G(T, Ti). (4)
We define the gain function based on BLEU:
G(T, Ti) = BLEU(T, Ti). BLEU is a
widely used metric in the automatic evaluation of
MT (Papineni et al, 2002). It measures the sim-
ilarity of two sentences by counting the overlap-
ping n-grams (n=1,2,3,4 in our experiments):
BLEU(T, Ti) = BP ?exp(
4?
n=1
wn log pn(T, Ti)),
where pn(T, Ti) is the n-gram precision of Ti and
wn = 1/4. BP (? 1) is a brevity penalty that
penalizes Ti if it is shorter than T .
In summary, for each sentence S, the MBR
based technique selects a paraphrase that is the
most similar to all candidates and the source sen-
tence. The underlying assumption is that correct
paraphrase substitutes should be common among
the candidates, while errors committed by the
single-pivot PG systems should be all different.
We denote this approach as S-1 hereafter.
Approaches for comparison. In the experiments,
we also design another two paraphrase selection
approaches S-2 and S-3 for comparison with S-1.
S-2: S-2 selects the best single-pivot PG
system from all the 54 ones. The selection
is also based on MBR and BLEU. For each
single-pivot PG system, we sum up its gain
function values over a set of source sentences
(i.e., ?S
?
TS?D?S G(TS , TSi)). Then we se-lect the one with the maximum gain value as
the best single-pivot system. In our experi-
ments, the selected best single-pivot PG system is
(ST, P,GG), the candidate paraphrases acquired
by which are then returned as the best paraphrases
in S-2.
S-3: S-3 is a simple baseline, which just ran-
domly selects a paraphrase from the 54 candidates
for each source sentence S.
4.2 Decoding-based Technique
The selection-based technique introduced above
has an inherent limitation that it can only select
a paraphrase from the candidates. That is to say, it
major cuts high-level civil servants
significant cuts senior officials
major cuts* high-level officials
important cuts senior civil servants
big cuts
great cuts
Table 4: Extracted phrase pairs. (*This is called
a self-paraphrase of the source phrase, which
is generated when a phrase keeps unchanged in
some of the candidate paraphrases.)
can never produce a perfect paraphrase if all the
candidates have some tiny flaws. To solve this
problem, we propose the decoding-based tech-
nique, which trains a MT model using the can-
didate paraphrases of each source sentence S and
generates a new paraphrase T for S with a MT
decoder.
In this work, we implement the decoding-based
technique using Giza++ (Och and Ney, 2000) and
Moses (Hoang and Koehn, 2008), both of which
are commonly used SMT tools. For a sentence
S, we first construct a set of parallel sentences
by pairing S with each of its candidate para-
phrases: {(S,T1),(S,T2),...,(S,TN )} (N = 54).
We then run word alignment on the set using
Giza++ and extract aligned phrase pairs as de-
scribed in (Koehn, 2004). Here we only keep the
phrase pairs that are aligned ?3 times on the set,
so as to filter errors brought by the noisy sentence
pairs. The extracted phrase pairs are stored in a
phrase table. Table 4 shows some extracted phrase
pairs.
Note that Giza++ is sensitive to the data size.
Hence it is interesting to examine if the alignment
can be improved by augmenting the parallel sen-
tence pairs. To this end, we have tried augmenting
the parallel set for each sentence S by pairing any
two candidate paraphrases. In this manner, C2N
sentence pairs are augmented for each S. We con-
duct word alignment using the (N+C2N ) sentence
pairs and extract aligned phrases from the original
N pairs. However, we have not found clear im-
provement after observing the results. Therefore,
we do not adopt the augmentation strategy in our
experiments.
1329
Using the extracted phrasal paraphrases, we
conduct decoding for the sentence S with Moses,
which is based on a log-linear model. The default
setting of Moses is used, except that the distortion
model for phrase reordering is turned off3. The
language model in Moses is trained using a 9 GB
English corpus. We denote the above approach as
D-1 in what follows.
Approach for comparison. The main advantage
of the decoding-based technique is that it allows
us to customize the paraphrases for different re-
quirements through tailoring the phrase table or
tuning the model parameters. As a case study,
this paper shows how to generate paraphrases with
varied paraphrase rates4.
D-2: The extracted phrasal paraphrases (in-
cluding self-paraphrases) are stored in a phrase ta-
ble, in which each phrase pair has 4 scores mea-
suring their alignment confidence (Koehn et al,
2003). Our basic idea is to control the paraphrase
rate by tuning the scores of the self-paraphrases.
We thus extend D-1 to D-2, which assigns a
weight ? (? > 0) to the scores of the self-
paraphrase pairs. Obviously, if we set ? < 1,
the self-paraphrases will be penalized and the de-
coder will prefer to generate a paraphrase with
more changes. If we set ? > 1, the decoder will
tend to generate a paraphrase that is more similar
to the source sentence. In our experiments, we set
? = 0.1 in D-2.
5 Experimental Setup
Our test sentences are extracted from the paral-
lel reference translations of a Chinese-to-English
MT evaluation5, in which each Chinese sentence
c has 4 English reference translations, namely e1,
e2, e3, and e4. We use e1 as a test sentence to para-
phrase and e2, e3, e4 as human paraphrases of e1
for comparison with the automatically generated
paraphrases. We process the test set by manually
filtering ill-formed sentences, such as the ungram-
matical or incomplete ones. 1182 out of 1357
3We conduct monotone decoding as previous work
(Quirk et al, 2004; Zhao et al, 2008a, Zhao et al, 2009).
4The paraphrase rate reflects how different a paraphrase
is from the source sentence.
52008 NIST Open Machine Translation Evaluation: Chi-
nese to English Task.
Score Adequacy Fluency
5 All Flawless English
4 Most Good English
3 Much Non-native English
2 Little Disfluent English
1 None Incomprehensible
Table 5: Five point scale for human evaluation.
test sentences are retained after filtering. Statistics
show that about half of the test sentences are from
news and the other half are from essays. The aver-
age length of the test sentences is 34.12 (words).
Manual evaluation is used in this work. A para-
phrase T of a sentence S is manually scored based
on a five point scale, which measures both the ?ad-
equacy? (i.e., how much of the meaning of S is
preserved in T ) and ?fluency? of T (See Table 5).
The five point scale used here is similar to that in
the human evaluation of MT (Callison-Burch et
al., 2007). In MT, adequacy and fluency are eval-
uated separately. However, we find that there is a
high correlation between the two aspects, which
makes it difficult to separate them. Thus we com-
bine them in this paper.
We compare our method with a state-of-the-
art approach SPG6 (Zhao et al, 2009), which
is a statistical approach specially designed for
PG. The approach first collects a large volume of
fine-grained paraphrase resources, including para-
phrase phrases, patterns, and collocations, from
various corpora using different methods. Then it
generates paraphrases using these resources with
a statistical model7.
6 Experimental Results
We evaluate six approaches, i.e., S-1, S-2, S-3, D-
1, D-2 and SPG, in the experiments. Each ap-
proach generates a 1-best paraphrase for a test
sentence S. We randomize the order of the 6 para-
phrases of each S to avoid bias of the raters.
6SPG: Statistical Paraphrase Generation.
7We ran SPG under the setting of baseline-2 as described
in (Zhao et al, 2009).
1330
00.5
1
1.52
2.53
3.54
4.5
score 3.92 3.52 2.78 3.62 3.36 3.47
S-1 S-2 S-3 D-1 D-2 SPG
Figure 1: Evaluation results of the approaches.
6.1 Human Evaluation Results
We have 6 raters in the evaluation, all of whom
are postgraduate students. In particular, 3 raters
major in English, while the other 3 major in com-
puter science. Each rater scores the paraphrases
of 1/6 test sentences, whose results are then com-
bined to form the final scoring result. The av-
erage scores of the six approaches are shown in
Figure 1. We can find that among the selection-
based approaches, the performance of S-3 is the
worst, which indicates that randomly selecting a
paraphrase from the candidates works badly. S-
2 performs much better than S-3, suggesting that
the quality of the paraphrases acquired with the
best single-pivot PG system are much higher than
the randomly selected ones. S-1 performs the best
in all the six approaches, which demonstrates the
effectiveness of the MBR-based selection tech-
nique. Additionally, the fact that S-1 evidently
outperforms S-2 suggests that it is necessary to ex-
tend a single-pivot approach to a multi-pivot one.
To get a deeper insight of S-1, we randomly
sample 100 test sentences and manually score all
of their candidates. We find that S-1 successfully
picks out a paraphrase with the highest score for
72 test sentences. We further analyze the remain-
ing 28 sentences for which S-1 fails and find that
the failures are mainly due to the BLEU-based
gain function. For example, S-1 sometimes se-
lects paraphrases that have correct phrases but in-
correct phrase orders, since BLEU is weak in eval-
uating phrase orders and sentence structures. In
the next step we shall improve the gain function
by investigating other features besides BLEU.
In the decoding-based approaches, D-1 ranks
the second in the six approaches only behind S-1.
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
S-1 S-2 S-3 D-1 D-2 SPG
r1 r2 r3 r4 r5 r6
Figure 2: Evaluation results from each rater.
We will further improve D-1 in the future rather
than simply use Moses in decoding with the de-
fault setting. However, the value of D-1 lies in
that it enables us to break down the candidates
and generate new paraphrases flexibly. The per-
formance decreases when we extend D-1 to D-2
to achieve a larger paraphrase rate. This is mainly
because more errors are brought in when more
parts of a sentence are paraphrased.
We can also find from Figure 1 that S-1, S-2,
and D-1 all get higher scores than SPG, which
shows that our method outperforms this state-of-
the-art approach. This is more important if we
consider that our method is lightweight, which
makes no effort to collect fine-grained paraphrase
resources beforehand. After observing the results,
we believe that the outperformance of our method
can be mainly ascribed to the selection-based and
decoding-based techniques, since we avoid many
errors by voting among the candidates. For in-
stance, an ambiguous phrase may be incorrectly
paraphrased by some of the single-pivot PG sys-
tems or the SPG approach. However, our method
may obtain the correct paraphrase through statis-
tics over all candidates and selecting the most
credible one.
The human evaluation of paraphrases is subjec-
tive. Hence it is necessary to examine the coher-
ence among the raters. The scoring results from
the six raters are depicted in Figure 2. As it can be
seen, they show similar trends though the raters
have different degrees of strictness.
1331
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
PR1 0.116 0.138 0.232 0.149 0.206 0.139 0.366 0.379 0.386PR2 0.211 0.272 0.427 0.22 0.3 0.234 0.607 0.602 0.694
S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3
Figure 3: Paraphrase rates of the approaches.
6.2 Paraphrase Rate
Human evaluation assesses the quality of para-
phrases. However, the paraphrase rates cannot be
reflected. A paraphrase that is totally transformed
from the source sentence and another that is al-
most unchanged may get the same score. There-
fore, we propose two strategies, i.e., PR1 and PR2,
to compute the paraphrase rate:
PR1(T ) = 1? OL(S, T )L(S) ; PR2(T ) =
ED(S, T )
L(S) .
Here, PR1 is defined based on word overlapping
rate, in which OL(S, T ) denotes the number of
overlapping words between a paraphrase T and its
source sentence S, L(S) denotes the number of
words in S. PR2 is defined based on edit distance,
in which ED(S, T ) denotes the edit distance be-
tween T and S. Obviously, PR1 only measures
the percentage of words that are changed from
S to T , whereas PR2 further takes word order
changes into consideration. It should be noted that
PR1 and PR2 not only count the correct changes
between S and T , but also count the incorrect
ones. We compute the paraphrase rate for each
of the six approaches by averaging the paraphrase
rates over the whole test set. The results are shown
in the left part of Figure 3.
On the whole, the paraphrase rates of the ap-
proaches are not high. In particular, we can see
that the paraphrase rate of D-2 is clearly higher
than D-1, which is in line with our intention of de-
signing D-2. We can also see that the paraphrase
rate of S-3 is the highest among the approaches.
We find it is mainly because the paraphrases gen-
erated with S-3 contain quite a lot of errors, which
contribute most of the changes.
7 Analysis
7.1 Effectiveness of the Proposed Method
Our analysis starts from the candidate paraphrases
acquired with the multi-pivot approach. Actu-
ally, the results of S-3 reflect the average qual-
ity of the candidate paraphrases. A score of 2.78
(See Figure 1) indicates that the candidates are
unacceptable according to the human evaluation
metrics. This is in line with our expectation that
the automatically acquired paraphrases through a
two-way translation are noisy. However, the re-
sults of S-1 and D-1 demonstrate that, using the
selection-based and decoding-based techniques,
we can produce paraphrases of good quality. Es-
pecially, S-1 gets a score of nearly 4, which sug-
gests that the paraphrases are pretty good accord-
ing to our metrics. Moreover, our method out-
performs SPG built on pre-extracted fine-grained
paraphrases. It shows that our method makes good
use of the paraphrase knowledge from the large
volume of bilingual data underlying the multiple
MT engines.
7.2 How to Choose Pivot Languages and MT
Engines in the Multi-pivot Approach
In our experiments, besides the six pivot lan-
guages used in the multi-pivot system, we have
also tried another five pivot languages, including
Arabic, Japanese, Korean, Russian, and Dutch.
They are finally abandoned since we find that they
perform badly. Our experience on choosing pivot
languages is that: (1) a pivot language should be
a language whose translation quality can be well
guaranteed by the MT engines; (2) it is better to
choose a pivot language similar to the source lan-
guage (e.g., French - English), which is easier to
translate; (3) the translation quality of a pivot lan-
guage should not vary a lot among the MT en-
gines. On the other hand, it is better to choose
MT engines built on diverse models and corpora,
which can provide different paraphrase options.
We plan to employ a syntax-based MT engine in
our further experiments besides the currently used
phrase-based SMT and rule-based MT engines.
1332
S he said there will be major cuts in the salaries of high-level civil servants .
S-1 he said that there will be significant cuts in the salaries of senior officials .
S-2 he said there will be major cuts in salaries of civil servants high level .
S-3 he said that there will be significant cuts in the salaries of senior officials .
D-1 he said , there will be significant cuts in salaries of senior civil servants .
D-2 he said , there will be significant cuts in salaries of senior officials .
SPG he said that there will be the main cuts in the wages of high-level civil servants .
HP1 he said there will be a big salary cut for high-level government employees .
HP2 he said salaries of senior public servants would be slashed .
HP3 he claimed to implement huge salary cut to senior civil servants .
Table 6: Comparing the automatically generated paraphrases with the human paraphrases.
7.3 Comparing the Selection-based and
Decoding-based Techniques
It is necessary to compare the paraphrases gener-
ated via the selection-based and decoding-based
techniques. As stated above, the selection-based
technique can only select a paraphrase from the
candidates, while the decoding-based technique
can generate a paraphrase different from all can-
didates. In our experiments, we find that for
about 90% test sentences, the paraphrases gener-
ated by the decoding-based approach D-1 are out-
side the candidates. In particular, we compare the
paraphrases generated by S-1 and D-1 and find
that, for about 40% test sentences, S-1 gets higher
scores than D-1, while for another 21% test sen-
tences, D-1 gets higher scores than S-18. This
indicates that the selection-based and decoding-
based techniques are complementary. In addition,
we find examples in which the decoding-based
technique can generate a perfect paraphrase for
the source sentence, even if all the candidate para-
phrases have obvious errors. This also shows that
the decoding-based technique is promising.
7.4 Comparing Automatically Generated
Paraphrases with Human Paraphrases
We also analyze the characteristics of the gener-
ated paraphrases and compare them with the hu-
man paraphrases (i.e., the other 3 reference trans-
lations in the MT evaluation, see Section 5, which
are denoted as HP1, HP2, and HP3). We find that,
compared with the automatically generated para-
phrases, the human paraphrases are more com-
8For the rest 39%, S-1 and D-1 get identical scores.
plicated, which involve not only phrase replace-
ments, but also structure reformulations and even
inferences. Their paraphrase rates are also much
higher, which can be seen in the right part of Fig-
ure 3. We show the automatic and human para-
phrases for the example sentence of this paper in
Table 6. To narrow the gap between the automatic
and human paraphrases, it is necessary to learn
structural paraphrase knowledge from the candi-
dates in the future work.
8 Conclusions and Future Work
We put forward an effective method for para-
phrase generation, which has the following con-
tributions. First, it acquires a rich fund of para-
phrase knowledge through the use of multiple MT
engines and pivot languages. Second, it presents
a MBR-based technique that effectively selects
high-quality paraphrases from the noisy candi-
dates. Third, it proposes a decoding-based tech-
nique, which can generate paraphrases that are
different from the candidates. Experimental re-
sults show that the proposed method outperforms
a state-of-the-art approach SPG.
In the future work, we plan to improve the
selection-based and decoding-based techniques.
We will try some standard system combination
strategies, like confusion networks and consensus
decoding. In addition, we will refine our evalu-
ation metrics. In the current experiments, para-
phrase correctness (adequacy and fluency) and
paraphrase rate are evaluated separately, which
seem to be incompatible. We plan to combine
them together and propose a uniform metric.
1333
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of ACL, pages 597-604.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
HLT-NAACL, pages 16-23.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics: Basic Ideas and Selected Topics.
Holden-Day Inc., Oakland, CA, USA.
Igor A. Bolshakov and Alexander Gelbukh. 2004.
Synonymous Paraphrasing Using WordNet and In-
ternet. In Proceedings of NLDB, pages 312-323.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz and Josh Schroeder. 2007.
(Meta-) Evaluation of Machine Translation. In Pro-
ceedings of ACL-2007 Workshop on Statistical Ma-
chine Translation, pages 136-158.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses Decoder for Statistical Machine Translation.
In Proceedings of ACL Workshop on Software en-
gineering, testing, and quality assurance for NLP,
pages 58-65.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293-312.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models: User Manual and Description for Ver-
sion 1.2.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT-NAACL, pages 127-133.
Raymond Kozlowski, Kathleen F. McCoy, and K.
Vijay-Shanker. 2003. Generation of single-
sentence paraphrases from predicate/argument
structure using lexico-grammatical resources. In
Proceedings of IWP, pages 1-8.
Aure?lien Max. 2009. Sub-sentential Paraphrasing by
Contextual Pivot Translation. In Proceedings of the
2009 Workshop on Applied Textual Inference, ACL-
IJCNLP 2009, pages 18-26.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
ACL, pages 440-447.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT-NAACL, pages 102-
109.
Kishore Papineni, Salim Roukos, ToddWard, Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of
ACL, pages 311-318.
Richard Power and Donia Scott. 2005. Automatic
generation of large-scale paraphrases. In Proceed-
ings of IWP, pages 73-79.
Chris Quirk, Chris Brockett, andWilliamDolan. 2004.
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP, pages 142-
149.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.
Yujie Zhang and Kazuhide Yamamoto. 2002. Para-
phrasing of Chinese Utterances. In Proceedings of
COLING, pages 1163-1169.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven Statistical Paraphrase Genera-
tion. In Proceedings of ACL-IJCNLP 2009, pages
834-842.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008a. CombiningMultiple Resources to
Improve SMT-based Paraphrasing Model. In Pro-
ceedings of ACL-08:HLT, pages 1021-1029.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.
1334
