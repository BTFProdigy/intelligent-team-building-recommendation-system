SOAT: A Semi-Automatic Domain Ontology Acquisition Tool  
from Chinese Corpus 
Shih-Hung WU 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
shwu@iis.sinica.edu.tw 
Wen-Lian HSU 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
hsu@iis.sinica.edu.tw 
 
Abstract  
In this paper, we focus on the domain ontology 
acquisition from Chinese corpus by extracting 
rules designed for Chinese phrases. These rules 
are noun sequences with part-of-speech tags. 
Experiments show that this process can construct 
domain ontology prototypes efficiently and 
effectively. 
1. Introduction 
Domain ontology is important for large-scale 
natural language application systems such as 
speech recognition (Flett & Brown 2001), 
question answering (QA), knowledge 
management and organization memory 
(KM/OM), information retrieval, machine 
translation (Guarino 1998), and grammar 
checking systems (Bredenkamp 2000). With the 
help of domain ontology, software systems can 
perform better in understanding natural language. 
However, building domain ontology is laborious 
and time consuming. 
Previous works suggest that ontology 
acquisition is an iterative process which includes 
keyword collection as well as structure 
reorganization. The ontology will be revised, 
refined, and filled in detail during iteration. (Noy 
and McGuinness 2001) For example (Hearst 
1992), in order to find a hyponym of a keyword, 
the human editor must observe sentences 
containing this keyword and its related 
hyponyms. The editor then deduces rules for 
finding more hyponyms of this keyword. As 
such cycle iterates, the editor refines the rules to 
obtain better quality pairs of keyword-hyponyms. 
In this work we try to speed up the above 
labor-intensive approach by designing 
acquisition rules that can be applied recursively. 
A human editor only has to verify the results of 
the acquisition.  
The extraction rules we specified are templates 
of part-of-speech (POS) tagged phrase structure. 
Parsing a phrase by POS tags (Abney 1991) is a 
well-known shallow parsing technique, which 
provides the natural language processing 
function for different natural language 
applications including ontology acquisition 
(Maedche and Staab 2000). 
In previous works (Hsu et al 2001), we have 
constructed a knowledge representation 
framework, InfoMap, to integrate various 
linguistic knowledge, commonsense knowledge 
and domain knowledge. InfoMap is designed to 
perform natural language understanding. It has 
been applied to many application domains, such 
as QA system and KM/OM (Wu et al 2002) and 
has obtained encouraging results. An important 
characteristic of InfoMap is to extract events 
from a sentence by capturing the topic words, 
usually noun-verb (NV) pairs or noun-noun (NN) 
pairs, which is defined in domain ontology. We 
design the SOAT as a semi-automatic domain 
ontology acquisition tool following the ontology 
framework, InfoMap. 
We shall review the InfoMap ontology 
framework in section 2. The domain ontology 
acquisition process and extraction rules will be 
discussed in Section 3. Experimental results are 
reported in section 4. We conclude our work in 
Section 5. 
2. The InfoMap Framework 
Gruber defines an ontology to be a description 
of concepts and relationships (Gruber 1993). 
Our knowledge representation scheme, InfoMap, 
can serve as an ontology framework. InfoMap 
provides the knowledge necessary for 
understanding natural language related to a 
 
certain knowledge domain. Thus, we need to 
integrate various linguistic knowledge, 
commonsense knowledge and domain 
knowledge in making inferences. 
2.1 The Structure of InfoMap 
InfoMap consists of domain concepts and their 
associated attributes, activities, etc., which are 
its related concepts. Each of the concepts forms 
a tree-like taxonomy. InfoMap defines 
?reference? nodes to connect nodes on different 
branches, thereby integrating these concepts into 
a semantic network.  
InfoMap not only classifies concepts, but also 
classifies the relationships among concepts. 
There are two types of nodes in InfoMap: 
concept nodes and function nodes. The root 
node of a domain is the name of the domain. 
Following the root node, topics are found in this 
domain that may be of interest to users. These 
topics have sub-categories that list related 
sub-topics in a recursive fashion. 
2.2 Function Nodes in InfoMap 
InfoMap uses function nodes to label different 
relationships among related concept nodes. The 
basic function nodes are: category, attribute, 
synonym, and activity, which are described 
below. 
1. Category: Various ways of dividing up a 
concept A. For example, for the concept of 
?people?, we can divide it into young, mid-age 
and old people according to ?age?. Another 
way is to divide it into men and women 
according to ?sex?, or rich and poor people 
according to ?wealth?, etc. For each such 
partition, we shall attach a ?cause?. Each such 
division can be regarded as an angle of 
viewing concept A. 
2. Attribute: Properties of concept A. For 
example, the attributes of a human being can 
be the organs, the height, the weight, hobbies, 
etc. 
3. Associated activity: Actions that can be 
associated with concept A. For example, if A is 
a ?car?, then it can be driven, parked, raced, 
washed, repaired, etc. 
4. Synonym: Expressions that are synonymous 
to concept A in the context. 
2.3 The Contextual View of InfoMap 
Generally speaking, an ontology consists of 
definitions of concepts, relations and axioms. A 
well known ontology, WordNet (Miller 1990), 
has the following features: hypernymy, 
hyponymy, antonymy, semantic relationship, 
and synset. Comparing with the globlal view of 
concepts in WordNet, InfoMap defines category, 
event, atttibute, and synonym in a more 
contextual fashion. For example, the synonym of 
a concept in InfoMap is valid only in this 
particular context. This is very different from the 
synset in WordNet. Each node B underneath a 
function node (synonym, attribute, activity or 
category) of A can be treated as a related concept 
of A and can be further expanded by describing 
other relations pertaining to B. However, the 
relations for B described therein will be ?limited 
under the context of A?. For example, if A is 
?organization? and B is the ?facility? attribute of 
A, then underneath the node B we shall list those 
facilities one can normally find in an 
organization, whereas for the ?facility? attribute 
of ?hotel?, we shall only list those existing 
facilities in hotel. 
2.4 The Inference Engine of InfoMap 
The kernel program can map a natural language 
sentence into a set of nodes and uses the edited 
knowledge to recognize the events in the user?s 
sentences. Technically, InfoMap matches a 
natural language sentence to a collection of 
concept notes. There is a firing mechanism that 
finds nodes in InfoMap relevent to the input 
sentence. Suppose we want to find the event of 
the following sentence: ?How do I invest in 
stocks?? and the interrogative word ?how? can 
fire the word ?method?. Then along the path 
from ?method? to ?stock? the above sentence 
has fired the concepts ?stock? and ?invest?. 
Thus, the above sentence will correspond to the 
path:  
stock - event - invest - attribute - method 
Given enough knowledge about the events 
related to the main concept, InfoMap can be 
used to parse Chinese sentences. Readers can 
refer to (Hsu et al 2001) for a thorough 
description of InfoMap. 
 
3. Automatic Domain Ontology Acquisition 
To build an ontology for a new domain, we need 
to collect domain keywords and find the 
relationships among them. An acquisition 
process, SOAT, is designed that can construct a 
new ontology through domain corpus. Thus, 
with little human intervention, SOAT can build 
a prototype of the domain ontology. 
As described in previous sections, InfoMap 
consists of two major relations among concepts, 
i.e., Taxonomic relations (category and synonym) 
and Non-taxonomic relations (attribute and 
event). We defined sentence templates, which 
consists of patterns of keywords and variables, 
to capture these relations. 
3.1 Description of SOAT 
Given the domain corpus with the POS tag, our 
SOAT can be described as follows. 
Input: domain corpus with the POS tag 
Output: domain ontology prototype 
Steps: 
1 Select a keyword (usually the name of the 
domain) in the corpus as the seed to form 
a potential root set R 
2 Begin the following recursive process:  
2.1 Pick a keyword A as the root from R 
2.2 Find a new related keyword B of the root 
A by extraction rules and add it into the 
domain ontology according to the rules.   
2.3 If there is no more related keywords, 
remove A from R 
2.4 Put B into the potential root set 
2.5 Repeat step 2, until either R becomes 
empty or the total number of nodes 
generated exceeds a prescribed threshold. 
We find that most of the domain keywords are 
not in the dictionary. So the traditional TF/IDF 
method would fail. Instead, we use the high 
frequency new words discovered by PAT-tree as 
the seeds. Ideally, SOAT can generate an 
domain ontology prototype automatically. 
However, the extraction rules need to be refined 
and updated by a human editor. The details of 
SOAT extraction rules are in Section 3.2. 
3.2 The Extraction Rules of SOAT 
The extraction rules in Tables 1, 2, 3 and 4, 
consists of a specific noun as the root, and the 
POS tags of the neighboring words. A rule is a 
linguistic template for finding keywords related 
to the root. The target of extraction is usually a 
word or a compound word, which has strong 
semantic links to the root. Our rules are 
especially effective in identifying essential 
compound words for a specific domain. 
We use POS tags defined by CKIP (CKIP 1993), 
in which Na is the generic noun, Nb is the 
proper noun, and Nc is the toponym. Generally, 
an Na can be a subject or an object in a sentence, 
including concrete noun and abstract noun, such 
as ?cloth?, ?table?, ?tax?, and ?technology?. An 
Nc is the name of a place. Readers can refer to 
CKIP (CKIP 1993) for more information about 
the POS tag. In our experiment, we focus on Na 
and Nc, because the topics that we are interested 
in usually fall in these two categories. The 
extraction rules of finding categorical (taxonomy) 
relationships from a given Na (or Nc) are in 
Table 1 (and 3). The rules of finding attribute 
(non-taxonomy) relationships from a given Na 
(or Nc) are in Table 2 (and 4). 
Table 1. Category extraction rules of an Na noun  
Extraction rule Extraction 
target 
Example 
A+Na?root? A ???A????Na? 
Na+Na?root? Na ???Na????Na?
Nb+Na?root? Nb ???Nb????Na? 
Nc+Na?root? Nc ???Nc????Na?
Ncd+Na?root? Ncd  
VH+Na?root? VH ???VH????Na?
Nc+Nc+Na
?root? 
Nc+Nc ???Nc????Nc?
???Na? 
Na+Na+Na
?root? 
Na+Na ????Na????Na?
???Na? 
VH+Na+Na
?root? 
VH+Na ???VH????Na?
???Na? 
Table 2. Attribute extraction rules of an Na noun 
Extraction rule Extraction 
target 
Example 
Na?root?+Na Na ???Na????Na?
Na?root?+Nc Nc ???Na????Nc?
Na?root?+ DE 
+Na 
Na ???Na??(DE)??
??Na? 
Table 3. Category extraction rules of an Nc noun  
Extraction rule Extraction 
target 
Example 
A+Nc?root? A ???A????Nc Root? 
Na+Nc?root? Na ???Na????Nc Root? 
Nb+Nc?root? Nb ???Nb????Nc Root? 
Nc+Nc?root? Nc ???Nc????Nc Root? 
Ncd+Nc?root? Ncd  
VH+Nc?root? VH ???VH????Nc 
 
Root? 
Na+Nb+Nc
?root? 
Na+Nb ???Na?????Nb?
???Nc Root? 
Nb+Na+Nc
?root? 
Nb+Na ???Nb????Na?
???Nc Root? 
Nb+VH+Nc
?root? 
Nb+VH  
Nc+A+Nc?root? Nc+A ???Nc????A??
??Nc? 
Nc+FW+Nc
?root? 
Nc+FW  
Nc+Na+Nc
?root? 
Nc+Na ????Nc????Na?
???Nc Root? 
Nc+Nb+Nc
?root? 
Nc+Nb ???Nc????Nb?
???Nc Root? 
Nc+VC+Nc
?root? 
Nc+VC ???Nc????VC?
???Nc Root? 
Nc+Nc+Na+Nc
?root? 
Nc+Nc+Na ???Nc????Nc?
???Na????Nc Root? 
Nc+Nc+VC+Nc
?root? 
Nc+Nc+VC ???Nc????Nc?
???VC????Nc 
Root? 
Table 4. Attribute extraction rules of an Nc noun 
Extraction rule Extraction 
target 
Example 
Nc?root?+Na Na ??????Nc???
?Na? 
Nc?root?+Nc Nc ??????Nc????
?Nc? 
Nc ? root ?
+Nc+Nc 
Nc+Nc ??????Nc????
?Nc???????Nc? 
Nc ? root ?
+DE+Na 
Na ??????Nc??(DE)
????Na? 
4. Discussion 
Li and Thompson (1981) describe Mandarin 
Chinese as a Topic-prominent language in which 
the subject or the object is not as obvious as in 
other languages. Therefore, the highly precise 
shallow parsing result (Munoz et al 1999) on 
NN and SV pairs in English is probably not 
applicable to Chinese. 
4.1 The Experiment of Extraction Rate 
To test the qualitative and quantitative 
performance of SOAT, we design two 
experiments. We construct three domain 
ontology prototypes for three different domains 
and corpora. Table 5 shows the result in which 
the frequently asked questions (FAQs) for stocks 
are taken from test sentences of the financial QA 
system. The university and bank corpora are 
collected from the CKIP corpus (CKIP 1995). 
We select sentences containing the keyword 
?University? or ?Bank? as the domain corpora. 
The results in Table 5 show that SOAT can 
capture related keywords and the relationships 
among them from limited sentences very 
efficiently without using the frequency. 
Table 5. The Extraction Rate in Different Domains 
Domains  
Stock University Bank 
Corpus FAQ 
question 
CKIP 
corpus 
CKIP 
corpus 
Sentences : S 3385 3526 785 
Extrated Nodes : N 1791 2800 120 
Extration Rate : N/S 0.53 0.79 0.15 
4.2 Results from Different Corpora 
We select three different corpora from different 
information resources in the ?network? domain. 
The first corpus is a collection of FAQ sentences 
about computer network. The second corpus is a 
collection of sentences containing the keyword 
?network? from the CKIP corpus. The third 
corpus is the collection of sentences from 
Windows 2000 online help document. To reduce 
the cost of human verification, we limit the size 
of corpus to 275 sentences. The result in Table 6 
shows that there is a trade-off between 
extraction rate and the accuracy rate.  
Table 6. The extraction and accuracy rate of three 
corpora in the same domain 
 Network Domain 
Corpus FAQs  CKIP 
corpus 
Online help 
documents 
Sentences : S 275 275 275 
Extracted Nodes : N 25 180 73 
Extraction Rate : N/S 0.09 0.65 0.27 
Human verified: H 19 25 45 
Accuracy rate : H/N 0.76 0.14 0.62 
4.3 The Advantage of a Semi-Automatic 
Domain Ontology Editor for QA System 
SOAT can help in QA system ontology editing. 
In our experience, a trained knowledgeable 
editor can compile about 100 FAQs into our 
ontology manually per day. On the other hand, 
with the help of SOAT, a knowledgeable editor 
can edit on the average 4 categories, 25 
attributes and 42 activities that SOAT extracted. 
 
The quantity is estimated on 4*(25+42)=268 
FAQ query concepts at least. Thus, the 
productivity of using SOAT is approximated 
268% times. It is obvious that SOAT can help 
reducing the cost of building a new domain 
ontology. 
5. Conclusion 
We present a semi-automatic process of domain 
ontology acquisition from domain corpus. The 
ontology schema we used is general enough for 
different applications and specific enough for the 
task of understanding the Chinese natural 
language. The main objective of the research is to 
extract useful relationships from domain articles 
to construct domain ontology prototypes in a 
semi-automatic fashion. The SOAT extraction 
rules we developed can identify keywords with 
strong semantic links, especially those compound 
words in the domain. 
We have discussed how to extract related NN 
pairs in Section 3 for SOAT. However, the 
extraction rules for NN pairs do not apply for NV 
pairs. In the future we shall follow the approach 
in (Tsai et al 2002) to extract the relationships 
between nouns and its related verbs.  
The main restriction of SOAT is that the quality 
of the corpus must be very high, namely, the 
sentences are accurate and abundant enough to 
include most of the important relationships to be 
extracted.  
References  
Abney, S.P. (1991), Parsing by chunks. In Berwick, 
R.C., Abney, S.P. and Tenny, C. (ed.), 
Principle-based parsing: Computation and 
Psycholinguistics, pp. 257-278. Kluwer, 
Dordrecht. 
Bredenkamp, A., Crysmann, B., and Petrea, M. 
(2000), Looking for Errors: A declarative 
formalism for resource-adaptive language 
checking, Proceedings of the 2nd International 
Conference on Language Resources and 
Evaluation, Athens, Greece. 
CKIP (1993), Chinese Part-of-speech analysis, 
Technical Report 93-05, Academia Sinica, 
Taipei. 
CKIP (1995), A Description to the Sinica Corpus, 
Technical Report 95-02, Academia Sinica, 
Taipei. 
Flett, A. and Brown, M. (2001), 
Enterprise-standard Ontology Environments for 
Intelligent E-Business,  Proceedings of 
IJCAI-01 Workshop on E-Business & the 
Intelligent Web, Seattle, USA. 
Gruber, T.R. (1993), A translation approach to 
portable ontologies. Knowledge Acquisition, 
5(2), pp. 199-220, 1993. 
Guarino, N. (1998), Formal Ontology and 
Information Systems, Proceedings of the 1st 
International Conference on Formal Ontologies 
in Information Systems, FOIS'98, Trento, Italy, 
pp. 3-15. IOS Press. 
Hearst, M.A. (1992), Automatic acquisition of 
hyponyms from large text corpora. In 
COLING-92, pp. 539-545. 
Hsu, W.L., Wu, S.H. and Chen, Y.S. (2001), Event 
Identification Based On The Information Map - 
InfoMap, in symposium NLPKE of the IEEE 
SMC Conference, Tucson Arizona, USA. 
Li, C.N. and S.A. Thompson (1981), Mandarin 
Chinese: a functional reference grammar, 
University of California press. 
Maedche, A. and Staab, S. (2000), Discovering 
Conceptual Relations from Text. In: Horn, W. 
(ed.): ECAI 2000. Proceedings of the 14th 
European Conference on Artificial Intelligence, 
IOS Press, Amsterdam.  
Munoz, M., Punyakanok, V., Roth, D., Zimak, D. 
(1999), A Learning Approach to Shallow 
Parsing, Proceedings of EMNLP-WVLC'99. 
Noy, N.F. and McGuinness D.L. (2001), Ontology 
Development 101: A Guide to Creating Your 
First Ontology, SMI technical report 
SMI-2001-0880, Stanford Medical Informatics. 
Tsai, J. L, Hsu, W.L. and Su, J.W. (2002), Word 
sense disambiguation and sense-based NV 
event-frame identifier. Computational 
Linguistics and Chinese Language Processing, 
7(1), pp. 1-18. 
Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L. 
(2002), FAQ-centered Organizational Memory, 
in Matta, N. and Dieng-Kuntz, R. (ed.), 
Knowledge Management and Organizational 
Memories, Kluwer Academic Publishers. 
 
 
Text Categorization Using Automatically Acquired Domain Ontology
 
 
Shih-Hung Wu, Tzong-Han Tsai, Wen-Lian Hsu 
Institute of Information Science 
Academia Sinica 
Nankang, Taipei, Taiwan, R.O.C. 
shwu@iis.sinica.edu.tw, thtsai@iis.sinica.edu.tw, hsu@iis.sinica.edu.tw 
 
 
 
Abstract 
 
In this paper, we describe ontology-based 
text categorization in which the domain 
ontologies are automatically acquired 
through morphological rules and statistical 
methods. The ontology-based approach is 
a promising way for general information 
retrieval applications such as knowledge 
management or knowledge discovery. As 
a way to evaluate the quality of domain 
ontologies, we test our method through 
several experiments.  Automatically 
acquired domain ontologies, with or 
without manual editing, have been used 
for text categorization. The results are 
quite satisfactory. Furthermore, we have 
developed an automatic method to 
evaluate the quality of our domain 
ontology.  
1. Introduction 
Domain ontology, consisting of important 
concepts and relationships of the concepts in the 
domain, is useful in a variety of applications 
(Gruber, 1993). However, evaluating the quality of 
domain ontologies is not straightforward. Reusing 
an ontology for several applications can be a 
practical method for evaluating domain ontology. 
Since text categorization is a general tool for 
information retrieval, knowledge management and 
knowledge discovery, we test the ability of 
domain ontology to categorize news clips in this 
paper. 
Traditional IR methods use keyword 
distribution form a training corpus to assign 
testing document. However, using only keywords 
in a training set cannot guarantee satisfactory 
results since authors may use different  keywords. 
We believe that, news clip events are categorized 
by concepts, not just keywords. Previous works 
shows that the latent semantic index (LSI) method 
and the n-gram method give good results for 
Chinese news categorization (Wu et al, 1998). 
However, the indices of LSI and n-grams are less 
meaningful semantically. The implicit rules 
acquired by these methods can be understood by 
computers, not humans. Thus, manual editing for 
exceptions and personalization are not possible 
and it is difficult to further reuse these indices for 
knowledge management. 
With good domain ontology we can identify 
the concept structure of sentences in a document. 
Our idea is to compile the concepts within 
documents in a training set and use these concepts 
to understand documents in a testing set. However, 
building rigorous domain ontology is laborious 
and time-consuming. Previous works suggest that 
ontology acquisition is an iterative process, which 
includes keyword collection and structure 
reorganization. The ontology is revised, refined, 
and accumulated by a human editor at each 
iteration (Noy and McGuinness, 2001). For 
example, in order to find a hyponym of a keyword, 
the human editor must observe sentences 
containing this keyword and its related hyponyms 
(Hearst, 1992). The editor then deduces rules for 
finding more hyponyms of this keyword. At each 
iteration the editor refines the rules to obtain better 
quality pairs of keyword-hyponyms. To speed up 
the above labor-intensive approach, semi-
automatic approaches have been designed in 
which a human editor only has to verify the results 
of the acquisition (Maedche and Staab, 2000).  
A knowledge representation framework, 
Information Map (InfoMap) in our previous work 
(Hsu et al, 2001), has been designed to integrate 
various linguistic, common-sense and domain 
knowledge. InfoMap is designed to perform 
natural language understanding, and applied to 
many application domains, such as question 
answering (QA), knowledge management and 
organization memory (Wu et al, 2002), and shows 
good results. An important characteristic of 
InfoMap is that it extracts events from a sentence 
by capturing the topic words, usually subject-verb 
pairs or hypernym-hyponym pairs, which are 
defined in the domain ontology.  
 
We shall review the InfoMap ontology 
framework in Section 2. The ontology acquisition 
process and extraction rules will be introduced in 
Section 3. We describe ontology-based text 
categorization in Section 4. Experimental results 
are reported in Section 5. We conclude our work 
in Section 6.  
2. Information Map 
InfoMap can serve as domain ontology as well as 
an inference engine. InfoMap is designed for NLP 
applications; its basic function is to identify the 
event structure of a sentence. We shall briefly 
describe InfoMap in this section. Figure 1 gives 
example ontology of the Central News Agency 
(CNA), the target in our experiment.  
2.1 InfoMap Structure Format 
As a domain ontology, InfoMap consists of 
domain concepts and their related sub-concepts 
such as categories, attributes, activities. The 
relationships of a concept and its associated sub-
concepts form a tree-like taxonomy. InfoMap also 
defines references to connect nodes from different 
branches which serves to integrate these 
hierarchical concepts into a network. InfoMap not 
only classifies concepts, but also connects the 
concepts by defining the relationships among them. 
 
Concept A
Category
Attribute
       Concept A'        
(Sub-concept of 
concept A)
   Concept B   
(relavant but not 
belong to concept A)
Action
      Concept  C      
(An activity of 
concept A)
Function node
Concept node
Legend
 
Figure 2. Skeleton of the Ontology Structure of 
InfoMap  
Figure 1. Ontology Structure for CNA News 
In InfoMap, concept nodes represent concepts 
and function nodes represent the relationships 
between concepts. The root node of a domain is 
the name of the domain. Following the root node, 
important topics are stored in a hierarchical order. 
These topics have sub-categories that list related 
sub-topics in a recursive fashion. Figure 1 is a 
partial view of the domain ontology of the CNA. 
Under each domain there are several topics and 
each topic might have sub-concepts and associated 
attributes. In this example, note that, the domain 
ontology is automatically acquired from a domain 
corpus, hence the quality is poor. Figure 2 shows 
the skeleton order of a concept using InfoMap.  
2.2 Event Structure 
Since concepts that are semantically related are 
often clustered together, one can use InfoMap to 
discern the main event structure in a natural 
language sentence. The process of identifying the 
event structure, we call a firing mechanism, which 
matches words in a sentence to both concepts and 
relationships in InfoMap. 
Suppose keywords of concept A and its sub-
concept B (or its hyponyms) appear in a sentence. 
It is likely that the author is describing an event ?B 
of A?. For example, when the words ?tire? and 
?car? appear in a sentence, normally this sentence 
would be about the tire of a car (not tire in the 
sense of fatigue). Therefore, a word-pair with a 
semantic relationship can give more concrete 
information than two words without a semantic 
relationship. Of course, certain syntactic 
constraints also need to be satisfied. This can be 
extended to a noun-verb pair or a combination of 
noun, verb and adjective. We call such words in a 
sentence an event structure. This mechanism 
seems to be especially effective for Chinese 
sentences. 
2.3 Domain Speculation 
With the help of domain ontologies, one can 
categorize a piece of text into a specific domain by 
categorizing each individual sentence within the 
text. There are many different ways to use domain 
ontology to categorize text. It can be used as a 
dictionary, as a keyword lists and as a structure to 
identify NL events. Take a single sentence for 
example. We first use InfoMap as a dictionary to 
do word segmentation (necessary for Chinese 
sentences) in which the ambiguity can be resolved 
by checking the domain topic in the ontology. 
After words are segmented, we can examine the 
distribution of these words in the ontology and 
effectively identify the densest cluster. Thus, we 
can use InfoMap to identify the domains of the 
sentences and their associated keywords. Section 
4.1 will further elaborate on this. 
3. Automatic Ontology Acquisition 
The automatically domain ontology acquisition 
from a domain corpus has three steps: 
1. Identify the domain keywords. 
2. Find the relative concepts. 
3. Merge the correlated activities. 
3.1 Domain Keyword Identification 
The first step of automatic domain ontology 
acquisition is to identify domain keywords. 
Identifying Chinese unknown words is difficult 
since the word boundary is not marked in Chinese 
corpus. According to an inspection of a 5 million 
word Chinese corpus (Chen et al, 1996), 3.51% of 
words are not listed in the CKIP lexicon (a 
Chinese lexicon with more than 80,000 entries). 
We use reoccurrence frequency and fan-out 
numbers to characterize words and their 
boundaries according to PAT-tree (Chien, 1999). 
We then adopt the TF/IDF classifier to choose 
domain keywords. The domain keywords serve as 
the seed topics in the ontology. We then apply 
SOAT to automatically obtain related concepts. 
3.2 SOAT 
To build the domain ontology for a new domain, 
we need to collect domain keywords and concepts 
by finding relationships among keywords. We 
adopt a semi-automatic domain ontology 
acquisition tool (SOAT, Wu et al, 2002), to 
construct a new ontology from a domain corpus. 
With a given domain corpus, SOAT can build a 
prototype of the domain ontology. 
InfoMap uses two major relationships among 
concepts: taxonomic relationships (category and 
synonym) and non-taxonomic relationships 
(attribute and action). SOAT defines rules, which 
consist of patterns of keywords and variables, to 
capture these relationships. The extraction rules in 
SOAT are morphological rules constructed from 
part-of-speech (POS) tagged phrase structure. 
Here we briefly introduce the SOAT process: 
Input: domain corpus with the POS tag 
Output: domain ontology prototype 
Steps: 
1 Select a keyword (usually the name of 
the domain) in the corpus as the seed to 
form a potential root set R 
2 Begin the following recursive process:  
2.1 Pick a keyword A as the root from R 
2.2 Find a new related keyword B of the 
root A by extraction rules and add it 
into the domain ontology according to 
the rules   
2.3 If there is no more related keywords, 
remove A from R 
2.4 Put B into the potential root set 
Repeat step 2 until either R becomes 
empty or the total number of nodes reach 
a threshold 
3.3 Morphological Rules 
To find the relative words of a keyword, we check 
the context in the sentence from which the 
keyword appears. We can then find attributes or 
hyponyms of the keyword. For example, in a 
sentence, we find a noun in front of a keyword 
(say, computer) may form a specific kind of 
concept (say, quantum computer). A noun (say, 
connector) followed by ?of? and a keyword may 
be an attribute of the keyword, (say, connector of 
computer). See (Wu et al, 2002) for details. 
3.4 Ontology Merging 
Ontologies can be created by merging different 
resources.  One NLP resource that we will merge 
into our domain ontology is the noun-verb event 
frame (NVEF) database (Tsai and Hsu, 2002). 
NVEF is a collection of permissible noun-verb 
sense-pairs that appear in general domain corpora. 
The noun will be the subject or object of the verb. 
This noun-verb sense-pair collection is domain 
independent. We can use nouns as domain 
keywords and find their correlated verbs. Adding 
these verbs into the domain ontology makes the 
ontology more suitable for NLP. The correlated 
verbs are added under the action function node. 
4. Ontology-Based Text Categorization 
To incorporate the domain ontology into a text 
categorization, we have to adjust both the training 
process and testing process. Section 4.1 describes 
how to make use of the ontology and the event 
structure during the training process. Section 4.2 
describes how to use ontology to perform domain 
speculation. Section 4.3 describes how to 
categorize news clippings. 
4.1 Feature and Threshold Selection 
With the event structure matched (fired) in the 
domain ontology, we have more features with 
which to index a text. To select useful features and 
a proper threshold, we apply Microsoft Decision 
Tree Algorithm to determine a path?s relevance as 
this algorithm can extract human interpretable 
rules (Soni et al, 2000). 
Features of the event structure include event 
structure score, node score, fired node level, and 
node type. During the training process, we record 
all features of the event structure fired by the news 
clippings in the domain-categorized training 
corpus. The decision tree shows that a threshold of 
0.85 is sufficient to evaluate event structure scores. 
We use event structure score to determine if the 
path is relevant. According to Figure 3, if the 
threshold of true probability is 85%, then the event 
structure score (Pathscore in the figure) should be 
65.75. And the relevance of a path p is true if p 
falls in a node on the decision tree whose ratio of true 
instance is greater than ? .  
 
 4.2 Domain Speculation  
The goal of domain speculation is to categorize a 
sentence S into a domain Dj according to the 
combined score of the keywords and the event 
structure in sentence S. We first calculate the 
similarity score of S and Dj. The keyword score 
and the event structure score are calculated 
independently.  
 
),(_*
),(_),(
SDScoretureEventStruc
SDScoreKeywordSDSimScore
j
jj
?
+=  
We use the TF/IDF classifier (Salton, 1989) to 
calculate the Keyword_Score of a sentence  as 
follows. First, we use a segmentation module to 
split a Chinese sentence into words. The TF/IDF 
classifier represents a domain as a weighted vector, 
Dj =( wj1, wj2,?, wjn), where n is the number of 
words in this domain and wk is the weight of word 
k. wk is defined as nfjk * idfjk, where nfjk is the term 
frequency (i.e., the number of times the word wk 
occurs in the domain j). Let DFk be the number of 
domains in which word k appears and |D| the total 
number of domains. idfk, the inverse document 
frequency, is given by:  
)||log(
k
k DF
Didf = . 
This weighting function assigns high values to 
domain-specific words, i.e. words which appear 
frequently in one domain and infrequently in 
others. Conversely, it will assign low weights to 
words appearing in many domains. The similarity 
between a domain j and a sentence represented by 
a vector Di is measured by the following cosine: 
??
?
==
==
=
n
k ik
n
k jk
n
k ikjk
ij
j
ww
ww
DDSim
SDScoreKeyword
1
2
1
2
1
)()(
),(
),(_  
The event structure score is calculated by 
InfoMap Engine. First, find all the nodes in 
ontology that match the words in the sentence. 
Then determine if there is any concept-attribute 
pair, or hypernym-hyponym pair. Finally, assign a 
score to each fired event structure according to the 
string length of words that match the nodes in the 
ontology. The selected event structure is the one 
with the highest score. 
))((
),(_
max SDkeywordsthStringLeng
SDScoretureEventStruc
j
Event
j
?= ?  
4.3 News Categorization 
Upon receiving a news clipping C, we split it into 
sentences Si. The sentences are scored and 
categorized according to domains.  Thus, every 
sentence has an individual score for each domain 
Score(D, Si). We add up these scores of every 
sentence in the text according to domain, giving us 
total domain scores for the entire text.  The 
domain which has the highest score is the domain 
into which the text is categorized. 
)),(()( maxarg ?
?
=
CS
i
D
SDScoreCDomain  
5. Refining Ontology through the Text 
Categorization Application 
The advantage of ontology compared to other 
implicit knowledge representation mechanism is 
that it can be read, interpreted and edited by 
human. Noise and errors can be detected and 
refined, especially for the automatically acquired 
ontology, in order to obtain a better ontology. 
Another advantage of allowing human editing is 
that the ontology produced can be shared by 
various applications, such as from a QA system to 
a knowledge management system. In contrast, the 
implicit knowledge represented in LSI or other 
representations is difficult to port from one 
application to another. 
Figure 3. Threshold selection using decision 
tree 
In this section, we show how the human 
editing feature improves news categorization. First, 
we can identify a common error type: ambiguity; 
then, depending on the degree of categorization 
ambiguity, the system can report to a human editor 
the possible errors of certain concepts in the 
domain ontology as clues. 
Consider the following common error type: 
event structure ambiguity. Some event structures 
are located in several domains due to the noise of 
training data. We define two formulas to find such 
event structures. The ambiguity of an event 
structure E(Si) is proportional to the number of 
domains in which it appears, and inversely 
proportional to its event score, where Si are the 
sentences that fire event E. 
GlobalCategorizationAmiguityFactor(E(Si) ) 
= number of domains fired by 
Si/average( EventScore(Si) ) 
We also measure the similarity between every 
two event structures by calculating the co-
occurrence multiplied by the global categorization 
ambiguity factor. 
GlobalCategorizationAmbiguityij (E i, E j) 
=Co-occurrence(E i, E j) * 
GlobalCategorizationAmbiguityFactor(E j) 
When the GlobalCategorizationAmbiguity of an 
event structure E i exceeds a threshold, the system 
will suggest that the human editor refine the 
ontology. 
6. Experiments 
To assess the power of domain identification of 
ontology, we test the text categorization ability on 
two different corpora. The ontology of the first 
experiment is edited manually; the ontology of the 
second experiment is automatically acquired. And 
we also conduct an experiment on the effect of 
human editing of the automatically acquired 
ontology. 
6.1 Single Sentence Test 
We test 9,143 sentences, edited manually for a QA 
system. The accuracy is 94%. These sentences are 
questions in the financial domain. Because the 
sentence topics are quite focused, the accuracy is 
very high. See Table 1. 
Table 1. Sentence Categorization Accuracy 
Domain # Sentence # Accuracy 
24 9143 94.01% 
6.2 News Clippings Collection 
The second experiment that we conduct is news 
categorization. We collect daily news from China 
News Agency (CNA) ranging from 1991 to 1999. 
Each news clipping is short with 352 Chinese 
characters (about 150 words) on the average. 
There are more than thirty domains and we choose 
10 major categories for the experiment. 
6.3 10 Categories News Categorization 
Our ten categories are: domestic arts and education 
(DD), foreign affairs (FA), finance report (FX), 
domestic health (HD), Taiwan local news (LD), 
Taiwan sports (LD), domestic military (MD), 
domestic politics (PD), Taiwan stock markets (SD), 
and weather report (WE). From each category, we 
choose the first 100 news clippings as the training 
set and the following 100 news clippings as the 
testing set. After data cleansing, the total training 
set has 979 news clippings, with 27,951 nodes and 
less than 10,000 distinct words. The training set 
for which domain ontologies are automatically 
acquired is shown in Table 2. A partial view of 
this ontology is in Figure 1. 
The result of text categorization based on this 
automatically acquired domain ontology is shown 
in Table 5, which contains the recall and precision 
for each domain. Note that, without the help of the 
event structure, the macro average f-score is 
85.16%. Even the total number of domain key 
concepts is less than 10,000 words (instead of 
100,000 words in standard dictionary), we can still 
obtain a good categorization result. With the help 
of event structure, the macro average f-score is 
85.55%.  
6.4 Human Editing 
To verify the refinement method, we conduct 
an experiment to compare the result of using 
automatically acquired domain ontology and that 
of limited human editing (on only one domain 
ontology). After the training process, we use 
domain ontologies to classify the training data, 
and to calculate the global categorization 
ambiguity factor formula in order to obtain 
ambiguous event structure pairs as candidates for 
human editing. For simplicity, we restrict the 
action of refinement to deletion. It takes a human 
editor one half day to finish the task and delete 
0.62% nodes (172 out of 27,951 nodes). In the 
testing phase, we select 928 new news clippings as 
the testing set. Table 3 shows the results from 
before and after human editing. Due to time 
constraints, we only edit the part of the ontology 
that might affect domain DD. The recall and 
precision of domain DD increase as well as both 
the average recall and average precision. In 
addition, the recall of domains having higher 
correlation with DD, such as PD and FA, 
decreases. Apparently, the event structures that 
mislead the categorization system to theses 
domain have mostly been deleted. The experiment 
result is very consistent with our intuition. 
Table 2. Ten Category training set CNA news 
Training set size 
Domain 
Doc# Char# 
DD 98 41870 
FA 97 38143 
FX 100 30771 
HD 96 39818 
JD 107 35381 
LD 96 36957 
MD 89 32903 
PD 100 43152 
SD 109 33030 
WE 87 30457 
total 979 362,482 
7. Discussions and Conclusions 
Compared to an ordinary n-gram dictionary, our 
ontology dictionary is quite small (roughly 10%) 
but records certain important relations between 
keywords.  
Our goal is to generate rules that are human 
readable via ontology. The experiment result 
shows that event structure enhances text 
categorization, even when the domain ontology is 
automatically acquired without human verification. 
To improve our ontological approach, our future 
work are: 1. human editing in more domains; 2. 
enlarge our dictionary by merging existing 
ontologies, e.g., the names of countries, capitals 
and important persons, which are absent from the 
training corpus; 3. incorporate more sense pairs 
such as N-A (noun-adjective), Adv-V (adverb-
verb); 4. use machine learning model on the 
weighting of the ontological features. 
Previous research shows that some NLP 
techniques can improve information retrieval. 
Ontology-based IR is one of them. However, the 
construction of domain ontology is too costly. 
Thus, automatic acquisition of domain ontology is 
becoming an interesting research topic. Previous 
research shows that implicit rules (such as LSI, N-
gram dictionaries) learned from a training corpus 
give better results than explicit rules generated by 
humans. However, it is hard to use these implicit 
rules or to combine them with other resources for 
further refinement. With the help of domain 
ontology, we can automatically generate rules that 
humans can understand. Since humans and 
machines can maintain ontology independently, 
the ontological approach can be applied more 
easily to other IR applications. Ontologies from 
different sources can be merged into the domain 
ontology. The system should include an editing 
interface that human thoughts can be incorporated 
to complement statistical rules. With semi-
automatically acquired domain ontology, text 
categorization can be adapted to personal 
preferences.  
8. References 
Chen, K.J., C.R. Huang, L.P. Chang & H.L. Hsu, 
SINICA CORPUS: Design Methodology for 
Balanced Corpora, in Proceedings of PACLIC 
11th Conference, pp.167-176, 1996. 
Chien, L.F., PAT-tree-based Adaptive keyphrase 
extraction for Intelligent Chinese Information 
Retrieval, Information Processing and 
Management, Vol. 35, pp. 501-521, 1999. 
Gruber, T.R. (1993), A translation approach to 
portable ontologies. Knowledge Acquisition, 
5(2), pp. 199-220, 1993. 
Hearst, M.A. (1992), Automatic acquisition of 
hyponyms from large text corpora. In 
COLING-92, pp. 539-545. 
Hsu, W.L., Wu, S.H. and Chen, Y.S., Event 
Identification Based On The Information Map - 
INFOMAP, in Natural Language Processing 
and Knowledge Engineering Symposium of the 
IEEE Systems, Man, and Cybernetics 
Conference, Tucson, Arizona, USA, 2001. 
Maedche, A. and Staab, S. (2000), Discovering 
Conceptual Relationships from Text. In: Horn, 
W. (ed.): ECAI 2000. Proceedings of the 14th 
European Conference on Artificial Intelligence, 
IOS Press, Amsterdam. 
Noy, N.F. and McGuinness D.L. (2001), Ontology 
Development 101: A Guide to Creating Your 
First Ontology, SMI technical report SMI-
2001-0880, Stanford Medical Informatics. 
Salton, G., Automatic Text Processing, Addison-
Wesley, Massachusetts, 1989. 
Soni, S, Tang, Z. and Yang, J., ?Microsoft 
Performance Study of Microsoft Data Mining 
Algorithms?, UniSys, 2000/12. 
Tsai, J.L. and Hsu, W.L., ?Applying an NVEF 
Word-Pair Identifier to the Chinese Syllable-to-
Word Conversion Problem,? COLING-02, 
Taipei, ACM press, 2002. 
Wu, S.H. and Hsu, W.L., SOAT: A Semi-
Automatic Domain Ontology Acquisition Tool 
from Chinese Corpus, COLING-02, Taipei, 
ACM press, 2002. 
Wu, S.H., Day, M.Y., Tsai, T.H. and Hsu, W.L., 
FAQ-centered Organizational Memory, in 
Nada Matta and Rose Dieng-Kuntz (ed.), 
Knowledge Management and Organizational 
Memories, Kluwer Academic Publishers, 
Boston, 2002. 
Wu, S.H., Yang, P.C. and Soo, V.W., An 
Assessment on Character-based Chinese News 
Filtering Using Latent Semantic Indexing, 
Computational Linguistics & Chinese 
Language Processing, Vol. 3, no.2, August 
1998. 
Table 3. Experiment result of CNA news categorization 
# of nodes 
automatically 
acquired 
#of nodes  
deleted in 
human editing 
TF/IDF(baseline)
TF/IDF+Event 
Structure(first 
improvement) 
TF/IDF+Event Structure 
with Human Editing 
(second improvement) 
The different between 
(second improvement) and 
(first improvement) Domain 
Before  After   #  % P% R% F% P% R% F% P% R% F% P+% R+% F+% 
DD 4616 4574 42 0.91 72.90 
82.9
8 
77.6
1 74.04 81.91 77.78 74.29 82.98 78.39 0.25 1.07 0.61
FA 8352 8348 4 0.05 75.83 
94.7
9 
84.2
6 71.32 95.83 81.78 76.67 95.83 85.19 5.35 0.00 3.41
FX 44 44 0 0.00 100 100 100 100 100 100 100 100 100 0.00 0.00 0.00
HD 3357 3348 9 0.27 78.79 
88.6
4 
83.4
2 80.21 87.50 83.70 78.79 88.64 83.42 -1.42 1.14 -0.28
JD 1854 1846 8 0.43 88 71.74 
79.0
4 87.18 73.91 80 87.84 70.65 78.31 0.66 -3.26 -1.69
LD 2925 2831 94 3.21 87.64 
80.4
1 
83.8
7 90.36 77.32 83.33 88.51 79.38 83.70 -1.85 2.06 0.37
MD 2010 1999 11 0.55 95.59 
66.3
3 
78.3
1 95.71 68.37 79.76 97.26 72.45 83.04 1.55 4.08 3.28
PD 3199 3195 4 0.13 65.81 
68.7
5 
67.2
5 70.43 72.32 71.37 66.67 69.64 68.12 -3.76 -2.68 -3.25
SD 585 585 0 0.00 100 100 100 100 100 100 100 100 100 0.00 0.00 0.00
WE 1009 1009 0 0.00 95.74 100 
97.8
3 95.74 100 97.83 95.74 100 97.83 0.00 0.00 0.00
Total 27951 27779 172 0.62                   
Macro  
Average         
86.0
3 
85.3
6 
85.1
6 86.50 85.72 85.55 86.58 85.96 85.80 0.08 0.24 0.25
 
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 25?28,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Capturing Errors in Written Chinese Words 
Chao-Lin Liu1 Kan-Wen Tien2 Min-Hua Lai3 Yi-Hsuan Chuang4 Shih-Hung Wu5
1-4National Chengchi University, 5Chaoyang University of Technology, Taiwan 
{1chaolin, 296753027, 395753023, 494703036}@nccu.edu.tw, 5shwu@cyut.edu.tw 
 
Abstract 
A collection of 3208 reported errors of Chinese 
words were analyzed. Among which, 7.2% in-
volved rarely used character, and 98.4% were 
assigned common classifications of their causes 
by human subjects. In particular, 80% of the er-
rors observed in writings of middle school stu-
dents were related to the pronunciations and 
30% were related to the compositions of words. 
Experimental results show that using intuitive 
Web-based statistics helped us capture only 
about 75% of these errors. In a related task, the 
Web-based statistics are useful for recommend-
ing incorrect characters for composing test items 
for "incorrect character identification" tests 
about 93% of the time. 
1 Introduction 
Incorrect writings in Chinese are related to our under-
standing of the cognitive process of reading Chinese 
(e.g., Leck et al, 1995), to our understanding of why 
people produce incorrect characters and our offering 
corresponding remedies (e.g., Law et al, 2005), and 
to building an environment for assisting the prepara-
tion of test items for assessing students? knowledge of 
Chinese characters (e.g., Liu and Lin, 2008). 
Chinese characters are composed of smaller parts 
that can carry phonological and/or semantic informa-
tion. A Chinese word is formed by Chinese characters. 
For example, ??? (Singapore) is a word that con-
tains three Chinese characters. The left (?) and the 
right (?) part of ?, respectively, carry semantic and 
phonological information. Evidences show that pro-
duction of incorrect characters are related to either 
phonological or the semantic aspect of the characters. 
In this study, we investigate several issues that are 
related to incorrect characters in Chinese words. In 
Section 2, we present the sources of the reported er-
rors. In Section 3, we analyze the causes of the ob-
served errors. In Section 4, we explore the effective-
ness of relying on Web-based statistics to correct the 
errors. The current results are encouraging but de-
mand further improvements. In Section 5, we employ 
Web-based statistics in the process of assisting teach-
ers to prepare test items for assessing students? 
knowledge of Chinese characters. Experimental re-
sults showed that our method outperformed the one 
reported in (Liu and Lin, 2008), and captured the best 
candidates for incorrect characters 93% of the time. 
2 Data Sources 
We obtained data from three major sources. A list that 
contains 5401 characters that have been believed to be 
sufficient for everyday lives was obtained from the 
Ministry of Education (MOE) of Taiwan, and we call 
the first list the Clist, henceforth. We have two lists of 
words, and each word is accompanied by an incorrect 
way to write certain words. The first list is from a 
book published by MOE (MOE, 1996). The MOE 
provided the correct words and specified the incorrect 
characters which were mistakenly used to replace the 
correct characters in the correct words. The second 
list was collected, in 2008, from the written essays of 
students of the seventh and the eighth grades in a 
middle school in Taipei. The incorrect words were 
entered into computers based on students? writings, 
ignoring those characters that did not actually exist 
and could not be entered.  
We will call the first list of incorrect words the 
Elist, and the second the Jlist from now on. Elist and 
Jlist contain, respectively, 1490 and 1718 entries. 
Each of these entries contains a correct word and the 
incorrect character. Hence, we can reconstruct the 
incorrect words easily. Two or more different ways to 
incorrectly write the same words were listed in differ-
ent entries and considered as two entries for simplic-
ity of presentation. 
3 Error Analysis of Written Words 
Two subjects, who are native speakers of Chinese and 
are graduate students in Computer Science, examined 
Elist and Jlist and categorized the causes of errors. 
They compared the incorrect characters with the cor-
rect characters to determine whether the errors were 
pronunciation-related or semantic-related. Referring 
to an error as being ?semantic-related? is ambiguous. 
Two characters might not contain the same semantic 
part, but are still semantically related. In this study, 
we have not considered this factor. For this reason we 
refer to the errors that are related to the sharing of 
semantic parts in characters as composition-related. 
It is interesting to learn that native speakers had a 
high consensus about the causes for the observed er-
rors, but they did not always agree. Hence, we studied 
the errors that the two subjects had agreed categoriza-
tions. Among the 1490 and 1718 words in Elist and 
Jlist, respectively, the two human subjects had con-
sensus over causes of 1441 and 1583 errors.  
The statistics changed when we disregarded errors 
that involved characters not included in Clist. An er-
ror would be ignored if either the correct or the incor-
rect character did not belong to the Clist. It is possible 
for students to write such rarely used characters in an 
incorrect word just by coincidence. 
After ignoring the rare characters, there were 1333 
and 1645 words in Elist and Jlist, respectively. The 
subjects had consensus over the categories for 1285 
25
and 1515 errors in Elist and Jlist, respectively.  
Table 1 shows the percentages of five categories of 
errors: C for the composition-related errors, P for the 
pronunciation-related errors, C&P for the intersection 
of C and P, NE for those errors that belonged to nei-
ther C nor P, and D for those errors that the subjects 
disagreed on the error categories. There were, respec-
tively, 505 composition-related and 1314 pronuncia-
tion-related errors in Jlist, so we see 30.70% 
(=505/1645) and 79.88% (=1314/1645) in the table. 
Notice that C&P represents the intersection of C and 
P, so we have to deduct C&P from the sum of C, P, 
NE, and D to find the total probability, namely 1. 
It is worthwhile to discuss the implication of the 
statistics in Table 1. For the Jlist, similarity between 
pronunciations accounted for nearly 80% of the errors, 
and the ratio for the errors that are related to composi-
tions and pronunciations is 1:2.6. In contrast, for the 
Elist, the corresponding ratio is almost 1:1. The Jlist 
and Elist differed significantly in the ratios of the er-
ror types. It was assumed that the dominance of pro-
nunciation-related errors in electronic documents was 
a result of the popularity of entering Chinese with 
pronunciation-based methods. The ratio for the Jlist 
challenges this popular belief, and indicates that even 
though the errors occurred during a writing process, 
rather than typing on computers, students still pro-
duced more pronunciation-related errors than compo-
sition-related errors. Distribution over error types is 
not as related to input method as one may have be-
lieved. Nevertheless, the observation might still be a 
result of students being so used to entering Chinese 
text with pronunciation-based method that the organi-
zation of their mental lexicons is also pronunciation 
related. The ratio for the Elist suggests that editors of 
the MOE book may have chosen the examples with a 
special viewpoint in their minds ? balancing the errors 
due to pronunciation and composition. 
4 Reliability of Web-based Statistics  
In this section, we examine the effectiveness of using 
Web-based statistics to differentiate correct and incor-
rect characters. The abundant text material on the 
Internet gives people to treat the Web as a corpus (e.g., 
webascorpus.org). When we send a query to Google, 
we will be informed of the number of pages (NOPs) 
that possibly contain relevant information. If we put 
the query terms in quotation marks, we should find 
the web pages that literally contain the query terms. 
Hence, it is possible for us to compare the NOPs for 
two competing phrases for guessing the correct way 
of writing. At the time of this writing, Google found 
107000 and 3220 pages, respectively, for ?strong tea? 
and ?powerful tea?. (When conducting such advanced 
searches with Google, the quotation marks are needed 
to ensure the adjacency of individual words.) Hence, 
?strong? appears to be a better choice to go with ?tea?. 
How does this strategy serve for learners of Chinese? 
We verified this strategy by sending the words in 
both the Elist and the Jlist to Google to find the NOPs. 
We can retrieve the NOPs from the documents re-
turned by Google, and compare the NOPs for the cor-
rect and the incorrect words to evaluate the strategy. 
Again, we focused on those in the 5401 words that the 
human subjects had consensus about their error types. 
Recall that we have 1285 and 1515 such words in 
Elist and Jlist, respectively. As the information avail-
able on the Web changes all the time, we also have to 
note that our experiments were conducted during the 
first half of March 2009. The queries were submitted 
at reasonable time intervals to avoid Google?s treating 
our programs as malicious attackers. 
Table 2 shows the results of our investigation. We 
considered that we had a correct result when we found 
that the NOP for the correct word larger than the NOP 
for the incorrect word. If the NOPs were equal, we 
recorded an ambiguous result; and when the NOP for 
the incorrect word is larger, we recorded an incorrect 
event. We use ?C?, ?A?, and ?I? to denote ?correct?, 
?ambiguous?, and ?incorrect? events in Table 2.  
The column headings of Table 2 show the setting 
of the searches with Google and the set of words that 
were used in the experiments. We asked Google to 
look for information from web pages that were en-
coded in traditional Chinese (denoted Trad). We 
could add another restriction on the source of infor-
mation by asking Google to inspect web pages from 
machines in Taiwan (denoted Twn+Trad). We were 
not sure how Google determined the languages and 
locations of the information sources, but chose to trust 
Google. The headings ?Comp? and ?Pron? indicate 
whether the words whose error types were composi-
tion and pronunciation-related, respectively.  
Table 2 shows eight distributions, providing ex-
perimental results that we observed under different 
settings. The distribution printed in bold face showed 
that, when we gathered information from sources that 
were encoded in traditional Chinese, we found the 
correct words 73.12% of the time for words whose 
error types were related to composition in Elist. Under 
the same experimental setting, we could not judge the 
correct word 4.58% of the time, and would have cho-
sen an incorrect word 22.30% of the time. 
Statistics in Table 2 indicate that web statistics is 
not a very reliable factor to judge the correct words. 
The average of the eight numbers in the ?C? rows is 
only 71.54% and the best sample is 76.59%, suggest-
Table 2. Reliability of Web-based statistics 
Trad Twn+Trad  
Comp Pron Comp Pron 
C 73.12% 73.80% 69.92% 68.72%
A 4.58% 3.76% 3.83% 3.76%
E
list 
I 22.30% 22.44% 26.25% 27.52%
C 76.59% 74.98% 69.34% 65.87%
A 2.26% 3.97% 2.47% 5.01%
Jlist 
I 21.15% 21.05% 28.19% 29.12%
Table 1. Error analysis for Elist and Jlist 
 C P C&P NE D 
Elist 66.09% 67.21% 37.13% 0.23% 3.60%
Jlist 30.70% 79.88% 20.91% 2.43% 7.90%
26
ing that we did not find the correct words frequently. 
We would made incorrect judgments 24.75% of the 
time. The statistics also show that it is almost equally 
difficult to find correct words for errors that are com-
position and pronunciation related. In addition, the 
statistics reveal that choosing more features in the 
advanced search affected the final results. Using 
?Trad? offered better results in our experiments than 
using ?Twn+Trad?. This observation may arouse a 
perhaps controversial argument. Although Taiwan has 
proclaimed to be the major region to use traditional 
Chinese, their web pages might not have used as ac-
curate Chinese as web pages located in other regions. 
We have analyzed the reasons for why using Web-
based statistics did not find the correct words. Fre-
quencies might not have been a good factor to deter-
mine the correctness of Chinese. However, the myriad 
amount of data on the Web should have provided a 
better performance. Google?s rephrasing our submit-
ted queries is an important factor, and, in other cases, 
incorrect words were more commonly used. 
5 Facilitating Test Item Authoring 
Incorrect character correction is a very popular type of 
test in Taiwan. There are simple test items for young 
children, and there are very challenging test items for 
the competitions among adults. Finding an attractive 
incorrect character to replace a correct character to 
form a test item is a key step in authoring test items.  
We have been trying to build a software environ-
ment for assisting the authoring of test items for in-
correct character correction (Liu and Lin, 2008, Liu et 
al., 2009). It should be easy to find a lexicon that con-
tains pronunciation information about Chinese charac-
ters. In contrast, it might not be easy to find visually 
similar Chinese characters with computational meth-
ods. We expanded the original Cangjie codes (OCC), 
and employed the expanded Cangjie codes (ECC) to 
find visually similar characters (Liu and Lin, 2008).  
With a lexicon, we can find characters that can be 
pronounced in a particular way. However, this is not 
enough for our goal. We observed that there were 
different symptoms when people used incorrect char-
acters that are related to their pronunciations. They 
may use characters that could be pronounced exactly 
the same as the correct characters. They may also use 
characters that have the same pronunciation and dif-
ferent tones with the correct character. Although rela-
tively infrequently, people may use characters whose 
pronunciations are similar to but different from the 
pronunciation of the correct character.  
As Liu and Lin (2008) reported, replacing OCC 
with ECC to find visually similar characters could 
increase the chances to find similar characters. Yet, it 
was not clear as to which components of a character 
should use ECC. 
5.1 Formalizing the Extended Cangjie Codes 
We analyzed the OCCs for all the words in Clist to 
determine the list of basic components. We treated a 
Cangjie basic symbol as if it was a word, and com-
puted the number of occurrences of n-grams based on 
the OCCs of the words in Clist. Since the OCC for a 
character contains at most five symbols, the longest n-
grams are 5-grams. Because the reason to use ECC 
was to find common components in characters, we 
disregarded n-grams that repeated no more than three 
times. In addition, the n-grams that appeared more 
than three times might not represent an actual compo-
nent in Chinese characters. Hence, we also removed 
such n-grams from the list of our basic components. 
This process naturally made our list include radicals 
that are used to categorize Chinese characters in typi-
cal printed dictionaries. The current list contains 794 
components, and it is possible to revise the list of ba-
sic components in our work whenever necessary. 
After selecting the list of basic components with 
the above procedure, we encoded the words in Elist 
with our list of basic components. We adopted the 12 
ways that Liu and Lin (2008) employed to decompose 
Chinese characters. There are other methods for de-
composing Chinese characters into components. 
Juang et al (2005) and the research team at the Sinica 
Academia propose 13 different ways for decomposing 
characters. 
5.2 Recommending Incorrect Alternatives 
With a dictionary that provides the pronunciation of 
Chinese characters and the improved ECC encodings 
for words in the Elist, we can create lists of candidate 
characters for replacing a specific correct character in 
a given word to create a test item for incorrect charac-
ter correction.  
There are multiple strategies to create the candidate 
lists. We may propose the candidate characters be-
cause their pronunciations have the same sound and 
the same tone with those of the correct character (de-
noted SSST). Characters that have same sounds and 
different tones (SSDT), characters that have similar 
sounds and same tones (MSST), and characters that 
have similar sounds and different tones (MSDT) can 
be considered as candidates as well. It is easy to judge 
whether two Chinese characters have the same tone. 
In contrast, it is not trivial to define ?similar? sound. 
We adopted the list of similar sounds that was pro-
vided by a psycholinguistic researcher (Dr. Chia-Ying 
Lee) at the Sinica Academia. 
In addition, we may propose characters that look 
similar to the correct character. Two characters may 
look similar for two reasons. They may contain the 
same components, or they contain the same radical 
and have the same total number of strokes (RS). 
When two characters contain the same component, the 
shared component might or might not locate at the 
same position within the bounding boxes of characters.  
In an authoring tool, we could recommend a lim-
ited number of candidate characters for replacing the 
correct character. We tried two strategies to compare 
and choose the visually similar characters. The first 
strategy (denoted SC1) gave a higher score to the 
shared component that located at the same location in 
the two characters being compared. The second strat-
27
egy (SC2) gave the same score to any shared compo-
nent even if the component did not reside at the same 
location in the characters. When there were more than 
20 characters that receive nonzero scores, we chose to 
select at most 20 characters that had leading scores as 
the list of recommended characters. 
5.3 Evaluating the Recommendations 
We examined the usefulness of these seven categories 
of candidates with errors in Elist and Jlist. The first 
set of evaluation (the inclusion tests) checked only 
whether the lists of recommended characters con-
tained the incorrect character in our records. The sec-
ond set of evaluation (the ranking tests) was designed 
for practical application in computer assisted item 
generation. Only for those words whose actual incor-
rect characters were included in the recommended list, 
we replaced the correct characters in the words with 
the candidate incorrect characters, submitted the in-
correct words to Google, and ordered the candidate 
characters based on their NOPs. We then recorded the 
ranks of the incorrect characters among all recom-
mended characters.  
Since the same character may appear simultane-
ously in SC1, SC2, and RS, we computed the union of 
these three sets, and checked whether the incorrect 
characters were in the union. The inclusion rate is 
listed under Comp. Similarly, we computed the union 
for SSST, SSDT, MSST, and MSDT, checked whether 
the incorrect characters were in the union, and re-
corded the inclusion rate under Pron. Finally, we 
computed the union of the lists created by the seven 
strategies, and recorded the inclusion rate under Both. 
The second and the third rows of Table 3 show the 
results of the inclusion tests. The data show the per-
centage of the incorrect characters being included in 
the lists that were recommended by the seven strate-
gies. Notice that the percentages were calculated with 
different denominators. The number of composition-
related errors was used for SC1, SC2, RS, and Comp 
(e.g. 505 that we mentioned in Section 3 for the Jlist); 
the number of pronunciation-related errors for SSST, 
SSDT, MSST, MSDT, and Pron (e.g., 1314 mentioned 
in Section 3 for the Jlist); the number of either of 
these two errors for Both (e.g., 1475 for Jlist).  
The results recorded in Table 3 show that we were 
able to find the incorrect character quite effectively, 
achieving better than 93% for both Elist and Jlist. The 
statistics also show that it is easier to find incorrect 
characters that were used for pronunciation-related 
problems. Most of the pronunciation-related problems 
were misuses of characters that had exactly the same 
pronunciations with the correct characters. Unex-
pected confusions, e.g., those related to pronuncia-
tions in Chinese dialects, were the main for the failure 
to capture the pronunciation-related errors. SSDT is a 
crucial complement to SSST. There is still room to 
improve our methods to find confusing characters 
based on their compositions. We inspected the list 
generated by SC1 and SC2, and found that, although 
SC2 outperformed SC1 on the inclusion rate, SC1 and 
SC2 actually generated complementary lists and 
should be used together. The inclusion rate achieved 
by the RS strategy was surprisingly high.  
The fourth and the fifth rows of Table 3 show the 
effectiveness of relying on Google to rank the candi-
date characters for recommending an incorrect charac-
ter. The rows show the average ranks of the included 
cases. The statistics show that, with the help of 
Google, we were able to put the incorrect character on 
top of the recommended list when the incorrect char-
acter was included.  This allows us to build an envi-
ronment for assisting human teachers to efficiently 
prepare test items for incorrect character identification. 
6 Summary  
The analysis of the 1718 errors produced by real stu-
dents show that similarity between pronunciations of 
competing characters contributed most to the ob-
served errors. Evidences show that the Web statistics 
are not very reliable for differentiating correct and 
incorrect characters. In contrast, the Web statistics are 
good for comparing the attractiveness of incorrect 
characters for computer assisted item authoring.  
Acknowledgements 
This research has been funded in part by the National 
Science Council of Taiwan under the grant NSC-97-
2221-E-004-007-MY2. We thank the anonymous re-
viewers for invaluable comments, and more responses 
to the comments are available in (Liu et al 2009). 
References  
D. Juang, J.-H. Wang, C.-Y. Lai, C.-C. Hsieh, L.-F. Chien, 
J.-M. Ho. 2005. Resolving the unencoded character 
problem for Chinese digital libraries, Proc. of the 5th 
ACM/IEEE Joint Conf. on Digital Libraries, 311?319. 
S.-P. Law, W. Wong, K. M. Y. Chiu. 2005. Whole-word 
phonological representations of disyllabic words in the 
Chinese lexicon: Data from acquired dyslexia, Behav-
ioural Neurology, 16, 169?177. 
K. J. Leck, B. S. Weekes, M. J. Chen. 1995. Visual and 
phonological pathways to the lexicon: Evidence from 
Chinese readers, Memory & Cognition, 23(4), 468?476. 
C.-L. Liu et al 2009. Phonological and logographic influ-
ences on errors in written Chinese words, Proc. of the 7th 
Workshop on Asian Language Resources, 47th ACL. 
C.-L. Liu, J.-H. Lin. 2008. Using structural information for 
identifying similar Chinese characters, Proc. of the 46th 
ACL, short papers, 93?96. 
MOE. 1996. Common Errors in Chinese Writings (???
???), Ministry of Education, Taiwan. 
Table 3. Incorrect characters were contained and ranked high in the recommended lists 
 SC1 SC2 RS SSST SSDT MSST MSDT Comp Pron Both 
Elist 73.92% 76.08% 4.08% 91.64% 18.39% 3.01% 1.67% 81.97% 99.00% 93.37% 
Jlist 67.52% 74.65% 6.14% 92.16% 20.24% 4.19% 3.58% 77.62% 99.32% 97.29% 
Elist 3.25 2.91 1.89 2.30 1.85 2.00 1.58 
Jlist 2.82 2.64 2.19 3.72 2.24 2.77 1.16 
28
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 84?91,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Phonological and Logographic Influences on Errors in Written 
Chinese Words 
Chao-Lin Liu1 Kan-Wen Tien2 Min-Hua Lai3 Yi-Hsuan Chuang4 Shih-Hung Wu5
1-4National Chengchi University, 5Chaoyang University of Technology, Taiwan 
{1chaolin, 296753027, 395753023, 494703036}@nccu.edu.tw, 5shwu@cyut.edu.tw 
 
Abstract 
We analyze a collection of 3208 reported errors 
of Chinese words. Among these errors, 7.2% in-
volved rarely used character, and 98.4% were 
assigned common classifications of their causes 
by human subjects. In particular, 80% of the er-
rors observed in the writings of middle school 
students were related to the pronunciations and 
30% were related to the logographs of the words. 
We conducted experiments that shed light on us-
ing the Web-based statistics to correct the errors, 
and we designed a software environment for pre-
paring test items whose authors intentionally re-
place correct characters with wrong ones. Ex-
perimental results show that using Web-based 
statistics can help us correct only about 75% of 
these errors. In contrast, Web-based statistics are 
useful for recommending incorrect characters for 
composing test items for ?incorrect character 
identification? tests about 93% of the time. 
1 Introduction 
Incorrect writings in Chinese are related to our under-
standing of the cognitive process of reading Chinese 
(e.g., Leck et al, 1995), to our understanding of why 
people produce incorrect characters and our offering 
corresponding remedies (e.g., Law et al, 2005), and 
to building an environment for assisting the prepara-
tion of test items for assessing students? knowledge of 
Chinese characters (e.g., Liu and Lin, 2008). 
Chinese characters are composed of smaller parts 
that can carry phonological and/or semantic informa-
tion. A Chinese word is formed by Chinese characters. 
For example, ??? (Singapore) is a word that con-
tains three Chinese characters. The left (?) and the 
right (?) part of ?, respectively, carry semantic and 
phonological information. The semantic information, 
in turn, is often related to the logographs that form the 
Chinese characters. Evidences show that production 
of incorrect characters are related to phonological, 
logographic, or the semantic aspect of the characters. 
Although the logographs of Chinese characters can be 
related to the lexical semantics, not all errors that are 
related to semantics were caused by the similarity in 
logographs. Some were due to the context of the 
words and/or permissible interpretations of different 
words.  
In this study, we investigate issues that are related 
to the phonological and logographical influences on 
the occurrences of incorrect characters in Chinese 
words. In Section 2, we present the details about the 
sources of the reported errors. We have collected er-
rors from a published book and from a group of mid-
dle school students. In Section 3, we analyze the 
causes of the observed errors. Native speakers of Chi-
nese were asked to label whether the observed errors 
were related to the phonological or the logographic 
reasons. In Section 4, we explore the effectiveness of 
relying on Web-based statistics to correct the errors. 
We submitted an incorrect word and a correct word 
separately to Google to find the number of web pages 
that contained these words. The correct and incorrect 
words differed in just the incorrect character. We ex-
amine whether the number of web pages that con-
tained the words can help us find the correct way of 
writing. In Section 5, we employ Web-based statistics 
in the process of assisting teachers to prepare test 
items for assessing students? knowledge of Chinese 
characters. Experimental results showed that our 
method outperformed the one reported in (Liu and Lin, 
2008), and captured the incorrect characters better 
than 93% of the time. 
2 Data Sources 
We obtained data from three major sources. A list that 
contains 5401 characters that have been believed to be 
sufficient for everyday lives was obtained from the 
Ministry of Education (MOE) of Taiwan, and we call 
the first list the Clist, henceforth. The 5401 characters 
form the core basis for the BIG-5 code, and an official 
introduction of these 5401 characters is available at 
http://www.cns11643.gov.tw/AIDB/encodings.do#encode4.  
We have two lists of words, and each word is ac-
companied by an incorrect way to write the word. The 
first list is from a book published by MOE (1996). 
The MOE provided the correct words and specified 
the incorrect characters which were mistakenly used 
to replace the correct characters in the correct words. 
The second list was collected, in 2008, from the writ-
ten essays of students of the seventh and the eighth 
grades in a middle school in Taipei. The incorrect 
characters were entered into computers based on stu-
dents? writings, ignoring those characters that did not 
actually exist and could not be entered.  
We will call the first list of word the Elist, and the 
second the Jlist from now on. Elist and Jlist contain, 
respectively, 1490 and 1718 entries. Each of these 
entries contains a correct word and the incorrect char-
acter. Hence, we can reconstruct the incorrect words 
84
easily. Two or more different ways to incorrectly 
write the same words were listed in different entries 
and considered as two entries for simplicity of presen-
tation. 
3 Error Analysis of Written Words 
Two human subjects, who are native speakers of Chi-
nese and are graduate students in Computer Science, 
examined Elist and Jlist and categorized the causes of 
errors. They compared the incorrect characters with 
the correct characters to determine whether the errors 
were pronunciation-related or logographs-related. 
Referring to an error as being ?semantics-related? is 
ambiguous. Two characters might not contain the 
same semantic part, but are still semantically related, 
e.g., misusing ???(tou1) for ???(tou2) in ????
??. In this study, we have not considered this factor. 
For this reason we refer to the errors that are related to 
the sharing of logographic parts in characters as com-
position-related. 
Among the 1490 and 1718 words in Elist and Jlist, 
respectively, the two human subjects had consensus 
over causes of 1441 and 1583 errors. It is interesting 
to learn that native speakers had a high consensus 
about the causes for the observed errors, but they did 
not always agree. To have a common standard in 
comparison, we studied the errors that the two sub-
jects had agreed categorizations.  
The statistics changed when we disregarded errors 
that involved characters not included in Clist. An er-
ror would be ignored if either the correct or the incor-
rect character did not belong to the Clist. It is possible 
for students to write such rarely used characters in an 
incorrect word just by coincidence.  
After ignoring the rare characters, there were 1333 
and 1645 words in Elist and Jlist, respectively. The 
subjects had consensus over the causes of errors for 
1285 and 1515 errors in Elist and Jlist, respectively.  
Table 1 shows the percentages of five categories of 
errors: C for the composition-related errors, P for the 
pronunciation-related errors, C&P for the intersection 
of C and P, NE for those errors that belonged to nei-
ther C nor P, and D for those errors that the subjects 
disagreed on the error categories. There were, respec-
tively, 505 composition-related and 1314 pronuncia-
tion-related errors in Jlist, so we see 
505/1645=30.70% and 1314/1645=79.88% in the 
table. Notice that C&P represents the intersection of 
C and P, so we have to deduct C&P from the sum of 
C, P, NE, and D to find the total probability, namely 1. 
It is worthwhile to discuss the implication of the 
statistics in Table 1. For the Jlist, similarity between 
pronunciations accounted for nearly 80% of the errors, 
and the ratio for the errors that are related to composi-
tions and pronunciations is 1:2.6. In contrast, for the 
Elist, the corresponding ratio is almost 1:1. The Jlist 
and Elist differed significantly in the ratios of the er-
ror types. It was assumed that the dominance of pro-
nunciation-related errors in electronic documents was 
a result of the popularity of entering Chinese with 
pronunciation-based methods. The ratio for the Jlist 
challenges this popular belief, and indicates that even 
though the errors occurred during a writing process, 
rather than typing on computers, students still pro-
duced more pronunciation-related errors than compo-
sition-related errors. Distribution over error types is 
not as related to input method as one may have be-
lieved. Nevertheless, the observation might still be a 
result of students being so used to entering Chinese 
text with pronunciation-based method that the organi-
zation of their mental lexicons is also pronunciation 
related. The ratio for the Elist suggests that editors of 
the MOE book may have chosen the examples with a 
special viewpoint in their minds ? balancing pronun-
ciation and composition related errors. 
4 Reliability of Web-based Statistics  
In this section, we examine the effectiveness of using 
Web-based statistics to differentiate correct and incor-
rect characters. The abundant text material on the 
Internet gives people to treat the Web as a corpus (e.g., 
webascorpus.org). When we send a query to Google, 
we will be informed of the number of pages (NOPs) 
that possibly contain relevant information. If we put 
the query terms in quotation marks, we should find 
the web pages that literally contain the query terms. 
Hence, it is possible for us to compare the NOPs for 
two competing phrases for guessing the correct way 
of writing. At the time of this writing, Google found 
107000 and 3220 pages, respectively, for ?strong tea? 
and ?powerful tea?. (When conducting such advanced 
searches with Google, the quotation marks are needed 
to ensure the adjacency of individual words.) Hence, 
?strong? appears to be a better choice to go with ?tea?. 
This is an idea similar to the approach that compute 
collocations based on word frequencies (cf. Manning 
and Sch?tze, 1999). Although the idea may not work 
very well for small database, the size of the current 
Web should be considered large enough. 
Using the quotation marks for the query terms en-
forced the influences of the surrounding characters in 
Chinese words, and provides a better clue for judging 
correct usage of Chinese characters. For instance,  
without the context, ??? and ??? might be used 
incorrectly to replace each other because they have 
the same pronunciation, i.e., Mei3. It is relatively 
unlikely for one to replace ??? with ??? when we 
write ???? (every one), but these two characters can 
become admissible candidates when we write ???? 
(USA) and ???? (every country).  
Table 1. Error analysis for Elist and Jlist 
 C P C&P NE D 
Elist 66.09% 67.21% 37.13% 0.23% 3.60%
Jlist 30.70% 79.88% 20.91% 2.43% 7.90%
85
4.1 Field Tests 
We test this strategy by sending the words in Elist and 
Jlist to Google to find the NOPs. We can retrieve the 
NOPs from the documents returned by Google, and 
compare the NOPs for the correct and the incorrect 
words to evaluate the strategy. Again, we focused on 
those in the 5401 words that the human subjects had 
consensus about their error types. Recall that we have 
1285 and 1515 such words in Elist and Jlist, respec-
tively. As the information available on the Web 
changes all the time, we also have to note that our 
experiments were conducted during the first half of 
March 2009. The queries were submitted at reason-
able time intervals to avoid Google?s treating our pro-
grams as malicious attackers. 
Table 2 shows the results of our investigation. We 
considered that we had a correct result when we found 
that the NOP for the correct word was larger than the 
NOP for the incorrect word. If the NOPs were equal, 
we recorded an ambiguous result; and when the NOP 
for the incorrect word was larger, we recorded an in-
correct event. We use ?C?, ?A?, and ?I? to denote ?cor-
rect?, ?ambiguous?, and ?incorrect? events in Table 2.  
The column headings of Table 2 show the setting 
of the searches with Google and the set of words that 
were used in the experiments. We asked Google to 
look for information from web pages that were en-
coded in traditional Chinese (denoted Trad). We 
could add another restriction on the source of infor-
mation by asking Google to inspect web pages from 
machines in Taiwan (denoted Twn+Trad). We were 
not sure how Google determined the languages and 
locations of the information sources, but chose to trust 
Google. The headings ?Comp? and ?Pron? indicate 
whether the words whose error types were composi-
tion and pronunciation-related, respectively.  
Table 2 shows eight distributions, providing ex-
perimental results that we observed under different 
settings. The distribution printed in bold face showed 
that, when we gathered information from sources that 
were encoded in traditional Chinese, we found the 
correct words 73.12% of the time for words whose 
error types were related to composition in Elist. Under 
the same experimental setting, we could not judge the 
correct word 4.58% of the time, and would have cho-
sen an incorrect word 22.30% of the time. 
Statistics in Table 2 indicate that web statistics is 
not a very reliable factor to judge the correct words. 
The average of the eight numbers in the ?C? rows is 
only 71.54% and the best sample is 76.59%, suggest-
ing that we did not find the correct words frequently. 
We would made incorrect judgments 24.75% of the 
time. The statistics also show that it is almost equally 
difficult to find correct words for errors that are com-
position and pronunciation related. In addition, the 
statistics reveal that choosing more features in the 
advanced search affected the final results. Using 
?Trad? offered better results in our experiments than 
using ?Twn+Trad?. This observation may arouse a 
perhaps controversial argument. Although Taiwan is 
the main area to use traditional Chinese, their web 
pages might not have used as accurate Chinese as web 
pages located in other regions. 
4.2 An Error Analysis for the Field Tests 
We have analyzed the reasons for why using Web-
based statistics did not always find the correct words. 
Frequencies might not have been a good factor to de-
termine the correctness of Chinese. However, the 
myriad amount of data on the Web should have pro-
vided a better performance.  
The most common reason for errors is that some of 
the words are really confusing such that the majority 
of the Web pages actually used the incorrect words. 
Some of errors were so popular that even one of the 
Chinese input methods on Windows XP offered 
wrong words as possible choices, e.g., ????? (the 
correct one) vs. ?????. It is interesting to note that 
people may intentionally use incorrect words in some 
occasions; for instance, people may choose to write 
homophones in advertisements.  
Another popular reason is that whether a word is 
correct depends on a larger context. For instance, ??
?? is more popular than ???? because the former 
is a popular nickname. Unless we had provided more 
contextual information about the queried words, 
checking only the NOPs of ???? and ???? led us 
to choose ????, which happened to be an incorrect 
word when we meant to find the right way to write 
????. Another difficult pair of words to distinguish 
is ???? and ????. 
Yet another reason for having a large NOP of the 
incorrect words was due to errors in segmenting Chi-
nese character strings. Consider a correct character 
string ?WXYZ?, where ?WX? and ?YZ? are two cor-
rect words. It is possible that ?XY? happens to be an 
incorrect way to write a correct word. This is the case 
for having the counts for ?????? to contribute to 
the count for ???? which is an incorrect form of 
????. 
5 Facilitating Test Item Authoring 
Incorrect character correction is a very popular type of 
test in Taiwan. There are simple test items for young 
children, and there are very challenging test items for 
the competitions among adults. Finding an attractive 
incorrect character to replace a correct character to 
form a test item is a key step in authoring test items.  
Table 2. Reliability of Web-based statistics 
Trad Twn+Trad  
Comp Pron Comp Pron 
C 73.12% 73.80% 69.92% 68.72%
A 4.58% 3.76% 3.83% 3.76%
E
list 
I 22.30% 22.44% 26.25% 27.52%
C 76.59% 74.98% 69.34% 65.87%
A 2.26% 3.97% 2.47% 5.01%
Jlist 
I 21.15% 21.05% 28.19% 29.12%
86
We have been trying to build a software environ-
ment for assisting the authoring of test items for in-
correct character correction (Liu and Lin, 2008, Liu et 
al., 2009). It should be easy to find a lexicon that con-
tains pronunciation information about Chinese charac-
ters. In contrast, it might not be easy to find visually 
similar Chinese characters with computational meth-
ods. We expanded the original Cangjie codes (OCC), 
and employed the expanded Cangjie codes (ECC) to 
find visually similar characters (Liu and Lin, 2008). 
Cangjie encoding (Chu, 2009) is a special system 
for representing the formation of Chinese characters 
with a sequence of at most five basic symbols. For 
instance, ??? and ??? are represented by ????
?? and ??????, respectively. It is evident that 
the Cangjie codes are useful for finding visually simi-
lar characters. 
With a lexicon, we can find characters that can be 
pronounced in a particular way. However, this is not 
enough for our goal. We observed that there were 
different symptoms when people used incorrect char-
acters that are related to their pronunciations. They 
may use characters that could be pronounced exactly 
the same as the correct characters. They may also use 
characters that have the same pronunciation and dif-
ferent tones with the correct character. Although rela-
tively infrequently, people may use characters whose 
pronunciations are similar to but different from the 
pronunciation of the correct character.  
We reported that replacing OCCs with ECCs to 
find visually similar characters could increase the 
chances to find similar characters. Instead of saving 
?????? for ??? directly, we divide a Chinese 
character into subareas systematically, and save the 
Cangjie codes for each of the subareas. A Chinese 
character is stored with the information about how it 
is divided into subareas and the Cangjie sequences for 
each of its subareas.  The internal code for how we 
divide ??? is 2, and the ECC for ??? has two parts: 
??? and ?????. Yet, it was not clear as to which 
components of a character should use ECCs (Liu and 
Lin, 2008; Liu et al, 2009). 
5.1 Formalizing the Extended Cangjie Codes 
We analyzed the OCCs for all the characters in Clist 
to determine the list of basic components, with com-
puter programs. We treated a basic Cangjie symbol as 
if it was a word, and computed the number of occur-
rences of n-grams based on the OCCs of the charac-
ters in Clist. Since the OCC for a character contains at 
most five symbols, the longest n-grams are 5-grams. 
Because the reason to use ECCs was to find common 
components in characters, we saved n-grams that re-
peated no less than three times in a list. After obtain-
ing this initial list of n-grams, we removed those n-
grams that were substrings of longer n-grams in the 
list.  
In addition, the n-grams that appeared no less than 
three times might not represent an actual part in any 
Chinese characters. This may happen by chance be-
cause we considered only frequencies of n-grams 
when we generated the initial list at the previous step. 
For instance, the OCC codes for ??? (shai4), ??? 
(wu4), and ??? (chen2) are ??????, ??????, 
and ??????, respectively. Although the substring 
????? appears three times, it does represent an 
actual part of Chinese characters. Hence, we manually 
examined all of the n-grams in the initial list, and re-
moved such n-grams from the list.  
In addition to considering the frequencies of n-
grams formed by the basic Cangjie codes to determine 
the list of components, we also took advantage of 
radicals that are used to categorize Chinese characters 
in typical printed dictionaries. Radicals that are stand-
alone Chinese words were included in the list of com-
ponents.  
After selecting the list of basic components with 
the above procedure, we encoded the words in Elist 
with these basic components. We inherited the 12 
ways reported in a previous work (Liu and Lin, 2008) 
to decompose Chinese characters. There are other 
methods for decomposing Chinese characters into 
components. Juang et al (2005) and their team at the 
Sinica Academia propose 13 different ways for de-
composing characters. 
At the same time when we annotated individual 
characters with their ECCs, we may revise the list of 
basic components. If a character that actually con-
tained an intuitively ?common? part and that part had 
not been included in the list of basic component, we 
would add this part into the list to make it a basic 
component and revised the ECC for all characters 
accordingly.  The judgment of being ?common? is 
subjective, but we still maintained the rule that such 
common parts must appear in more than three charac-
ters. When defining the basic components, not all 
judgments are completely objectively yet, and this is 
also the case of defining the original Cangjie codes. 
We tried to be as systematic as possible, but intuition 
sometimes stepped in. 
We repeated the procedure described in the preced-
ing paragraph five times to make sure that we were 
satisfied with the ECCs for all of the 5401 characters. 
The current list contains 794 components, and we can 
revise the list of basic components in our work when-
ever necessary. 
5.2 Recommending Incorrect Alternatives 
With the pronunciation of Chinese characters in a 
dictionary and with our ECC encodings for words in 
the Elist, we can create lists of candidate characters 
for replacing a specific correct character in a given 
word to create a test item for incorrect character cor-
rection.  
There are multiple strategies to create the candidate 
lists. We may propose the candidate characters be-
cause their pronunciations have the same sound and 
the same tone with those of the correct character (de-
noted SSST). Characters that have same sounds and 
87
different tones (SSDT), characters that have similar 
sounds and same tones (MSST), and characters that 
have similar sounds and different tones (MSDT) can 
be considered as candidates as well. It is easy to judge 
whether two Chinese characters have the same tone. 
In contrast, it is not trivial to define ?similar? sound. 
We adopted the list of similar sounds that was pro-
vided by a psycholinguistic researcher (Dr. Chia-Ying 
Lee) at the Sinica Academia. ??? (po) and ??? (bo) 
and ???(fan4) and ???(huan4) are pairs that have 
similar sounds. It was observed that these are four 
possible reasons that people used incorrect characters 
in writing. 
Because a Chinese character might be pronounced 
in multiple ways, character lists generated based on 
these strategies may include the same characters. 
More specifically, the lists SSST and SSDT may over-
lap when a character that can be pronounced in multi-
ple ways, and these pronunciations share the same 
sound and have different tones. The characters ??? 
and ??? are such examples. ??? can be pronounced 
as ?dai1? or ?dai4?, and ??? can be pronounced as 
?hao3? or ?hao4?. Hence, characters that can be pro-
nounced as ?hao3? will be listed in both SSST and 
SSDT for ???.  
In addition, we may propose characters that look 
similar to the correct character. Two characters may 
look similar for many reasons (Liu et al, 2009). The 
most common reason is that they contain the same 
components, and the other is that they belong to the 
same radical category and have the same total number 
of strokes (RS), e.g., the pairs ??? and ???, ??? 
and ???, and ??? and ???. When two characters 
contain the same component, the shared component 
might or might not locate at the same position, e.g., 
??? and ???.  
In an authoring tool, we could recommend a se-
lected number of candidate characters for replacing 
the correct character. We tried two different strategies 
to compare and choose the visually similar characters. 
The similarity is computed based on the number and 
the locations of shared Cangjie symbols in the ECCs 
of the characters. The first strategy (denoted SC1) 
gave a higher score to the shared component that lo-
cated at the same location in the two characters being 
compared. The second strategy (SC2) gave the same 
score to any shared component even if the component 
did not reside at the same location in the characters. 
The characters ???, ???, and ??? share the same 
component ???. When computing the similarity be-
tween these characters with SC1, the contribution of 
??? will be the same for any pair. When computing 
with SC2, the contribution of ??? will be larger for 
the pair ??? and ??? than for the pair ??? and ???. 
In the former case, ??? appears at the same location 
in the characters. 
 When there were more than 20 characters that re-
ceive nonzero scores in the SC1 and SC2 categories, 
we chose to select at most 20 characters that had lead-
ing scores as the list of recommended characters. 
We had to set a bound on the number of candidate 
characters, i.e., 20, for strategies SC1 and SC2.  The 
number of candidates generated from these two 
strategies can be large and artificial, depending on our 
scoring functions for determining similarities between 
characters. We did not limit the sizes of candidate 
lists that were generated by other strategies because 
those lists were created based on more objective 
methods. The rules for determining ?similar? sounds 
were given by the domain experts, so we considered 
the rules objective in this research. 
For the experiments that we reported in the follow-
ing subsection, we submitted more than 300 thousand 
of queries to Google. As we mentioned in Section 4.1, 
a frequent continual submission of queries to Google 
will make Google treat our programs as malicious 
processes. (We are studying the Google API for a 
more civilized solution.) Without the bound, it is pos-
sible to offer a very long list of candidates. On the 
other hand, it is also possible that our program does 
not find any visually similar characters for some spe-
cial characters, and this is considered a possible phe-
nomenon.  
5.3 Evaluating the Recommendations 
We examined the usefulness of these seven categories 
of candidates with errors in Elist and Jlist. The first 
set of evaluation (the inclusion tests) checked whether 
the lists of recommended characters contained the 
incorrect character in our records. The second set of 
evaluation (the ranking tests) was designed for practi-
cal application in computer assisted item generation. 
Only for those words whose actual incorrect charac-
ters were included in the recommended list, we re-
placed the correct characters in the words with the 
candidate incorrect characters, submitted the incorrect 
words to Google, and ordered the candidate characters 
based on their NOPs. We then recorded the ranks of 
the incorrect characters among all recommended 
characters.  
Since the same character may appear simultane-
ously in SC1, SC2, and RS, we computed the union of 
these three sets, and checked whether the incorrect 
characters were in the union. The inclusion rate is 
listed under Comp, representing the inclusion rate 
when we consider only logographic influences. Simi-
larly, we computed the union for SSST, SSDT, MSST, 
and MSDT, checked whether the incorrect characters 
were in the union, and recorded the inclusion rate 
under Pron, representing the inclusion rate when we 
consider only phonological influences. Finally, we 
computed the union of the lists created by the seven 
strategies, and recorded the inclusion rate under Both. 
The second and the third rows of Table 3 show the 
results of the inclusion tests when we recommended 
candidate characters with the methods indicated in the 
column headings. The data show the percentage of the 
incorrect characters being included in the lists that 
88
were recommended by the seven strategies. Notice 
that the percentages were calculated with different 
denominators. The number of composition-related 
errors was used for SC1, SC2, RS, and Comp (e.g., 
505 that we mentioned in Section 3 for Jlist); the 
number of pronunciation-related errors for SSST, 
SSDT, MSST, MSDT, and Pron (e.g., 1314 mentioned 
in Section 3 for the Jlist); the number of either of 
these two  types of errors for Both (e.g., 1475 for Jlist).  
The results recorded in Table 3 show that we were 
able to find the incorrect character quite effectively, 
achieving better than 93% for both Elist and Jlist. The 
statistics also show that it is easier to find incorrect 
characters that were used for pronunciation-related 
problems. Most of the pronunciation-related problems 
were misuses of homophones. Unexpected confusions, 
e.g., those related to pronunciations in Chinese dia-
lects, were the main reason for the failure to capture 
the pronunciation-related errors. (Namely, few pro-
nunciation-related errors were not considered in the 
information that the psycholinguist provided.) SSDT 
is a crucial complement to SSST.  
There is still room to improve our methods to find 
confusing characters based on their compositions. We 
inspected the list generated by SC1 and SC2, and 
found that, although SC2 outperformed SC1 on the 
inclusion rate, SC1 and SC2 actually generated com-
plementary lists in many cases, and should be used 
together. The inclusion rate achieved by the RS strat-
egy was surprisingly high. We found that many of the 
errors that were captured by the RS strategy were also 
captured by the SSST strategy. 
The fourth and the fifth rows of Table 3 show the 
effectiveness of relying on Google to rank the candi-
date characters for recommending an incorrect charac-
ter. The rows show the average ranks of the included 
cases. The statistics show that, with the help of 
Google, we were able to put the incorrect character on 
top of the recommended list when the incorrect char-
acter was included.  This allows us to build an envi-
ronment for assisting human teachers to efficiently 
prepare test items for incorrect character identification. 
Note that we did not provide data for all columns 
in the fourth and the firth rows. Unlike that we show 
the inclusion rates in the second and the third rows, 
the fourth and the fifth rows show how the actual in-
correct characters were ranked in the recommended 
lists. Hence, we need to have a policy to order the 
characters of different lists to find the ranks of the 
incorrect characters in the integrated list.  
However, integrating the lists is not necessary and 
can be considered confusing to the teachers. The se-
lection of incorrect characters from different lists is 
related to the goals of the assessment, and it is better 
to leave the lists separated for the teachers to choose. 
The same phenomenon and explanation apply to the 
sixth and the seventh rows as well. 
The sixth and the seventh rows show the average 
numbers of candidate characters proposed by different 
methods. Statistics shown between the second and the 
fifth rows are related to the recall rates (cf. Manning 
and Sch?tz, 1999) achieved by our system. For these 
four rows, we calculated how well the recommended 
lists contained the reported errors and how the actual 
incorrect characters ranked in the recommended lists. 
The sixth and the seventh rows showed the costs for 
these achievements, measured by the number of rec-
ommended characters. The sum of the sixth and the 
seventh rows, i.e., 103.59 and 108.75, are, respec-
tively, the average numbers of candidate characters 
that our system recommended as possible errors re-
corded in Elist and Jlist. (Note that some of these 
characters were repeated.) 
There are two ways to interpret the statistics shown 
in the sixth and the seventh rows. Comparing the cor-
responding numbers on the fourth and the sixth rows, 
e.g., 3.25 and 19.27, show the effectiveness of using 
the NOPs to rank the candidate characters. The ranks 
of the actual errors were placed at very high places, 
considering the number of the originally recom-
mended lists. The other way to use the statistics in the 
sixth and the seventh rows is to compute the average 
precision. For instance, we recommended an average 
19.13 characters in SSST to achieve the 91.64 inclu-
sion rate. The recall rate is very high, but the averaged 
precision is very low. This, however, is not a very 
convincing interpretation of the results. Having as-
sumed that there was only one best candidate as in our 
experiments, it was hard to achieve high precision 
rates. The recall rates are more important than the 
precision rates, particularly when we have proved that 
the actual errors were ranked among the top five al-
ternatives. 
When designing a system for assisting the author-
ing of test items, it is not really necessary to propose 
all of the characters in the categories. In the reported 
experiments, choosing the top 5 or top 10 candidates 
will contain the most of the actual incorrect characters 
based on the statistics shown in the fourth and the 
fifth rows. Hence the precision rates can be signifi-
cantly increased practically. We do not have to merge 
the candidate characters among different categories 
Table 3. Incorrect characters were contained and ranked high in the recommended lists 
 SC1 SC2 RS SSST SSDT MSST MSDT Comp Pron Both 
Elist 73.92% 76.08% 4.08% 91.64% 18.39% 3.01% 1.67% 81.97% 99.00% 93.37% 
Jlist 67.52% 74.65% 6.14% 92.16% 20.24% 4.19% 3.58% 77.62% 99.32% 97.29% 
Elist 3.25 2.91 1.89 2.30 1.85 2.00 1.58 
Jlist 2.82 2.64 2.19 3.72 2.24 2.77 1.16 
Elist 19.27 17.39 11.34 19.13 8.29 19.02 9.15 
Jlist 17.58 16.24 12.52 22.85 9.75 22.11 7.68 
89
because choosing the categories of incorrect charac-
ters depends on the purpose of the assessment. Reduc-
ing the length of the candidate list increases the 
chances of reducing the recall rates. Achieving the 
best trade off between precision and recall rates relies 
on a more complete set of experiments that involve 
human subjects. 
Furthermore, in a more realistic situation, there can 
be more than one ?good? incorrect character, not just 
one and only gold standard as in the reported experi-
ments. It is therefore more reasonable the compute the 
precision rates based the percentage of ?acceptable? 
incorrect characters. Hence, the precision rates are 
likely to increase and become less disconcerting.  
We reported experimental results in which we 
asked 20 human subjects to choose an incorrect char-
acter for 20 test items (Liu et al, 2009). The best so-
lutions were provided by a book. The recommenda-
tions provided by our previous system and chosen by 
the human subjects achieved comparable qualities.  
Notice that the numbers do not directly show the 
actual number of queries that we had to submit to 
Google to receive the NOPs for ranking the characters. 
Because the lists might contain the same characters, 
the sum of the rows showed just the maximum num-
ber of queries that we submitted. Nevertheless, they 
still served as good estimations, and we actually sub-
mitted 103.59?1441(=149273) and 108.75?1583 
(=172151) queries to Google for Elist and Jlist in ex-
periments from which we obtained the data shown in 
the fourth and the fifth rows. These quantities ex-
plained why we had to be cautious about how we 
submitted queries to Google. When we run our pro-
gram for just a limited number of characters, the prob-
lems caused by intensive queries should not be very 
serious. 
5.4 Discussions 
Dividing characters into subareas proved to be crucial 
in our experiments (Liu and Lin, 2008; Liu et al, 
2009), but this strategy is not perfect, and could not 
solve all of the problems. The way we divided Chi-
nese characters into subareas like (Juang et al, 2005; 
Liu and Lin, 2008) sometimes contributed to the fail-
ure of our current implementation to capture all of the 
errors that were related to the composition of the 
words. The most eminent reason is that how we di-
vide characters into areas. Liu and Lin (2008) fol-
lowed the division of Cangjie (Chu, 2009), and Juang 
et al (2005) proposed an addition way to split the 
characters.   
The best divisions of characters appear to depend 
on the purpose of the applications. Recall that each 
part of the character is represented by a string of 
Cangjie codes in ECCs. The separation of Cangjie 
codes in ECCs was instrumental to find the similarity 
of ??? and ??? because ??? is a standalone subpart 
in both ??? and ???. The Cangjie system has a set 
of special rules to divide Chinese characters (Chu, 
2009; Lee, 2008). Take ??? and ??? for example. 
The component ??? is recorded as an standalone part 
in ???, but is divided into two parts in ???. Hence, 
??? is stored as one string, ?????, in ??? and as 
two strings, ???? and ???, in ???. The different 
ways of saving ??? in two different words made it 
harder to find the similarity between ??? and ???. 
An operation of concatenation is in need, but the 
problems are that it is not obvious to tell when the 
concatenation operations are useful and which of the 
parts should be rejoined. Hence, using the current 
methods to divide Chinese characters, it is easy to 
find the similar between ??? and ??? but difficult to 
find the similar between ??? and ???. In contrast, if 
we enforce a rule to save ??? as one string of Cang-
jie code, it will turn the situations around. Determin-
ing the similarity between ??? and ??? will be more 
difficult than finding the similarity between ??? and 
???. 
Due to this observation, we have come to believe 
that it is better to save the Chinese characters with 
more detailed ECCs. By saving all detailed informa-
tion about a character, our system can offer candidate 
characters based on users? preferences which can be 
provided via a good user interface. This flexibility can 
be very helpful when we are preparing text materials 
for experiments for psycholinguistics or cognitive 
sciences (e.g., Leck et al 1995; Yeh and Li, 2002).  
6 Summary  
The analysis of the 1718 errors produced by real stu-
dents show that similarity between pronunciations of 
competing characters contributed most to the ob-
served errors. Evidences show that the Web statistics 
are not very reliable for differentiating correct and 
incorrect characters. In contrast, the Web statistics are 
good for comparing the attractiveness of incorrect 
characters for computer assisted item authoring.  
Acknowledgments 
This research was supported in part by the National 
Science Council of Taiwan under grant NSC-97-
2221-E-004-007-MY2. We thank anonymous review-
ers for their invaluable comments. 
References  
B.-F. Chu. 2009. Handbook of the Fifth Generation of 
the Cangjie Input Method, available at 
http://www.cbflabs.com/book/ocj5/ocj5/index.html. 
Last visited on 30 April 2009. 
D. Juang, J.-H. Wang, C.-Y. Lai, C.-C. Hsieh, L.-F. 
Chien, J.-M. Ho. 2005. Resolving the unencoded 
character problem for Chinese digital libraries, 
Proc. of the 5th ACM/IEEE Joint Conf. on Digital 
Libraries, 311?319. 
S.-P. Law, W. Wong, K. M. Y. Chiu. 2005. Whole-
word phonological representations of disyllabic 
90
words in the Chinese lexicon: Data from acquired 
dyslexia, Behavioural Neurology, 16, 169?177. 
K. J. Leck, B. S. Weekes, M. J. Chen. 1995. Visual 
and phonological pathways to the lexicon: Evi-
dence from Chinese readers, Memory & Cognition, 
23(4), 468?476. 
H. Lee. 2008. Cangjie Input Methods in 30 Days, 
http://input.foruto.com/cjdict/Search_1.php, Foruto 
Company, Hong Kong. Last visited on 30 April 
2009. 
C.-L. Liu, K.-W. Tien, Y.-H. Chuang, C.-B. Huang, 
J.-Y. Weng. 2009. Two applications of lexical in-
formation to computer-assisted item authoring for 
elementary Chinese, Proc. of the 22nd Int?l Conf. 
on Industrial Engineering & Other Applications of 
Applied Intelligent Systems, 470?480. 
C.-L. Liu, J.-H. Lin. 2008. Using structural informa-
tion for identifying similar Chinese characters, 
Proc. of the 46th ACL, short papers, 93?96. 
C. D. Manning, H. Sch?tze. Foundations of Statistical 
Natural Language Processing. The MIT Press. 
1999. 
MOE. 1996. Common Errors in Chinese Writings (?
?????), Ministry of Education, Taiwan. 
S.-L. Yeh, J.-L. Li. 2002. Role of structure and com-
ponent in judgments of visual similarity of Chinese 
characters, Journal of Experimental Psychology: 
Human Perception and Performance, 28(4), 933?
947.
 
91
Reducing the False Alarm Rate of Chinese Character Error Detection 
and Correction 
 
Shih-Hung Wu, Yong-Zhi Chen  
Chaoyang University of Technol-
ogy, Taichung Country 
shwu@cyut.edu.tw 
 
Ping-che Yang, Tsun Ku 
Institute for information industry,
Taipei City 
maciaclark@iii.org.tw, 
cujing@iii.org.tw 
Chao-Lin Liu  
National Chengchi Universi-
ty, Taipei City 
chaolin@nccu.edu.tw 
 
 
Abstract 
The main drawback of previous Chinese cha-
racter error detection systems is the high false 
alarm rate. To solve this problem, we propose 
a system that combines a statistic method and 
template matching to detect Chinese character 
errors. Error types include pronunciation-
related errors and form-related errors. Possible 
errors of a character can be collected to form a 
confusion set. Our system automatically gene-
rates templates with the help of a dictionary 
and confusion sets. The templates can be used 
to detect and correct errors in essays. In this 
paper, we compare three methods proposed in 
previous works. The experiment results show 
that our system can reduce the false alarm sig-
nificantly and give the best performance on f-
score. 
1 Introduction 
Since many Chinese characters have similar forms 
and similar or identical pronunciation, improperly 
used characters in Chinese essays are hard to be de-
tectted. Previous works collected these hard-to-
distinguish characters and used them to form confu-
sion sets. Confusion sets are critical for detecting and 
correcting improperly used Chinese characters. A 
confusion set of a Chinese character consists of cha-
racters with similar pronunciation, similar forms, and 
similar meaning. Most Chinese character detection 
systems were built based on confusion sets and a lan-
guage model. Ren et.al proposed a rule-based method 
that was also integrated with a language model to 
detect character errors in Chinese (Ren, Shi, & Zhou, 
1994). Chang used confusion sets to represent all 
possible errors to reduce the amount of computation. 
A language model was also used to make decisions. 
The confusion sets were edited manually. Zhang et al 
proposed a way to automatically generate confusion 
sets based on the Wubi input method (Zhang, Zhou, 
Huang, & Sun, 2000). The basic assumption was that 
characters with similar input sequences must have 
similar forms. Therefore, by replacing one code in the 
input sequence of a certain character, the system 
could generate characters with similar forms. In the 
following work, Zhang et al designed a Chinese cha-
racter detection system based on the confusion sets 
(Zhang, Zhou, Huang, & Lu, 2000). Another input 
method was also used to generate confusion sets. Lin 
et al used the Cangjie input method to generate con-
fusion sets (Lin, Huang, & Yu, 2002). The basic as-
sumption was the same. By replacing one code in the 
input sequence of a certain character, the system 
could generate characters with similar forms. Since 
the two input methods have totally different represen-
tations of the same character, the confusion set of any 
given character will be completely different. 
In recent years, new systems have been incorporat-
ing more NLP technology for Chinese character error 
detection. Huang et al proposed that a word segmen-
tation tool can be used to detect character error in 
Chinese (Huang, Wu, & Chang, 2007). They used a 
new word detection function in the CKIP word seg-
mentation toolkit to detect error candidates (CKIP, 
1999). With the help of a dictionary and confusion set, 
the system can decide whether a new word is a cha-
racter error or not. Hung et al proposed a system that 
can detect character errors in student essays and then 
suggest corrections (Hung & Wu, 2008). The system 
was based on common error templates which were 
manually edited. The precision of this system is the 
highest, but the recall remains average. The main 
drawback of this approach is the cost of editing com-
mon error templates. Chen et al proposed an automat-
ic method for common error template generation 
(Chen, Wu, Lu, & Ku, 2009). The common errors 
were collected from a large corpus automatically. The 
template is a short phrase with one error in it. The 
assumption is the frequency of a correct phrase must 
be higher than the frequency of the corresponding 
template, with one error character. Therefore, a statis-
tical test can be used to decide weather there is a 
common error or not. 
The main drawback of previous systems is the high 
false alarm rate. The drawback is found by comparing 
the systems with sentences without errors. As we will 
show in our experiments, the systems in previous 
works tent to report more errors in an essay than the 
real ones, thus, cause false alarms.  
In this paper, we will further improve upon the 
Chinese character checker using a new error model 
and a simplified common error template generation 
method. The idea of error model is adopted from the 
noise channel model, which is used in many natural 
language processing applications, but never on Chi-
nese character error detection. With the help of error 
model, we can treat the error detection problem as a 
kind of translation, where a sentence with errors can 
be translated into a sentence without errors. The sim-
plified template generation is based on given confu-
sion sets and a lexicon.  
The paper is organized as follows. We introduce 
briefly the methods in previous works in section 2. 
Section 3 reports the necessary language resources 
used to build such systems. Our approach is described 
in section 4. In section 5, we report the experiment 
settings and results of our system, as well as give the 
comparison of our system to the three previous sys-
tems. Finally, we give the conclusions in the final 
section. 
2 Previous works 
In this paper, we compare our method to previous 
works. Since they are all not open source systems, we 
will reconstruct the systems proposed by Chang 
(1995), Lin, Huang, & Yu (2002), and Huang, Wu, & 
Chang (2007). We cannot compare our system to the 
system proposed by Zhang, Zhou, Huang, & Sun 
(2000), since the rule-based system is not available. 
We describe the systems below. 
Chang?s system (1995) consists of five steps. First, 
the system segments the input article into sentences. 
Second, each character in the sentence is replaced by 
the characters in the corresponding confusion set. 
Third, the probability of a sentence is calculated ac-
cording to a bi-gram language model. Fourth, the 
probability of the sentences before and after replace-
ment is compared. If the replacement causes a higher 
probability, then the replacement is treated as a cor-
rection of a character error. Finally, the results are 
outputted. There are 2480 confusion sets used in this 
system. Each confusion set consists of one to eight 
characters with similar forms or similar pronunciation. 
The system uses OCR results to collect characters 
with similar forms. The average size of the confusion 
sets was less than two. The language model was built 
from a 4.7 million character news corpus. 
The system proposed by Lin, Huang, & Yu (2002) 
has two limitations. First, there is only one spelling 
error in one sentence. Second, the error was caused by 
the Cangjie input method. The system also has five 
steps. First, sentences are inputted. Second, a search is 
made of the characters in a sentence that have similar 
input sequences. Third, a language model is used to 
determine whether the replacement improves the 
probability of the sentence or not. Fourth, the three 
steps for all input sentences are repeated. Finally, the 
results are outputted. The confusion sets of this sys-
tem were constructed from the Cangjie input method. 
Similarity of characters in a confusion set is ranked 
according to the similarity of input sequences. The 
language model was built from a 59 million byte news 
corpus. 
The system by Huang, Wu, & Chang (2007) con-
sists of six steps. First, the input sentences are seg-
mented into words according to the CKIP word seg-
mentation toolkit. Second, each of the characters in 
the new words is replaced by the characters in the 
confusion sets. Third, a word after replacement 
checked in the dictionary. Fourth, a language model is 
used to assess the replacement. Fifth, the probability 
of the sentence before and after replacement is com-
pared. Finally, the result with the highest probability 
is outputted. The confusion set in this system, which 
also consists of characters with similar forms or simi-
lar pronunciation, was edited manually.  
Since the test data in the papers were all different 
test sets, it is improper to compare their results direct-
ly, therefore; there was no comparison available in the 
literature on this problem. To compare these systems 
with our method, we used a fixed dictionary, inde-
pendently constructed confusion sets, and a fixed lan-
guage model to reconstruct the systems. We per-
formed tests on the same test set. 
3 Data in Experiments  
3.1 Confusion sets 
Confusion sets are a collection of sets for each indi-
vidual Chinese character. A confusion set of a certain 
character consists of phonologically or logographical-
ly similar characters. For example, the confusion set 
of ??? might consist of the following characters with 
the same pronunciation????????? or with 
similar forms????????????????
??????. In this study, we use the confusion sets 
used by Liu, Tien, Lai, Chuang, & Wu (2009). The 
similar Cangjie (SC1 and SC2) sets of similar forms, 
and both the same-sound-same-tone (SSST) and 
same-sound-different-tone (SSDT) sets for similar 
pronunciation were used in the experiments. There 
were 5401 confusion sets for each of the 5401 high 
frequency characters. The size of each confusion set 
was one to twenty characters. The characters in each 
confusion set were ranked according to Google search 
results. 
3.2 Language model 
Since there is no large corpus of student essays, we 
used a news corpus to train the language model. The 
size of the news corpus is 1.5 GB, which consists of 
1,278,787 news articles published between 1998 and 
2001. The n-gram language model was adopted to 
calculate the probability of a sentence p(S). The gen-
eral n-gram formula is: 
)|()( 1 1
?
+?= n Nnn wwpSp    (1) 
Where N was set to two for bigram and N was set to 
one for unigram. The Maximum Likelihood Estima-
tion (MLE) was used to train the n-gram model. We 
adopted the interpolated Kneser-Ney smoothing me-
thod as suggested by Chen & Goodman (1996). As 
following: 
 
)()1()|(
)|(
1
1int
wpwwp
wwp
unigramibigram
ierpolate
?? ?+= ?
?
  (2)
 
To determine whether a replacement is good or not, 
our system use the modified perplexity:  
 
NSpPerplexity /))(log(2?=   (3) 
Where N is the length of a sentence and p(S) is the bi-
gram probability of a sentence after smoothing. 
3.3 Dictionary and test set 
We used a free online dictionary provided by Tai-
wan?s Ministry of Education, MOE (2007). We fil-
tered out one character words and used the remaining 
139,976 words which were more than one character as 
our lexicon in the following experiments. 
The corpus is 5,889 student essays collected from a 
primary high school. The students were 13 to 15 years 
old. The essays were checked by teachers manually, 
and all the errors were identified and corrected. Since 
our algorithm needed a training set, we divided the 
essays into two sets to test our method. The statistics 
is given in Table 1. There are less than two errors in 
an essay on average. We find that most (about 97%) 
of characters in the essays were among the 5,401 most 
common characters, and most errors were characters 
of similar forms or pronunciation. Therefore, the 
5,401 confusion sets constructed according to form 
and pronunciation were suitable for error detection. 
Table 2 shows the error types of errors in students? 
essays.  More than 70% errors are characters with 
similar pronunciation, 40% errors are characters with 
similar form, and there are 20% errors are characters 
with both similar pronunciation and similar form. 
Only 10% errors are in other types. Therefore, in this 
study, our system aimed to identify and correct the 
errors of the two common types. 
 
Table 1. Training set and test set statistics 
 # of Essays 
Average 
length 
of essay 
Average 
# of 
errors 
% of 
common 
characters
Training 
set 5085 403.18 1.76 96.69% 
Test set 804 387.08 1.2 97.11% 
 
Table 2. Error type analysis 
 Similar form Similar pronunciation Both Other
Training set 41.54% 72.60% 24.24% 10.10%
Test set 40.36% 76.98% 27.66% 10.30%
4 System Architecture 
4.1 System flowchart 
Figure 1 shows the flowchart of our system. First, the 
input essays are segmented into words. Second, the 
words are sent to two different error detection mod-
ules. The first one is the template module, which can 
detect character errors based on the stored templates 
as in the system proposed by Chen, Wu, Lu, & Ku, 
(2009). The second module is the new language mod-
el module, which treats error detection as a kind of 
translation. Third, the results of the two modules can 
be merged to get a better system result. The details 
will be described in the following subsections. 
 
Figure 1. System flowchart 
 
4.2 Word segmentation 
The first step in our system uses word segmentation to 
find possible errors. In this study, we do not use the 
CKIP word segmentation tool (CKIP, 1999) as Huang, 
Wu, & Chang (2007) did, since it has a merge 
algorithm that might merge error charactersto form 
new words (Ma & Chen, 2003).  We use a backward 
longest first approach to build our system. The lex-
icon is taken from an online dictionary (MOE, 2007).  
We consider an input sentence with an error, ????
????????????, as an example. The 
sentence will be segmented into ??|??|??|?
??|?|?|?|?|???. The sequence of single 
characters will be our focus. In this case, it is ????
??. These kinds of sequences will be the output of 
the first step and will be sent to the following two 
modules. The error character can be identified and 
corrected by a ????-???? template. 
4.3 Template Module 
The template module in this study is a simplified ver-
sion of a module from a previous work (Chen, Wu, 
Lu, & Ku, 2009), which collects templates from a 
corpus. The simplified approach replaces one 
character of each word in a dictionary with one 
character in the corresponding confusion set. For 
example, a correct word ???? might be written with 
an error character ???? since ??(bian4)? is in the 
confusion set of ??(ban4)??. This method generates 
all possible error words with the help of confusion 
sets. Once the error template ???? is matched in an 
essay, our system can conclude that the character is an 
error and make a suggestion on correction ???? 
based on the ????-???? template. 
4.4 Translate module 
To improve the n-gram language model method, we 
use a statistical machine translation formula (Brown, 
1993) as a new way to detect character error. We treat 
the sentences with/without errors as a kind of transla-
tion. Given a sentence S that might have character 
errors in the sentence in the source language, the out-
put sentence C
~
 is the sentence in the target language 
with the highest probability of different replacements 
C. The replacement of each character is treated as a 
translation without alignment. 
)|(maxarg
~
SCpC
c
=   (4) 
From the Bayesian rule and when the fixed value of 
p(w) is ignored, this equation can be rewritten as (5): 
)()|(maxarg
)(
)()|(
maxarg
~
CpCSp
Sp
CpCSp
C
c
c
?
=
  (5) 
The formula is known as noisy channel model. We 
call p(S|C) an ?error model?, that is,  the probability 
which a character can be incorrect. It can be defined 
as the product of the error probability of each charac-
ter in the sentence. 
?
=
=
n
i
jiij cspCWp
1
 )|()|(    (6) 
where n is the length of the sentence S, and si ith cha-
racter of input sentence S. Cj is the jth replacement 
and cij.is the ith character at the jth replacement. The 
error model was built from the training set of student 
essays. Where p(C) is the n-gram language model as 
was described in section 3.2. Note that the number of 
replacements is not fixed, since the number of re-
placements depends on the size of all possible errors 
in the training set. 
For example, consider a segmented sentence with 
an error: ??|??|?|?|?|???, we will use the 
error model to evaluate the replacement of each cha-
racter in the subsequence: ?????. Here p(?|?) 
and p(?|?) are 0.0456902 and 0.025729 respective-
ly, which are estimated according to the training cor-
pus. And in training corpus, no one write the character 
?, therefore, there is no any replacement. Therefore, 
the probability of our error model and the n-gram lan-
guage model can be shown in the following table. Our 
system then multiplies the two probabilities and gets 
the perplexity of each replacement. The replacement 
????? gets the lowest perplexity, therefore, it is 
the output of our system and is both a correct error 
detection and correction. 
 
Table 3. An example of calculating perplexity 
according the new error model 
  Error Model LM multiply Perplexity
??? 0.025728988 1.88E-05 4.83E-07 127.442812
??? 0.001175563 1.05E-04 1.24E-07 200.716961
??? 1 2.09E-09 2.09E-09 782.669809
??? 0.045690212 1.17E-08 5.34E-10 1232.6714
 
4.5 Merge corrections 
Since the two modules detect errors using an inde-
pendent information source, we can combine the deci-
sions of the two modules to get a higher precision or a 
higher recall on the detection and correction of errors. 
We designed two working modes, the Precision Mode 
(PM) and the Detection Mode (DM). The output of 
PM is the intersection of the output of the template 
module and translation module, while the output of 
DM is the union of the two modules. 
5 Experiment Settings and Results  
Since there is no open source system in previous 
works and the data in use is not available, we repro-
duced the systems with the same dictionary, the same 
confusion set, and the same language model. Then we 
performed a test on the same test set. Since the confu-
sion sets are quite large, to reduce the number of 
combinations during the experiment, the size must be 
limited. Since Liu?s experiments show that it takes 
about 3 candidates to find the correct character, we 
use the top 1 to top 10 similar characters as the candi-
dates only in our experiments. That is, we take 1 to 10 
characters from each of the SC1, SC2, SSST, and 
SSDT sets. Thus, the size of each confusion set is 
limited to 4 for the top 1 mode and 40 for the top 10 
mode. 
The evaluation metrics is the same as Chang?s 
(1995). We also define the precision rate, detection 
rate, and correction rate as follows: 
Precision = C / B * 100%  (7) 
Detection = C / A * 100%  (8) 
Correction = D / A * 100%  (9) 
where A is the number of all spelling errors, B is 
the number of errors detected by be system, C is the 
number of errors detected correctly by the system, and 
D is the number of spelling errors that is detected and 
corrected. Note that some errors can be detected but 
cannot be corrected. Since the correction is more im-
portant in an error detection and correction system, 
we define the corresponding f-score as: 
CorrectionPrecision
Correction*Precision*2
scoreF +=?  (10) 
 
 
 
 
 
Figure 2. The comparison of different methods on 
full test set 
5.1 Results of our initial system 
Table 4 shows the initial results of the template mod-
ule (TM), the translation module (LMM) and the 
combined results of the precision mode (PM) and 
detection mode (DM). We find that the precision 
mode gets the highest precision and f-score, while the 
detection mode gets the highest correction rate, as 
expected. The precision and detection rate improved 
dramatically. The precision improved from 14.28% to 
61.68% for the best setting and to 58.82% for the best 
f-score setting. The detection rate improved from 
58.06% to around 72%. The f-score improved from 
22.28% to 43.80%. The result shows that combining 
two independent methods yield better performance 
than each single method does. 
5.2 Results of our system when more know-
ledge and enlarged training sets are added 
The templates used in the initial system were the sim-
plified automatic generated templates, as described in 
section 4.3. Since there were many manually edited 
templates in previous works, we added the 6,701 ma-
nually edited templates and the automatically generat-
ed templates into our system. The results are shown in 
Table 5. All the performance increased for both the 
template module and the translation module. The best 
f-score increased from 43.80% to 45.03%. We believe 
that more knowledge will increase the performance of 
our system. 
5.3 Results of methods in previous works 
We compared the performance of our method to the 
methods in previous works. The result is shown in 
Table 6. Chang?s method has the highest detection 
rate, at 91.79%. Note that the price of this high detec-
tion rate is the high false alarm. The corresponding 
precision is only 0.94%. The precision mode in our 
method has the highest precision, correction, and f-
score. The comparison is shown in Figure 2. The ho-
rizontal axis is the size of confusion sets in our expe-
riment. We can find that the performances converge. 
That is, the size of confusion sets is large enough to 
detect and correct errors in students? essays. 
5.4 Comparison to methods in previous works 
related to sentences with errors 
The numbers in Table 6 are much lower than that in 
the original paper. The reason is the false alarms in 
sentences without any errors, since most previous 
works tested their systems on sentences with errors 
only. In addition, our test set was built on real essays, 
and there were only one or two errors in an essay. 
Most of the sentences contained no errors. The pre-
vious methods tend to raise false alarms.  
To clarify this point, we designed the last experiment 
to test the methods on sentences with at least one er-
ror. We extracted 949 sentences from our test set. 
Among them, 883 sentences have one error, 61 sen-
tences have two errors, 2 sentences have three errors, 
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Precision
Chang Lin Huang PM DM
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Detection
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
Correction
0%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
1 2 3 4 5 6 7 8 9 10
Top N of Confusion sets
F-Score
and 3 sentences that have four errors. The result is 
shown in Table 7. All the methods have better per-
formance. The precision of Chang?s method rose from 
3% to 43%. The precision of Lin?s method rose from 
3.5% to 61%. The precision of Huang?s method rose 
from 27% to 84%, while PM?s precision rose from 
60% to 97% and DM?s precision rose from 7% to 
62%. The detection mode of our system still has the 
highest f-score.  
The differences of performances in Table 7 and Table 
6 show that, systems in previous works tent to have 
false alarms in sentences without errors.  
5.5 Processing time comparison 
Processing complexity was not discussed in previous 
works. Since all the systems require different re-
sources, it is hard to compare the time or space com-
plexity. We list the average time it takes to process an 
essay for each method on our server as a reference. 
The processing time is less than 0.5 second for both 
our method and Huang?s method. Lin?s method re-
quired 3.85 sec and Chang?s method required more 
than 237 seconds. 
6 Conclusions 
In this paper, we proposed a new Chinese character 
checker that combines two kinds of technology and 
compared it to three previous methods. Our system 
achieved the best F-score performance by reducing 
the false alarm significantly. An error model adopted 
from the noisy channel model was proposed to make 
use of the frequency of common errors that we col-
lected from a training set. A simplified version of 
automatic template generation was also proposed to 
provide high precision character error detection. Fine 
tuning of the system can be done by adding more 
templates manually.  
The experiment results show that the main draw-
back of previous works is false alarms. Our systems 
have fewer false alarms. The combination of two in-
dependent methods gives the best results on real 
world data. In the future, we will find a way to com-
bine the independent methods with theoretical foun-
dation. 
Acknowledgement 
This study is conducted under the ?Intelligent Web - 
enabled Service Research and Development Project? 
of the Institute for Information Industry which is sub-
sidized by the Ministry of Economy Affairs of the 
Republic of China. 
References  
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. (1993). The ma-
thematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics 19 (pp. 
263-311). 
Chang, C.-H. (1995). A New Approach for Automatic 
Chinese Spelling Correction. In Proceedings of 
Natural Language Processing Pacific Rim Sympo-
sium, (pp. 278-283). Korea. 
Chen, S. F., & Goodman, J. (1996). An Empirical 
Study of Smoothing Techniques for Language 
Modeling. Proc. of the 34th annual meeting on As-
sociation for Computational Linguistics, (pp. 310-
318). Santa Cruz, California. 
Chen, Y.-Z., Wu, S.-H., Lu, C.-C., & Ku, T. (2009). 
Chinese Confusion Word Set for Automatic Gen-
eration of Spelling Error Detecting Template. The 
21th Conference on Computational Linguistics and 
Speech Processing (Rocling 2009), (pp. 359-372). 
Taichung. 
CKIP. (1999). AutoTag. Academia Sinica. 
Huang, C.-M., Wu, M.-C., & Chang, C.-C. (2007). 
Error Detection and Correction Based on Chinese 
Phonemic Alphabet in Chinese Text. Proceedings of 
the Fourth Conference on Modeling Decisions for 
Artificial Intelligence (MDAI IV), (pp. 463-476). 
Hung, T.-H., & Wu, S.-H. (2008). Chinese Essay Er-
ror Detection and Suggestion System. Taiwan E-
Learning Forum.  
Lin, Y.-J., Huang, F.-L., & Yu, M.-S. (2002). A 
CHINESE SPELLING ERROR CORRECTION 
SYSTEM. Proceedings of the Seventh Conference 
on Artificial Intelligence and Applications (TAAI).  
Liu, C.-L., Tien, K.-W., Lai, M.-H., Chuang, Y.-H., & 
Wu, S.-H. (2009). Capturing errors in written Chi-
nese words. Proceedings of the Forty Seventh An-
nual Meeting of the Association for Computational 
Linguistics (ACL'09), (pp. 25-28). Singapore. 
Liu, C.-L., Tien, K.-W., Lai, M.-H., Chuang, Y.-H., & 
Wu, S.-H. (2009). Phonological and logographic in-
fluences on errors in written Chinese words. Pro-
ceedings of the Seventh Workshop on Asian Lan-
guage Resources (ALR7), the Forty Seventh An-
nual Meeting of the Association for Computational 
Linguistics (ACL'09), (pp. 84-91). Singapore. 
Ma, W.-Y., & Chen, K.-J. (2003). A Bottom-up 
Merging Algorithm for Chinese. Proceedings of 
ACL workshop on Chinese Language Processing, 
(pp. 31-38). 
MOE. (2007). MOE Dictionary new edition. Taiwan: 
Ministry of Education. 
Ren, F., Shi, H., & Zhou, Q. (1994). A hybrid ap-
proach to automatic Chinese text checking and er-
ror correction. In Proceedings of the ARPA Work 
shop on Human Language Technology, (pp. 76-81). 
Zhang, L., Zhou, M., Huang, C., & Lu, M. (2000). 
Approach in automatic detection and correction of 
errors in Chinese text based on feature and learning. 
Proceedings of the 3rd world congress on Intelli-
gent Control and Automation, (pp. 2744-2748). He-
fei. 
Zhang, L., Zhou, M., Huang, C., & Sun, M. (2000). 
Automatic Chinese Text Error Correction Approach 
Based-on Fast Approximate Chinese Word-
Matching Algorithm. Proceedings of the 3rd world 
congress on Intelligent Control and Automation, (pp. 
2739-2743). Hefei. 
 
Table 4. Results of our initial system  
  Top 1 2 3 4 5 6 7 8 9 10 
TM 
P 5.74% 5.63% 5.21% 5.02% 4.90% 4.65% 4.36% 4.12% 4.06% 3.95% 
D 29.23% 41.25% 45.36% 49.17% 52.00% 53.47% 54.94% 55.13% 56.79% 57.09% 
C 26.00% 36.75% 40.08% 43.40% 45.65% 46.33% 46.92% 46.82% 48.00% 48.58% 
F 9.40% 9.76% 9.23% 8.99% 8.85% 8.46% 7.99% 7.58% 7.49% 7.31% 
LMM 
P 14.28% 
D 58.06% 
C 50.63% 
F 22.28% 
PM 
P 55.52% 60.03% 60.60% 61.58% 60.65% 61.68% 60.51% 61.19% 58.82% 59.03% 
D 21.60% 29.52% 31.28% 32.74% 34.21% 34.31% 34.31% 33.91% 35.19% 34.79% 
C 21.60% 29.42% 31.18% 32.64% 34.01% 34.11% 34.01% 33.62% 34.89% 34.50% 
F 31.10% 39.49% 41.17% 42.67% 43.58% 43.93% 43.55% 43.40% 43.80% 43.55% 
DM 
P 7.32% 6.15% 5.64% 5.33% 5.11% 4.87% 4.62% 4.42% 4.30% 4.19% 
D 62.75% 65.59% 67.44% 69.40% 70.38% 71.06% 71.94% 72.23% 72.62% 72.72%
C 54.05% 56.69% 58.16% 59.62% 60.60% 60.99% 61.68% 61.77% 61.58% 61.68%
F 12.89% 11.10% 10.28% 9.79% 9.43% 9.02% 8.60% 8.25% 8.04% 7.85% 
 
Table 5. Results of our system after adding more knowledge and enlarged the train set 
  Top 1 2 3 4 5 6 7 8 9 10 
TM 
P 7.31% 6.45% 5.73% 5.41% 5.12% 4.83% 4.51% 4.26% 4.20% 4.08% 
D 37.93% 47.70% 50.15% 53.18% 54.45% 55.62% 56.89% 57.09% 58.75% 59.04% 
C 34.70% 43.21% 44.87% 47.41% 47.70% 48.48% 48.88% 48.78% 49.95% 50.54% 
F 12.08% 11.23% 10.17% 9.70% 9.25% 8.79% 8.26% 7.84% 7.74% 7.55% 
LMM 
P 14.03% 
D 63.14% 
C 55.52% 
F 22.40% 
PM 
P 59.95% 62.72% 62.50% 62.88% 60.66% 61.72% 59.51% 60.29% 58.08% 58.54% 
D 27.66% 34.21% 35.19% 36.26% 35.58% 35.77% 35.77% 35.48% 36.85% 36.85% 
C 27.66% 34.11% 35.09% 36.16% 35.48% 35.67% 35.58% 35.28% 36.65% 36.65% 
F 37.85% 44.19% 44.95% 45.92% 44.77% 45.21% 44.53% 44.51% 44.94% 45.08%
DM 
P 7.76% 6.46% 5.85% 5.51% 5.28% 5.04% 4.78% 4.57% 4.45% 4.33% 
D 69.50% 71.26% 72.04% 73.50% 74.48% 75.26% 75.95% 76.05% 76.34% 76.34%
C 60.50% 62.17% 62.65% 63.73% 64.71% 65.29% 65.78% 65.68% 65.39% 65.39%
F 13.76% 11.70% 10.70% 10.14% 9.76% 9.36% 8.91% 8.55% 8.33% 8.12% 
 
Table 6. Results of methods in previous works 
  Top 1 2 3 4 5 6 7 8 9 10 
Chang 
P 2.82% 1.95% 1.63% 1.43% 1.25% 1.13% 1.07% 0.98% 0.94% 0.91% 
D 72.04% 81.72% 84.55% 88.27% 89.54% 90.32% 91.50% 91.50% 91.79% 91.59%
C 27.66% 39.10% 43.30% 45.45% 44.77% 45.16% 46.33% 45.26% 43.30% 44.28%
F 5.11% 3.71% 3.14% 2.77% 2.43% 2.21% 2.08% 1.92% 1.83% 1.77% 
Lin 
P 3.59% 3.19% 2.93% 2.82% 2.60% 2.51% 2.39% 2.35% 2.32% 2.31% 
D 25.12% 28.93% 29.91% 31.18% 30.98% 31.37% 31.18% 31.57% 32.16% 32.74%
C 19.45% 25.51% 26.78% 27.95% 28.05% 28.15% 28.25% 28.25% 28.73% 29.42%
F 6.06% 5.67% 5.28% 5.12% 4.76% 4.61% 4.41% 4.34% 4.29% 4.28% 
Huang 
P 27.02% 25.81% 25.02% 24.05% 23.30% 22.54% 22.04% 21.16% 20.98% 20.62%
D 10.75% 17.79% 23.06% 26.00% 28.54% 30.49% 31.37% 31.86% 33.33% 33.43%
C 8.30% 12.02% 15.54% 17.00% 17.39% 18.57% 19.64% 18.76% 17.69% 18.27%
F 12.70% 16.40% 19.17% 19.92% 19.92% 20.36% 20.77% 19.89% 19.20% 19.37%
 
Table 7. Results of methods in previous works on sentences with errors 
  Top 1 2 3 4 5 6 7 8 9 10 
Chang 
P 42.94% 37.21% 33.30% 31.18% 29.31% 27.19% 25.98% 24.48% 23.61% 23.14%
D 72.33% 81.62% 84.55% 88.26% 89.63% 90.51% 91.78% 91.79% 92.08% 91.89%
C 27.95% 39.58% 43.98% 46.23% 45.65% 46.04% 47.31% 46.14% 44.28% 45.26%
F 33.86% 38.36% 37.90% 37.24% 35.70% 34.19% 33.54% 31.99% 30.80% 30.62%
Lin 
P 60.59% 59.33% 57.32% 57.19% 55.10% 55.35% 54.27% 53.88% 53.80% 53.97%
D 25.70% 29.52% 30.59% 31.86% 31.67% 32.35% 32.25% 32.55% 33.13% 33.82%
C 19.55% 25.80% 27.37% 28.64% 29.03% 29.52% 29.61% 29.52% 30.00% 30.69%
F 29.56% 35.96% 37.05% 38.17% 38.03% 38.50% 38.32% 38.14% 38.52% 39.13%
Huang 
P 84.16% 76.99% 78.51% 76.11% 73.66% 74.07% 73.21% 70.19% 66.23% 66.66%
D 9.87% 16.03% 20.72% 23.36% 25.70% 27.37% 28.05% 28.54% 29.91% 29.91%
C 7.62% 10.85% 14.17% 15.64% 15.83% 16.71% 17.79% 17.20% 16.12% 16.61%
F 13.97% 19.02% 24.01% 25.95% 26.06% 27.27% 28.62% 27.63% 25.93% 26.59%
PM 
P 96.72% 96.66% 96.76% 96.57% 96.51% 96.54% 96.54% 96.23% 96.11% 96.10%
D 25.90% 31.09% 32.16% 33.04% 32.45% 32.75% 32.75% 32.45% 33.82% 33.72%
C 25.90% 30.98% 32.06% 32.94% 32.36% 32.65% 32.55% 32.26% 33.63% 33.53%
F 40.86% 46.92% 48.16% 49.13% 48.46% 48.80% 48.69% 48.32% 49.82% 49.71%
DM 
P 61.83% 58.45% 56.46% 54.75% 54.21% 53.48% 52.80% 51.53% 51.15% 50.45%
D 69.20% 70.97% 71.74% 73.22% 74.19% 74.98% 75.66% 75.76% 76.05% 76.05%
C 55.62% 57.28% 57.77% 58.84% 59.82% 60.41% 60.90% 60.80% 60.51% 60.51%
F 58.56% 57.86% 57.11% 56.72% 56.88% 56.73% 56.56% 55.78% 55.44% 55.03%
 
