Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 855?859,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying Sentiment Words
Using an Optimization-based Model without Seed Words
Hongliang Yu 1, Zhi-Hong Deng 2?, Shiyingxue Li 3
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing 100871, China
1yuhongliang324@gmail.com
2zhdeng@cis.pku.edu.cn
3rachellieinspace@gmail.com
Abstract
Sentiment Word Identification (SWI) is a
basic technique in many sentiment analy-
sis applications. Most existing research-
es exploit seed words, and lead to low ro-
bustness. In this paper, we propose a novel
optimization-based model for SWI. Unlike
previous approaches, our model exploits
the sentiment labels of documents instead
of seed words. Several experiments on re-
al datasets show that WEED is effective
and outperforms the state-of-the-art meth-
ods with seed words.
1 Introduction
In recent years, sentiment analysis (Pang et al,
2002) has become a hotspot in opinion mining and
attracted much attention. Sentiment analysis is to
classify a text span into different sentiment polar-
ities, i.e. positive, negative or neutral. Sentimen-
t Word Identification (SWI) is a basic technique
in sentiment analysis. According to (Ku et al,
2006)(Chen et al, 2012)(Fan et al, 2011), SWI
can be applied to many fields, such as determin-
ing critics opinions about a given product, tweeter
classification, summarization of reviews, and mes-
sage filtering, etc. Thus in this paper, we focus on
SWI.
Here is a simple example of how SWI is applied
to comment analysis. The sentence below is an
movie review in IMDB database:
? Bored performers and a lackluster plot and
script, do not make a good action movie.
In order to judge the sentence polarity (thus we can
learn about the preference of this user), one must
recognize which words are able to express senti-
ment. In this sentence, ?bored? and ?lackluster?
are negative while ?good? should be positive, yet
?Corresponding author
its polarity is reversed by ?not?. By such analy-
sis, we then conclude such movie review is a nega-
tive comment. But how do we recognize sentiment
words?
To achieve this, previous supervised approach-
es need labeled polarity words, also called seed
words, usually manually selected. The words
to be classified by their sentiment polarities are
called candidate words. Prior works study the re-
lations between labeled seed words and unlabeled
candidate words, and then obtain sentiment polar-
ities of candidate words by these relations. There
are many ways to generate word relations. The
authors of (Turney and Littman, 2003) and (Kaji
and Kitsuregawa, 2007) use statistical measures,
such as point wise mutual information (PMI), to
compute similarities in words or phrases. Kanaya-
ma and Nasukawa (2006) assume sentiment word-
s successively appear in the text, so one could
find sentiment words in the context of seed words
(Kanayama and Nasukawa, 2006). In (Hassan and
Radev, 2010) and (Hassan et al, 2011), a Markov
random walk model is applied to a large word re-
latedness graph, constructed according to the syn-
onyms and hypernyms in WordNet (Miller, 1995).
However, approaches based on seed words has
obvious shortcomings. First, polarities of seed
words are not reliable for various domains. As
a simple example, ?rise? is a neutral word most
often, but becomes positive in stock market. Sec-
ond, manually selection of seed words can be very
subjective even if the application domain is deter-
mined. Third, algorithms using seed words have
low robustness. Any missing key word in the set
of seed words could lead to poor performance.
Therefore, the seed word set of such algorithms
demands high completeness (by containing com-
mon polarity words as many as possible).
Unlike the previous research work, we identi-
fy sentiment words without any seed words in this
paper. Instead, the documents? bag-of-words in-
855
formation and their polarity labels are exploited in
the identification process. Intuitively, polarities of
the document and its most component sentimen-
t words are the same. We call such phenomenon
as ?sentiment matching?. Moreover, if a word is
found mostly in positive documents, it is very like-
ly a positive word, and vice versa.
We present an optimization-based model, called
WEED, to exploit the phenomenon of ?sentimen-
t matching?. We first measure the importance of
the component words in the labeled documents se-
mantically. Here, the basic assumption is that im-
portant words are more sentiment related to the
document than those less important. Then, we
estimate the polarity of each document using it-
s component words? importance along with their
sentiment values, and compare the estimation to
the real polarity. After that, we construct an op-
timization model for the whole corpus to weigh
the overall estimation error, which is minimized
by the best sentiment values of candidate words.
Finally, several experiments demonstrate the ef-
fectiveness of our approach. To the best of our
knowledge, this paper is the first work that identi-
fies sentiment words without seed words.
2 The Proposed Approach
2.1 Preliminary
We formulate the sentiment word identification
problem as follows. Let D = {d1, . . . , dn} denote
document set. Vector l? =
?
??
l1...
ln
?
?? represents their
labels. If document di is a positive sample, then
li = 1; if di is negative, then li = ?1. We use the
notation C = {c1, . . . , cV } to represent candidate
word set, and V is the number of candidate words.
Each document is formed by consecutive words in
C. Our task is to predict the sentiment polarity of
each word cj ? C.
2.2 Word Importance
We assume each document di ? D is presented
by a bag-of-words feature vector f?i =
?
??
fi1...
fiV
?
??,
where fij describes the importance of cj to di. A
high value of fij indicates word cj contributes a
lot to document di in semantic view, and vice ver-
sa. Note that fij > 0 if cj appears in di, while
fij = 0 if not. For simplicity, every f?i is normal-
ized to a unit vector, such that features of different
documents are relatively comparable.
There are several ways to define the word
importance, and we choose normalized TF-IDF
(Jones, 1972). Therefore, we have fij ?
TF?IDF (di, cj), and ?f?i? = 1.
2.3 Polarity Value
In the above description, the sentiment polarity has
only two states, positive or negative. We extend
both word and document polarities to polarity val-
ues in this section.
Definition 1 Word Polarity Value: For each word
cj ? C, we denote its word polarity value as
w(cj). w(cj) > 0 indicates cj is a positive word,
while w(cj) < 0 indicates cj is a negative word.
|w(cj)| indicates the strength of the belief of cj?s
polarity. Denote w(cj) as wj , and the word polar-
ity value vector w? =
?
??
w1...
wV
?
??.
For example, ifw(?bad?) < w(?greedy?) < 0, we
can say ?bad? is more likely to be a negative word
than ?greedy?.
Definition 2 Document Polarity Value: For each
document di, document polarity value is
y(di) = cosine(f?i, w?) =
f?i
T ? w?
?w?? . (1)
We denote y(di) as yi for short.
Here, we can regard yi as a polarity estimate
for di based on w?. To explain this, Table 1 shows
an example. ?MR1?, ?MR2? and ?MR3? are
three movie review documents, and ?compelling?
and ?boring? are polarity words in the vocabu-
lary. we simply use TF to construct the document
feature vectors without normalization. In the ta-
ble, these three vectors, f?1, f?2 and f?3, are (3, 1),
(2, 1) and (1, 3) respectively. Similarly, we can get
w? = (1,?1), indicating ?compelling? is a positive
word while ?boring? is negative. After normaliz-
ing f?1, f?2 and f?3, and calculating their cosine sim-
ilarities with w?, we obtain y1 > y2 > 0 > y3.
These inequalities tell us the first two reviews are
positive, while the last review is negative. Further-
more, we believe that ?MR1? is more positive than
?MR2?.
856
?compelling? ?boring?
MR1 3 1
MR2 2 1
MR3 1 3
w 1 -1
Table 1: Three rows in the middle shows the fea-
ture vectors of three movie reviews, and the last
row shows the word polarity value vector w?. For
simplicity, we use TF value to represent the word
importance feature.
2.4 Optimization Model
As mentioned above, we can regard yi as a polari-
ty estimate for document di. A precise prediction
makes the positive document?s estimator close to
1, and the negative?s close to -1. We define the
polarity estimate error for document di as:
ei = |yi ? li| = |
f?i
T ? w?
?w?? ? li|. (2)
Our learning procedure tries to decrease ei. We
obtain w? by minimizing the overall estimation er-
ror of all document samples
n?
i=1
e2i . Thus, the op-
timization problem can be described as
min
w?
n?
i=1
( f?i
T ? w?
?w?? ? li)
2. (3)
After solving this problem, we not only obtain the
polarity of each word cj according to the sign of
wj , but also its polarity belief based on |wj |.
2.5 Model Solution
We use normalized vector x? to substitute w??w?? , andderive an equivalent optimization problem:
min
x?
E(x?) =
n?
i=1
(f?i
T ? x?? li)2
s.t. ?x?? = 1.
(4)
The equality constraint of above model makes
the problem non-convex. We relax the equality
constraint to ?x?? ? 1, then the problem becomes
convex. We can rewrite the objective function
as the form of least square regression: E(x?) =
?F ? x? ? l??2, where F is the feature matrix, and
equals to
?
???
f?1
T
...
f?n
T
?
???.
Now we can solve the problem by convex op-
timization algorithms (Boyd and Vandenberghe,
2004), such as gradient descend method. In each
iteration step, we update x? by ?x? = ? ? (??E) =
2? ? (F T l? ? F TF x?), where ? > 0 is the learning
rate.
3 Experiment
3.1 Experimental Setup
We leverage two widely used document dataset-
s. The first dataset is the Cornell Movie Review
Data 1, containing 1,000 positive and 1,000 nega-
tive processed reviews. The other is the Stanford
Large Dataset 2 (Maas et al, 2011), a collection
of 50,000 comments from IMDB, evenly divided
into training and test sets.
The ground-truth is generated with the help of
a sentiment lexicon, MPQA subjective lexicon 3.
We randomly select 20% polarity words as the
seed words, and the remaining are candidate ones.
Here, the seed words are provided for the baseline
methods but not for ours. In order to increase the
difficulty of our task, several non-polarity words
are added to the candidate word set. Table 2 shows
the word distribution of two datasets.
Dataset Word Set pos neg non total
Cornell seed 135 201 - 336candidate 541 806 1232 2579
Stanford seed 202 343 - 545candidate 808 1370 2566 4744
Table 2: Word Distribution
In order to demonstrate the effectiveness of our
model, we select two baselines, SO-PMI (Turney
and Littman, 2003) and COM (Chen et al, 2012).
Both of them need seed words.
3.2 Top-K Test
In face of the long lists of recommended polarity
words, people are only concerned about the top-
ranked words with the highest sentiment value. In
this experiment we consider the accuracy of the
top K polarity words. The quality of a polarity
word list is measured by p@K = Nright,KK , where
Nright,K is the number of top-K words which are
correctly recommended.
1http://www.cs.cornell.edu/people/pabo/movie-review-
data/
2http://ai.stanford.edu/ amaas/data/sentiment/
3http://www.cs.pitt.edu/mpqa/
857
WEED SO-PMI COM
positive words negative words positive words negative words positive words negative words
great excellent bad stupid destiny lush cheap worst best great ridiculous bad
perfect perfectly worst mess brilliant skillfully ridiculous annoying will star plot evil
terrific best boring ridiculous courtesy courtesy damn pathetic bad fun star garish
true wonderfully awful plot gorgeous magnificent inconsistencies fool better plot dreadfully stupid
brilliant outstanding worse terrible temptation marvelously desperate giddy love horror pretty fun
Table 3: Case Study
(a) Cornell Dataset
(b) Stanford Dataset
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
p@10 p@20 p@50 p@100
WEED
SO_PMI
COM
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
p@10 p@20 p@50 p@100
WEED
SO_PMI
COM
Figure 1: Top-K Test
Figure 1 shows the final result of p@K, which
is the average score of the positive and negative
list. We can see that in both datasets, our approach
highly outperforms two baselines, and the preci-
sion is 14.4%-33.0% higher than the best baseline.
p@10s of WEED for Cornell and Stanford dataset-
s reach to 93.5% and 89.0%, and it shows the top
10 words in our recommended list is exceptionally
reliable. As the size of K increases, the accuracy
of all methods falls accordingly. This shows three
approaches rank the most probable polarity words
in the front of the word list. Compared with the
small dataset, we obtain a better result with large
K on the Stanford dataset.
3.3 Case Study
We conduct an experiment to illustrate the char-
acteristics of three methods. Table 3 shows top-
10 positive and negative words for each method,
where the bold words are the ones with correc-
t polarities. From the first two columns, we can
see the accuracy of WEED is very high, where
positive words are absolutely correct and negative
word list makes only one mistake, ?plot?. The oth-
er columns of this table shows the baseline meth-
ods both achieve reasonable results but do not per-
form as well as WEED.
Our approach is able to identify frequently used
sentiment words, which are vital for the applica-
tions without prior sentiment lexicons. The sen-
timent words identified by SO-PMI are not so
representative as WEED and COM. For example,
?skillfully? and ?giddy? are correctly classified but
they are not very frequently used. COM tends to
assign wrong polarities to the sentiment words al-
though these words are often used. In the 5th and
6th columns of Table 3, ?bad? and ?horror? are
recognized as positive words, while ?pretty? and
?fun? are recognized as negative ones. These con-
crete results show that WEED captures the gener-
ality of the sentiment words, and achieves a higher
accuracy than the baselines.
4 Conclusion and Future Work
We propose an effective optimization-based mod-
el, WEED, to identify sentiment words from the
corpus without seed words. The algorithm exploit-
s the sentiment information provided by the docu-
ments. To the best of our knowledge, this paper is
the first work that identifies sentiment words with-
out any seed words. Several experiments on real
datasets show that WEED outperforms the state-
of-the-art methods with seed words.
Our work can be considered as the first step
of building a domain-specific sentiment lexicon.
Once some sentiment words are obtained in a cer-
tain domain, our future work is to improve WEED
by utilizing these words.
858
Acknowledgments
This work is partially supported by National Nat-
ural Science Foundation of China (Grant No.
61170091).
References
S. Boyd and L. Vandenberghe. 2004. Convex optimiza-
tion. Cambridge university press.
L. Chen, W. Wang, M. Nagarajan, S. Wang, and A.P.
Sheth. 2012. Extracting diverse sentiment expres-
sions with target-dependent polarity from twitter. In
Proceedings of the Sixth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
50?57.
Wen Fan, Shutao Sun, and Guohui Song. 2011.
Probability adjustment na??ve bayes algorithm based
on nondomain-specific sentiment and evaluation
word for domain-transfer sentiment analysis. In
Fuzzy Systems and Knowledge Discovery (FSKD),
2011 Eighth International Conference on, volume 2,
pages 1043?1046. IEEE.
A. Hassan and D. Radev. 2010. Identifying text po-
larity using random walks. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 395?403. Association for
Computational Linguistics.
A. Hassan, A. Abu-Jbara, R. Jha, and D. Radev. 2011.
Identifying the semantic orientation of foreign word-
s. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?597.
K.S. Jones. 1972. A statistical interpretation of term
specificity and its application in retrieval. Journal of
documentation, 28(1):11?21.
N. Kaji and M. Kitsuregawa. 2007. Building lexicon
for sentiment analysis from massive collection of
html documents. In Proceedings of the joint confer-
ence on empirical methods in natural language pro-
cessing and computational natural language learn-
ing (EMNLP-CoNLL), pages 1075?1083.
H. Kanayama and T. Nasukawa. 2006. Fully automat-
ic lexicon expansion for domain-oriented sentiment
analysis. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?363. Association for Computational
Linguistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and track-
ing in news and blog corpora. In Proceedings of
AAAI-2006 spring symposium on computational ap-
proaches to analyzing weblogs, volume 2001.
A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng,
and C. Potts. 2011. Learning word vectors for sen-
timent analysis. In Proceedings of the 49th annu-
al meeting of the association for computational Lin-
guistics (acL-2011).
Miller. 1995. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learn-
ing techniques. In Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing-Volume 10, pages 79?86. Association for
Computational Linguistics.
P. Turney and M.L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation
from association.
859
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 218?223,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Novel Content Enriching Model for Microblog Using News Corpus
Yunlun Yang
1
, Zhihong Deng
2?
, Hongliang Yu
3
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing 100871, China
1
incomparable-lun@pku.edu.cn
2
zhdeng@cis.pku.edu.cn
3
yuhongliang324@gmail.com
Abstract
In this paper, we propose a novel model for
enriching the content of microblogs by ex-
ploiting external knowledge, thus improv-
ing the data sparseness problem in short
text classification. We assume that mi-
croblogs share the same topics with ex-
ternal knowledge. We first build an opti-
mization model to infer the topics of mi-
croblogs by employing the topic-word dis-
tribution of the external knowledge. Then
the content of microblogs is further en-
riched by relevant words from external
knowledge. Experiments on microblog
classification show that our approach is
effective and outperforms traditional text
classification methods.
1 Introduction
During the past decade, the short text represen-
tation has been intensively studied. Previous re-
searches (Phan et al, 2008; Guo and Diab, 2012)
show that while traditional methods are not so
powerful due to the data sparseness problem, some
semantic analysis based approaches are proposed
and proved effective, and various topic models are
among the most frequently used techniques in this
area. Meanwhile, external knowledge has been
found helpful (Hu et al, 2009) in tackling the da-
ta scarcity problem by enriching short texts with
informative context. Well-organized knowledge
bases such as Wikipedia and WordNet are com-
mon tools used in relevant methods.
Nowadays, most of the work on short text fo-
cuses on microblog. As a new form of short tex-
t, microblog has some unique features like infor-
mal spelling and emerging words, and many mi-
croblogs are strongly related to up-to-date topics
as well. Every day, a great quantity of microblogs
?
Corresponding author
more than we can read is pushed to us, and find-
ing what we are interested in becomes rather dif-
ficult, so the ability of choosing what kind of mi-
croblogs to read is urgently demanded by common
user. Such ability can be implemented by effective
short text classification.
Treating microblogs as standard texts and di-
rectly classifying them cannot achieve the goal of
effective classification because of sparseness prob-
lem. On the other hand, news on the Internet is
of information abundance and many microblogs
are news-related. They share up-to-date topics
and sometimes quote each other. Thus, external
knowledge, such as news, provides rich supple-
mentary information for analysing and mining mi-
croblogs.
Motivated by the idea of using topic model and
external knowledge mentioned above, we present
an LDA-based enriching method using the news
corpus, and apply it to the task of microblog clas-
sification. The basic assumption in our model is
that news articles and microblogs tend to share the
same topics. We first infer the topic distribution
of each microblog based on the topic-word distri-
bution of news corpus obtained by the LDA esti-
mation. With the above two distributions, we then
add a number of words from news as additional
information to microblogs by evaluating the relat-
edness of between each word and microblog, since
words not appearing in the microblog may still be
highly relevant.
To sum up, our contributions are:
(1) We formulate the topic inference problem for
short texts as a convex optimization problem.
(2) We enrich the content of microblogs by infer-
ring the association between microblogs and
external words in a probabilistic perspective.
(3) We evaluate our method on the real dataset-
s and experiment results outperform the base-
line methods.
218
2 Related Work
Based on the idea of exploiting external knowl-
edge, many methods are proposed to improve the
representation of short texts for classification and
clustering. Among them, some directly utilize
the structure information of organized knowledge
base or search engine. Banerjee et al (2007) use
the title and the description of news article as two
separate query strings to select related concept-
s as additional feature. Hu et al (2009) present
a framework to improve the performance of short
text clustering by mining informative context with
the integration of Wikipedia and WordNet.
However, to better leverage external resource,
some other methods introduce topic models. Phan
et al (2008) present a framework including an
approach for short text topic inference and adds
abstract words as extra feature. Guo and Diab
(2012) modify classic topic models and propos-
es a matrix-factorization based model for sentence
similarity calculation tasks.
Those methods without topic model usually re-
ly greatly on the performance of search system or
the completeness of knowledge base, and lack in-
depth analysis for external resources. Compared
with our method, the topic model based method-
s mentioned above remain in finding latent space
representation of short text and ignore that rele-
vant words from external knowledge are informa-
tive as well.
3 Our Model
We formulate the problem as follows. Let
EK = {d
e
1
, . . . , d
e
M
e
} denote external knowl-
edge consisting of M
e
documents. V
e
=
{w
e
1
, . . . , w
e
N
e
} represents its vocabulary. Let
MB = {d
m
1
, . . . , d
m
M
m
} denote microblog set and
its vocabulary is V
m
= {w
m
1
, . . . , w
m
N
m
}. Our
task is to enrich each microblog with additional
information so as to improve microblog?s repre-
sentation.
The model we proposed mainly consists of three
steps:
(a) Topic inference for external knowledge by
running LDA estimation.
(b) Topic inference for microblogs by employing
the word distributions of topics obtained from
step (a).
(c) Select relevant words from external knowl-
edge to enrich the content of microblogs.
3.1 Topic Inference for External Knowledge
We do topic analysis for EK using LDA esti-
mation (Blei et al, 2003) in this section and we
choose LDA as the topic analysis model because
of its broadly proved effectivity and ease of under-
standing.
In LDA, each document has a distribution over
all topics P (z
k
|d
j
), and each topic has a distri-
bution over all words P (w
i
|z
k
), where z
k
, d
j
and
w
i
represent the topic, document and word respec-
tively. The optimization problem is formulated as
maximizing the log likelihood on the corpus:
max
?
i
?
j
X
ij
log
?
k
P (z
k
|d
j
)P (w
i
|z
k
) (1)
In this formulation, X
ij
represents the term fre-
quency of word w
i
in document d
j
. P (z
k
|d
j
)
and P (w
i
|z
k
) are parameters to be inferred, cor-
responding to the topic distribution of each doc-
ument and the word distribution of each topic re-
spectively. Estimating parameters for LDA by di-
rectly and exactly maximizing the likelihood of
the corpus in (1) is intractable, so we use Gibbs
Sampling for estimation.
After performing LDA model (K topics) esti-
mation on EK , we obtain the topic distribution-
s of document d
e
j
(j = 1, . . . ,M
e
), denoted as
P (z
e
k
|d
e
j
) (k = 1, . . . ,K), and the word distri-
bution of topic z
e
k
(k = 1, . . . ,K), denoted as
P (w
e
i
|z
e
k
) (i = 1, . . . , N
e
). Step (b) greatly re-
lies on the word distributions of topics we have
obtained here.
3.2 Topic Inference for Microblog
In this section, we infer the topic distribution of
each microblog. Because of the assumption that
microblogs share the same topics with external
corpus, the ?topic distribution? here refers to a dis-
tribution over all topics on EK .
Differing from step (a), the method used for
topic inference for microblogs is not directly run-
ning LDA estimation on microblog collection but
following the topics from external knowledge to
ensure topic consistence. We employ the word
distributions of topics obtained from step (a), i.e.
P (w
e
i
|z
e
k
), and formulate the optimization prob-
lem in a similar form to Formula (1) as follows:
219
max
P (z
e
k
|d
m
j
)
?
i
?
j
X
ij
log
?
k
P (z
e
k
|d
m
j
)P (w
e
i
|z
e
k
),
(2)
where X
ij
represents the term frequency of word
w
e
i
in microblog d
m
j
, and P (z
e
k
|d
m
j
) denote the dis-
tribution of microblog d
m
j
over all topics on EK .
Obviously most X
ij
are zero and we ignore those
words that do not appear in V
e
.
Compared with the original LDA optimization
problem (1), the topic inference problem for mi-
croblog (2) follows the idea of document gener-
ation process, but replaces topics to be estimated
with known topics from other corpus. As a result,
parameters to be inferred are only the topic distri-
bution of every microblog.
It is noteworthy that since the word distribution
of every topic P (w
e
i
|z
e
k
) is known, Formula (2) can
be further solved by separating it into M
m
sub-
problems:
max
P (z
e
k
|d
m
j
)
?
i
X
ij
log
?
k
P (z
e
k
|d
m
j
)P (w
e
i
|z
e
k
)
for j = 1, . . . ,M
m
(3)
These M
m
subproblems correspond to the M
m
microblogs and can be easily proved convexity.
After solving them, we obtain the topic distribu-
tions of microblog d
m
j
(j = 1, . . . ,M
m
), denoted
as P (z
e
k
|d
m
j
) (k = 1, . . . ,K).
3.3 Select Relevant Words for Microblog
To enrich the content of every microblog, we s-
elect relevant words from external knowledge in
this section.
Based on the results of step (a)&(b), we calcu-
late the word distributions of microblogs as fol-
lows:
P (w
e
i
|d
m
j
) =
?
k
P (z
e
k
|d
m
j
)P (w
e
i
|z
e
k
), (4)
where P (w
e
i
|d
m
j
) represents the probability that
word w
e
i
will appear in microblog d
m
j
. In other
words, though some words may not actually ap-
pears in a microblog, there is still a probability that
it is highly relevant to the microblog. Intuitively,
this probability indicates the strength of associa-
tion between a word and a microblog. The word
distribution of every microblog is based on topic
analysis and its accuracy relies heavily on the ac-
curacy of topic inference in step (b). In fact, the
more words a microblog includes, the more accu-
rate its topic inference will be, and this can be re-
garded as an explanation of the low efficiency of
data sparseness problem.
For microblog d
m
j
, we sort all words by
P (w
e
i
|d
m
j
) in descending order. Having known
the top L relevant words according to the result of
sorting, we redefine the ?term frequency? of every
word after adding these L words to microblog d
m
j
as additional content. Supposing these L words
are w
e
j1
, w
e
j2
, . . . , w
e
jL
, the revised term frequency
of word w ? {w
e
j1
, . . . , w
e
jL
} is defined as fol-
lows:
RTF (w, d
m
j
) =
P (w|d
m
j
)
?
L
p=1
P (w
e
jp
|d
m
j
)
? L, (5)
where RTF (?) is the revised term frequency.
As the Equation (5) shows, the revised term fre-
quency of every word is proportional to probabili-
ty P (w
i
|d
m
j
) rather than a constant.
So far, we can add these L words and their re-
vised term frequency as additional information to
microblog d
m
j
. The revised term frequency plays
the same role as TF in common text representation
vector, so we calculate the TFIDF of the added
words as:
TFIDF (w, d
m
j
) = RTF (w, d
m
j
) ?IDF (w) (6)
Note that IDF (w) is changed as arrival of new
words for each microblog. The TFIDF vector of
a microblog with additional words is called en-
hanced vector.
4 Experiment
4.1 Experimental Setup
To evaluate our method, we build our own dataset-
s. We crawl 95028 Chinese news reports from
Sina News website, segment them, and remove
stop words and rare words. After preprocessing,
these news documents are used as external knowl-
edge. As for microblog, we crawl a number of
microblogs from Sina Weibo, and ask unbiased
assessors to manually classify them into 9 cate-
gories following the column setting of Sina News.
Sina News: http://news.sina.com.cn/
Sina Weibo: http://www.weibo.com/
220
After the manual classification, we remove short
microblogs (less than 10 words), usernames, links
and some special characters, then we segmen-
t them and remove rare words as well. Finally, we
get 1671 classified microblogs as our microblog
dataset. The size of each category is shown in Ta-
ble 1.
Category #Microblog
Finance 229
Stock 80
Entertainment 162
Military Affairs 179
Technologies 204
Digital Products 194
Sports 195
Society 214
Daily Life 214
Table 1: Microblog number of every category
There are some important details of our imple-
mentation. In step (a) of Section 3.1 we estimate
LDA model using GibbsLDA++, a C/C++ imple-
mentation of LDA using Gibbs Sampling. In step
(b) of Section3.2, OPTI toolbox on Matlab is used
to help solve the convex problems. In the clas-
sification tasks shown below, we use LibSVM as
classifier and perform ten-fold cross validation to
evaluate the classification accuracy.
4.2 Classification Results
Representation Average Accuracy
TFIDF vector 0.7552
Boolean vector 0.7203
Enhanced vector 0.8453
Table 2: Classification accuracy with different rep-
resentations
In this section, we report the average preci-
sion of each method as shown in Table 2. The
enhanced vector is the representation generated
by our method. Two baselines are TFIDF vec-
tor (Jones, 1972) and boolean vector (word oc-
currence) of the original microblog. In the table,
our method increases the classification accuracy
GibbsLDA++: http://gibbslda.sourceforge.net
OPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/
SVM.NET: http://www.matthewajohnson.org/software
/svm.html
from 75.52% to 84.53% when considering addi-
tional information, which means our method in-
deed improves the representation of microblogs.
4.3 Parameter Tuning
4.3.1 Effect of Added Words
0.805
0.81
0.815
0.82
0.825
0.83
0.835
0.84
0.845
0.85
50 100 150 200 300 400 500
A
ve
ra
ge
 A
cc
ur
ac
y
Number of Added Words (L)
Figure 1: Classification accuracy changes accord-
ing to topics and added words
The experiment corresponding to Figure 1 is to
discover how the classification accuracy changes
when we fix the number of topics (K = 100)
and change the number of added words (L) in our
method. Result shows that more added words do
not mean higher accuracy. By studying some cas-
es, we find out that if we add too many words,
the proportion of ?noisy words? will increase. We
reach the best result when number of added words
is 300.
4.3.2 Effect of Topic Number
0.82
0.825
0.83
0.835
0.84
0.845
0.85
50 100 200 300
A
ve
ra
ge
 A
cc
ur
ac
y
Number of Topics (K)
Figure 2: Classification accuracy changing ac-
cording to the number of topics
The experiment corresponding to Figure 2 is to
discover how the classification accuracy changes
when we fix the number of added words (L =
221
Microblog (Translated) Top Relevant Words (Translated)
Kim Jong Un held an emergency meeting this morn-
ing, and commanded the missile units to prepare for
attacking U.S. military bases at any time.
South Korea, America, North Korea, work,
safety, claim, military, exercise, united, report
Shenzhou Nine will carry three astronauts, including
the first Chinese female astronaut, and launch in a
proper time during the middle of June.
day, satellite, launch, research, technology,
system, mission, aerospace, success, Chang?e
Two
Table 3: Case study (Translated from Chinese)
300) and change the number of topics (K) in
our method. As we can see, the accuracy does
not grow monotonously as the number of topic-
s increases. Blindly enlarging the topic number
will not improve the accuracy. The best result is
reached when topic number is 100, and similar ex-
periments adding different number of words show
the same condition of reaching the best result.
4.3.3 Effect of Revised Term Frequency
0.805
0.81
0.815
0.82
0.825
0.83
0.835
0.84
0.845
0.85
50 100 150 200 300 400 500
A
ve
ra
ge
 A
cc
ur
ac
y
Number of Added Words (L)
Using RTF Using TF
Figure 3: Classification accuracy changing ac-
cording to the redefinition of term frequency
The experiment corrsponding to Figure 3 is to
discover whether our redefining ?term frequency?
as revised term frequency in step (c) of Section
3.3 will affect the classification accuracy and how.
The results should be analysed in two aspects. On
one hand, without redefinition, the accuracy re-
mains in a stable high level and tends to decrease
as we add more words. One reason for the de-
creasing is that ?noisy words? have a increasing
negative impact on the accuracy as the propor-
tion of ?noisy words? grows with the number of
added words. On the other hand, the best result
is reached when we use the revise term frequen-
cy. This suggests that our redefinition for term fre-
quency shows better improvement for microblog
representation under certain conditions, but is not
optimal under all situations.
4.4 Case Study
In Table 3, we select several cases consisting of
microblogs and their top relevant words .
In the first case, we successfully find the country
name according to its leader?s name and limited
information in the sentence. Other related coun-
tries and events are also selected by our model as
they often appear together in news. In the other
case, relevant words are among the most frequent-
ly used words in news and have close semantic re-
lations with the microblogs in certain aspects.
As we can see, based on topic analysis, our
model shows strong ability of mining relevan-
t words. Other cases show that the model can be
further improved by removing the noisy and mean-
ingless ones among added words.
5 Conclusion and Future Work
We propose an effective content enriching method
for microblog, to enhance classification accuracy.
News corpus is exploited as external knowledge.
As for techniques, our method uses LDA as its
topic analysis model and formulates topic infer-
ence for new data as convex optimization prob-
lems. Compared with traditional representation,
enriched microblog shows great improvement in
classification tasks.
As we do not control the quality of added words,
our future work starts from building a filter to se-
lect better additional information. And to make the
most of external knowledge, better ways to build
topic space should be considered.
Acknowledgments
This work is supported by National Natural Sci-
ence Foundation of China (Grant No. 61170091).
222
References
Banerjee, S., Ramanathan, K., and Gupta, A. 2007,
July. Clustering short texts using wikipedia. In Pro-
ceedings of the 30th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval (pp. 787-788). ACM.
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent
Dirichlet Allocation. In Journal of machine Learn-
ing research, 3, 993-1022.
Bollegala, D., Matsuo, Y., and Ishizuka, M. 2007.
Measuring semantic similarity between words using
web search engines. www, 7, 757-766.
Boyd, S. P., and Vandenberghe, L. 2004. Convex opti-
mization. Cambridge university press.
Gabrilovich, E., and Markovitch, S. 2007, January.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In IJCAI (Vol. 7,
pp. 1606-1611).
Guo,W., and Diab, M. 2012, July. Modeling sentences
in the latent space. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1 (pp. 864-872).
Guo, W., and Diab, M. 2012, July. Learning the latent
semantics of a concept from its definition. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Short Papers-
Volume 2 (pp. 140-144).
Hu, X., Sun, N., Zhang, C., and Chua, T. S. 2009,
November. Exploiting internal and external seman-
tics for the clustering of short texts using world
knowledge. In Proceedings of the 18th ACM con-
ference on Information and knowledge management
(pp. 919-928). ACM.
Jones, K. S. 1972. A statistical interpretation of term
specificity and its application in retrieval. In Journal
of documentation, 28(1), 11-21
Phan, X. H., Nguyen, L. M., and Horiguchi, S. 2008,
April. Learning to Classify Short and Sparse Text &
Web with Hidden Topics from Large-scale Data Col-
lections. In Proceedings of the 17th international
conference on World Wide Web (pp. 91-100). ACM.
Sahami, M., and Heilman, T. D. 2006, May. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web (pp. 377-
386). ACM.
Zubiaga, A., and Ji, H. 2013, May. Harnessing we-
b page directories for large-scale classification of
tweets. In Proceedings of the 22nd international
conference on World Wide Web companion (pp. 225-
226). InternationalWorld WideWeb Conferences S-
teering Committee.
223
