Syntactic features for high precision Word Sense Disambiguation 
 
David Mart?nez, Eneko Agirre  
IxA NLP Group 
University of the Basque Country 
Donostia, Spain 
{jibmaird,eneko}@si.ehu.es 
Llu?s M?rquez 
TALP Research Center 
Polytechnical University of Catalonia 
Barcelona, Spain 
lluism@lsi.upc.es 
 
Abstract 
This paper explores the contribution 
of a broad range of syntactic features 
to WSD: grammatical relations coded 
as the presence of adjuncts/arguments 
in isolation or as subcategorization 
frames, and instantiated grammatical 
relations between words. We have 
tested the performance of syntactic 
features using two different ML 
algorithms (Decision Lists and 
AdaBoost) on the Senseval-2 data. 
Adding syntactic features to a basic 
set of traditional features improves 
performance, especially for AdaBoost. 
In addition, several methods to build 
arbitrarily high accuracy WSD 
systems are also tried, showing that 
syntactic features allow for a precision 
of 86% and a coverage of 26% or 95% 
precision and 8% coverage.  
1. Introduction 
Supervised learning has become the most 
successful paradigm for Word Sense 
Disambiguation (WSD). This kind of algorithms 
follows a two-step process: 
1. Choosing the representation as a set of 
features for the context of occurrence of the 
target word senses.  
2. Applying a Machine Learning (ML) 
algorithm to train on the extracted features 
and tag the target word in the test examples.  
Current WSD systems attain high performances 
for coarse word sense differences (two or three 
senses) if enough training material is available. 
In contrast, the performance for finer-grained 
sense differences (e.g. WordNet senses as used 
in Senseval 2 (Preiss & Yarowsky, 2001)) is far 
from application needs. Nevertheless, recent 
work (Agirre and Martinez, 2001a) shows that it 
is possible to exploit the precision-coverage 
trade-off and build a high precision WSD system 
that tags a limited number of target words with a 
predefined precision.  
This paper explores the contribution of a 
broad set of syntactically motivated features that 
ranges from the presence of complements and 
adjuncts, and the detection of subcategorization 
frames, up to grammatical relations instantiated 
with specific words. The performance of the 
syntactic features is measured in isolation and in 
combination with a basic set of local and topical 
features (as defined in the literature), and using 
two ML algorithms: Decision Lists (Dlist) and 
AdaBoost (Boost). While Dlist does not attempt 
to combine the features, i.e. it takes the strongest 
feature only, Boost tries combinations of 
features and also uses negative evidence, i.e. the 
absence of features. 
Additionally, the role of syntactic features in 
a high-precision WSD system based on the 
precision-coverage trade-off is also investigated.  
The paper is structured as follows. Section 2 
reviews the features previously used in the 
literature. Section 3 defines a basic feature set 
based on the preceding review. Section 4 
presents the syntactic features as defined in our 
work, alongside the parser used. In section 5 the 
two ML algorithms are presented, as well as the 
strategies for the precision-coverage trade-off. 
Section 6 shows the experimental setting and the 
results. Finally section 7 draws the conclusions 
and summarizes further work. 
2. Previous work. 
Yarowsky (1994) defined a basic set of features 
that has been widely used (with some variations) 
by other WSD systems. It consisted on words 
appearing in a window of ?k positions around 
the target and bigrams and trigrams constructed 
with the target word. He used words, lemmas, 
coarse part-of-speech tags and special classes of 
words, such as ?Weekday?. These features have 
been used by other approaches, with variations 
such as the size of the window, the distinction 
between open class/closed class words, or the 
pre-selection of significative words to look up in 
the context of the target word.  
Ng (1996) uses a basic set of features similar 
to those defined by Yarowsky, but they also use 
syntactic information: verb-object and subject-
verb relations. The results obtained by the 
syntactic features are poor, and no analysis of 
the features or any reason for the low 
performance is given. 
Stetina et al (1998) achieve good results with 
syntactic relations as features. They use a 
measure of semantic distance based on WordNet 
to find similar features. The features are 
extracted using a statistical parser (Collins, 
1996), and consist of the head and modifiers of 
each phrase. Unfortunately, they do not provide 
a comparison with a baseline system that would 
only use basic features.  
The Senseval-2 workshop was held in 
Toulouse in July 2001 (Preiss & Yarowsky, 
2001). Most of the supervised systems used only 
a basic set of local and topical features to train 
their ML systems. Regarding syntactic 
information, in the Japanese tasks, several 
groups relied on dependency trees to extract 
features that were used by different models 
(SVM, Bayes, or vector space models). For the 
English tasks, the team from the University of 
Sussex extracted selectional preferences based 
on subject-verb and verb-object relations. The 
John Hopkins team applied syntactic features 
obtained using simple heuristic patterns and 
regular expressions. Finally, WASP-bench used 
finite-state techniques to create a grammatical 
relation database, which was later used in the 
disambiguation process. The papers in the 
proceedings do not provide specific evaluation 
of the syntactic features, and it is difficult to 
derive whether they were really useful or not.  
3. Basic feature set 
We have taken a basic feature set widely used in 
the literature, divided in topical features and 
local features (Agirre & Martinez, 2001b). 
Topical features correspond to open-class 
lemmas that appear in windows of different sizes 
around the target word. In this experiment, we 
used two different window-sizes: 4 lemmas 
around the target (coded as win_lem_4w), and 
the lemmas in the sentence plus the 2 previous 
and 2 following sentences (win_lem_2s). 
Local features include bigrams and trigrams 
(coded as big_, trig_ respectively) that contain 
the target word. An index (+1, -1, 0) is used to 
indicate the position of the target in the bigram 
or trigram, which can be formed by part of 
speech, lemmas or word forms (wf, lem, 
pos). We used TnT (Brants, 2000) for PoS 
tagging.  
For instance, we could extract the following 
features for the target word known from the 
sample sentence below: word form ?whole? 
occurring in a 2 sentence window (win_wf_2s), 
the bigram  ?known widely? where target is the 
last word (big_wf_+1) and the trigram ?RB RB N? 
formed by the two PoS before the target word 
(trig_pos_+1). 
 
?There is nothing in the whole range of human 
experience more widely known and universally ?? 
4. Set of Syntactic Features. 
In order to extract syntactic features from the 
tagged examples, we needed a parser that would 
meet the following requirements: free for 
research, able to provide the whole structure 
with named syntactic relations (in contrast to 
shallow parsers), positively evaluated on well-
established corpora, domain independent, and 
fast enough. 
Three parsers fulfilled all the requirements: 
Link Grammar (Sleator and Temperley, 1993), 
Minipar (Lin, 1993) and (Carroll & Briscoe, 
2001). We installed the first two parsers, and 
performed a set of small experiments (John 
Carroll helped out running his own parser). 
Unfortunately, we did not have a comparative 
evaluation to help choosing the best. We 
performed a little comparative test, and all 
parsers looked similar. At this point we chose 
Minipar mainly because it was fast, easy to 
install and the output could be easily processed. 
The choice of the parser did not condition the 
design of the experiments (cf. section 7). 
From the output of the parser, we extracted 
different sets of features. First, we distinguish 
between direct relations (words linked directly 
in the parse tree) and indirect relations (words 
that are two or more dependencies apart in the 
syntax tree, e.g. heads of prepositional modifiers 
of a verb). For example, from ?Henry was listed 
on the petition as the mayor's attorney? a direct 
verb-object relation is extracted between listed 
and Henry and the indirect relation ?head of a 
modifier prepositional phrase? between listed 
and petition. For each relation we store also its 
inverse. The relations are coded according to the 
Minipar codes (cf. Appendix): 
 
[Henry obj_word listed] 
[listed objI_word Henry] 
[petition mod_Prep_pcomp-n_N_word listed] 
[listed mod_Prep_pcomp-n_NI_word petition] 
 
For instance, in the last relation above, mod_Prep 
indicates that listed has some prepositional 
phrase attached, pcomp-n_N indicates that petition 
is the head of the prepositional phrase, I 
indicates that it is an inverse relation, and word 
that the relation is between words (as opposed to 
relations between lemmas).  
We distinguished two different kinds of 
syntactic relations: instantiated grammatical 
relations (IGR) and grammatical relations (GR). 
4.1. Instantiated Grammatical Relations 
IGRs are coded as [wordsense relation value] 
triples, where the value can be either the word 
form or the lemma. Some examples for the 
target noun ?church? are shown below. In the 
first example, a direct relation is extracted for 
the ?building? sense, and in the second example 
an indirect relation for the ?group of Christians? 
sense. 
 
Example 1: ?...Anglican churches have been 
demolished...? 
[Church#2 obj_lem  demolish] 
 
Example 2: ?...to whip men into a surrender to a 
particular churh...? 
[Church#1 mod_Prep_pcomp-n_N_lem surrender] 
4.2. Grammatical relations 
This kind of features refers to the grammatical 
relation themselves. In this case, we collect 
bigrams [wordsense relation] and also n-grams 
[wordsense relation1 relation2 relation3 ...]. The 
relations can refer to any argument, adjunct or 
modifier. N-grams are similar to verbal 
subcategorization frames. At present, they have 
been used only for verbs. Minipar provides 
simple subcategorization information in the PoS 
itself, e.g. V_N_N for a verb taking two 
arguments. We have defined 3 types of n-grams: 
? Ngram1: The subcategorization information 
included in the PoS data given by Minipar, 
e.g. V_N_N.  
? Ngram2: The subcategorization information 
in ngram1, filtered by the arguments that 
actually occur in the sentence. 
? Ngram3: Which includes all dependencies in 
the parse tree.  
The three types have been explored in order to 
account for the argument/adjunct distinction, 
which Minipar does not always assign correctly. 
In the first case, Minipar?s judgment is taken 
from the PoS. In the second case the PoS and the 
relations deemed as arguments are combined 
(adjuncts are hopefully filtered out, but some 
arguments might be also discarded). In the third, 
all relations (including adjuncts and arguments) 
are considered. 
In the example below, the ngram1 feature 
indicates that the verb has two arguments (i.e. it 
is transitive), which is an error of Minipar 
probably caused by a gap in the lexicon. The 
ngram2 feature indicates simply that it has a 
subject and no object, and the ngram3 feature 
denotes the presence of the adverbial modifier 
?still?. Ngram2 and ngram3 try to repair possible 
gaps in Minipar?s lexicon. 
 
Example: ?His mother was nudging him, but he 
was still falling? 
[Fall#1 ngram1 V_N_N] 
[Fall#1 ngram2 subj] 
[Fall#1 ngram3 amodstill+subj] 
5. ML algorithms. 
In order to measure the contribution of syntactic 
relations, we wanted to test them on several ML 
algorithms. At present we have chosen one 
algorithm which does not combine features 
(Decision Lists) and another which does 
combine features (AdaBoost).  
Despite their simplicity, Decision Lists (Dlist 
for short) as defined in Yarowsky (1994) have 
been shown to be very effective for WSD 
(Kilgarriff & Palmer, 2000). Features are 
weighted with a log-likelihood measure, and 
arranged in an ordered list according to their 
weight. In our case the probabilities have been 
estimated using the maximum likelihood 
estimate, smoothed adding a small constant (0.1) 
when probabilities are zero. Decisions taken 
with negative values were discarded (Agirre & 
Martinez, 2001b).  
AdaBoost (Boost for short) is a general 
method for obtaining a highly accurate 
classification rule by linearly combining many 
weak classifiers, each of which may be only 
moderately accurate (Freund, 1997). In these 
experiments, a generalized version of the Boost 
algorithm has been used, (Schapire, 1999), 
which works with very simple domain 
partitioning weak hypotheses (decision stumps) 
with confidence rated predictions. This 
particular boosting algorithm is able to work 
efficiently in very high dimensional feature 
spaces, and has been applied, with significant 
success, to a number of NLP disambiguation 
tasks, including word sense disambiguation 
(Escudero et al, 2000). Regarding 
parametrization, the smoothing parameter has 
been set to the default value (Schapire, 1999), 
and Boost has been run for a fixed number of 
rounds (200) for each word. No optimization of 
these parameters has been done at a word level. 
When testing, the sense with the highest 
prediction is assigned. 
5.1. Precision vs. coverage trade-off. 
A high-precision WSD system can be obtained 
at the cost of low coverage, preventing the 
system to return an answer in the lowest 
confidence cases. We have tried two methods on 
Dlists, and one method on Boost. 
The first method is based on a decision-
threshold (Dagan and Itai, 1994): the algorithm 
rejects decisions taken when the difference of 
the maximum likelihood among the competing 
senses is not big enough. For this purpose, a 
one-tailed confidence interval was created so we 
could state with confidence 1 - ? that the true 
value of the difference measure was bigger than 
a given threshold (named ?). As in (Dagan and 
Itai, 1994), we adjusted the measure to the 
amount of evidence. Different values of ? were 
tested, using a 60% confidence interval. The 
values of ? range from 0 to 4. For more details 
check (Agirre and Martinez, 2001b). 
The second method is based on feature 
selection (Agirre and Martinez, 2001a). Ten-
fold cross validation on the training data for 
each word was used to measure the precision of 
each feature in isolation. Thus, the ML 
algorithm would be used only on the features 
with precision exceeding a given threshold. This 
method has the advantage of being able to set 
the desired precision of the final system.  
In the case of Boost, there was no 
straightforward way to apply the first method. 
The application of the second method did not 
yield satisfactory results, so we turned to 
directly use the support value returned for each 
decision being made. We first applied a 
threshold directly on this support value, i.e. 
discarding decisions made with low support 
values. A second approximation, which is the 
one reported here, applies a threshold over the 
difference in the support for the winning sense 
and the second winning sense. Still, further work 
is needed in order to investigate how Boost 
could discard less-confident results. 
6. Experimental setting and results. 
We used the Senseval-2 data (73 nouns, verbs 
and adjectives), keeping the original training and 
testing sets. In order to measure the contribution 
of syntactic features the following experiments 
were devised (not all ML algorithms were used 
in all experiments, as specified): contribution of 
IGR-type and GR-type relations (Dlist), 
contribution of syntactic features over a 
combination of local and topical features (Dlist, 
Boost), and contribution of syntactic features in 
a high precision system (Dlist, Boost). 
Performance is measured as precision and 
coverage (following the definitions given in 
Senseval-2). We also consider F11 to compare 
the overall performance as it gives the harmonic 
average between precision and recall (where 
recall is in this case precision times the 
coverage). F1 can be used to select the best 
precision/coverage combination (cf. section 6.3). 
6.1. Results for different sets of syntactic 
features (Dlist). 
Table 1 shows the precision, coverage and F1 
figures for each of the grammatical feature sets 
as used by the decision list algorithm. 
Instantiated Grammatical Relations provide very 
good precision, but low coverage. The only 
exceptions are verbs, which get very similar 
precision for both kinds of syntactic relations. 
Grammatical Relations provide lower precision 
but higher coverage. A combination of both 
attains best F1, and is the feature set used in 
subsequent experiments.  
                                                     
1 F1=2*precision*recall/(precision+recall). In this 
case we use recall=precision*coverage. 
6.2. Results for different combinations of 
features (Dlist, Boost) 
Both ML algorithms were used on syntactic 
features, local features, a combination of 
local+topical features (also called basic), and a 
combination of all features (basic+syntax) in 
turn. Table 2 shows the F1 figures for each 
algorithm, feature set and PoS.  
All in all, Boost is able to outperform Dlist in 
all cases, except for local features. Syntactic 
features get worse results than local features. 
Regarding the contribution of syntactic 
features to the basic set, the last two columns in 
Table 2 show a "+" whenever the difference in 
the precision over the basic feature set is 
significant (McNemar's test). Dlist is able to 
scarcely profit from the additional syntactic 
features (only significant for verbs). Boost 
attains significant improvement, showing that 
basic and syntactic features are complementary.  
The difference 
algorithms could be 
Dlist is a conservati
that it only uses the 
by the first feature tha
(abstaining if none o
using a combination o
single-feature classife
negative evidence) 
positive predictions t
Dlist. Since the feat
covered and given th
accurate, Boost achie
it is a significant
approaching a 100% c
6.3. Precision vs. coverage: high precision 
systems (Dlist, Boost)  
Figure 1 shows the results for the three methods 
to exploit the precision/coverage trade-off in 
order to obtain a high-precision system. For each 
method two sets of features have been used: the 
basic set alne and the combination of both 
basic and syntactic features.  
The figure reveals an interesting behavior for 
different coverage ranges. In the high coverage 
range, Boost on basic+syntactic features attains 
the best performance. In the medium coverage 
area, the feature selection method for Dlist 
obtains the best results, also for basic+syntactic 
features. Finally, in the low coverage and high 
precision area the decision-threshold method for 
Dlist is able to reach precisions in the high 90?s, 
with no profit from syntactic features. 
The two methods to raise precision for Dlists 
are very effective. The decision-threshold 
e in performance 
 coverage. The 
s 86% precision 
ctic features, but 
.  
obtain extremely 
 of low coverage) 
most predictive 
ave had problems 
 algorithm for 
ions. 
r coverage over 
ures consistently 
ing that syntactic 
IGR GR All-syntax 
PoS Prec. Cov. F1 Prec. Cov. F1 Prec. Cov. F1 
A 81,6 21,8 29,2 70,1 65,4 55,4 70,7 68,9 57,7 
N 74,6 36,0 38,5 65,4 57,6 47,8 67,6 62,5 52,0 
V 68,6 32,2 33,4 67,3 41,2 39,2 66,3 52,7 45,4 
Ov. 72,9 31,9 35,2 67,1 52,1 46,0 67,7 59,5 50,4 
Table 1: precision and coverage for different sets of syntactic features (percentage). 
 
  Syntax Local Local+Topical (Basic) Basic + Syntax 
PoS MFS Dlist Boost Dlist Boost Dlist Boost Dlist Boost 
A 59,0 57,7 62,6 66,3 67,5 65,3 66,2 65,4     67,7 
N 57,1 52,0 60,0 63,6 65,3 63,2 67,9 63,3 69,3+ 
V 40,3 45,4 48,5 51,6 50,1 51,0 51,6   51,2+ 53,9+ 
Ov. 48,2 50,4 55,2 59,4 59,3 58,5 60,7 58,7 62,5+ 
Table 2: F1 results (perc.) for different feature sets. ?+? indicates statistical significance over Basic. between the two ML 
explained by the fact that 
ve algorithm in the sense 
positive information given 
t holds in the test example 
f them are applicable). By 
f the predictions of several 
rs (using both positive and 
Boost is able to assign 
o more test examples than 
ure space is more widely 
at the classifiers are quite 
ves better recall levels and 
ly better algorithm for 
method obtains constant increas
up to 93% precision with 7%
feature selection method attain
with 26% coverage using synta
there is no further improvement
In this case Dlist is able to 
good accuracy rates (at the cost
restricting to the use of the 
features. On the contrary, we h
in adjusting the AdaBoost
obtaining high precision predict
The figure also shows, fo
20%, that the syntactic feat
allow for better results, confirmoverage WSD system. features improve the results of the basic set. 
7. Conclusions and further work. 
This paper shows that syntactic features 
effectively contribute to WSD precision. We 
have extracted syntactic relations using the 
Minipar parser, but the results should be also 
applicable to other parsers with similar 
performance. Two kinds of syntactic features are 
defined: Instantiated Grammatical Relations  
(IGR) between words, and Grammatical 
Relations (GR) coded as the presence of 
adjuncts / arguments in isolation or as 
subcategorization frames.  
The experimental results were tried on the 
Senseval-2 data, comparing two different ML 
algorithms (Dlist and Boost) trained both on a 
basic set of widely used features alone, and on a 
combination of basic and syntactic features. The 
main conclusions are the following: 
? IGR get better precision than GR, but the 
best precision/coverage combination 
(measured with F1) is attained by the 
combination of both. 
? Boost is able to profit from the addition of 
syntactic features, obtaining better results 
than Dlist. This proves that syntactic 
features contain information that is not 
present in other traditional features.  
? Overall the improvement is around two 
points for Boost, with highest increase for 
verbs.  
Several methods to exploit the precision-
coverage trade-off where also tried: 
? The results show that syntactic features 
consistently improve the results on all data 
points except in the very low coverage 
range, confirming the contribution of syntax. 
? The results also show that Dlist are suited to 
build a system with high precision: either a 
precision of 86% and a coverage of 26%, or 
95% precision and 8% coverage. 
Regarding future work, a thorough analysis of 
the quality of each of the syntactic relations 
extracted should be performed. In addition, a 
word-by-word analysis would be interesting, as 
some words might profit from specific syntactic 
features, while others might not. A preliminary 
analysis has been performed in (Agirre & 
Martinez, 2001b). 
Other parsers rather than Minipar could be 
used. In particular, we found out that Minipar 
always returns unambiguous trees, often making 
erroneous attachment decisions. A parser 
returning ambiguous output could be more 
desirable. The results of this paper do not 
depend on the parser used, only on the quality of 
the output, which should be at least as good as 
Minipar. 
Concerning the performance of the algorithm 
as compared to other Senseval 2 systems, it is 
not the best. Getting the best results was not the 
objective of this paper, but to show that syntactic 
features are worth including. We plan to 
improve the pre-processing of our systems, the 
detection of multiword lexical entries, etc. which 
could improve greatly the results. In addition 
there can be a number of factors that could 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: prec./cov. curve for three high precision methods on basic and basic+syntactic features. 
0,50
0,55
0,60
0,65
0,70
0,75
0,80
0,85
0,90
0,95
1,00
0 0,2 0,4 0,6 0,8 1coverage
pr
ec
isi
on
dlist threshold basic dlist feat.sel. basic boost basic
dlist threshold basic+synt dlist feat.sel. basic+synt boost basic+synt
diminish or disguise the improvement in the 
results: hand-tagging errors, word senses 
missing from training or testing data, biased 
sense distributions, errors in syntactic relations, 
etc. Factor out this ?noise? could show the real 
extent of the contribution of syntactic features. 
On the other hand, we are using a high 
number of features. It is well known that many 
ML algorithms have problems to scale to high 
dimensional feature spaces, especially when the 
number of training examples is relatively low (as 
it is the case for Senseval-2 word senses). 
Researching on more careful feature selection 
(which is dependent of the ML algorithm) could 
also improve the contribution of syntactic 
features, and WSD results in general. In 
addition, alternative methods to produce a high 
precision method based on Boost need to be 
explored. 
Finally, the results on high precision WSD 
open the avenue for acquiring further examples 
in a bootstrapping framework.  
Acknowledgements 
This research has been partially funded by McyT 
(Hermes project TIC-2000-0335-C03-03). David 
Martinez was funded by the Basque 
Government, grant AE-BFI:01.245). 
References 
Agirre, E. and D. Martinez. 2001a. Decision Lists for 
English and Basque. Proceedings of the 
SENSEVAL-2 Workshop. In conjunction with 
ACL'2001/EACL'2001. Toulouse, France. 
Agirre, E. and D. Martinez. 2001b. Analysis of 
supervised word sense disambiguation systems. Int. 
report LSI 11-2001, available from the authors. 
Brants, T. 2000. TnT - A Statistical Part-of-Speech 
Tagger. In Proc. of the Sixth Applied Natural 
Language Processing Conference, Seattle, WA. 
Carroll, J. and E. Briscoe (2001) `High precision 
extraction of grammatical relations'. In Proceedings 
of the 7th ACL/SIGPARSE International Workshop 
on Parsing Technologies, Beijing, China. 78-89.  
Collins M. 1996. A new statistical parser based on 
bigram lexical dependencies. In Proceedings of the 
34th Annual Meeting of the ACL, pages 184-191. 
Dagan I., and A. Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics 
20:4, pp. 563--596. 
Freund Y. and R. E. Schapire. 1997. A Decision-
Theoretic Generalization of On-line Learning and 
an Application to Boosting. Journal of Computer 
and System Sciences, 55(1):119--139. 
Escudero G., L. M?rquez, G. Rigau. 2000. Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 12th European Conference on 
Machine Learning, ECML 2000. Barcelona, Spain. 
Kilgarriff, A. and M. Palmer. (eds). 2000. Special 
issue on SENSEVAL. Computer and the 
Humanities, 34 (1-2). 
Lin, D. 1993. Principle Based parsing without 
Overgeneration. In 31st Annual Meeting of the 
Association for Computational Linguistics. 
Columbus, Ohio. pp 112-120.  
Ng, H. T. and H. B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics. 
Preiss, J. and D. Yarowsky. 2001. Proc. of the 
Second Intl. Workshop on Evaluating Word Sense 
Disambiguation Systems (Senseval 2). In conj. with 
ACL'2001/EACL'2001. Toulouse, France. 
Schapire, R. E. and Y. Singer. 1999. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning, 37(3):297--336. 
Sleator, D. and D. Temperley. 1993. Parsing English 
with a Link Grammar. Third International 
Workshop on Parsing Technologies. 
Stetina J., S. Kurohashi, M. Nagao. 1998. General 
Word Sense Disambiguation Method Based on a 
Full Sentential Context. In Usage of WordNet in 
Natural Language Processing , Proceedings of 
COLING-ACL Workshop. Montreal (Canada).  
Yarowsky, D. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent 
Restoration in Spanish and French. Proceedings of 
the 32nd Annual Meeting of the Association for 
Computational Linguistics, pp. 88--95.  
Appendix: main Minipar relations. 
Relation Direct Indirect Description 
by-subj X  Subj. with passives 
C  X clausal complement 
Cn  X nominalized clause 
comp1 X  complement (PP, inf/fin clause) of noun 
Desc X  description  
Fc X  finite complement 
I  X see c and fc, dep. between clause and main verb 
Mod X  Modifier 
Obj X  Object 
pcomp-c X  clause of pp 
Pcomp-n X  nominal head of pp 
Pnmod X  postnominal modifier. 
Pred X  predicative (can be A or N) 
Sc X  sentential complement 
Subj X  subject 
Vrel X  passive verb modifier of nouns 
For each relation the acronym, whether it is used as a 
direct relation or to construct indirect relations, and a 
short description are provided. 
43
44
45
46
47
48
49
50
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 644?651, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Robust Combination Strategy for Semantic Role Labeling
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi Turmo
Technical University of Catalunya
Barcelona, Spain
{lluism,surdeanu,pcomas,turmo}@lsi.upc.edu
Abstract
This paper focuses on semantic role la-
beling using automatically-generated syn-
tactic information. A simple and robust
strategy for system combination is pre-
sented, which allows to partially recover
from input parsing errors and to signif-
icantly boost results of individual sys-
tems. This combination scheme is also
very flexible since the individual systems
are not required to provide any informa-
tion other than their solution. Extensive
experimental evaluation in the CoNLL-
2005 shared task framework supports our
previous claims. The proposed architec-
ture outperforms the best results reported
in that evaluation exercise.
1 Introduction
The task of Semantic Role Labeling (SRL), i.e.
the process of detecting basic event structures
such as who did what to whom, when and where,
has received considerable interest in the past few
years (Gildea and Jurafsky, 2002; Surdeanu et al,
2003; Xue and Palmer, 2004; Pradhan et al, 2005a;
Carreras and Ma`rquez, 2005). It was shown that
the identification of such event frames has a signif-
icant contribution for many Natural Language Pro-
cessing (NLP) applications such as Information Ex-
traction (Surdeanu et al, 2003) and Question An-
swering (Narayanan and Harabagiu, 2004).
Most current SRL approaches can be classified
in one of two classes: approaches that take ad-
vantage of complete syntactic analysis of text, pi-
oneered by Gildea and Jurafsky (2002), and ap-
proaches that use partial syntactic analysis, cham-
pioned by previous evaluations performed within
the Conference on Computational Natural Language
Learning (CoNLL) (Carreras and Ma`rquez, 2004).
The wisdom extracted from this volume of work in-
dicates that full syntactic analysis has a significant
contribution to the SRL performance, when using
hand-corrected syntactic information.
On the other hand, when only automatically-
generated syntax is available, the quality of the in-
formation provided through full syntax decreases
because the state-of-the-art of full parsing is less
robust and performs worse than the tools used for
partial syntactic analysis. Under such real-world
conditions, the difference between the two SRL ap-
proaches (with full or partial syntax) is not that high.
More interestingly, the two SRL strategies perform
better for different semantic roles. For example,
models that use full syntax recognize better agent
and theme roles, whereas models based on partial
syntax are better at recognizing explicit patient roles,
which tend to be farther from the predicate and accu-
mulate more parsing errors (Ma`rquez et al, 2005).
The above observations motivate the work pre-
sented in this paper. We introduce a novel semantic
role labeling approach that combines several indi-
vidual SRL systems. Intuitively, our approach can
be separated in two stages: a candidate generation
phase, where the solutions provided by several indi-
vidual models are combined into a pool of candidate
arguments, and an inference phase, where the candi-
dates are filtered using a binary classifier, and possi-
644
The luxury auto maker last year sold 1,214 cars in the U.S.
PPNP
VPNPNP
S
ARG0 ARGM?TMP P ARG1 ARGM?LOC
Figure 1: Sample PropBank sentence.
ble conflicts with domain knowledge constraints are
resolved to obtain the final solution.
For robustness, the inference model uses only
global attributes extracted from the solutions pro-
vided by the individual systems, e.g., the sequence
of role labels generated by each system for the cur-
rent predicate. We do not use any attributes spe-
cific to the individual models, not even the confi-
dence assigned by the individual classifiers. Besides
simplicity, the consequence of this decision is that
our approach does not impose any restrictions on the
individual SRL strategies, as long as one solution
is provided for each predicate. On the other hand,
probabilistic inference processes, which have been
successfully used for SRL (Koomen et al, 2005),
mandate that each individual candidate argument be
associated with its raw activation, or confidence, in
the given model. However, this information is not
directly available in two out of three of our individ-
ual models, which classify argument chunks and not
entire arguments.
Despite its simplicity, our approach obtains en-
couraging results: the combined system outperforms
any of the individual systems and, using exactly the
same data, it is also competitive with the best SRL
systems that participated in the latest CoNLL shared
task evaluation (Carreras and Ma`rquez, 2005).
2 Semantic Corpora
In this paper we report results using PropBank, an
approximately one-million-word corpus annotated
with predicate-argument structures (Kingsbury et
al., 2002). To date, PropBank addresses mainly
predicates lexicalized by verbs and a small num-
ber of predicates lexicalized by verb nominalizations
and adjectives.
The arguments of each predicate are numbered se-
quentially from ARG0 to ARG5. Generally, ARG0
stands for agent, ARG1 for theme or direct ob-
ject, and ARG2 for indirect object, benefactive or
instrument, but mnemonics tend to be verb spe-
cific. Additionally, predicates might have ?adjunc-
tive arguments?, referred to as ARGMs. For example,
ARGM-LOC indicates a locative and ARGM-TMP in-
dicates a temporal. Figure 1 shows a sample sen-
tence where one predicate (?sold?) has 4 arguments.
In a departure from ?traditional? SRL approaches
that train on the hand-corrected syntactic trees as-
sociated with PropBank, we do not use any syn-
tactic information from PropBank. Instead, we
develop our models using automatically-generated
syntax and named-entity (NE) labels, made avail-
able by the CoNLL shared task evaluation (Carreras
and Ma`rquez, 2005). From the CoNLL data, our
individual models based on full syntactic analysis
use the trees generated by the Charniak parser. The
partial-syntax model uses the chunk? i.e. basic syn-
tactic phrase ? labels and clause boundaries. All in-
dividual models make use of the provided NE labels.
Following the CoNLL-2005 setting we evaluated
our system also on a fresh test set, derived from the
Brown corpus. This second evaluation allows us to
re-enforce our robustness claim.
3 Approach Overview
The proposed architecture, summarized in Figure 2,
consists of two stages: a candidate generation phase
and an inference stage.
In the candidate generation step, we merge the so-
lutions of three individual SRL models into a unique
pool of candidate arguments. The proposed models
range from complete reliance on full parsing to us-
ing only partial syntactic information. The first two
models, Model 1 and 2, are developed as sequential
taggers (using the BIO tagging scheme) on a shared
framework. The major difference between the two
models is that Model 1 uses only partial syntactic
information (basic phrases and clause boundaries),
whereas Model 2 uses complete syntactic informa-
tion. To maximize diversity, Model 3 implements
a different strategy: it models only arguments that
map into exactly one syntactic constituent. Section 4
details all three individual models.
The inference stage starts with candidate filtering,
645
Candidate Filtering
Reliance on full syntax
Model 1 Model 2 Model 3
Conflict Resolution
Inference
Candidate
Generation
Figure 2: Architecture of the proposed system.
which reduces the number of candidate arguments
in the pool using a single binary classifier. Using
this classifier?s confidence values and a number of
domain-specific constraints, e.g. no two arguments
can overlap, the conflict resolution component en-
forces the consistency of the final solution using a
straightforward greedy strategy. The complete in-
ference model is detailed in Section 5.
4 Individual SRL Models
Models 1 and 2. These models approach SRL as
a sequential tagging task. In a pre-process step, the
input syntactic structures are traversed in order to
select a subset of constituents organized sequentially
(i.e. non embedding). Model 1 makes use only of
the partial tree defined by base chunks and clause
boundaries, while Model 2 explores full parse trees.
Precisely, the sequential tokens are selected as fol-
lows. First, the input sentence is splitted into dis-
joint segments by considering the clause boundaries
given by the syntactic structure. Second, for each
segment, the set of top-most non-overlapping syn-
tactic constituents completely falling inside the seg-
ment are selected as tokens. Note that this strategy
provides a set of sequential tokens covering the com-
plete sentence. Also, it is independent of the syn-
tactic annotation explored, given it provides clause
boundaries ? see (Ma`rquez et al, 2005) for more
details.
Due to this pre-processing stage, the upper-bound
recall figures are 95.67% for Model 1 and 90.32%
for Model 2 using the datasets defined in Section 6.
The nodes selected are labeled with B-I-O tags
(depending if they are at the beginning, inside, or
outside of a predicate argument) and they are con-
verted into training examples by considering a rich
set of features, mainly borrowed from state-of-the-
art systems. These features codify properties from:
(a) the argument constituent, (b) the target predicate,
Constituent type and head: extracted using common head-
word rules. If the first element is a PP chunk, then the
head of the first NP is extracted.
First and last words and POS tags of the constituent.
POS sequence: if it is less than 5 tags long.
2/3/4-grams of the POS sequence.
Bag-of-words of nouns, adjectives, and adverbs.
TOP sequence: sequence of types of the top-most syntactic
elements in the constituent (if it is less than 5 elements long).
In the case of full parsing this corresponds to the right-hand
side of the rule expanding the constituent node.
2/3/4-grams of the TOP sequence.
Governing category as in (Gildea and Jurafsky, 2002).
NamedEnt, indicating if the constituent embeds or
strictly matches a named entity along with its type.
TMP, indicating if the constituent embeds or strictly matches
a temporal keyword (extracted from AM-TMP arguments of
the training set).
Previous and following words and POS of the constituent.
The same features characterizing focus constituents are
extracted for the two previous and following tokens, provided
they are inside the clause boundaries of the codified region.
Table 1: Constituent structure features: Models 1/2
Predicate form, lemma, and POS tag.
Chunk type and type of verb phrase in which verb is
included: single-word or multi-word.
The predicate voice. We currently distinguish five voice
types: active, passive, copulative, infinitive, and progressive.
Binary flag indicating if the verb is a start/end of a clause.
Sub-categorization rule, i.e. the phrase structure rule that
expands the predicate immediate parent.
Table 2: Predicate structure features: Models 1/2
and (c) the distance between the argument and pred-
icate. The three feature sets are listed in Tables 1, 2,
and 3, respectively.1
Regarding the learning algorithm, we used gener-
alized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
lems to address multi-class classification. AdaBoost
binary classifiers are then used for labeling test se-
quences, from left to right, using a recurrent sliding
window approach with information about the tag as-
signed to the preceding token. This tagging module
enforces some basic constraints, e.g., BIO correct
structure, arguments cannot overlap with clause nor
chunk boundaries, discard ARG0-5 arguments not
present in PropBank frames for a certain verb, etc.
1Features extracted from partial parsing and Named Entities
are common to Model 1 and 2, while features coming from full
parse trees only apply to Model 2.
646
Relative position, distance in words and chunks, and level of
embedding (in #clause-levels) with respect to the constituent.
Constituent path as described in (Gildea and Jurafsky, 2002)
and all 3/4/5-grams of path constituents beginning at the
verb predicate or ending at the constituent.
Partial parsing path as described in (Carreras et al, 2004)
and all 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
Syntactic frame as described by Xue and Palmer (2004)
Table 3: Predicate?constituent features: Models 1/2
The syntactic label of the candidate constituent.
The constituent head word, suffixes of length 2, 3, and 4,
lemma, and POS tag.
The constituent content word, suffixes of length 2, 3, and
4, lemma, POS tag, and NE label. Content words, which
add informative lexicalized information different from
the head word, were detected using the heuristics
of (Surdeanu et al, 2003).
The first and last constituent words and their POS tags.
NE labels included in the candidate phrase.
Binary features to indicate the presence of temporal cue
words, i.e. words that appear often in AM-TMP phrases
in training.
For each TreeBank syntactic label we added a feature to
indicate the number of such labels included in the
candidate phrase.
The sequence of syntactic labels of the constituent
immediate children.
The phrase label, head word and POS tag of the
constituent parent, left sibling, and right sibling.
Table 4: Constituent structure features: Model 3
Model 3. The third individual SRL model makes
the strong assumption that each predicate argument
maps to one syntactic constituent. For example, in
Figure 1 ARG0 maps to a noun phrase, ARGM-LOC
maps to a prepositional phrase etcetera. This as-
sumption holds well on hand-corrected parse trees
and simplifies significantly the SRL process because
only one syntactic constituent has to be correctly
classified in order to recognize one semantic argu-
ment. On the other hand, this approach is limited
when using automatically-generated syntactic trees.
For example, only slightly over 91% of the argu-
ments can be mapped to one of the syntactic con-
stituents produced by the Charniak parser.
Using a bottom-up approach, Model 3 maps each
argument to the first syntactic constituent that has
the exact same boundaries and then climbs as high as
possible in the tree across unary production chains.
We currently ignore all arguments that do not map
to a single syntactic constituent.
The predicate word and lemma.
The predicate voice. Same definition as Models 1 and 2.
A binary feature to indicate if the predicate is frequent
(i.e., it appears more than twice in the training data) or not.
Sub-categorization rule. Same def. as Models 1 and 2.
Table 5: Predicate structure features: Model 3
The path in the syntactic tree between the argument phrase
and the predicate as a chain of syntactic labels along with
the traversal direction (up or down).
The length of the above syntactic path.
The number of clauses (S* phrases) in the path.
The number of verb phrases (VP) in the path.
The subsumption count, i.e. the difference between the
depths in the syntactic tree of the argument and predicate
constituents. This value is 0 if the two phrases share the
same parent.
The governing category, which indicates if NP
arguments are dominated by a sentence (typical for
subjects) or a verb phrase (typical for objects).
We generalize syntactic paths with more than 3
elements using two templates:
(a) Arg ? Ancestor ? Ni ? Pred, where Arg is the
argument label, Pred is the predicate label, Ancestor
is the label of the common ancestor, and Ni is instantiated
with all the labels between Pred and Ancestor in
the full path; and
(b) Arg ? Ni ? Ancestor ? Pred, where Ni is
instantiated with all the labels between Arg and
Ancestor in the full path.
The surface distance between the predicate and the
argument phrases encoded as: the number of tokens, verb
terminals (VB*), commas, and coordinations (CC) between
the argument and predicate phrases, and a binary feature to
indicate if the two constituents are adjacent.
A binary feature to indicate if the argument starts with a
predicate particle, i.e. a token seen with the RP* POS
tag and directly attached to the predicate in training.
Table 6: Predicate?constituent features: Model 3
Once the mapping process completes, Model 3
extracts a rich set of lexical, syntactic, and seman-
tic features. Tables 4, 5, and 6 present these features
organized in the same three categories as the previ-
ous Models 1 and 2 ? see (Surdeanu and Turmo,
2005) for more details.
Similarly with Models 1 and 2, Model 3 trains
one-vs-all classifiers using AdaBoost for the most
common argument labels. To reduce the sample
space, Model 3 selects training examples (both posi-
tive and negative) only from: (a) the first clause that
includes the predicate, or (b) from phrases that ap-
pear to the left of the predicate in the sentence. More
than 98% of the argument constituents fall into one
of these classes.
At prediction time the classifiers are combined us-
ing a simple greedy technique that iteratively assigns
647
to each predicate the argument classified with the
highest confidence. For each predicate we consider
as candidates all AM attributes, but only numbered
attributes indicated in the corresponding PropBank
frame. Additionally, this greedy strategy enforces a
limited number of domain knowledge constraints in
the generated solution: (a) arguments can not over-
lap in any form, and (b) no duplicate arguments are
allowed for ARG0-5.
5 The Inference Model
The most important component of our inference
model is candidate filtering, which decides if a can-
didate argument should be maintained in the global
solution or not. Candidate filtering is implemented
as a single binary classifier that uses only features
extracted from the solutions provided by the individ-
ual systems. For robustness, we do not use any fea-
tures that are specific to any of the individual mod-
els, nor the confidence value of their classifiers.
Table 7 lists the features extracted from each can-
didate argument by the filtering classifier. For sim-
plicity we have focused only on attributes that: (a)
are readily available in the solutions proposed by the
individual classifiers, and (b) allow the gathering of
simple and robust statistics. For example, the fil-
tering classifier might learn that a candidate is to be
trusted if: (a) two individual systems proposed it, (b)
if its label is ARG2 and it was generated by Model 1,
or (c) if it was proposed by Model 2 within a certain
argument sequence.
The candidate arguments that pass the filtering
stage are incorporated in the global solution by the
conflict resolution module, which enforces several
domain specific constraints. We have currently im-
plemented two constraints: (a) arguments can not
overlap or embed other arguments; and (b) no du-
plicate arguments are allowed for the numbered ar-
guments ARG0-5. Theoretically, the set of con-
straints can be extended with any other rules, but in
our particular case, we know that some constraints,
e.g. providing only arguments indicated in the cor-
responding PropBank frame, are already guaranteed
by the individual models. Conflicts are solved with
a straightforward greedy strategy: the pool of candi-
date arguments is inspected in descending order of
the confidence values assigned by the filtering clas-
The label of the candidate argument.
The number of systems that generated an argument with
this label and span.
The unique ids, e.g. M1 and M2, of all the systems that
generated an argument with this label and span.
The argument sequence for this predicate for all the systems
that generated an argument with this label and span. For
example, the argument sequence for the proposition
illustrated in Figure 1 is: ARG0 - ARGM-TMP - P -
ARG1 - ARGM-LOC.
The number and unique ids of all the systems that generated
an argument with the same span but different label.
The number and unique ids of all the systems that generated
an argument included in the current argument.
The number and unique ids of all the systems that generated
an argument that contains the current argument.
The number and unique ids of all the systems that generated
an argument that overlaps the current argument.
Table 7: Features used by the candidate filtering
classifier.
sifier, and candidates are appended to the global so-
lution only if they do not violate any of the domain
constraints with the arguments already selected. Our
inference system currently has a sequential architec-
ture, i.e. no feedback is sent from the conflict reso-
lution module to candidate filtering.
6 Experimental Results
We trained the individual models using the complete
CoNLL-2005 training set (PropBank/TreeBank sec-
tions 2 to 21). All models were developed using
AdaBoost with decision trees of depth 4 (i.e. each
branch may represent a conjunction of at most 4 ba-
sic features). Each classification model was trained
for up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds: (a) we have limited the number of nega-
tive examples in Model 3 to the first 500,000; (b)
we have trained only the most frequent argument la-
bels: top 41 for Model 1, top 35 for Model 2, and
top 24 for Model 3; and (c) we discarded all features
occurring less than 15 times in the training set.
The models were tuned on a separate develop-
ment partition (TreeBank section 24) and evaluated
on two corpora: (a) TreeBank section 23, which
consists of Wall Street Journal (WSJ) documents,
and (b) on three sections of the Brown corpus, se-
mantically annotated by the PropBank team for the
CoNLL 2005 shared task evaluation. Table 8 sum-
648
WSJ PProps Precision Recall F?=1
Model 1 48.45% 78.76% 72.44% 75.47 ?0.8
Model 2 52.04% 79.65% 74.92% 77.21 ?0.8
Model 3 45.28% 80.32% 72.95% 76.46 ?0.6
Brown PProps Precision Recall F?=1
Model 1 30.85% 67.72% 58.29% 62.65 ?2.1
Model 2 36.44% 71.82% 64.03% 67.70 ?1.9
Model 3 29.48% 72.41% 59.67% 65.42 ?2.1
Table 8: Overall results of the individual models on
the WSJ and Brown test sets.
marizes the results of the three models on the WSJ
and Brown corpora. In that table we include the
percentage of perfect propositions detected by each
model (?PProps?), i.e. predicates recognized with
all their arguments, the overall precision, recall, and
F?=1 measure2.
The results summarized in Table 8 indicate that
all individual systems have a solid performance. Al-
though none of them would rank in the top 3 in this
year?s CoNLL evaluation (Carreras and Ma`rquez,
2005), their performance is comparable to the best
individual systems presented at that evaluation exer-
cise3. As expected, the models based on full parsing
(2 and 3) perform better than the model based on
partial syntax. But, interestingly, the difference is
not large (e.g., less than 2 points in F?=1 in the WSJ
corpus), evincing that having base syntactic chunks
and clause boundaries is enough to obtain a compet-
itive performance with a simple system.
Consistently with other systems evaluated on the
Brown corpus, all our models experience a severe
performance drop in this corpus, due to the lower
performance of the linguistic processors.
6.1 Performance of Combination Systems
We have trained the candidate filtering binary classi-
fier on one third of the training partition. Its training
data was generated using individual models trained
on the other two thirds of the training partition. The
classifier was developed using Support Vector Ma-
chines (SVM) with a polynomial kernel of degree 2.
We trained combined models for all 4 possible com-
binations of our 3 individual models.
2The significance intervals for the F1 measure have been ob-
tained using bootstrap resampling (Noreen, 1989). F1 rates out-
side of these intervals are assumed to be significantly different
from the related F1 rate (p < 0.05).
3The best performing SRL systems at CoNLL were a com-
bination of several subsystems. See section 7 for details.
Table 9 summarizes the performance of the com-
bined systems on the WSJ and Brown corpora.4
The combined systems are compared against a base-
line combination system, which merges all the argu-
ments generated by the individual systems. For con-
flict resolution, the baseline uses the greedy strategy
introduced in Section 5, but using as argument or-
dering criterion a radix sort that orders the candidate
arguments in descending order of: number of mod-
els that agreed on this argument; argument length in
tokens; and performance of the individual system5.
Table 9 indicates that our combination strategy is
always successful: the results of all combined sys-
tems improve upon their individual models and they
are better the baseline when using the same num-
ber of individual models. As expected, the highest
scoring combined system includes all three individ-
ual models. Its F?=1 measure is 2.35 points higher
than the best individual model (Model 2) in the WSJ
test set and 1.30 points higher in the Brown test
set. Somewhat surprisingly, the highest percentage
of perfect propositions is not obtained by the over-
all best combination, but by the system that com-
bines the two models based on full parsing (Models
2 and 3). This happens because Model 1 is the weak-
est performing of the bunch, hence its arguments,
while providing useful information to the filtering
classifier, decrease the number of perfect proposi-
tions when selected.
We consider these results encouraging given the
simplicity of our inference model and the limited
amount of training data used to train the candidate
filtering classifier. Additionally, they compare fa-
vorably with respect to the best performing systems
at CoNLL-2005 shared task (see Section 7).
6.2 Upper Limit of the Combination Strategy
To explore the potential of our approach we have
constructed a hypothetical system where our candi-
date filtering module is replaced with a perfect clas-
sifier that selects only correct arguments and dis-
cards all others. Table 10 lists the results obtained
on the WSJ and Brown corpora by this hypothetical
system using all three individual models.
4For conciseness, in Table 9 we introduced the notation
M1+2+3 to indicate the combination of Models 1, 2, and 3
5This combination produced the highest-scoring baseline
model.
649
WSJ PProps Prec. Recall F?=1
M1+2 51.30% 81.30% 74.13% 77.55 ?0.7
M1+3 47.26% 81.21% 73.36% 77.08 ?0.8
M2+3 52.65% 81.55% 75.30% 78.30 ?0.7
M1+2+3 51.64% 84.89% 74.87% 79.56 ?0.7
baseline 51.09% 77.29% 78.67% 77.98 ?0.7
Brown PProps Prec. Recall F?=1
M1+2 35.95% 73.70% 62.93% 67.89 ?2.0
M1+3 28.98% 72.83% 58.84% 65.09 ?2.2
M2+3 37.06% 73.89% 63.30% 68.18 ?2.2
M1+2+3 34.20% 78.66% 61.46% 69.00 ?2.1
baseline 33.58% 67.66% 66.01% 66.82 ?1.8
Table 9: Overall results of the combination models
on the WSJ and Brown test sets.
Perfect props Precision Recall F?=1
WSJ 70.76% 99.12% 85.22% 91.64
Brown 51.87% 99.63% 74.32% 85.14
Table 10: Performance upper limit on the test sets.
Table 10 indicates that the upper limit of proposed
approach is relatively high: the F?=1 of this hy-
pothetical system is over 12 points higher than our
best combined system in the WSJ test set, and over
16 points higher in the Brown corpus. These re-
sults indicate that the potential of our combination
strategy is high, especially when compared with re-
ranking strategies, which are limited to the perfor-
mance of the best complete solution in the candidate
pool. By allowing the re-combination of arguments
from the individual candidate solutions we raise this
threshold significantly. Table 11 lists the contribu-
tion of the individual models to this upper limit on
the WSJ corpus. For conciseness, we list only the
?core? numbered arguments. ?? of 3? indicates the
percentage of correct arguments where all 3 mod-
els agreed, ?? of 2? indicates the percentage of cor-
rect arguments where any 2 models agreed, and the
other columns indicate the percentage of correct ar-
guments detected by a single model. Table 11 indi-
cates that, as expected, two or more individual mod-
els agreed on a large percentage of the correct argu-
ments. Nevertheless, a significant number of correct
arguments, e.g. over 22% of ARG3, come from a
single individual system. This proves that, in order
to achieve maximum performance, one has to look
beyond simple voting strategies that favor arguments
with high agreement between individual systems.
? of 3 ? of 2 M1 M2 M3
ARG0 80.45% 12.10% 3.47% 2.14% 1.84%
ARG1 69.82% 17.83% 7.45% 2.77% 2.13%
ARG2 56.04% 22.32% 12.20% 4.95% 4.49%
ARG3 56.03% 21.55% 12.93% 5.17% 4.31%
ARG4 65.85% 20.73% 6.10% 2.44% 4.88%
Table 11: Contribution of the individual systems to
the upper limit, for ARG0?ARG4 in the WSJ test set.
WSJ Brown
PProps F?=1 PProps F?=1
koomen 53.79% 79.44 ?0.8 32.34% 67.75 ?1.8
haghighi 56.52% 78.45 ?0.8 37.06% 67.71 ?2.0
pradhan 50.14% 77.37 ?0.7 36.44% 67.07 ?2.0
Table 12: Results of the best combined systems at
CoNLL-2005.
7 Related Work
The best performing systems at the CoNLL-2005
shared task included a combination of different base
subsystems to increase robustness and to gain cover-
age and independence from parse errors. Therefore,
they are closely related to the work of this paper.
Table 12 summarizes their results under exactly the
same experimental setting.
Koomen et al (2005) used a 2 layer architecture
similar to ours. The pool of candidates is generated
by running a full syntax SRL system on alternative
input information (Collins parsing, and 5-best trees
from Charniak?s parser). The combination of can-
didates is performed in an elegant global inference
procedure as constraint satisfaction, which, formu-
lated as Integer Linear Programming, can be solved
efficiently. Interestingly, the generalized inference
layer allows to include in the objective function,
jointly with the candidate argument scores, a num-
ber of linguistically-motivated constraints to obtain
a coherent solution. Differing from the strategy pre-
sented in this paper, their inference layer does not
include learning. Also, they require confidence val-
ues from individual classifiers. This is the best per-
forming system at CoNLL-2005.
Haghighi et al (2005) implemented a double re-
ranking model on top of the base SRL models to se-
lect the most probable solution among a set of can-
didates. The re-ranking is performed, first, on a set
of n-best solutions obtained by the base system run
on a single parse tree, and, then, on the set of best-
candidates coming from the n-best parse trees. The
650
re-ranking approach allows to define global complex
features applying to complete candidate solutions to
train the rankers. The main drawback, compared to
our approach, is that re-ranking does not permit to
combine different solutions since it is forced to se-
lect a complete candidate solution. This fact implies
that the performance upper limit strongly depends
on the ability of the base model to generate the com-
plete correct solution in the set of n-best candidates.
Finally, Pradhan et al (2005b) followed a stack-
ing approach by learning two individual systems
based on full syntax, whose outputs are used to
generate features to feed the training stage of a fi-
nal chunk-by-chunk SRL system. Although the fine
granularity of the chunking-based system allows to
recover from parsing errors, we find this combina-
tion scheme quite ad-hoc because it forces to break
argument candidates into chunks in the last stage.
8 Conclusions
This paper introduces a novel, robust combination
strategy for semantic role labeling. Our approach
is separated in two stages: a candidate generation
phase, which combines the solutions generated by
several individual models into a pool of candidate ar-
guments, followed by a simple inference model that
filters the candidate arguments using a single binary
classifier and then enforces an arbitrary number of
domain-specific constraints.
The proposed approach has several advantages.
First, because it combines the solutions provided by
the individual models, the inference model can re-
cover from errors produced in the generation phase.
Second, due to the diversity of the individual models
employed, the candidate pool contains a high per-
centage of the correct arguments. And lastly, our
approach is flexible and robust: it can incorporate
any SRL model in the candidate generation stage
because it does not require that the individual SRL
models provide any information, e.g. classification
confidence values, other than an argument solution.
Our results are better than the state of the art us-
ing automatically-generated syntactic information.
These results are encouraging considering the sim-
plicity of the proposed approach.
Acknowledgments
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Mihai Surdeanu is a research fellow within the
Ramo?n y Cajal program of the Spanish Ministry of
Education and Science.
References
X. Carreras and L. Ma`rquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proceedings of
CoNLL 2004.
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 Shared Task: Semantic Role Labeling. In Proceedings
of CoNLL-2005.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL 2004 shared task.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3).
A. Haghighi, K. Toutanova, and C. Manning. 2005. A joint
model for semantic role labeling. In Proceedings of CoNLL-
2005 shared task.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding se-
mantic annotation to the Penn Treebank. In Proceedings of
the Human Language Technology Conference.
P. Koomen, V. Punyakanok, D. Roth, and W. Yih. 2005. Gen-
eralized inference with multiple semantic role labeling sys-
tems. In Proceedings of CoNLL-2005 shared task.
L. Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`. 2005. Se-
mantic role labeling as sequential tagging. In Proceedings of
CoNLL-2005 shared task.
S. Narayanan and S. Harabagiu. 2004. Question answering
based on semantic structures. In International Conference
on Computational Linguistics (COLING 2004).
E. W. Noreen. 1989. Computer-Intensive Methods for Testing
Hypotheses. John Wiley & Sons.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and
D. Jurafsky. 2005a. Support vector learning for semantic
argument classification. Machine Learning, to appear.
S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Juraf-
sky. 2005b. Semantic role chunking combining complemen-
tary syntactic views. In Proceedings of CoNLL-2005.
R. E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
M. Surdeanu and J. Turmo. 2005. Semantic role labeling using
complete syntactic analysis. In Proceedings of CoNLL-2005
shared task.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003.
Using predicate-argument structures for information extrac-
tion. In Proceedings of ACL 2003.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP-2004.
651
Heterogeneous Automatic MT Evaluation
Through Non-Parametric Metric Combinations
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
Combining different metrics into a single
measure of quality seems the most direct
and natural way to improve over the quality
of individual metrics. Recently, several ap-
proaches have been suggested (Kulesza and
Shieber, 2004; Liu and Gildea, 2007; Al-
brecht and Hwa, 2007a). Although based
on different assumptions, these approaches
share the common characteristic of being
parametric. Their models involve a num-
ber of parameters whose weight must be
adjusted. As an alternative, in this work,
we study the behaviour of non-parametric
schemes, in which metrics are combined
without having to adjust their relative im-
portance. Besides, rather than limiting to
the lexical dimension, we work on a wide
set of metrics operating at different linguis-
tic levels (e.g., lexical, syntactic and se-
mantic). Experimental results show that
non-parametric methods are a valid means
of putting different quality dimensions to-
gether, thus tracing a possible path towards
heterogeneous automatic MT evaluation.
1 Introduction
Automatic evaluation metrics have notably acceler-
ated the development cycle of MT systems in the
last decade. There exist a large number of metrics
based on different similarity criteria. By far, the
most widely used metric in recent literature is BLEU
(Papineni et al, 2001). Other well-known metrics
are WER (Nie?en et al, 2000), NIST (Doddington,
2002), GTM (Melamed et al, 2003), ROUGE (Lin
and Och, 2004a), METEOR (Banerjee and Lavie,
2005), and TER (Snover et al, 2006), just to name
a few. All these metrics take into account informa-
tion at the lexical level1, and, therefore, their re-
liability depends very strongly on the heterogene-
ity/representativity of the set of reference transla-
tions available (Culy and Riehemann, 2003). In
order to overcome this limitation several authors
have suggested taking advantage of paraphrasing
support (Zhou et al, 2006; Kauchak and Barzilay,
2006; Owczarzak et al, 2006). Other authors have
tried to exploit information at deeper linguistic lev-
els. For instance, we may find metrics based on full
constituent parsing (Liu and Gildea, 2005), and on
dependency parsing (Liu and Gildea, 2005; Amigo?
et al, 2006; Mehay and Brew, 2007; Owczarzak et
al., 2007). We may find also metrics at the level
of shallow-semantics, e.g., over semantic roles and
named entities (Gime?nez and Ma`rquez, 2007), and
at the properly semantic level, e.g., over discourse
representations (Gime?nez, 2007).
However, none of current metrics provides, in iso-
lation, a global measure of quality. Indeed, all met-
rics focus on partial aspects of quality. The main
problem of relying on partial metrics is that we may
obtain biased evaluations, which may lead us to de-
rive inaccurate conclusions. For instance, Callison-
Burch et al (2006) and Koehn and Monz (2006)
have recently reported several problematic cases re-
lated to the automatic evaluation of systems ori-
ented towards maximizing different quality aspects.
Corroborating the findings by Culy and Riehemann
(2003), they showed that BLEU overrates SMT sys-
tems with respect to other types of systems, such
1ROUGE and METEOR may consider morphological vari-
ations. METEOR may also look up for synonyms in WordNet.
319
as rule-based, or human-aided. The reason is that
SMT systems are likelier to match the sublanguage
(e.g., lexical choice and order) represented by the
set of reference translations. We argue that, in order
to perform more robust, i.e., less biased, automatic
MT evaluations, different quality dimensions should
be jointly taken into account.
A natural solution to this challenge consists in
combining the scores conferred by different metrics,
ideally covering a heterogeneous set of quality as-
pects. In the last few years, several approaches to
metric combination have been suggested (Kulesza
and Shieber, 2004; Liu and Gildea, 2007; Albrecht
and Hwa, 2007a). In spite of working on a lim-
ited set of quality aspects, mostly lexical features,
these approaches have provided effective means of
combining different metrics into a single measure of
quality. All these methods implement a parametric
combination scheme. Their models involve a num-
ber of parameters whose weight must be adjusted
(see further details in Section 2).
As an alternative path towards heterogeneous MT
evaluation, in this work, we explore the possibility
of relying on non-parametric combination schemes,
in which metrics are combined without having to ad-
just their relative importance (see Section 3). We
have studied their ability to integrate a wide set of
metrics operating at different linguistic levels (e.g.,
lexical, syntactic and semantic) over several evalu-
ation scenarios (see Section 4). We show that non-
parametric schemes offer a valid means of putting
different quality dimensions together, effectively
yielding a significantly improved evaluation quality,
both in terms of human likeness and human accept-
ability. We have also verified that these methods port
well across test beds.
2 Related Work
Approaches to metric combination require two im-
portant ingredients:
Combination Scheme, i.e., how to combine sev-
eral metric scores into a single score. As
pointed out in Section 1, we distinguish be-
tween parametric and non-parametric schemes.
Meta-Evaluation Criterion, i.e., how to evaluate
the quality of a metric combination. The two
most prominent meta-evaluation criteria are:
? Human Acceptability: Metrics are evalu-
ated in terms of their ability to capture the
degree of acceptability to humans of auto-
matic translations, i.e., their ability to em-
ulate human assessors. The underlying as-
sumption is that ?good? translations should
be acceptable to human evaluators. Hu-
man acceptability is usually measured on
the basis of correlation between automatic
metric scores and human assessments of
translation quality2.
? Human Likeness: Metrics are evaluated in
terms of their ability to capture the fea-
tures which distinguish human from au-
tomatic translations. The underlying as-
sumption is that ?good? translations should
resemble human translations. Human
likeness is usually measured on the basis
of discriminative power (Lin and Och,
2004b; Amigo? et al, 2005).
In the following, we describe the most relevant
approaches to metric combination suggested in re-
cent literature. All are parametric, and most of them
are based on machine learning techniques. We dis-
tinguish between approaches relying on human like-
ness and approaches relying on human acceptability.
2.1 Approaches based on Human Likeness
The first approach to metric combination based
on human likeness was that by Corston-Oliver et
al. (2001) who used decision trees to distinguish
between human-generated (?good?) and machine-
generated (?bad?) translations. They focused on
evaluating only the well-formedness of automatic
translations (i.e., subaspects of fluency), obtaining
high levels of classification accuracy.
Kulesza and Shieber (2004) extended the ap-
proach by Corston-Oliver et al (2001) to take into
account other aspects of quality further than fluency
alone. Instead of decision trees, they trained Support
Vector Machine (SVM) classifiers. They used fea-
tures inspired by well-known metrics such as BLEU,
NIST, WER, and PER. Metric quality was evaluated
both in terms of classification accuracy and correla-
tion with human assessments at the sentence level.
2Usually adequacy, fluency, or a combination of the two.
320
A significant improvement with respect to standard
individual metrics was reported.
Gamon et al (2005) presented a similar approach
which, in addition, had the interesting property that
the set of human and automatic translations could
be independent, i.e., human translations were not re-
quired to correspond, as references, to the set of au-
tomatic translations.
2.2 Approaches based on Human Acceptability
Quirk (2004) applied supervised machine learning
algorithms (e.g., perceptrons, SVMs, decision trees,
and linear regression) to approximate human quality
judgements instead of distinguishing between hu-
man and automatic translations. Similarly to the
work by Gamon et al (2005) their approach does
not require human references.
More recently, Albrecht and Hwa (2007a; 2007b)
re-examined the SVM classification approach by
Kulesza and Shieber (2004) and, inspired by the
work of Quirk (2004), suggested a regression-based
learning approach to metric combination, with and
without human references. The regression model
learns a continuous function that approximates hu-
man assessments in training examples.
As an alternative to methods based on machine
learning techniques, Liu and Gildea (2007) sug-
gested a simpler approach based on linear combina-
tions of metrics. They followed a Maximum Corre-
lation Training, i.e., the weight for the contribution
of each metric to the overall score was adjusted so
as to maximize the level of correlation with human
assessments at the sentence level.
As expected, all approaches based on human ac-
ceptability have been shown to outperform that of
Kulesza and Shieber (2004) in terms of human ac-
ceptability. However, no results in terms of human
likeness have been provided, thus leaving these com-
parative studies incomplete.
3 Non-Parametric Combination Schemes
In this section, we provide a brief description of the
QARLA framework (Amigo? et al, 2005), which is,
to our knowledge, the only existing non-parametric
approach to metric combination. QARLA is non-
parametric because, rather than assigning a weight
to the contribution of each metric, the evaluation of
a given automatic output a is addressed through a
set of independent probabilistic tests (one per met-
ric) in which the goal is to falsify the hypothesis that
a is a human reference. The input for QARLA is a
set of test cases A (i.e., automatic translations), a set
of similarity metrics X, and a set of models R (i.e.,
human references) for each test case. With such a
testbed, QARLA provides the two essential ingredi-
ents required for metric combination:
Combination Scheme Metrics are combined inside
the QUEEN measure. QUEEN operates under
the unanimity principle, i.e., the assumption
that a ?good? translation must be similar to
all human references according to all metrics.
QUEENX(a) is defined as the probability, over
R ? R ? R, that, for every metric in X, the
automatic translation a is more similar to a hu-
man reference r than two other references, r?
and r??, to each other. Formally:
QUEENX,R(a) = Prob(?x ? X : x(a, r) ? x(r?, r??))
where x(a, r) stands for the similarity between
a and r according to the metric x. Thus,
QUEEN allows us to combine different similar-
ity metrics into a single measure, without hav-
ing to adjust their relative importance. Besides,
QUEEN offers two other important advantages
which make it really suitable for metric com-
bination: (i) it is robust against metric redun-
dancy, i.e., metrics covering similar aspects of
quality, and (ii) it is not affected by the scale
properties of metrics. The main drawback of
the QUEEN measure is that it requires at least
three human references, when in most cases
only a single reference translation is available.
Meta-evaluation Criterion Metric quality is eval-
uated using the KING measure of human like-
ness. All human references are assumed to be
equally optimal and, while they are likely to
be different, the best similarity metric is the
one that identifies and uses the features that
are common to all human references, group-
ing them and separating them from automatic
translations. Based on QUEEN, KING repre-
sents the probability that a human reference
321
does not receive a lower score than the score at-
tained by any automatic translation. Formally:
KINGA,R(X) = Prob(?a ? A : QUEENX,R?{r}(r) ?
QUEENX,R?{r}(a))
KING operates, therefore, on the basis of dis-
criminative power. The closest measure to
KING is ORANGE (Lin and Och, 2004b), which
is, however, not intended for the purpose of
metric combination.
Apart from being non-parametric, QARLA ex-
hibits another important feature which differentiates
it form other approaches; besides considering the
similarity between automatic translations and hu-
man references, QARLA also takes into account the
distribution of similarities among human references.
However, QARLA is not well suited to port from
human likeness to human acceptability. The reason
is that QUEEN is, by definition, a very restrictive
measure ?a ?good? translation must be similar to
all human references according to all metrics. Thus,
as the number of metrics increases, it becomes eas-
ier to find a metric which does not satisfy the QUEEN
assumption. This causes QUEEN values to get close
to zero, which turns correlation with human assess-
ments into an impractical meta-evaluation measure.
We have simulated a non-parametric scheme
based on human acceptability by working on uni-
formly averaged linear combinations (ULC) of met-
rics. Our approach is similar to that of Liu and
Gildea (2007) except that in our case all the metrics
in the combination are equally important3. In other
words, ULC is indeed a particular case of a paramet-
ric scheme, in which the contribution of each metric
is not adjusted. Formally:
ULCX(a,R) =
1
|X|
?
x?X
x(a,R)
where X is the metric set, and x(a,R) is the simi-
larity between the automatic translation a and the set
of references R, for the given test case, according to
the metric x. Since correlation with human assess-
ments at the system level is vaguely informative (it
is often estimated on very few system samples), we
3That would be assuming that all metrics operate in the same
range of values, which is not always the case.
AE04 CE04 AE05 CE05
#human references 5 5 5 4
#system outputs 5 10 7 10
#outputsassessed 5 10 6 5
#sentences 1,353 1,788 1,056 1,082
#sentencesassessed 347 447 266 272
Table 1: Description of the test beds
evaluate metric quality in terms of correlation with
human assessments at the sentence level (Rsnt). We
use the sum of adequacy and fluency to simulate a
global assessment of quality.
4 Experimental Work
In this section, we study the behavior of the two
combination schemes presented in Section 3 in the
context of four different evaluation scenarios.
4.1 Experimental Settings
We use the test beds from the 2004 and 2005
NIST MT Evaluation Campaigns (Le and Przy-
bocki, 2005)4. Both campaigns include two differ-
ent translations exercises: Arabic-to-English (?AE?)
and Chinese-to-English (?CE?). Human assessments
of adequacy and fluency are available for a subset
of sentences, each evaluated by two different human
judges. See, in Table 1, a brief numerical descrip-
tion including the number of human references and
system outputs available, as well as the number of
sentences per output, and the number of system out-
puts and sentences per system assessed.
For metric computation, we have used the IQMT
v2.1, which includes metrics at different linguistic
levels (lexical, shallow-syntactic, syntactic, shallow-
semantic, and semantic). A detailed description may
be found in (Gime?nez, 2007)5.
4.2 Evaluating Individual Metrics
Prior to studying the effects of metric combination,
we study the isolated behaviour of individual met-
rics. We have selected a set of metric representa-
tives from each linguistic level. Table 2 shows meta-
evaluation results for the test beds described in Sec-
tion 4.1, according both to human likeness (KING)
4http://www.nist.gov/speech/tests/
summaries/2005/mt05.htm
5The IQMT Framework may be freely downloaded from
http://www.lsi.upc.edu/?nlp/IQMT.
322
KING Rsnt
Level Metric AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05
1-WER 0.70 0.51 0.48 0.61 0.53 0.47 0.38 0.47
1-PER 0.64 0.43 0.45 0.58 0.50 0.51 0.29 0.40
1-TER 0.73 0.54 0.53 0.66 0.54 0.50 0.38 0.49
BLEU 0.70 0.49 0.52 0.59 0.50 0.46 0.36 0.39
NIST 0.74 0.53 0.55 0.68 0.53 0.55 0.37 0.46
Lexical GTM.e1 0.67 0.49 0.48 0.61 0.41 0.50 0.26 0.29
GTM.e2 0.69 0.52 0.51 0.64 0.49 0.54 0.43 0.48
ROUGEL 0.73 0.59 0.49 0.65 0.58 0.60 0.41 0.52
ROUGEW 0.75 0.62 0.54 0.68 0.59 0.57 0.48 0.54
METEORwnsyn 0.75 0.56 0.57 0.69 0.56 0.56 0.35 0.41
SP-Op-* 0.66 0.48 0.49 0.59 0.51 0.57 0.38 0.41
SP-Oc-* 0.65 0.44 0.46 0.59 0.55 0.58 0.42 0.41
Shallow SP-NISTl 0.73 0.51 0.55 0.66 0.53 0.54 0.38 0.44
Syntactic SP-NISTp 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39
SP-NISTiob 0.69 0.48 0.49 0.59 0.32 0.36 0.27 0.26
SP-NISTc 0.60 0.42 0.39 0.52 0.26 0.27 0.16 0.16
DP-HWCw 0.58 0.40 0.42 0.53 0.41 0.08 0.35 0.40
DP-HWCc 0.50 0.32 0.33 0.41 0.41 0.17 0.38 0.32
DP-HWCr 0.56 0.40 0.37 0.46 0.42 0.16 0.39 0.43
DP-Ol-* 0.58 0.48 0.41 0.52 0.52 0.48 0.36 0.37
Syntactic DP-Oc-* 0.65 0.45 0.44 0.55 0.49 0.51 0.43 0.41
DP-Or-* 0.71 0.57 0.54 0.64 0.55 0.55 0.50 0.50
CP-Op-* 0.67 0.47 0.47 0.60 0.53 0.57 0.38 0.46
CP-Oc-* 0.66 0.51 0.49 0.62 0.57 0.59 0.45 0.50
CP-STM 0.64 0.42 0.43 0.58 0.39 0.13 0.34 0.30
NE-Oe-** 0.65 0.45 0.46 0.57 0.47 0.56 0.32 0.39
Shallow SR-Or-* 0.48 0.22 0.34 0.41 0.28 0.10 0.32 0.21
Semantic SR-Orv 0.36 0.13 0.24 0.27 0.27 0.12 0.25 0.24
DR-Or-* 0.62 0.47 0.50 0.55 0.47 0.46 0.43 0.37
Semantic DR-Orp-* 0.58 0.42 0.43 0.50 0.37 0.35 0.36 0.26
Optimal Combination 0.79 0.64 0.61 0.70 0.64 0.63 0.54 0.61
Table 2: Metric Meta-evaluation
and human acceptability (Rsnt), computed over the
subsets of sentences for which human assessments
are available.
The first observation is that the two meta-
evaluation criteria provide very similar metric qual-
ity rankings for a same test bed. This seems to in-
dicate that there is a relationship between the two
meta-evaluation criteria employed. We have con-
firmed this intuition by computing the Pearson cor-
relation coefficient between values in columns 1 to
4 and their counterparts in columns 5 to 8. There
exists a high correlation (R = 0.79).
A second observation is that metric quality varies
significantly from task to task. This is due to the sig-
nificant differences among the test beds employed.
These are related to three main aspects: language
pair, translation domain, and system typology. For
instance, notice that most metrics exhibit a lower
quality in the case of the ?AE05? test bed. The reason
is that, while in the rest of test beds all systems are
statistical, the ?AE05? test bed presents the particu-
larity of providing automatic translations produced
by heterogeneous MT systems (i.e., systems belong-
ing to different paradigms)6. The fact that most sys-
tems are statistical also explains why, in general,
lexical metrics exhibit a higher quality. However,
highest levels of quality are not in all cases attained
by metrics at the lexical level (see highlighted val-
ues). In fact, there is only one metric, ?ROUGEW ?
(based on lexical matching), which is consistently
among the top-scoring in all test beds according to
both meta-evaluation criteria. The underlying cause
is simple: current metrics do not provide a global
measure of quality, but account only for partial as-
pects of it. Apart from evincing the importance of
the meta-evaluation process, these results strongly
suggest the need for conducting heterogeneous MT
evaluations.
6Specifically, all systems are statistical except one which is
human-aided.
323
Opt.K(AE.04) = {SP-NISTp}
Opt.K(CE.04) = {ROUGEW , SP-NISTp, ROUGEL}
Opt.K(AE.05) = {METEORwnsyn, SP-NISTp, DP-Or-*}
Opt.K(CE.05) = {SP-NISTp}
Opt.R(AE.04) = {ROUGEW ,ROUGEL,CP-Oc-*,METEORwnsyn,DP-Or-*,DP-Ol-*,GTM.e2,DR-Or-*,CP-STM}
Opt.R(CE.04) = {ROUGEL,CP-Oc-*,ROUGEW , SP-Op-*,METEORwnsyn,DP-Or-*,GTM.e2, 1-WER,DR-Or-*}
Opt.R(AE.05) = {DP-Or-*,ROUGEW }
Opt.R(CE.05) = {ROUGEW ,ROUGEL,DP-Or-*,CP-Oc-*, 1-TER,GTM.e2,DP-HWCr,CP-STM}
Table 3: Optimal metric sets
4.3 Finding Optimal Metric Combinations
In that respect, we study the applicability of the two
combination strategies presented. Optimal metric
sets are determined by maximizing over the corre-
sponding meta-evaluation measure (KING or Rsnt).
However, because exploring all possible combina-
tions was not viable, we have used a simple algo-
rithm which performs an approximate search. First,
individual metrics are ranked according to their
quality. Then, following that order, metrics are
added to the optimal set only if in doing so the global
quality increases. Since no training is required it has
not been necessary to keep a held-out portion of the
data for test (see Section 4.4 for further discussion).
Optimal metric sets are displayed in Table 3. In-
side each set, metrics are sorted in decreasing quality
order. The ?Optimal Combination? line in Table 2
shows the quality attained by these sets, combined
under QUEEN in the case of KING optimization, and
under ULC in the case of optimizing over Rsnt. In
most cases optimal sets consist of metrics operat-
ing at different linguistic levels, mostly at the lexical
and syntactic levels. This is coherent with the find-
ings in Section 4.2. Metrics at the semantic level
are selected only in two cases, corresponding to the
Rsnt optimization in ?AE04? and ?CE04? test beds.
Also in two cases, corresponding to the KING opti-
mization in ?AE04? and ?CE05? test beds, it has not
been possible to find any metric combination which
outperforms the best individual metric. This is not
a discouraging result. After all, in these cases, the
best metric alone achieves already a very high qual-
ity (0.79 and 0.70, respectively). The fact that a sin-
gle feature suffices to discern between manual and
automatic translations indicates that MT systems are
easily distinguishable, possibly because of their low
quality and/or because they are all based on the same
translation paradigm.
4.4 Portability
It can be argued that metric set optimization is itself
a training process; each metric would have an asso-
ciated binary parameter controlling whether it is se-
lected or not. For that reason, in Table 4, we have
analyzed the portability of optimal metric sets (i)
across test beds and (ii) across combination strate-
gies. As to portability across test beds (i.e., across
language pairs and years), the reader must focus
on the cells for which the meta-evaluation criterion
guiding the metric set optimization matches the cri-
terion used in the evaluation, i.e., the top-left and
bottom-right 16-cell quadrangles. The fact that the
4 values in each subcolumn are in a very similar
range confirms that optimal metric sets port well
across test beds. We have also studied the portabil-
ity of optimal metric sets across combination strate-
gies. In other words, although QUEEN and ULC
are thought to operate on metric combinations re-
spectively optimized on the basis of human likeness
and human acceptability, we have studied the effects
of applying either measure over metric combina-
tions optimized on the basis of the alternative meta-
evaluation criterion. In this case, the reader must
compare top-left vs. bottom-left (KING) and top-
right vs. bottom-right (Rsnt) 16-cell quadrangles. It
can be clearly seen that optimal metric sets, in gen-
eral, do not port well across meta-evaluation criteria,
particularly from human likeness to human accept-
ability. However, interestingly, in the case of ?AE05?
(i.e., heterogeneous systems), the optimal metric set
ports well from human acceptability to human like-
ness. We speculate that system heterogeneity has
contributed positively for the sake of robustness.
5 Conclusions
As an alternative to current parametric combination
techniques, we have presented two different meth-
324
Metric KING Rsnt
Set AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05
Opt.K(AE.04) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39
Opt.K(CE.04) 0.78 0.64 0.57 0.67 0.49 0.51 0.39 0.43
Opt.K(AE.05) 0.74 0.63 0.61 0.66 0.48 0.51 0.39 0.42
Opt.K(CE.05) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39
Opt.R(AE.04) 0.62 0.56 0.52 0.49 0.64 0.61 0.53 0.58
Opt.R(CE.04) 0.68 0.59 0.55 0.56 0.63 0.63 0.51 0.57
Opt.R(AE.05) 0.75 0.64 0.59 0.69 0.62 0.60 0.54 0.57
Opt.R(CE.05) 0.64 0.56 0.51 0.52 0.63 0.57 0.53 0.61
Table 4: Portability of combination strategies
ods: a genuine non-parametric method based on hu-
man likeness, and a parametric method based human
acceptability in which the parameter weights are set
equiprobable. We have shown that both strategies
may yield a significantly improved quality by com-
bining metrics at different linguistic levels. Besides,
we have shown that these methods generalize well
across test beds. Thus, a valid path towards hetero-
geneous automatic MT evaluation has been traced.
We strongly believe that future MT evaluation cam-
paigns should benefit from these results specially for
the purpose of comparing systems based on different
paradigms. These techniques could also be used to
build better MT systems by allowing system devel-
opers to perform more accurate error analyses and
less biased adjustments of system parameters.
As an additional result, we have found that there
is a tight relationship between human acceptability
and human likeness. This result, coherent with the
findings by Amigo? et al (2006), suggests that the
two criteria are interchangeable. This would be a
point in favour of combination schemes based on hu-
man likeness, since human assessments ?which are
expensive to acquire, subjective and not reusable?
are not required. We also interpret this result as an
indication that human assessors probably behave in
many cases in a discriminative manner. For each test
case, assessors would inspect the source sentence
and the set of human references trying to identify
the features which ?good? translations should com-
ply with, for instance regarding adequacy and flu-
ency. Then, they would evaluate automatic transla-
tions roughly according to the number and relevance
of the features they share and the ones they do not.
For future work, we plan to study the inte-
gration of finer features as well as to conduct a
rigorous comparison between parametric and non-
parametric combination schemes. This may involve
reproducing the works by Kulesza and Shieber
(2004) and Albrecht and Hwa (2007a). This would
also allow us to evaluate their approaches in terms of
both human likeness and human acceptability, and
not only on the latter criterion as they have been
evaluated so far.
Acknowledgements
This research has been funded by the Spanish Min-
istry of Education and Science, project OpenMT
(TIN2006-15307-C03-02). Our NLP group has
been recognized as a Quality Research Group (2005
SGR-00130) by DURSI, the Research Department
of the Catalan Government. We are thankful to En-
rique Amigo?, for his generous help and valuable
comments. We are also grateful to the NIST MT
Evaluation Campaign organizers, and participants
who agreed to share their system outputs and human
assessments for the purpose of this research.
References
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of
ACL, pages 880?887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of ACL, pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
325
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research. In Proceedings of EACL.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to
the Automatic Evaluation of Machine Translation. In
Proceedings of ACL, pages 140?147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd IHLT.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation.
Jesu?s Gime?nez. 2007. IQMT v 2.1. Technical
Manual. Technical report, TALP Research Center.
LSI Department. http://www.lsi.upc.edu/?nlp/IQMT/-
IQMT.v2.1.pdf.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of NLH-
NAACL.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102?121.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. Technical
report, NIST, August.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of COLING.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization.
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American chapter of the As-
sociation for Computational Linguistics (NAACL-07).
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. Evaluation Tool for Machine Trans-
lation: Fast Evaluation for MT Research. In Proceed-
ings of the 2nd LREC.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. In
Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
pages 148?155.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalu-
ation of machine translation, RC22176, IBM. Techni-
cal report, IBM T.J. Watson Research Center.
Chris Quirk. 2004. Training a Sentence-Level Ma-
chine Translation Confidence Metric. In Proceedings
of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, , and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of AMTA, pages 223?231.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with Para-
phrase Support. In Proceedings of EMNLP.
326
Semantic Role Labeling: An Introduction to
the Special Issue
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Xavier Carreras??
Massachusetts Institute of Technology
Kenneth C. Litkowski?
CL Research
Suzanne Stevenson?
University of Toronto
Semantic role labeling, the computational identification and labeling of arguments in text,
has become a leading task in computational linguistics today. Although the issues for this
task have been studied for decades, the availability of large resources and the development of
statistical machine learning methods have heightened the amount of effort in this field. This
special issue presents selected and representative work in the field. This overview describes
linguistic background of the problem, the movement from linguistic theories to computational
practice, the major resources that are being used, an overview of steps taken in computational
systems, and a description of the key issues and results in semantic role labeling (as revealed in
several international evaluations). We assess weaknesses in semantic role labeling and identify
important challenges facing the field. Overall, the opportunities and the potential for useful
further research in semantic role labeling are considerable.
1. Introduction
The sentence-level semantic analysis of text is concerned with the characterization of
events, such as determining ?who? did ?what? to ?whom,? ?where,? ?when,? and
?how.? The predicate of a clause (typically a verb) establishes ?what? took place,
and other sentence constituents express the participants in the event (such as ?who? and
?where?), as well as further event properties (such as ?when? and ?how?). The primary
task of semantic role labeling (SRL) is to indicate exactly what semantic relations hold
among a predicate and its associated participants and properties, with these relations
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Polite`cnica de Catalunya, Jordi Girona
Salgado 1?3, 08034 Barcelona, Spain. E-mail: lluism@lsi.upc.edu.
?? Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT, 32 Vassar St., Cambridge, MA
02139, USA. E-mail: carreras@csail.mit.edu.
? CL Research, 9208 Gue Road, Damascus, MD 20872 USA. E-mail: ken@clres.com.
? Department of Computer Science, 6 King?s College Road, Toronto, ON M5S 3G4, Canada.
E-mail: suzanne@cs.toronto.edu.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
drawn from a pre-specified list of possible semantic roles for that predicate (or class of
predicates). In order to accomplish this, the role-bearing constituents in a clause must
be identified and their correct semantic role labels assigned, as in:
[The girl on the swing]Agent [whispered]Pred to [the boy beside her]Recipient
Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities
participating in an event, and Temporal and Manner for the characterization of other
aspects of the event or participant relations. This type of role labeling thus yields a first-
level semantic representation of the text that indicates the basic event properties and
relations among relevant entities that are expressed in the sentence.
Research has proceeded for decades on manually created lexicons, grammars, and
other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000)
in support of deep semantic analysis of language input, but such approaches have been
labor-intensive and often restricted to narrow domains. The 1990s saw a growth in
the development of statistical machine learning methods across the field of computa-
tional linguistics, enabling systems to learn complex linguistic knowledge rather than
requiring manual encoding. These methods were shown to be effective in acquiring
knowledge necessary for semantic interpretation, such as the properties of predicates
and the relations to their arguments?for example, learning subcategorization frames
(Briscoe and Carroll 1997) or classifying verbs according to argument structure prop-
erties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large
corpora have been manually annotated with semantic roles in FrameNet (Fillmore,
Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and
NomBank (Meyers et al 2004), enabling the development of statistical approaches
specifically for SRL.
With the advent of supporting resources, SRL has become a well-defined task with
a substantial body of work and comparative evaluation (see, among others, Gildea and
Jurafsky [2002], Surdeanu et al [2003], Xue and Palmer [2004], Pradhan et al [2005a],
the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The
identification of event frames may potentially benefit many natural language processing
(NLP) applications, such as information extraction (Surdeanu et al 2003), question
answering (Narayanan and Harabagiu 2004), summarization (Melli et al 2005), and
machine translation (Boas 2002). Related work on classifying the semantic relations in
noun phrases has also been encouraging for NLP tasks (Moldovan et al 2004; Rosario
and Hearst 2004).
Although the use of SRL systems in real-world applications has thus far been
limited, the outlook is promising for extending this type of analysis to many appli-
cations requiring some level of semantic interpretation. SRL represents an excellent
framework with which to perform research on computational techniques for acquiring
and exploiting semantic relations among the different components of a text.
This special issue of Computational Linguistics presents several articles represent-
ing the state-of-the-art in SRL, and this overview is intended to provide a broader
context for that work. First, we briefly discuss some of the linguistic views on se-
mantic roles that have had the most influence on computational approaches to SRL
and related NLP tasks. Next, we show how the linguistic notions have influenced
the development of resources that support SRL. We then provide an overview of
SRL methods and describe the state-of-the-art as well as current open problems in the
field.
146
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
2. Semantic Roles in Linguistics
Since the foundational work of Fillmore (1968), considerable linguistic research has been
devoted to the nature of semantic roles. Although there is substantial agreement on
major semantic roles, such as Agent and Theme, there is no consensus on a definitive
list of semantic roles, or even whether such a list exists. Proposed lists range from a
large set of situation-specific roles, such as Suspect, Authorities, and Offense (Fillmore,
Ruppenhofer, and Baker 2004), to a relatively small set of general roles, such as Agent,
Theme, Location, and Goal (typically referred to as thematic roles, as in Jackendoff
[1990]), to the set of two core roles, Proto-Agent and Proto-Theme, whose entailments
determine the precise relation expressed (Dowty 1991). This uncertainty within linguis-
tic theory carries over into computational work on SRL, where there is much variability
on the roles assumed in different resources.
A major focus of work in the linguistics community is on the mapping between the
predicate?argument structure that determines the roles, and the syntactic realization of
the recipients of those roles (Grimshaw 1990; Levin 1993; Levin and Rappaport Hovav
2005). Semantic role lists are generally viewed as inadequate for explaining the mor-
phosyntactic behavior of argument expression, with argument realization dependent
on a deeper lexical semantic representation of the components of the event that the
predicate describes. Although much of the mapping from argument structure to syntax
is predictable, this mapping is not completely regular, nor entirely understood. An
important question for SRL, therefore, is the extent to which performance is degraded
by the irregularities noted in linguistic studies of semantic roles.
Nonetheless, sufficient regularity exists to provide the foundation for meaningful
generalizations. Much research has focused on explaining the varied expression of verb
arguments within syntactic positions (Levin 1993). A major conclusion of that work is
that the patterns of syntactic alternation exhibit regularity that reflects an underlying
semantic similarity among verbs, forming the basis for verb classes. Such classes, and
the argument structure specifications for them, have proven useful in a number of NLP
tasks (Habash, Dorr, and Traum 2003; Shi and Mihalcea 2005), including SRL (Swier and
Stevenson 2004), and have provided the foundation for the computational verb lexicon
VerbNet (Kipper, Dang, and Palmer 2000).
This approach to argument realization focuses on the relation of morphosyntactic
behavior to argument semantics, and typically leads to a general conceptualization of
semantic roles. In frame semantics (Fillmore 1976), on the other hand, a word activates
a frame of semantic knowledge that relates linguistic semantics to encyclopedic knowl-
edge. This effort has tended to focus on the delineation of situation-specific frames (e.g.,
an Arrest frame) and correspondingly more specific semantic roles (e.g., Suspect and
Authorities) that codify the conceptual structure associated with lexical items (Fillmore,
Ruppenhofer, and Baker 2004). With a recognition that many lexical items could activate
any such frame, this approach leads to lexical classes of a somewhat different nature
than those of Levin (1993). Whereas lexical items in a Levin class are syntactically
homogeneous and share coarse semantic properties, items in a frame may syntactically
vary somewhat but share fine-grained, real-world semantic properties.
A further difference in these perspectives is the view of the roles themselves. In
defining verb classes that capture argument structure similarities, Levin (1993) does not
explicitly draw on the notion of semantic role, instead basing the classes on behavior
that is hypothesized to reflect the properties of those roles. Other work also eschews
the notion of a simple list of roles, instead postulating underlying semantic structure
that captures the relevant properties (Levin and Rappaport Hovav 1998). Interestingly,
147
Computational Linguistics Volume 34, Number 2
as described in Fillmore, Ruppenhofer, and Baker (2004), frame semantics also avoids a
predefined list of roles, but for different reasons. The set of semantic roles, called frame
elements, are chosen for each frame, rather than being selected from a predefined list
that may not capture the relevant distinctions in that particular situation. Clearly, to
the extent that disagreement persists on semantic role lists and the nature of the roles
themselves, SRL may be working on a shifting target.
These approaches also differ in the broad characterization of event participants
(and their roles) as more or less essential to the predicate. In the more syntactic-oriented
approaches, roles are typically divided into two categories: arguments, which cap-
ture a core relation, and adjuncts, which are less central. In frame semantics, the roles
are divided into core frame elements (e.g., Suspect, Authorities, Offense) and periph-
eral or extra-thematic elements (e.g., Manner, Time, Place). These distinctions carry
over into SRL, where we see that systems generally perform better on the more central
arguments.
Finally, although predicates are typically expressed as verbs, and thus much work
in both linguistics and SRL focuses on them, some nouns and adjectives may be used
predicatively, assigning their own roles to entities (as in the adjective phrase proud that
we finished the paper, where the subordinate clause is a Theme argument of the adjective
proud). Frame semantics tends to include in a frame relevant non-verb lexical items,
due to the emphasis on a common situation semantics. In contrast, the morphosyntactic
approaches have focused on defining classes of verbs only, because they depend on
common syntactic behavior that may not be apparent across syntactic categories.
Interestingly, prepositions have a somewhat dual status with regard to role labeling.
In languages like English, prepositions serve an important function in signaling the rela-
tion of a participant to a verb. For example, it is widely accepted that to in give the book to
Mary serves as a grammatical indicator of the Recipient role assigned by the verb, rather
than as a role assigner itself. In other situations, however, a preposition can be viewed
as a role-assigning predicate in its own right. Although some work in computational
linguistics is tackling the issue of the appropriate characterization of prepositions and
their contribution to semantic role assignment (as we see subsequently), much work
remains in order to fully integrate linguistic theories of prepositional function and
semantics into SRL.
3. From Linguistic Theory to Computational Resources
The linguistic approaches to semantic roles discussed previously have greatly influ-
enced current work on SRL, leading to the creation of significant computational lexicons
capturing the foundational properties of predicate?argument relations.
In the FrameNet project (Fillmore, Ruppenhofer, and Baker 2004), lexicographers
define a frame to capture some semantic situation (e.g., Arrest), identify lexical items
as belonging to the frame (e.g., apprehend and bust), and devise appropriate roles for
the frame (e.g., Suspect, Authorities, Offense). They then select and annotate example
sentences from the British National Corpus and other sources to illustrate the range of
possible assignments of roles to sentence constituents for each lexical item (at present,
over 141,000 sentences have been annotated).
FrameNet thus consists of both a computational lexicon and a role-annotated cor-
pus. The existence of such a corpus enabled Gildea and Jurafsky (2002) to develop the
first statistical machine learning approach to SRL, using various lexical and syntactic
features such as phrase type and grammatical function calculated over the annotated
constituents. Although this research spurred the current wave of SRL work that has
148
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
refined and extended Gildea and Jurafsky?s approach, the FrameNet data has not been
used extensively. One issue is that the corpus is not a representative sample of the
language, but rather consists of sentences chosen manually to illustrate the possible
role assignments for a given lexical item. Another issue is that the semantic roles are
situation-specific, rather than general roles like Agent, Theme, and Location that can be
used across many situations and genres.
The computational verb lexicon, VerbNet (Kipper, Dang, and Palmer 2000), instead
builds on Levin?s (1993) work on defining verb classes according to shared argument re-
alization patterns. VerbNet regularizes and extends the original Levin classes; moreover,
each class is explicitly associated with argument realization specifications that state the
constituents that a verb can occur with and the role assigned to each. The roles are
mostly drawn from a small set (around 25) of general roles widely used in linguistic
theory. This lexicon has been an important resource in computational linguistics, but
because of the lack of an associated role-annotated corpus, it has only been used directly
in SRL in an unsupervised setting (Swier and Stevenson 2004).
Research on VerbNet inspired the development of the Proposition Bank (PropBank;
Palmer, Gildea, and Kingsbury 2005), which has emerged as a primary resource for
research in SRL (and used in four of the articles in this special issue). PropBank ad-
dresses some of the issues for SRL posed by the FrameNet data. First, the PropBank
project has annotated the semantic roles for all verbs in the Penn Treebank corpus (the
Wall Street Journal [WSJ] news corpus). This provides a representative sample of text
with role-annotations, in contrast to FrameNet?s reliance on manually selected, illus-
trative sentences. Importantly, PropBank?s composition allows for consideration of the
statistical patterns across natural text. Although there is some concern about the limited
genre of its newspaper text, this aspect has the advantage of allowing SRL systems to
benefit from the state-of-the-art syntactic parsers and other resources developed with
the WSJ TreeBank data. Moreover, current work is extending the PropBank annotation
to balanced corpora such as the Brown corpus.
The lexical information associated with verbs in PropBank also differs significantly
from the situation-specific roles of FrameNet. At the same time, the PropBank designers
recognize the difficulty of providing a small, predefined list of semantic roles that is suf-
ficient for all verbs and predicate?argument relations, as in VerbNet. PropBank instead
takes a ?theory-neutral? approach to the designation of core semantic roles. Each verb
has a frameset listing its allowed role labelings in which the arguments are designated
by number (starting from 0). Each numbered argument is provided with an English-
language description specific to that verb. Participants typically considered as adjuncts
are given named argument roles, because there is more general agreement on such
modifiers as Temporal or Manner applying consistently across verbs. Different senses
for a polysemous verb have different framesets; however, syntactic alternations which
preserve meaning (as identified in Levin [1993]) are considered to be a single frameset.
While the designations of Arg0 and Arg1 are intended to indicate the general roles of
Agent and Theme/Patient across verbs, other argument numbers do not consistently
correspond to general (non-verb-specific) semantic roles.
Given the variability in the sets of roles used across the computational resources,
an important issue is the extent to which different role sets affect the SRL task, as well
as subsequent use of the output in other NLP applications. Gildea and Jurafsky (2002)
initiated this type of investigation by exploring whether their results were dependent
on the set of semantic roles they used. To this end, they mapped the FrameNet frame
elements into a set of abstract thematic roles (i.e., more general roles such as Agent,
Theme, Location), and concluded that their system could use these thematic roles
149
Computational Linguistics Volume 34, Number 2
without degradation. Similar questions must be investigated in the context of PropBank,
where the framesets for the verbs may have significant domain-specific meanings and
arguments due to the dependence of the project on WSJ data. Given the uncertainty in
the linguistic status of semantic role lists, and the lack of evidence about which types
of roles would be most useful in various NLP tasks, an important ongoing focus of
attention is the value of mapping between the role sets of the different resources (Swier
and Stevenson 2005; Loper, Yi, and Palmer 2007; Yi, Loper, and Palmer 2007).
We noted previously the somewhat special part that prepositions play in marking
semantic relations, in some sense mediating the role assignment of a verb to an argu-
ment. The resources noted earlier differ in their treatment of prepositions. In VerbNet,
for example, prepositions are listed explicitly as part of the syntactic context in which
a role is assigned (e.g., Agent V Prep(for) Recipient), but it is the NP object of the prep-
osition that receives the semantic role. In FrameNet and PropBank, on the other hand,
the full prepositional phrase is considered as the frame element (the constituent re-
ceiving the role). Clearly, further work needs to proceed on how to best capture the in-
teraction between verbs and prepositions in SRL. This is especially complex given
the high polysemy of prepositions, and work has proceeded on relating preposition
disambiguation to role assignment (e.g., O?Hara and Wiebe 2003). For such approaches
to make meaningful progress, resources are needed that elaborate the senses of prepo-
sitions and relate those senses to semantic roles. In The Preposition Project (TPP;
Litkowski and Hargraves 2005), a comprehensive, hierarchical characterization of the
semantic roles for all preposition senses in English is being developed. TPP has sense-
tagged more than 25,000 preposition instances in FrameNet sentences, allowing for
comprehensive investigation of the linking between preposition sense and semantic role
assignment.
4. Approaches to Automatic SRL
The work on SRL has included a broad spectrum of probabilistic and machine-learning
approaches to the task. We focus here on supervised systems, because most SRL research
takes an approach requiring training on role-annotated data. We briefly survey the main
approaches to automatic SRL, and the types of learning features used.
4.1 SRL Step by Step
Given a sentence and a designated verb, the SRL task consists of identifying the bound-
aries of the arguments of the verb predicate (argument identification) and labeling
them with semantic roles (argument classification). The most common architecture for
automatic SRL consists of the following steps to achieve these subtasks.
The first step in SRL typically consists of filtering (or pruning) the set of argu-
ment candidates for a given predicate. Because arguments may be a continuous or
discontinuous sequence of words, any subsequence of words in the sentence is an
argument candidate. Exhaustive exploration of this space of candidates is not feasible,
because it is both very large and imbalanced (i.e., the vast majority of candidates are
not actual arguments of the verb). The simple heuristic rules of Xue and Palmer (2004)
are commonly used to perform filtering because they greatly reduce the set of candidate
arguments, while maintaining a very high recall.
The second step consists of a local scoring of argument candidates by means of
a function that outputs probabilities (or confidence scores) for each of the possible
150
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
role labels, plus an extra ?no-argument? label indicating that the candidate should
not be considered an argument in the solution. In this step, candidates are usually
treated independently of each other. A crucial aspect in local scoring (see Section 4.2)
is the representation of candidates with features, rather than the particular choice of
classification algorithm.
Argument identification and classification may be treated jointly or separately in
the local scoring step. In the latter case, a pipeline of two subprocesses is typically
applied, first scoring between ?argument? and ?no-argument? labels, and then scoring
the particular argument labels. Because argument identification is closely related to
syntax and argument classification is more a semantic issue, useful features for the two
subtasks may be very different?that is, a good feature for addressing recognition may
hurt classification and vice versa (Pradhan et al 2005a).
The third step in SRL is to apply a joint scoring (or global scoring) in order to
combine the predictions of local scorers to produce a good structure of labeled argu-
ments for the predicate. In this step, dependencies among several arguments of the same
predicate can be exploited. For instance, Punyakanok, Roth, and Yih (this issue) ensure
that a labeling satisfies a set of structural and SRL-dependent constraints (arguments
do not overlap, core arguments do not repeat, etc.). Also in this issue, Toutanova,
Haghighi, and Manning apply re-ranking to select the best among a set of candidate
complete solutions produced by a base SRL system. Finally, probabilistic models have
also been applied to produce the structured output, for example, generative models
(Thompson, Levy, and Manning 2003), sequence tagging with classifiers (Ma`rquez et al
2005; Pradhan et al 2005b), and Conditional Random Fields on tree structures (Cohn
and Blunsom 2005). These approaches at a global level may demand considerable extra
computation, but current optimization techniques help solve them quite efficiently.
Some variations in the three-step architecture are found. Systems may bypass one
of the steps, by doing only local scoring, or skipping directly to joint scoring. A fourth
step may consist of fixing common errors or enforcing coherence in the final solution.
This postprocess usually consists of a set of hand-developed heuristic rules that are
dependent on a particular architecture and corpus of application.
An important consideration within this general SRL architecture is the combination
of systems and input annotations. Most SRL systems include some kind of combi-
nation to increase robustness, gain coverage, and reduce effects of parse errors. One
may combine: (1) the output of several independent SRL basic systems (Surdeanu
et al 2007; Pradhan et al 2005b), or (2) several outputs from the same SRL system
obtained by changing input annotations or other internal parameters (Koomen et al
2005; Toutanova, Haghighi, and Manning 2005). The combination can be as simple as
selecting the best among the set of complete candidate solutions, but usually consists of
combining fragments of alternative solutions to construct the final output. Finally, the
combination component may involve machine learning or not. The gain in performance
from the combination step is consistently between two and three F1 points. However, a
combination approach increases system complexity and penalizes efficiency.
Several exceptions to this described architecture for SRL can be found in the lit-
erature. One approach entails joint labeling of all predicates of the sentence, instead
of proceeding one by one. This opens the possibility of exploiting dependencies among
the different verbs in the sentence. However, the complexity may grow significantly, and
results so far are inconclusive (Carreras, Ma`rquez, and Chrupa?a 2004; Surdeanu et al
2007). Other promising approaches draw on dependency parsing rather than traditional
phrase structure parsing (Johansson and Nugues 2007), or combine parsing and SRL
into a single step of semantic parsing (Musillo and Merlo 2006).
151
Computational Linguistics Volume 34, Number 2
4.2 Feature Engineering
As previously noted, devising the features with which to encode candidate arguments
is crucial for obtaining good results in the SRL task. Given a verb and a candidate argu-
ment (a syntactic phrase) to be classified in the local scoring step, three types of features
are typically used: (1) features that characterize the candidate argument and its context;
(2) features that characterize the verb predicate and its context; and (3) features that cap-
ture the relation (either syntactic or semantic) between the candidate and the predicate.
Gildea and Jurafsky (2002) presented a compact set of features across these three
types, which has served as the core of most of the subsequent SRL work: (1) the phrase
type, headword, and governing category of the constituent; (2) the lemma, voice, and
subcategorization pattern of the verb; and (3) the left/right position of the constituent
with respect to the verb, and the category path between them. Extensions to these fea-
tures have been proposed in various directions. Exploiting the ability of some machine
learning algorithms to work with very large feature spaces, some authors have largely
extended the representation of the constituent and its context, including among others:
first and last words (and part-of-speech) in the constituent, bag-of-words, n-grams of
part of speech, and sequence of top syntactic elements in the constituent. Parent and
sibling constituents in the tree may also be codified with all the previous structural and
lexical features (Pradhan et al 2005a; Surdeanu et al 2007). Other authors have designed
new features with specific linguistic motivations. For instance, Surdeanu et al (2003)
generalized the concept of headword with the content word feature. They also used
named entity labels as features. Xue and Palmer (2004) presented the syntactic frame
feature, which captures the overall sentence structure using the verb predicate and the
constituent as pivots. All these features resulted in a significant increase in performance.
Finally, regarding the relation between the constituent and the predicate, several
variants of Gildea and Jurafsky?s syntactic path have been proposed in the literature
(e.g., generalizations to avoid sparsity, and adaptations to partial parsing). Also, some
attempts have been made at characterizing the semantic relation between the predicate
and the constituent. In Zapirain, Agirre, and Ma`rquez (2007) and Erk (2007), selectional
preferences between predicate and headword of the constituent are explored to generate
semantic compatibility features. Using conjunctions of several of the basic features is
also common practice. This may be very relevant when the machine learning method
used is linear in the space of features.
Joint scoring and combination components open the door to richer types of fea-
tures, which may take into account global properties of the candidate solution plus de-
pendencies among the different arguments. The most remarkable work in this direction
is the reranking approach by Toutanova, Haghighi, and Manning in this issue. When
training the ranker to select the best candidate solution they codify pattern features as
strings containing the whole argument structure of the candidate. Several variations
of this type of feature (with different degrees of generalization to avoid sparseness)
allow them to significantly increase the performance of the base system. Also related,
Pradhan et al (2005b) and Surdeanu et al (2007) convert the confidence scores of several
base SRL systems into features for training a final machine learning?based combination
system. Surdeanu et al (2007) develop a broad spectrum of features, with sentence-
based information, describing the role played by the candidate argument in every
solution proposed by the different base SRL systems.
A completely different approach to feature engineering is the use of kernel meth-
ods to implicitly exploit all kinds of substructures in the syntactic representation of
the candidates. This knowledge poor approach intends to take advantage of a massive
152
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
quantity of features without the need for manual engineering of specialized features.
This motivation might be relevant for fast system development and porting, especially
when specialized linguistic knowledge of the language of application is not available.
The most studied approach consists of using some variants of the ?all subtrees kernel?
applied to the sentence parse trees. The work by Moschitti, Pighin, and Basili in this
issue is the main representative of this family.
5. Empirical Evaluations of SRL Systems
Many experimental studies have been conducted since the work of Gildea and Jurafsky
(2002), including seven international evaluation tasks in ACL-related conferences and
workshops: the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and Ma`rquez
2004, 2005), the SIGLEX Senseval-3 in 2004 (Litkowski 2004), and four tasks in the
SIGLEX SemEval in 2007 (Pradhan et al 2007; Ma`rquez et al 2007; Baker, Ellsworth, and
Erk 2007; Litkowski and Hargraves 2007). In the subsequent sections, we summarize
their main features, results, and conclusions, although note that the scores are not
directly comparable across different exercises, due to differences in scoring and in the
experimental methodologies.
5.1 Task Definition and Evaluation Metrics
The standard experiment in automatic SRL can be defined as follows: Given a sentence
and a target predicate appearing in it, find the arguments of the predicate and label
them with semantic roles. A system is evaluated in terms of precision, recall, and F1 of
the labeled arguments. In evaluating a system, an argument is considered correct when
both its boundaries and the semantic role label match a gold standard. Performance
can be divided into two components: (1) the precision, recall, and F1 of unlabeled
arguments, measuring the accuracy of the system at segmenting the sentence; and (2)
the classification accuracy of assigning semantic roles to the arguments that have been
correctly identified. In calculating the metrics, the de facto standard is to give credit only
when a proposed argument perfectly matches an argument in the reference solution;
nonetheless, variants that give some credit for partial matching also exist.
5.2 Shared Task Experiments Using FrameNet, PropBank, and VerbNet
To date, most experimental work has made use of English data annotated either with
PropBank or FrameNet semantic roles.
The CoNLL shared tasks in 2004 and 2005 were based on PropBank (Carreras and
Ma`rquez 2004, 2005), which is the largest evaluation benchmark available today, and
also the most used by researchers?all articles in this special issue dealing with English
use this benchmark. In the evaluation, the best systems obtained an F1 score of ?80%,
and have achieved only minimal improvements since then. The articles in this issue by
Punyakanok, Roth, and Yih; Toutanova, Haghighi, and Manning; and Pradhan, Ward,
and Martin describe such efforts. An analysis of the outputs in CoNLL-2005 showed
that argument identification accounts for most of the errors: a system will recall ?81%
of the correct unlabeled arguments, and ?95% of those will be assigned the correct
semantic role. The analysis also showed that systems recognized core arguments better
than adjuncts (with F1 scores from the high 60s to the high 80s for the former, but below
60% for the latter). Finally, it was also observed that, although systems performed better
153
Computational Linguistics Volume 34, Number 2
on verbs appearing frequently in training, the best systems could recognize arguments
of unseen verbs with an F1 in the low 70s, not far from the overall performance.
1
SemEval-2007 included a task on semantic evaluation for English, combining word
sense disambiguation and SRL based on PropBank (Pradhan et al 2007). Unlike the
CoNLL tasks, this task concentrated on 50 selected verbs. Interestingly, the data was
annotated using verb-independent roles using the PropBank/VerbNet mapping from
Yi, Loper, and Palmer (2007). The two participating systems could predict VerbNet roles
as accurately as PropBank verb-dependent roles.
Experiments based on FrameNet usually concentrate on a selected list of frames.
In Senseval-3, 40 frames were selected for an SRL task with the goal of replicating
Gildea and Jurafsky (2002) and improving on them (Litkowski 2004). Participants were
evaluated on assigning semantic roles to given arguments, with best F1 of 92%, and on
the task of segmenting and labeling arguments, with best F1 of 83%.
SemEval-2007 also included an SRL task based on FrameNet (Baker, Ellsworth, and
Erk 2007). It was much more complete, realistic, and difficult than its predecessor in
Senseval-3. The goal was to perform complete analysis of semantic roles on unseen
texts, first determining the appropriate frames of predicates, and then determining their
arguments labeled with semantic roles. It also involved creating a graph of the sentence
representing part of its semantics, by means of frames and labeled arguments. The
test data of this task consisted of novel manually-annotated documents, containing a
number of frames and roles not in the FrameNet lexicon. Three teams submitted results,
with precision percentages in the 60s, but recall percentages only in the 30s.
To our knowledge, there is no evidence to date on the relative difficulty of assigning
FrameNet or PropBank roles.
5.3 Impact of Syntactic Processing in SRL
Semantic roles are closely related to syntax, and, therefore, automatic SRL heavily relies
on the syntactic structure of the sentence. In PropBank, over 95% of the arguments
match with a single constituent of the parse tree. If the output produced by a statistical
parser is used (e.g., Collins?s or Charniak?s) the exact matching is still over 90%. More-
over, some simple rules can be used to join constituents and fix a considerable portion
of the mismatches (Toutanova, Haghighi, and Manning 2005). Thus, it has become a
common practice to use full parse trees as the main source for solving SRL.
The joint model presented in this issue by Toutanova, Haghighi, and Manning
obtains an F1 at ?90% on the WSJ test of the CoNLL-2005 evaluation when using gold-
standard trees; but with automatic syntactic analysis, its best result falls to ?80%. This
and other work consistently show that the drop in performance occurs in identifying
argument boundaries; when arguments are identified correctly with predicted parses,
the accuracy of assigning semantic roles is similar to that with correct parses.
A relevant question that has been addressed in experimental work concerns the
use of a partial parser instead of a parser that produces full WSJ trees. In the CoNLL-
2004 task, systems were restricted to the use of base syntactic phrases (i.e., chunks)
and clauses, and the best results that could be obtained were just below 70%. But the
training set in that evaluation was about five times smaller than that of the 2005 task.
Punyakanok, Roth, and Yih (this issue) and Surdeanu et al (2007) have shown that, in
1 The analysis summarized here was presented in the oral session at CoNLL-2005. The slides of the session,
containing the results supporting this analysis, are available in the CoNLL-2005 shared task Web site.
154
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
fact, a system working with partial parsing can do almost as well as a system working
with full parses, with differences in F1 of only ?2?3 points.
Currently, the top-performing systems on the CoNLL data make use of several
outputs of syntactic parsers, as discussed in Section 4. It is clear that many errors in
SRL are caused by having incorrect syntactic constituents, as reported by Punyakanok,
Roth, and Yih in this issue. By using many parses, the recognition of semantic roles is
more robust to parsing errors. Yet, it remains unanswered what is the most appropriate
level of syntactic analysis needed in SRL.
5.4 Generalization of SRL Systems to New Domains
Porting a system to a new domain, different than the domain used to develop and train
the system, is a challenging question in NLP. SRL is no exception, with the particular
difficulty that a predicate in a new domain may exhibit a behavior not contemplated
in the dictionary of frames at training time. This difficulty was identified as a major
challenge in the FrameNet-based task in SemEval-2007 (Baker, Ellsworth, and Erk 2007).
In the CoNLL-2005 task, WSJ-trained systems were tested on three sections of
the Brown corpus annotated by the PropBank team. The performance of all systems
dropped dramatically: The best systems scored F1 below 70%, as opposed to figures at
?80% when testing on WSJ data. This is perhaps not surprising, taking into account that
the pre-processing systems involved in the analysis (tagger and parser) also experienced
a significant drop in performance. The article in this issue by Pradhan, Ward, and
Martin further investigates the robustness across text genres when porting a system
from WSJ to Brown. Importantly, the authors claim that the loss in accuracy takes place
in assigning the semantic roles, rather than in the identification of argument boundaries.
5.5 SRL on Languages Other Than English
SemEval-2007 featured the first evaluation exercise of SRL systems for languages other
than English, namely for Spanish and Catalan (Ma`rquez et al 2007). The data was part
of the CESS-ECE corpus, consisting of ?100K tokens for each language. The semantic
role annotations are similar to PropBank, in that role labels are specific to each verb,
but also include a verb-independent thematic role label similar to the scheme proposed
in VerbNet. The task consisted of assigning semantic class labels to target verbs, and
identifying and labeling arguments of such verbs, in both cases using gold-standard
syntax. Only two teams participated, with best results at ?86% for disambiguating
predicates, and at ?83% for labeling arguments.
The work by Xue in this issue studies semantic role labeling for Chinese, using the
Chinese PropBank and NomBank corpora. Apart from working also with nominalized
predicates, this work constitutes the first comprehensive study on SRL for a language
different from English.
5.6 SRL with Other Parts-of-Speech
The SemEval-2007 task on disambiguating prepositions (Litkowski and Hargraves 2007)
used FrameNet sentences as the training and test data, with over 25,000 sentences for
the 34 most common English prepositions. Although not overtly defined as semantic
role labeling, each instance was characterized with a semantic role name and also had
an associated FrameNet frame element. Almost 80% of the prepositional phrases in the
instances were identified as core frame elements, and are likely to be closely associated
155
Computational Linguistics Volume 34, Number 2
with arguments of the words to which they are attached. The three participants used a
variety of methods, with the top performing team using machine learning techniques
similar to those in other semantic role labeling tasks.
6. Final Remarks
To date, SRL systems have been shown to perform reasonably well in some controlled
experiments, with F1 measures in the low 80s on standard test collections for English.
Still, a number of important challenges exist for future research on SRL. It remains
unclear what is the appropriate level of syntax needed to support robust analysis of
semantic roles, and to what degree improved performance in SRL is constrained by the
state-of-the-art in tagging and parsing. Beyond syntax, the relation of semantic roles to
other semantic knowledge (such as WordNet, named entities, or even a catalogue of
frames) has scarcely been addressed in the design of current SRL models. A deeper
understanding of these questions could help in developing methods that yield im-
proved generalization, and that are less dependent on large quantities of role-annotated
training data.
Indeed, the requirement of most SRL approaches for such training data, which is
both difficult and highly expensive to produce, is the major obstacle to the widespread
application of SRL across different genres and different languages. Given the degrada-
tion of performance when a supervised system is faced with unseen events or a testing
corpus different from training, this is a major impediment to increasing the application
of SRL even within English, a language for which two major annotated corpora are
available. It is critical for the future of SRL that research broadens to include wider
investigation of unsupervised and minimally supervised learning methods.
In addition to these open research problems, there are also methodological issues
that need to be addressed regarding how research is conducted and evaluated. Shared
task frameworks have been crucial in SRL development by supporting explicit compar-
isons of approaches, but such benchmark testing can also overly focus research efforts
on small improvements in particular evaluation measures. Improving the entire SRL
approach in a significant way may require more open-ended investigation and more
qualitative analysis.
Acknowledgments
We are grateful for the insightful comments
of two anonymous reviewers whose input
helped us to improve the article. This work
was supported by the Spanish Ministry of
Education and Science (Ma`rquez); the
Catalan Ministry of Innovation, Universities
and Enterprise; and a grant from NTT, Agmt.
Dtd. 6/21/1998 (Carreras); and NSERC of
Canada (Stevenson).
References
Baker, C., M. Ellsworth, and K. Erk. 2007.
SemEval-2007 Task 19: Frame semantic
structure extraction. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 99?104,
Prague, Czech Republic.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1364?1371,
Las Palmas de Gran Canaria, Spain.
Briscoe, T. and J. Carroll. 1997. Automatic
extraction of subcategorization from
corpora. In Proceedings of the 5th ACL
Conference on Applied Natural Language
Processing (ANLP), pages 356?363,
Washington, DC.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004 Shared
Task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL-2004),
pages 89?97, Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005 Shared
156
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
Task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, X., L. Ma`rquez, and G. Chrupa?a.
2004. Hierarchical recognition of
propositional arguments with perceptrons.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004), pages 106?109, Boston, MA.
Cohn, T. and P. Blunsom. 2005. Semantic role
labelling with tree conditional random
fields. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 169?172,
Ann Arbor, MI.
Copestake, A. and D. Flickinger. 2000. An
open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC),
pages 591?600, Athens, Greece.
Dowty, D. 1991. Thematic proto-roles and
argument selection. Language, 67:547?619.
Erk, K. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
Fillmore, C. 1968. The case for case. In
E. Bach and R. T. Harms, editors, Universals
in Linguistic Theory. Holt, Rinehart &
Winston, New York, pages 1?88.
Fillmore, C. J. 1976. Frame semantics and the
nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin
and Development of Language and Speech,
280:20?32.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. Framenet and representing
the link between semantic and syntactic
relations. In Churen Huang and Winfried
Lenders, editors, Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grimshaw, J. 1990. Argument Structure. MIT
Press, Cambridge, MA.
Habash, N., B. J. Dorr, and D. Traum. 2003.
Hybrid natural language generation from
lexical conceptual structures. Machine
Translation, 18(2):81?128.
Hirst, G. 1987. Semantic Interpretation and
the Resolution of Ambiguity. Cambridge
University Press, Cambridge.
Jackendoff, R. 1990. Semantic Structures. MIT
Press, Cambridge, MA.
Johansson, R. and P. Nugues. 2007. LTH:
Semantic structure extraction using
nonprojective dependency trees. In
Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague,
Czech Republic.
Kipper, K., H. T. Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon.
In Proceedings of the 17th National Conference
on Artificial Intelligence (AAAI-2000),
Austin, TX.
Koomen, P., V. Punyakanok, D. Roth, and
W. Yih. 2005. Generalized inference
with multiple semantic role labeling
systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, B. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
The University of Chicago Press,
Chicago, IL.
Levin, B. and M. Rappaport Hovav. 1998.
Building verb meanings. In M. Butt and
W. Geuder, editors, The Projection of
Arguments: Lexical and Compositional
Factors. CSLI Publications, Stanford, CA,
pages 97?134.
Levin, B. and M. Rappaport Hovav. 2005.
Argument Realization. Cambridge
University Press, Cambridge.
Litkowski, K. C. 2004. Senseval-3 task:
Automatic labeling of semantic roles. In
Proceedings of the 3rd International Workshop
on the Evaluation of Systems for the Semantic
Analysis of Text (Senseval-3), pages 9?12,
Barcelona, Spain.
Litkowski, K. C. and O. Hargraves. 2005.
The preposition project. In Proceedings
of the ACL-SIGSEM Workshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic
Formalisms and Applications, pages 171?179,
Colchester, UK.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 24?29, Prague, Czech Republic.
Loper, E., S. Yi, and M. Palmer. 2007.
Combining lexical resources: Mapping
between PropBank and VerbNet. In
Proceedings of the 7th International Workshop
on Computational Semantics, pages 118?128,
Tilburg, The Netherlands.
157
Computational Linguistics Volume 34, Number 2
Ma`rquez, L., P. R. Comas, J. Gime?nez, and
N. Catala`. 2005. Semantic role labeling
as sequential tagging. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 193?196, Ann Arbor, MI.
Ma`rquez, L., L. Villarejo, M.A. Mart??, and
M. Taule?. 2007. SemEval-2007 Task 09:
Multilevel semantic annotation of Catalan
and Spanish. In Proceedings of the 4th
International Workshop on Semantic
Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question answering summary handler
for the DUC-2005 Summarization Task.
In Proceedings of the HLT/EMNLP
Document Understanding Workshop (DUC),
Vancouver, Canada, available at
http://duc.nist.gov/pubs/2005papers/
simonfraseru.sarkar.pdf.
Merlo, P. and S. Stevenson. 2001. Automatic
verb classification based on statistical
distributions of argument structure.
Computational Linguistics, 27(3):373?408.
Meyers, A., R. Reeves, C. Macleod,
R. Szekely, V. Zielinska, B. Young, and
R. Grishman. 2004. The NomBank Project:
An interim report. In Proceedings of
the HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moldovan, D., A. Badulescu, M. Tatu,
D. Antohe, and R. Girju. 2004. Models for
the semantic classification of noun
phrases. In Proceedings of the HLT-NAACL
2004 Workshop on Computational Lexical
Semantics, pages 60?67, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate
parsing of the proposition bank. In
Proceedings of the Human Language
Technology Conference of the NAACL,
pages 101?104, New York, NY.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
O?Hara, T. and J. Wiebe. 2003. Preposition
semantic classification via Penn Treebank
and FrameNet. In Proceedings of the
Seventh Conference on Computational
Natural Language Learning (CoNLL-2003),
pages 79?86, Edmonton, Canada.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The Proposition Bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pradhan, S., K. Hacioglu, V. Krugler,
W. Ward, J. Martin, and D. Jurafsky. 2005a.
Support vector learning for semantic
argument classification. Machine Learning,
60(1):11?39.
Pradhan, S., K. Hacioglu, W. Ward, J. H.
Martin, and D. Jurafsky. 2005b. Semantic
role chunking combining complementary
syntactic views. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 217?220, Ann Arbor, MI.
Pradhan, S., E. Loper, D. Dligach, and
M. Palmer. 2007. SemEval-2007 Task 17:
English lexical sample, SRL and all words.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague,
Czech Republic.
Pustejovsky, J. 1995. The Generative Lexicon.
MIT Press, Cambridge, MA.
Rosario, B. and M. Hearst. 2004. Classifying
semantic relations in bioscience text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 430?437, Barcelona, Spain.
Schulte im Walde, S. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces
together: Combining FrameNet, VerbNet
and WordNet for robust semantic parsing.
In Computational Linguistics and Intelligent
Text Processing; Sixth International
Conference, CICLing 2005, Proceedings,
LNCS, vol 3406, pages 100?111, Mexico
City, Mexico.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Surdeanu, M., L. Ma`rquez, X. Carreras, and
P. R. Comas. 2007. Combination strategies
for semantic role labeling. Journal of
Artificial Intelligence Research (JAIR),
29:105?151.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 95?102, Barcelona, Spain.
Swier, R. and S. Stevenson. 2005. Exploiting
a verb lexicon in automatic semantic role
158
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
labelling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMLNP), pages
883?890, Vancouver, B.C., Canada.
Thompson, C. A., R. Levy, and C. Manning.
2003. A generative model for semantic role
labeling. In Proceedings of the 14th European
Conference on Machine Learning (ECML),
pages 397?408, Dubrovnik, Croatia.
Toutanova, K., A. Haghighi, and C. Manning.
2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 589?596, Ann Arbor, MI.
Xue, N. and M. Palmer. 2004. Calibrating
features for semantic role labeling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 88?94, Barcelona, Spain.
Yi, S., E. Loper, and M. Palmer. 2007. Can
semantic roles generalize across corpora?
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 548?555, Rochester, NY.
Zapirain, B., E. Agirre, and L. Ma`rquez. 2007.
UBC-UPC: Sequential SRL using
selectional preferences: an approach with
maximum entropy Markov models. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 354?357, Prague, Czech Republic.
159

Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
MT Evaluation: Human-like vs. Human Acceptable
Enrique Amigo?
 
, Jesu?s Gime?nez  , Julio Gonzalo   , and Llu??s Ma`rquez 
 
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
Juan del Rosal, 16, E-28040, Madrid

enrique,julio  @lsi.uned.es
 TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado, 1?3, E-08034, Barcelona
 jgimenez,lluism  @lsi.upc.edu
Abstract
We present a comparative study on Ma-
chine Translation Evaluation according to
two different criteria: Human Likeness
and Human Acceptability. We provide
empirical evidence that there is a relation-
ship between these two kinds of evalu-
ation: Human Likeness implies Human
Acceptability but the reverse is not true.
From the point of view of automatic eval-
uation this implies that metrics based on
Human Likeness are more reliable for sys-
tem tuning.
Our results also show that current evalua-
tion metrics are not always able to distin-
guish between automatic and human trans-
lations. In order to improve the descrip-
tive power of current metrics we propose
the use of additional syntax-based met-
rics, and metric combinations inside the
QARLA Framework.
1 Introduction
Current approaches to Automatic Machine Trans-
lation (MT) Evaluation are mostly based on met-
rics which determine the quality of a given transla-
tion according to its similarity to a given set of ref-
erence translations. The commonly accepted crite-
rion that defines the quality of an evaluation metric
is its level of correlation with human evaluators.
High levels of correlation (Pearson over 0.9) have
been attained at the system level (Eck and Hori,
2005). But this is an average effect: the degree of
correlation achieved at the sentence level, crucial
for an accurate error analysis, is much lower.
We argue that there is two main reasons that ex-
plain this fact:
Firstly, current MT evaluation metrics are based
on shallow features. Most metrics work only at the
lexical level. However, natural languages are rich
and ambiguous, allowing for many possible differ-
ent ways of expressing the same idea. In order to
capture this flexibility, these metrics would require
a combinatorial number of reference translations,
when indeed in most cases only a single reference
is available. Therefore, metrics with higher de-
scriptive power are required.
Secondly, there exists, indeed, two different
evaluation criteria: (i) Human Acceptability, i.e.,
to what extent an automatic translation could be
considered acceptable by humans; and (ii) Human
Likeness, i.e., to what extent an automatic transla-
tion could have been generated by a human trans-
lator. Most approaches to automatic MT evalu-
ation implicitly assume that both criteria should
lead to the same results; but this assumption has
not been proved empirically or even discussed.
In this work, we analyze this issue through em-
pirical evidence. First, in Section 2, we inves-
tigate to what extent current evaluation metrics
are able to distinguish between human and auto-
matic translations (Human Likeness). As individ-
ual metrics do not capture such distinction well, in
Section 3 we study how to improve the descrip-
tive power of current metrics by means of met-
ric combinations inside the QARLA Framework
(Amigo? et al, 2005), including a new family of
metrics based on syntactic criteria. Second, we
claim that the two evaluation criteria (Human Ac-
ceptability and Human Likeness) are indeed of a
different nature, and may lead to different results
(Section 4). However, translations exhibiting a
high level of Human Likeness obtain good results
in human judges. Therefore, automatic evaluation
metrics based on similarity to references should be
17
optimized over their capacity to represent Human
Likeness. See conclusions in Section 5.
2 Descriptive Power of Standard Metrics
In this section we perform a simple experiment in
order to measure the descriptive power of current
state-of-the-art metrics, i.e., their ability to capture
the features which characterize human translations
with respect to automatic ones.
2.1 Experimental Setting
We use the data from the Openlab 2006 Initiative1
promoted by the TC-STAR Consortium2. This
test suite is entirely based on European Parlia-
ment Proceedings3, covering April 1996 to May
2005. We focus on the Spanish-to-English transla-
tion task. For the purpose of evaluation we use the
development set which consists of 1008 sentences.
However, due to lack of available MT outputs for
the whole set we used only a subset of 504 sen-
tences corresponding to the first half of the devel-
opment set. Three human references per sentence
are available.
We employ ten system outputs; nine are based
on Statistical Machine Translation (SMT) sys-
tems (Gime?nez and Ma`rquez, 2005; Crego et al,
2005), and one is obtained from the free Sys-
tran4 on-line rule-based MT engine. Evalua-
tion results have been computed by means of the
IQMT5 Framework for Automatic MT Evaluation
(Gime?nez and Amigo?, 2006).
We have selected a representative set of 22 met-
ric variants corresponding to six different fami-
lies: BLEU (Papineni et al, 2001), NIST (Dodding-
ton, 2002), GTM (Melamed et al, 2003), mPER
(Leusch et al, 2003), mWER (Nie?en et al, 2000)
and ROUGE (Lin and Och, 2004a).
2.2 Measuring Descriptive Power of
Evaluation Metrics
Our main assumption is that if an evaluation met-
ric is able to characterize human translations, then,
human references should be closer to each other
than automatic translations to other human refer-
ences. Based on this assumption we introduce two
measures (ORANGE and KING) which analyze
1http://tc-star.itc.it/openlab2006/
2http://www.tc-star.org/
3http://www.europarl.eu.int/
4http://www.systransoft.com.
5The IQMT Framework may be freely downloaded at
http://www.lsi.upc.edu/?nlp/IQMT.
the descriptive power of evaluation metrics from
diferent points of view.
ORANGE Measure
ORANGE compares automatic and manual
translations one-on-one. Let  and  be the sets
of automatic and reference translations, respec-
tively, and 	
 an evaluation metric which out-
puts the quality of an automatic translation 

by comparison to  . ORANGE measures the de-
scriptive power as the probability that a human ref-
erence  is more similar than an automatic transla-
tion 
 to the rest of human references:

Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287?294,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Low-cost Enrichment of Spanish WordNet with Automatically Translated
Glosses: Combining General and Specialized Models
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
This paper studies the enrichment of Span-
ish WordNet with synset glosses automat-
ically obtained from the English Word-
Net glosses using a phrase-based Statisti-
cal Machine Translation system. We con-
struct the English-Spanish translation sys-
tem from a parallel corpus of proceed-
ings of the European Parliament, and study
how to adapt statistical models to the do-
main of dictionary definitions. We build
specialized language and translation mod-
els from a small set of parallel definitions
and experiment with robust manners to
combine them. A statistically significant
increase in performance is obtained. The
best system is finally used to generate a
definition for all Spanish synsets, which
are currently ready for a manual revision.
As a complementary issue, we analyze the
impact of the amount of in-domain data
needed to improve a system trained en-
tirely on out-of-domain data.
1 Introduction
Statistical Machine Translation (SMT) is today a
very promising approach. It allows to build very
quickly and fully automatically Machine Trans-
lation (MT) systems, exhibiting very competitive
results, only from a parallel corpus aligning sen-
tences from the two languages involved.
In this work we approach the task of enriching
Spanish WordNet with automatically translated
glosses1. The source glosses for these translations
are taken from the English WordNet (Fellbaum,
1Glosses are short dictionary definitions that accompany
WordNet synsets. See examples in Tables 5 and 6.
1998), which is linked, at the synset level, to Span-
ish WordNet. This resource is available, among
other sources, through the Multilingual Central
Repository (MCR) developed by the MEANING
project (Atserias et al, 2004).
We start by empirically testing the performance
of a previously developed English?Spanish SMT
system, built from the large Europarl corpus2
(Koehn, 2003). The first observation is that this
system completely fails to translate the specific
WordNet glosses, due to the large language varia-
tions in both domains (vocabulary, style, grammar,
etc.). Actually, this is confirming one of the main
criticisms against SMT, which is its strong domain
dependence. Since parameters are estimated from
a corpus in a concrete domain, the performance
of the system on a different domain is often much
worse. This flaw of statistical and machine learn-
ing approaches is well known and has been largely
described in the NLP literature, for a variety of
tasks (e.g., parsing, word sense disambiguation,
and semantic role labeling).
Fortunately, we count on a small set of Spanish
hand-developed glosses in MCR3. Thus, we move
to a working scenario in which we introduce a
small corpus of aligned translations from the con-
crete domain of WordNet glosses. This in-domain
corpus could be itself used as a source for con-
structing a specialized SMT system. Again, ex-
periments show that this small corpus alone does
not suffice, since it does not allow to estimate
good translation parameters. However, it is well
suited for combination with the Europarl corpus,
to generate combined Language and Translation
2The Europarl Corpus is available at: http://-
people.csail.mit.edu/people/koehn/publications/europarl
3About 10% of the 68,000 Spanish synsets contain a defi-
nition, generated without considering its English counterpart.
287
Models. A substantial increase in performance is
achieved, according to several standard MT eval-
uation metrics. Although moderate, this boost
in performance is statistically significant accord-
ing to the bootstrap resampling test described by
Koehn (2004b) and applied to the BLEU metric.
The main reason behind this improvement is
that the large out-of-domain corpus contributes
mainly with coverage and recall and the in-domain
corpus provides more precise translations. We
present a qualitative error analysis to support these
claims. Finally, we also address the important
question of how much in-domain data is needed
to be able to improve the baseline results.
Apart from the experimental findings, our study
has generated a very valuable resource. Currently,
we have the complete Spanish WordNet enriched
with one gloss per synset, which, far from being
perfect, constitutes an axcellent starting point for
a posterior manual revision.
Finally, we note that the construction of a
SMT system when few domain-specific data are
available has been also investigated by other au-
thors. For instance, Vogel and Tribble (2002) stud-
ied whether an SMT system for speech-to-speech
translation built on top of a small parallel corpus
can be improved by adding knowledge sources
which are not domain specific. In this work, we
look at the same problem the other way around.
We study how to adapt an out-of-domain SMT
system using in-domain data.
The rest of the paper is organized as follows.
In Section 2 the fundamentals of SMT and the
components of our MT architecture are described.
The experimental setting is described in Section 3.
Evaluation is carried out in Section 4. Finally, Sec-
tion 5 contains error analysis and Section 6 con-
cludes and outlines future work.
2 Background
Current state-of-the-art SMT systems are based on
ideas borrowed from the Communication Theory
field. Brown et al (1988) suggested that MT can
be statistically approximated to the transmission
of information through a noisy channel. Given a
sentence f = f1..fn (distorted signal), it is possi-
ble to approximate the sentence e = e1..em (origi-
nal signal) which produced f . We need to estimate
P (e|f), the probability that a translator produces
f as a translation of e. By applying Bayes? rule it
is decomposed into: P (e|f) = P (f |e)?P (e)P (f) .
To obtain the string e which maximizes the
translation probability for f , a search in the prob-
ability space must be performed. Because the de-
nominator is independent of e, we can ignore it for
the purpose of the search: e = argmaxeP (f |e) ?
P (e). This last equation devises three compo-
nents in a SMT system. First, a language model
that estimates P (e). Second, a translation model
representing P (f |e). Last, a decoder responsi-
ble for performing the arg-max search. Language
models are typically estimated from large mono-
lingual corpora, translation models are built out
from parallel corpora, and decoders usually per-
form approximate search, e.g., by using dynamic
programming and beam search.
However, in word-based models the modeling
of the context in which the words occur is very
weak. This problem is significantly alleviated by
phrase-based models (Och, 2002), which repre-
sent nowadays the state-of-the-art in SMT.
2.1 System Construction
Fortunately, there is a number of freely available
tools to build a phrase-based SMT system. We
used only standard components and techniques for
our basic system, which are all described below.
The SRI Language Modeling Toolkit (SRILM)
(Stolcke, 2002) supports creation and evaluation
of a variety of language models. We build trigram
language models applying linear interpolation and
Kneser-Ney discounting for smoothing.
In order to build phrase-based translation mod-
els, a phrase extraction must be performed on
a word-aligned parallel corpus. We used the
GIZA++ SMT Toolkit4 (Och and Ney, 2003) to
generate word alignments We applied the phrase-
extract algorithm, as described by Och (2002), on
the Viterbi alignments output by GIZA++. We
work with the union of source-to-target and target-
to-source alignments, with no heuristic refine-
ment. Phrases up to length five are considered.
Also, phrase pairs appearing only once are dis-
carded, and phrase pairs in which the source/target
phrase was more than three times longer than the
target/source phrase are ignored. Finally, phrase
pairs are scored by relative frequency. Note that
no smoothing is performed.
Regarding the arg-max search, we used the
Pharaoh beam search decoder (Koehn, 2004a),
which naturally fits with the previous tools.
4http://www.fjoch.com/GIZA++.html
288
3 Data Sets and Evaluation Metrics
As a general source of English?Spanish parallel
text, we used a collection of 730,740 parallel sen-
tences extracted from the Europarl corpus. These
correspond exactly to the training data from the
Shared Task 2: Exploiting Parallel Texts for Sta-
tistical Machine Translation from the ACL-2005
Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond5.
To be used as specialized source, we extracted,
from the MCR , the set of 6,519 English?Spanish
parallel glosses corresponding to the already de-
fined synsets in Spanish WordNet. These defini-
tions corresponded to 5,698 nouns, 87 verbs, and
734 adjectives. Examples and parenthesized texts
were removed. Parallel glosses were tokenized
and case lowered. We discarded some of these
parallel glosses based on the difference in length
between the source and the target. The gloss av-
erage length for the resulting 5,843 glosses was
8.25 words for English and 8.13 for Spanish. Fi-
nally, gloss pairs were randomly split into training
(4,843), development (500) and test (500) sets.
Additionally, we counted on two large mono-
lingual Spanish electronic dictionaries, consisting
of 142,892 definitions (2,112,592 tokens) (?D1?)
(Mart??, 1996) and 168,779 definitons (1,553,674
tokens) (?D2?) (Vox, 1990), respectively.
Regarding evaluation, we used up to four dif-
ferent metrics with the aim of showing whether
the improvements attained are consistent or not.
We have computed the BLEU score (accumu-
lated up to 4-grams) (Papineni et al, 2001), the
NIST score (accumulated up to 5-grams) (Dod-
dington, 2002), the General Text Matching (GTM)
F-measure (e = 1, 2) (Melamed et al, 2003),
and the METEOR measure (Banerjee and Lavie,
2005). These metrics work at the lexical level by
rewarding n-gram matches between the candidate
translation and a set of human references. Addi-
tionally, METEOR considers stemming, and al-
lows for WordNet synonymy lookup.
The discussion of the significance of the results
will be based on the BLEU score, for which we
computed a bootstrap resampling test of signifi-
cance (Koehn, 2004b).
5http://www.statmt.org/wpt05/.
4 Experimental Evaluation
4.1 Baseline Systems
As explained in the introduction we built two indi-
vidual baseline systems. The first baseline (?EU?)
system is entirely based on the training data from
the Europarl corpus. The second baseline system
(?WNG?) is entirely based on the training set from
of the in-domain corpus of parallel glosses. In the
second case phrase pairs occurring only once in
the training corpus are not discarded due to the ex-
tremely small size of the corpus.
Table 1 shows results of the two baseline sys-
tems, both for the development and test sets. We
compare the performance of the ?EU? baseline on
these data sets with respect to the (in-domain) Eu-
roparl test set provided by the organizers of the
ACL-2005 MT workshop. As expected, there is
a very significant decrease in performance (e.g.,
from 0.24 to 0.08 according to BLEU) when the
?EU? baseline system is applied to the new do-
main. Some of this decrement is also due to a cer-
tain degree of free translation exhibited by the set
of available ?quasi-parallel? glosses. We further
discuss this issue in Section 5.
The results obtained by ?WNG? are also very
low, though slightly better than those of ?EU?. This
is a very interesting fact. Although the amount of
data utilized to construct the ?WNG? baseline is
150 times smaller than the amount utilized to con-
struct the ?EU? baseline, its performance is higher
consistently according to all metrics. We interpret
this result as an indicator that models estimated
from in-domain data provide higher precision.
We also compare the results to those of a com-
mercial system such as the on-line version 5.0 of
SYSTRAN6, a general-purpose MT system based
on manually-defined lexical and syntactic trans-
fer rules. The performance of the baseline sys-
tems is significantly worse than SYSTRAN?s on
both development and test sets. This means that
a rule-based system like SYSTRAN is more ro-
bust than the SMT-based systems. The difference
against the specialized ?WNG? also suggests that
the amount of data used to train the ?WNG? base-
line is clearly insufficient.
4.2 Combining Sources: Language Models
In order to improve results, in first place we turned
our eyes to language modeling. In addition to
6http://www.systransoft.com/.
289
system BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EU-baseline 0.0737 2.8832 0.3131 0.2216 0.2881
WNG-baseline 0.1149 3.3492 0.3604 0.2605 0.3288
SYSTRAN 0.1625 3.9467 0.4257 0.2971 0.4394
test
EU-baseline 0.0790 2.8896 0.3131 0.2262 0.2920
WNG-baseline 0.0951 3.1307 0.3471 0.2510 0.3219
SYSTRAN 0.1463 3.7873 0.4085 0.2921 0.4295
acl05-test
EU-baseline 0.2381 6.5848 0.5699 0.2429 0.5153
Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the ?EU?
baseline system on the ACL-2005 SMT workshop test set extracted from the Europarl Corpus. BLEU.n4 shows the accumulated
BLEU score for 4-grams. NIST.n5 shows the accumulated NIST score for 5-grams. GTM.e1 and GTM.e2 show the GTM F1-
measure for different values of the e parameter (e = 1, e = 2, respectively). METEOR reflects the METEOR score.
the language model built from the Europarl cor-
pus (?EU?) and the specialized language model
based on the small training set of parallel glosses
(?WNG?), two specialized language models, based
on the two large monolingual Spanish electronic
dictionaries (?D1? and ?D2?) were used. We tried
several configurations. In all cases, language mod-
els are combined with equal probability. See re-
sults, for the development set, in Table 2.
As expected, the closer the language model is
to the target domain, the better results. Observe
how results using language models ?D1? and ?D2?
outperform results using ?EU?. Note also that best
results are in all cases consistently attained by us-
ing the ?WNG? language model. This means that
language models estimated from small sets of in-
domain data are helpful. A second conclusion is
that a significant gain is obtained by incrementally
adding (in-domain) specialized language models
to the baselines, according to all metrics but BLEU
for which no combination seems to significantly
outperform the ?WNG? baseline alone. Observe
that best results are obtained, except in the case
of BLEU, by the system using ?EU? as translation
model and ?WNG? as language model. We inter-
pret this result as an indicator that translation mod-
els estimated from out-of-domain data are help-
ful because they provide recall. A third interest-
ing point is that adding an out-of-domain language
model (?EU?) does not seem to help, at least com-
bined with equal probability than in-domain mod-
els. Same conclusions hold for the test set, too.
4.3 Tuning the System
Adjusting the Pharaoh parameters that control
the importance of the different probabilities that
govern the search may yield significant improve-
ments. In our case, it is specially important to
properly adjust the contribution of the language
models. We adjusted parameters by means of a
software based on the Downhill Simplex Method
in Multidimensions (William H. Press and Flan-
nery, 2002). The tuning was based on the improve-
ment attained in BLEU score over the develop-
ment set. We tuned 6 parameters: 4 language mod-
els (?lmEU , ?lmD1, ?lmD2, ?lmWNG), the transla-
tion model (??), and the word penalty (?w)7.
Results improve substantially. See Table 3. Best
results are still attained using the ?EU? translation
model. Interestingly, as suggested by Table 2, the
weight of language models is concentrated on the
?WNG? language model (?lmWNG = 0.95).
4.4 Combining Sources: Translation Models
In this section we study the possibility of combin-
ing out-of-domain and in-domain translation mod-
els aiming at achieving a good balance between
precision and recall that yields better MT results.
Two different strategies have been tried. In
a first stragegy we simply concatenate the out-
of-domain corpus (?EU?) and the in-domain cor-
pus (?WNG?). Then, we construct the translatation
model (?EUWNG?) as detailed in Section 2.1. A
second manner to proceed is to linearly combine
the two different translation models into a single
translation model (?EU+WNG?). In this case, we
can assign different weights (?) to the contribution
of the different models to the search. We can also
determine a certain threshold ? which allows us
7Final values when using the ?EU? translation model are
?lmEU = 0.22, ?lmD1 = 0, ?lmD2 = 0.01, ?lmWNG =
0.95, ?? = 1, and ?w = ?2.97, while when using the
?WNG? translation model final values are ?lmEU = 0.17,
?lmD1 = 0.07, ?lmD2 = 0.13, ?lmWNG = 1, ?? = 0.95,
and ?w = ?2.64.
290
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
EU EU 0.0737 2.8832 0.3131 0.2216 0.2881
EU WNG 0.1062 3.4831 0.3714 0.2631 0.3377
EU D1 0.0959 3.2570 0.3461 0.2503 0.3158
EU D2 0.0896 3.2518 0.3497 0.2482 0.3163
EU D1 + D2 0.0993 3.3773 0.3585 0.2579 0.3244
EU EU + D1 + D2 0.0960 3.2851 0.3472 0.2499 0.3160
EU D1 + D2 + WNG 0.1094 3.4954 0.3690 0.2662 0.3372
EU EU + D1 + D2 + WNG 0.1080 3.4248 0.3638 0.2614 0.3321
WNG EU 0.0743 2.8864 0.3128 0.2202 0.2689
WNG WNG 0.1149 3.3492 0.3604 0.2605 0.3288
WNG D1 0.0926 3.1544 0.3404 0.2418 0.3050
WNG D2 0.0845 3.0295 0.3256 0.2326 0.2883
WNG D1 + D2 0.0917 3.1185 0.3331 0.2394 0.2995
WNG EU + D1 + D2 0.0856 3.0361 0.3221 0.2312 0.2847
WNG D1 + D2 + WNG 0.0980 3.2238 0.3462 0.2479 0.3117
WNG EU + D1 + D2 + WNG 0.0890 3.0974 0.3309 0.2373 0.2941
Table 2: MT Results on development set, for several translation/language model configurations. ?EU? and ?WNG? refer to
the models estimated from the Europarl corpus and the training set of parallel WordNet glosses, respectively. ?D1?, and ?D2?
denote the specialized language models estimated from the two dictionaries.
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EU EU + D1 + D2 + WNG 0.1272 3.6094 0.3856 0.2727 0.3695
WNG EU + D1 + D2 + WNG 0.1269 3.3740 0.3688 0.2676 0.3452
test
EU EU + D1 + D2 + WNG 0.1133 3.4180 0.3720 0.2650 0.3644
WNG EU + D1 + D2 + WNG 0.1015 3.1084 0.3525 0.2552 0.3343
Table 3: MT Results on development and test sets after tuning for the ?EU + D1 + D2 + WNG? language model configuration
for the two translation models, ?EU? and ?WNG?.
to discard phrase pairs under a certain probability.
These weights and thresholds were adjusted8 as
detailed in Subsection 4.3. Interestingly, at combi-
nation time the importance of the ?WNG? transla-
tion model (?tmWNG = 0.9) is much higher than
that of the ?EU? translation model (?tmEU = 0.1).
Table 4 shows results for the two strategies.
As expected, the ?EU+WNG? strategy consistently
obtains the best results according to all metrics
both on the development and test sets, since it
allows to better adjust the relative importance of
each translation model. However, both techniques
achieve a very competitive performance. Results
improve, according to BLEU, from 0.13 to 0.16,
and from 0.11 to 0.14, for the development and
test sets, respectively.
We measured the statistical signficance of
the overall improvement in BLEU.n4 attained
with respect to the baseline results by ap-
plying the bootstrap resampling technique de-
scribed by Koehn (2004b). The 95% confi-
dence intervals extracted from the test set after
8We used values ?tmEU = 0.1, ?tmWNG = 0.9,
?tmEU = 0.1, and ?tmWNG = 0.01
10,000 samples are the following: IEU?base =
[0.0642, 0.0939], IWNG?base = [0.0788, 0.1112],
IEU+WNG?best = [0.1221, 0.1572]. Since the in-
tervals are not ovelapping, we can conclude that
the performance of the best combined method is
statistically higher than the ones of the two base-
line systems.
4.5 How much in-domain data is needed?
In principle, the more in-domain data we have the
better, but these may be difficult or expensive to
collect. Thus, a very interesting issue in the con-
text of our work is how much in-domain data is
needed in order to improve results attained using
out-of-domain data alone. To answer this question
we focus on the ?EU+WNG? strategy and analyze
the impact on performance (BLEU.n4) of special-
ized models extracted from an incrementally big-
ger number of example glosses. The results are
presented in the plot of Figure 1. We compute
three variants separately, by considering the use of
the in-domain data: only for the translation model
(TM), only for the language model (LM), and si-
multaneously in both models (TM+LM). In order
291
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EUWNG WNG 0.1288 3.7677 0.3949 0.2832 0.3711
EUWNG EU + D1 + D2 + WNG 0.1182 3.6034 0.3835 0.2759 0.3552
EUWNG EU + D1 + D2 + WNG (TUNED) 0.1554 3.8925 0.4081 0.2944 0.3998
EU+WNG WNG 0.1384 3.9743 0.4096 0.2936 0.3804
EU+WNG EU + D1 + D2 + WNG 0.1235 3.7652 0.3911 0.2801 0.3606
EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1618 4.1415 0.4234 0.3029 0.4130
test
EUWNG WNG 0.1123 3.6777 0.3829 0.2771 0.3595
EUWNG EU + D1 + D2 + WNG 0.1183 3.5819 0.3737 0.2772 0.3518
EUWNG EU + D1 + D2 + WNG (TUNED) 0.1290 3.6478 0.3920 0.2810 0.3885
EU+WNG WNG 0.1227 3.8970 0.3997 0.2872 0.3723
EU+WNG EU + D1 + D2 + WNG 0.1199 3.7353 0.3846 0.2812 0.3583
EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1400 3.8930 0.4084 0.2907 0.3963
Table 4: MT Results on development and test sets for the two strategies for combining translations models.
 0.06
 0.07
 0.08
 0.09
 0.1
 0.11
 0.12
 0.13
 0.14
 0  500  1000  1500  2000  2500  3000  3500  4000  4500
BL
EU
.n
4
# glosses
baseline
TM + LM impact
TM impact
LM impact
Figure 1: Impact of the size of in-domain data on
MT system performance for the test set.
to avoid the possible effect of over-fitting we focus
on the behavior on the test set. Note that the opti-
mization of parameters is performed at each point
in the x-axis using only the development set.
A significant initial gain of around 0.3 BLEU
points is observed when adding as few as 100
glosses. In all cases, it is not until around 1,000
glosses are added that the ?EU+WNG? system sta-
bilizes. After that, results continue improving as
more in-domain data are added. We observe a
very significant increase by just adding around
3,000 glosses. Another interesting observation is
the boosting effect of the combination of TM and
LM specialized models. While individual curves
for TM and LM tend to be more stable with more
than 4,000 added examples, the TM+LM curve
still shows a steep increase in this last part.
5 Error Analysis
We inspected results at the sentence level based on
the GTM F-measure (e = 1) for the best config-
uration of the ?EU+WNG? system. 196 sentences
out from the 500 obtain an F-measure equal to or
higher than 0.5 on the development set (181 sen-
tences in the case of test set), whereas only 54
sentences obtain a score lower than 0.1. These
numbers give a first idea of the relative useful-
ness of our system. Table 5 shows some trans-
lation cases selected for discussion. For instance,
Case 1 is a clear example of unfair low score. The
problem is that source and reference are not par-
allel but ?quasi-parallel?. Both glosses define the
same concept but in a different way. Thus, metrics
based on rewarding lexical similarities are not well
suited for these cases. Cases 2, 3, 4 are examples
of proper cooperation between ?EU? and ?WNG?
models. ?EU? models provides recall, for instance
by suggesting translation candidates for ?bombs?
or ?price below?. ?WNG? models provide preci-
sion, for instance by choosing the right translation
for ?an attack? or ?the act of?.
We also compared the ?EU+WNG? system to
SYSTRAN. In the case of SYSTRAN 167 sen-
tences obtain a score equal to or higher than 0.5
whereas 79 sentences obtain a score lower than
0.1. These numbers are slightly under the per-
formance of the ?EU+WNG? system. Table 6
shows some translation cases selected for discus-
sion. Case 1 is again an example of both sys-
tems obtaining very low scores because of ?quasi-
parallelism?. Cases 2 and 3 are examples of SYS-
TRAN outperforming our system. In case 2 SYS-
TRAN exhibits higher precision in the translation
of ?accompanying? and ?illustration?, whereas in
case 3 it shows higher recall by suggesting ap-
propriate translation candidates for ?fibers?, ?silk-
worm?, ?cocoon?, ?threads?, and ?knitting?. Cases
292
FE FW FEW Source OutE OutW OutEW Reference
0.0000 0.1333 0.1111 of the younger de acuerdo con de la younger de acuerdo con que tiene
of two boys el ma?s joven de dos boys el ma?s joven de menos edad
with the same de dos boys tiene el mismo dos muchachos
family name con la misma nombre familia tiene el mismo
familia fama nombre familia
0.2857 0.2500 0.5000 an attack atacar por ataque ataque ataque con
by dropping cayendo realizado por realizado por bombas
bombs bombas dropping bombs cayendo bombas
0.1250 0.7059 0.5882 the act of acto de la accio?n y efecto accio?n y efecto accio?n y efecto
informing by informacio?n de informing de informaba de informar
verbal report por verbales por verbal por verbales con una expli-
ponencia explicacio?n explicacio?n cacio?n verbal
0.5000 0.0000 0.5000 a price below un precio por una price un precio por precio que esta?
the standard debajo de la below nu?mbero debajo de la por debajo de
price norma precio esta?ndar price esta?ndar precio lo normal
Table 5: MT output analysis of the ?EU?, ?WNG? and ?EU+WNG? systems. FE , FW and FEW refer to the GTM (e = 1)
F-measure attained by the ?EU?, ?WNG? and ?EU+WNG? systems, respectively. ?Source?, OutE , OutW and OutEW refer to
the input and the output of the systems. ?Reference? corresponds to the expected output.
4 and 5 are examples where our system outper-
forms SYSTRAN. In case 4, our system provides
higher recall by suggesting an adequate transla-
tion for ?top of something?. In case 5, our system
shows higher precision by selecting a better trans-
lation for ?rate?. However, we observed that SYS-
TRAN tends in most cases to construct sentences
exhibiting a higher degree of grammaticality.
6 Conclusions
In this work, we have enriched every synset in
Spanish WordNet with a preliminary gloss, which
can be later updated in a lighter process of manual
revision. Though imperfect, this material consti-
tutes a very valuable resource. For instance, Word-
Net glosses have been used in the past to generate
sense tagged corpora (Mihalcea and Moldovan,
1999), or as external knowledge for Question An-
swering systems (Hovy et al, 2001).
We have also shown the importance of using a
small set of in-domain parallel sentences in or-
der to adapt a phrase-based general SMT sys-
tem to a new domain. In particular, we have
worked on specialized language and translation
models and on their combination with general
models in order to achieve a proper balance be-
tween precision (specialized in-domain models)
and recall (general out-of-domain models). A sub-
stantial increase is consistently obtained according
to standard MT evaluation metrics, which has been
shown to be statistically significant in the case
of BLEU. Broadly speaking, we have shown that
around 3,000 glosses (very short sentence frag-
ments) suffice in this domain to obtain a signifi-
cant improvement. Besides, all the methods used
are language independent, assumed the availabil-
ity of the required in-domain additional resources.
In the future we plan to work on domain inde-
pendent translation models built from WordNet it-
self. We may use the WordNet topology to pro-
vide translation candidates weighted according to
the given domain. Moreover, we are experiment-
ing the applicability of current Word Sense Dis-
ambiguation (WSD) technology to MT. We could
favor those translation candidates showing a closer
semantic relation to the source. We believe that
coarse-grained is sufficient for the purpose of MT.
Acknowledgements
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02) and the Spanish Ministry of
Education and Science (TRANGRAM, TIN2004-
07925-C03-02). Our research group, TALP Re-
search Center, is recognized as a Quality Research
Group (2001 SGR 00254) by DURSI, the Re-
search Department of the Catalan Government.
Authors are grateful to Patrik Lambert for pro-
viding us with the implementation of the Simplex
Method, and specially to German Rigau for moti-
vating in its origin all this work.
References
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
293
FEW FS Source OutEW OutS Reference
0.0000 0.0000 a newspaper that perio?dico que un perio?dico publicacio?n
is published se publica diario que se publica perio?dica
every day cada d??a monotema?tica
0.1818 0.8333 brief description breve descripcio?n breve descripcio?n pequen?a descripcio?n
accompanying an adjuntas un aclaracio?n que acompan?a que acompan?a
illustration una ilustracio?n una ilustracio?n
0.1905 0.7333 fibers from silkworm fibers desde silkworm las fibras de los fibras de los capullos
cocoons provide cocoons proporcionan capullos del gusano de gusano de seda
threads for knitting threads para knitting de seda proporcionan que proporcionan
los hilos de rosca hilos para tejer
para hacer punto
1.0000 0.0000 the top of something parte superior de la tapa algo parte superior de
una cosa una cosa
0.6667 0.3077 a rate at which un ritmo al que una tarifa en la ritmo al que
something happens sucede algo cual algo sucede sucede una cosa
Table 6: MT output analysis of the ?EU+WNG? and SYSTRAN systems. FEW and FS refer to the GTM (e = 1) F-measure
attained by the ?EU+WNG? and SYSTRAN systems, respectively. ?Source?, OutEW and OutS refer to the input and the output
of the systems. ?Reference? corresponds to the expected output.
Vossen. 2004. The MEANING Multilingual Cen-
tral Repository. In Proceedings of 2nd GWC.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, Robert L.
Mercer, , and Paul S. Roossin. 1988. A statistical
approach to language translation. In Proceedings of
COLING?88.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternation Conference on Human Language Technol-
ogy, pages 138?145.
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin.
2001. The Use of External Knowledge of Factoid
QA. In Proceedings of TREC.
Philipp Koehn. 2003. Europarl: A Multilin-
gual Corpus for Evaluation of Machine Transla-
tion. Technical report, http://people.csail.mit.edu/-
people/koehn/publications/europarl/.
Philipp Koehn. 2004a. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA?04.
Philipp Koehn. 2004b. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP?04.
Mar??a Antonia Mart??, editor. 1996. Gran dic-
cionario de la Lengua Espan?ola. Larousse Planeta,
Barcelona.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL?03.
Rada Mihalcea and Dan Moldovan. 1999. An Au-
tomatic Method for Generating Sense Tagged Cor-
pora. In Proceedings of AAAI.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, IBM Research Re-
port, RC22176. Technical report, IBM T.J. Watson
Research Center.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of IC-
SLP?02.
Stephan Vogel and Alicia Tribble. 2002. Improv-
ing Statistical Machine Translation for a Speech-to-
Speech Translation Task. In Proceedings of ICSLP-
2002 Workshop on Speech-to-Speech Translation.
Vox, editor. 1990. Diccionario Actual de la Lengua
Espan?ola. Bibliograf, Barcelona.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing. Cambridge
University Press.
294
Proceedings of ACL-08: HLT, pages 550?558,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robustness and Generalization of Role Sets: PropBank vs. VerbNet
Ben?at Zapirain and Eneko Agirre
IXA NLP Group
University of the Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical University of Catalonia
lluism@lsi.upc.edu
Abstract
This paper presents an empirical study on the
robustness and generalization of two alterna-
tive role sets for semantic role labeling: Prop-
Bank numbered roles and VerbNet thematic
roles. By testing a state?of?the?art SRL sys-
tem with the two alternative role annotations,
we show that the PropBank role set is more
robust to the lack of verb?specific semantic
information and generalizes better to infre-
quent and unseen predicates. Keeping in mind
that thematic roles are better for application
needs, we also tested the best way to generate
VerbNet annotation. We conclude that tagging
first PropBank roles and mapping into Verb-
Net roles is as effective as training and tagging
directly on VerbNet, and more robust for do-
main shifts.
1 Introduction
Semantic Role Labeling is the problem of analyzing
clause predicates in open text by identifying argu-
ments and tagging them with semantic labels indi-
cating the role they play with respect to the verb.
Such sentence?level semantic analysis allows to de-
termine ?who? did ?what? to ?whom?, ?when? and
?where?, and, thus, characterize the participants and
properties of the events established by the predi-
cates. This kind of semantic analysis is very inter-
esting for a broad spectrum of NLP applications (in-
formation extraction, summarization, question an-
swering, machine translation, etc.), since it opens
the door to exploit the semantic relations among lin-
guistic constituents.
The properties of the semantically annotated cor-
pora available have conditioned the type of research
and systems that have been developed so far. Prop-
Bank (Palmer et al, 2005) is the most widely used
corpus for training SRL systems, probably because
it contains running text from the Penn Treebank cor-
pus with annotations on all verbal predicates. Also,
a few evaluation exercises on SRL have been con-
ducted on this corpus in the CoNLL-2004 and 2005
conferences. However, a serious criticisms to the
PropBank corpus refers to the role set it uses, which
consists of a set of numbered core arguments, whose
semantic translation is verb-dependent. While Arg0
and Arg1 are intended to indicate the general roles
of Agent and Theme, other argument numbers do
not generalize across verbs and do not correspond
to general semantic roles. This fact might compro-
mise generalization and portability of SRL systems,
especially when the training corpus is small.
More recently, a mapping from PropBank num-
bered arguments into VerbNet thematic roles has
been developed and a version of the PropBank cor-
pus with thematic roles has been released (Loper et
al., 2007). Thematic roles represent a compact set of
verb-independent general roles widely used in lin-
guistic theory (e.g., Agent, Theme, Patient, Recipi-
ent, Cause, etc.). We foresee two advantages of us-
ing such thematic roles. On the one hand, statisti-
cal SRL systems trained from them could generalize
better and, therefore, be more robust and portable,
as suggested in (Yi et al, 2007). On the other hand,
roles in a paradigm like VerbNet would allow for in-
ferences over the assigned roles, which is only pos-
sible in a more limited way with PropBank.
In a previous paper (Zapirain et al, 2008), we pre-
sented a first comparison between the two previous
role sets on the SemEval-2007 Task 17 corpus (Prad-
han et al, 2007). The SemEval-2007 corpus only
550
comprised examples about 50 different verbs. The
results of that paper were, thus, considered prelim-
inary, as they could depend on the small amount of
data (both in training data and number of verbs) or
the specific set of verbs being used. Now, we ex-
tend those experiments to the entire PropBank cor-
pus, and we include two extra experiments on do-
main shifts (using the Brown corpus as test set) and
on grouping VerbNet labels. More concretely, this
paper explores two aspects of the problem. First,
having in mind the claim that general thematic roles
should be more robust to changing domains and
unseen predicates, we study the performance of a
state-of-the-art SRL system trained on either codi-
fication of roles and some specific settings, i.e. in-
cluding/excluding verb-specific information, label-
ing unseen verb predicates, or domain shifts. Sec-
ond, assuming that application scenarios would pre-
fer dealing with general thematic role labels, we ex-
plore the best way to label a text with thematic roles,
namely, by training directly on VerbNet roles or by
using the PropBank SRL system and perform a pos-
terior mapping into thematic roles.
The results confirm our preliminary findings (Za-
pirain et al, 2008). We observe that the PropBank
roles are more robust in all tested experimental con-
ditions, i.e., the performance decrease is more se-
vere for VerbNet. Besides, tagging first PropBank
roles and then mapping into VerbNet roles is as ef-
fective as training and tagging directly on VerbNet,
and more robust for domain shifts.
The rest of the paper is organized as follows: Sec-
tion 2 contains some background on PropBank and
VerbNet role sets. Section 3 presents the experimen-
tal setting and the base SRL system used for the role
set comparisons. In Section 4 the main compara-
tive experiments on robustness are described. Sec-
tion 5 is devoted to analyze the posterior mapping of
PropBank outputs into VerbNet thematic roles, and
includes results on domain?shift experiments using
Brown as test set. Finally, Sections 6 and 7 contain
a discussion of the results.
2 Corpora and Semantic Role Sets
The PropBank corpus is the result of adding a se-
mantic layer to the syntactic structures of Penn Tree-
bank II (Palmer et al, 2005). Specifically, it pro-
vides information about predicate-argument struc-
tures to all verbal predicates of the Wall Street Jour-
nal section of the treebank. The role set is theory?
neutral and consists of a set of numbered core ar-
guments (Arg0, Arg1, ..., Arg5). Each verb has a
frameset listing its allowed role labels and mapping
each numbered role to an English-language descrip-
tion of its semantics.
Different senses for a polysemous verb have dif-
ferent framesets, but the argument labels are seman-
tically consistent in all syntactic alternations of the
same verb?sense. For instance in ?Kevin broke [the
window]Arg1 ? and in ?[The door]Arg1 broke into a
million pieces?, for the verb broke.01, both Arg1 ar-
guments have the same semantic meaning, that is
?broken entity?. Nevertheless, argument labels are
not necessarily consistent across different verbs (or
verb senses). For instance, the same Arg2 label is
used to identify the Destination argument of a propo-
sition governed by the verb send and the Beneficiary
argument of the verb compose. This fact might com-
promise generalization of systems trained on Prop-
Bank, which might be focusing too much on verb?
specific knowledge. It is worth noting that the two
most frequent arguments, Arg0 and Arg1, are in-
tended to indicate the general roles of Agent and
Theme and are usually consistent across different
verbs. However, this correspondence is not total.
According to the study by (Yi et al, 2007), Arg0
corresponds to Agent 85.4% of the time, but also
to Experiencer (7.2%), Theme (2.1%), and Cause
(1.9%). Similarly, Arg1 corresponds to Theme in
47.0% of the occurrences but also to Topic (23.0%),
Patient (10.8%), and Product (2.9%), among others.
Contrary to core arguments, adjuncts (Temporal and
Location markers, etc.) are annotated with a closed
set of general and verb-independent labels.
VerbNet (Kipper et al, 2000) is a computational
verb lexicon in which verbs are organized hier-
archically into classes depending on their syntac-
tic/semantic linking behavior. The classes are based
on Levin?s verb classes (Levin, 1993) and each con-
tains a list of member verbs and a correspondence
between the shared syntactic frames and the se-
mantic information, such as thematic roles and se-
lectional constraints. There are 23 thematic roles
(Agent, Patient, Theme, Experiencer, Source, Ben-
eficiary, Instrument, etc.) which, unlike the Prop-
551
Bank numbered arguments, are considered as gen-
eral verb-independent roles.
This level of abstraction makes them, in princi-
ple, better suited (compared to PropBank numbered
arguments) for being directly exploited by general
NLP applications. But, VerbNet by itself is not an
appropriate resource to train SRL systems. As op-
posed to PropBank, the number of tagged examples
is far more limited in VerbNet. Fortunately, in the
last years a twofold effort has been made in order
to generate a large corpus fully annotated with the-
matic roles. Firstly, the SemLink1 resource (Loper
et al, 2007) established a mapping between Prop-
Bank framesets and VerbNet thematic roles. Sec-
ondly, the SemLink mapping was applied to a repre-
sentative portion of the PropBank corpus and man-
ually disambiguated (Loper et al, 2007). The re-
sulting corpus is currently available for the research
community and makes possible comparative studies
between role sets.
3 Experimental Setting
3.1 Datasets
The data used in this work is the benchmark corpus
provided by the SRL shared task of CoNLL-2005
(Carreras and Ma`rquez, 2005). The dataset, of over
1 million tokens, comprises PropBank sections 02?
21 for training, and sections 24 and 23 for develop-
ment and test, respectively. From the input informa-
tion, we used part of speech tags and full parse trees
(generated using Charniak?s parser) and discarded
named entities. Also, we used the publicly avail-
able SemLink mapping from PropBank into Verb-
Net roles (Loper et al, 2007) to generate a replicate
of the CoNLL-2005 corpus containing also the Verb-
Net annotation of roles.
Unfortunately, SemLink version 1.0 does not
cover all propositions and arguments in the Prop-
Bank corpus. In order to have an homogeneous cor-
pus and not to bias experimental evaluation, we de-
cided to discard all incomplete examples and keep
only those propositions that were 100% mapped into
VerbNet roles. The resulting corpus contains 56% of
the original propositions, that is, over 50,000 propo-
sitions in the training set. This subcorpus is much
larger than the SemEval-2007 Task 17 dataset used
1http://verbs.colorado.edu/semlink/
in our previous experimental work (Zapirain et al,
2008). The difference is especially noticeable in
the diversity of predicates represented. In this case,
there are 1,709 different verbs (1,505 lemmas) com-
pared to the 50 verbs of the SemEval corpus. We
believe that the size and richness of this corpus is
enough to test and extract reliable conclusions on
the robustness and generalization across verbs of the
role sets under study.
In order to study the behavior of both role sets
in out?of?domain data, we made use of the Prop-
Banked Brown corpus (Marcus et al, 1994) for test-
ing, as it is also mapped into VerbNet thematic roles
in the SemLink resource. Again, we discarded those
propositions that were not entirely mapped into the-
matic roles (45%).
3.2 SRL System
Our basic Semantic Role Labeling system represents
the tagging problem as a Maximum Entropy Markov
Model (MEMM). The system uses full syntactic
information to select a sequence of constituents
from the input text and tags these tokens with Be-
gin/Inside/Outside (BIO) labels, using state-of-the-
art classifiers and features. The system achieves very
good performance in the CoNLL-2005 shared task
dataset and in the SRL subtask of the SemEval-2007
English lexical sample task (Zapirain et al, 2007).
Check this paper for a complete description of the
system.
When searching for the most likely state se-
quence, the following constraints are observed2:
1. No duplicate argument classes for Arg0?Arg5
PropBank (or VerbNet) roles are allowed.
2. If there is a R-X argument (reference), then
there has to be a X argument before (referent).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token.
5. Given a predicate, only the arguments de-
scribed in its PropBank (or VerbNet) lexical en-
try (i.e., the verbal frameset) are allowed.
2Note that some of the constraints are dependent of the role
set used, i.e., PropBank or VerbNet
552
Regarding the last constraint, the lexical entries
of the verbs were constructed from the training data
itself. For instance, the verb build appears with
four different PropBank core roles (Arg0?3) and five
VerbNet roles (Product, Material, Asset, Attribute,
Theme), which are the only ones allowed for that
verb at test time. Note that in the cases where the
verb sense was known we could constraint the pos-
sible arguments to those that appear in the lexical en-
try of that sense, as opposed of using the arguments
that appear in all senses.
4 On the Generalization of Role Sets
We first seek a basic reference of the comparative
performance of the classifier on each role set. We
devised two settings based on our dataset. In the
first setting (?SemEval?) we use all the available in-
formation provided in the corpus, including the verb
senses in PropBank and VerbNet. This information
was available both in the training and test, and was
thus used as an additional feature by the classifier
and to constrain further the possible arguments when
searching for the most probable Viterbi path. We call
this setting ?SemEval? because the SemEval-2007
competition (Pradhan et al, 2007) was performed
using this configuration.
Being aware that, in a real scenario, the sense in-
formation will not be available, we devised the sec-
ond setting (?CoNLL?), where the hand-annotated
verb sense information was discarded. This is the
setting used in the CoNLL 2005 shared task (Car-
reras and Ma`rquez, 2005).
The results for the first setting are shown in the
?SemEval setting? rows of Table 1. The correct,
excess, missed, precision, recall and F1 measures
are reported, as customary. The significance inter-
vals for F1 are also reported. They have been ob-
tained with bootstrap resampling (Noreen, 1989).
F1 scores outside of these intervals are assumed to
be significantly different from the related F1 score
(p < 0.05). The results for PropBank are slightly
better, which is reasonable, as the number of labels
that the classifier has to learn in the case of VerbNet
should make the task harder. In fact, given the small
difference, one could think that VerbNet labels, be-
ing more numerous, are easier to learn, perhaps be-
cause they are more consistent across verbs.
In the second setting (?CoNLL setting? row in
the same table) the PropBank classifier degrades
slightly, but the difference is not statistically signif-
icant. On the contrary, the drop of 1.6 points for
VerbNet is significant, and shows greater sensitivity
to the absence of the sense information for verbs.
One possible reason could be that the VerbNet clas-
sifier is more dependant on the argument filter (i.e.,
the 5th constraint in Section 3.2, which only allows
roles that occur in the verbal frameset) used in the
Viterbi search, and lacking the sense information
makes the filter less useful. In fact, we have attested
that the 5th constrain discard more than 60% of the
possible candidates for VerbNet, making the task of
the classifier easier.
In order to test this hypothesis, we run the CoNLL
setting with the 5th constraint disabled (that is, al-
lowing any argument). The results in the ?CoNLL
setting (no 5th)? rows of Table 1 show that the drop
for PropBank is negligible and not significant, while
the drop for VerbNet is more important, and statisti-
cally significant.
Another view of the data is obtained if we com-
pute the F1 scores for core arguments and adjuncts
separately (last two columns in Table 1). The per-
formance drop for PropBank in the first three rows
is equally distributed on both core arguments and ad-
juncts. On the contrary, the drop for VerbNet roles
is more acute in core arguments (3.7 points), while
adjuncts with the 5th constraint disabled get results
close to the SemEval setting. These results confirm
that the information in the verbal frameset is more
important in VerbNet than in PropBank, as only core
arguments are constrained in the verbal framesets.
The explanation could stem from the fact that cur-
rent SRL systems rely more on syntactic information
than pure semantic knowledge. While PropBank ar-
guments Arg0?5 are easier to distinguish on syntac-
tic grounds alone, it seems quite difficult to distin-
guish among roles like Theme and Topic unless we
have access to the specific verbal frameset. This cor-
responds nicely with the performance drop for Verb-
Net when there is less information about the verb in
the algorithm (i.e., sense or frameset).
We further analyzed the results by looking at each
of the individual core arguments and adjuncts. Ta-
ble 2 shows these results on the CoNLL setting. The
performance for the most frequent roles is similar
553
PropBank
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 6,022 1,378 1,722 81.38 77.76 79.53 ?0.9 82.25 72.48
CoNLL setting 5,977 1,424 1,767 80.76 77.18 78.93 ?0.9 81.64 71.90
CoNLL setting (no 5th) 5,972 1,434 1,772 80.64 77.12 78.84 ?0.9 81.49 71.50
No verbal features 5,557 1,828 2,187 75.25 71.76 73.46 ?1.0 74.87 70.11
Unseen verbs 267 89 106 75.00 71.58 73.25 ?4.0 76.21 64.92
VerbNet
Experiment correct excess missed precision recall F1 F1 core F1 adj.
SemEval setting 5,927 1,409 1,817 80.79 76.54 78.61 ?0.9 81.28 71.83
CoNLL setting 5,816 1,548 1,928 78.98 75.10 76.99 ?0.9 79.44 70.20
CoNLL setting (no 5th) 5,746 1,669 1,998 77.49 74.20 75.81 ?0.9 77.60 71.67
No verbal features 4,679 2,724 3,065 63.20 60.42 61.78 ?0.9 59.19 69.95
Unseen verbs 207 136 166 60.35 55.50 57.82 ?4.3 55.04 63.41
Table 1: Basic results using PropBank (top) and VerbNet (bottom) role sets on different settings.
for both. Arg0 gets 88.49, while Agent and Expe-
riencer get 87.31 and 87.76 respectively. Arg2 gets
79.91, but there is more variation on Theme, Topic
and Patient (which get 75.46, 85.70 and 78.64 re-
spectively).
Finally, we grouped the results according to the
frequency of the verbs in the training data. Table 3
shows that both PropBank and VerbNet get decreas-
ing results for less frequent verbs. PropBank gets
better results in all frequency ranges, except for the
most frequent, which contains a single verb (say).
Overall, the results on this section point out at the
weaknesses of the VerbNet role set regarding robust-
ness and generalization. The next sections examine
further its behavior.
4.1 Generalization to Unseen Predicates
In principle, the PropBank core roles (Arg0?4) get
a different interpretation depending of the verb, that
is, the meaning of each of the roles is described sepa-
rately for each verb in the PropBank framesets. Still,
the annotation criteria used with PropBank tried to
make the two main roles (Arg0 and Arg1, which ac-
count for most of the occurrences) consistent across
verbs. On the contrary, in VerbNet al roles are com-
pletely independent of the verb, in the sense that the
interpretation of the role does not vary across verbs.
But, at the same time, each verbal entry lists the pos-
sible roles it accepts, and the combinations allowed.
This experiment tests the sensitivity of the two ap-
proaches when the SRL system encounters a verb
which does not occur in the training data. In prin-
ciple, we would expect the VerbNet semantic la-
bels, which are more independent across verbs, to be
more robust at tagging new predicates. It is worth
noting that this is a realistic scenario, even for the
verb-specific PropBank labels. Predicates which do
not occur in the training data, but do have a Prop-
Bank lexicon entry, could appear quite often in the
text to be analyzed.
For this experiment, we artificially created a test
set for unseen verbs. We chose 50 verbs at random,
and split them into 40 verbs for training and 10 for
testing (yielding 13,146 occurrences for training and
2,723 occurrences for testing; see Table 4).
The results obtained after training and testing the
classifier are shown in the last rows in Table 1. Note
that they are not directly comparable to the other re-
sults mentioned so far, as the train and test sets are
smaller. Figures indicate that the performance of the
PropBank argument classifier is considerably higher
than the VerbNet classifier, with a ?15 point gap.
This experiment shows that lacking any informa-
tion about verbal head, the classifier has a hard time
to distinguish among VerbNet roles. In order to con-
firm this, we performed the following experiment.
4.2 Sensitivity to Verb-dependent Features
In this experiment we want to test the sensitivity of
the role sets when the classifier does not have any in-
formation of the verb predicate. We removed from
the training and testing data all the features which
make any reference to the verb, including, among
others: the surface form, lemma and POS of the
verb, and all the combined features that include the
verb form (please, refer to (Zapirain et al, 2007) for
a complete description of the feature set).
The results are shown in the ?No verbal features?
554
CoNLL setting No verb features
PBank VNet PBank VNet
corr. F1 corr. F1 F1 F1
Overall 5977 78.93 5816 76.99 73.46 61.78
Arg0 1919 88.49 84.02
Arg1 2240 79.81 73.29
Arg2 303 65.44 48.58
Arg3 10 52.63 14.29
Actor1 44 85.44 0.00
Actor2 10 71.43 25.00
Agent 1603 87.31 77.21
Attribut. 25 71.43 50.79
Cause 51 62.20 5.61
Experien. 215 87.76 86.69
Location 31 64.58 25.00
Patient1 38 67.86 5.71
Patient 208 78.64 25.06
Patient2 21 67.74 43.33
Predicate 83 62.88 28.69
Product 44 61.97 2.44
Recipient 85 79.81 62.73
Source 29 60.42 30.95
Stimulus 39 63.93 13.70
Theme 1021 75.46 52.14
Theme1 20 57.14 4.44
Theme2 21 70.00 23.53
Topic 683 85.70 73.58
ADV 132 53.44 129 52.12 52.67 53.31
CAU 13 53.06 13 52.00 53.06 45.83
DIR 22 53.01 27 56.84 40.00 46.34
DIS 133 77.78 137 79.42 77.25 78.34
LOC 126 61.76 126 61.02 59.56 57.34
MNR 109 58.29 111 54.81 52.99 51.49
MOD 249 96.14 248 95.75 96.12 95.57
NEG 124 98.41 124 98.80 98.41 98.01
PNC 26 44.07 29 44.62 38.33 41.79
TMP 453 75.00 450 73.71 73.06 73.89
Table 2: Detailed results on the CoNLL setting. Refer-
ence arguments and verbs have been omitted for brevity,
as well as those with less than 10 occ. The last two
columns refer to the results on the CoNLL setting with
no verb features.
Freq. PBank VNet Freq. PBank VNet
0-50 74,21 71,11 500-900 77,97 75,77
50-100 74,79 71,83 > 900 91,83 92,23
100-500 77,16 75,41
Table 3: F1 results split according to the frequency of the
verb in the training data.
Train affect, announce, ask, attempt, avoid, believe, build, care,
cause, claim, complain, complete, contribute, describe,
disclose, enjoy, estimate, examine, exist, explain, express,
feel, fix, grant, hope, join, maintain, negotiate, occur,
prepare, promise, propose, purchase, recall, receive,
regard, remember, remove, replace, say
Test allow, approve, buy, find, improve, kill, produce, prove,
report, rush
Table 4: Verbs used in the unseen verb experiment
rows of Table 1. The performance drops more than
5 points in PropBank, but the drop for VerbNet is
dramatic, with more than 15 points.
A closer look at the detailed role-by-role perfor-
mances can be done if we compare the F1 rows in the
CoNLL setting and in the ?no verb features? setting
in Table 2. Those results show that both Arg0 and
Arg1 are quite robust to the lack of target verb in-
formation, while Arg2 and Arg3 get more affected.
Given the relatively low number of Arg2 and Arg3
arguments, their performance drop does not affect
so much the overall PropBank performance. In the
case of VerbNet, the picture is very different. Focus-
ing on the most frequent roles first, while the perfor-
mance drop for Experiencer, Agent and Topic is of
1, 10 and 12 points respectively, the other roles get
very heavy losses (e.g. Theme and Patient drop 23
and 50 points), and the rest of roles are barely found.
It is worth noting that the adjunct labels get very
similar performances in both PropBank and Verb-
Net cases. In fact, Table 1 in the last two rows shows
very clearly that the performance drop is caused by
the core arguments.
The better robustness of the PropBank roles can
be explained by the fact that, when creating Prop-
Bank, the human PropBank annotators tried to be
consistent when tagging Arg0 and Arg1 across
verbs. We also think that both Arg0 and Arg1 can
be detected quite well relying on unlexicalized syn-
tactic features only, that is, not knowing which are
the verbal and nominal heads. On the other hand,
distinguishing between Arg2?4 is more dependant
on the subcategorization frame of the verb, and thus
more sensitive to the lack of verbal information.
In the case of VerbNet, the more fine-grained dis-
tinction among roles seems to depend more on the
meaning of the predicate. For instance, distinguish-
ing between Agent?Experiencer, or Theme?Topic?
Patient. The lack of the verbal head makes it much
more difficult to distinguish among those roles. The
same phenomena can be observed among the roles
not typically realized as Subject or Object such as
Recipient, Source, Product, or Stimulus.
5 Mapping into VerbNet Thematic Roles
As mentioned in the introduction, the interpretation
of PropBank roles depends on the verb, and that
555
Test on WSJ all core adj.
PropBank to VerbNet (hand) 79.17 ?0.9 81.77 72.50
VerbNet (SemEval setting) 78.61 ?0.9 81.28 71.84
PropBank to VerbNet (MF) 77.15 ?0.9 79.09 71.90
VerbNet (CoNLL setting) 76.99 ?0.9 79.44 70.88
Test on Brown
PropBank to VerbNet (MF) 64.79 ?1.0 68.93 55.94
VerbNet (CoNLL setting) 62.87 ?1.0 67.07 54.69
Table 5: Results on VerbNet roles using two different
strategies. Topmost 4 rows for the usual test set (WSJ),
and the 2 rows below for the Brown test set.
makes them less suitable for NLP applications. On
the other hand, VerbNet roles have a direct inter-
pretation. In this section, we test the performance
of two different approaches to tag input sentences
with VerbNet roles: (1) train on corpora tagged with
VerbNet, and tag the input directly; (2) train on cor-
pora tagged with PropBank, tag the input with Prop-
Bank roles, and use a PropBank to VerbNet mapping
to output VerbNet roles.
The results for the first approach are already avail-
able (cf. Table 1). For the second approach, we
just need to map PropBank roles into VerbNet roles
using SemLink (Loper et al, 2007). We devised
two experiments. In the first one we use the hand-
annotated verb class in the test set. For each predi-
cate we translate PropBank roles into VerbNet roles
making use of the SemLink mapping information
corresponding to that verb lemma and its verbal
class.
For instance, consider an occurrence of allow in a
test sentence. If the occurrence has been manually
annotated with the VerbNet class 29.5, we can use
the following entry in SemLink to add the VerbNet
role Predicate to the argument labeled with Arg1,
and Agent to the Arg0 argument.
<predicate lemma="allow">
<argmap pb-roleset="allow.01" vn-class="29.5">
<role pb-arg="1" vn-theta="Predicate" />
<role pb-arg="0" vn-theta="Agent" />
</argmap>
</predicate>
The results obtained using the hand-annotated
VerbNet classes (and the SemEval setting for Prop-
Bank), are shown in the first row of Table 5. If we
compare these results to those obtained by VerbNet
in the SemEval setting (second row of Table 5), they
are 0.5 points better, but the difference is not statis-
tically significant.
experiment corr. F1
Grouped (CoNLL Setting) 5,951 78.11?0.9
PropBank to VerbNet to Grouped 5,970 78.21?0.9
Table 6: Results for VerbNet grouping experiments.
In a second experiment, we discarded the sense
annotations from the dataset, and tried to predict the
VerbNet class of the target verb using the most fre-
quent class for the verb in the training data. Sur-
prisingly, the accuracy of choosing the most fre-
quent class is 97%. In the case of allow the most
frequent class is 29.5, so we would use the same
SemLink entry as above. The third row in Table 5
shows the results using the most frequent VerbNet
class (and the CoNLL setting for PropBank). The
performance drop compared to the use of the hand-
annotated VerbNet class is of 2 points and statisti-
cally significant, and 0.2 points above the results ob-
tained using VerbNet directly on the same conditions
(fourth row of the same Table).
The last two rows in table 5 show the results when
testing on the the Brown Corpus. In this case, the
difference is larger, 1.9 points, and statistically sig-
nificant in favor of the mapping approach. These
results show that VerbNet roles are less robust to
domain shifts. The performance drop when mov-
ing to an out?of?domain corpus is consistent with
previously published results (Carreras and Ma`rquez,
2005).
5.1 Grouping experiments
VerbNet roles are more numerous than PropBank
roles, and that, in itself, could cause a drop in per-
formance. Motivated by the results in (Yi et al,
2007), we grouped the 23 VerbNet roles in 7 coarser
role groups. Note that their groupings are focused
on the roles which map to PropBank Arg2. In our
case we are interested in a more general grouping
which covers all VerbNet roles, so we added two
additional groups (Agent-Experiencer and Theme-
Topic-Patient). We re-tagged the roles in the datasets
with those groups, and then trained and tested our
SRL system on those grouped labels. The results
are shown in the first row of Table 6. In order to
judge if our groupings are easier to learn, we can
see that he performance gain with respect to the un-
grouped roles (fourth row of Table 5) is small (76.99
556
vs. 78.11) but significant. But if we compare them
to the results of the PropBank to VerbNet mapping,
where we simply substitute the fine-grained roles by
their corresponding groups, we see that they still lag
behind (second row in Table 6).
Although one could argue that better motivated
groupings could be proposed, these results indicate
that the larger number of VerbNet roles does not ex-
plain in itself the performance difference when com-
pared to PropBank.
6 Related Work
As far as we know, there are only two other works
performing comparisons of alternative role sets on
a common test data. Gildea and Jurafsky (2002)
mapped FrameNet frame elements into a set of ab-
stract thematic roles (i.e., more general roles such as
Agent, Theme, Location), and concluded that their
system could use these thematic roles without degra-
dation in performance.
(Yi et al, 2007) is a closely related work. They
also compare PropBank and VerbNet role sets, but
they focus on the performance of Arg2. They show
that splitting Arg2 instances into subgroups based on
VerbNet thematic roles improves the performance of
the PropBank-based classifier. Their claim is that
since VerbNet uses argument labels that are more
consistent across verbs, they would provide more
consistent training instances which would general-
ize better, especially to new verbs and genres. In fact
they get small improvements in PropBank (WSJ)
and a large improvement when testing on Brown.
An important remark is that Yi et al use a com-
bination of grouped VerbNet roles (for Arg2) and
PropBank roles (for the rest of arguments). In con-
trast, our study compares both role sets as they stand,
without modifications or mixing. Another difference
is that they compare the systems based on the Prop-
Bank roles ?by mapping the output VerbNet labels
back to PropBank Arg2? while in our case we de-
cided to do just the contrary (i.e., mapping PropBank
output into VerbNet labels and compare there). As
we already said, we think that VerbNet?based labels
can be more useful for NLP applications, so our tar-
get is to have a SRL system that provides VerbNet
annotations. While not in direct contradiction, both
studies show different angles of the complex relation
between the two role sets.
7 Conclusion and Future work
In this paper we have presented a study of the per-
formance of a state-of-the-art SRL system trained
on two alternative codifications of roles (PropBank
and VerbNet) and some particular settings, e.g., in-
cluding/excluding verb?specific information in fea-
tures, labeling of infrequent and unseen verb pred-
icates, and domain shifts. We observed that Prop-
Bank labeling is more robust in all previous experi-
mental conditions, showing less performance drops
than VerbNet labels.
Assuming that application-based scenarios would
prefer dealing with general thematic role labels, we
explore the best way to label a text with VerbNet
thematic roles, namely, by training directly on Verb-
Net roles or by using the PropBank SRL system
and performing a posterior mapping into thematic
roles. While results are similar and not statistically
significant in the WSJ test set, when testing on the
Brown out?of?domain test set the difference in favor
of PropBank plus mapping step is statistically signif-
icant. We also tried to map the fine-grained VerbNet
roles into coarser roles, but it did not yield better re-
sults than the mapping from PropBank roles. As a
side-product, we show that a simple most frequent
sense disambiguation strategy for verbs is sufficient
to provide excellent results in the PropBank to Verb-
Net mapping.
Regarding future work, we would like to explore
ways to improve the performance on VerbNet roles,
perhaps using selectional preferences. We also want
to work on the adaptation to new domains of both
roles sets.
Acknowledgements
We are grateful to Martha Palmer and Edward Loper
for kindly providing us with the SemLink map-
pings. This work has been partially funded by
the Basque Government (IT-397-07) and by the
Ministry of Education (KNOW TIN2006-15049,
OpenMT TIN2006-15307-C03-02). Ben?at is sup-
ported by a PhD grant from the University of the
Basque Country.
557
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Ido Dagan and Daniel Gildea, editors, Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), pages 152?
164, Ann Arbor, Michigan, USA, June. Association
for Computational Linguistics.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class based construction of a verb lexicon. In
Proceedings of the 17th National Conference on Arti-
ficial Intelligence (AAAI-2000), Austin, TX, July.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press, Chicago.
Edward Loper, Szu-Ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: annotating predicate argument structure. In
HLT ?94: Proceedings of the workshop on Human
Language Technology, pages 114?119, Morristown,
NJ, USA. Association for Computational Linguistics.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 87?92, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Szu-Ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In Pro-
ceedings of the Human Language Technology Con-
ferences/North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT/NAACL-2007).
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2007.
Sequential SRL Using Selectional Preferences. An
Approach with Maximum Entropy Markov Models. In
Proceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 354?357.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez. 2008.
A Preliminary Study on the Robustness and General-
ization of Role Sets for Semantic Role Labeling. In
Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2008), pages 219?230, Haifa, Israel,
February.
558
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 73?76,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Generalizing over Lexical Features:
Selectional Preferences for Semantic Role Classification
Be
?
nat Zapirain, Eneko Agirre
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s M
`
arquez
TALP Research Center
Technical University of Catalonia
Barcelona, Catalonia
lluism@lsi.upc.edu
Abstract
This paper explores methods to allevi-
ate the effect of lexical sparseness in the
classification of verbal arguments. We
show how automatically generated selec-
tional preferences are able to generalize
and perform better than lexical features in
a large dataset for semantic role classifi-
cation. The best results are obtained with
a novel second-order distributional simi-
larity measure, and the positive effect is
specially relevant for out-of-domain data.
Our findings suggest that selectional pref-
erences have potential for improving a full
system for Semantic Role Labeling.
1 Introduction
Semantic Role Labeling (SRL) systems usually
approach the problem as a sequence of two sub-
tasks: argument identification and classification.
While the former is mostly a syntactic task, the
latter requires semantic knowledge to be taken
into account. Current systems capture semantics
through lexicalized features on the predicate and
the head word of the argument to be classified.
Since lexical features tend to be sparse (especially
when the training corpus is small) SRL systems
are prone to overfit the training data and general-
ize poorly to new corpora.
This work explores the usefulness of selectional
preferences to alleviate the lexical dependence of
SRL systems. Selectional preferences introduce
semantic generalizations on the type of arguments
preferred by the predicates. Therefore, they are
expected to improve generalization on infrequent
and unknown words, and increase the discrimina-
tive power of the argument classifiers.
For instance, consider these two sentences:
JFK was assassinated (in Dallas)
Location
JFK was assassinated (in November)
Temporal
Both share syntactic and argument structure, so
the lexical features (i.e., the words ?Dallas? and
?November?) represent the most important knowl-
edge to discriminate between the two different ad-
junct roles. The problem is that, in new text,
one may encounter similar expressions with new
words like Texas or Autumn.
We propose a concrete classification problem as
our main evaluation setting for the acquired selec-
tional preferences: given a verb occurrence and
a nominal head word of a constituent dependant
on that verb, assign the most plausible role to the
head word according to the selectional preference
model. This problem is directly connected to ar-
gument classification in SRL, but we have iso-
lated the evaluation from the complete SRL task.
This first step allows us to analyze the potential
of selectional preferences as a source of seman-
tic knowledge for discriminating among different
role labels. Ongoing work is devoted to the inte-
gration of selectional preference?derived features
in a complete SRL system.
2 Related Work
Automatic acquisition of selectional preferences
is a relatively old topic, and will mention the
most relevant references. Resnik (1993) proposed
to model selectional preferences using semantic
classes from WordNet in order to tackle ambiguity
issues in syntax (noun-compounds, coordination,
PP-attachment).
Brockman and Lapata (2003) compared sev-
eral class-based models (including Resnik?s se-
lectional preferences) on a syntactic plausibility
judgement task for German. The models re-
turn weights for (verb, syntactic function, noun)
triples, and the correlation with human plausibil-
ity judgement is used for evaluation. Resnik?s
selectional preference scored best among class-
based methods, but it performed equal to a simple,
purely lexical, conditional probability model.
73
Distributional similarity has also been used to
tackle syntactic ambiguity. Pantel and Lin (2000)
obtained very good results using the distributional
similarity measure defined by Lin (1998).
The application of selectional preferences to se-
mantic roles (as opposed to syntactic functions)
is more recent. Gildea and Jurafsky (2002) is
the only one applying selectional preferences in
a real SRL task. They used distributional clus-
tering and WordNet-based techniques on a SRL
task on FrameNet roles. They report a very small
improvement of the overall performance when us-
ing distributional clustering techniques. In this pa-
per we present complementary experiments, with
a different role set and annotated corpus (Prop-
Bank), a wider range of selectional preference
models, and the analysis of out-of-domain results.
Other papers applying semantic preferences
in the context of semantic roles, rely on the
evaluation on pseudo tasks or human plausibil-
ity judgments. In (Erk, 2007) a distributional
similarity?based model for selectional preferences
is introduced, reminiscent of that of Pantel and
Lin (2000). The results over 100 frame-specific
roles showed that distributional similarities get
smaller error rates than Resnik and EM, with Lin?s
formula having the smallest error rate. Moreover,
coverage of distributional similarities and Resnik
are rather low. Our distributional model for selec-
tional preferences follows her formalization.
Currently, there are several models of distri-
butional similarity that could be used for selec-
tional preferences. More recently, Pad?o and Lap-
ata (2007) presented a study of several parameters
that define a broad family of distributional similar-
ity models, including publicly available software.
Our paper tests similar techniques to those pre-
sented above, but we evaluate selectional prefer-
ence models in a setting directly related to SR
classification, i.e., given a selectional preference
model for a verb we find the role which fits best
for a given head word. The problem is indeed
qualitatively different: we do not have to choose
among the head words competing for a role (as
in the papers above) but among selectional prefer-
ences competing for a head word.
3 Selectional Preference Models
In this section we present all the variants for ac-
quiring selectional preferences used in our study,
and how we apply them to the SR classification.
WordNet-based SP models: we use Resnik?s se-
lectional preference model.
Distributional SP models: Given the availabil-
ity of publicly available resources for distribu-
tional similarity, we used 1) a ready-made the-
saurus (Lin, 1998), and 2) software (Pad?o and La-
pata, 2007) which we run on the British National
Corpus (BNC).
In the first case, Lin constructed his thesaurus
based on his own similarity formula run over a
large parsed corpus comprising journalism texts.
The thesaurus lists, for each word, the most sim-
ilar words, with their weight. In order to get the
similarity for two words, we could check the entry
in the thesaurus for either word. But given that
the thesaurus is not symmetric, we take the av-
erage of both similarities. We will refer to this
similarity measure as sim
th
lin
. Another option is
to use second-order similarity, where we compute
the similarity of two words using the entries in the
thesaurus, either using the cosine or Jaccard mea-
sures. We will refer to these similarity measures
as sim
th2
jac
and sim
th2
cos
hereinafter.
For the second case, we tried the optimal pa-
rameters as described in (Pad?o and Lapata, 2007,
p. 179): word-based space, medium context, log-
likelihood association, and 2,000 basis elements.
We tested Jaccard, cosine and Lin?s measure (Lin,
1998) for similarity, yielding sim
jac
, sim
cos
and
sim
lin
, respectively.
3.1 Role Classification with SP Models
Given a target sentence where a predicate and sev-
eral potential argument and adjunct head words
occur, the goal is to assign a role label to each of
the head words. The classification of candidate
head words is performed independently of each
other.
Since we want to evaluate the ability of selec-
tional preference models to discriminate among
different roles, this is the only knowledge that will
be used to perform classification (avoiding the in-
clusion of any other feature commonly used in
SRL). Thus, for each head word, we will simply
select the role (r) of the predicate (p) which fits
best the head word (w). This selection rule is for-
malized as:
R(p, w) = argmax
r?Roles(p)
S(p, r, w)
being S(p, r, w) the prediction of the selectional
preference model, which can be instantiated with
all the variants mentioned above.
74
For the sake of comparison we also define a lex-
ical baseline model, which will determine the con-
tribution of lexical features in argument classifica-
tion. For a test pair (p, w) the model returns the
role under which the head word occurred most of-
ten in the training data given the predicate.
4 Experimental Setting
The data used in this work is the benchmark cor-
pus provided by the CoNLL-2005 shared task on
SRL (Carreras and M`arquez, 2005). The dataset,
of over 1 million tokens, comprises PropBank sec-
tions 02-21 for training, and sections 24 and 23 for
development and test, respectively. In these ex-
periments, NEG, DIS and MOD arguments have
been discarded because, apart from not being con-
sidered ?pure? adjunct roles, the selectional pref-
erences implemented in this study are not able to
deal with non-nominal argument heads.
The predicate?rol?head (p, r, w) triples for gen-
eralizing the selectional preferences are extracted
from the arguments of the training set, yield-
ing 71,240 triples, from which 5,587 different
predicate-role selectional preferences (p, r) are
derived by instantiating the different models in
Section 3.
Selectional preferences are then used, to predict
the corresponding roles of the (p, w) pairs from
the test corpora. The test set contains 4,134 pairs
(covering 505 different predicates) to be classified
into the appropriate role label. In order to study
the behavior on out-of-domain data, we also tested
on the PropBanked part of the Brown corpus. This
corpus contains 2,932 (p, w) pairs covering 491
different predicates.
The performance of each selectional preference
model is evaluated by calculating the standard pre-
cision, recall and F
1
measures. It is worth men-
tioning that none of the models is able to predict
the role when facing an unknown head word. This
happens more often with WordNet based models,
which have a lower word coverage compared to
distributional similarity?based models.
5 Results and Discussion
The results are presented in Table 1. The lexi-
cal row corresponds to the baseline lexical match
method. The following row corresponds to the
WordNet-based selectional preference model. The
distributional models follow, including the results
obtained by the three similarity formulas on the
prec. rec. F
1
prec. recall F
1
lexical .779 .349 .482 .663 .059 .108
res .589 .495 .537 .505 .379 .433
sim
Jac
.573 .564 .569 .481 .452 .466
sim
cos
.607 .598 .602 .507 .476 .491
sim
Lin
.580 .560 .570 .500 .470 .485
sim
th
Lin
.635 .625 .630 .494 .464 .478
sim
th2
Jac
.657 .646 .651 .531 .499 .515
sim
th2
cos
.654 .644 .649 .531 .499 .515
Table 1: Results for WSJ test (left), and Brown
test (right)
co-occurrences extracted from the BNC (sim
Jac
,
sim
cos
sim
Lin
), and the results obtained when
using Lin?s thesaurus directly (sim
th
Lin
) and as a
second-order vector (sim
th2
Jac
and sim
th2
cos
).
As expected, the lexical baseline attains very
high precision in all datasets, which underscores
the importance of the lexical head word features
in argument classification. The recall is quite
low, specially in Brown, confirming and extend-
ing (Pradhan et al, 2008), which also reports sim-
ilar performance drops when doing argument clas-
sification on out-of-domain data.
One of the main goals of our experiments is to
overcome the data sparseness of lexical features
both on in-domain and out-of-domain data. All
our selectional preference models improve over
the lexical matching baseline in recall, up to 30
absolute percentage points in the WSJ test dataset
and 44 absolute percentage points in the Brown
corpus. This comes at the cost of reduced preci-
sion, but the overall F-score shows that all selec-
tional preference models improve over the base-
line, with up to 17 absolute percentage points
on the WSJ datasets and 41 absolute percentage
points on the Brown dataset. The results, thus,
show that selectional preferences are indeed alle-
viating the lexical sparseness problem.
As an example, consider the following head
words of potential arguments of the verb wear
found in the test set: doctor, men, tie, shoe. None
of these nouns occurred as heads of arguments of
wear in the training data, and thus the lexical fea-
ture would be unable to predict any role for them.
Using selectional preferences, we successfully as-
signed the Arg0 role to doctor and men, and the
Arg1 role to tie and shoe.
Regarding the selectional preference variants,
WordNet-based and first-order distributional sim-
ilarity models attain similar levels of precision,
but the former are clearly worse on recall and F
1
.
75
The performance loss on recall can be explained
by the worse lexical coverage of WordNet when
compared to automatically generated thesauri. Ex-
amples of words missing in WordNet include ab-
breviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony). The second-order distribu-
tional similarity measures perform best overall,
both in precision and recall. As far as we know,
it is the first time that these models are applied to
selectional preference modeling, and they prove to
be a strong alternative to first-order models. The
relative performance of the methods is consistent
across the two datasets, stressing the robustness of
all methods used.
Regarding the use of similarity software (Pad?o
and Lapata, 2007) on the BNC vs. the use of
Lin?s ready-made thesaurus, both seem to perform
similarly, as exemplified by the similar results of
sim
Lin
and sim
th
Lin
. The fact that the former per-
formed better on the Brown data, and worse on the
WSJ data could be related to the different corpora
used to compute the co-occurrence, balanced cor-
pus and journalism texts respectively. This could
be an indication of the potential of distributional
thesauri to adapt to the target domain.
Regarding the similarity metrics, the cosine
seems to perform consistently better for first-order
distributional similarity, while Jaccard provided
slightly better results for second-order similarity.
The best overall performance was for second-
order similarity, also using the cosine. Given
the computational complexity involved in build-
ing a complete thesaurus based on the similarity
software, we used the ready-made thesaurus of
Lin, but could not try the second-order version on
BNC.
6 Conclusions and Future Work
We have empirically shown how automatically
generated selectional preferences, using WordNet
and distributional similarity measures, are able to
effectively generalize lexical features and, thus,
improve classification performance in a large-
scale argument classification task on the CoNLL-
2005 dataset. The experiments show substantial
gains on recall and F
1
compared to lexical match-
ing, both on the in-domain WSJ test and, espe-
cially, on the out-of-domain Brown test.
Alternative selectional models were studied and
compared. WordNet-based models attain good
levels of precision but lower recall than distribu-
tional similarity methods. A new second-order
similarity method proposed in this paper attains
the best results overall in all datasets.
The evidence gathered in this paper suggests
that using semantic knowledge in the form of se-
lectional preferences has a high potential for im-
proving the results of a full system for SRL, spe-
cially when training data is scarce or when applied
to out-of-domain corpora.
Current efforts are devoted to study the integra-
tion of the selectional preference models presented
in this paper in a in-house SRL system. We are
particularly interested in domain adaptation, and
whether distributional similarities can profit from
domain corpora for better performance.
Acknowledgments
This work has been partially funded by the EU Commis-
sion (project KYOTO ICT-2007-211423) and Spanish Re-
search Department (project KNOW TIN2006-15049-C03-
01). Be?nat enjoys a PhD grant from the University of the
Basque Country.
References
Carsten Brockmann and Mirella Lapata. 2003. Evaluating
and combining approaches to selectional preference ac-
quisition. In Proceedings of the 10th Conference of the
European Chapter of the ACL, pages 27?34.
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages 152?
164, Ann Arbor, MI, USA.
Katrin Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL, pages 768?774.
Sebastian Pad?o and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199, June.
Patrick Pantel and Dekang Lin. 2000. An unsupervised ap-
proach to prepositional phrase attachment using contex-
tually similar words. In Proceedings of the 38th Annual
Conference of the ACL, pages 101?108.
S. Pradhan, W. Ward, and J. H. Martin. 2008. Towards robust
semantic role labeling. Computational Linguistics, 34(2).
Philip Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proceedings of the workshop on Human Language
Technology, pages 278?283, Morristown, NJ, USA.
76
Tutorial Abstracts of ACL-IJCNLP 2009, page 3,
Suntec, Singapore, 2 August 2009.
c
?2009 ACL and AFNLP
Semantic Role Labeling: Past, Present and Future
Llu??s M
`
arquez
TALP Research Center
Software Department
Technical University of Catalonia
lluism@lsi.upc.edu
1 Introduction
Semantic Role Labeling (SRL) consists of, given
a sentence, detecting basic event structures such
as ?who? did ?what? to ?whom?, ?when? and
?where?. From a linguistic point of view, a key
component of the task corresponds to identifying
the semantic arguments filling the roles of the sen-
tence predicates. Typical predicate semantic argu-
ments include Agent, Patient, and Instrument, but
semantic roles may also be found as adjuncts (e.g.,
Locative, Temporal, Manner, and Cause). The
identification of such event frames holds potential
for significant impact in many NLP applications,
such as Information Extraction, Question Answer-
ing, Summarization and Machine Translation.
Recently, the compilation and manual annota-
tion with semantic roles of several corpora has
enabled the development of supervised statistical
approaches to SRL, which has become a well-
defined task with a substantial body of work and
comparative evaluation. Significant advances in
many directions have been reported over the last
several years, including but not limited to: ma-
chine learning algorithms and architectures spe-
cialized for the task, feature engineering, inference
to force coherent solutions, and system combina-
tions.
However, despite all the efforts and the con-
siderable degree of maturity of the SRL technol-
ogy, the use of SRL systems in real-world ap-
plications has so far been limited and, certainly,
below the initial expectations. This fact has to
do with the weaknesses and limitations of current
systems, which have been highlighted by many
of the evaluation exercises and keep unresolved
for a few years (e.g., poor generalization across
corpora, low scalability and efficiency, knowledge
poor features, too high complexity, absolute per-
formance below 90%, etc.).
2 Content Overview and Outline
This tutorial has two differentiated parts. In
the first one, the state-of-the-art on SRL will be
overviewed, including: main techniques applied,
existing systems, and lessons learned from the
CoNLL and SemEval evaluation exercises. This
part will include a critical review of current prob-
lems and the identification of the main challenges
for the future. The second part is devoted to the
lines of research oriented to overcome current lim-
itations. This part will include an analysis of
the relation between syntax and SRL, the devel-
opment of joint systems for integrated syntactic-
semantic analysis, generalization across corpora,
and engineering of truly semantic features. See
the outline below.
1. Introduction
? Problem definition and properties
? Importance of SRL
? Main computational resources and systems avail-
able for SRL
2. State-of-the-art SRL systems
? Architecture
? Training of different components
? Feature engineering
3. Empirical evaluation of SRL systems
? Evaluation exercises at SemEval and CoNLL
conferences
? Main lessons learned
4. Current problems and challenges
5. Keys for future progress
? Relation to syntax: joint learning of syntactic and
semantic dependencies
? Generalization across domains and text genres
? Use of semantic knowledge
? SRL systems in applications
6. Conclusions
3
In: Proceedings of CoNLL-2000 and LLL-2000, pages 31-36, Lisbon, Portugal, 2000. 
A Comparison between Supervised Learning Algorithms for Word 
Sense Disambiguation* 
Gerard  Escudero  and Lluis Mhrquez  and German R igau  
TALP Research Center. LSI Department. Universitat Polit~cnica de Catalunya (UPC) 
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia 
{escudero, lluism, g.rigau}@Isi.upc.es 
Abst ract  
This paper describes a set of comparative exper- 
iments, including cross-corpus evaluation, be- 
tween five alternative algorithms for supervised 
Word Sense Disambiguation (WSD), namely 
Naive Bayes, Exemplar-based learning, SNOW, 
Decision Lists, and Boosting. Two main conclu- 
sions can be drawn: 1) The LazyBoosting algo- 
rithm outperforms the other four state-of-the- 
art algorithms in terms of accuracy and ability 
to tune to new domains; 2) The domain depen- 
dence of WSD systems eems very strong and 
suggests that some kind of adaptation or tun- 
ing is required for cross-corpus application. 
1 In t roduct ion  
Word Sense Disambiguation (WSD) is the prob- 
lem of assigning the appropriate meaning (or 
sense) to a given word in a text or discourse. 
Resolving the ambiguity of words is a central 
problem for large scale language understanding 
applications and their associate tasks (Ide and 
V4ronis, 1998). Besides, WSD is one of the most 
important open problems in NLP. Despite the 
wide range of approaches investigated (Kilgar- 
rift and Rosenzweig, 2000) and the large effort 
devoted to tackling this problem, to date, no 
large-scale broad-coverage and highly accurate 
WSD system has been built. 
One of the most successful current lines of 
research is the corpus-based approach, in which 
statistical or Machine Learning (M L) algorithms 
have been applied to learn statistical models 
or classifiers from corpora in order to per- 
* This research has been partially funded by the Spanish 
Research Department (CICYT's project TIC98-0423- 
C06), by the EU Commission (NAMIC I8T-1999-12392), 
and by the Catalan Research Department (CIRIT's 
consolidated research group 1999SGR-150 and CIRIT's 
grant 1999FI 00773). 
form WSD. Generally, supervised approaches 
(those that learn from previously semantically 
annotated corpora) have obtained better esults 
than unsupervised methods on small sets of se- 
lected ambiguous words, or artificial pseudo- 
words. Many standard M L algorithms for su- 
pervised learning have been applied, such as: 
Decision Lists (Yarowsky, 1994; Agirre and 
Martinez, 2000), Neural Networks (Towell and 
Voorhees, 1998), Bayesian learning (Bruce and 
Wiebe, 1999), Exemplar-based learning (Ng, 
1997), Boosting (Escudero et al, 2000a), etc. 
Further, in (Mooney, 1996) some of the previ- 
ous methods are compared jointly with Decision 
Trees and Rule Induction algorithms, on a very 
restricted omain. 
Although some published works include the 
comparison between some alternative algo- 
rithms (Mooney, 1996; Ng, 1997; Escudero et 
al., 2000a; Escudero et al, 2000b), none of 
them addresses the issue of the portability of 
supervised ML algorithms for WSD, i.e., testing 
whether the accuracy of a system trained on 
a certain corpus can be extrapolated to other 
corpora or not. We think that the study of the 
domain dependence of WSD -- in the style of 
other studies devoted to parsing (Sekine, 1997; 
Ratnaparkhi, 1999)-- is needed to assure the 
validity of the supervised approach, and to de- 
termine to which extent a tuning pre-process i
necessary to make real WSD systems portable. 
In this direction, this work compares five differ- 
ent M L algorithms and explores their portability 
and tuning ability by training and testing them 
on different corpora. 
2 Learn ing  A lgor i thms Tested  
Naive-Bayes (NB). Naive Bayes is intended 
as a simple representative of statistical learning 
methods. It has been used in its most classi- 
31 
cal setting (Duda and Hart, 1973). That is, 
assuming the independence of features, it clas- 
sifies a new example by assigning the class that 
maximizes the conditional probability of the 
class given the observed sequence of features 
of that example. Model probabilities are esti- 
mated during the training process using relative 
frequencies. To avoid the effect of zero counts, a 
very simple smoothing technique has been used, 
which was proposed in (Ng, 1997). 
Despite its simplicity, Naive Bayes is claimed 
to obtain state-of-the-art accuracy on super- 
vised WSD in many papers (Mooney, 1996; Ng, 
1997; Leacock et al, 1998). 
Exemplar -based  Classif ier (EB). In exem- 
plar, instance, or memory-based learning (Aha 
et al, 1991) no generalization of training ex- 
amples is performed. Instead, the examples are 
simply stored in memory and the classification 
of new examples is based on the most similar 
stored exemplars. In our implementation, all
examples are kept in memory and the classifica- 
tion is based on a k-NN (Nearest-Neighbours) 
algorithm using Hamming distance to measure 
closeness. For k's greater than 1, the resulting 
sense is the weighted majority sense of the k 
nearest neighbours --where each example votes 
its sense with a strength proportional to its 
closeness to the test example. 
Exemplar-based learning is said to be the 
best option for WSD (Ng, 1997). Other au- 
thors (Daelemans et al, 1999) point out that 
exemplar-based methods tend to be superior in 
language learning problems because they do not 
forget exceptions. 
The SNoW Arch i tec ture  (SN). SNoWis a 
Sparse Network of linear separators which uti- 
lizes the Winnow learning algorithm 1. In the 
SNo W architecture there is a winnow node for 
each class, which learns to separate that class 
from all the rest. During training, which is per- 
formed in an on-line fashion, each example is 
considered a positive example for the winnow 
node associated to its class and a negative x- 
ample for all the others. A key point that allows 
a fast learning is that the winnow nodes are not 
connected to all features but only to those that 
1The Winnow algorithm (Littlestone, 1988) consists 
of a linear threshold algorithm with multiplicative weight 
updating for 2-class problems. 
are "relevant" for their class. When classify- 
ing a new example, SNo W is similar to a neural 
network which takes the input features and out- 
puts the class with the highest activation. Our 
implementation f SNo W for WSD is explained 
in (Escudero et al, 2000c). 
SNoW is proven to perform very well in 
high dimensional NLP problems, where both the 
training examples and the target function reside 
very sparsely in the feature space (Roth, 1998), 
e.g: context-sensitive spelling correction, POS 
tagging, PP-attachment disambiguation, etc. 
Decis ion Lists (DL). In this setting, a Deci- 
sion List is a list of features extracted from the 
training examples and sorted by a log-likelihood 
measure. This measure stimates how strong a 
particular feature is as an indicator of a specific 
sense (Yarowsky, 1994). When testing, the deci- 
sion list is checked in order and the feature with 
the highest weight that matches the test exam- 
ple is used to select the winning word sense. 
Thus, only the single most reliable piece of ev- 
idence is used to perform disambiguation. Re- 
garding the details of implementation (smooth- 
ing, pruning of the decision list, etc.) we have 
followed (Agirre and Martinez, 2000). 
Decision Lists were one of the most success- 
ful systems on the 1st Senseval competition for 
WSD (Kilgarriff and Rosenzweig, 2000). 
LazyBoost ing  (LB). The main idea of boost- 
ing algorithms is to combine many simple and 
moderately accurate hypotheses (weak classi- 
fiers) into a single, highly accurate classifier. 
The weak classifiers are trained sequentially 
and, conceptually, each of them is trained on the 
examples which were most difficult to classify by 
the preceding weak classifiers. These weak hy- 
potheses are then linearly combined into a single 
rule called the combined hypothesis. 
Schapire and Singer's real AdaBoost.MH al- 
gorithm for multiclass multi-label classifica- 
tion (Schapire and Singer, 1999) has been used. 
It constructs a combination of very simple 
weak hypotheses that test the value of a single 
boolean predicate and make a real-valued pre- 
diction based on that value. LazyBoosting (Es- 
cudero et al, 2000a) is a simple modification 
of the AdaBoost.MH algorithm, which consists 
in reducing the feature space that is explored 
when learning each weak classifier. This mod- 
ification significantly increases the efficiency of 
32 
the learning process with no loss in accuracy. 
3 Set t ing  
A number of comparative experiments has been 
carried out on a subset of 21 highly ambiguous 
words of the DSO corpus, which is a semanti- 
cally annotated English corpus collected by Ng 
and colleagues (Ng and Lee, 1996). Each word 
is treated as a different classification problem. 
The 21 words comprise 13 nouns (age, art, body, 
car, child, cost, head, interest, line, point, state, 
thing, work) and 8 verbs (become, fall, grow, lose, 
set, speak, strike, tell), which frequently appear 
in the WSD literature. The average number of 
senses per word is close to 10 and the number 
of training examples is around 1,000. 
The DSO corpus contains entences from two 
different corpora, namely Wall Street Journal 
(WSJ) and Brown Corpus (BC). Therefore, it is 
easy to perform experiments about the porta- 
bility of systems by training them on the WSJ 
part (A part, hereinafter) and testing them on 
the BC part (B part, hereinafter), or vice-versa. 
Two kinds of information are used to train 
classifiers: local and topical context. Let 
... " be ~ W-3 W--2 W--1 W W-i_ 1 W+2 W+3. . .  
the context of consecutive words around the 
word w to be disambiguated, and P?i ( -3  < 
i < 3) be the part-of-speech tag of word 
w?i. Attributes referring to local context 
are the following 15: P-3, P-2, P- l ,  P+I, 
P+2, P+3, w- l ,  W-t-1 , (W-2, W-1), (W-i,W+I), 
(W+I ,W+2) ,  (W-3,  W--2, W--1), (W-2, W- i ,W+I ) ,  
(W--l, W+i , W+2), and (W+l, w+2, w+3), where 
the last seven correspond to collocations of two 
and three consecutive words. The topical con- 
text is formed by c l , . . . ,  Cm, which stand for the 
unordered set of open class words appearing in 
the sentence 2. Details about how the different 
algorithms translate this information into fea- 
tures can be found in (Escudero et al, 2000c). 
4 Compar ing  the  five approaches  
The five algorithms, jointly with a naive Most- 
Frequent-sense Classifier (MFC), have been 
tested, by 10-fold cross validation, on 7 different 
combinations of training-test sets 3. Accuracy 
2This set of attributes corresponds to that used in (Ng 
and Lee, 1996), with the exception of the morphology of 
the target word and the verb-object syntactic relation. 
3The combinations of training-test sets are called: 
A+B-A+B, A-I-B-A, A+B-B, A-A, B-B, A-B, and B-A, 
figures, micro-averaged over the 21 words and 
over the ten folds, are reported in table 1. The 
comparison leads to the following conclusions: 
As expected, the five algorithms ignificantly 
outperform the baseline M FC classifier. Among 
them, three groups can be observed: Ni3, DL, 
and SN perform similarly; LB outperforms all 
the other algorithms in all experiments; and EB 
is somewhere in between. The difference be- 
tween \[B and the rest is statistically significant 
in all cases except when comparing \[B to the EB 
approach in the case marked with an asterisk 4. 
Extremely poor results are observed when 
testing the portability of the systems. Restrict- 
ing to LB results, it can be observed that the 
accuracy obtained in A-B is 47.1%, while the 
accuracy in B-B (which can be considered an 
upper bound for LB in B corpus) is 59.0%, that 
is, that there is a difference of 12 points. Fur- 
thermore, 47.1% is only slightly better than the 
most frequent sense in corpus B, 45.5%. 
Apart from accuracy figures, the comparison 
between the predictions made by the five meth- 
ods on the test sets provides interesting infor- 
mation about the relative behaviour of the algo- 
rithms. Table 2 shows the agreement rates and 
the Kappa statistics 5 between all pairs of meth- 
ods in the A+B-A+B experiment. Note that 
'DSO' stands for the annotation of DSO corpus, 
which is taken as the correct one. 
It can be observed that N B obtains the most 
similar results with regard to M FC in agreement 
and Kappa values. The agreement ratio is 74%, 
that is, almost 3 out of 4 times it predicts the 
most frequent sense. On the other extreme, LB 
obtains the most similar results with regard to 
DSO in agreement and Kappa values, and it has 
the least similar with regard to M FC, suggesting 
respectively. In this notation, the training set is placed 
on the left hand side of symbol "-", while the test set 
is on the right hand side. For instance, A-B means that 
the training set is corpus A and the test set is corpus B. 
The symbol "+" stands for set union. 
4Statistical tests of significance applied: McNemar's 
test and 10-fold cross-validation paired Student's t-test 
at a confidence value of 95% (Dietterich, 1998). 
~The Kappa statistic (Cohen, 1960) is a better mea- 
sure of inter-annotator agreement which reduces the ef- 
fect of chance agreement. It has been used for measur- 
ing inter-annotator agreement during the construction 
of semantic annotated corpora (V~ronis, 1998; Ng et al, 
1999). A Kappa value of 1 indicates perfect agreement, 
while 0.8 is considered as indicating ood agreement. 
33 
Accuracy (%) 
LazyBoosting 
A+B-A+B A+B-A A+B-B A-A B-B 
MFC 46.55?o.71 53.90?2.ol 39.21?i.9o 55.94?Mo 45.52?1.27 
Naive Bayes 61.55?1.o4 67.25?1.o7 55.85?1.81 65.86?1.11 56.80?1.12 
Decision Lists 61.58?o.98 67.64?0.94 55.53?1.85 67.57?1.44 56.56?1.59 
SNoW 60.92?1.o9 65.57?1.33 56.28?1.1o 67.12?1.16 56.13?1.23 
Exemplar-based 63.01?o.93 69.08?1.66 56.97?1.22 68.98?1.o6 57.36?1.68 
66.32?1.34 71.79?1.51 60.85~L81 71,26?1.i5 58.96?1.86 
A-B B-A 
36.40 38.71 
41.38 47.66 
43.01 48.83 
44.07 49.76 
45.32 51.13 
47.10 51.99" 
Table 1: Accuracy results (=h standard eviation) of the methods on all training-test combinations 
A+B-A+B 
DSO MFC NB EB SN DL LB 
DSO - -  46.6 61.6 63.0 60.9 61.6 66.3 
MFC -0.19 --  73.9 60.0 55.9 64.9 54.9 
NB 0.24 -0.09 --  76.3 74.5 76.8 71.4 
EB 0.36 -0.15 0.44 - -  69.6 70.7 72.5 
SN 0.36 -0.17 0.44 0.44 - -  67.5 69.0 
DL 0.32 -0.13 0.40 0.41 0.38 --  69.9 
LB 0.44 -0.17 0.37 0.50 0.46 0.42 - -  
Table 2: Kappa statistic (below diagonal) and 
% of agreement (above diagonal) between all 
methods in the A+B-A+B experiment 
that LB is the algorithm that better learns the 
behaviour of the DSO examples. 
In absolute terms, the Kappa values are very 
low. But, as it is suggested in (Vdronis, 1998), 
evaluation measures hould be computed rela- 
tive to the agreement between the human an- 
notators of the corpus and not to a theoreti- 
cal 100%. It seems pointless to expect more 
agreement between the system and the refer- 
ence corpus than between the annotators them- 
selves. Contrary to the intuition that the agree- 
ment between human annotators should be very 
high in the WSD task, some papers report sur- 
prisingly low figures. For instance, (Ng et al, 
1999) reports an accuracy rate of 56.7% and a 
Kappa value of 0.317 when comparing the anno- 
tation of a subset of the DSO corpus performed 
by two independent research groups. From this 
perspective, the Kappa value of 0.44 achieved 
by LB in A+B-A+B could be considered an ex- 
cellent result. Unfortunately, the subset of the 
\[:)SO corpus studied by (Ng et al, 1999) and 
that used in this report are not the same and, 
thus, a direct comparison is not possible. 
4.1 About  the  tun ing  to new domains  
This experiment explores the effect of a sim- 
ple tuning process consisting in adding to the 
original training set A a relatively small sample 
of manually sense-tagged examples of the new 
domain B. The size of this supervised portion 
varies from 10% to 50% of the available corpus 
in steps of 10% (the remaining 50% is kept for 
testing) 6. This experiment will be referred to 
as A+%B-B T. In order to determine to which 
extent the original training set contributes to 
accurately disambiguating in the new domain, 
we also calculate the results for %B-B, that is, 
using only the tuning corpus for training. 
Figure 1 graphically presents the results ob- 
tained by all methods. Each plot contains the 
A+%B-B and %B-B curves, and the straight 
lines corresponding to the lower bound MFC, 
and to the upper bounds B-B and A+B-B. 
As expected, the accuracy of all methods 
grows (towards the upper bound) as more tun- 
ing corpus is added to the training set. How- 
ever, the relation between A+%B-B and %B-B 
reveals some interesting facts. In plots (c) and 
(d), the contribution of the original training cor- 
pus is null, while in plots (a) and (b), a degrada- 
tion onthe  accuracy is observed. Summarizing, 
these results suggest hat for NB, DL, SN, and 
EB methods it is not worth keeping the original 
training examples. Instead, a better (but dis- 
appointing) strategy would be simply using the 
tuning corpus. However, this is not the situa- 
tion of LB - -plot  (d) - -  for which a moderate, 
but consistent, improvement of accuracy is ob- 
served when retaining the original training set. 
6Tuning examples can be weighted more highly than 
the training examples to force the learning algorithm to 
adapt more quickly to the new corpus. Some experi- 
ments in this direction revealed that slightly better e- 
sults can be obtained, though the improvement was not 
statistically significant. 
7The converse xperiment B-F%A-A is not reported 
in this paper due to space limitations. Results can be 
found in (Escudero et al, 2000c). 
34 
58 
56 
54 
g 52 
al 
~ 50  
~ 46 
44 
42  
40  
(a) Naive Bayes 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  M~S o 
........................................................... ~ ................. B...B . . . . .  
A+B-B ........ 
A+%B-B ~-  
%B-B . . . . .  .. 
56 
~ 54  
o 52 
~ 48 
46 
44  
58  
56 
54 
16 
44  
42  
(b) Decision Lists 
. . . . . . . . . . . . . . . . . . .  G . . . . . . . . . . . . . . . . . . .  o . . . . . . . . . . . . . . . . . . .  ~ . . . . . . . . . . . .  AYS:B : : i : : :  .... 
A+%B-B 
%B-B . . . . .  ' 
, , , , , 
5 10 15 20 25 30 35 40 45 50 5 10 15 
(d )  SNoW 
58 
E::E::E.~:E.~:E.~:EZ::Z~:E.~:Z.Z..~YE::LL:::Y.:~.'S::47724:L:; 
B-B . . . . .  
Zoi~:~ '?~ 
5 10 15 20 25 30 35 40 45 50 
0 20 25 30 35 40 45 50 
62 
60 
58 
~ 56 
~ 52 
48 
46  
44  
(c) Exemplar Based 
58 
MFS 
56 =::=:==::=Q:~=:::=:==:::a=:====::==~=::=:::=:=:Q-~,~=--~---~ A+B-B ........ 
A+%B-B 
54 %B-B ....... 
52 
50 ..... ~ 
46  / o  , , , 
' ' ' 44  ' ' ' ' ' ' ' ' ' 
5 10 15 20 25 30 35 40 45 50 
(e) Lazygoosting 
.................. o................... ~ ................... ~ ................ MF$- -~- -  
B-B ...... 
...................................................... A~'B-B . . . . . . .  
A+%B-B ~- -  
%B-B . . . . .  
/ / 
i / 
5 10 15 20 25 30 35 40 45 50 
Figure 1: Results of the tuning experiment 
We observed that part of the poor results 
obtained is explained by: 1) corpus A and 
B have a very different distribution of senses, 
and, therefore, different a-priori biases; further- 
more, 2) examples of corpus A and B contain 
different information and, therefore, the learn- 
ing algorithms acquire different (and non inter- 
changeable) classification clues from both cor- 
pora. The study of the rules acquired by Lazy- 
Boosting from WSJ and BC helped understand- 
ing the differences between corpora. On the one 
hand, the type of features used in the rules was 
significantly different between corpora and, ad- 
ditionally, there were very few rules that applied 
to both sets. On the other hand, the sign of the 
prediction of many of these common rules was 
somewhat contradictory between corpora. See 
(Escudero et al, 2000c) for details. 
4.2 About  the tra in ing data qual i ty 
The observation of the rules acquired by Lazy- 
Boosting could also help improving data quality 
in a semi-supervised fashion. It is known that 
mislabelled examples resulting from annotation 
errors tend to be hard examples to classify cor- 
rectly and, therefore, tend to have large weights 
in the final distribution. This observation al- 
lows both to identify the noisy examples and 
use LazyBoosting as a way to improve the train- 
ing corpus. 
A preliminary experiment has been carried 
out in this direction by studying the rules ac- 
quired by LazyBoosting from the training ex- 
amples of the word state. The manual revi- 
sion, by four different people, of the 50 high- 
est scored rules, allowed us to identify 28 noisy 
training examples. 11 of them were clear tag- 
ging errors, and the remaining 17 were not co- 
herently tagged and very difficult to judge, since 
the four annotators achieved systematic dis- 
agreement (probably due to the extremely fine 
grained sense definitions involved in these ex- 
amples). 
5 Conc lus ions  
This work reports a comparative study of five 
ML algorithms for WSD, and provides some re- 
sults on cross corpora evaluation and domain 
re-tuning. 
Regarding portability, it seems that the per- 
formance of supervised sense taggers is not 
guaranteed when moving from one domain to 
another (e.g. from a balanced corpus, such 
as BC, to an economic domain, such as WSJ). 
35 
These results imply that some kind of adap- 
tation is required for cross-corpus application. 
Consequently, it is our belief that a number of 
issues regarding portability, tuning, knowledge 
acquisition, etc., should be thoroughly studied 
before stating that the supervised M k paradigm 
is able to resolve a realistic WSD problem. 
Regarding the ML algorithms tested, kazy- 
Boosting emerges as the best option, since 
it outperforms the other four state-of-the-art 
methods in all experiments. Furthermore, this 
algorithm shows better properties when tuned 
to new domains. Future work is planned for 
an extensive valuation of kazyBoosting on the 
WSD task. This would include taking into ac- 
count additional/alternative attributes, learn- 
ing curves, testing the algorithm on other cor- 
pora, etc. 
Re ferences  
E. Agirre and D. Martinez. 2000. Decision Lists and 
Automatic Word Sense Disambiguation. In Pro- 
ceedings of the COLING Workshop on Semantic 
Annotation and Intelligent Content. 
D. Aha, D. Kibler, and M. Albert. 1991. Instance- 
based Learning Algorithms. Machine Learning, 
7:37-66. 
R. F. Bruce and J. M. Wiebe. 1999. Decomposable 
Modeling in Natural Language Processing. Com- 
putational Linguistics, 25(2):195-207. 
J. Cohen. 1960. A Coefficient of Agreement for 
Nominal Scales. Journal of Educational and Psy- 
chological Measurement, 20:37-46. 
W. Daelemans, A. van den Bosch, and J. Zavrel. 
1999. Forgetting Exceptions is Harmful in Lan- 
guage Learning. Machine Learning, 34:11-41. 
T. G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10(7). 
R. O. Duda and P. E. Hart. 1973. Pattern Classifi- 
cation and Scene Analysis. Wiley ~: Sons. 
G. Escudero, L. M~rquez, and G. Rigau. 2000a. 
Boosting Applied to Word Sense Disambiguation. 
In Proceedings of the 12th European Conference 
on Machine Learning, ECML. 
G. Escudero, L. M~rquez, and G. Rigau. 2000b. 
Naive Bayes and Exemplar-Based Approaches to 
Word Sense Disambiguation Revisited. In Pro- 
ceedings of the 14th European Conference on Ar- 
tificial Intelligence, ECAL 
G. Escudero, L. M~rquez, and G. Rigau. 2000c. On 
the Portability and Tuning of Supervised Word 
Sense Disambiguation Systems. Research Report 
LSI-00-30-R, Software Department (LSI). Techni- 
cal University of Catalonia (UPC). 
N. Ide and J. V@ronis. 1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguistics, 
24(1):1-40. 
A. Kilgarriff and J. Rosenzweig. 2000. English SEN- 
SEVAL: Report and Results. In Proceedings of the 
2nd International Conference on Language Re- 
sources and Evaluation, LREC. 
C. Leacock, M. Chodorow, and G. A. Miller. 1998. 
Using Corpus Statistics and WordNet Relations 
for Sense Identification. Computational Linguis- 
tics, 24(1):147-166. 
N. Littlestone. 1988. Learning Quickly when Ir- 
relevant Attributes Abound. Machine Learning, 
2:285-318. 
R. J. Mooney. 1996. Comparative Experiments on 
Disambiguating Word Senses: An Illustration of 
the Role of Bias in Machine Learning. In Proceed- 
ings of the 1st Conference on Empirical Methods 
in Natural Language Processing, EMNLP. 
H. T. Ng and H. B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Senses: 
An Exemplar-based Approach. In Proceedings of 
the 3~th Annual Meeting of the ACL. 
H. T. Ng, C. Lim, and S. Foo. 1999. A Case Study 
on Inter-Annotator Agreement for Word Sense 
Disambiguation. In Procs. of the ACL SIGLEX 
Workshop: Standardizing Lexical Resources. 
H. T. Ng. 1997. Exemplar-Base Word Sense Disam- 
biguation: Some Recent Improvements. In Procs. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing, EMNLP. 
A. Ratnaparkhi. 1999. Learning to Parse Natural 
Language with Maximum Entropy Models. Ma- 
chine Learning, 34:151-175. 
D. Roth. 1998. Learning to Resolve Natural Lan- 
guage Ambiguities: A Unified Approach. In Pro- 
ceedings of the National Conference on Artificial 
Intelligence, AAAI  '98. 
R. E. Schapire and Y. Singer. 1999. Improved 
Boosting Algorithms Using Confidence-rated Pre- 
dictions. Machine Learning, 37(3):297-336. 
S. Sekine. 1997. The Domain Dependence of Pars- 
ing. In Proceedings of the 5th Conference on Ap- 
plied Natural Language Processing, ANLP. 
G. Towell and E. M. Voorhees. 1998. Disambiguat- 
ing Highly Ambiguous Words. Computational 
Linguistics, 24(1):125-146. 
J. V@ronis. 1998. A study of polysemy judgements 
and inter-annotator agreement. In Programme 
and advanced papers of the Senseval workshop, 
Herstmonceux Castle, England. 
D. Yarowsky. 1994. Decision Lists for Lexical Ambi- 
guity Resolution: Application to Accent Restora- 
tion in Spanish and French. In Proceedings of the 
32nd Annual Meeting of the ACL. 
36 
An Empirical Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems* 
Gerard  Escudero ,  L lu is  M~trquez~ and German R igau  
TALP Research Center. LSI Department. Universitat Polit~cnica de Catalunya (UPC) 
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia 
{escudero, lluism, g. rigau}@isi, upc. es 
Abst ract  
This paper describes a set of experiments car- 
ried out to explore the domain dependence 
of alternative supervised Word Sense Disam- 
biguation algorithms. The aim of the work is 
threefold: studying the performance of these 
algorithms when tested on a different cor- 
pus from that they were trained on; explor- 
ing their ability to tune to new domains, 
and demonstrating empirically that the Lazy- 
Boosting algorithm outperforms tate-of-the- 
art supervised WSD algorithms in both previ- 
ous situations. 
Keywords:  Cross-corpus evaluation of Ni_P 
systems, Word Sense Disambiguation, Super- 
vised Machine Learning 
1 In t roduct ion  
Word Sense Disambiguation (WSD) is the 
problem of assigning the appropriate meaning 
(sense) to a given word in a text or discourse. 
Resolving the ambiguity of words is a central 
problem for large scale language understand- 
ing applications and their associate tasks (Ide 
and V4ronis, 1998), e.g., machine transla- 
tion, information retrieval, reference resolu- 
tion, parsing, etc. 
WSD is one of the most important open 
problems in NLP. Despite the wide range of 
approaches investigated and the large effort 
devoted to tackle this problem, to date, no 
large-scale broad-coverage and highly accu- 
rate WSD system has been built --see the 
main conclusions of the first edition of Sen- 
sEval (Kilgarriff and Rosenzweig, 2000). 
One of the most successful current lines 
of research is the corpus-based approach in 
" This research has been partially funded by the Span- 
ish Research Department (CICYT's project TIC98- 
0423-C06). by the EU Commission (NAMIC IST- 
1999-12392), and by the Catalan Research Depart- 
ment (CIRIT's consolidated research group 1999SGR- 
150 and CIRIT's grant 1999FI 00773). 
which statistical or Machine Learning (ML) al- 
gorithms are applied to learn statistical mod- 
els or classifiers from corpora in order to per- 
form WSD. Generally, supervised approaches 1 
have obtained better results than unsuper- 
vised methods on small sets of selected am- 
biguous words, or artificial pseudo-words. 
Many standard M L algorithms for supervised 
learning have been applied, such as: Decision 
Lists (?arowsky, 1994; Agirre and Martinez, 
2000), Neural Networks (Towell and Voorhees, 
1998), Bayesian learning (Bruce and Wiebe, 
1999), Exemplar-Based learning (Ng, 1997a; 
Fujii et al, 1998), Boosting (Escudero et al, 
2000a), etc. Unfortunately, there have been 
very few direct comparisons between alterna- 
tive methods for WSD. 
In general, supervised learning presumes 
that the training examples are somehow re- 
flective of the task that will be performed by 
the trainee on other data. Consequently, the 
performance of such systems is commonly es- 
timated by testing the algorithm on a separate 
part of the set of training examples (say 10- 
20% of them), or by N-fold cross-validation, 
in which the set of examples i partitioned into 
N disjoint sets (or folds), and the training- 
test procedure is repeated N times using all 
combinations of N-1  folds for training and 1 
fold for testing. In both cases, test examples 
are different from those used for training, but 
they belong to the same corpus, and, there- 
fore, they are expected to be quite similar. 
Although this methodology could be valid 
for certain NLP problems, such as English 
Part-of-Speech tagging, we think that there 
exists reasonable vidence to say that, in 
WSD, accuracy results cannot be simply ex- 
trapolated to other domains (contrary to the 
opinion of other authors (Ng, 1997b)): On the 
aSupervised approaches, also known as data-driven 
or corpus-dmven, are those that learn from a previ- 
ously semantically annotated corpus. 
172 
one hand, WSD is very dependant to the do- 
main of application (Gale et al, 1992b) --see 
also (Ng and Lee, 1996; Ng, 1997a), in which 
quite different accuracy figures are obtained 
when testing an exemplar-based WSD classi- 
fier on two different corpora. Oi1 the other 
hand, it does not seem reasonable to think 
that the training material is large and repre- 
sentative nough to cover "all" potential types 
of examples. 
To date, a thorough study of the domain 
dependence of WSD - - in  the style of other 
studies devoted to parsing (Sekine, 1997)-- 
has not been carried out. We think that such 
an study is needed to assess the validity of 
the supervised approach, and to determine to 
which extent a tuning process is necessary to 
make real WSD systems portable. In order 
to corroborate the previous hypotheses, this 
paper explores the portability and tuning of 
four different ML algorithms (previously ap- 
plied to WSD) by training and testing them 
on different corpora. 
Additionally, supervised methods suffer 
from the "knowledge acquisition bottle- 
neck" (Gale et al, 1992a). (Ng, 1997b) esti- 
mates that the manual annotation effort nec- 
essary to build a broad coverage semantically 
annotated English corpus is about 16 person- 
years. This overhead for supervision could be 
much greater if a costly tuning procedure is 
required before applying any existing system 
to each new domain. 
Due to this fact, recent works have focused 
on reducing the acquisition cost as well as the 
need for supervision i  corpus-based methods. 
It is our belief that the research by (Leacock et 
al., 1998; Mihalcea and Moldovan, 1999) 2 pro- 
vide enough evidence towards the "opening" 
of the bottleneck in the near future. For that 
reason, it is worth further investigating the 
robustness and portability of existing super- 
vised ML methods to better resolve the WSD 
problem. 
It is important o note that the focus of 
this work will be on the empirical cross- 
corpus evaluation of several M L supervised al- 
gorithms. Other important issues, such as: 
selecting the best attribute set, discussing an 
appropriate definition of senses for the task, 
etc., are not addressed in this paper. 
eIn the line of using lexical resources and search en- 
gunes to automatically collect training examples from 
large text collections or Internet. 
This paper is organized as follows: Section 2 
presents the four ML algorithms compared. 
In section 3 the setting is presented in de- 
tail, including the corpora and the experimen- 
tal methodology used. Section 4 reports the 
experiments carried out and the results ob- 
tained. Finally, section 5 concludes and out- 
lines some lines for further esearch. 
2 Learn ing  A lgor i thms Tested  
2.1 Naive-Bayes (NB) 
Naive Bayes is intended as a simple represen- 
tative of statistical learning methods. It has 
been used in its most classical setting (Duda 
and Hart, 1973). That is, assuming indepen- 
dence of features, it classifies a new example 
by assigning the class that maximizes the con- 
ditional probability of the class given the ob- 
served sequence of features of that example. 
Model probabilities are estimated uring 
training process using relative frequencies. To 
avoid the effect of zero counts when esti- 
mating probabilities, a very simple smooth- 
ing technique has been used, which was pro- 
posed in (Ng, 1997a). Despite its simplicity, 
Naive Bayes is claimed to obtain state-of-the- 
art accuracy on supervised WSD in many pa- 
pers (Mooney, 1996; Ng, 1997a; Leacock et 
al., 1998). 
2.2 Exemplar -based  Classif ier (EB) 
In Exemplar-based learning (Aha et al, 1991) 
no generalization of training examples is per- 
formed. Instead, the examples are stored 
in memory and the classification of new ex- 
amples is based on the classes of the most 
similar stored examples. In our implemen- 
tation, all examples are kept in memory and 
the classification of a new example is based 
on a k-NN (Nearest-Neighbours) algorithm 
using Hamming distance 3 to measure close- 
ness (in doing so, all examples are examined). 
For k's greater than 1, the resulting sense is 
the weighted majority sense of the k near- 
est neighbours --where each example votes its 
sense with a strength proportional to its close- 
ness to the test example. 
In the experiments explained in section 4, 
the EB algorithm is run several times using 
different number of nearest neighbours (1, 3, 
SAlthough the use of MVDM metric (Cost and 
Salzberg, 1993) could lead to better results, current 
implementations have prohivitive computational over- 
heads(Escudero et al, 2000b) 
173 
5, 7, 10, 15, 20 and 25) and the results corre- 
sponding to the best choice are reported 4.
Exemplar-based learning is said to be the 
best option for VSD (Ng, 1997a). Other au- 
thors (Daelemans et al, 1999) point out that 
exemplar-based methods tend to be superior 
in language learning problems because they 
do not forget exceptions. 
2.3 Snow: A Winnow-based  Classif ier 
Snow stands for Sparse Network Of Winnows, 
and it is intended as a representative of on- 
line learning algorithms. 
The basic component is the Winnow al- 
gorithm (Littlestone, 1988). It consists of a 
linear threshold algorithm with multiplicative 
weight updating for 2-class problems, which 
learns very fast in the presence of many bi- 
nary input features. 
In the Snow architecture there is a winnow 
node for each class, which learns to separate 
that class from all the rest. During training, 
each example is considered a positive xample 
for winnow node associated to its class and 
a negative example for all the rest. A key 
point that allows a fast learning is that the 
winnow nodes are not connected to all features 
but only to those that are "relevant" for their 
class. When classifying a new example, Snow 
is similar to a neural network which takes the 
input features and outputs the class with the 
highest activation. 
Snow is proven to perform very well in 
high dimensional domains, where both, the 
training examples and the target function re- 
side very sparsely in the feature space (Roth, 
1998), e.g: text categorization, context- 
sensitive spelling correction, WSD, etc. 
In this paper, our approach to WSD using 
Snow follows that of (Escudero et al, 2000c). 
2.4 LazyBoost ing  (LB) 
The main idea of boosting algorithms is to 
combine many simple and moderately accu- 
rate hypotheses (called weak classifiers) into 
a single, highly accurate classifier. The weak 
classifiers are trained sequentially and, con- 
ceptually, each of them is trained on the ex- 
amples which were most difficult to classify 
by the preceding weak classifiers. These weak 
4In order to construct a real EB-based system for 
WSD, the k parameter should be estimated by cross- 
validation using only the training set (Ng, 1997a), 
however, in our case, this cross-validation i side the 
cross-validation i volved in the testing process would 
generate a prohibitive overhead. 
hypotheses are then linearly combined into a 
single rule called the combined hypothesis. 
More particularly, the Schapire and Singer's 
real AdaBoost.MH algorithm for multi- 
class multi-label classification (Schapire and 
Singer, to appear) has been used. As in that 
paper, very simple weak hypotheses are used. 
They test the value of a boolean predicate and 
make a real-valued prediction based on that 
value. The predicates used, which are the bi- 
narization of the attributes described in sec- 
tion 3.2, are of the form "f = v", where f is a 
feature and v is a value (e.g: "-r v" p e mus_word 
= hosp i ta l " ) .  Each weak rule uses a single 
feature, and, therefore, they can be seen as 
simple decision trees with one internal node 
(testing the value of a binary feature) and two 
leaves corresponding to the yes/no answers to 
that test. 
LazyBoosting (Escudero et al, 2000a), is a 
simple modification of the AdaBoost.MH al- 
gorithm, which consists of reducing the fea- 
ture space that is explored when learning each 
weak classifier. More specifically, a small pro- 
portion p of attributes are randomly selected 
and the best weak rule is selected only among 
them. The idea behind this method is that 
if the proportion p is not too small, probably 
a sufficiently good rule can be found at each 
iteration. Besides, the chance for a good rule 
to appear in the whole learning process is very 
high. Another important characteristic is that 
no attribute needs to be discarded and, thus, 
the risk of eliminating relevant attributes is 
avoided. The method seems to work quite well 
since no important degradation is observed in 
performance for values of p greater or equal 
to 5% (this may indicate that there are many 
irrelevant or highly dependant attributes in 
the WSD domain). Therefore, this modifica- 
tion significantly increases the efficiency of the 
learning process (empirically, up to 7 times 
faster) with no loss in accuracy. 
3 Set t ing  
3.1 The  DSO Corpus  
The DSO corpus is a semantically annotated 
corpus containing 192,800 occurrences of 121 
nouns and 70 verbs, corresponding to the most 
frequent and ambiguous English words. This 
corpus was collected by Ng and colleagues (Ng 
and Lee, 1996) and it is available from the 
Linguistic Data Consortium (LDC) 5. 
5LDC address: http://www. Idc.upeaa. ed~/ 
174 
The D50 corpus contains sentences from 
two different corpora, namely Wall Street 
Journal (WSJ) and Brown Corpus (BC). 
Therefore, it is easy to perform experiments 
about the portability of alternative systems 
by training them on the WSJ part and testing 
them on the BE part, or vice-versa. Here- 
inafter, the WSJ part of DSO will be referred 
to as corpus A, and the BC part to as corpus B. 
At a word level, we force the number of exam- 
ples of corpus A and B be the same 6 in order 
to have symmetry and allow the comparison 
in both directions. 
From these corpora, a group of 21 words 
which frequently appear in the WSD litera- 
ture has been selected to perform the com- 
parative experiments (each word is treated 
as a different classification problem). These 
words are 13 nouns (age, art, body, car, child, 
cost, head, interest, line, point, state, thing, 
work) and 8 verbs (become, fall, grow, lose, 
set, speak, strike, tell). Table 1 contains in- 
formation about the number of examples, the 
number of senses, and the percentage of the 
most frequent sense (MF5) of these reference 
words, grouped by nouns, verbs, and all 21 
words. 
3.2 At t r ibutes  
Two kinds of information are used to perform 
disambiguation: local and topical context. 
Let "... w-3 w-2 w-1 w W+l w+2 w+3..." 
be the context of consecutive words around 
the word w to be disambiguated, and p?, 
( -3  < i _< 3)be  the part-of-speech tag 
of word w?~. Attributes referring to local 
context are the following 15: P-3, P-2, 
P- l ,  P+i, P+2, P+3, w- l ,  W+l, (W-2,W-1), 
(w-i.w+i), (w+l,w+2), 
(w-2, W-l, w+l), (w-i ,  w+l, w+2), and 
(w+l,w+2, w+3), where the last seven cor- 
respond to collocations of two and three 
consecutive words. 
The topical context is formed by Cl,..., Cm, 
which stand for the unordered set of open class 
words appearing in the sentence 7. 
The four methods tested translate this 
information into features in different ways. 
Snow and LB algorithms require binary fea- 
6This is achieved by ramdomly reducing the size of 
the largest corpus to the size of the smallest. 
7The already described set of attributes contains 
those attributes used in (Ng and Lee, 1996), with the 
exception of the morphology of the target word and 
the verb-object syntactic relation. 
tures. Therefore, local context attributes have 
to be binarized in a preprocess, while the top- 
ical context attributes remain as binary tests 
about the presence/absence of a concrete word 
in the sentence. As a result the number of 
attributes is expanded to several thousands 
(from 1,764 to 9,900 depending on the partic- 
ular word). 
The binary representation of attributes is 
not appropriate for NB and EB algorithms. 
Therefore, the 15 local-context attributes are 
taken straightforwardly. Regarding the binary 
topical-context attributes, we have used the 
variants described in (Escudero et al, 2000b). 
For EB, the topical information is codified as 
a single set-valued attribute (containing all 
words appearing in the sentence) and the cal- 
culation of closeness is modified so as to han- 
dle this type of attribute. For NB, the top- 
ical context is conserved as binary features, 
but when classifying new examples only the 
information of words appearing in the exam- 
ple (positive information) is taken into ac- 
count. In that paper, these variants are called 
positive Exemplar-based (PEB) and positive 
Naive Bayes (PNB), respectively. PNB and 
PEB algorithms are empirically proven to per- 
form much better in terms of accuracy and 
efficiency in the WSD task. 
3.3 Exper imenta l  Methodo logy  
The comparison of algorithms has been per- 
formed in series of controlled experiments us- 
ing exactly the same training and test sets. 
There are 7 combinations of training-test sets 
called: A+B-A+B, A+B-A, A+B-B, A-A, B- 
B, A-B, and B-A, respectively. In this nota- 
tion, the training set is placed at the left hand 
side of symbol "-", while the test set is at the 
right hand side. For instance, A-B means that 
the training set is corpus A and the test set 
is corpus B. The symbol "+" stands for set 
union, therefore A+B-B means that the train- 
ing set is A union B and the test set is B. 
When comparing the performance oftwo al- 
gorithms, two different statistical tests of sig- 
nificance have been apphed depending on the 
case. A-B and B-A combinations represent a 
single training-test experiment. In this cases, 
the McNemar's test of significance is used 
(with a confidence value of: X1,0.952 = 3.842), 
which is proven to be more robust than a sim- 
ple test for the difference of tw0_proportions. 
In the other combinations, a 10-fold cross- 
validation was performed in order to prevent 
175 
nouns 
verbs 
AorB  
examples 
rain 
senses 
min max avg 
A 
i MFS (%) 
min min 
senses 
B 
MFS (%! 
min max max avg max avg max avg avg 
122 714 420 2 24 7.7 37.9 90.7 59.8 3 24 8.8 21.0 87.7 45.3 
101 741 369 4 13 8.9120.8 81.6 49.3 4 14 11.4 28.0 71.7 46.3 
101 741 401 2 24 8.1 J20.8 90.7 56.1 3 24 9.8 21.0 87.7 45.6 
Table 1: Information about the set of 21 words of reference. 
testing on the same material used for training. 
In these cases, accuracy/error rate figures re- 
ported in section 4 are averaged over the re- 
sults of the 10 folds. The associated statistical 
tests of significance is a paired Student's t-test 
with a confidence value of: t9,0.975 = 2.262. 
Information about both statistical tests can 
be found at (Dietterich, 1998). 
4 Exper iments  
4.1 F i rs t  Exper iment  
Table 2 shows the accuracy figures of the four 
methods in all combinations of training and 
test sets . Standard deviation numbers are 
supplied in all cases involving cross valida- 
tion. M FC stands for a Most-Frequent-sense 
Classifier, that is, a naive classifier that learns 
the most frequent sense of the training set 
and uses it to classify all examples of the test 
set. Averaged results are presented for nouns. 
verbs, and overall, and the best results for 
each case are printed in boldface. 
The following conclusions can be drawn: 
? LB outperforms all other methods in 
all cases. Additionally, this superiority 
is statistically significant, except when 
comparing LB to the PEB approach in the 
cases marked with an asterisk. 
? Surprisingly, LB in A+B-A (or A+B-B) 
does not achieve substantial improvement 
to the results of A-A (or B-S) w in  fact, 
the first variation is not statistically sig- 
nificant and the second is only slightly 
significant. That is, the addition of extra 
examples from another domain does not 
necessarily contribute to improve the re- 
sults on the original corpus. This effect is 
also observed in the other methods, spe- 
cially in some cases (e.g. Snow in A+B-A 
vs. A-A) in which the joining of both 
training corpora is even counterproduc- 
tive. 
SThe second and third column correspond to the 
train and test sets used by (Ng and Lee, 1996; Ng, 
1997a) 
? Regarding the portability of the systems, 
very disappointing results are obtained. 
Restricting to \[B results, we observe that 
the accuracy obtained in A-B is 47.1% 
while the accuracy in B-B (which can 
be considered an upper bound for LB in 
B corpus) is 59.0%, that is, a drop of 
12 points. Furthermore, 47.1% is only 
slightly better than the most frequent 
sense in corpus B, 45.5%. The compari- 
son in the reverse direction is even worse: 
a drop from 71.3% (A-A) to 52.0% (B- 
A), which is lower than the most frequent 
sense of corpus A, 55.9%. 
4.2 Second Exper iment  
The previous experiment shows that classi- 
tiers trained on the A corpus do not work well 
on the B corpus, and vice-versa. Therefore, 
it seems that some kind of tuning process is 
necessary to adapt supervised systems to each 
new domain. 
This experiment explores the effect of a sim- 
ple tuning process consisting of adding to the 
original training set a relatively small sarn- 
ple of manually sense tagged examples of the 
new domain. The size of this supervised por- 
tion varies from 10% to 50% of the available 
corpus in steps of 10% (the remaining 50% is 
kept for testing). This set of experiments will 
be referred to as A+%B-B, or conversely, to 
B+%A-A. 
In order to determine to which extent the 
original training set contributes to accurately 
disambiguate in the new domain, we also cal- 
culate the results for %A-A (and %B-B), that 
is, using only the tuning corpus for training. 
Figure 1 graphically presents the results ob- 
tained by all methods. Each plot contains the 
X+%Y-Y and %Y-Y curves, and the straight 
lines corresponding to the lower bound MFC, 
and to the upper bounds Y-Y and X+Y-Y. 
As expected, the accuracy of all methods 
grows (towards the upper bound) as more tun- 
ing corpus is added to the training set. How- 
ever, the relation between X+%Y-Y and %Y- 
Y reveals some interesting facts. In plots 2a, 
176 
nouns 
MFC verbs 
total 
nouns 
PNB verbs 
total  
nouns  
PEB verbs 
total 
nouns  
Snow verbs 
total 
nouns 
LB verbs 
total 
A+B-A+B 
46.59?1.08 
46.49?1.37 
46.55?0.71 
62.29?1.25 
60.18?1.64 
61.55?1.04 
62.66?0.87 
63.67?1.94 
63.01?0.93 
61.24?1.14 
60.35?1.57 
60.92?1.09 
66.00?1.47 
66.91?2.25 
66.32?1.34 
A+B-A 
56.68?2.79 
48.74?1.98 
53.90?2.01 
68.89?0.93 
64.21?2.26 
67.25?1.07 
69.45?1 51 
68.39?3.25 
69.08?1.66 
66.36?1 57 
64.11?2.76 
65.57?1.33 
2.09?1.61 
71.23?2.99 
71.79?1.51 
Accuracy (%) 
A+B-B 
36.49?2.41 
44.23?2.67 
39.21?1.90 
55.69?1.94 
56.14?2.79 
55.85?1.81 
56.09?1.12 
58.58?2.40 
56.97?1.22 
56.11?1.45 
56.58?2.45 
56.28?1.10 
59.92?1.93 
62.58?2.93 
60.85?1.81 
A-A 
59.77?1.44 
48.85?2.09 
55.94?1.10 
66.93?1.44 
63.87?1.80 
65.86?1.11 
69.38?1.24 
68.25?2.84 
68.98?1.06 
68.85?1.36 
63.91?1.51 
67.12?1.16 
71.69?1.54 
70.45?2.14" 
71.26?1.15 
B-B \[ A-B B-A 
45.28?1.81 33.97 39.46 
45.96?2.6O 40.91 37.31 
45.52?1.27 36.40 38.71 
56.17?1.60 36.62 45.99 
57.97?2.86 50.20 50.75 
56.80?1.12 41.38 47.66 
56.17?1.80 42.15 50.53 
59.57?2.86 51.19 52.24 
57.36?1.68 45.32 51.13 
56.55?1.31 42.13 49.96 
55.36?3.27 47.66 49.39 
56.13?1.23 44.07 49.76 
58.33?2.26 43.92 51.28" 
60.14?3.43" 52.99 53.29* 
58.96?1.86 47.10 51.99" 
Table 2: Accuracy results (:i: standard eviation) of the methods on all training-test combina- 
tions 
3a, and lb the contribution of the original 
training corpus is null. Furthermore, in plots 
la, 2b, and 3b a degradation on the accuracy 
performance is observed. Summarizing, these 
six plots show that for Naive Bayes, Exemplar 
Based, and Snow methods it is not worth keep- 
ing the original training examples. Instead, a 
better (but disappointing) strategy would be 
simply using the tuning corpus. 
However, this is not the situation of Lazy- 
Boosting (plots 4a and 4b), for which a mod- 
erate (but consistent) improvement of accu- 
racy is observed when retaining the original 
training set. Therefore, Lazy\[3oosting shows 
again a better behaviour than their competi- 
tors when moving from one domain to an- 
other. 
4.3 Th i rd  Exper iment  
The bad results about portability could be ex- 
plained by, at least, two reasons: 1) Corpus 
A and \[3 have a very different distribution of 
senses, and, therefore, different a-priori bi- 
ases; 2) Examples of corpus A and \[3 con- 
tain different information, and, therefore, the 
learning algorithms acquire different (and non 
interchangeable) classification cues from both 
corpora,. 
The first hypothesis confirmed by observ- 
ing the bar plots of figure 2, which contain the 
distribution of the four most frequent senses 
of some sample words in the corpora A and 
B. respectively. In order to check the second 
hypothesis, two new sense-balanced corpora 
have been generated from the DSO corpus, by 
equilibrating the number of examples of each 
sense between A and B parts. In this way, the 
first difficulty is artificially overrided and the 
algorithms hould be portable if examples of 
both parts are quite similar. 
Table 3 shows the results obtained by Lazy- 
Boosting on these new corpora. 
Regarding portability, we observe a signifi- 
cant accuracy decrease of 7 and 5 points from 
A-A to B-A, and from B-B to A-B, respec- 
tively 9. That is, even when the sazne distri- 
bution of senses is conserved between training 
and test examples, the portability of the su- 
pervised WSD systems is not guaranteed. 
These results imply that examples have to 
be largely different from one corpus to an- 
other. By studying the weak rules generated 
by kazyBoosting in both cases, we could cor- 
roborate this fact. On the one hand, the type 
of features used in the rules were significantly 
different between corpora, and, additionally, 
there were very few rules that apply to both 
sets; On the other hand, the sign of the pre- 
diction of many of these common rules was 
somewhat contradictory between corpora. 
9This loss in accuracy is not as important as m the 
first experiment, due to the simplification provided by 
the balancing ofsense distributions. 
177 
Naive Bayes 
Exemplar Based 
Snow 
LazyBoosting 
58 
56 1 
54 
~?52 
o 
50 
44 
4O 
58 
56 
Af~ 
52 
~o 
~ 48 
46  
58 
56 ' 
54 
o 52 
50 
46 
62 
60 ' 
58  
~o 
48 
46  
4.4 
Test on B corpus 
(la) 
. . . . .  -MF'~ . . . .  
o B,-B 
A+B-B o 
A+%B-B 
%B-B . . . . .  
/ . f "  
5 10 15 20 25 30 35 40 45 50 
(2a) 
. . . . . . .  ? ; . . . .  ; . . . . . .  ~ --:-" ~S-- :~. : : , - -  ~ 
B-B --- 
A+B-B o 
A+%B-B * - -  
%B-B - "u ' -  
/+.- 
J 
5 10 15 20 25 30 355 40 45 50 
(3a) 
MFS 
A+B-B o 
A+%B-B ~- -  
%B-B ==- 
5 10 15 20 25 30 35 40 45 50 
(4a) 
B-B  ~ - -  
A+%B-B . . . .  
%B-B - - - -  
/ /+  / / " "  - 
-/ , /  
.+ 
$+* 
, , = , = , i , , 
5 10 15 20 25 30 35 40 45 50 
72 
70 
68 
~66 
~,64 
62 
60 
58 
56 
54 
72 
70 
68 
66 
64 
62. 
60 
58 
56 
54 
72 
7O 
68 ~66 
60 
58 
56 
54 
72 
7O 
68 
o  .64 
~: eo 
58 
56 
54 
Test on A corpus 
(lb) 
MFS 
A-A - - - -  
B+A-A 
B+%A-A ~ - -  
o o . %,~oA -~- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
/ 
, , , , , . . . .  
5 10 15 20 25 30 35 40 45 50 
(2b) 
MFS 
A-A~- -  
B+A-A . . . . . . . . . . . . . . . . . . .  ~ . . . . . .  = . -  ~. . , .Z~:~. . .  ~ ~.~ 
%A-A . . . .  
_~. - -  
+ 
+ ? , , , , , , , , , 
5 10 15 20 25 30 35 40 45 50 
(3b) 
MF$ 
A-A . . . .  
B+A-A a 
B+%A-A ~-  
%A-A . . . . .  
j." 
/ 
5 10 15 20 25 30 ,35 40 45 50 
(4b) 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  f~tE~ . . . . .  
A-A . . . .  
B+A=A 
B+%A-A . . . .  
/P  +/"  
+" 1" 
/ 
/ 
/ 
/,' 
, , , , , , , , i 
5 10  15  20  25  30  35  40  45  50  
Figure 1: Results of the tuning experiment 
5 Conc lus ions  and  Fur ther  Work  
This work has pointed out some difficulties 
regarding the portability of supervised WSD 
systems, a very important issue that has been 
paid little attention up to the present. 
According to our experiments, it seems that 
the performance of supervised sense taggers is 
not guaranteed when moving from one domain 
to another (e.g. from a balanced corpus, such 
as BC, to an economic domain, such as WSJ). 
These results implies that some_kind of adap- 
tation is required for cross-corpus application. 
178 
. . . . .  il ii\[ 
"~ head ~a I r l temt  ~o fa l l  oo grow 
mL. 
Figure 2: Distribution of the four most frequent senses for two nouns (head, interest) and two 
verbs (line, state). Black bars = A corpus; Grey bars = B corpus 
nouns 
MFC verbs 
total 
nouns 
LB verbs 
total 
Accuracy (%) 
A+B-A+B A+B-B A-A 
48.75?0.91 
48.22?1 68 
48.55?1 6 
62.82?1.43 
66.82?1.53 
64.35?1.16 
A+B-A 
48.90?1.69 
48.22?1.90 
48.64?1.04 
64.26?2.07 
69.33?2.92 
66.20?2.12 
48.61?0.96 
48.22?3 06 
48.46?1.21 
61.38?2.08 
64.32?3.27 
62.50?1.47 
48.87?1 68 
48.22?1.90 
48.62?1.09 
63.19?1.65 
68.51?2.45 
65.22?1.50 
B-B A-B B-A 
48.61?0.96 48.99 48.99 
48.22?3.06 48.22 48.22 
48.46?1.21 48.70 48.70 
60.65?1.01 53.45 55.27 
63.49?2.27 60.44 62.55 
61.74?1.18 56.12 58.05 
Table 3: Accuracy results (5= standard eviation) of LazyBoosting on the sense-balanced corpora 
Furthermore, these results are in contradic- 
tion with the idea of "robust broad-coverage 
WSD" introduced by (Ng, 1997b), in which a 
supervised system trained on a large enough 
corpora (say a thousand examples per word) 
~hould provide accurate disambiguation on 
any corpora (or, at least significantly better 
than MFS). 
Consequently, it is our belief that a number 
of issues regarding portability, tuning, knowl- 
edge acquisition, etc., should be thoroughly 
studied before stating that the supervised ML 
paradigm is able to resolve a realistic WSD 
problem. 
Regarding the M L algorithms tested, the 
contribution of this work consist of empiri- 
cally demonstrating that the LazyBoosting al- 
gorithm outperforms other three state-of-the- 
art supervised ML methods for WSD. Further- 
more. this algorithm is proven to have better 
properties when is applied to new domains. 
Further work is planned to be done in the 
following directions: 
? Extensively evaluate LazyBoosting on the 
WSD task. This would include tak- 
ing into account additional/alternative 
attributes and testing the algorithm in 
other corpora --specially on sense-tagged 
corpora automatically obtained from In- 
ternet or large text collections using non- 
supervised methods (Leazock et al, 1998; 
Mihalcea and Moldovan, 1999). 
? Since most of the knowledge l arned from 
a domain is not useful when changing 
to a new domain, further investigation is 
needed on tuning strategies, pecially on 
those using non-supervised algorithms. 
? It is known that mislabelled examples re- 
sulting from annotation errors tend to be 
hard examples to classify correctly, and, 
therefore, tend to have large weights in 
the final distribution. This observation 
allows both to identify the noisy exam- 
ples and use LazyBoosting as a way to 
improve data quality. Preliminary exper- 
iments have been already carried out in 
this direction on the DSO corpus. 
? Moreover, the inspection of the rules 
learned by kazyBoosting could provide 
evidence about similar behaviours of a- 
priori different senses. This type of 
knowledge could be useful to perform 
clustering of too fine-grained or artificial 
senses. 
Re ferences  
E. Agirre and D. Martinez. 2000. Decision Lists 
and Automatic Word Sense Disambiguation. In
Proceedings o\] the COLING Workshop on Se- 
mantic Annotation and Intelligent Content 
D. Aha, D. Kibler, and M. Albert. 1991. Instance- 
based Learning Algorithms. Machine Learning, 
7:37-66. 
R. F. Bruce and J. M. Wiebe. 1999. Decompos- 
179 
able Modeling in Natural Language Processing. 
Computatwnal Linguistics. 25(2):195-207. 
S. Cost and S. Salzberg. 1993. A weighted nearest 
neighbor algorithm for learning with symbolic 
features. Machine Learning, 10(1), 57-78. 
W. Daelemans, A. van den Bosch, and J. Zavrel. 
1999. Forgetting Exceptions is Harmful in Lan- 
guage Learning. Machine Learning, 34:11-41. 
T. G. Dietterich. 1998. Approximate Statisti- 
cal Tests for Comparing Supervised Classifi- 
cation Learning Algorithms. Neural Computa- 
tion, 10(7). 
R. O. Duda and P. E. Hart. 1973. Pattern Clas- 
sificatwn and Scene Analysis. Wiley. 
G. Escudero, L. M~rquez, and G. Rigau. 2000a. 
Boosting Applied to Word Sense Disam- 
biguation. In Proceedings of the 12th Euro- 
pean Conference on Machine Learning, ECML, 
Barcelona, Spain. 
G. Escudero. L. M~rquez, and G. Rigau. 2000b. 
Naive Bayes and Exemplar-Based Approaches 
to Word Sense Disambiguation Revisited. In 
To appear in Proceedings of the 14th European 
Conference on Artificial Intelligence, ECAI. 
G. Escudero, L. M~quez, and G. Rigau. 2000c. 
On the Portability and Tuning of Super- 
vised Word Sense Disambiguation Systems. Re- 
search Report LSI-00-30-R, Software Depart- 
ment (LSI). Technical University of Catalonia 
(UPC). 
A. Fujii, K. Inui. T. Tokunaga, and H. Tanaka. 
1998. Selective Sampling for Example-based 
W'ord Sense Disambiguation. Computatwnal 
Linguistics, 24(4):573-598. 
W. Gale, K. W. Church, and D. Yarowsky. 1992a. 
A Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26:415-439. 
W. Gale, K. W. Church, and D. Yarowsky. 1992b. 
Estimating Upper and Lower Bounds on the 
Performance of Word Sense Disambiguation. 
In Proceedings of the 30th Annual Meeting of 
the Association for Computational Linguistics. 
ACL. 
N. Ide and J. V@ronis. 1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguis- 
tics, 24(1):1-40. 
A. Kilgarriff and J. Rosenzweig. 2000. English 
SENSEVAL: Report and Results. In Proceed- 
ings of the 2nd International Conference on 
Language Resources and Evaluatwn, LREC, 
Athens, Greece. 
C. Leacock, M. Chodorow, and G. A. Miller. 1998. 
Using Corpus Statistics and WordNet Relations 
for Sense Identification. Computatwnal Lin- 
guistwcs, 24(1):147-166. 
N. Littlestone. 1988. Learning Quickly when Irrel- 
evant Attributes Abound. Machine Learning, 
2:285-318. 
R. Mihalcea and I. Moldovan. 1999. An Au- 
tomatic Method for Generating Sense Tagged 
Corpora. In Proceedings of the 16th National 
Conference on Artificial Intelligence. AAAI 
Press. 
R. J. Mooney. 1996. Comparative Experiments 
on Disambiguating Word Senses: An Illustra- 
tion of the Role of Bias in Machine Learning. 
In Proceedings of the 1st Conference on Empir- 
ical Methods m Natural Language Processing, 
EMNLP. 
H. T. Ng and H. B. Lee. 1996. Integrating Multi- 
ple Knowledge Sources to Disambiguate Word 
Sense: An Exemplar-based Approach. In Pro- 
ceedmgs of the 3~th Annual Meeting of the As- 
sociation for Computational Linguistics. ACL. 
H. T. Ng. 1997a. Exemplar-Base Wbrd Sense Dis- 
ambiguation: Some Recent Improvements. In 
Proceedings of the 2nd Conference on Empir- 
zcal Methods in Natural Language Processing, 
EMNLP. 
H. T. Ng. 1997b. Getting Serious about Word 
Sense Disambiguation. In Proceedings of the 
ACL SIGLEX Workshop: Tagging Text with 
Lexical Semantics: Why, what and how?, Wash- 
ington, USA. 
D. Roth. 1998. Learning to Resolve Natural Lan- 
guage Ambiguities: A Unified Approach. In 
Proceedings of the National Conference on Ar- 
tzficial Intelhgence, AAAI 'Y8, July. 
R. E. Schapire and Y. Singer. to appear. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning. Also appearing 
in Proceedings of the 11th Annual Conference on 
Computatzonal Learning Theory, 1998. 
S. Sekine. 1997. The Domain Dependence ofPars- 
ing. In Proceedings o\] the 5th Conference on 
Applied Natural Language Processing, ANLP, 
Washington DC. ACL. 
G. Towell and E. M. Voorhees. 1998. Disam- 
biguating Highly Ambiguous Words. Computa- 
tional Lingu~stzcs. 24(1):125-146. 
D. Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent 
Restoration i  Spanish and French. In Proceed- 
ings of the 32nd Annual Meeting of the Associ- 
ation for Computational Linguistics, pages 88- 
95, Las Cruces, NM. ACL. 
180 
 	
	ff 
		ffA Simple Named Entity Extractor using AdaBoost
Xavier Carreras and Llu??s Ma`rquez and Llu??s Padro?
TALP Research Center
Departament de Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
{carreras,lluism,padro}@lsi.upc.es
1 Introduction
This paper presents a Named Entity Extraction (NEE)
system for the CoNLL-2003 shared task competition. As
in the past year edition (Carreras et al, 2002a), we have
approached the task by treating the two main sub?tasks of
the problem, recognition (NER) and classification (NEC),
sequentially and independently with separate modules.
Both modules are machine learning based systems, which
make use of binary and multiclass AdaBoost classifiers.
Named Entity recognition is performed as a greedy se-
quence tagging procedure under the well?known BIO la-
belling scheme. This tagging process makes use of three
binary classifiers trained to be experts on the recognition
of B, I, and O labels, respectively. Named Entity classifi-
cation is viewed as a 4?class classification problem (with
LOC, PER, ORG, and MISC class labels), which is straight-
forwardly addressed by the use of a multiclass learning
algorithm.
The system presented here consists of a replication,
with some minor changes, of the system that obtained the
best results in the CoNLL-2002 NEE task. Therefore, it
can be considered as a benchmark of the state?of?the?
art technology for the current edition, and will allow also
to make comparisons about the training corpora of both
editions.
2 Learning the Decisions
We use AdaBoost with confidence rated predictions as
learning algorithm for the classifiers involved in the sys-
tem. More particularly, the basic binary version has been
used to learn the I, O, and B classifiers for the NER
module, and the multiclass multilabel extension (namely
AdaBoost.MH) has been used to perform entity classifi-
cation.
The idea of these algorithms is to learn an accurate
strong classifier by linearly combining, in a weighted vot-
ing scheme, many simple and moderately?accurate base
classifiers or rules. Each base rule is learned sequen-
tially by presenting the base learning algorithm a weight-
ing over the examples, which is dynamically adjusted de-
pending on the behavior of the previously learned rules.
AdaBoost has been applied, with significant success, to
a number of problems in different areas, including NLP
tasks (Schapire, 2002). We refer the reader to (Schapire
and Singer, 1999) for details about the general algorithms
(for both the binary and multiclass variants), and (Car-
reras and Ma`rquez, 2001; Carreras et al, 2002b) for par-
ticular applications to NLP domains.
In our setting, the boosting algorithm combines sev-
eral small fixed?depth decision trees, as base rules. Each
branch of a tree is, in fact, a conjunction of binary fea-
tures, allowing the strong boosting classifier to work with
complex and expressive rules.
3 Feature Representation
A window W anchored in a word w represents the local
context of w used by a classifier to make a decision on
that word. In the window, each word around w is cod-
ified with a set of primitive features, together with its
relative position to w. Each primitive feature with each
relative position and each possible value forms a final bi-
nary feature for the classifier (e.g., ?the word form at
position(-2) is street?). The kind of information coded
in those features may be grouped in the following kinds:
? Lexical: Word forms and their position in the win-
dow (e.g., W (3)=?bank?). When available, word
lemmas and their position in the window.
? Syntactic: Part-of-Speech tags and Chunk tags.
? Orthographic: Word properties with regard to how
is it capitalized (initial-caps, all-caps), the kind
of characters that form the word (contains-digits,
all-digits, alphanumeric, roman-number), the pres-
ence of punctuation marks (contains-dots, contains-
hyphen, acronym), single character patterns (lonely-
initial, punctuation-mark, single-char), or the mem-
bership of the word to a predefined class (functional-
word1), or pattern (URL).
? Affixes: The prefixes and suffixes of the word (up to
4 characters).
? Word Type Patterns: Type pattern of consecutive
words in the context. The type of a word is ei-
ther functional (f), capitalized (C), lowercased (l),
punctuation mark (.), quote (?) or other (x). For
instance, the word type pattern for the phrase ?John
Smith payed 3 euros? would be CClxl.
? Left Predictions: The {B,I,O} tags being predicted
in the current classification (at recognition stage), or
the predicted category for entities in left context (at
classification stage).
? Bag-of-Words: Form of the words in the window,
without considering positions (e.g., ?bank?? W ).
? Trigger Words: Triggering properties of window
words. An external list is used to determine whether
a word may trigger a certain Named Entity (NE)
class (e.g., ?president? may trigger class PER).
? Gazetteer Features: Gazetteer information for win-
dow words. An external gazetteer is used to deter-
mine possible classes for each word.
4 The NER Module
The Named Entity recognition task is performed as a
combination of local classifiers which test simple deci-
sions on each word in the text.
According to a BIO labelling scheme, each word is
tagged as either the beginning of a NE (B tag), a word
inside a NE (I tag), or a word outside a NE (O tag).
We use three binary classifiers for the tagging, one cor-
responding to each tag. All the words in the train set are
used as training examples, applying a one-vs-all binariza-
tion. When tagging, the sentence is processed from left to
right, greedily selecting for each word the tag with maxi-
mum confidence that is coherent with the current solution
(e.g., O tags cannot be followed by I tags). Despite its
simplicity, the greedy BIO tagging performed very well
for the NER task. Other more sophisticated represen-
tations and tagging schemes, studied in the past edition
(Carreras et al, 2002a), did not improve the performance
at all.
The three classifiers use the same information to codify
examples. According to the information types introduced
in section 3, all the following features are considered for
each target word: lexical, syntactic, orthographic, and
affixes in a {-3,+3} window; left predictions in a {-3,-1}
1Functional words are determiners and prepositions which
typically appear inside NEs.
window; and all the word type patterns that cover the 0
position in a {-3,+3} window.
The semantic information represented by the rest
of features, namely bag-of-words, trigger words, and
gazetteer features, did not help the recognition of
Named Entities, and therefore was not used.
5 The NEC Module
NEC is regarded as a classification task, consisting of as-
signing the NE type to each already recognized NE. In
contrast to the last year system, the problem has not been
binarized and treated in an ECOC (error correcting out-
put codes) combination scheme. Instead, the multiclass
multilabel AdaBoost.MH algorithm has been used. The
reason is that although ECOC provides slightly better re-
sults, its computational cost is also much higher than the
required for AdaBoost.MH.
The algorithm has been employed with different pa-
rameterizations, by modeling NEC either as a three-class
classification problem (in which MISC is selected only
when the entity is negatively classified as PER, ORG and
LOC) or as a four-class problem, in which MISC is just
one more class. The latter turned out to be the best choice
(with very significant differences).
The window information described in section 3 is
used in the NEC module computing all features for a
{-3,+3} window around the NE being classified, ex-
cept for the bag-of-words group, for which a {-5,+5}
window is used. Information relative to orthographic,
left predictions, and bag-of-words features is straight-
forwardly coded as described above, but other requires
further detail:
? Lexical features: Apart from word form and lemma
for each window position, two additional binary fea-
tures are used: One is satisfied when the focus NE
form and lemma coincide exactly, and the other
when they coincide after turning both of them into
lowercase.
? Syntactic features: Part-of-Speech (PoS) and
Chunk tags of window words (e.g., W (3).PoS=NN).
PoS and Chunk pattern of the NE (e.g.,
NNPS POS JJ for the NE ?People ?s Daily?)
? Affix features: Prefixes and suffixes of all window
words. Prefixes and suffixes of the NE being classi-
fied and of its internal components (e.g., considering
the entity ?People ?s Daily?, ?ly? is taken as a suf-
fix of the NE, ?ple? is taken as a suffix of the first
internal word, etc.).
? Trigger Words: Triggering properties of window
words (e.g., W (3).trig=PER). Triggering properties
of components of the NE being classified (e.g., for
the entity ?Bank of England? we could have a fea-
ture NE(1).trig=ORG). Context patterns to the left
of the NE, where each word is marked with its trig-
gering properties, or with a functional?word tag if
appropriate (e.g., the phrase ?the president of United
States?, would produce the pattern f ORG f for the
NE ?United States?, assuming that the word ?presi-
dent? is listed as a possible trigger for ORG).
? Gazetteer Features: Gazetteer information for
the NE being classified and for its components
(e.g., for the entity ?Bank of England?, features
NE(3).gaz=LOC and NE.gaz=ORG would be acti-
vated if ?England? is found in the gazetteer as LOC
and ?Bank of England? as ORG, respectively.
? Additionally, binary features encoding the length in
words of the NE being classified are also used.
6 Experimental Setting
The list of functional words for the task has been automat-
ically constructed using the training set. The lowercased
words inside a NE that appeared more than 3 times were
selected as functional words for the language.
Similarly, a gazetteer was constructed with the NEs in
the training set. When training, only a random 40% of the
entries in the gazetteer were considered. Moreover, we
used external knowledge in the form of a list of trigger
words for NEs and an external gazetteer. These knowl-
edge sources are the same that we used in the last year
competition for Spanish NEE. The entries of the trigger?
word list were linked to the Spanish WordNet, so they
have been directly translated by picking the correspond-
ing synsets of the English WordNet. The gazetteer has
been left unchanged, assuming interlinguality of most of
the entries. The gazetteer provided by the CoNLL-2003
organization has not been used in the work reported in
this paper.
In all cases, a preprocess of attribute filtering was per-
formed in order to avoid overfitting and to speed?up
learning. All features that occur less than 3 times in the
training corpus were discarded.
For each classification problem we trained the corre-
sponding AdaBoost classifiers, learning up to 4,000 base
decision trees per classifier, with depths ranging from 1
(decision stumps) to 4. The depth of the base rules and
the number of rounds were directly optimized on the de-
velopment set. The set of unlabelled examples provided
by the organization was not used in this work.
7 Results
The described system has been applied to both languages
in the shared task, though German and English environ-
ments are not identical: The German corpus enables the
use of lemma features while English does not. Also, the
used trigger word list is available for English but not for
German.
The results of the BIO model for the NER task on
the development and test sets for English and German
are presented in table 1. As will be seen later for the
whole task, the results are systematically better for En-
glish than for German. As it can be observed, the be-
haviour on the development and test English sets is quite
different. While in the development set the NER mod-
ule achieves a very good balance between precision and
recall, in the test set the precision drops almost 4 points,
being the F1 results much worse. On the contrary, de-
velopment and test sets for German are much more sim-
ilar. In this case, recall levels obtained for the language
are much lower compared to precision ones. This fact is
indicating the difficulty for reliably detecting the begin-
nings of the Named Entities in German (all common and
proper nouns are capitalized). Probably, a non?greedy
tagging procedure would have the chance to improve the
recognition results.
Precision Recall F?=1
English devel. 95.65% 95.51% 95.58
English test 91.93% 94.02% 92.96
German devel. 88.15% 71.55% 78.99
German test 85.87% 72.61% 78.68
Table 1: Results of the BIO recognizer for the NER task
Regarding NEC task, optimal feature selection is dif-
ferent for each language: Chunk information is almost
useless in English (or even harmful, when combined with
PoS features), but useful in German. On the contrary, al-
though the use of left predictions for NEC is useful for
English, the lower accuracy of the German system ren-
ders those features harmful (they are very useful when
assuming perfect left predictions). Table 2 presents NEC
accuracy results assuming perfect recognition of entities.
English German
features accuracy features accuracy
basic 91.47% basic 79.02%
basic+P 92.14% basic+P 79.29%
basic+C 91.60% basic+C 79.04%
basic+PC 92.12% basic+PC 79.91%
basic+Pg 93.86% basic+PCg 81.54%
basic+PG 95.05% basic+PCG 85.12%
basic+PGT 95.14%
Table 2: NEC accuracy on the development set assuming
a perfect recognition of named entities
The basic feature set includes all lexical, orthographic,
affix and bag?of?words information. P stands for Part-of-
Speech features, C for chunking?related information, T
for trigger?words features and g/G for gazetteer?related
information2. In general, more complex features sets
yield better results, except for the C case in English, as
commented above.
Table 4 presents the results on the NEE task obtained
by pipelining the NER and NEC modules. The NEC
module used both knowledge extracted from the training
set as well as external sources such as the gazetteer or
trigger word lists.
Almost the same conclusions extracted from the NER
results apply to the complete task, although here the re-
sults are lower due to the cascade of errors introduced by
the two modules: 1) Results on English are definitely bet-
ter than on German; 2) Development and test sets present
a regular behaviour in German, while for English they are
significantly different. We find the latter particularly dis-
appointing because it is indicating that no reliable con-
clusions can be extracted about the generalization error
of the NEE system constructed, by testing it on a 3,000
sentence corpus. This may be caused by the fact that the
training set is no representative enough, or by a too biased
learning of the NEE system towards the development set.
Regarding particular categories, we can see that for En-
glish the results are not extremely dissimilar (F1 values
fall in a range of 10 points for each set), being LOC and
PER the most easy to identify and ORG and MISC the most
difficult. Comparatively, in the German case bigger dif-
ferences are observed (F1 ranges from 52.58 to 80.79 in
the test set), e.g., recognition of MISC entities is far worse
than all the rest. Another slight difference against English
is that the easiest category is PER instead of LOC.
In order to allow fair comparison with other systems,
table 3 presents the results achieved on the development
set without using external knowledge. The features used
correspond to the basic model plus Part-of-Speech infor-
mation (plus Chunks for German), plus a gazetteer build
with the entities appearing in the training corpus.
Precision Recall F?=1
English devel. 90.34% 90.21% 90.27
English test 83.19% 85.07% 84.12
German devel. 74.87% 60.77% 67.09
German test 74.69% 63.16% 68.45
Table 3: Overall results using no external knowledge
Acknowledgments
This research has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Dept. (Hermes, TIC2000-0335-C03-02; Pe-
2g refers to a gazetteer containing only entities appearing in
the training set while G includes also external knowledge
English devel. Precision Recall F?=1
LOC 95.33% 94.39% 94.86
MISC 89.94% 83.41% 86.55
ORG 86.98% 88.14% 87.56
PER 91.79% 94.68% 93.21
Overall 91.51% 91.37% 91.44
English test Precision Recall F?=1
LOC 88.14% 90.41% 89.26
MISC 82.02% 75.36% 78.54
ORG 78.40% 80.43% 79.41
PER 86.36% 91.65% 88.93
Overall 84.05% 85.96% 85.00
German devel. Precision Recall F?=1
LOC 75.72% 73.67% 74.68
MISC 72.34% 42.48% 53.52
ORG 76.89% 63.82% 69.75
PER 83.84% 68.88% 75.63
Overall 77.90% 63.23% 69.80
German test Precision Recall F?=1
LOC 70.31% 70.92% 70.61
MISC 64.91% 44.18% 52.58
ORG 71.70% 54.08% 61.65
PER 87.59% 74.98% 80.79
Overall 75.47% 63.82% 69.15
Table 4: Final results for English and German
tra - TIC2000-1735-C02-02). Xavier Carreras holds a
grant by the Catalan Government Research Department.
References
X. Carreras and L. Ma`rquez. 2001. Boosting Trees for
Clause Splitting. In Proceedings of the 5th CoNLL,
Tolouse, France.
X. Carreras, L. Ma`rquez, and L. Padro?. 2002a. Named
Entity Extraction using AdaBoost. In Proceedings of
the 6th CoNLL, Taipei, Taiwan.
X. Carreras, L. Ma`rquez, V. Punyakanok, and D. Roth.
2002b. Learning and Inference for Clause Identifica-
tion. In Proceedings of the 14th European Conference
on Machine Learning, ECML, Helsinki, Finland.
R. E. Schapire and Y. Singer. 1999. Improved Boosting
Algorithms Using Confidence-rated Predictions. Ma-
chine Learning, 37(3).
R. E. Schapire. 2002. The Boosting Approach to Ma-
chine Learning. An Overview. In Proceedings of the
MSRI Workshop on Nonlinear Estimation and Classi-
fication, Berkeley, CA.
Learning a Perceptron-Based Named Entity Chunker
via Online Recognition Feedback
Xavier Carreras and Llu??s Ma`rquez and Llu??s Padro?
TALP Research Center
Departament de Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
{carreras,lluism,padro}@lsi.upc.es
1 Introduction
We present a novel approach for the problem of Named
Entity Recognition and Classification (NERC), in the
context of the CoNLL-2003 Shared Task.
Our work is framed into the learning and inference
paradigm for recognizing structures in Natural Language
(Punyakanok and Roth, 2001; Carreras et al, 2002). We
make use of several learned functions which, applied
at local contexts, discriminatively select optimal partial
structures. On the top of this local recognition, an infer-
ence layer explores the partial structures and builds the
optimal global structure for the problem.
For the NERC problem, the structures to be recognized
are the named entity phrases (NE) of a sentence. First, we
apply learning at word level to identify NE candidates by
means of a Begin-Inside classification. Then, we make
use of functions learned at phrase level ?one for each
NE category? to discriminate among competing NEs.
We propose a simple online learning algorithm for
training all the involved functions together. Each function
is modeled as a voted perceptron (Freund and Schapire,
1999). The learning strategy works online at sentence
level. When visiting a sentence, the functions being
learned are first used to recognize the NE phrases, and
then updated according to the correctness of their solu-
tion. We analyze the dependencies among the involved
perceptrons and a global solution in order to design a
global update rule based on the recognition of named-
entities, which reflects to each individual perceptron its
committed errors from a global perspective.
The learning approach presented here is closely re-
lated to ?and inspired by? some recent works in the area
of NLP and Machine Learning. Collins (2002) adapted
the perceptron learning algorithm to tagging tasks, via
sentence-based global feedback. Crammer and Singer
(2003) presented an online topic-ranking algorithm in-
volving several perceptrons and ranking-based update
rules for training them.
2 Named-Entity Phrase Chunking
In this section we describe our NERC approach as a
phrase chunking problem. First we formalize the prob-
lem of NERC, then we propose a NE-Chunker.
2.1 Problem Formalization
Let x be a sentence belonging to the sentence space X ,
formed by n words xi with i ranging from 0 to n?1. Let
K be the set of NE categories, which in the CoNLL-2003
setting is K = {LOC, PER, ORG, MISC}.
A NE phrase, denoted as (s, e)k, is a phrase spanning
from word xs to word xe, having s ? e, with category
k ? K. Let NE be the set of all potential NE phrases,
expressed as NE = {(s, e)k | 0 ? s ? e, k ? K} .
We say that two different NE phrases ne1 = (s1, e1)k1
and ne2 = (s2, e2)k2 overlap, denoted as ne1?ne2 iff
e1 ? s2 ? e2 ? s1. A solution for the NERC problem
is a set y formed by NE phrases that do not overlap, also
known as a chunking. We define the set Y as the set of all
possible chunkings. Formally, it can be expressed as:
Y = {y ? NE | ?ne1, ne2 ? y ne16?ne2}
The goal of the NE extraction problem is to identify
the correct solution y ? Y for a given sentence x.
2.2 NE-Chunker
The NE-Chunker is a function which given a sentence
x ? X identifies the set of NE phrases y ? Y:
NEch : X ? Y
The NE-Chunker recognizes NE phrases in two lay-
ers of processing. In the first layer, a set of NE can-
didates for a sentence is identified, out of all the po-
tential phrases in NE . To do so, we apply learning at
word level in order to perform a Begin-Inside classifica-
tion. That is, we assume a function hB(w) which de-
cides whether a word w begins a NE phrase or not, and a
function hI(w) which decides whether a word is inside a
NE phrase or not. Furthermore, we define the predicate
BI?, which tests whether a certain phrase is formed by
a starting begin word and subsequent inside words. For-
mally, BI?((s, e)k) = (hB(s) ? ?i : s < i ? e : hI(i)).
The recognition will only consider solutions formed by
phases in NE which satisfy the BI? predicate. Thus, this
layer is used to filter out candidates from NE and conse-
quently reduce the size of the solution space Y . Formally,
the solution space that is explored can be expressed as
YBI? = {y ? Y | ?ne?y BI?(ne)}.
The second layer selects the best coherent set of NE
phrases by applying learning at phrase level. We assume
a number of scoring functions, which given a NE phrase
produce a real-valued score indicating the plausibility of
the phrase. In particular, for each category k ? K we as-
sume a function scorek which produces a positive score if
the phrase is likely to belong to category k, and a negative
score otherwise.
Given this, the NE-Chunker is a function which
searches a NE chunking for a sentence x according to
the following optimality criterion:
NEch(x) = arg max
y?YBI?
?
(s,e)k?y
scorek(s, e)
That is, among the considered chunkings of the sen-
tence, the optimal one is defined to be the one whose
NE phrases maximize the summation of phrase scores.
Practically, there is no need to explicitly enumerate each
possible chunking in YBI? . Instead, by using dynamic
programming the optimal chunking can be found in
quadratic time over the sentence length, performing a
Viterby-style exploration from left to right (Punyakanok
and Roth, 2001).
Summarizing, the NE-Chunker recognizes the set of
NE phrases of a sentence as follows: First, NE candidates
are identified in linear time, applying a linear number of
decisions. Then, the optimal coherent set of NE phrases
is selected in quadratic time, applying a quadratic number
of decisions.
3 Learning via Recognition Feedback
We now present an online learning strategy for training
the learning components of the NE-Chunker, namely the
functions hB and hI and the functions scorek, for k ? K.
Each function is implemented using a perceptron1 and
a representation function.
A perceptron is a linear discriminant function hw? :
Rn ? R parametrized by a weight vector w? in Rn.
Given an instance x? ? Rn, a perceptron outputs as
prediction the inner product between vectors x? and w?,
hw?(x) = w? ? x?.
1Actually, we use a variant of the model called the voted
perceptron, explained below.
The representation function ? : X ? Rn codifies an
instance x belonging to some space X into a vector inRn
with which the perceptron can operate.
The functions hB and hI predict whether a word begins
or is inside a NE phrase, respectively. Each one consists
of a perceptron weight vector, w?B and w?I, and a shared
representation function ?w, explained in section 4. Each
function is computed as hl = w?l ? ?w(x), for l ? {B, I},
and the sign is taken as the binary classification.
The functions scorek, for k ? K, compute a score for
a phrase (s, e) being a NE phrase of category k. For each
function there is a vector w?k, and a shared representation
function ?p, also explained in section 4. The score is
given by the expression scorek(s, e) = w?k ? ?p(s, e).
3.1 Learning Algorithm
We propose a mistake-driven online learning algorithm
for training the parameter vectors w? of each perceptron all
in one go. The algorithm starts with all vectors initialized
to 0?, and then runs repeatedly in a number of epochs T
through all the sentences in the training set. Given a sen-
tence, it predicts its optimal chunking as specified above
using the current vectors. If the predicted chunking is not
perfect the vectors which are responsible of the incorrect
predictions are updated additively.
The sentence-based learning algorithm is as follows:
? Input: {(x1, y1), . . . , (xm, ym)}.
? Define: W = {w?B, w?I} ? {w?k|k ? K}.
? Initialize: ?w? ? W w? = 0?;
? for t = 1 . . . T , for i = 1 . . .m :
1. y? = NEchW (xi)
2. learning feedback(W,xi, yi, y?)
? Output: the vectors in W .
We now describe the learning feedback. Let y? be the
gold set of NE phrases for a sentence x, and y? the set pre-
dicted by the NE-Chunker. Let goldB(i) and goldI(i) be
respectively the perfect indicator functions for the begin
and inside classifications, that is, they return 1 if word
xi begins or is inside some phrase in y? and 0 otherwise.
We differentiate three kinds of phrases in order to give
feedback to the functions being learned:
? Phrases correctly identified: ?(s, e)k ? y? ? y?:
? Do nothing, since they are correct.
? Missed phrases: ?(s, e)k ? y? \ y?:
1. Update begin word, if misclassified:
if (w?B ? ?w(xs) ? 0) then
w?B = w?B + ?w(xs)
2. Update misclassified inside words:
?i : s < i ? e : such that (w?I ? ?w(xi) ? 0)
w?I = w?I + ?w(xi)
3. Update score function, if it has been applied:
if (w?B ? ?w(xs) > 0 ?
?i : s < i ? e : w?I ? ?w(xi) > 0) then
w?k = w?k + ?p(s, e)
? Over-predicted phrases: ?(s, e)k ? y? \ y?:
1. Update score function:
w?k = w?k ? ?p(s, e)
2. Update begin word, if misclassified :
if (goldB(s) = 0) then
w?B = w?B ? ?w(xs)
3. Update misclassified inside words :
?i : s < i ? e : such that (goldI(i) = 0)
w?I = w?I ? ?w(xi)
This feedback models the interaction between the two
layers of the recognition process. The Begin-Inside iden-
tification filters out phrase candidates for the scoring
layer. Thus, misclassifying words of a correct phrase
blocks the generation of the candidate and produces a
missed phrase. Therefore, we move the begin or end
prediction vectors toward the misclassified words of a
missed phrase. When an incorrect phrase is predicted,
we move away the prediction vectors of the begin and in-
side words, provided that they are not in the beginning or
inside a phrase in the gold chunking. Note that we delib-
erately do not care about false positives begin or inside
words which do not finally over-produce a phrase.
Regarding the scoring layer, each category prediction
vector is moved toward missed phrases and moved away
from over-predicted phrases.
3.2 Voted Perceptron and Kernelization
Although the analysis above concerns the perceptron al-
gorithm, we use a modified version, the voted perceptron
algorithm, introduced in (Freund and Schapire, 1999).
The key point of the voted version is that, while train-
ing, it stores information in order to make better predic-
tions on test data. Specifically, all the prediction vec-
tors w?j generated after every mistake are stored, together
with a weight cj , which corresponds to the number of
decisions the vector w?j survives until the next mistake.
Let J be the number of vector that a perceptron accumu-
lates. The final hypothesis is an averaged vote over the
predictions of each vector, computed with the expression
hw?(x?) =
?J
j=1 c
j(w?j ? x?) .
Moreover, we work with the dual formulation of the
vectors, which allows the use of kernel functions. It is
shown in (Freund and Schapire, 1999) that a vector w can
be expressed as the sum of instances xj that were added
(sxj = +1) or subtracted (sxj = ?1) in order to create it,
as w =
?J
j=1 sxjx
j
. Given a kernel function K(x, x?),
the final expression of a dual voted perceptron becomes:
hw?(x?) =
J?
j=1
cj
j?
l=1
sxlK(x?
l, x?)
In this paper we work with polynomial kernels
K(x, x?) = (x ? x? + 1)d, where d is the degree of the
kernel.
4 Feature-Vector Representation
In this section we describe the representation functions
?w and ?p, which respectively map a word or a phrase
and their local context into a feature vector in Rn, partic-
ularly, {0, 1}n. First, we define a set of predicates which
are computed on words and return one or more values:
? Form(w), PoS(w): The form and PoS of word w.
? Orthographic(w): Binary flags of word w with re-
gard to how is it capitalized (initial-caps, all-caps),
the kind of characters that form the word (contains-
digits, all-digits, alphanumeric, Roman-number),
the presence of punctuation marks (contains-
dots, contains-hyphen, acronym), single character
patterns (lonely-initial, punctuation-mark, single-
char), or the membership of the word to a predefined
class (functional-word2), or pattern (URL).
? Affixes(w): The prefixes and suffixes of the word w
(up to 4 characters).
? Word Type Patterns(ws . . . we): Type pattern of
consecutive words ws . . . we. The type of a word
is either functional (f), capitalized (C), lowercased
(l), punctuation mark (.), quote (?) or other (x).
For instance, the word type pattern for the phrase
?John Smith payed 3 euros? would be CClxl.
For the function ?w(xi) we compute the predicates in
a window of words around xi, that is, words xi+l with
l ? [?Lw,+Lw]. Each predicate label, together with
each relative position l and each returned value forms a
final binary indicator feature. The word type patterns are
evaluated in all sequences within the window which in-
clude the central word i.
For the function ?p(s, e) we represent the context of
the phrase by evaluating a [?Lp, 0] window of predicates
at the s word and a separate [0,+Lp] window at the e
word. At the s window, we also codify the named enti-
ties already recognized at the left context, capturing their
category and relative position. Furthermore, we represent
the (s, e) phrase by evaluating the predicates without cap-
turing the relative position in the features. In particular,
2Functional words are determiners and prepositions which
typically appear inside NEs.
for the words within (s, e) we evaluate the form, affixes
and type patterns of sizes 2, 3 and 4. We also evaluate the
complete concatenated form of the phrase and the word
type pattern spanning the whole phrase. Finally, we make
use of a gazetteer to capture possible NE categories of the
whole NE form and each single word within it.
5 Experiments and Results
A list of functional words was automatically extracted
from each language training set, selecting those lower-
cased words within NEs appearing 3 times or more. For
each language, we also constructed a gazetteer with the
NEs in the training set. When training, only a random
40% of the entries was considered.
We performed parameter tuning on the English lan-
guage. Concerning the features, we set the window sizes
(Lw and Lp) to 3 (we tested 2 and 3) , and we did not con-
sidered features occurring less than 5 times in the data.
When moving to German, we found better to work with
lemmas instead of word forms.
Concerning the learning algorithm, we evaluated ker-
nel degrees from 1 to 5. Degrees 2 and 3 performed some-
what better than others, and we chose degree 2. We then
ran the algorithm through the English training set for up
to five epochs, and through the German training set for up
to 3 epochs. 3 On both languages, the performance was
still slightly increasing while visiting more training sen-
tences. Unfortunately, we were not able to run the algo-
rithm until performance was stable. Table 1 summarizes
the obtained results on all sets. Clearly, the NERC task
on English is much easier than on German. Figures indi-
cate that the moderate performance on German is mainly
caused by the low recall, specially for ORG and MISC en-
tities. It is interesting to note that while in English the
performance is much better on the development set, in
German we achieve better results on the test set. This
seems to indicate that the difference in performance be-
tween development and test sets is due to irregularities
in the NEs that appear in each set, rather than overfitting
problems of our learning strategy.
The general performance of phrase recognition system
we present is fairly good, and we think it is competitive
with state-of-the-art named entity extraction systems.
Acknowledgments
This research has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Dept. (Hermes, TIC2000-0335-C03-02; Pe-
tra - TIC2000-1735-C02-02). Xavier Carreras holds a
grant by the Catalan Government Research Department.
3Implemented in PERL and run on a Pentium IV (Linux,
2.5GHz, 512Mb) it took about 120 hours for English and 70
hours for German.
English devel. Precision Recall F?=1
LOC 90.77% 93.63% 92.18
MISC 91.98% 80.80% 86.03
ORG 86.02% 83.52% 84.75
PER 91.37% 90.77% 91.07
Overall 90.06% 88.47% 89.26
English test Precision Recall F?=1
LOC 86.66% 89.15% 87.88
MISC 84.90% 72.08% 77.97
ORG 82.73% 77.60% 80.09
PER 88.25% 86.39% 87.31
Overall 85.81% 82.84% 84.30
German devel. Precision Recall F?=1
LOC 75.21% 67.32% 71.05
MISC 76.90% 42.18% 54.48
ORG 76.80% 47.22% 58.48
PER 76.87% 60.96% 67.99
Overall 76.36% 55.06% 63.98
German test Precision Recall F?=1
LOC 72.89% 65.22% 68.84
MISC 67.14% 42.09% 51.74
ORG 77.67% 42.30% 54.77
PER 87.23% 70.88% 78.21
Overall 77.83% 58.02% 66.48
Table 1: Results obtained for the development and the
test data sets for the English and German languages.
References
X. Carreras, L. Ma`rquez, V. Punyakanok, and D. Roth.
2002. Learning and Inference for Clause Identifica-
tion. In Proceedings of the 14th European Conference
on Machine Learning, ECML, Helsinki, Finland.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments Perceptron Algorithms. In Proceedings of the
EMNLP?02.
K. Crammer and Y. Singer. 2003. A Family of Additive
Online Algorithms for Category Ranking. Journal of
Machine Learning Research, 3:1025?1058.
Y. Freund and R. E. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
V. Punyakanok and D. Roth. 2001. The Use of Clas-
sifiers in Sequential Inference. In Proceedings of the
NIPS-13.
Low?cost Named Entity Classification for Catalan: Exploiting
Multilingual Resources and Unlabeled Data
Llu??s Ma`rquez, Adria` de Gispert, Xavier Carreras, and Llu??s Padro?
TALP Research Center
Universitat Polite`cnica de Catalunya
Jordi Girona, 1?3, E-08034, Barcelona
 
lluism,agispert,carreras,padro  @talp.upc.es
Abstract
This work studies Named Entity Classi-
fication (NEC) for Catalan without mak-
ing use of large annotated resources of
this language. Two views are explored
and compared, namely exploiting solely
the Catalan resources, and a direct training
of bilingual classification models (Span-
ish and Catalan), given that a large col-
lection of annotated examples is available
for Spanish. The empirical results ob-
tained on real data point out that multi-
lingual models clearly outperform mono-
lingual ones, and that the resulting Cata-
lan NEC models are easier to improve by
bootstrapping on unlabelled data.
1 Introduction
There is a wide consensus about that Named Entity
Recognition and Classification (NERC) are Natural
Language Processing tasks which may improve the
performance of many applications, such as Informa-
tion Extraction, Machine Translation, Question An-
swering, Topic Detection and Tracking, etc. Thus,
interest on detecting and classifying those units in a
text has kept on growing during the last years.
Previous work in this topic is mainly framed in the
Message Understanding Conferences (MUC), de-
voted to Information Extraction, which included a
NERC competition task. More recent approaches
can be found in the proceedings of the shared task
at the 2002 and 2003 editions of the Conference
on Natural Language Learning (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003),
where several machine?learning (ML) systems were
compared at the NERC task for several languages.
One remarkable aspect of most widely used ML
algorithms is that they are supervised, that is, they
require a set of labelled data to be trained on. This
may cause a severe bottleneck when such data is not
available or is expensive to obtain, which is usu-
ally the case for minority languages with few pre?
existing linguistic resources and/or limited funding
possibilities. This is one of the main causes for
the recent growing interest on developing language?
independent NERC systems, which may be trained
from small training sets by taking advantage of un-
labelled examples (Collins and Singer, 1999; Abney,
2002), and which are easy to adapt to changing do-
mains (being all these aspects closely related).
This work focuses on exploring the construc-
tion of a low?cost Named Entity classification
(NEC) module for Catalan without making use of
large/expensive resources of the language. In doing
so, the paper first explores the training of classifi-
cation models by using only Catalan resources and
then proposes a training scheme, in which a Cata-
lan/Spanish bilingual classifier is trained directly
from a training set including examples of the two
languages. In both cases, the bootstrapping of the
resulting classifiers is also explored by using a large
unannotated Catalan corpus. The strategy used for
training the bilingual NE classification models has
been also applied with good results to NE recogni-
tion in (Carreras et al, 2003), a work that can be
considered complementary to this one.
When considering the training of bilingual mod-
els, we take advantage of the facts that Spanish
and Catalan are two Romance languages with sim-
ilar syntactic structure, and that ?since Spanish
and Catalan social and cultural environments greatly
overlap? many Named Entities appear in both lan-
guages corpora. Relying on this structural and con-
tent similarity, we will build our Catalan NE classi-
fier on the following assumptions: (a) Named Enti-
ties appear in the same contexts in both languages,
and (b) Named Entities are composed by similar pat-
terns in both languages.
The paper presents an extensive experimental
evaluation, giving strong evidence about the advan-
tage of using multilingual models for training on a
language with scarce resources. Additionally, the
Catalan NEC models resulting from the bilingual
training are easier to improve by bootstrapping on
unlabelled data.
The paper is organized as follows. Section 2
describes the Catalan and Spanish resources avail-
able and the feature codification of examples. Sec-
tion 3 briefly describes the learning algorithms used
to train the classifiers. Section 4 is devoted to the
learning of NEC modules using only Catalan re-
sources, while section 5 presents and evaluates the
bilingual approach. Finally, the main conclusions of
the work are summarized in section 6.
2 Setting
2.1 Corpus and data resources
The experimentation of this work has been carried
on two corpora, one for each language. Both corpora
consist of sentences extracted from news articles of
the year 2,000. The Catalan data, extracted from the
Catalan edition of the daily newspaper El Perio?dico
de Catalunya, has been randomly divided into three
sets: a training set (to train a system) and a test set
(to perform evaluation) for manual annotation, and
a remaining set left as unlabelled. The Spanish data
corresponds to the CoNLL 2002 Shared Task Span-
ish data, the original source being the EFE Spanish
Newswire Agency. The training set has been used
to improve classification for Catalan, whereas the
test set has been used to evaluate the bilingual classi-
fier. The original development set has not been used.
Table 1 shows the number of sentences, words and
lang. set #sent. #words #NEs
es train. 8,322 264,715 18,797
es test 1,516 51,533 3,558
ca train. 817 23,177 1,232
ca test 844 23,595 1,338
ca unlab. 83,725 2,201,712 75,038 
Table 1: Sizes of Spanish and Catalan data sets
Named Entities in each set. Although a large amount
of Catalan unlabelled NEs is available, it must be ob-
served that these are automatically recognised with a
91.5% accurate NER module, introducing a certain
error that might undermine bootstrapping results.
Considered classes include MUC categories PER
LOC and ORG, plus a fourth category MIS, includ-
ing named entities such as documents, measures and
taxes, sport competitions, titles of art works and oth-
ers. For Catalan, we find 33.0% of PER, 17.1% of
LOC, 43.5% of ORG and 6.4% of MIS out of the
2,570 manually annotated NEs, whereas for Span-
ish, out of the 22,355 labelled NEs, 22.6% are PER,
26.8% are LOC, 39.4% are ORG and the remaining
11.2% are MIS.
Additionally, we used a Spanish 7,427 trigger?
word list typically accompanying persons, organiza-
tions, locations, etc., and an 11,951 entry gazetteer
containing geographical and person names. These
lists have been semi-automatically extracted from
lexical resources and manually enriched afterwards.
They have been used in some previous works allow-
ing significant improvements for the Spanish NERC
task (Carreras et al, 2002; Carreras et al, 2003).
Trigger?words are annotated with the correspond-
ing Spanish synsets in the EuroWordNet lexical
knowledge base. Since there are translation links
among Spanish and Catalan (and other languages)
for the majority of these words, an equivalent ver-
sion of the trigger?word list for Catalan has been
automatically derived. In this work, we consider
the gazetteer as a language independent resource and
is indistinctly used for training Catalan and Spanish
models.
2.2 Feature codification
The features that characterise the NE examples are
defined in a window  anchored at a word  , repre-
senting its local context used by a classifier to make
a decision. In the window, each word around  is
codified with a set of primitive features, requiring no
linguistic pre?processing, together with its relative
position to  . Each primitive feature with each rela-
tive position and each possible value forms a final bi-
nary feature for the classifier (e.g., ?the word form
at position(-2) is street?). The kind of information
coded in these features may be grouped in the fol-
lowing kinds:
 Lexical: Word forms and their position in the
window (e.g., 	
 =?bank?), as well as word
forms appearing in the named entity under con-
sideration, independent from their position.
 Orthographic: Word properties regarding
how it is capitalised (initial-caps, all-caps),
the kind of characters that form the word
(contains-digits, all-digits, alphanumeric,
roman-number), the presence of punctua-
tion marks (contains-dots, contains-hyphen,
acronym), single character patterns (lonely-
initial, punctuation-mark, single-char), or the
membership of the word to a predefined class
(functional-word1) or pattern (URL).
 Affixes: The prefixes and suffixes up to 4 char-
acters of the NE being classified and its internal
components.
 Word Type Patterns: Type pattern of consec-
utive words in the context. The type of a word
is either functional (f), capitalised (C), lower-
cased (l), punctuation mark (.), quote (?) or
other (x).
 Bag-of-Words: Form of the words in the
window, without considering positions (e.g.,
?bank?  ).
 Trigger Words: Triggering properties of win-
dow words, using an external list to deter-
mine whether a word may trigger a certain
Named Entity (NE) class (e.g., ?president? may
trigger class PER). Also context patterns to
the left of the NE are considered, where each
word is marked with its triggering properties,
or with a functional?word tag, if appropriate
(e.g., the phrase ?the president of United Na-
tions? produces pattern f ORG f for the NE
1Functional words are determiners and prepositions which
typically appear inside NEs.
?United Nations?, assuming that ?president? is
listed as a possible trigger for ORG).
 Gazetteer Features: Gazetteer information for
window words. A gazetteer entry consists of a
set of possible NE categories.
 Additionally, binary features encoding the
length in words of the NE being classified.
All features are computed for a  -3,+3  window
around the NE being classified, except for the Bag-
of-Words, for which a  -5,+5  window is used.
3 Learning Algorithms
As previously said, we compare two learning ap-
proaches when learning from Catalan examples: su-
pervised (using the AdaBoost algorithm), and unsu-
pervised (using the Greedy Agreement Algorithm).
Both of them are briefly described below.
3.1 Supervised Learning
We use the multilabel multiclass AdaBoost.MH
algorithm (with confidence?rated predictions) for
learning the classification models. The idea of this
algorithm is to learn an accurate strong classifier by
linearly combining, in a weighted voting scheme,
many simple and moderately?accurate base classi-
fiers or rules. Each base rule is sequentially learned
by presenting the base learning algorithm a weight-
ing over the examples (denoting importance of ex-
amples), which is dynamically adjusted depending
on the behaviour of the previously learned rules. We
refer the reader to (Schapire and Singer, 1999) for
details about the general algorithm, and to (Schapire,
2002) for successful applications to many areas, in-
cluding several NLP tasks. Additionally, a NERC
system based on the AdaBoost algorithm obtained
the best results in the CoNLL?02 Shared Task com-
petition (Carreras et al, 2002).
In our setting, the boosting algorithm combines
several small fixed?depth decision trees. Each
branch of a tree is, in fact, a conjunction of binary
features, allowing the strong boosting classifier to
work with complex and expressive rules.
3.2 Unsupervised Learning
We have implemented the Greedy Agreement Algo-
rithm (Abney, 2002) which, based on two indepen-
dent views of the data, is able to learn two binary
classifiers from a set of hand-typed seed rules. Each
classifier is a majority vote of several atomic rules,
which abstains when the voting ends in a tie. The
atomic rules are just mappings of a single feature
into a class (e.g., if suffix ?lez? then PER). When
learning, the atomic rule that maximally reduces the
disagreement on unlabelled data between both clas-
sifiers is added to one of the classifiers, and the
process is repeated alternating the classifiers. See
(Abney, 2002) for a formal proof that this algo-
rithm tends to gradually reduce the classification er-
ror given the adequate seed rules.
For its extreme simplicity and potentially good re-
sults, this algorithm is very appealing for the NEC
task. In fact, results are reported to be competitive
against more sophisticated methods (Co-DL, Co-
Boost, etc.) for this specific task in (Abney, 2002).
Three important questions arise from the algo-
rithm. First, what features compose each view. Sec-
ond, how seed rules should be selected or whether
this selection strongly affects the final classifiers.
Third, how the algorithm, presented in (Abney,
2002) for binary classification, can be extended to
a multiclass problem.
In order to answer these questions and gain some
knowledge on how the algorithm works empirically,
we performed initial experiments on the big labelled
portion of the Spanish data.
When it comes to view selection, we tried two
alternatives. The first, suggested in (Collins and
Singer, 1999; Abney, 2002), divides into one view
capturing internal features of the NE, and the other
capturing features of its left-right contexts (here-
after referred to as Greedy Agreement pure, or GA  ).
Since the contextual view turned out to be quite lim-
ited in performance, we interchanged some feature
groups between the views. Specifically, we moved
the Lexical features independent of their position to
the contextual view, and the the Bag-of-Words fea-
tures to the internal one (we will refer to this divi-
sion as Greedy Agreement mixed, or GA  ). The lat-
ter, containing redundant and conditionally depen-
dent features, yielded slightly better results in terms
of precision?coverage trade?off.
As for seed rules selection, we have tried two dif-
ferent strategies. On the one hand, blindly choos-
ing as many atomic rules as possible that decide at
least in 98% of the cases for a class in a small vali-
dation set of labelled data, and on the other, manu-
ally selecting from these atomic rules only those that
might be valid still for a bigger data set. This second
approach proved empirically better, as it provided a
much higher starting point in the test set (in terms
of precision), whereas a just slightly lower coverage
value, presenting a better learning curve.
Finally, we have approached the multiclass set-
ting by a one?vs?all binarization, that is, divid-
ing the classification problem into four binary de-
cisions (one per class), and combining the resul-
tant rules. Several techniques to combine them have
been tested, from making a prediction only when
one classifier assigns positive for the given instance
and all other classifiers assign negative (very high
precision, low coverage), to much unrestrictive ap-
proaches, such as combining all votes from each
classifier (lower precision, higher coverage). Re-
sults proved that the best approach is to sum all votes
from all non-abstaining binary classifiers, where a
vote of a concrete classifier for the negative class is
converted to one vote for each of the other classes.
The best results obtained in terms of cover-
age/precision and evaluated over the whole set of
training data (and thus more significant than over a
small test set) are 80.7/84.9. These results are com-
parable to the ones presented in (Abney, 2002), tak-
ing into account, apart from the language change,
that we have introduced a fourth class to be treated
the same as the other three. Results when using
Catalan data are presented in section 4.
4 Using only Catalan resources
This section describes the results obtained by using
only the Catalan resources and comparing the fully
unsupervised Greedy Agreement algorithm with the
AdaBoost supervised learning algorithm.
4.1 Unsupervised vs. supervised learning
In this experiment, we used the Catalan training set
for extracting seed rules of the GA algorithm and to
train an AdaBoost classifier. The whole unlabelled
Catalan corpus was used for bootstrapping the GA
algorithm. All the results were computed over the
Catalan test set.
Figure 1 shows a precision?coverage plot of
AdaBoost (noted as CA, for CAtalan training) and
20
30
40
50
60
70
80
90
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n
Coverage
CA
GA(p)
GA(m)
Figure 1: Precision?coverage plot of GA  , GA  , and
CA models trained on Catalan resources
the Greedy Agreement algorithm for the two views
selections (noted GA  and GA  , respectively). The
curve for CA has been computed by varying a confi-
dence threshold: CA abstains when the highest pre-
diction of AdaBoost is lower than this threshold.
On the one hand, it can be seen that GA  is more
precise than GA  for low values of coverage but their
asymptotic behaviour is quite similar. By stopping
at the best point in the validation set, the Greedy
Agreement algorithm (GA  ) achieves a precision of
76.53% with a coverage of 83.62% on the test set.
On the other hand, the AdaBoost classifier clearly
outperforms both GA models at all levels of cover-
age, indicating that the supervised training is prefer-
able even when using really small training sets (an
accuracy around 70% is obtained by training Ad-
aBoost only with the 20% of the learning examples,
i.e., 270 examples).
The first three rows of table 2 contain the accu-
racy of these systems (i.e., precision when coverage
is 100%), detailed at the NE type level (best results
printed in boldface)2. The fourth row (BTS) corre-
sponds to the best results obtained when additional
unlabelled Catalan examples are taken into account,
as explained below.
It can be observed that the GA models are highly
biased towards the most frequent NE types (ORG and
PER) and that the accuracy achieved on the less rep-
2In order to obtain a 100% coverage with the GA models we
have introduced a naive algorithm for breaking ties in favour of
the most frequent categories, in the cases in which the algorithm
abstains.
LOC ORG PER MIS avg.
GA  14.66 83.64 93.88 0.00 66.66
GA  20.67 95.30 76.94 4.00 68.28
CA 61.65 86.84 91.67 40.00 79.83
BTS 65.41 87.22 91.94 37.33 80.63
Table 2: Accuracy results of all models trained on
Catalan resources
resented categories is very low for LOC and negli-
gible for MIS. The MIS category is rather difficult
to learn (also for the supervised algorithm), proba-
bly because it does not account for any concrete NE
type and does not show many regularities. Consid-
ering this fact, we learned the models using only the
LOC, ORG, and PER categories and treated the MIS
as a default value (assigned whenever the classifier
does not have enough evidence for any of the cate-
gories). The results obtained were even worse.
4.2 Bootstrapping AdaBoost models using
unlabelled examples
Ideally, the supervised approach can be boosted by
using the unlabelled Catalan examples in a kind of
iterative bootstrapping procedure. We have tested
a quite simple strategy for bootstrapping. The
unlabelled data in Catalan has been randomly di-
vided into a number of equal?size disjoint subsets

. . .

, containing 1,000 sentences each. Given
the initial training set for Catalan, noted as  , the
process is as follows:
1. Learn the ff 
		 Senseval-3: The Spanish Lexical Sample Task
L. Ma`rquez   , M. Taule?  , M.A. Mart??  , M. Garc??a  , N. Artigas  , F.J. Real   , D. Ferre?s  

TALP Research Center, Software Department
Universitat Polite`cnica de Catalunya

lluism,fjreal,dferres  @lsi.upc.es

CLiC, Centre de Llenguatge i Computacio?
Universitat de Barcelona

mtaule,amarti  @ub.edu,

nuripa,mar  @clic.fil.ub.es
1 Introduction
In this paper we describe the Spanish Lexical Sam-
ple task. This task was initially devised for evaluat-
ing the role of unlabeled examples in supervised and
semi-supervised learning systems for WSD and it
was coordinated with five other lexical sample tasks
(Basque, Catalan, English, Italian, and Rumanian)
in order to share part of the target words.
Firstly, we describe the methodology followed to
develop the linguistic resources necessary for the
task: the MiniDir-2.1 lexicon and the MiniCors cor-
pus. Secondly, we summarize the participant sys-
tems, the results obtained, and a comparative anal-
ysis. Participant systems include pure supervised,
semi?supervised, and unsupervised learning.
2 The Spanish Lexicon: MiniDir-2.1
Due to the enormous effort needed for rigorously
developing lexical resource and manually annotated
corpora, we limited our work to the treatment of
46 words of three syntactic categories: 21 nouns,
7 adjectives, and 18 verbs. The selection was made
trying to maintain the core words of the Senseval-
2 Spanish task and sharing around 10 of the target
words with Basque, Catalan, English, Italian, and
Rumanian lexical tasks. Table 1 shows the set of
selected words.
We used the MiniDir-2.1 dictionary as the lexical
resource for corpus tagging, which is a subset of the
broader MiniDir1. MiniDir-2.1 was designed as a
resource oriented to WSD tasks, i.e., with a granu-
larity level low enough to avoid the overlapping of
senses that commonly characterizes lexical sources.
Regarding the words selected, the average number
of senses per word is 5.33, corresponding to 4.52
senses for the nouns subgroup, 6.78 for verbs and 4
for adjectives (see table 1, right numbers in column
?#senses?).
The content of MiniDir-2.1 has been checked and
refined in order to guarantee not only its consis-
1MiniDir is a dictionary under development by the CLiC
research group, http://clic.fil.ub.es.
#LEMMA:conducir #POS:VM #SENSE:2
#DEF.: Manejar un veh??culo para desplazarse
#EXAMPLE: conducir un camio?n; conduce bien
#SYNONYMS: manejar
#COLLOC.: carne? de conducir; permiso de conducir
#SYNSETS: 01100152v;01099937v;01101463v;01176439v
Figure 1: Example of a Minidir-2.1 lexical entry
tency and coverage but also the quality of the gold
standard. Each sense in Minidir-2.1 is linked to
the corresponding synset numbers in EuroWordNet
(Vossen, 1999) and contains syntagmatic informa-
tion as collocates and examples extracted from cor-
pora2. Regarding the dictionary entries, every sense
is organized in nine lexical fields. See figure 1 for an
example of one sense of the lexical entry conducir
(?to drive?).
3 The Spanish Corpus: MiniCors
MiniCors is a semantically tagged corpus according
to the Senseval lexical sample setting, labeled with
the MiniDir-2.1 sense repository. The MiniCors
corpus is formed by 12,625 tagged examples, cov-
ering 35,875 sentences and 1,506,233 words. The
context considered for each example includes the
target sentence, plus the previous and the following
ones. All the examples have been extracted from the
year-2000 corpus of the Spanish EFE News Agency,
which includes 289,066 news (2,814,291 sentences
and 95,344,946 words) spanning from January to
December of 2000.
For every word, a minimum of 200 examples
have been manually tagged by three independent ex-
pert human annotators and disagreement cases have
been resolved by another lexicographer (assigning
a unique sense to each example). The annotation
process has been assisted by a graphical Perl-Tk
interface specifically designed for this task, and a
2We have used corpora from newspapers, El Perio?dico (3.5
million words), La Vanguardia (12.5 million words), and the
Lexesp corpus (Sebastia?n et al, 2000), a balanced corpus of
5.5. million words.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
word.POS #senses #train / test / unlab %MFS
actuar.v 3 / 4 133 / 67 / 1,500 73.13
apoyar.v 3 / 4 259 / 128 / 1,500 92.97
apuntar.v 4 / 9 213 / 106 / 1,500 50.94
arte.n 3 / 4 251 / 121 / 1,500 95.87
autoridad.n 2 / 4 268 / 132 / 1,500 96.97
bajar.v 3 / 5 235 / 115 / 1,500 84.35
banda.n 4 / 7 230 / 114 / 1,500 70.18
brillante.a 2 / 2 126 / 63 / 1,369 88.89
canal.n 4 / 6 262 / 131 / 1,500 65.65
canalizar.v 2 / 3 253 / 126 / 700 96.83
ciego.a 3 / 5 102 / 52 / 390 59.62
circuito.n 4 / 5 261 / 132 / 1,500 56.82
columna.n 7 / 8 129 / 64 / 1,258 20.31
conducir.v 4 / 5 134 / 66 / 1,094 45.45
corazo?n.n 3 / 6 123 / 62 / 1,500 45.16
corona.n 3 / 4 124 / 64 / 916 68.75
duplicar.v 2 / 2 254 / 126 / 1,500 96.03
explotar.v 5 / 5 212 / 103 / 1,500 45.63
ganar.v 3 / 8 237 / 118 / 1,500 90.68
gracia.n 3 / 5 72 / 38 / 1,209 50.00
grano.n 3 / 4 117 / 61 / 524 60.66
hermano.n 2 / 3 128 / 66 / 1,500 90.91
jugar.v 3 / 5 236 / 117 / 1,500 90.60
letra.n 5 / 5 226 / 114 / 1,251 34.21
masa.n 3 / 4 172 / 85 / 1,151 43.53
mina.n 2 / 4 134 / 66 / 1,458 51.52
natural.a 5 / 6 215 / 107 / 1,500 46.73
naturaleza.n 3 / 4 258 / 128 / 1,500 67.19
operacio?n.n 3 / 4 134 / 66 / 1,500 54.55
o?rgano.n 2 / 3 263 / 131 / 1,500 85.50
partido.n 2 / 2 133 / 66 / 1,500 56.06
pasaje.n 4 / 4 220 / 111 / 375 45.95
perder.v 4 / 11 218 / 106 / 1,500 60.38
popular.a 3 / 3 133 / 67 / 1,500 44.78
programa.n 3 / 3 267 / 133 / 1,500 75.19
saltar.v 8 / 15 200 / 101 / 1,117 29.70
simple.a 3 / 4 117 / 61 / 1,500 70.49
subir.v 3 / 5 231 / 114 / 1,500 74.56
tabla.n 3 / 6 130 / 64 / 1,500 76.56
tocar.v 6 / 13 158 / 78 / 1,500 28.21
tratar.v 3 / 12 143 / 72 / 1,235 43.06
usar.v 2 / 3 263 / 130 / 1,500 97.69
vencer.v 3 / 7 134 / 65 / 1,500 80.00
verde.a 2 / 5 69 / 33 / 1,500 60.61
vital.a 2 / 3 131 / 65 / 1,500 75.38
volar.v 3 / 6 122 / 60 / 705 53.33
avg/total 3.30 / 5.33 8,430 / 4,195 / 61,252 67.72
Table 1: Information about Spanish datasets
tagging handbook for the annotators. The inter?
annotator complete agreement achieved was 90%
for nouns, 83% for adjectives, and 83% for verbs.
These are the best results obtained in a compara-
tive study (Taule? et al, 2004) with other dictionaries
used for tagging the same corpus. The senses cor-
responding to multi?word expressions were elimi-
nated since they are not considered in MiniDir-2.1.
The initial goal was to obtain for each word at
least 75 examples plus 15 examples per sense. For
the words below these figures we performed a sec-
ond round by labeling up to 200 examples more.
After that, senses with less than 15 occurrences
(   3.5% of the examples) have been simply dis-
carded from the datasets. See table 1, left numbers
in column ?#senses?, for the final ambiguity rates.
We know that this is a quite controversial decision
that leads to a simplified setting. But we preferred
to maintain the proportions of the senses naturally
appearing in the EFE corpus rather than trying to ar-
tificially find examples of low frequency senses by
mixing examples from many sources or by getting
them with specific predefined patterns. Thus, sys-
tems trained on the MiniCors corpus are intended
to discriminate between the typical word senses ap-
pearing in a news corpus.
4 Resources Provided to Participants
Participants were provided with the complete
Minidir-2.1 dictionary, a training set with 2/3 of the
labeled examples, a test set with 1/3 of the exam-
ples and a complementary big set of unlabeled ex-
amples, limited to 1,500 for each word (when avail-
able). Each example is provided with a non null list
of category-labels marked according to two annota-
tion schemes: ANPA and IPTC3.
Aiming at helping teams with few resources on
the Spanish language, sentences in all datasets were
tokenized, lemmatized and POS tagged, using the
Spanish linguistic processors developed at TALP?
CLiC4, and provided as complementary files. Ta-
ble 1 contains information about the sizes of the
datasets and the proportion of the most-frequent
sense for each word (MFC). The baseline MFC clas-
sifier obtains a high accuracy of 67.72% due to the
moderate number of senses considered.
5 The Participant Systems
Seven teams took part on the Spanish Lexical Sam-
ple task, presenting a total of nine systems. We
will refer to them as: IRST, UA-NSM, UA-NP,
UA-SRT, UMD, UNED, SWAT, Duluth-SLSS, and
CSUSMCS. From them, seven are supervised and
two unsupervised (UA-NSM, UA-NP). Only one of
the participant systems uses a mixed learning strat-
egy that allows to incorporate the knowledge from
the unlabeled examples, namely UA-SRT. It is a
Maximum Entropy?based system, which makes use
of a re-training algorithm (inspired by Mitchell?s co-
training) for iteratively relabeling unannotated ex-
amples with high precision and adding them to the
training of the MaxEnt algorithm.
3All the datasets of the Spanish Lexical Sample task
and an extended version of this paper are available at:
http://www.lsi.upc.es/  nlp/senseval-3/Spanish.html.
4http://www.lsi.upc.es/  nlp/freeling.
Regarding the supervised learning approaches
applied, we find Naive Bayes and Decision Lists
(SWAT), Maximum Entropy (UA-SRT), Decision
Trees (Duluth-SLSS), Support Vector Machines
(IRST), AdaBoost (CSUSMCS), and a similarity
method based on co-occurrences (UNED). Some
systems used a voted combination of these basic
learning algorithms to produce the final WSD sys-
tem (SWAT, Duluth-SLSS). The two unsupervised
algorithms apply only to nouns and target at obtain-
ing high precision results (the annotations on ad-
jectives and verbs come from a supervised MaxEnt
system). UA-NSM method is called Specification
Marks and uses the words that co-occur with the
target word and their relation in the noun WordNet
hierarchy. UA-NP bases the disambiguation on syn-
tactic patterns and unsupervised corpus, relying on
the ?one sense per pattern? assumption.
All supervised teams used the POS and lemmati-
zation provided by the organization, except Duluth-
SLSS, which only used raw lexical information. A
few systems used also the category labels provided
with the examples. Apparently, none of them used
the extra information in MiniDir (examples, collo-
cations, synonyms, WordNet links, etc.), nor syntac-
tic information. Thus, we think that there is room
for substantial improvement in the feature set de-
sign. It is worth mentioning that the IRST system
makes use of a kernel including semantic informa-
tion within the SVM framework.
6 Results and System Comparison
Table 2 presents the global results of all participant
systems, including the MFC baseline (most frequent
sense classifier) and sorted by the combined F   mea-
sure. The COMB row stands for a voted combina-
tion of the best systems (see the last part of the sec-
tion). As it can be seen, IRST and UA-SRT are the
best performing systems, with no significant differ-
ences between them5.
All supervised systems outperformed the MFC
baseline, with a best overall improvement of 16.48
points (51.05% relative error reduction)6 . Both un-
supervised systems performed below MFC.
It is also observed that the POS and lemma infor-
mation used by most supervised systems is relevant,
since Duluth-SLSS (based solely on raw lexical in-
formation) performed significantly worse than the
rest of supervised systems7.
5Statistical significance was tested with a  -test (0.95 con-
fidence level) for the difference of two proportions.
6These improvement figures are better than those observed
in the Senseval-2 Spanish lexical sample task: 17 points but
only 32.69% of error reduction.
7With the exception of CSUSMCS, which according to ta-
System prec. recall cover. F 
IRST 84.20% 84.20% 100.0% 84.20
UA-SRT 84.00% 84.00% 100.0% 84.00
UMD 82.48% 82.48% 100.0% 82.48
UNED 81.76% 81.76% 100.0% 81.76
SWAT 79.45% 79.45% 100.0% 79.45
D-SLSS 74.29% 75.02% 100.0% 74.65
CSUSMCS 67.84% 67.82% 99.9% 67.83
UA-NSM 61.93% 61.93% 100.0% 61.93
UA-NP 84.31% 47.27% 56.1% 60.58
MFC 67.72% 67.72% 100.0% 67.72
COMB 85.98% 85.98% 100.0% 85.98
Table 2: Overall results of all systems
Detailed results by groups of words are showed
in table 3. Word groups include part?of?speech, in-
tervals of the proportion of the most frequent sense
(%MFS), intervals of the ratio number of examples
per sense (ExS), and the words in the retraining set
used by UA-SRT (those with a MFC accuracy lower
than 70% in the training set). Each cell contains
precision and recall. Bold face results correspond
to the best system in terms of the F   score. Last
column,

-error, contains the best F   improvement
over the baseline: absolute difference and error re-
duction (%).
As in many other previous WSD works, verbs
are the most difficult words (13.07 improvement
and 46.7% error reduction), followed by adjectives
(19.64, 52.1%), and nouns (20.78, 59.4%). The gain
obtained by all methods on words with high MFC
(more than 90%) is really low, indicating the diffi-
culties of supervised ML algorithms at acquiring in-
formation about non-frequent senses). On the con-
trary, the gain obtained on the lowest MFC words
is really good (44.3 points and 62.5% error reduc-
tion). This is a very good property of the Spanish
dataset and the participant systems, which is not al-
ways observed in other empirical studies using other
WSD corpora (e.g., in the Senseval-2 Spanish task
values of 29.9 and 43.1% were observed). The two
unsupervised systems failed at achieving a perfor-
mance on nouns comparable to the baseline classi-
fier. UA-NP has the best precision but at a cost of
an extremely low recall (below 5%).
It is also observed that participant systems
are quite different along word groups, being the
best performances shared between IRST, UA-SRT,
UMD, and UNED systems. Interestingly, IRST is
the best system addressing the words with less ex-
amples per sense, suggesting that SVM is a good
learning algorithm for training on small datasets,
but loses this advantage for the words with more
ble 3 shows a non-regular behavior with abnormal low results
on some groups of words.
IRST UA-SRT UMD UNED SWAT D-SLSS CSU... UA-NSM UA-NP MFC   -error
adjs (prec) 81.92 81.25 74.78 75.67 74.55 71.27 66.74 81.47 81.47 62.28 19.64
(rec) 81.92 81.25 74.78 75.67 74.55 71.43 66.74 81.47 81.47 62.28 52.1%
nouns 83.89 84.25 85.79 85.58 80.25 73.60 67.42 36.38 88.68 65.01 20.78
83.89 84.25 85.79 85.58 80.25 74.65 67.42 36.38 4.82 65.01 59.4%
verbs 85.09 84.43 80.81 79.14 79.81 75.80 68.56 84.76 84.76 72.02 13.07
85.09 84.43 80.81 79.14 79.81 76.31 68.56 84.76 84.76 72.02 46.7%
%MFS 97.17 97.17 96.69 96.69 96.69 96.69 97.17 83.31 96.90 96.69 0.48
(95,100) 97.17 97.17 96.69 96.69 96.69 96.69 97.17 83.31 64.09 96.69 14.5%
%MFS 92.77 92.54 91.38 91.84 91.38 91.61 65.50 90.68 92.56 91.38 1.39
(90,95) 92.77 92.54 91.38 91.84 91.38 91.61 65.50 90.68 78.32 91.38 16.1%
%MFS 89.04 90.11 86.36 90.37 86.10 85.71 83.16 63.10 88.89 84.76 5.61
(80,90) 89.04 90.11 86.36 90.37 86.10 86.63 83.16 63.10 57.75 84.76 36.8%
%MFS 83.82 88.51 80.91 85.11 78.64 75.08 75.89 59.06 85.59 73.62 14.89
(70,80) 83.82 88.51 80.91 85.11 78.64 75.08 75.89 59.06 46.12 73.62 56.4%
%MFS 79.73 78.59 80.31 80.88 69.98 66.22 57.93 48.37 80.25 64.44 16.4
(60,70) 79.73 78.59 80.31 80.88 69.98 66.35 57.93 48.37 24.86 64.44 46.2%
%MFS 81.06 76.11 80.38 77.82 76.96 67.91 60.34 44.54 70.04 54.27 26.8
(50,60) 81.06 76.11 80.38 77.82 76.96 68.26 60.34 44.54 28.33 54.27 58.6%
%MFS 78.75 75.33 72.07 66.57 71.03 61.18 56.02 57.80 78.31 45.17 33.6
(40,50) 78.75 75.33 72.07 66.57 71.03 61.81 56.02 57.80 48.29 45.17 61.3%
%MFS 68.35 73.39 71.43 64.71 62.75 49.35 37.54 49.30 65.92 29.13 44.3
(0,40) 68.35 73.39 71.43 64.71 62.75 52.94 37.54 49.30 33.05 29.13 62.5%
ExS 92.55 93.42 91.17 92.55 90.39 89.47 88.92 68.57 95.64 89.26 4.16
 120 92.55 93.42 91.17 92.55 90.39 89.78 88.92 68.57 47.53 89.26 38.7%
ExS 86.70 88.32 86.04 88.41 83.38 80.44 64.77 68.00 91.61 75.21 13.2
(90,120) 86.70 88.32 86.04 88.41 83.38 80.44 64.77 68.00 54.99 75.21 53.2%
ExS 79.39 78.00 77.71 74.58 74.07 65.32 58.89 54.55 77.34 53.97 25.42
(60,90) 79.39 78.00 77.71 74.58 74.07 65.99 58.85 54.55 39.04 53.97 55.2%
ExS 74.92 72.31 70.68 66.12 64.17 56.04 53.42 55.54 70.42 45.11 29.81
(30,60) 74.92 72.31 70.68 66.12 64.17 58.14 53.42 55.54 51.95 45.11 54.3%
retrain 78.34 76.79 77.05 73.86 71.59 62.75 55.46 48.42 74.42 50.73 27.61
78.34 76.79 77.05 73.86 71.59 63.78 55.44 48.42 32.80 50.73 56.0%
Table 3: Results of all participant systems on some selected subsets of words
examples. These facts opens the avenue for further
improvements on the Spanish dataset by combining
the outputs of the best performing systems. As a
first approach, we conducted some simple experi-
ments on system combination by considering a vot-
ing scheme, in which each system votes and the ma-
jority sense is selected (ties are decided favoring the
best method prediction). From all possible sets, the
best combination includes the five systems with the
best precision figures: UA-NP, IRST, UMD, UNED,
and SWAT. The resulting F   measure is 85.98, 1.78
points higher than the best single system (see table
2). This improvement comes mainly from the better
F
 
performance on nouns: from 83.89 to 87.28.
We also calculated the agreement rate and the
Kappa statistic between each pair of systems. The
agreement ratios ranged from 40.93% to 88.10%,
and the Kappa values from 0.40 to 0.87. It is worth
noting that the system relying on the simplest fea-
ture set (Duluth-SLSS) obtained the most similar
output to the most frequent sense classifier.
7 Acknowledgements
This work has been supported by the Spanish research
projects: XTRACT-2, BFF2002-04226-C03-03; FIT-
150-500-2002-244; and HERMES, TIC2000-0335-C03-
02. Francis Real holds a research grant by the Catalan
Government (2002FI-00648). Authors want to thank the
linguists of CLiC and UNED who collaborated in the an-
notation task.
References
N. Sebastia?n, M. A. Mart??, M. F. Carreiras, and
F. Cuetos Go?mez. 2000. Lexesp, le?xico informa-
tizado del espan?ol. Edicions de la Universitat de
Barcelona.
M. Taule?, M. Civit, N. Artigas, M. Garc??a,
L. Ma?rquez, M. A. Mart??, and B. Navarro.
2004. MiniCors and Cast3LB: Two Semantically
Tagged Spanish Corpora. In Proceedings of the
4th LREC, Lisbon.
P. Vossen, editor. 1999. EuroWordNet: A Multilin-
gual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht.
TALP System for the English Lexical Sample Task
Gerard Escudero?, Llu??s Ma`rquez? and German Rigau?
?TALP Research Center. EUETIB. LSI. UPC. escudero@lsi.upc.es
?TALP Research Center. LSI. UPC. lluism@lsi.upc.es
? IXA Group. UPV/EHU. rigau@si.ehu.es
1 Introduction
This paper describes the TALP system on the En-
glish Lexical Sample task of the Senseval-31 event.
The system is fully supervised and relies on a par-
ticular Machine Learning algorithm, namely Sup-
port Vector Machines. It does not use extra exam-
ples than those provided by Senseval-3 organisers,
though it uses external tools and ontologies to ex-
tract part of the representation features.
Three main characteristics have to be pointed out
from the system architecture. The first thing is the
way in which the multiclass classification problem
posed by WSD is addressed using the binary SVM
classifiers. Two different approaches for binarizing
multiclass problems have been tested: one?vs?all
and constraint classification. In a cross-validation
experimental setting the best strategy has been se-
lected at word level. Section 2 is devoted to explain
this issue in detail.
The second characteristic is the rich set of fea-
tures used to represent training and test examples.
Topical and local context features are used as usual,
but also syntactic relations and semantic features in-
dicating the predominant semantic classes in the ex-
ample context are taken into account. A detailed
description of the features is presented in section 3.
And finally, since each word represents a learning
problem with different characteristics, a per?word
feature selection has been applied. This tuning pro-
cess is explained in detail in section 4.
The last two sections discuss the experimental re-
sults (section 5) and present the main conclusions of
the work performed (section 6).
2 Learning Framework
The TALP system belongs to the supervised Ma-
chine Learning family. Its core algorithm is the
Support Vector Machines (SVM) learning algorithm
(Cristianini and Shawe-Taylor, 2000). Given a set
of binary training examples, SVMs find the hy-
perplane that maximizes the margin in a high di-
1http://www.senseval.org
mensional feature space (transformed from the in-
put space through the use of a non-linear function,
and implicitly managed by using the kernel trick),
i.e., the hyperplane that separates with maximal dis-
tance the positive examples from the negatives. This
learning bias has proven to be very effective for pre-
venting overfitting and providing good generalisa-
tion. SVMs have been also widely used in NLP
problems and applications.
One of the problems in using SVM for the WSD
problem is how to binarize the multiclass classifi-
cation problem. The two approximations tested in
the TALP system are the usual one?vs?all and the
recently introduced constraint?classification frame-
work (Har-Peled et al, 2002).
In the one?vs?all approach, the problem is de-
composed into as many binary problems as classes
has the original problem, and one classifier is
trained for each class trying to separate the exam-
ples of that class (positives) from the examples of
all other classes (negatives). This method assumes
the existence of a separator between each class and
the set of all other classes. When classifying a new
example, all binary classifiers predict a class and
the one with highest confidence is selected (winner?
take?all strategy).
2.1 Constraint Classification
Constraint classification (Har-Peled et al, 2002) is
a learning framework that generalises many multi-
class classification and ranking schemes. It consists
of labelling each example with a set of binary con-
straints indicating the relative order between pairs
of classes. For the WSD setting of Senseval-3, we
have one constraint for each correct class (sense)
with each incorrect class, indicating that the clas-
sifier to learn should give highest confidence to the
correct classes than to the negatives. For instance, if
we have 4 possible senses {1, 2, 3, 4} and a training
example with labels 2 and 3, the constraints corre-
sponding to the example are {(2>1), (2>4), (3>1),
and (3>4)}. The aim of the methodology is to learn
a classifier consistent with the partial order defined
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
by the constraints. Note that here we are not as-
suming that perfect separators can be constructed
between each class and the set of all other classes.
Instead, the binary decisions imposed are more con-
servative.
Using Kesler?s construction for multiclass classi-
fication, each training example is expanded into a
set of (longer) binary training examples. Finding
a vector?based separator in this new training set is
equivalent to find a separator for each of the binary
constraints imposed by the problem. The construc-
tion is general, so we can use SVMs directly on the
expanded training set to solve the multiclass prob-
lem. See (Har-Peled et al, 2002) for details.
3 Features
We have divided the features of the system in 4 cat-
egories: local, topical, knowledge-based and syn-
tactic features. First section of table 1 shows the
local features. The basic aim of these features
is to modelize the information of the surrounding
words of the target word. All these features are ex-
tracted from a ?3?word?window centred on the tar-
get word. The features also contain the position of
all its components. To obtain Part?of?Speech and
lemma for each word, we used FreeLing 2. Most
of these features have been doubled for lemma and
word form.
Three types of Topical features are shown in the
second section of table 1. Topical features try to
obtain non?local information from the words of the
context. For each type, two overlapping sets of
redundant topical features are considered: one ex-
tracted from a ?10?word?window and another con-
sidering all the example.
The third section of table 1 presents the
knowledge?based features. These features have
been obtained using the knowledge contained into
the Multilingual Central Repository (MCR) of the
MEANING project3 (Atserias et al, 2004). For each
example, the feature extractor obtains, from each
context, all nouns, all their synsets and their associ-
ated semantic information: Sumo labels, domain la-
bels, WordNet Lexicographic Files, and EuroWord-
Net Top Ontology. We also assign to each label a
weight which depends on the number of labels as-
signed to each noun and their relative frequencies
in the whole WordNet. For each kind of seman-
tic knowledge, summing up all these weights, the
program finally selects those semantic labels with
higher weights.
2http://www.lsi.upc.es/?nlp/freeling
3http://www.lsi.upc.es/?meaning
local feats.
Feat. Description
form form of the target word
locat all part?of?speech / forms / lemmas in
the local context
coll all collocations of two part?of?speech /
forms / lemmas
coll2 all collocations of a form/lemma and a
part?of?speech (and the reverse)
first form/lemma of the first noun / verb /
adjective / adverb to the left/right of the
target word
topical feats.
Feat. Description
topic bag of forms/lemmas
sbig all form/lemma bigrams of the example
comb forms/lemmas of consecutive (or not)
pairs of the open?class?words in the
example
knowledge-based feats.
Feat. Description
f sumo first sumo label
a sumo all sumo labels
f semf first wn semantic file label
a semf all wn semantic file labels
f tonto first ewn top ontology label
a tonto all ewn top ontology labels
f magn first domain label
a magn all domain labels
syntactical feats.
Feat. Description
tgt mnp syntactical relations of the target word
from minipar
rels mnp all syntactical relations from minipar
yar noun NounModifier, ObjectTo, SubjectTo
for nouns
yar verb Object, ObjectToPreposition, Preposi-
tion for verbs
yar adjs DominatingNoun for adjectives
Table 1: Feature Set
Finally, the last section of table 1 describes
the syntactic features which contains features ex-
tracted using two different tools: Dekang Lin?s
Minipar4 and Yarowsky?s dependency pattern ex-
tractor.
It is worth noting that the set of features presented
is highly redundant. Due to this fact, a feature se-
lection process has been applied, which is detailed
in the next section.
4 Experimental Setting
For each binarization approach, we performed a fea-
ture selection process consisting of two consecutive
steps:
4http://www.cs.ualberta.ca/?lindek/minipar.htm
? POS feature selection: Using the Senseval?2
corpus, an exhaustive selection of the best set
of features for each particular Part?of?Speech
was performed. These feature sets were taken
as the initial sets in the feature selection pro-
cess of Senseval-3.
? Word feature selection: We applied a
forward(selection)?backward(deletion) two?
step procedure to obtain the best feature
selection per word. For each word, the process
starts with the best feature set obtained in the
previous step according to its Part?of?Speech.
Now, during selection, we consider those
features not selected during POS feature
selection, adding all features which produce
some improvement. During deletion, we con-
sider only those features selected during POS
feature selection, removing all features which
produces some improvement. Although this
addition?deletion procedure could be iterated
until no further improvement is achieved, we
only performed a unique iteration because
of the computational overhead. One brief
experiment (not reported here) for one?vs?all
achieves an increase of 2.63% in accuracy
for the first iteration and 0.52% for a second
one. First iteration improves the accuracy of
53 words and the second improves only 15.
Comparing the evolution of these 15 words,
the increase in accuracy is of 2.06% for the
first iteration and 1.68% for the second one.
These results may suggest that accuracy could
be increased by this iteration procedure.
The result of this process is the selection of the
best binarization approach and the best feature set
for each individual word.
Considering feature selection, we have inspected
the selected attributes for all the words and we ob-
served that among these attributes there are fea-
tures of all four types. The most selected features
are the local ones, and among them those of ?first
noun/adjective on the left/right?; from topical fea-
tures the most selected ones are the ?comb? and in a
less measure the ?topic?; from the knowledge?based
the most selected feature are those of ?sumo? and
?domains labels?; and from syntactical ones, those
of ?Yarowsky?s patterns?. All the features previ-
ously mentioned where selected at least for 50 of
the 57 Senseval?3 words. Even so, it is useful the
use of all features when a selection procedure is ap-
plied. These general features do not work fine for
all words. Some words make use of the less selected
features; that is, every word is a different problem.
Regarding the implementation details of the sys-
tem, we used SVMlight (Joachims, 2002), a very ro-
bust and complete implementation of Support Vec-
tor Machines learning algorithms, which is freely
available for research purposes5 . A simple lineal
kernel with a regularization C value of 0.1 was
applied. This parameter was empirically decided
on the basis of our previous experiments on the
Senseval?2 corpus. Additionally, previous tests us-
ing non?linear kernels did not provide better results.
The selection of the best feature set and the bi-
narization scheme per word described above, have
been performed using a 5-fold cross validation pro-
cedure on the Senseval-3 training set. The five parti-
tions of the training set were obtained maintaining,
as much as possible, the initial distribution of exam-
ples per sense.
After several experiments considering the ?U? la-
bel as an additional regular class, we found that we
obtained better results by simply ignoring it. Then,
if a training example was tagged only with this la-
bel, it was removed from the training set. If the ex-
ample was tagged with this label and others, the ?U?
label was also removed from the learning example.
In that way, the TALP system do not assigns ?U?
labels to the test examples.
Due to lack of time, the TALP system presented
at the competition time did not include a com-
plete model selection for the constraint classifica-
tion binarization setting. More precisely, 14 words
were processed within the complete model selection
framework, and 43 were adjusted with a fixed one?
vs?all approach but a complete feature selection.
After the competition was closed, we implemented
the constraint classification setting more efficiently
and we reprocessed again the data. Section 5 shows
the results of both variants.
A rough estimation of the complete model selec-
tion time for both approaches is the following. The
training spent about 12 hours (OVA setting) and 5
days (CC setting) to complete6 , suggesting that the
main drawback of these approaches is the computa-
tional overhead. Fortunately, the process time can
be easily reduced: the CC layer could be ported
from Perl to C++ and the model selection could be
easily parallelized (since the treatment of each word
is independent).
5 Results
Table 2 shows the accuracy obtained on the train-
ing set and table 3 the results of our system (SE3,
5http://svmlight.joachims.org
6These figures were calculated using a 800 MHz Pentium
III PC with 320 Mb of memory.
TALP), together with the most frequent sense base-
line (mfs), the recall result of the best system in the
task (best), and the recall median between all par-
ticipant systems (avg). These last three figures were
provided provided by the organizers of the task.
OVA(base) in table 2 stands for the results of the
one?vs?all approach on the starting feature set (5?
fold?cross validation on the training set). CC(base)
refers to the constrain?classification setting on the
starting feature set. OVA(best) and CC(best) mean
one?vs?all and constraint?classification with their
respective feature selection. Finally, SE3 stands for
the system officially presented at competition time7
and TALP stands for the complete architecture.
method accuracy
OVA(base) 72 38%
CC(base) 72.28%
OVA(best) 75.27%
CC(best) 75.70%
SE3 75.62%
TALP 76.02%
Table 2: Overall results of all system variants on the
training set
It can be observed that the feature selection pro-
cess consistently improves the accuracy by around
3 points, both in OVA and CC binarization set-
tings. Constraint?classification is slightly better
than one?vs?all approach when feature selection
is performed, though this improvement is not con-
sistent along all individual words (detailed results
omitted) neither statistically significant (z?test with
0.95 confidence level). Finally, the combined
binarization?feature selection further increases the
accuracy in half a point (again this difference is not
statistically significant).
measure mfs avg best SE3 TALP
fine 55.2 65.1 72.9 71.3 71.6
coarse 64.5 73.7 79.5 78.2 78.2
Table 3: Overall results on the Senseval-3 test set
However, when testing the complete architecture
on the official test set, we obtained an accuracy de-
crease of more than 4 points. It remains to be ana-
lyzed if this difference is due to a possible overfit-
ting to the training corpus during model selection,
or simply is due to the differences between train-
ing and test corpora. Even so, the TALP system
achieves a very good performance, since there is a
7Only 14 words were processed with the full architecture.
difference of only 1.3 points in fine and coarse re-
call respect to the best system of the English lexical
sample task of Senseval?3.
6 Conclusions
Regarding supervised Word Sense Disambiguation,
each word can be considered as a different classi-
fication problem. This implies that each word has
different feature models to describe its senses.
We have proposed and tested a supervised sys-
tem in which the examples are represented through
a very rich and redundant set of features (using the
information content coherently integrated within the
Multilingual Central Repository of the MEANING
project), and which performs a specialized selection
of features and binarization process for each word.
7 Acknowledgments
This research has been partially funded by the Eu-
ropean Commission (Meaning Project, IST-2001-
34460), and by the Spanish Research Department
(Hermes Project: TIC2000-0335-C03-02).
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, P. Vossen 2004. The MEAN-
ING Multilingual Central Repository. In Pro-
ceedings of the Second International WordNet
Conference.
N. Cristianini and J. Shawe-Taylor 2000. An Intro-
duction to Support Vector Machines. Cambridge
University Press.
T. Joachims 2002. Learning to Classify Text Using
Support Vector Machines. Dissertation, Kluwer.
S. Har-Peled and D. Roth and D. Zimak 2002. Con-
straint Classification for Multiclass Classification
and Ranking. In Proceedings of the 15th Work-
shop on Neural Information Processing Systems.
Senseval-3: The Catalan Lexical Sample Task
L. Ma`rquez   , M. Taule?  , M.A. Mart??  , M. Garc??a  , F.J. Real   , and D. Ferre?s  

TALP Research Center, Software Department
Universitat Polite`cnica de Catalunya

lluism,fjreal,dferres  @lsi.upc.es

CLiC, Centre de Llenguatge i Computacio?
Universitat de Barcelona

mtaule,amarti  @ub.edu, mar@clic.fil.ub.es
1 Introduction
In this paper we describe the Catalan Lexical Sam-
ple task. This task was initially devised for evalu-
ating the role of unlabeled examples in supervised
and semi-supervised learning systems for WSD and
it is the counterpart of the Spanish Lexical Sample
task. It was coordinated also with other lexical sam-
ple tasks (Basque, English, Italian, Rumanian, and
Spanish) in order to share part of the target words.
Firstly, we describe the methodology followed
for developing the specific linguistic resources nec-
essary for the task: the MiniDir-Cat lexicon and
the MiniCors-Cat corpus. Secondly, we briefly de-
scribe the seven participant systems, the results ob-
tained, and a comparative evaluation between them.
All participant teams applied only pure supervised
learning algorithms.
2 The Catalan Lexicon: MiniDir-Cat
Catalan language participates for the first time in the
Senseval evaluation exercise. Due to the time con-
straints we had to reduce the initial expectations on
providing annotated corpora for up to 45 words to
the final 27 word set treated. We preferred to re-
duce the number of words, while maintaining the
quality in the dictionary development, corpus anno-
tation process, and number of examples per word.
These words belong to three syntactic categories:
10 nouns, 5 adjectives, and 12 verbs. The selec-
tion was made by choosing a subset of the Spanish
lexical sample task and trying to share around 10 of
the target words with Basque, English, Italian, and
Rumanian lexical sample tasks. See table 1 for a
complete list of the words.
We used the MiniDir-Cat dictionary as the lexi-
cal resource for corpus tagging, which is a dictio-
nary being developed by the CLiC research group1.
MiniDir-Cat was conceived specifically as a re-
source oriented to WSD tasks: we have emphasized
low granularity in order to avoid the overlapping
of senses usually present in many lexical sources.
1http://clic.fil.ub.es.
#LEMMA:banda #POS:NCFS #SENSE:2
#DEFINITION: Grup de persones que s?uneixen amb fins co-
muns, especialment delictius
#EXAMPLE: una banda que prostitu??a dones i robava cotxes
de luxe; la banda ultra de l?Atle`tic de Madrid
#SYNONYMS: grup; colla
#COLLOCATIONS: banda armada; banda juvenil; banda
de delinqu?ents; banda mafiosa; banda militar; banda organ-
itzada; banda paramilitar; banda terrorista; banda ultra
#SYNSETS: 05249044n; 05269792n
Figure 1: Example of a MiniDir-Cat entry
Regarding the polysemy of the selected words, the
average number of senses per word is 5.37, corre-
sponding to 4.30 senses for the nouns subset, 6.83
for verbs and 4 for adjectives (see table 1, right num-
bers in column ?#senses?).
The content of MiniDir-2.1 has been checked and
refined in order to guarantee not only its consistency
and coverage but also the quality of the gold stan-
dard. Each sense in Minidir-2.1 is linked to the
corresponding synset numbers in the semantic net
EuroWordNet (Vossen, 1999) (zero, one, or more
synsets per sense) and contains syntagmatic infor-
mation as collocates and examples extracted from
corpora2 . Every sense is organized in the nine fol-
lowing lexical fields: LEMMA, POS, SENSE, DEF-
INITION, EXAMPLES, SYNONYMS, ANTONYMS
(only in the case of adjectives), COLLOCATIONS,
and SYNSETS. See figure 1 for an example of one
sense of the lexical entry banda (noun ?gang?).
3 The Catalan Corpus: MiniCors-Cat
MiniCors-Cat is a semantically tagged corpus ac-
cording to the Senseval lexical sample setting, so
one single target word per example is semanti-
cally labeled with the MiniDir-Cat sense reposi-
tory. The MiniCors-Cat corpus is formed by 6,722
tagged examples, covering 45,509 sentences and
1,451,778 words (with an average of 31.90 words
2We have used a 3.5 million subset of the newspaper El
Perio?dico in the Catalan version.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
word.POS #senses #train / test / unlab %MFS
actuar.v 2 / 3 197 / 99 / 2,442 80.81
apuntar.v 5 / 11 184 / 93 / 1,881 50.54
autoritat.n 2 / 2 188 / 93 / 102 87.10
baixar.v 3 / 4 189 / 92 / 1,572 59.78
banda.n 3 / 5 149 / 75 / 180 60.00
canal.n 3 / 6 188 / 95 / 551 56.84
canalitzar.v 2 / 2 196 / 99 / 0 79.80
circuit.n 4 / 4 165 / 83 / 55 46.99
conduir.v 5 / 7 198 / 101 / 764 63.37
cor.n 4 / 7 144 / 72 / 634 50.00
explotar.v 3 / 4 193 / 98 / 69 72.45
guanyar.v 2 / 6 184 / 92 / 2,106 76.09
jugar.v 4 / 4 115 / 61 / 0 57.38
lletra.n 5 / 6 166 / 86 / 538 30.23
massa.n 2 / 3 145 / 74 / 33 59.46
mina.n 2 / 4 185 / 92 / 121 90.22
natural.a 3 / 6 170 / 88 / 2,320 80.68
partit.n 2 / 2 180 / 89 / 2,233 95.51
passatge.n 2 / 4 140 / 70 / 0 55.71
perdre.v 2 / 8 157 / 78 / 2,364 91.03
popular.a 3 / 3 137 / 70 / 2,472 51.43
pujar.v 2 / 4 191 / 95 / 730 71.58
saltar.v 6 / 17 111 / 60 / 134 38.33
simple.a 2 / 3 148 / 75 / 310 85.33
tocar.v 6 / 12 161 / 78 / 789 37.18
verd.a 2 / 5 128 / 64 / 1,315 79.69
vital.a 3 / 3 160 / 81 / 220 60.49
avg/total 3.11 / 5.37 4,469 / 2,253 / 23,935 66.36
Table 1: Information on the Catalan datasets
per sentence). The context considered for each ex-
ample includes the paragraph in which the target
word occurs, plus the previous and the following
paragraphs. All the examples have been extracted
from the corpus of the ACN Catalan news agency,
which includes about 110,588 news (January 2000?
December 2003). This corpus has been tagged with
POS. Following MiniDir-2.1, those examples con-
taining the current word in a multiword expression
have been discarded.
For every word, a total of 300 examples have
been manually tagged by two independent expert
human annotators, though some of them had to be
discarded due to errors in the automatic POS tag-
ging and multiword filtering. In the cases of dis-
agreement a third lexicographer defined the defini-
tive sense tags. All the annotation process has been
assisted by a graphical Perl-Tk interface specifi-
cally designed for this task (in the framework of the
Meaning European research project), and a tagging
handbook for the annotators (Artigas et al, 2003).
The inter?annotator agreement achieved was very
high: 96.5% for nouns, 88.7% for adjectives, 92.1%
for verbs, 93.16% overall.
The initial goal was to obtain, for each word, at
least 75 examples plus 15 examples per sense. How-
ever, after the labeling of the 300 examples, senses
with less than 15 occurrences were simply discarded
from the Catalan datasets. See table 1, left numbers
in column ?#senses?, for the final ambiguity rates.
We know that this is a quite controversial decision
that leads to a simplified setting. But we preferred to
maintain the proportions of the senses naturally ap-
pearing in the ACN corpus rather than trying to ar-
tificially find examples of low frequency senses by
mixing examples from many sources or by getting
them with specific predefined patterns. Thus, sys-
tems trained on the MiniCors-Cat corpus are only
intended to discriminate between the most impor-
tant word senses appearing in a general news cor-
pus.
4 Resources Provided to Participants
Participants were provided with the complete
Minidir-Cat dictionary, a training set with 2/3 of the
labeled examples, a test set with 1/3 of the exam-
ples and a complementary large set of all the avail-
able unlabeled examples in the ACN corpus (with
a maximum of 2,472 extra examples for the adjec-
tive popular). Each example is provided with a non
null list of category-labels marked according to the
newspaper section labels (politics, sports, interna-
tional, etc.)3. Aiming at helping teams with few re-
sources on the Catalan language, all corpora were
tokenized, lemmatized and POS tagged, using the
Catalan linguistic processors developed at TALP?
CLiC4, and provided to participants.
Table 1 contains information about the sizes of
the datasets and the proportion of the most-frequent
sense for each word (MFC). This baseline classifier
obtains a high accuracy of 66.36% due to the small
number of senses considered.
5 The Participant Systems
Five teams took part on the Catalan Lexical Sam-
ple task, presenting a total of seven systems. We
will refer to them as: IRST, SWAT-AB, SWAT-CP,
SWAT-CA, UNED, UMD, and Duluth-CLSS. All
of them are purely supervised machine learning ap-
proaches, so, unfortunately, none of them incorpo-
rates the knowledge from the unlabeled examples.
Most of these systems participated also in the Span-
ish lexical sample task, with almost identical con-
figurations.
Regarding the supervised learning approaches
applied, we find AdaBoost, Naive Bayes, vector?
based cosine similarity, and Decision Lists (SWAT
systems), Decision Trees (Duluth-CLSS), Support
3All the datasets of the Catalan Lexical Sample task
and an extended version of this paper are available at:
http://www.lsi.upc.es/   nlp/senseval-3/Catalan.html.
4http://www.lsi.upc.es/   nlp/freeling.
Vector Machines (IRST), and a similarity method
based on co-occurrences (UNED). Some systems
used a combination of these basic learning algo-
rithms to produce the final WSD system. For in-
stance, Duluth-CLSS applies a bagging?based en-
semble of Decision Trees. SWAT-CP performs a
majority voting of Decision Lists, the cosine?based
vector model and the Bayesian classifier. SWAT-CA
combines, again by majority voting, the previous
three classifiers with the AdaBoost based SWAT-AB
system. The Duluth-CLSS system is a replica of the
one presented at the Senseval-2 English lexical sam-
ple task.
All teams used the POS and lemmatization pro-
vided by the organization, except Duluth-CLSS,
which only used raw lexical information. A few sys-
tems used also the category labels provided with the
examples. Apparently, none of them used the ex-
tra information in MiniDir (examples, collocations,
synonyms, WordNet links, etc.), nor syntactic infor-
mation. Thus, we think that there is room for sub-
stantial improvement in the feature set design. It is
worth mentioning that the IRST system makes use
of a kernel within the SVM framework, including
semantic information. See IRST system description
paper for more information.
6 Results and System Comparison
Table 2 presents the global results of all participant
systems, including the MFC baseline (most frequent
sense classifier), sorted by the combined F   mea-
sure. The COMB row stands for a voted combina-
tion of the best systems (see last part of the section
for a description). As in the Spanish lexical sample
task the IRST system is the best performing one. In
this case it achieves a substantial improvement with
respect to the second system (SWAT-AB)5.
All systems obtained better results than the base-
line MFC classifier, with a best overall improvement
of 18.87 points (56.09% relative error reduction)6 .
For the multiple systems presented by SWAT, the
combination of learning algorithms in the SWAT-
CP and SWAT-CA did not help improving the ac-
curacy of the basic AdaBoost?based system SWAT-
AB. It is also observed that the POS and Lemma
information used by most systems is relevant, since
the system relying only on raw lexical information
5The difference is statistically significant using a  -test for
the difference of two proportions with a confidence level of
0.90. If we raise the confidence level to 0.95 we lose signif-
icance by a short margin:  
	
 
 .
6These improvement figures are better than those observed
in the Senseval-2 Spanish lexical sample task: 17 points and
32.69% of error reduction.
(Duluth-CLSS) performed significantly worse than
the rest (confidence level 0.95).
System prec. recall cover. F 
IRST 85.82% 84.64% 98.6% 85.23
SWAT-AB 83.39% 82.47% 98.9% 82.93
UNED 81.85% 81.85% 100.0% 81.85
UMD 81.46% 80.34% 98.6% 80.89
SWAT-CP 79.67% 79.67% 100.0% 79.67
SWAT-CA 79.58% 79.58% 100.0% 79.58
Duluth-CLSS 75.37% 76.48% 100.0% 75.92
MFC 66.36% 66.36% 100.0% 66.36
COMB 86.86% 86.86% 100.0% 86.86
Table 2: Overall results of all systems
Detailed results by groups of words are showed
in table 3. Word groups include part?of?speech, in-
tervals of the proportion of the most frequent sense
(%MFS), and intervals of the ratio: number of ex-
amples per sense (ExS). Each cell contains preci-
sion and recall. Bold face results correspond to
the best system in terms of the F   score. Last col-
umn,  -error, contains the best F   improvement
over the baseline: absolute difference and error re-
duction(%).
As in many other previous WSD works, verbs are
significantly more difficult (16.67 improvement and
49.3% error reduction) than nouns (23.46, 65.6%).
The improvements obtained by all methods on
words with high MFC (more than 90%) is generally
low. This is not really surprising, since statistically?
based supervised ML algorithms have difficulties
at acquiring information about non-frequent senses.
Notice, however, the remarkable 44.9% error re-
duction obtained by SWAT-AB, the best system on
this subset. On the contrary, the gain obtained on
the lowest MFC words is really good (34.2 points
and 55.3% error reduction). This is a good prop-
erty of the Catalan dataset and the participant sys-
tems, which is not always observed in other empir-
ical studies using other WSD corpora. It is worth
noting that even better results were observed in the
Spanish lexical sample task.
Systems are quite different along word groups:
IRST is globally the best but not on the words
with highest (between 80% and 100%) an lowest
(less than 50%) MFC, in which SWAT-AB is bet-
ter. UNED and UMD are also very competitive on
nouns but overall results are penalized by the lower
performance on adjectives (specially UNED) and
verbs (specially UMD). Interestingly, IRST is the
best system addressing the words with few exam-
ples per sense, suggesting that SVM is a good al-
gorithm for training on small datasets, but loses this
advantage for the words with more examples.
All, these facts, open the avenue for further im-
IRST SWAT-ME UNED UMD SWAT-CA SWAT-CP D-CLSS MFC   -error
adjs (prec) 86.51% 76.20% 79.10% 82.28% 83.33% 85.45% 79.63% 71.69% 14.82
(rec) 86.51% 71.16% 79.10% 82.28% 83.33% 85.45% 79.63% 71.69% 52.3%
nouns 87.68% 87.45% 86.61% 87.44% 82.87% 80.70% 78.38% 64.17% 23.46
87.58% 87.45% 86.61% 87.33% 82.87% 80.70% 80.46% 64.17% 65.5%
verbs 84.06% 82.60% 79.06% 76.28% 75.62% 76.77% 71.43% 66.16% 16.67
81.64% 82.60% 79.06% 74.09% 75.62% 76.77% 72.18% 66.16% 49.3%
%MFS 94.16% 95.75% 94.98% 94.16% 93.82% 93.82% 93.05% 92.28% 3.47
(90,100) 93.44% 95.75% 94.98% 93.44% 93.82% 93.82% 93.05% 92.28% 44.9%
%MFS 88.73% 90.42% 87.04% 85.63% 87.04% 87.61% 83.66% 83.38% 7.04
(80,90) 88.73% 90.42% 87.04% 85.63% 87.04% 87.61% 83.66% 83.38% 42.4%
%MFS 89.86% 85.71% 86.83% 83.11% 82.59% 84.60% 7.82% 75.67% 13.79
(70,80) 89.06% 85.71% 86.83% 82.37% 82.59% 84.60% 80.36% 75.67% 56.7%
%MFS 85.17% 81.09% 80.85% 80.62% 77.78% 78.25% 75.35% 60.76% 23.90
(59,70) 84.16% 81.09% 80.85% 79.67% 77.78% 78.25% 75.89% 60.76% 60.9%
%MFS 82.93% 77.98% 74.19% 75.05% 72.89% 71.80% 66.25% 53.58% 28.99
(50,59) 82.21% 73.75% 74.19% 74.40% 72.89% 71.80% 68.55% 53.58% 62.5%
%MFS 74.23% 72.31% 70.36% 73.88% 67.10% 65.15% 59.12% 38.11% 34.20
(0,50) 70.36% 72.31% 70.36% 70.03% 67.10% 65.15% 61.24% 38.11% 55.3%
ExS 91.77% 91.96% 91.35% 88.72% 88.62% 87.56% 85.05% 82.85% 9.11
 120 91.35% 91.96% 91.35% 88.32% 88.62% 87.56% 85.43% 82.85% 53.1%
ExS 87.66% 88.08% 86.38% 86.25% 83.90% 85.45% 80.03% 69.50% 18.58
(90,120) 86.84% 88.08% 86.38% 85.45% 83.90% 85.45% 81.27% 69.50% 60.9%
ExS 83.06% 76.11% 76.10% 77.42% 76.51% 75.70% 71.40% 61.04% 21.86
(60,90) 82.73% 72.29% 76.10% 77.11% 76.51% 75.70% 71.69% 61.04% 56.1%
ExS 77.21% 71.78% 67.78% 67.91% 64.00% 63.78% 59.40% 43.56% 31.89
(30,60) 73.78% 71.78% 67.78% 64.89% 64.00% 63.78% 61.78% 43.56% 56.5%
Table 3: Results of all participant systems on some selected subsets of words
provements on the Catalan dataset by combining the
outputs of the best performing systems, or by per-
forming a selection of the best at word level. As a
first approach, we conducted some simple experi-
ments on system combination by considering a vot-
ing scheme, in which each system votes and the ma-
jority sense is selected (ties are decided favoring the
best method prediction). From all possible sets, the
best combination of systems turned out to be: IRST,
SWAT-AB, and UNED. The resulting F   measure is
86.86, 1.63 points higher than the best single sys-
tem (see table 2). This improvement comes mainly
from the better F   performance on noun and verb
categories: from 87.63 to 90.11 and from 82.63 to
85.47, respectively.
Finally, see the agreement rates and the Kappa
statistic between each pair of systems in table
4. Due to space restrictions we have indexed the
systems by numbers: 1=MFC, 2=UMD, 3=IRST,
4=UNED, 5=D-CLSS, 6=SWAT-AB, 7=SWAT-CP,
and 8=SWAT-CA. The upper diagonal contains the
agreement ratios varying from 70.13% to 96.01%,
and the lower diagonal contains the corresponding
Kappa values, ranging from 0.67 and 0.95. It is
worth noting that the system relying on the simplest
feature set (Duluth-CLSS) obtains the most simi-
lar output to the most frequent sense classifier, and
that the combination-based systems SWAT-CP and
SWAT-CA generate almost the same output.
1 2 3 4 5 6 7 8
1 ? 78.96 72.79 74.43 87.84 70.13 82.25 84.42
2 0.77 ? 82.78 85.66 83.95 82.51 84.55 87.31
3 0.70 0.81 ? 81.05 81.98 80.74 85.13 85.18
4 0.71 0.84 0.79 ? 79.87 82.56 81.76 83.31
5 0.86 0.82 0.80 0.77 ? 77.19 86.77 88.88
6 0.67 0.81 0.79 0.81 0.75 ? 79.09 82.69
7 0.80 0.83 0.83 0.80 0.85 0.77 ? 96.01
8 0.82 0.86 0.83 0.81 0.87 0.81 0.95 ?
Table 4: Agreement and Kappa values between each
pair of systems
7 Acknowledgements
This work has been supported by the research
projects: XTRACT-2, BFF2002-04226-C03-03;
FIT-150-500-2002-244; HERMES, TIC2000-0335-
C03-02; and MEANING, IST-2001-34460. Francis
Real holds a predoctoral grant by the Catalan Gov-
ernment (2002FI-00648).
References
N. Artigas, M. Garc??a, M. Taule?, and M. A. Mart??.
2003. Manual de anotacio?n sema?ntica. Working
Paper XTRACT-03/03, CLiC, UB.
P. Vossen, editor. 1999. EuroWordNet: A Multilin-
gual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Introduction to the CoNLL-2004 Shared Task:
Semantic Role Labeling
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.es
Abstract
In this paper we describe the CoNLL-2004
shared task: semantic role labeling. We intro-
duce the specification and goal of the task, de-
scribe the data sets and evaluation methods, and
present a general overview of the systems that
have contributed to the task, providing compar-
ative description.
1 Introduction
In recent years there has been an increasing interest in
semantic parsing of natural language, which is becoming
a key issue in Information Extraction, Question Answer-
ing, Summarization, and, in general, in all NLP applica-
tions requiring some kind of semantic interpretation.
The shared task of CoNLL-2004 1 concerns the recog-
nition of semantic roles, for the English language. We
will refer to it as Semantic Role Labeling (SRL). Given a
sentence, the task consists of analyzing the propositions
expressed by some target verbs of the sentence. In par-
ticular, for each target verb all the constituents in the sen-
tence which fill a semantic role of the verb have to be
extracted (see Figure 1 for a detailed example). Typical
semantic arguments include Agent, Patient, Instrument,
etc. and also adjuncts such as Locative, Temporal, Man-
ner, Cause, etc.
Most existing systems for automatic semantic role la-
beling make use of a full syntactic parse of the sentence
in order to define argument boundaries and to extract rel-
evant information for training classifiers to disambiguate
between role labels. Thus, the task has been usually ap-
proached as a two phase procedure consisting of recogni-
tion and labeling of arguments.
1CoNLL-2004 Shared Task web page ?with
data, software and systems? outputs available? at
http://cnts.uia.ac.be/conll2004/roles .
Regarding the learning component of the systems,
we find pure probabilistic models (Gildea and Juraf-
sky, 2002; Gildea and Palmer, 2002; Gildea and Hock-
enmaier, 2003), Maximum Entropy (Fleischman et al,
2003), generative models (Thompson et al, 2003), De-
cision Trees (Surdeanu et al, 2003; Chen and Ram-
bow, 2003), and Support Vector Machines (Hacioglu and
Ward, 2003; Pradhan et al, 2003a; Pradhan et al, 2003b).
There have also been some attempts at relaxing the ne-
cessity of using syntactic information derived from full
parse trees. For instance, in (Pradhan et al, 2003a; Ha-
cioglu and Ward, 2003), a SVM-based SRL system is
devised which performs an IOB sequence tagging using
only shallow syntactic information at the level of phrase
chunks.
Nowadays, there exist two main English corpora with
semantic annotations from which to train SRL systems:
PropBank (Palmer et al, 2004) and FrameNet (Fillmore
et al, 2001). In the CoNLL-2004 shared task we concen-
trate on the PropBank corpus, which is the Penn Treebank
corpus enriched with predicate?argument structures. It
addresses predicates expressed by verbs and labels core
arguments with consecutive numbers (A0 to A5), try-
ing to maintain coherence along different predicates. A
number of adjuncts, derived from the Treebank functional
tags, are also included in PropBank annotations.
To date, the best results reported on the PropBank cor-
respond to a F1 measure slightly over 83, when using
the gold standard parse trees from Penn Treebank as the
main source of information (Pradhan et al, 2003b). This
performance drops to 77 when a real parser is used in-
stead. Comparatively, the best SRL system based solely
on shallow syntactic information (Pradhan et al, 2003a)
performs more than 15 points below. Although these re-
sults are not directly comparable to the ones obtained in
the CoNLL-2004 shared task (different datasets, differ-
ent version of PropBank, etc.) they give an idea about the
state-of-the art results on the task.
The challenge for CoNLL-2004 shared task is to come
up with machine learning strategies which address the
SRL problem on the basis of only partial syntactic in-
formation, avoiding the use of full parsers and external
lexico-semantic knowledge bases. The annotations pro-
vided for the development of systems include, apart from
the argument boundaries and role labels, the levels of pro-
cessing treated in the previous editions of the CoNLL
shared task, i.e., words, PoS tags, base chunks, clauses,
and named entities.
The rest of the paper is organized as follows. Section
2 describes the general setting of the task. Section 3 pro-
vides a detailed description of training, development and
test data. Participant systems are described and compared
in section 4. In particular, information about learning
techniques, SRL strategies, and feature development is
provided, together with performance results on the devel-
opment and test sets. Finally, section 5 concludes.
2 Task Description
The goal of the task is to develop a machine learning sys-
tem to recognize arguments of verbs in a sentence, and
label them with their semantic role. A verb and its set of
arguments form a proposition in the sentence, and typi-
cally, a sentence will contain a number of propositions.
There are two properties that characterize the structure
of the arguments in a proposition. First, arguments do not
overlap, and are organized sequentially. Second, an argu-
ment may appear split into a number of non-contiguous
phrases. For instance, in the sentence ?[A1 The apple],
said John, [C?A1 is on the table]?, the utterance argument
(labeled with type A1) appears split into two phrases.
Thus, there is a set of non-overlapping arguments la-
beled with semantic roles associated with each proposi-
tion. The set of arguments of a proposition can be seen as
a chunking of the sentence, in which chunks are parts of
the semantic roles of the proposition predicate.
In practice, number of target verbs are marked in a sen-
tence, each governing one proposition. A system has to
recognize and label the arguments of each target verb.
2.1 Methodological Setting
Training and development data are provided to build the
learning system. Apart from the correct output, both data
sets contain the correct input, as well as predictions of the
input made by state-of-the-art processors. The training
set is used for training systems, whereas the development
set is used to tune parameters of the learning systems and
select the best model.
Systems have to be developed strictly with the data
provided, which consists of input and output data and the
official external resources (described below). Since the
correct annotations for the input data are provided, a sys-
tem is allowed either to be trained to predict the input
part, or to make use of an external tool developed strictly
within this setting, such as previous CoNLL shared task
systems.
2.2 Evaluation
Evaluation is performed on a separate test set, which in-
cludes only predicted input data. A system is evaluated
with respect to precision, recall and the F1 measure. Pre-
cision (p) is the proportion of arguments predicted by a
system which are correct. Recall (r) is the proportion of
correct arguments which are predicted by a system. Fi-
nally, the F1 measure computes the harmonic mean of
precision and recall, and is the final measure to com-
pare the performance of systems. It is formulated as:
F?=1 = 2pr/(p + r).
For an argument to be correctly recognized, the words
spanning the argument as well as its semantic role have
to be correct. 2
As an exceptional case, the verb argument of each
proposition is excluded from the evaluation. This argu-
ment is the lexicalization of the predicate of the proposi-
tion. Most of the time, the verb corresponds to the target
verb of the proposition, which is provided as input, and
only in few cases the verb participant spans more words
than the target verb.
Except for non-trivial cases, this situation makes the
verb fairly easy to identify and, since there is one verb
with each proposition, evaluating its recognition over-
estimates the overall performance of a system. For this
reason, the verb argument is excluded from evaluation.
3 Data
The data consists of six sections of the Wall Street Jour-
nal part of the Penn Treebank (Marcus et al, 1993), and
follows the setting of past editions of the CoNLL shared
task: training set (sections 15-18), development set (sec-
tion 20) and test set (section 21). We first describe anno-
tations related to argument structure. Then, we describe
the preprocessing of input data. Finally, we describe the
format of the data sets.
3.1 PropBank
The Proposition Bank (PropBank) (Palmer et al, 2004)
annotates the Penn Treebank with verb argument struc-
ture. The semantic roles covered by PropBank are the
following:
? Numbered arguments (A0?A5, AA): Arguments
defining verb-specific roles. Their semantics de-
pends on the verb and the verb usage in a sentence,
or verb sense. In general, A0 stands for the agent
2The srl-eval.pl program is the official program to
evaluate the performance of a system. It is available at the
Shared Task web page.
and A1 corresponds to the patient or theme of the
proposition, and these two are the most frequent
roles. However, no consistent generalization can be
made across different verbs or different senses of the
same verb. PropBank takes the definition of verb
senses from VerbNet, and for each verb and each
sense defines the set of possible roles for that verb
usage, called the roleset. The definition of rolesets
is provided in the PropBank Frames files, which is
made available for the shared task as an official re-
source to develop systems.
? Adjuncts (AM-): General arguments that any verb
may take optionally. There are 13 types of adjuncts:
AM-ADV : general-purpose AM-MOD : modal verb
AM-CAU : cause AM-NEG : negation marker
AM-DIR : direction AM-PNC : purpose
AM-DIS : discourse marker AM-PRD : predication
AM-EXT : extent AM-REC : reciprocal
AM-LOC : location AM-TMP : temporal
AM-MNR : manner
? References (R-): Arguments representing argu-
ments realized in other parts of the sentence. The
role of a reference is the same as the role of the ref-
erenced argument. The label is an R- tag prefixed to
the label of the referent, e.g. R-A1.
? Verbs (V): Participant realizing the verb of the
proposition, with exactly one verb for each one.
We used the February 2004 release of PropBank. Most
predicative verbs were annotated, although not all of
them (for example, most of the occurrences of the verb
?to have? and ?to be? were not annotated). We applied
procedures to check consistency of propositions, looking
for overlapping arguments, and incorrect semantic role
labels. Also, co-referenced arguments were annotated as
a single item in PropBank, and we automatically distin-
guished between the referent and the reference with sim-
ple rules matching pronominal expressions, which were
tagged as R arguments. A total number of 68 proposi-
tions were not compliant with our procedures, and were
filtered out from the CoNLL data sets. The predicate-
argument annotations, thus, are not necessarily complete
in a sentence. Table 1 provides counts of the number of
sentences, annotated propositions, distinct verbs and ar-
guments in the three data sets.
3.2 Preprocessing
In this section we describe the pipeline of processors to
compute the annotations which form the input part of
the data: part-of-speech (PoS) tags, chunks, clauses and
named entities. The preprocessors correspond to the fol-
lowing state-of-the-art systems for each level of annota-
tion:
Training Devel. Test
Sentences 8,936 2,012 1,671
Tokens 211,727 47,377 40,039
Propositions 19,098 4,305 3,627
Distinct Verbs 1,838 978 855
All Arguments 50,182 11,121 9,598
A0 12,709 2,875 2,579
A1 18,046 4,064 3,429
A2 4,223 954 714
A3 784 149 150
A4 626 147 50
A5 14 4 2
AA 5 0 0
AM-ADV 1,727 352 307
AM-CAU 283 53 49
AM-DIR 231 60 50
AM-DIS 1,077 204 213
AM-EXT 152 49 14
AM-LOC 1,279 230 228
AM-MNR 1,337 334 255
AM-MOD 1,753 389 337
AM-NEG 687 131 127
AM-PNC 446 100 85
AM-PRD 10 3 3
AM-REC 2 1 0
AM-TMP 3,567 759 747
R-A0 738 162 159
R-A1 360 74 70
R-A2 49 17 9
R-A3 8 0 1
R-AA 1 0 0
R-AM-ADV 1 0 0
R-AM-LOC 27 4 4
R-AM-MNR 4 0 1
R-AM-PNC 1 0 1
R-AM-TMP 35 6 14
Table 1: Counts on the three data sets.
? PoS tagger: (Gime?nez and Ma`rquez, 2003), based
on Support Vector Machines, and trained on Penn
Treebank sections 0?18.
? Chunker and Clause Recognizer: (Carreras and
Ma`rquez, 2003), based on Voted Perceptrons, and
following the CoNLL settings of 2000 and 2001
tasks (Tjong Kim Sang and Buchholz, 2000; Tjong
Kim Sang and De?jean, 2001). These two processors
form a coherent partial syntax of a sentence, that is,
chunks and clauses form a tree.
? Named entities with (Chieu and Ng, 2003), based
on Maximum-Entropy classifiers, and following the
CoNLL-2003 task setting (Tjong Kim Sang and
De Meulder, 2003).
Precision Recall F1/Acc.
PoS Dev. (acc.) ? ? 96.88
PoS Test (acc.) ? ? 96.70
Chunking Dev. 94.28% 93.65% 93.96
Chunking Test 93.80% 92.93% 93.36
Clauses Dev. 90.51% 86.12% 88.26
Clauses Test 88.73% 82.92% 85.73
Named Entities 88.12% 88.51% 88.31
Table 2: Results of the preprocessing modules on the de-
velopment and test sets. Named Entity figures are based
on the CoNLL-2003 test set.
Such processors were ran in a pipeline, from PoS tags,
to chunks, clauses and finally named entities. Table 2
summarizes the performance of the processors on the de-
velopment and test sections. These figures differ from the
original results in the original due to a better quality of the
input information in our runs. The figures of the named
entity extractor are based on the corpus of the CoNLL-
2003 shared task, since gold annotations of named enti-
ties were not available for the current corpus.
3.3 Format
Figure 1 shows an example of a fully-annotated sentence.
Annotations of a sentence are given using a flat represen-
tation in columns, separated by spaces. Each column en-
codes an annotation by associating a tag with every word.
For each sentence, the following columns are provided:
1. Words.
2. Part of Speech tags.
3. Chunks in IOB2 format.
4. Clauses in Start-End format.
5. Named Entities in IOB2 format.
6. Target verbs, marking n predicative verbs. This
column, provided as input, specifies the governing
verbs of the propositions to be analyzed. Each target
verb is in the base form. Occasionally this column
does not mark any verb (i.e., n may be 0).
7. For each of the n target verbs, a column in Start-End
format specifying the arguments of the proposition.
These columns are the output of a system, that is,
the ones to be predicted, and are not available for
the test set.
IOB2 format. Represents chunks which do not overlap
nor embed. Words outside a chunk receive the tag O. For
words forming a chunk of type k, the first word receives
the B-k tag (Begin), and the remaining words receive the
tag I-k (Inside).
Start-End format. Represents non-overlapping
phrases (clauses or arguments) which may be embed-
ded3 inside one another. Each tag indicates whether
a clause starts or ends at that word and is of the form
START*END. The START part is a concatenation of (k
parentheses, each representing that a phrase of type k
starts at that word. The END part is a concatenation of
k) parentheses, each representing that a phrase of type
k ends at that word. For example, the * tag represents
a word with no starts and ends; the (A0*A0) tag
represents a word constituting an A0 argument; and the
(S(S*S) tag represents a word which constitutes a
base clause (labeled S) and starts another higher-level
clause. Finally, the concatenation of all tags constitutes
a well-formed bracketing. For the particular case of split
arguments, of type k, the first part appears as a phrase
with label k, and the remaining as phrases with label
C-k (continuation prefix). See examples of annotations
at columns 4th, 7th and 8th of Figure 1.
4 Participating Systems
Ten systems have participated in the CoNLL-2004 shared
task. They approached the task in several ways, using dif-
ferent learning components and labeling strategies. The
following subsections briefly summarize the most impor-
tant properties of each system and provide a qualitative
comparison between them, together with a quantitative
evaluation on the development and test sets.
4.1 Learning techniques
Up to six different learning algorithms have been ap-
plied in the CoNLL-2004 shared task. None of them
is new with respect to the past editions. Two teams
used the Maximum Entropy (ME) statistical framework
(Baldewein et al, 2004; Lim et al, 2004). Two teams
used Brill?s Transformation-based Error-driven Learning
(TBL) (Higgins, 2004; Williams et al, 2004). Two other
groups applied Memory-Based Learning (MBL) (van den
Bosch et al, 2004; Kouchnir, 2004). The remaining four
teams employed vector-based linear classifiers of differ-
ent types: Hacioglu et al (2004) and Park et al (2004)
used Support Vector Machines (SVM) with polyno-
mial kernels, Carreras et al (2004) used Voted Percep-
trons (VP) also with polynomial kernels, and finally,
Punyakanok et al (2004) used SNoW, a Winnow-based
network of linear separators. Additionally, the team of
Baldewein et al (2004) used a EM?based clustering al-
gorithm for feature development (see section 4.3).
As a main difference with respect to past editions, less
effort has been put into combining different learning al-
gorithms and outputs. Instead, the main effort of partici-
pants went into developing useful SRL strategies and into
the development of features (see sections 4.2 and 4.3).
As an exception, van den Bosch et al (2004) applied a
3Arguments in data do not embed, though format allows so.
The DT B-NP (S* O - (A0* *
San NNP I-NP * B-ORG - * *
Francisco NNP I-NP * I-ORG - * *
Examiner NNP I-NP * I-ORG - *A0) *
issued VBD B-VP * O issue (V*V) *
a DT B-NP * O - (A1* (A1*
special JJ I-NP * O - * *
edition NN I-NP * O - *A1) *A1)
around IN B-PP * O - (AM-TMP* *
noon NN B-NP * O - *AM-TMP) *
yesterday NN B-NP * O - (AM-TMP*AM-TMP) *
that WDT B-NP (S* O - (C-A1* (R-A1*R-A1)
was VBD B-VP (S* O - * *
filled VBN I-VP * O fill * (V*V)
entirely RB B-ADVP * O - * (AM-MNR*AM-MNR)
with IN B-PP * O - * *
earthquake NN B-NP * O - * (A2*
news NN I-NP * O - * *
and CC I-NP * O - * *
information NN I-NP *S)S) O - *C-A1) *A2)
. . O *S) O - * *
Figure 1: An example of an annotated sentence, in columns. Input consists of words (1st), PoS tags (2nd), base chunks
(3rd), clauses (4th) and named entities (5th). The 6th column marks target verbs, and their propositions are found in
remaining columns. According to the PropBank Frames, for issue (7th), the A0 annotates the issuer, and the A1 the
thing issued, which appears split into two parts. For fill (8th), A1 is the the destination, and A2 the theme.
voting strategy to derive the final sequence tagging as
a voted combination of three overlapping n-gram output
sequences. The same team also applied a meta-learning
step, by using iterative classifier stacking, for correcting
systematic errors committed by the low?level classifiers.
This work is also worth mentioning because of the exten-
sive work done on parameter tuning and feature selection.
4.2 SRL approaches
SRL is a complex task which has to be decomposed into
a number of simpler decisions and tagging schemes in
order to be addressed by learning techniques.
One first issue is the annotation of the different propo-
sitions of a sentence. Most of the groups treated the
annotation of semantic roles for each verb predicate as
an independent problem. An exception is the system of
Carreras et al (2004), which performs the annotation of
all propositions simultaneously. As a consequence, the
former teams treat the problem as the recognition of se-
quential structures (a.k.a. chunking), while the latter di-
rectly derives a hierarchical structure formed by the argu-
ments of all propositions. Table 3 summarizes the main
properties of each system regarding the SRL strategy im-
plemented. This property corresponds to the first column.
Regarding the labeling strategy, we can distinguish at
least three different strategies. The first one consists of
performing role identification directly by a IOB-type se-
quence tagging. The second approach consists of divid-
ing the problem into two independent phases: recogni-
tion, in which the arguments are recognized, and label-
ing, in which the already recognized arguments are as-
signed role labels. The third approach also proceeds in
two phases: filtering, in which a set of argument can-
didates are decided and labeling, in which the set of
optimal arguments is derived from the proposed can-
didates. As a variant of the first two-phase strategy,
van den Bosch et al (2004) first perform a direct classi-
fication of chunks into argument labels, and then decide
the actual arguments in a post-process by joining previ-
ously classified argument fragments. All this information
is summarized in the second column of Table 3.
An implication of implementing the two-phase strat-
egy is the ability to work with argument candidates in
the second phase, allowing to develop feature patterns for
complete arguments. Regarding the first phase, the recog-
nition of candidate arguments is performed by means
of a IOB or open?close tagging using classifiers, either
argument?independent, or specialized by argument type.
It is also worth noting that all participant systems per-
formed learning of predicate-independent classifiers in-
stead of specializing by the verb predicate. Information
about verb predicates is captured through features and
some global restrictions.
Another important issue is the granularity at which
the sentence elements are processed. It has become very
clear that a good election for this problem is phrase-by-
phrase processing (P-by-P, using the notation introduced
by Hacioglu et al (2004)) instead of word-by-word (W-
by-W). The motivation is twofold: (1) phrase boundaries
are almost always consistent with argument boundaries;
(2) P-by-P processing is computationally less expensive
and allows to explore a relatively larger context. Most of
the groups performed a P-by-P processing, but admitting
a processing by words within the target verb chunks. The
system by Baldewein et al (2004) works with a bit more
general elements called ?chunk sequences?, extracted in
a preprocess using heuristic rules. This information is
presented in the third column of Table 3.
Information regarding clauses has proven to be very
useful, as can be seen in section 4.3. All systems captured
some kind of clause information through feature codifica-
tion. However, some of the systems restrict the search for
arguments only to the immediate clause (Park et al, 2004;
Williams et al, 2004) and others use the clause hierarchy
to guide the exploration of the sentence (Lim et al, 2004;
Carreras et al, 2004).
Very relevant to the SRL strategy is the availability of
global sentential information when decisions are taken.
Almost all of the systems try to capture some global level
information by collecting features describing the target
predicate and its context, the ?syntactic path? from the
element under consideration to the predicate, etc. (see
section 4.3). But only some of them include a global
optimization procedure at sentence level in the labeling
strategy. The systems working with Maximum Entropy
Models (Baldewein et al, 2004; Lim et al, 2004) use
beam search to find taggings that maximize the prob-
ability of the output sequence. Carreras et al (2004)
and Punyakanok et al (2004) also define a global scor-
ing function to maximize. At this point, the system of
Punyakanok et al (2004) deserves special consideration,
since it formally implements a set of structural and lin-
guistic constraints directly in the global cost function to
maximize. These constraints act as a filter for valid out-
put sequences and ensure coherence of the output. Au-
thors refer to this part of the system as the inference
layer and they implement it using integer linear program-
ming. The iterative classifier stacking mechanism used
by van den Bosch et al (2004) also tries to alleviate the
problem of locality of the low-level classifiers. This in-
formation is found in the fourth column of Table 3.
Finally, some systems use some kind of postprocess-
ing to ensure coherence of the final labeling, correct some
systematic errors, or to treat some types of adjunctive ar-
guments. In most of the cases, this postprocess is per-
formed on the basis of simple ad-hoc rules. This infor-
mation is included in the last column of Table 3.
4.3 Features
With a very few exceptions all the participant systems
have used all levels of linguistic information provided in
the training data sets, that is, words, PoS and chunk la-
bels, clauses, and named entities.
It is worth mentioning that the general type of features
prop. lab. gran. glob. post
hacioglu s t P-by-P no no
punyakanok s fl W-by-W yes no
carreras j fl P-by-P yes no
lim s t P-by-P yes no
park s rc P-by-P no yes
higgins s t W-by-W no yes
van den bosch s cj P-by-P part. yes
kouchnir s rc P-by-P no yes
baldewein s rc P-by-P yes no
williams s t mixed no no
Table 3: Main properties of the SRL strategies imple-
mented by the ten participant teams (sorted by perfor-
mance on the test set). ?prop.? stands for the treatment of
all propositions of a sentence; possible values are: s (sep-
arate) and j (joint). ?lab.? stands for labeling strategy;
possible values are: t (one step tagging), rc (recognition
+ classification), fl (filtering + labeling), cj (classifica-
tion + joining). ?gran.? stands for granularity; ?glob.?
stands for global optimization. ?post? stands for post-
processing.
derived from the basic information are strongly inspired
by previous works on the SRL task (Gildea and Jurafsky,
2002; Surdeanu et al, 2003; Pradhan et al, 2003a). Many
systems used the same kind of ideas but implemented
in different ways, since the particular learning strategies
used (see section 4.2) impose different constraints on the
type of information available or the way of expressing it.
As a general idea, we can divide the features into four
types: (1) basic features, evaluating some kind of local
information on the context of the word or constituent be-
ing treated; (2) Features characterizing the internal struc-
ture of a candidate argument; (3) Features describing
properties of the target verb predicate; (4) Features that
capture the relations between the verb predicate and the
constituent under consideration.
All systems used some kind of basic features. Roughly
speaking, they consist of words, PoS tags, chunks, clause
labels, and named entities extracted from a window-
based context. These values can be considered with
or without the relative position with respect to the el-
ement under consideration, and some n-grams of them
can also be computed. If the granularity of the sys-
tem is at phrase level then typically a representative
head word of the phrase is used as lexical information.
As an exception to the general approach, the system of
Williams et al (2004) does not make use of word forms.
The rest of the features are more interesting since they
are task dependent, and deserve special attention. Table
4 summarizes the type of features exploited by systems.
To represent an argument itself, few attributes are of
general usage. Some systems count the length of it,
with different granularities. Others make use of heuris-
tics to derive its syntactic type. There are systems that
extract a structured representation of the argument, ei-
ther homogeneous (capturing different sequences of head
words, PoS tags, chunks or clauses), or heterogeneous
(combining all elements, based on the syntactic hierar-
chy). A few systems have captured the existence of
neighboring arguments, previously identified in the pro-
cess. Interestingly, the system of Lim et al (2004) rep-
resents the context of an argument relative to the syntac-
tic hierarchy by means of relative constituent sequences
and syntactic levels. Concerning lexicalization of the
argument, most of the techniques rely on head word
rules based on Collins?, or content word rules as in
Surdeanu et al (2003). Only Carreras et al (2004) de-
cide to use a bag-of-words model, apart from heuristic-
based lexicalization.
Regarding the target verb, the voice feature of the verb
is generally used, in addition to basic features capturing
the form and PoS tag of the verb. Some systems captured
statistics on frequent argument patterns for each predi-
cate. Also, systems represented the elements in the prox-
imity of the target verb, inspired by local subcategoriza-
tion patterns of a predicate.
As for features related to a constituent-predicate pair,
all systems use the simple feature describing the relative
position between them, and to a lesser degree, the dis-
tance and the difference in clausal levels. Again, there is
a general tendency to describe the structured path from
the argument to the verb. Its design goes from sim-
ple homogeneous sequences of head words or chunks, to
more sophisticated paths combining chunks and clauses,
and capturing hierarchical properties. The system of
Park et al (2004) also tracks the number of different syn-
tactic elements found between the pair. Remarkably, the
system of Baldewein et al (2004) uses an EM clustering
technique to derive features representing the affinity of an
argument and a predicate.
On top of basic feature extraction, all teams work-
ing with SVM and VP used polynomial kernels of de-
gree 2. Similar in expressiveness, the system designed
by Punyakanok et al (2004) expanded the feature space
with all pairs of basic features.
4.4 Evaluation
A baseline rate was computed for the task. It was pro-
duced by a system developed by Erik Tjong Kim Sang,
from the University of Antwerp, Belgium. The base-
line processor finds semantic roles based on the following
seven rules:
? Tag target verb and successive particles as V.
? Tag not and n?t in target verb chunk as AM-NEG.
? Tag modal verbs in target verb chunk as AM-MOD.
? Tag first NP before target verb as A0.
? Tag first NP after target verb as A1.
? Tag that, which and who before target verb as
R-A0.
? Switch A0 and A1, and R-A0 and R-A1 if the target
verb is part of a passive VP chunk. A VP chunk is
considered in passive voice if it contains a form of
to be and the verb does not end in ing.
Table 5 presents the overall results obtained by the
ten participating systems, on the development and test
sets. The best performance was obtained by the SVM-
based IOB tagger of (Hacioglu et al, 2004), which al-
most reached the performance of 70 in F1 on the test.
The seven best systems obtained F1 scores in the range
of 60-70, and only three systems scored below that.
Comparing the results across development and test cor-
pora, most systems experienced a decrease in perfor-
mance between 1.5 and 3 points. As in previous editions
of the shared task, we attribute this behavior to a greater
difficulty of the test set instead of an overfitting effect.
Interestingly, the three systems performing below 60 in
the development set did not experienced this decrease. In
fact (Williams et al, 2004) and (Baldewein et al, 2004)
even improved the results on the test set.
Table 6 details the performance of systems for the A0-
A4 arguments, on the test set. Consistently, the best per-
forming system of the task also outperforms all other sys-
tems on these semantic roles.
5 Conclusion
We have described the CoNLL-2004 shared task on se-
mantic role labeling. The task was based on the Prop-
Bank corpus, and the challenge was to come up with ma-
chine learning techniques to recognize and label semantic
roles on the basis of partial syntactic structure. Ten sys-
tems have participated to the task, contributing with a va-
riety of standard or novel learning architectures. The best
system, presented by the most experienced group on the
task (Hacioglu et al, 2004), achieved a moderate perfor-
mance of 69.49 at the F1 measure. It is based on a SVM
tagging system, performing IOB decisions on the chunks
of the sentence, and exploiting a wide variety of features
based on partial syntax.
Most of the systems advance the state-of-the-art on se-
mantic role labeling on the basis of partial syntax. How-
ever, state-of-the-art systems working with full syntax
still perform substantially better, although far from a de-
sired behavior for real-task application. Two questions
remain open: which syntactic structures are needed as in-
put for the task, and what other sources of information are
required to obtain a real-world, accurate performance.
As a future line, a more thorough experimental eval-
uation is required to see which are the components that
sy ne al at as aw an vv vs vf vc rp di pa ex
hacioglu + + + ? ? + ? + + ? + + + + +
punyakanok + + + + + + ? + ? + + + ? + +
carreras + ? ? ? + + ? + ? ? ? + ? + +
lim + ? ? ? ? + + + ? ? ? + ? + ?
park + ? ? ? ? ? ? + ? ? + + + + +
higgins + + ? ? ? ? + + ? ? ? + + + ?
van den bosch + + ? ? ? ? ? + + ? ? + + ? ?
kouchnir + ? + ? + + ? + ? + ? + + ? ?
baldewein + + + + + + ? + + ? ? + + ? ?
williams + + ? ? ? ? ? ? ? ? ? + ? ? ?
Table 4: Main feature types used by the 10 participating systems in the CoNLL-2004 shared task, sorted by perfor-
mance on the test set. ?sy?: use of partial syntax (all levels); ?ne?: use of named entities; ?al?: argument length; ?at?:
argument type; ?as?: argument internal structure; ?aw?: head-word lexicalization of arguments; ?an?: neighboring
arguments; ?vv?: verb voice; ?vs?: verb statistics; ?vf?: verb features derived from PropBank frames; ?vc?: verb local
context; ?rp?: relative position; ?di?: distance (horizontal or in the hierarchy); ?pa?: path; ?ex?: feature expansion.
most contributed to the performance of systems.
Acknowledgements
Authors would like to thank the following people and
institutions. The PropBank team, and specially Martha
Palmer and Scott Cotton, for making the corpus available.
The CoNLL-2004 board for fruitful discussions and sug-
gestions. In particular, Erik Tjong Kim Sang for useful
comments from his valuable experience, and for making
the baseline SRL processor available. Llu??s Padro? and
Mihai Surdeanu, Grzegorz Chrupa?a, and Hwee Tou Ng
for helping us in the reviewing process and the prepara-
tion of this document. Finally, the teams contributing to
shared task, for their great interest in participating.
This work has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Department (Aliado, TIC2002-04447-C02).
Xavier Carreras is supported by a pre-doctoral grant from
the Catalan Research Department.
References
Ulrike Baldewein, Katrin Erk, Sebastian Pado?, and Detlef
Prescher. 2004. Semantic role labeling with chunk
sequences. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003, Borovets, Bulgaria.
Xavier Carreras, Llu??s Ma`rquez, and Grzegorz Chrupa?a.
2004. Hierarchical recognition of propositional argu-
ments with perceptrons. In Proceedings of CoNLL-
2004.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL-2003, Edmonton, Canada.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Informa tion and
Computation, Hong Kong, China.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for framenet clas-
sification. In Proceedings of EMNLP-2003, Sapporo,
Japan.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of EMNLP-2003, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL 2002, Philadelphia, USA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and accu-
rate part-of-speech tagging: The svm approach revis-
ited. In Proceedings of RANLP-2003, Borovets, Bul-
garia.
Kadri Hacioglu and Wayne Ward. 2003. Target word de-
tection and semantic role chunking using support vec-
tor machines. In Proceedings of HLT-NAACL 2003,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2004. Semantic role la-
beling by tagging syntactic chunks. In Proceedings of
CoNLL-2004.
Derrick Higgins. 2004. A transformation-based ap-
proach to argument labeling. In Proceedings of
CoNLL-2004.
development Precision Recall F1
hacioglu 74.18% 69.43% 71.72
punyakanok 71.96% 64.93% 68.26
carreras 73.40% 63.70% 68.21
lim 69.78% 62.57% 65.97
park 67.27% 64.36% 65.78
higgins 65.59% 60.16% 62.76
van den bosch 69.06% 57.84% 62.95
kouchnir 44.93% 63.12% 52.50
baldewein 64.90% 41.61% 50.71
williams 53.37% 32.43% 40.35
baseline 50.63% 30.30% 37.91
test Precision Recall F1
hacioglu 72.43% 66.77% 69.49
punyakanok 70.07% 63.07% 66.39
carreras 71.81% 61.11% 66.03
lim 68.42% 61.47% 64.76
park 65.63% 62.43% 63.99
higgins 64.17% 57.52% 60.66
van den bosch 67.12% 54.46% 60.13
kouchnir 56.86% 49.95% 53.18
baldewein 65.73% 42.60% 51.70
williams 58.08% 34.75% 43.48
baseline 54.60% 31.39% 39.87
Table 5: Overall precision, recall and F1 rates obtained by
the ten participating systems in the CoNLL-2004 shared
task on the development and test sets.
Beata Kouchnir. 2004. A memory-based approach for
semantic role labeling. In Proceedings of CoNLL-
2004.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, and
Hae-Chang Rim. 2004. Semantic role labeling using
maximum entropy model. In Proceedings of CoNLL-
2004.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2004. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics. Submit-
ted.
Kyung-Mi Park, Young-Sook Hwang, and Hae-Chang
Rim. 2004. Two-phase semantic role labeling
based on support vector machines. In Proceedings of
CoNLL-2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2003a. Support vector learning for semantic argument
classification. Technical Report TR-CSLR-2003-03,
Center for Spoken Language Research, University of
Colorado.
A0 A1 A2 A3 A4
hacioglu 81.37 71.63 49.33 51.11 66.67
punyakanok 79.38 68.16 46.69 34.04 65.22
carreras 79.05 66.96 43.28 31.22 62.07
lim 77.42 66.00 49.07 41.77 54.55
park 76.38 66.14 46.57 42.32 51.76
higgins 70.67 62.72 45.52 40.00 39.64
van den bosch 74.95 60.83 40.41 37.44 62.37
kouchnir 65.49 54.48 30.95 19.71 36.07
baldewein 66.76 53.37 37.60 22.89 27.69
williams 56.24 49.05 00.00 00.00 00.00
baseline 57.65 34.19 00.00 00.00 00.00
Table 6: F1 scores on the most frequent core argument
types obtained by the ten participating systems in the
CoNLL-2004 shared task on the test set. Systems sorted
by overall performance on the test set.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003b. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003), Melbourne, USA.
Vasin Punyakanok, Dan Roth, Wen-Tau Yih, Dav Zimak,
and Yuancheng Tu. 2004. Semantic role labeling via
generalized inference over classifiers. In Proceedings
of CoNLL-2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic
role labeling. In Proceedings of ECML?03, Dubrovnik,
Croatia.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference on Natural Lan-
guage Learning, CoNLL-2000.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
Erik F. Tjong Kim Sang and Herve? De?jean. 2001. Intro-
duction to the CoNLL-2001 shared task: Clause identi-
fication. In Proceedings of the 5th Conference on Nat-
ural Language Learning, CoNLL-2001.
Antal van den Bosch, Sander Canisius, Walter Daele-
mans, Iris Hendrickx, and Erik Tjong Kim Sang.
2004. Memory-based semantic role labeling: Optimiz-
ing features, algorithm, and output. In Proceedings of
CoNLL-2004.
Ken Williams, Christopher Dozier, and Andrew McCul-
loh. 2004. Learning transformation rules for semantic
role labeling. In Proceedings of CoNLL-2004.
Hierarchical Recognition of Propositional Arguments with Perceptrons
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.es
Grzegorz Chrupa?a
GRIAL Research Group
University of Barcelona (UB)
grzegorz@pithekos.net
1 Introduction
We describe a system for the CoNLL-2004 Shared Task
on Semantic Role Labeling (Carreras and Ma`rquez,
2004a). The system implements a two-layer learning ar-
chitecture to recognize arguments in a sentence and pre-
dict the role they play in the propositions. The explo-
ration strategy visits possible arguments bottom-up, navi-
gating through the clause hierarchy. The learning compo-
nents in the architecture are implemented as Perceptrons,
and are trained simultaneously online, adapting their be-
havior to the global target of the system. The learn-
ing algorithm follows the global strategy introduced in
(Collins, 2002) and adapted in (Carreras and Ma`rquez,
2004b) for partial parsing tasks.
2 Semantic Role Labeling Strategy
The strategy for recognizing propositional arguments in
sentences is based on two main observations about argu-
ment structure in the data. The first observation is the
relation of the arguments of a proposition with the chunk
and clause hierarchy: a proposition places its arguments
in the clause directly containing the verb (local clause),
or in one of the ancestor clauses. Given a clause, we de-
fine the sequence of top-most syntactic elements as the
words, chunks or clauses which are directly rooted at the
clause. Then, arguments are formed as subsequences of
top-most elements of a clause. Finally, for local clauses
arguments are found strictly to the left or to the right of
the target verb, whereas for ancestor clauses arguments
are usually to the left of the verb. This observation holds
for most of the arguments in the data. A general excep-
tion are arguments of type V, which are found only in the
local clause, starting at the position of the target verb.
The second observation is that the arguments of all
propositions of a sentence do not cross their boundaries,
and that arguments of a particular proposition are usually
found strictly within an argument of a higher level propo-
sition. Thus, the problem can be thought of as finding a
hierarchy of arguments in which arguments are embed-
ded inside others, and each argument is related to a num-
ber of propositions of a sentence in a particular role. If an
argument is related to a certain verb, no other argument
linking to the same verb can be found within it.
The system presented in this paper translates these ob-
servations into constraints which are enforced to hold in
a solution, and guide the recognition strategy. A limita-
tion of the system is that it makes no attempt to recognize
arguments which are split in many phrases.
In what follows, x is a sentence, and xi is the i-th word
of the sentence. We assume a mechanism to access the
input information of x (PoS tags, chunks and clauses),
as well as the set of target verbs V , represented by their
position. A solution y ? Y for a sentence x is a set of
arguments of the form (s, e)kv , where (s, e) represents an
argument spanning from word xs to word xe, playing a
semantic role k ? K with a verb v ? V . Finally, [S,E]
denotes a clause spanning from word xS to word sE .
The SRL(x) function, predicting semantic roles of a
sentence x, implements the following strategy:
1. Initialize set of arguments, A, to empty.
2. Define the level of each clause as its distance to the
root clause.
3. Explore clauses bottom-up, i.e. from deeper levels
to the root clause. For a clause [S,E]:
A := A ? arg search(x, [S,E])
4. Return A
2.1 Building Argument Hierarchies
Here we describe the function arg search, which builds a
set of arguments organized hierarchically, within a clause
[S,E] of a sentence x. The function makes use of two
learning-based components, defined here and described
below. First, a filtering function F, which, given a can-
didate argument, determines its plausible categories, or
rejects it when no evidence for it being an argument
is found. Second, a set of k-score functions, for each
k ? K, which, given an argument, predict a score of plau-
sibility for it being of role type k of a certain proposition.
The function arg search searches for the argument hi-
erarchy which optimizes a global score on the hierarchy.
As in earlier works, we define the global score (?) as the
summation of scores of each argument in the hierarchy.
The function explores all possible arguments in the clause
formed by contiguous top-most elements, and selects the
subset which optimizes the global score function, forcing
a hierarchy in which the arguments linked to the same
verb do not embed.
Using dynamic programming, the function can be
computed in cubic time. It considers fragments of top-
most elements, which are visited bottom-up, incremen-
tally in length, until the whole clause is explored. While
exploring, it maintains a two-dimensional matrix A of
partial solutions: each position [s, e] contains the optimal
argument hierarchy for the fragment from s to e. Finally,
the solution is found at A[S,E]. For a fragment from s to
e the algorithm is as follows:
1. A := A[s, r] ? A[r+1, e] where
r := arg maxs?r<e ?
(
A[s, r]
)
+ ?
(
A[r+1, e]
)
2. For each prop v ? V :
(a) K := F((s, e), v)
(b) Compute k? such that
k? := arg maxk?K k-score((s, e), v, x)
Set ? to the score of category k?.
(c) Set Av as the arguments in A linked to v.
(d) If (?(Av) < ?
)
then A := A\Av ?{(s, e)k
?
v }
3. A[s, e] := A
Note that an argument is visited once, and that its score
can be stored to efficiently compute the ? global score.
2.2 Start-End Filtering
The function F determines which categories in K are
plausible for an argument (s, e) to relate to a verb v.
This is done via start-end filters (FkS and FkE), one for
each type in K1. They operate on words, independently
of verbs, deciding whether a word is likely to start or end
some argument of role type k.
The selection of categories is conditional to the relative
level of the verb and the clause, and to the relative posi-
tion of the verb and the argument. The conditions are:
? v is local to the clause, and (v=s) and FVE(xe):
K := {V}
? v is local, and (e<v ? v<s):
K := {k ? K | FkS(xs) ? FkE(xe)}
1Actually, we share start-end filters for A0-A5 arguments.
? v is at deeper level, and (e<v):
K := {k ? K | k 6?K(v) ? FkS(xs) ? FkE(xe)}
where K(v) is the set of categories already assigned
to the verb in deeper clauses.
? Otherwise, K is set to empty.
Note that setting K to empty has the effect of filter-
ing out the argument for the proposition. Note also that
Start-End classifications do not depend on the verb, thus
they can be performed once per candidate word, before
entering the exploration of clauses. Then, when visiting
a clause, the Start-End filtering can be performed with
stored predictions.
3 Learning with Perceptrons
In this section we describe the learning components of
the system, namely start, end and score functions, and the
Perceptron-based algorithm to train them together online.
Each function is implemented using a linear separator,
hw : Rn ? R, operating in a feature space defined by
a feature extraction function, ? : X ? Rn, for some
instance space X . The start-end functions (FkS and FkE)
are formed by a prediction vector for each type, noted as
wkS or w
k
E, and a shared representation function ?w which
maps a word in context to a feature vector. A prediction
is computed as FkS(x) = wkS ??w(x), and similarly for the
FkE, and the sign is taken as the binary classification.
The score functions compute real-valued scores for
arguments (s, e)v . We implement these functions with
a prediction vector wk for each type k ? K, and
a shared representation function ?a which maps an
argument-verb pair to a feature vector. The score pre-
diction for a type k is then given by the expression:
k-score((s, e), v, x) = wk ? ?a((s, e), v, x).
3.1 Perceptron Learning Algorithm
We describe a mistake-driven online algorithm to train
prediction vectors together. The algorithm is essentially
the same as the one introduced in (Collins, 2002). Let W
be the set of prediction vectors:
? Initialize: ?w?W w := 0
? For each epoch t := 1 . . . T ,
for each sentence-solution pair (x, y) in training:
1. y? = SRLW (x)
2. learning feedback(W,x, y, y?)
? Return W
3.2 Learning Feedback for Filtering-Ranking
We now describe the learning feedback rule, introduced
in earlier works (Carreras and Ma`rquez, 2004b). We dif-
ferentiate two kinds of global errors in order to give feed-
back to the functions being learned: missed arguments
and over-predicted arguments. In each case, we identify
the prediction vectors responsible for producing the in-
correct argument and update them additively: vectors are
moved towards instances predicted too low, and moved
away from instances predicted too high.
Let y? be the gold set of arguments for a sentence
x, and y? those predicted by the SRL function. Let
goldS(xi, k) and goldE(xi, k) be, respectively, the per-
fect indicator functions for start and end boundaries of
arguments of type k. That is, they return 1 if word xi
starts/ends some k-argument in y? and -1 otherwise. The
feedback is as follows:
? Missed arguments: ?(s, e)kv ? y?\y?:
1. Update misclassified boundary words:
if (wkS ? ?w(xs) ? 0) then wkS = wkS + ?w(xs)
if (wkE ??w(xe) ? 0) then wkE = wkE +?w(xe)
2. Update score function, if applied:
if (k ? F ((s, e), v) then
wk = wk + ?a((s, e), v, x)
? Over-predicted arguments: ?(s, e)kp ? y?\y?:
1. Update score function:
wk = wk ? ?a((s, e), v, x)
2. Update words misclassified as S or E:
if (goldS(xs, k)=?1) then wkS = wkS??w(xs)
if (goldE(xe, k)=?1) then wkE =wkE??w(xe)
3.3 Kernel Perceptrons with Averaged Predictions
Our final architecture makes use of Voted Perceptrons
(Freund and Schapire, 1999), which compute a predic-
tion as an average of all vectors generated during train-
ing. Roughly, each vector contributes to the average pro-
portionally to the number of correct positive training pre-
dictions the vector has made. Furthermore, a prediction
vector can be expressed in dual form as a combination of
training instances, which allows the use of kernel func-
tions. We use standard polynomial kernels of degree 2.
4 Features
The features of the system are extracted from three types
of elements: words, target verbs, and arguments. They
are formed making use of PoS tags, chunks and clauses
of the sentence. The functions ?w and ?a are defined
in terms of a collection of feature extraction patterns,
which are binarized in the functions: each extracted pat-
tern forms a binary dimension indicating the existence of
the pattern in a learning instance.
Extraction on Words. The list of features extracted
from a word xi is the following:
? PoS tag.
? Form, if the PoS tag does not match with the Perl
regexp /?(CD|FW|J|LS|N|POS|SYM|V)/.
? Chunk type, of the chunk containing the word.
? Binary-valued flags: (a) Its chunk is one-word or
multi-word; (b) Starts and/or ends, or is strictly
within a chunk (3 flags); (c) Starts and/or ends
clauses (2 flags); (d) Aligned with a target verb; and
(e) First and/or last word of the sentence (2 flags).
Given a word xi, the ?w function implements a ?3
window, that is, it returns the features of the words xi+r,
with ?3?r?+3, each with its relative position r.
Extraction on Target Verbs. Given a target verb v, we
extract the following features from the word xv:
? Form, PoS tag, and target verb infinitive form.
? Voice : passive, if xv has PoS tag VBN, and either its
chunk is not VP or xv is preceded by a form of ?to
be? or ?to get? within its chunk; otherwise active.
? Chunk type.
? Binary-valued flags: (a) Its chunk is multi-word or
not; and (b) Starts and/or ends clauses (2 flags).
Extraction on Arguments. The ?a function performs
the following feature extraction for an argument (s, e)
linked to a verb v:
? Target verb features, of verb v.
? Word features, of words s?1, s, e, and e+1, each
anchored with its relative position.
? Distance of v to s and to e: for both pairs, a flag
indicating if distance is {0, 1,?1, >1, <1}.
? PoS Sequence, of PoS tags from s to e: (a) n-grams
of size 2, 3 and 4; and (b) the complete PoS pattern,
if it is less than 5 tags long.
? TOP sequence: tags of the top-most elements found
strictly from s to e. The tag of a word is its PoS. The
tag of a chunk is its type. The tag of a clause is its
type (S) enriched as follows: if the PoS tag of the
first word matches /?(IN|W|TO)/ the tag is en-
riched with the form of that word (e.g. S-to); if
that word is a verb, the tag is enriched with its PoS
(e.g. S-VBG); otherwise, it is just S. The follow-
ing features are extracted: (a) n-grams of sizes 2, 3
and 4; (b) The complete pattern, if it is less than 5
tags long; and (c) Anchored tags of the first, second,
penultimate and last elements.
? PATH sequence: tags of elements found between
the argument and the verb. It is formed by a con-
catenation of horizontal tags and vertical tags. The
horizontal tags correspond to the TOP sequence of
elements at the same level of the argument, from it to
the phrase containing the verb, both excluded. The
vertical part is the list of tags of the phrases which
contain the verb, from the phrase at the level of the
argument to the verb. The tags of the PATH se-
quence are extracted as in the TOP sequence, with
an additional mark indicating whether an element is
horizontal to the left or to the right of the argument,
or vertical. The following features are extracted: (a)
n-grams of sizes 4 and 5; and (b) The complete pat-
tern, if it is less than 5 tags long.
? Bag of Words: we consider the top-most elements
of the argument which are not clauses, and extract
all nouns, adjectives and adverbs. We then form a
separate bag for each category.
? Lexicalization: we extract the form of the head of
the first top-most element of the argument, via com-
mon head word rules; if the first element is a PP
chunk, we also extract the head of the first NP found.
5 Experiments and Results
We have build a system which implements the presented
architecture for recognizing arguments and their semantic
roles. The configuration of learning functions, related to
the roles in the CoNLL-2004 data, is set as follows :
? Five score functions for the A0?A4 types, and two
shared filtering functions FANS and FANE .
? For each of the 13 adjunct types (AM-*), a score
function and a pair of filtering functions.
? Three score functions for the R0?R2 types, and two
filtering functions FRS and FRE shared among them.
? For verbs, a score function and an end filter.
We ran the learning algorithm on the training set (with
predicted input syntax) with a polynomial kernel of de-
gree 2, for up to 8 epochs. Table 1 presents the ob-
tained results on the development set, either artificial or
real. The second and third rows provide, respectively, the
loss suffered because of errors in the filtering and scor-
ing layer. The filtering layer performs reasonably well,
since 89.44% recall can be achieved on the top of it.
However, the scoring functions clearly moderate the per-
formance, since working with perfect start-end functions
only achieve an F1 at 75.60. Finally, table 2 presents final
detailed results on the test set.
Precision Recall F?=1
g?FS, g?FE, g-score 99.92% 94.73% 97.26
FS, FE, g?score 99.90% 89.44% 94.38
g?FS, g?FE, score 85.12% 67.99% 75.60
FS, FE, score 73.40% 63.70% 68.21
Table 1: Overall results on the development set. Func-
tions with prefix g are gold functions, providing bounds
of our performance. The top row is the upper bound per-
formance of our architecture. The bottom row is the real
performance.
Precision Recall F?=1
Overall 71.81% 61.11% 66.03
A0 81.83% 76.46% 79.05
A1 68.73% 65.27% 66.96
A2 59.41% 34.03% 43.28
A3 58.18% 21.33% 31.22
A4 72.97% 54.00% 62.07
A5 0.00% 0.00% 0.00
AM-ADV 54.50% 35.50% 43.00
AM-CAU 58.33% 28.57% 38.36
AM-DIR 64.71% 22.00% 32.84
AM-DIS 64.06% 57.75% 60.74
AM-EXT 100.00% 50.00% 66.67
AM-LOC 35.62% 22.81% 27.81
AM-MNR 50.89% 22.35% 31.06
AM-MOD 97.57% 95.25% 96.40
AM-NEG 90.23% 94.49% 92.31
AM-PNC 36.11% 15.29% 21.49
AM-PRD 0.00% 0.00% 0.00
AM-TMP 61.86% 48.86% 54.60
R-A0 78.85% 77.36% 78.10
R-A1 64.29% 51.43% 57.14
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 98.32% 98.24% 98.28
Table 2: Results on the test set
Acknowledgements
This research is supported by the European Commission
(Meaning, IST-2001-34460) and the Spanish Research Depart-
ment (Aliado, TIC2002-04447-C02). Xavier Carreras is sup-
ported by a grant from the Catalan Research Department.
References
Xavier Carreras and Llu?is Ma`rquez. 2004a. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu?is Ma`rquez. 2004b. Online
learning via global feedback for phrase recognition.
In Advances in Neural Information Processing Systems
16. MIT Press.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of the
EMNLP?02.
Y. Freund and R. E. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 152?164, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Introduction to the CoNLL-2005 Shared Task:
Semantic Role Labeling
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.edu
Abstract
In this paper we describe the CoNLL-
2005 shared task on Semantic Role La-
beling. We introduce the specification and
goals of the task, describe the data sets and
evaluation methods, and present a general
overview of the 19 systems that have con-
tributed to the task, providing a compara-
tive description and results.
1 Introduction
In the few last years there has been an increasing
interest in shallow semantic parsing of natural lan-
guage, which is becoming an important component
in all kind of NLP applications. As a particular case,
Semantic Role Labeling (SRL) is currently a well-
defined task with a substantial body of work and
comparative evaluation. Given a sentence, the task
consists of analyzing the propositions expressed by
some target verbs of the sentence. In particular, for
each target verb all the constituents in the sentence
which fill a semantic role of the verb have to be rec-
ognized. Typical semantic arguments include Agent,
Patient, Instrument, etc. and also adjuncts such as
Locative, Temporal, Manner, Cause, etc.
Last year, the CoNLL-2004 shared task aimed
at evaluating machine learning SRL systems based
only on partial syntactic information. In (Carreras
and Ma`rquez, 2004) one may find a detailed review
of the task and also a brief state-of-the-art on SRL
previous to 2004. Ten systems contributed to the
task, which was evaluated using the PropBank cor-
pus (Palmer et al, 2005). The best results were
around 70 in F1 measure. Though not directly com-
parable, these figures are substantially lower than the
best results published up to date using full parsing
as input information (F1 slightly over 79). In addi-
tion to the CoNLL-2004 shared task, another evalua-
tion exercise was conducted in the Senseval-3 work-
shop (Litkowski, 2004). Eight systems relying on
full parsing information were evaluated in that event
using the FrameNet corpus (Fillmore et al, 2001).
From the point of view of learning architectures and
study of feature relevance, it is also worth mention-
ing the following recent works (Punyakanok et al,
2004; Moschitti, 2004; Xue and Palmer, 2004; Prad-
han et al, 2005a).
Following last year?s initiative, the CoNLL-2005
shared task1 will concern again the recognition of
semantic roles for the English language. Compared
to the shared task of CoNLL-2004, the novelties in-
troduced in the 2005 edition are:
? Aiming at evaluating the contribution of full
parsing in SRL, the complete syntactic trees
given by two alternative parsers have been pro-
vided as input information for the task. The
rest of input information does not vary and cor-
responds to the levels of processing treated in
the previous editions of the CoNLL shared task,
i.e., words, PoS tags, base chunks, clauses, and
named entities.
? The training corpus has been substantially en-
larged. This allows to test the scalability of
1The official CoNLL-2005 shared task web page, in-
cluding data, software and systems? outputs, is available at
http://www.lsi.upc.edu/?srlconll.
152
learning-based SRL systems to big datasets and
to compute learning curves to see how much
data is necessary to train. Again, we concen-
trate on the PropBank corpus (Palmer et al,
2005), which is the Wall Street Journal part
of the Penn TreeBank corpus enriched with
predicate?argument structures.
? In order to test the robustness of the pre-
sented systems, a cross-corpora evaluation is
performed using a fresh test set from the Brown
corpus.
Regarding evaluation, two different settings were
devised depending if the systems use the informa-
tion strictly contained in the training data (closed
challenge) or they make use of external sources
of information and/or tools (open challenge). The
closed setting allows to compare systems under
strict conditions, while the open setting aimed at ex-
ploring the contributions of other sources of infor-
mation and the limits of the current learning-based
systems on the SRL task. At the end, all 19 systems
took part in the closed challenge and none of them
in the open challenge.
The rest of the paper is organized as follows. Sec-
tion 2 describes the general setting of the task. Sec-
tion 3 provides a detailed description of training,
development and test data. Participant systems are
described and compared in section 4. In particular,
information about learning techniques, SRL strate-
gies, and feature development is provided, together
with performance results on the development and
test sets. Finally, section 5 concludes.
2 Task Description
As in the 2004 edition, the goal of the task was to
develop a machine learning system to recognize ar-
guments of verbs in a sentence, and label them with
their semantic role. A verb and its set of arguments
form a proposition in the sentence, and typically, a
sentence contains a number of propositions.
There are two properties that characterize the
structure of the arguments in a proposition. First, ar-
guments do not overlap, and are organized sequen-
tially. Second, an argument may appear split into
a number of non-contiguous phrases. For instance,
in the sentence ?[A1 The apple], said John, [C?A1
is on the table]?, the utterance argument (labeled
with type A1) appears split into two phrases. Thus,
there is a set of non-overlapping arguments labeled
with semantic roles associated with each proposi-
tion. The set of arguments of a proposition can be
seen as a chunking of the sentence, in which chunks
are parts of the semantic roles of the proposition
predicate.
In practice, number of target verbs are marked
in a sentence, each governing one proposition. A
system has to recognize and label the arguments of
each target verb. To support the role labeling task,
sentences contain input annotations, that consist of
syntactic information and named entities. Section 3
describes in more detail the annotations of the data.
2.1 Evaluation
Evaluation is performed on a collection of unseen
test sentences, that are marked with target verbs and
contain only predicted input annotations.
A system is evaluated with respect to precision,
recall and the F1 measure of the predicted argu-
ments. Precision (p) is the proportion of arguments
predicted by a system which are correct. Recall (r)
is the proportion of correct arguments which are pre-
dicted by a system. Finally, the F1 measure com-
putes the harmonic mean of precision and recall, and
is the final measure to compare the performance of
systems. It is formulated as: F?=1 = 2pr/(p + r).
For an argument to be correctly recognized, the
words spanning the argument as well as its semantic
role have to be correct. 2
As an exceptional case, the verb argument of each
proposition is excluded from the evaluation. This ar-
gument is the lexicalization of the predicate of the
proposition. Most of the time, the verb corresponds
to the target verb of the proposition, which is pro-
vided as input, and only in few cases the verb par-
ticipant spans more words than the target verb. Ex-
cept for non-trivial cases, this situation makes the
verb fairly easy to identify and, since there is one
verb with each proposition, evaluating its recogni-
tion over-estimates the overall performance of a sys-
tem. For this reason, the verb argument is excluded
from evaluation.
2The srl-eval.pl program is the official program to
evaluate the performance of a system. It is available at the
Shared Task web page.
153
And CC * (S* (S* * - (AM-DIS*) (AM-DIS*)
to TO (VP* (S* (S(VP* * - * (AM-PNC*
attract VB *) * (VP* * attract (V*) *
younger JJR (NP* * (NP* * - (A1* *
listeners NNS *) *) *)))) * - *) *)
, , * * * * - * *
Radio NNP (NP* * (NP* (ORG* - (A0* (A0*
Free NNP * * * * - * *
Europe NNP *) * *) *) - *) *)
intersperses VBZ (VP*) * (VP* * intersperse * (V*)
the DT (NP* * (NP(NP* * - * (A1*
latest JJS *) * *) * - * *
in IN (PP*) * (PP* * - * *
Western JJ (NP* * (NP* (MISC*) - * *
rock NN * * * * - * *
groups NNS *) * *)))) * - * *)
. . * *) *) * - * *
Figure 1: An example of an annotated sentence, in columns. Input consists of words (1st column), PoS
tags (2nd), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th). The 7th
column marks target verbs, and their propositions are found in remaining columns. According to the
PropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; for
intersperse (9th), A0 is the arranger, and A1 the entity interspersed.
2.2 Closed Challenge Setting
The organization provided training, development
and test sets derived from the standard sections of
the Penn TreeBank (Marcus et al, 1993) and Prop-
Bank (Palmer et al, 2005) corpora.
In the closed challenge, systems have to be built
strictly with information contained in the training
sections of the TreeBank and PropBank. Since this
collection contains the gold reference annotations
of both syntactic and predicate-argument structures,
the closed challenge allows: (1) to make use of any
preprocessing system strictly developed within this
setting, and (2) to learn from scratch any annotation
that is contained in the data. To support the former,
the organization provided the output of state-of-the-
art syntactic preprocessors, described in Section 3.
The development set is used to tune the parame-
ters of a system. The gold reference annotations are
also available in this set, but only to evaluate the per-
formance of different parametrizations of a system,
and select the optimal one. Finally, the test set is
used to evaluate the performance of a system. It is
only allowed to use predicted annotations in this set.
Since all systems in this setting have had access to
the same training and development data, the evalua-
tion results on the test obtained by different systems
are comparable in a fair manner.
3 Data
The data consists of sections of the Wall Street Jour-
nal part of the Penn TreeBank (Marcus et al, 1993),
with information on predicate-argument structures
extracted from the PropBank corpus (Palmer et al,
2005). In this edition of the CoNLL shared task,
we followed the standard partition used in syntactic
parsing: sections 02-21 for training, section 24 for
development, and section 23 for test. In addition, the
test set of the shared task includes three sections of
the Brown corpus (namely, ck01-03). The predicate-
argument annotations of the latter test material were
kindly provided by the PropBank team, and are very
valuable, as they allow to evaluate learning systems
on a portion of data that comes from a different
source than training.
We first describe the annotations related to argu-
ment structures. Then, we describe the preprocess-
ing systems that have been selected to predict the
input part of the data. Figure 1 shows an example of
a fully-annotated sentence.
3.1 PropBank
The Proposition Bank (PropBank) (Palmer et al,
2005) annotates the Penn TreeBank with verb argu-
ment structure. The semantic roles covered by Prop-
Bank are the following:
154
? Numbered arguments (A0?A5, AA): Argu-
ments defining verb-specific roles. Their se-
mantics depends on the verb and the verb us-
age in a sentence, or verb sense. The most
frequent roles are A0 and A1 and, commonly,
A0 stands for the agent and A1 corresponds to
the patient or theme of the proposition. How-
ever, no consistent generalization can be made
across different verbs or different senses of the
same verb. PropBank takes the definition of
verb senses from VerbNet, and for each verb
and each sense defines the set of possible roles
for that verb usage, called the roleset. The def-
inition of rolesets is provided in the PropBank
Frames files, which is made available for the
shared task as an official resource to develop
systems.
? Adjuncts (AM-): General arguments that any
verb may take optionally. There are 13 types of
adjuncts:
AM-ADV : general-purpose AM-MOD : modal verb
AM-CAU : cause AM-NEG : negation marker
AM-DIR : direction AM-PNC : purpose
AM-DIS : discourse marker AM-PRD : predication
AM-EXT : extent AM-REC : reciprocal
AM-LOC : location AM-TMP : temporal
AM-MNR : manner
? References (R-): Arguments representing ar-
guments realized in other parts of the sentence.
The role of a reference is the same as the role of
the referenced argument. The label is an R- tag
prefixed to the label of the referent, e.g. R-A1.
? Verbs (V): Argument corresponding to the verb
of the proposition. Each proposition has exa-
clty one verb argument.
We used PropBank-1.0. Most predicative verbs
were annotated, although not all of them (for exam-
ple, most of the occurrences of the verb ?to have?
and ?to be? were not annotated). We applied proce-
dures to check consistency of propositions, looking
for overlapping arguments, and incorrect semantic
role labels. Also, co-referenced arguments were an-
notated as a single item in PropBank, and we au-
tomatically distinguished between the referent and
the reference with simple rules matching pronomi-
nal expressions, which were tagged as R arguments.
Train. Devel. tWSJ tBrown
Sentences 39,832 1,346 2,416 426
Tokens 950,028 32,853 56,684 7,159
Propositions 90,750 3,248 5,267 804
Verbs 3,101 860 982 351
Arguments 239,858 8,346 14,077 2,177
A0 61,440 2,081 3,563 566
A1 84,917 2,994 4,927 676
A2 19,926 673 1,110 147
A3 3,389 114 173 12
A4 2,703 65 102 15
A5 68 2 5 0
AA 14 1 0 0
AM 7 0 0 0
AM-ADV 8,210 279 506 143
AM-CAU 1,208 45 73 8
AM-DIR 1,144 36 85 53
AM-DIS 4,890 202 320 22
AM-EXT 628 28 32 5
AM-LOC 5,907 194 363 85
AM-MNR 6,358 242 344 110
AM-MOD 9,181 317 551 91
AM-NEG 3,225 104 230 50
AM-PNC 2,289 81 115 17
AM-PRD 66 3 5 1
AM-REC 14 0 2 0
AM-TMP 16,346 601 1,087 112
R-A0 4,112 146 224 25
R-A1 2,349 83 156 21
R-A2 291 5 16 0
R-A3 28 0 1 0
R-A4 7 0 1 0
R-AA 2 0 0 0
R-AM-ADV 5 0 2 0
R-AM-CAU 41 3 4 2
R-AM-DIR 1 0 0 0
R-AM-EXT 4 1 1 0
R-AM-LOC 214 9 21 4
R-AM-MNR 143 6 6 2
R-AM-PNC 12 0 0 0
R-AM-TMP 719 31 52 10
Table 1: Counts on the data sets.
A total number of 80 propositions were not compli-
ant with our procedures (one in the Brown files, the
rest in WSJ) and were filtered out from the CoNLL
data sets.
Table 1 provides counts of the number of sen-
tences, tokens, annotated propositions, distinct
verbs, and arguments in the four data sets.
3.2 Preprocessing Systems
In this section we describe the selected processors
that computed input annotations for the SRL sys-
tems. The annotations are: part-of-speech (PoS)
tags, chunks, clauses, full syntactic trees and named
entities. As it has been noted, participants were also
155
allowed to use any processor developed within the
same WSJ partition.
The preprocessors correspond to the following
state-of-the-art systems:
? UPC processors, consisting of:
? PoS tagger: (Gime?nez and Ma`rquez,
2003), based on Support Vector Machines,
and trained on WSJ sections 02-21.
? Base Chunker and Clause Recognizer:
(Carreras and Ma`rquez, 2003), based on
Voted Perceptrons, trained on WSJ sec-
tions 02-21. These two processors form a
coherent partial syntax of a sentence, that
is, chunks and clauses form a partial syn-
tactic tree.
? Full parser of Collins (1999), with ?model 2?.
Predicts WSJ full parses, with information of
the lexical head for each syntactic constituent.
The PoS tags (required by the parser) have been
computed with (Gime?nez and Ma`rquez, 2003).
? Full parser of Charniak (2000). Jointly predicts
PoS tags and full parses.
? Named Entities predicted with the Maximum-
Entropy based tagger of Chieu and Ng (2003).
The tagger follows the CoNLL-2003 task set-
ting (Tjong Kim Sang and De Meulder, 2003),
and thus is not developed with WSJ data. How-
ever, we allowed its use because there is no
available named entity recognizer developed
with WSJ data. The reported performance on
the CoNLL-2003 test is F1 = 88.31, with
Prec/Rec. at 88.12/88.51.
Tables 2 and 3 summarize the performance of
the syntactic processors on the development and test
sets. The performance of full parsers on the WSJ
test is lower than that reported in the correspond-
ing papers. The reason is that our evaluation fig-
ures have been computed in a strict manner with re-
spect to punctuation tokens, while the full parsing
community usually does not penalize for punctua-
tion wrongly placed in the tree.3 As it can be ob-
3Before evaluating Collins?, we raised punctuation to the
highest point in the tree, using a script that is available at the
shared task webpage. Otherwise, the performance would have
Prec./Recall figures below 37.
Dev. tWSJ tBrown
UPC PoS-tagger 97.13 97.36 94.73
Charniak (2000) 92.01 92.29 87.89
Table 2: Accuracy (%) of PoS taggers.
served, the performance of all syntactic processors
suffers a substantial loss in the Brown test set. No-
ticeably, the parser of Collins (1999) seems to be the
more robust when moving from WSJ to Brown.
4 A Review of Participant Systems
Nineteen systems participated in the CoNLL-2005
shared task. They approached the task in several
ways, using different learning components and la-
beling strategies. The following subsections briefly
summarize the most important properties of each
system and provide a qualitative comparison be-
tween them, together with a quantitative evaluation
on the development and test sets.
4.1 Learning techniques
Up to 8 different learning algorithms have been ap-
plied to train the learning components of partici-
pant systems. See the ?ML-method? column of ta-
ble 4 for a summary of the following information.
Log?linear models and vector-based linear classi-
fiers dominated over the rest. Probably, this is due to
the versatility of the approaches and the availability
of very good software toolkits.
In particular, 8 teams used the Maximum En-
tropy (ME) statistical framework (Che et al, 2005;
Haghighi et al, 2005; Park and Rim, 2005; Tjong
Kim Sang et al, 2005; Sutton and McCallum, 2005;
Tsai et al, 2005; Yi and Palmer, 2005; Venkatapathy
et al, 2005). Support Vector Machines (SVM) were
used by 6 teams. Four of them with the standard
polynomial kernels (Mitsumori et al, 2005; Tjong
Kim Sang et al, 2005; Tsai et al, 2005; Pradhan et
al., 2005b), another one using Gaussian kernels (Oz-
gencil and McCracken, 2005), and a last group using
tree-based kernels specifically designed for the task
(Moschitti et al, 2005). Another team used also a re-
lated learning approach, SNoW, which is a Winnow-
based network of linear separators (Punyakanok et
al., 2005).
Decision Tree learning (DT) was also represented
156
Devel. Test WSJ Test Brown
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1
UPC Chunker 94.66 93.17 93.91 95.26 94.52 94.89 92.64 90.85 91.73
UPC Clauser 90.38 84.73 87.46 90.93 85.94 88.36 84.21 74.32 78.95
Collins (1999) 85.02 83.55 84.28 85.63 85.20 85.41 82.68 81.33 82.00
Charniak (2000) 87.60 87.38 87.49 88.20 88.30 88.25 80.54 81.15 80.84
Table 3: Results of the syntactic parsers on the development, and WSJ and Brown test sets. Unlike in full
parsing, the figures have been computed on a strict evaluation basis with respect to punctuation.
by Ponzetto and Strube (2005), who used C4.5.
Ensembles of decision trees learned through the
AdaBoost algorithm (AB) were applied by Ma`rquez
et al (2005) and Surdeanu and Turmo (2005). Tjong
Kim Sang et al (2005) applied, among others,
Memory-Based Learning (MBL).
Regarding novel learning paradigms not applied
in previous shared tasks, we find Relevant Vector
Machine (RVM), which is a kernel?based linear dis-
criminant inside the framework of Sparse Bayesian
Learning (Johansson and Nugues, 2005) and Tree
Conditional Random Fields (T-CRF) (Cohn and
Blunsom, 2005), that extend the sequential CRF
model to tree structures. Finally, Lin and Smith
(2005) presented a proposal radically different from
the rest, with very light learning components. Their
approach (Consensus in Pattern Matching, CPM)
contains some elements of Memory-based Learning
and ensemble classification.
From the Machine Learning perspective, system
combination is another interesting component ob-
served in many of the proposals. This fact, which is
a difference from last year shared task, is explained
as an attempt of increasing the robustness and cover-
age of the systems, which are quite dependent on in-
put parsing errors. The different outputs to combine
are obtained by varying input information, chang-
ing learning algorithm, or considering n-best solu-
tion lists. The combination schemes presented in-
clude very simple voting-like combination heuris-
tics, stacking of classifiers, and a global constraint
satisfaction framework modeled with Integer Linear
Programming. Global models trained to re-rank al-
ternative outputs represent a very interesting alter-
native that has been proposed by two systems. All
these issues are reviewed in detail in section 4.2.
4.2 SRL approaches
SRL is a complex task, which may be decomposed
into a number of simpler decisions and annotating
schemes in order to be addressed by learning tech-
niques. Table 4 contains a summary of the main
properties of the 19 systems presented. In this sec-
tion we will explain the contents of that table by
columns (from left-to-right).
One first issue to consider is the input structure
to navigate in order to extract the constituents that
will form labeled arguments. The majority of sys-
tems perform parse tree node labeling, searching
for a one?to?one map between arguments and parse
constituents. This information is summarized in the
?synt? column of Table 4. ?col?, ?cha?, ?upc? stand
for the syntactic parse trees (the latter is partial) pro-
vided as input by the organization. Additionally,
some teams used lists of n-best parsings generated
by available tools (?n-cha? by Charniak parser; ?n-
bikel? by Bikel?s implementation of Collins parser).
Interestingly, Yi and Palmer (2005) retrained Rat-
naparkhi?s parser using the WSJ training sections
enriched with semantic information coming from
PropBank annotations. These are referred to as AN
and AM parses. As it can be seen, Charniak parses
were used by most of the systems. Collins parses
were used also in some of the best performing sys-
tems based on combination.
The exceptions to the hierarchical processing are
the systems by Pradhan et al (2005b) and Mitsumori
et al (2005), which perform a chunking-based se-
quential tokenization. As for the former, the system
is the same than the one presented in the 2004 edi-
tion. The system by Ma`rquez et al (2005) explores
hierarchical syntactic structures but selects, in a pre-
process, a sequence of tokens to perform a sequen-
tial tagging afterwards.
157
ML-method synt pre label embed glob post comb type
punyakanok SNoW n-cha,col x&p i+c defer yes no n-cha+col ac-ILP
haghighi ME n-cha ? i+c dp-prob yes no n-cha re-rank
marquez AB cha,upc seq bio !need no no cha+upc s-join
pradhan SVM cha,col/chunk ? c/bio ? no no cha+col?chunk stack
surdeanu AB cha prun c g-top no yes no ?
tsai ME,SVM cha x&p c defer yes no ME+SVM ac-ILP
che ME cha no c g-score no yes no ?
moschitti SVM cha prun i+c !need no no no ?
tjongkimsang ME,SVM,TBL cha prun i+c !need no yes ME+SVM+TBL s-join
yi ME cha,AN,AM x&p i+c defer no no cha+AN+AM ac-join
ozgencil SVM cha prun i+c g-score no no no ?
johansson RVM cha softp i+c ? no no no ?
cohn T-CRF col x&p c g-top yes no no ?
park ME cha prun i+c ? no no no ?
mitsumori SVM chunk no bio !need no no no ?
venkatapathy ME col prun i+c frames yes no no ?
ponzetto DT col prun c g-top no yes no ?
lin CPM cha gt-para i+c !need no no no ?
sutton ME n-bikel x&p i+c dp-prob yes no n-bikel re-rank
Table 4: Main properties of the SRL strategies implemented by the participant teams, sorted by F1 per-
formance on the WSJ+Brown test set. synt stands for the syntactic structure explored; pre stands for
pre-processing steps; label stands for the labeling strategy; embed stands for the technique to ensure non-
embedding of arguments; glob stands for global optimization; post stands for post-processing; comb stands
for system output combination, and type stands for the type of combination. Concrete values appearing in
the table are explained in section 4.1. The symbol ??? stands for unknown values not reported by the system
description papers.
In general, the presented systems addressed the
SRL problem by applying different chained pro-
cesses. In Table 4 the column ?pre? summarizes pre-
processing. In most of the cases this corresponds to
a pruning procedure to filter out constituents that are
not likely to be arguments. As in feature develop-
ment, the related bibliography has been followed for
pruning. For instance, many systems used the prun-
ing strategy described in (Xue and Palmer, 2004)
(?x&p?) and other systems used the soft pruning
rules described in (Pradhan et al, 2005a) (?softp?).
Remarkably, Park and Rim (2005) parametrize the
pruning procedure and then study the effect of be-
ing more or less aggressive at filtering constituents.
In the case of Ma`rquez et al (2005), pre-processing
corresponds to a sequentialization of syntactic hier-
archical structures. As a special case, Lin and Smith
(2005) used the GT-PARA analyzer for converting
parse trees into a flat representation of all predicates
including argument boundaries.
The second stage, reflected in column ?label? of
Table 4, is the proper labeling of selected candi-
dates. Most of the systems used a two-step proce-
dure consisting of first identifying arguments (e.g.,
with a binary ?null? vs. ?non-null? classifier) and
then classifying them. This is referred to as ?i+c? in
the table. Some systems address this phase in a sin-
gle classification step by adding a ?null? category
to the multiclass problem (referred to as ?c?). The
methods performing a sequential tagging use a BIO
tagging scheme (?bio?). As a special case, Mos-
chitti et al (2005) subdivide the ?i+c? strategy into
four phases: after identification, heuristics are ap-
plied to assure compatibility of identified arguments;
and, before classifying arguments into roles, a pre-
classification into core vs. adjunct arguments is per-
formed. Venkatapathy et al (2005) use three labels
instead of two in the identification phase : ?null?,
?mandatory?, and ?optional?.
Since arguments in a solution do not embed and
most systems identify arguments as nodes in a hier-
archical structure, non-embedding constraints must
be resolved in order to generate a coherent argu-
ment labeling. The ?embed? column of Table 4 ac-
counts for this issue. The majority of systems ap-
plied specific greedy procedures that select a subset
of consistent arguments. The families of heuristics
to do that selection include prioritizing better scored
158
constituents (?g-score?), or selecting the arguments
that are first reached in a top-down exploration (?g-
top?). Some probabilistic systems include the non-
embedding constraints within the dynamic program-
ming inference component, and thus calculate the
most probable coherent labeling (?dp-prob?). The
?defer? value means that this is a combination sys-
tem and that coherence of the individual system pre-
dictions is not forced, but deferred to the later com-
bination step. As a particular case, Venkatapathy et
al. (2005) use PropBank subcategorization frames to
force a coherent solution. Note that tagging-based
systems do not need to check non-embedding con-
straints (?!need? value).
The ?glob? column of Table 4 accounts for the lo-
cality/globality of the process used to calculate the
output solution given the argument prediction candi-
dates. Systems with a ?yes? value in that column de-
fine some kind of scoring function (possibly proba-
bilistic) that applies to complete candidate solutions,
and then calculate the solution that maximizes the
scoring using an optimization algorithm.
Some systems use some kind of postprocessing to
improve the final output of the system by correct-
ing some systematic errors, or treating some types
of simple adjunct arguments. This information is in-
cluded in the ?post? column of Table 4. In most of
the cases, this postprocess is performed on the basis
of simple ad-hoc rules. However, it is worth men-
tioning the work of Tjong Kim Sang et al (2005)
in which spelling error correction techniques are
adapted for improving the resulting role labeling. In
that system, postprocessing is applied before system
combination.
Most of the best performing systems included a
combination of different base subsystems to increase
robustness of the approach and to gain coverage and
independence from parse errors. Last 2 columns of
Table 4 present this information. In the ?comb? col-
umn the source of the combination is reported. Basi-
cally, the alternative outputs to combine can be gen-
erated by different input syntactic structures or n-
best parse candidates, or by applying different learn-
ing algorithms to the same input information.
The type of combination is reported in the last col-
umn. Ma`rquez et al (2005) and Tjong Kim Sang
et al (2005) performed a greedy merging of the ar-
guments of base complete solutions (?s-join?). Yi
and Palmer (2005) did also a greedy merging of ar-
guments but taking into account not complete so-
lutions but all candidate arguments labeled by base
systems (?ac-join?). In a more sophisticated way,
Punyakanok et al (2005) and Tsai et al (2005) per-
formed global inference as constraint satisfaction
using Integer Linear Programming, also taking into
account all candidate arguments (?ac-ILP?). It is
worth noting that the generalized inference applied
in those papers allows to include, jointly with the
combination of outputs, a number of linguistically-
motivated constraints to obtain a coherent solution.
Pradhan et al (2005b) followed a stacking ap-
proach by learning a chunk-based SRL system in-
cluding as features the outputs of two syntax-based
systems. Finally, Haghighi et al (2005) and Sut-
ton and McCallum (2005) performed a different ap-
proach by learning a re-ranking function as a global
model on top of the base SRL models. Actually,
Haghighi et al (2005) performed a double selection
step: an inner re-ranking of n-best solutions coming
from the base system on a single tree; and an outer
selection of the final solution among the candidate
solutions coming from n-best parse trees. The re-
ranking approach allows to define global complex
features applying to complete candidate solutions to
train the rankers.
4.3 Features
Looking at the description of the different systems, it
becomes clear that the general type of features used
in this edition is strongly based on previous work on
the SRL task (Gildea and Jurafsky, 2002; Surdeanu
et al, 2003; Pradhan et al, 2005a; Xue and Palmer,
2004). With no exception, all systems have made
intensive use of syntax to extract features. While
most systems work only on the output of a parser
?Charniak?s being the most preferred? some sys-
tems depend on many syntactic parsers. In the latter
situation, either a system is a combination of many
individual systems (each working with a different
parser), or a system extracts features from many dif-
ferent parse trees while exploring the nodes of only
one parse tree. Most systems have also considered
named entities for extracting features.
The main types of features seen in this SRL edi-
tion can be divided into four general categories: (1)
Features characterizing the structure of a candidate
159
sources argument verb arg?verb p
synt ne at aw ab ac ai pp sd v sc rp di ps pv pi sf as
punyakanok cha,col,upc + + h + t + + ? + + + c + ? + + ?
haghighi cha ? + h + p,s ? + + + + + t + + ? ? +
marquez cha,upc + + h + t + ? + + + + w,c + + ? + ?
pradhan cha,col,upc + + h,c + p,s,t + + ? + + + c,t + + + + ?
surdeanu cha + + h,c + p,s + ? + + + + w,t + + + ? ?
tsai cha,upc + + h + p,s,t ? ? ? + + + w + ? ? ? ?
che cha + + h + ? ? + ? + + + t + + ? ? ?
moschitti cha ? + h + p + + ? + + + t + + ? + ?
tjongkimsang cha + + ? + p,t ? + ? + + + w,t + + + ? ?
yi cha,an,am ? + h,c ? p,s ? + ? + + + w + ? ? + ?
ozgencil cha ? + h ? p ? ? ? + + + ? + + ? ? ?
johansson cha,upc + + h ? ? ? ? ? + + + ? + + ? ? ?
cohn col ? + h + p,s ? + ? + + + w + ? + + ?
park cha ? + h,c ? p ? ? ? + + + ? + ? + ? ?
mitsumori upc,cha + + ? + t ? ? + + ? + c,t ? + ? ? ?
venkatapathy col + + h + ? ? ? ? + ? + ? + ? ? ? ?
ponzetto col,upc + + h + ? + ? ? + ? ? w,c,t ? ? + ? ?
lin cha ? + h + ? ? ? ? + ? + w ? ? ? ? ?
sutton bik ? + h + p,s ? ? ? + ? + ? + ? ? ? +
Table 5: Main feature types used by the 19 participating systems in the CoNLL-2005 shared task, sorted by
performance on the WSJ+Brown test set. Sources: synt: use of parsers, namely Charniak (cha), Collins
(col), UPC partial parsers (upc), Bikel?s Collins model (bik) and/or argument-enriched parsers (an,am); ne:
use of named entities. On the argument: at: argument type; aw: argument words, namely the head (h)
and/or content words (c); ab: argument boundaries, i.e. form and PoS of first and/or last argument words; ac:
argument context, capturing features of the parent (p) and/or left/right siblings (s), or the tokens surrounding
the argument (t); ai: indicators of the structure of the argument (e,g., on internal constituents, surround-
ing/boundary punctuation, governing category, etc.); pp: specific features for prepositional phrases; sd:
semantic dictionaries. On the verb: v: standard verb features (voice, word/lemma, PoS); sc: subcatego-
rization. On the arg-verb relation: rp: relative position; di: distance, based on words (w), chunks (c) or
the syntactic tree (t); ps: standard path; pv: path variations; pi: scalar indicator variables on the path (of
chunks, clauses, or other phrase types), common ancestor, etc.; sf: syntactic frame (Xue and Palmer, 2004);
On the complete proposition: as: sequence of arguments of a proposition.
argument; (2) Features describing properties of the
target verb predicate; (3) Features that capture the
relation between the verb predicate and the con-
stituent under consideration; and (4) Global features
describing the complete argument labeling of a pred-
icate. The rest of the section describes the most com-
mon feature types in each category. Table 5 summa-
rizes the type of features exploited by systems.
To represent an argument itself, all systems make
use of the syntactic type of the argument. Almost
all teams used the heuristics of Collins (1999) to ex-
tract the head word of the argument, and used fea-
tures that capture the form, lemma and PoS tag of
the head. In the same line, some systems also use
features of the content words of the argument, using
the heuristics of Surdeanu et al (2003). Very gen-
erally also, many systems extract features from the
first and last words of the argument. Regarding the
syntactic elements surrounding the argument, many
systems working on full trees have considered the
parent and siblings of the argument, capturing their
syntactic type and head word. Differently, other
systems have captured features from the left/right
tokens surrounding the argument, which are typi-
cally words, but can be chunks or general phrases in
systems that sequentialize the task (Ma`rquez et al,
2005; Pradhan et al, 2005b; Mitsumori et al, 2005).
Many systems use a variety of indicator features that
capture properties of the argument structure and its
local syntactic annotations. For example, indicators
of the immediate syntactic types that form the argu-
ment, flags raised by punctuation tokens in or nearby
the argument, or the governing category feature of
Gildea and Jurafsky (2002). It is also somewhat gen-
160
eral the use of specific features that apply when the
constituent is a prepositional phrase, such as look-
ing for the head word of the noun phrase within it.
A few systems have also built semantic dictionaries
from training data, that collect words appearing fre-
quently in temporal, locative or other arguments.
To represent the predicate, all systems have used
features codifying the form, lemma, PoS tag and
voice of the verb. It is also of general use the subcat-
egorization feature, capturing the syntactic rule that
expands the parent of the predicate. Some systems
captured statistics related to the frequency of a verb
in training data (not in Table 5).
Regarding features related to an argument-verb
pair, almost all systems use the simple feature de-
scribing the relative position between them. To
a lesser degree, systems have computed distances
from one to the other, based on the number of words
or chunks between them, or based on the syntactic
tree. Not surprisingly, all systems have extracted the
path from the argument to the verb. While almost
all systems use the standard path of (Gildea and Ju-
rafsky, 2002), many have explored variations of it.
A common one consists of the path from the argu-
ment to the lowest common ancestor of the verb and
the argument. Another variation is the partial path,
that is built of chunks and clauses only. Indicator
features that capture scalar values of the path are
also common, and concentrate mainly on looking
at the common ancestor, capturing the difference of
clausal levels, or looking for punctuation and other
linguistic elements in the path. In this category, it is
also noticeable the use of the syntactic frame feature,
proposed by Xue and Palmer (2004).
Finally, in this edition two systems apply learn-
ing at a global context (Haghighi et al, 2005; Sut-
ton and McCallum, 2005) and, consequently, they
are able to extract features from a complete labeling
of a predicate. Basically, the central feature in this
context extracts the sequential pattern of predicate
arguments. Then, this pattern can be enriched with
syntactic categories, broken down into role-specific
indicator variables, or conjoined with the predicate
lemma.
Apart from basic feature extraction, combination
of features has also been explored in this edition.
Many of the combinations depart from the manually
selected conjunctions of Xue and Palmer (2004).
4.4 Evaluation
A baseline rate was computed for the task. It
was produced using a system developed in the past
shared task edition by Erik Tjong Kim Sang, from
the University of Amsterdam, The Netherlands. The
baseline processor finds semantic roles based on the
following seven rules:
? Tag target verb and successive particles as V.
? Tag not and n?t in target verb chunk as
AM-NEG.
? Tag modal verbs in target verb chunk as
AM-MOD.
? Tag first NP before target verb as A0.
? Tag first NP after target verb as A1.
? Tag that, which and who before target verb
as R-A0.
? Switch A0 and A1, and R-A0 and R-A1 if the
target verb is part of a passive VP chunk. A
VP chunk is considered in passive voice if it
contains a form of to be and the verb does
not end in ing.
Table 6 presents the overall results obtained by
the nineteen systems plus the baseline, on the de-
velopment and test sets (i.e., Development, Test
WSJ, Test Brown, and Test WSJ+Brown). The sys-
tems are sorted by the performance on the combined
WSJ+Brown test set.
As it can be observed, all systems clearly outper-
formed the baseline. There are seven systems with a
final F1 performance in the 75-78 range, seven more
with performances in the 70-75 range, and five with
a performance between 65 and 70. The best perfor-
mance was obtained by Punyakanok et al (2005),
which almost reached an F1 at 80 in the WSJ test
set and almost 78 in the combined test. Their results
on the WSJ test equal the best results published so
far on this task and datasets (Pradhan et al, 2005a),
though they are not directly comparable due to a
different setting in defining arguments not perfectly
matching the predicted parse constituents. Since the
evaluation in the shared task setting is more strict,
we believe that the best results obtained in the shared
task represent a new breakthrough in the SRL task.
It is also quite clear that the systems using com-
bination are better than the individuals. It is worth
noting that the first 4 systems are combined. The
161
Development Test WSJ Test Brown Test WSJ+Brown
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1
punyakanok 80.05 74.83 77.35 82.28 76.78 79.44 73.38 62.93 67.75 81.18 74.92 77.92
haghighi 77.66 75.72 76.68 79.54 77.39 78.45 70.24 65.37 67.71 78.34 75.78 77.04
marquez 78.39 75.53 76.93 79.55 76.45 77.97 70.79 64.35 67.42 78.44 74.83 76.59
pradhan 80.90 75.38 78.04 81.97 73.27 77.37 73.73 61.51 67.07 80.93 71.69 76.03
surdeanu 79.14 71.57 75.17 80.32 72.95 76.46 72.41 59.67 65.42 79.35 71.17 75.04
tsai 81.13 72.42 76.53 82.77 70.90 76.38 73.21 59.49 65.64 81.55 69.37 74.97
che 79.65 71.34 75.27 80.48 72.79 76.44 71.13 59.99 65.09 79.30 71.08 74.97
moschitti 74.95 73.10 74.01 76.55 75.24 75.89 65.92 61.83 63.81 75.19 73.45 74.31
tjongkimsang 76.79 70.01 73.24 79.03 72.03 75.37 70.45 60.13 64.88 77.94 70.44 74.00
yi 75.70 69.99 72.73 77.51 72.97 75.17 67.88 59.03 63.14 76.31 71.10 73.61
ozgencil 73.57 71.87 72.71 74.66 74.21 74.44 65.52 62.93 64.20 73.48 72.70 73.09
johansson 73.40 70.85 72.10 75.46 73.18 74.30 65.17 60.59 62.79 74.13 71.50 72.79
cohn 73.51 68.98 71.17 75.81 70.58 73.10 67.63 60.08 63.63 74.76 69.17 71.86
park 72.68 69.16 70.87 74.69 70.78 72.68 64.58 60.31 62.38 73.35 69.37 71.31
mitsumori 71.68 64.93 68.14 74.15 68.25 71.08 63.24 54.20 58.37 72.77 66.37 69.43
venkatapathy 71.88 64.76 68.14 73.76 65.52 69.40 65.25 55.72 60.11 72.66 64.21 68.17
ponzetto 71.82 61.60 66.32 75.05 64.81 69.56 66.69 52.14 58.52 74.02 63.12 68.13
lin 70.11 61.96 65.78 71.49 64.67 67.91 65.75 52.82 58.58 70.80 63.09 66.72
sutton 64.43 63.11 63.76 68.57 64.99 66.73 62.91 54.85 58.60 67.86 63.63 65.68
baseline 50.00 28.98 36.70 51.13 29.16 37.14 62.66 33.07 43.30 52.58 29.69 37.95
Table 6: Overall precision, recall and F1 rates obtained by the 19 participating systems in the CoNLL-2005
shared task on the development and test sets. Systems sorted by F1 score on the WSJ+Brown test set.
best individual system on the task is that of Sur-
deanu and Turmo (2005), which obtained F1=75.04
on the combined test set, about 3 points below than
the best performing combined system. On the de-
velopment set, that system achieved a performace
of 75.17 (slightly below than the 75.27 reported by
Che et al (2005) on the same dataset). Accord-
ing to the description papers, we find that other
individual systems, from which the combined sys-
tems are constructed, performed also very well. For
instance, Tsai et al (2005) report F1=75.76 for a
base system on the development set, Ma`rquez et al
(2005) report F1=75.75, Punyakanok et al (2005)
report F1=74.76, and Haghighi et al (2005) report
F1=74.52.
The best results in the CoNLL-2005 shared task
are 10 points better than those of last year edition.
This increase in performance should be attributed to
a combination of the following factors: 1) training
sets have been substantially enlarged; 2) predicted
parse trees are available as input information; and 3)
more sophisticated combination schemes have been
implemented. In order to have a more clear idea of
the impact of enriching the syntactic information,
we refer to (Ma`rquez et al, 2005), who developed
an individual system based only on partial parsing
(?upc? input information). That system performed
F1=73.57 on the development set, which is 2.18
points below the F1=75.75 obtained by the same ar-
chitecture using full parsing, and 4.47 points below
the best performing combined system on the devel-
opment set (Pradhan et al, 2005b).
Comparing the results across development and
WSJ test corpora, we find that, with two exceptions,
all systems experienced a significant increase in per-
formance (normally between 1 and 2 F1 points).
This fact may be attributed to the different levels of
difficulty found across WSJ sections. The linguistic
processors and parsers perform slightly worse in the
development set. As a consequence, the matching
between parse nodes and actual arguments is lower.
Regarding the evaluation using the Brown test
set, all systems experienced a severe drop in perfor-
mance (about 10 F1 points), even though the base-
line on the Brown test set is higher than that of
the WSJ test set. As already said in previous sec-
tions, all the linguistic processors, from PoS tag-
ging to full parsing, showed a much lower perfor-
mance than in the WSJ test set, evincing that their
performance cannot be extrapolated across corpora.
Presumably, this fact is the main responsible of the
performace drop, though we do not discard an ad-
ditional overfitting effect due to the design of spe-
cific features that do not generalize well. More im-
162
portantly, this results impose (again) a severe criti-
cism on the current pipelined architecture for Natu-
ral Language Processing. Error propagation and am-
plification through the chained modules make the fi-
nal output generalize very badly when changing the
domain of application.
5 Conclusion
We have described the CoNLL-2005 shared task
on semantic role labeling. Contrasting with the
CoNLL-2004 edition, the current edition has in-
corporated the use of full syntax as input to the
SRL systems, much larger training sets, and cross-
corpora evaluation. The first two novelties have
most likely contributed to an improvement of re-
sults. The latter has evinced a major drawback of
natural language pipelined architectures.
Nineteen teams have participated to the task, con-
tributing with a variety of learning algorithms, la-
beling strategies, feature design and experimenta-
tion. While, broadly, all systems make use of the
same basic techniques described in existing SRL
literature, some novel aspects have also been ex-
plored. A remarkable aspect, common in the four
top-performing systems and many other, is that
of combining many individual SRL systems, each
working on different syntactic structures. Combin-
ing systems improves robustness, and overcomes
the limitations in coverage that working with a sin-
gle, non-correct syntactic structure imposes. The
best system, presented by Punyakanok et al (2005),
achieves an F1 at 79.44 on the WSJ test. This per-
formance, of the same order than the best reported
in literature, is still far from the desired behavior of
a natural language analyzer. Furthermore, the per-
formance of such SRL module in a real application
will be about ten points lower, as demonstrated in
the evaluation on the sentences from Brown.
We conclude with two open questions. First, what
semantic knowledge is needed to improve the qual-
ity and performance of SRL systems. Second, be-
yond pipelines, what type of architectures and lan-
guage learning methodology ensures a robust per-
formance of processors.
Acknowledgements
Authors would like to thank the following people and institu-
tions. The PropBank team, and specially Martha Palmer and
Benjamin Snyder, for making available PropBank-1.0 and the
prop-banked Brown files. The Linguistic Data Consortium, for
issuing a free evaluation license for the shared task to use the
TreeBank. Hai Leong Chieu and Hwee Tou Ng, for running
their Named Entity tagger on the task data. Finally, the teams
contributing to the shared task, for their great enthusiasm.
This work has been partially funded by the European Com-
munity (Chil - IP506909; PASCAL - IST-2002-506778) and
the Spanish Ministry of Science and Technology (Aliado,
TIC2002-04447-C02).
References
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003, Borovets, Bulgaria.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy inspired
parser. In Proceedings of NAACL-2000.
Wanxiang Che, Ting Liu, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings of
CoNLL-2005.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL-2003, Edmonton, Canada.
Trevor Cohn and Philip Blunsom. 2005. Semantic role
labelling with tree conditional random fields. In Pro-
ceedings of CoNLL-2005.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Informa tion and
Computation, Hong Kong, China.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and accu-
rate part-of-speech tagging: The svm approach revis-
ited. In Proceedings of RANLP-2003, Borovets, Bul-
garia.
163
Aria Haghighi, Kristina Toutanova, and Christopher
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of CoNLL-2005.
Richard Johansson and Pierre Nugues. 2005. Sparse
bayesian classification of predicate arguments. In Pro-
ceedings of CoNLL-2005.
Chi-San Lin and Tony C. Smith. 2005. Semantic role
labeling via consensus in pattern-matching. In Pro-
ceedings of CoNLL-2005.
Ken Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In Proceedings of the Senseval-3
ACL-SIGLEX Workshop.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus
Catala`. 2005. Semantic role labeling as sequential
tagging. In Proceedings of CoNLL-2005.
Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda,
Kouichi Doi, and Hirohumi Doi. 2005. Semantic role
labeling using support vector machines. In Proceed-
ings of CoNLL-2005.
Alessandro Moschitti, Ana-Maria Giuglea, Bonaventura
Coppola, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In Proceedings of CoNLL-2005.
Alessandro Moschitti. 2004. A study on convolution
kernel for shallow semantic parsing. In Proceedings
of the 42nd Annual Conference of the Association for
Computational Linguistics (ACL-2004).
Necati Ercan Ozgencil and Nancy McCracken. 2005.
Semantic role labeling using libSVM. In Proceedings
of CoNLL-2005.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1).
Kyung-Mi Park and Hae-Chang Rim. 2005. Maximum
entropy based semantic role labeling. In Proceedings
of CoNLL-2005.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling using lexical statistical informa-
tion. In Proceedings of CoNLL-2005.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Daniel Jurafsky.
2005a. Support vector learning for semantic argu-
ment classification. Machine Learning. Special issue
on Speech and Natural Language Processing. To ap-
pear.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2005b. Semantic role
chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005.
Vasin Punyakanok, Dan Roth, Wen-Tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In Proceedings of
CoNLL-2005.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Pro-
ceedings of CoNLL-2005.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Charles Sutton and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In Proceedings of
CoNLL-2005.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
Erik Tjong Kim Sang, Sander Canisius, Antal van den
Bosch, and Toine Bogers. 2005. Applying spelling er-
ror correction techniques for improving semantic role
labelling. In Proceedings of CoNLL-2005.
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-
Lian Hsu. 2005. Exploiting full parsing information
to label semantic roles using an ensemble of me and
svm via integer linear programming. In Proceedings
of CoNLL-2005.
Sriram Venkatapathy, Akshar Bharati, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In Proceedings of CoNLL-2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Szu-ting Yi and Martha Palmer. 2005. The integration of
syntactic parsing and semantic role labeling. In Pro-
ceedings of CoNLL-2005.
164
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 193?196, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling as Sequential Tagging
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez and Neus Catala`
TALP Research Centre
Technical University of Catalonia (UPC)
{lluism,pcomas,jgimenez,ncatala}@lsi.upc.edu
Abstract
In this paper we present a semantic role
labeling system submitted to the CoNLL-
2005 shared task. The system makes
use of partial and full syntactic informa-
tion and converts the task into a sequen-
tial BIO-tagging. As a result, the label-
ing architecture is very simple . Build-
ing on a state-of-the-art set of features, a
binary classifier for each label is trained
using AdaBoost with fixed depth decision
trees. The final system, which combines
the outputs of two base systems performed
F1=76.59 on the official test set. Addi-
tionally, we provide results comparing the
system when using partial vs. full parsing
input information.
1 Goals and System Architecture
The goal of our work is twofold. On the one hand,
we want to test whether it is possible to implement
a competitive SRL system by reducing the task to a
sequential tagging. On the other hand, we want to
investigate the effect of replacing partial parsing in-
formation by full parsing. For that, we built two dif-
ferent individual systems with a shared sequential
strategy but using UPC chunks-clauses, and Char-
niak?s parses, respectively. We will refer to those
systems as PPUPC and FPCHA, hereinafter.
Both partial and full parsing annotations provided
as input information are of hierarchical nature. Our
system navigates through these syntactic structures
in order to select a subset of constituents organized
sequentially (i.e., non embedding). Propositions are
treated independently, that is, each target verb gen-
erates a sequence of tokens to be annotated. We call
this pre-processing step sequentialization.
The sequential tokens are selected by exploring
the sentence spans or regions defined by the clause
boundaries1. The top-most syntactic constituents
falling inside these regions are selected as tokens.
Note that this strategy is independent of the input
syntactic annotation explored, provided it contains
clause boundaries. It happens that, in the case of
full parses, this node selection strategy is equivalent
to the pruning process defined by Xue and Palmer
(2004), which selects sibling nodes along the path of
ancestors from the verb predicate to the root of the
tree2. Due to this pruning stage, the upper-bound re-
call figures are 95.67% for PPUPC and 90.32% for
FPCHA. These values give F1 performance upper
bounds of 97.79 and 94.91, respectively, assuming
perfect predictors (100% precision).
The nodes selected are labeled with B-I-O tags
depending if they are at the beginning, inside, or out-
side of a verb argument. There is a total of 37 argu-
ment types, which amount to 37*2+1=75 labels.
Regarding the learning algorithm, we used gen-
eralized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
1Regions to the right of the target verb corresponding to an-
cestor clauses are omitted in the case of partial parsing.
2With the unique exception of the exploration inside sibling
PP constituents proposed by (Xue and Palmer, 2004).
193
lems to address multi-class classification.
AdaBoost binary classifiers are used for labeling
test sequences in a left-to-right tagging scheme us-
ing a recurrent sliding window approach with infor-
mation about the tag assigned to the preceding to-
ken. This tagging module ensures some basic con-
straints, e.g., BIO correct structure, arguments do
not cross clause boundaries nor base chunk bound-
aries, A0-A5 arguments not present in PropBank
frames for a certain verb are not allowed, etc. We
also tried beam search on top of the classifiers? pre-
dictions to find the sequence of labels with highest
sentence-level probability (as a summation of indi-
vidual predictions). But the results did not improve
the basic greedy tagging.
Regarding feature representation, we used all
input information sources, with the exception of
verb senses and Collins? parser. We did not con-
tribute with significantly original features. Instead,
we borrowed most of them from the existing liter-
ature (Gildea and Jurafsky, 2002; Carreras et al,
2004; Xue and Palmer, 2004). Broadly speaking, we
considered features belonging to four categories3:
(1) On the verb predicate:
? Form; Lemma; POS tag; Chunk type and Type of
verb phrase in which verb is included: single-word or
multi-word; Verb voice: active, passive, copulative, in-
finitive, or progressive; Binary flag indicating if the verb
is a start/end of a clause.
? Subcategorization, i.e., the phrase structure rule expand-
ing the verb parent node.
(2) On the focus constituent:
? Type; Head: extracted using common head-word rules;
if the first element is a PP chunk, then the head of the first
NP is extracted;
? First and last words and POS tags of the constituent.
? POS sequence: if it is less than 5 tags long; 2/3/4-grams
of the POS sequence.
? Bag-of-words of nouns, adjectives, and adverbs in the
constituent.
? TOP sequence: sequence of types of the top-most syn-
tactic elements in the constituent (if it is less than 5 ele-
ments long); in the case of full parsing this corresponds to
the right-hand side of the rule expanding the constituent
node; 2/3/4-grams of the TOP sequence.
? Governing category as described in (Gildea and Juraf-
sky, 2002).
3Features extracted from partial parsing and Named Enti-
ties are common to PPUPC and FPCHA models, while features
coming from Charniak parse trees are implemented exclusively
in the FPCHA model.
? NamedEnt, indicating if the constituent embeds or
strictly-matches a named entity along with its type.
? TMP, indicating if the constituent embeds or strictly
matches a temporal keyword (extracted from AM-TMP ar-
guments of the training set).
(3) Context of the focus constituent:
? Previous and following words and POS tags of the con-
stituent.
? The same features characterizing focus constituents are
extracted for the two previous and following tokens,
provided they are inside the clause boundaries of the cod-
ified region.
(4) Relation between predicate and constituent:
? Relative position; Distance in words and chunks; Level
of embedding with respect to the constituent: in number
of clauses.
? Constituent path as described in (Gildea and Jurafsky,
2002); All 3/4/5-grams of path constituents beginning at
the verb predicate or ending at the constituent.
? Partial parsing path as described in (Carreras et al,
2004); All 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
? Syntactic frame as described by Xue and Palmer (2004)
2 Experimental Setting and Results
We trained the classification models using the com-
plete training set (sections from 02 to 21). Once con-
verted into one sequence per target predicate, the re-
sulting set amounts 1,049,049 training examples in
the PPUPC model and 828,811 training examples in
the FPCHA model. The average number of labels per
argument is 2.071 and 1.068, respectively. This fact
makes ?I? labels very rare in the FPCHA model.
When running AdaBoost, we selected as weak
rules decision trees of fixed depth 4 (i.e., each branch
may represent a conjunction of at most 4 basic fea-
tures) and trained a classification model per label for
up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds. First, we discarded all the argument la-
bels that occur very infrequently and trained only
the 41 most frequent labels in the case of PPUPC
and the 35 most frequent in the case of FPCHA.
The remaining labels where joined in a new label
?other? in training and converted into ?O? when-
ever the SRL system assigns a ?other? label dur-
ing testing. Second, we performed a simple fre-
quency filtering by discarding those features occur-
ring less than 15 times in the training set. As an
194
exception, the frequency threshold for the features
referring to the verb predicate was set to 3. The final
number of features we worked with is 105,175 in the
case of PPUPC and 80,742 in the case of FPCHA.
Training with these very large data and feature
sets becomes an issue. Fortunately, we could split
the computation among six machines in a Linux
cluster. Using our current implementation combin-
ing Perl and C++ we could train the complete mod-
els in about 2 days using memory requirements be-
tween 1.5GB and 2GB. Testing with the ensembles
of 2,000 decision trees per label is also not very effi-
cient, though the resulting speed is admissible, e.g.,
the development set is tagged in about 30 minutes
using a standard PC.
The overall results obtained by our individual
PPUPC and FPCHA SRL systems are presented in ta-
ble 1, with the best results in boldface. As expected,
the FPCHA system significantly outperformed the
PPUPC system, though the results of the later can
be considered competitive. This fact is against the
belief, expressed as one of the conclusions of the
CoNLL-2004 shared task, that full-parsing systems
are about 10 F1 points over partial-parsing systems.
In this case, we obtain a performance difference of
2.18 points in favor of FPCHA.
Apart from resulting performance, there are addi-
tional advantages when using the FPCHA approach.
Due to the coarser granularity of sequence tokens,
FPCHA sequences are shorter. There are 21% less
training examples and a much lower quantity of ?I?
tags to predict (the mapping between syntactic con-
stituents and arguments is mostly one-to-one). As
a consequence, FPCHA classifiers train faster with
less memory requirements, and achieve competitive
results (near the optimal) with much less rounds of
boosting. See figure 1. Also related to the token
granularity, the number of completely correct out-
puts is 4.13 points higher in FPCHA, showing that
the resulting labelings are structurally better than
those of PPUPC.
Interestingly, the PPUPC and FPCHA systems
make quite different argument predictions. For in-
stance, FPCHA is better at recognizing A0 and A1
arguments since parse constituents corresponding to
these arguments tend to be mostly correct. Compar-
atively, PPUPC is better at recognizing A2-A4 argu-
ments since they are further from the verb predicate
 64
 66
 68
 70
 72
 74
 76
 78
 200  400  600  800  1000  1200  1400  1600  1800  2000
O
ve
ra
ll F
1
Number of rounds
PP-upc
FP-cha
PP best
FP-cha best
Figure 1: Overall F1 performance of individual sys-
tems on the development set with respect to the num-
ber of learning rounds
Perfect props Precision Recall F?=1
PPUPC 47.38% 76.86% 70.55% 73.57
FPCHA 51.51% 78.08% 73.54% 75.75
Combined 51.39% 78.39% 75.53% 76.93
Table 1: Overall results of the individual systems on
the development set.
and tend to accumulate more parsing errors, while
the fine granularity of the PPUPC sequences still al-
low to capture them4. Another interesting observa-
tion is that the precision of both systems is much
higher than the recall.
The previous two facts suggest that combining the
outputs of the two systems may lead to a significant
improvement. We experimented with a greedy com-
bination scheme for joining the maximum number of
arguments from both solutions in order to increase
coverage and, hopefully, recall. It proceeds depart-
ing from an empty solution by: First, adding all the
arguments from FPCHA in which this method per-
forms best; Second, adding all the arguments from
PPUPC in which this method performs best; and
Third, making another loop through the two meth-
ods adding the arguments not considered in the first
loop. At each step, we require that the added argu-
ments do not overlap/embed with arguments in the
current solution and also that they do not introduce
repetitions of A0-A5 arguments. The results on the
4As an example, the F1 performance of PPUPC on A0 and
A2 arguments is 79.79 and 65.10, respectively. The perfor-
mance of FPCHA on the same arguments is 84.03 and 62.36.
195
Precision Recall F?=1
Development 78.39% 75.53% 76.93
Test WSJ 79.55% 76.45% 77.97
Test Brown 70.79% 64.35% 67.42
Test WSJ+Brown 78.44% 74.83% 76.59
Test WSJ Precision Recall F?=1
Overall 79.55% 76.45% 77.97
A0 87.11% 86.28% 86.69
A1 79.60% 76.72% 78.13
A2 69.18% 67.75% 68.46
A3 76.38% 56.07% 64.67
A4 79.78% 69.61% 74.35
A5 0.00% 0.00% 0.00
AM-ADV 59.15% 52.37% 55.56
AM-CAU 73.68% 57.53% 64.62
AM-DIR 71.43% 35.29% 47.24
AM-DIS 77.14% 75.94% 76.54
AM-EXT 63.64% 43.75% 51.85
AM-LOC 62.74% 54.27% 58.20
AM-MNR 54.33% 52.91% 53.61
AM-MOD 96.16% 95.46% 95.81
AM-NEG 99.13% 98.70% 98.91
AM-PNC 53.49% 40.00% 45.77
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.68% 78.75% 78.21
R-A0 86.84% 88.39% 87.61
R-A1 75.32% 76.28% 75.80
R-A2 54.55% 37.50% 44.44
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 69.81% 71.15% 70.48
V 99.16% 99.16% 99.16
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
development set (presented in table 1) confirm our
expectations, since a performance increase of 1.18
points over the best individual system was observed,
mainly caused by recall improvement. The final sys-
tem we presented at the shared task performs exactly
this solution merging procedure. When applied on
the WSJ test set, the combination scheme seems to
generalize well, since an improvement is observed
with respect to the development set. See the offi-
cial results of our system, which are presented in ta-
ble 2. Also from that table, it is worth noting that the
F1 performance drops by more than 9 points when
tested on the Brown test set, indicating that the re-
sults obtained on the WSJ corpora do not generalize
well to corpora with other genres. The study of the
sources of this lower performance deserves further
investigation, though we do not believe that it is at-
tributable to the greedy combination scheme.
3 Conclusions
We have presented a simple SRL system submit-
ted to the CoNLL-2005 shared task, which treats
the SRL problem as a sequence tagging task (us-
ing a BIO tagging scheme). Given the simplic-
ity of the approach, we believe that the results are
very good and competitive compared to the state-
of-the-art. We also provided a comparison between
two SRL systems sharing the same architecture, but
build on partial vs. full parsing, respectively. Al-
though the full parsing approach obtains better re-
sults and has some implementation advantages, the
partial parsing system shows also a quite competi-
tive performance. The results on the development
set differ in 2.18 points, but the outputs generated
by the two systems are significantly different. The
final system, which scored F1=76.59 in the official
test set, is a combination of both individual systems
aiming at increasing coverage and recall.
Acknowledgements
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Jesu?s Gime?nez is a research fellow from the Span-
ish Ministry of Science and Technology (ALIADO
project, TIC2002-04447-C02). We would like to
thank also Xavier Carreras for providing us with
many software components and Mihai Surdeanu for
fruitful discussions on the problem and feature engi-
neering.
References
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL-2004.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
R. E. Schapire and Y. Singer. 1999. Improved Boosting Algo-
rithms Using Confidence-rated Predictions. Machine Learn-
ing, 37(3).
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
196
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 145?148,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Combining Linguistic Data Views for Phrase-based SMT
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
We describe the Spanish-to-English LDV-
COMBO system for the Shared Task 2:
?Exploiting Parallel Texts for Statistical
Machine Translation? of the ACL-2005
Workshop on ?Building and Using Par-
allel Texts: Data-Driven Machine Trans-
lation and Beyond?. Our approach ex-
plores the possibility of working with
alignments at different levels of abstrac-
tion, using different degrees of linguistic
annotation. Several phrase-based trans-
lation models are built out from these
alignments. Their combination significa-
tively outperforms any of them in isola-
tion. Moreover, we have built a word-
based translation model based on Word-
Net which is used for unknown words.
1 Introduction
The main motivation behind our work is to intro-
duce linguistic information, other than lexical units,
to the process of building word and phrase align-
ments. Many other authors have tried to do so. See
(Och and Ney, 2000), (Yamada and Knight, 2001),
(Koehn and Knight, 2002), (Koehn et al, 2003),
(Schafer and Yarowsky, 2003) and (Gildea, 2003).
Far from full syntactic complexity, we suggest to
go back to the simpler alignment methods first de-
scribed by (Brown et al, 1993). Our approach ex-
ploits the possibility of working with alignments at
two different levels of granularity, lexical (words)
and shallow parsing (chunks). In order to avoid con-
fusion so forth we will talk about tokens instead of
words as the minimal alignment unit.
Apart from redefining the scope of the alignment
unit, we may use different degrees of linguistic an-
notation. We introduce the general concept of data
view, which is defined as any possible representation
of the information contained in a bitext. We enrich
data view tokens with features further than lexical
such as PoS, lemma, and chunk label.
As an example of the applicability of data views,
suppose the case of the word ?plays? being seen in
the training data acting as a verb. Representing this
information as ?playsVBZ? would allow us to distin-
guish it from its homograph ?playsNNS? for ?plays? as
a noun. Ideally, one would wish to have still deeper
information, moving through syntax onto semantics,
such as word senses. Therefore, it would be possible
to distinguish for instance between two realizations
of ?plays? with different meanings: ?hePRP playsVBG
guitarNN? and ?hePRP playsVBG basketballNN?.
Of course, there is a natural trade-off between the
use of data views and data sparsity. Fortunately, we
hava data enough so that statistical parameter esti-
mation remains reliable.
2 System Description
The LDV-COMBO system follows the SMT architec-
ture suggested by the workshop organizers.
First, training data are linguistically annotated for
the two languages involved (See subsection 2.1).
10 different data views have been built. Notice
that it is not necessary that the two parallel coun-
terparts of a bitext share the same data view, as
145
long as they share the same granularity. How-
ever, in all our experiments we have annotated both
sides with the same linguistic information. See
token descriptions: (W) word, (WL) word and
lemma, (WP) word and PoS, (WC) word and chunk
label, (WPC) word, PoS and chunk label, (Cw)
chunk of words (Cwl), chunk of words and lem-
mas, (Cwp) chunk of words and PoS (Cwc) chunk
of words and chunk labels (Cwpc) chunk of words,
PoS and chunk labels. By chunk label we re-
fer to the IOB label associated to every word in-
side a chunk, e.g. ?IB?NP declareB?V P resumedI?V P
theB?NP sessionI?NP ofB?PP theB?NP EuropeanI?NP
ParliamentI?NP .O?). We build chunk tokens by ex-
plicitly connecting words in the same chunk, e.g.
?(I)NP (declare resumed)V P (the session)NP (of)PP
(the European Parliament)NP ?. See examples of
some of these data views in Table 1.
Then, running GIZA++, we obtain token align-
ments for each of the data views. Combined phrase-
based translation models are built on top of the
Viterbi alignments output by GIZA++. See details
in subsection 2.2. Combo-models must be then post-
processed in order to remove the additional linguis-
tic annotation and split chunks back into words, so
they fit the format required by Pharaoh.
Moreover, we have used the Multilingual Central
Repository (MCR), a multilingual lexical-semantic
database (Atserias et al, 2004), to build a word-
based translation model. We back-off to this model
in the case of unknown words, with the goal of im-
proving system recall. See subsection 2.3.
2.1 Data Representation
In order to achieve robustness the same tools have
been used to linguistically annotate both languages.
The SVMTool1 has been used for PoS-tagging
(Gime?nez and Ma`rquez, 2004). The Freeling2 pack-
age (Carreras et al, 2004) has been used for lemma-
tizing. Finally, the Phreco software by (Carreras et
al., 2005) has been used for shallow parsing.
No additional tokenization or pre-processing
steps other than case lowering have been performed.
Special treatment of named entities, dates, numbers,
1The SVMTool may be freely downloaded at
http://www.lsi.upc.es/?nlp/SVMTool/ .
2Freeling Suite of Language Analyzers may be downloaded
at http://www.lsi.upc.es/?nlp/freeling/
currency, etc., should be considered so as to further
enhance the system.
2.2 Building Combined Translation Models
Because data views capture different, possibly com-
plementary, aspects of the translation process it
seems reasonable to combine them. We consider
two different ways of building such combo-models:
LPHEX Local phrase extraction. To build a separate
phrase-based translation model for each data
view alignment, and then combine them. There
are two ways of combining translation models:
MRG Merging translation models. We work on
a weighted linear interpolation of models.
These weights may be tuned, although a
uniform weight selection yields good re-
sults. Additionally, phrase-pairs may be
filtered out by setting a score threshold.
noMRG Passing translation models directly to
the Pharaoh decoder. However, we en-
countered many problems with phrase-
pairs that were not seen in all single mod-
els. This obliged us to apply arbitrary
smoothing values to score these pairs.
GPHEX Global phrase extraction. To build a sin-
gle phrased-based translation model from the
union of alignments from several data views.
In its turn, any MRG operation performed on a
combo-model results again in a valid combo-model.
In any case, phrase extraction3 is performed as de-
picted by (Och, 2002).
2.3 Using the MCR
Outer knowledge may be supplied to the Pharaoh
decoder by annotating the input with alternative
translation options via XML-markup. We enrich
every unknown word by looking up every possi-
ble translation for all of its senses in the MCR.
These are scored by relative frequency according to
the number of senses that lexicalized in the same
manner. Let wf , pf be the source word and PoS,
and we be the target word, we define a function
3We always work with the union of alignments, no heuristic
refinement, and phrases up to 5 tokens. Phrase pairs appearing
only once have been discarded. Scoring is performed by relative
frequency. No smoothing is applied.
146
It[PRP :B?NP ] would[MD:B?V P ] appear[VB:I?V P ] that[IN:B?SBAR] a[DT :B?NP ] speech[NN:I?NP ] made[VBN:B?V P ]
at[IN:B?PP ] the[DT :B?NP ] weekend[NN:I?NP ] by[IN:B?PP ] Mr[NNP :B?NP ] Fischler[NNP :I?NP ]
indicates[VBZ:B?V P ] a[DT :B?NP ] change[NN:I?NP ] of[IN:B?PP ] his[PRP$:B?NP ] position[NN:I?NP ] .[.:O]
WPC
Fischler[VMN:B?V P ] pronuncio?[VMI:B?V P ] un[DI:B?NP ] discurso[NC:I?NP ] este[DD:B?NP ] fin[NC:I?NP ]
de[SP :B?PP ] semana[NC:B?NP ] en[SP :B?PP ] el[DA:B?SBAR] que[PR0:I?SBAR] parec??a[VMI:B?V P ]
haber[VAN:I?V P ] cambiado[VMP :I?V P ] de[SP :B?PP ] actitud[NC:B?NP ] .[Fp:O]
(It[PRP :B?NP ]]) (would[MD:B?V P ]] appear[VB:I?V P ]) (that[IN:B?SBAR]) (a[DT :B?NP ] speech[NN:I?NP ])
(made[VBN:B?V P ]) (at[IN:B?PP ]) (the[DT :B?NP ] weekend[NN:I?NP ]) (by[IN:B?PP ])
(Mr[NNP :B?NP ] Fischler[NNP :I?NP ]) (indicates[VBZ:B?V P ]) (a[DT :B?NP ] change[NN:I?NP ])
(of[IN:B?PP ]) (his[PRP$:B?NP ] position[NN:I?NP ]) (.[.:O])
Cwpc
(Fischler[VMN:B?V P ]) (pronuncio?[VMI:B?V P ]) (un[DI:B?NP ] discurso[NC:I?NP ]) (este[DD:B?NP ] fin[NC:I?NP ])
(de[SP :B?PP ]) (semana[NC:B?NP ]) (en[SP :B?PP ]) (el[DA:B?SBAR] que[PR0:I?SBAR])
(parec??a[VMI:B?V P ] haber[VAN:I?V P ] cambiado[VMP :I?V P ]) (de[SP :B?PP ]) (actitud[NC:B?NP ]) (.[Fp:O])
Table 1: An example of 2 rich data views: (WPC) word, PoS and IOB chunk label (Cwpc) chunk of word, PoS and chunk label.
Scount(wf , pf , we) which counts the number of
senses for (wf , pf ) which can lexicalize as we. A
translation pair is scored as:
score(wf , pf |we) =
Scount(wf , pf , we)
?
(wf ,pf ) Scount(wf , pf , we)
(1)
Better results would be expected working with
word sense disambiguated text. We are not at this
point yet. A first approach could be to work with the
most frequent sense heuristic.
3 Experimental Results
3.1 Data and Evaluation Metrics
We have used the data sets and language model pro-
vided by the organization. No extra training or de-
velopment data were used in our experiments.
We evaluate results with 3 different metrics: GTM
F1-measure (e = 1, 2), BLEU score (n = 4) as pro-
vided by organizers, and NIST score (n = 5).
3.2 Experimenting with Data Views
Table 2 presents MT results for the 10 elementary
data views devised in Section 2. Default parameters
are used for ?tm, ?lm, and ?w. No tuning has been
performed. As expected, word-based views obtain
significatively higher results than chunk-based. All
data views at the same level of granularity obtain
comparable results.
In Table 3 MT results for different data view com-
binations are showed. Merged model weights are
set equiprobable, and no phrase-pair score filtering
data view GTM-1 GTM-2 BLEU NIST
W 0.6108 0.2609 25.92 7.1576
WL 0.6110 0.2601 25.77 7.1496
WP 0.6096 0.2600 25.74 7.1415
WC 0.6124 0.2600 25.98 7.1852
WPC 0.6107 0.2587 25.79 7.1595
Cw 0.5749 0.2384 22.73 6.6149
Cwl 0.5756 0.2385 22.73 6.6204
Cwp 0.5771 0.2395 23.06 6.6403
Cwc 0.5759 0.2390 22.86 6.6207
Cwpc 0.5744 0.2379 22.77 6.5949
Table 2: MT Results for the 10 elementary data views on the
development set.
is performed. We refer to the W model as our base-
line. In this view, only words are used. The 5W-MRG
and 5W-GPHEX models use a combination of the 5
word-based data views, as in MRG and GPHEX, re-
spectively. The 5C-MRG and 5C-GPHEX system use
a combination of the 5 chunk based data views, as
in MRG and GPHEX, respectively. The 10-MRG sys-
tem uses all 10 data views combined as in MRG. The
10-GPHEX/MRG system uses the 5 word based views
combined as in GPHEX, the 5 chunk based views
combined as in GPHEX, and then a combination of
these two combo-models as in MRG.
data view GTM-1 GTM-2 BLEU NIST
W 0.6108 0.2609 25.92 7.1576
5W-MRG 0.6134 0.2631 26.25 7.2122
5W-GPHEX 0.6172 0.2615 26.95 7.2823
5C-MRG 0.5786 0.2407 23.18 6.6754
5C-GPHEX 0.5739 0.2368 22.80 6.5714
10-MRG 0.6130 0.2624 26.24 7.2196
10-GPHEX/MRG 0.6142 0.2600 26.58 7.2542
Table 3: MT Results without tuning, for some data view com-
binations on the development set.
147
It can be seen that results improve by combining
several data views. Furthermore, global phrase ex-
traction (GPHEX) seems to work much finer than lo-
cal phrase extraction (LPHEX).
Table 4 shows MT results after optimizing ?tm,
?lm, ?w, and the weights for the MRG operation,
by means of the Downhill Simplex Method in Multi-
dimensions (William H. Press and Flannery, 2002).
Observe that tuning the system improves the perfor-
mance considerably. The ?w parameter is particu-
larly sensitive to tuning.
Even though the performance of chunk-based
models is poor, the best results are obtained by com-
binining the two levels of abstraction, thus proving
that syntactically motivated phrases may help. 10-
MRG and 10-GPHEX models achieve a similar per-
formance. The 10-MRG-bestWN system corresponds
to the 10-MRG model using WordNet. The 10-MRG-
subWN system is this same system at the time of sub-
mission. Results using WordNet, taking into account
that the number of unknown4 words in the develop-
ment set was very small, are very promising.
data view GTM-1 GTM-2 BLEU NIST
W 0.6174 0.2583 28.13 7.1540
5W-MRG 0.6206 0.2605 28.50 7.2076
5W-GPHEX 0.6207 0.2603 28.38 7.1992
5C-MRG 0.5882 0.2426 25.06 6.6773
5C-GPHEX 0.5816 0.2387 24.40 6.5595
10-MRG 0.6218 0.2623 28.88 7.2491
10-GPHEX/MRG 0.6229 0.2622 28.82 7.2414
10-MRGWN 0.6228 0.2625 28.90 7.2583
10-MRG-subWN 0.6228 0.2622 28.79 7.2528
Table 4: MT Results for some data view combinations after
tuning on the development set.
4 Conclusions
We have showed that it is possible to obtain better
phrase-based translation models by utilizing align-
ments built on top of different linguistic data views.
These models can be robustly combined, signifi-
cantly outperforming all of their components in iso-
lation. We leave for further work the experimen-
tation of new data views such as word senses and
semantic roles, as well as their natural porting and
evolution from the alignment step to phrase extrac-
tion and decoding.
4Translation for 349 unknown words was found in the MCR.
Acknowledgements
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02). Authors are thankful to Pa-
trik Lambert for providing us with the implementa-
tion of the Simplex Method used for tuning.
References
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The meaning multilingual central
repository. In Proceedings of GWC, Brno, Czech Re-
public, January. ISBN 80-210-3302-9.
Peter E Brown, Stephen A. Della Pietra, Robert L. Mer-
cer, and Vincent J. Della Pietra. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. Freeling: An open-source suite of lan-
guage analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of ACL.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. Svmtool: A
general pos tagger generator based on support vector
machines. In Proceedings of 4th LREC.
Philipp Koehn and Kevin Knight. 2002. Chunkmt:
Statistical machine translation with richer linguistic
knowledge. Draft.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Charles Schafer and David Yarowsky. 2003. Statistical
machine translation using coercive two-level syntactic
transduction. In Proceedings of EMNLP.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing. Cambridge
University Press.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
148
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 181?185, New York City, June 2006. c?2006 Association for Computational Linguistics
Projective Dependency Parsing with Perceptron
Xavier Carreras, Mihai Surdeanu and Llu??s Ma`rquez
TALP Research Centre ? Software Department (LSI)
Technical University of Catalonia (UPC)
Campus Nord - Edifici Omega, Jordi Girona Salgado 1?3, E-08034 Barcelona
{carreras,surdeanu,lluism}@lsi.upc.edu
Abstract
We describe an online learning depen-
dency parser for the CoNLL-X Shared
Task, based on the bottom-up projective
algorithm of Eisner (2000). We experi-
ment with a large feature set that mod-
els: the tokens involved in dependencies
and their immediate context, the surface-
text distance between tokens, and the syn-
tactic context dominated by each depen-
dency. In experiments, the treatment of
multilingual information was totally blind.
1 Introduction
We describe a learning system for the CoNLL-X
Shared Task on multilingual dependency parsing
(Buchholz et al, 2006), for 13 different languages.
Our system is a bottom-up projective dependency
parser, based on the cubic-time algorithm by Eisner
(1996; 2000). The parser uses a learning function
that scores all possible labeled dependencies. This
function is trained globally with online Perceptron,
by parsing training sentences and correcting its pa-
rameters based on the parsing mistakes. The features
used to score, while based on the previous work in
dependency parsing (McDonald et al, 2005), intro-
duce some novel concepts such as better codification
of context and surface distances, and runtime infor-
mation from dependencies previously parsed.
Regarding experimentation, the treatment of mul-
tilingual data has been totally blind, with no spe-
cial processing or features that depend on the lan-
guage. Considering its simplicity, our system
achieves moderate but encouraging results, with an
overall labeled attachment accuracy of 74.72% on
the CoNLL-X test set.
2 Parsing and Learning Algorithms
This section describes the three main components of
the dependency parsing: the parsing model, the pars-
ing algorithm, and the learning algorithm.
2.1 Model
Let 1, . . . , L be the dependency labels, defined be-
forehand. Let x be a sentence of n words, x1 . . . xn.
Finally, let Y(x) be the space of well-formed depen-
dency trees for x. A dependency tree y ? Y(x) is a
set of n dependencies of the form [h,m, l], where
h is the index of the head word (0 ? h ? n,
where 0 means root), m is the index of the modi-
fier word (1 ? m ? n), and l is the dependency
label (1 ? l ? L). Each word of x participates as a
modifier in exactly one dependency of y.
Our dependency parser, dp, returns the maximum
scored dependency tree for a sentence x:
dp(x,w) = argmax
y?Y(x)
?
[h,m,l]?y
sco([h,m, l], x, y,w)
In the formula, w is the weight vector of the
parser, that is, the set of parameters used to score de-
pendencies during the parsing process. It is formed
by a concatenation of L weight vectors, one for each
dependency label, w = (w1, . . . ,wl, . . . ,wL). We
assume a feature extraction function, ?, that repre-
sents an unlabeled dependency [h,m] in a vector of
D features. Each of the wl has D parameters or
dimensions, one for each feature. Thus, the global
181
weight vector w maintains L ? D parameters. The
scoring function is defined as follows:
sco([h,m, l], x, y,w) = ?(h,m, x, y) ? wl
Note that the scoring of a dependency makes use
of y, the tree that contains the dependency. As de-
scribed next, at scoring time y just contains the de-
pendencies found between h and m.
2.2 Parsing Algorithm
We use the cubic-time algorithm for dependency
parsing proposed by Eisner (1996; 2000). This pars-
ing algorithm assumes that trees are projective, that
is, dependencies never cross in a tree. While this as-
sumption clearly does not hold in the CoNLL-X data
(only Chinese trees are actually 100% projective),
we chose this algorithm for simplicity. As it will be
shown, the percentage of non-projective dependen-
cies is not very high, and clearly the error rates we
obtain are caused by other major factors.
The parser is a bottom-up dynamic programming
algorithm that visits sentence spans of increasing
length. In a given span, from word s to word e, it
completes two partial dependency trees that cover
all words within the span: one rooted at s and the
other rooted at e. This is done in two steps. First, the
optimal dependency structure internal to the span is
chosen, by combining partial solutions from inter-
nal spans. This structure is completed with a depen-
dency covering the whole span, in two ways: from
s to e, and from e to s. In each case, the scoring
function is used to select the dependency label that
maximizes the score.
We take advantage of this two-step processing to
introduce features for the scoring function that rep-
resent some of the internal dependencies of the span
(see Section 3 for details). It has to be noted that
the parsing algorithm we use does not score depen-
dencies on top of every possible internal structure.
Thus, by conditioning on features extracted from y
we are making the search approximative.
2.3 Perceptron Learning
As learning algorithm, we use Perceptron tailored
for structured scenarios, proposed by Collins (2002).
In recent years, Perceptron has been used in a num-
ber of Natural Language Learning works, such as in
w = 0
for t = 1 to T
foreach training example (x, y) do
y? = dp(x,w)
foreach [h,m, l] ? y\y? do
wl = wl + ?(h,m, x, y?)
foreach [h,m, l] ? y?\y do
wl = wl ? ?(h,m, x, y?)
return w
Figure 1: Pseudocode of the Perceptron Algorithm. T is a
parameter that indicates the number of epochs that the algorithm
cycles the training set.
partial parsing (Carreras et al, 2005) or even depen-
dency parsing (McDonald et al, 2005).
Perceptron is an online learning algorithm that
learns by correcting mistakes made by the parser
when visiting training sentences. The algorithm is
extremely simple, and its cost in time and memory
is independent from the size of the training corpora.
In terms of efficiency, though, the parsing algorithm
must be run at every training sentence.
Our system uses the regular Perceptron working
in primal form. Figure 1 sketches the code. Given
the number of languages and dependency types in
the CoNLL-X exercise, we found prohibitive to
work with a dual version of Perceptron, that would
allow the use of a kernel function to expand features.
3 Features
The feature extraction function, ?(h,m, x, y), rep-
resents in a feature vector a dependency from word
positions m to h, in the context of a sentence x and a
dependency tree y. As usual in discriminative learn-
ing, we work with binary indicator features: if a cer-
tain feature is observed in an instance, the value of
that feature is 1; otherwise, the value is 0. For con-
venience, we describe ? as a composition of several
base feature extraction functions. Each extracts a
number of disjoint features. The feature extraction
function ?(h,m, x, y) is calculated as:
?token(x, h, ?head?) + ?tctx(x, h, ?head?) +
?token(x,m, ?mod?) + ?tctx(x,m, ?mod?) +
?dep(x,mmdh,m) + ?dctx(x,mmdh,m) +
?dist(x,mmdh,m) + ?runtime(x, y, h,m, dh,m)
where ?token extracts context-independent token
features, ?tctx computes context-based token fea-
tures, ?dep computes context-independent depen-
182
?token(x, i, type)
type ? w(xi)
type ? l(xi)
type ? cp(xi)
type ? fp(xi)
foreach(ms): type ?ms(xi)
type ? w(xi) ? cp(xi)
foreach(ms): type ? w(xi) ?ms(xi)
?tctx(x, i, type)
?token(x, i? 1, type ? string(i? 1))
?token(x, i? 2, type ? string(i? 2))
?token(x, i+ 1, type ? string(i+ 1))
?token(x, i+ 2, type ? string(i+ 2))
type ? cp(xi) ? cp(xi?1)
type ? cp(xi) ? cp(xi?1) ? cp(xi?2)
type ? cp(xi) ? cp(xi+1)
type ? cp(xi) ? cp(xi+1) ? cp(xi+2)
Table 1: Token features, both context-independent (?token)
and context-based (?tctx). type - token type, i.e. ?head? or
?mod?, w - token word, l - token lemma, cp - token coarse part-
of-speech (POS) tag, fp - token fine-grained POS tag, ms -
token morpho-syntactic feature. The ? operator stands for string
concatenation.
?dep(x, i, j,dir)
dir ? w(xi) ? cp(xi) ? w(xj) ? cp(xj)
dir ? cp(xi) ? w(xj) ? cp(xj)
dir ? w(xi) ? w(xj) ? cp(xj)
dir ? w(xi) ? cp(xi) ? cp(xj)
dir ? w(xi) ? cp(xi) ? w(xj)
dir ? w(xi) ? w(xj)
dir ? cp(xi) ? cp(xj)
?dctx(x, i, j,dir)
dir ? cp(xi) ? cp(xi+1) ? cp(xj?1) ? cp(xj)
dir ? cp(xi?1) ? cp(xi) ? cp(xj?1) ? cp(xj)
dir ? cp(xi) ? cp(xi+1) ? cp(xj) ? cp(xj+1)
dir ? cp(xi?1) ? cp(xi) ? cp(xj) ? cp(xj+1)
Table 2: Dependency features, both context-independent
(?dep) and context-based (?dctx), between two points i and j,
i < j. dir - dependency direction: left to right or right to left.
dency features, ?dctx extracts contextual depen-
dency features, ?dist calculates surface-distance fea-
tures between the two tokens, and finally, ?runtime
computes dynamic features at runtime based on the
dependencies previously built for the given interval
during the bottom-up parsing. mmdh,m is a short-
hand for a triple of numbers: min(h,m), max(h,m)
and dh,m (a sign indicating the direction, i.e., +1 if
m < h, and ?1 otherwise).
We detail the token features in Table 1, the depen-
dency features in Table 2, and the surface-distance
features in Table 3. Most of these features are in-
spired by previous work in dependency parsing (Mc-
Donald et al, 2005; Collins, 1999). What is impor-
?dist(x, i, j,dir)
foreach(k ? (i, j)): dir ? cp(xi) ? cp(xk) ? cp(xj)
number of tokens between i and j
number of verbs between i and j
number of coordinations between i and j
number of punctuations signs between i and j
Table 3: Surface distance features between points i and j. Nu-
meric features are discretized using ?binning? to a small number
of intervals.
?runtime(x,y,h,m,dir)
let l1, . . . , lS be the labels of dependencies
in y that attach to h and are found from m to h.
foreach i, 1? i?S : dir ? cp(xh) ? cp(xm) ? li
if S?1 , dir ? cp(xh) ? cp(xm) ? l1
if S?2 , dir ? cp(xh) ? cp(xm) ? l1 ? l2
if S?3 , dir ? cp(xh) ? cp(xm) ? l1 ? l2 ? l3
if S?4 , dir ? cp(xh) ? cp(xm) ? l1 ? l2 ? l3 ? l4
if S=0 , dir ? cp(xh) ? cp(xm) ? null
if 0<S?4 , dir ? cp(xh) ? cp(xm) ? regular
if S>4 , dir ? cp(xh) ? cp(xm) ? big
Table 4: Runtime features of y between m and h.
tant for the work presented here is that we construct
explicit feature combinations (see above tables) be-
cause we configured our linear predictors in primal
form, in order to keep training times reasonable.
While the features presented in Tables 1, 2, and 3
are straightforward exploitations of the training data,
the runtime features (?runtime) take a different, and
to our knowledge novel in the proposed framework,
approach: for a dependency from m to h, they rep-
resent the dependencies found between m and h
that attach also to h. They are described in detail
in Table 4. As we have noted above, these fea-
tures are possible because of the parsing scheme,
which scores a dependency only after all dependen-
cies spanned by it are scored.
4 Experiments and Results
We experimented on the 13 languages proposed
in the CoNLL-X Shared Task (Hajic? et al, 2004;
Simov et al, 2005; Simov and Osenova, 2003; Chen
et al, 2003; Bo?hmova? et al, 2003; Kromann, 2003;
van der Beek et al, 2002; Brants et al, 2002;
Kawata and Bartels, 2000; Afonso et al, 2002;
Dz?eroski et al, 2006; Civit and Mart??, 2002; Nilsson
et al, 2005; Oflazer et al, 2003; Atalay et al, 2003).
Our approach to deal with many different languages
was totally blind: we did not inspect the data to mo-
tivate language-specific features or processes.
183
We did feature filtering based on frequency
counts. Our feature extraction patterns, that ex-
ploit both lexicalization and combination, gener-
ate millions of feature dimensions, even with small
datasets. Our criterion was to use at most 500,000
different dimensions in each label weight vector. For
each language, we generated all possible features,
and then filtered out most of them according to the
counts. Depending on the number of training sen-
tences, our counts cut-offs vary from 3 to 15.
For each language, we held out from training data
a portion of sentences (300, 500 or 1000 depend-
ing on the total number of sentences) and trained a
model for up to 20 epochs in the rest of the data. We
evaluated each model on the held out data for differ-
ent number of training epochs, and selected the op-
timum point. Then, we retrained each model on the
whole training set for the selected number of epochs.
Table 5 shows the attachment scores obtained
by our system, both unlabeled (UAS) and labeled
(LAS). The first column (GOLD) presents the LAS
obtained with a perfect scoring function: the loss in
accuracy is related to the projectivity assumption of
our parsing algorithm. Dutch turns out to be the
most non-projective language, with a loss in accu-
racy of 5.44%. In our opinion, the loss in other lan-
guages is relatively small, and is not a major limita-
tion to achieve a high performance in the task. Our
system achieves an overall LAS of 74.72%, with
substantial variation from one language to another.
Turkish, Arabic, Dutch, Slovene and Czech turn out
to be the most difficult languages for our system,
with accuracies below 70%. The easiest language
is clearly Japanese, with a LAS of 88.13%, followed
by Chinese, Portuguese, Bulgarian and German, all
with LAS above 80%.
Table 6 shows the contribution of base feature ex-
traction functions. For four languages, we trained
models that increasingly incorporate base functions.
It can be shown that all functions contribute to a bet-
ter score. Contextual features (?3) bring the system
to the final order of performance, while distance (?4)
and runtime (?) features still yield substantial im-
provements.
5 Analysis and Conclusions
It is difficult to explain the difference in performance
across languages. Nevertheless, we have identified
GOLD UAS LAS
Bulgarian 99.56 88.81 83.30
Arabic 99.76 72.65 60.94
Chinese 100.0 88.65 83.68
Czech 97.78 77.44 68.82
Danish 99.18 85.67 79.74
Dutch 94.56 71.39 67.25
German 98.84 85.90 82.41
Japanese 99.16 90.79 88.13
Portuguese 98.54 87.76 83.37
Slovene 98.38 77.72 68.43
Spanish 99.96 80.77 77.16
Swedish 99.64 85.54 78.65
Turkish 98.41 70.05 58.06
Overall 98.68 81.19 74.72
Table 5: Results of the system on test data. GOLD: labeled
attachment score using gold scoring functions; the loss in ac-
curacy is caused by the projectivity assumption made by the
parser. UAS : unlabeled attachment score. LAS : labeled at-
tachment score, the measure to compare systems in CoNLL-X.
Bulgarian is excluded from overall scores.
?1 ?2 ?3 ?4 ?
Turkish 33.02 48.00 55.33 57.16 58.06
Spanish 12.80 53.80 68.18 74.27 77.16
Portuguese 47.10 64.74 80.89 82.89 83.37
Japanese 38.78 78.13 86.87 88.27 88.13
Table 6: Labeled attachment scores at increasing feature con-
figurations. ?1 uses only ?token at the head and modifier. ?2
extends ?1 with ?dep. ?3 incorporates context features, namely
?tctx at the head and modifier, and ?dctx. ?4 extends ?3 with
?dist. Finally, the final feature extraction function ? increases
?4 with ?runtime.
four generic factors that we believe caused the most
errors across all languages:
Size of training sets: the relation between the
amount of training data and performance is strongly
supported in learning theory. We saw the same re-
lation in this evaluation: for Turkish, Arabic, and
Slovene, languages with limited number of train-
ing sentences, our system obtains accuracies below
70%. However, one can not argue that the training
size is the only cause of errors: Czech has the largest
training set, and our accuracy is also below 70%.
Modeling large distance dependencies: even
though we include features to model the distance
between two dependency words (?dist), our analy-
sis indicates that these features fail to capture all the
intricacies that exist in large-distance dependencies.
Table 7 shows that, for the two languages analyzed,
the system performance decreases sharply as the dis-
tance between dependency tokens increases.
184
to root 1 2 3 ? 6 >= 7
Spanish 83.04 93.44 86.46 69.97 61.48
Portuguese 90.81 96.49 90.79 74.76 69.01
Table 7: F?=1 score related to dependency token distance.
Modeling context: many attachment decisions, e.g.
prepositional attachment, depend on additional con-
text outside of the two dependency tokens. To ad-
dress this issue, we have included in our model fea-
tures to capture context, both static (?dctx and ?tctx)
and dynamic (?runtime). Nevertheless, our error
analysis indicates that our model is not rich enough
to capture the context required to address complex
dependencies. All the top 5 focus words with the
majority of errors for Spanish and Portuguese ? ?y?,
?de?, ?a?, ?en?, and ?que? for Spanish, and ?em?,
?de?, ?a?, ?e?, and ?para? for Portuguese ? indicate
complex dependencies such as prepositional attach-
ments or coordinations.
Projectivity assumption: Dutch is the language
with most crossing dependencies in this evaluation,
and the accuracy we obtain is below 70%.
On the Degree of Lexicalization We conclude the
error analysis of our model with a look at the de-
gree of lexicalization in our model. A quick analy-
sis of our model on the test data indicates that only
34.80% of the dependencies for Spanish and 42.94%
of the dependencies for Portuguese are fully lexical-
ized, i.e. both the head and modifier words appear
in the model feature set (see Table 8). There are
two reasons that cause our model to be largely un-
lexicalized: (a) in order to keep training times rea-
sonable we performed heavy filtering of all features
based on their frequency, which eliminates many
lexicalized features from the final model, and (b)
due to the small size of most of the training cor-
pora, most lexicalized features simply do not ap-
pear in the testing section. Considering these re-
sults, a reasonable question to ask is: how much
are we losing because of this lack of lexical infor-
mation? We give an approximate answer by ana-
lyzing the percentage of fully-lexicalized dependen-
cies that are correctly parsed by our model. As-
suming that our model scales well, the accuracy on
fully-lexicalized dependencies is an indication for
the gain (or loss) to be had from lexicalization. Our
model parses fully-lexicalized dependencies with an
Fully One token Fully
lexicalized unlexicalized unlexicalized
Spanish 34.80% 54.77% 10.43%
Portuguese 42.94% 49.26% 7.80%
Table 8: Degree of dependency lexicalization.
accuracy of 74.81% LAS for Spanish (2.35% lower
than the overall score) and of 83.77% LAS for Por-
tuguese (0.40% higher than the overall score). This
analysis indicates that our model has limited gains
(if any) from lexicalization.
In order to improve the quality of our dependency
parser we will focus on previously reported issues
that can be addressed by a parsing model: large-
distance dependencies, better modeling of context,
and non-projective parsing algorithms.
Acknowledgements
This work was partially funded by the European Union Com-
mission (PASCAL - IST-2002-506778) and Spanish Ministry
of Science and Technology (TRANGRAM - TIN2004-07925-
C03-02). Mihai Surdeanu was supported by a Ramon y Cajal
fellowship of the latter institution.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc. of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL-X). SIGNLL.
X. Carreras, Llu??s Ma`rquez, and J. Castro. 2005. Filtering-
ranking perceptron learning for partial parsing. Machine
Learning, 1?3(60):41?71.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. of EMNLP-2002.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. of the 16th Intern.
Conf. on Computational Linguistics (COLING).
J. Eisner. 2000. Bilexical grammars and their cubic-time pars-
ing algorithms. In H. C. Bunt and A. Nijholt, editors, New
Developments in Natural Language Parsing, pages 29?62.
Kluwer Academic Publishers.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. of the 43rd
Annual Meeting of the ACL.
185
Proceedings of the Workshop on Statistical Machine Translation, pages 166?169,
New York City, June 2006. c?2006 Association for Computational Linguistics
The LDV-COMBO system for SMT
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
We describe the LDV-COMBO system pre-
sented at the Shared Task. Our approach
explores the possibility of working with
alignments at different levels of abstrac-
tion using different degrees of linguis-
tic analysis from the lexical to the shal-
low syntactic level. Translation mod-
els are built on top of combinations of
these alignments. We present results
for the Spanish-to-English and English-to-
Spanish tasks. We show that liniguistic in-
formation may be helpful, specially when
the target language has a rich morphology.
1 Introduction
The main motivation behind our work is to introduce
linguistic information, other than lexical units, to the
process of building word and phrase alignments. In
the last years, many efforts have been devoted to this
matter (Yamada and Knight, 2001; Gildea, 2003).
Following our previous work (Gime?nez and
Ma`rquez, 2005), we use shallow syntactic informa-
tion to generate more precise alignments. Far from
full syntactic complexity, we suggest going back to
the simpler alignment methods first described by
IBM (1993). Our approach exploits the possibil-
ity of working with alignments at two different lev-
els of granularity, lexical (words) and shallow pars-
ing (chunks). Apart from redefining the scope of
the alignment unit, we may use different linguistic
data views. We enrich tokens with features further
than lexical such as part-of-speech (PoS), lemma,
and chunk IOB label.
For instance, suppose the case illustrated in Fig-
ure 1 where the lexical item ?plays? is seen acting as
a verb and as a noun. Considering these two words,
with the same lexical realization, as a single token
adds noise to the word alignment process. Repre-
senting this information, by means of linguistic data
views, as ?playsV BZ? and ?playsNNS? would allow us
to distinguish between the two cases. Ideally, one
would wish to have still deeper information, moving
through syntax onto semantics, such as word senses.
Therefore, it would be possible to distinguish for
instance between two realizations of ?plays? with
different meanings: ?hePRP playsV BG guitarNN ? and
?hePRP playsV BG footballNN ?. Of course, there is a
natural trade-off between the use of linguistic data
views and data sparsity. Fortunately, we hava data
enough so that statistical parameter estimation re-
mains reliable.
The approach which is closest to ours is that by
Schafer and Yarowsky (2003) who suggested a com-
bination of models based on shallow syntactic anal-
ysis (part-of-speech tagging and phrase chunking).
They followed a backoff strategy in the application
of their models. Decoding was based on Finite State
Automata. Although no significant improvement in
MT quality was reported, results were promising
taking into account the short time spent in the de-
velopment of the linguistic tools utilized.
Our system is further described in Section 2. Re-
sults are reported in Section 3. Conclusions and fur-
ther work are briefly outlined in Section 4.
166
Figure 1: A case of word alignment possibilities on top of lexical units (a) and linguistic data views (b).
2 System Description
The LDV-COMBO system follows the SMT architec-
ture suggested by the workshop organizers. We use
the Pharaoh beam-search decoder (Koehn, 2004).
First, training data are linguistically annotated. In
order to achieve robustness the same tools have been
used to linguistically annotate both languages. The
SVMTool1 has been used for PoS-tagging (Gime?nez
and Ma`rquez, 2004). The Freeling2 package (Car-
reras et al, 2004) has been used for lemmatizing.
Finally, the Phreco software (Carreras et al, 2005)
has been used for shallow parsing. In this paper we
focus on data views at the word level. 6 different
data views have been built: (W) word, (L) lemma,
(WP) word and PoS, (WC) word and chunk IOB la-
bel, (WPC) word, PoS and chunk IOB label, (LC)
lemma and chunk IOB label.
Then, running GIZA++ (Och and Ney, 2003), we
obtain token alignments for each of the data views.
Combined phrase-based translation models are built
on top of the Viterbi alignments output by GIZA++.
Phrase extraction is performed following the phrase-
extract algorithm depicted by Och (2002). We do
not apply any heuristic refinement. We work with
phrases up to 5 tokens. Phrase pairs appearing only
once have been discarded. Scoring is performed by
relative frequency. No smoothing is applied.
In this paper we focus on the global phrase ex-
traction (GPHEX) method described by Gime?nez
1The SVMTool may be freely downloaded at
http://www.lsi.upc.es/?nlp/SVMTool/ .
2Freeling Suite of Language Analyzers may be downloaded
at http://www.lsi.upc.es/?nlp/freeling/
and Ma`rquez (2005). We build a single translation
model from the union of alignments from the 6 data
views described above. This model must match the
input format. For instance, if the input is annotated
with word and PoS (WP), so must be the translation
model. Therefore either the input must be enriched
with linguistic annotation or translation models must
be post-processed in order to remove the additional
linguistic annotation. We did not observe significant
differences in either alternative. Therefore, we sim-
ply adapted translations models to work under the
assumption of unannotated inputs (W).
3 Experimental Work
3.1 Setting
We have used only the data sets and language model
provided by the organization. For evaluation we
have selected a set of 8 metric variants correspond-
ing to seven different families: BLEU (n = 4) (Pa-
pineni et al, 2001), NIST (n = 5) (Lin and Hovy,
2002), GTM F1-measure (e = 1, 2) (Melamed et al,
2003), 1-WER (Nie?en et al, 2000), 1-PER (Leusch
et al, 2003), ROUGE (ROUGE-S*) (Lin and Och,
2004) and METEOR3 (Banerjee and Lavie, 2005).
Optimization of the decoding parameters (?tm, ?lm,
?w) is performed by means of the Downhill Simplex
Method in Multidimensions (William H. Press and
Flannery, 2002) over the BLEU metric.
3For Spanish-to-English we applied all available modules:
exact + stemming + WordNet stemming + WordNet synonymy
lookup. However, for English-to-Spanish we were forced to use
the exact module alone.
167
Spanish-to-English
System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S*
Baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643
LDV-COMBO 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671
English-to-Spanish
System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S*
Baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028
LDV-COMBO 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240
Table 1: MT results comparing the LDV-COMBO system to a baseline system, for the test set both on the
Spanish-to-English and English-to-Spanish tasks.
English Reference: consider germany , where some leaders [...]
Spanish Reference: pensemos en alemania , donde algunos dirigentes [...]
English-to-Spanish Baseline estiman que alemania , donde algunos dirigentes [...]
LDV-COMBO pensemos en alemania , donde algunos dirigentes [...]
Table 2: A case of error analysis.
3.2 Results
Table 1 presents MT results for the test set both
for the Spanish-to-English and English-to-Spanish
tasks. The variant of the LDV-COMBO system de-
scribed in Section 2 is compared to a baseline vari-
ant based only on lexical items. In the case of
Spanish-to-English performance varies from metric
to metric. Therefore, an open issue is which metric
should be trusted. In any case, the differences are
minor. However, in the case of English-to-Spanish
all metrics but ?1-WER? agree to indicate that the
LDV-COMBO system significantly outperforms the
baseline. We suspect this may be due to the richer
morphology of Spanish. In order to test this hy-
pothesis we performed an error analysys at the sen-
tence level based on the GTM F-measure. We found
many cases where the LDV-COMBO system outper-
forms the baseline system by choosing a more ac-
curate translation. For instance, in Table 2 we may
see a fragment of the case of sentence 2176 in the
test set. A better translation for ?consider? is pro-
vided, ?pensemos?, which corresponds to the right
verb and verbal form (instead of ?estiman?). By in-
specting translation models we confirmed the better
adjustment of probabilities.
Interestingly, LDV-COMBO translation models are
between 30% and 40% smaller than the models
based on lexical items alone. The reason is that we
are working with the union of alignments from dif-
ferent data views, thus adding more constraints into
the phrase extraction step. Fewer phrase pairs are
extracted, and as a consequence we are also effec-
tively eliminating noise from translation models.
4 Conclusions and Further Work
Many researchers remain sceptical about the use-
fulness of linguistic information in SMT, because,
except in a couple of cases (Charniak et al, 2003;
Collins et al, 2005), little success has been reported.
In this work we have shown that liniguistic informa-
tion may be helpful, specially when the target lan-
guage has a rich morphology (e.g. Spanish).
Moreover, it has often been argued that linguistic
information does not yield significant improvements
in MT quality, because (i) linguistic processors in-
troduce many errors and (ii) the BLEU score is not
specially sensitive to the grammaticality of MT out-
put. We have minimized the impact of the first ar-
gument by using highly accurate tools for both lan-
guages. In order to solve the second problem more
sophisticated metrics are required. Current MT eval-
uation metrics fail to capture many aspects of MT
168
quality that characterize human translations with re-
spect to those produced by MT systems. We are de-
voting most of our efforts to the deployment of a new
MT evaluation framework which allows to combine
several similarity metrics into a single measure of
quality (Gime?nez and Amigo?, 2006).
We also leave for further work the experimenta-
tion of new data views such as word senses and se-
mantic roles, as well as their natural porting from the
alignment step to phrase extraction and decoding.
Acknowledgements
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02). Authors are thankful to Pa-
trik Lambert for providing us with the implementa-
tion of the Simplex Method used for tuning.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Peter E Brown, Stephen A. Della Pietra, Robert L. Mer-
cer, and Vincent J. Della Pietra. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter Esti-
mation. Computational Linguistics, 19(2):263?311.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro.
2005. Filtering-Ranking Perceptron Learning for Par-
tial Parsing. Machine Learning, 59:1?31.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based Language Models for Machine
Translation. In Proceedings of MT SUMMIT IX.
Michael Collins, Philipp Koehn, and Ivona Kucerova?.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL.
Daniel Gildea. 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of ACL.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In Pro-
ceedings of the Workshop on Building and Using Par-
allel Texts, ACL.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA.
G. Leusch, N. Ueffing, and H. Ney. 2003. A Novel
String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Proceedings of
MT Summit IX.
Chin-Yew Lin and E.H. Hovy. 2002. Automatic Eval-
uation of Machine Translation Quality Using N-gram
Co-Occurrence Statistics. Technical report, National
Institute of Standards and Technology.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of ACL.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL.
S. Nie?en, F.J. Och, G. Leusch, and H. Ney. 2000. Eval-
uation Tool for Machine Translation: Fast Evaluation
for MT Research. In Proceedings of the 2nd Interna-
tional Conference on Language Resources and Evalu-
ation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation, rc22176. Technical report,
IBM T.J. Watson Research Center.
Charles Schafer and David Yarowsky. 2003. Statistical
Machine Translation Using Coercive Two-Level Syn-
tactic Transduction. In Proceedings of EMNLP.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing. Cambridge
University Press.
Kenji Yamada and Kevin Knight. 2001. A Syntax-based
Statistical Translation Model. In Proceedings of ACL.
169
Proceedings of the Second Workshop on Statistical Machine Translation, pages 159?166,
Prague, June 2007. c?2007 Association for Computational Linguistics
Context-aware Discriminative Phrase Selection
for Statistical Machine Translation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
In this work we revise the application
of discriminative learning to the problem
of phrase selection in Statistical Machine
Translation. Inspired by common tech-
niques used in Word Sense Disambiguation,
we train classifiers based on local context
to predict possible phrase translations. Our
work extends that of Vickrey et al (2005) in
two main aspects. First, we move from word
translation to phrase translation. Second, we
move from the ?blank-filling? task to the ?full
translation? task. We report results on a set
of highly frequent source phrases, obtaining
a significant improvement, specially with re-
spect to adequacy, according to a rigorous
process of manual evaluation.
1 Introduction
Translations tables in Phrase-based Statistical Ma-
chine Translation (SMT) are often built on the ba-
sis of Maximum-likelihood Estimation (MLE), be-
ing one of the major limitations of this approach that
the source sentence context in which phrases occur
is completely ignored (Koehn et al, 2003).
In this work, inspired by state-of-the-art Word
Sense Disambiguation (WSD) techniques, we sug-
gest using Discriminative Phrase Translation (DPT)
models which take into account a wider feature
context. Following the approach by Vickrey et al
(2005), we deal with the ?phrase translation? prob-
lem as a classification problem. We use Support
Vector Machines (SVMs) to predict phrase transla-
tions in the context of the whole source sentence.
We extend the work by Vickrey et al (2005) in two
main aspects. First, we move from ?word transla-
tion? to ?phrase translation?. Second, we move from
the ?blank-filling? task to the ?full translation? task.
Our approach is fully described in Section 2. We
apply it to the Spanish-to-English translation of Eu-
ropean Parliament Proceedings. In Section 3, prior
to considering the ?full translation? task, we ana-
lyze the impact of using DPT models for the iso-
lated ?phrase translation? task. In spite of working
on a very specific domain, a large room for improve-
ment, coherent with WSD performance, and results
by Vickrey et al (2005), is predicted. Then, in Sec-
tion 4, we tackle the full translation task. DPT mod-
els are integrated in a ?soft? manner, by making them
available to the decoder so they can fully interact
with other models. Results using a reduced set of
highly frequent source phrases show a significant
improvement, according to several automatic eval-
uation metrics. Interestingly, the BLEU metric (Pap-
ineni et al, 2001) is not able to reflect this improve-
ment. Through a rigorous process of manual eval-
uation we have verified the gain. We have also ob-
served that it is mainly related to adequacy. These
results confirm that better phrase translation proba-
bilities may be helpful for the full translation task.
However, the fact that no gain in fluency is reported
indicates that the integration of these probabilities
into the statistical framework requires further study.
2 Discriminative Phrase Translation
In this section we describe the phrase-based SMT
baseline system and how DPT models are built and
integrated into this system in a ?soft? manner.
159
2.1 Baseline System
The baseline system is a phrase-based SMT sys-
tem (Koehn et al, 2003), built almost entirely us-
ing freely available components. We use the SRI
Language Modeling Toolkit (Stolcke, 2002) for lan-
guage modeling. We build trigram language models
applying linear interpolation and Kneser-Ney dis-
counting for smoothing. Translation models are
built on top of word-aligned parallel corpora linguis-
tically annotated at the level of shallow syntax (i.e.,
lemma, part-of-speech, and base phrase chunks)
as described by Gime?nez and Ma`rquez (2005).
Text is automatically annotated, using the SVM-
Tool (Gime?nez and Ma`rquez, 2004), Freeling (Car-
reras et al, 2004), and Phreco (Carreras et al, 2005)
packages. We used the GIZA++ SMT Toolkit1 (Och
and Ney, 2003) to generate word alignments. We
apply the phrase-extract algorithm, as described by
Och (2002), on the Viterbi alignments output by
GIZA++ following the ?global phrase extraction?
strategy described by Gime?nez and Ma`rquez (2005)
(i.e., a single phrase translation table is built on top
of the union of alignments corresponding to dif-
ferent linguistic data views). We work with the
union of source-to-target and target-to-source align-
ments, with no heuristic refinement. Phrases up to
length five are considered. Also, phrase pairs ap-
pearing only once are discarded, and phrase pairs
in which the source/target phrase is more than three
times longer than the target/source phrase are ig-
nored. Phrase pairs are scored on the basis of un-
smoothed relative frequency (i.e., MLE). Regard-
ing the argmax search, we used the Pharaoh beam
search decoder (Koehn, 2004), which naturally fits
with the previous tools.
2.2 DPT for SMT
Instead of relying on MLE estimation to score the
phrase pairs (fi, ej) in the translation table, we
suggest considering the translation of every source
phrase fi as a multi-class classification problem,
where every possible translation of fi is a class.
We use local linear SVMs 2. Since SVMs are bi-
nary classifiers, the problem must be binarized. We
1http://www.fjoch.com/GIZA++.html
2We use the SVMlight package, which is freely available at
http://svmlight.joachims.org (Joachims, 1999).
have applied a simple one-vs-all binarization, i.e., a
SVM is trained for every possible translation candi-
date ej . Training examples are extracted from the
same training data as in the case of MLE models,
i.e., an aligned parallel corpus, obtained as described
in Section 2.1. We use each sentence pair in which
the source phrase fi occurs to generate a positive ex-
ample for the classifier corresponding to the actual
translation of fi in that sentence, according to the
automatic alignment. This will be as well a negative
example for the classifiers corresponding to the rest
of possible translations of fi.
2.2.1 Feature Set
We consider different kinds of information, al-
ways from the source sentence, based on standard
WSD methods (Yarowsky et al, 2001). As to the
local context, inside the source phrase to disam-
biguate, and 5 tokens to the left and to the right,
we use n-grams (n ? {1, 2, 3}) of: words, parts-
of-speech, lemmas and base phrase chunking IOB
labels. As to the global context, we collect topical
information by considering the source sentence as a
bag of lemmas.
2.2.2 Decoding. A Trick.
At translation time, we consider every instance of
fi as a separate case. In each case, for all possi-
ble translations of fi, we collect the SVM score, ac-
cording to the SVM classification rule. We are in
fact modeling P (ej |fi). However, these scores are
not probabilities. We transform them into proba-
bilities by applying the softmax function described
by Bishop (1995). We do not constrain the decoder
to use the translation ej with highest probability. In-
stead, we make all predictions available and let the
decoder choose. We have avoided implementing a
new decoder by pre-computing all the SVM pre-
dictions for all possible translations for all source
phrases appearing in the test set. We input this in-
formation onto the decoder by replicating the entries
in the translation table. In other words, each distinct
occurrence of every single source phrase has a dis-
tinct list of phrase translation candidates with their
corresponding scores. Accordingly, the source sen-
tence is transformed into a sequence of identifiers,
160
in our case a sequence of (w, i) pairs3, which allow
us to uniquely identify every distinct instance of ev-
ery word in the test set during decoding, and to re-
trieve DPT predictions in the translation table. For
that purpose, source phrases in the translation table
must comply with the same format.
This imaginative trick4 saved us in the short run
a gigantic amount of work. However, it imposes a
severe limitation on the kind of features which the
DPT system may use. In particular, features from
the target sentence under construction and from
the correspondence between source and target (i.e.,
alignments) can not be used.
3 Phrase Translation
Analogously to the ?word translation? definition by
Vickrey et al (2005), rather than predicting the sense
of a word according to a given sense inventory, in
?phrase translation?, the goal is to predict the correct
translation of a phrase, for a given target language,
in the context of a sentence. This task is simpler than
the ?full translation? task, but provides an insight to
the gain prospectives.
We used the data from the Openlab 2006 Initia-
tive5 promoted by the TC-STAR Consortium6. This
test suite is entirely based on European Parliament
Proceedings. We have focused on the Spanish-to-
English task. The training set consists of 1,281,427
parallel sentences. Performing phrase extraction
over the training data, as described in Section 2.1,
we obtained translation candidates for 1,729,191
source phrases. We built classifiers for all the source
phrases with more than one possible translation and
more than 10 occurrences. 241,234 source phrases
fulfilled this requirement. For each source phrase,
we used 80% of the instances for training, 10% for
development, and 10% for test.
Table 1 shows ?phrase translation? results over
the test set. We compare the performance, in terms
of accuracy, of DPT models and the ?most fre-
quent translation? baseline (?MFT?). The MFT base-
3w is a word and i corresponds to the number of instances
of word w seen in the test set before the current instance.
4We have checked that results following this type of decod-
ing when translation tables are estimated on the basis of MLE
are identical to regular decoding results.
5http://tc-star.itc.it/openlab2006/
6http://www.tc-star.org/
phrase set model macro micro
all MFT 0.66 0.70
DPT 0.68 0.76
frequent MFT 0.76 0.75
DPT 0.86 0.86
Table 1: ?Phrase Translation? Accuracy (test set).
line is equivalent to selecting the translation candi-
date with highest probability according to MLE. The
?macro? column shows macro-averaged results over
all phrases, i.e., the accuracy for each phrase counts
equally towards the average. The ?micro? column
shows micro-averaged accuracy, where each test ex-
ample counts equally. The ?all? set includes results
for the 241,234 phrases, whereas the ?frequent? set
includes results for a selection of 41 very frequent
phrases ocurring more than 50,000 times.
A priori, DPT models seem to offer a significant
room for potential improvement. Although phrase
translation differs from WSD in a number of as-
pects, the increase with respect to the MFT baseline
is comparable. Results are also coherent with those
attained by Vickrey et al (2005).
-1
-0.5
 0
 0.5
 1
 0  50000  100000  150000  200000  250000  300000
a
cc
u
ra
cy
(D
PT
) -
 ac
cu
rac
y(M
LE
)
#examples
Figure 1: Analysis of ?Phrase Translation? Results
on the development set (Spanish-to-English).
Figure 1 shows the relationship between the accu-
racy7 gain and the number of training examples. In
general, with a sufficient number of examples (over
10,000), DPT outperforms the MFT baseline.
7We focus on micro-averaged accuracy.
161
4 Full Translation
In the ?phrase translation? task the predicted phrase
does not interact with the rest of the target sentence.
In this section we analyze the impact of DPT models
when the goal is to translate the whole sentence.
For evaluation purposes we count on a set of 1,008
sentences. Three human references per sentence are
available. We randomly split this set in two halves,
and use them for development and test, respectively.
4.1 Evaluation
Evaluating the effects of using DPT predictions, di-
rected towards a better word selection, in the full
translation task presents two serious difficulties.
In first place, the actual room for improvement
caused by a better translation modeling is smaller
than estimated in Section 3. This is mainly due to
the SMT architecture itself which relies on a search
over a probability space in which several models co-
operate. For instance, in many cases errors caused
by a poor translation modeling may be corrected by
the language model. In a recent study, Vilar et al
(2006) found that only around 25% of the errors are
related to word selection. In half of these cases er-
rors are caused by a wrong word sense disambigua-
tion, and in the other half the word sense is correct
but the lexical choice is wrong.
In second place, most conventional automatic
evaluation metrics have not been designed for this
purpose. For instance, metrics such as BLEU (Pa-
pineni et al, 2001) tend to favour longer n-gram
matchings, and are, thus, biased towards word or-
dering. We might find better suited metrics, such
as METEOR (Banerjee and Lavie, 2005), which is
oriented towards word selection8. However, a new
problem arises. Because different metrics are biased
towards different aspects of quality, scores conferred
by different metrics are often controversial.
In order to cope with evaluation difficulties we
have applied several complementary actions:
1. Based on the results from Section 3, we focus
on a reduced set of 41 very promising phrases
trained on more than 50,000 examples. This
set covers 25.8% of the words in the test set,
8METEOR works at the unigram level, may consider word
stemming and, for the case of English is also able to perform a
lookup for synonymy in WordNet (Fellbaum, 1998).
and exhibits a potential absolute accuracy gain
around 11% (See Table 1).
2. With the purpose of evaluating the changes re-
lated only to this small set of very promis-
ing phrases, we introduce a new measure, Apt,
which computes ?phrase translation? accuracy
for a given list of source phrases. For every
test case, Apt counts the proportion of phrases
from the list appearing in the source sentence
which have a valid9 translation both in the tar-
get sentence and in any of the reference trans-
lations. In fact, because in general source-to-
target algnments are not known, Apt calculates
an approximate10 solution.
3. We evaluate overall MT quality on the basis
of ?Human Likeness?. In particular, we use
the QUEEN11 meta-measure from the QARLA
Framework (Amigo? et al, 2005). QUEEN op-
erates under the assumption that a good trans-
lation must be similar to all human references
according to all metrics. Given a set of auto-
matic translations A, a set of similarity metrics
X, and a set of human references R, QUEEN is
defined as the probability, over R?R?R, that
for every metric in X the automatic translation
a is more similar to a reference r than two other
references r? and r?? to each other. Formally:
QUEENX,R(a) = Prob(?x ? X : x(a, r) ? x(r?, r??))
QUEEN captures the features that are common
to all human references, rewarding those auto-
matic translations which share them, and pe-
nalizing those which do not. Thus, QUEEN pro-
vides a robust means of combining several met-
rics into a single measure of quality. Following
the methodology described by Gime?nez and
Amigo? (2006), we compute the QUEEN mea-
sure over the metric combination with high-
est KING, i.e., discriminative power. We have
considered all the lexical metrics12 provided by
9Valid translations are provided by the translation table.
10Current Apt implementation searches phrases from left to
right in decreasing length order.
11QUEEN is available inside the IQMT package for MT
Evaluation based on ?Human Likeness? (Gime?nez and Amigo?,
2006). http://www.lsi.upc.edu/?nlp/IQMT
12Consult the IQMT Technical Manual v1.3 for a detailed de-
scription of the metric set. http://www.lsi.upc.edu/
?nlp/IQMT/IQMT.v1.3.pdf
162
QUEEN Apt BLEU METEOR ROUGE
P (e) + PMLE(f |e) 0.43 0.86 0.59 0.77 0.42
P (e) + PMLE(e|f) 0.45 0.87 0.62 0.77 0.43
P (e) + PDPT (e|f) 0.47 0.89 0.62 0.78 0.44
Table 2: Automatic evaluation of the ?full translation? results on the test set.
IQMT. The optimal set is:
{ METEORwnsyn, ROUGEw 1.2 }
which includes variants of METEOR, and
ROUGE (Lin and Och, 2004).
4.2 Adjustment of Parameters
Models are combined in a log-linear fashion:
logP (e|f) ? ?lmlogP (e) + ?glogPMLE(f |e)
+ ?dlogPMLE(e|f) + ?DPT logPDPT (e|f)
P (e) is the language model probability.
PMLE(f |e) corresponds to the MLE-based
generative translation model, whereas PMLE(e|f)
corresponds to the analogous discriminative model.
PDPT (e|f) corresponds to the DPT model which
uses SVM-based predictions in a wider feature
context. In order to perform fair comparisons,
model weights must be adjusted.
Because we have focused on a reduced set of fre-
quent phrases, in order to translate the whole test set
we must provide alternative translation probabilities
for all the source phrases in the vocabulary which
do not have a DPT prediction. We have used MLE
predictions to complete the model. However, inter-
action between DPT and MLE models is problem-
atic. Problems arise when, for a given source phrase,
fi, DPT predictions must compete with MLE pre-
dictions for larger phrases fj overlapping with or
containing fi (See Section 4.3). We have alleviated
these problems by splitting DPT tables in 3 subta-
bles: (1) phrases with DPT prediction, (2) phrases
with DPT prediction only for subphrases of it, and
(3) phrases with no DPT prediction for any sub-
phrase; and separately adjusting their weights.
Counting on a reliable automatic measure of qual-
ity is a crucial issue for system development. Opti-
mal configurations may vary very significantly de-
pending on the metric governing the optimization
process. We optimize the system parameters over
the QUEEN measure, which has proved to lead to
more robust system configurations than BLEU (Lam-
bert et al, 2006). We exhaustively try all possible
parameter configurations, at a resolution of 0.1, over
the development set and select the best one. In order
to keep the optimization process feasible, in terms of
time, the search space is pruned13 during decoding.
4.3 Results
We compare the systems using the generative and
discriminative MLE-based translation models to the
discriminative translation model which uses DPT
predictions for the set of 41 very ?frequent? source
phrases. Table 2 shows automatic evaluation re-
sults on the test set, according to several metrics.
Phrase translation accuracy (over the ?frequent? set
of phrases) and MT quality are evaluated by means
of the Apt and QUEEN measures, respectively. For
the sake of informativeness, BLEU, METEORwnsyn
and ROUGEw 1.2 scores are provided as well.
Interestingly, discriminative models outperform
the (noisy-channel) default generative model. Im-
provement in Apt measure also reveals that DPT pre-
dictions provide a better translation for the set of
?frequent? phrases than the MLE models. This im-
provement remains when measuring overall transla-
tion quality via QUEEN. If we take into account that
DPT predictions are available for only 25% of the
words in the test set, we can say that the gain re-
ported by the QUEEN and Apt measures is consistent
with the accuracy prospectives predicted in Table 1.
METEORwnsyn and ROUGEw 1.2 reflect a slight im-
provement as well. However, according to BLEU
there is no difference between both systems. We
suspect that BLEU is unable to accurately reflect the
possible gains attained by a better ?phrase selection?
over a small set of phrases because of its tendency
13For each phrase only the 30 top-scoring translations are
used. At all times, only the 100 top-scoring solutions are kept.
We also disabled distortion and word penalty models. There-
fore, translations are monotonic, and source and target tend to
have the same number of words (that is not mandatory).
163
to reward long n-gram matchings. In order to clar-
ify this scenario a rigorous process of manual evalu-
ation has been conducted. We have selected a subset
of sentences based on the following criteria:
? sentence length between 10 and 30 words.
? at least 5 words have a DPT prediction.
? DPT and MLE outputs differ.
A total of 114 sentences fulfill these require-
ments. In each translation case, assessors must judge
whether the output by the discriminative ?MLE? sys-
tem is better, equal to or worse than the output by
the ?DPT? system, with respect to adequacy, fluency,
and overall quality. In order to avoid any bias in the
evaluation, we have randomized the respective posi-
tion in the display of the sentences corresponding to
each system. Four judges participated in the evalua-
tion. Each judge evaluated only half of the cases.
Each case was evaluated by two different judges.
Therefore, we count on 228 human assessments.
Table 3 shows the results of the manual system
comparison. Statistical significance has been deter-
mined using the sign-test (Siegel, 1956). According
to human assessors, the ?DPT? system outperforms
the ?MLE? system very significantly with respect to
adequacy, whereas for fluency there is a slight ad-
vantage in favor of the ?MLE? system. Overall, there
is a slight but significant advantage in favor of the
?DPT? system. Manual evaluation confirms our sus-
picion that the BLEU metric is less sensitive than
QUEEN to improvements related to adequacy.
Error Analysis
Guided by the QUEEN measure, we carefully inspect
particular cases. We start, in Table 4, by show-
ing a positive case. The three phrases highlighted
in the source sentence (?tiene?, ?sen?ora? and ?una
cuestio?n?) find a better translation with the help of
the DPT models: ?tiene? translates into ?has? instead
of ?i give?, ?sen?ora? into ?mrs? instead of ?lady?, and
?una cuestio?n? into ?a point? instead of ?a ... motion?.
In contrast, Table 5 shows a negative case. The
translation of the Spanish word ?sen?ora? as ?mrs? is
acceptable. However, it influences very negatively
the translation of the following word ?diputada?,
whereas the ?MLE? system translates the phrase
?sen?ora diputada?, which does not have a DPT pre-
diction, as a whole. Similarly, the translation of
Adequacy Fluency Overall
MLE > DPT 39 84 83
MLE = DPT 100 76 46
MLE < DPT 89 68 99
Table 3: Manual evaluation of the ?full translation?
results on the test set. Counts on the number of
translation cases for which the ?MLE? system is bet-
ter than (>), equal to (=), or worse than (<) the
?DPT? system, with respect to adequacy, fluency,
and overall MT quality, are presented.
?cuestio?n? as ?matter?, although acceptable, is break-
ing the phrase ?cuestio?n de orden? of high cohe-
sion, which is commonly translated as ?point of or-
der?. The cause underlying these problems is that
DPT predictions are available only for a subset of
phrases. Thus, during decoding, for these cases our
DPT models may be in disadvantage.
5 Related Work
Recently, there is a growing interest in the appli-
cation of WSD technology to MT. For instance,
Carpuat and Wu (2005b) suggested integrating
WSD predictions into a SMT system in a ?hard?
manner, either for decoding, by constraining the set
of acceptable translation candidates for each given
source word, or for post-processing the SMT sys-
tem output, by directly replacing the translation of
each selected word with the WSD system predic-
tion. They did not manage to improve MT quality.
They encountered several problems inherent to the
SMT architecture. In particular, they described what
they called the ?language model effect? in SMT:
?The lexical choices are made in a way that heav-
ily prefers phrasal cohesion in the output target sen-
tence, as scored by the language model.?. This prob-
lem is a direct consequence of the ?hard? interaction
between their WSD and SMT systems. WSD pre-
dictions cannot adapt to the surrounding target con-
text. In a later work, Carpuat and Wu (2005a) ana-
lyzed the converse question, i.e. they measured the
WSD performance of SMT models. They showed
that dedicated WSD models significantly outper-
form current state-of-the-art SMT models. Conse-
quently, SMT should benefit from WSD predictions.
Simultaneously, Vickrey et al (2005) studied the
164
Source tiene la palabra la sen?ora mussolini para una cuestio?n de orden .
Ref 1 mrs mussolini has the floor for a point of order .
Ref 2 you have the floor , missus mussolini , for a question of order .
Ref 3 ms mussolini has now the floor for a point of order .
P (e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion .
P (e) + PDPT (e|f) has the floor the mrs mussolini on a point of order .
Table 4: Case of Analysis of sentence #422. DPT models help.
Source sen?ora diputada , e?sta no es una cuestio?n de orden .
Ref 1 mrs mussolini , that is not a point of order .
Ref 2 honourable member , this is not a question of order .
Ref 3 my honourable friend , this is not a point of order .
P (e) + PMLE(e|f) honourable member , this is not a point of order .
P (e) + PDPT (e|f) mrs karamanou , this is not a matter of order .
Table 5: Case of Analysis of sentence #434. DPT models fail.
application of discriminative models based on WSD
technology to the ?blank-filling? task, a simplified
version of the translation task, in which the target
context surrounding the word translation is avail-
able. They did not encounter the ?language model
effect? because they approached the task in a ?soft?
way, i.e., allowing their WSD models to interact
with other models during decoding. Similarly, our
DPT models are, as described in Section 2.2, softly
integrated in the decoding step, and thus do not suf-
fer from the detrimental ?language model effect? ei-
ther, in the context of the ?full translation? task. Be-
sides, DPT models enforce phrasal cohesion by con-
sidering disambiguation at the level of phrases.
6 Conclusions and Further Work
Despite the fact that measuring improvements in
word selection is a very delicate issue, we have
showed that dedicated discriminative translation
models considering a wider feature context provide
a useful mechanism in order to improve the qual-
ity of current phrase-based SMT systems, specially
with regard to adequacy. However, the fact that no
gain in fluency is reported indicates that the integra-
tion of these probabilities into the statistical frame-
work requires further study.
Moreover, there are several open issues. First, for
practical reasons, we have limited to a reduced set of
?frequent? phrases, and we have disabled reordering
and word penalty models. We are currently studying
the impact of a larger set of phrases, covering over
99% of the words in the test set. Experiments with
enabled reordering and word penalty models should
be conducted as well. Second, automatic evalua-
tion of the results revealed a low agreement between
BLEU and other metrics. For system comparison, we
solved this through a process of manual evaluation.
However, this is impractical for the adjustment of
parameters, where hundreds of different configura-
tions are tried. In this work we have relied on auto-
matic evaluation based on ?Human Likeness? which
allows for metric combinations and provides a sta-
ble and robust criterion for the metric set selection.
Other alternatives could be tried. The crucial issue,
in our opinion, is that the metric guiding the opti-
mization is able to capture the changes.
Finally, we argue that, if DPT models considered
features from the target side, and from the corre-
spondence between source and target, results could
further improve. However, at the short term, the in-
corporation of these type of features will force us to
either build a new decoder or extend an existing one,
or to move to a new MT architecture, for instance,
in the fashion of the architectures suggested by Till-
mann and Zhang (2006) or Liang et al (2006).
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
165
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
TC-STAR Consortium for providing such very valu-
able data sets.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Christopher M. Bishop. 1995. 6.4: Modeling conditional
distributions. In Neural Networks for Pattern Recog-
nition, page 215. Oxford University Press.
Marine Carpuat and Dekai Wu. 2005a. Evaluating the
Word Sense Disambiguation Performance of Statisti-
cal Machine Translation. In Proceedings of IJCNLP.
Marine Carpuat and Dekai Wu. 2005b. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of ACL.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In Pro-
ceedings of the Workshop on Building and Using Par-
allel Texts, ACL.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. The MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA.
Patrik Lambert, Jesu?s Gime?nez, Marta R. Costa-jussa?,
Enrique Amigo?, Rafael E. Banchs, Llu??s Ma?rquez, and
J.A. R. Fonollosa. 2006. Machine Translation Sys-
tem Development based on Human Likeness. In Pro-
ceedings of IEEE/ACL 2006 Workshop on Spoken Lan-
guage Technology.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, , and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
COLING-ACL06.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation, rc22176. Technical report,
IBM T.J. Watson Research Center.
Sidney Siegel. 1956. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Christoph Tillmann and Tong Zhang. 2006. A Discrim-
inative Global Training Algorithm for Statistical MT.
In Proceedings of COLING-ACL06.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-Sense Disambiguation for Machine Translation.
In Proceedings of HLT/EMNLP.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proceedings of the 5th LREC.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles
Schafer, and Richard Wicentowski. 2001. The Johns
Hopkins Senseval2 System Descriptions. In Proceed-
ings of Senseval-2: Second International Workshop on
Evaluating Word Sense Disambiguation Systems.
166
Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264,
Prague, June 2007. c?2007 Association for Computational Linguistics
Linguistic Features for Automatic Evaluation of Heterogenous MT Systems
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
Evaluation results recently reported by
Callison-Burch et al (2006) and Koehn and
Monz (2006), revealed that, in certain cases,
the BLEU metric may not be a reliable MT
quality indicator. This happens, for in-
stance, when the systems under evaluation
are based on different paradigms, and there-
fore, do not share the same lexicon. The
reason is that, while MT quality aspects are
diverse, BLEU limits its scope to the lex-
ical dimension. In this work, we suggest
using metrics which take into account lin-
guistic features at more abstract levels. We
provide experimental results showing that
metrics based on deeper linguistic informa-
tion (syntactic/shallow-semantic) are able to
produce more reliable system rankings than
metrics based on lexical matching alone,
specially when the systems under evaluation
are of a different nature.
1 Introduction
Most metrics used in the context of Automatic Ma-
chine Translation (MT) Evaluation are based on
the assumption that ?acceptable? translations tend to
share the lexicon (i.e., word forms) in a predefined
set of manual reference translations. This assump-
tion works well in many cases. However, several
results in recent MT evaluation campaigns have cast
some doubts on its general validity. For instance,
Callison-Burch et al (2006) and Koehn and Monz
(2006) reported and analyzed several cases of strong
disagreement between system rankings provided by
human assessors and those produced by the BLEU
metric (Papineni et al, 2001). In particular, they
noted that when the systems under evaluation are
of a different nature (e.g., rule-based vs. statistical,
human-aided vs. fully automatical, etc.) BLEU may
not be a reliable MT quality indicator. The reason is
that BLEU favours MT systems which share the ex-
pected reference lexicon (e.g., statistical systems),
and penalizes those which use a different one.
Indeed, the underlying cause is much simpler. In
general, lexical similarity is nor a sufficient neither
a necessary condition so that two sentences convey
the same meaning. On the contrary, natural lan-
guages are expressive and ambiguous at different
levels. Consequently, the similarity between two
sentences may involve different dimensions. In this
work, we hypothesize that, in order to ?fairly? evalu-
ate MT systems based on different paradigms, simi-
larities at more abstract linguistic levels must be an-
alyzed. For that purpose, we have compiled a rich
set of metrics operating at the lexical, syntactic and
shallow-semantic levels (see Section 2). We present
a comparative study on the behavior of several met-
ric representatives from each linguistic level in the
context of some of the cases reported by Koehn and
Monz (2006) and Callison-Burch et al (2006) (see
Section 3). We show that metrics based on deeper
linguistic information (syntactic/shallow-semantic)
are able to produce more reliable system rankings
than those produced by metrics which limit their
scope to the lexical dimension, specially when the
systems under evaluation are of a different nature.
256
2 A Heterogeneous Metric Set
For our experiments, we have compiled a represen-
tative set of metrics1 at different linguistic levels.
We have resorted to several existing metrics, and
we have also developed new ones. Below, we group
them according to the level at which they operate.
2.1 Lexical Similarity
Most of the current metrics operate at the lexical
level. We have selected 7 representatives from dif-
ferent families which have been shown to obtain
high levels of correlation with human assessments:
BLEU We use the default accumulated score up to
the level of 4-grams (Papineni et al, 2001).
NIST We use the default accumulated score up to
the level of 5-grams (Doddington, 2002).
GTM We set to 1 the value of the e parame-
ter (Melamed et al, 2003).
METEOR We run all modules: ?exact?, ?porter-
stem?, ?wn stem? and ?wn synonymy?, in that
order (Banerjee and Lavie, 2005).
ROUGE We used the ROUGE-S* variant (skip bi-
grams with no max-gap-length). Stemming is
enabled (Lin and Och, 2004a).
mWER We use 1 ? mWER (Nie?en et al, 2000).
mPER We use 1 ? mPER (Tillmann et al, 1997).
Let us note that ROUGE and METEOR may con-
sider stemming (i.e., morphological variations). Ad-
ditionally, METEOR may perform a lookup for syn-
onyms in WordNet (Fellbaum, 1998).
2.2 Beyond Lexical Similarity
Modeling linguistic features at levels further than
the lexical level requires the usage of more complex
linguistic structures. We have defined what we call
?linguistic elements? (LEs).
2.2.1 Linguistic Elements
LEs are linguistic units, structures, or relation-
ships, such that a sentence may be partially seen as a
?bag? of LEs. Possible kinds of LEs are: word forms,
parts-of-speech, dependency relationships, syntactic
phrases, named entities, semantic roles, etc. Each
1All metrics used in this work are publicly available inside
the IQMT Framework (Gime?nez and Amigo?, 2006). http://
www.lsi.upc.edu/?nlp/IQMT
LE may consist, in its turn, of one or more LEs,
which we call ?items? inside the LE. For instance, a
?phrase? LE may consist of ?phrase? items, ?part-of-
speech? (PoS) items, ?word form? items, etc. Items
may be also combinations of LEs. For instance, a
?phrase? LE may be seen as a sequence of ?word-
form:PoS? items.
2.2.2 Similarity Measures
We are interested in comparing linguistic struc-
tures, and linguistic units. LEs allow for compar-
isons at different granularity levels, and from dif-
ferent viewpoints. For instance, we might compare
the semantic structure of two sentences (i.e., which
actions, semantic arguments and adjuncts exist) or
we might compare lexical units according to the se-
mantic role they play inside the sentence. For that
purpose, we use two very simple kinds of similarity
measures over LEs: ?Overlapping? and ?Matching?.
We provide a general definition:
Overlapping between items inside LEs, according
to their type. Formally:
Overlapping(t) =
X
i?itemst(hyp)
count?hyp(i, t)
X
i?itemst(ref)
countref (i, t)
where t is the LE type2, itemst(s) refers to the
set of items occurring inside LEs of type t in
sentence s, countref(i, t) denotes the number
of times item i appears in the reference trans-
lation inside a LE of type t, and count?hyp(i, t)
denotes the number of times i appears in the
candidate translation inside a LE of type t, lim-
ited by the number of times i appears in the ref-
erence translation inside a LE of type t. Thus,
?Overlapping? provides a rough measure of the
proportion of items inside elements of a cer-
tain type which have been ?successfully? trans-
lated. We also introduce a coarser metric, ?Over-
lapping(*)?, which considers the uniformly aver-
aged ?overlapping? over all types:
Overlapping(?) = 1|T |
X
t?T
Overlapping(t)
where T is the set of types.
2LE types vary according to the specific LE class. For in-
stance, in the case of Named Entities types may be ?PER? (i.e.,
person), ?LOC? (i.e., location), ?ORG? (i.e., organization), etc.
257
Matching between items inside LEs, according to
their type. Its definition is analogous to the
?Overlapping? definition, but in this case the
relative order of the items is important. All
items inside the same element are considered as
a single unit (i.e., a sequence in left-to-right or-
der). In other words, we are computing the pro-
portion of ?fully? translated elements, accord-
ing to their type. We also introduce a coarser
metric, ?Matching(*)?, which considers the uni-
formly averaged ?Matching? over all types.
notes:
? ?Overlapping? and ?Matching? operate on the
assumption of a single reference translation.
The extension to the multi-reference setting is
computed by assigning the maximum value at-
tained over all human references individually.
? ?Overlapping? and ?Matching? are general met-
rics. We may apply them to specific scenarios
by defining the class of linguistic elements and
items to be used. Below, we instantiate these
measures over several particular cases.
2.3 Shallow Syntactic Similarity
Metrics based on shallow parsing (?SP?) analyze
similarities at the level of PoS-tagging, lemmati-
zation, and base phrase chunking. Outputs and
references are automatically annotated using state-
of-the-art tools. PoS-tagging and lemmatization
are provided by the SVMTool package (Gime?nez and
Ma`rquez, 2004), and base phrase chunking is pro-
vided by the Phreco software (Carreras et al, 2005).
Tag sets for English are derived from the Penn Tree-
bank (Marcus et al, 1993).
We instantiate ?Overlapping? over parts-of-speech
and chunk types. The goal is to capture the propor-
tion of lexical items correctly translated, according
to their shallow syntactic realization:
SP-Op-t Lexical overlapping according to the part-
of-speech ?t?. For instance, ?SP-Op-NN? roughly
reflects the proportion of correctly translated
singular nouns. We also introduce a coarser
metric, ?SP-Op-*? which computes average
overlapping over all parts-of-speech.
SP-Oc-t Lexical overlapping according to the
chunk type ?t?. For instance, ?SP-Oc-NP? roughly
reflects the successfully translated proportion
of noun phrases. We also introduce a coarser
metric, ?SP-Oc-*? which considers the average
overlapping over all chunk types.
At a more abstract level, we use the NIST
metric (Doddington, 2002) to compute accumu-
lated/individual scores over sequences of:
Lemmas ? SP-NIST(i)l-n
Parts-of-speech ? SP-NIST(i)p-n
Base phrase chunks ? SP-NIST(i)c-n
For instance, ?SP-NISTl-5? corresponds to the accu-
mulated NIST score for lemma n-grams up to length
5, whereas ?SP-NISTip-5? corresponds to the individ-
ual NIST score for PoS 5-grams.
2.4 Syntactic Similarity
We have incorporated, with minor modifications,
some of the syntactic metrics described by Liu and
Gildea (2005) and Amigo? et al (2006) based on de-
pendency and constituency parsing.
2.4.1 On Dependency Parsing (DP)
?DP? metrics capture similarities between depen-
dency trees associated to automatic and reference
translations. Dependency trees are provided by the
MINIPAR dependency parser (Lin, 1998). Similari-
ties are captured from different viewpoints:
DP-HWC(i)-l This metric corresponds to the HWC
metric presented by Liu and Gildea (2005). All
head-word chains are retrieved. The fraction of
matching head-word chains of a given length,
?l?, is computed. We have slightly modified
this metric in order to distinguish three differ-
ent variants according to the type of items head-
word chains may consist of:
Lexical forms ? DP-HWC(i)w -l
Grammatical categories ? DP-HWC(i)c-l
Grammatical relationships ? DP-HWC(i)r-l
Average accumulated scores up to a given chain
length may be used as well. For instance,
?DP-HWCiw-4? retrieves the proportion of match-
ing length-4 word-chains, whereas ?DP-HWCw -
4? retrieves average accumulated proportion of
matching word-chains up to length-4. Anal-
ogously, ?DP-HWCc-4?, and ?DP-HWCr -4? com-
258
pute average accumulated proportion of cate-
gory/relationship chains up to length-4.
DP-Ol|Oc|Or These metrics correspond exactly to
the LEVEL, GRAM and TREE metrics intro-
duced by Amigo? et al (2006).
DP-Ol-l Overlapping between words hanging
at level ?l?, or deeper.
DP-Oc-t Overlapping between words directly
hanging from terminal nodes (i.e. gram-
matical categories) of type ?t?.
DP-Or-t Overlapping between words ruled
by non-terminal nodes (i.e. grammatical
relationships) of type ?t?.
Node types are determined by grammatical cat-
egories and relationships defined by MINIPAR.
For instance, ?DP-Or-s? reflects lexical overlap-
ping between subtrees of type ?s? (subject). ?DP-
Oc-A? reflects lexical overlapping between ter-
minal nodes of type ?A? (Adjective/Adverbs).
?DP-Ol-4? reflects lexical overlapping between
nodes hanging at level 4 or deeper. Addition-
ally, we consider three coarser metrics (?DP-Ol-
*?, ?DP-Oc-*? and ?DP-Or -*?) which correspond
to the uniformly averaged values over all lev-
els, categories, and relationships, respectively.
2.4.2 On Constituency Parsing (CP)
?CP? metrics capture similarities between con-
stituency parse trees associated to automatic and
reference translations. Constituency trees are pro-
vided by the Charniak-Johnson?s Max-Ent reranking
parser (Charniak and Johnson, 2005).
CP-STM(i)-l This metric corresponds to the STM
metric presented by Liu and Gildea (2005).
All syntactic subpaths in the candidate and the
reference trees are retrieved. The fraction of
matching subpaths of a given length, ?l?, is
computed. For instance, ?CP-STMi-5? retrieves
the proportion of length-5 matching subpaths.
Average accumulated scores may be computed
as well. For instance, ?CP-STM-9? retrieves av-
erage accumulated proportion of matching sub-
paths up to length-9.
2.5 Shallow-Semantic Similarity
We have designed two new families of metrics, ?NE?
and ?SR?, which are intended to capture similari-
ties over Named Entities (NEs) and Semantic Roles
(SRs), respectively.
2.5.1 On Named Entities (NE)
?NE? metrics analyze similarities between auto-
matic and reference translations by comparing the
NEs which occur in them. Sentences are automati-
cally annotated using the BIOS package (Surdeanu
et al, 2005). BIOS requires at the input shallow
parsed text, which is obtained as described in Sec-
tion 2.3. See the list of NE types in Table 1.
Type Description
ORG Organization
PER Person
LOC Location
MISC Miscellaneous
O Not-a-NE
DATE Temporal expressions
NUM Numerical expressions
ANGLE QUANTITY
DISTANCE QUANTITY
SIZE QUANTITY Quantities
SPEED QUANTITY
TEMPERATURE QUANTITY
WEIGHT QUANTITY
METHOD
MONEY
LANGUAGE Other
PERCENT
PROJECT
SYSTEM
Table 1: Named Entity types.
We define two types of metrics:
NE-Oe-t Lexical overlapping between NEs accord-
ing to their type t. For instance, ?NE-Oe-PER? re-
flects lexical overlapping between NEs of type
?PER? (i.e., person), which provides a rough es-
timate of the successfully translated proportion
of person names. The ?NE-Oe-*? metric consid-
ers the average lexical overlapping over all NE
types. This metric includes the NE type ?O?
(i.e., Not-a-NE). We introduce another variant,
?NE-Oe-**?, which considers only actual NEs.
NE-Me-t Lexical matching between NEs accord-
ing to their type t. For instance, ?NE-Me-LOC?
reflects the proportion of fully translated NEs
of type ?LOC? (i.e., location). The ?NE-Me-*?
259
metric considers the average lexical matching
over all NE types, this time excluding type ?O?.
Other authors have measured MT quality over
NEs in the recent literature. In particular, the ?NE-
Me-*? metric is similar to the ?NEE? metric defined
by Reeder et al (2001).
2.5.2 On Semantic Roles (SR)
?SR? metrics analyze similarities between auto-
matic and reference translations by comparing the
SRs (i.e., arguments and adjuncts) which occur in
them. Sentences are automatically annotated using
the SwiRL package (Ma`rquez et al, 2005). This
package requires at the input shallow parsed text en-
riched with NEs, which is obtained as described in
Section 2.5.1. See the list of SR types in Table 2.
Type Description
A0
A1
A2 arguments associated with a verb predicate,
A3 defined in the PropBank Frames scheme.
A4
A5
AA Causative agent
AM-ADV Adverbial (general-purpose) adjunct
AM-CAU Causal adjunct
AM-DIR Directional adjunct
AM-DIS Discourse marker
AM-EXT Extent adjunct
AM-LOC Locative adjunct
AM-MNR Manner adjunct
AM-MOD Modal adjunct
AM-NEG Negation marker
AM-PNC Purpose and reason adjunct
AM-PRD Predication adjunct
AM-REC Reciprocal adjunct
AM-TMP Temporal adjunct
Table 2: Semantic Roles.
We define three types of metrics:
SR-Or-t Lexical overlapping between SRs accord-
ing to their type t. For instance, ?SR-Or-A0? re-
flects lexical overlapping between ?A0? argu-
ments. ?SR-Or -*? considers the average lexical
overlapping over all SR types.
SR-Mr-t Lexical matching between SRs accord-
ing to their type t. For instance, the met-
ric ?SR-Mr-AM-MOD? reflects the proportion of
fully translated modal adjuncts. The ?SR-Mr -*?
metric considers the average lexical matching
over all SR types.
SR-Or This metric reflects ?role overlapping?, i.e..
overlapping between semantic roles indepen-
dently from their lexical realization.
Note that in the same sentence several verbs, with
their respective SRs, may co-occur. However, the
metrics described above do not distinguish between
SRs associated to different verbs. In order to account
for such a distinction we introduce a more restric-
tive version of these metrics (?SR-Mrv-t?, ?SR-Orv-t?,
?SR-Mrv -*?, ?SR-Orv -*?, and ?SR-Orv ?), which require
SRs to be associated to the same verb.
3 Experimental Work
In this section, we study the behavior of some
of the metrics described in Section 2, according
to the linguistic level at which they operate. We
have selected a set of coarse-grained metric vari-
ants (i.e., accumulated/average scores over linguis-
tic units and structures of different kinds)3. We ana-
lyze some of the cases reported by Koehn and Monz
(2006) and Callison-Burch et al (2006). We distin-
guish different evaluation contexts. In Section 3.1,
we study the case of a single reference translation
being available. In principle, this scenario should
diminish the reliability of metrics based on lexical
matching alone, and favour metrics based on deeper
linguistic features. In Section 3.2, we study the case
of several reference translations available. This sce-
nario should alleviate the deficiencies caused by the
shallowness of metrics based on lexical matching.
We also analyze separately the case of ?homoge-
neous? systems (i.e., all systems being of the same
nature), and the case of ?heterogenous? systems (i.e.,
there exist systems based on different paradigms).
As to the metric meta-evaluation criterion, the two
most prominent criteria are:
Human Acceptability Metrics are evaluated on the
basis of correlation with human evaluators.
Human Likeness Metrics are evaluated in terms of
descriptive power, i.e., their ability to distin-
guish between human and automatic transla-
tions (Lin and Och, 2004b; Amigo? et al, 2005).
In our case, metrics are evaluated on the basis of
?Human Acceptability?. Specifically, we use Pear-
son correlation coefficients between metric scores
3When computing ?lexical? overlapping/matching, we use
lemmas instead of word forms.
260
and the average sum of adequacy and fluency as-
sessments at the document level. The reason is
that meta-evaluation based on ?Human Likeness? re-
quires the availability of heterogenous test beds (i.e.,
representative sets of automatic outputs and human
references), which, unfortunately, is not the case of
all the tasks under study. First, because most transla-
tion systems are statistical. Second, because in most
cases only one reference translation is available.
3.1 Single-reference Scenario
We use some of the test beds corresponding to
the ?NAACL 2006 Workshop on Statistical Machine
Translation? (WMT 2006) (Koehn and Monz, 2006).
Since linguistic features described in Section 2 are
so far implemented only for the case of English be-
ing the target language, among the 12 translation
tasks available, we studied only the 6 tasks corre-
sponding to the Foreign-to-English direction. A sin-
gle reference translation is available. System out-
puts consist of 2000 and 1064 sentences for the ?in-
domain? and ?out-of-domain? test beds, respectively.
In each case, human assessments on adequacy and
fluency are available for a subset of systems and sen-
tences. Table 3 shows the number of sentences as-
sessed in each case. Each sentence was evaluated
by two different human judges. System scores have
been obtained by averaging over all sentence scores.
in out sys
French-to-English 2,247 1,274 11/14
German-to-English 2,401 1,535 10/12
Spanish-to-English 1,944 1,070 11/15
Table 3: WMT 2006. ?in? and ?out? columns
show the number of sentences assessed for the ?in-
domain? and ?out-of-domain? subtasks. The ?sys?
column shows the number of systems counting on
human assessments with respect to the total number
of systems which presented to each task.
Evaluation of Heterogeneous Systems
In four of the six translation tasks under study, all
the systems are statistical except ?Systran?, which is
rule-based. This is the case of the German/French-
to-English in-domain/out-of-domain tasks. Table 4
shows correlation with human assessments for some
metric representatives at different linguistic levels.
fr2en de2en
Level Metric in out in out
1-PER 0.73 0.64 0.57 0.46
1-WER 0.73 0.73 0.32 0.38
BLEU 0.71 0.87 0.60 0.67
Lexical NIST 0.74 0.82 0.56 0.63
GTM 0.84 0.86 0.12 0.70
METEOR 0.92 0.95 0.76 0.81
ROUGE 0.85 0.89 0.65 0.79
SP-Op-* 0.81 0.88 0.64 0.71
SP-Oc-* 0.81 0.89 0.65 0.75
Shallow SP-NISTl-5 0.75 0.81 0.56 0.64
Syntactic SP-NISTp-5 0.75 0.91 0.77 0.77
SP-NISTc-5 0.73 0.88 0.71 0.54
DP-HWCw-4 0.76 0.88 0.64 0.74
DP-HWCc-4 0.93 0.97 0.88 0.72
DP-HWCr-4 0.92 0.96 0.91 0.76
Syntactic DP-Ol-* 0.87 0.94 0.84 0.84
DP-Oc-* 0.91 0.95 0.88 0.87
DP-Or-* 0.87 0.97 0.91 0.88
CP-STM-9 0.93 0.95 0.93 0.87
NE-Me-* 0.80 0.79 0.93 0.63
NE-Oe-* 0.79 0.76 0.91 0.59
NE-Oe-** 0.81 0.87 0.63 0.70
SR-Mr-* 0.83 0.95 0.92 0.84
Shallow SR-Or-* 0.89 0.95 0.88 0.90
Semantic SR-Or 0.95 0.85 0.80 0.75
SR-Mrv-* 0.77 0.92 0.72 0.85
SR-Orv-* 0.81 0.93 0.76 0.94
SR-Orv 0.84 0.93 0.81 0.92
Table 4: WMT 2006. Evaluation of Heterogeneous
Systems. French-to-English (fr2en) / German-to-
English (de2en), in-domain and out-of-domain.
Although the four cases are different, we have
identified several regularities. For instance, BLEU
and, in general, all metrics based on lexical match-
ing alone, except METEOR, obtain significantly
lower levels of correlation than metrics based on
deeper linguistic similarities. The problem with lex-
ical metrics is that they are unable to capture the ac-
tual quality of the ?Systran? system. Interestingly,
METEOR obtains a higher correlation, which, in
the case of French-to-English, rivals the top-scoring
metrics based on deeper linguistic features. The rea-
son, however, does not seem to be related to its ad-
ditional linguistic operations (i.e., stemming or syn-
onymy lookup), but rather to the METEOR matching
strategy itself (unigram precision/recall).
Metrics at the shallow syntactic level are in the
same range of lexical metrics. At the properly
syntactic level, metrics obtain in most cases high
correlation coefficients. However, the ?DP-HWCw-4?
metric, which, although from the viewpoint of de-
261
pendency relationships, still considers only lexical
matching, obtains a lower level of correlation. This
reinforces the idea that metrics based on rewarding
long n-grams matchings may not be a reliable qual-
ity indicator in these cases.
At the level of shallow semantics, while ?NE?
metrics are not equally useful in all cases, ?SR? met-
rics prove very effective. For instance, correlation
attained by ?SR-Or-*? reveals that it is important to
translate lexical items according to the semantic role
they play inside the sentence. Moreover, correlation
attained by the ?SR-Mr-*? metric is a clear indication
that in order to achieve a high quality, it is impor-
tant to ?fully? translate ?whole? semantic structures
(i.e., arguments/adjuncts). The existence of all the
semantic structures (?SR-Or?), specially associated to
the same verb (?SR-Orv?), is also important.
Evaluation of Homogeneous Systems
In the two remaining tasks, Spanish-to-English
in-domain/out-of-domain, all the systems are sta-
tistical. Table 5 shows correlation with human as-
sessments for some metric representatives. In this
case, BLEU proves very effective, both in-domain
and out-of-domain. Indeed, all metrics based on lex-
ical matching obtain high levels of correlation with
human assessments. However, still metrics based on
deeper linguistic analysis attain in most cases higher
correlation coefficients, although not as significantly
higher as in the case of heterogeneous systems.
3.2 Multiple-reference Scenario
We study the case reported by Callison-Burch et
al. (2006) in the context of the Arabic-to-English
exercise of the ?2005 NIST MT Evaluation Cam-
paign?4 (Le and Przybocki, 2005). In this case all
systems are statistical but ?LinearB?, a human-aided
MT system (Callison-Burch, 2005). Five reference
translations are available. System outputs consist of
1056 sentences. We obtained permission5 to use 7
system outputs. For six of these systems we counted
4http://www.nist.gov/speech/tests/
summaries/2005/mt05.htm
5Due to data confidentiality, we contacted each participant
individually and asked for permission to use their data. A num-
ber of groups and companies responded positively: Univer-
sity of Southern California Information Sciences Institute (ISI),
University of Maryland (UMD), Johns Hopkins University &
University of Cambridge (JHU-CU), IBM, University of Edin-
burgh, MITRE and LinearB.
es2en
Level Metric in out
1-PER 0.82 0.78
1-WER 0.88 0.83
BLEU 0.89 0.87
Lexical NIST 0.88 0.84
GTM 0.86 0.80
METEOR 0.84 0.81
ROUGE 0.89 0.83
SP-Op-* 0.88 0.80
SP-Oc-* 0.89 0.84
Shallow SP-NISTl-5 0.88 0.85
Syntactic SP-NISTp-5 0.85 0.86
SP-NISTc-5 0.84 0.83
DP-HWCw-4 0.94 0.83
DP-HWCc-4 0.91 0.87
DP-HWCr-4 0.91 0.88
Syntactic DP-Ol-* 0.91 0.84
DP-Oc-* 0.88 0.83
DP-Or-* 0.88 0.84
CP-STM-9 0.89 0.86
NE-Me-* 0.75 0.76
NE-Oe-* 0.71 0.71
NE-Oe-** 0.88 0.80
SR-Mr-* 0.86 0.82
Shallow SR-Or-* 0.92 0.92
Semantic SR-Or 0.91 0.92
SR-Mrv-* 0.89 0.88
SR-Orv-* 0.91 0.92
SR-Orv 0.91 0.91
Table 5: WMT 2006. Evaluation of Homogeneous
Systems. Spanish-to-English (es2en), in-domain
and out-of-domain.
on a subjective manual evaluation based on ade-
quacy and fluency for a subset of 266 sentences (i.e.,
1596 sentences were assessed). Each sentence was
evaluated by two different human judges. System
scores have been obtained by averaging over all sen-
tence scores.
Table 6 shows the level of correlation with hu-
man assessments for some metric representatives
(see ?ALL? column). In this case, lexical metrics
obtain extremely low levels of correlation. Again,
the problem is that lexical metrics are unable to cap-
ture the actual quality of ?LinearB?. At the shallow
syntactic level, only metrics which do not consider
any lexical information (?SP-NISTp-5? and ?SP-NISTc-
5?) attain a significantly higher quality. At the prop-
erly syntactic level, all metrics attain a higher corre-
lation. At the shallow semantic level, again, while
?NE? metrics are not specially useful, ?SR? metrics
prove very effective.
On the other hand, if we remove ?LinearB? (see
262
ar2en
Level Metric ALL SMT
1-PER -0.35 0.75
1-WER -0.50 0.69
BLEU 0.06 0.83
Lexical NIST 0.04 0.81
GTM 0.03 0.92
ROUGE -0.17 0.81
METEOR 0.05 0.86
SP-Op-* 0.05 0.84
SP-Oc-* 0.12 0.89
Shallow SP-NISTl-5 0.04 0.82
Syntactic SP-NISTp-5 0.42 0.89
SP-NISTc-5 0.44 0.68
DP-HWCw-4 0.52 0.86
DP-HWCc-4 0.80 0.75
DP-HWCr-4 0.88 0.86
Syntactic DP-Ol-* 0.51 0.94
DP-Oc-* 0.53 0.91
DP-Or-* 0.72 0.93
CP-STM-9 0.74 0.95
NE-Me-* 0.33 0.78
NE-Oe-* 0.24 0.82
NE-Oe-** 0.04 0.81
SR-Mr-* 0.72 0.96
Shallow SR-Or-* 0.61 0.87
Semantic SR-Or 0.66 0.75
SR-Mrv-* 0.68 0.97
SR-Orv-* 0.47 0.84
SR-Orv 0.46 0.81
Table 6: NIST 2005. Arabic-to-English (ar2en) ex-
ercise. ?ALL? refers to the evaluation of all systems.
?SMT? refers to the evaluation of statistical systems
alone (i.e., removing ?LinearB?).
?SMT? column), lexical metrics attain a much higher
correlation, in the same range of metrics based on
deeper linguistic information. However, still met-
rics based on syntactic parsing, and semantic roles,
exhibit a slightly higher quality.
4 Conclusions
We have presented a comparative study on the
behavior of a wide set of metrics for automatic
MT evaluation at different linguistic levels (lexical,
shallow-syntactic, syntactic, and shallow-semantic)
under different scenarios. We have shown, through
empirical evidence, that linguistic features at more
abstract levels may provide more reliable system
rankings, specially when the systems under evalu-
ation do not share the same lexicon.
We strongly believe that future MT evaluation
campaigns should benefit from these results, by in-
cluding metrics at different linguistic levels. For in-
stance, the following set could be used:
{ ?DP-HWCr-4?, ?DP-Oc-*?, ?DP-Ol-*?, ?DP-Or-*?, ?CP-
STM-9?, ?SR-Or-*?, ?SR-Orv? }
All these metrics are among the top-scoring in all
the translation tasks studied. However, none of these
metrics provides, in isolation, a ?global? measure of
quality. Indeed, all these metrics focus on ?partial?
aspects of quality. We believe that, in order to per-
form ?global? evaluations, different quality dimen-
sions should be integrated into a single measure of
quality. With that purpose, we are currently explor-
ing several metric combination strategies. Prelim-
inary results, based on the QUEEN measure inside
the QARLA Framework (Amigo? et al, 2005), indi-
cate that metrics at different linguistic levels may be
robustly combined.
Experimental results also show that metrics re-
quiring linguistic analysis seem very robust against
parsing errors committed by automatic linguistic
processors, at least at the document level. That
is very interesting, taking into account that, while
reference translations are supposedly well formed,
that is not always the case of automatic translations.
However, it remains pending to test the behaviour at
the sentence level, which could be very useful for er-
ror analysis. Moreover, relying on automatic proces-
sors implies two other important limitations. First,
these tools are not available for all languages. Sec-
ond, usually they are too slow to allow for massive
evaluations, as required, for instance, in the case of
system development. In the future, we plan to incor-
porate more accurate, and possibly faster, linguistic
processors, also for languages other than English, as
they become publicly available.
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
WMT organizers for providing such valuable test
beds. Authors are also thankful to Audrey Le (from
NIST), and to the 2005 NIST MT Evaluation Cam-
paign participants who agreed to share their system
263
outputs and human assessments for the purpose of
this research.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research. In Proceedings of EACL.
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the 2nd IHLT.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102?121.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. Technical
report, NIST, August.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of COLING.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL.
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. Robust Combination Strat-
egy for Semantic Role Labeling. In Proceedings of
HLT/EMNLP.
S. Nie?en, F.J. Och, G. Leusch, and H. Ney. 2000. Eval-
uation Tool for Machine Translation: Fast Evaluation
for MT Research. In Proceedings of the 2nd LREC.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalu-
ation of machine translation, rc22176, ibm. Technical
report, IBM T.J. Watson Research Center.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Con-
fusion of Tongues: an MT Metric. In Proceedings
of the Workshop on MT Evaluation ?Who did what to
whom?? at MT Summit VIII, pages 55?59.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based Search for Statistical
Translation. In Proceedings of European Conference
on Speech Communication and Technology.
264
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 09: Multilevel Semantic Annotation of
Catalan and Spanish
Llu??s Ma`rquez and Luis Villarejo
TALP Research Center
Technical University of Catalonia
{lluism,luisv}@lsi.upc.edu
M. A. Mart?? and Mariona Taule?
Centre de Llenguatge i Computacio?, CLiC
Universitat de Barcelona
{amarti,mtaule}@ub.edu
Abstract
In this paper we describe SemEval-2007 task
number 9 (Multilevel Semantic Annotation
of Catalan and Spanish). In this task, we
aim at evaluating and comparing automatic
systems for the annotation of several seman-
tic linguistic levels for Catalan and Spanish.
Three semantic levels are considered: noun
sense disambiguation, named entity recogni-
tion, and semantic role labeling.
1 Introduction
The Multilevel Semantic Annotation of Catalan and
Spanish task is split into the following three sub-
tasks:
Noun Sense Disambiguation (NSD): Disambigua-
tion of all frequent nouns (?all words? style).
Named Entity Recognition (NER): The annotation
of (possibly embedding) named entities with basic
entity types.
Semantic Role Labeling (SRL): Including also two
subtasks, i.e., the annotation of verbal predicates
with semantic roles (SR), and verb tagging with
semantic?class labels (SC).
All semantic annotation tasks are performed on
exactly the same corpora for each language. We pre-
sented all the annotation levels together as a com-
plex global task, since we were interested in ap-
proaches which address these problems jointly, pos-
sibly taking into account cross-dependencies among
them. However, we were also accepting systems ap-
proaching the annotation in a pipeline style, or ad-
dressing any of the particular subtasks in any of the
languages.
In Section 2 we describe the methodology fol-
lowed to develop the linguistic corpora for the task.
Sections 3 and 4 summarize the task setting and the
participant systems, respectively. Finally, Section 5
presents a comparative analysis of the results. For
any additional information on corpora, resources,
formats, tagsets, annotation manuals, etc. we refer
the reader to the official website of the task1.
2 Linguistic corpora
The corpora used in this SemEval task are a subset of
CESS-ECE, a multilingual Treebank, composed of
a Spanish (CESS-ESP) and a Catalan (CESS-CAT)
corpus of 500K words each (Mart?? et al, 2007b).
These corpora were enriched with different kinds of
semantic information: argument structure, thematic
roles, semantic class, named entities, and WordNet
synsets for the 150 most frequent nouns. The an-
notation process was carried out in a semiautomatic
way, with a posterior manual revision of all auto-
matic processes.
A sequential approach was adopted for the anno-
tation of the corpus, beginning with the basic lev-
els of analysis, i.e., POS tagging and chunking (au-
tomatically performed) and followed by the more
complex levels: syntactic constituents and functions
(manually tagged) and semantic annotation (man-
ual and semiautomatic processes with manual com-
pletion and posterior revision). Furthermore, some
experiments concerning inter-annotator agreement
1www.lsi.upc.edu/?nlp/semeval/msacs.html
42
were carried out at the syntactic (Civit et al, 2003)
and semantic levels (Ma`rquez et al, 2004) in order
to evaluate the quality of the results.
2.1 Syntactic Annotation
The syntactic annotation consists of the labeling of
constituents, including elliptical subjects, and syn-
tactic functions. The surface order was maintained
and only those constituents directly attached to any
kind of ?Sentence? root node were considered (?S?,
?S.NF?, ?S.F?, ?S*?). The syntactic functions are:
subject (SUJ), direct object (OD), indirect object
(OI), attribute (ATR), predicative (CPRED), agent
complement (CAG), and adjunct (CC). Other func-
tions such as textual element (ET), sentence adjunct
(AO), negation (NEG), vocative (VOC) and verb
modifiers (MOD) were tagged, but did not receive
any thematic role.
2.2 Lexical Semantic Information: WordNet
We selected the 150 most frequent nouns in the
whole corpus and annotated their occurrences with
WordNet synsets. No other word categories were
treated (verbs, adjectives and adverbs). We used a
steady version of Catalan and Spanish WordNets,
linked to WordNet 1.6. Each noun either matched
a WordNet synset or a special label indicating a spe-
cific circumstance (for instance, the tag C2S indi-
cates that the word does not appear in the dictio-
nary). All this process was carried out manually.
2.3 Named Entities
The corpora were annotated with both strong and
weak Named Entities. Strong NEs correspond to sin-
gle lexical tokens (e.g., ?[U.S.]LOC?), while weak
NEs include, by definition, some strong entities
(e.g., ?The [president of [US]LOC]PER?). (Are?valo
et al, 2004). Thus, NEs may embed. Six basic se-
mantic categories were distinguished: Person, Orga-
nization, Location, Date, Numerical expression, and
Others (Borrega et al, 2007).
Two golden rules underlie the definition of NEs in
Spanish and Catalan. On the one hand, only a noun
phrase can be a NE. On the other hand, its referent
must be unique and unambiguous. Finally, another
hard rule (although not 100% reliable) is that only a
definite singular noun phrase might be a NE.
2.4 Thematic Role Labeling / Semantic Class
Basic syntactic functions were tagged with both ar-
guments and thematic roles, taking into account the
semantic class related to the verbal predicate (Taule?
et al, 2006b). We characterized predicates by means
of a limited number of Semantic Classes based on
Event Structure Patterns, according to four basic
event classes: states, activities, accomplishments,
and achievements. These general classes were split
into 17 subclasses, depending on thematic roles and
diathesis alternations.
Similar to PropBank, the set of arguments se-
lected by the verb are incrementally numbered ex-
pressing the degree of proximity of an argument in
relation to the verb (Arg0, Arg1, Arg2, Arg3, Arg4).
In our proposal, each argument includes the the-
matic role in its label (e.g., Arg1-PAT). Thus, we
have two different levels of semantic description:
the argument position and the specific thematic role.
This information was previously stored in a verbal
lexicon for each language. In these lexicons, a se-
mantic class was established for each verbal sense,
and the mapping between their syntactic functions
with the corresponding argument structure and the-
matic roles was declared. These classes resulted
from the analysis of 1,555 verbs from the Span-
ish corpus and 1,077 from the Catalan. The anno-
tation process was performed in two steps: firstly,
we annotated automatically the unambiguous cor-
respondences between syntactic functions and the-
matic roles (Mart?? et al, 2007a); secondly, we man-
ually checked the outcome of the previous process
and completed the rest of thematic role assignments.
2.5 Subset for SemEval-2007
The corpora extracted from CESS-ECE to conform
SemEval-2007 datasets are: (a) SemEval-CESS-
ESP (Spanish), made of 101,136 words (3,611 sen-
tences), with 29% of the corpus coming from the
Spanish EFE News Agency and 71% coming from
Lexesp, a Spanish balanced corpus; (b) SemEval-
CESS-CAT (Catalan), consisting of 108,207 words
(3,202 sentences), with 71% of the corpus consistinf
of Catalan news from EFE News Agency and 29%
coming from the Catalan News Agency (ACN).
These corpora were split into training and test
subsets following a a 90%?10% proportion. Each
43
test set was also partitioned into two subsets: ?in-
domain? and ?out-of-domain? test corpora. The first
is intended to be homogeneous with respect to the
training corpus and the second was extracted from
a part of the CESS-ECE corpus annotated later and
not involved in the development of the resources
(e.g., verbal dictionaries).2
3 Task setting
Data formats are similar to those of CoNLL-
2004/2005 shared tasks on SRL (column style pre-
sentation of levels of annotation), in order to be
able to share evaluation tools and already developed
scripts for format conversion.
In Figure 1 you can find an example of a fully an-
notated sentence in the column-based format. There
is one line for each token, and a blank line after the
last token of each sentence. The columns, separated
by blank spaces, represent different annotations of
the sentence with a tagging along words. For struc-
tured annotations (parse trees, named entities, and
arguments), we use the Start-End format. Columns
1?6 correspond to the input information; columns 7
and above contain the information to be predicted.
We can group annotations in five main categories:
BASIC INPUT INFO (columns 1?3). The basic input
information, including: (a) WORD (column 1) words
of the sentence; (b) TN (column 2) target nouns of
the sentence, marked with ?*? (those that are to be
assigned WordNet synsets); (c) TV (column 3) target
verbs of the sentence, marked with ?*? (those that are
to be annotated with semantic roles).
EXTRA INPUT INFO (columns 4?6). The extra input
information, including: (a) LEMMA (column 4) lem-
mas of the words; (b) POS (column 5) part-of-speech
tags; (c) SYNTAX (column 6) Full syntactic tree.
NE (column 7). Named Entities.
NS (column 8). WordNet sense of target nouns.
SR (columns 9 and above). Information on semantic
roles, including: (a) SC (column 9). Semantic class
of the verb; (b) PROPS (columns 10 and above). For
each target verb, a column representing the argu-
ment structure. Core numbered arguments include
2For historical reasons we referred to these splits as ?3LB?
and ?CESS-ECE?, respectively. Participants in the task are ac-
tually using these names, but we opted for using a more simple
notation in this paper (see Section 5).
the thematic role labels. ArgM?s are the adjuncts.
Columns are ordered according to the textual order
of the predicates.
All these annotations in column format are ex-
tracted automatically from the syntactic-semantic
trees from the CESS-ECE corpora, which were dis-
tributed with the datasets. Participants were also
provided with the whole Catalan and Spanish Word-
Nets (v1.6), the verbal lexicons used in the role la-
beling annotation, the annotation guidelines as well
as the annotated corpora.
4 Participant systems
About a dozen teams expressed their interest in the
task. From those, only 5 registered and downloaded
datasets, and finally, only two teams met the dead-
line and submitted results. ILK2 (Tilburg Univer-
sity) presented a system addressing Semantic Role
Labeling, and UPC* (Technical University of Cat-
alonia) presented a system addressing all subtasks
independently3 . The ILK2 SRL system is based
on memory-based classification of syntactic con-
stituents using a rich feature set. UPC* used several
machine learning algorithms for addressing the dif-
ferent subtasks (AdaBoost, SVM, Perceptron). For
SRL, the system implements a re-ranking strategy
using global features. The candidates are generated
using a state?of?the?art SRL base system.
Although the task targeted at systems addressing
all subtasks jointly none of the participants did it.4
We believe that the high complexity of the whole
task together with the short period of time avail-
able were the main reasons for this failure. From
this point of view, the conclusions are somehow dis-
appointing. However, we think that we have con-
tributed with a very valuable resource for the future
research and, although not complete, the current sys-
tems provide also valuable insights about the task
and are very good baselines for the systems to come.
5 Evaluation
In the following subsections we present an analysis
of the results obtained by participant systems in the
3Some members of this team are also task organizers. This
is why we mark the team name with an asterisk.
4The UPC* team tried some inter-task features to improve
SRL but initial results were not successful.
44
INPUT--------------------------------------------------------------> OUTPUT-----------------------------------
BASIC_INPUT_INFO-----> EXTRA_INPUT_INFO---------------------------> NE NS-------> SR------------------------>
WORD TN TV LEMMA POS SYNTAX NE NS SC PROPS----------->
---------------------------------------------------------------------------------------------------------------
Las - - el da0fp0 (S(sn-SUJ(espec.fp*) * - - * (Arg1-TEM*
conclusiones * - conclusion ncfp000 (grup.nom.fp* * 05059980n - * *
de - - de sps00 (sp(prep*) * - - * *
la - - el da0fs0 (sn(espec.fs*) (ORG* - - * *
comision * - comision ncfs000 (grup.nom.fs* * 06172564n - * *
Zapatero - - Zapatero np00000 (grup.nom*) (PER*) - - * *
, - - , Fc (S.F.R* * - - * *
que - - que pr0cn00 (relatiu-SUJ*) * - - (Arg0-CAU*) *
ampliara - * ampliar vmif3s0 (gv*) * - a1 (V*) *
el - - el da0ms0 (sn-CD(espec.ms*) * - - (Arg1-PAT* *
plazo * - plazo ncms000 (grup.nom.ms* * 10935385n - * *
de - - de sps00 (sp(prep*) * - - * *
trabajo * - trabajo ncms000 (sn(grup.nom.ms*))))) * 00377835n - *) *
, - - , Fc *)))))) *) - - * *)
quedan - * quedar vmip3p0 (gv*) * - b3 * (V*)
para - - para sps00 (sp-CC(prep*) * - - * (ArgM-TMP*
despues_del - - despues_del spcms (sp(prep*) * - - * *
verano * - verano ncms000 (sn(grup.nom.ms*)))) * 10946199n - * *)
. - - . Fp *) * - - * *
Figure 1: An example of an annotated sentence.
three subtasks. Results on the test set are presented
along 2 dimensions: (a) language (?ca?=Catalan;
?es?=Spanish); (b) corpus source (?in?=in?domain
corpus; ?out?=out?of?domain corpus). We will use
a language.source pair to denote a particular test set.
Finally, ?*? will denote the addition of the two sub-
corpora, either in the language or source dimensions.
5.1 NSD
Results on the NSD subtask are presented in Table 1.
BSL stands for a baseline system consisting of as-
signing to each word occurrence the most frequent
sense in the training set. For new nouns the first
sense in the corresponding WordNet is selected. The
UPC* team trained a SVM classifier for each word in
a pre-selected subset and applied the baseline in the
rest of cases. The selected words are frequent words
(more than 15 occurrences in the training corpus)
showing a not too skewed distribution of senses in
the training set (the most predominant sense covers
less than 90% of the cases). No other teams pre-
sented results for this task.
All words Selected words
Test BSL UPC* BSL UPC*
ca.* 85.49% 86.47% 70.06% 72.75%
es.* 84.22% 85.10% 61.80% 65.17%
*.in 84.84% 86.49% 67.30% 72.24%
*.out 85.02% 85.33% 67.07% 67.87%
*.* 84.94% 85.87% 67.19% 70.12%
Table 1: Overall accuracy on the NSD subtask
The left part of the table (?all words?) contains
results on the complete test sets, while the right part
(?selected words?) contains the results restricted to
the set of words with trained SVM classifiers. This
set covers 31.0% of the word occurrences in the
training set and 28.2% in the complete test set.
The main observation is that training/test corpora
contain few sense variations. Sense distributions are
very skewed and, thus, the simple baseline shows a
very high accuracy (almost 85%). The UPC* system
only improves BSL accuracy by one point. This can
be partly explained by the small size of the word-
based training corpora. Also, this improvement is
diminished because UPC* only treated a subset of
words. However, looking at the right?hand side
of the table, the improvement over the baseline is
still modest (?3 points) when focusing only on the
treated words. As a final observation, no significant
differences are observed across languages and cor-
pora sources.
5.2 NER
Results on the NER subtask are presented in Table 2.
This time, BSL stands for a baseline system consist-
ing of collecting a gazetteer with the strong NEs ap-
pearing in the training set and assigning the longest
matches of these NEs in the test set. Weak entities
are simply ignored by BSL. UPC* presented a system
which treats strong and weak NEs in a pipeline of
two processors. Classifiers trained with multiclass
45
AdaBoost are used to predict the strong and weak
NEs. See authors? paper for details.
BSL UPC*
Test Prec. Recall F1 Prec. Recall F1
ca.* 75.85 15.45 25.68 80.94 77.96 79.42
es.* 71.88 12.07 20.66 70.65 65.69 68.08
*.in 83.06 17.43 28.82 78.21 74.04 76.09
*.out 68.63 12.20 20.72 76.21 72.51 74.31
*.* 74.45 14.11 23.72 76.93 73.08 74.96
Table 2: Overall results on the NER subtask
UPC* system largely overcomes the baseline,
mainly due to the low recall of the latter. By lan-
guages, results on Catalan are significantly better
than those on Spanish. We think this is attributable
mainly to corpora variations across languages. By
corpus source, ?in-domain? results are slightly bet-
ter, but the difference is small (1.78 points). Overall,
the results for the NER task are in the mid seventies,
a remarkable result given the small training set and
the complexity of predicting embedded NEs.
Detailed results on concrete entity types are pre-
sented in Table 3 (sorted by decreasing F1). As ex-
pected, DAT and NUM are the easiest entities to rec-
ognize since they can be easily detected by simple
patterns and POS tags. On the contrary, entity types
requiring more semantic information present fairly
lower results. ORG PER and LOC are in the sev-
enties, while OTH is by far the most difficult class,
showing a very low recall. This is not surprising
since OTH agglutinates a wide variety of entity cases
which are difficult to characterize as a whole.
Prec. Recall F1
DAT 97.38% 96.88% 97.13
NUM 98.05% 89.68% 93.68
ORG 75.72% 75.36% 75.54
PER 70.48% 75.97% 73.13
LOC 73.41% 68.29% 70.76
OTH 56.90% 37.79% 45.41
Table 3: Detailed results on the NER subtask: UPC*
team; Test corpus *.*
Another interesting analysis is to study the differ-
ences between strong and weak entities (see Table
4) . Contrary to our first expectations, results on
weak entities are much better (up to 11 F1 points
higher). Weak NEs are simpler for two reasons: (a)
there exist simple patters to characterize them, with-
out the need of fully recognizing their internal strong
NEs; (b) there is some redundancy in the corpus
when tagging many equivalent weak NEs in embed-
ded noun phrases. It is worth noting that the low re-
sults for strong NEs come from classification rather
than recognition (recognition is almost 100% given
the ?proper noun? PoS tag), thus the recall for weak
entities is not diminished by the errors in strong en-
tity classification.
Prec. Recall F1
Strong NEs 73.04% 63.36% 67.85
Weak NEs 78.96% 78.91% 78.93
Table 4: Results on strong vs. weak named entities:
UPC* team; Test corpus *.*
5.3 SRL
SRL is the most complex and interesting problem in
the task. We had two participants ILK2 and UPC*,
which participated in both subproblems, i.e., label-
ing arguments of verbal predicates with thematic
roles (SR), and assigning semantic class labels to
target verbs (SC). Detailed results of the two sys-
tems are presented in Tables 5 and 6.
UPC* ILK2
Test Prec. Recall F1 Prec. Recall F1
ca.* 84.49 77.97 81.10 84.72 82.12 83.40
es.* 83.88 78.49 81.10 84.30 83.98 84.14
*.in 84.17 82.90 83.53 84.71 84.12 84.41
*.out 84.19 72.77 78.06 84.26 81.84 83.03
*.* 84.18 78.24 81.10 84.50 83.07 83.78
Table 5: Overall results on the SRL subtask: seman-
tic role labeling (SR)
The ILK2 system outperforms UPC* in both SR
and SC. For SR, both systems use a traditional ar-
chitecture of labeling syntactic tree nodes with the-
matic roles using supervised classifiers. We would
attribute the overall F1 difference (2.68 points) to
a better feature engineering by ILK2, rather than
to differences in the Machine Learning techniques
used. Overall results in the eighties are remarkably
high given the training set size and the granularity
of the thematic roles (though we have to take into
account that systems work with gold parse trees).
Again, the results are comparable across languages
and slightly better in the ?in-domain? test set.
46
UPC* ILK2
Test Prec. Recall F1 Prec. Recall F1
ca.* 86.57 86.57 86.57 90.25 88.50 89.37
es.* 81.05 81.05 81.05 84.30 83.63 83.83
*.in 81.17 81.17 81.17 84.68 83.11 83.89
*.out 86.72 86.72 86.72 90.04 89.08 89.56
*.* 83.86 83.86 83.86 87.12 85.81 86.46
Table 6: Overall results on the SRL subtask: seman-
tic class tagging (SC)
In the SC subproblem, the differences are simi-
lar (2.60 points). In this case, ILK2 trained special-
ized classifiers for the task, while UPC* used heuris-
tics based on the SR outcomes. As a reference,
the baseline consisting of tagging each verb with
its most frequent semantic class achieves F1 values
of 64.01, 63.97, 41.00, and 57.42 on ca.in, ca.out,
es.in, es.out, respectively. Now, the results are sig-
nificantly better in Catalan, and, surprisingly, the
?out? test corpora makes F1 to raise. The latter is an
anomalous situation provoked by the ?es.in? tset.5
Table 7 shows the global SR results by numbered
arguments and adjuncts Interestingly, tagging ad-
juncts is far more difficult than tagging core argu-
ments (this result was also observed for English in
previous works). Moreover, the global difference
between ILK2 and UPC* systems is explained by
their ability to tag adjuncts (70.22 vs. 58.37). In
the core arguments both systems are tied. Also in
the same table we can see the overall results on a
simplified SR setting, in which the thematic roles are
eliminated from the SR labels keeping only the argu-
ment number (like other evaluations on PropBank).
The results are only ?2 points higher in this setting.
UPC* ILK2
Test Prec. Recall F1 Prec. Recall F1
Arg 90.41 87.73 89.05 89.42 88.58 88.99
Adj 64.72 53.16 58.37 72.54 68.04 70.22
A-TR 92.91 90.15 91.51 91.31 90.45 90.88
Table 7: Global results on numbered arguments
(Arg), adjuncts (Adj), and numbered arguments
without thematic role tag (A-TR). Test corpus *.*
Finally, Table 8 compares overall SR results on
known vs. new predicates. As expected, the re-
5By chance, the genre of this part of corpus is mainly liter-
ary. We are currently studying how this is affecting performance
results on all subtasks and, particularly, semantic class tagging.
sults on the verbs not appearing in the training set
are lower, but the performance decrease is not dra-
matic (3?6 F1 points) indicating that generalization
to new predicates is fairly good.
UPC* ILK2
Test Prec. Recall F1 Prec. Recall F1
Known 84.39 78.43 81.30 84.88 83.46 84.16
New 81.31 75.56 78.33 79.34 77.81 78.57
Table 8: Global results on semantic role labeling for
known versus new predicates. Test corpus *.*
Acknowledgements The organizers would like to
thank the following people for their hard work on
the corpora used in the task: Juan Aparicio, Manu
Bertran, Oriol Borrega, Nu?ria Buf??, Joan Castellv??,
Maria Jesu?s D??az, Marina Lloberes, Difda Mon-
terde, Aina Peris, Lourdes Puiggro?s, Marta Re-
casens, Santi Reig, and Ba`rbara Soriano. This re-
search has been partially funded by the Spanish
government: Lang2World (TIN2006-15265-C06-
06) and CESS-ECE (HUM-2004-21127-E) projects.
References
Are?valo, M., M. Civit and M. A. Mart??. 2004. MICE: a Module
for Named-Entities Recognition and Classification. Interna-
tional Journal of Corpus Linguistics, 9(1). John Benjamins,
Amsterdam.
Borrega, O., M. Taule?, M. A. Mart??. 2007. What do we mean
when we speak about Named Entities? In Proceedings of
Corpus Linguistics (forthcoming). Birmingham, UK.
Civit, M., A. Ageno, B. Navarro, N. Buf?? and M. A. Mart??.
2003. Qualitative and Quantitative Analysis of Annotatotrs:
Agreement in the Development of Cast3LB. In Proceed-
ings of 2nd Workshop on Treebanks and Linguistics Theories
(TLT-2003), 33?45. Vaxjo, Sweden.
Ma`rquez, L., M. Taule?, L. Padro?, L. Villarejo and M. A. Mart??.
2004. On the Quality of Lexical Resources for Word Sense
Disambiguation. In Proceedings of the 4th EsTAL Confer-
ence, Advances in natural Language Processing, LNCS, vol.
3230, 209?221. Alicante, Spain.
Mart??, M. A., M. Taule?, L. Ma`rquez, and M. Bertran. 2007a.
Anotacio?n semiautoma?tica con papeles tema?ticos de los cor-
pus CESS-ECE. In Revista de la SEPLN - Monograf??a TIMM
(forthcoming).
Mart??, M. A., M. Taule?, L. Ma`rquez, and M. Bertran. 2007b.
CESS-ECE: A multilingual and Multilevel Annotated Cor-
pus. E-pub., http://www.lsi.upc.edu/?mbertran/cess-ece
Taule?, M., J. Castellv?? and M. A. Mart??. 2006. Semantic
Classes in CESS-LEX: Semantic Annotation of CESS-ECE.
In Proceedings of the Fifth Workshop on Treebanks and Lin-
guistic Theories (TLT-2006). Prague, Czech Republic.
47
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354?357,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UPC: Sequential SRL Using Selectional Preferences.
An aproach with Maximum Entropy Markov Models
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical University of Catalonia
Barcelona, Catalonia
lluism@lsi.upc.edu
Abstract
We present a sequential Semantic Role La-
beling system that describes the tagging
problem as a Maximum Entropy Markov
Model. The system uses full syntactic in-
formation to select BIO-tokens from input
data, and classifies them sequentially us-
ing state-of-the-art features, with the addi-
tion of Selectional Preference features. The
system presented achieves competitive per-
formance in the CoNLL-2005 shared task
dataset and it ranks first in the SRL subtask
of the Semeval-2007 task 17.
1 Introduction
In Semantic Role Labeling (SRL) the goal is to iden-
tify word sequences or arguments accompanying the
predicate and assign them labels depending on their
semantic relation. In this task we disambiguate ar-
gument structures in two ways: predicting VerbNet
(Kipper et al, 2000) thematic roles and PropBank
(Palmer et al, 2005) numbered arguments, as well
as adjunct arguments.
In this paper we describe our system for the SRL
subtask of the Semeval2007 task 17. It is based on
the architecture and features of the system named
?model 2? of (Surdeanu et al, forthcoming), but it
introduces two changes: we use Maximum Entropy
for learning instead of AdaBoost and we enlarge the
feature set with combined features and other seman-
tic features.
Traditionally, most of the features used in SRL
are extracted from automatically generated syntac-
tic and lexical annotations. In this task, we also ex-
periment with provided hand labeled semantic infor-
mation for each verb occurrence such as the Prop-
Bank predicate sense and the Levin class. In addi-
tion, we use automatically learnt Selectional Prefer-
ences based on WordNet to generate a new kind of
semantic based features.
We participated in both the ?close? and the ?open?
tracks of Semeval2007 with the same system, mak-
ing use, in the second case, of the larger CoNLL-
2005 training set.
2 System Description
2.1 Data Representation
In order to make learning and labeling easier, we
change the input data representation by navigating
through provided syntactic structures and by extract-
ing BIO-tokens from each of the propositions to be
annotated as shown in (Surdeanu et al, forthcom-
ing). These sequential tokens are selected by ex-
ploring the sentence spans or regions defined by the
clause boundaries, and they are labeled with BIO
tags depending on the location of the token: at the
beginning, inside, or outside of a verb argument. Af-
ter this data pre-processing step, we obtain a more
compact and easier to process data representation,
making also impossible overlapping and embedded
argument predictions.
2.2 Feature Representation
Apart from Selectional Preferences (cf. Section 3)
and those extracted from provided semantic infor-
mation, most of the features we used are borrowed
from the existing literature (Gildea and Jurafsky,
2002; Xue and Palmer, 2004; Surdeanu et al, forth-
coming).
354
On the verb predicate:
? Form; Lemma; POS tag; Chunk type and Type
of verb phrase; Verb voice; Binary flag indicat-
ing if the verb is a start/end of a clause.
? Subcategorization, i.e., the phrase structure rule
expanding the verb parent node.
? VerbNet class of the verb (in the ?close? track
only).
On the focus constituent:
? Type; Head;
? First and last words and POS tags of the con-
stituent.
? POS sequence.
? Bag-of-words of nouns, adjectives, and adverbs
in the constituent.
? TOP sequence: right-hand side of the rule ex-
panding the constituent node; 2/3/4-grams of
the TOP sequence.
? Governing category as described in (Gildea
and Jurafsky, 2002).
Context of the focus constituent:
? Previous and following words and POS tags of
the constituent.
? The same features characterizing focus con-
stituents are extracted for the two previous and
following tokens, provided they are inside the
clause boundaries of the codified region.
Relation between predicate and constituent:
? Relative position; Distance in words and
chunks; Level of embedding with respect to the
constituent: in number of clauses.
? Binary position; if the argument is after or be-
fore the predicate.
? Constituent path as described in (Gildea and
Jurafsky, 2002); All 3/4/5-grams of path con-
stituents beginning at the verb predicate or end-
ing at the constituent.
? Partial parsing path as described in (Carreras
et al, 2004)); All 3/4/5-grams of path elements
beginning at the verb predicate or ending at the
constituent.
? Syntactic frame as described by Xue and
Palmer (2004)
Combination Features
? Predicate and Phrase Type
? Predicate and binary position
? Head Word and Predicate
? Predicate and PropBank frame sense
? Predicate, PropBank frame sense, VerbNet
class (in the ?close? track only)
2.3 Maximum Entropy Markov Models
Maximum Entropy Markov Models are a discrimi-
native model for sequential tagging that models the
local probability P (sn | sn?1, o), where o is the
context of the observation.
Given a MEMM, the most likely state sequence is
the one that maximizes the following
S = argmax
n?
i=1
P (si | si?1, o)
Translating the problem to SRL, we have
role/argument labels connected to each state in the
sequence (or proposition), and the observations are
the features extracted in these points (token fea-
tures). We get the most likely label sequence finding
out the most likely state sequence (Viterbi).
All the conditional probabilities are given by the
Maximum Entropy classifier with a tunable Gaus-
sian prior from the Mallet Toolkit1.
Some restrictions are considered when we search
the most likely sequence2:
1. No duplicate argument classes for A0-A5 and
thematic roles.
2. If there is a R-X argument (reference), then
there has to be a X argument before (refer-
enced).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token (because of the BIO encoding).
5. Given a predicate and its PropBank sense, only
some arguments are allowed (e.g. not all the
verbs support A2 argument).
6. Given a predicate and its Verbnet class, only
some thematic roles are allowed.
3 Including Selectional Preferences
Selectional Preferences (SP) try to capture the fact
that linguistic elements prefer arguments of a cer-
tain semantic class, e.g. a verb like ?eat? prefers as
subject edible things, and as subject animate entities,
as in ?She was eating an apple? They can be learned
from corpora, generalizing from the observed argu-
ment heads (e.g. ?apple?, ?biscuit?, etc.) into ab-
stract classes (e.g. edible things). In our case we
1http://mallet.cs.umass.edu
2Restriction 5 applies to PropBank output. Restriction 6 ap-
plies to VerbNet output
355
follow (Agirre and Martinez, 2001) and use Word-
Net (Fellbaum, 1998) as the generalization classes
(the concept <food,nutrient>).
The aim of using Selectional Preferences (SP) in
SRL is to generalize from the argument heads in
the training instances into general word classes. In
theory, using word classes might overcome the data
sparseness problem for the head-based features, but
at the cost of introducing some noise.
More specifically, given a verb, we study the oc-
currences of the target verb in a training corpus (e.g.
the PropBank corpus), and learn a set of SPs for
each argument and adjunct of that verb. For in-
stance, given the verb ?kill? we would have 2 SPs
for each argument type, and 4 SPs for some of the
observed adjuncts: kill A0, kill A1, kill AM-
LOC, kill AM-MNR, kill AM-PNC and kill AM-
TMP.
Rather than coding the SPs directly as features,
we code the predictions instead, i.e. for each propo-
sition in the training and testing set, we check the
SPs for all the argument (and adjunct) headwords,
and the SP which best fits the headword (see below)
is the one that is selected. We codify the predicted
argument (or adjunct) label as features, and we insert
them among the corresponding argument features.
For instance, let?s assume that the word ?railway?
appears as the headword of a candidate argument of
?kill?. WordNet 1.6 yields the following hypernyms
for ?railway? (from most general to most specific, we
include the WordNet 1.6 concept numbers preceded
by their specifity level);
1 00001740 1 00017954
2 00009457 2 05962976
3 00011937 3 05997592
4 03600463 4 06004580
5 03243979 5 06008236
6 03526208 6 06005839
7 03208595 7 02927599
8 03209020
Note that we do not care about the sense ambigu-
ity and the explosion of concepts that it carries. Our
algorithm will check each of the hypernyms of rail-
way and match them with the concepts in the SPs of
?kill?, giving preference to the most specific concept.
In case that equally specific concepts match different
SPs, we will choose the SP that has the concept that
ranks highest in the SP, and code the SP feature with
the label of the SP where the match succeeds. In the
example, these are the most specific matches:
AM-LOC Con:03243979 Level:5 Ranking:32
A0 Con:06008236 Level:5 Ranking:209
There is a tie in the level, so we choose the one
with the highest rank. All in all, this means that ac-
cording to the learnt SPs we would predict that ?rail-
way? is a location feature for ?kill?, and we would
therefore insert the ?SP:AM-LOC? feature among
the argument features.
If ?railway? appears as the headword of other
verbs, the predicted argument might be different.
See for instance, the following verbs:
destroy:A1 Con:03243979 Level:5 Ranking:43
go:A0 Con:02927599 Level:7 Ranking:131
go:A2 Con:02927599 Level:7 Ranking:721
build:A1 Con:03209020 Level:8 Ranking:294
Note that our training examples did not contain
?railway? as an argument of any of these verbs, but
due to the SPs we are able to code into a feature that
?railway? belongs to a concrete semantic class which
contains conceptually similar headwords.
We decided to code the prediction of the SPs,
rather than the SPs themselves, in order to be more
robust to noise.
There is a further subtlety with our SP system. In
order to label training and testing sets in similar con-
ditions and avoid overfitting problems as much as
possible, we split the training set into five folds and
tagged each one with SPs learnt from the other four.
For extracting SP features from test set examples,
we use SPs learnt in the whole training set.
4 Experiments and Results
We participated in the ?close? and the ?open? tracks
with the same classification model, but using dif-
ferent training sets in each one. In the close track
we only use the provided training set, and in the
open, the CoNLL-2005 training set (without Verb-
Net classes or thematic roles).
Before our participation, we tested the system in
the CoNLL-2005 close track setting and it achieved
competitive performance in comparison to the state-
of-the-art results published in that challenge.
4.1 Semeval2007 setting
The data provided in the close track consists of the
propositions of 50 different verb lemmas from Prop-
Bank (sections 02-21). The data for the CoNLL-
2005 is also a subset of the PropBank data, but it
356
Track Label rank prec. rec. F1
Close VerbNet 1st 85.31 82.08 83.66
Close PropBank 1st 85.04 82.07 83.52
Open PropBank 1st 84.51 82.24 83.36
Table 1: Results in the SRL subtask of SemEval-
2007 task 17
includes all the propositions in sections 02-21 and
no VerbNet classes nor thematic roles for learning.
There is a total of 21 argument types for Prop-
Bank and 47 roles for VerbNet, which amounts to
21 ? 2 + 1 = 43 BIO-labels for PropBank predic-
tions and 47 ? 2 + 1 = 95 for VerbNet. We filtered
the less frequent (<5).
We trained the Maximum Entropy classifiers with
114,380 examples for the close track, and with
828,811 for the open track. We tuned the classifier
by setting the Exponential Gaussian prior in 0.1
4.2 Results
In the close track we trained two classifiers, one
to label PropBank numbered arguments and a sec-
ond to label VerbNet thematic roles. Due to lack
of time, we only trained the PropBank labels in the
open track. Table 1 shows the results obtained in the
SRL subtask. We ranked first in all of them, out of
two participants.
4.3 Discussion
The results indicate that in the close track the system
performs similarly on both PropBank arguments and
VerbNet roles. The absence of VerbNet class-based
features in the CoNLL-2005 training data could
cause the loss of performance in the open track. We
plan to perform the experiment on VerbNet roles for
the open track to check the ability of the classifier to
generalize across verbs.
Regarding the use of SP features, nowadays, we
have not obtained relevant improvements in the pre-
dictions of the classifiers. It is our first approach to
these kind of semantic features and there are more
sophisticated but evident extraction variants which
we are exploring.
Although the general performance is very simi-
lar without SP features, using them our system ob-
tains better results in ARG3 core arguments and in
the most frequent adjuncts such as location (LOC),
general-purpose (ADV) and temporal (TMP).
We reproduced this improvements in experiments
realized with CoNLL-2005 larger test sets. In that
case, we improved ARG3-ARG4 core arguments as
well as the mentioned adjuncts. There were more
examples to be classified and we get better overall
performance, but we need further experiments to be
more conclusive.
5 Conclusions
We have presented a sequential semantic role la-
beling system for the Semeval-2007 task 17 (SRL).
Based on Maximum Entropy Markov Models, it ob-
tains competitive and promising results. We also
have introduced semantic features extracted from
Selectional Restrictions but we only have prelimi-
nary evidence of their usefulness.
Acknowledgements
We thank David Martinez for kindly providing the
software that learnt the selectional preferences. This
work has been partially funded by the Spanish ed-
ucation ministry (KNOW). Ben?at is supported by a
PhD grant from the University of the Basque Coun-
try.
References
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL-
2001, Toulouse, France.
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hi-
erarchical recognition of propositional arguments with
perceptrons. In Proceedings of CoNLL 2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics , 28(3).
K. Kipper, Hoa Trang Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon. In Pro-
ceedings of AAAI-2000 Seventeenth National Confer-
ence on Artificial Intellingence, Austin, TX .
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics , 31(1).
M. Surdeanu, L. Ma`rquez, X. Carreras, and P. Comas.
(forthcoming). Combination strategies for semantic
role labeling. In Journal of Artificial Intelligence Re-
search.
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings of EMNLP-2004 .
357
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 426?429,
Prague, June 2007. c?2007 Association for Computational Linguistics
UPC: Experiments with Joint Learning within SemEval Task 9
Llu??s Ma`rquez, Llu??s Padro?, Mihai Surdeanu, Luis Villarejo
Technical University of Catalonia
{lluism,padro,surdeanu,luisv}@lsi.upc.edu
1 Introduction
This paper describes UPC?s participation in the
SemEval-2007 task 9 (Ma`rquez et al, 2007).1 We
addressed all four subtasks using supervised learn-
ing. The paper introduces several novel issues:
(a) for the SRL task, we propose a novel re-
ranking algorithm based on the re-ranking Percep-
tron of Collins and Duffy (2002); and (b) for the
same task we introduce a new set of global features
that extract information not only at proposition level
but also from the complete set of frame candidates.
We show that in the SemEval setting, i.e., small
training corpora, this approach outperforms previ-
ous work. Additionally, we added NSD and NER
information in the global SRL model but this exper-
iment was unsuccessful.
2 Named Entity Recognition
For the NER subtask we recognize first strong NEs,
followed by weak NE identification. Any single to-
ken with the np0000, W, or Z PoS tag is consid-
ered a strong entity and is classified using the (At-
serias et al, 2006) implementation of a multi-label
AdaBoost.MH algorithm, with a configuration sim-
ilar to the NE classification module of Carreras et
al. (2003). The classifier yields predictions for four
classes (person, location, organization, misc). En-
tities with NUM and DAT are detected separately
solely based on POS tags.
The features used by the strong NE classifier
model a [-3,+3] context around the focus word, and
include bag-of-words, positional lexical features,
1Two of the authors of this paper, Llu??s Ma`rquez and Luis
Villarejo, are organizers of the SemEval-2007 task 9.
PoS tags, orthographic features, as well as features
indicating whether the focus word, some of its com-
ponents, or some word in the context are included in
external gazetteers or trigger words files.
The second step starts by selecting all noun
phrases (np) that cover a span of more than one to-
ken and include a strong NE as weak entity candi-
dates. This strategy covers more than 95% of the
weak NEs. A second AdaBoost.MH classifier is
then applied to decide the right class for the noun
phrase among the possible six (person, location, or-
ganization, misc, number, date) plus a NONE class
indicating that the noun phrase is not a weak NE.
The features used for weak NE classification are:
(1) simple features ? length in tokens, head word,
lemma, and POS of the np, syntactic function of the
np (if any), minimum and maximum number of np
nodes in the path from the candidate noun phrase to
any of the strong NEs included in it, and number and
type of the strong NEs predicted by the first?level
classifier that fall inside the candidate; (2) bag of
content words inside the candidate; and (3) pattern-
based features, consisting in codifying the sequence
of lexical tokens spanned by the candidate according
to some generalizations. When matching, tokens are
generalized to: the POS tag (in case of np0000,
W, Z, and punctuation marks), trigger-word of class
X, word-in-gazetteer of class X, and strong-NE of
type X, predicted by the first level classifier. The
rest of words are abstracted to a common form (?w?
standing for a single word and ?w+? standing for a
sequence of n > 1 words). Beginning and end of the
span are also codified explicitly in the pattern?based
features. Finally, to avoid sparsity, only paths of up
426
to length 6 are codified as features. Also, for each
path, n?grams of length 2, 3 and 4 are considered.
We filter out features that occur less than 10 times.
3 Noun Sense Disambiguation
We have approached the NSD subtask using su-
pervised learning. In particular, we used SVMlight
(Joachims, 1999), which is a freely available imple-
mentation of Support Vector Machines (SVM).
We trained binary SVM classifiers for every sense
of words with more than 15 examples in the training
set and a probability distribution over its senses in
which no sense is above 90%. The words not cov-
ered by the SVM classifiers are disambiguated using
the most frequent sense (MFS) heuristic. The MFS
was calculated from the relative frequencies in the
training corpus. To the words that do not appear in
the training corpus we assigned the first WordNet
sense.
We used a fairly regular set of features from the
WSD literature. We included: (1) a bag of con-
tent words appearing in a ?10-word window; (2) a
bag of content words appearing in the clause of the
target word; (3) {1, . . . , n}?grams of POS tags and
lemmas in a ?n-word window (n is 3 for POS and
2 for lemmas); (4) unigrams and bigrams of (POS-
tag,lemma) pairs in a?2-word window; and (5) syn-
tactic features, i.e., label of the syntactic constituent
from which the target noun is the head, syntactic
function of that constituent (if any), and the verb.
Regarding the empirical setting, we filtered out
features occurring less than 3 times, we used linear
SVMs with a 0.5 value for the C regularization pa-
rameter (trade-off between training error and mar-
gin), and we applied one-vs-all binarization.
4 Semantic Role Labeling
The SRL approach deployed here implements a re-
ranking strategy that selects the best argument frame
for each predicate from the top N frames generated
by a base model. We describe the two models next.
4.1 The Local Model
The local (i.e., base) model is an adaption of Model
3 of Ma`rquez et al (2005). This SRL approach
maps each frame argument to one syntactic con-
stituent and trains one-vs-all AdaBoost (Schapire
and Singer, 1999) classifiers to jointly identify and
classify constituents in the full syntactic tree of the
sentence as arguments. The model was adapted to
the languages and corpora used in the SemEval eval-
uations by removing the features that were specific
either to English or PropBank (governing category,
content word, and temporal cue words) and adding
several new features: (a) syntactic function features
? the syntactic functions available in the data often
point to specific argument labels (e.g., SUJ usually
indicates an Arg0); and (b) back-off features for
syntactic labels and POS tags ? for the features that
include POS tags or syntactic labels we add a back-
off version of the feature where the POS tags and
syntactic labels are reduced to a small set.
In addition to feature changes we modified the
candidate filtering heuristic: we select as candidates
only syntactic constituents that are immediate de-
scendents of S phrases that include the correspond-
ing predicate (for both languages, over 99.6% of the
candidates match this constraint).
4.2 The Global Model
We base our re-ranking approach on a variant of the
re-ranking Perceptron of Collins and Duffy (2002).
We modify the original algorithm in two ways to
make it more robust to the small training set avail-
able: (a) instead of comparing the score of the cor-
rect frame only with that of the best candidate for
each frame, we sequentially compare it with the
score of each candidate in order to acquire more in-
formation, and (b) we learn not only when the pre-
diction is incorrect but also when the prediction is
not confident enough.
The algorithm is listed in Algorithm 1: w is the
vector of model parameters, h generates the feature
vector for one example, and xij denotes the jth can-
didate for the ith frame in the training data. xi1,
which denotes the ?correct? candidate for frame i, is
selected to maximize the F1 score for each frame.
The algorithm sequentially inspects all candidates
for each frame and learns when the difference be-
tween the scores of the correct and the current candi-
date is less than a threshold ? . During testing we use
the average of all acquired model vectors, weighted
by the number of iterations they survived in train-
ing. We tuned all system parameters through cross-
validation on the training data. For both languages
we set ? = 10 (we do not normalize feature vectors)
427
Algorithm 1: Re-ranking Perceptron
w = ~0
for i = 1 to n do
for j = 2 to ni do
if w ? h(xij) > w ? h(xi1)? ? then
w? w + h(xi1)? h(xij)
and the number of training epochs to 2.
With respect to the features used, we focus only
on global features that can be extracted indepen-
dently of the local models. We show in Section 6
that this approach performs better on the small
SemEval corpora than approaches that include fea-
tures from the local models. We group the features
into two sets: (a) features that extract information
from the whole candidate set, and (b) features that
model the structure of each candidate frame:
Features from the whole candidate set:
(1) Position of the current candidate in the whole set.
Frame candidates are generated using the dynamic
programming algorithm of Toutanova et al (2005),
and then sorted in descending order of the log prob-
ability of the whole frame (i.e., the sum of all ar-
gument log probabilities as reported by the local
model). Hence, smaller positions indicate candi-
dates that the local model considers better.
(2) For each argument in the current frame, we store
its number of repetitions in the whole candidate set.
The intuition is that an argument that appears in
many candidate frames is most likely correct.
Features from each candidate frame:
(3) The complete sequence of argument labels, ex-
tended with the predicate lemma and voice, similar
to Toutanova et al (2005).
(4) Maximal overlap with a frame from the verb lex-
icon. Both the Spanish and Catalan TreeBanks con-
tain a static lexicon that lists the accepted sequences
of arguments for the most common verbs. For each
candidate frame, we measure the maximal overlap
with the lexicon frames for the given verb and use
the precision, recall, and F1 scores as features.
(5) Average probability (from the local model) of all
arguments in the current frame.
(6) For each argument label that repeats in the cur-
rent frame, we add combinations of the predicate
lemma, voice, argument label, and the number of
label repetitions as features. The intuition is that ar-
gument repetitions typically indicate an error (even
if allowed by the domain constraints).
5 Semantic Class Detection
The semantic class detection subtask has been per-
formed using a naive cascade of heuristics: (1) the
predicted frame for each verb is compared with the
frames present in the provided verbal lexicon, and
the class of the lexicon frame with the largest num-
ber of matching arguments is chosen; (2) if there is
more than one verb with the maximum score, the
first one in the lexicon (i.e., the most frequent) is
used; (3) if the focus verb is not found in the lexicon,
its most frequent class in the training corpus is used;
(4) if the verb does not appear in the training data,
the most frequent class overall (D2) is assigned. The
results obtained on the training corpus are 81.1% F1
for Spanish and 86.6% for Catalan. As a baseline,
assigning the most frequent class for each verb (or
D2 if not seen in training), yields F1 values of 48.1%
for Spanish and 64.0% for Catalan.
6 Results and Discussion
Table 1 lists the results of our system on the Se-
mEval test data. Our results are encouraging con-
sidering the size of the training corpus (e.g., the En-
glish PropBank is 10 times larger than the corpus
used here) and the complexity of the problem (e.g.,
the NER task includes both weak and strong entities;
the SRL task contains 33 core arguments for Span-
ish vs. 6 for English). We analyze the behavior of
our system next.
The first issue that deserves further analysis is the
contribution of our global SRL model. We list the
results of this analysis in Table 2 as improvements
over the local SRL model. We report results for 6
corpora: the 4 test corpora and the 2 training cor-
pora, where the results are generated through 5-fold
cross validation. The first block in the table shows
the contribution of our best re-ranking model. The
second block shows the results of a re-ranking model
using our best feature set but the original re-ranking
Perceptron of Collins and Duffy (2002). The third
block shows the performance of our re-ranking al-
gorithm configured with the features proposed by
Toutanova et al (2005). We draw several conclu-
sions from this experiment: (a) our re-ranking model
428
NER NSD SRL SC
P R F1 P R F1 P R F1 F1
ca.CESS-ECE 79.92% 76.63% 78.24 87.47% 87.47% 87.47 82.16% 70.05% 75.62 85.71
es.CESS-ECE 72.53% 68.48% 70.45 83.30% 83.30% 83.30 86.24% 75.58% 80.56 87.74
ca.3LB 82.04% 79.42% 80.71 85.69% 85.53% 85.61 86.36% 85.30% 85.83 87.35
es.3LB 62.03% 53.85% 57.65 88.14% 88.14% 88.14 82.23% 80.78% 81.50 76.01
Table 1: Official results on the test data. Due to space constraints, we show only the F1 score for SC.
Re-ranking Collins Toutanova
P R F1 P R F1 P R F1
ca.train +1.87 +1.79 +1.83 +1.56 +1.48 +1.52 -6.81 -6.67 -6.73
es.train +3.16 +3.12 +3.14 +2.96 +2.93 +2.95 -6.51 -6.96 -6.75
ca.CESS-ECE +0.77 +0.66 +0.71 +0.99 +0.84 +0.91 -8.11 -6.29 -7.10
es.CESS-ECE +1.85 +1.94 +1.91 +1.45 +1.85 +1.68 -10.84 -8.46 -9.54
ca.3LB +1.58 +1.47 +1.53 +1.48 +1.39 +1.44 -7.71 -7.57 -7.64
es.3LB +2.57 +2.83 +2.71 +2.71 +2.91 +2.82 -10.53 -11.95 -11.26
Table 2: Analysis of the re-ranking model for SRL.
using only global information always outperforms
the local model, with F1 score improvements rang-
ing from 0.71 to 3.14 points; (b) the re-ranking Per-
ceptron proposed here performs better than the orig-
inal algorithm, but the improvement is minimal; and
(c) the feature set proposed here achieve significant
better performance on the SemEval corpora than the
set proposed by Toutanova et al, which never im-
proves over the local model. The model configured
with the Toutanova et al feature set performs mod-
estly because the features are too sparse for the small
SemEval corpora (e.g., all features from the local
model are included, concatenated with the label of
the corresponding argument). On the other hand, we
replicate the behavior of the local model just with
feature (1), and furthermore, all the other 5 global
features proposed have a positive contribution.
In a second experiment we investigated simple
strategies for model combination. We incorporated
NER and NSD information in the re-ranking model
for SRL as follows: for each frame argument, we
add features that concatenate the predicate lemma,
the argument label, and the NER or NSD labels for
the argument head word (we add features both with
and without the predicate lemma). We used only the
best NER/NSD labels from the local models. To re-
duce sparsity, we converted word senses to coarser
classes based on the corresponding WordNet seman-
tic files. This new model boosts the F1 score of our
best re-ranking SRL model with an average of 0.13
points on two corpora (es.3LB and ca.CESS-ECE),
but it reduces the F1 of our best SRL model with an
average of 0.17 points on the other 4 corpora. We
can conclude that, in the current setting, NSD and
NER do not bring useful information to the SRL
problem. However, it is soon to state that problem
combination is not useful. To have a conclusive an-
swer one will have to investigate true joint learning
of the three subtasks.
References
J. Atserias, B. Casas, E. Comelles, M. Gonza`lez, L. Padro?, and
M. Padro?. 2006. Freeling 1.3: Syntactic and semantic ser-
vices in an open-source NLP library. In Proc. of LREC.
X. Carreras, L. Ma`rquez, and L. Padro?. 2003. A simple named
entity extractor using AdaBoost. In CoNLL 2003 Shared
Task Contribution.
M. Collins and N. Duffy. 2002. New ranking algorithms for
parsing and tagging: Kernels over discrete structures, and
the voted perceptron. In Proc. of ACL.
T. Joachims. 1999. Making large-scale SVM learning practi-
cal, Advances in Kernel Methods - Support Vector Learning.
MIT Press, Cambridge, MA.
L. Ma`rquez, M. Surdeanu, P. Comas, and J. Turmo. 2005. A
robust combination strategy for semantic role labeling. In
Proc. of EMNLP.
L. Ma`rquez, M.A. Mart??, M. Taule?, and L. Villarejo. 2007.
SemEval-2007 task 09: Multilevel semantic annotation of
Catalan and Spanish. In Proc. of SemEval-2007, the 4th
Workshop on Semantic Evaluations. Association for Com-
putational Linguistics.
R.E. Schapire and Y. Singer. 1999. Improved boosting algo-
rithms using confidence-rated predictions. Machine Learn-
ing, 37(3).
K. Toutanova, A. Haghighi, and C. Manning. 2005. Joint learn-
ing improves semantic role labeling. In Proc. of ACL.
429
Proceedings of the Third Workshop on Statistical Machine Translation, pages 195?198,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Smorgasbord of Features for Automatic MT Evaluation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
This document describes the approach by the
NLP Group at the Technical University of Cat-
alonia (UPC-LSI), for the shared task on Au-
tomatic Evaluation of Machine Translation at
the ACL 2008 Third SMT Workshop.
1 Introduction
Our proposal is based on a rich set of individual
metrics operating at different linguistic levels: lex-
ical (i.e., on word forms), shallow-syntactic (e.g., on
word lemmas, part-of-speech tags, and base phrase
chunks), syntactic (e.g., on dependency and con-
stituency trees), shallow-semantic (e.g., on named
entities and semantic roles), and semantic (e.g., on
discourse representations). Although from differ-
ent viewpoints, and based on different similarity as-
sumptions, in all cases, translation quality is mea-
sured by comparing automatic translations against
human references. Extensive details on the met-
ric set may be found in the IQMT technical manual
(Gime?nez, 2007).
Apart from individual metrics, we have also
applied a simple integration scheme based on
uniformly-averaged linear metric combinations
(Gime?nez and Ma`rquez, 2008a).
2 What is new?
The main novelty, with respect to the set of metrics
presented last year (Gime?nez and Ma`rquez, 2007),
is the incorporation of a novel family of metrics
at the properly semantic level. DR metrics ana-
lyze similarities between automatic and reference
translations by comparing their respective discourse
representation structures (DRS), as provided by the
the C&C Tools (Clark and Curran, 2004). DRS are
essentially a variation of first-order predicate calcu-
lus which can be seen as semantic trees. We use
three different kinds of metrics:
DR-STM Semantic Tree Matching, a la Liu and
Gildea (2005), but over DRS instead of over
constituency trees.
DR-Or-? Lexical overlapping over DRS.
DR-Orp-? Morphosyntactic overlapping on DRS.
Further details on DR metrics can be found in
(Gime?nez and Ma`rquez, 2008b).
2.1 Improved Sentence Level Behavior
Metrics based on deep linguistic analysis rely on
automatic processors trained on out-domain data,
which may be, thus, prone to error. Indeed, we found
out that in many cases, metrics are unable to pro-
duce a result due to the lack of linguistic analysis.
For instance, in our experiments, for SR metrics, we
found that the semantic role labeler was unable to
parse 14% of the sentences. In order to improve the
recall of these metrics, we have designed two simple
variants. Given a linguistic metric x, we define:
? xb ? by backing off to lexical overlapping,
Ol, only when the linguistic processor is not
able to produce a linguistic analysis. Other-
wise, x score is returned. Lexical scores are
conveniently scaled so that they are in a similar
range to scores of x. Specifically, we multiply
195
them by the average x score attained over all
other test cases for which the parser succeeded.
? xi ? by linearly interpolating x and Ol scores
for all test cases, via the arithmetic mean.
In both cases, system scores are calculated by av-
eraging over all sentence scores. Currently, these
variants are applied only to SR and DR metrics.
2.2 Uniform Linear Metric Combinations
We have simulated a non-parametric combination
scheme based on human acceptability by working
on uniformly averaged linear combinations (ULC)
of metrics (Gime?nez and Ma`rquez, 2008a). Our ap-
proach is similar to that of Liu and Gildea (2007)
except that in our case the contribution of each met-
ric to the overall score is not adjusted.
Optimal metric sets are determined by maximiz-
ing the correlation with human assessments, either
at the document or sentence level. However, because
exploring all possible combinations was not viable,
we have used a simple algorithm which performs an
approximate search. First, metrics are ranked ac-
cording to their individual quality. Then, following
that order, metrics are added to the optimal set only
if in doing so the global quality increases.
3 Experimental Work
We use all into-English test beds from the 2006
and 2007 editions of the SMT workshop (Koehn
and Monz, 2006; Callison-Burch et al, 2007).
These include the translation of three differ-
ent language-pairs: German-to-English (de-en),
Spanish-to-English (es-en), and French-to-English
(fr-en), over two different scenarios: in-domain (Eu-
ropean Parliament Proceedings) and out-of-domain
(News Commentary Corpus)1. In all cases, a single
reference translation is available. In addition, hu-
man assessments on adequacy and fluency are avail-
able for a subset of systems and sentences. Each
sentence has been evaluated at least by two different
judges. A brief numerical description of these test
beds is available in Table 1.
1We have not used the out-of-domain Czech-to-English test
bed from the 2007 shared task because it includes only 4 sys-
tems, and only 3 of them count on human assessments.
WMT 2006
in-domain out-of-domain
2,000 cases 1,064 cases
#snt #sys #snt #sys
de-en 2,281 10/12 1,444 10/12
es-en 1,852 11/15 1,008 11/15
fr-en 2,268 11/14 1,281 11/14
WMT 2007
in-domain out-of-domain
2,000 cases 2,007 cases
#snt #sys #snt #sys
de-en 956 7/8 947 5/6
es-en 812 8/10 675 7/9
fr-en 624 7/8 741 7/7
Table 1: Test bed description. ?#snt? columns show the
number of sentences assessed (considering all systems).
?#sys? columns shows the number of systems counting
on human assessments with respect to the total number
of systems which participated in each task.
Metrics are evaluated in terms of human accept-
ability, i.e., according to their ability to capture
the degree of acceptability to humans of automatic
translations. We measure human acceptability by
computing Pearson correlation coefficients between
automatic metric scores and human assessments of
translation quality both at document and sentence
level. We use the sum of adequacy and fluency to
simulate a global assessment of quality. Assess-
ments from different judges over the same test case
are averaged into a single score.
3.1 Individual Performance
In first place, we study the behavior of individual
metrics. Table 2 shows meta-evaluation results, over
into-English WMT 2007 test beds, in-domain and
out-of-domain, both at the system and sentence lev-
els, for a set of selected representatives from several
linguistic levels.
At the system level (columns 1-6), corroborating
previous findings by Gime?nez and Ma`rquez (2007),
highest levels of correlation are attained by met-
rics based on deep linguistic analysis (either syn-
tactic or semantic). In particular, two kinds of met-
rics, respectively based on head-word chain match-
ing over grammatical categories and relations (?DP-
196
System Level Sentence Level
de-en es-en fr-en de-en es-en fr-en
Level Metric in out in out in out in out in out in out
1-TER 0.64 0.41 0.83 0.58 0.72 0.47 0.43 0.29 0.23 0.23 0.29 0.20
BLEU 0.87 0.76 0.88 0.70 0.74 0.54 0.46 0.27 0.33 0.20 0.20 0.12
Lexical GTM (e = 2) 0.82 0.69 0.93 0.71 0.76 0.60 0.56 0.36 0.43 0.33 0.27 0.18
ROUGEW 0.87 0.91 0.96 0.78 0.85 0.83 0.58 0.40 0.43 0.35 0.30 0.31
METEORwn 0.83 0.92 0.96 0.74 0.91 0.86 0.53 0.41 0.35 0.28 0.33 0.32
Ol 0.79 0.75 0.91 0.55 0.81 0.66 0.48 0.33 0.35 0.30 0.30 0.21
CP-Oc-? 0.84 0.88 0.95 0.62 0.84 0.76 0.49 0.37 0.38 0.33 0.32 0.25
DP-HWCw-4 0.85 0.93 0.96 0.68 0.84 0.80 0.31 0.26 0.33 0.07 0.10 0.14
Syntactic DP-HWCc-4 0.91 0.98 0.96 0.90 0.98 0.95 0.30 0.25 0.23 0.06 0.13 0.12
DP-HWCr-4 0.89 0.97 0.97 0.92 0.97 0.95 0.33 0.28 0.29 0.08 0.16 0.16
DP-Or-? 0.88 0.96 0.97 0.84 0.89 0.89 0.57 0.41 0.44 0.36 0.33 0.30
CP-STM-4 0.88 0.97 0.97 0.79 0.89 0.89 0.49 0.39 0.40 0.37 0.32 0.26
NE-Me-? -0.13 0.79 0.95 0.68 0.87 0.92 -0.03 0.07 0.07 -0.05 0.05 0.06
NE-Oe-?? -0.18 0.78 0.95 0.58 0.81 0.71 0.32 0.26 0.37 0.26 0.31 0.20
SR-Or-? 0.55 0.96 0.94 0.69 0.89 0.85 0.26 0.14 0.30 0.11 0.08 0.19
SR-Or-?b 0.24 0.98 0.94 0.68 0.92 0.87 0.33 0.21 0.35 0.15 0.18 0.24
Shallow SR-Or-?i 0.51 0.95 0.93 0.67 0.88 0.83 0.37 0.26 0.38 0.19 0.24 0.27
Semantic SR-Mr-? 0.38 0.95 0.96 0.83 0.79 0.75 0.32 0.18 0.28 0.18 0.08 0.14
SR-Mr-?b 0.14 0.98 0.97 0.82 0.84 0.79 0.37 0.23 0.32 0.21 0.15 0.17
SR-Mr-?i 0.38 0.94 0.96 0.80 0.79 0.74 0.40 0.27 0.36 0.24 0.20 0.20
SR-Or 0.73 0.99 0.94 0.66 0.97 0.93 0.12 0.09 0.16 0.07 -0.04 0.17
SR-Ori 0.66 0.99 0.94 0.64 0.95 0.89 0.29 0.25 0.29 0.19 0.15 0.28
DR-Or-? 0.87 0.89 0.96 0.71 0.78 0.75 0.50 0.40 0.37 0.35 0.27 0.28
DR-Or-?b 0.91 0.93 0.97 0.72 0.83 0.80 0.52 0.41 0.38 0.34 0.28 0.27
DR-Or-?i 0.87 0.87 0.96 0.68 0.79 0.74 0.53 0.42 0.39 0.35 0.30 0.28
DR-Orp-? 0.92 0.98 0.99 0.81 0.91 0.89 0.42 0.32 0.29 0.25 0.21 0.30
Semantic DR-Orp-?b 0.93 0.98 0.99 0.81 0.94 0.91 0.45 0.34 0.32 0.22 0.22 0.30
DR-Orp-?i 0.91 0.95 0.98 0.75 0.89 0.85 0.50 0.38 0.36 0.28 0.27 0.33
DR-STM-4 0.89 0.95 0.98 0.79 0.85 0.87 0.28 0.29 0.25 0.21 0.15 0.22
DR-STM-4b 0.92 0.97 0.98 0.80 0.90 0.91 0.36 0.31 0.29 0.21 0.19 0.23
DR-STM-4i 0.91 0.94 0.97 0.74 0.87 0.86 0.43 0.35 0.34 0.26 0.24 0.27
Optimal07 0.93 1.00 0.99 0.92 0.98 0.95 0.60 0.46 0.47 0.42 0.36 0.39
Optimal06 0.01 0.95 0.96 0.75 0.97 0.87 0.50 0.41 0.40 0.20 0.27 0.30
ULC Optimal?07 0.93 0.98 0.99 0.81 0.94 0.91 0.58 0.45 0.46 0.39 0.35 0.34
Optimal?06 0.34 0.96 0.98 0.82 0.92 0.93 0.54 0.41 0.42 0.32 0.32 0.34
Optimalh 0.87 0.98 0.97 0.79 0.91 0.89 0.56 0.44 0.43 0.32 0.31 0.35
Table 2: Meta-evaluation results based on human acceptability for the WMT 2007 into-English translation tasks
HWCc-4?, ?DP-HWCr-4?), and morphosyntactic over-
lapping over discourse representations (?DR-Orp-??),
are consistently among the top-scoring in all test
beds. At the lexical level, variants of ROUGE and
METEOR attain the best results, close to the perfor-
mance of syntactic and semantic features. It can also
be observed that metrics based on semantic roles
and named entities have serious troubles with the
German-to-English in-domain test bed (column 1).
At the sentence level, the highest levels of corre-
lation are attained by metrics based on lexical simi-
larity alone, only rivaled by lexical overlapping over
dependency relations (?DP-Or-??) and discourse rep-
resentations (?DR-Or-??). We speculate the underly-
ing cause might be on the side of parsing errors. In
that respect, lexical back-off strategies report in all
cases a significant improvement.
It can also be observed that, over these test beds,
metrics based on named entities are completely use-
less at the sentence level, at least in isolation. The
reason is that they capture a very partial aspect of
quality which may be not relevant in many cases.
This has been verified by computing the ?NE-Oe-
??? variant which considers also lexical overlapping
over regular items. Observe how this metric attains
a much higher correlation with human assessments.
197
3.2 Metric Combinations
We also study the behavior of metric combinations
under the ULC scheme. Last 5 rows in Table 2
shows meta-evaluation results following 3 different
optimization strategies:
Optimal: the metric set is optimized for each test
bed (language-pair and domain) individually.
Optimal?: the metric set is optimized over the
union of all test beds.
Optimalh: the metric set is heuristically defined
so as to include several of the top-scoring
representatives from each level: Optimalh =
{ ROUGEW , METEORwnsyn, DP-HWCc-4, DP-
HWCr-4, DP-Or-?, CP-STM-4, SR-Mr-?i, SR-
Or-?i, SR-Ori, DR-Or-?i, DR-Orp-?b }.
We present results optimizing over the 2006 and
2007 data sets. Let us provide, as an illustration,
Optimal?07 sets. For instance, at the system level,
no combination improved the isolated global perfor-
mance of the ?DR-Orp-?b? metric (R=0.94). In con-
trast, at the sentence level, the optimal metric set
contains several metrics from each linguistic level:
Optimal?07 = { ROUGEW , DP-Or-?, CP-STM-4, SR-
Or-?i, SR-Mr-?i, DR-Or-?i }. A similar pattern is
observed for all test beds, both at the system and
sentence levels, although with different metrics.
The behavior of optimal metric sets is in general
quite stable, except for the German-to-English in-
domain test bed which presents an anomalous be-
havior when meta-evaluating WMT 2006 optimal
metric sets at the system level. The reason for this
anomaly is in the ?NE-Me-?? metric, which is in-
cluded in the 2006 optimal set: { ?NE-Me-??, ?SR-
Ori? }. ?NE-Me-?? is based on lexical matching over
named entities, and attains in the 2006 German-to-
English in-domain test bed a very high correlation
of 0.95 with human assessments. This partial aspect
of quality seems to be of marginal importance in the
2007 test bed. We have verified this hypothesis by
computing optimal metrics sets without considering
NE variants. Correlation increases to more reason-
able values (e.g., from 0.01 to 0.66 and from 0.34
to 0.91. This result suggests that more robust metric
combination schemes should be pursued.
For future work, we plan to apply parametric
combination schemes based on human likeness clas-
sifiers, as suggested by Kulesza and Shieber (2004).
We must also further investigate the impact of pars-
ing errors on the performance of linguistic metrics.
Acknowledgments
This research has been funded by the Spanish Min-
istry of Education and Science (OpenMT, TIN2006-
15307-C03-02). Our group is recognized by DURSI
as a Quality Research Group (2005 SGR-00130).
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
Evaluation of Machine Translation. In Proceedings of
the ACL Second SMT Workshop, pages 136?158.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and Log-Linear Models. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 104?111.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Second SMT
Workshop, pages 256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
IJCNLP, pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008b. On the
Robustness of Linguistic Features for Automatic MT
Evaluation. To be published.
Jesu?s Gime?nez. 2007. IQMT v 2.1. Tech-
nical Manual (LSI-07-29-R). Technical re-
port, TALP Research Center. LSI Department.
http://www.lsi.upc.edu/ nlp/IQMT/IQMT.v2.1.pdf.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102?121.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th TMI, pages 75?84.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization.
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of
NAACL, pages 41?48.
198
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 250?258,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
On the Robustness of Syntactic and Semantic
Features for Automatic MT Evaluation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
Linguistic metrics based on syntactic and
semantic information have proven very
effective for Automatic MT Evaluation.
However, no results have been presented
so far on their performance when applied
to heavily ill-formed low quality transla-
tions. In order to glean some light into this
issue, in this work we present an empirical
study on the behavior of a heterogeneous
set of metrics based on linguistic analysis
in the paradigmatic case of speech transla-
tion between non-related languages. Cor-
roborating previous findings, we have ver-
ified that metrics based on deep linguis-
tic analysis exhibit a very robust and sta-
ble behavior at the system level. How-
ever, these metrics suffer a significant de-
crease at the sentence level. This is in
many cases attributable to a loss of recall,
due to parsing errors or to a lack of parsing
at all, which may be partially ameliorated
by backing off to lexical similarity.
1 Introduction
Recently, there is a growing interest in the devel-
opment of automatic evaluation metrics which ex-
ploit linguistic knowledge at the syntactic and se-
mantic levels. For instance, we may find metrics
which compute similarities over shallow syntac-
tic structures/sequences (Gime?nez and Ma`rquez,
2007; Popovic and Ney, 2007), constituency
trees (Liu and Gildea, 2005) and dependency
trees (Liu and Gildea, 2005; Amigo? et al, 2006;
Mehay and Brew, 2007; Owczarzak et al, 2007).
We may also find metrics operating over shallow
semantic structures, such as named entities and se-
mantic roles (Gime?nez and Ma`rquez, 2007).
Linguistic metrics have been proven to produce
more reliable system rankings than metrics limit-
ing their scope to the lexical dimension, in partic-
ular when applied to test beds with a rich system
typology, i.e., test beds in which there are auto-
matic outputs produced by systems based on dif-
ferent paradigms, e.g., statistical, rule-based and
human-aided (Gime?nez and Ma`rquez, 2007). The
reason is that they are able to capture deep MT
quality distinctions which occur beyond the shal-
low level of lexical similarities.
However, these metrics have the limitation of
relying on automatic linguistic processors, tools
which are not equally available for all languages
and whose performance may vary depending on
the type of analysis conducted and the applica-
tion domain. Thus, it could be argued that lin-
guistic metrics should suffer a significant quality
drop when applied to a different translation do-
main, or to ill-formed sentences. Clearly, met-
ric scores computed on partial or wrong syntac-
tic/semantic structures will be less informed. But,
should this necessarily lead to less reliable eval-
uations? In this work, we have analyzed this is-
sue by conducting a contrastive empirical study on
the behavior of a heterogeneous set of metrics over
several evaluation scenarios of decreasing transla-
tion quality. In particular, we have studied the case
of Chinese-to-English speech translation, which is
a paradigmatic example of low quality and heavily
ill-formed output.
The rest of the paper is organized as follows. In
Section 2, prior to presenting experimental work,
we describe the set of metrics employed in our
experiments. We also introduce a novel family
of metrics which operate at the properly semantic
level by analyzing similarities over discourse rep-
resentations. Experimental work is then presented
in Section 3. Metrics are evaluated both in terms of
human likeness and human acceptability (Amigo?
et al, 2006). Finally, in Section 4, main conclu-
sions are summarized and future work is outlined.
250
2 A Heterogeneous Metric Set
We have used a heterogeneous set of metrics se-
lected out from the metric repository provided
with the IQMT evaluation package (Gime?nez and
Ma`rquez, 2007)1. We have considered several
metric representatives from different linguistic
levels (lexical, syntactic and semantic). A brief
description of the metric set is available in Ap-
pendix A.
In addition, taking advantage of newly available
semantic processors, we have designed a novel
family of metrics based on the Discourse Repre-
sentation Theory, a theoretical framework offer-
ing a representation language for the examination
of contextually dependent meaning in discourse
(Kamp, 1981). A discourse is represented in a
discourse representation structure (DRS), which is
essentially a variation of first-order predicate cal-
culus ?its forms are pairs of first-order formulae
and the free variables that occur in them.
2.1 Exploiting Semantic Similarity for
Automatic MT Evaluation
?DR? metrics analyze similarities between auto-
matic and reference translations by comparing
their respective DRSs. These are automatically
obtained using the C&C Tools (Clark and Cur-
ran, 2004)2. Sentences are first parsed on the basis
of a combinatory categorial grammar (Bos et al,
2004). Then, the BOXER component (Bos, 2005)
extracts DRSs. As an illustration, Figure 1 shows
the DRS representation for the sentence ?Every
man loves Mary.?. The reader may find the out-
put of the BOXER component (top) together with
the equivalent first-order formula (bottom).
DRS may be viewed as semantic trees, which
are built through the application of two types of
DRS conditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
Three kinds of metrics have been defined:
1http://www.lsi.upc.edu/?nlp/IQMT
2http://svn.ask.it.usyd.edu.au/trac/
candc
DR-STM-l (Semantic Tree Matching) These
metrics are similar to the Syntactic Tree
Matching metric defined by Liu and Gildea
(2005), in this case applied to DRSs instead
of constituency trees. All semantic subpaths
in the candidate and the reference trees are
retrieved. The fraction of matching subpaths
of a given length, l ? [1..9], is computed.
Then, average accumulated scores up to a
given length are retrieved. For instance, ?DR-
STM-4? corresponds to the average accumu-
lated proportion of matching subpaths up to
length-4.
DR-Or-t These metrics compute lexical overlap-
ping3 between discourse representation struc-
tures (i.e., discourse referents and discourse
conditions) according to their type ?t?. For
instance, ?DR-Or -pred? roughly reflects lexi-
cal overlapping between the referents associ-
ated to predicates (i.e., one-place properties),
whereas ?DR-Or -imp? reflects lexical overlap-
ping between referents associated to implica-
tion conditions. We also introduce the ?DR-
Or -?? metric, which computes average lexical
overlapping over all DRS types.
DR-Orp-t These metrics compute morphosyn-
tactic overlapping (i.e., between parts of
speech associated to lexical items) between
discourse representation structures of the
same type t. We also define the ?DR-Orp-??
metric, which computes average morphosyn-
tactic overlapping over all DRS types.
Note that in the case of some complex condi-
tions, such as implication or question, the respec-
tive order of the associated referents in the tree
is important. We take this aspect into account
by making order information explicit in the con-
struction of the semantic tree. We also make ex-
plicit the type, symbol, value and date of condi-
tions when these are applicable (e.g., predicates,
relations, named entities, time expressions, cardi-
nal expressions, or anaphoric conditions).
Finally, the extension to the evaluation setting
based on multiple references is computed by as-
signing the maximum score attained against each
individual reference.
3Overlapping is measured following the formulae and
definitions by Gime?nez and Ma`rquez (2007). A short defi-
nition may be found in Appendix A.
251
Formally:
?y named(y,mary, per) ? (?x man(x) ? ?z love(z) ? event(z) ? agent(z, x) ? patient(z, y))
Figure 1: DRS representation for ?Every man loves Mary.?
3 Experimental Work
In this section, we present an empirical study on
the behavior of a heterogeneous set of metrics
based on linguistic analysis in the case of speech
translation between non-related languages.
3.1 Evaluation Scenarios
We have used the test bed from the Chinese-
to-English translation task at the ?2006 Evalua-
tion Campaign on Spoken Language Translation?
(Paul, 2006)4. The test set comprises 500 transla-
tion test cases corresponding to simple conversa-
tions (question/answer scenario) in the travel do-
main. In addition, there are 3 different evalua-
tion subscenarios of increasing translation diffi-
culty, according to the translation source:
CRR: Translation of correct recognition results
(as produced by human transcribers).
ASR read: Translation of automatic read speech
recognition results.
ASR spont: Translation of automatic sponta-
neous speech recognition results.
For the purpose of automatic evaluation, 7 hu-
man reference translations and automatic outputs
by 14 different MT systems for each evaluation
subscenario are available. In addition, we count
on the results of a process of manual evaluation.
4http://www.slc.atr.jp/IWSLT2006/
For each subscenario, 400 test cases from 6 differ-
ent system outputs were evaluated, by three human
assessors each, in terms of adequacy and fluency
on a 1-5 scale (LDC, 2005). A brief numerical de-
scription of these test beds is available in Table 1.
It includes the number of human references and
system outputs available, as well as the number
of sentences per output, and the number of system
outputs and sentences per system assessed. For the
sake of completeness, we report the performance
of the Automatic Speech Recognition (ASR) sys-
tem, in terms of accuracy, over the source Chinese
utterances, both at the word and sentence levels.
Also, in order to give an idea of the translation
quality exhibited by automatic systems, average
adequacy and fluency scores are also provided.
3.2 Meta-Evaluation
Our experiment requires a mechanism for evaluat-
ing the quality of evaluation metrics, i.e., a meta-
evaluation criterion. The two most prominent are:
? Human Acceptability: Metrics are evaluated
in terms of their ability to capture the de-
gree of acceptability to humans of automatic
translations, i.e., their ability to emulate hu-
man assessors. The underlying assumption
is that good translations should be acceptable
to human evaluators. Human acceptability is
usually measured on the basis of correlation
between automatic metric scores and human
assessments of translation quality.
252
CRR ASR read ASR spont
#human-references 7 7 7
#system-outputs 14 14 13
#sentences 500 500 500
#outputsassessed 6 6 6
#sentencesassessed 400 400 400
Word Recognition Accuracy ? 0.74 0.68
Sentence Recognition Accuracy ? 0.23 0.17
Average Adequacy 1.40 1.02 0.93
Average Fluency 1.16 0.98 0.98
Table 1: IWSLT 2006 MT Evaluation Campaign. Chinese-to-English test bed description
? Human Likeness: Metrics are evaluated in
terms of their ability to capture the fea-
tures which distinguish human from auto-
matic translations. The underlying assump-
tion is that good translations should resemble
human translations. Human likeness is usu-
ally measured on the basis of discriminative
power (Lin and Och, 2004b; Amigo? et al,
2005).
In this work, metrics are evaluated both in terms
of human acceptability and human likeness. In the
case of human acceptability, metric quality is mea-
sured on the basis of correlation with human as-
sessments both at the sentence and document (i.e.,
system) levels. We compute Pearson correlation
coefficients. The sum of adequacy and fluency is
used as a global measure of quality. Assessments
from different judges have been averaged.
In the case of human likeness, we use the proba-
bilistic KING measure defined inside the QARLA
Framework (Amigo? et al, 2005). KING repre-
sents the probability, estimated over the set of test
cases, that the score attained by a human reference
is equal or greater than the score attained by any
automatic translation. Although KING computa-
tions do not require human assessments, for the
sake of comparison, we have limited to the set of
test cases counting on human assessments.
3.3 Results
Table 2 presents meta-evaluation results for a set
of metric representatives from different linguistic
levels over the three subscenarios defined (?CRR?,
?ASR read? and ?ASR spont?). Highest scores
in each column have been highlighted. Lowest
scores appear in italics.
System-level Behavior
At the system level (Rsys, columns 7-9), the high-
est quality is in general attained by metrics based
on deep linguistic analysis, either syntactic or se-
mantic. Among lexical metrics, the highest cor-
relation is attained by BLEU and the variant of
GTM rewarding longer matchings (e = 2).
As to the impact of sentence ill-formedness,
while most metrics at the lexical level suffer a sig-
nificant variation across the three subscenarios, the
performance of metrics at deeper linguistic levels
is in general quite stable. However, in the case of
the translation of automatically recognized spon-
taneous speech (ASR spont) we have found that
the ?SR-Or-?? and ?SR-Mr-?? metrics, respectively
based on lexical overlapping and matching over
semantic roles, suffer a very significant decrease
far below the performance of most lexical metrics.
Although ?SR-Or-?? has performed well on other
test beds (Gime?nez and Ma`rquez, 2007), its low
performance over the BTEC data suggests that it
is not fully portable across all kind of evaluation
scenarios.
Finally, it is highly remarkable the degree of ro-
bustness exhibited by semantic metrics introduced
in Section 2.1. In particular, the metric variants
based on lexical and morphosyntactic overlapping
over discourse representations (?DR-Or-?? and ?DR-
Orp-??, respectively), obtain a high system-level
correlation with human assessments across the
three subscenarios.
Sentence-level Behavior
At the sentence level (KING and Rsnt, columns
1-6), highest quality is attained in most cases by
metrics based on lexical matching. This result was
expected since all MT systems are statistical and
the test set is in-domain, that is it belongs to the
253
Human Likeness Human Acceptability
KING Rsnt Rsys
ASR ASR ASR ASR ASR ASR
Level Metric CRR read spont CRR read spont CRR read spont
1-WER 0.63 0.69 0.71 0.47 0.50 0.48 0.50 0.32 0.52
1-PER 0.71 0.79 0.79 0.44 0.48 0.45 0.67 0.39 0.60
1-TER 0.69 0.75 0.77 0.49 0.52 0.50 0.66 0.36 0.62
BLEU 0.69 0.72 0.73 0.54 0.53 0.52 0.79 0.74 0.62
Lexical NIST 0.79 0.84 0.85 0.53 0.54 0.53 0.12 0.26 -0.02
GTM (e = 1) 0.75 0.81 0.83 0.50 0.52 0.52 0.35 0.10 -0.09
GTM (e = 2) 0.72 0.78 0.79 0.62 0.64 0.61 0.78 0.65 0.62
METEORwnsyn 0.81 0.86 0.86 0.44 0.50 0.48 0.55 0.39 0.08
ROUGEW 1.2 0.74 0.79 0.81 0.58 0.60 0.58 0.53 0.69 0.43
Ol 0.74 0.81 0.82 0.57 0.62 0.58 0.77 0.51 0.34
SP-Op-? 0.75 0.80 0.82 0.54 0.59 0.56 0.77 0.54 0.48
SP-Oc-? 0.74 0.81 0.82 0.54 0.59 0.55 0.82 0.52 0.49
Shallow SP-NISTl 0.79 0.84 0.85 0.52 0.53 0.52 0.10 0.25 -0.03
Syntactic SP-NISTp 0.74 0.78 0.80 0.44 0.42 0.43 -0.02 0.24 0.04
SP-NISTiob 0.65 0.69 0.70 0.33 0.32 0.35 -0.09 0.17 -0.09
SP-NISTc 0.55 0.59 0.59 0.24 0.22 0.25 -0.07 0.19 0.08
CP-Op-? 0.75 0.81 0.82 0.57 0.63 0.59 0.84 0.67 0.52
CP-Oc-? 0.74 0.80 0.82 0.60 0.64 0.61 0.71 0.53 0.43
DP-Ol-? 0.68 0.75 0.76 0.48 0.50 0.50 0.84 0.77 0.67
DP-Oc-? 0.71 0.76 0.77 0.41 0.46 0.43 0.76 0.65 0.71
Syntactic DP-Or -? 0.75 0.80 0.81 0.51 0.53 0.51 0.81 0.75 0.62
DP-HWCw 0.54 0.57 0.57 0.29 0.32 0.28 0.73 0.74 0.37
DP-HWCc 0.48 0.51 0.52 0.17 0.18 0.22 0.73 0.64 0.67
DP-HWCr 0.44 0.49 0.48 0.20 0.21 0.25 0.71 0.58 0.56
CP-STM 0.71 0.77 0.80 0.53 0.56 0.54 0.65 0.58 0.47
SR-Mr -? 0.40 0.43 0.45 0.29 0.28 0.29 0.52 0.60 0.20
SR-Or -? 0.45 0.49 0.51 0.35 0.35 0.36 0.56 0.58 0.14
Shallow SR-Or 0.31 0.33 0.35 0.16 0.15 0.18 0.68 0.73 0.53
Semantic SR-Mrv -? 0.38 0.41 0.42 0.33 0.34 0.34 0.79 0.81 0.42
SR-Orv -? 0.40 0.44 0.45 0.36 0.38 0.38 0.64 0.72 0.72
SR-Orv 0.36 0.40 0.40 0.27 0.31 0.29 0.34 0.78 0.38
DR-Or -? 0.67 0.73 0.75 0.48 0.53 0.50 0.86 0.74 0.77
Semantic DR-Orp-? 0.59 0.64 0.65 0.34 0.35 0.33 0.84 0.78 0.95
DR-STM 0.58 0.63 0.65 0.23 0.26 0.26 0.75 0.62 0.67
Table 2: Meta-evaluation results for a set of metric representatives from different linguistic levels
same domain in which systems have been trained.
Therefore, translation outputs have a strong ten-
dency to share the sublanguage (i.e., word selec-
tion and word ordering) represented by the prede-
fined set of human reference translations.
Metrics based on lexical overlapping and
matching over shallow syntactic categories and
syntactic structures (?SP-Op-??, ?SP-Oc-??, ?CP-Op-??,
?CP-Oc-??, ?DP-Ol-??, ?DP-Oc-??, and ?DP-Or-??) per-
form similarly to lexical metrics. However, com-
puting NIST scores over base phrase chunk se-
quences (?SP-NISTiob?, ?SP-NISTc?) is not as effec-
tive. Metrics based on head-word chain match-
ing (?DP-HWCw?, ?DP-HWCc?, ?DP-HWCr?) suffer also
a significant decrease. Interestingly, the metric
based on syntactic tree matching (?CP-STM?) per-
formed well in all scenarios.
Metrics at the shallow semantic level suffer also
a severe drop in performance. Particularly signif-
icant is the case case of the ?SR-Or? metric, which
does not consider any lexical information. Inter-
estingly, the ?SR-Orv? variant, which only differs
in that it distinguishes between SRs associated to
different verbs, performs slightly better.
At the semantic level, metrics based on lex-
ical and morphosyntactic overlapping over dis-
course representations (?DR-Or-?? and ?DR-Orp-??)
suffer only a minor decrease, whereas semantic
tree matching (?DR-STM?) reports as a specially bad
predictor of human acceptability (Rsnt).
However, the most remarkable result, in rela-
tion to the goal of this work, is that the behavior
of syntactic and semantic metrics across the three
evaluation subscenarios is, in general, quite stable
?the three values in each subrow are in a very
similar range. Therefore, answering the question
posed in the introduction, sentence ill-formedness
is not a limiting factor in the performance of lin-
guistic metrics.
254
Human Likeness Human Acceptability
KING Rsnt Rsys
ASR ASR ASR ASR ASR ASR
Level Metric CRR read spont CRR read spont CRR read spont
Lexical NIST 0.79 0.84 0.85 0.53 0.54 0.53 0.12 0.26 -0.02
GTM (e = 2) 0.72 0.78 0.79 0.62 0.64 0.61 0.78 0.65 0.62
METEORwnsyn 0.81 0.86 0.86 0.44 0.50 0.48 0.55 0.39 0.08
Ol 0.74 0.81 0.82 0.57 0.62 0.58 0.77 0.51 0.34
CP-Op-? 0.75 0.81 0.82 0.57 0.63 0.59 0.84 0.67 0.52
Syntactic CP-Oc-? 0.74 0.80 0.82 0.60 0.64 0.61 0.71 0.53 0.43
DP-Ol-? 0.68 0.75 0.76 0.48 0.50 0.50 0.84 0.77 0.67
SR-Mr -? 0.40 0.43 0.45 0.29 0.28 0.29 0.52 0.60 0.20
SR-Mr -?b 0.68 0.72 0.73 0.31 0.30 0.31 0.52 0.60 0.20
SR-Mr -?i 0.84 0.86 0.88 0.34 0.34 0.34 0.56 0.63 0.25
SR-Or -? 0.45 0.49 0.51 0.35 0.35 0.36 0.56 0.58 0.14
SR-Or -?b 0.71 0.75 0.78 0.38 0.38 0.38 0.56 0.58 0.14
SR-Or -?i 0.84 0.88 0.89 0.41 0.41 0.41 0.62 0.60 0.22
SR-Or 0.31 0.33 0.35 0.16 0.15 0.18 0.68 0.73 0.53
SR-Or b 0.54 0.58 0.60 0.19 0.18 0.20 0.68 0.73 0.53
Shallow SR-Or i 0.72 0.77 0.79 0.26 0.26 0.27 0.80 0.73 0.67
Semantic SR-Mrv -? 0.38 0.41 0.42 0.33 0.34 0.34 0.79 0.81 0.42
SR-Mrv -?b 0.70 0.73 0.74 0.34 0.35 0.34 0.79 0.81 0.42
SR-Mrv -?i 0.88 0.90 0.92 0.36 0.38 0.37 0.81 0.82 0.45
SR-Orv -? 0.40 0.44 0.45 0.36 0.38 0.38 0.64 0.72 0.72
SR-Orv -?b 0.72 0.76 0.77 0.38 0.40 0.39 0.64 0.72 0.72
SR-Orv -?i 0.88 0.90 0.91 0.40 0.42 0.41 0.69 0.74 0.74
SR-Orv 0.36 0.40 0.40 0.27 0.31 0.29 0.34 0.78 0.38
SR-Orv b 0.66 0.70 0.71 0.29 0.32 0.30 0.34 0.78 0.38
SR-Orv i 0.83 0.86 0.88 0.33 0.36 0.33 0.49 0.82 0.56
DR-Or -? 0.67 0.73 0.75 0.48 0.53 0.50 0.86 0.74 0.77
DR-Or -?b 0.69 0.75 0.77 0.50 0.53 0.50 0.90 0.69 0.56
Semantic DR-Or -?i 0.83 0.87 0.89 0.53 0.57 0.53 0.88 0.70 0.61
DR-Orp-? 0.59 0.64 0.65 0.34 0.35 0.33 0.84 0.78 0.95
DR-Orp-?b 0.61 0.65 0.67 0.35 0.36 0.34 0.86 0.71 0.57
DR-Orp-?i 0.80 0.84 0.85 0.43 0.46 0.43 0.90 0.75 0.70
DR-STM 0.58 0.63 0.65 0.23 0.26 0.26 0.75 0.62 0.67
DR-STM-b 0.64 0.68 0.71 0.23 0.26 0.27 0.75 0.62 0.67
DR-STM-i 0.83 0.87 0.87 0.33 0.36 0.36 0.84 0.63 0.66
Table 3: Meta-evaluation results. Improved sentence-level evaluation of SR and DR metrics
Improved Sentence-level Behavior
By inspecting particular instances, we have found
that linguistic metrics are, in many cases, unable to
produce any evaluation result. The number of un-
scored sentences is particularly significant in the
case of SR metrics. For instance, the ?SR-Or-??
metric is unable to confer an evaluation score in
57% of the cases. Several reasons explain this fact.
The first and most important is that linguistic met-
rics rely on automatic processors trained on out-
of-domain data, which are, thus, prone to error.
Second, we argue that the test bed itself does not
allow for fully exploiting the capabilities of these
metrics. Apart from being based on a reduced vo-
cabulary (2,346 distinct words), test cases consist
mostly of very short segments (14.64 words on av-
erage), which in their turn consist of even shorter
sentences (8.55 words on average)5.
5Vocabulary size and segment/sentence average lengths
have been computed over the set of reference translations.
A possible solution could be to back off to
a measure of lexical similarity in those cases in
which linguistic processors are unable to produce
any linguistic analysis. This should significantly
increase their recall. With that purpose, we have
designed two new variants for each of these met-
rics. Given a linguistic metric x, we define:
? xb ? by backing off to lexical overlapping,
Ol, only when the linguistic processor was
not able to produce a parsing. Lexical scores
are conveniently scaled so that they are in a
similar range to x scores. Specifically, we
multiply them by the average x score attained
over all other test cases for which the parser
succeeded. Formally, given a test case t be-
longing to a set of test cases T :
xb(t) =
{
x(t) if t ? ok(T )
Ol(t)
P
j?ok(T ) x(j)
|ok(T )| otherwise
255
where ok(T ) is the subset of test cases in T
which were successfully parsed.
? xi ? by linearly interpolating x and Ol
scores for all test cases, via arithmetic mean:
xi(t) =
x(t) + Ol(t)
2
In both cases, system-level scores are calculated
by averaging over all sentence-level scores.
Table 3 shows meta-evaluation results on the
performance of these variants for several repre-
sentatives from the SR and DR families. For the
sake of comparison, we also show the scores at-
tained by the base versions, and by some of the
top-scoring metrics from other linguistic levels.
The first observation is that in all cases the new
variants outperform their respective base metric,
being linear interpolation the best alternative. The
increase is particularly significant in terms of hu-
man likeness. New variants even outperform lex-
ical metrics, including the Ol metric, which sug-
gests that, in spite of its simplicity, this is a valid
combination scheme. However, in terms of human
acceptability, the gain is only moderate, and still
their performance is far from top-scoring metrics.
Sentence-level improvements are also reflected
at the system level, although to a lesser extent.
Interestingly, in the case of the translation of au-
tomatically recognized spontaneous speech (ASR
spont, column 9), mixing with lexical overlap-
ping improves the low-performance ?SR-Or? and
?SR-Orv? metrics, at the same time that it causes
a significant drop in the high-performance ?DR-Or?
and ?DR-Orp? metrics. Still, the performance of lin-
guistic metrics at the sentence level is under the
performance of lexical metrics. This is not sur-
prising. After all, apart from relying on automatic
processors, linguistic metrics focus on very par-
tial aspects of quality. However, since they operate
at complementary quality dimensions, their scores
are suitable for being combined.
4 Conclusions and Future Work
We have presented an empirical study on the ro-
bustness of a heterogeneous set of metrics operat-
ing at different linguistic levels for the particular
case of Chinese-to-English speech translation of
basic travel expressions. As an additional contri-
bution, we have presented a novel family of met-
rics which operate at the semantic level by analyz-
ing discourse representations.
Corroborating previous findings by Gime?nez
and Ma`rquez (2007), results at the system level,
show that metrics guided by deeper linguistic
knowledge, either syntactic or semantic, are, in
general, more effective and stable than metrics
which limit their scope to the lexical dimension.
However, at the sentence level, results indicate
that metrics based on deep linguistic analysis are
not as reliable overall quality estimators as lexical
metrics, at least when applied to low quality trans-
lations, as it is the case. This behavior is mainly at-
tributable a drop in recall due to parsing errors. By
inspecting particular sentences we have observed
that in many cases these metrics are unable to pro-
duce any result. In that respect, we have showed
that backing off to lexical similarity is a valid and
effective strategy so as to improve the performance
of these metrics.
But the most remarkable result, in relation to the
goal of this work, is that syntactic and semantic
metrics exhibit a very robust behavior across the
three evaluation subscenarios of decreasing trans-
lation quality analyzed. Therefore, sentence ill-
formedness is not a limiting factor in the perfor-
mance of linguistic metrics. The quality drop,
when moving from the system to the sentence
level, seems, thus, more related to a shift in the
application domain.
For future work, we are currently studying the
possibility of further improving the sentence-level
behavior of present evaluation methods by com-
bining the outcomes of metrics at different linguis-
tic levels into a single measure of quality (citation
omitted for the sake of anonymity).
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, project
OpenMT (TIN2006-15307-C03-02). Our NLP
group has been recognized as a Quality Research
Group (2005 SGR-00130) by DURSI, the Re-
search Department of the Catalan Government.
We are grateful to the SLT Evaluation Campaign
organizers and participants for providing such
valuable test beds.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Sumarization. In Pro-
256
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
Like vs. Human Acceptable. In Proceedings of
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
Coverage Semantic Representations from a CCG
Parser. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), pages 1240?1246.
Johan Bos. 2005. Towards Wide-Coverage Seman-
tic Interpretation. In Proceedings of the Sixth In-
ternational Workshop on Computational Semantics,
pages 42?53.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and Log-Linear Models. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternation Conference on Human Language Technol-
ogy, pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Hans Kamp. 1981. A Theory of Truth and Seman-
tic Representation. In J.A.G. Groenendijk, T.M.V.
Janssen, , and M.B.J. Stokhof, editors, Formal Meth-
ods in the Study of Language, pages 277?322, Ams-
terdam. Mathematisch Centrum.
LDC. 2005. Linguistic Data Annotation Specification:
Assessment of Adequacy and Fluency in Trans-
lations. Revision 1.5. Technical report, Linguis-
tic Data Consortium. http://www.ldc.upenn.edu/-
Projects/TIDES/Translation/TransAssess04.pdf.
Chin-Yew Lin and Franz Josef Och. 2004a. Au-
tomatic Evaluation of Machine Translation Qual-
ity Using Longest Common Subsequence and Skip-
Bigram Statics. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. OR-
ANGE: a Method for Evaluating Automatic Evalu-
ation Metrics for Machine Translation. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING).
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evalu-
ation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-Based Automatic Eval-
uation for Machine Translation. In Proceedings of
SSST, NAACL-HLT/AMTA Workshop on Syntax and
Structure in Statistical Translation, pages 80?87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, RC22176. Technical
report, IBM T.J. Watson Research Center.
Michael Paul. 2006. Overview of the IWSLT 2006
Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Trans-
lation, pages 1?15.
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Appli-
cations for Error Analysis. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 48?55, Prague, Czech Republic, June.
Association for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223?231.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based Search for
Statistical Translation. In Proceedings of European
Conference on Speech Communication and Technol-
ogy.
257
A Metric Set
Metrics are grouped according to the linguistic di-
mension at which they operate:
? Lexical Similarity
WER (Nie?en et al, 2000).
PER (Tillmann et al, 1997).
BLEU (Papineni et al, 2001).
NIST (Doddington, 2002).
GTM (Melamed et al, 2003).
ROUGE (Lin and Och, 2004a).
METEOR. (Banerjee and Lavie, 2005).
TER (Snover et al, 2006).
Ol (Gime?nez and Ma`rquez, 2007). Ol is a
short name for lexical overlapping. Au-
tomatic and reference translations are
considered as unordered sets of lexical
items. Ol is computed as the cardinal-
ity of the intersection of the two sets di-
vided into the cardinality of their union.
? Shallow Syntactic Similarity (SP)
SP-Op-?. Average lexical overlapping over
parts-of-speech.
SP-Oc-?. Average lexical overlapping over
base phrase chunk types.
SP-NIST. NIST score over sequences of:
SP-NISTl Lemmas.
SP-NISTp Parts-of-speech.
SP-NISTc Base phrase chunks.
SP-NISTiob Chunk IOB labels.
? Syntactic Similarity
On Dependency Parsing (DP)
DP-HWC Head-word chain matching
(HWCM), as presented by Liu and
Gildea (2005), but slightly modi-
fied so as to consider different head-
word chain types:
DP-HWCw words.
DP-HWCc categories.
DP-HWCr relations.
In all cases only chains up to length
4 are considered.
DP-Ol|Oc|Or These metrics cor-
respond exactly to the LEVEL,
GRAM and TREE metrics intro-
duced by Amigo? et al (2006):
DP-Ol-? Average overlapping be-
tween words hanging at the same
level of the tree.
DP-Oc-? Average overlapping be-
tween words assigned the same
grammatical category.
DP-Or-? Average overlapping be-
tween words ruled by the same
type of grammatical relations.
On Constituency Parsing (CP)
CP-STM Syntactic tree matching
(STM), as presented by Liu and
Gildea (2005), i.e., limited up to
length-4 subtrees.
CP-Op-? Average lexical overlap-
ping over parts-of-speech, similarly
to ?SP-Op-??, except that parts-of-
speech are now consistent with the
full parsing.
CP-Oc-? Average lexical overlapping
over phrase constituents. The differ-
ence between this metric and ?SP-Oc-
?? is in the phrase scope. In con-
trast to base phrase chunks, con-
stituents allow for phrase embed-
ding and overlapping.
? Shallow-Semantic Similarity
On Semantic Roles (SR)
SR-Or-? Average lexical overlapping
between SRs of the same type.
SR-Mr-? Average lexical matching
between SRs of the same type.
SR-Or Overlapping between semantic
roles independently from their lexi-
cal realization.
We also consider a more restrictive ver-
sion of these metrics (?SR-Mrv -??, ?SR-
Orv -??, and ?SR-Orv ?), which require
SRs to be associated to the same verb.
? Semantic Similarity
On Discourse Representations (DR)
DR-STM Average semantic tree
matching considering semantic
subtrees up to length 4.
DR-Or-? Average lexical overlapping
between DRSs of the same type.
DR-Orp-? Average morphosyntactic
overlapping between DRSs of the
same type.
258
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 79?84,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Second-Order Joint Eisner Model for Syntactic and Semantic Dependency
Parsing
Xavier Llu??s Stefan Bott Llu??s Ma`rquez
TALP Research Center ? Software Department (LSI)
Technical University of Catalonia (UPC)
{xlluis,sbott,lluism}@lsi.upc.edu
Abstract
We present a system developed for the
CoNLL-2009 Shared Task (Hajic? et al, 2009).
We extend the Carreras (2007) parser to
jointly annotate syntactic and semantic depen-
dencies. This state-of-the-art parser factor-
izes the built tree in second-order factors. We
include semantic dependencies in the factors
and extend their score function to combine
syntactic and semantic scores. The parser is
coupled with an on-line averaged perceptron
(Collins, 2002) as the learning method. Our
averaged results for all seven languages are
71.49 macro F1, 79.11 LAS and 63.06 seman-
tic F1.
1 Introduction
Systems that jointly annotate syntactic and semantic
dependencies were introduced in the past CoNLL-
2008 Shared Task (Surdeanu et al, 2008). These
systems showed promising results and proved the
feasibility of a joint syntactic and semantic pars-
ing (Henderson et al, 2008; Llu??s and Ma`rquez,
2008).
The Eisner (1996) algorithm and its variants are
commonly used in data-driven dependency pars-
ing. Improvements of this algorithm presented
by McDonald et al (2006) and Carreras (2007)
achieved state-of-the-art performance for English in
the CoNLL-2007 Shared Task (Nivre et al, 2007).
Johansson and Nugues (2008) presented a sys-
tem based on the Carreras? extension of the Eis-
ner algorithm that ranked first in the past CoNLL-
2008 Shared Task. We decided to extend the Car-
reras (2007) parser to jointly annotate syntactic and
semantic dependencies.
The present year Shared Task has the incentive
of being multilingual with each language presenting
their own particularities. An interesting particularity
is the direct correspondence between syntactic and
semantic dependencies provided in Catalan, Spanish
and Chinese. We believe that these correspondences
can be captured by a joint system. We specially look
at the syntactic-semantic alignment of the Catalan
and Spanish datasets.
Our system is an extension of the Llu??s and
Ma`rquez (2008) CoNLL-2008 Shared Task system.
We introduce these two following novelties:
? An extension of the second-order Car-
reras (2007) algorithm to annotate semantic
dependencies.
? A combined syntactic-semantic scoring for
Catalan and Spanish to exploit the syntactic-
semantic mappings.
The following section outlines the system archi-
tecture. The next sections present in more detail the
system novelties.
2 Architecture
The architecture consists on four main components:
1) Preprocessing and feature extraction. 2) Syntactic
preparsing. 3) Joint syntactic-semantic parsing. 4)
Predicate classification.
The preprocessing and feature extraction is in-
tended to ease and improve the performance of
the parser precomputing a binary representation of
79
each sentence features. These features are borrowed
from existing and widely-known systems (Xue and
Palmer, 2004; McDonald et al, 2005; Carreras et al,
2006; Surdeanu et al, 2007).
The following step is a syntactic pre-parse. It
is only required to pre-compute additional features
(e.g., syntactic path, syntactic frame) from the syn-
tax. These new features will be used for the semantic
role component of the following joint parser.
The joint parser is the core of the system. This
single algorithm computes the complete parse that
optimizes a score according to a function that de-
pends on both syntax and semantics. Some of the
required features that could be unavailable or expen-
sive to compute at that time are provided by the pre-
vious syntactic pre-parse.
The predicate sense classification is performed as
the last step. Therefore no features representing the
predicate sense are employed during the training.
The predicates are labeled with the most frequent
sense extracted from the training corpus.
No further postprocessing is applied.
3 Second-order Eisner model
The Carreras? extension of the Eisner inference al-
gorithm is an expensive O(n4) parser. The number
of assignable labels for each dependency is a hidden
multiplying constant in this asymptotic cost.
We begin describing a first-order dependency
parser. It receives a sentence x and outputs a de-
pendency tree y. A dependency, or first-order factor,
is defined as f1 = ?h,m, l?. Where h is the head
token, m the modifier and l the syntactic label. The
score for this factor f1 is computed as:
score1(f1, x,w) = ?(h,m, x) ?w(l)
Where w(l) is the weight vector for the syntactic la-
bel l and ? a feature extraction function.
The parser outputs the best tree y? from the set
T (x) of all projective dependency trees.
y?(x) = argmax
y?T (x)
?
f1?y
score(f1, x,w)
The second-order extension decomposes the de-
pendency tree in factors that include some children
of the head and modifier. A second-order factor is:
f2 = ?h,m, l, ch, cmo, cmi?
where ch is the daughter of h closest to m within
the tokens [h, . . . ,m]; cmo is the outermost daugh-
ter of m outside [h, . . . ,m]; and cmi is the furthest
daughter of m inside [h, . . . ,m].
The score for these new factors is computed by
score2(f2, x,w) = ?(h,m, x) ?w(l) +
?(h,m, ch, x) ?w(l)ch +
?(h,m, cmi, x) ?w(l)cmi +
?(h,m, cmo, x) ?w(l)cmo
The parser builds the best-scoring projective tree
factorized in second-order factors. The score of the
tree is also defined as the sum of the score of its
factors.
3.1 Joint second-order model
We proceeded in an analogous way in which the
Llu??s and Ma`rquez (2008) extended the first-order
parser. That previous work extended a first-order
model by including semantic labels in first-order de-
pendencies.
Now we define a second-order joint factor as:
f2syn-sem =?
h,m, l, ch, cmo, cmi, lsemp1 , . . . , lsempq
?
Note that we only added a set of semantic labels
lsemp1 , . . . , lsempq to the second-order factor. Each
one of these semantic labels represent, if any, one
semantic relation between the argument m and the
predicate pi. There are q predicates in the sentence,
labeled p1, . . . , pq.
The corresponding joint score to a given joint fac-
tor is computed by adding a semantic score to the
previously defined score2 second-order score func-
tion:
score2syn-sem(f2syn-sem, x,w) =
score2(f2, x,w) +
?
pi
scoresem(h,m, pi, lsempi , x,w)
q
where,
scoresem(h,m, pi, lsem, x,w) =
?sem(h,m, pi, x) ?w(lsem)
80
We normalize the semantic score by the number
of predicates q. The semantic score is computed as a
score betweenm and each sentence predicate pi. No
second-order relations are considered in these score
functions. The search of the best ch, cmo and cmi is
independent of the semantic components of the fac-
tor. The computational cost of the algorithm is in-
creased by one semantic score function call for every
m, h, and pi combination. The asymptotic cost of
this operation is O(q ? n2) and it is sequentially per-
formed among other O(n2) operations in the main
loop of the algorithm.
Algorithm 1 Extension of the Carreras (2007) algo-
rithm
C[s][t][d][m]? 0, ?s, t, d,m
O[s][t][d][l]? 0,?s, t, d, l
for k = 1, . . . , n do
for s = 0, . . . , n? k do
t? s+ k
?l O[s][t][?][l] = maxr,cmi,ch
C[s][r][?][cmi] + C[r + 1][t][?][ch]
+score(t, s, l)+scorecmi(t, s, cmi, l)+
scorech(t, s, l, ch)+?
pi maxlsemscoresem(t, s, pi, lsem)/q
?l O[s][t][?][l] = maxr,cmi,ch
C[s][r][?][ch] + C[r + 1][t][?][cmi]+
score(s, t, l)+scorecmi(s, t, cmi, l)+
scorech(s, t, l, ch)+?
pi maxlsemscoresem(t, s, pi, lsem)/q
?m C[s][t][?][m] = maxl,cmo
C[s][m][?][cmo] +O[m][t][?][l]+
scorecmo(s,m, l, cmo)
?m C[s][t][?][m] = maxl,cmo
O[s][m][?][l] + C[m][t][?][cmo]+
scorecmo(m, t, l, cmo)
end for
end for
Our implementation slightly differs from the orig-
inal Carreras algorithm description. The main dif-
ference is that no specific features are extracted for
the second-order factors. This allows us to reuse the
feature extraction mechanism of a first-order parser.
Algorithm 1 shows the Carreras? extension of the
Eisner algorithm including our proposed joint se-
mantic scoring.
The tokens s and t represent the start and end
tokens of the current substring, also called span.
The direction d ? {?,?} defines whether t or
s is the head of the last dependency built inside
the span. The score functions scorech,scorecmi and
scorecmo are the linear functions that build up the
previously defined second-order global score, e.g.,
scorech= ?(h,m, ch, x)?w(l)ch . The two tablesC and
O maintain the dynamic programming structures.
Note that the first steps of the inner loop are ap-
plied for all l, the syntactic label, but the semantic
score function does not depend on l. Therefore the
best semantic label can be chosen independently.
For simplicity, we omitted the weight vectors re-
quired in each score function and the backpointers
tables to save the local decisions. We also omit-
ted the definition of the domain of some variables.
Moreover, the filter of the set of assignable labels
is not shown. A basic filter regards the POS of the
head and modifier to filter out the set of possible ar-
guments for each predicate. Another filter extract
the set of allowed arguments for each predicate from
the frames files. These last filters were applied to the
English, German and Chinese.
3.2 Catalan and Spanish joint model
The Catalan and Spanish datasets (Taule? et al, 2008)
present two interesting properties. The first prop-
erty, as previously said, is a direct correspondence
between syntactic and semantic labels. The second
interesting property is that all semantic dependen-
cies exactly overlap with the syntactic tree. Thus
the semantic dependency between a predicate and
an argument always has a matching syntactic depen-
dency between a head and a modifier. The Chinese
data also contains direct syntactic-semantic map-
pings. But due to the Shared Task time constraints
we did not implemented a specific parsing method
for this language.
The complete overlap between syntax and seman-
tics can simplify the definition of a second-order
joint factor. In this case, a second-order factor will
only have, if any, one semantic dependency. We only
allow at most one semantic relation lsem between
the head token h and the modifier m. Note that h
must be a sentence predicate and m its argument if
81
lsem is not null. We extend the second-order fac-
tors with a single and possibly null semantic label,
i.e., f2syn-sem = ?h,m, l, ch, cmo, cmi, lsem?. This
slightly simplifies the scoring function:
score2syn-sem(f2syn-sem, x,w) =
score2(f2, x,w) +
? ? scoresem(h,m, x,w)
where ? is an adjustable parameter of the model and,
scoresem(h,m, x,w) = ?sem(h,m, x) ?w(lsem)
The next property that we are intended to exploit
is the syntactic-semantic mappings. These map-
pings define the allowed combinations of syntactic
and semantic labels. The label combinations can
only be exploited when there is semantic depen-
dency between the head h and the modifier m of a
factor. An argument identification classifier deter-
mines the presence of a semantic relation, given h
is a predicate. In these cases we only generate fac-
tors that are compliant with the mappings. If a syn-
tactic label has many corresponding semantic labels
we will score all of them and select the combination
with the highest score.
The computational cost is not significantly in-
creased as there is a bounded number of syntactic
and semantic combinations to score. In addition, the
only one-argument-per-factor constraint reduces the
complexity of the algorithm with respect to the pre-
vious joint extension.
We found some inconsistencies in the frames files
provided by the organizers containing the correspon-
dences between syntax and semantics. For this rea-
son we extracted them directly from the corpus. The
extracted mappings discard the 7.9% of the cor-
rect combinations in the Catalan development cor-
pus that represent a 1.7% of its correct syntactic de-
pendencies. The discarded semantic labels are the
5.14% for Spanish representing the 1.3% of the syn-
tactic dependencies.
4 Results and discussion
Table 1 shows the official results for all seven lan-
guages, including out-of-domain data labeled as
ood. The high computational cost of the second-
order models prevented us from carefully tuning the
system parameters. After the shared task evaluation
deadline, some bug were corrected, improving the
system performance. The last results are shown in
parenthesis.
The combined filters for Catalan and Spanish hurt
the parsing due to the discarded correct labels but
we believe that this effect is compensated by an im-
proved precision in the cases where the correct la-
bels are not discarded. For example, in Spanish
these filters improved the syntactic LAS from 85.34
to 86.77 on the development corpus using the gold
syntactic tree as the pre-parse tree.
Figure 1 shows the learning curve for the English
and Czech language. The results are computed in
the development corpus. The semantic score is com-
puted using gold syntax and gold predicate sense
classification. We restricted the learning curve to
the first epoch. Although the this first epoch is very
close to the best score, some languages showed im-
provements until the fourth epoch. In the figure we
can see better syntactic results for the joint system
with respect to the syntactic-only parser. We should
not consider this improvement completely realistic
as the semantic component of the joint system uses
gold features (i.e., a gold pre-parse). Nonetheless,
it points that a highly accurate semantic component
could improve the syntax.
Table 2 shows the training time for a second-order
syntactic and joint configurations of the parser. Note
that the time per instance is an average and some
sentences could require a significantly higher time.
Recall that our parser is O(n4) dependant on the
sentence length. We discarded large sentences dur-
ing training for efficiency reasons. We discarded
sentences with more than 70 words for all languages
except for Catalan and Spanish where the thresh-
old was set to 100 words in the syntactic parser.
This larger number of sentences is aimed to im-
prove the syntactic performance of these languages.
The shorter sentences used in the joint parsing and
the pruning of the previously described filters re-
duced the training time for Catalan and Spanish. The
amount of main memory consumed by the system is
0.5?1GB. The machine used to perform the compu-
tations is an AMD64 Athlon 5000+.
82
avg cat chi cze eng ger jap spa
macro F1 71.49 (74.90) 56.64 (73.21) 66.18 (70.91) 75.95 81.69 72.31 81.76 65.91 (68.46)
syn LAS 79.11 (82.22) 64.21(84.20) 70.53 (70.90) 75.00 87.48 81.94 91.55 83.09 (84.48)
semantic F1 63.06 (67.41) 46.79 (61.68) 59.72 (70.88) 76.90 75.86 62.66 71.60 47.88 (52.30)
ood macro F1 71.92 - - 74.56 73.91 67.30 - -
ood syn LAS 75.09 - - 72.11 80.92 72.25 - -
ood sem F1 68.74 - - 77.01 66.88 62.34 - -
Table 1: Overall results. In parenthesis post-evaluation results.
cat chi cze eng ger jap spa
syntax only (s/sentence) 18.39 8.07 3.18 2.56 1.30 1.07 15.31
joint system (s/sentence) 10.91 9.49 3.99 3.13 2.36 1.25 12.29
Table 2: Parsing time per sentence.
 
70
 
72
 
74
 
76
 
78
 
80
 
82
 
84
 
86
 
88
 
90
 
92  10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90
 
100
semanic f1, LAS
% of c
orpus
syn cz
syn cz
 joint
sem 
cz joint syn eng
syn en
g joint
sem 
eng join
t
Figure 1: Learning curves for the syntactic-only and joint
parsers in Czech and English.
5 Conclusion
We have shown that a joint syntactic-semantic
parsing can be based on the state-of-the-art Car-
reras (2007) parser at an expense of a reasonable
cost. Our second-order parser still does not repro-
duce the state-of-the art results presented by similar
systems (Nivre et al, 2007). Although we achieved
mild results we believe that a competitive system
based in our model can be built. Further tuning is
required and a complete set of new second-order fea-
tures should be implemented to improve our parser.
The multilingual condition of the task allows us to
evaluate our approach in seven different languages.
A detailed language-dependent evaluation can give
us some insights about the strengths and weaknesses
of our approach across different languages. Unfor-
tunately we believe that this objective was possibly
not accomplished due to the time constraints.
The Catalan and Spanish datasets presented in-
teresting properties that could be exploited. The
mapping between syntax and semantics should be
specially useful for a joint system. In addition
the semantic dependencies for these languages are
aligned with the projective syntactic dependencies,
i.e., the predicate-argument pairs exactly match syn-
tactic dependencies. This is a useful property to si-
multaneously build joint dependencies.
6 Future and ongoing work
Our syntactic and semantic parsers, as many others,
is not exempt of bugs. Furthermore, very few tuning
and experimentation was done during the develop-
ment of our parser due to the Shared Task time con-
straints. We believe that we still did not have enough
data to fully evaluate our approach. Further exper-
imentation is required to asses the improvement of
a joint architecture vs. a pipeline architecture. Also
a careful analysis of the system across the different
languages is to be performed.
Acknowledgments
We thank the corpus providers (Taule? et al, 2008;
Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006; Kawahara et al,
2002) for their effort in the annotation and conver-
sion of the seven languages datasets.
83
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras, Mihai Surdeanu, and Llu??s Ma`rquez.
2006. Projective dependency parsing with perceptron.
In Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL-2006).
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
11th Conference on Computational Natural Language
Learning (CoNLL-2007).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING-96).
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of the 12th Conference on
Computational Natural Language Learning (CoNLL-
2008), Manchester, UK.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with propbank and nombank. In Proceedings of the
12th Conference on Computational Natural Language
Learning (CoNLL-2008), Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s and Llu??s Ma`rquez. 2008. A joint model
for parsing syntactic and semantic dependencies. In
Proceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL-2008), Manch-
ester, UK.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In 11th Conference of the European Chapter of
the Association for Computational Linguistics (EACL-
2006).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
2005).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence
Research.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In Proceedings of the
Empirical Methods in Natural Language Processing
(EMNLP-2004).
84
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70?75,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages 
 
 
Marta Recasens, Toni Mart?, Mariona Taul? Llu?s M?rquez, Emili Sapena 
Centre de Llenguatge i Computaci? (CLiC) TALP Research Center,  
University of Barcelona Technical University of Catalonia 
Gran Via de les Corts Catalanes 585 
08007 Barcelona 
Jordi Girona Salgado 1-3 
08034 Barcelona 
{mrecasens, amarti, mtaule} 
@ub.edu 
{lluism, esapena} 
@lsi.upc.edu 
 
 
 
Abstract 
This paper presents the task ?Coreference 
Resolution in Multiple Languages? to be run 
in SemEval-2010 (5th International Workshop 
on Semantic Evaluations). This task aims to 
evaluate and compare automatic coreference 
resolution systems for three different lan-
guages (Catalan, English, and Spanish) by 
means of two alternative evaluation metrics, 
thus providing an insight into (i) the portabil-
ity of coreference resolution systems across 
languages, and (ii) the effect of different scor-
ing metrics on ranking the output of the par-
ticipant systems. 
1 Introduction 
Coreference information has been shown to be 
beneficial in many NLP applications such as In-
formation Extraction (McCarthy and Lehnert, 
1995), Text Summarization (Steinberger et al, 
2007), Question Answering (Morton, 2000), and 
Machine Translation. In these systems, there is a 
need to identify the different pieces of information 
that refer to the same discourse entity in order to 
produce coherent and fluent summaries, disam-
biguate the references to an entity, and solve ana-
phoric pronouns.  
Coreference is an inherently complex phenome-
non. Some of the limitations of the traditional rule-
based approaches (Mitkov, 1998) could be over-
come by machine learning techniques, which allow 
automating the acquisition of knowledge from an-
notated corpora. 
 
This task will promote the development of lin-
guistic resources ?annotated corpora1? and ma-
chine-learning techniques oriented to coreference 
resolution. In particular, we aim to evaluate and 
compare coreference resolution systems in a multi-
lingual context, including Catalan, English, and 
Spanish languages, and by means of two different 
evaluation metrics.  
By setting up a multilingual scenario, we can 
explore to what extent it is possible to implement a 
general system that is portable to the three lan-
guages, how much language-specific tuning is nec-
essary, and the significant differences between 
Romance languages and English, as well as those 
between two closely related languages such as 
Spanish and Catalan. Besides, we expect to gain 
some useful insight into the development of multi-
lingual NLP applications.  
As far as the evaluation is concerned, by em-
ploying B-cubed (Bagga and Baldwin, 1998) and 
CEAF (Luo, 2005) algorithms we can consider 
both the advantages and drawbacks of using one or 
the other scoring metric. For comparison purposes, 
the MUC score will also be reported. Among oth-
ers, we are interested in the following questions: 
Which evaluation metric provides a more accurate 
picture of the accuracy of the system performance? 
Is there a strong correlation between them? Can 
                                                           
1 Corpora annotated with coreference are scarce, especially for 
languages other than English.  
70
statistical systems be optimized under both metrics 
at the same time? 
The rest of the paper is organized as follows. 
Section 2 describes the overall task. The corpora 
and the annotation scheme are presented in Section 
3. Conclusions and final remarks are given in Sec-
tion 4. 
 
2 Task description  
The SemEval-2010 task ?Coreference Resolution 
in Multiple Languages? is concerned with auto-
matic coreference resolution for three different 
languages: Catalan, English, and Spanish.  
2.1 Specific tasks  
Given the complexity of the coreference phenom-
ena, we will concentrate only in two tractable as-
pects, which lead to the two following subtasks for 
each of the languages: 
i) Detection of full coreference chains, com-
posed by named entities, pronouns, and full 
noun phrases (NPs). 
ii) Pronominal resolution, i.e. finding the antece-
dents of the pronouns in the text.  
 
 
The example in Figure 1 illustrates the two sub-
tasks.2 Given a text in which NPs are identified and 
indexed (including elliptical subjects, represented 
as ?), the goal of (i) is to extract all coreference 
chains: 1?5?6?30?36, 9?11, and 7?18; while the 
goal of (ii) is to identify the antecedents of pro-
nouns 5 and 6, which are 1 and 5 (or 1), respec-
tively. Note that (b) is a simpler subtask of (a) and 
that for a given pronoun there can be multiple an-
tecedents (e.g. both 1 and 5 are correct antecedents 
for 6).  
We restrict the task to solving ?identity? rela-
tions between NPs (coreference chains), and be-
tween pronouns and antecedents. Nominal 
predicates and appositions as well as NPs with a 
non-nominal antecedent (discourse deixis) will not 
been taken into consideration in the recognition of 
coreference chains (see Section 3.1 for more in-
formation about decisions concerning the annota-
tion scheme). 
Although we target at general systems address-
ing the full multilingual task, we will allow taking 
part on any subtask of any language in order to 
promote participation. 
 
 
Figure 1.  NPs in a sample from the Catalan training 
data (left) and the English translation (right). 
                                                           
2 The example in Figure 1 is a simplified version of the anno-
tated format. See Section 2.2 for more details. 
[The beneficiaries of [[spouse?s]3 pensions]2]1 will 
be able to keep [the payment]4 even if [they]5 re-
marry provided that [they]6 fulfill [a series of [con-
ditions]8]7, according to [the royal decree approved 
yesterday by [the Council of Ministers]10]9.  
[The new rule]11 affects [the recipients of [a 
[spouse?s]13 pension]12 [that]14 get married after 
[January_1_,_2002]16]17. 
[The first of [the conditions]18]19 is being older 
[than 61 years old]20 or having [an officially rec-
ognized permanent disability [that]22 makes one 
disabled for [any [profession]24 or [job]25]23]21. 
[The second one]26 requires that [the pension]27 be 
[the main or only source of [the [pensioner?s]30 in-
come]29]28, and provided that [the annual amount 
of [the pension]32]31 represents, at least, [75% of 
[the total [yearly income of [the pen-
sioner]36]35]34]33. 
[Els beneficiaris de [pensions de [viudetat]3]2]1 po-
dran conservar [la paga]4 encara_que [?]5 es tornin 
a casar si [?]6 compleixen [una s?rie de [condi-
cions]8]7 , segons [el reial decret aprovat ahir pel 
[Consell_de_Ministres]10]9 .  
[La nova norma]11 afecta [els perceptors d' [una 
pensi? de [viudetat]13]12 [que]14 contreguin [matri-
moni]15 a_partir_de [l' 1_de_gener_del_2002]16]17 .  
[La primera de [les condicions]18]19 ?s tenir [m?s 
de 61 anys]20 o tenir reconeguda [una incapacitat 
permanent [que]22 inhabiliti per a [tota [professi?]24 
o [ofici]25]23]21. 
[La segona]26 ?s que [la pensi?]27 sigui [la principal 
o ?nica font d' [ingressos del [pensionista]30]29]28 , i 
sempre_que [l' import anual de [la mateixa pen-
si?]32]31 representi , com_a_m?nim , [el 75% del 
[total dels [ingressos anuals del [pensionis-
ta]36]35]34]33.  
 
71
2.2 Evaluation  
2.1.1 Input information 
The input information for the task will consist of: 
word forms, lemmas, POS, full syntax, and seman-
tic role labeling. Two different scenarios will be 
considered regarding the source of the input infor-
mation: 
 
i) In the first one, gold standard annotation will 
be provided to participants. This input annota-
tion will correctly identify all NPs that are part 
of coreference chains. This scenario will be 
only available for Catalan and Spanish. 
ii) In the second, state-of-the-art automatic lin-
guistic analyzers for the three languages will 
be used to generate the input annotation of the 
data. The matching between the automatically 
generated structure and the real NPs interven-
ing in the chains does not need to be perfect in 
this setting. 
  
By defining these two experimental settings, we 
will be able to check the performance of corefer-
ence systems when working with perfect linguistic 
(syntactic/semantic) information, and the degrada-
tion in performance when moving to a more realis-
tic scenario with noisy input annotation.  
2.1.2 Closed/open challenges 
In parallel, we will also consider the possibility of 
differentiating between closed and open chal-
lenges, that is, when participants are allowed to use 
strictly the information contained in the training 
data (closed) and when they make use of some ex-
ternal resources/tools (open). 
2.1.3 Scoring measures 
Regarding evaluation measures, we will have spe-
cific metrics for each of the subtasks, which will be 
computed by language and overall.  
Several metrics have been proposed for the task 
of coreference resolution, and each of them pre-
sents advantages and drawbacks. For the purpose 
of the current task, we have selected two of them ? 
B-cubed and CEAF ? as the most appropriate ones. 
In what follows we justify our choice.  
The MUC scoring algorithm (Vilain et al, 1995) 
has been the most widely used for at least two rea-
sons. Firstly, the MUC corpora and the MUC 
scorer were the first available systems. Secondly, 
the MUC scorer is easy to understand and imple-
ment. However, this metric has two major weak-
nesses: (i) it does not give any credit to the correct 
identification of singleton entities (chains consist-
ing of one single mention), and (ii) it intrinsically 
favors systems that produce fewer coreference 
chains, which may result in higher F-measures for 
worse systems. 
A second well-known scoring algorithm, the 
ACE value (NIST, 2003), owes its popularity to 
the ACE evaluation campaign. Each error (a miss-
ing element, a misclassification of a coreference 
chain, a mention in the response not included in the 
key) made by the response has an associated cost, 
which depends on the type of entity (e.g. person, 
location, organization) and on the kind of mention 
(e.g. name, nominal, pronoun). The fact that this 
metric is entity-type and mention-type dependent, 
and that it relies on ACE-type entities makes this 
measure inappropriate for the current task. 
The two measures that we are interested in com-
paring are B-cubed (Bagga and Baldwin, 1998) 
and CEAF (Luo, 2005). The former does not look 
at the links produced by a system as the MUC al-
gorithm does, but looks at the presence/absence of 
mentions for each entity in the system output. Pre-
cision and recall numbers are computed for each 
mention, and the average gives the final precision 
and recall numbers.  
CEAF (Luo, 2005) is a novel metric for evaluat-
ing coreference resolution that has already been 
used in some published papers (Ng, 2008; Denis 
and Baldridge, 2008). It mainly differs from B-
cubed in that it finds the best one-to-one entity 
alignment between the gold and system responses 
before computing precision and recall. The best 
mapping is that which maximizes the similarity 
over pairs of chains. The CEAF measure has two 
variants: a mention-based, and an entity-based one. 
While the former scores the similarity of two 
chains as the absolute number of common men-
tions between them, the latter scores the relative 
number of common mentions. 
Luo (2005) criticizes the fact that a response 
with all mentions in the same chain obtains 100% 
B-cubed recall, whereas a response with each men-
tion in a different chain obtains 100% B-cubed 
72
precision. However, precision will be penalized in 
the first case, and recall in the second case, each 
captured by the corresponding F-measure. Luo?s 
entity alignment might cause that a correctly iden-
tified link between two mentions is ignored by the 
scoring metric if that entity is not aligned. Finally, 
as far as the two CEAF metrics are concerned, the 
entity-based measure rewards alike a correctly 
identified one-mention entity and a correctly iden-
tified five-mention entity, while the mention-based 
measure takes into account the size of the entity. 
Given this series of advantages and drawbacks, 
we opted for including both B-cubed and CEAF 
measures in the final evaluation of the systems. In 
this way we will be able to perform a meta-
evaluation study, i.e. to evaluate and compare the 
performance of metrics with respect to the task 
objectives and system rankings. It might be inter-
esting to break B-cubed and CEAF into partial re-
sults across different kinds of mentions in order to 
get a better understanding of the sources of errors 
made by each system. Additionally, the MUC met-
ric will also be included for comparison purposes 
with previous results.  
Finally, for the setting with automatically gener-
ated input information (second scenario in Section 
2.1.1), it might be desirable to devise metric vari-
ants accounting for partial matches of NPs. In this 
case, capturing the correct NP head would give 
most of the credit. We plan to work in this research 
line in the near future.  
Official scorers will be developed in advance 
and made available to participants when posting 
the trial datasets. The period in between the release 
of trial datasets and the start of the full evaluation 
will serve as a test for the evaluation metrics. De-
pending on the feedback obtained from the partici-
pants we might consider introducing some 
improvements in the evaluation setting.  
3 AnCora-CO corpora  
The corpora used in the task are AnCora-CO, 
which are the result of enriching the AnCora cor-
pora (Taul? et al, 2008) with coreference informa-
tion. AnCora-CO is a multilingual corpus 
annotated at different linguistic levels consisting of 
400K words in Catalan3, 400K words in Spanish2, 
                                                           
3 Freely available for research purposes from the following 
URL: http://clic.ub.edu/ancora 
and 120K words in English. For the purpose of the 
task, the corpora are split into a training (85%) and 
test (15%) set. Each file corresponds to one news-
paper text.  
AnCora-CO consists mainly of newspaper and 
newswire articles: 200K words from the Spanish 
and Catalan versions of El Peri?dico newspaper, 
and 200K words from the EFE newswire agency in 
the Spanish corpus, and from the ACN newswire 
agency in the Catalan corpus. The source corpora 
for Spanish and Catalan are the AnCora corpora, 
which were annotated by hand with full syntax 
(constituents and functions) as well as with seman-
tic information (argument structure with thematic 
roles, semantic verb classes, named entities, and 
WordNet nominal senses). The annotation of 
coreference constitutes an additional layer on top 
of the previous syntactic-semantic information. 
The English part of AnCora-CO consists of a se-
ries of documents of the Reuters newswire corpus 
(RCV1 version).4 The RCV1 corpus does not come 
with any syntactic nor semantic annotation. This is 
why we only count with automatic linguistic anno-
tation produced by statistical taggers and parsers 
on this corpus. 
Although the Catalan, English, and Spanish cor-
pora used in the task all belong to the domain of 
newspaper texts, they do not form a three-way par-
allel corpus. 
3.1 Coreference annotation 
The annotation of a corpus with coreference in-
formation is highly complex due to (i) the lack of 
information in descriptive grammars about this 
topic, and (ii) the difficulty in generalizing the in-
sights from one language to another. Regarding (i), 
a wide range of units and relations occur for which 
it is not straightforward to determine whether they 
are or not coreferent. Although there are theoretical 
studies for English, they cannot always be ex-
tended to Spanish or Catalan since coreference is a 
very language-specific phenomenon, which ac-
counts for (ii). 
In the following we present some of the linguis-
tic issues more problematic in relation to corefer-
ence annotation, and how we decided to deal with 
them in AnCora-CO (Recasens, 2008). Some of 
them are language dependent (1); others concern 
                                                           
4 Reuters Corpus RCV1 is distributed by NIST at the follow-
ing URL: http://trec.nist.gov/data/reuters/reuters.html 
73
the internal structure of the mentions (2), or the 
type of coreference link (3). Finally, we present 
those NPs that were left out from the annotation 
for not being referential (4). 
 
1. Language-specific issues 
- Since Spanish and Catalan are pro-drop 
languages, elliptical subjects were intro-
duced in the syntactic annotation, and they 
are also annotated with coreference.  
- Expletive it pronouns, which are frequent 
in English and to a lesser extent in Spanish 
and Catalan are not referential, and so they 
do not participate in coreference links. 
- In Spanish, clitic forms for pronouns can 
merge into a single word with the verb; in 
these cases the whole verbal node is anno-
tated for coreference. 
2. Issues concerning the mention structure  
- In possessive NPs, only the reference of 
the thing possessed (not the possessor) is 
taken into account. For instance, su libro 
?his book? is linked with a previous refer-
ence of the same book; the possessive de-
terminer su ?his? does not constitute an NP 
on its own. 
- In the case of conjoined NPs, three (or 
more) links can be encoded: one between 
the entire NPs, and additional ones for 
each of the constituent NPs. AnCora-CO 
captures links at these different levels. 
3. Issues concerning types of coreference links 
- Plural NPs can refer to two or more ante-
cedents that appear separately in the text. 
In these cases an entity resulting from the 
addition of two or more entities is created.  
- Discourse deixis is kept under a specific 
link tag because not all coreference resolu-
tion systems can handle such relations. 
- Metonymy is annotated as a case of iden-
tity because both mentions pragmatically 
corefer. 
4. Non-referential NPs 
- In order to be linguistically accurate (van 
Deemter and Kibble, 2000), we distinguish 
between referring and attributive NPs: 
while the first point to an entity, the latter 
express some of its properties. Thus, at-
tributive NPs like apposition and predica-
tive phrases are not treated as identity 
coreference in AnCora-CO (they are kept 
distinct under the ?predicative link? tag).  
- Bound anaphora and bridging reference go 
beyond coreference and so are left out 
from consideration. 
The annotation process of the corpora is outlined in 
the next section. 
3.2 Annotation process 
The Ancora coreference annotation process in-
volves: (a) marking of mentions, and (b) marking 
of coreference chains (entities). 
(a) Referential full NPs (including proper nouns) 
and pronouns (including elliptical and clitic pro-
nouns) are the potential mentions of a coreference 
chain.  
(b) In the current task only identity relations 
(coreftype=?ident?) will be considered, which link 
referential NPs that point to the same discourse 
entity. Coreferent mentions are annotated with the 
attribute entity. Mentions that point to the same 
entity share the same entity number. In Figure 1, 
for instance, el reial decret aprovat ahir pel Con-
sell_de_Ministres ?the royal decree approved yes-
terday by the Council of Ministers? is 
entity=?entity9? and la nova norma ?the new rule? 
is also entity=?entity9? because they corefer. 
Hence, mentions referring to the same discourse 
entity all share the same entity number.  
The corpora were annotated by a total of seven 
annotators (qualified linguists) using the An-
CoraPipe annotation tool (Bertran et al, 2008), 
which allows different linguistic levels to be anno-
tated simultaneously and efficiently. AnCoraPipe 
supports XML in-line annotations.  
An initial reliability study was performed on a 
small portion of the Spanish AnCora-CO corpus. 
In that study, eight linguists annotated the corpus 
material in parallel. Inter-annotator agreement was 
computed with Krippendorff?s alpha, achieving a 
result above 0.8. Most of the problems detected 
were attributed either to a lack of training of the 
coders or to ambiguities that are left unresolved in 
the discourse itself. After carrying out this reliabil-
ity study, we opted for annotating the corpora in a 
two-stage process: a first pass in which all mention 
attributes and coreference links were coded, and a 
second pass in which the already annotated files 
were revised. 
 
74
4 Conclusions 
The SemEval-2010 multilingual coreference reso-
lution task has been presented for discussion.  
Firstly, we aim to promote research on coreference 
resolution from a learning-based perspective in a 
multilingual scenario in order to: (a) explore port-
ability issues; (b) analyze language-specific tuning 
requirements; (c) facilitate cross-linguistic com-
parisons between two Romance languages and be-
tween Romance languages and English; and (d) 
encourage researchers to develop linguistic re-
sources ? annotated corpora ? oriented to corefer-
ence resolution for other languages. 
Secondly, given the complexity of the corefer-
ence phenomena we split the coreference resolu-
tion task into two (full coreference chains and 
pronominal resolution), and we propose two dif-
ferent scenarios (gold standard vs. automatically 
generated input information) in order to evaluate to 
what extent the performance of a coreference reso-
lution system varies depending on the quality of 
the other levels of information. 
Finally, given that the evaluation of coreference 
resolution systems is still an open issue, we are 
interested in comparing different coreference reso-
lution metrics: B-cubed and CEAF measures. In 
this way we will be able to evaluate and compare 
the performance of these metrics with respect to 
the task objectives and system rankings. 
Acknowledgments 
This research has been supported by the projects 
Lang2World (TIN2006-15265-C06), TEXT-MESS 
(TIN2006-15265-C04), OpenMT (TIN2006-
15307-C03-02), AnCora-Nom (FFI2008-02691-E), 
and the FPU grant (AP2006-00994) from the Span-
ish Ministry of Education and Science, and the 
funding given by the government of the Generalitat 
de Catalunya.  
References  
Bagga, Amit and Breck Baldwin. 1998. Algorithms for 
scoring coreference chains. In Proceedings of Lan-
guage Resources and Evaluation Conference. 
Bertran, Manuel, Oriol Borrega, Marta Recasens, and 
B?rbara Soriano. 2008. AnCoraPipe: A tool for mul-
tilevel annotation, Procesamiento del Lenguaje Natu-
ral, n. 41: 291-292, SEPLN. 
Denis, Pascal and Jason Baldridge. 2008. Specialized 
models and ranking for coreference resolution. Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). 
Luo, Xiaoqiang. 2005. On coreference resolution per-
formance metrics. Proceedings of HLT/NAACL 2005. 
McCarthy Joseph and Wendy Lehnert.  1995. Using 
decision trees for coreference resolution. Proceed-
ings of the Fourteenth International Joint Conference 
on Artificial Intelligence. 
Mitkov, Ruslan. 1998. Robust pronoun resolution with 
limited knowledge. Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics, and 17th International Conference on Com-
putational Linguistics (COLING-ACL98). 
Morton, Thomas. 2000. Using coreference for question 
answering. Proceedings of the 8th Text REtrieval 
Conference (TREC-8). 
Ng, Vincent. 2008. Unsupervised models for corefer-
ence resolution. Proceedings of the Empirical Meth-
ods in Natural Language Processing (EMNLP 2008). 
NIST. 2003. In Proceedings of ACE 2003 workshop. 
Booklet, Alexandria, VA. 
Recasens, Marta. 2008. Towards Coreference Resolu-
tion for Catalan and Spanish. Master Thesis. Univer-
sity of Barcelona.  
Steinberger, Josef, Massimo Poesio, Mijail Kabadjov, 
and Karel Jezek. 2007. Two uses of anaphora resolu-
tion in summarization. Information Processing and 
Management, 43:1663?1680.  
Taul?, Mariona, Ant?nia Mart?, and Marta Recasens. 
2008. AnCora: Multilevel corpora with coreference 
information for Spanish and Catalan. In Proceedings 
of the Language Resources and Evaluation Confer-
ence (LREC 2008). 
van Deemter, Kees and Rodger Kibble. 2000. Squibs 
and Discussions: On coreferring: coreference in 
MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629-637. 
Vilain, Marc, John Burger, John Aberdeen, Dennis 
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings 
of MUC-6. 
 
75
Selectional Preferences for
Semantic Role Classification
Ben?at Zapirain?
University of the Basque Country
Eneko Agirre??
University of the Basque Country
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Mihai Surdeanu?
University of Arizona
This paper focuses on a well-known open issue in Semantic Role Classification (SRC) research:
the limited influence and sparseness of lexical features. We mitigate this problem using models
that integrate automatically learned selectional preferences (SP). We explore a range of models
based on WordNet and distributional-similarity SPs. Furthermore, we demonstrate that the SRC
task is better modeled by SP models centered on both verbs and prepositions, rather than verbs
alone. Our experiments with SP-based models in isolation indicate that they outperform a lexical
baseline with 20 F1 points in domain and almost 40 F1 points out of domain. Furthermore, we
show that a state-of-the-art SRC system extended with features based on selectional preferences
performs significantly better, both in domain (17% error reduction) and out of domain (13%
error reduction). Finally, we show that in an end-to-end semantic role labeling system we obtain
small but statistically significant improvements, even though our modified SRC model affects
only approximately 4% of the argument candidates. Our post hoc error analysis indicates that
the SP-based features help mostly in situations where syntactic information is either incorrect or
insufficient to disambiguate the correct role.
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: benat.zapirain@ehu.es.
?? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: e.agirre@ehu.es.
? UPC Campus Nord (Omega building), Jordi Girona 1?3, 08034 Barcelona, Catalonia.
E-mail: lluism@lsi.upc.edu.
? 1040 E. 4th Street, Tucson, AZ 85721. E-mail: msurdeanu@arizona.edu.
Submission received: 14 November 2011; revised submission received: 31 May 2012; accepted for publication:
15 August 2012.
doi:10.1162/COLI a 00145
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Semantic Role Labeling (SRL) is the problem of analyzing clause predicates in text by
identifying arguments and tagging them with semantic labels indicating the role they
play with respect to the predicate. Such sentence-level semantic analysis allows the
determination of who did what to whom, when and where, and thus characterizes the
participants and properties of the events established by the predicates. For instance,
consider the following sentence, in which the arguments of the predicate to send have
been annotated with their respective semantic roles.1
(1) [Mr. Smith]Agent sent [the report]Object [to me]Recipient [this morning]Temporal.
Recognizing these event structures has been shown to be important for a broad
spectrum of NLP applications. Information extraction, summarization, question
answering, machine translation, among others, can benefit from this shallow semantic
analysis at sentence level, which opens the door for exploiting the semantic relations
among arguments (Boas 2002; Surdeanu et al 2003; Narayanan and Harabagiu 2004;
Melli et al 2005; Moschitti et al 2007; Higashinaka and Isozaki 2008; Surdeanu,
Ciaramita, and Zaragoza 2011). In Ma`rquez et al (2008) the reader can find a broad
introduction to SRL, covering several historical and definitional aspects of the problem,
including also references to the main resources and systems.
State-of-the-art systems leverage existing hand-tagged corpora (Fillmore,
Ruppenhofer, and Baker 2004; Palmer, Gildea, and Kingsbury 2005) to learn supervised
machine learning systems, and typically perform SRL in two sequential steps:
argument identification and argument classification. Whereas the former is mostly a
syntactic recognition task, the latter usually requires semantic knowledge to be taken
into account. The semantic knowledge that most current systems capture from text is
basically limited to the predicates and the lexical units contained in their arguments,
including the argument head. These ?lexical features? tend to be sparse, especially
when the training corpus is small, and thus SRL systems are prone to overfit the
training data and generalize poorly to new corpora (Pradhan, Ward, and Martin 2008).
As a simplified example of the effect of sparsity, consider the following sentences
occurring in an imaginary training data set for SRL:
(2) [JFK]Patient was assassinated [in Dallas]Location
(3) [John Lennon]Patient was assassinated [in New York]Location
(4) [JFK]Patient was assassinated [in November]Temporal
(5) [John Lennon]Patient was assassinated [in winter]Temporal
All four sentences share the same syntactic structure, so the lexical features (i.e., the
words Dallas, New York, November, and winter) represent the most relevant knowledge
for discriminating between the Location and Temporal adjunct labels in learning.
1 For simplicity, in this paper we talk about arguments in the most general sense. Unless noted otherwise,
argument will refer to both core-arguments (Agent, Patient, Instrument, etc.) and adjuncts (Manner,
Temporal, Location, etc.).
632
Zapirain et al Selectional Preferences for Semantic Role Classification
The problem is that, as in the following sentences, for the same predicate, one may
encounter similar expressions with new words like Texas or December, which the
classifiers cannot match with the lexical features seen during training, and thus become
useless for classification:
(6) [Smith] was assassinated [in Texas]
(7) [Smith] was assassinated [in December]
This problem is exacerbated when SRL systems are applied to texts coming from
new domains where the number of new predicates and argument heads increases
considerably. The CoNLL-2004 and 2005 evaluation exercises on semantic role labeling
(Carreras and Ma`rquez 2004, 2005) reported a significant performance degradation
of around 10 F1 points when applied to out-of-domain texts from the Brown corpus.
Pradhan, Ward, and Martin (2008) showed that this performance degradation is
essentially caused by the argument classification subtask, and suggested the lexical
data sparseness as one of the main reasons.
In this work, we will focus on Semantic Role Classification (SRC), and we will show
that selectional preferences (SP) are useful for generalizing lexical features, helping
fight sparseness and domain shifts, and improving SRC results. Selectional preferences
try to model the kind of words that can fill a specific argument of a predicate, and
have been widely used in computational linguistics since the early days (Wilks 1975).
Both semantic classes from existing lexical resources like WordNet (Resnik 1993b) and
distributional similarity based on corpora (Pantel and Lin 2000) have been successfully
used for acquiring selectional preferences, and in this work we have used several of
those models.
The contributions of this work to the field of SRL are the following:
1. We formalize and implement a method that applies several selectional
preference models to Semantic Role Classification, introducing for the first
time the use of selectional preferences for prepositions, in addition to
selectional preferences for verbs.
2. We show that the selectional preference models are able to generalize
lexical features and improve role classification performance in a controlled
experiment disconnected from a complete SRL system. The positive effect
is consistently observed in all variants of WordNet and distributional
similarity measures and is especially relevant for out-of-domain data. The
separate learning of SPs for verbs and prepositions contributes
significantly to the improvement of the results.
3. We integrate the information of several SP models in a state-of-the-art SRL
system (SwiRL)2 and obtain significant improvements in semantic role
classification and, as a consequence, in the end-to-end SRL task. The key
for the improvement lies in the combination of the predictions provided
by SwiRL and the several role classification models based on selectional
preferences.
2 http://surdeanu.info/mihai/swirl/.
633
Computational Linguistics Volume 39, Number 3
4. We present a manual analysis of the output of the combined role
classification system. By observing a set of real examples, we categorized
and quantified the situations in which SP models tend to help role
classification. By inspecting also a set of negative cases, this analysis also
sheds light on the limitations of the current approach and identifies
opportunities for further improvements.
The use of selectional preferences for improving role classification was first pre-
sented in Zapirain, Agirre, and Ma`rquez (2009), and later extended in Zapirain et al
(2010) to a full-fledged SRC system. In the current paper, we provide more detailed
background information and details of the selectional preference models, as well as
complementary experiments on the integration in a full-fledged system. More impor-
tantly, we incorporate a detailed analysis of the output of the system, comparing it with
that of a state-of-the-art SRC system not using SPs.
The rest of the paper is organized as follows. Section 2 provides background on the
automatic acquisition of selectional preference, and its recent relation to the semantic
role labeling problem. In Section 3, the SP models investigated in this paper are ex-
plained in all their variants. The results of the SP models in laboratory conditions are
presented in Section 4. Section 5 describes the method for integrating the SP models in a
state-of-the-art SRL system and discusses the results obtained. In Section 6 the qualita-
tive analysis of the system output is presented, including a detailed discussion of several
examples. Finally, Section 7 concludes and outlines some directions for future research.
2. Background
The simplest model for generating selectional preferences would be to collect all heads
filling each role of the target predicate. This is akin to the lexical features used by current
SRL systems, and we refer to this model as the lexical model. More concretely, the
lexical model for verb-role selectional preferences consists of the list of words appearing
as heads of the role arguments of the predicate verb. This model can be extracted
automatically from the SRL training corpus using straightforward techniques. When
using this model for role classification, it suffices to check whether the head word of
the argument matches any of the words in the lexical model. The lexical model is the
baseline for our other SP models, all of which build on that model.
In order to generalize the lexical model, semantic classes can be used. Although in
principle any lexical resource listing semantic classes for nouns could be applied, most
of the literature has focused on the use of WordNet (Resnik 1993b). In the WordNet-
based model, the words occurring in the lexical model are projected over the semantic
hierarchy of WordNet, and the semantic classes which represent best those words are
selected. Given a new example, the SRC system has to check whether the new word
matches any of those semantic classes. For instance, in example sentences (2)?(5), the
semantic class <time period> covers both training examples for Temporal (i.e., November
and winter), and <geographical area> covers the examples for Location. When test
words Texas and December occur in Examples (6) and (7), the semantic classes to which
they belong can be used to tag the first as Location and the second as Temporal.
As an alternative to the use of WordNet, one can also apply automatically acquired
distributional similarity thesauri. Distributional similarity methods analyze the co-
occurrence patterns of words and are able to capture, for instance, that December is more
closely related to November than to Dallas (Grefenstette 1992). Distributional similarity is
typically used on-line (i.e., given a pair of words, their similarity is computed on the go),
634
Zapirain et al Selectional Preferences for Semantic Role Classification
but, in order to speed up its use, it has also been used to produce off-line a full thesauri,
storing, for every word, the weighted list of all outstanding similar words (Lin 1998).
In the Distributional similarity model, when test item Texas in Example (6) is to be
labeled, the higher similarity to Dallas and New York, in contrast to the lower similarity
to November and winter, would be used to label the argument with the Location role.
The automatic acquisition of selectional preferences is a well-studied topic in NLP.
Many methods using semantic classes and selectional preferences have been proposed
and applied to a variety of syntactic?semantic ambiguity problems, including syntactic
parsing (Hindle 1990; Resnik 1993b; Pantel and Lin 2000; Agirre, Baldwin, and Martinez
2008; Koo, Carreras, and Collins 2008; Agirre et al 2011), word sense disambiguation
(Resnik 1993a; Agirre and Martinez 2001; McCarthy and Carroll 2003), pronoun res-
olution (Bergsma, Lin, and Goebel 2008) and named-entity recognition (Ratinov and
Roth 2009). In addition, selectional preferences have been shown to be effective to
improve the quality of inference and information extraction rules (Pantel et al 2007;
Ritter, Mausam, and Etzioni 2010). In some cases, the aforementioned papers do not
mention selectional preferences, but all of them use some notion of preferring certain
semantic types over others in order to accomplish their respective task.
In fact, one could use different notions of semantic types. In one extreme, we would
have a small set of coarse semantic classes. For instance, some authors have used the
26 so-called ?semantic fields? used to classify all nouns in WordNet (Agirre, Baldwin,
and Martinez 2008; Agirre et al 2011). The classification could be more fine-grained, as
defined by the WordNet hierarchy (Resnik 1993b; Agirre and Martinez 2001; McCarthy
and Carroll 2003), and other lexical resources could be used as well. Other authors have
used automatically induced hierarchical word classes, clustered according to occurrence
information from corpora (Koo, Carreras, and Collins 2008; Ratinov and Roth 2009).
On the other extreme, each word would be its own semantic class, as in the lexical
model, but one could also model selectional preference using distributional similarity
(Grefenstette 1992; Lin 1998; Pantel and Lin 2000; Erk 2007; Bergsma, Lin, and Goebel
2008). In this paper we will focus on WordNet-based models that use the whole hierarchy
and on distributional similarity models, and we will use the lexical model as baseline.
2.1 WordNet-Based Models
Resnik (1993b) proposed the modeling of selectional preferences using semantic classes
from WordNet and applied the model to tackle some ambiguity issues in syntax, such
as noun-compounds, coordination, and prepositional phrase attachment. Given two
alternative structures, Resnik used selectional preferences to choose the attachment
maximizing the fitness of the head to the selectional preferences of the attachment
points. This is similar to our task, but in our case we compare the target head to the selec-
tional preference models for each possible role label (i.e., given a verb and the head of an
argument, we need to find the role with the selectional preference that fits the head best).
In Resnik?s model, he first characterizes the restrictiveness of the selectional pref-
erence of an argument position r of a governing predicate p, noted as R(p, r). For that,
given a set of classes C from the WordNet nominal hierarchies, he takes the relative en-
tropy or Kullback-Leibler distance between the prior distribution P(C) and the posterior
distribution P(C|p, r):
R(p, r) =
?
c?C
P(c|p, r)logP(c|p, r)
P(c)
(1)
635
Computational Linguistics Volume 39, Number 3
The priors can be computed from any corpora, computing frequencies of classes
and using maximum likelihood estimates. The frequencies for classes cannot be directly
observed, but they can be estimated from the lexical frequencies of the nouns under
the class, as in Equation (2). Note that in WordNet, hypernyms (?hyp? for short)
correspond to superclass relations, and therefore hyp(n) returns all superclasses of
noun n.
freq(c) =
?
{n|c?hyp(n)}
freq(n) (2)
A complication arises because of the polysemy of nouns. If each occurrence of a
noun counted once in all classes that its senses belong to, polysemous nouns would
account for more probability mass than monosemous nouns, even if they occurred the
same number of times. As a solution, the frequency of polysemous nouns is split among
its senses uniformly. For instance, the probability of the class <time period> can be
estimated according to the frequencies of nouns like November, spring, and the rest of
nouns under it. November has a single sense, so every occurrence counts as 1, but spring
has six different senses, so each occurrence should only count as 0.16. Note that with
this method we are implicitly dealing with the word sense ambiguity problem. When
encountering a polysemous noun as an argument of a verb, we record the occurrence
of all of its senses. Given enough occurrences of nouns, the classes generalizing the
intended sense of the nouns will gather more counts than competing classes. In the
example, <time period> would have 1.16 compared with 0.16 <tool> (i.e., for the metal
elastic device meaning of spring). Researchers have used this fact to perform Word Sense
Disambiguation using selectional preferences (Resnik 1993a; Agirre and Martinez 2001;
McCarthy and Carroll 2003).
The posterior probability can be computed similarly, but it takes into account occur-
rences of the nouns in the required argument position of the predicate, and thus requires
a corpus annotated with roles.
The selectional preference of a predicate p and role r for a head w0 of any potential
argument, noted as SPRes(p, r, w0), is formulated as follows:3
SPRes(p, r, w0) = max
c0?hyp(w0 )
P(c0|p, r)log P(c0|p,r)P(c0)
R(p, r)
(3)
The numerator formalizes the goodness of fit for the best semantic class c0 that
contains w0. The hypernym (i.e., superclass) of w0 yielding the maximum value is
chosen. The denominator models how restrictive the selectional preference is for p and
r, as modeled in Equation (1).
Variations of Resnik?s idea to find a suitable level of generalization have been
explored in later years. Li and Abe (1998) applied the minimum-description length
principle. Alternatively, Clark and Weir (2002) devised a procedure to decide when a
class should be preferred rather than its children.
Brockmann and Lapata (2003) compared several class-based models (including
Resnik?s selectional preferences) on a syntactic plausibility judgment task for German.
3 We slightly modified the notation of Resnik (1993b) in order to be coherent with the formulae presented
in this paper.
636
Zapirain et al Selectional Preferences for Semantic Role Classification
The models return weights for (verb, syntactic function, noun) triples, and correla-
tion with human plausibility judgment is used for evaluation. Resnik?s selectional
preference scored best among WordNet-based methods (Li and Abe 1998; Clark and
Weir 2002). Despite its earlier publication, Resnik?s method is still the most popular
representative among WordNet-based methods (Pado?, Pado?, and Erk 2007; Erk, Pado?,
and Pado? 2010; Baroni and Lenci 2010). We also chose to use Resnik?s model in this
paper.
One of the disadvantages of the WordNet-based models, compared with the distri-
butional similarity models, is that they require that the heads are present in WordNet.
This limitation can negatively influence the coverage of the model, and also its general-
ization ability.
2.2 Distributional Similarity Models
Distributional similarity models assume that a word is characterized by the words it
co-occurs with. In the simplest model, co-occurring words are taken from a fixed-size
context window. Each word w would be represented by the set of words that co-occur
with it, T(w). In a more elaborate model, each word w would be represented as a vector
of words T(w) with weights, where Ti(w) corresponds to the weight of the ith word in
the vector. The weights can be calculated following a simple frequency of co-occurrence,
or using some other formula.
Then, given two words w and w0, their similarity can be computed using any simi-
larity measure between their co-occurrence sets or vectors. For instance, early work by
Grefenstette (1992) used the Jaccard similarity coefficient of the two sets T(w) and T(w0)
(cf. Equation (4) in Figure 1). Lee (1999) reviews a wide range of similarity functions,
including Jaccard and the cosine between two vectors T(w) and T(w0) (cf. Equation (5)
in Figure 1).
In the context of lexical semantics, the similarity measure defined by Lin (1998)
has been very successful. This measure (cf. Equation (6) in Figure 1) takes into account
syntactic dependencies (d) in its co-occurrence model. In this case, the set T(w) of co-
occurrences of w contains pairs (d,v) of dependencies and words, representing the fact
simJac(w, w0) =
|T(w) ? T(w0)|
|T(w) ? T(w0)|
(4)
simcos(w, w0) =
?n
i=1
Ti(w)Ti(w0)
?
?n
i=1
Ti(w)2
?
?n
i=1
Ti(w0)2
(5)
simLin(w, w0) =
?
(d,v)?T(w)?T(w0 )(I(w, d, v) + I(w0, d, v))
?
(d,v)?T(w) I(w, d, v) +
?
(d,v)?T(w0 ) I(w0, d, v)
(6)
Figure 1
Similarity measures used in the paper. Jac and cos stand for Jaccard and cosine similarity metrics.
T(w) is the set of words co-occurring with w, Ti(w) is the weight of the ith element of the vector
of words co-occurring with w, and I(w, d, v) is the mutual information between w and d, v.
637
Computational Linguistics Volume 39, Number 3
that the corpus contains an occurrence of w having dependency d with v. For instance,
if the corpus contains John loves Mary, then the pair (ncsubj, love) would be in the set
T for John. The measure uses information-theoretic principles, and I(w, d, v) represents
the information content of the triple (Lin 1998).
Although the use of co-occurrence vectors for words to compute similarity has been
standard practice, some authors have argued for more complex uses. Schu?tze (1998)
builds vectors for each context of occurrence of a word, combining the co-occurrence
vectors for each word in the context. The vectors for contexts were used to induce
senses and to improve information retrieval results. Edmonds (1997) built a lexical co-
occurrence network, and applied it to a lexical choice task. Chakraborti et al (2007)
used transitivity over co-occurrence relations, with good results on several classification
tasks. Note that all these works use second order and higher order to refer to their method.
In this paper, we will also use second order to refer to a new method which goes beyond
the usual co-occurrence vectors (cf. Section 3.3).
A full review of distributional models is out of the scope of this paper, as we are in-
terested in showing that some of those models can be used successfully to improve SRC.
Pado? and Lapata (2007) present a review of distributional models for word similarity,
and a study of several parameters that define a broad family of distributional similarity
models, including Jaccard and Lin. They provide publicly available software,4 which
we have used in this paper, as explained in the next section. Baroni and Lenci (2010)
present a framework for extracting distributional information from corpora that can be
used to build models for different tasks.
Distributional similarity models were first used to tackle syntactic ambiguity. For
instance, Pantel and Lin (2000) obtained very good results on PP-attachment using the
distributional similarity measure defined by Lin (1998). Distributional similarity was
used to overcome sparsity problems: Alongside the counts in the training data of the
target words, the counts of words similar to the target ones were also used. Although
not made explicit, Lin was actually using a distributional similarity model of selectional
preferences.
The application of distributional selectional preferences to semantic roles (as op-
posed to syntactic functions) is more recent. Gildea and Jurafsky (2002) are the only ones
applying selectional preferences in a real SRL task. They used distributional clustering
and WordNet-based techniques on a SRL task on FrameNet roles. They report a very
small improvement of the overall performance when using distributional clustering
techniques. In this paper we present complementary experiments, with a different role
set and annotated corpus (PropBank), a wider range of selectional preference models,
and the analysis of out-of-domain results.
Other papers applying semantic preferences in the context of semantic roles rely on
the evaluation of artificial tasks or human plausibility judgments. Erk (2007) introduced
a distributional similarity?based model for selectional preferences, reminiscent of that
of Pantel and Lin (2000). Her approach models the selectional preference SPsim(p, r, w0)
of an argument position r of governing predicate p for a possible head-word w0 as
follows:
SPsim(p, r, w0) =
?
w?Seen(p,r)
sim(w0, w) ? weight(p, r, w) (7)
4 http://www.coli.uni-saarland.de/?pado/dv/dv.html.
638
Zapirain et al Selectional Preferences for Semantic Role Classification
where sim(w0, w) is the similarity between the seen and potential heads, Seen(p, r) is the
set of heads of role r for predicate p seen in the training data set (as in the lexical model),
and weight(p, r, w) is the weight of the seen head word w. Our distributional model for
selectional preferences follows her formalization.
Erk instantiated the basic model with several corpus-based distributional similarity
measures, including Lin?s similarity, Jaccard, and cosine (Figure 1) among others, and
several implementations of the weight function such as the frequency. The quality of
each model instantiation, alongside Resnik?s model and an expectation maximization
(EM)-based clustering model, was tested in a pseudo-disambiguation task where the
goal was to distinguish an attested filler of the role and a randomly chosen word. The
results over 100 frame-specific roles showed that distributional similarities attain similar
error rates to Resnik?s model but better than EM-based clustering, with Lin?s formula
having the smallest error rate. Moreover, the coverage of distributional similarity mea-
sures was much better than Resnik?s. In a more recent paper, Erk, Pado?, and Pado? (2010)
extend the aforementioned work, including evaluation to human plausibility judgments
and a model for inverse selectional preferences.
In this paper we test similar techniques to those presented here, but we evaluate
selectional preference models in a setting directly related to semantic role classification,
namely, given a selectional preference model for a verb we find the role which fits
best the given head word. The problem is indeed qualitatively different from previous
work in that we do not have to choose among the head words competing for a role but
among selectional preferences of roles competing for a head word.
More recent work on distributional selectional preference has explored the use of
discriminative models (Bergsma, Lin, and Goebel 2008) and topical models (O? Se?aghdha
2010; Ritter, Mausam, and Etzioni 2010). These models would be a nice addition to those
implemented in this paper, and if effective, they would improve further our results with
respect to the baselines which don?t use selectional preferences.
Contrary to WordNet-based models, distributional preferences do not rely on a
hand-built resource. Their coverage and generalization ability depend on the corpus
from which the distributional similarity model was computed. This fact makes this
approach more versatile in domain adaptation scenarios, as more specific and test-set
focused generalization corpora could be used to modify, enrich, or even replace the
original corpus.
2.3 PropBank
In this work we use the semantic roles defined in PropBank. The Proposition Bank
(Palmer, Gildea, and Kingsbury 2005) emerged as a primary resource for research in
SRL. It provides semantic role annotation for all verbs in the Penn Treebank corpus.
PropBank takes a ?theory-neutral? approach to the designation of core semantic roles.
Each verb has a frameset listing its allowed role labelings in which the arguments are
designated by number (starting from 0). Each numbered argument is provided with an
English language description specific to that verb. The most frequent roles are Arg0 and
Arg1 and, generally, Arg0 stands for the prototypical agent and Arg1 corresponds to the
prototypical patient or theme of the proposition. The rest of arguments (Arg2 to Arg5)
do not generalize across verbs, that is, they have verb specific interpretations.
Apart from the core numbered roles, there are 13 labels to designate adjuncts:
AM-ADV (general-purpose), AM-CAU (cause), AM-DIR (direction), AM-DIS (dis-
course marker), AM-EXT (extent), AM-LOC (location), AM-MNR (manner), AM-MOD
639
Computational Linguistics Volume 39, Number 3
Table 1
Example of verb-role lexical SP models for write, listed in alphabetical order. Number of heads
indicates the number of head words attested, Unique heads indicates the number of distinct
head words attested, and Examples lists some of the heads in alphabetical order.
Verb-role Number of Unique Examples
heads heads
write-Arg0 98 84 Angrist anyone baker ball bank Barlow Bates ...
write-Arg1 97 69 abstract act analysis article asset bill book ...
write-Arg2 7 7 bank commander hundred jaguar Kemp member ...
write-AM-LOC 2 2 paper space
write-AM-TMP 1 1 month
(modal verb), AM-NEG (negation marker), AM-PNC (purpose), AM-PRD (predication),
AM-REC (reciprocal), and AM-TMP (temporal).
3. Selectional Preference Models for Argument Classification
Our approach for applying selectional preferences to semantic role classification is
discriminative. That is, the SP-based models provide a score for every possible role
label given a verb (or preposition), the head word of the argument, and the selectional
preferences for the verb (or preposition). These scores can be used to directly assign the
most probable role or to codify new features to train enriched semantic role classifiers.
In this section we first present all the variants for acquiring selectional preferences
used in our study, and then present the method to apply them to semantic role classifi-
cation. We selected several variants that have been successful in some previous works.
3.1 Lexical SP Model
In order to implement the lexical model we gathered all heads w of arguments filling
a role r of a predicate p and obtained freq(p, r, w) from the corresponding training data
(cf. Section 4.1). Table 1 shows a sample of the heads of arguments attested in the
corpus for the verb write. The lexical SP model can be simply formalized as follows:
SPlex(p, r, w0) = freq(p, r, w0) (8)
3.2 WordNet-Based SP Models
We instantiated the model based on (Resnik 1993b) presented in the previous sec-
tion (SPRes, cf. Equation (3)) using the implementation of Agirre and Martinez (2001).
Tables 2 and 3 show the synsets5 that generalize best the head words in Table 1
for write-Arg0 and write-Arg1, according to the weight assigned to those synsets by
Equation (1). According to this model, and following basic intuition, the words attested
as being Arg0s of write are best generalized by semantic classes such as living things,
5 The WordNet terminology for concepts is synset. In this paper we use concept, synset, and semantic class
interchangeably.
640
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 2
Excerpt from the selectional preferences for write-Arg0 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00002086 5.875 life form organism being living thing any living entity
n#00001740 5.737 entity something anything having existence (living or nonliving)
n#00009457 4.782 object physical object a physical (tangible and visible) entity;
n#00004123 4.351 person individual someone somebody mortal human soul
a human being;
Table 3
Excerpt from the selectional preferences for write-Arg1 according to SPRes, showing the synsets
that generalize best the head words in Table 1. Weight lists the weight assigned to those synsets
by Equation (1). Description includes the words and glosses in the synset.
Synset Weight Description
n#00019671 7.956 communication something that is communicated between people
or groups
n#04949838 4.257 message content subject matter substance what a communication
that . . .
n#00018916 3.848 relation an abstraction belonging to or characteristic of two entities
n#00013018 3.574 abstraction a concept formed by extracting common features
from examples
entities, physical objects, and human beings, whereas Arg1s by communication, mes-
sage, relation, and abstraction.
Resnik?s method performs well among Wordnet-based methods, but we realized
that it tends to overgeneralize. For instance, in Table 2, the concept for ?entity? (one of
the unique beginners of the WordNet hierarchy) has a high weight. This means that a
head like ?grant? would be assigned Arg0. In fact, any noun which is under concept
n#00001740 (entity) but not under n#04949838 (message) would be assigned Arg0. This
observation led us to speculate on an alternative method which would try to generalize
as little as possible.
Our intuition is that general synsets can fit several selectional preferences at the
same time. For instance, the <entity> class, as a superclass of most words, would be a
correct generalization for the selectional preferences of all agent, patient, and instrument
roles of a predicate like break. On the contrary, specific concepts are usually more useful
for characterizing selectional preferences, as in the <tool> class for the instrument role
of break. The priority of using specific synsets over more general ones is, thus, justified
in the sense that they may better represent the most relevant semantic characteristics of
the selectional preferences.
The alternative method (SPwn) is based on the depth of the concepts in the WordNet
hierarchy and the frequency of the nouns. The use of the depth in hierarchies to model
the specificity of concepts (the deeper the more specific) is not new (Rada et al 1989;
Sussna 1993; Agirre and Rigau 1996). Our method tries to be conservative with respect
to generalization: When we check which SP is a better fit for a given target head, we
always prefer the SP that contains the most specific generalization for the target head
(the lowest synset which is a hypernym of the target word).
641
Computational Linguistics Volume 39, Number 3
Table 4
Excerpt from the selectional preferences for write-Arg0 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#01967203 9 1 humanoid human being any living or extinct member of the . . .
n#07603319 8 1 spy undercover agent a secret agent hired by a state to . . .
n#07151308 8 1 woman a human female who does housework
n#06183656 8 1 Federal Reserve the central bank of the US
Table 5
Excerpt from the selectional preferences for write-Arg1 according to SPwn, showing from deeper
to shallower the synsets in WordNet which are connected to head words in Table 1. Depth lists
the depth of synsets in WordNet. Description includes the words and glosses in the synset.
Synset Depth Freq. Description
n#05403815 13 1 information formal accusation of a crime
n#05401516 12 1 accusation accusal a formal charge of wrongdoing brought . . .
n#04925620 11 1 charge complaint a pleading describing some wrong or offense
n#04891230 11 1 memoir an account of the author?s personal experiences
More concretely, we model selectional preferences as a multiset6 of synsets, storing
all hypernyms of the heads seen in the training data for a certain role of a given
predicate, that is:
Smul(p, r) =
?
w?Seen(p,r)
hyp(w) (9)
where Seen(p, r) are all the argument heads for predicate p and role r, and hyp(w) returns
all the synsets and hypernyms of w, including hypernyms of hypernyms recursively up
to the top synsets.
For any given synset s, let d(s) be the depth of the synset in the WordNet hierarchy,
and let 1Smul(p,r)(s) be the multiplicity function which returns how many times s is con-
tained in the multiset Smul(p, r). We define a partial order among synsets a, b ? Smul(p, r)
as follows: ord(a) > ord(b) iff d(a) > d(b) or d(a) = d(b) ? 1Smul(p,r)(a) > 1Smul(p,r)(b).
Tables 4 and 5 show the most specific synsets (according to their depth) for write-Arg0
and write-Arg1.
We can then measure the goodness of fit of the selectional preference for a word as
the rank in the partial order of the first hypernym of the head that is also present in the
selectional preference. For that, we introduce SPwn(p, r, w), which following the previous
notation is defined as:
SPwn(p, r, w) = arg max
s?hyp(w)?Smul(p,r)
ord(s) (10)
6 Multisets are similar to sets, but allow for repeated members.
642
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 6
Most similar words for Texas and December according to Lin (1998).
Texas Florida 0.249, Arizona 0.236, California 0.231, Georgia 0.221, Kansas 0.217,
Minnesota 0.214, Missouri 0.214, Michigan 0.213, Colorado 0.208, North
Carolina 0.207, Oklahoma 0.207, Arkansas 0.205, Alabama 0.205, Nebraska
0.201, Tennessee 0.197, New Jersey 0.194, Illinois 0.189, Virginia 0.188,
Kentucky 0.188, Wisconsin 0.188, Massachusetts 0.184, New York 0.183
December June 0.341, October 0.340, November 0.333, April 0.330, February 0.329,
September 0.328, July 0.323, January 0.322, August 0.317, may 0.305, March
0.250, Spring 0.147, first quarter 0.135, mid-December 0.131, month 0.130,
second quarter 0.129, mid-November 0.128, fall 0.125, summer 0.125,
mid-October 0.121, autumn 0.121, year 0.121, third quarter 0.119
In case of ties, the role coming first in alphabetical order would be returned. Note that,
similar to the Resnik model (cf. Section 2.1), this model implicitly deals with the word
ambiguity problem.
As with any other approximation to measure specificity of concepts, the use of
depth has some issues, as some deeply rooted stray synsets would take priority. For
instance, Table 4 shows that synset n#01967203 for human being is the deepest synset. In
practice, when we search the synsets of a target word in the SPwn models following Eq.
(10), the most specific synsets (specially stray synsets) are not found, and synsets higher
in the hierarchy are used.
3.3 Distributional SP Models
All our distributional SP models are based on Equation (7). We have used several vari-
ants for sim(w0, w), as presented subsequently, but in all cases, we used the frequency
freq(p, r, w) as the weight in the equation. Given the availability of public resources for
distributional similarity, rather than implementing sim(w0, w) afresh we used (1) the pre-
compiled similarity measures by Lin (1998),7 and (2) the software for semantic spaces
by Pado? and Lapata (2007).
In the first case, Lin computed the similarity numbers for an extensive vocabulary
based on his own similarity formula (cf. Equation (6) in Figure 1) run over a large
parsed corpus comprising journalism texts from different sources: WSJ (24 million
words), San Jose Mercury (21 million words) and AP Newswire (19 million words).
The resource includes, for each word in the vocabulary, its most similar words with
the similarity weight. In order to get the similarity for two words, we can check the
entry in the thesaurus for either word. We will refer to this similarity measure as
simpreLin. Table 6 shows the most similar words for Texas and December according to this
resource.
For the second case, we applied the software to the British National Corpus to
extract co-occurrences, using the optimal parameters as described in Pado? and Lapata
(2007, page 179): word-based space, medium context, log-likelihood association, and
7 http://www.cs.ualberta.ca/?lindek/downloads.htm.
643
Computational Linguistics Volume 39, Number 3
Table 7
Summary of distributional similarity measures used in this work.
Similarity measure Source
simcos cosine BNC
simJac Jaccard BNC
simLin Lin BNC
simpreLin Lin Pre-computed
simpreLin?cos cosine (2nd order) Pre-computed
simpreLin?Jac Jaccard (2nd order) Pre-computed
2,000 basis elements. We tested Jaccard, cosine, and Lin?s measure for similarity, yielding
simJac, simcos, and simLin, respectively.
In addition to measuring the similarity of two words directly, that is, using the co-
occurrence vectors of each word as in Section 2, we also tried a variant which we will
call second-order similarity. In this case each word is represented by a vector which
contains all similar words with weights, where those weights come from first order
similarity. That is, in order to obtain the second-order vector for word w, we need to
compute its first order similarity with all other words in the vocabulary. The second-
order similarity of two words is then computed according to those vectors. For this, we
just need to change the definition of T and T in the similarity formulas in Figure 1: Now
T(w) would return the list of words which are taken to be similar to w, and T(w) would
return the same list but as a vector with weights.
This approximation is computationally expensive, as we need to compute the
square matrix of similarities for all word pairs in the vocabulary, which is highly time-
consuming. Fortunately, the pre-computed similarity scores of Lin (1998) (which use
simLin) are readily available, and thus the second-order similarity vectors can be easily
computed. We used Jaccard and cosine to compute the similarity of the vectors, and we
will refer to these similarity measures as simpreLin?Jac and sim
pre
Lin?cos hereinafter. Due to the
computational complexity, we did not compute second order similarity for the semantic
space software of Pado? and Lapata (2007).
Table 7 summarizes all similarity measures used in this study, and the corpus or
pre-computed similarity list used to build them.
3.4 Selectional Preferences for Prepositions
All the previously described models have been typically applied to verb-role selectional
preferences for NP arguments. Applying them to general semantic role labeling may
not be straightforward, however, and may require some extensions and adaptations.
For instance, not all argument candidates are noun phrases. Common arguments with
other syntactic types include prepositional, adjectival, adverbial, and verb phrases. Any
candidate argument without a nominal head cannot be directly treated by the models
described so far.
644
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 8
Example of prep-role lexical models for the preposition from, listed in alphabetical order.
Prep-role Number of Unique Examples
heads heads
from-Arg0 32 30 Abramson agency association barrier cut ...
from-Arg1 173 118 accident ad agency appraisal arbitrage ...
from-Arg2 708 457 academy account acquisition activity ad ...
from-Arg3 396 165 activity advertising agenda airport ...
from-Arg4 5 5 europe Golenbock system Vizcaya west
from-AM-ADV 19 17 action air air conception datum everyone ...
from-AM-CAU 5 4 air air design experience exposure
from-AM-DIR 79 71 agency alberta amendment america arson ...
from-AM-LOC 20 17 agency area asia body bureau orlando ...
from-AM-MNR 29 28 agency Carey company earnings floor ...
from-AM-TMP 33 21 april august beginning bell day dec. half ...
A particularly interesting case is that of prepositional phrases.8 Prepositions define
relations between the preposition attachment point and the preposition complement.
Prepositions are ambiguous with respect to these relations, which allows us to talk
about preposition senses. The Preposition Project (Litkowski and Hargraves 2005, 2006)
is an effort that produced a detailed sense inventory for English prepositions, which
was later used in a preposition sense disambiguation task at SemEval-2007 (Litkowski
and Hargraves 2007). Sense labels are defined as semantic relations, similar to those of
semantic role labels. In a more recent work, Srikumar and Roth (2011) presented a joint
model for extended semantic role labeling in which they show that determining the
sense of the preposition is mutually related to the task of labeling the argument role of
the prepositional phrase. Following the previous work, we also think that prepositions
define implicit selectional preferences, and thus decided to explore the use of preposi-
tional preferences with the aim of improving the selection of the appropriate semantic
roles. Addressing other arguments with non-nominal heads has been intentionally left
for further work.
The most straightforward way of including prepositional information in SP models
would be to add the preposition as an extra parameter of the SP. Initial experiments
revealed sparseness problems with collecting the ?verb, preposition, NP-head, role?
4-tuples from the training set. A simpler approach consists of completely disregarding
the verb information while collecting the prepositional preferences. That is, the selec-
tional preference for a preposition p and role r is defined as the union of all nouns w
found as heads of noun phrases embedded in prepositional phrases headed by p and
labeled with semantic role r. Then, one can apply any of the variants described in the
previous sections to calculate SP(p, r, w). Table 8 shows a sample of the lexical model for
the preposition from, organized according to the roles it plays.
These simple prep-role preferences largely avoided the sparseness problem while
still being able to capture relevant information to distinguish the appropriate roles in
many PP arguments. In particular, they proved to be relevant to distinguish between
adjuncts of the type ?[in New York]Location? vs. ?[in Winter]Temporal.? Nonetheless, we
8 Prepositional phrase is the second most frequent type of syntactic constituent for semantic arguments
(13%), after noun phrases (45%).
645
Computational Linguistics Volume 39, Number 3
are aware that not taking into account verb information also introduces some lim-
itations. In particular, the simplification could damage the performance on PP core
arguments, which are verb-dependent.9 For instance, our prepositional preferences
would not be able to suggest appropriate roles for the following two PP arguments:
?increase [ from seven cents a share]Arg3? and ?receive [ from the funds]Arg2,? because
the two head nouns (cents and funds) are semantically very similar. Assigning the
correct roles in these cases clearly depends on the information carried by the verbs.
Arg3 is the starting point for the predicate increase, whereas Arg2 refers to the source for
receive.
Our perspective on making this simple definition of prep-role SPs was practical and
just a starting point to play with the argument preferences introduced by prepositions.
A more complex model, distinguishing between prepositional phrases in adjunct and
core argument positions, should be able to model the linguistics better yet aleviate the
sparseness problem, and would hopefully produce better results.
The combination scheme for applying verb-role and prep-role is also very simple.
Depending on the syntactic type of the argument we apply one or the other model, both
in learning and testing:
 When the argument is a noun phrase, we use verb-role selectional
preferences.
 When the argument is a prepositional phrase, we use prep-role
selectional preferences.
We thus use a straightforward method to combine both kinds of SPs. More complex
possibilities like doing mixtures of both SPs are left for future work.
3.5 Role Classification with SP Models
Selectional preference models can be directly used to perform role classification. Given
a target predicate p and noun phrase candidate argument with head w, we simply select
the role r of the predicate which best fits the head according to the SP model. This
selection rule is formalized as:
ROLE(p, w) = arg max
r?Roles(p)
SP(p, r, w) (11)
with Roles(p) being the set of all roles applicable to the predicate p, and SP(p, r, w)
the goodness of fit of the selectional preference model for the head w, which can be
instantiated with all the variants mentioned in the previous subsections, including
the lexical model (Equation (8)) WordNet-based SP models (Equations (3) and (10)),
and distributional SP models (Equation (7)), using different similarity models as in
Table 7. Ties were broken returning the role coming first according to alphabetical
order. Note that in the case of SPwn (Equation 10) we need to use arg min rather than
arg max.
9 The percentage of prepositional phrases in core argument position is 48%, slightly lower than in adjunct
position (52%).
646
Zapirain et al Selectional Preferences for Semantic Role Classification
Note that if the candidate argument is a prepositional phrase with preposition p?
and embedded NP head word w, the classification rule uses the prep-role SP model,
that is:
ROLE(p, p?, w) = arg max
r?Roles(p? )
SP(p?, r, w)
4. Experiments with Selectional Preferences in Isolation
In this section we evaluate the ability of selectional preference models to discriminate
among different roles. For that, SP models will be used in isolation, according to the clas-
sification rule in Equation (11), to predict role labels for a set of (predicate, argument-head)
pairs. That is, we are interested in the discriminative power of the semantic information
carried by the SPs, factoring out any other feature commonly used by the state-of-the-
art SRL systems. The data sets used and the experimental results are presented in the
following.
4.1 Data Sets
The data used in this work are the benchmark corpus provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez 2005). The data set, of over 1 million tokens,
comprises PropBank Sections 02?21 for training, and Sections 24 and 23 for develop-
ment and testing, respectively. The Selectional Preferences implemented in this study
are not able to deal with non-nominal argument heads, such us those of NEG, DIS,
MOD (i.e., SPs never predict NEG, DIS, or MOD roles); but, in order to replicate the
same evaluation conditions of typical PropBank-based SRL experiments all arguments
are evaluated. That is, our SP models don?t return any prediction for those, and the
evaluation penalizes them accordingly.
The predicate?role?head triples (p, r, w) for generalizing the selectional preferences
are extracted from the arguments of the training set, yielding 71,240 triples, from which
5,587 different predicate-role selectional preferences (p, r) are derived by instantiating
the different models in Section 3. Tables 9 and 10 show additional statistics about some
of the most (and least) frequent verbs and prepositions in these tuples.
The test set contains 4,134 pairs (covering 505 different predicates) to be classified
into the appropriate role label. In order to study the behavior on out-of-domain data,
we also tested on the PropBanked part of the Brown corpus (Marcus et al 1994). This
corpus contains 2,932 (p, w) pairs covering 491 different predicates.
4.2 Results
The performance of each selectional preference model is evaluated by calculating
the customary precision (P), recall (R), and F1 measures.10 For all experiments re-
ported in this paper, we checked for statistical significance using bootstrap resampling
(100 samples) coupled with one-tailed paired t-test (Noreen 1989). We consider a result
significantly better than another if it passes this test at the 99% confidence interval.
10 P = Correct/Predicted ? 100, R = Correct/Gold ? 100, where Correct is the number of correct predictions,
Predicted is the number of predictions, and Gold is the total number of gold annotations.
F1 = 2PR/(P + R) is the harmonic mean of P and R.
647
Computational Linguistics Volume 39, Number 3
Table 9
Statistics of the three most and least frequent verbs in the training set. Role frame lists the types
of arguments seen in training for each verb; Heads indicates the total number of arguments for
the verb; Heads per role shows the average number of head words for each role; and Unique
heads per role lists the average number of unique head words for each verb?s role.
Verb Role frame Heads Heads Unique heads
per role per role
say Arg0,Arg1,Arg3,AM-ADV, AM-LOC, 7,488 1,069 371
AM-MNR, AM-TMP, AM-LOC,AM-MNR
have Arg0,Arg1,AM-ADV,AM-LOC 3,487 498 189
AM-MNR,AM-NEG,AM-TMP
make Arg0,Arg1,Arg2,AM-ADV 2,207 315 143
AM-LOC,AM-MNR,AM-TMP
... ... ... ... ...
accrete Arg1 1 1 1
accede Arg0 1 1 1
absolve Arg0 1 1 1
Table 10
Statistics of the three most and least frequent prepositions in the training set. Role frame lists
the types of arguments seen in training for each preposition; Heads indicates the total number
of arguments for the preposition; Heads per role shows the average number of head words for
each role; and Unique heads per role lists the average number of unique head words for each
preposition?s role.
Preposition Role frame Heads Heads Unique heads
per role per role
in Arg0,Arg1,Arg2,Arg3,Arg4,Arg5 6,859 403 81
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-NEG,
AM-PNC,AM-PRD,AM-TMP
to Arg0,Arg1,Arg2,Arg3,Arg4, 3,495 233 94
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-EXT,AM-LOC,AM-MNR,AM-PNC,
AM-PRD,AM-TMP
for Arg0,Arg1,Arg2,Arg3,Arg4, 2,935 225 74
AM-ADV,AM-CAU,AM-DIR,AM-DIS,
AM-LOC,AM-MNR,AM-PNC,AM-TMP
... ... ... ... ...
beside Arg2, AM-LOC 2 1 1
atop Arg2, AM-DIR 2 1 1
aboard AM-LOC 1 1 1
Tables 11 and 12 list the results of the various selectional preference models in
isolation. Table 11 shows the results for verb-role SPs, and Table 12 lists the results
for the combination of verb-role and preposition-role SPs as described in Section 3.4.11
It is worth noting that the results of Tables 11 and 12 are calculated over exactly the
11 Note that the results reported here are not identical to those we reported in Zapirain, Agirre, and
Ma`rquez (2009). The differences are two-fold: (a) in our previous experiments we discarded roles such
as MOD, DIS, and NEG, whereas here we evaluate on all roles, and (b) our previous work used only the
subset of the data that could be mapped to VerbNet (around 50%), whereas here we inspect all tuples.
648
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 11
Results for verb-role SPs in the development partition of WSJ, the test partition of WSJ, and the
Brown corpus. For each experiment, we show precision (P), recall (R), and F1. Values in boldface
font are the highest in the corresponding column. F1 values marked with ? are significantly
lower than the highest F1 score in the same column.
Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 73.94 21.81 33.69? 70.75 26.66 39.43? 59.39 05.51 10.08?
SPRes 43.65 35.70 39.28? 45.07 37.11 40.71? 36.34 27.58 31.33?
SPwn 53.09 43.35 47.73? 55.44 45.58 50.03? 41.76 31.58 35.96?
SPsimLin 53.88 44.35 48.65? 52.27 45.13 48.66? 48.30 32.08 38.56?
SPsimJac 48.40 45.53 46.92? 48.85 46.38 47.58? 42.10 34.34 37.82?
SPsimcos 52.37 49.26 50.77? 53.13 50.44 51.75? 43.24 35.27 38.85?
SPsimpreLin
60.29 59.54 59.91 59.93 59.38 59.65 50.79 48.39 49.56
SPsimpreLin?Jac
60.56 56.97 58.71 61.76 58.63 60.16 51.97 42.39 46.69?
SPsimpreLin?cos
60.22 56.64 58.37 61.12 58.12 59.63 51.92 42.35 46.65?
Table 12
Results for combined verb-role and prep-role SPs in the development partition of WSJ, the test
partition of WSJ, and the Brown corpus. For each experiment, we show precision (P), recall (R),
and F1. Values in boldface font are the highest in the corresponding column. F1 values marked
with ? are significantly lower from the highest F1 score in the same column.
Preposition-role and Verb-role SPs
Development WSJ Test Brown Test
P R F1 P R F1 P R F1
lexical 82.05 39.17 53.02? 82.98 43.77 57.31? 68.47 13.60 22.69?
SPRes 63.72 53.09 57.93? 63.47 53.24 57.91? 55.12 44.15 49.03?
SPwn 71.72 59.68 65.15? 65.70 63.88 64.78? 60.08 48.10 53.43?
SPsimLin 63.84 54.58 58.85? 63.75 56.40 59.85? 54.27 39.96 46.04?
SPsimJac 61.75 61.13 61.44? 61.83 61.40 61.61? 55.42 53.45 54.42?
SPsimcos 64.81 64.17 64.49? 64.67 64.22 64.44? 56.56 54.54 55.53?
SPsimpreLin
67.78 67.10 67.44? 68.34 67.87 68.10? 58.43 56.35 57.37?
SPsimpreLin?Jac
69.90 69.20 69.55 70.82 70.33 70.57 62.37 60.15 61.24
SPsimpreLin?cos
69.47 68.78 69.12 70.28 69.80 70.04 62.36 60.14 61.23
same example set. PP arguments are treated by the verb-role SPs by just ignoring the
preposition and considering the head noun of the NP immediately embedded in the PP.
It is worth mentioning that none of the SP models is able to predict the role when
facing a head word missing from the model. This is especially noticeable in the lexical
model, which can only return predictions for words seen in the training data and is
649
Computational Linguistics Volume 39, Number 3
penalized in recall. WordNet based models, which have a lower word coverage com-
pared to distributional similarity?based models, are also penalized in recall.
In both tables, the lexical row corresponds to the baseline lexical match method.
The following rows correspond to the WordNet-based selectional preference models.
The distributional models follow, including the results obtained by the three similarity
formulas on the co-occurrences extracted from the BNC (simJac, simcos simLin), and the
results obtained when using Lin?s pre-computed similarities directly (simpreLin) and as a
second-order vector (simpreLin?Jac and sim
pre
Lin?cos).
First and foremost, this experiment proves that splitting SPs into verb- and
preposition-role SPs yields better results. The comparison of Tables 11 and 12 shows
that the improvements are seen for both precision and recall, but especially remarkable
for recall. The overall F1 improvement is of up to 10 points. Unless stated otherwise, the
rest of the analysis will focus on Table 12.
As expected, the lexical baseline attains a very high precision in all data sets, which
underscores the importance of the lexical head word features in argument classification.
Its recall is quite low, however, especially in Brown, confirming and extending Pradhan,
Ward, and Martin (2008), who also report a similar performance drop for argument
classification on out-of-domain data. All our selectional preference models improve
over the lexical matching baseline in recall, with up to 24 absolute percentage points
in the WSJ test data set and 47 absolute percentage points in the Brown corpus. This
comes at the cost of reduced precision, but the overall F-score shows that all selectional
preference models are well above the baseline, with up to 13 absolute percentage
points on the WSJ data sets and 39 absolute percentage points on the Brown data set.
The results, thus, show that selectional preferences are indeed alleviating the lexical
sparseness problem.12
As an example, consider the following head words of potential arguments of the
verb wear found in the test set: doctor, men, tie, shoe. None of these nouns occurred as
heads of arguments of wear in the training data, and thus the lexical feature would
be unable to predict any role for them. Using selectional preferences, we successfully
assigned the A0 role to doctor and men, and the A1 role to tie and shoe.
Regarding the selectional preference variants, WordNet-based and first-order distri-
butional similarity models attain similar levels of precision, but the former have lower
recall and F1. The performance loss on recall can be explained by the limited lexical
coverage of WordNet when compared with automatically generated thesauri. Examples
of words missing in WordNet include abbreviations (e.g., Inc., Corp.) and brand names
(e.g., Texaco, Sony).
The comparison of the WordNet-based models indicates that our proposal for a
lighter method of WordNet-based selectional preference was successful, as our simpler
variant performs better than Resnik?s method. In manual analysis, we realized that
Resnik?s model tends to always predict the most frequent roles whereas our model
covers a wider role selection. Resnik?s tendency to overgeneralize makes more frequent
roles cover all the vocabulary, and the weighting system penalizes roles with fewer
occurrences.
12 We verified that the lexical model shows higher classification accuracy than all the more elaborate SP
models on the subset of cases covered by both the lexical and the SP models. In this situation, if we aimed
at constructing the best role classifier with SPs alone we could devise a back-off strategy, in the style of
Chambers and Jurafsky (2010), which uses the predictions of the lexical model when present and one of
the SP models if not. As presented in Section 5, however, our main goal is to integrate these SP models
in a real end-to-end SRL system, so we keep their analysis as independent predictors for the moment.
650
Zapirain et al Selectional Preferences for Semantic Role Classification
The results for distributional models indicate that the SPs using Lin?s ready-made
thesaurus (simpreLin) outperforms Pado? and Lapata?s distributional similarity model (Pado?
and Lapata 2007) calculated over the BNC (simLin) in both Tables 11 and 12. This might
be due to the larger size of the corpus used by Lin, but also by the fact that Lin used a
newspaper corpus, compared with the balanced BNC corpus. Further work would be
needed to be more conclusive, and, if successful, could improve further the results of
some SP models.
Among the three similarity metrics using Pado? and Lapata?s software, the cosine
seems to perform consistently better. Regarding the comparison between first-order and
second-order using pre-computed similarity models, the results indicate that second-
order is best when using both the verb-role and prep-role models (cf. Table 12), although
the results for verb-roles are mixed (cf. Table 11). Jaccard seems to provide slightly better
results than cosine for second-order vectors.
In summary, the use of separate verb-role and prep-role models produces the best
results, and second-order similarity is highly competitive. As far as we know, this is
the first time that prep-role models and second-order models are applied to selectional
preference modeling.
5. Semantic Role Classification Experiments
In this section we advance the use of SP in SRL one step further and show that selec-
tional preferences are able to effectively improve performance of a state-of-the-art SRL
system. More concretely, we integrate the information of selectional preference models
in a SRL system and show significant improvements in role classification, especially
when applied to out-of-domain corpora.13
We will use some of the selectional preference models presented in the previous
section. We will focus on the combination of verb-role and prep-role models. Regarding
the similarity models, we will choose the best two performing models from each of
the three families that we tried, namely, the two WordNet models, the two best models
based on the BNC corpus (simJac,simcos), and the two best models based on Lin?s precom-
puted similarity metrics (sim2Jac,sim
2
cos). We left the exploration of other combinations for
future work.
5.1 Integrating Selectional Preferences in Role Classification
For these experiments, we modified the SwiRL SRL system, a state-of-the-art semantic
role labeling system (Surdeanu et al 2007). SwiRL ranked second among the systems
that did not implement model combination at the CoNLL-2005 shared task and fifth
overall (Carreras and Ma`rquez 2005). Because the focus of this section is on role classi-
fication, we modified the SRC component of SwiRL to use gold argument boundaries,
that is, we assume that semantic role identification works perfectly. Nevertheless, for a
realistic evaluation, all the features in the role classification model are generated using
actual syntactic trees generated by the Charniak parser (Charniak 2000).
The key idea behind our approach is model combination: We generate a battery of
base models using all resources available and we combine their outputs using multi-
ple strategies. Our pool of base models contains 13 different models: The first is the
13 The data sets used for the experiments reported in this section are exactly the ones described in
Section 4.1.
651
Computational Linguistics Volume 39, Number 3
unmodified SwiRL SRC, the next six are the selected SP models from the previous
section, and the last six are variants of SwiRL SRC. In each variant, the feature set of
the unmodified SwiRL SRC model is extended with a single feature that models the
choice of a given SP, for example, SRC+SPres contains an extra feature that indicates the
choice of Resnik?s SP model.14
We combine the outputs of these base models using two different strategies: (a)
majority voting, which selects the label predicted by most models, and (b) meta-
classification, which uses a supervised model to learn the strengths of each base model.
For the meta-classification model, we opted for a binary classification approach: First,
for each constituent we generate n data points, one for each distinct role label proposed
by the pool of base models; then we use a binary meta-classifier to label each candidate
role as either correct or incorrect. We trained the meta-classifier on the usual PropBank
training partition, using 10-fold cross-validation to generate outputs for the base
models that require the same training material. At prediction time, for each candidate
constituent we selected the role label that was classified as correct with the highest
confidence.
The binary meta-classifier uses the following set of features:
 Labels proposed by the base models, for example, the feature SRC+SPres=Arg0
indicates that the SRC+SPres base model proposed the Arg0 label. We add
13 such features, one for each base model. Intuitively, this feature allows
the meta-classifier to learn the strengths of each base model with respect
to role labels: SRC+SPres should be trusted for the Arg0 role, and so on.
 Boolean value indicating agreement with the majority vote, for example, the
feature Majority=true indicates that the majority of the base models
proposed the same label as the one currently considered by the
meta-classifier.
 Number of base models that proposed this data point?s label. To reduce sparsity,
for each number of base models, N, we generate N distinct features
indicating that the number of base models that proposed this label is
larger than k, where k ? [0, N). For example, if two base models proposed
the label under consideration, we generate the following two features:
BaseModelNumber>0 and BaseModelNumber>1. This feature provides finer
control over the number of votes received by a label than the majority
voter, for example, the meta-classifier can learn to trust a label if more
than two base models proposed it, even if the majority vote disagrees.
 List of actual base models that proposed this data point?s label. We store a
distinct feature for each base model that proposed the current label, and
also a concatenation of all these base model names. The latter feature is
designed to allow the meta-classifier to learn preferences for certain
combinations of base models. For example, if two base models, SPres and
SPwn, proposed the label under consideration, we generate three features:
Base=SPres, Base=SPwn, and Base=SPres+SPwn.
14 Adding more than one SP output as a feature in SwiRL?s SRC model did not improve performance in
development over the single-SP SRC model. Our conjecture is that the large number of features in SRC
has the potential to drown the SP-based features. This may be accentuated when there are more SP-based
features because their signal is divided among them due to their overlap. We have also tried to add the
input features of the SP models directly to the SRC model but this also proved to be unsuccessful during
development.
652
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 13
Results for the combination approaches. Accuracy shows the overall results. Core and Adj
contain F1 results restricted to the core numbered roles and adjuncts, respectively. SRC is
SwiRL?s standalone SRC model; +SPx stands for the SRC model extended with a feature given by
the corresponding SP model. Values in boldface font are the highest in the corresponding
column. Accuracy values marked with ? are significantly lower than the highest accuracy score
in the same column.
WSJ test Brown test
Acc. Core F1 Adj. F1 Acc. Core F1 Adj. F1
SRC 90.83? 93.25 81.31 79.52 84.42 57.76
+SPRes 90.76? 93.17 81.08 79.86? 84.52 59.24
+SPwn 90.56? 92.88 81.11 79.73? 84.26 59.69
+SPsimJac 90.86? 93.37 80.30 79.83? 84.43 59.54
+SPsimcos 90.87? 93.33 80.92 80.50? 85.14 60.16
+SPsimpreLin?Jac
90.95? 93.03 82.75 80.75? 85.62 59.63
+SPsimpreLin?cos
91.23? 93.78 80.56 80.48? 84.95 61.01
Meta-classifier 92.43 94.62 84.00 81.94 86.25 63.36
Voting 92.36 94.57 83.68 82.15 86.37 63.78
5.2 Results for Semantic Role Classification
Table 13 compares the performance of both combination approaches against the stand-
alone SRC model. In the table, the SRC+SP? models stand for SRC classifiers enhanced
with one feature from the corresponding SP. The meta-classifier shown in the table com-
bines the output of all the 13 base models introduced previously. We implemented the
meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial
kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results
of the voting strategy, over the same set of base models.
In the columns we show overall classification accuracy and F1 results for both core
arguments (Core) and adjunct arguments (Adj.). Note that for the overall SRC scores, we
report classification accuracy, defined as ratio of correct predictions over total number
of arguments to be classified. The reason for this is that the models in this section always
return a label for all arguments to be classified, and thus accuracy, precision, recall, and
F1 are all equal.
Table 13 indicates that four out of the six SRC+SP? models perform better than the
standalone SRC model in domain (WSJ), and all of them outperform SRC out of domain
(Brown). The improvements are small, however, and, generally, not statistically signifi-
cant. On the other hand, the meta-classifier outperforms the original SRC model both
in domain (17.4% relative error reduction; 1.60 points of accuracy improvement) and
out of domain (13.4% relative error reduction; 2.42 points of accuracy improvement),
and the differences are statistically significant. This experiment proves our claim that
SPs can be successfully used to improve semantic role classification. It also underscores
the fact that combining SRC and SPs is not trivial, however. Our hypothesis is that this
15 http://svmlight.joachims.org.
16 We have also trained the meta-classifier with other learning algorithms (e.g., logistic regression with
L2 regularization) and we obtained similar but slightly lower results.
653
Computational Linguistics Volume 39, Number 3
is caused by the large performance disparity (20 F1 points in domain and 18 out of
domain) between the original SRC model and the standalone SP methods.
Interestingly, the meta-classifier performs only marginally better than the voting ap-
proach in domain and slightly worse out of domain. We believe that this is another effect
of the above observation: Given the weaker SP-based features, the meta-classifier does
not learn much beyond a majority vote, which is exactly what the simpler, unsuper-
vised voting method models. Nevertheless, regardless of the combination method, this
experiment emphasizes that infusing SP information in the SRC task is beneficial.
Table 13 also shows that our approach yields consistent improvements for both
core and adjunct arguments. Out of domain, we see a bigger accuracy improvement
for adjunct arguments (6.02 absolute points) vs. core arguments (1.83 points, for the
voting model). This is to be expected, as most core arguments fall under the Arg0 and
Arg1 classes, which can typically be disambiguated based on syntactic information (i.e.,
subject vs. object). On the other hand, there are no syntactic hints for adjunct arguments,
so the system learns to rely more on SP information in this case.
Regarding the performance of individual combinations of SRC and SP methods
(e.g., SRC+SPRes), the differences among SP models in Table 13 are much smaller
than in Table 12. SPsimpreLin?cos and SPsim
pre
Lin?Jac
yield the best results in both cases, and
distributional methods are slightly stronger than WordNet-based methods. SPRes and
SPwn perform similarly when combined, with a small lead for Resnik?s method. The
smaller differences and changes in the rank among SP methods are due to the complex
interactions when combining SP models with the SRC system.
Table 14
Precision (P), recall (R), and F1 results per argument type for the standalone SRC model and
the meta-classifier, in the two test data sets (WSJ and Brown). Due to space limitations, the
AM- prefix has been dropped from the labels of all adjuncts. When classifying all arguments
(last row), the F1 score is an accuracy score because in this scenario P = R = F1. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SRC Meta-classifier SRC Meta-classifier
P R F1 P R F1 P R F1 P R F1
Arg0 93.6 96.7 95.1 95.1 97.4 96.2 87.6 89.3 88.4 89.4 91.0 90.2
Arg1 93.3 94.5 93.9 94.2 95.7 95.0 84.3 90.6 87.3 86.2 91.9 89.0
Arg2 86.0 82.6 84.3 87.8 87.4 87.6 52.7 56.8 54.7 55.9 59.9 57.8
Arg3 77.6 63.4 69.8 82.4 68.3 74.7 36.4 19.0 25.0 45.8 26.2 33.3
Arg4 86.8 78.6 82.5 89.5 81.0 85.0 59.4 34.5 43.7 67.9 34.5 45.8
Core 92.9 93.6 93.3 94.2 95.1 94.6 82.6 86.3 84.4 84.6 87.9 86.3
ADV 58.5 51.4 54.7 64.4 52.3 57.7 45.1 24.3 31.6 51.9 25.7 34.4
CAU 61.1 71.0 65.7 80.0 77.4 78.7 64.7 45.8 53.7 84.6 45.8 59.5
DIR 46.2 25.0 32.4 68.8 45.8 55.0 64.7 45.8 53.7 73.9 44.5 55.6
DIS 84.3 82.7 83.5 95.6 82.7 88.7 52.6 27.0 35.7 54.5 32.4 40.7
EXT 50.0 12.5 20.0 50.0 12.5 20.0 0.0 0.0 0.0 0.0 0.0 0.0
LOC 85.2 80.9 83.0 85.0 84.7 84.8 67.8 61.2 64.3 68.3 68.7 68.5
MNR 55.8 54.1 55.0 68.9 61.7 65.1 47.4 38.9 42.7 59.2 49.3 53.8
PNC 51.9 37.8 43.8 62.5 40.5 49.2 51.7 39.5 44.8 53.3 42.1 47.1
TMP 93.6 95.9 94.7 92.8 95.9 94.4 79.0 78.1 78.5 84.1 83.2 83.7
Adj 83.1 79.6 81.3 86.2 81.9 84.0 64.9 52.1 57.8 69.8 58.0 63.4
All ? ? 90.8? ? ? 92.4 ? ? 79.5? ? ? 81.9
654
Zapirain et al Selectional Preferences for Semantic Role Classification
Lastly, Table 14 shows a breakdown of the results by argument type for the orig-
inal SRC model and the meta-classifier (results are also presented over all numbered
arguments, Core, adjuncts, and Adj). This comparison emphasizes the previous obser-
vation that SPs are more useful for arguments that are independent of syntax than for
arguments that are usually tied to certain syntactic constructs (i.e., Arg0 and Arg1). For
example, in domain the meta-classifier improves Arg0 classification with 1.1 F1 points,
but it boosts the classification performance for causative arguments (AM-CAU) with 13
absolute points. A similar behavior is observed out of domain. For example, whereas
Arg0 classification is improved with 1.7 points, the classification of manner arguments
(AM-MNR) is improved by 11 points. All in all, with two exceptions, selectional prefer-
ences improve classification accuracy for all argument types, both in and out of domain.
The previous experiments showed that a meta-classifier (and a voting approach)
over a battery of base models improves over the performance of each individual clas-
sifier. Given that half of our base models are all relatively minor changes of the same
original classifier (SwiRL), however, it would be desirable to ensure that the overall
performance gain of the meta-classification system is due to the infusion of semantic
information that is missing in the baseline SRC, and not to a regularization effect coming
from the ensemble of classifiers. The qualitative analysis presented in Section 6 will
reinforce this hypothesis.
5.3 Results for End-to-End Semantic Role Labeling
Lastly, we investigate the contribution of SPs in an end-to-end SRL system. As discussed
before, our approach focuses on argument classification, a subtask of complete SRL,
because this component suffers in the presence of lexical data sparseness (Pradhan,
Ward, and Martin 2008). To understand the impact of SPs on the complete SRL task we
compared two SwiRL models: one that uses the original classification model (the SRC
line in Table 13) and another that uses our meta-classifier model (the Meta-classifier
line in Table 13). To implement this experiment we had to modify the publicly down-
loadable SwiRL model, which performs identification and classification jointly, using a
single multi-class model. We changed this framework to a pipeline model, which first
performs argument identification (i.e., is this constituent an argument or not?), followed
by argument classification (i.e., knowing that this constituent is an argument, what is
its label?).17 We used the same set of features as the original SwiRL system and the
original model to identify argument boundaries. This pipeline model allowed us to
easily plug in different classification models, which offers a simple platform to evaluate
the contribution of SPs in an end-to-end SRL system.
Table 15 compares the original SwiRL pipeline (SwiRL in the table) with the pipeline
model where the classification component was replaced with the meta-classifier previ-
ously introduced (SwiRL w/ meta). The latter model backs off to the original classifi-
cation model for candidates that are not covered by our current selectional preferences
(i.e., are not noun phrases or prepositional phrases containing a noun phrase as the
second child). We report results for the test partitions of WSJ and Brown in the same
table. Note that these results are not directly comparable with the results in Tables 13
and 14, because in those initial experiments we used gold argument boundaries whereas
17 This pipeline model performs slightly worse than the original SwiRL on the WSJ data and slightly better
on Brown.
655
Computational Linguistics Volume 39, Number 3
Table 15
Precision (P), recall (R), and F1 results per argument for the end-to-end semantic role labeling
task. We compared two models: the original SwiRL model and the one where the classification
component was replaced with the meta-classifier introduced at the beginning of the section. We
used the official CoNLL-2005 shared-task scorer to produce these results. We checked for
statistical significance for the overall F1 scores (All row). Values in boldface font indicate the
highest F1 score in the corresponding row and block. F1 values marked with ? are significantly
lower than the corresponding highest F1 score.
WSJ test Brown test
SwiRL SwiRL w/ meta SwiRL SwiRL w/ meta
P R F1 P R F1 P R F1 P R F1
Arg0 87.0 81.6 84.2 87.8 81.9 84.8 86.6 81.3 83.9 87.3 81.7 84.4
Arg1 79.1 71.8 75.3 79.4 72.1 75.6 70.2 64.6 67.3 71.1 65.2 68.0
Arg2 70.0 56.6 62.6 69.2 58.3 63.3 41.8 42.7 42.2 42.3 44.6 43.4
Arg3 72.4 43.9 54.7 72.6 44.5 55.2 36.4 12.9 19.0 34.6 14.5 20.5
Arg4 73.3 61.8 67.0 73.8 60.8 66.7 48.8 25.6 33.6 44.4 25.6 32.5
ADV 59.4 50.6 54.6 59.5 50.0 54.4 49.0 38.2 42.9 49.9 38.5 43.5
CAU 61.5 43.8 51.2 66.0 45.2 53.7 58.7 35.5 44.3 59.1 34.2 43.3
DIR 44.7 20.0 27.6 50.0 22.6 30.9 59.0 27.2 37.2 61.3 25.9 36.5
DIS 76.1 63.8 69.4 77.0 63.8 69.7 58.8 41.0 48.3 59.7 41.3 48.9
EXT 72.7 50.0 59.3 72.7 50.0 59.3 20.0 8.1 11.5 21.4 8.1 11.8
LOC 64.7 52.9 58.2 64.8 55.4 59.7 48.3 37.7 42.3 46.8 40.5 43.5
MNR 59.1 52.0 55.3 61.4 51.7 56.2 53.8 47.3 50.3 55.9 48.3 51.8
PNC 47.1 34.8 40.0 46.4 33.9 39.2 51.8 26.4 35.0 52.4 26.7 35.1
TMP 78.7 71.4 74.9 78.4 71.5 73.8 59.7 60.6 60.2 61.0 61.2 61.1
All 79.7 70.9 75.0? 80.0 71.3 75.4 71.8 64.2 67.8? 72.4 64.6 68.4
Table 15 shows results for an end-to-end model, which includes predicted argument
boundaries.
Table 15 shows that the use of selectional preferences improves overall results when
using predicted argument boundaries as well. Selectional preferences improve F1 scores
for four out of five core arguments in both WSJ and Brown, for six out of nine modifier
arguments in WSJ, and for seven out of nine modifier arguments in Brown. Notably, the
SPs improve results for the most common argument types (Arg0 and Arg1). All in all,
SPs yield a 0.4 F1 point improvement in WSJ and 0.6 F1 point improvement in Brown.
These improvements are small but they are statistically significant. We consider these re-
sults encouraging, especially considering that only a small percentage of arguments are
actually inspected by selectional preferences. This analysis is summarized in Table 16,
which lists how many argument candidates are inspected by the system in its different
stages. The table indicates that the vast majority of argument candidates are filtered
out by the argument identification component, which does not use SPs. Because of this,
even though approximately 50% of the role classification decisions can be reinforced
with SPs, only 4.5% and 3.6% of the total number of argument candidates in WSJ and
Brown, respectively, are actually inspected by the classification model that uses SPs.
6. Analysis and Discussion
We conducted a complementary manual analysis to further verify the usefulness of the
semantic information provided by the selectional preferences. We manually inspected
100 randomly selected classification cases, 50 examples in which the meta-classifier is
656
Zapirain et al Selectional Preferences for Semantic Role Classification
Table 16
Counts for argument candidates for the two test partitions on the end-to-end semantic role
labeling task. The Predicted non-arguments line indicates how many candidate arguments are
classified as non-arguments by the argument identification classifier. The Incompatible with SPs
line indicates how many candidates were classified as arguments but cannot be modeled by our
current SPs (i.e., they are not noun phrases or prepositional phrases containing a noun phrase as
the second child). Lastly, the Compatible with SPs line lists how many candidates were both
classified as likely arguments and can be modeled by the SPs.
WSJ test Brown test
Predicted non-arguments 158,310 184,958
Incompatible with SPs 5,739 11,167
Compatible with SPs 7,691 7,867
Total 171,740 203,992
correct and the baseline SRC (SwiRL) is wrong, and 50 where the meta-classifier chooses
the incorrect classifier and the SRC is right. Interestingly, we observed that the majority
of cases have a clear linguistic interpretation, shedding light on the reasons why the
meta-classifier using SP information manages to correct some erroneous predictions of
the original SRC model, but also on the limitations of selectional preferences.
Regarding the success of the meta-classifier, the studied cases generally correspond
to low frequency verb?argument head pairs, in which the baseline SRC might have
had problems with generalization. In 29 of the cases (?58%), the syntactic information
is not enough to disambiguate the proper role, tends to indicate a wrong role label,
or it confuses the SRC because it contains errors. Most of the semantically based SP
predictions are correct, however, so the meta-classifier does select the correct role label.
In another 15 cases (?30%) the source of the baseline SRC error is not clear, but still,
several SP models suggest the correct role, giving the opportunity to the meta-classifier
to make the right choice. Finally, in the remaining six cases (?12%) a ?chance effect? is
observed: The failure of the baseline SRC model does not have a clear interpretation and,
moreover, most SP predictions are actually wrong. In these situations, several labels are
predicted with the same confidence, and the meta-classifier selects the correct one by
chance.
Figure 2 shows four real examples in which we see the importance of the infor-
mation provided by the selectional preferences. In example (a), the verb flash never
occurs in training with the argument head word news. The syntactic structure alone
strongly suggests Arg0, because the argument is an NP just to the left of a verb in active
form. This is probably why the baseline SRC incorrectly predicts Arg0. Some semantic
information is needed to know that the word news is not the agent of the predicate
(Arg0), but rather the theme (thing shining, Arg1). Selectional preferences make this
work perfectly, because all variants predict the correct label by signaling that news is
much more compatible with flash in Arg1 position rather than Arg0.
In example (b), the predicate promise expects a person as Arg1 (person promised to,
Recipient) and an action as Arg2 (promised action, Theme). Moreover, the presence of
Arg2 is obligatory. The syntactic structure is correct but does not provide the semantic
(Arg1 should be a person) or structural information (the assignment of Arg1 would have
required an additional Arg2) needed to select the appropriate role. SwiRL does not have
it either, and it assigns the incorrect Arg1 label. Most SP models correctly predict that
investigation is more similar to the heads of Arg2 arguments of promise than to the
heads of Arg1 arguments, however.
657
Computational Linguistics Volume 39, Number 3
(a) Several traders could be seen shaking their heads when (([the news]Arg0?Arg1)NP
( flashed)VP)S .
(b) Italian President Francesco Cossiga (promised ([a quick investigation into
whether Olivetti broke Cocom rules]Arg1?Arg2)NP)VP.
(c) Annual payments (will more than double ([from (a year ago)NP]TMP?Arg3)PP to
about $240 million ? ? ? )VP ? ? ?
(d) Procter & Gamble Co. plans to (begin ((testing (next month)NP)VP)S ([a superco.
detergent that ? ? ? washload]Arg0?Arg1)NP)VP .
Figure 2
Examples of incorrect SwiRL role assignments fixed by the meta-classifier. In each sentence, the
verb is emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the incorrect label x
assigned by the baseline SwiRL model is corrected into role label y by the combined system.
Finally, examples also contain simplified syntactic annotations from the test set predicted
syntactic layer, which are used for the discussion in the text.
In example (c) we see the application of prep-role selectional preferences. In that
sentence, the baseline SRC is likely confused by the content word feature of the PP
?from a year ago? (Surdeanu et al 2003). In PropBank, ?year? is a strong indicator
of a temporal adjunct (AM-TMP). The predicate double, however, describes the Arg3
argument as ?starting point? of the action and it is usually introduced by the preposition
from. This is very common also for other motion verbs (go, rise, etc.), resulting in the
from-Arg3 selectional preference containing a number of heads of temporal expressions,
in particular many more instances of the word year than the from-AM-TMP selectional
preference. As a consequence, the majority of SP models predict the correct Arg3 label.
Finally, example (d) highlights that selectional preferences increase robustness in
front of parsing errors. In this example, the NP ?a superco. detergent? is incorrectly
attached to ?begin? instead of the predicate testing by the syntactic parser. This produces
many incorrect features derived from syntax (syntactic frame, path, etc.) that may con-
fuse the baseline SRC model, which ends up producing an incorrect Arg0 assignment.
Most of the SP models, however, predict that detergent is not a plausible Agent for test
(?examiner?), but instead it fits best with the Arg1 position (?examined?).
Nevertheless, selectional preferences have a significant limitation: They do not
model syntactic structures, which often give strong hints for classification. In fact, the
vast majority of the situations where the meta-classifier performs worse than the origi-
nal SRC model are cases that are syntax-driven, hence situations that are incompletely
addressed by the current SP models. Even though the SRC and the SRC+SP models
have features that model syntax, they can be overwhelmed by the SP features and
standalone models, which leads to incorrect meta-classification results. Figure 3 shows a
few representative examples in this category. In the first example in the figure, the meta-
classifier changes the correctly assigned label Arg2 to Arg1, because most SP models
favor the Arg1 label for the argument ?test.? In the PropBank training corpus, however,
the argument following the verb fail is labeled Arg2 in 79% of the cases. Because the
SP models do not take into account syntax or positional information, this syntactic
preference is lost. Similarly, SPs do not model the fact that the verb buy is seldom
preceded by an Arg1 argument, or the argument immediately following the verb precede
tends to be Arg1, hence the incorrect classifications in Figure 3 (b) and (c). All these
658
Zapirain et al Selectional Preferences for Semantic Role Classification
(a) Some ?circuit breakers? installed after the October 1987 crash (failed ([their first
test ]Arg2?Arg1)NP)VP...
(b) Many fund managers argue that now?s ([the time]TMP?Arg1)NP (to buy)VP)S .
(c) Telephone volume was up sharply, but it was still at just half the level of the
weekend (preceding ([Black Monday ]Arg1?TMP)NP)VP .
Figure 3
Examples of incorrect assignments by the meta-classifier. In each sentence, the verb is
emphasized in italics and the head word for the selectional preferences is boldfaced. The
argument under focus is marked within square brackets. x ? y means that the correct
x label assigned by the baseline model is wrongly converted into y by the meta-classifier.
As in Figure 2, examples also contain simplified syntactic annotations taken from the test
set predicted syntactic layer.
examples are strong motivation for SP models that model both lexical and syntactic
preferences. We will address such models in future work.
7. Conclusions
Current systems usually perform SRL in two pipelined steps: argument identification
and argument classification. Whereas identification is mostly syntactic, classification
requires semantic knowledge to be taken into account. In this article we have shown
that the lexical heads seen in training data are too sparse to assign the correct role,
and that selectional preferences are able to generalize those lexical heads. In fact, we
show for the first time that the combination of the predictions of several selectional
preference models with a state-of-the-art SRC system yields significant improvements in
both in-domain and out-of-domain test sets. These improvements to role classification
translate into small but statistically significant improvements in an end-to-end semantic
role labeling system. We find these results encouraging considering that in the complete
semantic role labeling task only a small percentage of argument candidates are affected
by our modified role classification model. The experiments were carried out over the
well-known CoNLL-2005 data set, based on PropBank.
We applied several selectional preference models, based on WordNet and distribu-
tional similarity. Our experiments show that all models outperform the pure lexical
matching approach, with distributional methods performing better that WordNet-based
methods, and second-order similarity models being the best. In addition to the tradi-
tional selectional preferences for verbs, we introduce the use of selectional preferences
for prepositions, which are applied to classifying prepositional phrases. The combi-
nation of both types of selectional preferences improves over the use of selectional
preferences for verbs alone.
The analysis performed over the cases where the base SRC system and the com-
bined system differed showed that the selectional preferences are specially helpful when
syntactic information is either incorrect or insufficient to disambiguate the correct role.
The analysis also highlighted that the limitations of selectional preferences for modeling
syntactic structures introduce some errors in the combined model. Those errors could
be addressed if the SP models included some syntactic information.
Our research leaves the door open for tighter integration of semantic and syntactic
information for Semantic Role Labeling. We introduced selectional preferences in the
SRC system as simple features, but models which extend syntactic structures with
659
Computational Linguistics Volume 39, Number 3
selectional preferences (or vice versa) could overcome some of the errors that our system
introduced. Extending the use of selectional preferences to other syntactic types beyond
noun phrases and prepositional phrases would be also of interest. In addition, the
method for combining selectional preferences for verbs and prepositions was naive,
and we expect that a joint model of verb and preposition preferences for prepositional
phrases would improve results further. Finally, individual selectional preference meth-
ods could be improved and newer methods incorporated, which could further improve
the results.
Acknowledgments
The authors would like to thank the three
anonymous reviewers for their detailed
and insightful comments on the submitted
version of this manuscript, which helped
us to improve it significantly in this revision.
This work was partially funded by
the Spanish Ministry of Science and
Innovation through the projects OpenMT-2
(TIN2009-14675-C03) and KNOW2
(TIN2009-14715-C04-04). It also received
financial support from the Seventh
Framework Programme of the EU
(FP7/2007- 2013) under grant agreements
247762 (FAUST) and 247914 (MOLTO).
Mihai Surdeanu was supported by the Air
Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion
or recommendations expressed in this
material are those of the authors and do
not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Agirre, Eneko, Timothy Baldwin, and
David Martinez. 2008. Improving
parsing and PP attachment performance
with sense information. In Proceedings
of ACL-08: HLT, pages 317?325,
Columbus, OH.
Agirre, Eneko, Kepa Bengoetxea, Koldo
Gojenola, and Joakim Nivre. 2011.
Improving dependency parsing with
semantic classes. In Proceedings of the
49th Annual Meeting of the Association
for Computational Linguistics: Human
Language Technologies, pages 699?703,
Portland, OR.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the 2001
Workshop on Computational Natural
Language Learning (CoNLL-2001),
pages 1?8, Toulouse.
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings of the
16th Conference on Computational
Linguistics - Volume 1, COLING ?96,
pages 16?22, Stroudsburg, PA.
Baroni, Marco and Alessandro Lenci.
2010. Distributional memory: A general
framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59?68,
Honolulu, HI.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1,364?1,371,
Las Palmas de Gran Canaria.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 10th
Conference of the European Chapter of the
Association of Computational Linguistics
(EACL-2003), pages 27?34, Budapest.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004
Shared Task: Semantic Role Labeling.
In Proceedings of the Eighth Conference
on Computational Natural Language
Learning (CoNLL-2004), pages 89?97,
Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling.
In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Chakraborti, Sutanu, Nirmalie Wiratunga,
Robert Lothian, and Stuart Watt. 2007.
Acquiring word similarities with higher
order association mining. In Proceedings
of the 7th International Conference on
Case-Based Reasoning: Case-Based Reasoning
Research and Development, ICCBR ?07,
pages 61?76, Berlin.
660
Zapirain et al Selectional Preferences for Semantic Role Classification
Chambers, Nathanael and Daniel Jurafsky.
2010. Improving the use of pseudo-words
for evaluating selectional preferences. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics,
pages 445?453, Uppsala, Sweden.
Charniak, E. 2000. A maximum-entropy
inspired parser. In Proceedings of the 1st
Meeting of the North American Chapter
of the Association for Computational
Linguistics (NAACL-2000), pages 132?139,
Seattle, WA.
Clark, Stephen and Stephen Weir. 2002.
Class-based probability estimation
using a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Edmonds, Philip. 1997. Choosing the word
most typical in context using a lexical
co-occurrence network. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and Eighth
Conference of the European Chapter of the
Association for Computational Linguistics,
ACL ?98, pages 507?509, Stroudsburg, PA.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL-2007), pages 216?223, Prague.
Erk, Katrin, Sebastian Pado?, and Ulrike Pado?.
2010. A flexible, corpus-driven model of
regular and inverse selectional preferences.
Computational Linguistics, 36(4):723?763.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. FrameNet and representing
the link between semantic and syntactic
relations. In Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grefenstette, Gregory. 1992. Sextant:
Exploring unexplored contexts for
semantic extraction from syntactic
analysis. In ACL?92, pages 324?326,
Newark, DE.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-1990), pages 268?275, Pittsburgh, PA.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings
of ACL-08: HLT, pages 595?603,
Columbus, OH.
Lee, Lillian. 1999. Measures of distributional
similarity. In 37th Annual Meeting of the
Association for Computational Linguistics,
pages 25?32, College Park, MD.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL-1998), pages 768?774,
Montreal.
Litkowski, K. C. and O. Hargraves. 2005. The
preposition project. In Proceedings of the
ACL-SIGSEM Workshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistic Formalisms and
Applications, pages 171?179, Colchester.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 24?29, Prague.
Litkowski, Ken and Orin Hargraves.
2006. Coverage and inheritance in the
preposition project. In Prepositions ?06:
Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, pages 37?44,
Trento.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. In Proceedings of the Workshop on
Human Language Technology (HLT-94),
pages 114?119, Plainsboro, NJ.
Ma`rquez, Llu??s, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008.
Semantic role labeling: An introduction to
the special issue. Computational Linguistics,
34(2):145?159.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguisties, 29:639?654.
Melli, Gabor, Yang Wang, Yudong Liu,
Mehdi M. Kashani, Zhongmin Shi,
661
Computational Linguistics Volume 39, Number 3
Baohua Gu, Anoop Sarkar, and Fred
Popowich. 2005. Description of SQUASH,
the SFU question answering summary
handler for the DUC-2005 summarization
task. In Proceedings of Document
Understanding Workshop, HLT/EMNLP
Annual Meeting, Vancouver.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the
45th Conference of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva.
Noreen, E. W. 1989. Computer-Intensive
Methods for Testing Hypotheses:
An Introduction, Wiley.
O? Se?aghdha, Diarmuid. 2010. Latent
variable models of selectional preference.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 435?444, Uppsala.
Pado?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pado?, Sebastian, Ulrike Pado?, and Katrin Erk.
2007. Flexible, corpus-based modelling
of human plausibility judgements. In
Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL-2007),
pages 400?409, Prague.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The proposition bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences.
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 564?571, Rochester, NY.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of the 38th
Annual Conference of the Association of
Computational Linguistics (ACL-2000),
pages 101?108, Hong Kong.
Pradhan, S., W. Ward, and J. H. Martin. 2008.
Towards robust semantic role labeling.
Computational Linguistics, 34(2):289?310.
Rada, R., H. Mili, E. Bicknell, and M. Blettner.
1989. Development and application of a
metric on semantic nets. IEEE Transactions
on Systems, Man, and Cybernetics,
19(1):17?30.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Thirteenth Conference on Computational
Natural Language Learning (CoNLL-2009),
pages 147?155, Boulder, CO.
Resnik, Philip. 1993a. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Resnik, Philip. 1993b. Semantic classes and
syntactic ambiguity. In Proceedings of the
Workshop on Human Language Technology,
pages 278?283, Morristown, NJ.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent Dirichlet alocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434,
Uppsala.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Srikumar, V. and D. Roth. 2011. A joint
model for extended semantic role labeling.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 129?139, Edinburgh.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using predicate-
argument structures for information
extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL-2003), pages 8?15,
Sapporo.
Surdeanu, Mihai, Massimiliano Ciaramita,
and Hugo Zaragoza. 2011. Learning to
rank answers to non-factoid questions
from Web collections. Computational
Linguistics, 37(2):351?383.
Surdeanu, Mihai, Llu??s Ma`rquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research (JAIR), 29:105?151.
Sussna, Michael. 1993. Word sense
disambiguation for free-text indexing
using a massive semantic network. In
Proceedings of the Second International
662
Zapirain et al Selectional Preferences for Semantic Role Classification
Conference on Information and Knowledge
Management, CIKM ?93, pages 67?74,
New York, NY.
Wilks, Yorick. 1975. Preference semantics.
In E. L. Kaenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, MA, pages 329?348.
Zapirain, Ben?at, Eneko Agirre, and Llu??s
Ma`rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the Association for Computational
Linguistics and the 4th International Joint
Conference on Natural Language Processing
(ACL-IJCNLP-2009), pages 73?76, Suntec.
Zapirain, Ben?at, Eneko Agirre, Llu??s
Ma`rquez, and Mihai Surdeanu. 2010.
Improving semantic role classification
with selectional preferences. In Proceedings
of the 11th Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL HLT
2010), pages 373?376, Los Angeles, CA.
663

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 373?376,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Semantic Role Classification with Selectional Preferences
Ben?at Zapirain, Eneko Agirre
IXA NLP Group
Basque Country Univ.
{benat.zapirain,e.agirre}@ehu.es
Llu??s Ma`rquez
TALP Research Center
Technical Univ. of Catalonia
lluism@lsi.upc.edu
Mihai Surdeanu
Stanford NLP Group
Stanford Univ.
mihais@stanford.edu
Abstract
This work incorporates Selectional Prefer-
ences (SP) into a Semantic Role (SR) Clas-
sification system. We learn separate selec-
tional preferences for noun phrases and prepo-
sitional phrases and we integrate them in a
state-of-the-art SR classification system both
in the form of features and individual class
predictors. We show that the inclusion of the
refined SPs yields statistically significant im-
provements on both in domain and out of do-
main data (14.07% and 11.67% error reduc-
tion, respectively). The key factor for success
is the combination of several SP methods with
the original classification model using meta-
classification.
1 Introduction
Semantic Role Labeling (SRL) is the process of
extracting simple event structures, i.e., ?who? did
?what? to ?whom?, ?when? and ?where?. Current
systems usually perform SRL in two pipelined steps:
argument identification and argument classification.
While identification is mostly syntactic, classifica-
tion requires semantic knowledge to be taken into
account. Semantic information is usually captured
through lexicalized features on the predicate and the
head?word of the argument to be classified. Since
lexical features tend to be sparse, SRL systems are
prone to overfit the training data and generalize
poorly to new corpora.
Indeed, the SRL evaluation exercises at CoNLL-
2004 and 2005 (Carreras and Ma`rquez, 2005) ob-
served that all systems showed a significant perfor-
mance degradation (?10 F1 points) when applied to
test data from a different genre of that of the training
set. Pradhan et al (2008) showed that this perfor-
mance degradation is essentially caused by the argu-
ment classification subtask, and suggested the lexi-
cal data sparseness as one of the main reasons. The
same authors studied the contribution of the different
feature types in SRL and concluded that the lexical
features were the most salient features in argument
classification (Pradhan et al, 2007).
In recent work, we showed (Zapirain et al, 2009)
how automatically generated selectional preferences
(SP) for verbs were able to perform better than pure
lexical features in a role classification experiment,
disconnected from a full-fledged SRL system. SPs
introduce semantic generalizations on the type of ar-
guments preferred by the predicates and, thus, they
are expected to improve results on infrequent and
unknown words. The positive effect was especially
relevant for out-of-domain data. In this paper we ad-
vance (Zapirain et al, 2009) in two directions:
(1) We learn separate SPs for prepositions and verbs,
showing improvement over using SPs for verbs
alone.
(2) We integrate the information of several SP mod-
els in a state-of-the-art SRL system (SwiRL1) and
show significant improvements in SR classifica-
tion. The key for the improvement lies in a meta-
classifier, trained to select among the predictions
provided by several role classification models.
2 SPs for SR Classification
SPs have been widely believed to be an impor-
tant knowledge source when parsing and perform-
ing SRL, especially role classification. Still, present
parsers and SRL systems use just lexical features,
which can be seen as the most simple form of SP,
1http://www.surdeanu.name/mihai/swirl/
373
where the headword needs to be seen in the training
data, and otherwise the SP is not satisfied. Gildea
and Jurafsky (2002) showed barely significant im-
provements in semantic role classification of NPs
for FrameNet roles using distributional clusters. In
(Erk, 2007) a number of SP models are tested in
a pseudo-task related to SRL. More recently, we
showed (Zapirain et al, 2009) that several methods
to automatically generate SPs generalize well and
outperform lexical match in a large dataset for se-
mantic role classification, but the impact on a full
system was not explored.
In this work we apply a subset of the SP meth-
ods proposed in (Zapirain et al, 2009). These meth-
ods can be split in two main families, depending on
the resource used to compute similarity: WordNet-
based methods and distributional methods. Both
families define a similarity score between a word
(the headword of the argument to be classified) and a
set of words (the headwords of arguments of a given
role).
WordNet-based similarity: One of the models
that we used is based on Resnik?s similarity mea-
sure (1993), referring to it as res. The other model is
an in-house method (Zapirain et al, 2009), referred
as wn, which only takes into account the depth of
the most common ancestor, and returns SPs that are
as specific as possible.
Distributional similarity: Following (Zapirain et
al., 2009) we considered both first order and second
order similarity. In first order similarity, the simi-
larity of two words was computed using the cosine
(or Jaccard measure) of the co-occurrence vectors of
the two words. Co-occurrence vectors where con-
structed using freely available software (Pado? and
Lapata, 2007) run over the British National Corpus.
We used the optimal parameters (Pado? and Lapata,
2007, p. 179). We will refer to these similarities as
simcos and simJac, respectively. In contrast, sec-
ond order similarity uses vectors of similar words,
i.e., the similarity of two words was computed us-
ing the cosine (or Jaccard measure) between the
thesaurus entries of those words in Lin?s thesaurus
(Lin, 1998). We refer to these as sim2cos and sim
2
Jac.
Given a target sentence with a verb and its argu-
ments, the task of SR classification is to assign the
correct role to each of the arguments. When using
SPs alone, we only use the headwords of the ar-
guments, and each argument is classified indepen-
dently of the rest. For each headword, we select the
role (r) of the verb (c) which fits best the head word
(w), where the goodness of fit (SPsim(v, r, w)) is
modeled using one of the similarity models above,
between the headword w and the headwords seen in
training data for role r of verb v. This selection rule
is formalized as follows:
Rsim(v, w) = arg max
r?Roles(v)
SPsim(v, r, w) (1)
In our previous work (Zapirain et al, 2009), we
modelled SPs for pairs of predicates (verbs) and ar-
guments, independently of the fact that the argu-
ment is a core argument (typically a noun) or an
adjunct argument (typically a prepositional phrase).
In contrast, (Litkowski and Hargraves, 2005) show
that prepositions have SPs of their own, especially
when functioning as adjuncts. We therefore decided
to split SPs according to whether the potential argu-
ment is a Prepositional Phrase (PP) or a Noun Phrase
(NP). For NPs, which tend to be core arguments2,
we use the SPs of the verb (as formalized above).
For PPs, which have an even distribution between
core and adjunct arguments, we use the SPs of the
prepositions alone, ignoring the verbs. Implementa-
tion wise, this means that in Eq. (1), we change v
for p, where p is the preposition heading the PP.
3 Experiments with SPs in isolation
In this section we evaluate the use of SPs for classi-
fication in isolation, i.e., we use formula 1, and no
other information. In addition we contrast the use
of both verb-role and preposition-role SPs, as com-
pared to the use of verb-role SPs alone.
The dataset used in these experiments (and in Sec-
tion 4) is the same as provided by the CoNLL-2005
shared task on SRL (Carreras and Ma`rquez, 2005).
This dataset comprises several sections of the Prop-
Bank corpus (news from the WSJ) as well as an ex-
tract of the Brown Corpus. Sections 02-21 are used
for generating the SPs and training, Section 00 for
development, and Section 23 for testing, as custom-
ary. The Brown Corpus is used for out-of-domain
testing, but due to the limited size of the provided
section, we extended it with instances from Sem-
Link3. Since the focus of this work is on argument
2In our training data, NPs are adjuncts only 5% of the times
3http://verbs.colorado.edu/semlink/
374
Verb-Role SPs Preposition-Role and Verb-Role SPs
WSJ-test Brown WSJ-test Brown
prec. rec. F1 prec. rec. F1 prec. rec. F1 prec. rec. F1
lexical 70.75 26.66 39.43 59.39 05.51 10.08 82.98 43.77 57.31 68.47 13.60 22.69
SPres 45.07 37.11 40.71 36.34 27.58 31.33 63.47 53.24 57.91 55.12 44.15 49.03
SPwn 55.44 45.58 50.03 41.76 31.58 35.96 65.70 63.88 64.78 60.08 48.10 53.43
SPsimJac 48.85 46.38 47.58 42.10 34.34 37.82 61.83 61.40 61.61 55.42 53.45 54.42
SPsimcos 53.13 50.44 51.75 43.24 35.27 38.85 64.67 64.22 64.44 56.56 54.54 55.53
SPsim2
Jac
61.76 58.63 60.16 51.97 42.39 46.69 70.82 70.33 70.57 62.37 60.15 61.24
SPsim2cos 61.12 58.12 59.63 51.92 42.35 46.65 70.28 69.80 70.04 62.36 60.14 61.23
Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.
Labels proposed by the base models
Number of base models that proposed this datum?s label
List of actual base models that proposed this datum?s label
Table 2: Features of the binary meta-classifier.
classification, we use the gold PropBank data to
identify argument boundaries. Considering that SPs
can handle only nominal arguments, in these exper-
iments we used only arguments mapped to NPs and
PPs containing a nominal head. From the training
sections, we extracted over 140K such arguments for
the supervised generation of SPs. The development
and test sections contain over 5K and 8K examples,
respectively, and the portion of the Brown Corpus
comprises an amount of 8.1K examples.
Table 1 lists the results of the different SPs in iso-
lation. The results reported in the left part of Table
1 are comparable to those we reported in (Zapirain
et al, 2009). The differences are due to the fact that
we do not discard roles like MOD, DIS, NEG and
that our previous work used only the subset of the
data that could be mapped to VerbNet (around 50%).
All in all, the table shows that splitting SPs into verb
and preposition SPs yields better results, both in pre-
cision and recall, improving F1 up to 10 points in
some cases.
4 Integrating SPs in a SRL system
For these experiments we modified SwiRL (Sur-
deanu et al, 2007): (a) we matched the gold bound-
aries against syntactic constituents predicted inter-
nally using the Charniak parser (Charniak, 2000);
and (b) we classified these constituents with their
semantic role using a modified version of SwiRL?s
feature set.
We explored two different strategies for integrat-
ing SPs in SwiRL. The first, obvious method is to
extend SwiRL?s feature set with features that model
the preferences of the SPs, i.e., for each SP model
SPi we add a feature whose value is Ri. The second
method combines SwiRL?s classification model and
our SP models using meta-classification. We opted
for a binary classification approach: first, for each
constituent we generate n datums, one for each dis-
tinct role label proposed by the pool of base models;
then we use a binary meta-classifier to label each
candidate role as correct or incorrect. Table 2 lists
the features of the meta-classifier. We trained the
meta-classifier on the usual PropBank training par-
tition, using cross-validation to generate outputs for
the base models that require the same training ma-
terial. At prediction time, for each candidate con-
stituent we selected the role label that was classified
as correct with the highest confidence.
Table 3 compares the performance of both
combination approaches against the standalone
SwiRL classifier. We show results for both core
arguments (Core), adjunct arguments (Arg) and
all arguments combined (All). In the table, the
SwiRL+SP? models stand for SwiRL classifiers
enhanced with one feature from the correspond-
ing SP. Adding more than one SP-based feature to
SwiRL did not improve results. Our conjecture
is that the SwiRL classifier enhanced with SP-
based features does not learn relevant weights for
these features because their signal is ?drowned? by
SwiRL?s large initial feature set and the correlation
between the different SPs. This observation moti-
vated the development of the meta-classifier. The
meta-classifier shown in the table combines the out-
put of the SwiRL+SP? models with the predictions
of SP models used in isolation. We implemented
the meta-classifier using Support Vector Machines
(SVM)4 with a quadratic polynomial kernel, and
4http://svmlight.joachims.org
375
WSJ-test Brown
Core Adj All Core Adj All
SwiRL 93.25 81.31 90.83 84.42 57.76 79.52
+SPRes 93.17 81.08 90.76 84.52 59.24 79.86
+SPwn 92.88 81.11 90.56 84.26 59.69 79.73
+SPsimJac 93.37 80.30 90.86 84.43 59.54 79.83
+SPsimcos 93.33 80.92 90.87 85.14 60.16 80.50
+SPsim2
Jac
93.03 82.75 90.95 85.62 59.63 80.75
+SPsim2cos 93.78 80.56 91.23 84.95 61.01 80.48
Meta 94.37 83.40 92.12 86.20 63.40 81.91
Table 3: Classification accuracy for the combination ap-
proaches. +SPx stands for SwiRL plus each SP model.
C = 0.01 (tuned in development).
Table 3 indicates that four out of the six
SwiRL+SP? models perform better than SwiRL in
domain (WSJ-test), and all of them outperform
SwiRL out of domain (Brown). However, the im-
provements are small and, generally, not statistically
significant. On the other hand, the meta-classifier
outperforms SwiRL both in domain (14.07% error
reduction) and out of domain (11.67% error reduc-
tion), and the differences are statistically signifi-
cant (measured using two-tailed paired t-test at 99%
confidence interval on 100 samples generated us-
ing bootstrap resampling). We also implemented
two unsupervised voting baselines, one unweighted
(each base model has the same weight) and one
weighted (each base model is weighted by its accu-
racy in development). However, none of these base-
lines outperformed the standalone SwiRL classifier.
This is further proof that, for SR classification, meta-
classification is crucial because it can learn the dis-
tinct specializations of the various base models.
Finally, Table 3 shows that our approach yields
consistent improvements for both core and adjunct
arguments. Out of domain, we see a bigger accuracy
improvement for adjunct arguments (5.64 absolute
points) vs. core arguments (1.78 points). This is
to be expected, as most core arguments fall under
the Arg0 and Arg1 classes, which can typically be
disambiguated based on syntactic information, i.e.,
subject vs. object. On the other hand, there are no
syntactic hints for adjunct arguments, so the system
learns to rely more on SP information in this case.
5 Conclusions
This paper is the first work to show that SPs improve
a state-of-the-art SR classification system. Sev-
eral decisions were crucial for success: (a) we de-
ployed separate SP models for verbs and preposi-
tions, which in conjunction outperform SP models
for verbs alone; (b) we incorporated SPs into SR
classification using a meta-classification approach
that combines eight base models, developed from
variants of a state-of-the-art SRL system and the
above SP models. We show that the resulting system
outperforms the original SR classification system for
arguments mapped to nominal or prepositional con-
stituents. The improvements are statistically sig-
nificant both on in-domain and out-of-domain data
sets.
Acknowledgments
This work was partially supported by projects KNOW-
2 (TIN2009-14715-C04-01 / 04), KYOTO (ICT-2007-
211423) and OpenMT-2 (TIN2009-14675C03)
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic role labeling. In
Proc. of CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
K. Erk. 2007. A simple, similarity-based model for se-
lectional preferences. In Proc. of ACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING-ACL.
K. Litkowski and O. Hargraves. 2005. The preposi-
tion project. In Proceedings of the Workshop on The
Linguistic Dimensions of Prepositions and their Use
in Computational Linguistic Formalisms and Applica-
tions.
S. Pado? and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2).
S. Pradhan, W. Ward, and J. Martin. 2007. Towards ro-
bust semantic role labeling. In Proc. of NAACL-HLT.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards ro-
bust semantic role labeling. Computational Linguis-
tics, 34(2).
P. Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proc. of HLT.
M. Surdeanu, L. Ma`rquez, X. Carreras, and P.R. Comas.
2007. Combination strategies for semantic role label-
ing. Journal of Artificial Intelligence Research, 29.
B. Zapirain, E. Agirre, and L. Ma`rquez. 2009. General-
izing over lexical features: Selectional preferences for
semantic role classification. In Proc. of ACL-IJCNLP.
376
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 139?144,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Graphical Interface for MT Evaluation and Error Analysis
Meritxell Gonza`lez and Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{mgonzalez,jgimenez,lluism}@lsi.upc.edu
Abstract
Error analysis in machine translation is a nec-
essary step in order to investigate the strengths
and weaknesses of the MT systems under de-
velopment and allow fair comparisons among
them. This work presents an application that
shows how a set of heterogeneous automatic
metrics can be used to evaluate a test bed of
automatic translations. To do so, we have
set up an online graphical interface for the
ASIYA toolkit, a rich repository of evaluation
measures working at different linguistic lev-
els. The current implementation of the inter-
face shows constituency and dependency trees
as well as shallow syntactic and semantic an-
notations, and word alignments. The intelli-
gent visualization of the linguistic structures
used by the metrics, as well as a set of navi-
gational functionalities, may lead towards ad-
vanced methods for automatic error analysis.
1 Introduction
Evaluation methods are a key ingredient in the de-
velopment cycle of machine translation (MT) sys-
tems. As illustrated in Figure 1, they are used to
identify and analyze the system weak points (error
analysis), to introduce new improvements and adjust
the internal system parameters (system refinement),
and to measure the system performance in compari-
son to other systems or previous versions of the same
system (evaluation).
We focus here on the processes involved in the
error analysis stage in which MT developers need to
understand the output of their systems and to assess
the improvements introduced.
Automatic detection and classification of the er-
rors produced by MT systems is a challenging prob-
lem. The cause of such errors may depend not only
on the translation paradigm adopted, but also on the
language pairs, the availability of enough linguistic
resources and the performance of the linguistic pro-
cessors, among others. Several past research works
studied and defined fine-grained typologies of trans-
lation errors according to various criteria (Vilar et
al., 2006; Popovic? et al, 2006; Kirchhoff et al,
2007), which helped manual annotation and human
analysis of the systems during the MT development
cycle. Recently, the task has received increasing at-
tention towards the automatic detection, classifica-
tion and analysis of these errors, and new tools have
been made available to the community. Examples
of such tools are AMEANA (Kholy and Habash,
2011), which focuses on morphologically rich lan-
guages, and Hjerson (Popovic?, 2011), which ad-
dresses automatic error classification at lexical level.
In this work we present an online graphical inter-
face to access ASIYA, an existing software designed
to evaluate automatic translations using an heteroge-
neous set of metrics and meta-metrics. The primary
goal of the online interface is to allow MT develop-
ers to upload their test beds, obtain a large set of met-
ric scores and then, detect and analyze the errors of
their systems using just their Internet browsers. Ad-
ditionally, the graphical interface of the toolkit may
help developers to better understand the strengths
and weaknesses of the existing evaluation measures
and to support the development of further improve-
ments or even totally new evaluation metrics. This
information can be gathered both from the experi-
139
Figure 1: MT systems development cycle
ence of ASIYA?s developers and also from the statis-
tics given through the interface to the ASIYA?s users.
In the following, Section 2 gives a general
overview of the ASIYA toolkit. Section 3 describes
the variety of information gathered during the eval-
uation process, and Section 4 provides details on the
graphical interface developed to display this infor-
mation. Finally, Section 5 overviews recent work re-
lated to MT error analysis, and Section 6 concludes
and reports some ongoing and future work.
2 The ASIYA Toolkit
ASIYA is an open toolkit designed to assist devel-
opers of both MT systems and evaluation measures
by offering a rich set of metrics and meta-metrics
for assessing MT quality (Gime?nez and Ma`rquez,
2010a). Although automatic MT evaluation is still
far from manual evaluation, it is indeed necessary
to avoid the bottleneck introduced by a fully man-
ual evaluation in the system development cycle. Re-
cently, there has been empirical and theoretical justi-
fication that a combination of several metrics scoring
different aspects of translation quality should corre-
late better with humans than just a single automatic
metric (Amigo? et al, 2011; Gime?nez and Ma`rquez,
2010b).
ASIYA offers more than 500 metric variants for
MT evaluation, including the latest versions of the
most popular measures. These metrics rely on dif-
ferent similarity principles (such as precision, recall
and overlap) and operate at different linguistic layers
(from lexical to syntactic and semantic). A general
classification based on the similarity type is given
below along with a brief summary of the informa-
tion they use and the names of a few examples1.
Lexical similarity: n-gram similarity and edit dis-
tance based on word forms (e.g., PER, TER,
WER, BLEU, NIST, GTM, METEOR).
Syntactic similarity: based on part-of-speech tags,
base phrase chunks, and dependency and con-
stituency trees (e.g., SP-Overlap-POS, SP-
Overlap-Chunk, DP-HWCM, CP-STM).
Semantic similarity: based on named entities, se-
mantic roles and discourse representation (e.g.,
NE-Overlap, SR-Overlap, DRS-Overlap).
Such heterogeneous set of metrics allow the user
to analyze diverse aspects of translation quality at
system, document and sentence levels. As discussed
in (Gime?nez and Ma`rquez, 2008), the widely used
lexical-based measures should be considered care-
fully at sentence level, as they tend to penalize trans-
lations using different lexical selection. The combi-
nation with complex metrics, more focused on ad-
equacy aspects of the translation (e.g., taking into
account also semantic information), should help re-
ducing this problem.
3 The Metric-dependent Information
ASIYA operates over a fixed set of translation test
cases, i.e., a source text, a set of candidate trans-
lations and a set of manually produced reference
translations. To run ASIYA the user must provide
a test case and select the preferred set of metrics
(it may depend on the evaluation purpose). Then,
ASIYA outputs complete tables of score values for
all the possible combination of metrics, systems,
documents and segments. This kind of results is
valuable for rapid evaluation and ranking of trans-
lations and systems. However, it is unfriendly for
MT developers that need to manually analyze and
compare specific aspects of their systems.
During the evaluation process, ASIYA generates
a number of intermediate analysis containing par-
tial work outs of the evaluation measures. These
data constitute a priceless source for analysis pur-
poses since a close examination of their content al-
lows for analyzing the particular characteristics that
1A more detailed description of the metric set and its imple-
mentation can be found in (Gime?nez and Ma`rquez, 2010b).
140
Reference The remote control of the Wii
helps to diagnose an infantile
ocular disease .
Ol score
Candidate 1 The Wii Remote to help diag-
nose childhood eye disease .
7
17 = 0.41
Candidate 2 The control of the Wii helps
to diagnose an ocular infantile
disease .
13
14 = 0.93
Table 1: The reference sentence, two candidate
translation examples and the Ol scores calculation
differentiate the score values obtained by each can-
didate translation.
Next, we review the type of information used by
each family of measures according to their classifi-
cation, and how this information can be used for MT
error analysis purposes.
Lexical information. There are several variants un-
der this family. For instance, lexical overlap (Ol)
is an F-measure based metric, which computes sim-
ilarity roughly using the Jaccard coefficient. First,
the sets of all lexical items that are found in the ref-
erence and the candidate sentences are considered.
Then, Ol is computed as the cardinality of their in-
tersection divided by the cardinality of their union.
The example in Table 1 shows the counts used to cal-
culate Ol between the reference and two candidate
translations (boldface and underline indicate non-
matched items in candidate 1 and 2, respectively).
Similarly, metrics in another category measure the
edit distance of a translation, i.e., the number of
word insertions, deletions and substitutions that are
needed to convert a candidate translation into a ref-
erence. From the algorithms used to calculate these
metrics, these words can be identified in the set of
sentences and marked for further processing. On
another front, metrics as BLEU or NIST compute
a weighted average of matching n-grams. An inter-
esting information that can be obtained from these
metrics are the weights assigned to each individual
matching n-gram. Variations of all of these mea-
sures include looking at stems, synonyms and para-
phrases, instead of the actual words in the sentences.
This information can be obtained from the imple-
mentation of the metrics and presented to the user
through the graphical interface.
Syntactic information. ASIYA considers three lev-
els of syntactic information: shallow, constituent
and dependency parsing. The shallow parsing an-
notations, that are obtained from the linguistic pro-
cessors, consist of word level part-of-speech, lem-
mas and chunk Begin-Inside-Outside labels. Use-
ful figures such as the matching rate of a given
(sub)category of items are the base of a group of
metrics (i.e., the ratio of prepositions between a
reference and a candidate). In addition, depen-
dency and constituency parse trees allow for captur-
ing other aspects of the translations. For instance,
DP-HCWM is a specific subset of the dependency
measures that consists of retrieving and matching all
the head-word chains (or the ones of a given length)
from the dependency trees. Similarly, CP-STM, a
subset of the constituency parsing family of mea-
sures, consists of computing the lexical overlap ac-
cording to the phrase constituent of a given type.
Then, for error analysis purposes, parse trees com-
bine the grammatical relations and the grammati-
cal categories of the words in the sentence and dis-
play the information they contain. Figure 2 and 3
show, respectively, several annotation levels of the
sentences in the example and the constituency trees.
Semantic information. ASIYA distinguishes also
three levels of semantic information: named enti-
ties, semantic roles and discourse representations.
The former are post-processed similarly to the lex-
ical annotations discussed above; and the semantic
predicate-argument trees are post-processed and dis-
played in a similar manner to the syntactic trees.
Instead, the purpose of the discourse representation
analysis is to evaluate candidate translations at doc-
ument level. In the nested discourse structures we
could identify the lexical choices for each discourse
sub-type. Presenting this information to the user re-
mains as an important part of the future work.
4 The Graphical Interface
This section presents the web application that makes
possible a graphical visualization and interactive ac-
cess to ASIYA. The purpose of the interface is
twofold. First, it has been designed to facilitate the
use of the ASIYA toolkit for rapid evaluation of test
beds. And second, we aim at aiding the analysis of
the errors produced by the MT systems by creating
141
Figure 2: PoS, chunk and named entity annota-
tions on the source, reference and two translation
hypotheses
Figure 3: Constituency trees for the reference and
second translation candidate
a significant visualization of the information related
to the evaluation metrics.
The online interface consists of a simple web form
to supply the data required to run ASIYA, and then,
it offers several views that display the results in
friendly and flexible ways such as interactive score
tables, graphical parsing trees in SVG format and
interactive sentences holding the linguistic annota-
tions captured during the computation of the met-
rics, as described in Section 3.
4.1 Online MT evaluation
ASIYA allows to compute scores at three granular-
ity levels: system (entire test corpus), document and
sentence (or segment). The online application ob-
tains the measures for all the metrics and levels and
generates an interactive table of scores displaying
the values for all the measures. Table organiza-
Figure 4: The bar charts plot to compare the metric
scores for several systems
tion can swap among the three levels of granularity,
and it can also be transposed with respect to sys-
tem and metric information (transposing rows and
columns). When the metric basis table is shown, the
user can select one or more metric columns in or-
der to re-rank the rows accordingly. Moreover, the
source, reference and candidate translation are dis-
played along with metric scores. The combination of
all these functionalities makes it easy to know which
are the highest/lowest-scored sentences in a test set.
We have also integrated a graphical library2 to
generate real-time interactive plots to show the met-
ric scores graphically. The current version of the in-
terface shows interactive bar charts, where different
metrics and systems can be combined in the same
plot. An example is shown in Figure 4.
4.2 Graphically-aided Error Analysis and
Diagnosis
Human analysis is crucial in the development cy-
cle because humans have the capability to spot er-
rors and analyze them subjectively, in relation to the
underlying system that is being examined and the
scores obtained. Our purpose, as mentioned previ-
ously, is to generate a graphical representation of
the information related to the source and the trans-
lations, enabling a visual analysis of the errors. We
have focused on the linguistic measures at the syn-
tactic and semantic level, since they are more robust
than lexical metrics when comparing systems based
on different paradigms. On the one hand, one of
the views of the interface allows a user to navigate
and inspect the segments of the test set. This view
highlights the elements in the sentences that match a
2http://www.highcharts.com/
142
given criteria based on the various linguistic annota-
tions aforementioned (e.g., PoS prepositions). The
interface integrates also the mechanisms to upload
word-by-word alignments between the source and
any of the candidates. The alignments are also vi-
sualized along with the rest of the annotations, and
they can be also used to calculate artificial annota-
tions projected from the source in such test beds for
which there is no linguistic processors available. On
the other hand, the web application includes a library
for SVG graph generation in order to create the de-
pendency and the constituent trees dynamically (as
shown in Figure 3).
4.3 Accessing the Demo
The online interface is fully functional and accessi-
ble at http://nlp.lsi.upc.edu/asiya/. Al-
though the ASIYA toolkit is not difficult to install,
some specific technical skills are still needed in or-
der to set up all its capabilities (i.e., external com-
ponents and resources such as linguistic processors
and dictionaries). Instead, the online application re-
quires only an up to date browser. The website in-
cludes a tarball with sample input data and a video
recording, which demonstrates the main functional-
ities of the interface and how to use it.
The current web-based interface allows the user
to upload up to five candidate translation files, five
reference files and one source file (maximum size of
200K each, which is enough for test bed of about
1K sentences). Alternatively, the command based
version of ASIYA can be used to intensively evaluate
a large set of data.
5 Related Work
In the literature, we can find detailed typologies of
the errors produced by MT systems (Vilar et al,
2006; Farru?s et al, 2011; Kirchhoff et al, 2007) and
graphical interfaces for human classification and an-
notation of these errors, such as BLAST (Stymne,
2011). They represent a framework to study the
performance of MT systems and develop further re-
finements. However, they are defined for a specific
pair of languages or domain and they are difficult
to generalize. For instance, the study described in
(Kirchhoff et al, 2007) focus on measures relying on
the characterization of the input documents (source,
genre, style, dialect). In contrast, Farru?s et al (2011)
classify the errors that arise during Spanish-Catalan
translation at several levels: orthographic, morpho-
logical, lexical, semantic and syntactic errors.
Works towards the automatic identification and
classification of errors have been conducted very re-
cently. Examples of these are (Fishel et al, 2011),
which focus on the detection and classification of
common lexical errors and misplaced words using
a specialized alignment algorithm; and (Popovic?
and Ney, 2011), which addresses the classifica-
tion of inflectional errors, word reordering, missing
words, extra words and incorrect lexical choices us-
ing a combination of WER, PER, RPER and HPER
scores. The AMEANA tool (Kholy and Habash,
2011) uses alignments to produce detailed morpho-
logical error diagnosis and generates statistics at dif-
ferent linguistic levels. To the best of our knowl-
edge, the existing approaches to automatic error
classification are centered on the lexical, morpho-
logical and shallow syntactic aspects of the transla-
tion, i.e., word deletion, insertion and substitution,
wrong inflections, wrong lexical choice and part-
of-speech. In contrast, we introduce additional lin-
guistic information, such as dependency and con-
stituent parsing trees, discourse structures and se-
mantic roles. Also, there exist very few tools de-
voted to visualize the errors produced by the MT
systems. Here, instead of dealing with the automatic
classification of errors, we deal with the automatic
selection and visualization of the information used
by the evaluation measures.
6 Conclusions and Future Work
The main goal of the ASIYA toolkit is to cover the
evaluation needs of researchers during the develop-
ment cycle of their systems. ASIYA generates a
number of linguistic analyses over both the candi-
date and the reference translations. However, the
current command-line interface returns the results
only in text mode and does not allow for fully ex-
ploiting this linguistic information. We present a
graphical interface showing a visual representation
of such data for monitoring the MT development cy-
cle. We believe that it would be very helpful for car-
rying out tasks such as error analysis, system com-
parison and graphical representations.
143
The application described here is the first release
of a web interface to access ASIYA online. So
far, it includes the mechanisms to analyze 4 out of
10 categories of metrics: shallow parsing, depen-
dency parsing, constituent parsing and named en-
tities. Nonetheless, we aim at developing the sys-
tem until we cover all the metric categories currently
available in ASIYA.
Regarding the analysis of the sentences, we have
conducted a small experiment to show the ability of
the interface to use word level alignments between
the source and the target sentences. In the near fu-
ture, we will include the mechanisms to upload also
phrase level alignments. This functionality will also
give the chance to develop a new family of evalua-
tion metrics based on these alignments.
Regarding the interactive aspects of the interface,
the grammatical graphs are dynamically generated
in SVG format, which proffers a wide range of inter-
active functionalities. However their interactivity is
still limited. Further development towards improved
interaction would provide a more advanced manip-
ulation of the content, e.g., selection, expansion and
collapse of branches.
Concerning the usability of the interface, we will
add an alternative form for text input, which will re-
quire users to input the source, reference and candi-
date translation directly without formatting them in
files, saving a lot of effort when users need to ana-
lyze the translation results of one single sentence.
Finally, in order to improve error analysis capa-
bilities, we will endow the application with a search
engine able to filter the results according to varied
user defined criteria. The main goal is to provide
the mechanisms to select a case set where, for in-
stance, all the sentences are scored above (or below)
a threshold for a given metric (or a subset of them).
Acknowledgments
This research has been partially funded by the Span-
ish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and the European Commu-
nity?s Seventh Framework Programme under grant
agreement numbers 247762 (FAUST project, FP7-
ICT-2009- 4-247762) and 247914 (MOLTO project,
FP7-ICT-2009-4- 247914).
References
Enrique Amigo?, Julio Gonzalo, Jesu?s Gime?nez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proc. of the
EMNLP, Edinburgh, UK, pages 455?466.
Mireia Farru?s, Marta R. Costa-Jussa`, Jose? B. Marin?o,
Marc Poch, Adolfo Herna?ndez, Carlos Henr??quez, and
Jose? A. Fonollosa. 2011. Overcoming Statistical Ma-
chine Translation Limitations: Error Analysis and Pro-
posed Solutions for the Catalan?Spanish Language
Pair. LREC, 45(2):181?208.
Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and Jan Berka.
2011. Automatic Translation Error Analysis. In Proc.
of the 14th TSD, volume LNAI 3658. Springer Verlag.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards Het-
erogeneous Automatic MT Error Analysis. In Proc. of
LREC, Marrakech, Morocco.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):77?86.
Ahmed El Kholy and Nizar Habash. 2011. Automatic
Error Analysis for Morphologically Rich Languages.
In Proc. of the MT Summit XIII, Xiamen, China, pages
225?232.
Katrin Kirchhoff, Owen Rambow, Nizar Habash, and
Mona Diab. 2007. Semi-Automatic Error Analysis for
Large-Scale Statistical Machine Translation Systems.
In Proc. of the MT Summit XI, Copenhagen, Denmark.
Maja Popovic? and Hermann Ney. 2011. Towards Auto-
matic Error Analysis of Machine Translation Output.
Computational Linguistics, 37(4):657?688.
Maja Popovic?, Hermann Ney, Adria` de Gispert, Jose? B.
Marin?o, Deepa Gupta, Marcello Federico, Patrik Lam-
bert, and Rafael Banchs. 2006. Morpho-Syntactic
Information for Automatic Error Analysis of Statisti-
cal Machine Translation Output. In Proc. of the SMT
Workshop, pages 1?6, New York City, USA. ACL.
Maja Popovic?. 2011. Hjerson: An Open Source Tool
for Automatic Error Classification of Machine Trans-
lation Output. The Prague Bulletin of Mathematical
Linguistics, 96:59?68.
Sara Stymne. 2011. Blast: a Tool for Error Analysis of
Machine Translation Output. In Proc. of the 49th ACL,
HLT, Systems Demonstrations, pages 56?61.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proc. of the LREC, pages 697?702,
Genoa, Italy.
144
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 181?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
tSEARCH: Flexible and Fast Search over Automatic Translations for
Improved Quality/Error Analysis
Meritxell Gonza`lez and Laura Mascarell and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{mgonzalez,lmascarell,lluism}@lsi.upc.edu
Abstract
This work presents tSEARCH, a web-based
application that provides mechanisms for
doing complex searches over a collection
of translation cases evaluated with a large
set of diverse measures. tSEARCH uses the
evaluation results obtained with the ASIYA
toolkit for MT evaluation and it is connected
to its on-line GUI, which makes possible
a graphical visualization and interactive ac-
cess to the evaluation results. The search
engine offers a flexible query language al-
lowing to find translation examples match-
ing a combination of numerical and struc-
tural features associated to the calculation of
the quality metrics. Its database design per-
mits a fast response time for all queries sup-
ported on realistic-size test beds. In sum-
mary, tSEARCH, used with ASIYA, offers
developers of MT systems and evaluation
metrics a powerful tool for helping transla-
tion and error analysis.
1 Introduction
In Machine Translation (MT) system develop-
ment, a qualitative analysis of the translations is a
fundamental step in order to spot the limitations of
a system, compare the linguistic abilities of differ-
ent systems or tune the parameters during system
refinement. This is especially true in statistical
MT systems, where usually no special structured
knowledge is used other than parallel data and lan-
guage models, but also on systems that need to
reason over linguistic structures. The need for an-
alyzing and comparing automatic translations with
respect to evaluation metrics is also paramount
for developers of translation quality metrics, who
need elements of analysis to better understand the
behavior of their evaluation measures.
This paper presents tSEARCH, a web applica-
tion that aims to alleviate the burden of manual
analysis that developers have to conduct to as-
sess the translation quality aspects involved in the
above mentioned situations. As a toy example,
consider for instance an evaluation setting with
two systems, s1 and s2, and two evaluation met-
rics m1 and m2. Assume also that m1 scores s1 to
be better than s2 in a particular test set, while m2
predicts just the contrary. In order to analyze this
contradictory evaluation one might be interested in
inspecting from the test set the particular transla-
tion examples that contribute to these results, i.e.,
text segments t for which the translation provided
by s1 is scored better by m1 than the translation
provided by s2 and the opposite behavior regard-
ing metric m2. tSEARCH allows to retrieve (vi-
sualize and export) these sentences with a simple
query in a fast time response. The search can be
further constrained, by requiring certain margins
on the differences, by including other systems or
metrics, or by requiring some specific syntactic or
semantic constructs to appear in the examples.
tSEARCH is build on top of ASIYA (Gime?nez
and Ma`rquez, 2010), an open-source toolkit for
MT evaluation; and it can be used along with
the ASIYA ON-LINE INTERFACE (Gonza`lez et al,
2012), which provides an interactive environment
to examine the sentences. ASIYA allows to ana-
lyze a wide range of linguistic aspects of candi-
date and reference translations using a large set
of automatic and heterogeneous evaluation met-
rics. In particular, it offers a especially rich set
of measures that use syntactic and semantic infor-
mation. The intermediate structures generated by
the parsers, and used to compute the scoring mea-
sures, could be priceless for MT developers, who
can use them to compare the structures of several
translations and see how they affect the perfor-
mance of the metrics, providing more understand-
ing in order to interpret the actual performance of
the automatic translation systems.
tSEARCH consists of: 1) a database that stores
181
the resources generated by ASIYA, 2) a query lan-
guage and a search engine able to look through
the information gathered in the database, and 3) a
graphical user interface that assists the user to
write a query, returns the set of sentences that ful-
fill the conditions, and allows to export these re-
sults in XML format. The application is publicly
accessible on-line1, and a brief explanation of its
most important features is given in the demonstra-
tive video.
In the following, Section 2 gives an overview
of the ASIYA toolkit and the information gathered
from the evaluation output. Section 3 and Sec-
tion 4 describe in depth the tSEARCH application
and the on-line interface, respectively. Finally,
Section 5 reviews similar applications in compari-
son to the functionalities addressed by tSEARCH.
2 MT Evaluation with the ASIYA Toolkit
Currently, ASIYA contains more than 800 variants
of MT metrics to measure the similarity between
two translations at several linguistic dimensions.
Moreover, the scores can be calculated at three
granularity levels: system (entire test-set), docu-
ment and sentence (or segment).
As shown in Figure 1, ASIYA requires the user
to provide a test suite. Then, the input files are
processed in order to calculate the annotations, the
parsing trees and the final metric scores. Sev-
eral external components are used for both, met-
ric computation and automatic linguistic analysis2.
The use of these tools depends on the languages
supported and the type of measures that one needs
to obtain. Hence, for instance, lexical-based
measures are computed using the last version
of most popular metrics, such as BLEU, NIST,
METEOR or ROUGE. The syntax-wise measures
need the output of taggers, lemmatizers, parsers
1http://asiya.lsi.upc.edu/demo
2A complete list of external components can be found in
the Technical Manual at the ASIYA web-site
Figure 1: ASIYA processes and data files
and other analyzers. In those cases, ASIYA uses
the SVMTool (Gime?nez and Ma`rquez, 2004),
BIOS (Surdeanu et al, 2005), the Charniak-
Johnson and Berkeley constituent parsers (Char-
niak and Johnson, 2005; Petrov and Klein, 2007),
and the MALT dependency parser (Nivre et al,
2007), among others.
In the tSEARCH platform, the system manages
the communication with an instance of the ASIYA
toolkit running on the server. For every test suite,
the system maintains a synchronized representa-
tion of the input data, the evaluation results and the
linguistic information generated. Then, the system
updates a database where the test suites are stored
for further analysis using the tSEARCH tool, as de-
scribed next.
3 The tSEARCH Tool
tSEARCH offers a graphical search engine to ana-
lyze a given test suite. The system core retrieves
all translation examples that satisfy certain prop-
erties related to either the evaluation scores or the
linguistic structures. The query language designed
is simple and flexible, and it allows to combine
many properties to build sophisticated searches.
The tSEARCH architecture consists of the three
components illustrated in Figure 2: the web-based
interface, the storage system based on NoSQL
technology and the tSEARCH core, composed of
a query parser and a search engine.
The databases (Section 3.1) are fed through the
tSearch Data Loader API used by ASIYA. At
run-time, during the calculation of the measures,
ASIYA inserts all the information being calcu-
lated (metrics and parses) and a number of pre-
calculated variables (e.g., average, mean and per-
centiles). These operations are made in parallel,
which makes the overhead of filling the database
marginal.
The query parser (Section 3.2) receives the
query from the on-line interface and converts it
Figure 2: tSEARCH architecture
182
(a) Scores Column Family (b) Statistics Column Family
(c) Linguistic Elements Column Family
Figure 3: tSEARCH data model
into a binary tree structure where each leaf is a sin-
gle part of an operation and each node combines
the partial results of the children. The search en-
gine obtains the final results by processing the tree
bottom-up until the root is reached.
3.1 Data Representation, Storage and Access
The amount of data generated by ASIYA can be
very large for test sets with thousands of sen-
tences. In order to handle the high volume of
information, we decided to use the Apache Cas-
sandra database3, a NoSQL (also known as not
only SQL) solution that deals successfully with
this problem.
It is important to remark that there is no similar-
ity between NoSQL and the traditional relational
database management system model (RDBMS).
Actually, RDBMS uses SQL as its query language
and requires a relational model, whereas NoSQL
databases do not. Besides, the tSEARCH query
language can be complex, with several conditions,
which makes RDBMS perform poorly due the
complexity of the tables. In contrast, NoSQL-
databases use big-tables having many querying
information precalculated as key values, which
yields for direct access to the results.
The Cassandra data model is based on column
families (CF). A CF consists of a set of rows that
are uniquely identified by its key and have a set
of columns as values. So far, the tSEARCH data
model has the three CFs shown in Figure 3. The
scores CF in Figure 3(a) stores information related
to metrics and score values. Each row slot contains
the list of segments that matches the column key.
3http://cassandra.apache.org/
The statistics CF in Figure 3(b) stores basic statis-
tics, such as the minimum, maximum, average,
median and percentiles values for every evaluation
metric. The CF having the linguistic elements in
Figure 3(c) stores the results of the parsers, such
as part-of-speech, grammatical categories and de-
pendency relationships.
One of the goals of NoSQL databases is to ob-
tain the information required in the minimum ac-
cess time. Therefore, the data is stored in the
way required by the tSEARCH application. For
instance, the query BLEU > 0.4 looks for all
segments in the test suite having a BLEU score
greater than 0.4. Thus, in order to get the query
result in constant time, we use the metric identi-
fier as a part of the key for the scores CF, and the
score 0.4 as the column key.
3.2 The Query Language and Parser
The Query Parser module is one of the key ingre-
dients in the tSEARCH application because it de-
termines the query grammar and the allowed op-
erations, and it provides a parsing method to an-
alyze any query and produce a machine-readable
version of its semantics. It is also necessary in or-
der to validate the query.
There are several types of queries, depending on
the operations used: arithmetic comparisons, sta-
tistical functions (e.g., average, quartiles), range
of values, linguistic elements and logical opera-
tors. Furthermore, the queries can be applied at
segment-, document- and/or system-level, and it
is even possible to create any group of systems
or metrics. This is useful, for instance, in or-
der to limit the search to certain type of systems
(e.g., rule-based vs. statistical) and specific met-
183
Figure 4: (top) Query operations and functions, (bottom) Queries for group of systems and metrics
rics (e.g., lexical vs. syntactic). All possible query
types are described in the following subsections
(3.2.1 to 3.2.3) and listed in Figure 4.
3.2.1 Segment-level and Metric-based
Queries
The most basic queries are those related to
segment level scores, i.e., obtain all segments
scored above/below a value for a concrete met-
ric. The common comparison operators are sup-
ported, such as for instance, BLEU > 0.4 and
BLEU gt 0.4, that are both correct and equiva-
lent queries.
Basic statistics are also calculated at run-time,
which allows to use statistic variables as values,
e.g., obtain the segments scored in the fourth quar-
tile of BLEU. The maximum, minimum, average,
median and percentile values of each metric are
precalculated and saved into the MAX, MIN, AVG,
MEDIAN and PERC variables, respectively. The
thresholds and quartiles (TH,Q) are calculated at
run-time based on percentiles. MIN and MAX can
also be used and allow to get al segments in the
test set (i.e.,BLEU ge MIN).
The threshold function implies a percentage.
The query BLEU > TH(20) gets all segments
that have a BLEU score greater than the score
value of the bottom 20% of the sentences.
It is also possible to specify an interval of values
using the operator IN[x,y]. The use of paren-
thesis is allowed in order to exclude the bound-
aries. The arguments for this operator can be
either numerical values or the predefined func-
tions for quartiles and percentiles. Therefore,
the following example BLEU IN [TH(20),
TH(30)] returns all segments with a BLEU score
in the range between the threshold of the 20% (in-
cluded) and the 30% (excluded).
The quartile function Q(X) takes a value be-
tween 1 and 4 and returns all segments that
have their score in that quartile. In contrast,
the percentile function generalizes the previous:
PERC(n,M), where 1 < M <= 100; 1 <= n <=
M , returns all the segments with a score in the nth
part, when the range of scores is divided in M parts
of equal size.
Finally, a query can be composed of more than
one criterion. To do so, the logical operators AND
and OR are used to specify intersection and union,
respectively.
3.2.2 System- and Document-level Queries
The queries described next implement the search
procedures for more sophisticated queries involv-
ing system and document level properties, and
also the linguistic information used in the calcu-
184
lation of the evaluation measures. The purpose of
this functionality is to answer questions related to
groups of systems and/or metrics.
As explained in the introduction, one may
want to find the segments with good scores
for lexical metrics and, simultaneously, bad
scores for syntactic-based ones, or viceversa.
The following query illustrates how to do
it: ((srb[LEX] > AVG) OR (s3[LEX]
< AVG)) AND ((srb[SYN] < AVG) OR
(s3[SYN] > AVG) ), where srb = {s1, s2}
is the definition of a group of the rule-based
systems s1 and s2, s3 is another transla-
tion system, and LEX={BLEU,NIST} and
SYN={CP-Op(*),SP-Oc(*)} are two groups
of lexical- and syntactic-based measures, respec-
tively. The output of this kind of queries can help
developers to inspect the differences between the
systems that meet these criteria.
Concerning queries at document level, its struc-
ture is the same but applied at document scope.
They may help to find divergences when translat-
ing documents from different domains.
3.2.3 Linguistic Element-based Queries
The last functionality in tSEARCH allows search-
ing the segments that contain specific linguistic
elements (LE), estimated with any of the ana-
lyzers used to calculate the linguistic structures.
Linguistic-wise queries will allow the user to
find segments which match the criteria for any
linguistic feature calculated by ASIYA: part-of-
speech, lemmas, named entities, grammatical cat-
egories, dependency relations, semantic roles and
discourse structures.
We have implemented queries that match
n-grams of lemmas (lemma), parts-of-speech
(pos) and items of shallow (SP) or constituent
parsing (CP), dependency relations (DP) and se-
mantic roles SR, such as LE[lemma(be),
pos(NN,adj), SP(NP,ADJP,VP),
CP(VP,PP)]. The DP function allows also
specifying a compositional criterion (i.e., the
categories of two words and their dependency
relationship) and even a chain of relations, e.g.,
LE[DP(N,nsubj,V,dep,V)]. In turn, the
SR function obtains the segments that match a
verb and its list of arguments, e.g., LE[SR(ask,
A0, A1)].
The asterisk symbol can be used to substi-
tute any LE-item, e.g., LE[SP(NP,*,PP),
DP(*,*,V)]. When combined with semantic
roles, one asterisk substitutes any verb that has all
the arguments specified, e.g., LE[SR(*, A0,
A1)], whereas two asterisks in a row allow
arguments to belong to different verbs in the
same sentence. For instance, LE[SR(**, A1,
AM-TMP)] matches the sentence Those who pre-
fer to save money, may try to wait a few more days,
where the verb wait has the argument AM-TMP
and the verb prefer has the argument A1.
4 On-line Interface and Export of the
Results
tSEARCH is fully accessible on-line through the
ASIYA ON-LINE INTERFACE. The web applica-
tion runs ASIYA remotely, calculates the scores
and fills the tSEARCH database. It also offers the
chance to upload the results of a test suite previ-
ously processed. This way it feeds the database
directly, without the need to run ASIYA.
Anyhow, once the tSEARCH interface is already
accessible, one can see a tools icon on the right
of the search box. It shows the toolbar with all
available metrics, functions and operations. The
search box allows to query the database using the
query language described in Section 3.2.
After typing a query, the user can navigate the
results using three different views that organize
them according to the user preferences: 1) All
segments shows all segments and metrics men-
tioned in the query, the segments can be sorted
by the score, in ascendent or descendent order,
just tapping on the metric name; 2) Grouped by
system groups the segments by system and, for
Figure 5: The tSEARCH Interface
185
each system, by document; 3) Grouped by segment
displays the segment organization, which allows
an easy comparison between several translations.
Each group contains all the information related to
a segment number, such as the source and the ref-
erence sentences along with the candidate transla-
tions that matched the query.
Additionally, moving the mouse over the seg-
ments displays a floating box as illustrated in Fig-
ure 5. It shows some relevant information, such
as the source and references segments, the system
that generated the translation, the document which
the segment belongs to, and the scores.
Finally, all output data obtained during the
search can be exported as an XML file. It is possi-
ble to export all segments, or the results structured
by system, by segment, or more specific informa-
tion from the views.
5 Related Work and Conclusions
The ultimate goal of tSEARCH is to provide the
community with a user-friendly tool that facilitates
the qualitative analysis of automatic translations.
Currently, there are no freely available automatic
tools for aiding MT evaluation tasks. For this rea-
son, we believe that tSEARCH can be a useful tool
for MT system and evaluation metric developers.
So far, related works in the field address (semi)-
automatic error analysis from different perspec-
tives. A framework for error analysis and classifi-
cation was proposed in (Vilar et al, 2006), which
has inspired more recent works in the area, such
as (Fishel et al, 2011). They propose a method
for automatic identification of various error types.
The methodology proposed is language indepen-
dent and tackles lexical information. Nonetheless,
it can also take into account language-dependent
information if linguistic analyzers are available.
The user interface presented in (Berka et al, 2012)
provides also automatic error detection and clas-
sification. It is the result of merging the Hjer-
son tool (Popovic?, 2011) and Addicter (Zeman et
al., 2011). This web application shows alignments
and different types of errors colored.
In contrast, the ASIYA interface and the
tSEARCH tool together facilitate the qualitative
analysis of the evaluation results yet providing
a framework to obtain multiple evaluation met-
rics and linguistic analysis of the translations.
They also provide the mechanisms to search and
find relevant translation examples using a flexible
query language, and to export the results.
Acknowledgments
This research has been partially funded by
the Spanish Ministry of Education and Science
(OpenMT-2, TIN2009-14675-C03), the European
Community?s Seventh Framework Programme un-
der grant agreement number 247762 (FAUST,
FP7-ICT-2009-4-247762) and the EAMT Spon-
sorhip of Activities: Small research and develop-
ment project, 2012.
References
Jan Berka, Ondrej Bojar, Mark Fishel, Maja Popovic,
and Daniel Zeman. 2012. Automatic MT error anal-
ysis: Hjerson helping Addicter. In Proc. 8th LREC.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proc. 43rd Meeting of the ACL.
Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and Jan
Berka. 2011. Automatic Translation Error Analy-
sis. In Proc. 14th Text, Speech and Dialogue (TSD).
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proc. 4th Intl. Conf. LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94.
Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??s
Ma`rquez. 2012. A Graphical Interface for MT Eval-
uation and Error Analysis. In Proc. 50th Meeting of
the ACL. System Demonstration.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering, 13.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. HLT.
Maja Popovic?. 2011. Hjerson: An Open Source
Tool for Automatic Error Classification of Machine
Translation Output. The Prague Bulletin of Mathe-
matical Linguistics, 96.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proc. 9th INTERSPEECH.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proc. 5th LREC.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondrej Bo-
jar. 2011. Addicter: What Is Wrong with My Trans-
lations? The Prague Bulletin of Mathematical Lin-
guistics, 96.
186
Transactions of the Association for Computational Linguistics, 1 (2013) 219?230. Action Editor: Brian Roark.
Submitted 1/2013; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Joint Arc-factored Parsing of Syntactic and Semantic Dependencies
Xavier Llu??s and Xavier Carreras and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
Jordi Girona 1?3, 08034 Barcelona
{xlluis,carreras,lluism}@lsi.upc.edu
Abstract
In this paper we introduce a joint arc-factored
model for syntactic and semantic dependency
parsing. The semantic role labeler predicts
the full syntactic paths that connect predicates
with their arguments. This process is framed
as a linear assignment task, which allows
to control some well-formedness constraints.
For the syntactic part, we define a standard
arc-factored dependency model that predicts
the full syntactic tree. Finally, we employ dual
decomposition techniques to produce consis-
tent syntactic and predicate-argument struc-
tures while searching over a large space of
syntactic configurations. In experiments on
the CoNLL-2009 English benchmark we ob-
serve very competitive results.
1 Introduction
Semantic role labeling (SRL) is the task of identi-
fying the arguments of lexical predicates in a sen-
tence and labeling them with semantic roles (Gildea
and Jurafsky, 2002; Ma`rquez et al, 2008). SRL is
an important shallow semantic task in NLP since
predicate-argument relations directly represent se-
mantic properties of the type ?who? did ?what? to
?whom?, ?how?, and ?why? for events expressed by
predicates (typically verbs and nouns).
Predicate-argument relations are strongly related
to the syntactic structure of the sentence: the ma-
jority of predicate arguments correspond to some
syntactic constituent, and the syntactic structure that
connects an argument with the predicate is a strong
indicator of its semantic role. Actually, semantic
roles represent an abstraction of the syntactic form
of a predicative event. While syntactic functions of
arguments change with the form of the event (e.g.,
active vs. passive forms), the semantic roles of argu-
ments remain invariant to their syntactic realization.
Consequently, since the first works, SRL systems
have assumed access to the syntactic structure of the
sentence (Gildea and Jurafsky, 2002; Carreras and
Ma`rquez, 2005). A simple approach is to obtain
the parse trees as a pre-process to the SRL system,
which allows the use of unrestricted features of the
syntax. However, as in other pipeline approaches in
NLP, it has been shown that the errors of the syn-
tactic parser severely degrade the predictions of the
SRL model (Gildea and Palmer, 2002). A common
approach to alleviate this problem is to work with
multiple alternative syntactic trees and let the SRL
system optimize over any input tree or part of it
(Toutanova et al, 2008; Punyakanok et al, 2008).
As a step further, more recent work has proposed
parsing models that predict syntactic structure aug-
mented with semantic predicate-argument relations
(Surdeanu et al, 2008; Hajic? et al, 2009; Johansson,
2009; Titov et al, 2009; Llu??s et al, 2009), which is
the focus of this paper. These joint models should
favor the syntactic structure that is most consistent
with the semantic predicate-argument structures of
a sentence. In principle, these models can exploit
syntactic and semantic features simultaneously, and
could potentially improve the accuracy for both syn-
tactic and semantic relations.
One difficulty in the design of joint syntactic-
semantic parsing models is that there exist impor-
tant structural divergences between the two layers.
219
Mary loves to play guitar .
?
SBJ OPRD IM OBJ
P
ARG0 ARG1
ARG0
ARG1
Figure 1: An example . . .
Das et al (2012) . . .
Riedel and McCallum (2011) . . .
3 A Syntactic-Semantic Dependency
Model
We will describe structures of syntactic and seman-
tic dependencies with vectors of binary variables.
We will denote by yh,m,l a syntactic dependency
from head token h to dependant token m labeled
with syntactic function l. Similarly we will denote
by zp,a,r a semantic dependency between predicate
token p and argument token a labeled with seman-
tic role r. We will use y and z to denote vectors of
binary variables indexed by syntactic and semantic
dependencies, respectively.
A joint model for syntactic and semantic depen-
dency parsing could be defined as:
argmax
y,z
s syn(x,y) + s srl(x, z,y) .
In the equation, s syn(x,y) gives a score for the
syntactic tree y. In the literature, it is standard to
use arc-factored models defined as
s syn(x,y) =
?
yh,m,l=1
s syn(x, h,m, l) ,
where we overload s syn to be a function that
computes scores for individual labeled syntactic
dependencies. In discriminative models one has
s syn(x, h,m, l) = wsyn ? fsyn(x, h,m, l), where
fsyn is a feature vector for the syntactic dependency
and wsyn is a vector of parameters (McDonald et al,
2005).
The other term, s srl(x, z,y), gives a score for
a semantic dependency structure using the syntactic
structure y as features. Previous work has empiri-
cally proved the importance of exploiting syntactic
features in the semantic component (Gildea and Ju-
rafsky, 2002; Xue and Palmer, 2004; Punyakanok et
al., 2008). However, without further assumptions,
this property makes the optimization problem com-
putationally hard. One simple approximation is to
use a pipeline model: first compute the optimal syn-
tactic tree, and then optimize for the best semantic
structure given the syntactic tree. In the rest of the
paper we describe a method that searches over syn-
tactic and semantic dependency structures jointly.
We first impose the assumption that syntactic fea-
tures of the semantic component are restricted to the
syntactic path between a predicate and an argument,
following previous work (Johansson, 2009). For-
mally, for a predicate p, argument a and role r we
will define a vector of dependency indicators ?p,a,r
similar to the ones above: ?p,a,rh,m,l indicates if a de-
pendency ?h,m, l? is part of the syntactic path that
links predicate p with token a. Figure 1 gives an ex-
ample of one such paths. Given full syntactic and
semantic structures y and z it is trivial to construct a
vector ? that is a concatenation of vectors ?p,a,r for
all ?p, a, r? in z. We can now define a linear seman-
tic model as
s srl(x, z,?) =
?
zp,a,r=1
s srl(x, p, a, r,?p,a,r) ,
(1)
where s srl computes a score for a semantic de-
pendency ?p, a, r? together with its syntactic path
?p,a,r. As in the syntactic component, this function
is typically defined as a linear function over a set of
features of the semantic dependency and its path.
With this joint model, the inference problem can
be formulated as:
argmax
y,z,?
s syn(x,y) + s srl(x, z,?) (2)
subject to
cTree : y is a valid dependency tree
cRole : ?p, r :
?
a
zp,a,r ? 1
cArg : ?p, a :
?
r
zp,a,r ? 1
cPath : ?p, a, r : if zp,a,r = 1 then
?p,a,r is a path from p to a
otherwise ?p,a,r = 0
cSubtree : ?p, r, a : ?p,r,a is a subtree of y
Figure 1: A sentenc with synt ctic dependencies (top)
and semantic dependencies for the predicates ?loves? and
?play? (bottom). The thick arcs illustrate a structural di-
vergence where the argument ?Mary? is linked to ?play?
with a path involving three syntactic dependencies.
This is cl arly seen in dependency-based representa-
tions of syntax and semantic roles (Surdeanu et al,
2008), such as in the example in Figure 1: the con-
struct ?loves to? causes the argument ?Mary? to be
syntactically distant fro the predicate ?play?. Lin-
guistic phenomena such as auxiliary verbs, control
and raising, typically result in syntactic structures
where semantic arguments are not among the direct
depend ts of their predicate ?e.g., abou 25% of
arguments are distant in the English development set
of the CoNLL-2009 shared task. Besides, standard
models for dependency parsing crucially depend on
arc factorizations of the dependency structure (Mc-
Donald et al, 2005; Nivre and Nilsson, 2005), other-
wise their computational properties break. Hence, it
is challenging to define efficient methods for syntac-
tic and semantic dependency parsing that can exploit
features of both layers simultaneously. In this paper
we propose a method for this joint task.
In our method we define predicate-centric seman-
tic models that, rather than predicting just the ar-
gument that realizes each semantic role, they pre-
dict the full syntactic path that connects the predi-
cate with the argument. We show how efficient pre-
dictions with these models can be made using as-
signment algorithms in bipartite graphs. Simulta-
neously, we use a standard arc-factored dependency
model that predicts the full syntactic tree of the sen-
tence. Finally, we employ dual decomposition tech-
niques (Koo et al, 2010; Rush et al, 2010; Sontag
et al, 2010) to find agreement between the full de-
pendency tree and the partial syntactic trees linking
each predicate with its arguments. In summary, the
main contributions of this paper are:
? We frame SRL as a weighted assignment prob-
lem in a bipartite graph. Under this framework
we can control assignment constraints between
roles and arguments. Key to our method, we
c n efficiently search over a large space of syn-
actic realizations of se antic argument .
? We solve joint inference of syntactic and se-
mantic dependencies with a dual decomposi-
tion method, similar to that of Koo et al (2010).
Our system produces consistent syntactic and
predicate-argument structures while searching
over a large space of syntactic configurations.
In the experimental section we compare joint
and pipeline models. The final results of our joint
syntactic-semantic system are competitive with the
state-of-the-art and improve over the best results
published by a joint method on the CoNLL-2009
English dataset.
2 A Syntactic-S mantic Dependency
Model
We first describe how we represent structures of syn-
tactic and semantic dependencies like the one in Fig-
ure 1. Throughout the paper, we will assume a fixed
input sentence x with n tokens where lexical predi-
cates are marked. We will also assume fixed sets of
syntactic functions Rsyn and semantic roles Rsem.
We will represent depencency structures using vec-
tors of binary variables. A variable yh,m,l will in-
dicate the presence of a syntactic dependency from
head token h to dependant tokenm labeled with syn-
tactic function l. Then, a syntactic tree will be de-
noted as a vector y of variables indexed by syntactic
dependencies. Similarly, a variable zp,a,r will indi-
cate the presence of a semantic dependency between
predicate token p and argument token a labeled with
semantic role r. We will represent a semantic role
structure as a vector z indexed by semantic depen-
dencies. Whenever we enumerate syntactic depen-
dencies ?h,m, l? we will assume that they are in the
valid range for x, i.e. 0 ? h ? n, 1 ? m ? n,
h 6= m and l ? Rsyn where h = 0 stands for a
special root token. Similarly, for semantic depen-
dencies ?p, a, r? we will assume that p points to a
predicate of x, 1 ? a ? n and r ? Rsem.
220
A joint model for syntactic and semantic depen-
dency parsing could be defined as:
argmax
y,z
s syn(x,y) + s srl(x, z,y) . (1)
In the equation, s syn(x,y) gives a score for the
syntactic tree y. In the literature, it is standard to
use arc-factored models defined as
s syn(x,y) =
?
yh,m,l=1
s syn(x, h,m, l) , (2)
where we overload s syn to be a function that
computes scores for individual syntactic depen-
dencies. In linear discriminative models one has
s syn(x, h,m, l) = wsyn ? fsyn(x, h,m, l), where
fsyn is a feature vector for a syntactic dependency
and wsyn is a vector of parameters (McDonald et
al., 2005). In Section 6 we describe how we trained
score functions with discriminative methods.
The other term in Eq. 1, s srl(x, z,y), gives a
score for a semantic dependency structure z using
features of the syntactic structure y. Previous work
has empirically proved the importance of exploit-
ing syntactic features in the semantic component
(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;
Punyakanok et al, 2008). However, without further
assumptions, this property makes the optimization
problem computationally hard. One simple approx-
imation is to use a pipeline model: first compute the
optimal syntactic tree y, and then optimize for the
best semantic structure z given y. In the rest of the
paper we describe a method that searches over syn-
tactic and semantic dependency structures jointly.
We first note that for a fixed semantic dependency,
the semantic component will typically restrict the
syntactic features representing the dependency to a
specific subtree of y. For example, previous work
has restricted such features to the syntactic path that
links a predicate with an argument (Moschitti, 2004;
Johansson, 2009), and in this paper we employ this
restriction. Figure 1 gives an example of a sub-
tree, where we highlight the syntactic path that con-
nects the semantic dependency between ?play? and
?Mary? with role ARG0.
Formally, for a predicate p, argument a and role
r we define a local syntactic subtree pip,a,r repre-
sented as a vector: pip,a,rh,m,l indicates if a dependency
?h,m, l? is part of the syntactic path that links pred-
icate p with token a and role r.1 Given full syntactic
and semantic structures y and z it is trivial to con-
struct a vector pi that concatenates vectors pip,a,r for
all ?p, a, r? in z. The semantic model becomes
s srl(x, z,pi) =
?
zp,a,r=1
s srl(x, p, a, r,pip,a,r) ,
(3)
where s srl computes a score for a semantic de-
pendency ?p, a, r? together with its syntactic path
pip,a,r. As in the syntactic component, this function
is typically defined as a linear function over a set of
features of the semantic dependency and its path.
The inference problem of our joint model is:
argmax
y,z,pi
s syn(x,y) + s srl(x, z,pi) (4)
subject to
cTree : y is a valid dependency tree
cRole : ?p, r : ?
a
zp,a,r ? 1
cArg : ?p, a : ?
r
zp,a,r ? 1
cPath : ?p, a, r : if zp,a,r = 1 then
pip,a,r is a path from p to a,
otherwise pip,a,r = 0
cSubtree : ?p, a, r : pip,a,r is a subtree of y
Constraint cTree dictates that y is a valid depen-
dency tree; see (Martins et al, 2009) for a detailed
specification. The next two sets of constraints con-
cern the semantic structure only. cRole imposes that
each semantic role is realized at most once.2 Con-
versely, cArg dictates that an argument can realize
at most one semantic role in a predicate. The final
two sets of constraints model the syntactic-semantic
interdependencies. cPath imposes that each pip,a,r
represents a syntactic path between p and a when-
ever there exists a semantic relation. Finally, cSub-
tree imposes that the paths in pi are consistent with
the full syntactic structure, i.e. they are subtrees.
1In this paper we say that structures pip,a,r are paths from
predicates to arguments, but they could be more general sub-
trees. The condition to build a joint system is that these subtrees
must be parseable in the way we describe in Section 3.1.
2In general a semantic role can be realized with more than
one argument, though it is rare. It is not hard to modify our
framework to allow for a maximum number of occurrences of a
semantic role.
221
In Section 3 we define a process that optimizes
the semantic structure ignoring constraint cSubtree.
Then in Section 4 we describe a dual decomposition
method that uses the first process repeatedly to solve
the joint problem.
3 SRL as Assignment
In this section we frame the problem of finding se-
mantic dependencies as a linear assignment task.
The problem we optimize is:
argmax
z,pi
s srl(x, z,pi) (5)
subject to cRole, cArg, cPath
In this case we dropped the full syntactic structure
y from the optimization in Eq. 4, as well as the
corresponding constraints cTree and cSubtree. As
a consequence, we note that the syntactic paths pi
are not tied to any consistency constraint other than
each of the paths being a well-formed sequence of
dependencies linking the predicate to the argument.
In other words, the optimal solution in this case does
not guarantee that the set of paths from a predicate to
all of its arguments satisfies tree constraints. We first
describe how these paths can be optimized locally.
Then we show how to find a solution z satisfying
cRole and cArg using an assignment algorithm.
3.1 Local Optimization of Syntactic Paths
Let z? and p?i be the optimal values of Eq. 5. For any
?p, a, r?, let
p?ip,a,r = argmax
pip,a,r
s srl(x, p, a, r,pip,a,r) . (6)
For any ?p, a, r? such that z?p,a,r = 1 it has to be that
p?ip,a,r = p?ip,a,r. If this was not true, replacing p?ip,a,r
with p?ip,a,r would improve the objective of Eq. 5
without violating the constraints, thus contradicting
the hypothesis about optimality of p?i. Therefore, for
each ?p, a, r? we can optimize its best syntactic path
locally as defined in Eq. 6.
In this paper, we will assume access to a list of
likely syntactic paths for each predicate p and argu-
ment candidate a, such that the optimization in Eq. 6
can be solved explicitly by looping over each path in
the list. The main advantage of this method is that,
since paths are precomputed, our model can make
unrestricted use of syntactic path features.
(1)
Mary
(2)
plays
(3)
guitar (4)NULL (5)NULL (6)NULL
(1)
ARG0
(2)
ARG1
(3)
ARG2 (4)NULL (5)NULL (6)NULL
W1,1
W4,2W2,3
W3,4
W5,5 W6,6
Figure 2: Illustration of the assignment graph for the sen-
tence ?Mary plays guitar?, where the predicate ?plays?
can have up to three roles: ARG0 (agent), ARG1 (theme)
and ARG2 (benefactor). Nodes labeled NULL represent
a null role or token. Highlighted edges are the correct
assignment.
It is simple to employ a probabilistic syntactic de-
pendency model to create the list of likely paths for
each predicate-argument pair. In the experiments we
explore this approach and show that with an average
of 44 paths per predicate we can recover 86.2% of
the correct paths.
We leave for future work the development of ef-
ficient methods to recover the most likely syntactic
structure linking an argument with its predicate.
3.2 The Assignment Algorithm
Coming back to solving Eq. 5, it is easy to see
that an optimal solution satisfying constraints cRole
and cArg can be found with a linear assignment
algorithm. The process we describe determines
the predicate-argument relations separately for each
predicate. Assume a bipartite graph of size N with
role nodes r1 . . . rN on one side and argument nodes
a1 . . . aN on the other side. Assume also a matrix of
non-negative scoresWi,j corresponding to assigning
argument aj to role ri. A linear assignment algo-
rithm finds a bijection f : i? j from roles to argu-
ments that maximizes?Ni=1Wi,f(i). The Hungarian
algorithm finds the exact solution to this problem in
O(N3) time (Kuhn, 1955; Burkard et al, 2009).
All that is left is to construct a bipartite graph rep-
resenting predicate roles and sentence tokens, such
that some roles and tokens can be left unassigned,
which is a common setting for assignment tasks. Al-
gorithm 1 describes a procedure for constructing a
weighted bipartite graph for SRL, and Figure 2 il-
lustrates an example of a bipartite graph. We then
222
Algorithm 1 Construction of an Assignment Graph
for Semantic Role Labeling
Let p be a predicate with k possible roles. Let n be the
number of argument candidates in the sentence. This al-
gorithm creates a bipartite graph withN = n+k vertices
on each side.
1. Create role vertices ri for i = 1 . . . N , where
? for 1 ? i ? k, ri is the i-th role,
? for 1 ? i ? n, rk+i is a special NULL role.
2. Create argument vertices aj for j = 1 . . . N , where
? for 1 ? j ? n, aj is the j-th argument candidate,
? for 1 ? j ? k, an+j is a special NULL argument.
3. Define a matrix of model scores S ? R(k+1)?n:
(a) Optimization of syntactic paths:
For 1 ? i ? k, 1 ? j ? n
Si,j = max
pip,aj,ri
s srl(x, p, aj , ri,pip,aj ,ri)
(b) Scores of NULL assignments3:
For 1 ? j ? n
Sk+1,j = 0
4. Let S0 = mini,j Si,j , the minimum of any score
in S. Define a matrix of non-negative scores W ?
RN?N as follows:
(a) for 1 ? i ? k, 1 ? j ? n
Wi,j = Si,j ? S0
(b) for k < i ? N, 1 ? j ? n
Wi,j = Sk+1,j ? S0
(c) for 1 < i ? N, n < j ? N
Wi,j = 0
run the Hungarian algorithm on the weighted graph
and obtain a bijection f : ri ? aj , from which it
is trivial to recover the optimal solution of Eq. 5.
Finally, we note that it is simple to allow for multi-
ple instances of a semantic role by adding more role
nodes in step 1; it would be straightforward to add
penalties in step 3 for multiple instances of roles.
4 A Dual Decomposition Algorithm
We now present a dual decomposition method to op-
timize Eq. 4, that uses the assignment algorithm pre-
sented above as a subroutine. Our method is sim-
ilar to that of Koo et al (2010), in the sense that
3In our model we fix the score of null assignments to 0. It is
straightforward to compute a discriminative score instead.
our joint optimization can be decomposed into two
sub-problems that need to agree on the syntactic
dependencies they predict. For a detailed descrip-
tion of dual decomposition methods applied to NLP
see (Sontag et al, 2010; Rush et al, 2010).
We note that in Eq. 4 the constraint cSubtree ties
the syntactic and semantic structures, imposing that
any path pip,a,r that links a predicate p with an argu-
ment a must be a subtree of the full syntactic struc-
ture y. Formally the set of constraints is:
yh,m,l ? pip,a,rh,m,l ? p, a, r, h,m, l .
These constraints can be compactly written as
c ? yh,m,l ?
?
p,a,r
pip,a,rh,m,l ? h,m, l ,
where c is a constant equal to the number of dis-
tinct semantic dependencies ?p, a, r?. In addition,
we can introduce a vector non-negative slack vari-
ables ? with a component for each syntactic depen-
dency ?h,m,l, turning the constraints into:
c ? yh,m,l ?
?
p,a,r
pip,a,rh,m,l ? ?h,m,l = 0 ? h,m, l
We can now rewrite Eq. 4 as:
argmax
y,z,pi,??0
s syn(x,y) + s srl(x, z,pi) (7)
subject to
cTree, cRole, cArg, cPath
?h,m, l : c ? yh,m,l ?
?
p,a,r
pip,a,rh,m,l ? ?h,m,l = 0
As in Koo et al (2010), we will relax subtree cons-
traints by introducing a vector of Lagrange multipli-
ers ? indexed by syntactic dependencies, i.e. each
coordinate ?h,m,l is a Lagrange multiplier for the
constraint associated with ?h,m, l?. The Lagrangian
of the problem is:
L(y, z,pi, ?,?)= s syn(x,y) + s srl(x, z,pi)
+ ? ?
(
c ? y ?
?
p,a,r
pip,a,r ? ?
)
(8)
We can now formulate Eq. 7 as:
max
y,z,pi,??0
s.t. cTree,cRole,cArg,cPath
c?y??p,a,r pip,a,r??=0
L(y, z,pi, ?,?) (9)
223
This optimization problem has the property that its
optimum value is the same as the optimum of Eq. 7
for any value of ?. This is because whenever the
constraints are satisfied, the terms in the Lagrangian
involving ? are zero. If we remove the subtree con-
straints from Eq. 9 we obtain the dual objective:
D(?) = max
y,z,pi,??0
s.t. cTree,cRole,cArg,cPath
L(y, z,pi, ?,?) (10)
= max
y s.t. cTree
(
s syn(x,y) + c ? y ? ?
)
+ maxz,pi
s.t. cRole,cArg,cPath
(
s srl(x, z,pi)? ? ?
?
p,a,r
pip,a,r
)
+ max
??0
(?? ? ?) (11)
The dual objective is an upper bound to the opti-
mal value of primal objective of Eq. 7. Thus, we
are interested in finding the minimum of the dual in
order to tighten the upper-bound. We will solve
min
?
D(?) (12)
using a subgradient method. Algorithm 2 presents
pseudo-code. The algorithm takes advantage of the
decomposed form of the dual in Eq. 11, where
we have rewritten the Lagrangian such that syntac-
tic and semantic structures appear in separate terms.
This will allow to compute subgradients efficiently.
In particular, the subgradient of D at a point ? is:
?(?) = c ? y? ?
?
p,a,r
p?ip,a,r ? ?? (13)
where
y? = argmax
y s.t. cTree
(
s syn(x,y) + c ? y ? ?
) (14)
z?, p?i = argmax
z,pi s.t.
cRole,cArg,cPath
s srl(x, z,pi)? ??
?
p,a,r
pip,a,r (15)
?? = argmax
??0
?? ? ? (16)
Whenever p?i is consistent with y? the subgradient
will be zero and the method will converge. When
paths p?i contain a dependency ?h,m, l? that is in-
consistent with y?, the associated dual ?h,m,l will in-
crease, hence lowering the score of all paths that use
?h,m, l? at the next iteration; at same time, the to-
tal score for that dependency will increase, favoring
syntactic dependency structures alternative to y?. As
Algorithm 2 A dual-decomposition algorithm for
syntactic-semantic dependency parsing
Input: x, a sentence; T , number of iterations;
Output: syntactic and semantic structures y? and z?
Notation: we use cSem= cRole ? cArg ? cPath
1: ?1 = 0 # initialize dual variables
2: c =number of distinct ?h,m, l? in x
3: for t = 1 . . . T do
4: y? = argmaxy s.t. cTree
(
s syn(x,y) + c ? ?t ? y
)
5: z?, p?i = argmax z,pi
s.t. cSem
(
s srl(x, z,pi)
??t ??p,a,r pip,a,r
)
6: ?t+1 = ?t # dual variables for the next iteration
7: Set ?t, the step size of the current iteration
8: for each ?h,m, l? do
9: q = ?p,a,r ?p,a,rh,m,l # num. paths using ?h,m, l?
10: if q > 0 and y?h,m,l = 0 then
11: ?t+1h,m,l = ?t+1h,m,l + ?tq
12: break if ?t+1 = ?t # convergence
13: return y?, z?
in previous work, in the algorithm a parameter ?t
controls the size of subgradient steps at iteration t.
The key point of the method is that solutions to
Eq. 14 and 15 can be computed efficiently using sep-
arate processes. In particular, Eq. 14 corresponds
to a standard dependency parsing problem, where
for each dependency ?h,m, l? we have an additional
score term c ??h,m,l ?in our experiments we use the
projected dependency parsing algorithm by (Eisner,
2000). To calculate Eq. 15 we use the assignment
method described in Section 3, where it is straight-
forward to introduce additional score terms ??h,m,l
to every factor pip,a,rh,m,l. It can be shown that wheneverthe subgradient method converges, the solutions y?
and z? are the optimal solutions to our original prob-
lem in Eq. 4 (see (Koo et al, 2010) for a justifi-
cation). In practice we run the subgradient method
for a maximum number of iterations, and return the
solutions of the last iteration if it does not converge.
5 Related Work
Recently, there have been a number of approaches
to joint parsing of syntactic and semantic dependen-
cies, partly because of the availability of treebanks in
this format popularized by the CoNLL shared tasks
(Surdeanu et al, 2008; Hajic? et al, 2009).
Like in our method, Johansson (2009) defined a
model that exploits features of a semantic depen-
224
dency together with the syntactic path connecting
the predicate and the argument. That method uses an
approximate parsing algorithm that employs k-best
inference and beam search. Similarly, Llu??s et al
(2009) defined a joint model that forces the predi-
cate structure to be represented in the syntactic de-
pendency tree, by enriching arcs with semantic in-
formation. The semantic component uses features of
pre-computed syntactic structures that may diverge
from the joint structure. In contrast, our joint pars-
ing method is exact whenever the dual decomposi-
tion algorithm converges.
Titov et al (2009) augmented a transition-based
dependency parser with operations that produce
synchronous derivations of syntactic and semantic
structures. Instead of explicitly representing seman-
tic dependencies together with a syntactic path, they
induce latent representations of the interactions be-
tween syntactic and semantic layers.
In all works mentioned the model has no con-
trol of assignment constraints that disallow label-
ing multiple arguments with the same semantic role.
Punyakanok et al (2008) first introduced a system
that explicitly controls these constraints, as well as
other constraints that look at pairwise assignments
which we can not model. They solve SRL using
general-purpose Integer Linear Programming (ILP)
methods. In similar spirit, Riedel and McCallum
(2011) presented a model for extracting structured
events that controls interactions between predicate-
argument assignments. They take into account pair-
wise assignments and solve the optimization prob-
lem with dual decomposition. More recently, Das
et al (2012) proposed a dual decomposition method
that deals with several assignment constraints for
predicate-argument relations. Their method is an
alternative to general ILP methods. To our knowl-
edge, our work is the first that frames SRL as a linear
assignment task, for which simple and exact algo-
rithms exist. We should note that these works model
predicate-argument relations with assignment con-
straints, but none of them predicts the underlying
syntactic structure.
Our dual decomposition method follows from that
of Koo et al (2010). In both cases two separate pro-
cesses predict syntactic dependency structures, and
the dual decomposition algorithm seeks agreement
at the level of individual dependencies. One dif-
ference is that our semantic process predicts partial
syntax (restricted to syntactic paths connecting pred-
icates and arguments), while in their case each of the
two processes predicts the full set of dependencies.
6 Experiments
We present experiments using our syntactic-
semantic parser on the CoNLL-2009 Shared Task
English benchmark (Hajic? et al, 2009). It consists
of the usual WSJ training/development/test sections
mapped to dependency trees, augmented with se-
mantic predicate-argument relations from PropBank
(Palmer et al, 2005) and NomBank (Meyers et al,
2004) also represented as dependencies. It also con-
tains a PropBanked portion of the Brown corpus as
an out-of-domain test set.
Our goal was to evaluate the contributions of pars-
ing algorithms in the following configurations:
Base Pipeline Runs a syntactic parser and then runs
an SRL parser constrained to paths of the best
syntactic tree. In the SRL it only enforces con-
straint cArg, by simply classifying the candi-
date argument in each path into one of the pos-
sible semantic roles or as NULL.
Pipeline with Assignment Runs the assignment al-
gorithm for SRL, enforcing constraints cRole
and cArg, but constrained to paths of the best
syntactic tree.
Forest Runs the assignment algorithm for SRL on
a large set of precomputed syntactic paths, de-
scribed below. This configuration corresponds
to running Dual Decomposition for a single it-
eration, and is not guaranteed to predict consis-
tent syntactic and semantic structures.
Dual Decomposition (DD) Runs dual decomposi-
tion using the assignment algorithm on the set
of precomputed paths. Syntactic and semantic
structures are consistent when it reaches con-
vergence.
All four systems used the same type of discrimina-
tive scorers and features. Next we provide details
about these systems. Then we present the results.
6.1 Implementation
Syntactic model We used two discriminative arc-
factored models for labeled dependency parsing: a
225
first-order model, and a second-order model with
grandchildren interactions, both reimplementations
of the parsers by McDonald et al (2005) and Car-
reras (2007) respectively. In both cases we used
projective dependency parsing algorithms based on
(Eisner, 2000).4 To learn the models, we used a
log-linear loss function following Koo et al (2007),
which trains probabilistic discriminative parsers.
At test time, we used the probabilistic parsers to
compute marginal probabilities p(h,m, l | x), us-
ing inside-outside algorithms for first/second-order
models. Hence, for either of the parsing models, we
always obtain a table of first-order marginal scores,
with one score per labeled dependency. Then we
run first-order inference with these marginals to ob-
tain the best tree. We found that the higher-order
parser performed equally well on development us-
ing this method as using second-order inference to
predict trees: since we run the parser multiple times
within Dual Decomposition, our strategy results in
faster parsing times.
Precomputed Paths Both Forest and Dual De-
composition run assignment on a set of precomputed
paths, and here we explain how we build it. We first
observed that 98.4% of the correct arguments in de-
velopment data are either direct descendants of the
predicate, direct descendants of an ancestor of the
predicate, or an ancestor of the predicate.5 All meth-
ods we test are restricted to this syntactic scope. To
generate a list of paths, we did as follows:
? Calculate marginals of unlabeled dependencies
using the first-order parser: p(h,m | x) =?
l p(h,m, l | x). Note that for each m, the
probabilities p(h,m|x) for all h form a distri-
bution (i.e. they sum to one). Then, for eachm,
keep the most-likely dependencies that cover at
least 90% of the mass, and prune the rest.
? Starting from a predicate p, generate a path
by taking any number of dependencies that as-
cend, and optionally adding one dependency
that descends. We constrained paths to be pro-
jective, and to have a maximum number of 6
4Our method allows to use non-projective dependency pars-
ing methods seamlessly.
5This is specific to CoNLL-2009 data for English. In gen-
eral, for other languages the coverage of these rules may be
lower. We leave this question to future work.
ascendant dependencies.
? Label each unlabeled edge ?h,m? in the paths
with l = argmaxl p(h,m, l | x).
On development data, this procedure generated an
average of 43.8 paths per predicate that cover 86.2%
of the correct paths. In contrast, enumerating paths
of the single-best tree covers 79.4% of correct paths
for the first-order parser, and 82.2% for the second-
order parser.6
SRL model We used a discriminative model with
similar features to those in the system of Johansson
(2009). In addition, we included the following:
? Unigram/bigram/trigram path features. For
all n-grams in the syntactic path, patterns
of words and POS tags (e.g., mary+loves+to,
mary+VB+to).
? Voice features. The predicate voice together
with the word/POS of the argument (e.g., pas-
sive+mary).
? Path continuity. Count of non-consecutive to-
kens in a predicate-argument path.
To train SRL models we used the averaged per-
ceptron (Collins, 2002). For the base pipeline we
trained standard SRL classifiers. For the rest of
models we used the structured Perceptron running
the assignment algorithm as inference routine. In
this case, we generated a large set of syntactic paths
for training using the procedure described above,
and we set the loss function to penalize mistakes in
predicting the semantic role of arguments and their
syntactic path.
Dual Decomposition We added a parameter ?
weighting the syntactic and semantic components of
the model as follows:
(1? ?) s syn(x,y) + ? s srl(x, z,pi) .
As syntactic scores we used normalized marginal
probabilities of dependencies, either from the first
or the higher-order parser. The scores of all factors
of the SRL model were normalized at every sen-
tence to be between -1 and 1. The rest of details
6One can evaluate the maximum recall on correct arguments
that can be obtained, irrespective of whether the syntactic path
is correct: for the set of paths it is 98.3%, while for single-best
trees it is 91.9% and 92.7% for first and second-order models.
226
o LAS UAS semp semr semF1 sempp
Pipeline 1 85.32 88.86 86.23 67.67 75.83 45.64
w. Assig. 1 85.32 88.86 84.08 71.82 77.47 51.17
Forest - - - 80.67 73.60 76.97 51.33
Pipeline 2 87.77 90.96 87.07 68.65 76.77 47.07
w. Assig. 2 87.77 90.96 85.21 73.41 78.87 53.80
Table 1: Results on development for the baseline and as-
signment pipelines, running first and second-order syn-
tactic parsers, and the Forest method. o indicates the or-
der of syntactic inference.
of the method were implemented following Koo et
al. (2010), including the strategy for decreasing the
step size ?t. We ran the algorithm for up to 500 it-
erations, with initial step size of 0.001.
6.2 Results
To evaluate syntactic dependencies we use unla-
beled attachment score (UAS), i.e., the percentage
of words with the correct head, and labeled attach-
ment scores (LAS), i.e., the percentage of words
with the correct head and syntactic label. Semantic
predicate-argument relations are evaluated with pre-
cision (semp), recall (semr) and F1 measure (semF1)
at the level of labeled semantic dependencies. In ad-
dition, we measure the percentage of perfectly pre-
dicted predicate structures (sempp).7
Table 1 shows the results on the development set
for our three first methods. We can see that the
pipeline methods running assignment improve over
the baseline pipelines in semantic F1 by about 2
points, due to the application of the cRole constraint.
The Forest method also shows an improvement in
recall of semantic roles with respect to the pipeline
methods. Presumably, the set of paths available in
the Forest model allows to recognize a higher num-
ber of arguments at an expense of a lower preci-
sion. Regarding the percentage of perfect predicate-
argument structures there is a remarkable improve-
ment in the systems that apply the full set of con-
7Our evaluation metrics differ slightly from the official met-
ric at CoNLL-2009. That metric considers predicate senses
as special semantic dependencies and, thus, it includes them
in the calculation of the evaluation metrics. In this paper, we
are not addressing predicate sense disambiguation and, conse-
quently, we ignore predicate senses when presenting evaluation
results. When we report the performance of CoNLL systems,
their scores will be noticeably lower than the scores reported at
the shared task. This is because predicate disambiguation is a
reasonably simple task with a very high baseline around 90%.
o ? LAS UAS semp semr semF1 sempp %conv
1 0.1 85.32 88.86 84.09 71.84 77.48 51.77 100
1 0.4 85.36 88.91 84.07 71.94 77.53 51.85 100
1 0.5 85.38 88.93 84.08 72.03 77.59 51.96 100
1 0.6 85.41 88.95 84.05 72.19 77.67 52.03 99.8
1 0.7 85.44 89.00 84.10 72.42 77.82 52.24 99.7
1 0.8 85.48 89.02 83.99 72.69 77.94 52.57 99.5
1 0.9 85.39 88.93 83.68 72.82 77.88 52.49 99.8
2 0.1 87.78 90.96 85.20 73.11 78.69 53.74 100
2 0.4 87.78 90.96 85.21 73.12 78.70 53.74 100
2 0.5 87.78 90.96 85.19 73.12 78.70 53.72 100
2 0.6 87.78 90.96 85.20 73.13 78.70 53.72 99.9
2 0.7 87.78 90.96 85.19 73.13 78.70 53.72 99.8
2 0.8 87.80 90.98 85.20 73.18 78.74 53.77 99.8
2 0.9 87.84 91.02 85.20 73.23 78.76 53.82 100
Table 2: Results of the dual decomposition method on
development data, for different values of the ? parame-
ter. o is the order of the syntactic parser. %conv is the
percentage of examples that converged.
straints using the assignment algorithm. We believe
that the cRole constraint that ensures no repeated
roles for a given predicate is a key factor to predict
the full set of arguments of a predicate.
The Forest configuration is the starting point to
run the dual decomposition algorithm. We ran ex-
periments for various values of the ? parameter. Ta-
ble 2 shows the results. We see that as we increase
?, the SRL component has more relative weight, and
the syntactic structure changes. The DD methods are
always able to improve over the Forest methods, and
find convergence in more than 99.5% of sentences.
Compared to the pipeline running assignment, DD
improves semantic F1 for first-order inference, but
not for higher-order inference, suggesting that 2nd
order predictions of paths are quite accurate. We
also observe slight benefits in syntactic accuracy.
Table 3 presents results of our system on the
test sets, where we run Pipeline with Assignment
and Dual Decomposition with our best configura-
tion (? = 0.8/0.9 for 1st/2nd order syntax). For
comparison, the table also reports the results of
the best CoNLL?2009 joint system, Merlo09 (Ges-
mundo et al, 2009), which proved to be very com-
petitive ranking third in the closed challenge. We
also include Llu??s09 (Llu??s et al, 2009), which is an-
other joint syntactic-semantic system from CoNLL?
2009.8 In the WSJ test DD obtains the best syntactic
accuracies, while the Pipeline obtains the best se-
8Another system to compare to is the joint system by Jo-
hansson (2009). Unfortunately, a direct comparison is not possi-
ble because it is evaluated on the CoNLL-2008 datasets, which
227
WSJ LAS UAS semp semr semF1 sempp
Llu??s09 87.48 89.91 73.87 67.40 70.49 39.68
Merlo09 88.79 91.26 81.00 76.45 78.66 54.80
Pipe-Assig 1st 86.85 89.68 85.12 73.78 79.05 54.12
DD 1st 87.04 89.89 85.03 74.56 79.45 54.92
Pipe-Assig 2nd 89.19 91.62 86.11 75.16 80.26 55.96
DD 2nd 89.21 91.64 86.01 74.84 80.04 55.73
Brown LAS UAS semp semr semF1 sempp
Llu??s09 80.92 85.96 62.29 59.22 60.71 29.79
Merlo09 80.84 86.32 68.97 63.06 65.89 38.92
Pipe-Assig 1st 80.96 86.58 72.91 60.16 65.93 38.44
DD 1st 81.18 86.86 72.53 60.76 66.12 38.13
Pipe-Assig 2nd 82.56 87.98 73.94 61.63 67.23 38.99
DD 2nd 82.61 88.04 74.12 61.59 67.28 38.92
Table 3: Comparative results on the CoNLL?2009 En-
glish test sets, namely the WSJ test (top table) and the
out of domain test from the Brown corpus (bottom table).
mantic F1. The bottom part of Table 3 presents re-
sults on the out-of-domain Brown test corpus. In this
case, DD obtains slightly better results than the rest,
both in terms of syntactic accuracy and semantic F1.
Table 4 shows statistical significance tests for the
syntactic LAS and semantic F1 scores of Table 3.
We have applied the sign test (Wackerly et al, 2007)
and approximate randomization tests (Yeh, 2000)
to all pairs of systems outputs. The differences be-
tween systems in the WSJ test can be considered
significant in almost all cases with p = 0.05. In
the Brown test set, results are more unstable and dif-
ferences are not significant in general, probably be-
cause of the relatively small size of that test.
Regarding running times, our implementation of
the baseline pipeline with 2nd order inference parses
the development set (1,334 sentences) in less than
7 minutes. Running assignment in the pipeline in-
creases parsing time by ?8% due to the overhead
from the assignment algorithm. The Forest method,
with an average of 61.3 paths per predicate, is?13%
slower than the pipeline due to the exploration of the
space of precomputed paths. Finally, Dual Decom-
position with 2nd order inference converges in 36.6
iterations per sentence on average. The first itera-
tion of DD has to perform roughly the same work
as Forest, while subsequent iterations only need to
re-parse the sentence with respect to the dual up-
are slightly different. However, note that Merlo09 is an applica-
tion of the system by Titov et al (2009). In that paper authors
report results on the CoNLL-2008 datasets, and they are com-
parable to Johansson?s.
WSJ Brown
ME PA1 DD1 PA2 DD2 ME PA1 DD1 PA2 DD2
LL ?? ?? ? ?? ??    ?? ??
ME ?? ?? ?? ?? ? ?
PA1 ?? ?? ?? ? ?? ??
DD1 ?? ?? ?? ??
PA2 ?
Table 4: Statistical tests of significance for LAS and
semF1 differences between pairs of systems from Table 3.
?/? = LAS difference is significant by the sign/ approxi-
mate randomization tests at 0.05 level. / = same mean-
ing for semF1 . The legend for systems is: LL: Llu??s09,
ME: Merlo09, PA1/2: Pipeline with Assignment, 1st/2nd
order, DD1/2: Dual Decomposition, 1st/2nd order.
dates, which are extremely sparse. Our current im-
plementation did not take advantage of the sparsity
of updates, and overall, DD was on average 13 times
slower than the pipeline running assignment and 15
times slower than the baseline pipeline.
7 Conclusion
We have introduced efficient methods to parse
syntactic dependency structures augmented with
predicate-argument relations, with two key ideas.
One is to predict the local syntactic structure that
links a predicate with its arguments, and seek agree-
ment with the full syntactic structure using dual
decomposition techniques. The second is to con-
trol linear assignment constraints in the predicate-
argument structure.
In experiments we observe large improvements
resulting from the assignment constraints. As for
the dual decomposition technique for joint parsing,
it does improve over the pipelines when we use a
first order parser. This means that in this configu-
ration the explicit semantic features help to find a
solution that is better in both layers. To some ex-
tent, this empirically validates the research objec-
tive of joint models. However, when we move to
second-order parsers the differences with respect to
the pipeline are insignificant. It is to be expected
that as syntactic parsers improve, the need of joint
methods is less critical. It remains an open question
to validate if large improvements can be achieved
by integrating syntactic-semantic features. To study
this question, it is necessary to have efficient pars-
ing algorithms for joint dependency structures. This
paper contributes with a method that has optimality
228
guarantees whenever it converges.
Our method can incorporate richer families of fea-
tures. It is straightforward to incorporate better se-
mantic representations of predicates and arguments
than just plain words, e.g. by exploiting WordNet or
distributional representations as in (Zapirain et al,
2013). Potentially, this could result in larger im-
provements in the performance of syntactic and se-
mantic parsing.
It is also necessary to experiment with differ-
ent languages, where the performance of syntactic
parsers is lower than in English, and hence there is
potential for improvement. Our treatment of local
syntactic structure that links predicates with argu-
ments, based on explicit enumeration of likely paths,
was simplistic. Future work should explore meth-
ods that model the syntactic structure linking predi-
cates with arguments: whenever this structure can be
parsed efficiently, our dual decomposition algorithm
can be employed to define an efficient joint system.
Acknowledgments
We thank the editor and the anonymous reviewers for
their valuable feedback. This work was financed by
the European Commission for the XLike project (FP7-
288342); and by the Spanish Government for project
OpenMT-2 (TIN2009-14675-C03-01), project Skater
(TIN2012-38584-C06-01), and a Ramo?n y Cajal contract
for Xavier Carreras (RYC-2008-02223).
References
Rainer Burkard, Mario Dell?Amico, and Silvano
Martello. 2009. Assignment Problems. Society for
Industrial and Applied Mathematics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 152?164, Ann Arbor, Michigan, June.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with Perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, pages 1?8, July.
Dipanjan Das, Andre? F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for
shallow semantic parsing with constraints. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings of
the main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, SemEval ?12, pages 209?217,
Stroudsburg, PA, USA.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 37?42, Boulder,
Colorado, June.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288, September.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of 40th Annual Meeting of the Association for
Computational Linguistics, pages 239?246, Philadel-
phia, Pennsylvania, USA, July.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning (CoNLL-2009): Shared Task, pages 1?18,
Boulder, Colorado, USA, June.
Richard Johansson. 2009. Statistical bistratal depen-
dency parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 561?569, Singapore, August.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 141?
150, Prague, Czech Republic, June.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288?1298, Cambridge, MA, October.
229
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(1-2):83?97.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2009): Shared Task,
pages 79?84, Boulder, Colorado, June.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145?159, June.
Andre? Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350, Sun-
tec, Singapore, August.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank Project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 335?
342, Barcelona, Spain, July.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 99?106, Ann Ar-
bor, Michigan, June.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, March.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(3):257?287, June.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 1?12,
Edinburgh, Scotland, UK., July.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11, Cambridge, MA, October.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2010. Introduction to dual decomposition for infer-
ence. In S. Sra, S. Nowozin, and S. J. Wright, editors,
Optimization for Machine Learning. MIT Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of the 21st international jont
conference on Artifical intelligence, IJCAI?09, pages
1562?1567.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191, June.
Dennis D. Wackerly, William Mendenhall, and
Richard L. Scheaffer, 2007. Mathematical Statis-
tics with Applications, chapter 15: Nonparametric
statistics. Duxbury Press.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain, July.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th conference on Computational linguis-
tics, pages 947?953.
Ben?at Zapirain, Eneko Agirre, Llu??s Ma`rquez, and Mihai
Surdeanu. 2013. Selectional preferences for semantic
role classification. Computational Linguistics, 39(3).
230
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1?8,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
Marta Recasens
?
Llu??s M
`
arquez
?
Emili Sapena
?
M. Ant
`
onia Mart??
?
Mariona Taul
?
e
?
V
?
eronique Hoste
?
Massimo Poesio

Yannick Versley
??
?: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu
?: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu
?: University College Ghent, veronique.hoste@hogent.be
: University of Essex/University of Trento, poesio@essex.ac.uk
??: University of T?ubingen, versley@sfs.uni-tuebingen.de
Abstract
This paper presents the SemEval-2010
task on Coreference Resolution in Multi-
ple Languages. The goal was to evaluate
and compare automatic coreference reso-
lution systems for six different languages
(Catalan, Dutch, English, German, Italian,
and Spanish) in four evaluation settings
and using four different metrics. Such a
rich scenario had the potential to provide
insight into key issues concerning corefer-
ence resolution: (i) the portability of sys-
tems across languages, (ii) the relevance of
different levels of linguistic information,
and (iii) the behavior of scoring metrics.
1 Introduction
The task of coreference resolution, defined as the
identification of the expressions in a text that re-
fer to the same discourse entity (1), has attracted
considerable attention within the NLP community.
(1) Major League Baseball sent its head of se-
curity to Chicago to review the second in-
cident of an on-field fan attack in the last
seven months. The league is reviewing se-
curity at all ballparks to crack down on
spectator violence.
Using coreference information has been shown to
be beneficial in a number of NLP applications
including Information Extraction (McCarthy and
Lehnert, 1995), Text Summarization (Steinberger
et al, 2007), Question Answering (Morton, 1999),
and Machine Translation. There have been a few
evaluation campaigns on coreference resolution in
the past, namely MUC (Hirschman and Chinchor,
1997), ACE (Doddington et al, 2004), and ARE
(Orasan et al, 2008), yet many questions remain
open:
? To what extent is it possible to imple-
ment a general coreference resolution system
portable to different languages? How much
language-specific tuning is necessary?
? How helpful are morphology, syntax and se-
mantics for solving coreference relations?
How much preprocessing is needed? Does its
quality (perfect linguistic input versus noisy
automatic input) really matter?
? How (dis)similar are different coreference
evaluation metrics?MUC, B-CUBED,
CEAF and BLANC? Do they all provide the
same ranking? Are they correlated?
Our goal was to address these questions in a
shared task. Given six datasets in Catalan, Dutch,
English, German, Italian, and Spanish, the task
we present involved automatically detecting full
coreference chains?composed of named entities
(NEs), pronouns, and full noun phrases?in four
different scenarios. For more information, the
reader is referred to the task website.
1
The rest of the paper is organized as follows.
Section 2 presents the corpora from which the task
datasets were extracted, and the automatic tools
used to preprocess them. In Section 3, we describe
the task by providing information about the data
format, evaluation settings, and evaluation met-
rics. Participating systems are described in Sec-
tion 4, and their results are analyzed and compared
in Section 5. Finally, Section 6 concludes.
2 Linguistic Resources
In this section, we first present the sources of the
data used in the task. We then describe the auto-
matic tools that predicted input annotations for the
coreference resolution systems.
1
http://stel.ub.edu/semeval2010-coref
1
Training Development Test
#docs #sents #tokens #docs #sents #tokens #docs #sents #tokens
Catalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260
Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007
English 229 3,648 79,060 39 741 17,044 85 1,141 24,206
German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287
Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586
Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040
Table 1: Size of the task datasets.
2.1 Source Corpora
Catalan and Spanish The AnCora corpora (Re-
casens and Mart??, 2009) consist of a Catalan and
a Spanish treebank of 500k words each, mainly
from newspapers and news agencies (El Peri?odico,
EFE, ACN). Manual annotation exists for ar-
guments and thematic roles, predicate semantic
classes, NEs, WordNet nominal senses, and coref-
erence relations. AnCora are freely available for
research purposes.
Dutch The KNACK-2002 corpus (Hoste and De
Pauw, 2006) contains 267 documents from the
Flemish weekly magazine Knack. They were
manually annotated with coreference information
on top of semi-automatically annotated PoS tags,
phrase chunks, and NEs.
English The OntoNotes Release 2.0 corpus
(Pradhan et al, 2007) covers newswire and broad-
cast news data: 300k words from The Wall Street
Journal, and 200k words from the TDT-4 col-
lection, respectively. OntoNotes builds on the
Penn Treebank for syntactic annotation and on the
Penn PropBank for predicate argument structures.
Semantic annotations include NEs, words senses
(linked to an ontology), and coreference informa-
tion. The OntoNotes corpus is distributed by the
Linguistic Data Consortium.
2
German The T?uBa-D/Z corpus (Hinrichs et al,
2005) is a newspaper treebank based on data taken
from the daily issues of ?die tageszeitung? (taz). It
currently comprises 794k words manually anno-
tated with semantic and coreference information.
Due to licensing restrictions of the original texts, a
taz-DVD must be purchased to obtain a license.
2
Italian The LiveMemories corpus (Rodr??guez
et al, 2010) will include texts from the Italian
Wikipedia, blogs, news articles, and dialogues
2
Free user license agreements for the English and German
task datasets were issued to the task participants.
(MapTask). They are being annotated according
to the ARRAU annotation scheme with coref-
erence, agreement, and NE information on top
of automatically parsed data. The task dataset
included Wikipedia texts already annotated.
The datasets that were used in the task were ex-
tracted from the above-mentioned corpora. Ta-
ble 1 summarizes the number of documents
(docs), sentences (sents), and tokens in the train-
ing, development and test sets.
3
2.2 Preprocessing Systems
Catalan, Spanish, English Predicted lemmas
and PoS were generated using FreeLing
4
for
Catalan/Spanish and SVMTagger
5
for English.
Dependency information and predicate semantic
roles were generated with JointParser, a syntactic-
semantic parser.
6
Dutch Lemmas, PoS and NEs were automat-
ically provided by the memory-based shallow
parser for Dutch (Daelemans et al, 1999), and de-
pendency information by the Alpino parser (van
Noord et al, 2006).
German Lemmas were predicted by TreeTagger
(Schmid, 1995), PoS and morphology by RFTag-
ger (Schmid and Laws, 2008), and dependency in-
formation by MaltParser (Hall and Nivre, 2008).
Italian Lemmas and PoS were provided by
TextPro,
7
and dependency information by Malt-
Parser.
8
3
The German and Dutch training datasets were not com-
pletely stable during the competition period due to a few er-
rors. Revised versions were released on March 2 and 20, re-
spectively. As to the test datasets, the Dutch and Italian doc-
uments with formatting errors were corrected after the eval-
uation period, with no variations in the ranking order of sys-
tems.
4
http://www.lsi.upc.es/ nlp/freeling
5
http://www.lsi.upc.edu/ nlp/SVMTool
6
http://www.lsi.upc.edu// xlluis/?x=cat:5
7
http://textpro.fbk.eu
8
http://maltparser.org
2
3 Task Description
Participants were asked to develop an automatic
system capable of assigning a discourse entity to
every mention,
9
thus identifying all the NP men-
tions of every discourse entity. As there is no
standard annotation scheme for coreference and
the source corpora differed in certain aspects, the
coreference information of the task datasets was
produced according to three criteria:
? Only NP constituents and possessive deter-
miners can be mentions.
? Mentions must be referential expressions,
thus ruling out nominal predicates, appos-
itives, expletive NPs, attributive NPs, NPs
within idioms, etc.
? Singletons are also considered as entities
(i.e., entities with a single mention).
To help participants build their systems, the
task datasets also contained both gold-standard
and automatically predicted linguistic annotations
at the morphological, syntactic and semantic lev-
els. Considerable effort was devoted to provide
participants with a common and relatively simple
data representation for the six languages.
3.1 Data Format
The task datasets as well as the participants?
answers were displayed in a uniform column-
based format, similar to the style used in previous
CoNLL shared tasks on syntactic and semantic de-
pendencies (2008/2009).
10
Each dataset was pro-
vided as a single file per language. Since corefer-
ence is a linguistic relation at the discourse level,
documents constitute the basic unit, and are de-
limited by ?#begin document ID? and ?#end doc-
ument ID? comment lines. Within a document, the
information of each sentence is organized verti-
cally with one token per line, and a blank line after
the last token of each sentence. The information
associated with each token is described in several
columns (separated by ?\t? characters) represent-
ing the following layers of linguistic annotation.
ID (column 1). Token identifiers in the sentence.
Token (column 2). Word forms.
9
Following the terminology of the ACE program, a men-
tion is defined as an instance of reference to an object, and
an entity is the collection of mentions referring to the same
object in a document.
10
http://www.cnts.ua.ac.be/conll2008
ID Token Intermediate columns Coref
1 Major . . . (1
2 League . . .
3 Baseball . . . 1)
4 sent . . .
5 its . . . (1)|(2
6 head . . .
7 of . . .
8 security . . . (3)|2)
9 to . . .
. . . . . . . . . . . .
27 The . . . (1
28 league . . . 1)
29 is . . .
Table 2: Format of the coreference annotations
(corresponding to example (1) in Section 1).
Lemma (column 3). Token lemmas.
PoS (column 5). Coarse PoS.
Feat (column 7). Morphological features (PoS
type, number, gender, case, tense, aspect,
etc.) separated by a pipe character.
Head (column 9). ID of the syntactic head (?0? if
the token is the tree root).
DepRel (column 11). Dependency relations cor-
responding to the dependencies described in
the Head column (?sentence? if the token is
the tree root).
NE (column 13). NE types in open-close notation.
Pred (column 15). Predicate semantic class.
APreds (column 17 and subsequent ones). For
each predicate in the Pred column, its seman-
tic roles/dependencies.
Coref (last column). Coreference relations in
open-close notation.
The above-mentioned columns are ?gold-
standard columns,? whereas columns 4, 6, 8, 10,
12, 14, 16 and the penultimate contain the same
information as the respective previous column but
automatically predicted?using the preprocessing
systems listed in Section 2.2. Neither all layers
of linguistic annotation nor all gold-standard and
predicted columns were available for all six lan-
guages (underscore characters indicate missing in-
formation).
The coreference column follows an open-close
notation with an entity number in parentheses (see
Table 2). Every entity has an ID number, and ev-
ery mention is marked with the ID of the entity
it refers to: an opening parenthesis shows the be-
ginning of the mention (first token), while a clos-
ing parenthesis shows the end of the mention (last
3
token). For tokens belonging to more than one
mention, a pipe character is used to separate mul-
tiple entity IDs. The resulting annotation is a well-
formed nested structure (CF language).
3.2 Evaluation Settings
In order to address our goal of studying the effect
of different levels of linguistic information (pre-
processing) on solving coreference relations, the
test was divided into four evaluation settings that
differed along two dimensions.
Gold-standard versus Regular setting. Only
in the gold-standard setting were participants al-
lowed to use the gold-standard columns, includ-
ing the last one (of the test dataset) with true
mention boundaries. In the regular setting, they
were allowed to use only the automatically pre-
dicted columns. Obtaining better results in the
gold setting would provide evidence for the rel-
evance of using high-quality preprocessing infor-
mation. Since not all columns were available for
all six languages, the gold setting was only possi-
ble for Catalan, English, German, and Spanish.
Closed versus Open setting. In the closed set-
ting, systems had to be built strictly with the in-
formation provided in the task datasets. In con-
trast, there was no restriction on the resources that
participants could utilize in the open setting: sys-
tems could be developed using any external tools
and resources to predict the preprocessing infor-
mation, e.g., WordNet, Wikipedia, etc. The only
requirement was to use tools that had not been de-
veloped with the annotations of the test set. This
setting provided an open door into tools or re-
sources that improve performance.
3.3 Evaluation Metrics
Since there is no agreement at present on a stan-
dard measure for coreference resolution evalua-
tion, one of our goals was to compare the rank-
ings produced by four different measures. The
task scorer provides results in the two mention-
based metrics B
3
(Bagga and Baldwin, 1998) and
CEAF-?
3
(Luo, 2005), and the two link-based
metrics MUC (Vilain et al, 1995) and BLANC
(Recasens and Hovy, in prep). The first three mea-
sures have been widely used, while BLANC is a
proposal of a new measure interesting to test.
The mention detection subtask is measured with
recall, precision, and F
1
. Mentions are rewarded
with 1 point if their boundaries coincide with those
of the gold NP, with 0.5 points if their boundaries
are within the gold NP including its head, and
with 0 otherwise.
4 Participating Systems
A total of twenty-two participants registered for
the task and downloaded the training materials.
From these, sixteen downloaded the test set but
only six (out of which two task organizers) sub-
mitted valid results (corresponding to nine system
runs or variants). These numbers show that the
task raised considerable interest but that the final
participation rate was comparatively low (slightly
below 30%).
The participating systems differed in terms of
architecture, machine learning method, etc. Ta-
ble 3 summarizes their main properties. Systems
like BART and Corry support several machine
learners, but Table 3 indicates the one used for the
SemEval run. The last column indicates the exter-
nal resources that were employed in the open set-
ting, thus it is empty for systems that participated
only in the closed setting. For more specific details
we address the reader to the system description pa-
pers in Erk and Strapparava (2010).
5 Results and Evaluation
Table 4 shows the results obtained by two naive
baseline systems: (i) SINGLETONS considers each
mention as a separate entity, and (ii) ALL-IN-ONE
groups all the mentions in a document into a sin-
gle entity. These simple baselines reveal limita-
tions of the evaluation metrics, like the high scores
of CEAF and B
3
for SINGLETONS. Interestingly
enough, the naive baseline scores turn out to be
hard to beat by the participating systems, as Ta-
ble 5 shows. Similarly, ALL-IN-ONE obtains high
scores in terms of MUC. Table 4 also reveals dif-
ferences between the distribution of entities in the
datasets. Dutch is clearly the most divergent cor-
pus mainly due to the fact that it only contains sin-
gletons for NEs.
Table 5 displays the results of all systems for all
languages and settings in the four evaluation met-
rics (the best scores in each setting are highlighted
in bold). Results are presented sequentially by lan-
guage and setting, and participating systems are
ordered alphabetically. The participation of sys-
tems across languages and settings is rather irreg-
ular,
11
thus making it difficult to draw firm conclu-
11
Only 45 entries in Table 5 from 192 potential cases.
4
System Architecture ML Methods External Resources
BART
(Broscheit et al, 2010) Closest-first with entity-
mention model (English),
Closest-first model (German,
Italian)
MaxEnt (English, Ger-
man), Decision trees
(Italian)
GermaNet & gazetteers (Ger-
man), I-Cab gazetteers (Italian),
Berkeley parser, Stanford NER,
WordNet, Wikipedia name list,
U.S. census data (English)
Corry
(Uryupina, 2010) ILP, Pairwise model SVM Stanford parser & NER, Word-
Net, U.S. census data
RelaxCor
(Sapena et al, 2010) Graph partitioning (solved by
relaxation labeling)
Decision trees, Rules WordNet
SUCRE
(Kobdani and Sch?utze, 2010) Best-first clustering, Rela-
tional database model, Regular
feature definition language
Decision trees, Naive
Bayes, SVM, MaxEnt
?
TANL-1
(Attardi et al, 2010) Highest entity-mention simi-
larity
MaxEnt PoS tagger (Italian)
UBIU
(Zhekova and K?ubler, 2010) Pairwise model MBL ?
Table 3: Main characteristics of the participating systems.
sions about the aims initially pursued by the task.
In the following, we summarize the most relevant
outcomes of the evaluation.
Regarding languages, English concentrates the
most participants (fifteen entries), followed by
German (eight), Catalan and Spanish (seven each),
Italian (five), and Dutch (three). The number of
languages addressed by each system ranges from
one (Corry) to six (UBIU and SUCRE); BART and
RelaxCor addressed three languages, and TANL-1
five. The best overall results are obtained for En-
glish followed by German, then Catalan, Spanish
and Italian, and finally Dutch. Apart from differ-
ences between corpora, there are other factors that
might explain this ranking: (i) the fact that most of
the systems were originally developed for English,
and (ii) differences in corpus size (German having
the largest corpus, and Dutch the smallest).
Regarding systems, there are no clear ?win-
ners.? Note that no language-setting was ad-
dressed by all six systems. The BART system,
for instance, is either on its own or competing
against a single system. It emerges from par-
tial comparisons that SUCRE performs the best in
closed?regular for English, German, and Italian,
although it never outperforms the CEAF or B
3
sin-
gleton baseline. While SUCRE always obtains the
best scores according to MUC and BLANC, Re-
laxCor and TANL-1 usually win based on CEAF
and B
3
. The Corry system presents three variants
optimized for CEAF (Corry-C), MUC (Corry-M),
and BLANC (Corry-B). Their results are consis-
tent with the bias introduced in the optimization
(see English:open?gold).
Depending on the evaluation metric then, the
rankings of systems vary with considerable score
differences. There is a significant positive corre-
lation between CEAF and B
3
(Pearson?s r = 0.91,
p< 0.01), and a significant lack of correlation be-
tween CEAF and MUC in terms of recall (Pear-
son?s r = 0.44, p< 0.01). This fact stresses the
importance of defining appropriate metrics (or a
combination of them) for coreference evaluation.
Finally, regarding evaluation settings, the re-
sults in the gold setting are significantly better than
those in the regular. However, this might be a di-
rect effect of the mention recognition task. Men-
tion recognition in the regular setting falls more
than 20 F
1
points with respect to the gold setting
(where correct mention boundaries were given).
As for the open versus closed setting, there is only
one system, RelaxCor for English, that addressed
the two. As expected, results show a slight im-
provement from closed?gold to open?gold.
6 Conclusions
This paper has introduced the main features of
the SemEval-2010 task on coreference resolution.
5
CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P Blanc
SINGLETONS: Each mention forms a separate entity.
Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3
Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3
English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6
German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7
Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6
Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4
ALL-IN-ONE: All mentions are grouped into a single entity.
Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6
Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2
English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6
German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1
Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5
Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4
Table 4: Baseline scores.
The goal of the task was to evaluate and compare
automatic coreference resolution systems for six
different languages in four evaluation settings and
using four different metrics. This complex sce-
nario aimed at providing insight into several as-
pects of coreference resolution, including portabil-
ity across languages, relevance of linguistic infor-
mation at different levels, and behavior of alterna-
tive scoring metrics.
The task attracted considerable attention from a
number of researchers, but only six teams submit-
ted their final results. Participating systems did not
run their systems for all the languages and evalu-
ation settings, thus making direct comparisons be-
tween them very difficult. Nonetheless, we were
able to observe some interesting aspects from the
empirical evaluation.
An important conclusion was the confirmation
that different evaluation metrics provide different
system rankings and the scores are not commen-
surate. Attention thus needs to be paid to corefer-
ence evaluation. The behavior and applicability of
the scoring metrics requires further investigation
in order to guarantee a fair evaluation when com-
paring systems in the future. We hope to have the
opportunity to thoroughly discuss this and the rest
of interesting questions raised by the task during
the SemEval workshop at ACL 2010.
An additional valuable benefit is the set of re-
sources developed throughout the task. As task
organizers, we intend to facilitate the sharing of
datasets, scorers, and documentation by keeping
them available for future research use. We believe
that these resources will help to set future bench-
marks for the research community and will con-
tribute positively to the progress of the state of the
art in coreference resolution. We will maintain and
update the task website with post-SemEval contri-
butions.
Acknowledgments
We would like to thank the following peo-
ple who contributed to the preparation of the
task datasets: Manuel Bertran (UB), Oriol
Borrega (UB), Orph?ee De Clercq (U. Ghent),
Francesca Delogu (U. Trento), Jes?us Gim?enez
(UPC), Eduard Hovy (ISI-USC), Richard Johans-
son (U. Trento), Xavier Llu??s (UPC), Montse
Nofre (UB), Llu??s Padr?o (UPC), Kepa Joseba
Rodr??guez (U. Trento), Mihai Surdeanu (Stan-
ford), Olga Uryupina (U. Trento), Lente Van Leu-
ven (UB), and Rita Zaragoza (UB). We would also
like to thank LDC and die tageszeitung for dis-
tributing freely the English and German datasets.
This work was funded in part by the Span-
ish Ministry of Science and Innovation through
the projects TEXT-MESS 2.0 (TIN2009-13391-
C04-04), OpenMT-2 (TIN2009-14675-C03), and
KNOW2 (TIN2009-14715-C04-04), and an FPU
doctoral scholarship (AP2006-00994) held by
M. Recasens. It also received financial sup-
port from the Seventh Framework Programme
of the EU (FP7/2007-2013) under GA 247762
(FAUST), from the STEVIN program of the Ned-
erlandse Taalunie through the COREA and SoNaR
projects, and from the Provincia Autonoma di
Trento through the LiveMemories project.
6
Mention detection CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P F
1
R P Blanc
Catalan
closed?gold
RelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6
TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4
UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2
closed?regular
SUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2
TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0
UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8
open?gold
open?regular
Dutch
closed?gold
SUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3
closed?regular
SUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9
UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3
open?gold
open?regular
English
closed?gold
RelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8
TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1
UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0
closed?regular
SUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2
TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3
UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0
open?gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
open?regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
German
closed?gold
SUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4
TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4
UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5
closed?regular
SUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6
TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7
UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0
open?gold
BART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8
open?regular
BART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3
Italian
closed?gold
SUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9
closed?regular
SUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7
UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2
open?gold
open?regular
BART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6
TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5
Spanish
closed?gold
RelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5
TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1
UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3
closed?regular
SUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4
TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4
UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4
open?gold
open?regular
Table 5: Official results of the participating systems for all languages, settings, and metrics.
7
References
Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi.
2010. TANL-1: coreference resolution by parse
analysis and similarity clustering. In Proceedings
of SemEval-2.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563?566.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodr??guez, Lorenza Ro-
mano, Olga Uryupina, Yannick Versley, and Roberto
Zanoli. 2010. BART: A multilingual anaphora res-
olution system. In Proceedings of SemEval-2.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL 1999.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program ? Tasks, data, and evaluation.
In Proceedings of LREC 2004, pages 837?840.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of SemEval-2.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German (PaGe 2008), pages
47?54.
Erhard W. Hinrichs, Sandra K?ubler, and Karin Nau-
mann. 2005. A unified representation for morpho-
logical, syntactic, semantic, and referential annota-
tions. In Proceedings of the ACL Workshop on Fron-
tiers in Corpus Annotation II: Pie in the Sky, pages
13?20.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 Coreference Task Definition ? Version 3.0.
In Proceedings of MUC-7.
V?eronique Hoste and Guy De Pauw. 2006. KNACK-
2002: A richly annotated corpus of Dutch written
text. In Proceedings of LREC 2006, pages 1432?
1437.
Hamidreza Kobdani and Hinrich Sch?utze. 2010. SU-
CRE: A modular system for coreference resolution.
In Proceedings of SemEval-2.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI 1995, pages 1050?1055.
Thomas S. Morton. 1999. Using coreference in ques-
tion answering. In Proceedings of TREC-8, pages
85?89.
Constantin Orasan, Dan Cristea, Ruslan Mitkov, and
Ant?onio Branco. 2008. Anaphora Resolution Exer-
cise: An overview. In Proceedings of LREC 2008.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
the International Conference on Semantic Comput-
ing (ICSC 2007), pages 517?526.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens and M. Ant`onia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI:10.1007/s10579-009-9108-x.
Kepa Joseba Rodr??guez, Francesca Delogu, Yannick
Versley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of Wikipedia and blogs in
the Live Memories Corpus. In Proceedings of
LREC 2010, pages 157?163.
Emili Sapena, Llu??s Padr?o, and Jordi Turmo. 2010.
RelaxCor: A global relaxation labeling approach to
coreference resolution for the SemEval-2 Corefer-
ence Task. In Proceedings of SemEval-2.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of COLING 2008, pages 777?784.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT Workshop, pages
47?50.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jeek. 2007. Two uses of anaphora
resolution in summarization. Information Process-
ing and Management: an International Journal,
43(6):1663?1680.
Olga Uryupina. 2010. Corry: A system for corefer-
ence resolution. In Proceedings of SemEval-2.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of LREC 2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52.
Desislava Zhekova and Sandra K?ubler. 2010. UBIU:
A language-independent system for coreference res-
olution. In Proceedings of SemEval-2.
8
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 143?147, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UPC-CORE: What Can Machine Translation Evaluation Metrics and
Wikipedia Do for Estimating Semantic Textual Similarity?
Alberto Barro?n-Ceden?o1,2 Llu??s Ma`rquez1 Maria Fuentes1 Horacio Rodr??guez1 Jordi Turmo1
1 TALP Research Center, Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, 08034, Barcelona, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid
Boadilla del Monte, 28660 Madrid, Spain
albarron, lluism, mfuentes, horacio, turmo @lsi.upc.edu
Abstract
In this paper we discuss our participation to
the 2013 Semeval Semantic Textual Similarity
task. Our core features include (i) a set of met-
rics borrowed from automatic machine trans-
lation, originally intended to evaluate auto-
matic against reference translations and (ii) an
instance of explicit semantic analysis, built
upon opening paragraphs of Wikipedia 2010
articles. Our similarity estimator relies on a
support vector regressor with RBF kernel. Our
best approach required 13 machine transla-
tion metrics + explicit semantic analysis and
ranked 65 in the competition. Our post-
competition analysis shows that the features
have a good expression level, but overfitting
and ?mainly? normalization issues caused
our correlation values to decrease.
1 Introduction
Our participation to the 2013 Semantic Textual Sim-
ilarity task (STS) (Agirre et al, 2013)1 was focused
on the CORE problem: GIVEN TWO SENTENCES,
s1 AND s2, QUANTIFIABLY INFORM ON HOW SIMI-
LAR s1 AND s2 ARE. We considered real-valued fea-
tures from four different sources: (i) a set of linguis-
tic measures computed with the Asiya Toolkit for
Automatic MT Evaluation (Gime?nez and Ma`rquez,
2010b), (ii) an instance of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), built on top
of Wikipedia articles, (iii) a dataset predictor, and
(iv) a subset of the features available in Takelab?s
Semantic Text Similarity system (?Saric? et al, 2012).
1http://ixa2.si.ehu.es/sts/
Our approaches obtained an overall modest result
compared to other participants (best position: 65 out
of 89). Nevertheless, our post-competition analysis
shows that the low correlation was caused mainly by
a deficient data normalization strategy.
The paper distribution is as follows. Section 2 of-
fers a brief overview of the task. Section 3 describes
our approach. Section 4 discuss our experiments and
obtained results. Section 5 provides conclusions.
2 Task Overview
Detecting two similar text fragments is a difficult
task in cases where the similarity occurs at seman-
tic level, independently of the implied lexicon (e.g
in cases of dense paraphrasing). As a result, simi-
larity estimation models must involve features other
than surface aspects. The STS task is proposed as
a challenge focused in short English texts of dif-
ferent nature: from automatic machine translation
alternatives to human descriptions of short videos.
The test partition also included texts extracted from
news headlines and FrameNet?Wordnet pairs.
The range of similarity was defined between 0
(no relation) up to 5 (semantic equivalence). The
gold standard values were averaged from different
human-made annotations. The expected system?s
output was composed of a real similarity value, to-
gether with an optional confidence level (our confi-
dence level was set constant).
Table 1 gives an overview of the development
(2012 training and test) and test datasets. Note
that both collections extracted from SMT data are
highly biased towards the maximum similarity val-
ues (more than 75% of the instances have a similar-
143
Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri-
bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.
similarity distribution length
dataset instances [0, 1) [1, 2) [2, 3) [3, 4) [4, 5] mean min max
dev-[train + test]
MSRpar 1,500 1.20 8.13 17.13 48.73 24.80 17.84 5 30
MSRvid 1,500 31.00 14.13 15.47 20.87 18.53 6.66 2 24
SMTEuroparl 1,193 0.67 0.42 1.17 12.32 85.4 21.13 1 72
OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34
SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28
test
headlines 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22
OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22
FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71
SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96
ity higher than 4) and include the longest instances.
On the other hand, the FNWN instances are shifted
towards low similarity levels (more than 60% have a
similarity lower than 2).
3 Approach
Our similarity assessment model relies upon
SVMlight?s support vector regressor, with RBF ker-
nel (Joachims, 1999).2 Our model estimation pro-
cedure consisted of two steps: parameter defini-
tion and backward elimination-based feature selec-
tion. The considered features belong to four fami-
lies, briefly described in the following subsections.
3.1 Machine Translation Evaluation Metrics
We consider a set of linguistic measures originally
intended to evaluate the quality of automatic trans-
lation systems. These measures compute the quality
of a translation by comparing it against one or sev-
eral reference translations, considered as gold stan-
dard. A straightforward application of these mea-
sures to the problem at hand is to consider s1 as the
reference and s2 as the automatic translation, or vice
versa. Some of the metrics are not symmetric so we
compute similarity between s1 and s2 in both direc-
tions and average the resulting scores.
The measures are computed with the Asiya
Toolkit for Automatic MT Evaluation (Gime?nez and
Ma`rquez, 2010b). The only pre-processing carried
out was tokenization (Asiya performs additional in-
box pre-processing operations, though). We consid-
2We also tried with linear kernels, but RBF always obtained
better results.
ered a sample from three similarity families, which
was proposed in (Gime?nez and Ma`rquez, 2010a) as
a varied and robust metric set, showing good corre-
lation with human assessments.3
Lexical Similarity Two metrics of Translation
Error Rate (Snover et al, 2006) (i.e. the esti-
mated human effort to convert s1 into s2): -TER
and -TERpA. Two measures of lexical precision:
BLEU (Papineni et al, 2002) and NIST (Dod-
dington, 2002). One measure of lexical recall:
ROUGEW (Lin and Och, 2004). Finally, four vari-
ants of METEOR (Banerjee and Lavie, 2005) (exact,
stemming, synonyms, and paraphrasing), a lexical
metric accounting for F -Measure.
Syntactic Similarity Three metrics that estimate
the similarity of the sentences over dependency
parse trees (Liu and Gildea, 2005): DP-HWCMic-4
for grammatical categories chains, DP-HWCMir-4
over grammatical relations, and DP-Or(?) over
words ruled by non-terminal nodes. Also, one mea-
sure that estimates the similarity over constituent
parse trees: CP-STM4 (Liu and Gildea, 2005).
Semantic Similarity Three measures that esti-
mate the similarities over semantic roles (i.e. ar-
guments and adjuncts): SR-Or, SR-Mr(?), and
SR-Or(?). Additionally, two metrics that es-
timate similarities over discourse representations:
DR-Or(?) and DR-Orp(?).
3Asiya is available at http://asiya.lsi.upc.edu.
Full descriptions of the metrics are available in the Asiya Tech-
nical Manual v2.0, pp. 15?21.
144
3.2 Explicit Semantic Analysis
We built an instance of Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) with
the first paragraph of 100k Wikipedia articles (dump
from 2010).Pre-processing consisted of tokenization
and lemmatization.
3.3 Dataset Prediction
Given the similarity shifts in the different datasets
(cf. Table 1), we tried to predict what dataset an in-
stance belonged to on the basis of its vocabulary. We
built binary maxent classifiers for each dataset in the
development set, resulting in five dataset likelihood
features: dMSRpar, dSMTeuroparl, dMSRvid,
dOnWN, and dSMTnews.4 Pre-processing consisted
of tokenization and lemmatization.
3.4 Baseline
We considered the features included in the Takelab
Semantic Text Similarity system (?Saric? et al, 2012),
one of the top-systems in last year competition. This
system is used as a black box. The resulting features
are named tklab n, where n = [1, 21].
Our runs departed from three increasing subsets
of features: AE machine translation evaluation met-
rics and explicit semantic analysis, AED the pre-
vious set plus dataset prediction, and AED T the
previous set plus Takelab?s baseline features (cf. Ta-
ble 3). We performed a feature normalization, which
relied on the different feature?s distribution over the
entire dataset. Firstly, features were bounded in the
range ??3??2 in order to reduce the potentially neg-
ative impact of outliers. Secondly, we normalized
according to the z-score (Nardo et al, 2008, pp. 28,
84); i.e. x = (x ? ?)/?. As a result, each real-
valued feature distribution in the dataset has ? = 0
and ? = 1. During the model tuning stage we tried
with other numerous normalization options: normal-
izing each dataset independently, together with the
training set, and without normalization at all. Nor-
malizing according to the entire dev-test dataset led
to the best results
4We used the Stanford classifier; http://nlp.
stanford.edu/software/classifier.shtml
Table 2: Tuning process: parameter definition and feature
selection. Number of features at the beginning and end
of the feature selection step included.
run parameter def. feature sel.
c ? ? corr b e corr
AE 3.7 0.06 0.3 0.8257 19 14 0.8299
AED 3.8 0.03 0.2 0.8413 24 19 0.8425
AED T 2.9 0.02 0.3 0.8761 45 33 0.8803
4 Experiments and Results
Section 4.1 describes our model tuning strategy.
Sections 4.2 and 4.3 discuss the official and post-
competition results.
4.1 Model Tuning
We used only the dev-train partition (2012 training)
for tuning. By means of a 10-fold cross validation
process, we defined the trade-off (c), gamma (?),
and tube width (?) parameters for the regressor and
performed a backward-elimination feature selection
process (Witten and Frank, 2005, p. 294), indepen-
dently for the three experiments.
The results for the cross-validation process are
summarized in Table 2. The three runs allow for cor-
relations higher than 0.8. On the one hand, the best
regressor parameters obtain better results as more
features are considered, still with very small differ-
ences. On the other hand, the low correlation in-
crease after the feature selection step shows that a
few features are indeed irrelevant.
A summary of the features considered in each ex-
periment (also after feature selection) is displayed in
Table 3. The correlation obtained over the dev-test
partition are corrAE = 0.7269, corrAED = 0.7638,
and corrAEDT = 0.8044 ?it would have appeared
in the top-10 ranking of the 2012 competition.
4.2 Official Results
We trained three new regressors with the features
considered relevant by the tuning process, but using
the entire development dataset. The test 2013 parti-
tion was normalized again by means of z-score, con-
sidering the means and standard deviations of the en-
tire test dataset. Table 4 displays the official results.
Our best approach ?AE?, was positioned in rank
65. The worst results of run AED can be explained
by the difference in the nature of the test respect to
145
Table 3: Features considered at the beginning of each run, represented as empty squares (). Filled squares ()
represent features considered relevant after feature selection.
Feature AE AED AED T Feature AE AED AED T Feature AED T
DP-HWCM c-4    METEOR-pa    tklab 7 
DP-HWCM r-4    METEOR-st    tklab 8 
DP-Or(*)    METEOR-sy    tklab 9 
CP-STM-4    ESA    tklab 10 
SR-Or(*)    dMSRpar   tklab 11 
SR-Mr(*)    dSMTeuroparl   tklab 12 
SR-Or    dMSRvid   tklab 13 
DR-Or(*)    dOnWN   tklab 14 
DR-Orp(*)    dSMTnews   tklab 15 
BLEU    tklab 1  tklab 16 
NIST    tklab 2  tklab 17 
-TER    tklab 3  tklab 18 
-TERp-A    tklab 4  tklab 19 
ROUGE-W    tklab 5  tklab 20 
METEOR-ex    tklab 6  tklab 21 
Table 4: Official results for the three runs (rank included).
run headlines OnWN FNWN SMT mean
AE (65) 0.6092 0.5679 -0.1268 0.2090 0.4037
AED (83) 0.4136 0.4770 -0.0852 0.1662 0.3050
AED T (72) 0.5119 0.6386 -0.0464 0.1235 0.3671
the development dataset. AED T obtains worst re-
sults than AE on the headlines and SMT datasets.
The reason behind this behavior can be in the dif-
ference of vocabularies respect to that stored in the
Takelab system (it includes only the vocabulary of
the development partition). This could be the same
reason behind the drop in performance with respect
to the results previously obtained on the dev-test par-
tition (cf. Section 4.1).
4.3 Post-Competition Results
Our analysis of the official results showed the main
issue was normalization. Thus, we performed a
manifold of new experiments, using the same con-
figuration as in run AE, but applying other normal-
ization strategies: (a) z-score normalization, but ig-
noring the FNWN dataset (given its shift through
low values); (b) z-score normalization, but consid-
ering independent means and standard deviations for
each test dataset; and (c) without normalizing any of
dataset (including the regressor one).
Table 5 includes the results. (a) makes evident
that the instances in FNWN represent ?anomalies?
that harm the normalized values of the rest of sub-
sets. Run (b) shows that normalizing the test sets
Table 5: Post-competition experiments results
run headlines OnWN FNWN SMT mean
AE (a) 0.6210 0.5905 -0.0987 0.2990 0.4456
AE (b) 0.6072 0.4767 -0.0113 0.3236 0.4282
AE (c) 0.6590 0.6973 0.1547 0.3429 0.5208
independently is not a good option, as the regressor
is trained considering overall normalizations, which
explains the correlation decrease. Run (c) is com-
pletely different: not normalizing any dataset ?
both in development and test? reduces the influ-
ence of the datasets to each other and allows for the
best results. Indeed, this configuration would have
advanced practically forty positions at competition
time, locating us in rank 27.
Estimating the adequate similarities over FNWN
seems particularly difficult for our systems. We ob-
serve two main factors. (i) FNWN presents an im-
portant similarity shift respect to the other datasets:
nearly 90% of the instances similarity is lower than
2.5 and (ii) the average lengths of s1 and s2 are very
different: 30 vs 9 words. These characteristics made
it difficult for our MT evaluation metrics to estimate
proper similarity values (be normalized or not).
We performed two more experiments over
FNWN: training regressors with ESA as the only
feature, before and after normalization. The correla-
tion was 0.16017 and 0.3113, respectively. That is,
the normalization mainly affects the MT features.
146
5 Conclusions
In this paper we discussed on our participation to the
2013 Semeval Semantic Textual Similarity task. Our
approach relied mainly upon a combination of au-
tomatic machine translation evaluation metrics and
explicit semantic analysis. Building an RBF support
vector regressor with these features allowed us for a
modest result in the competition (our best run was
ranked 65 out of 89).
Acknowledgments
We would like to thank the organizers of this chal-
lenging task for their efforts.
This research work was partially carried out dur-
ing the tenure of an ERCIM ?Alain Bensoussan?
Fellowship. The research leading to these results re-
ceived funding from the EU FP7 Programme 2007-
2013 (grants 246016 and 247762). Our research
work is partially supported by the Spanish research
projects OpenMT-2 and SKATER (TIN2009-14675-
C03, TIN2012-38584-C06-01).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pilot on
Typed-Similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Gold-
stein et al (Goldstein et al, 2005), pages 65?72.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-Gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138?145, San Francisco, CA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial Intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94).
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):209?240.
Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare
Voss, editors. 2005. Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization. Asso-
ciation for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods ?
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical. MIT Press.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Stroudsburg, PA. Association for Com-
putational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Goldstein
et al (Goldstein et al, 2005), pages 25?32.
Michela Nardo, Michaela Saisana, Andrea Saltelli, Ste-
fano Tarantola, Anders Hoffmann, and Enrico Giovan-
nini. 2008. Handbook on Constructing Composite In-
dicators: Methodology and User Guide. OECD Pub-
lishing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311?318,
Philadelphia, PA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. TakeLab: Sys-
tems for Measuring Semantic Text. In First Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 441?448, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, CA, 2 edition.
147
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159?177
Manchester, August 2008
The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies
Mihai Surdeanu
?,?
Richard Johansson
?
Adam Meyers
?
Llu??s M
`
arquez
??
Joakim Nivre
??,??
?: Barcelona Media Innovation Center, mihai.surdeanu@barcelonamedia.org
?: Yahoo! Research Barcelona, mihais@yahoo-inc.com
?: Lund University, richard@cs.lth.se
?: New York University, meyers@cs.nyu.edu
??: Technical University of Catalonia, lluism@lsi.upc.edu
??: V?axj?o University, joakim.nivre@vxu.se
??: Uppsala University, joakim.nivre@lingfil.uu.se
Abstract
The Conference on Computational Natu-
ral Language Learning is accompanied ev-
ery year by a shared task whose purpose
is to promote natural language processing
applications and evaluate them in a stan-
dard setting. In 2008 the shared task was
dedicated to the joint parsing of syntactic
and semantic dependencies. This shared
task not only unifies the shared tasks of
the previous four years under a unique
dependency-based formalism, but also ex-
tends them significantly: this year?s syn-
tactic dependencies include more informa-
tion such as named-entity boundaries; the
semantic dependencies model roles of both
verbal and nominal predicates. In this pa-
per, we define the shared task and describe
how the data sets were created. Further-
more, we report and analyze the results and
describe the approaches of the participat-
ing systems.
1 Introduction
In 2004 and 2005 the shared tasks of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL) were dedicated to semantic role la-
beling (SRL), in a monolingual setting (English).
In 2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using cor-
pora from up to 13 languages. The CoNLL-2008
shared task
1
proposes a unified dependency-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.yr-bcn.es/conll2008
formalism, which models both syntactic depen-
dencies and semantic roles. Using this formalism,
this shared task merges both the task of syntactic
dependency parsing and the task of identifying se-
mantic arguments and labeling them with semantic
roles. Conceptually, the 2008 shared task can be
divided into three subtasks: (i) parsing of syntactic
dependencies, (ii) identification and disambigua-
tion of semantic predicates, and (iii) identification
of arguments and assignment of semantic roles for
each predicate. Several objectives were addressed
in this shared task:
? SRL is performed and evaluated using a
dependency-based representation for both
syntactic and semantic dependencies. While
SRL on top of a dependency treebank has
been addressed before (Hacioglu, 2004),
our approach has several novelties: (i) our
constituent-to-dependency conversion strat-
egy transforms all annotated semantic argu-
ments in PropBank and NomBank not just a
subset; (ii) we address propositions centered
around both verbal (PropBank) and nominal
(NomBank) predicates.
? Based on the observation that a richer set
of syntactic dependencies improves seman-
tic processing (Johansson and Nugues, 2007),
the syntactic dependencies modeled are more
complex than the ones used in the previous
CoNLL shared tasks. For example, we now
include apposition links, dependencies de-
rived from named entity (NE) structures, and
better modeling of long-distance grammatical
relations.
? A practical framework is provided for the
joint learning of syntactic and semantic de-
pendencies.
159
Given the complexity of this shared task, we
limited the evaluation to a monolingual, English-
only setting. The evaluation is separated into two
different challenges: a closed challenge, where
systems have to be trained strictly with informa-
tion contained in the given training corpus, and an
open challenge, where systems can be developed
making use of any kind of external tools and re-
sources. The participants could submit results in
either one or both challenges.
This paper is organized as follows. Section 2
defines the task, including the format of the data,
the evaluation metrics, and the two challenges.
Section 3 introduces the corpora used and our
constituent-to-dependency conversion procedure.
Section 4 summarizes the results of the submit-
ted systems. Section 5 discusses the approaches
implemented by participants. Section 6 analyzes
the results using additional non-official evaluation
measures. Section 7 concludes the paper.
2 Task Definition
In this section we provide the definition of the
shared task, starting with the format of the shared
task data, followed by a description of the eval-
uation metrics used and a discussion of the two
shared task challenges, i.e., closed and open.
2.1 Data Format
The data format used in this shared task was highly
influenced by the formats used in the 2004?2007
shared tasks. The data follows these general rules:
? The files contain sentences separated by a
blank line.
? A sentence consists of one or more tokens and
the information for each token is represented
on a separate line.
? A token consists of at least 11 fields. The
fields are separated by one or more whites-
pace characters (spaces or tabs). Whitespace
characters are not allowed within fields.
Table 1 describes the fields stored for each token
in the closed-track data sets. Columns 1?3 and
5?8 are available at both training and test time.
Column 4, which contains gold-standard part-of-
speech (POS) tags, is not given at test time. The
same holds for columns 9 and above, which con-
tain the syntactic and semantic dependency struc-
tures that the systems should predict.
The PPOS and PPOSS fields were automati-
cally predicted using the SVMTool POS tagger
(Gim?enez, 2004). To predict the tags in the train-
ing set, a 5-fold cross-validation procedure was
used. The LEMMA and SPLIT LEMMA fields
were predicted using the built-in lemmatizer in
WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and part-of-speech tag.
Since NomBank uses a sub-word anal-
ysis in some hyphenated words (such as
[finger]
ARG
-[pointing]
PRED
), the data for-
mat represents the parts in hyphenated words as
separate tokens (columns 6?8). However, the
format also represents how the parts originally fit
together before splitting (columns 2?5). Padding
characters (? ?) are used in columns 2?5 to
ensure the same number of rows for all columns
corresponding to one sentence. All syntactic and
semantic dependencies are annotated relative to
the split word forms (columns 6?8).
Table 2 shows the columns available to the sys-
tems participating in the open challenge: named-
entity labels as in the CoNLL-2003 Shared Task
(Tjong Kim San and De Meulder, 2003) and
from the BBN Wall Street Journal Entity Corpus,
2
WordNet supersense tags, and the output of an off-
the-shelf dependency parser (Nivre et al, 2007b).
Columns 1?3 were predicted using the tagger of
Ciaramita and Altun (2006). Because the BBN
corpus shares lexical content with the Penn Tree-
bank, we generated the BBN tags using a 2-fold
cross-validation procedure.
2.2 Evaluation Measures
We separate the evaluation measures into two
groups: (i) official measures, which were used for
the ranking of participating systems, and (ii) addi-
tional unofficial measures, which provide further
insight into the performance of the participating
systems.
2.2.1 Official Evaluation Measures
The official evaluation measures consist of three
different scores: (i) syntactic dependencies are
scored using the labeled attachment score (LAS),
(ii) semantic dependencies are evaluated using a
labeled F
1
score, and (iii) the overall task is scored
with a macro average of the two previous scores.
We describe all these scoring measures next.
The LAS score is defined similarly as in the pre-
vious two shared tasks, as the percentage of to-
2
LDC catalog number LDC2005T33.
160
Number Name Description
1 ID Token counter, starting at 1 for each new sentence.
2 FORM Unsplit word form or punctuation symbol.
3 LEMMA Predicted lemma of FORM.
4 GPOS Gold part-of-speech tag from the Treebank (empty at test time).
5 PPOS Predicted POS tag.
6 SPLIT FORM Tokens split at hyphens and slashes.
7 SPLIT LEMMA Predicted lemma of SPLIT FORM.
8 PPOSS Predicted POS tags of the split forms.
9 HEAD Syntactic head of the current token, which is either a value of ID or zero (0).
10 DEPREL Syntactic dependency relation to the HEAD.
11 PRED Rolesets of the semantic predicates in this sentence.
12. . . ARG Columns with argument labels for each semantic predicate following textual order.
Table 1: Column format in the closed-track data. The columns in the lower part of the table are unseen
at test time and are to be predicted by systems.
Number Name Description
1 CONLL2003 Named entity labels using the tag set from the CoNLL-2003 shared task.
2 BBN NE labels using the tag set from the BBN Wall Street Journal Entity Corpus.
3 WNSS WordNet super senses.
4 MALT HEAD Head of the syntactic dependencies generated by MaltParser.
5 MALT DEPREL Label of syntactic dependencies generated by MaltParser.
Table 2: Column format in the open-track data.
kens for which a system has predicted the correct
HEAD and DEPREL columns (see Table 1). Same
as before, our scorer also computes the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens with correct HEAD, and label accuracy, i.e.,
the percentage of tokens with correct DEPREL.
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we
create a semantic dependency from every predicate
to all its individual arguments. These dependen-
cies are labeled with the labels of the correspond-
ing arguments. Additionally, we create a seman-
tic dependency from each predicate to a virtual
ROOT node. The latter dependencies are labeled
with the predicate senses. This approach guaran-
tees that the semantic dependency structure con-
ceptually forms a single-rooted, connected (but not
necessarily acyclic) graph. More importantly, this
scoring strategy implies that if a system assigns
the incorrect predicate sense, it still receives some
points for the arguments correctly assigned. For
example, for the correct proposition:
verb.01: ARG0, ARG1, ARGM-TMP
the system that generates the following output for
the same argument tokens:
verb.02: ARG0, ARG1, ARGM-LOC
receives a labeled precision score of 2/4 because
two out of four semantic dependencies are incor-
rect: the dependency to ROOT is labeled 02 in-
stead of 01 and the dependency to the ARGM-TMP
is incorrectly labeled ARGM-LOC. Using this strat-
egy we compute precision, recall, and F
1
scores
for both labeled and unlabeled semantic dependen-
cies.
Finally, we combine the syntactic and semantic
measures into one global measure using macro av-
eraging. We compute macro precision and recall
scores by averaging the labeled precision and re-
call for semantic dependencies with the LAS for
syntactic dependencies:
3
LMP = W
sem
? LP
sem
+ (1?W
sem
) ? LAS (1)
LMR = W
sem
? LR
sem
+ (1?W
sem
) ? LAS (2)
where LMP is the labeled macro precision and
LP
sem
is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LR
sem
is the labeled recall for semantic
dependencies. W
sem
is the weight assigned to the
semantic task.
4
The macro labeled F
1
score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3
We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the pre-
dicted number of dependencies is equal to the number of gold
dependencies.
4
We assign equal weight to the two tasks, i.e., W
sem
=
0.5.
161
2.2.2 Additional Evaluation Measures
We used several additional evaluation measures
to further analyze the performance of the partici-
pating systems.
The first additional measure used is Exact
Match, which reports the percentage of sentences
that are completely correct, i.e., all the generated
syntactic dependencies are correct and all the se-
mantic propositions are present and correct. While
this score is significantly lower than any of the of-
ficial scores, it will award systems that performed
joint learning or optimization for all subtasks.
In the same spirit but focusing on the seman-
tic subtasks, we report the Perfect Proposition F
1
score, where we score entire semantic frames or
propositions. This measure is similar to the PProps
accuracy score from the 2005 shared task (Carreras
and M`arquez, 2005), with the caveat that this year
this score is implemented as an F
1
measure, be-
cause predicates are not provided in the test data.
Hence, propositions may be over or under gener-
ated at prediction time.
Lastly, we analyze systems based on the ratio
between labeled F
1
score for semantic dependen-
cies and the LAS for syntactic dependencies. In
other words, this measure normalizes the seman-
tic scores relative to the performance of the pars-
ing component. This measure estimates the true
overall performance of the semantic subtasks, in-
dependent of the syntactic parser.
5
For example,
this score addresses the situations where the se-
mantic labeled F
1
score of one system is artificially
low because the corresponding syntactic compo-
nent does not perform well.
2.3 Closed and Open Challenges
Similarly to the CoNLL-2005 shared task, this
shared task evaluation is separated into two chal-
lenges:
Closed Challenge - systems have to be built
strictly with information contained in the given
training corpus, and tuned with the development
section. In addition, the PropBank and NomBank
lexical frames can be used. These restrictions
mean that constituent-based parsers or SRL sys-
tems can not be used in this challenge because the
constituent-based annotations are not provided in
our training set. The aim of this challenge is to
5
A correct evaluation of the stand-alone SRL systems
would require the usage of gold syntactic dependencies, but
these were not provided for the testing corpora.
compare the performance of the participating sys-
tems in a fair environment.
Open Challenge - systems can be developed mak-
ing use of any kind of external tools and resources.
The only condition is that such tools or resources
must not have been developed with the annota-
tions of the test set, both for the input and out-
put annotations of the data. In this challenge,
we are interested in learning methods which make
use of any tools or resources that might improve
the performance. For example, we encourage the
use of semantic information, as provided by NE
recognition or word-sense disambiguation (WSD)
systems (such state-of-the-art annotations are pro-
vided by the organizers, see Table 2). Also, in
this challenge participants are encouraged to use
constituent-based parsers and SRL systems, as
long as these systems were trained only with the
sections of Penn Treebank used in the shared task
training corpus. To encourage the participation of
the groups that are only interested in SRL, the or-
ganizers provide also the output of a state-of-the-
art dependency parser as input in this challenge.
The comparison of different systems in this setting
may not be fair, and thus ranking of systems is not
necessarily important.
3 Data
The corpora used in the shared task evaluation
were generated through a process that merges
several input corpora and converts them from
the constituent-based formalism to dependencies.
This section starts with an introduction of the in-
put corpora used, followed by a description of
the constituent-to-dependency conversion process.
The section concludes with an overview of the
shared task corpora.
3.1 Input Corpora
Input to our merging procedures includes the Penn
Treebank, BBN?s named entity corpus, PropBank
and NomBank. In this section, we will pro-
vide brief descriptions of these annotations in
terms of both form and content. All annotations
are currently being distributed by the Linguistic
Data Consortium, with the exception of NomBank,
which is freely downloadable.
6
6
http://nlp.cs.nyu.edu/meyers/NomBank.
html
162
3.1.1 Penn Treebank 3
The Penn Treebank 3 corpus (Marcus et al,
1993) consists of hand-coded parses of the Wall
Street Journal (test, development and training) and
a small subset of the Brown corpus (W. N. Fran-
cis and H. Ku?cera, 1964) (test only). These hand
parses are notated in-line and sometimes involve
changing the strings of the input data. For ex-
ample, in file wsj 0309, the token fearlast in the
text corresponds to the two tokens fear and last
in the annotated data. In a similar way, cannot
is regularly split to can and not. It is significant
that the other annotations assume the tokeniza-
tion of the Penn Treebank, as this makes it easier
for us to merge the annotation. The Penn Tree-
bank syntactic annotation includes phrases, parts
of speech, empty category representations of vari-
ous filler/gap constructions and other phenomena,
based on a theoretical perspective similar to that
of Government and Binding Theory (Chomsky,
1981).
3.1.2 BBN Pronoun Coreference and Entity
Type Corpus
BBN?s NE annotation of the Wall Street Journal
corpus (Weischedel and Brunstein, 2005) takes the
form of SGML inline markup of text, tokenized
to be completely compatible with the Penn Tree-
bank annotation, e.g., fearlast and cannot are split
in the same ways. Named entity categories in-
clude: Person, Organization, Location, GPE, Fa-
cility, Money, Percent, Time and Date, based on
the definitions of these categories in MUC (Chin-
chor and Robinson, 1998) and ACE
7
tasks. Sub-
categories are included as well. Note however that
from this corpus we only use NE boundaries to
derive NAME dependencies between NE tokens,
e.g., we create a NAME dependency from Mary to
Smith given the NE mention Mary Smith.
3.1.3 Proposition Bank I (PropBank)
The PropBank annotation (Palmer et al, 2005)
classifies the arguments of all the main verbs in the
Penn Treebank corpus, other than be. Arguments
are numbered (ARG0, ARG1, . . .) based on lexical
entries or frame files. Different sets of arguments
are assumed for different rolesets. Dependent con-
stituents that fall into categories independent of
the lexical entries are classified as various types
7
http://projects.ldc.upenn.edu/ace/
of ARGM (TMP, ADV, etc.).
8
Rather than us-
ing PropBank directly, we used the version created
for the CoNLL-2005 shared task (Carreras and
M`arquez, 2005). PropBank?s pointers to subtrees
are converted into the list of leaves of those sub-
trees, minus the empty categories. On occasion,
arguments of verbs end up being two non-adjacent
substrings. For example, the argument of claims in
the following sentence is indicated in bold: This
sentence, Mary claims, is self-referential. The
CoNLL-2005 format handles this by marking both
strings A1 (This sentence and is self-referential),
but adding a C- prefix to the argument tag on the
second argument. Another difference between the
PropBank annotation and the CoNLL-2005 ver-
sion of it is their treatments of filler gap construc-
tions involving empty categories. PropBank an-
notation includes the whole chain of empty cate-
gories, as well as the antecedent of the empty cate-
gory (the filler of the gap). In contrast, the CoNLL-
2005 version only includes the filler of the gap and
if there is no filler, the argument is omitted, e.g.,
no ARG0 (subject) for leave would be included in
I said to leave because the subject of leave is un-
specified.
3.1.4 NomBank
NomBank annotation (Meyers et al, 2004) uses
essentially the same framework as PropBank to an-
notate arguments of nouns. Differences between
PropBank and NomBank stem from differences
between noun and verb argument structure; differ-
ences in treatment of nouns and verbs in the Penn
Treebank; and differences in the sophistication of
previous research about noun and verb argument
structure. Only the subset of nouns that take ar-
guments are annotated in NomBank and only a
subset of the non-argument siblings of nouns are
marked as ARGM. These limitations were nec-
essary to make the NomBank task consistent and
tractable. In addition, long distance dependencies
of nouns, e.g., the relation between Mary and walk
in Mary took dozens of walks is handled as fol-
lows: Mary is marked as the ARG0 of walk and
took + dozens + of is marked as a support chain
in NomBank. In contrast, verbal long distance de-
pendencies can be handled by means of empty cat-
egories in the Penn Treebank, e.g., the relation be-
8
PropBank I is used here. Later versions of PropBank
mark instances of be in addition to other verbs. PropBank?s
use of the terms roleset and ARGM correspond approximately
to sense and adjunct in common usage.
163
tween John and walked in John seemed t to walk.
Support chains are needed because nominal long
distance dependencies are not captured under the
Penn Treebank?s system of empty categories.
3.2 Conversion to Dependencies
3.2.1 Syntactic Dependencies
There exists no large-scale dependency tree-
bank for English, and we thus had to construct a
dependency-annotated corpus automatically from
the Penn Treebank (Marcus et al, 1993). Since
dependency syntax represents grammatical struc-
ture by means of labeled binary head?dependent
relations rather than phrases, the task of the con-
version procedure is to identify and label the
head?dependent pairs. The idea underpinning
constituent-to-dependency conversion algorithms
(Magerman, 1994; Collins, 1999; Yamada and
Matsumoto, 2003) is that head?dependent pairs are
created from constituents by selecting one word in
each phrase as the head and setting all other as its
dependents. The dependency labels are then in-
ferred from the phrase?subphrase or phrase?word
relations.
Our conversion procedure (Johansson and
Nugues, 2007) differs from this basic approach by
exploiting the rich structure of the constituent for-
mat used in Penn Treebank 3:
? Grammatical function labels that often can be
directly used in the dependency framework.
? Long-distance grammatical relations repre-
sented by means of empty categories and sec-
ondary edges, which can be used to create (of-
ten nonprojective) dependency links.
Of the grammatical function tags available in the
Treebank, we removed the HLN, NOM, TPC, and
TTL tags since they represent structural properties
of single phrases rather than binary relations. For
compatibility between the WSJ and Brown cor-
pora, we removed the ETC, UNF, and IMP tags
from Brown and the CLR tag from WSJ.
Algorithms 1 and 2 show the constituent-to-
dependency conversion algorithm and function la-
beling, respectively. The first steps apply structural
transformations to the constituent trees. Next, a
head word is assigned to each constituent. After
this, grammatical functions are inferred, allowing
a dependency tree to be created.
To find head children (used in
assign-heads), a system of rules is used
Algorithm 1: Pseudocode for constituent-to-
dependency conversion.
procedure constituents-to-dependencies(T )
import-glarf(T )
reattach-traces(T )
split-small-clauses(T )
assign-heads(T.root)
assign-functions(T )
return create-dependency-tree(T )
procedure import-glarf(T )
Import a GLARF surface dependency graph G
for each multi-word name N in G
for each token d in N
Set the function tag of d to NAME
for each dependency link h ?
L
d in G
if L ? { APPOSITE, A-POS, N-POS, POST-HON, Q-POS,
RED-RELATIVE, SUFFIX, T-POS, TITLE }
or if h and d are inside a split word
Set the function tag of d to L in T
if h and d are part of a larger constituent
Add an NX constituent to T that brackets h and d
procedure reattach-traces(T )
for each empty category t in T
if t is linked to a constituent C via a secondary edge label L
and L ? {
*
ICH
*
,
*
T
*
,
*
RNR
*
}
disconnect C
disconnect the secondary edge
attach C to the parent of t
procedure split-small-clauses(T )
for each verb phrase C in T
if C has a child S and the phrase label of S is S
and S is not preceded by a ?? or , tag
and S has a subject child s
disconnect s
attach s to C
set the function tag of s to OBJ
set the function tag of S to OPRD
procedure assign-heads(N)
for each child C of N
assign-heads(C)
if is-coordinated(N)
e ? index of first CC or CONJP or , or :
else
e ? index of last child of N
find head child H between 1 and e according to head rules (Table 3)
N.head ? H.head
procedure is-coordinated(N)
if N has the label UCP return True
if N has a CC or CONJP child which is not leftmost return True
if N has a , or : child c, and c is not leftmost or rightmost or
crossed by an apposition link, return True
else return False
procedure create-dependency-tree(T )
D ? {}
for each token t in T
let C be the highest constituent that t is the head of
let P be the parent of C
let L be the function tag of C
D ? D ? P.head ?
L
t
return D
(Table 3). The first column in the table indicates
the phrase type, the second is the search direction,
and the third is a priority list of phrase types to
look for. For instance, to find the head of an S
phrase, we look from right to left for a VP. If
no VP is found, look for anything with a PRD
function tag, and so on.
Moreover, since the grammatical structure in-
164
ADJP ? NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT
FW RBR RBS SBAR RB
ADVP ? RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN
CONJP ? CC RB IN
FRAG ? (NN
*
| NP) W
*
SBAR (PP | IN) (ADJP | JJ) ADVP
RB
INTJ ?
*
LST ? LS :
NAC, NP, NX, WHNP ? (NN
*
| NX) NP-? JJR CD JJ JJS RB QP NP
PP, WHPP ? IN TO VBG VBN RP FW
PRN ? S
*
N
*
W
*
PP|IN ADJP|JJ
*
ADVP|RB
*
PRT ? RP
QP ? $ IN NNS NN JJ RB DT CD NCD QP JJR JJS
RRC ? VP NP ADVP ADJP PP
S ? VP
*
-PRD S SBAR ADJP UCP NP
SBAR ? S SQ SINV SBAR FRAG IN DT
SBARQ ? SQ S SINV SBARQ FRAG
SINV ? VBZ VBD VBP VB MD VP
*
-PRD S SINV ADJP NP
SQ ? VBZ VBD VBP VB MD
*
-PRD VP SQ
UCP ?
*
VP ? VBD VBN MD VBZ VB VBG VBP VP
*
-PRD ADJP NN NNS
NP
WHADJP ? CC WRB JJ ADJP
WHADVP ? CC WRB
X ?
*
Table 3: Head rules.
Algorithm 2: Pseudocode for the function la-
beling procedure.
procedure assign-functions(T )
for each constituent C in T
if C has no function tag from Penn or GLARF
L ? infer-function(C)
Set the function tag of C to L
procedure infer-function(C)
let c be the head of C, P the parent of C, and p the head of P
if C is an object return OBJ
if C is PRN return PRN
if h is punctuation return P
if C is coordinated with P return COORD
if C is PP, ADVP, or SBAR and P is VP return ADV
if C is PRT and P is VP return PRT
if C is VP and P is VP, SQ, or SINV return VC
if C is TO and P is VP return IM
if P is SBAR and p is IN return SUB
if P is VP, S, SBAR, SBARQ, SINV, or SQ and C is RB return ADV
if P is NP, NX, NAC, or WHNP return NMOD
if P is ADJP, ADVP, WHADJP, or WHADVP return AMOD
if P is PP or WHPP return PMOD
else return DEP
side noun phrases (NP) is under-specified in the
Penn Treebank, we imported dependencies in-
side NPs and hyphenated words from a version
of the Penn Treebank mapped into GLARF, the
Grammatical and Logical Argument Representa-
tion Framework (Meyers et al, 2007).
The parts of GLARF?s NP analysis that are most
relevant to this task include: (i) identifying ap-
posites (APPO, e.g., that book depends on gift in
Mary?s gift, a book about cheese; (ii) the iden-
tification of name boundaries taken from BBN?s
NE annotation, e.g., identifying that Smith de-
pends on Mary which depends on appointment
in the Mary Smith appointment; (iii) identifying
TITLE and POSTHON dependencies, e.g., deter-
mining that Ms. and III depend on Mary in Ms.
Mary Smith III. These identifications were car-
ried out by hand-coded rules that have been fine
tuned as part of GLARF, over the past several
years. For example, identifying apposition con-
structions requires identifying that both the head
and the apposite can stand alone ? proper nouns
(John Smith), plural nouns (books), and singular
common nouns with determiners (the book) are
stand-alone cases, whereas singular nouns without
determiners (green book) do not qualify.
We split Treebank tokens at a hyphen (-) or a
forward slash (/) if the segments on either side of
these delimiters are: (a) a word in a dictionary
(COMLEX Syntax or any of the dictionaries avail-
able on the NOMLEX website); (b) part of a mark-
able Named Entity;
9
or (c) a prefix from the list:
co, pre, post, un, anti, ante, ex, extra, fore, non,
over, pro, re, super, sub, tri, bi, uni, ultra. For ex-
ample, York-based was split into 3 segments: (1)
York, (2) - and (3) based.
9
The CoNLL-2008 website contains a Named Entity To-
ken gazetteer to aid in this segmentation.
165
3.2.2 Semantic Dependencies
When encoding the semantic dependencies, it
was necessary to convert the underlying con-
stituent analysis of PropBank and NomBank into
a dependency analysis. Because semantic predi-
cates are already assigned to individual tokens in
both PropBank (the version used for the CoNLL-
2005 shared task) and NomBank, constituent-to-
dependency conversion is thus necessary only for
semantic arguments. Conceptually, this conver-
sion can be handled using similar heuristics as de-
scribed in Section 3.2.1. However, in order to
avoid replicating this effort and to ensure compat-
ibility between syntactic and semantic dependen-
cies, we decided to generate semantic dependen-
cies using only argument boundaries and the syn-
tactic dependencies generated in Section 3.2.1, i.e.,
ignoring syntactic constituents. Given this input,
we identify the head of a semantic argument using
the following heuristic:
The head of a semantic argument is as-
signed to the token inside the argument
boundaries whose head is a token out-
side the argument boundaries.
This heuristic works remarkably well: over 99%
of the PropBank arguments in the training corpus
have a single token whose head is located outside
of the argument boundaries. As a simple example,
consider the following annotated text: [sold]
PRED
[1214 cars]
ARG1
[in the U.S.]
ARGM-LOC
. Us-
ing the above heuristic, the head of the ARG1 ar-
gument is set to cars, because it has an OBJ de-
pendency to sold, and the head of the ARGM-
LOC argument is set to in, because it modifies sold
through a LOC dependency.
While this heuristic processes the vast majority
of arguments, there are several cases that require
special treatment. We discuss these situations in
the remainder of this section.
Arguments with several syntactic heads
For 0.7% of the semantic arguments, the above
heuristic detects several syntactic heads for the
given boundary. For example, in the text [it]
ARG0
[expects]
PRED
[its U.S. sales to remain steady
at about 1200 cars]
ARG1
, the above heuris-
tic assigns two syntactic heads to ARG1: sales,
which modifies expects through an OBJ depen-
dency, and to, which modifies expects through a
PRD dependency. These situations are caused
by the constituent-to-dependency conversion pro-
cess described in Section 3.2.1, which in some
cases interprets syntax differently than the orig-
inal Treebank annotation, e.g., the raising phe-
nomenon for the PRD dependency in the above
example. In such cases, we split the original argu-
ment into a sequence of discontinuous arguments,
e.g., the ARG1 in the above example becomes [its
U.S. sales]
ARG1
[to remain steady at about 1200
cars]
C-ARG1
.
Merging discontinuous arguments
While in the above case we split arguments, there
are situations where we can merge arguments that
were initially discontinuous in PropBank or Nom-
Bank. This typically happens when the Prop-
Bank/NomBank predicate is infixed inside one of
its arguments. For example, in the text [Million-
dollar conferences]
ARG1
were [held]
PRED
[to
chew on subjects such as... ]
C-ARG1
, PropBank
lists multiple constituents as aggregately filling the
ARG1 slot of held. These cases are detected au-
tomatically because the least common ancestor of
the argument pieces is actually one of the argument
segments. In the above example, to chew on sub-
jects such as... depends on Million-dollar confer-
ences because to modifies conferences through a
NMOD dependency. In these situations, we treat
the least common ancestor, e.g., conferences in the
above text, as the true argument. This heuristic al-
lowed us to merge 1665 (or 0.6% of total) argu-
ments that were initially discontinuous in the Prop-
Bank training corpus.
Empty categories
PropBank and NomBank both encode chains of
empty categories. As with the 2005 shared task
(Carreras and M`arquez, 2005), we used the head
of the antecedent of empty categories as arguments
rather than empty categories. Furthermore, empty
category arguments with no antecedents were ig-
nored.
10
For example, given The man wanted t to
make a speech, we assume that the A0 of make and
speech is man, rather than the chain consisting of
the empty category represented as t and man.
Annotation disagreements
NomBank and Penn Treebank annotators some-
times disagree about constituent structure. Nom-
10
Under our approach to filler gap constructions, the filler
is a shared argument (as in Relational Grammar, most Feature
Structure and Dependency Grammar frameworks), in con-
trast with the Penn Treebank?s empty category antecedent ap-
proach (more closely resembling the various Chomskian ap-
proaches).
166
Label Freq. Description
NMOD 324834 Modifier of nominal
P 135260 Punctuation
PMOD 115988 Modifier of preposition
SBJ 89371 Subject
OBJ 66677 Object
ROOT 49178 Root
ADV 47379 General adverbial
NAME 41138 Name-internal link
VC 35250 Verb chain
COORD 31140 Coordination
DEP 29456 Unclassified
TMP 26305 Temporal adverbial or nominal modifier
CONJ 24522 Second conjunct (dependent on conjunction)
LOC 18500 Locative adverbial or nominal modifier
AMOD 17868 Modifier of adjective or adverbial
PRD 16265 Predicative complement
APPO 16163 Apposition
IM 16071 Infinitive verb (dependent on infinitive marker to)
HYPH 14073 Token part of a hyphenated word (dependent on a preceding part of the hyphenated word)
HMOD 13885 Token inside a hyphenated word (dependent on the head of the hyphenated word)
SUB 12995 Subordinated clause (dependent on subordinating conjuction)
OPRD 11707 Predicative complement of raising/control verb
SUFFIX 10548 Possessive suffix (dependent on possessor)
DIR 6145 Adverbial of direction
TITLE 5917 Title (dependent on name)
MNR 4753 Adverbial of manner
POSTHON 4377 Posthonorific modifier of nominal
PRP 4013 Adverbial of purpose or reason
PRT 3235 Particle (dependent on verb)
LGS 3115 Logical subject of a passive verb
EXT 2374 Adverbial of extent
PRN 2176 Parenthetical
EXTR 658 Extraposed element in cleft
DTV 496 Dative complement (to) in dative shift
PUT 271 Complement of the verb put
BNF 44 Benefactor complement (for) in dative shift
VOC 24 Vocative
Table 4: Statistics for atomic syntactic labels.
Bank annotators are in effect assuming that the
constituents provided form a phrase. In this case,
the constituents are adjacent to each other. For ex-
ample, consider the NP the human rights discus-
sion. In this case, the Penn Treebank would treat
each of the four words the, human, rights, discus-
sion as daughters of a single NP node. However,
NomBank would treat human rights as a single
ARG1 of discussion. Since noun noun modifica-
tion constructions are head final, we can easily de-
termine (via GLARF) that rights is the markable
dependent of discussion.
Support chains
Finally, NomBank?s encoding of support chains is
handled as chains of dependencies in the data (al-
though these are not scored). For example, given
Mary took dozens of walks, where Mary is the
ARG0 of walks, the support chain took + dozens +
of is represented as a sequence of dependencies: of
depends on Mary, dozens depends on of and took
depends on dozens. Each of these dependencies is
labeled SU.
3.3 Overview of Corpora
The syntactic dependency types are divided into
atomic types that consist of a single label, and non-
atomic types consisting of more than one label.
There are 38 atomic and 70 non-atomic labels in
the corpus. There are three types of non-atomic
labels: those consisting of a PRD or OPRD con-
catenated with an adverbial label such as LOC or
TMP; gapping labels such as GAP-SBJ; and com-
bined adverbial tags such as LOC-TMP.
Table 4 shows statistics for the atomic syntac-
tic dependencies: label type, the frequency of the
label in the complete corpus, and a description of
the label. Table 5 shows the corresponding statis-
tics for non-atomic dependencies, excluding gap-
ping dependencies. The non-atomic labels are rare,
which made it difficult to learn these relations ef-
167
Label Frequency
LOC-PRD 798
PRD-TMP 51
PRD-PRP 45
LOC-OPRD 31
DIR-PRD 4
MNR-PRD 3
LOC-TMP 2
MNR-TMP 1
LOC-MNR 1
DIR-OPRD 1
Table 5: Statistics for non-atomic syntactic labels
excluding gapping labels.
Label Frequency
GAP-SBJ 116
GAP-OBJ 102
DEP-GAP 83
GAP-TMP 69
GAP-PRD 66
GAP-LGS 44
GAP-LOC 42
DIR-GAP 37
GAP-PMOD 22
GAP-VC 20
EXT-GAP 16
ADV-GAP 15
GAP-NMOD 13
GAP-LOC-PRD 6
DTV-GAP 6
AMOD-GAP 6
GAP-MNR 5
GAP-PRP 4
EXTR-GAP 3
GAP-SUB 1
GAP-PUT 1
GAP-OPRD 1
Table 6: Statistics for non-atomic labels containing
a gapping label.
fectively. Table 6 shows the table for non-atomic
labels containing a gapping label.
A dependency link w
i
? w
j
is said to be pro-
jective if all words occurring between w
i
and w
j
in
the surface word order are dominated by w
i
(where
dominance is the transitive closure of the direct
link relation). Nonprojective links are impossible
to handle for the search procedures in many types
of dependency parsers. It has been previously ob-
served that the majority of dependencies in all lan-
guages are projective, and this is particularly true
for English ? in the complete corpus, only 4118
links (0.4%) are nonprojective. 3312 sentences, or
7.6%, contain at least one nonprojective link.
Table 7 shows statistics for different types of
nonprojective links: nonprojectivity caused by
wh-movement, such as in Where are you going?
or What have you done?; split clauses such as
Type Frequency
wh-movement 1709
Split clause 734
Split noun phrase 590
Other 1085
Table 7: Statistics for nonprojective links.
POS Frequency
NN 68477
NNS 30048
VBD 24106
VB 23650
VBN 19339
VBG 14245
VBZ 10883
VBP 6330
Other 83
Table 8: Statistics for predicates, by POS tags.
Even to make love, he says, you need experience;
split noun phrases such as hold a hearing tomor-
row on the topic; and all other types of nonprojec-
tive links.
Lastly, Tables 8 and 9 summarizes statistics for
semantic predicates and roles. Table 8 shows the
number of non-support predicates with a given
POS tag in the whole corpus (we used GPOS or
PPOSS for predicates inside hyphenated words).
The last line shows the number of predicates with
a POS tag that does not start with NN or VB. This
last table entry is generated by POS tagger mis-
takes when producing the PPOSS tags, or by errors
in our NomBank/PropBank conversion software.
11
Nevertheless, the overall picture given by the table
indicates that predicates are almost perfectly dis-
tributed between nouns and verbs: there are 98525
nominal and 98553 verbal predicates.
Table 9 shows the number of arguments with a
given role label. For brevity we list only labels that
are instantiated at least 10 times in the whole cor-
pus. The total number of arguments labeled with a
role label with frequency lower than 10 is listed
in the last line in the table. The table indicates
that, while the top three most common role labels
are ?core? labels (A1, A0, A2), modifier arguments
(AM-
*
) account for approximately 20% of the total
number of arguments. On the other hand, discon-
tinuous arguments are not common: only 0.7% of
the total number of arguments have a continuation
label (C-
*
).
11
In very few situations, we select incorrect head tokens for
multi-word predicates.
168
Label Frequency
A1 161409
A0 109437
A2 51197
AM-TMP 25913
AM-MNR 13080
AM-LOC 11409
A3 10269
AM-MOD 9986
AM-ADV 9496
AM-DIS 5369
R-A0 4432
AM-NEG 4097
A4 3281
C-A1 3118
R-A1 2565
AM-PNC 2445
AM-EXT 1428
AM-CAU 1346
AM-DIR 1318
R-AM-TMP 797
R-A2 307
R-AM-LOC 246
R-AM-MNR 155
A5 91
AM-PRD 78
C-A0 70
C-A2 65
R-AM-CAU 50
C-A3 37
R-A3 29
C-AM-MNR 24
C-AM-ADV 20
AM-REC 16
AA 14
R-AM-PNC 12
C-AM-EXT 11
C-AM-TMP 11
C-A4 11
Frequency < 10 70
Table 9: Statistics for semantic roles.
4 Submissions and Results
Nineteen groups submitted test runs in the closed
challenge and five groups participated in the open
challenge. Three of the latter groups participated
only in the open challenge, and two of these sub-
mitted results only for the semantic subtask. These
results are summarized in Tables 10 and 11.
Table 10 summarizes the official results ? i.e.,
results at evaluation deadline ? for the closed chal-
lenge. Note that several teams corrected bugs
and/or improved their systems and they submit-
ted post-evaluation scores (accounted in the shared
task website). The table indicates that most of the
top results cluster together: three systems had a
labeled macro F
1
score on the WSJ+Brown cor-
pus around 82 points (che, ciaramita, and zhao);
five systems scored around 79 labeled macro F
1
points (yuret, samuelsson, zhang, henderson, and
watanabe). Remarkably, the top-scoring system
(johansson) is in a class of its own, with scores
2?3 points higher than the next system. This is
most likely caused by the fact that Johansson and
Nugues (2008) implemented a thorough system
that addressed all facets of the task with state-of-
the-art methods: second-order parsing model, ar-
gument identification/classification models sepa-
rately tuned for PropBank and NomBank, rerank-
ing inference for the SRL task, and, finally, joint
optimization of the complete task using meta-
learning (more details in Section 5).
Table 11 lists the official results in the open chal-
lenge. The results in this challenge are lower than
in the closed challenge, but this was somewhat
to be expected considering that there were fewer
participants in this challenge and none of the top
five groups in the closed challenge submitted re-
sults in the open challenge. Only one of the sys-
tems that participated in both challenges (zhang)
improved the results submitted in the closed chal-
lenge. Zhang et al (2008) achieved this by ex-
tracting features for their semantic subtask mod-
els both from the parser used in the closed chal-
lenge and a secondary parser that was trained on
a different corpus. The improvements measured
were relatively small for the in-domain WSJ cor-
pus (0.2 labeled macro F
1
points) but larger for the
out-of-domain Brown corpus (approximately 1 la-
beled macro F
1
point).
Tables 10 and 11 indicate that in both chal-
lenges the results on the out-of-domain corpus
(Brown) are much lower than the results measured
in-domain (WSJ). The difference is around 7?8
LAS points for the syntactic subtask and 12?14 la-
beled F
1
points for semantic dependencies. Over-
all, this yields a drop of approximately 10 labeled
macro F
1
points for most systems. This perfor-
mance decrease on out-of-domain corpora is con-
sistent with the results reported in CoNLL-2005
on SRL (using the same Brown corpus). These
results indicate that domain adaptation is a prob-
lem that is far from being solved for both syntactic
and semantic analysis of text. Furthermore, as the
scores on the syntactic and semantic subtasks in-
dicate, domain adaptation becomes even harder as
the task to be solved gets more complex.
We describe the participating systems in the next
section. Then, in Section 6, we revert to result
analysis using different evaluation measures and
different views of the data.
169
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06
che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37
ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24
zhao 81.44 (4) 82.62 71.78 86.66 (8) 87.52 79.83 76.16 (4) 77.67 63.69
yuret 79.84 (5) 80.97 70.55 86.62 (10) 87.39 80.46 73.06 (5) 74.54 60.62
samuelsson 79.79 (6) 80.92 70.49 86.63 (9) 87.36 80.77 72.94 (6) 74.47 60.18
zhang 79.32 (7) 80.41 70.48 87.32 (2) 88.14 80.80 71.31 (7) 72.67 60.16
henderson 79.11 (8) 80.19 70.34 86.91 (4) 87.78 80.01 70.97 (8) 72.26 60.38
watanabe 79.10 (9) 80.30 69.29 87.18 (3) 88.06 80.17 70.84 (9) 72.37 58.21
morante 78.43 (10) 79.52 69.55 86.07 (12) 86.88 79.58 70.51 (10) 71.88 59.23
li 78.35 (11) 79.38 70.01 86.69 (6) 87.42 80.8 69.95 (11) 71.27 59.17
baldridge 77.49 (12) 78.57 68.53 86.67 (7) 87.42 80.64 67.92 (14) 69.35 55.95
chen 77.00 (13) 77.95 69.23 84.47 (16) 85.20 78.58 69.45 (12) 70.62 59.81
lee 76.90 (14) 77.96 68.34 84.82 (15) 85.69 77.83 68.71 (13) 69.95 58.63
sun 76.28 (15) 77.10 69.58 85.75 (13) 86.37 80.75 66.61 (15) 67.62 58.26
choi 71.23 (16) 72.22 63.44 77.56 (17) 78.58 69.46 64.78 (16) 65.72 57.4
trandabat 63.45 (17) 64.21 57.41 85.21 (14) 85.96 79.24 40.63 (17) 41.36 34.75
lluis 63.29 (18) 63.74 59.65 71.95 (18) 72.30 69.14 54.52 (18) 55.09 49.95
neumann 19.93 (19) 20.13 18.14 16.25 (19) 16.22 16.47 22.36 (19) 22.86 17.94
Table 10: Official results in the closed challenge (post-evaluation scores are available on the shared
task website). Teams are denoted by the last name of the first author of the corresponding paper in
the proceedings or the last name of the person who registered the team if no paper was submitted.
Italics indicate that there is no corresponding paper in the proceedings. Results are sorted in descending
order of the labeled macro F
1
score on the WSJ+Brown corpus. The number in parentheses next to the
WSJ+Brown scores indicates the system rank in the corresponding task.
Labeled Macro F
1
Labeled Attachment Score Labeled F
1
(complete task) (syntactic dependencies) (semantic dependencies)
WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
vickrey ? ? ? ? ? ? 76.17 (1) 77.38 66.23
riedel ? ? ? ? ? ? 74.59 (2) 75.72 65.38
zhang 79.61 (1) 80.61 71.45 87.32 (1) 88.14 80.80 71.89 (3) 73.08 62.11
li 77.84 (2) 78.87 69.51 86.69 (2) 87.42 80.80 68.99 (4) 70.32 58.22
wang 76.19 (3) 78.39 59.89 84.56 (3) 85.50 77.06 67.12 (5) 70.41 42.67
Table 11: Official results in the open challenge (post-evaluation scores are available on the shared task
website). Teams are denoted by the last name of the first author of the corresponding paper in the
proceedings or the last name of the person who registered the team if no paper was submitted. Italics
indicate that there is no corresponding paper in the proceedings. Results are sorted in descending order of
the labeled F
1
score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses
next to the WSJ+Brown scores indicates the system rank in the corresponding task.
5 Approaches
Table 5 summarizes the properties of the sys-
tems that participated in the closed the open chal-
lenges. The second column of the table high-
lights the overall architectures. We used + to in-
dicate that the components are sequentially con-
nected. The lack of a + sign indicates that the cor-
responding tasks are performed jointly. For exam-
ple, Riedel and Meza-Ruiz (2008) perform pred-
icate and argument identification and classifica-
tion jointly, whereas Ciaramita et al (2008) im-
plemented a pipeline architecture of three compo-
nents. We use the || to indicate that several differ-
ent architectures that span multiple subtasks were
deployed in parallel.
This summary of system architectures indicates
that it is common that systems combine sev-
eral components in the semantic or syntactic sub-
tasks ? e.g., nine systems jointly performed pred-
icate/argument identification and classification ?
but only four systems combined components be-
tween the syntactic and semantic subtasks: Hen-
derson et al (2008), who implemented a generative
history-based model (Incremental Sigmoid Belief
Networks with vectors of latent variables) where
syntactic and semantic structures are separately
170
generated but using a synchronized derivation (se-
quence of actions); Samuelsson et al (2008),
who, within an ensemble-based architecture, im-
plemented a joint syntactic-semantic model using
MaltParser with labels enriched with semantic in-
formation; Llu??s and M`arquez, who used a modi-
fied version of the Eisner algorithm to jointly pre-
dict syntactic and semantic dependencies; and fi-
nally, Sun et al (2008), who integrated depen-
dency label classification and argument identifi-
cation using a maximum-entropy Markov model.
Additionally, Johansson and Nugues (2008), who
had the highest ranked system in the closed chal-
lenge, integrate syntactic and semantic analysis in
a final reranking step, which maximizes the joint
syntactic-semantic score in the top k solutions. In
the same spirit, Chen et al (2008) search in the
top k solutions for the one that maximizes a global
measure, in this case the joint probability of the
complete problem. These joint learning strategies
are summarized in the Joint Learning/Opt. col-
umn in the table. The system of Riedel and Meza-
Ruiz (2008) deserves a special mention: even
though Riedel and Meza-Ruiz did not implement
a syntactic parser, they are the only group that per-
formed the complete SRL subtask ? i.e., predicate
identification and classification, argument identifi-
cation and classification ? jointly, simultaneously
for all the predicates in a sentence. They imple-
mented a joint SRL model using Markov Logic
Networks and they selected the overall best solu-
tion using inference based on the cutting-plane al-
gorithm.
Although some of the systems that implemented
joint approaches obtained good results, the top
five systems in the closed challenge are essen-
tially systems with pipeline architectures. Further-
more, Johansson and Nugues (2008) and Riedel
and Meza-Ruiz (2008) showed that joint learn-
ing/optimization improves the overall results, but
the improvement is not large. These initial ef-
forts indicate at least that the joint modeling of this
problem is not a trivial task.
The D Arch. and D Inference columns summa-
rize the parsing architectures and the correspond-
ing inference strategies. Similar to last year?s
shared task (Nivre et al, 2007), the vast majority of
parsing models fall in two classes: transition-based
(?trans? in the table) or graph-based (?graph?)
models. By and large, transition-based models use
a greedy inference strategy, whereas graph-based
models used different Maximum Spanning Tree
(MST) algorithms: Carreras (2007) ? MST
C
, Eis-
ner (2000) ? MST
E
, or Chu-Liu/Edmonds (Mc-
Donald et al, 2005; Chu and Liu, 1965; Edmonds,
1967) ? MST
CL/E
. More interestingly, most of
the best systems used some strategy to mitigate
parsing errors. In the top three systems in the
closed challenge, two (che and ciaramita) used
parser combination through voting and/or stacking
of different models (see the D Comb. column).
Samuelsson et al (2008) perform a MST infer-
ence with the bag of all dependencies output by
the individual MALT parser variants. Johansson
and Nugues (2008) use a single parsing model, but
this model is extended with second-order features.
The PA Arch. and PA Inference columns sum-
marize the architectures and inference strategies
used for the identification and classification of
predicates and arguments. The columns indicate
that most systems modeled the SRL problem as a
token-by-token classification problem (?class? in
the table) with a corresponding greedy inference
strategy. Some systems (e.g., yuret, samuelsson,
henderson, lluis) incorporate SRL within parsing,
in which case we report the corresponding parsing
architecture and inference approach. Vickrey and
Koller (2008) simplify the sentences to be labeled
using a set of hand-crafted rules before deploying
a classification model on top of a constituent-based
representation. Unlike in the case of parsing, few
systems (yuret, samuelssson, and morante) com-
bine several PA models and the combination is lim-
ited to simple voting strategies (see the PA Comb.
column).
Finally, the ML Methods column lists the Ma-
chine Learning (ML) methods used. The column
indicates that maximum entropy (ME) was the
most popular method (12 distinct systems relied
on it). Support Vector Machines (SVM) (eight sys-
tems) and the Perceptron algorithm (three systems)
were also popular ML methods.
6 Analysis
Section 4 summarized the results in the closed
and open challenges using the official evaluation
measures. In this section, we analyze the sub-
mitted runs using different evaluation measures,
e.g., Exact Match or Perfect Proposition F
1
scores,
and different views of the data, e.g., only non-
projective dependencies or NomBank versus Prop-
Bank frames.
171
O
v
e
r
a
l
l
D
D
D
P
A
P
A
P
A
J
o
i
n
t
M
L
c
l
o
s
e
d
A
r
c
h
.
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
A
r
c
h
.
C
o
m
b
.
I
n
f
e
r
e
n
c
e
L
e
a
r
n
i
n
g
/
O
p
t
.
M
e
t
h
o
d
s
j
o
h
a
n
s
s
o
n
D
+
P
I
+
P
C
+
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
C
c
l
a
s
s
n
o
r
e
r
a
n
k
r
e
r
a
n
k
P
e
r
c
e
p
t
r
o
n
,
M
E
c
h
e
D
+
P
I
+
P
C
+
A
I
C
g
r
a
p
h
s
t
a
c
k
i
n
g
M
S
T
C
L
/
E
c
l
a
s
s
n
o
I
L
P
n
o
M
E
c
i
a
r
a
m
i
t
a
D
+
P
I
C
+
A
I
C
t
r
a
n
s
v
o
t
i
n
g
,
g
r
e
e
d
y
c
l
a
s
s
n
o
r
e
r
a
n
k
n
o
S
V
M
,
M
E
,
s
t
a
c
k
i
n
g
P
e
r
c
e
p
t
r
o
n
z
h
a
o
D
+
A
I
C
+
P
I
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
M
E
y
u
r
e
t
D
+
(
P
I
C
+
A
I
+
A
C
|
|
g
r
a
p
h
n
o
M
S
T
E
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
L
E
,
P
I
C
+
A
I
C
)
g
e
n
e
r
a
t
i
v
e
M
B
L
s
a
m
u
e
l
s
s
o
n
D
+
P
I
+
t
r
a
n
s
M
S
T
C
L
/
E
g
r
e
e
d
y
c
l
a
s
s
,
v
o
t
i
n
g
g
r
e
e
d
y
u
n
i
fi
e
d
S
V
M
(
A
I
+
A
C
|
|
D
A
I
C
)
+
P
C
b
l
e
n
d
i
n
g
t
r
a
n
s
l
a
b
e
l
s
z
h
a
n
g
D
+
P
I
+
A
I
+
A
C
+
P
C
g
r
a
p
h
,
m
e
t
a
-
M
S
T
C
L
/
E
,
c
l
a
s
s
n
o
g
r
e
e
d
y
n
o
S
V
M
,
M
E
t
r
a
n
s
l
e
a
r
n
i
n
g
g
r
e
e
d
y
h
e
n
d
e
r
s
o
n
D
P
A
I
C
+
D
g
e
n
e
r
a
t
i
v
e
,
n
o
b
e
a
m
t
r
a
n
s
n
o
b
e
a
m
s
y
n
c
h
r
o
n
i
z
e
d
I
S
B
N
t
r
a
n
s
s
e
a
r
c
h
s
e
a
r
c
h
d
e
r
i
v
a
t
i
o
n
w
a
t
a
n
a
b
e
D
I
+
D
C
+
P
I
+
P
C
+
A
I
+
A
C
r
e
l
a
t
i
v
e
p
r
e
f
e
r
e
n
c
e
n
o
g
r
e
e
d
y
t
o
u
r
n
a
m
e
n
t
c
l
a
s
s
n
o
n
o
n
o
S
V
M
,
m
o
d
e
l
m
o
d
e
l
,
V
i
t
e
r
b
i
C
R
F
,
M
B
L
m
o
r
a
n
t
e
D
+
P
I
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
S
V
M
,
M
B
L
l
i
D
+
P
I
C
+
A
I
C
g
r
a
p
h
n
o
M
S
T
C
L
/
E
c
l
a
s
s
v
o
t
i
n
g
g
r
e
e
d
y
n
o
M
E
c
h
e
n
D
+
P
I
+
P
C
+
A
I
C
t
r
a
n
s
n
o
p
r
o
b
c
l
a
s
s
n
o
p
r
o
b
g
l
o
b
a
l
p
r
o
b
a
b
i
l
i
t
y
M
E
o
p
t
i
m
i
z
a
t
i
o
n
l
e
e
D
+
P
I
+
A
I
C
+
P
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
s
u
n
D
I
+
P
I
+
D
C
A
I
+
A
C
g
r
a
p
h
n
o
M
S
T
E
,
g
r
a
p
h
n
o
V
i
t
e
r
b
i
,
M
E
M
M
,
M
E
V
i
t
e
r
b
i
I
L
P
V
i
t
e
r
b
i
l
l
u
i
s
D
+
P
I
+
D
A
I
C
+
P
C
g
r
a
p
h
n
o
M
S
T
E
g
r
a
p
h
n
o
M
S
T
E
M
S
T
E
P
e
r
c
e
p
t
r
o
n
,
S
V
M
n
e
u
m
a
n
n
D
+
P
I
+
P
C
+
A
I
+
A
C
t
r
a
n
s
n
o
g
r
e
e
d
y
c
l
a
s
s
n
o
n
o
n
o
M
E
o
p
e
n
v
i
c
k
r
e
y
A
I
+
A
C
+
P
I
+
P
C
?
?
?
s
e
n
t
e
n
c
e
n
o
g
r
e
e
d
y
?
M
E
s
i
m
p
l
i
fi
c
a
t
i
o
n
,
c
l
a
s
s
r
i
e
d
e
l
P
A
I
C
?
?
?
M
a
r
k
o
v
n
o
C
u
t
t
i
n
g
?
M
I
R
A
L
o
g
i
c
P
l
a
n
e
N
e
t
w
o
r
k
w
a
n
g
P
I
+
A
I
C
t
r
a
n
s
,
n
o
g
r
e
e
d
y
,
c
l
a
s
s
n
o
p
r
o
b
n
o
S
V
M
,
M
E
g
r
a
p
h
M
S
T
C
L
/
E
M
I
R
A
T
a
b
l
e
1
2
:
S
u
m
m
a
r
y
o
f
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
t
h
e
c
l
o
s
e
d
a
n
d
o
p
e
n
c
h
a
l
l
e
n
g
e
s
.
T
h
e
c
l
o
s
e
d
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
m
a
c
r
o
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
B
e
c
a
u
s
e
s
o
m
e
o
p
e
n
-
c
h
a
l
l
e
n
g
e
s
y
s
t
e
m
s
d
i
d
n
o
t
i
m
p
l
e
m
e
n
t
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
,
t
h
e
s
e
s
y
s
t
e
m
s
a
r
e
s
o
r
t
e
d
b
y
l
a
b
e
l
e
d
F
1
s
c
o
r
e
o
f
t
h
e
s
e
m
a
n
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
o
n
t
h
e
W
S
J
+
B
r
o
w
n
c
o
r
p
u
s
.
O
n
l
y
t
h
e
s
y
s
t
e
m
s
t
h
a
t
h
a
v
e
a
c
o
r
r
e
s
p
o
n
d
i
n
g
p
a
p
e
r
i
n
t
h
e
p
r
o
c
e
e
d
i
n
g
s
a
r
e
i
n
c
l
u
d
e
d
.
S
y
s
t
e
m
s
t
h
a
t
p
a
r
t
i
c
i
p
a
t
e
d
i
n
b
o
t
h
c
h
a
l
l
e
n
g
e
s
a
r
e
l
i
s
t
e
d
o
n
l
y
i
n
t
h
e
c
l
o
s
e
d
c
h
a
l
l
e
n
g
e
.
A
c
r
o
n
y
m
s
u
s
e
d
:
D
-
s
y
n
t
a
c
t
i
c
d
e
p
e
n
d
e
n
c
i
e
s
,
P
-
p
r
e
d
i
c
a
t
e
,
A
-
a
r
g
u
m
e
n
t
,
I
-
i
d
e
n
t
i
fi
c
a
t
i
o
n
,
C
-
c
l
a
s
s
i
fi
c
a
t
i
o
n
.
O
v
e
r
a
l
l
a
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
a
r
c
h
i
t
e
c
t
u
r
e
;
D
A
r
c
h
.
s
t
a
n
d
s
f
o
r
t
h
e
a
r
c
h
i
t
e
c
t
u
r
e
o
f
t
h
e
s
y
n
t
a
c
t
i
c
p
a
r
s
e
r
;
D
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
fi
n
a
l
p
a
r
s
e
r
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
u
s
i
n
g
p
a
r
s
e
r
c
o
m
b
i
n
a
t
i
o
n
;
D
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
s
y
n
t
a
c
t
i
c
p
a
r
s
i
n
g
;
P
A
A
r
c
h
.
s
t
a
n
d
s
t
h
e
t
y
p
e
o
f
a
r
c
h
i
t
e
c
t
u
r
e
u
s
e
d
f
o
r
P
A
I
C
;
P
A
C
o
m
b
.
i
n
d
i
c
a
t
e
s
i
f
t
h
e
P
A
o
u
t
p
u
t
w
a
s
g
e
n
e
r
a
t
e
d
t
h
r
o
u
g
h
s
y
s
t
e
m
c
o
m
b
i
n
a
t
i
o
n
;
P
A
I
n
f
e
r
e
n
c
e
s
t
a
n
d
s
f
o
r
t
h
e
t
h
e
t
y
p
e
o
f
i
n
f
e
r
e
n
c
e
u
s
e
d
f
o
r
P
A
I
C
;
J
o
i
n
t
L
e
a
r
n
i
n
g
/
O
p
t
.
i
n
d
i
c
a
t
e
s
i
f
s
o
m
e
f
o
r
m
o
f
j
o
i
n
t
l
e
a
r
n
i
n
g
o
r
o
p
t
i
m
i
z
a
t
i
o
n
w
a
s
i
m
p
l
e
m
e
n
t
e
d
f
o
r
t
h
e
s
y
n
t
a
c
t
i
c
+
s
e
m
a
n
t
i
c
g
l
o
b
a
l
t
a
s
k
;
M
L
m
e
t
h
o
d
s
l
i
s
t
s
t
h
e
M
L
m
e
t
h
o
d
s
u
s
e
d
t
h
r
o
u
g
h
o
u
t
t
h
e
c
o
m
p
l
e
t
e
s
y
s
t
e
m
.
172
Exact Match Perfect Proposition F
1
(complete task) (semantic dependencies)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 12.46 (1) 12.46 12.68 54.12 (1) 56.12 36.90
che 10.37 (2) 10.21 11.50 48.05 (2) 50.15 30.90
ciaramita 9.27 (3) 9.04 10.80 46.05 (3) 48.05 28.61
zhao 9.20 (4) 9.00 10.56 43.19 (4) 45.23 26.14
henderson 8.11 (5) 7.75 10.33 39.24 (5) 40.64 27.51
watanabe 7.79 (6) 7.54 9.39 36.44 (6) 38.09 22.72
yuret 7.65 (7) 7.33 9.62 34.61 (9) 36.13 21.78
zhang 7.40 (8) 7.46 7.28 34.96 (8) 36.25 24.22
li 7.12 (9) 6.71 9.62 32.08 (10) 33.45 20.62
samuelsson 6.94 (10) 6.62 8.92 35.20 (7) 36.96 20.22
chen 6.83 (11) 6.46 9.15 31.02 (12) 32.08 22.14
lee 6.69 (12) 6.29 9.15 31.40 (11) 32.52 22.18
morante 6.44 (13) 6.04 8.92 30.41 (14) 31.97 17.49
sun 5.38 (14) 4.96 7.98 30.43 (13) 31.51 21.40
baldridge 5.24 (15) 4.92 7.28 25.35 (15) 26.57 15.26
choi 3.33 (16) 3.50 2.58 24.77 (16) 25.71 17.37
trandabat 3.26 (17) 3.08 4.46 6.59 (18) 6.81 4.76
lluis 2.55 (18) 1.96 6.10 16.07 (17) 16.46 13.00
neumann 0.11 (19) 0.12 0.23 0.30 (19) 0.31 0.20
open
vickrey ? ? ? 44.94 (1) 46.68 30.28
riedel ? ? ? 42.77 (2) 44.18 31.15
zhang 8.14 (1) 8.04 8.92 35.46 (3) 36.74 24.84
li 6.90 (2) 6.46 9.62 29.91 (4) 31.30 18.41
wang 5.17 (3) 5.12 5.63 18.63 (5) 20.31 7.09
Table 13: Exact Match and Perfect Proposition F
1
scores for runs submitted in the closed and open
challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on
the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect
Proposition F
1
score. The number in parentheses next to the WSJ+Brown scores indicates the system
rank according to the corresponding scoring measure.
6.1 Exact Match and Perfect Propositions
Table 13 lists the Exact Match and Perfect Propo-
sition F
1
scores for test runs submitted in both
challenges. Both these scores measure the capac-
ity of a system to correctly parse structures with
granularity much larger than a simple dependency,
i.e., entire sentences for Exact Match and complete
propositions for Perfect Proposition F
1
(see Sec-
tion 2.2.2 for a formal definition of these evalua-
tion measures). The table indicates that these val-
ues are much smaller than the scores previously
reported, e.g., labeled macro F
1
. This is to be
expected: the probability of an incorrectly parsed
unit (sentence or proposition) is much larger given
its granularity. However, the main purpose of this
analysis is to investigate if systems that focused
on joint learning or optimization performed bet-
ter than others with respect to these global mea-
sures. This indeed seems to be the case for at
least two systems. The system of Johansson and
Nugues (2008), which jointly optimizes the la-
beled F
1
score (for semantic dependencies) and
then the labeled macro F
1
score (for the complete
task), increases its distance from the next ranked
system: its Perfect Proposition F
1
score is over
6 points higher than the score of the second sys-
tem in Table 13. The system of Henderson et
al. (2008), which was designed for joint learning
of the complete task, improves its rank from eighth
to fifth compared to the official results (Table 10).
6.2 Nonprojectivity
Table 14 shows the unlabeled F1 scores for pre-
diction of nonprojective syntactic dependencies.
Since nonprojectivity is quite rare, many teams
chose to ignore this issue. The table shows only
those systems that submitted well-formed depen-
dency trees, and whose output contained at least
one nonprojective link. The small number of non-
projective links in the training set makes it hard to
learn to predict such links, and this is also reflected
in the figures. In general, the figures for nonpro-
jective wh-movements and split clauses are higher,
and they are also the most common types. Also,
they are detectable by fairly simple patterns, such
as the presence of a wh-word or a pair of commas.
173
System All wh-mov. SpCl SpNP
choi 25.43 49.49 45.47 8.72
lee 46.26 50.30 64.84 20.69
nugues 46.15 58.96 59.26 11.32
samuelsson 24.47 38.15 0 9.83
titov 42.32 50.56 48.71 0
zhang 13.39 5.71 12.33 7.3
Table 14: Unlabeled F1-measures for nonprojec-
tive links. Results are given for all links, wh-
movements, split clauses, and split noun phrases.
6.3 Normalized SRL Performance
Table 6.3 lists the scores for the semantic sub-
task measured as the ratio of the labeled F
1
score
and LAS. As previously mentioned, this score es-
timates the performance of the SRL component
independent of the performance of the syntactic
parser. This analysis is not a substitute for the
actual experiment where the SRL components are
evaluated using correct syntactic information but,
nevertheless, it indicates several interesting facts.
First, the ranking of the top three systems in Ta-
ble 10 changes: the system of Che et al (2008)
is now ranked first, and the system of Johansson
and Nugues (2008) is second. This shows that Che
et al have a relatively stronger SRL component,
whereas Johansson and Nugues developed a bet-
ter parser. Second, several other systems improved
their ranking compared to Table 10: e.g., chen
from position thirteenth to ninth and choi from six-
teenth to eighth. This indicates that these systems
were penalized in the official ranking mainly due
to the relative poor performance of their parsers.
Note that this experiment is relevant only for
systems that implemented pipeline architectures,
where the semantic components are in fact sep-
arated from the syntactic ones; this excludes the
systems that blended syntax with SRL: henderson,
sun, and lluis. Furthermore, systems that had sig-
nificantly lower scores in syntax will receive an un-
reasonable boost in ranking according to this mea-
sure. Fortunately, there was only one such outlier
in this evaluation (neumann), shown in gray in the
table.
6.4 PropBank versus NomBank
Table 16 lists the labeled F
1
scores for semantic
dependencies for two different views of the test-
ing data sets: for propositions centered around ver-
bal predicates, i.e., from PropBank, and for propo-
sitions centered around nominal predicates, i.e.,
from NomBank.
Labeled F
1
/ LAS
closed WSJ+Brown WSJ Brown
neumann 137.60 (1) 140.94 108.93
che 90.51 (2) 91.42 82.21
johansson 89.98 (3) 90.70 83.40
ciaramita 89.49 (4) 90.32 81.89
zhao 87.88 (5) 88.75 79.78
yuret 84.35 (6) 85.30 75.34
samuelsson 84.20 (7) 85.24 74.51
choi 83.52 (8) 83.63 82.64
chen 82.22 (9) 82.89 76.11
morante 81.92 (10) 82.73 74.43
zhang 81.67 (11) 82.45 74.46
henderson 81.66 (12) 82.32 75.47
watanabe 81.26 (13) 82.18 72.61
lee 81.01 (14) 81.63 75.33
li 80.69 (15) 81.53 73.23
baldridge 78.37 (16) 79.33 69.38
sun 77.68 (17) 78.29 72.15
lluis 75.77 (18) 76.20 72.24
trandabat 47.68 (19) 48.12 43.85
open
zhang 82.33 82.91 76.87
li 79.58 80.44 72.05
wang 79.38 82.35 55.37
Table 15: Ratio of the labeled F
1
score for seman-
tic dependencies and LAS for syntactic dependen-
cies. Systems are sorted in descending order of this
ratio score on the WSJ+Brown corpus. We only
show systems that participated in both the syntac-
tic and semantic subtasks.
The table indicates that, generally, systems per-
formed much worse on nominal predicates than
on verbal predicates. This is to be expected con-
sidering that there is significant body of previ-
ous work that analyzes the SRL problem on Prop-
Bank, but minimal work for NomBank. On aver-
age, the difference between the labeled F
1
scores
for verbal predicates and nominal predicates on the
WSJ+Brown corpus is 7.84 points. Furthermore,
the average difference between labeled F
1
scores
on the Brown corpus alone is 12.36 points. This in-
dicates that the problem of SRL for nominal predi-
cates is more sensitive to domain changes than the
equivalent problem for verbal predicates. Our con-
jecture is that, because there is very little syntac-
tic structure between nominal predicates and their
arguments, SRL models for nominal predicates se-
lect mainly lexical features, which are more brittle
than syntactic or other non-lexicalized features.
Remarkably, there is one system (baldridge)
which performed better on the WSJ+Brown for
nominal predicates than verbal predicates. Un-
fortunately, this group did not submit a system-
description paper so it is not clear what was their
approach.
174
Labeled F
1
Labeled F
1
(verbal predicates) (nominal predicates)
closed WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown
johansson 84.45 (1) 86.37 71.87 74.32 (2) 75.42 60.13
che 80.46 (2) 82.17 69.33 75.18 (1) 76.64 56.87
ciaramita 80.15 (3) 82.09 67.62 73.17 (4) 74.42 57.69
zhao 77.67 (4) 79.40 66.38 73.28 (3) 74.69 54.81
samuelsson 76.17 (5) 78.03 64.00 68.13 (7) 69.58 49.24
yuret 75.91 (6) 77.88 63.02 68.81 (5) 69.98 53.58
zhang 74.82 (7) 76.62 63.15 65.61 (11) 66.82 50.18
li 74.36 (8) 76.14 62.92 62.61 (14) 63.76 47.09
henderson 73.80 (9) 75.40 63.36 66.26 (10) 67.44 50.73
watanabe 73.06 (10) 75.02 60.34 67.15 (8) 68.37 50.92
sun 72.97 (11) 74.45 63.50 58.68 (15) 59.73 45.75
morante 72.81 (12) 74.36 62.72 66.50 (9) 67.92 47.97
lee 72.34 (13) 74.15 60.49 62.83 (13) 63.66 52.18
chen 72.02 (14) 73.49 62.46 65.02 (12) 66.14 50.48
choi 70.00 (15) 71.28 61.71 56.16 (16) 57.19 44.05
baldridge 67.02 (16) 68.64 56.50 68.57 (6) 69.78 52.96
lluis 62.42 (17) 63.49 55.49 42.15 (17) 42.81 34.22
trandabat 42.88 (18) 43.79 37.06 37.14 (18) 37.89 27.50
neumann 22.87 (19) 23.53 18.24 21.7 (19) 22.04 17.14
open
vickrey 78.41 (1) 79.75 69.57 71.86 (1) 73.29 53.25
riedel 77.13 (2) 78.72 66.75 70.25 (2) 71.03 60.17
zhang 75.00 (3) 76.62 64.44 66.76 (3) 67.79 53.76
li 73.74 (4) 75.57 62.05 61.24 (5) 62.38 46.36
wang 67.50 (5) 70.34 49.72 66.53 (4) 69.83 28.96
Table 16: Labeled F
1
scores for frames centered around verbal and nominal predicates. The number in
parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.
Systems can mitigate the inherent differences
between verbal and nominal predicates with dif-
ferent models for the two sub-problems. This was
indeed the approach taken by two out of the top
three systems (johansson and che). Johansson and
Nugues (2008) developed different models for ver-
bal and nominal predicates and implemented sep-
arate feature selection processes for each model.
Che et al (2008) followed the same method but
they also implemented separate domain constraints
for inference for the two models.
7 Conclusion
The previous four CoNLL shared tasks popular-
ized and, without a doubt, boosted research in se-
mantic role labeling and dependency parsing. This
year?s shared task introduces a new task that es-
sentially unifies the problems addressed in the past
four years under a unique, dependency-based for-
malism. This novel task is attractive both from
a research perspective and an application-oriented
perspective:
? We believe that the proposed dependency-
based representation is a better fit for many
applications (e.g., Information Retrieval, In-
formation Extraction) where it is often suffi-
cient to identify the dependency between the
predicate and the head of the argument con-
stituent rather than extracting the complete ar-
gument constituent.
? It was shown that the extraction of syntac-
tic and semantic dependencies can be per-
formed with state-of-the-art performance in
linear time (Ciaramita et al, 2008). This can
give a significant boost to the adoption of this
technology in real-world applications.
? We hope that this shared task will motivate
several important research directions. For ex-
ample, is the dependency-based representa-
tion better for SRL than the constituent-based
formalism? Does joint learning improve syn-
tactic and semantic analysis?
? Surface (string related patterns, syntax, etc.)
linguistic features can often be detected with
greater reliability than deep (semantic) fea-
tures. In contrast, deep features can cover
more ground because they regularize across
differences in surface strings. Machine learn-
ing systems can be more effective by using
evidence from both deep and surface features
jointly (Zhao, 2005).
175
Even though this shared task was more complex
than the previous shared tasks, 22 different teams
submitted results in at least one of the challenges.
Building on this success, we hope to expand this
effort in the future with evaluations on multiple
languages and on larger out-of-domain corpora.
Acknowledgments
We want to thank the following people who helped
us with the generation of the data sets: Jes?us
Gim?enez, for generating the predicted POS tags
with his SVMTool POS tagger, and Massimiliano
Ciaramita, for generating columns 1, 2 and 3 in the
open-challenge corpus with his semantic tagger.
We also thank the following people who helped
us with the organization of the shared task: Paola
Merlo and James Henderson for the idea and the
implementation of the Exact Match measure, Se-
bastian Riedel for his dependency visualization
software,
12
Hai Zhao, for the the idea of the F
1
ratio score, and Carlos Castillo, for help with the
shared task website. Last but not least, we thank
the organizers of the previous four shared tasks:
Sabine Buchholz, Xavier Carreras, Ryan McDon-
ald, Amit Dubey, Johan Hall, Yuval Krymolowski,
Sandra K?ubler, Erwin Marsi, Jens Nilsson, Sebas-
tian Riedel, and Deniz Yuret. This shared task
would not have been possible without their previ-
ous effort.
Mihai Surdeanu is a research fellow in the
Ram?on y Cajal program of the Spanish Ministry of
Science and Technology. Richard Johansson was
funded by the Swedish National Graduate School
of Language Technology (GSLT). Adam Meyers?
participation was supported by the National Sci-
ence Foundation, award CNS-0551615 (Towards
a Comprehensive Linguistic Annotation of Lan-
guage) and IIS-0534700 (Collaborative Research:
Structure Alignment-based Machine Translation).
Llu??s M`arquez?s participation was supported by
the Spanish Ministry of Education and Science,
through research projects Trangram (TIN2004-
07925-C03-02) and OpenMT (TIN2006-15307-
C03-02).
References
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of CoNLL-
2007 Shared Task.
12
http://code.google.com/p/whatswrong/
X. Carreras and L. M`arquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005.
X. Carreras and L. M`arquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2004.
W. Che, Z. Li, Y. Hu, Y. Li, B. Qin, T. Liu and S.
Li. 2008. A Cascaded Syntactic and Semantic De-
pendency Parsing System. In Proc. of CoNLL-2008
Shared Task.
E. Chen, L. Shi and D. Hu. 2008. Probabilistic Model
for Syntactic and Semantic Dependency Parsing. In
Proc. of CoNLL-2008 Shared Task.
Chinchor, N. and P. Robinson. 1998. MUC-7
Named Entity Task Definition. In Proc. of Seventh
Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/
/muc/proceedings/muc 7 toc.html.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. In Science Sinica,
14:1396-1400.
M. Ciaramita, G. Attardi, F. Dell?Orletta, and M. Sur-
deanu. 2008. DeSRL: A Linear-Time Semantic
Role Labeling System. In Proc. of CoNLL-2008
Shared Task.
M. Ciaramita and Y. Altun. 2006. Broad Coverage
Sense Disambiguation and Information Extraction
with a Supersense Sequence Tagger. In Proc. of
EMNLP.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Edmonds. 1967. Optimum Branchings. In Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. New Developments in
Parsing Algorithms, Kluwer Academic Publishers.
W. N. Francis and H. Ku?cera. 1964. Brown Corpus.
Manual of Information to accompany A Standard
Corpus of Present-Day Edited American English, for
use with Digital Computers. Revised 1971, Revised
and Amplified 1979.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
J. Gim?enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proc. of LREC.
K. Hacioglu. 2004. Semantic Role Labeling Using De-
pendency Trees. In Proc. of COLING-2004.
176
J. Henderson, P. Merlo, G. Musillo and I. Titov. 2008.
A Latent Variable Model of Synchronous Parsing for
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
R. Johansson and P. Nugues. 2008. Dependency-
based Syntactic?Semantic Analysis with PropBank
and NomBank. In Proc. of CoNLL-2008 Shared
Task.
R. Johansson and P. Nugues. 2007. Extended
Constituent-to-Dependency Conversion for English.
In Proc. of NODALIDA.
X. Llu??s and L. M`arquez. 2008. A Joint Model for
Parsing Syntactic and Semantic Dependencies. In
Proc. of CoNLL-2008 Shared Task.
D. Magerman. 1994. Natural Language Parsing as Sta-
tistical Pattern Recognition. Ph.D. thesis, Stanford
University.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic.
2005. Non-Projective Dependency Parsing using
Spanning Tree Algorithms In Proc. of HLT-EMNLP.
A. Meyers, R. Grishman, M. Kosaka, and S. Zhao.
2001. Covering Treebanks with GLARF. In Proc.
of the ACL/EACL 2001 Workshop on Sharing Tools
and Resources for Research and Education.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank Project: An Interim Report. In
NAACL/HLT 2004 Workshop Frontiers in Corpus
Annotation, Boston.
J. Nivre, J. Hall, J. Nilsson and G. Eryigit. 2006. La-
beled Pseudo-Projective Dependency Parsing with
Support Vector Machines. In Proc. of CoNLL-X
Shared Task.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, D. Yuret. 2007. The CoNLL 2007 Shared
Task on Dependency Parsing. In Proc. of CoNLL-
2007.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryi?git, S.
K?ubler, S. Marinov, and E. Marsi. 2007b. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Seman-
tic Roles. Computational Linguistics, 31(1).
S. Riedel and I. Meza-Ruiz. 2008. Collective Seman-
tic Role Labelling with Markov Logic. In Proc. of
CoNLL-2008 Shared Task.
Y. Samuelsson, O. T?ackstr?om, S. Velupillai, J. Eklund,
M. Fishel and M. Saers. 2008. Mixing and Blending
Syntactic and Semantic Dependencies. In Proc. of
CoNLL-2008 Shared Task.
W. Sun, H. Li and Z. Sui. 2008. The Integration of De-
pendency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov
Models. In Proc. of CoNLL-2008 Shared Task.
E. F. Tjong Kim San and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003.
D. Vickrey and D. Koller. 2008. Applying Sentence
Simplification to the CoNLL-2008 Shared Task. In
Proc. of CoNLL-2008 Shared Task.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Linguistic Data Consortium.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
Y. Zhang, R. Wang and H. Uszkoreit. 2008. Hybrid
Learning of Dependency Structures from Heteroge-
neous Linguistic Resources. In Proc. of CoNLL-
2008 Shared Task.
Zhao, S. 2005. Information Extraction from Multiple
Syntactic Sources. Ph.D. thesis, NYU.
177
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 188?192
Manchester, August 2008
A Joint Model for Parsing Syntactic and Semantic Dependencies
Xavier Llu??s and Llu??s M
`
arquez
TALP Research Centre ? Software Department (LSI)
Technical University of Catalonia (UPC)
{xlluis,lluism}@lsi.upc.edu
Abstract
This paper describes a system that jointly
parses syntactic and semantic dependen-
cies, presented at the CoNLL-2008 shared
task (Surdeanu et al, 2008). It combines
online Peceptron learning (Collins, 2002)
with a parsing model based on the Eisner
algorithm (Eisner, 1996), extended so as
to jointly assign syntactic and semantic la-
bels. Overall results are 78.11 global F
1
,
85.84 LAS, 70.35 semantic F
1
. Official re-
sults for the shared task (63.29 global F
1
;
71.95 LAS; 54.52 semantic F
1
) were sig-
nificantly lower due to bugs present at sub-
mission time.
1 Introduction
The main goal of this work was to construct a joint
learning architecture for syntactic-semantic pars-
ing and to test whether the syntactic and semantic
layers can benefit each other from the global train-
ing and inference.
All the components of our system were built
from scratch for this shared task. Due to strong
time limitations, our design decisions were biased
towards constructing a simple and feasible system.
Our proposal is a first order linear model that re-
lies on an online averaged Perceptron for learning
(Collins, 2002) and an extended Eisner algorithm
for the joint parsing inference.
Systems based on Eisner algorithm (Carreras et
al., 2006; Carreras, 2007) showed a competitive
performance in the syntactic parsing of the English
language in some past CoNLL shared tasks. Also,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
we believe that extending the Eisner algorithm to
jointly parse syntactic and semantic dependencies
it is a natural step to follow.
Note that syntactic and semantic tasks are re-
lated but not identical. Semantic dependencies can
take place between words loosely related by the
syntactic structure. Another difficulty is that state
of the art SRL systems (Surdeanu et al, 2007)
strongly rely on features extracted from the syn-
tactic tree. The joint model grows syntactic and
semantic structures at the same time, so features
extracted from the syntactic tree (e.g., a syntactic
path between a modifier and a distant predicate)
are not available or expensive to compute within
the Eisner algorithm. We overcome this problem
again with a very simple (though not elegant) solu-
tion, consisting of introducing a previous syntactic
parsing step.
2 System architecture
This section briefly describes the main components
of our system: 1) Preprocessing, 2) Syntactic pars-
ing, 3) Predicate identification, 4) Joint syntactic-
semantic parsing, and 5) Postprocessing.
In preprocessing, the training corpus is traversed
and feature extraction performed. Main features
are borrowed from pre-existing well-known sys-
tems (see next subsection). The initial syntactic
parsing is based on an Eisner parser trained with
Perceptron and it is merely intended to allow the
extraction of syntactic-based features for all the
following phases (which share exactly the same
feature set extracted from these parse trees). Pred-
icate identification recognizes predicates by apply-
ing SVM classifiers
1
and a set of simple heuristic
rules. The joint syntactic-semantic parsing phase
1
We used SVM-light (see www.joachims.org for details).
188
is the core module of this work. It simultaneously
derives the syntactic and semantic dependencies
by using a first order Eisner model, extended with
semantic labels and trained with averaged Percep-
tron. Finally, postprocessing simply selects the
most frequent sense for each predicate.
2.1 Preprocessing and feature extraction
All features in our system are calculated in the pre-
processing phase. We use the features described
in McDonald et al (2005) and Carreras et al
(2006) as input for the syntactic parsing phase, ex-
cept for the dynamic features from Carreras et al
(2006). The joint syntactic-semantic parser uses
all the previous features and also specific features
for semantic parsing from Xue and Palmer (2004)
and Surdeanu et al (2007). The features have been
straightforwardly adapted to the dependency struc-
ture used in this shared task, by substituting any
reference to a syntactic constituent by the head of
that constituent. About 5M features were extracted
from the training corpus. The number of features
was reduced to ?222K using a frequency thresh-
old filter. A detailed description of the feature set
can be found at Llu??s (Forthcoming 2008).
2.2 Syntactic parsing
Our system uses the Eisner algorithm combined
with an online averaged Pereceptron. We define
the basic model, which is also the starting point
for the joint model. Let L be the set of syntactic
labels, x = x
1
, . . . , x
n
a sentence with n words,
and Y(x) the set of all possible projective depen-
dency trees for x.
A dependency tree y ? Y(x) is a labeled tree
with arcs of the form ?h,m, l? that is rooted on
an artificial node, 0, added for this purpose. The
head, h, and modifier, m, for a dependency index
words in the sentence and can take values in 0 ?
h ? n and 1 ? m ? n. l ? L is the label of the
dependency.
The dependency parser (dp) is interested in find-
ing the best scored tree for a given sentence x:
dp(x,w) = argmax
y?Y(x)
score tree(y, x,w)
Using an arc-based first order factorization, the
function score tree(y, x,w) is defined as the sum-
mation of scores of the dependencies in y:
?
?h,m,l??y
score(?h,m, l? , x,w) ,
where w is the weight vector of the parser, com-
puted using an online perceptron. The weight vec-
tor w can be seen as a concatenation of |L| weight
vectors of d components, one for each of the la-
bels: w = (w
(1)
, . . . ,w
(l)
, . . . ,w
(|L|)
). A func-
tion ? is assumed to extract features from a de-
pendency ?h,m, l? and from the whole sentence x.
This function represents the extracted features as a
d-dimensional vector.
With all these elements, the score of a depen-
dency ?h,m, l? is computed as a linear function:
score(?h,m, l? , x,w) = ? (?h,m, l? , x) ?w
(l)
2.3 Predicate identification
We identified as verb predicates all verbs exclud-
ing the auxiliaries and the verb to be. These simple
rules based on the POS and lemma of the tokens
are enough to correctly identify almost all verb
predicates. With regard to noun predicates, we di-
rectly identified as predicates the lemmas which
appeared always as predicates with a minimum fre-
quency in the training corpus. The remaining noun
predicates were identified by a degree-2 polyno-
mial SVM. This classifier was trained with the
same features used in subsequent phases, but ex-
cluding those requiring identified predicates.
2.4 Joint syntactic and semantic Parsing
The previously described basic parsing model will
be extended to jointly assign semantic dependency
labels. Let S be the set of semantic labels. Note
that at this point, a sentence x has a set of q words
already identified as predicates. We will refer to
them as p
1
, . . . , p
q
, where p
i
? {1, . . . , n}. We
consider that each dependency has a set of se-
mantic tags l
sem p
1
, . . . , l
sem p
q
one for each sen-
tence predicate p
i
. Also, we consider an extra
no-argument label in the set of semantic labels S.
Thus, an extended dependency d
s
is defined as:
d
s
=
?
h,m, l
syn
, l
sem p
1
, . . . , l
sem p
q
?
,
where l
syn
denotes the syntactic label for the de-
pendency.
Again, the best parse tree is that maximizing the
score of a first order factorization:
dp(x,w, y
?
) = argmax
y?Y(x)
score tree(y, x,w, y
?
)
score tree(y, x,w, y
?
) =
=
?
?h,m,l??y
score(?h,m, l? , x,w, y
?
) ,
189
where the dependency label is now extended to
l = ?l
syn
, l
sem p
1
, . . . , l
sem p
q
? and y
?
denotes the
precomputed syntax tree. The score of a syntactic-
semantic dependency is:
score
(
?h,m, l? , x,w, y
?
)
=
syntactic score (h,m, l
syn
, x,w)+
sem score
(
h,m, l
sem p
1
, . . . , l
sem p
q
, x,w, y
?
)
The syntactic score is computed as described in the
basic model. Finally, the semantic scoring func-
tion computes the semantic score as the sum of the
semantic scores for each predicate semantic label:
sem score
(
h,m, l
sem p
1
, . . . , l
sem p
q
, x,w, y
?
)
=
?
l
sem p
i
?
sem
(?h,m, l
sem p
i
? , x, y
?
) ?w
(l
sem p
i
)
q
Note that each sentence x has a different number
of predicates q. To avoid an excessive weight of
the semantic component in the global score and a
bias towards sentences with many predicates, the
score is normalized by the number of predicates in
the sentence.
Figure 1 shows an example of a sen-
tence fragment with syntactic and seman-
tic dependencies. The three predicates
of the sentence are already identified:
{p
1
= completed, p
2
= announced, p
3
=
acquisition}. All dependencies are of the
form d = ?h,m, l
syn
, l
sem p
1
, l
sem p
2
, l
sem p
3
?.
Note that semantic labels express semantic
relations between a modifier and a predicate
that can be anywhere in the sentence. The
semantic labeling is not restricted to predicates
that are the head of the modifier. In this ex-
ample, the correct output for the dependency
previously-announced is h = announced,
m = previously, l
syn
= AMOD, l
sem p
1
= null,
l
sem p
2
= AM-TMP, l
sem p
3
= null.
The above described factorization allows the
parser to simultaneously assign syntactic and
semantic labels and also to maximize a joint
syntactic-semantic score of the tree. Note that the
semantic scoring function ?
sem
extracts features
from the modifier, the head and the predicate of the
parsed dependencies. The proposed model allows
to capture interactions between syntax and seman-
tics not only because the syntactic and semantic
scores are combined but also because the semantic
scoring function relies on features extracted from
the head-modifier-predicate relations. Thus, the
semantic scoring function depends on the syntactic
dependency being built, and, in reverse, the seman-
tic score can modify the dependency chosen.
Regarding implementation issues, note that we
compute |L|+ |S| ? q scores to assign q + 1 labels
to a given dependency. The scores are computed
independently for each label. Otherwise, interac-
tions among these labels, would raise the num-
ber of possible combined labels to an exponential
number, |L| ? |S|
q
, making the exhaustive evalu-
ation infeasible in practice. Also related to effi-
ciency, we apply syntactic and semantic filters in
order to reduce the number of score evaluations.
In particular, the set of assignable labels is filtered
by the POS of the head and modifier (discarding
all labels not previously seen in the training corpus
between words with the same POS). Another fil-
ter removes the core arguments not present in the
frame file of each predicate. This strategy allowed
us to significantly improve efficiency without any
loss in accuracy.
2.5 Postprocess
A simple postprocess assigns the most frequent
sense to each identified predicate. Frequencies
were computed from the training corpus. Ex-
periments performed combining the best and sec-
ond output of the joint parser and enforcing do-
main constraints via ILP (Punyakanok et al, 2004)
showed no significant improvements.
3 Experiments and Results
All the experiments reported here were done using
the full training corpus, and results are presented
on the development set. The number of features
used by the syntactic parser is ?177K. The joint
parser uses?45K additional features for recogniz-
ing semantic dependencies.
Figure 2 shows the learning curves from epoch
1 to 17 for several subsystems and variants. More
specifically, it includes LAS performance on syn-
tactic parsing, both for the individual parser and
for the syntactic annotation coming from the joint
syntactic-semantic parser. For the latter, also the
F
1
score on semantic dependencies and global
F
1
results are presented. We can observe that
the syntactic LAS scores for the syntactic and
joint parsers are very similar, showing that there
is no loss in syntactic performance when using
the joint syntactic-semantic strategy. Overall re-
190
SBJ, A0, _, A0
OBJ, _, _, Su
NMOD, _, _, _ AMOD, _, AM-TMP, _ NMOD, _, _, _
OBJ, A1, A1, _
Figure 1: Syntactic and semantic dependencies.
sults are quite stable from epoch 4 (syntax slightly
decreases but semantics slightly increases). The
overall results on the test set (78.11 global F
1
,
85.84 LAS, 70.35 semantic F
1
) were computed by
using 5 epochs of training, the optimal on the de-
velopment set.
 
68
 
70
 
72
 
74
 
76
 
78
 
80
 
82
 
84
 
86
 
2
 
4
 
6
 
8
 
10
 
12
 
14
 
16
LAS s
ingle s
yn
LAS s
yn joint F1 sem
 joint
F1 glo
bal joint
Figure 2: Learning curves for the syntactic-only
and joint parsers.
The global F
1
result on the WSJ test corpus is
79.16, but these results drop 9.32 F
1
points on
the out-of-domain Brown corpus. Also, a signif-
icant performance drop is observed when mov-
ing from verb argument classification (74.58 F
1
,
WSJ test) to noun argument classification (56.65
F
1
, WSJ test). Note that the same features were
used for training noun and verb argument classi-
fiers. These results point out that there is room for
improvement on noun argument classification. Fi-
nally, a comparison to a simple equivalent pipeline
architecture, consisting of applying the syntactic
base parser followed by an independent classifica-
tion of semantic dependencies (using exactly the
same features) revealed that the joint model out-
performed the pipeline by 4.9 F
1
points in the an-
notation of semantic dependencies.
Regarding efficiency, the proposed architecture
is really feasible. About 0.7GB of memory is re-
quired for the syntactic parser and 1.5GB for the
joint parser. Most of these memory needs are due
to the filters used. The filters allowed for a reduc-
tion of the computational cost by a factor of 5 with
no loss in accuracy. These filters have almost no
effect on the theoretical upper bound discarding
the correct labels for only 0.2% of the syntactic de-
pendencies and 0.44% of the semantic arguments
in the development corpus. The semantic exten-
sion of the Eisner algorithm requires only a new
table with backpointers for each predicate. Using a
single processor of an amd64 Athlon x2 5000+, the
syntactic parser can be trained at 0.2 s/sentence,
and the joint parser at 0.3 s/sentence. Efficiency at
test times is only slightly better.
4 Discussion
We have presented a novel joint approach to per-
form syntactic and semantic parsing by extend-
ing Eisner?s algorithm. Our model allows to cap-
ture syntactic-semantic interactions as the com-
puted syntactic-semantic score is globally opti-
mized. The computational cost of the new setting
is admissible in practice, leading to fairly efficient
parsers, both in time and memory requirements.
Results obtained with the presented joint ap-
proach are promising, though not outstanding in
the context of the CoNLL-2008 shared task. We
believe that there is room for substantial improve-
ment since many of the current system components
191
are fairly simple. For instance, higher order ex-
tensions to the Eisner algorithm and well-known
tricks for dealing with non-projective structures
can be incorporated in our model. Also, we plan
to incorporate other subtasks in the training of the
joint model, such as predicate identification and ar-
gument recognition.
One of the potential drawbacks of our current
approach is the need for a syntactic parsing pre-
ceding the joint model. This previous parse is
simply included to permit the extraction of syntax
based features. These features (including the syn-
tactic path) could be dynamically computed when
performing the joint parsing in the cases in which
the predicate coincides with the head of the modi-
fier being processed. These cases account for only
63.6% of the training corpus arguments. If a pred-
icate is located in a sibling sentence span, the dy-
namic programming algorithm has not yet chosen
which of the possible spans will be included in
the final parse tree. Also, the predicate can be
located at a lower level within the current span.
These cases would require to recompute the score
of the current span because syntactic path features
are not available. The resulting cost would be pro-
hibitive and approximate search needed. Our pre-
vious parsing phase is just an efficient and simple
solution to the feature extraction problem in the
joint model.
As previously seen, the joint model showed a
similar syntactic performance and clearly better
semantic performance than an equivalent pipeline
system, showing that some degree of syntactic-
semantic overlap is exploitable. Regarding the for-
mer, there is only a moderate degree (63.6%) of
direct overlap between the syntactic head-modifier
and semantic predicate-modifier relations. If the
semantic score is highly dependent on a correct
head the resulting increased score could benefit the
choosing of a correct dependency. Otherwise, joint
scores can introduce a significant amount of noise.
All in all, further research is required in this direc-
tion.
Acknowledgements
This research has been partially funded by the
Spanish Ministry of Education and Science,
projects Trangram (TIN2004-07925-C03-02) and
OpenMT (TIN2006-15307-C03-02).
References
Carreras, Xavier, Mihai Surdeanu, and Llu??s M`arquez.
2006. Projective dependency parsing with percep-
tron. In Proceedings of the 10th Conference on Com-
putational Natural Language Learning (CoNLL-
2006).
Carreras, Xavier. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of the 11th Conference on Computational Natural
Language Learning (CoNLL-2007).
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing.
Eisner, Jason M. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING-96).
Llu??s, Xavier. Forthcoming 2008. Joint learning of
syntactic and semantic dependencies. Master?s the-
sis, Technical University of Catalonia (UPC).
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL-2005).
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of Col-
ing 2004.
Surdeanu, Mihai, Llu??s M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Xue, Nianwen and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004).
192
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 333?338,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Document-level Automatic MT Evaluation
based on Discourse Representations
Jesu?s Gime?nez and
Llu??s Ma`rquez
TALP UPC
Barcelona, Spain
{jgimenez, lluism}
@lsi.upc.edu
Elisabet Comelles and
Irene Castello?n
Universitat de Barcelona
Barcelona, Spain
{elicomelles,
icastellon} @ub.edu
Victoria Arranz
ELDA/ELRA
Paris, France
arranz@elda.org
Abstract
This paper describes the joint submission of
Universitat Polite`cnica de Catalunya and Uni-
versitat de Barcelona to the Metrics MaTr
2010 evaluation challenge, in collaboration with
ELDA/ELRA. Our work is aimed at widening the
scope of current automatic evaluation measures
from sentence to document level. Preliminary ex-
periments, based on an extension of the metrics by
Gime?nez and Ma`rquez (2009) operating over dis-
course representations, are presented.
1 Introduction
Current automatic similarity measures for Ma-
chine Translation (MT) evaluation operate all,
without exception, at the segment level. Trans-
lations are analyzed on a segment-by-segment1
fashion, ignoring the text structure. Document
and system scores are obtained using aggregate
statistics over individual segments. This strategy
presents the main disadvantage of ignoring cross-
sentential/discursive phenomena.
In this work we suggest widening the scope
of evaluation methods. We have defined genuine
document-level measures which are able to ex-
ploit the structure of text to provide more informed
evaluation scores. For that purpose we take advan-
tage of two coincidental facts. First, test beds em-
ployed in recent MT evaluation campaigns include
a document structure grouping sentences related
to the same event, story or topic (Przybocki et al,
2008; Przybocki et al, 2009; Callison-Burch et al,
2009). Second, we count on automatic linguistic
processors which provide very detailed discourse-
level representations of text (Curran et al, 2007).
Discourse representations allow us to focus on
relevant pieces of information, such as the agent
1A segment typically consists of one or two sentences.
(who), location (where), time (when), and theme
(what), which may be spread all over the text.
Counting on a means of discerning the events, the
individuals taking part in each of them, and their
role, is crucial to determine the semantic equiva-
lence between a reference document and a candi-
date translation.
Moreover, the discourse analysis of a document
is not a mere concatenation of the analyses of its
individual sentences. There are some phenom-
ena which may go beyond the scope of a sen-
tence and can only be explained within the con-
text of the whole document. For instance, in a
newspaper article, facts and entities are progres-
sively added to the discourse and then referred
to anaphorically later on. The following extract
from the development set illustrates the impor-
tance of such a phenomenon in the discourse anal-
ysis: ?Among the current or underlying crises in
the Middle East, Rod Larsen mentioned the Arab-
Israeli conflict and the Iranian nuclear portfolio,
as well as the crisis between Lebanon and Syria.
He stated: ?All this leads us back to crucial val-
ues and opinions, which render the situation prone
at any moment to getting out of control, more so
than it was in past days.??. The subject pronoun
?he? works as an anaphoric pronoun whose an-
tecedent is the proper noun ?Rod Larson?. The
anaphoric relation established between these two
elements can only be identified by analyzing the
text as a whole, thus considering the gender agree-
ment between the third person singular masculine
subject pronoun ?he? and the masculine proper
noun ?Rod Larson?. However, if the two sen-
tences were analyzed separately, the identification
of this anaphoric relation would not be feasible
due to the lack of connection between the two ele-
ments. Discourse representations allow us to trace
links across sentences between the different facts
and entities appearing in them. Therefore, provid-
ing an approach to the text more similar to that of
333
a human, which implies taking into account the
whole text structure instead of considering each
sentence separately.
The rest of the paper is organized as follows.
Section 2 describes our evaluation methods and
the linguistic theory upon which they are based.
Experimental results are reported and discussed in
Section 3. Section 4 presents the metric submitted
to the evaluation challenge. Future work is out-
lined in Section 5.
As an additional result, document-level metrics
generated in this study have been incorporated to
the IQMT package for automatic MT evaluation2.
2 Metric Description
This section provides a brief description of our ap-
proach. First, in Section 2.1, we describe the un-
derlying theory and give examples on its capabili-
ties. Then, in Section 2.2, we describe the associ-
ated similarity measures.
2.1 Discourse Representations
As previously mentioned in Section 1, a document
has some features which need to be analyzed con-
sidering it as a whole instead of dividing it up
into sentences. The anaphoric relation between
a subject pronoun and a proper noun has already
been exemplified. However, this is not the only
anaphoric relation which can be found inside a
text, there are some others which are worth men-
tioning:
? the connection between a possessive adjec-
tive and a proper noun or a subject pro-
noun, as exemplified in the sentences ?Maria
bought a new sweater. Her new sweater is
blue.?, where the possessive feminine adjec-
tive ?her? refers to the proper noun ?Maria?.
? the link between a demonstrative pronoun
and its referent, which is exemplified in the
sentences ?He developed a new theory on
grammar. However, this is not the only the-
ory he developed?. In the second sentence,
the demonstrative pronoun ?this? refers back
to the noun phrase ?new theory on grammar?
which occurs in the previous sentence.
? the relation between a main verb and an aux-
iliary verb in certain contexts, as illustrated in
the following pair of sentences ?Would you
2http://www.lsi.upc.edu/
?
nlp/IQMT
like more sugar? Yes, I would?. In this ex-
ample, the auxiliary verb ?would? used in
the short answer substitutes the verb phrase
?would like?.
In addition to anaphoric relations, other features
need to be highlighted, such as the use of discourse
markers which help to give cohesion to the text,
link parts of a discourse and show the relations es-
tablished between them. Below, some examples
are given:
? ?Moreover?, ?Furthermore?, ?In addition?
indicate that the upcoming sentence adds
more information.
? ?However?, ?Nonetheless?, ?Nevertheless?
show contrast with previous ideas.
? ?Therefore?, ?As a result?, ?Consequently?
show a cause and effect relation.
? ?For instance?, ?For example? clarify or il-
lustrate the previous idea.
It is worth noticing that anaphora, as well as dis-
course markers, are key features in the interface
between syntax, semantics and pragmatics. Thus,
when dealing with these phenomena at a text level
we are not just looking separately at the different
language levels, but we are trying to give a com-
plete representation of both the surface and the
deep structures of a text.
2.2 Definition of Similarity Measures
In this work, as a first proposal, instead of elabo-
rating on novel similarity measures, we have bor-
rowed and extended the Discourse Representation
(DR) metrics defined by Gime?nez and Ma`rquez
(2009). These metrics analyze similarities be-
tween automatic and reference translations by
comparing their respective discourse representa-
tions over individual sentences.
For the discursive analysis of texts, DR met-
rics rely on the C&C Tools (Curran et al, 2007),
specifically on the Boxer component (Bos, 2008).
This software is based on the Discourse Represen-
tation Theory (DRT) by Kamp and Reyle (1993).
DRT is a theoretical framework offering a rep-
resentation language for the examination of con-
textually dependent meaning in discourse. A dis-
course is represented in a discourse representation
structure (DRS), which is essentially a variation of
first-order predicate calculus ?its forms are pairs
334
of first-order formulae and the free variables that
occur in them.
DRSs are viewed as semantic trees, built
through the application of two types of DRS con-
ditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
For instance, the DRS representation for the
sentence ?Every man loves Mary.? is as follows:
?y named(y,mary, per) ? (?x man(x) ?
?z love(z) ? event(z) ? agent(z, x) ?
patient(z, y)). DR integrates three different
kinds of metrics:
DR-STM These metrics are similar to the Syntac-
tic Tree Matching metric defined by Liu and
Gildea (2005), in this case applied to DRSs
instead of constituent trees. All semantic sub-
paths in the candidate and reference trees are
retrieved. The fraction of matching subpaths
of a given length (l=4 in our experiments) is
computed.
DR-Or(?) Average lexical overlap between dis-
course representation structures of the same
type. Overlap is measured according to the
formulae and definitions by Gime?nez and
Ma`rquez (2007).
DR-Orp(?) Average morphosyntactic overlap,
i.e., between grammatical categories ?parts-
of-speech? associated to lexical items, be-
tween discourse representation structures of
the same type.
We have extended these metrics to operate at
document level. For that purpose, instead of run-
ning the C&C Tools in a sentence-by-sentence
fashion, we run them document by document.
This is as simple as introducing a ?<META>? tag
at the beginning of each document to denote doc-
ument boundaries3 .
3Details on the advanced use of Boxer are avail-
able at http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/BoxerComplex.
3 Experimental Work
In this section, we analyze the behavior of the new
DR metrics operating at document level with re-
spect to their sentence-level counterparts.
3.1 Settings
We have used the ?mt06? part of the development
set provided by the Metrics MaTr 2010 organiza-
tion, which corresponds to a subset of 25 docu-
ments from the NIST 2006 Open MT Evaluation
Campaign Arabic-to-English translation. The to-
tal number of segments is 249. The average num-
ber of segments per document is, thus, 9.96. The
number of segments per document varies between
2 and 30. For the purpose of automatic evaluation,
4 human reference translations and automatic out-
puts by 8 different MT systems are available. In
addition, we count on the results of a process of
manual evaluation. Each translation segment was
assessed by two judges. After independently and
completely assessing the entire set, the judges re-
viewed their individual assessments together and
settled on a single final score. Average system ad-
equacy is 5.38.
In our experiments, metrics are evaluated in
terms of their correlation with human assess-
ments. We have computed Pearson, Spearman
and Kendall correlation coefficients between met-
ric scores and adequacy assessments. Document-
level and system-level assessments have been ob-
tained by averaging over segment-level assess-
ments. We have computed correlation coefficients
and confidence intervals applying bootstrap re-
sampling at a 99% statistical significance (Efron
and Tibshirani, 1986; Koehn, 2004). Since the
cost of exhaustive resampling was prohibitive, we
have limited to 1,000 resamplings. Confidence in-
tervals, not shown in the tables, are in all cases
lower than 10?3.
3.2 Metric Performance
Table 1 shows correlation coefficients at the docu-
ment level for several DR metric representatives,
and their document-level counterparts (DRdoc).
For the sake of comparison, the performance of
the METEOR metric is also reported4.
Contrary to our expectations, DRdoc variants
obtain lower levels of correlation than their DR
4We have used METEOR version 1.0 with default param-
eters optimized by its developers over adequacy and fluency
assessments. The METEOR metric is publicly available at
http://www.cs.cmu.edu/
?
alavie/METEOR/
335
Metric Pearson? Spearman? Kendall?
METEOR 0.9182 0.8478 0.6728
DR-Or(?) 0.8567 0.8061 0.6193
DR-Orp(?) 0.8286 0.7790 0.5875
DR-STM 0.7880 0.7468 0.5554
DRdoc-Or(?) 0.7936 0.7784 0.5875
DRdoc-Orp(?) 0.7219 0.6737 0.4929
DRdoc-STM 0.7553 0.7421 0.5458
Table 1: Meta-evaluation results at document level
Metric Pearson? Spearman? Kendall?
METEOR 0.9669 0.9151 0.8533
DR-Or(?) 0.9100 0.6549 0.5764
DR-Orp(?) 0.9471 0.7918 0.7261
DR-STM 0.9295 0.7676 0.7165
DRdoc-Or(?) 0.9534 0.8434 0.7828
DRdoc-Orp(?) 0.9595 0.9101 0.8518
DRdoc-STM 0.9676 0.9655 0.9272
DR-Or(?)? 0.9836 0.9594 0.9296
DR-Orp(?)? 0.9959 1.0000 1.0000
DR-STM? 0.9933 0.9634 0.9307
Table 2: Meta-evaluation results at system level
counterparts. There are three different factors
which could provide a possible explanation for
this negative result. First, the C&C Tools, like any
other automatic linguistic processor are not per-
fect. Parsing errors could be causing the metric
to confer less informed scores. This is especially
relevant taking into account that candidate transla-
tions are not always well-formed. Secondly, we
argue that the way in which we have obtained
document-level quality assessments, as an average
of segment-level assessments, may be biasing the
correlation. Thirdly, perhaps the similarity mea-
sures employed are not able to take advantage of
the document-level features provided by the dis-
course analysis. In the following subsection we
show some error analysis we have conducted by
inspecting particular cases.
Table 2 shows correlation coefficients at system
level. In the case of DR and DRdoc metrics, sys-
tem scores are computed by simple average over
individual documents. Interestingly, in this case
DRdoc variants seem to obtain higher correlation
than their DR counterparts. The improvement is
especially substantial in terms of Spearman and
Kendall coefficients, which do not consider ab-
solute values but ranking positions. However, it
could be the case that it was just an average ef-
fect. While DR metrics compute system scores as
an average of segment scores, DRdoc metrics av-
erage directly document scores. In order to clarify
this result, we have modified DR metrics so as to
compute system scores as an average of document
scores (DR? variants, the last three rows in the ta-
ble). It can be observed that DR? variants out-
perform their DRdoc counterparts, thus confirming
our suspicion about the averaging effect.
3.3 Analysis
It is worth noting that DRdoc metrics are able to
detect and deal with several linguistic phenomena
related to both syntax and semantics at sentence
and document level. Below, several examples il-
lustrating the potential of this metric are presented.
Control structures. Control structures (either
subject or object control) are always a
difficult issue as they mix both syntactic and
semantic knowledge. In Example 1 a couple
of control structures must be identified
and DRdoc metrics deal correctly with the
argument structure of all the verbs involved.
Thus, in the first part of the sentence, a
subject control verb can be identified being
?the minister? the agent of both verb forms
?go? and ?say?. On the other hand, in the
336
quoted question, the verb ?invite? works as
an object control verb because its patient
?Chechen representatives? is also the agent
of the verb visit.
Example 1: The minister went on to say,
?What would Moscow say if we were to invite
Chechen representatives to visit Jerusalem??
Anaphora and pronoun resolution. Whenever
there is a pronoun whose antecedent is a
named entity (NE), the metric identifies
correctly its antecedent. This feature is
highly valuable because a relationship be-
tween syntax and semantics is established.
Moreover, when dealing with Semantic
Roles the roles of Agent or Patient are given
to the antecedents instead of the pronouns.
Thus, in Example 2 the antecedent of the
relative pronoun ?who? is the NE ?Putin?
and the patient of the verb ?classified? is
also the NE ?Putin? instead of the relative
pronoun ?who?.
Example 2: Putin, who was not classified
as his country Hamas as ?terrorist organiza-
tions?, recently said that the European Union
is ?a big mistake? if it decided to suspend fi-
nancial aid to the Palestinians.
Nevertheless, although Boxer was expected
to deal with long-distance anaphoric relations
beyond the sentence, after analyzing several
cases, results show that it did not succeed in
capturing this type of relations as shown in
Example 3. In this example, the antecedent
of the pronoun ?he? in the second sentence
is the NE ?Roberto Calderoli? which ap-
pears in the first sentence. DRdoc metrics
should be capable of showing this connec-
tion. However, although the proper noun
?Roberto Calderoli? is identified as a NE, it
does not share the same reference as the third
person singular pronoun ?he?.
Example 3: Roberto Calderoli does not in-
tend to apologize. The newspaper Corriere
Della Sera reported today, Saturday, that
he said ?I don?t feel responsible for those
deaths.?
4 Our Submission
Instead of participating with individual metrics,
we have combined them by averaging their scores
as described in (Gime?nez and Ma`rquez, 2008).
This strategy has proven as an effective means of
combining the scores conferred by different met-
rics (Callison-Burch et al, 2008; Callison-Burch
et al, 2009). Metrics submitted are:
DRdoc an arithmetic mean over a heuristically-
defined set of DRdoc metric variants, respec-
tively computing lexical overlap, morphosyn-
tactic overlap, and semantic tree match-
ing (M = {?DRdoc-Or(?)?, ?DRdoc-Orp(?)?, ?DRdoc-
STM4?}). Since DRdoc metrics do not operate
over individual segments, we have assigned
each segment the score of the document in
which it is contained.
DR a measure analog to DRdoc but using the de-
fault version of DR metrics operating at the
segment level (M = {?DR-Or(?)?, ?DR-Orp(?)?,
?DR-STM4?}).
ULCh an arithmetic mean over a heuristically-
defined set of metrics operating at differ-
ent linguistic levels, including lexical met-
rics, and measures of overlap between con-
stituent parses, dependency parses, seman-
tic roles, and discourse representations (M =
{?ROUGEW ?, ?METEOR?, ?DP-HWCr?, ?DP-Oc(?)?,
?DP-Ol(?)?, ?DP-Or(?)?, ?CP-STM4?, ?SR-Or(?)?,
?SR-Orv?, ?DR-Orp(?)?}). This metric corre-
sponds exactly to the metric submitted in our
previous participation.
The performance of these metrics at the docu-
ment and system levels is shown in Table 3.
5 Conclusions and Future Work
We have presented a modified version of the DR
metrics by Gime?nez and Ma`rquez (2009) which,
instead of limiting their scope to the segment level,
are able to capture and exploit document-level fea-
tures. However, results in terms of correlation
with human assessments have not reported any im-
provement of these metrics over their sentence-
level counterparts as document and system quality
predictors. It must be clarified whether the prob-
lem is on the side of the linguistic tools, in the
similarity measure, or in the way in which we have
built document-level human assessments.
For future work, we plan to continue the er-
ror analysis to clarify why DRdoc metrics do not
outperform their DR counterparts at the document
level, and how to improve their behavior. This
337
Document level System level
Metric Pearson? Spearman? Kendall? Pearson? Spearman? Kendall?
ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145
ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435
ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638
Table 3: Meta-evaluation results at document and system level for submitted metrics
may imply defining new metrics possibly using
alternative linguistic processors. In addition, we
plan to work on the identification and analysis
of discourse markers. Finally, we plan to repeat
this experiment over other test beds with docu-
ment structure, such as those from the 2009 Work-
shop on Statistical Machine Translation shared
task (Callison-Burch et al, 2009) and the 2009
NIST MT Evaluation Campaign (Przybocki et al,
2009). In the case that document-level assess-
ments are not provided, we will also explore the
possibility of producing them ourselves.
Acknowledgments
This work has been partially funded by the
Spanish Government (projects OpenMT-2,
TIN2009-14675-C03, and KNOW, TIN-2009-
14715-C0403) and the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement numbers 247762 (FAUST
project, FP7-ICT-2009-4-247762) and 247914
(MOLTO project, FP7-ICT-2009-4-247914). We
are also thankful to anonymous reviewers for their
comments and suggestions.
References
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 33?36.
Bradley Efron and Robert Tibshirani. 1986. Bootstrap
Methods for Standard Errors, Confidence Intervals,
and Other Measures of Statistical Accuracy. Statis-
tical Science, 1(1):54?77.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A Smor-
gasbord of Features for Automatic MT Evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Dordrecht: Kluwer.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388?395.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2008. NIST Metrics for Machine Translation
2008 Evaluation (MetricsMATR08). Technical re-
port, National Institute of Standards and Technol-
ogy.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2009. NIST Open Machine Translation 2009
Evaluation (MT09). Technical report, National In-
stitute of Standards and Technology.
338
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Automatic Projection of Semantic Structures:
an Application to Pairwise Translation Ranking
Daniele Pighin Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{pighin,lluism}@lsi.upc.edu
Abstract
We present a model for the inclusion of se-
mantic role annotations in the framework of
confidence estimation for machine translation.
The model has several interesting properties,
most notably: 1) it only requires a linguis-
tic processor on the (generally well-formed)
source side of the translation; 2) it does
not directly rely on properties of the transla-
tion model (hence, it can be applied beyond
phrase-based systems). These features make
it potentially appealing for system ranking,
translation re-ranking and user feedback eval-
uation. Preliminary experiments in pairwise
hypothesis ranking on five confidence estima-
tion benchmarks show that the model has the
potential to capture salient aspects of transla-
tion quality.
1 Introduction
The ability to automatically assess the quality of
translation hypotheses is a key requirement to-
wards the development of accurate and depend-
able translation models. While it is largely agreed
that proper transfer of predicate-argument structures
from source to target is a very strong indicator of
translation quality, especially in relation to ade-
quacy (Lo and Wu, 2010a; 2010b), the incorpora-
tion of this kind of information in the Statistical Ma-
chine Translation (SMT) evaluation pipeline is still
limited to few and isolated cases, e.g., (Gime?nez and
Ma`rquez, 2010).
In this paper, we propose a general model for
the incorporation of predicate-level semantic anno-
tations in the framework of Confidence Estimation
(CE) for machine translation, with a specific focus
on the sub-problem of pairwise hypothesis ranking.
The model is based on the following underlying as-
sumption: by observing how automatic alignments
project semantic annotations from source to target
in a parallel corpus, it is possible to isolate features
that are characteristic of good translations, such as
movements of specific arguments for some classes
of predicates. The presence (or absence) of these
features in automatic translations can then be used as
an indicator of their quality. It is important to stress
that we are not claiming that the projections pre-
serve the meaning of the original annotation. Still,
it should be possible to observe regularities that can
be helpful to rank alternative translation hypotheses.
The general workflow (which can easily be ex-
tended to cope with different annotation layers,
such as sequences of meaningful phrase boundaries,
named entities or sequences of chunks or POS tags)
is exemplified in Figure 1. During training (on the
left), the system receives a parallel corpus of source
sentences and the corresponding reference transla-
tions. Source sentences are annotated with a lin-
guistic processor. The annotations are projected us-
ing training alignments, obtaining gold projections
that we can use to learn a model that captures cor-
rect annotation movements, i.e., observed in refer-
ence translations. At test time, we want to assess
the quality of a translation hypothesis given a source
sentence. As shown on the right side of Figure 1, the
first part of the process is the same as during train-
ing: the source sentence is annotated, and the an-
notation is projected onto the translation hypothesis
via automatic alignments. The model is then used
1
References Source Source Hypothesis
Align Annotate Annotate Align
Project Project
Learn Compare Score
Alignments
Parallel
Annotations
Model
AlignmentsAnnotations
Training Test
Figure 1: Architectural overview.
to compare the observed projection against the ex-
pected projection given the source annotation. The
distance between the two projections (observed and
expected) can then be used as a measure of the qual-
ity of the hypothesis.
As it only considers one-sided annotations, our
framework does not require the availability of com-
parable linguistic processors and linguistic annota-
tions, tagsets, etc., on both sides of the translation
process. In this way, it overcomes one of the main
obstacles to the adoption of linguistic analysis for
MT confidence estimation. Furthermore, the fact
that source data is generally well-formed lowers the
requirements on the linguistic processor in terms of
robustness to noisy data, making it possible to em-
ploy a wider range of linguistic processors.
Within this framework, in this paper we describe
our attempt to bridge Semantic Role Labeling (SRL)
and CE by modeling proposition-level semantics for
pairwise translation ranking. The extent to which
this kind of annotations are transferred from source
to target has indeed a very high correlation with re-
spect to human quality assessments (Lo and Wu,
2010a; 2010b). The measure that we propose is then
an ideal addition to already established CE mea-
sures, e.g., (Specia et al, 2009; Blatz et al, 2004),
as it attempts to explicitly model the adequacy of
translation hypotheses as a function of predicate-
argument structure coverage. While we are aware of
the fact that the current definition of the model can
be improved in many different ways, our preliminary
investigation, on five English to Spanish translation
benchmarks, shows promising accuracy on the dif-
ficult task of pairwise translation ranking, even for
translations with very few distinguishing features.
To capture different aspects of the projection of
SRL annotations we employ two instances of the
abstract architecture shown in Figure 1. The first
works at the proposition level, and models the cor-
rect movement of arguments from source to target.
The second works at the argument level, and models
the fluency and adequacy of individual arguments
within each predicate-argument structure. The mod-
els that we learn during training are simple phrase-
based translation models working on different kinds
of sequences, i.e., role labels in the former case and
words in the latter. To evaluate the adequacy of an
automatically projected proposition or argument, we
force the corresponding translation model to gener-
ate it (via constrained decoding). The reachability
and confidence of each translation are features that
we exploit to compare alternative translations, by
combining them in a simple voting scheme.
To score systems which are not under our direct
control (the typical scenario in CE benchmarks), we
introduce a component that generates source-target
alignments for any pair of aligned test sentences.
This addition has the nice property of allowing us
to handle the translation as a black-box, decoupling
the evaluation from a specific system and, in theory,
allowing the model to cope with phrase-based, rule-
based or hierarchical systems alike, as well as with
human-generated translations.
The rest of the paper is structured as follows: in
Section 2 we will review a selection of related work;
in Section 3 we will detail our approach; in Section 4
we will present the results of our evaluation; finally,
in Section 5 we will draw our conclusions.
2 Related work
Confidence estimation is the sub-problem within
MT evaluation concerned with the assessment of
translation quality in the absence of reference trans-
lations. A relevant initial work on this topic is
the survey by Blatz et al (2004), in which the au-
thors define a rich set of features based on source
data, translation hypotheses, n-best lists and model
characteristics to classify translations as ?good?
or ?bad?. In their observations, they conclude
2
that the most relevant features are those based on
source/target pairs and on characteristics of the
translation model.
Specia et al (2009) build on top these results by
designing a feature-selection framework for confi-
dence estimation. Translations are considered as
black-boxs (i.e., no system or model-dependent fea-
tures are employed), and novel features based on the
number of content words, a POS language model on
the target side, punctuation and number matchers in
source and target translations and the percentage of
uni-grams are introduced. Features are selected via
Partial Least Squares (PLS) regression (Wold et al,
1984). Inductive Confidence Machines (Papadopou-
los et al, 2002) are used to estimate an optimal
threshold to distinguish between ?good? and ?bad?
translations. Even though the authors show that a
small set of shallow features and some supervision
can produce good results on a specific benchmark,
we are convinced that more linguistic features are
needed for these methods to perform better across a
wider spectrum of domains and applications.
Concerning the usage of SRL for SMT, Wu and
Fung (2009) reported a first successful application of
semantic role labels to improve translation quality.
They note that improvements in translation quality
are not reflected by traditional MT evaluation met-
rics (Doddington, 2002; Papineni et al, 2002) based
on n-gram overlaps. To further investigate the topic,
Lo and Wu (2010a; 2010b) involved human annota-
tors to demonstrate that the quality of semantic role
projection on translated sentences is very highly cor-
related with human assessments.
Gime?nez and Ma`rquez (2010) describe a frame-
work for MT evaluation and meta-evaluation com-
bining a rich set of n-gram-based and linguistic met-
rics, including several variants of a metric based on
SRL. Automatic and reference translations are anno-
tated independently, and the lexical overlap between
corresponding arguments is employed as an indica-
tor of translation quality. The authors show that syn-
tactic and semantic information can achieve higher
reliability in system ranking than purely lexical mea-
sures.
Our original contribution lies in the attempt to ex-
ploit SRL for assessing translation quality in a CE
scenario, i.e., in the absence of reference transla-
tions. By accounting for whole predicate-argument
sequences as well as individual arguments, our
model has the potential to capture aspects which
relate both to the adequacy and to the fluency of
a translation. Furthermore, we outline a general
framework for the inclusion of linguistic processors
in CE that has the advantage of requiring resources
and software tools only on the source side of the
translation, where well-formed input can reasonably
be expected.
3 Model
The task of semantic role labeling (SRL) consists
in recognizing and automatically annotating seman-
tic relations between a predicate word (not nec-
essarily a verb) and its arguments in natural lan-
guage texts. The resulting predicate-argument struc-
tures are commonly referred to as propositions, even
though we will also use the more general term anno-
tations.
In PropBank (Palmer et al, 2005) style anno-
tations, which our model is based on, predicates
are generally verbs and roles are divided into two
classes: core roles (labeled A0, A1, . . . A5), whose
semantic value is defined by the predicate syntactic
frame, and adjunct roles (labeled AM-*, e.g., AM-
TMP or AM-LOC) 1 which are a closed set of verb-
independent semantic labels accounting for predi-
cate aspects such as temporal, locative, manner or
purpose. For instance, in the sentence ?The com-
mission met to discuss the problem? we can iden-
tify two predicates, met and discuss. The corre-
sponding annotations are ?[A0 The commission] [pred
met] [AM-PRP to discuss the problem]? and ?[A0 The
commission] met to [pred discuss] [A1 the problem]?.
Here, A0 and A1 play the role of prototypical sub-
ject and object, respectively, and AM-PRP is an ad-
junct modifier expressing a notion of purpose.
Sentence annotations are inherently non-
sequential, as shown by the previous example in
which the predicate and one of the arguments of
the second proposition (i.e., discuss and A1) are
completely embedded within an argument of the
first proposition (i.e., AM-PRP). Following a widely
adopted simplification, the annotations in a sentence
are modeled independently. Furthermore we de-
1The actual role labels are in the form Arg0, . . . Arg1 and
ArgM-*, but we prefer to adopt their shorter form.
3
scribe each annotation at two levels: a proposition
level, where we model the movement of arguments
from source to target; and an argument level, were
we model the adequacy and fluency of individual
argument translations. The comparison of two
alternative translations takes into account all these
factors but it models each of them independently,
i.e., we consider how properly each propositions is
rendered in each hypothesis, and how properly each
argument is translated within each proposition.
3.1 Annotation and argument projection
At the proposition level, we simply represent the se-
quence of role-label in each proposition, ignoring
their lexical content with the exception of the pred-
icate word. Considering the previous example, the
sentence would then be represented by the two se-
quences ?A0 met AM-PRP? and ?A0 * discuss A1?.
In the latter case, the special character ?*? marks
a ?gap? between A0 and the predicate word. The
annotation is projected onto the translation via di-
rect word alignments obtained through a constrained
machine translation process (i.e., we force the de-
coder to generate the desired translation). Eventual
discontinuities in the projection of an argument are
modeled as gaps. If two arguments insist on a shared
subset of words, then their labels are combined. If
the projection of an argument is a subset of the pro-
jection of the predicate word, then the argument is
discarded. If the overlap is partial, then the non-
overlapping part of the projection is represented.
If a word insertion occurs next to an argument
or the predicate, then we include it in the final se-
quence. This decision is motivated by the consider-
ation that insertions at the boundary of an argument
may be a clue of different syntactic realizations of
the same predicate across the two languages (Levin,
1993). For example, the English construct ?A0 give
A2 A1? could be rendered as ?doy A1 a A2? in Span-
ish. Here, the insertion of the preposition ?a? at de-
coding can be an important indicator of translation
quality.
This level of detail is insufficient to model some
important features of predicate-argument structures,
such as inter-argument semantic or syntactic depen-
dencies, but it is sufficient to capture a variety of
interesting linguistic phenomena. For instance, A0-
predicate inversion translating SVO into VSO lan-
guages, or the convergence of multiple source argu-
ments into a single target argument when translating
into a morphologically richer language. We should
also stress again that we are not claiming that the
structures that we observe on the target side are lin-
guistically motivated, but only that they contain rel-
evant clues to assess quality aspects of translation.
As for the representation of individual arguments,
we simply represent their surface form, i.e., the
sequence of words spanning each argument. So,
for example, the argument representations extracted
from ?[A0 The commission] [pred met] [AM-PRP to
discuss the problem]? would be ?The commission?,
?met?, ?to discuss the problem?. To project each ar-
gument we align all its words with the target side.
The leftmost and the rightmost aligned words de-
fine the boundaries of the argument in the target sen-
tence. All the words in between (including eventual
gaps) are considered as part of the projection of the
argument. This approach is consistent with Prop-
Bank style annotations, in which arguments are con-
tiguous word sequences, and it allows us to employ a
standard translation model to evaluate the fluency of
the argument projection. The rationale here is that
we rely on proposition level annotations to convey
the semantic structure of the sentence, while at the
argument level we are more interested in evaluating
the lexical appropriateness of their realization.
The projection of a proposition and its arguments
for an example sentence is shown in Figure 2. Here,
s is the original sentence and h1 and h2 are two
translation hypotheses. The figure shows how the
whole proposition (p) and the predicate word (pred)
along with its arguments (A0, A1 and A2) are repre-
sented after projection on the two hypotheses. As we
can observe, in both cases thank (the predicate word)
gets aligned with the word gracias. For h1, the de-
coder aligns I (A0) to doy, leaving a gap between A0
and the predicate word. The gap gets filled by gen-
erating the word las. Since the gap is adjacent to at
least one argument, las is included in the representa-
tion of p for h1. In h2, the projection of A0 exactly
overlaps the projection of the predicate (?Gracias?),
and therefore A0 is not included in n for h2.
3.2 Comparing hypotheses
At test time, we want to use our model to com-
pare translation pairs and recognize the most reli-
4
s I thank the commissioner for the detailed reply
h1 Doy las gracias al comisario por la detallada respuesta
h2 Gracias , al sen?or comisario por para el respuesta
p A0 thank A1 A2 pred thank
h1 A0 +las gracias A1 A2 h1 gracias
h2 Gracias A1 A2 h2 Gracias
A1 the commissioner A0 I
h1 al comisario h1 doy
h2 al sen?or comisario h2 Gracias
A2 for the detailed reply
h2 por la detallada respuesta
h2 para el respuesta
Figure 2: Comparison between two alternative transla-
tions h1 and h2 for the source sentence s.
able. Let s be the source sentence, and h1 and h2
be two translation hypotheses. For each proposition
p in s, we assign a confidence value to its represen-
tation in h1 and h2, i.e., p1 and p2, by forcing the
proposition-level translation system to generate the
projection observed in the corresponding hypothe-
sis. The reachability of p1 (respectively, p2) and the
decoder confidence in translating p as p1 are used as
features to estimate p1 (p2) accuracy. Similarly, for
each argument a in each proposition p we generate
its automatic projection on h1 and h2, i.e., a1 and
a2. We force the argument-level decoder to translate
a into a1 and a2, and use the respective reachability
and translation confidence as features accounting for
their appropriateness.
The best translation hypothesis (h1 or h2) is then
selected according to the following decision func-
tion:
h? = argmax
i?{0,1}
?
k
fk(hi, hj 6=i, s) (1)
where each feature function fk(?, ?, ?) defines a com-
parison measure between its first two arguments, and
returns 1 if the first argument is greater (better) than
the second, and 0 otherwise. In short, the decision
function selects the hypothesis that wins the highest
number of comparisons.
The feature functions that we defined account
for the following factors, the last three being eval-
uated once for each proposition in s: (1) Num-
ber of successfully translated propositions; (2) Av-
erage translation confidence for projected proposi-
tions; (3) Number of times that a proposition in hi
has higher confidence than the corresponding propo-
sition in hi 6=j ; (4) Number of successfully translated
arguments; (5) Average translation confidence for
projected arguments; (6) Number of times that an
argument in hi has higher confidence than the corre-
sponding argument in hi 6=j .
With reference to Figure 2, the two translation hy-
potheses have been scored 4 (very good) and 2 (bad)
by human annotators. The score assigned by the
proposition decoder to p1 is higher than p2, hence
comparisons (2) and (3) are won by h1. Accord-
ing to the arguments decoder, h1 does a better job
at representing A0 and A2; h2 is better at rendering
A1, and pred is a tie. Therefore, h1 also prevails
according to (6). Given the very high confidence as-
signed to the translation of A2 in h1, the hypothesis
also prevails in (5). In this case, (1) and (4) do not
contribute to the decision as the two projections have
the same coverage.
4 Evaluation
In this section, we present the results obtained by
applying the proposed method to the task of rank-
ing consistency, or pairwise ranking of alternative
translations: that is, given a source sentence s, and
two candidate translations h1 and h2, decide which
one is a better translation for s. Pairwise ranking
is a simplified setting for CE that is general enough
to model the selection of the best translation among
a finite set of alternatives. Even though it cannot
measure translation quality in isolation, a reliable
pairwise ranking model would be sufficient to solve
many common practical CE problems, such as sys-
tem ranking, user feedback filtering or hypotheses
re-ranking.
4.1 Datasets
We ran our experiments on the human assessments
released as part of the ACL Workshops on Machine
Translations in 2007 (Callison-Burch et al, 2007),
2008 (Callison-Burch et al, 2008), 2009 (Callison-
Burch et al, 2009) and 2010 (Callison-Burch et al,
2010). These datasets will be referred to as wm-
tYY(t) in the remainder, YY being the last two digits
of the year of the workshop and t = n for newswire
data or t = e for Europarl data. So, for example,
wmt08e is the Europarl test set of the 2008 edition
5
of the workshop. As our system is trained on Eu-
roparl data, newswire test sets are to be considered
out-of-domain. All the experiments are relative to
English to Spanish translations.
The wmt08, wmt09 and wmt10 datasets provide
a ranking among systems within the range [1,5] (1
being the worst system, and 5 the best). The dif-
ferent datasets contain assessments for a different
number of systems, namely: 11 for wmt08(e), 10 for
wmt08(n), 9 for wmt09 and 16 for wmt10n. Gener-
ally, multiple annotations are available for each an-
notated sentence. In all cases in which multiple as-
sessments are available, we used the average of the
assessments.
The wmt07 dataset would be the most interesting
of all, in that it provides separate assessments for
the two main dimensions of translation quality, ade-
quacy and fluency, as well as system rankings. Un-
luckily, the number of annotations in this dataset is
very small, and after eliminating the ties the num-
bers are even smaller. As results on such small num-
bers would not be very representative, we decided
not to include them in our evaluation.
We also evaluated on the dataset described
in (Specia et al, 2010), which we will refer to as
specia. As the system is based on Europarl data, it
is to be considered an in-domain benchmark. The
dataset includes results produced by four different
systems, each translation being annotated by only
one judge. Given the size of the corpus (the output
of each system has been annotated on the same set
of 4,000 sentences), this dataset is the most repre-
sentative among those that we considered. It is also
especially interesting for two other reasons: 1) sys-
tems are assigned a score ranging from 1 (bad) to 4
(good as it is) based on the number of edits required
to produce a publication-ready translation. There-
fore, here we have an absolute measure of transla-
tion accuracy, as opposed to relative rankings; 2)
each system involved in the evaluation has very pe-
culiar characteristics, hence they are very likely to
generate quite different translations for the same in-
put sentences.
4.2 Setup
Our model consists of four main components: an
automatic semantic role labeler (to annotate source
sentences); a lexical translation model (to gener-
ate the alignments required to map the annotations
onto a translation hypothesis); a translation model
for predicate-argument structures, to assign a score
to projected annotations; and a translation model for
role fillers, to assign a score to the projection of each
argument.
To automatically label our training data with se-
mantic roles we used the Swirl system2 (Surdeanu
and Turmo, 2005) with the bundled English mod-
els for syntactic and semantic parsing. On the
CoNLL-2005 benchmark (Carreras and Ma`rquez,
2005), Swirl sports an F1-measure of 76.46. This
figure drops to 75 for mixed data, and to 65.42 on
out-of-domain data, which we can regard as a con-
servative estimate of the accuracy of the labeler on
wmt benchmarks.
For all the translation tasks we employed the
Moses phrase-based decoder3 in a single-factor con-
figuration. The -constraint command line pa-
rameter is used to force Moses to output the desired
translation. For the English to Spanish lexical trans-
lation model, we used an already available model
learned using all available wmt10e data.
To build the proposition level translation system,
we first annotated all the English sentences from the
wmt10e (en?es) training set with Swirl; then, we
forced the lexical translation model to generate the
alignments for the reference translations and pro-
jected the annotations on the target side. The process
resulted in 2,493,476 parallel annotations. 5,000 an-
notations were held-out for model tuning. The train-
ing data was used to estimate a 5-gram language
model and the translation model, which we later op-
timized on held-out data.
As for the argument level translator, we trained
it on parallel word sequences spanning the same
role in an annotation and its projection. Each such
pair constitutes a training example for the argu-
ment translator, each argument representation being
modeled independently from the others. With the
same setup used for the proposition translator, we
collected 4,578,480 parallel argument fillers from
wmt10e en?es training data, holding out 20,000
pairs for model tuning.
2http://www.surdeanu.name/mihai/swirl/
3http://www.statmt.org/moses/
6
4.3 A note on recall
The main limitation of the model in its current im-
plementation is its low recall. The translation model
that we use to generate the alignments is mostly re-
sponsible for it. In fact, in approximately 35% of the
cases the constrained translation model is not able
to generate the required hypothesis. An obvious im-
provement would consist in using just an alignment
model for this task, instead of resorting to transla-
tion, for instance following the approach adopted in
(Espla` et al, 2011). It should also be noted that,
while this component adds the interesting property
of decoupling the measure from the system that pro-
duced the hypothesis, it is not strictly necessary in
all those cases in which translation alignments are
already available, e.g., for N-best re-ranking.
The second component that suffers from recall
problems is the semantic role labeler, which fails in
annotating sentences in approximately 6% of the re-
maining cases. These failures are by and large due
to the lack of proper verbal predicates in the target
sentence, and as such expose a limiting factor of the
underlying model. In another 3% of the cases, an
annotation is produced but it cannot be projected on
the hypothesis, since the predicate word on the target
side gets deleted during translation.
Another important consideration is that no mea-
sure for CE is conceived to be used in isolation, and
our measure is no exception. In combination with
others, the measure should only trigger when ap-
propriate, i.e., when it is able to capture interesting
patterns that are significant to discriminate transla-
tion quality. If it abstains, the other measures would
compensate for the missing values. In this respect,
we should also consider that not being able to pro-
duce a translation may be inherently considered an
indicator of translation quality.
4.4 Results
Table 1 lists, in each block of rows, pairwise classifi-
cation accuracy results obtained on a specific bench-
mark. The benchmarks are sorted in order of re-
verse relevance, the largest benchmark (specia) be-
ing listed first. In each row, we show results obtained
for different configurations in which the variable is
the distance d between two assessment scores. So,
for example, the row d = 1 accounts for all the
specia Corr Wrong Und(%) Acc(%)
d = 1 1076 656 14.26 62.12
d = 2 272 84 11.00 76.40
d = 3 30 8 13.64 78.95
d ? 1 1378 748 13.72 64.82
d ? 2 302 92 11.26 76.65
d ? 3 30 8 13.64 78.95
wmt10n Corr Wrong Und(%) Acc(%)
d = 1 428 374 15.04 53.37
d = 2 232 196 18.01 54.21
d = 3 98 74 16.50 56.98
d ? 1 784 664 16.20 54.14
d ? 2 356 290 17.60 55.11
d ? 3 124 94 16.79 56.88
wmt09n Corr Wrong Und(%) Acc(%)
d = 1 70 60 19.75 53.85
d = 2 30 40 20.45 42.86
d = 3 26 10 18.18 72.22
d ? 1 134 116 19.87 53.60
d ? 2 64 56 20.00 53.33
d ? 3 34 16 19.35 68.00
wmt08n Corr Wrong Und(%) Acc(%)
d = 1 64 36 12.28 64.00
d = 2 26 24 19.35 52.00
d = 3 12 6 18.18 66.67
d ? 1 104 70 14.71 59.77
d ? 2 40 34 17.78 54.05
d ? 3 14 10 14.29 58.33
wmt08e Corr Wrong Und(%) Acc(%)
d = 1 62 34 21.31 64.58
d = 2 40 30 10.26 57.14
d = 3 22 8 11.76 73.33
d ? 1 134 80 15.75 62.62
d ? 2 72 46 10.61 61.02
d ? 3 32 16 11.11 66.67
Table 1: Results on five confidence estimation bench-
marks. An n next to the task name (e.g. wmt08n) stands
for a news (i.e. out of domain) corpus, whereas an e (e.g.
wmt08e) stands for a Europarl (i.e. in domain) corpus.
The specia corpus is in-domain.
comparisons in which the distance between scores
is exactly one, while row d ? 2 considers all the
cases in which the distance is at least 2. For each
test, the columns show: the number of correct (Corr)
and wrong (Wrong) decisions, the percentage of un-
decidable cases (Und), i.e., the cases in which the
scoring function cannot decide between the two hy-
potheses, and the accuracy of classification (Acc)
measured without considering the unbreakable ties.
7
The accuracy for d ? 1, i.e., on all the available
annotations, is shown in bold.
First, we can observe that the results are above the
baseline (an accuracy of 50% for evenly distributed
binary classification) on all the benchmarks and for
all configurations. The only outlier is wmt09n for
d = 2, with an accuracy of 42.86%. Across the
different datasets, results vary from promising (spe-
cia and wmt08e, where accuracy is generally above
60%) to mildly good (wmt10n), but across all the
board the method seems to be able to provide useful
clues for confidence estimation.
As expected, the accuracy of classification tends
to increase as the difference between hypotheses be-
comes more manifest. In four cases out of six, the
accuracy for d = 3 is above 60%, with the notable
peaks on specia, wmt09n and wmt08e where it goes
over 70% (on the first, it arrives almost at 80%).
Unluckily, very few translations have very different
quality (a measure of the difficulty of the task). Nev-
ertheless, the general trend seems to support the re-
liability of the approach.
When we consider the results on the whole
datasets (i.e., d ? 1), pairwise classification accu-
racy ranges from 54% (for wmt09n and wmt10n,
both out-of-domain), to 63-64% (for specia and
wmt08e, both in-domain). Interestingly, the perfor-
mance on wmt08n, which is also out-of-domain, is
closer to in-domain benchmarks, i.e., 60%. These
figures suggest that the method is consistently reli-
able on in-domain data, but also out-of-domain eval-
uation can benefit from its application. The differ-
ence in performance between wmt08n and the other
out-of-domain benchmarks will be reason of further
investigation as future work, as well as the drop in
performance for d = 2 on three of the benchmarks.
5 Conclusions
We have presented a model to exploit the rich in-
formation encoded by predicate-argument structures
for confidence estimation in machine translation.
The model is based on a battery of translation sys-
tems, which we use to study the movement and
the internal representation of propositions and ar-
guments projected from source to target via auto-
matic alignments. Our preliminary results, obtained
on five different benchmarks, suggest that the ap-
proach is well grounded and that semantic annota-
tions have the potential to be successfully employed
for this task.
The model can be improved in many ways, its ma-
jor weakness being its low recall as discussed in Sec-
tion 4.3. Another area in which there is margin for
improvement is the representation of predicate ar-
gument structures. It is reasonable to assume that
different representations could yield very different
results. Introducing more clues about the seman-
tic content of the whole predicate argument struc-
ture, e.g., by including argument head words in the
representation of the proposition, or considering a
more fine-grained representation at the proposition
level, could make it possible to assess the quality of
a translation reducing the need to back-off to indi-
vidual arguments. As for the representation of ar-
guments, a first and straightforward improvement
would be to train a separate model for each argument
class, or to move to a factored model that would al-
low us to model explicitly the insertion of words or
the overlap of argument words due to the projection.
Another important research direction involves the
combination of this measure with already assessed
metric sets for CE, e.g., (Specia et al, 2010), to un-
derstand to what extent it can contribute to improve
the overall performance. In this respect, we would
also like to move from a heuristic scoring function
to a statistical model.
Finally, we would like to test the generality of the
approach by designing other features based on the
same ?annotate, project, measure? framework, as we
strongly believe that it is an effective yet simple way
to combine several linguistic features for machine
translation evaluation. For example, we would like
to apply a similar framework to model the movement
of chunks or POS sequences.
Acknowledgments
We would like to thank the anonymous reviewers for their
valuable comments. This research has been partially funded
by the Spanish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) under grant
agreement numbers 247762 (FAUST project, FP7-ICT-2009-
4-247762) and 247914 (MOLTO project, FP7-ICT-2009-4-
247914).
8
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. ACL.
Chris Callison-Burch, Philipp Koehn, Cameron Shaw
Fordyce, and Christof Monz, editors. 2007. Proceed-
ings of the Second Workshop on Statistical Machine
Translation. ACL, Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Josh Schroeder, and Cameron Shaw Fordyce, editors.
2008. Proceedings of the Third Workshop on Statisti-
cal Machine Translation. ACL, Columbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder, editors. 2009. Proceedings of the
Fourth Workshop on Statistical Machine Translation.
ACL, Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR. ACL, Uppsala,
Sweden.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 152?164, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Miquel Espla`, Felipe Sa?nchez-Mart??nez, and Mikel L.
Forcada. 2011. Using word alignments to assist
computer-aided translation users by marking which
target-side words to change or keep unedited. In Pro-
ceedings of the 15th Annual Conference of the Euro-
pean Associtation for Machine Translation.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Lin-
guistic measures for automatic machine transla-
tion evaluation. Machine Translation, 24:209?240.
10.1007/s10590-011-9088-7.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Chi-kiu Lo and Dekai Wu. 2010a. Evaluating machine
translation utility via semantic role labels. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associa-
tion (ELRA).
Chi-kiu Lo and Dekai Wu. 2010b. Semantic vs. syntac-
tic vs. n-gram structure for machine translation evalu-
ation. In Proceedings of the 4th Workshop on Syntax
and Structure in Statistical Translation, pages 52?60,
Beijing, China.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Comput. Linguist., 31(1):71?106.
Harris Papadopoulos, Kostas Proedrou, Volodya Vovk,
and Alexander Gammerman. 2002. Inductive confi-
dence machines for regression. In AMAI?02.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. ACL.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Nicola Cancedda, and Marc Dymetman.
2010. A dataset for assessing machine translation
evaluation metrics. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta. European Lan-
guage Resources Association (ELRA).
Mihai Surdeanu and Jordi Turmo. 2005. Seman-
tic role labeling using complete syntactic analysis.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 221?224, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
S. Wold, A. Ruhe, H Wold, and W.J. Dunn. 1984. The
collinearity problem in linear regression. the partial
least squares (pls) approach to generalized inverses.
5:735?743.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: a hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, NAACL-Short ?09, pages 13?16,
Stroudsburg, PA, USA. ACL.
9
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 127?132,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The UPC Submission to the WMT 2012 Shared Task on Quality Estimation
Daniele Pighin Meritxell Gonza?lez Llu??s Ma`rquez
Universitat Polite`cnica de Catalunya, Barcelona
{pighin,mgonzalez,lluism}@lsi.upc.edu
Abstract
In this paper, we describe the UPC system that
participated in the WMT 2012 shared task on
Quality Estimation for Machine Translation.
Based on the empirical evidence that fluency-
related features have a very high correlation
with post-editing effort, we present a set of
features for the assessment of quality estima-
tion for machine translation designed around
different kinds of n-gram language models,
plus another set of features that model the
quality of dependency parses automatically
projected from source sentences to transla-
tions. We document the results obtained on
the shared task dataset, obtained by combining
the features that we designed with the baseline
features provided by the task organizers.
1 Introduction
Quality Estimation (QE) for Machine Translations
(MT) is the task concerned with the prediction of the
quality of automatic translations in the absence of
reference translations. The WMT 2012 shared task
on QE for MT (Callison-Burch et al, 2012) required
participants to score and rank a set of automatic
English to Spanish translations output by a state-
of-the-art phrase based machine translation system.
Task organizers provided a training dataset of 1, 832
source sentences, together with reference, automatic
and post-edited translations, as well as human qual-
ity assessments for the automatic translations. Post-
editing effort, i.e., the amount of editing required to
produce an accurate translation, was selected as the
quality criterion, with assessments ranging from 1
(extremely bad) to 5 (good as it is). The organizers
also provided a set of linguistic resources and pro-
cessors to extract 17 global indicators of translation
quality (baseline features) that participants could de-
cide to employ for their models. For the evaluation,
these features are used to learn a baseline predictors
for participants to compare against. Systems partic-
ipating in the evaluation are scored based on their
ability to correctly rank the 422 test translations (us-
ing DeltaAvg and Spearman correlation) and/or to
predict the human quality assessment for each trans-
lation (using Mean Average Error - MAE and Root
Mean Squared Error - RMSE).
Our initial approach to the task consisted of sev-
eral experiments in which we tried to identify com-
mon translation errors and correlate them with qual-
ity assessments. However, we soon realized that
simple regression models estimated on the baseline
features resulted in more consistent predictors of
translation quality. For this reason, we eventually
decided to focus on the design of a set of global in-
dicators of translation quality to be combined with
the strong features already computed by the baseline
system.
An analysis of the Pearson correlation of the
baseline features (Callison-Burch et al, 2012)1
with human quality assessments shows that the two
strongest individual predictors of post-editing ef-
fort are the n-gram language model perplexities es-
timated on source and target sentences. This ev-
idence suggests that a reasonable approach to im-
1Baseline features are also described in http://www.
statmt.org/wmt12/quality-estimation-task.
html.
127
Feature Pearson |r| Feature Pearson |r|
BL/4 0.3618 DEP/C+/Q4/R 0.0749
BL/5 0.3544 BL/13 0.0741
BL/12 0.2823 DEP/C?/Q1/W 0.0726
BL/14 0.2675 DEP/C+/Q4/W 0.0718
BL/2 0.2667 DEP/C+/Q34/R 0.0687
BL/1 0.2620 BL/3 0.0623
BL/8 0.2575 DEP/C+/Q34/W 0.0573
BL/6 0.2143 SEQ/sys-ref/W 0.0495
DEP/C?/S 0.2072 SEQ/sys/W 0.0492
BL/10 0.2033 SEQ/ref-sys/W 0.0390
DEP/C?/Q12/S 0.1858 BL/7 0.0351
BL/17 0.1824 SEQ/sys/SStop 0.0312
BL/16 0.1725 SEQ/sys/RStop 0.0301
DEP/C?/W 0.1584 SEQ/sys-ref/SStop 0.0291
DEP/C?/R 0.1559 SEQ/sys-ref/RStop 0.0289
DEP/C?/Q12/R 0.1447 DEP/Coverage/S 0.0286
DEP/Coverage/W 0.1419 SEQ/ref-sys/S 0.0232
DEP/C?/Q1/S 0.1413 SEQ/ref-sys/R 0.0205
BL/15 0.1368 SEQ/ref-sys/RStop 0.0187
DEP/C+/Q4/S 0.1257 SEQ/sys-ref/R 0.0184
DEP/Coverage/R 0.1239 SEQ/sys/R 0.0177
SEQ/ref-sys/PStop 0.1181 SEQ/ref-sys/Chains 0.0125
SEQ/sys/PStop 0.1173 SEQ/ref-sys/SStop 0.0104
SEQ/sys-ref/PStop 0.1170 SEQ/sys/S 0.0053
DEP/C?/Q12/W 0.1159 SEQ/sys-ref/S 0.0051
DEP/C?/Q1/R 0.1113 SEQ/sys/Chains 0.0032
DEP/C+/Q34/S 0.0933 SEQ/sys-ref/Chains 0.0014
BL/9 0.0889 BL/11 0.0001
Table 1: Pearson correlation (in absolute value) of the
baseline (BL) features and the extended feature set (SEQ
and DEP) with the quality assessments.
prove the accuracy of the baseline would be to con-
centrate on the estimation of other n-gram language
models, possibly working at different levels of lin-
guistic analysis and combining information coming
from the source and the target sentence. On top of
that, we add another class of features that capture
the quality of grammatical dependencies projected
from source to target via automatic alignments, as
they could provide clues about translation quality
that may not be captured by sequential models.
The novel features that we incorporate are de-
scribed in full detail in the next section; in Sec-
tion 3 we describe the experimental setup and the
resources that we employ, while in Section 4 we
present the results of the evaluation; finally, in Sec-
tion 5 we draw our conclusions.
2 Extended features set
We extend the set of 17 baseline features with 35
new features:
SEQ: 21 features based on n-gram language mod-
els estimated on reference and automatic trans-
lations, combining lexical elements of the tar-
get sentence and linguistic annotations (POS)
automatically projected from the source;
DEP: 18 features that estimate a language model
on dependency parse trees automatically pro-
jected from source to target via unsupervised
alignments.
All the related models are estimated on a cor-
pus of 150K newswire sentences collected from the
training/development corpora of previous WMT edi-
tions (Callison-Burch et al, 2007; Callison-Burch et
al., 2011). We selected this resource because we pre-
fer to estimate the models only on in-domain data.
The models for SEQ features are computed based
on reference translations (ref ) and automatic trans-
lations generated by the same Moses (Koehn et al,
2007) configuration used by the organizers of this
QE task. As features, we encode the perplexity of
observed sequences with respect to the two models,
or the ratio of these values. For DEP features, we es-
timate a model that explicitly captures the difference
between reference and automatic translations for the
same sentence.
2.1 Sequential features (SEQ)
The simplest sequential models that we estimate
are 3-gram language models2 on the following se-
quences:
W: (Word), the sequence of words as they appear
in the target sentence;
R: (Root), the sequence of the roots of the words in
the target;
S: (Suffix) the sequence of the suffixes of the words
in the target;
As features, for each automatic translation we en-
code:
? The perplexity of the corresponding sequence
according to automatic (sys) translations: for
2We also considered using longer histories, i.e., 5-grams, but
since we could not observe any noticeable difference we finally
selected the least over-fitting alternative.
128
example, SEQ/sys/R and SEQ/sys/W are the
root-sequence and word-sequence perplexities
estimated on the corpus of automatic transla-
tions;
? The ratio between the perplexities according
the two sets of translations: for example,
SEQ/ref-sys/S is the ratio between the perplex-
ity of suffix-sequences on reference and auto-
matic translations, and SEQ/sys-ref/S is its in-
verse.3
We also estimate 3-gram language models on
three variants of a sequence in which non-stop words
(i.e., all words belonging to an open class) are re-
placed with either:
RStop: the root of the word;
SStop: the suffix of the word;
PStop: the POS of the aligned source word(s).
This last model (PStop) is the only one that requires
source/target pairs in order to be estimated. If the
target word is aligned to more than one word, we
use the ordered concatenation of the source words
POS tags; if the word cannot be aligned, we replace
it with the placeholder ?*?, e.g.: ?el NN de * VBZ
JJ en muchos NNS .?. Also in this case, different
features encode the perplexity with respect to au-
tomatic translations (e.g., SEQ/sys/PStop) or to the
ratio between automatic and reference translations
(e.g., SEQ/ref-sys/RStop).
Finally, a last class of sequences (Chains) col-
lapses adjacent stop words into a single token.
Content-words or isolated stop-words are not in-
cluded in the sequence, e.g: ?mediante la de los
de la y de las y la a los?. Again, we consider
the same set of variants, e.g. SEQ/sys/Chains or
SEQ/sys-ref/Chains.
Since there are 7 sequence types and 3 combinations
(sys, sys-ref, ref-sys) we end up with 21 new fea-
tures.
3Features extracted solely from reference translations have
been considered, but they were dropped during development
since we could not observe a noticeable effect on prediction
quality.
2.2 Dependency features (DEP)
These features are based on the assumption that
by observing how dependency parses are projected
from source to target we can gather clues concern-
ing translation quality that cannot be captured by se-
quential models. The features encode the extent to
which the edges of the projected dependency tree are
observed in reference-quality translations.
The model for DEP features is estimated on
the same set of 150K English sentences and the
corresponding reference and automatic translations,
based on the following algorithm:
1. Initialize two maps M+ and M? to store edge
counts;
2. Then, for each source sentence s: parse s with
a dependency parser;
3. Align the words of s with the reference and the
automatic translations r and a;
4. For each dependency relation ?d, sh, sm? ob-
served in the source, where d is the relation
type and sh and sm are the head and modifier
words, respectively:
(a) Identify the aligned head/modifier words
in r and a, i.e., ?rh, rm? and ?ah, am?;
(b) If rh = ah and rm = am, then incre-
ment M+?d,ah,am? by one, otherwise incre-
ment M??d,ah,am?.
In other terms, M+ keeps track of how many times
a projected dependency is the same in the automatic
and in the reference translation, while M? accounts
for the cases in which the two projections differ.
Let T be the set of dependency relations projected
on an automatic translation. In the feature space we
represent:
Coverage: The ratio of dependency edges found in
M? or M+ over the total number of projected
edges, i.e.
Coverage(T ) =
?
D?T M
+
D +M
?
D
|T |
;
C+: The quantity C+ = 1|T |
?
D?T
M+D
M+D?M
?
D
;
129
C?: The quantity C? = 1|T |
?
D?T
M?D
M+D?M
?
D
.
Intuitively, high values of C+ mean that most pro-
jected dependencies have been observed in reference
translations; conversely, high values of C? suggest
that most of the projected dependencies were only
observed in automatic translations.
Similarly to SEQ features, also in this case we ac-
tually employ three variants of these features: one in
which we use word forms (i.e., DEP/Coverage/W,
DEP/C+/W and DEP/C?/W), one in which we
look at roots (i.e., DEP/Coverage/R, DEP/C+/R
and DEP/C?/R) and one in which we only con-
sider suffixes (i.e., DEP/Coverage/S, DEP/C+/S and
DEP/C?/S).
Moreover, we also estimate C+ in the top (Q4)
and top two (Q34) fourths of edge scores, and C? in
the bottom (Q1) and bottom two (Q12) fourths. As
an example, the feature DEP/C+/Q4/R encodes the
value of C+ within the top fourth of the ranked list of
projected dependencies when only considering word
roots, while DEP/C?/W is the value of C? on the
whole edge set estimated using word forms.
3 Experiment setup
To extract the extended feature set we use an align-
ment model, a POS tagger and a dependency parser.
Concerning the former, we trained an unsupervised
model with the Berkeley aligner4, an implementa-
tion of the symmetric word-alignment model de-
scribed by Liang et al (2006). The model is trained
on Europarl and newswire data released as part of
WMT 2011 (Callison-Burch et al, 2011) training
data. For POS tagging and semantic role annota-
tion we use SVMTool5 (Jesu?s Gime?nez and Llu??s
Ma`rquez, 2004) and Swirl6 (Surdeanu and Turmo,
2005), respectively, with default configurations. To
estimate the SEQ and DEP features we use refer-
ence and automatic translations of the newswire sec-
tion of WMT 2011 training data. The automatic
translations are generated by the same configura-
tion generating the data for the quality estimation
task. The n-gram models are estimated with the
4http://code.google.com/p/berkeleyaligner
5http://www.lsi.upc.edu/?nlp/SVMTool/
6http://www.surdeanu.name/mihai/swirl/
Feature set DeltaAvg MAE
Baseline 0.4664 0.6346
Extended 0.4694 0.6248
Table 2: Comparison of the baseline and extended feature
set on development data.
SRILM toolkit 7, with order equal to 3 and Kneser-
Ney (Kneser and Ney, 1995) smoothing.
As a learning framework we resort to Support
Vector Regression (SVR) (Smola and Scho?lkopf,
2004) and learn a linear separator using the SVM-
Light optimizer by Joachims (1999)8. We represent
feature values by means of their z-scores, i.e., the
number of standard deviations that separate a value
from the average of the feature distribution. We
carry out the system development via 5-fold cross
evaluation on the 1,832 development sentences for
which we have quality assessments.
4 Evaluation
In Table 1 we show the absolute value of the Pear-
son correlation of the features used in our model,
i.e., the 17 baseline features (BL/*), the 21 sequence
(SEQ/*) and the 18 dependency (DEP/*) features,
with the human quality assessments. The more cor-
related features are in the top (left) part of the ta-
ble. At a first glance, we can see that 9 of the 10
features having highest correlation are already en-
coded by the baseline. We can also observe that
DEP features show a higher correlation than SEQ
features. This evidence seems to contradict our ini-
tial expectations, but it can be easily ascribed to the
limited size of the corpus used to estimate the n-
gram models (150K sentences). This point is also
confirmed by the fact that the three variants of the
*PStop model (based on sequences of target stop-
words interleaved by POS tags projected from the
source sentence and, hence, on a very small vocab-
ulary) are the three sequential models sporting the
highest correlation. Alas, the lack of lexical anchors
makes them less useful as predictors of translation
quality than BL/4 and BL/5. Another interesting as-
7http://www-speech.sri.com/projects/
srilm
8http://svmlight.joachims.org/
130
System DeltaAvg MAE
Baseline 0.55 0.69
Official Evaluation 0.22 0.84
Amended Evaluation 0.51 0.71
Table 3: Official and amended evaluation on test data of
the extended feature sets.
pect is that DEP/C? features show higher correlation
than DEP/C+. This is an expected behaviour, as be-
ing indicators of possible errors they are intended to
have discriminative power with respect to the human
assessments. Finally, we can see that more than 50%
of the included features, including five baseline fea-
tures, have negligible (less than 0.1) correlation with
the assessments. Even though these features may not
have predictive power per se, their combination may
be useful to learn more accurate models of quality.9
Table 2 shows a comparison of the baseline fea-
tures against the extended feature set as the average
DeltaAvg score and Mean Absolute Error (MAE) on
the 10 most accurate development configurations. In
both cases, the extended feature set results in slightly
more accurate models, even though the improve-
ment is hardly significant.
Table 3 shows the results of the official evaluation.
Our submission to the final evaluation (Official) was
plagued by a bug that affected the values of all the
baseline features on the test set. As a consequence,
the official performance of the model is extremely
poor. The row labeled Amended shows the results
that we obtained after correcting the problem. As we
can see, on both tasks the baseline outperforms our
model, even though the difference between the two
is only marginal. Ranking-wise, our official submis-
sion is last on the ranking task and last-but-one on
the quality prediction task. In contrast, the amended
model shows very similar accuracy to the baseline,
as the majority of the systems that took part in the
evaluation.
9Our experiments on development data were not signifi-
cantly affected by the presence or removal of low-correlation
features. Given the relatively small feature space, we adopted
a conservative strategy and included all the features in the final
models.
5 Discussion and conclusions
We have described the system with which we par-
ticipated in the WMT 2012 shared task on quality
estimation. The model incorporates all the base-
line features, plus two sets of novel features based
on: 1) n-gram language models estimated on mixed
sequences of target sentence words and linguistic
annotations projected from the source sentence by
means of automatic alignments; and 2) the likeli-
hood of the projection of dependency relations from
source to target.
On development data we found out that the ex-
tended feature set granted only a very marginal im-
provement with respect to the strong feature set of
the baseline. In the official evaluation, our submis-
sion was plagued by a bug affecting the generation
of baseline features for the test set, and as a result
we had an incredibly low performance. After fix-
ing the bug, re-evaluating on the test set confirmed
that the extended set of features, at least in the cur-
rent implementation, does not have the potential to
significantly improve over the baseline features. On
the contrary, the accuracy of the corrected model is
slightly lower than the baseline on both the ranking
and the quality estimation task.
During system development it was clear that im-
proving significantly over the results of the base-
line features would be very difficult. In our expe-
rience, this is especially due to the presence among
the baseline features of extremely strong predictors
of translation quality such as the perplexity of the
automatic translation. We could also observe that
the parametrization of the learning algorithm had
a much stronger impact on the final accuracy than
the inclusion/exclusion of specific features from the
model.
We believe that the information that we encode,
and in particular dependency parses and stop-word
sequences, has the potential to be quite relevant for
this task. On the other hand, it may be necessary to
estimate the models on much larger datasets in order
to compensate for their inherent sparsity. Further-
more, more refined methods may be required in or-
der to incorporate the relevant information in a more
determinant way.
131
Acknowledgments
This research has been partially funded by
the Spanish Ministry of Education and Science
(OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement numbers
247762 (FAUST project, FP7-ICT-2009-4-247762)
and 247914 (MOLTO project, FP7-ICT-2009-4-
247914).
References
[Callison-Burch et al2007] Chris Callison-Burch,
Philipp Koehn, Cameron Shaw Fordyce, and Christof
Monz, editors. 2007. Proceedings of the Second
Workshop on Statistical Machine Translation. ACL,
Prague, Czech Republic.
[Callison-Burch et al2011] Chris Callison-Burch,
Philipp Koehn, Christof Monz, and Omar F. Zaidan,
editors. 2011. Proceedings of the Sixth Workshop
on Statistical Machine Translation. Association for
Computational Linguistics, Edinburgh, Scotland, July.
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
[Jesu?s Gime?nez and Llu??s Ma`rquez2004] Jesu?s Gime?nez
and Llu??s Ma`rquez. 2004. SVMTool: A general POS
tagger generator based on Support Vector Machines.
In Proceedings of the 4th LREC.
[Joachims1999] Thorsten Joachims. 1999. Making large-
Scale SVM Learning Practical. In B. Scho?lkopf,
C. Burges, and A. Smola, editors, Advances in Kernel
Methods - Support Vector Learning.
[Kneser and Ney1995] Reinhard Kneser and Hermann
Ney. 1995. Improved backing-off for m-gram lan-
guage modeling. In In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume I, pages 181?184, Detroit, Michi-
gan, May.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Liang et al2006] Percy Liang, Benjamin Taskar, and
Dan Klein. 2006. Alignment by agreement. In HLT-
NAACL.
[Smola and Scho?lkopf2004] Alex J. Smola and Bernhard
Scho?lkopf. 2004. A tutorial on support vector regres-
sion. Statistics and Computing, 14(3):199?222, Au-
gust.
[Surdeanu and Turmo2005] Mihai Surdeanu and Jordi
Turmo. 2005. Semantic Role Labeling Using Com-
plete Syntactic Analysis. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 221?224, Ann
Arbor, Michigan, June.
132
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134?140,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Phrase-based Translation Systems for WMT13:
System Combination with Morphology Generation,
Domain Adaptation and Corpus Filtering
Llu??s Formiga?, Marta R. Costa-jussa`?, Jose? B. Marin?o?
Jose? A. R. Fonollosa?, Alberto Barro?n-Ceden?o??, Llu??s Ma`rquez?
?TALP Research Centre ?Facultad de Informa?tica
Universitat Polite`cnica de Catalunya Universidad Polite?cnica de Madrid
Barcelona, Spain Madrid, Spain
{lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu
{albarron, lluism}@lsi.upc.edu
Abstract
This paper describes the TALP participa-
tion in the WMT13 evaluation campaign.
Our participation is based on the combi-
nation of several statistical machine trans-
lation systems: based on standard phrase-
based Moses systems. Variations include
techniques such as morphology genera-
tion, training sentence filtering, and do-
main adaptation through unit derivation.
The results show a coherent improvement
on TER, METEOR, NIST, and BLEU
scores when compared to our baseline sys-
tem.
1 Introduction
The TALP-UPC center (Center for Language and
Speech Technologies and Applications at Univer-
sitat Polite`cnica de Catalunya) focused on the En-
glish to Spanish translation of the WMT13 shared
task.
Our primary (contrastive) run is an internal
system selection comprised of different train-
ing approaches (without CommonCrawl, unless
stated): (a) Moses Baseline (Koehn et al,
2007b), (b) Moses Baseline + Morphology Gener-
ation (Formiga et al, 2012b), (c) Moses Baseline
+ News Adaptation (Henr??quez Q. et al, 2011),
(d) Moses Baseline + News Adaptation + Mor-
phology Generation , and (e) Moses Baseline +
News Adaptation + Filtered CommonCrawl Adap-
tation (Barro?n-Ceden?o et al, 2013). Our sec-
ondary run includes is the full training strategy
marked as (e) in the previous description.
The main differences with respect to our last
year?s participation (Formiga et al, 2012a) are: i)
the inclusion of the CommonCrawl corpus, using
a sentence filtering technique and the system com-
bination itself, and ii) a system selection scheme
to select the best translation among the different
configurations.
The paper is organized as follows. Section 2
presents the phrase-based system and the main
pipeline of our baseline system. Section 3 de-
scribes the our approaches to improve the baseline
system on the English-to-Spanish task (special at-
tention is given to the approaches that differ from
last year). Section 4 presents the system combi-
nation approach once the best candidate phrase of
the different subsystems are selected. Section 5
discusses the obtained results considering both in-
ternal and official test sets. Section 6 includes con-
clusions and further work.
2 Baseline system: Phrase-Based SMT
Our contribution is a follow up of our last year par-
ticipation (Formiga et al, 2012a), based on a fac-
tored Moses from English to Spanish words plus
their Part-of-Speech (POS). Factored corpora aug-
ments words with additional information, such as
POS tags or lemmas. In that case, factors other
than surface (e.g. POS) are usually less sparse, al-
lowing the construction of factor-specific language
models with higher-order n-grams. Such language
models can help to obtain syntactically more cor-
rect outputs.
We used the standard models available in Moses
as feature functions: relative frequencies, lexi-
cal weights, word and phrase penalties, wbe-msd-
bidirectional-fe reordering models, and two lan-
guage models (one for surface and one for POS
tags). Phrase scoring was computed using Good-
Turing discounting (Foster et al, 2006).
As aforementioned, we developed five factored
Moses-based independent systems with different
134
approaches. We explain them in Section 3. As
a final decision, we applied a system selection
scheme (Formiga et al, 2013; Specia et al, 2010)
to consider the best candidate for each sentence,
according to human trained quality estimation
(QE) models. We set monotone reordering of
the punctuation signs for the decoding using the
Moses wall feature.
We tuned the systems using the Moses
MERT (Och, 2003) implementation. Our focus
was on minimizing the BLEU score (Papineni et
al., 2002) of the development set. Still, for ex-
ploratory purposes, we tuned configuration (c) us-
ing PRO (Hopkins and May, 2011) to set the ini-
tial weights at every iteration of the MERT algo-
rithm. However, it showed no significant differ-
ences compared to the original MERT implemen-
tation.
We trained the baseline system using all
the available parallel corpora, except for
common-crawl. That is, European Parlia-
ment (EPPS) (Koehn, 2005), News Commentary,
and United Nations. Regarding the monolingual
data, there were more News corpora organized
by years for Spanish. The data is available at
the Translation Task?s website1. We used all
the News corpora to busld the language model
(LM). Firstly, a LM was built for every corpus
independently. Afterwards, they were combined
to produce de final LM.
For internal testing we used the News 2011 and
News 2012 data and concatenated the remaining
three years of News data as a single parallel corpus
for development.
We processed the corpora as in our participa-
tion to WMT12 (Formiga et al, 2012a). Tok-
enization and POS-tagging in both Spanish and
English was obtained with FreeLing (Padro? et al,
2010). Stemming was carried out with Snow-
ball (Porter, 2001). Words were conditionally case
folded based on their POS: proper nouns and ad-
jectives were separated from other categories to
determine whether a string should be fully folded
(no special property), partially folded (noun or ad-
jective) or not folded at all in (acronym).
Bilingual corpora was filtered with the clean-
corpus-n script of Moses (Koehn et al, 2007a), re-
moving those pairs in which a sentence was longer
than 70. For the CommonCrawl corpus we used a
more complex filtering step (cf. Section 3.3).
1http://www.statmt.org/wmt13/translation-task.html
Postprocessing included two special scripts to
recover contractions and clitics. Detruecasing was
done forcing the capitals after the punctuation
signs. Furthermore we used an additional script in
order to check the casing of output names with re-
spect to the source. We reused our language mod-
els and alignments (with stems) from WMT12.
3 Improvement strategies
We tried three different strategies to improve the
baseline system. Section 3.1 shows a strategy
based on morphology simplification plus genera-
tion. Its aim is dealing with the problems raised
by morphology-rich languages, such as Spanish.
Section 3.2 presents a domain?adaptation strategy
that consists of deriving new units. Section 3.3
presents an advanced strategy to filter the good bi-
sentences from the CommonCrawl corpus, which
might be useful to perform the domain adaptation.
3.1 Morphology generation
Following the success of our WMT12 participa-
tion (Formiga et al, 2012a), our first improve-
ment is based on the morphology generalization
and generation approach (Formiga et al, 2012b).
We focus our strategy on simplifying verb forms
only.
The approach first translates into Spanish sim-
plified forms (de Gispert and Marin?o, 2008). The
final inflected forms are predicted through a mor-
phology generation step, based on the shallow
and deep-projected linguistic information avail-
able from both source and target language sen-
tences.
Lexical sparseness is a crucial aspect to deal
with for an open-domain robust SMT when trans-
lating to morphology-rich languages (e.g. Span-
ish) . We knew beforehand (Formiga et al, 2012b)
that morphology generalization is a good method
to deal with generic translations and it provides
stability to translations of the training domain.
Our morphology prediction (generation) sys-
tems are trained with the WMT13 corpora (Eu-
roparl, News, and UN) together with noisy data
(OpenSubtitles). This combination helps to obtain
better translations without compromising the qual-
ity of the translation models. These kind of mor-
phology generation systems are trained with a rel-
atively short amount of parallel data compared to
standard SMT training corpora.
Our main enhancement to this strategy is the
135
addition of source-projected deep features to the
target sentence in order to perform the morphol-
ogy prediction. These features are Dependency
Features and Semantic Role Labelling, obtained
from the source sentence through Lund Depen-
dency Parser2. These features are then projected
to the target sentence as explained in (Formiga et
al., 2012b).
Projected deep features are important to pre-
dict the correct verb morphology from clean and
fluent text. However, the projection of deep fea-
tures is sentence-fluency sensitive, making it un-
reliable when the baseline MT output is poor. In
other words, the morphology generation strategy
becomes more relevant with high-quality MT de-
coders, as their output is more fluent, making the
shallow and deep features more reliable classifier
guides.
3.2 Domain Adaptation through pivot
derived units
Usually the WMT Translation Task focuses on
adapting a system to a news domain, offering an
in-domain parallel corpus to work with. How-
ever this corpus is relatively small compared to
the other corpora. In our previous participation
we demonstrated the need of performing a more
aggressive domain adaptation strategy. Our strat-
egy was based on using in-domain parallel data to
adapt the translation model, but focusing on the
decoding errors that the out-of-domain baseline
system makes when translating the in-domain cor-
pus.
The idea is to identify the system mistakes and
use the in-domain data to learn how to correct
them. To that effect, we interpolate the transla-
tion models (phrase and lexical reordering tables)
with a new adapted translation model with derived
units. We obtained the units identifying the mis-
matching parts between the non-adapted transla-
tion and the actual reference (Henr??quez Q. et al,
2011). This derivation approach uses the origi-
nal translation as a pivot to find a word-to-word
alignment between the source side and the target
correction (word-to-word alignment provided by
Moses during decoding).
The word-to-word monolingual alignment be-
tween output translation target correction was ob-
tained combining different probabilities such as
i)lexical identity, ii) TER-based alignment links,
2http://nlp.cs.lth.se/software/
Corpus Sent. Words Vocab. avg.len.
Original EN 1.48M 29.44M 465.1k 19.90ES 31.6M 459.9k 21.45
Filtered EN 0.78M 15.3M 278.0k 19.72ES 16.6M 306.8k 21.37
Table 1: Commoncrawl corpora statistics for
WMT13 before and after filtering.
iii) lexical model probabilities, iv) char-based Lev-
enshtein distance between tokens and v) filtering
out those alignments from NULL to a stop word
(p = ??).
We empirically set the linear interpolation
weight as w = 0.60 for the baseline translation
models and w = 0.40 for the derived units trans-
lations models. We applied the pivot derived units
strategy to the News domain and to the filtered
Commoncrawl corpus (cf. Section 5). The proce-
dure to filter out the Commoncrawl corpus is ex-
plained next.
3.3 CommonCrawl Filtering
We used the CommonCrawl corpus, provided for
the first time by the organization, as an impor-
tant source of information for performing aggres-
sive domain adaptation. To decrease the impact
of the noise in the corpus, we performed an auto-
matic pre-selection of the supposedly more correct
(hence useful) sentence pairs: we applied the au-
tomatic quality estimation filters developed in the
context of the FAUST project3. The filters? pur-
pose is to identify cases in which the post-editions
provided by casual users really improve over auto-
matic translations.
The adaptation to the current framework is as
follows. Example selection is modelled as a bi-
nary classification problem. We consider triples
(src, ref , trans), where src and ref stand for the
source-reference sentences in the CommonCrawl
corpus and trans is an automatic translation of the
source, generated by our baseline SMT system. A
triple is assigned a positive label iff ref is a bet-
ter translation from src than trans. That is, if the
translation example provided by CommonCrawl is
better than the output of our baseline SMT system.
We used four feature sets to characterize the
three sentences and their relationships: sur-
face, back-translation, noise-based and similarity-
based. These features try to capture (a) the simi-
larity between the different texts on the basis of
3http://www.faust-fp7.eu
136
diverse measures, (b) the length of the different
sentences (including ratios), and (c) the likelihood
of a source or target text to include noisy text.4
Most of them are simple, fast-calculation and
language-independent features. However, back-
translation features require that trans and ref are
back-translated into the source language. We did
it by using the TALP es-en system from WMT12.
Considering these features, we trained lin-
ear Support Vector Machines using SVMlight
(Joachims, 1999). Our training collection was the
FFF+ corpus, with +500 hundred manually anno-
tated instances (Barro?n-Ceden?o et al, 2013). No
adaptation to CommonCrawl was performed. To
give an idea, classification accuracy over the test
partition of the FFF+ corpus was only moderately
good (?70%). However, ranking by classification
score a fresh set of over 6,000 new examples, and
selecting the top ranked 50% examples to enrich a
state-of-the-art SMT system, allowed us to signifi-
cantly improve translation quality (Barro?n-Ceden?o
et al, 2013).
For WMT13, we applied these classifiers to
rank the CommonCrawl translation pairs and then
selected the top 53% instances to be processed by
the domain adaptation strategy. Table 1 displays
the corpus statistics before and after filtering.
4 System Combination
We approached system combination as a system
selection task. More concretely, we applied Qual-
ity Estimation (QE) models (Specia et al, 2010;
Formiga et al, 2013) to select the highest qual-
ity translation at sentence level among the trans-
lation candidates obtained by our different strate-
gies. The QE models are trained with human
supervision, making use of no system-dependent
features.
In a previous study (Formiga et al, 2013),
we showed the plausibility of building reliable
system-independent QE models from human an-
notations. This type of task should be addressed
with a pairwise ranking strategy, as it yields bet-
ter results than an absolute quality estimation ap-
proach (i.e., regression) for system selection. We
also found that training the quality estimation
models from human assessments, instead of au-
tomatic reference scores, helped to obtain better
4We refer the interested reader to (Barro?n-Ceden?o et al,
2013) for a detailed description of features, process, and eval-
uation.
models for system selection for both i) mimicking
the behavior of automatic metrics and ii) learning
the human behavior when ranking different trans-
lation candidates.
For training the QE models we used the data
from the WMT13 shared task on quality estima-
tion (System Selection Quality Estimation at Sen-
tence Level task5), which contains the test sets
from other WMT campaigns with human assess-
ments. We used five groups of features, namely:
i) QuestQE: 17 QE features provided by the Quest
toolkit6; ii) AsiyaQE: 26 QE features provided by
the Asiya toolkit for MT evaluation (Gime?nez and
Ma`rquez, 2010a); iii) LM (and LM-PoS) perplex-
ities trained with monolingual data; iv) PR: Clas-
sical lexical-based measures -BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Denkowski and Lavie, 2011)- computed
with a pseudo-reference approach, that is, using
the other system candidates as references (Sori-
cut and Echihabi, 2010); and v) PROTHER: Ref-
erence based metrics provided by Asiya, including
GTM, ROUGE, PER, TER (Snover et al, 2008),
and syntax-based evaluation measures also with a
pseudo-reference approach.
We trained a Support Vector Machine ranker by
means of pairwise comparison using the SVMlight
toolkit (Joachims, 1999), but with the ?-z p? pa-
rameter, which can provide system rankings for
all the members of different groups. The learner
algorithm was run according to the following pa-
rameters: linear kernel, expanding the working set
by 9 variables at each iteration, for a maximum of
50,000 iterations and with a cache size of 100 for
kernel evaluations. The trade-off parameter was
empirically set to 0.001.
Table 2 shows the contribution of different fea-
ture groups when training the QE models. For
evaluating performance, we used the Asiya nor-
malized linear combination metric ULC (Gime?nez
and Ma`rquez, 2010b), which combines BLEU,
NIST, and METEOR (with exact, paraphrases and
synonym variants). Within this scenario, it can
be observed that the quality estimation features
(QuestQE and AsiyaQE) did not obtain good re-
sults, perhaps because of the high similarity be-
tween the test candidates (Moses with different
configurations) in contrast to the strong differ-
ence between the candidates in training (Moses,
5http://www.quest.dcs.shef.ac.uk/wmt13 qe.html
6http://www.quest.dcs.shef.ac.uk
137
Features Asiya ULCWMT?11 WMT?12 AVG WMT?13
QuestQE 60.46 60.64 60.55 60.06
AsiyaQE 61.04 60.89 60.97 60.29
QuestQE+AsiyaQE 60.86 61.07 60.96 60.42
LM 60.84 60.63 60.74 60.37
QuestQE+AsiyaQE+LM 60.80 60.55 60.67 60.21
QuestQE+AsiyaQE+PR 60.97 61.12 61.05 60.54
QuestQE+AsiyaQE+PR+PROTHER 61.05 61.19 61.12 60.69
PR 61.24 61.08 61.16 61.04
PR+PROTHER 61.19 61.16 61.18 60.98
PR+PROTHER+LM 61.11 61.29 61.20 61.03
QuestQE+AsiyaQE+PR+PROTHER+LM 60.70 60.88 60.79 60.14
Table 2: System selection scores (ULC) obtained using QE models trained with different groups of
features. Results displayed for WMT11, WMT12 internal tests, their average, and the WMT13 test
EN?ES BLEU TER
wmt13 Primary 29.5 0.586
wmt13 Secondary 29.4 0.586
Table 4: Official automatic scores for the WMT13
English?Spanish translations.
RBMT, Jane, etc.). On the contrary, the pseudo-
reference-based features play a crucial role in the
proper performance of the QE model, confirming
the hypothesis that PR features need a clear dom-
inant system to be used as reference. The PR-
based configurations (with and without LM) had
no big differences between them. We choose the
best AVG result for the final system combination:
PR+PROTHER+LM, which it is consistent with
the actual WMT13 evaluated afterwards.
5 Results
Evaluations were performed considering different
quality measures: BLEU, NIST, TER, and ME-
TEOR in addition to an informal manual analy-
sis. This manifold of metrics evaluates distinct as-
pects of the translation. We evaluated both over
the WMT11 and WMT12 test sets as internal in-
dicators of our systems. We also give our perfor-
mance on the WMT13 test dataset.
Table 3 presents the obtained results for the
different strategies: (a) Moses Baseline (w/o
commoncrawl) (b) Moses Baseline+Morphology
Generation (w/o commoncrawl) (c) Moses Base-
line+News Adaptation through pivot based align-
ment (w/o commoncrawl) (d) Moses Baseline +
News Adaptation (b) + Morphology Generation
(c) (e) Moses Baseline + News Adaptation (b) +
Filtered CommonCrawl Adaptation.
The official results are in Table 4. Our primary
(contrastive) run is the system combination strat-
egy whereas our secondary run is the full training
strategy marked as (e) on the system combination.
Our primary system was ranked in the second clus-
ter out of ten constrained systems in the official
manual evaluation.
Independent analyzes of the improvement
strategies show that the highest improvement
comes from the CommonCrawl Filtering + Adap-
tation strategy (system e). The second best strat-
egy is the combination of the morphology pre-
diction system plus the news adaptation system.
However, for the WMT12 test the News Adap-
tation strategy contributes to main improvement
whereas for the WMT13 this major improvement
is achieved with the morphology strategy. Analyz-
ing the distance betweem each test set with respect
to the News and CommonCrawl domain to further
understand the behavior of each strategy seems an
interesting future work. Specifically, for further
contrasting the difference in the morphology ap-
proach, it would be nice to analyze the variation in
the verb inflection forms. Hypothetically, the per-
son or the number of the verb forms used may have
a higher tendency to be different in the WMT13
test set, implying that our morphology approach is
further exploited.
Regarding the system selection step (internal
WMT12 test), the only automatic metric that has
an improvement is TER. However, TER is one of
138
EN?ES BLEU NIST TER METEOR
wmt12 Baseline 32.97 8.27 49.27 49.91
wmt12 + Morphology Generation 33.03 8.29 49.02 50.01
wmt12 + News Adaptation 33.22 8.31 49.00 50.16
wmt12 + News Adaptation + Morphology Generation 33.29 8.32 48.83 50.29
wmt12 + News Adaptation + Filtered CommonCrawl Adaptation 33.61 8.35 48.82 50.52
wmt12 System Combination 33.43 8.34 48.78 50.44
wmt13 Baseline 29.02 7.72 51.92 46.96
wmt13 Morphology Generation 29.35 7.73 52.04 47.04
wmt13 News Adaptation 29.19 7.74 51.91 47.07
wmt13 News Adaptation + Morphology Generation 29.40 7.74 51.96 47.12
wmt13 News Adaptation + Filtered CommonCrawl Adaptation 29.47 7.77 51.82 47.22
wmt13 System Combination 29.54 7.77 51.76 47.34
Table 3: Automatic scores for English?Spanish translations.
the most reliable metrics according to human eval-
uation. Regarding the actual WMT13 test, the sys-
tem selection step is able to overcome all the auto-
matic metrics.
6 Conclusions and further work
This paper described the TALP-UPC participa-
tion for the English-to-Spanish WMT13 transla-
tion task. We applied the same systems as in last
year, but enhanced with new techniques: sentence
filtering and system combination.
Results showed that both approaches performed
better than the baseline system, being the sentence
filtering technique the one that most improvement
reached in terms of all the automatic quality indi-
cators: BLEU, NIST, TER, and METEOR. The
system combination was able to outperform the
independent systems which used morphological
knowledge and/or domain adaptation techniques.
As further work would like to focus on further
advancing on the morphology-based techniques.
Acknowledgments
This work has been supported in part by
Spanish Ministerio de Econom??a y Competitivi-
dad, contract TEC2012-38939-C03-02 as well
as from the European Regional Development
Fund (ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the fol-
lowing grants: 247762 (FAUST, FP7-ICT-2009-
4-247762), 29951 (the International Outgoing
Fellowship Marie Curie Action ? IMTraP-2011-
29951) and 246016 (ERCIM ?Alain Bensoussan?
Fellowship).
References
Alberto Barro?n-Ceden?o, Llu??s Ma`rquez, Carlos A.
Henr??quez Q, Llu??s Formiga, Enrique Romero, and
Jonathan May. 2013. Identifying Useful Hu-
man Correction Feedback from an On-line Machine
Translation Service. In Proceedings of the Twenty-
Third International Joint Conference on Artificial
Intelligence. AAAI Press.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication, 50(11-12):1034?
1046.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012a. The TALP-UPC phrase-
based translation systems for WMT12: Morphol-
ogy simplification and domain adaptation. In Pro-
ceedings of the Seventh Workshop on Statistical
Machine Translation, pages 275?282, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Llu??s Formiga, Adolfo Herna?ndez, Jose? B. Marin?, and
Enrique Monte. 2012b. Improving english to
spanish out-of-domain translations by morphology
generalization and generation. In Proceedings of
139
the AMTA Monolingual Machine Translation-2012
Workshop.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 53?61, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
measures for automatic machine translation evalu-
ation. Machine Translation, 24(3-4):209?240, De-
cember.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using
small additional corpora. In Proceedings of the 15th
Conference of the European Association for Ma-
chine Translation.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007a. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007b. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010), La Val-
letta, MALTA, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
140
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 359?364,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Approach to System Selection: ASIYA Features
and Pairwise Classification using Random Forests
Llu??s Formiga1, Meritxell Gonza`lez1, Alberto Barro?n-Ceden?o1,2
Jose? A. R. Fonollosa1 and Llu??s Ma`rquez1
1 TALP Research Center, Universitat Polite`cnica de Catalunya, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid, Spain
{lluis.formiga,jose.fonollosa}@upc.edu, {mgonzalez,albarron,lluism}@lsi.upc.edu
Abstract
This paper describes the TALP-UPC par-
ticipation in the WMT?13 Shared Task
on Quality Estimation (QE). Our partic-
ipation is reduced to task 1.2 on System
Selection. We used a broad set of fea-
tures (86 for German-to-English and 97
for English-to-Spanish) ranging from stan-
dard QE features to features based on
pseudo-references and semantic similarity.
We approached system selection by means
of pairwise ranking decisions. For that,
we learned Random Forest classifiers es-
pecially tailored for the problem. Evalua-
tion at development time showed consider-
ably good results in a cross-validation ex-
periment, with Kendall?s ? values around
0.30. The results on the test set dropped
significantly, raising different discussions
to be taken into account.
1 Introduction
In this paper we discuss the TALP-UPC1 partici-
pation in the WMT?13 Shared Task on Quality Es-
timation (QE). Our participation is circumscribed
to task 1.2, which deals with System Selection.
Concretely, we were required to rank up to five al-
ternative translations for the same source sentence
produced by multiple MT systems, in the absence
of any reference translation.
We used a broad set of features; mainly avail-
able through the last version of the ASIYA toolkit
for MT evaluation2 (Gime?nez and Ma`rquez,
2010). Concretely, we derived 86 features for
the German-to-English subtask and 97 features for
English-to-Spanish. These features cover different
approaches and include standard Quality Estima-
tion features, as provided by the above mentioned
1Center for Language and Speech Technologies and Ap-
plications (TALP), Technical University of Catalonia (UPC).
2http://asiya.lsi.upc.edu
ASIYA toolkit and Quest (Specia et al, 2010),
but also a variety of features based on pseudo-
references (Soricut and Echihabi, 2010), explicit
semantic analysis (Gabrilovich and Markovitch,
2007) and specialized language models. See sec-
tion 3 for details.
In order to model the ranking problem associ-
ated to the system selection task, we adapted it
to a classification task of pairwise decisions. We
trained Random Forest classifiers (and compared
them to SVM classifiers), expanding the work of
Formiga et al (2013), from which a full ranking
can be derived and the best system per sentence
identified.
Evaluation at development time, using cross-
validation, showed considerably good and stable
results for both language pairs, with correlation
values around 0.30 (Kendall ? coefficient) classi-
fication accuracies around 52% (pairwise classifi-
cation) and 41% (best translation identification).
Unfortunately, the results on the test set were sig-
nificantly lower. Current research is devoted to ex-
plain the behavior of the system at testing time. On
the one hand, it seems clear that more research re-
garding the assignment of ties is needed in order
to have a robust model. On the other hand, the re-
lease of the gold standard annotations for the test
set will facilitate a deeper analysis and understand-
ing of the current results.
The rest of the paper is organized as follows.
Section 2 describes the ranking models studied for
the system selection problem. Section 3 describes
the features used for learning. Section 4 presents
the setting for parameter optimization and feature
selection and the results obtained. Finally, Sec-
tion 5 summarizes the lessons learned so far and
outlines some lines for further research.
2 Ranking Model
We considered two learning strategies to obtain the
best translation ranking model: SVM and Random
359
Forests. Both strategies were based on predicting
pairwise quality ranking decisions by means of su-
pervised learning. These decision was motivated
from our previous work (Formiga et al, 2013)
were we learned that they were more consistent to
select the best system (according to human and au-
tomatic metrics) compared to absolute regression
approaches. In that work we used only the subset
of features 1, 2, 3 and 8 described in Section 3.
For this shared task we have introduced additional
similarity measures (subsets 4 to 7) that feature se-
mantic analysis and automatic alignments between
the source and the translations.
The rationale for transforming a ranking prob-
lem to a pairwise classification problem has been
described previously in several work (Joachims,
2002; Burges et al, 2005). The main idea is to en-
semble the features of both individuals and assign
a class {-1,1} which tries to predict the pairwise
relation among them. For linear based approach
this adaptation is as simple to compute the differ-
ence between features between all the pairs of the
training data.
We used two different learners to perform that
task. First, we trained a Support Vector Machine
ranker by means of pairwise comparison using
the SVMlight toolkit (Joachims, 1999), but with
the ?-z p? parameter, which can provide system
rankings for all the members of different groups.
The learner algorithm was run according to the
following parameters: RBF-kernel, expanding the
working set by 9 variables at each iteration, for a
maximum of 50,000 iterations and with a cache
size of 100 for kernel evaluations. The trade-off
parameter was empirically set to 0.001. This im-
plementation ignores the ties for the training step
as it only focuses in better than/ worse than rela-
tions.
Secondly, we used Random Forests (Breiman,
2001), the rationale was the same as ranking-to-
pairwise implementation from SVMlight. How-
ever, SVMlight considers two different data pre-
processing methods depending on the kernel of
the classifier: LINEAR and RBF-Kernel. We
used the same data-preprocessing algorithm from
SVMlight in order to train a Random Forest clas-
sifier with ties (three classes: {0,-1,1}) based
upon the pairwise relations. We used the Random
Forests implementation of scikit-learn toolkit (Pe-
dregosa et al, 2011) with 50 estimators.
Once the classes are given by the Random For-
est, we build a graph by means of the adjacency
matrix of the pairwise decision. Once the adja-
cency matrix has been built, we assign the final
ranking through a dominance scheme similar to
Pighin et al (2012). In that case, however, there
are not topological problems as the pairwise rela-
tions are complete across all the edges.
3 Features Sets
We considered a broad set of features: 97 and
86 features for English-to-Spanish (en-es) and
German-to-English (de-en), respectively. We
grouped them into the following categories: base-
line QE metrics, comparison against pseudo-
references, source-translation, and adapted lan-
guage models. We describe them below. Unless
noted otherwise, the features apply to both lan-
guage pairs.
3.1 Baseline Features
The baseline features are composed of well-known
quality estimation metrics:
1. Quest Baseline (QQE)
Seventeen baseline features from Specia et
al. (2010). This set includes token counts
(and their ratio), LM probabilities for source
and target sentences, percentage of n-grams
in different quartiles of a reference corpus,
number of punctuation marks, and fertility
ratios. We used these features in the en-es
partition only.
2. ASIYA?s QE-based features (AQE)
Twenty-six QE features provided by
ASIYA (Gonza`lez et al, 2012), comprising
bilingual dictionary ambiguity and overlap;
ratios concerning chunks, named-entities and
PoS; source and candidate LM perplexities
and inverse perplexities over lexical forms,
chunks and PoS; and out-of-vocabulary word
indicators.
3.2 Pseudo-Reference-based Features
Soricut and Echihabi (2010) introduced the con-
cept of pseudo-reference-based features (PR) for
translation ranking estimation. The principle is
that, in the lack of human-produced references,
automatic ones are still good for differentiating
good from bad translations. One or more sec-
ondary MT systems are required to generate trans-
lations starting from the same input, which are
360
taken as pseudo-references. The similarity to-
wards the pseudo-references can be calculated
with any evaluation measure or text similarity
function, which gives us all feature variants in this
group. We consider the following PR-based fea-
tures:
3. Derived from ASIYA?s metrics (APR)
Twenty-three PR features, including GTM-l
(l?{1,2,3}) to reward different length match-
ing (Melamed et al, 2003), four variants of
ROUGE (-L, -S*, -SU* and -W) (Lin and
Och, 2004), WER (Nie?en et al, 2000),
PER (Tillmann et al, 1997), TER, and
TERbase (i.e., without stemming, synonymy
look-up, nor paraphrase support) (Snover et
al., 2009), and all the shallow and full pars-
ing measures (i.e., constituency and depen-
dency parsing, PoS, chunking and lemmas)
that ASIYA provides either for Spanish or En-
glish as target languages.
4. Lexical similarity (NGM)
Cosine and Jaccard coefficient similarity
measures for both token and character
n-grams considering n ? [2, 5] (i.e., sixteen
features). Additionally, one Jaccard-based
similarity measure for ?pseudo-prefixes?
(considering only up to four initial characters
for every token).
5. Based on semantic information (SEM)
Twelve features calculated with named
entity- and semantic role-based evaluation
measures (again, provided by ASIYA). Sen-
tences are automatically annotated using
SwiRL (Surdeanu and Turmo, 2005) and
BIOS (Surdeanu et al, 2005). We used these
features in the de-en subtask only.
6. Explicit semantic analysis (ESA)
Two versions of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), a
semantic similarity measure, built on top of
Wikipedia (we used the opening paragraphs
of 100k Wikipedia articles as in 2010).
3.3 Source-Translation Extra Features
Source-translation features include explicit com-
parisons between the source sentence and its trans-
lation. They are meant to measure how adequate
the translation is, that is, to what extent the trans-
lation expresses the same meaning as the source.
Note that a considerable amount of the features
described in the baseline group (QQE and AQE)
fall in this category. In this subsection we include
some extra features we devised to capture source?
translation dependencies.
7. Alignment-based features (ALG / ALGPR)
One measure calculated over the aligned
words between a candidate translation and
the source (ALG); and two measures based on
the comparison between these alignments for
two different translations (e.g., candidate and
pseudo-reference) and the source (ALGPR).3
8. Length model (LeM)
A measure to estimate the quality likeli-
hood of a candidate sentence by considering
the ?expected length? of a proper translation
from the source. The measure was introduced
by (Pouliquen et al, 2003) to identify docu-
ment translations. We estimated its param-
eters over standard MT corpora, including
Europarl, Newswire, Newscommentary and
UN.
3.4 Adapted Language-Model Features
We interpolated different language models com-
prising the WMT?12 Monolingual corpora (EPPS,
News, UN and Gigafrench for English). The in-
terpolation weights were computed as to minimize
the perplexity according to the WMT Translation
Task test data (2008-2010)4. The features are as
follow:
9. Language Model Features (LM)
Two log-probabilities of the translation can-
didate with respect to the above described in-
terpolated language models over word forms
and PoS labels.
4 Experiments and Results
In this section we describe the experiments car-
ried out to select the best feature set, learner, and
learner configuration. Additionally, we present
the final performance within the task. The set-
up experiments were addressed doing two separate
10-fold cross validations on the training data and
averaging the final results. We evaluated the re-
sults through three indicators: Kendall?s ? with no
3Alignments were computed with the Berkeley aligner
https://code.google.com/p/berkeleyaligner/
4http://www.statmt.org/wmt13/translation-task.html
361
penalization for the ties, accuracy in determining
the pairwise relationship between candidate trans-
lations, and global accuracy in selecting the best
candidate for each source sentence.
First, we compared our SVM learner against
Random Forests with the two variants of data
preprocessing (LINEAR and RBF). In terms of
Kendall?s ? , we found that the Random Forests
(RF) were clearly better compared to SVM imple-
mentation. Concretely, depending on the final fea-
ture set, we found that RF achieved a ? between
0.23 and 0.29 while SVM achieved a ? between
0.23 and 0.25. With respect to the accuracy mea-
sures we did not find noticeable differences be-
tween methods as their results moved from 49% to
52%. However, considering the accuracy in terms
of selecting only the best system there was a dif-
ference of two points (42.2% vs. 40.0%) between
methods, being RF again the best system. Regard-
ing the pairwise preprocessing the results between
RBF and LINEAR based preprocessing were com-
parable, being RBF slightly better than LINEAR.
Hence, we selected Random Forests with RBF
pairwise preprocessing as our final learner.
de-en ? with ties AccuracyIgnored Penalized All Best
AQE+LeM+ALGPR+LM 33.70 15.72 52.56 41.57
AQE+SEM+LM 32.49 14.61 52.72 40.92
AQE+LeM+ALGPR+ESA+LM 32.08 13.81 52.71 41.37
AQE+ALG+ESA+SEM+LM 32.06 13.96 52.20 40.64
AQE+ALG+LM 31.97 14.29 52.00 40.83
AQE+LeM+ALGPR+SEM+LM 31.93 13.57 52.52 40.98
AQE+ESA+SEM+LM 31.79 13.68 52.50 40.76
AQE+LeM+ALGPR+ESA+SEM+LM 31.72 14.01 52.65 40.83
AQE+ALG+SEM+LM 31.17 12.86 52.18 40.51
AQE+ALG+SEM 30.72 12.58 51.75 39.66
AQE+LeM+ALGPR+ESA+SEM 30.47 11.79 51.85 39.58
AQE+ESA+LM 30.31 12.23 52.60 40.69
AQE+ALG+ESA+LM 30.26 12.40 52.03 40.99
AQE+LeM+ALGPR 30.24 11.83 51.96 40.42
AQE+LeM+ALGPR+SEM 30.23 11.84 52.10 40.32
AQE+LeM+ALGPR+ESA 29.89 11.87 51.83 40.07
AQE+ALG+ESA 29.81 11.30 51.37 39.47
AQE+SEM 29.80 12.06 51.75 39.52
AQE+NGM+APR+ESA+SEM+LM 29.34 10.58 51.33 38.55
AQE+ESA+SEM 29.31 11.46 51.66 39.24
AQE+ESA 29.13 11.12 51.82 39.90
AQE+ALG+ESA+SEM 28.35 10.32 51.37 38.98
AQE+NGM+APR+ESA+SEM 27.55 9.22 51.01 38.12
Table 1: Set-up results for de-en
For the feature selection process, we considered
the most relevant combinations of feature groups.
Table 1 shows the set-up results for the de-en sub-
task and Table 2 shows the results for the en-es
subtask.
In terms of ? we observed similar results be-
tween the two language pairs. However accura-
cies for the de-en subtask were one point above
the ones for en-es. Regarding the features used, we
found that the best feature combination to use was
composed of: i) a baseline QE feature set (Asiya
or Quest) but not both of them, ii) Length Model,
iii) Pseudo-reference aligned based features and
the use of iv) adapted language models. However,
within the de-en subtask, we found that substitut-
ing Length Model and Aligned Pseudo-references
by the features based on Semantic Roles (SEM)
could bring marginally better accuracy. We also
noticed that the learner was sensitive to the fea-
tures used so selecting the appropriate set of fea-
tures was crucial to achieve a good performance.
en-es ? with ties AccuracyIgnored Penalized All Best
QQE+LeM+ALGPR+LM 33.81 15.87 51.66 41.01
AQE+LeM+ALGPR+LM 33.75 16.44 51.56 41.52
QQE+AQE+LM 32.71 14.59 51.18 41.02
QQE+AQE+LM+ESA 32.69 15.30 51.48 41.30
QQE+AQE+LeM+ALGPR+LM+ESA 32.63 13.64 51.39 40.48
QQE+AQE+LeM+ALGPR+LM 32.41 14.06 51.43 40.49
QQE+LeM+ALGPR+LM+ESA 31.66 13.39 51.37 41.05
QQE+AQE+ALG+LM 31.46 13.62 51.28 41.29
AQE+LeM+ALGPR+LM+ESA 31.29 14.10 51.55 41.43
QQE+AQE+ALG+LM+ESA 31.25 13.58 51.64 41.66
QQE+AQE+NGM+APR+LM+ESA 30.58 12.48 50.93 40.66
QQE+AQE+NGM+APR+LM 29.94 12.54 50.95 40.25
QQE+AQE 28.98 10.92 49.97 39.65
QQE+AQE+LeM+ALGPR 28.94 10.48 49.99 39.71
QQE+AQE+NGM+ESA+LM 28.85 11.88 50.90 40.22
AQE+LeM+ALGPR 28.81 10.11 50.06 40.01
QQE+AQE+ESA 28.68 10.31 49.96 39.27
AQE+ESA 28.67 10.81 50.35 39.18
AQE 28.65 10.68 49.76 38.90
QQE+AQE+ALG 28.47 9.63 49.67 39.66
QQE+AQE+NGM+APR+ESA 28.43 9.75 49.67 38.74
QQE+AQE+NGM 27.23 9.10 49.44 38.98
QQE+AQE+ALG+ESA 27.08 7.93 50.26 39.71
QQE+AQE+LeM+ALGPR+ESA 27.03 8.65 50.35 40.49
AQE+LeM+ALGPR+ESA 26.96 8.26 50.30 39.47
QQE+AQE+NGM+ESA 26.59 7.56 49.52 38.62
QQE+AQE+NGM+APR 25.39 6.97 49.90 39.53
Table 2: Setup results for en-es
de-en ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.31
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
Baseline Random-ranks-with-ties -0.12
Worst -0.49
Table 3: Official results for the de-en subtask (ties
penalized)
en-es ? (ties penalized,
ID non-symmetric between [-1,1])
Best 0.15
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
Baseline Random-ranks-with-ties -0.23
Worst -0.63
Table 4: Official results for the en-es subtask (ties
penalized)
In Tables 3, 4, 5 and 6 we present the official re-
sults for the WMT?13 Quality Estimation Task, in
all evaluation variants. In each table we compare
to the best/worst performing systems and also to
the official baseline.
We can observe that in general the results on
the test sets drop significantly, compared to our
362
de-en ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.31 882
UPC AQE+SEM+LM 0.27 768
UPC AQE+LeM+ALGPR+LM 0.24 788
Baseline Random-ranks-with-ties 0.08 718
Worst -0.03 558
Table 5: Official results for the de-en subtask (ties
ignored)
en-es ? (ties ignored, Non-ties
ID symmetric /between [-1,1]) (882 dec.)
Best 0.23 192
UPC QQE+LeM+ALGPR+LM 0.11 554
UPC AQE+LeM+ALGPR+LM 0.08 554
Baseline Random-ranks-with-ties 0.03 507
Worst -0.11 633
Table 6: Official results for the en-es subtask (ties
ignored)
set-up experiments. Restricting to the evaluation
setting in which ties are not penalized (i.e., cor-
responding to our setting during system and pa-
rameter tuning), we can see that the results corre-
sponding to de-en (Table 5) are comparable to our
set-up results and close to the best performing sys-
tem. However, in the en-es language pair the final
results are comparatively much lower (Table 6).
We find this behavior strange. In this respect, we
analyzed the inter-annotator agreement within the
gold standard. Concretely we computed the Co-
hen?s ? for all overlapping annotations concerning
at least 4 systems for both language pairs. The re-
sults of our analysis are presented in Table 7 and
therefore it confirms our hypothesis that en-es an-
notations had more noise providing an explanation
for the accuracy decrease of our QE models and
setting the subtask into a more challenging sce-
nario. However, further research will be needed to
analyze other factors such as oracles and improve-
ment on automatic metrics prediction and reliabil-
ity compared to linguistic expert annotators.
Another remaining issue for our research con-
cerns investigating better ways to deal with ties,
as their penalization lowered our results dramati-
cally. In this direction we plan to work further on
# of Lang Cohen?s # ofsystems ? elements
4 en-es 0.210 560de-en 0.369 640
5 en-es 0.211 130de-en 0.375 145
Table 7: Golden standard test set agreement coef-
ficients measured by Cohen?s ?
the adjacency matrix reconstruction heuristics and
presenting the features to the learner in a struc-
tured form.
5 Conclusions
This paper described the TALP-UPC participation
in the WMT?13 Shared Task. We approached the
Quality Estimation task based on system selection,
where different systems have to be ranked accord-
ing to their quality. We derive a full ranking and
identify the best system per sentence on the basis
of Random Forest classifiers.
After the model set-up, we observed consid-
erably good and robust results for both transla-
tion directions, German-to-English and English-
to-Spanish: Kendall?s ? around 0.30 as well as
accuracies around 52% on pairwise classification
and 41% on best translation identification. How-
ever, the results over the official test set were
significantly lower. We have found that the low
inter-annotator agreement between users on that
set might provide an explanation to the poor per-
formance of our QE models.
Our current efforts are centered on explaining
the behavior of our QE models when facing the of-
ficial test sets. We are following two directions: i)
studying the ties? impact to come out with a more
robust model and ii) revise the English-to-Spanish
gold standard annotations in terms of correlation
with automatic metrics to facilitate a deeper un-
derstanding of the results.
Acknowledgments
Acknowledgements
This work has been partially funded by the
Spanish Ministerio de Econom??a y Competitivi-
dad, under contracts TEC2012-38939-C03-02
and TIN2009-14675-C03, as well as from
the European Regional Development Fund
(ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the
following grants: 247762 (FAUST, FP7-ICT-
2009-4-247762) and 246016 (ERCIM ?Alain
Bensoussan? Fellowship).
References
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
363
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89?96. ACM.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Artifi-
cial Intelligence, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya: An
Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Meritxell Gonza`lez, Jesu?s Gime?nez, and Llu??s
Ma`rquez. 2012. A graphical interface for mt evalu-
ation and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM, editor, Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD).
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 605?612, Barcelona,
Spain, July.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and recall of machine translation.
In HLT-NAACL.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd Language Resources and
Evaluation Conference (LREC 2000).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python . Journal of Machine Learning Re-
search, 12:2825?2830.
Daniele Pighin, Llu??s Formiga, and Llu??s Ma`rquez.
2012. A graph-based strategy to streamline trans-
lation quality assessments. In Proceedings of the
Tenth Conference of the Association for Machine
Translation in the Americas (AMTA?2012), San
Diego, USA, October. AMTA.
Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.
2003. Automatic Identification of Document Trans-
lations in Large Multilingual Document Collections.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), pages 401?408, Borovets, Bulgaria.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: Paraphrase,
Semantic, and Alignment Enhancements to Trans-
lation Edit Rate. Machine Translation, 23(2):117?
127.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
Role Labeling Using Complete Syntactic Analysis.
In Proceedings of CoNLL Shared Task.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H Sawaf. 1997. Accelerated dp based search for
statistical translation. In Proceedings of European
Conference on Speech Communication and Technol-
ogy.
364
