Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Coupling a Linguistic Formalism and a Script Language 
 
 
 
  
 
Abstract 
This article presents a novel syntactic 
parser architecture, in which a linguistic 
formalism can be enriched with all sorts 
of constraints, included extra-linguistic 
ones, thanks to the seamless coupling of 
the formalism with a programming lan-
guage. 
1 Introduction 
The utilization of constraints in natural language 
parsers (see Blache and Balfourier,2001 or 
Tapanainen and J?rvinen , 1994) is central to 
most systems today. However, these constraints 
are often limited to purely linguistic features, 
such as linearity or dependency relations be-
tween categories within a given syntactic tree.  
Most linguistic formalisms have been created 
with the sole purpose of extracting linguistic in-
formation from bits and pieces of text. They usu-
ally use a launch and forget strategy, where a text 
is analyzed according to local constraints, dis-
played and then discarded to make room for the 
next block of text. These parsers take each sen-
tence as an independent input, on which gram-
mar rules are applied together with constraints. 
However, no sentence is independent of a text, 
and no text is really independent of extra-
linguistic information. In order to assess cor-
rectly the phrase President Bush, we need to 
know that Bush is a proper name, whose function 
is ?President?. Washington can be a town, a 
state, the name of a famous president, but also 
the name of an actor. Moreover, the analysis of a 
sentence is never an independent process, if 
President Bush is found in a text, the reference to 
president, later in the document will be related to 
this phrase.  
These problems are certainly not new and a 
dense literature has been written about how to 
better deal with these issues.  However, most 
solutions rely on formalism enrichments with 
solutions ?engraved in stone?, that makes it diffi-
cult to adapt a grammar to new domains (see De-
clerck, 2002, or Roux, 2004), even though they 
use XML representation or database to store 
huge amounts of extra-linguistic information. 
The interpretation of these data is intertwined 
into the very fabric of the parser and requires 
deep modifications to use new sources with a 
complete different DTD.  
Of course, there are many other ways to solve 
these problems. For instance, in the case of lan-
guages such as Prolog or Lisp, the grammar for-
malism is often indistinguishable from the pro-
gramming language itself. For these parsers, the 
querying of external information is easily solved 
as grammar rules can be naturally augmented 
with non-linguistic procedures that are written in 
the same language. In other cases, when the 
parser is independent from any specific pro-
gramming languages, the problem can prove dif-
ficult to solve. The formalism can of course be 
augmented with new instructions to tackle the 
querying of external information. However the 
time required to enrich the parser language may 
not be worth the effort, as the development of a 
complete new instruction set is a heavy and 
complex task that is loosely related to linguistic 
parser programming. 
We propose in this article a new way of building 
natural language parsers, with the coupling of a 
script language with an already rich linguistic 
formalism. 
2 Scripting 
The system we describe in this article mix a 
natural language parser, namely Xerox Incre-
mental Parser (XIP hereafter, see A?t-Mohktar et 
al., 2002, Roux, 1999) with a scripting language, 
in our case Python. The interaction of a grammar 
with scripting instructions is almost as old as 
Claude Roux 
Xerox Research Centre Europe/ 6, 
chemin de Maupertuis, 38240 Meylan, 
France 
Claude.roux@xrce.xerox.com 
33
computational linguistics. Pereira and Shieber for 
instance, in their book: Prolog and Natural Lan-
guage Processing (see Pereira and Shieber, 1987) 
already suggested mixing grammar rules with 
extra-linguistic information. This m?lange was 
made possible thanks to the homogeneity be-
tween grammar rules and the external code, writ-
ten in both cases in the same programming lan-
guage. However, these programming languages 
are not exactly tuned to do linguistic analyses; 
they are often slow and cumbersome. Moreover, 
they blur the boundary between program and 
grammar rules, as the programming language is 
both the algorithm language and the rule lan-
guage. Allen (see Allen, 1994) proposes a differ-
ent approach in his TRAINS Parsing system. The 
grammar formalism is independent to a certain 
extent from the implementation language, which 
is LISP in this case. However, since the grammar 
is translated into a LISP program, it is easy for a 
linguist to specialize the generated rules with 
external LISP procedures. Nevertheless, the 
grammar formalism remains very close to LISP 
data description, which makes the grammar rules 
somewhat difficult to read.  
The other solution, which is usually favored by 
computational linguists, is to store the external 
information in databases, which are accessed 
with some pre-defined instructions and translated 
into linguistic features. For instance (see De-
clerk, 2002 or Roux, 2004), the external informa-
tion is presented as an XML document whose 
DTD is defined once and for all. This DTD is 
then enriched with extra-linguistic information 
that a parser can exploit to guide rule application. 
This method alleviates the necessity of a com-
plex interaction mechanism between the parser 
and its external data sources. The XPath lan-
guage is used to query this document in order to 
retrieve salient information at parsing time, 
which is then translated into local linguistic fea-
tures. However, only static information can be 
exploited, as these XML databases must be built 
beforehand. 
Similar mechanisms have also been proposed in 
other architectures to help heterogeneous linguis-
tic modules to communicate through a common 
XML interface (see Cunningham et al,2002, 
Blache and Gu?not , 2003). These architectures 
are very powerful as they connect together tools 
that only need to comply with a common in-
put/output DTD.  Specialized Java modules can 
then be written which are applied to intermediate 
representations to add their own touch of extra-
linguistic data. Since, the intermediate represen-
tation is an XML document, the number of pos-
sible enrichments is almost limitless, as each 
module will only extract from this document the 
XML markup tags that it is designed to handle. 
However, since XML is by nature furiously ver-
bose, the overall system might be very slow as it 
might spend a large amount of time translating 
external XML representation into internal repre-
sentations.  
Furthermore, applications that require natural 
language processing also have different pur-
poses, different needs. They may require a shal-
low output, such as a simple tokenization with a 
whiff of tagging, or a much deeper analysis. Syn-
tactic parsing is usually integrated as a black box 
into these architectures, with little control left 
over the grammar execution, control which nev-
ertheless might prove very important in many 
cases. An XML document, for instance, often 
contains some specific markup tags to identify a 
title, a section or author name. If the parser is 
given some indications about the input, it could 
be guided through the grammar maze to favor 
these rules that are better suited to analyze a title 
for example.  
Finally, syntactic parsing, when it is limited to 
lexical information, often fails to assess correctly 
some ambiguous relations. Thus, the only way to 
deal with PP-attachment or anaphoric pronoun 
antecedents is to use both previous analyses and 
external information. However, most syntactic 
parsers are often ill geared to link with external 
modules. The formalism is engraved into a C 
program as in Link Grammar (see Grinberg et 
al.,1995) or as in Sylex (see Constant, 1995) 
which offers little or no opening to the rest of the 
world, as it is mainly designed to accomplish one 
unique task. We will show how the seamless in-
tegration of a script language into the very fabric 
of the formalism simplifies the task of keeping 
track of previous analyses together with the use 
of external sources of data. 
3 Xerox Incremental Parser (XIP) 
The XIP engine has been developed by a re-
search team in computational linguistics at the 
Xerox Research Centre Europe (see A?t-Mokhtar 
et al, 2001). It has been designed from the be-
ginning to follow a strictly incremental strategy, 
where rules apply one after the other. There is 
only one analysis path that is followed for a 
given linguistic unit (phrase, sentence or even 
paragraph): the failure of a rule does not prevent 
the whole analysis from continuing to comple-
34
tion. Since the system never backtracks on any 
rules, XIP cannot propel itself into a combinato-
rial explosion. 
 
XIP can be divided into two main components: 
? A component that builds a chunk tree on 
the basis of lexical nodes. 
? A component that creates functions or 
dependencies that connect together distant 
nodes from the chunk tree. 
 
The central goal of this parser is the extraction of 
dependencies. A dependency is a function that 
connects together distant nodes within a chunk 
tree. The system constructs a dependency be-
tween two nodes, if these two nodes are in a spe-
cific configuration within the chunk tree or if a 
specific set of dependencies has already been 
extracted for some of these nodes (see Hagege 
and Roux, 2002).  The notion of constraint em-
bedded in XIP is both configurational and Boo-
lean. The configuration part is based on tree 
regular rules which express constraints over node 
configuration, while the Boolean constraints are 
expressed over dependencies. 
3.1 Three Level of Analysis 
The parsing is done in three different stages: 
 
? Part-of-speech disambiguation and 
chunking. 
? Dependency Extraction between words 
on the basis of sub-tree patterns over the 
chunk sequence. 
? Combination of those dependencies with 
Boolean operators to generate new de-
pendencies, or to modify or delete existing 
dependencies. 
3.2 The Different Steps of Analysis 
Below is an example of how a sentence is parsed. 
We present a little grammar, written in the XIP 
formalism, together with the output yielded by 
these rules. 
 
Example 
The chunking rules produce a chunk tree. 
 
In a first stage, chunking rules are applied and 
the following chunk tree is built for this sen-
tence. 
Below is a small XIP grammar that can analyze 
the above example: 
 
1> AP = Adj. 
2> NP @= Det,(AP),(Noun),Noun. 
3> FV= verb. 
4> SC= NP,FV. 
 
Each rule is associated with a layer number, 
which defines the order in which the rules must 
be executed. 
If this grammar is applied to the above sentence, 
the result is the following: 
 
TOP{SC{NP{The AP{chunking} rules}  
       FV{produce}}  
    NP{a chunk tree}  
    .} 
 
TOP is a node that is automatically created, once 
all chunking rules have applied, to transform this 
sequence of chunks into a tree. 
(The ?@? denotes a longest match strategy. The 
rule is then applied to the longest sequence of 
categories found in the linguistic unit) 
 
The next step consists of extracting some basic 
dependencies from this tree. These dependencies 
are obtained with some very basic rules that only 
connect nodes that occur in a specific sub-tree 
configuration. 
 
SUBJ(produce,rules) 
OBJ(produce,tree) 
 
SUBJ is a subject relation, which has been ex-
tracted with the following rule: 
 
| NP{?*, noun#1}, FV{?*,verb#2}|       
   SUBJ(#2,#1). 
 
This rule links together the noun and the verb 
respectively the sub-nodes of a NP and a VP that 
are next to each other. The ?{?}? denotes a pat-
tern over sub-nodes. 
 
Other rules may then be applied to this output, 
to add or modify existing dependencies. 
 
if (SUBJ(#1,#2) & OBJ(#1,#3)) 
 TRIPLET(#2,#1,#3). 
 
For instance, the above rule will generate a three 
slot dependency TRIPLET with the nodes ex-
tracted from the subject and object dependencies. 
If we apply this rule to our previous example, we 
will create: TRIPLET(rules,produce,tree). 
35
3.3 Script Language 
The utilization of a script language deeply in-
grained into the parser fabric might sound like a 
pure technical gadget with very little influence 
on parsing theories. However, the development 
of a parser often poses some very trivial prob-
lems, which we can sum up in the three questions 
below: 
 
? How can we use previous analyses? 
? How do we access external information? 
? How do we control the grammar from an 
embedding application? 
 
Usually, the answer for each of these questions 
leads to three different implementations, as none 
of these problems seem to have any connections 
whatsoever. Their only common point seems to 
be some extra-programming into the parser en-
gine. If a grammar and a parser are both written 
in the same programming language, the problem 
is relatively simple to solve. However, if the 
grammar is written in a formalism specifically 
designed for linguistic analysis interpreted with a 
linguistic compiler (as it is the case for XIP), 
then any new features that would implement 
some of these instructions translate into a modi-
fication of the parsing engine itself. However, 
one cannot expand the parser engine forever. The 
solution that has been chosen in XIP is to de-
velop a script language, which linguists can use 
to enrich the original grammatical formalism 
with new instructions.  
3.4 First attempts 
The first attempts to add scripting instructions to 
XIP consisted in enriching the grammar with 
numerical and string variables together with 
some instructions to handle these values. For 
instance, it is possible in XIP to declare a string 
variable, to instantiate it with the lemma value of 
a syntactic node and to apply some string modi-
fications upon it. However, the development of 
such a script language, however useful it proved, 
became closer and closer to a general-purpose 
programming language, which XIP was not de-
signed to be. The task of developing a full-
fledged programming language with a large in-
struction set is a complex ongoing process, 
which has little connection with parsing theories. 
Nevertheless, there was a need for such an ad-
dendum, which led the development team to link 
XIP with Python, whose own ongoing develop-
ment is backed up by thousands of dedicated 
computer scientists. 
3.5 Python 
Scripting languages have been around for a very 
long time. Thus Perl and Awk have been part of 
the Unix OS for at least twenty years. Python is 
already an old language, in computational time 
scale. It has been central to the Linux environ-
ment for more than ten years. Most of the basic 
installation procedures are written in that lan-
guage. It has also been ported to a variety of plat-
forms such as Windows or Mac OS. The lan-
guage syntax is close to C, but lacks type verifi-
cation. However, the language is thoroughly 
documented and a large quantity of specialized 
libraries is available. Python has also been cho-
sen because of the simplicity of its API, which 
allows programmers to link easily a Python en-
gine to their own application or to enlarge the 
language with new libraries. The other reason of 
this choice, over for instance a more conven-
tional language such as C or Java is the fact that 
it is an interpreted language. A XIP grammar is a 
set of text files, which are all compiled on the fly 
in memory every time the parser is run. It stems 
from this choice that any addenda to this gram-
mar should be written in a language that is also 
compiled on the fly. In this way, the new instruc-
tions can be developed in parallel with the 
grammar and immediately put in test. It also 
simplifies the non-trivial task of debugging a 
complete grammar as any modifications on any 
parts of the grammar can be immediately ex-
perimented together with the python script.  
We have produced two different versions of the 
XIP-python parsing engine.  
3.6 Python Embedded within XIP  
We have linked the python engine to XIP, which 
allows us to call and execute python scripts from 
within the parsing engine. In this case, a gram-
mar rule can call a python script to verify spe-
cific conditions. The python scripts are then ap-
pended to the grammar itself. These scripts have 
full access to all linguistic objects constructed so 
far. XIP is the master program with python 
scripts being triggered by grammar rules.  
3.7 XIP as a Python Library 
We have created a specific XIP library which can 
be freely imported in python. In this case, the 
XIP library exports a basic API, compliant with 
the python programming interface, which allows 
python developers to benefit from the XIP en-
36
gine. The XIP results are then returned as python 
objects. Since the purpose in this article is to 
show how a grammar formalism can be enriched 
with new instructions, we will mainly concen-
trate on the first point. 
3.8 Interfacing Python and a XIP grammar 
A XIP grammar mainly handles syntactic nodes, 
features, categories, and dependencies. In order 
to be efficient, a Python script, called from a XIP 
grammar, should have access to all this informa-
tion in a simple and natural way. The notion of 
procedure has already been added to the XIP 
formalism. They can be used in any sort of rule.  
 
Example 
 
if (subject(#1,#2) & TestNode(#1))  
    ambiguous(#1). 
 
The above rule tests the existence of a subject 
dependency and will use the TestNode procedure 
to check some properties of the #1 node. If all 
these conditions are true, then a new depend-
ency: ambiguous is created with #1 as parameter. 
3.9 Interface 
The TestNode procedure is declared in a XIP 
grammar in the following way: 
 
Example 
Python: //XIP field name 
TestNode(#1). //the XIP procedure name, with 
  XIP parameter style. 
 
//All that follows is in Python 
def TestNode(node): 
? 
 
The only constraint is that the XIP procedure 
name (TestNode) should also have been imple-
mented as a Python procedure. If this Python 
procedure is missing, then the grammar compila-
tion fails. 
The system works as a very simple linker, where 
the code integrity is verified to the presence of 
common names in XIP and Python. 
However, the next step, which consists in trans-
lating XIP data into Python data, is done at run-
time.  
XIP recognizes many different sorts of data, 
which can all be transmitted to a Python script, 
such as syntactic nodes, dependencies, integer 
variables, string variables, or even vector vari-
ables. Each of these data is then translated into 
simple Python variables. However, the syntactic 
nodes and the dependencies are not directly 
transformed into Python objects; we simply 
propagate them into the Python code as integers. 
Each node and each dependency has a unique 
index, which simplifies the task of sharing pa-
rameters between XIP and Python.  
 
3.10 XIP API 
Python procedures have access to all internal 
parsing data through a specific API. This API 
consists of a dozen instructions, which can be 
called anywhere in the Python code. For in-
stance, XIP provides Python instructions to re-
turn a node or a dependency object on the basis 
of its index. We have implemented the Python 
XipNode class, with the following fields: 
 
class XipNode 
 index #the unique index of the node 
 POS #the part of speech 
 Lemma #a vector of possible lemmas  
   for the node 
 Surface #the surface form as it ap 
   pears in the sentence 
 features  #a vector of attribute-value  
   features 
 leftoffset,rightoffset  #the text offsets
 next,previous,parent,child # indexes 
 
A XipNode object is automatically created when 
the object creator is called with the node index as 
parameter. We can also travel through the syn-
tactic tree, thanks to the next, previous, parent, 
child indexes that are provided by this class.  
There is a big difference between using this API 
and exploiting the regular output of a syntactic 
parser. Since the Python procedures are called at 
runtime from the grammar, they have full access 
to the on-going linguistic data. Second, the selec-
tion of syntactic nodes on which to apply Python 
procedures is done at the grammar level, which 
means that the access of specific nodes is done 
through the parsing engine itself, without any 
need to duplicate any sorts of tree operators, 
which would be mandatory in the case of a Java, 
XML or C++ object. Finally, the memory foot-
print is only limited to the nodes that are re-
quested by the application, there is no need to 
reduplicate the whole linguistic data structure. 
The memory footprint reduction also has the ef-
fect of speeding up the execution.  
 
37
3.11 Other Basic Instructions 
XIP provides the following Python instructions: 
 
? XipDependency(index) builds a Xip-
Dependency object. 
? nodeset(POS) returns a vector of node 
indices corresponding to a POS: node-
set(?noun?) 
? dependencyset(POS) returns a vector of 
dependency indices corresponding to a 
dependency name: dependen-
cyset(?SUBJECT?) 
? dependencyonfirstnode(n) returns a 
vector of dependency indices, whose first 
parameter is the node index n: depend-
encyonfirstnode(12) 
These basic instructions make it possible for a 
Python script to access all internal XIP data at 
any stages. 
3.12 An Example 
Let us define the Python code of TestNbSenses, 
which checks whether a verbal node is highly 
ambiguous according to WordNet. As a demon-
stration, a verb will be said to be highly ambigu-
ous if the number of its senses is larger than 10. 
 
def TestNbSenses(i): 
    n=XipNode(i) 
    senses=N[n.lemma].getSenses() 
    if len(senses)>=10: 
       return 1 
    return 0 
 
We can now use this procedure in a syntactic 
rule to test the ambiguity of a verb in order to 
guide the grammar:  
 
if (subject(#1,#2) & TestNbSenses(#1))  
 ambiguous(#1). 
 
The dependency ambiguous will be created for a 
verbal node, if this verb is highly ambiguous. 
 
4 Back to the Initial Questions 
The questions we wish to answer are the follow-
ing: 
? How can we use previous analyses? 
? How do we access external information? 
? How do we control the grammar from an 
embedding application? 
We have shown in the previous section how new 
instructions could be easily defined and thus be-
come part of the XIP formalism. These instruc-
tions are mapped to a Python program which 
offers all we need to answer the above questions. 
4.1 How can we use previous analyses? 
Since, we have a full access to the internal lin-
guistic representation of XIP, we can store what-
ever data we might find useful for a given task. 
For instance, we could decide to count the num-
ber of time a word has been detected in the 
course of parsing. This could be implemented 
with a Python dictionary variable. 
 
Python: 
countword(#1).  
getcount(#1). 
? 
 
The first procedure countword receives a node 
index as input. It translates it into a XipNode, 
and it uses the lemma as an entry for the Python 
dictionary wordcounter. At the end of the proc-
ess, wordcounter contains a list of words with 
their number of occurrences. The second proce-
dure implements a simple test which returns the 
number of time a word has been found. It returns 
0, if it is an unknown word. 
The grammar rule below is used to count words: 
 
|Noun#1| {  
countword(#1); 
} 
 
The instruction |noun#1| automatically loops 
between all noun nodes. 
The rule below is used to test if a word has al-
ready been found: 
 
if (subject(#1,#2) & getcount(#2)) ? 
 
4.2 How do we access external information? 
We have already given an example with Word-
Net. Thanks to the large number of libraries 
available, a Python script can benefit from 
WordNet information. It can also connect to a 
variety of databases such as MySQL, which also 
allows a grammar to query a database for spe-
cific data. 
For instance, we could store in a database verb-
noun couples that have been extracted from a 
38
large corpus. Then, at runtime, a grammar could 
check whether a certain verb and a certain noun 
have already been found together in another 
document.  
 
Example 
 
Python: 
TestCouple(#1,#2). 
 
def TestCouple(v,n): 
noun=XipNode(n) 
verb=XipNode(v) 
cmd=?select * from couples where ? 
cmd+=?verb=?+verb.lemma+" 
cmd+=? and noun=?+noun.lemma+?;? 
nb=mysql.execute(cmd) 
return nb 
 
In the XIP grammar: 
 
|FV{verb#1},PP{prep,NP{noun#2}}| 
   if (TestCouple(#1,#2)) 
       Complement(#1,#2). 
 
If we have a verb followed by a PP, then if we 
have already found in a previous analysis a link 
between the verb and the noun embedded in the 
PP, we create a dependency Complement over 
the verb and the noun. 
4.3 How do we control the grammar from 
an embedding application? 
Since a Python script can exploit any sort of in-
put, from text files to databases; it becomes rela-
tively simple to implement a simple Python pro-
cedure that blocks the execution of certain 
grammar rules. If we examine the above exam-
ple, we can see how the grammar execution can 
be modified by an external calling program. For 
instance, the selection of a different database will 
have a strong influence on how dependencies are 
constructed.  
5 Expression Power 
The main goal of this article is to describe a way 
to articulate no-linguistic constraints with a dedi-
cated linguistic formalism. The notion of con-
straint in this perspective does not only apply to 
purely linguistic properties such as category or-
der or dependency building constraints; it is 
enlarged to encompass properties that are rarely 
taken into account in syntactic theories. It should 
be noted, however, that if most theories are de-
signed to apply to a single sentence, nothing pre-
vents these formalisms to benefit from extra-
linguistic data through a complex feature system 
that would encode the sentence context. How 
these features are instantiated is nevertheless out 
the realm of these theories. The originality of our 
system lies in the fact that we intertwine from the 
beginning these constraints into the fabric of the 
formalism. Since any rules can be governed by a 
Boolean expression, which in turn can accept any 
Boolean python functions, it becomes feasible to 
define a formalism in which a constraint is no 
longer reduced to only linguistic data, but to any 
properties that a full-fledged programming lan-
guage can allow. Thus, any rule can be con-
strained during its application with complex con-
straints which are implemented as a python script.  
 
Example 
 
pythontest is a generic Boolean python function, 
which  any XIP rules can embed within its own 
set of constraints. 
 
Below are some examples of XIP rules, which 
are constrained with this generic python function. 
A constraint in XIP is introduced with the key-
word ?if?.  
? A chunking rule: 
PP = prep, NP#1, if (pythontest(#1)). 
? A dependency rule: 
if (subject(#1,#2) & pythontest(#1)) ? 
 
However, since any rule might be constrained 
with an external process it should be noted that 
this system can no longer be described as a pure 
linguistic parser. Its expression power largely 
exceeds what is usually expected from a syntac-
tic formalism.  
6 Implementation Examples 
We have successfully used Python in our gram-
mars in two different applications so far. The 
first implementation consists of a script that is 
called at the end of any sentence analysis to store 
the results in a MySQL database. Since the sav-
ing is done with a Python program, it is very 
simple to modify this script to store only infor-
mation that is salient to a particular application. 
In this respect, the maintenance of such a script 
is much simpler and much flexible than its C++ 
or Java counterpart. The storage is also done at 
runtime which limits the amount of data kept in 
memory. 
39
The second example is the implementation of a 
co-reference system (Salah A?t-Mohktar to ap-
pear), which uses Python as a backup language 
to keep a specific representation of linguistic in-
formation that is used at the end of the analysis 
to link together pronouns and their antecedents. 
Once again, this program could have been cre-
ated in C++ or Java, using the C++ or the Java 
XIP API, however, the development of such a 
system in python benefits from the simplicity of 
the language itself and its direct bridge to inter-
nal XIP representation.  
7 Conclusion 
The integration of a linguistic parser into an ap-
plication has always posed some tricky prob-
lems. First, the grammar, whether it has been 
compiled into an external library or run through 
an interpreter, often works as a black box, which 
allows little or no possibility of interfering with 
the internal execution. Second, the output is usu-
ally frozen into one single object which forces 
the calling applications to perform format trans-
lation afterward. In many systems (Cunningham 
et al,2002, Grinberg et al, 1995), the output is 
often a large, complex object, or a large XML 
document. This has an impact on both memory 
footprint (these objects might be very large) and 
the analysis speed as the system must re-
implement some tree operators to traverse these 
objects. Thereby, the automatic extraction of all 
nodes that share a common property on the basis 
of these objects requires some cumbersome pro-
gramming, when this could be more elegantly 
handled through the linguistic formalism. Third, 
the use of extra-linguistic information often im-
poses a modification of the parsing engine itself, 
which prevents developers from switching 
quickly between heterogeneous data sources. For 
a long time, linguistic formalisms have been 
conceived as specialized theoretical languages 
with little if no algorithmic possibilities. How-
ever, today, the use of syntactic parsers in large 
applications triggers the need for more than just 
pure linguistic description. For all these reasons, 
the integration of a script language as part of the 
formalism seems a reasonable solution, as it will 
transform dedicated linguistic formalisms to lin-
guistically driven programming languages. 
Reference 
Gazdar G., Klein E., Pullum G., Sag A. I., 1985. Gen-
eralized Phrase Structure Grammar, Blackwell, 
Cambridge Mass., Harvard University Press. 
Pereira F. and S. Shieber, 1987. Prolog and Natural 
Language Analysis, CSLI, Chicago University 
Press. 
Allen J. F, 1994. TRAINS Parsing System, Natural 
Language Understanding, Second Ed., chapters 
3,4,5. 
Tapanainen P., J?rvinen T. 1994.  Syntactic analysis 
of natural language using linguistic rules and cor-
pus-based patterns, Proceedings of the 15th con-
ference on Computational linguistics, Kyoto, Japan, 
pages: 629-634. 
Constant P. 1995. L'analyseur Linguistique SYLEX, 5 
?me ?cole d'?t? du CNET. 
Grinberg D., Lafferty John, Sleator D., 1995. A robust 
parsing algorithm for link grammars, Carnegie 
Mellon University Computer Science technical re-
port CMU-CS-95-125, also Proceedings of the 
Fourth International Workshop on Parsing Tech-
nologies, Prague, September, 1995. 
Fellbaum C., 1998. WordNet: An Electronic Lexical 
Database, Rider University and Princeton Univer-
sity, Cambridge, MA: The MIT Press (Language, 
speech, and communication series), 1998, xxii+423 
pp; hardbound, ISBN 0-262-06197-X. 
Roux  C. 1999. Phrase-Driven Parser,Proceedings of 
VEXTALL 99, Venezia, San Servolo, V.I.U. - 22-
24. 
Blache P., Balfourier J.-M., 2001. Property Gram-
mars: a Flexible Constraint-Based Approach to 
Parsing, in proceedings of IWPT-2001. 
A?t-Mokhtar S., Chanod J-P., Roux C., 2002. Robust-
ness beyond shallowness incremental dependency 
parsing, NLE Journal, 2002. 
Hag?ge C., Roux C.,2002. A Robust And Flexible 
Platform for Dependency Extraction, in proceed-
ings of LREC 2002. 
Declerck T. 2002, A set of tools for integrating lin-
guistic and non-linguistic information, Proceedings 
of SAAKM. 
H. Cunningham, D. Maynard, K. Bontcheva, V. Tab-
lan.,2002. GATE: A Framework and Graphical 
Development Environment for Robust NLP Tools 
and Applications, Proceedings of the 40th Anni-
versary Meeting of the Association for Computa-
tional Linguistics (ACL'02), Philadelphia, July 
2002.  
Blache P., Gu?not M-L. 2003. Flexible Corpus Anno-
tation with Property Grammars, BulTreeBank Pro-
ject 
Roux C., 2004. Une Grammaire XML, TALN Confe-
rence, Fez, Morocco, April, 19-22, 2004. 
[Python] http://www.python.org/ 
 
40
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1764?1772, Dublin, Ireland, August 23-29 2014.
Part of Speech Tagging for French Social Media Data
Farhad Nooralahzadeh
?
Ecole Polytechnique
de Montr?eal
Montr?eal, PQ, Canada
nooralahzadeh
@gmail.com
Caroline Brun
Xerox Research Centre
Europe
Meylan, France
caroline.brun
@xrce.xerox.com
Claude Roux
Xerox Research Centre
Europe
Meylan, France
claude.roux
@xrce.xerox.com
Abstract
In the context of Social Media Analytics, Natural Language Processing tools face new chal-
lenges on on-line conversational text, such as microblogs, chat, or text messages, because of the
specificity of the language used in these channels. This work addresses the problem of Part-
Of-Speech tagging (initially for French but also for English) on noisy language usage from the
popular social media services like Twitter, Facebook and forums. We employ a linear-chain con-
ditional random fields (CRFs) model, enriched with several morphological, orthographic, lexical
and large-scale word clustering features. Our experiments used different feature configurations
to train the model. We achieved a higher tagging performance with these features, compared to
baseline results on French social media bank. Moreover, experiments on English social media
content show that our model improves over previous works on these data.
1 Introduction
There are many challenges inherent to applying standard natural language analysis techniques to social
media. On-line conversational texts, such as tweets are quite challenging for text mining tools, and in
particular for opinion mining, as they contain very little contextual information and assume too much
implicit knowledge. They expose much more language variation and tend to be less grammatical than
regular texts such as news articles or books. Furthermore, they contain unusual capitalization, and make
frequent use of emoticons, abbreviations and hash-tags, which can form an important part of their in-
ner meaning (Maynard et al., 2012). Conventional natural language processing tools for regular texts
have achieved reasonably high accuracy thanks to machine learning techniques on large annotated data
set. However, ?off the shelf? language processing systems fail to work on social media data and their
performance on this domain degrade very fast. For example, in English Part-Of-Speech tagging, the
accuracy of the Stanford tagger (Toutanova et al., 2003) falls from 97% on Wall Street Journal text to
85% accuracy on Twitter (Gimpel et al., 2011), similarly the MElt POS tagger (Denis and Sagot, 2012)
drops from 97.7% on the French Treebank (called the FTB-UC by (Candito and Crabb?e, 2009)) to 85.2%
on on-line conversational texts (Seddah et al., 2012). In Named Entity Recognition, the CoNLL-trained
Stanford recognizer achieves 44% F-measure (Ritter et al., 2011), down from 86% on the CoNLL test
set (Finkel et al., 2005); regarding parsing, see for example (Foster et al., 2011; Seddah et al., 2012),
poor performances have been reported for different state-of-the-art parsers applied to English and French
social media content.
The main objective of this work is to implement a dedicated Part-Of-Speech (POS) tagger for French
social media content such as Twitter, Facebook, blogs, forums and customer reviews. We used the
first user-generated content resource for French presented by Seddah et al. (2012), which contains a
fine-grained tag set and has been extracted from various social media contents. We have designed and
implemented a POS tagger considering one of the well-known discriminative type of sequence-based
methods; Conditional Random Fields (CRF) (Lafferty et al., 2001). To deal with sparsity and unknown
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1764
words, we have applied unsupervised techniques to enrich the feature set. Finally, we have evaluated our
tagger performance with different configurations on annotated corpora from French social media.
We will first present related work in Part-Of-Speech tagging (Section 2) on noisy data like social media
content. In Section 3, the annotated dataset and its characteristics (e.g., tag set) are described. Section
4 presents the result of applying the MElt POS tagger to user generated text as our baseline (Seddah et
al., 2012). In Section 5, we explain how we design and implement our POS tagger. Section 6 is devoted
to experiments and performance of our tagger. Section 7 describes the evaluation of the new tagger on
English social media texts. Conclusion and future work are given in Section 8.
2 Related work
Online conversational texts, typified by micro-blogs, chat, and text messages, are a challenge for natural
language processing. Unlike the highly edited genres for which conventional NLP tools have been de-
veloped, conversational texts contain many non-standard lexical items and syntactic patterns. These are
the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative
use of language and orthography (Eisenstein, 2013)
The language technology research community proposes two approaches to deal with noisy texts,
namely normalization and domain adaptation, which are briefly described here.
2.1 Normalization
One way to deal with ill-formed language is to turn it into a well-formed language as a pre-processing
task: ?normalizing? social media or SMS messages to better conform to the language that the technology
expects. For example, (Han and Baldwin, 2011) propose the lexical normalization of short text messages,
such as tweets, based on string and distributional similarity. They describe a method to identify and
normalize ill-formed words. Word similarity and context are exploited to select the best candidate for
noisy tokens.
2.2 Domain adaptation
The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold
of ?NLP for Twitter,? including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity
recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011),
dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt
various parts of the natural language processing pipeline for social media text, and make use of a range
of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional
features, Distance supervision) (Eisenstein, 2013).
Recently, Seddah et al. (2012) followed the second approach on French social media content and
provided new labeled data and annotation schemes. They applied the MElt POS tagger (Denis and Sagot,
2012) embedded within text normalization and correction to noisy user generated texts and presented
baseline POS tagging and statistical constituency parsing results.
3 Annotated Dataset
A set of 1,700 sentences (38k tokens) has been extracted from various types of French Web 2.0 user
generated content (Facebook, Twitter, Video games and medical web forums) by Seddah et al. (2012).
They selected these corpora through direct examination of various search queries and ranked the texts
according to their distance from the French Treebank style, by measuring noisiness using the kullback-
Leibler divergence between the distribution of trigrams of characters in given corpus and the distribution
of trigrams of characters in the French Treebank reference. Some properties of this corpora are shown in
Table 1.
They targeted the annotation scheme of the FTB-UC in order to annotate the French social media
bank. The tagset includes 28 POS tags from FTB-UC and compound tags with additional categories
specific to social media, including HT for Twitter hashtags and META for meta-textual tokens, such as
1765
Twitter?s ?RT?. Twitter at-mention as well as URLs and e-mail addresses have been tagged NPP which
is the main difference with other works on on-line conversational texts. The inter-annotator agreement
rate in this corpora range between 93.4% for FACEBOOK data and 97.44% for JEUXVIDEOS.COM
(Table 1) which indicates an almost perfect agreement on the corpus (Landis and Koch, 1977).
Corpus Name # sent. # tokens Inter Annotator Agreement %
TWITTER 216 2465 95.40
FACEBOOK 452 4200 93.40
JEUXVIDEOS.COM 199 3058 97.44
DOCTISSIMO 771 10834 95.05
Table 1: Annotated datasets
4 Baseline
This section presents the performance of a state-of-the-art POS tagger for French, conducted by Seddah
et al. (2012). They used FTB-UC as training, development and test data. First, they applied several
correction processes in order to wrap the POS tagger to tag a sequence of tokens as close as possible to
standard French and training corpus. Then, the MElt tagger has been used with a set of 15 language-
independent rules, that aim at assigning the correct POS to tokens that belong to categories not found
in training corpus (e.g., URLs, e-mail addresses, emoticons). The preliminary evaluation experiments
with normalization and correction wrapper showed 84.72% and 85.28% token accuracy over annotated
development and test set respectively.
5 New POS Tagger Development
Conversational style context and 140-character limitation in micro-blogs require users to express their
thought or reply to others? messages within a short text. Therefore, without being ambiguous, some
words are usually abbreviated with a special spelling. For example, c t usually means c??etait (it was); qil
denotes qu? il (that it/he).
Our tagger is based on sequence labeling models (CRF), enabling arbitrary local features to be inte-
grated into a log-linear model. We employed three categories of feature templates to deal with syntactic
variations on social media contents and alleviating the data sparseness problem.
5.1 Basic Feature Templates
The feature templates we use here are a superset of the largely language independent features used by
(Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). These features fall into
two main categories. A first set of features tries to capture the lexical form of the word being tagged:
it includes prefixes and suffixes (of at most 10 characters) from the current word, together with binary
features based on the presence of special characters such as numbers, hyphens, and uppercase letters,
within w
i
. A second set of features directly models the context of the current word and tag: it includes
the previous tag, surrounding word forms in a 5 tokens window. The detailed list of feature templates we
used in this category is shown in Table 2.
1
Context
w
i
= X ,i ? [?2,?1, 0, 1, 2] & t
0
= T
w
i
w
j
= XY , (i, j) ? {(?1, 0), (0, 1), (?2, 0), (0, 2)} & t
0
= T
w
i
w
j
w
k
= XY Z , (i, j, k) ? {(?2,?1, 0), (0, 1, 2), (?1, 0, 1)} & t
0
= T
w
i
w
j
w
k
w
l
w
m
=XY ZPQ , (i, j, k, l,m) =(?2,?1, 0, 1, 2) & t
0
= T
t
?1
& t
0
= T
Lexical and Orthographic
f(w
i
),i ? [?1, 0, 1] ,f ? F & t
0
= T
m(w
i
), i ? [?1, 0, 1], m ?M & t
0
= T
Table 2: Basic Feature Templates
1
w
0
means the token at the current position while w
?1
means the previous token.
1766
The model generates the feature space by scanning each pair in the training data with the feature
templates given in Table 2. For example, if we consider the following tweet from the training set, the
generated features based on the first template can be seen in Table 3, in which the current word is ?vous?
(position 6) .
Sample tweet : ?@Marie Je vais tener De vous produire la vid?eo *-* ?
word: @Marie Je vais tener De vous produire la vid?eo *-*
Tag: NPP CLS-SUJ V VINF P CLO-A OBJ VINF DET NC I
Position: 1 2 3 4 5 6 7 8 9 10
w
0
=vous &t
0
=O
w
?1
=De &t
0
=O
w
?2
=tener &t
0
=O
w
+1
=produire &t
0
=O
w
+2
=la &t
0
=O
Table 3: Generated features with template :
w
i
= X ,i ? [?2,?1, 0, 1, 2] &t
0
= T
We defined two sets of operations, F and M . Each operation maps tokens to equivalence classes.
F is a set of regular expression rules that detect specific patterns on w
i
and return binary values. The
functions f(w
i
) ? F include the rules as detailed in the following list (List 1):
List 1: Set of regular expression rules (F )
. Return ?True? if the w
i
contains Punctuation marks otherwise return ?False?
. Return ?True? if the w
i
is list of Punctuation marks otherwise return ?False?
. Return ?True? if the w
i
contains digits otherwise return ?False?
. Return ?True? if the w
i
number otherwise return ?False?
. Return ?True? if all letters of w
i
are capitalized otherwise return ?False? allNumber
. Return ?True? if the w
i
starts with capital letter otherwise return ?False?
. Return ?True? if the w
i
has?URL? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Email? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Abbreviation? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Arrow? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?Time ? pattern otherwise return ?False?
. Return ?True? if the w
i
has ?NumberWithCommas? pattern otherwise return ?False?
. Return ?True? if the w
i
has symbol representing ?RT:retweeting? form otherwise return ?False?
. Return ?True? if the w
i
has symbol representing ?At-Mention? form otherwise return ?False?
. Return ?True? if the w
i
has symbol representing ?hash-tagh? form otherwise return ?False?
M is a set of orthographic transformations that maps a string to another string via a simple surface
level transformation. The functions m(w
i
) ?M are given in List 2 :
List 2: Set of orthographic transformation (M )
. Return capitalized type of w
i
,These types are (allCap, shortCap, longCap, noCap, initCap, mixCap)
(e.g.,?Plus-tard?? ?initCap? ,?RT???allCap,longCap? )
. Return the type of w
i
, obtained by replacing [a? z] with x, [A? Z] with X , and [0? 9] with 9
(e.g.,., ?@DJRyan1der?? ?@XXXxxx9xxx?)
. Return a vector of Unicode matching of the string w
i
(e.g., ?@DJRyan1der?? ?[64? 68? 74? 82? 121? 97? 110? 49? 100? 101? 114]?)
. Return the first n character of x (n-gram prefix), where 1 ? n ? 10
. Return the last n character of x (n-gram suffix), where 1 ? n ? 10
5.2 Word Clustering Feature Templates
To bridge the gap between high and low frequency words, we employed word clustering to acquire
knowledge about paradigmatic lexical relations from large-scale texts. Our work is inspired by the suc-
1767
cessful application of word clustering in supervised NLP models (Miller et al., 2004; Turian et al., 2010;
Ritter et al., 2011; Owoputi et al., 2013).
Various clustering techniques have been proposed, some of which, for example, perform automatic
word clustering optimizing a maximum likelihood criterion with iterative clustering algorithms. In this
work, we focus on distributional word clustering, based on the assumption that the words that appear in
similar contexts (especially surrounding words) tend to have similar meanings.
5.2.1 Brown Clustering
We used our unlabeled Twitter corpus (4M tweets) to improve our tagger performance. This corpus
has been extracted in the framework of a French government funded ANR project called Imagiweb,
whose goal is to develop tools to analyse the brand image of entities (persons or companies) on social
media. More specifically, one of the focus of the project is to analyse the brand image of politicians on
Twitter. Therefore, data about the two main candidates (F. Hollande and N. Sarkozy) in the last French
presidential election in May 2012 have been crawled from Twitter, using Twitter API, from 6 months
before to 6 months after the elections. Our unlabeled Twitter data is a sub-set of this corpus.
We obtained hierarchical word clusters via Brown Clustering (Brown et al., 1992) on a large set of
unlabeled tweets. This algorithm generates a hard clustering, each word belongs to exactly one cluster.
The input to the algorithm is a sequence of words w
i
, . . . , w
n
. Initially, the algorithm starts with each
word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two
clusters that maximize the resulting cluster quality. The quality is defined on the class-based bigram
language model as follows, where C maps a word w to its class C(w).
p(w
i
|w
1
, . . . , w
i?1
) = p(C(w
i
)|C(w
i?1
))p(w
i
|C(w
i
))
We ended up with 500 clusters (the optimal number of clusters according to the performance of the
tagger among different number of clusters) with 222,788 word types by keeping the words appearing 10
or more times. Since Brown clustering creates hierarchical clusters in a binary tree, we used the feature
template which maps the word w
i
to the cluster at depths 2, 4, . . . , 16 containing w
i
. If w
i
was not seen
while constructing the clusters and thus does not belong to any cluster we tried to find similar words
by computing Jaro-Winkler distance (Philips, 1990; Winkler, 2006) and mapped the best match to the
cluster depths. Nevertheless, if we couldn?t find the best match (the threshold of the similarity score is
0.9), we mapped it to a special NULL cluster. The detailed list of feature templates we used in this
category is shown in Table 5.
2
5.2.2 MKCLS Clustering
We also did some experiments, using another popular clustering method based on the exchange algorithm
(Kneser and Ney, 1993). The objective function maximizes the likelihood
?
n
i=1
P (w
i
|w
1
, . . . , w
i?1
) of
the training data given a partially class-based bigram model of the form as follows:
p(w
i
|w
1
, . . . , w
i?1
) ? p(C(w
i
)|w
i?1
)p(w
i
|C(w
i
))
We use the publicly available implementation MKCLS
3
to train this model on our French Twitter data
(4M tweets). This algorithm provides us with 500 word clusters with 2,768,297 different words.
Word Cluster
c(w
i
) = X ,i ? [?2,?1, 0, 1, 2] and c ? C & t
0
= T
c(w
i
)c(w
j
) = XY , (i, j) ? {(?1, 0), (0, 1)} and c ? C & t
0
= T
c(w
i
)C(w
j
)c(w
k
) = XY Z , (i, j, k) ? {(?2,?1, 0), (0, 1, 2), (?1, 0, 1)}
and c ? C
& t
0
= T
c(w
i
)c(w
j
)c(w
k
)c(w
l
)c(w
m
)=XY ZPQ , (i, j, k, l,m) =(?2,?1, 0, 1, 2) and
c ? C
& t
0
= T
Table 5: Word Clustering Feature Templates
2
c(w
i
) ? C map the word w
i
to the clusters at depths 2, 4, . . . , 16
3
https://code.google.com/p/giza-pp/
1768
6 Experiments
For the implementation of discriminative sequential model, we chose the Wapiti
4
toolkit (Lavergne et al.,
2010). Wapiti is a very fast toolkit for segmenting and labeling sequences with discriminative models.
It is based on maxent models, maximum entropy Markov models and linear-chain CRF and proposes
various optimization and regularization methods to improve both the computational complexity and the
prediction performance of standard models. Wapiti has been ranked first on the sequence tagging task
for more than a year on MLcomp
5
web site.
6.1 Training and parameter regularization
In the training of log-linear models, regularization is normally required to prevent the model from over
fitting on the training data. The two most common regularization methods are called L1 and L2 regular-
ization (Tsuruoka et al., 2009). Wapiti uses the elastic-net penalty of the form:
?
1
? |?|
1
+
?
2
2
? ||?||
2
2
and it is implemented with 3 different algorithms: Orthant-Wise Limited-memory Quasi-Newton (OWL-
QN: L-BFGS), Stochastic Gradient Descent (SGD) and Block Coordinate Descent. We trained with
L-BFGS, a classical Quasi-Newton optimization algorithm with limited memory which minimizes the
regularized objective and uses elastic net regularization. Using even a very small L1 penalty excludes
many irrelevant or highly noisy features. We carried out a grid search for the regularization values,
assessing with F-measure and accuracy. We conducted a first order linear chain CRF model on the
French corpora with classical setting (training set: 80%, development set: 10% and test set: 10%) for
L1 ? {0, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16} and L2 ? {0, 0.0325, 0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16} (Owoputi
et al., 2013). In any experiment, the result of the regularization values were close to each other, there-
fore we selected L1,L2=(0.25, 0.5) achieving 80.4% and 90.6% F-measure and accuracy on the corpora
respectively.
6.2 Performance
In order to assess how the results of our tagger based on the current limited corpora could be general-
ized to an independent data set, a set of 10-fold cross validation experiments has been performed. We
investigated the effect of each feature template on the tagging. We used ?c: compact? option in Wapiti
which enables model compaction at the end of the training. This removes all inactive observations from
the model, leading to a much smaller model when an L1-penalty is used.
Table 6 shows the result of each experiment, measured by token and sentence accuracy. It shows that
word clustering is a very strong source of lexical knowledge and significantly increases the performance
of our tagger.
Feature Templates Token Accuracy % Sentence Accuracy %
B 88.2 45.8
B+C1 90.8 49.9
B+C2 90.3 50.3
B+C1+C2 91.9 51.1
B: Basic Feature Templates
C1: Brown word-Clustering Feature Templates
C2: MKCLS word-Clustering Feature Templates
Table 6: Performance of new tagger based on CRF with different configurations
The CRF model with all set of features (B+C1+C2) is the best model with 91.9% and 51.1% token
and sentence accuracy on 10-fold cross validation. All of these tagging accuracies are significantly above
previous results on the French social bank (baseline).
4
http://wapiti.limsi.fr/
5
http://mlcomp.org/
1769
7 Evaluation on English social media Content
In order to implement a tagger for English dedicated to social media content, we used the publicly avail-
able clusters data set (Owoputi et al., 2013) to build Brown clustering features. Moreover we performed
the same process as in Section 5.2.2 in order to provide MKCLS clustering features with English Twitter
data (1 million tweets obtained from
6
).
We applied our tagger with the best configuration to the annotated dataset provided by Ritter et al.
(2011). This dataset contains 800 tweets that have been annotated with the Penn Treebank (PTB) tagset
(Marcus et al., 1993). We trained and test our system with 10-fold cross validation. Table 7 shows our
tagger performance compared to other state-of-art taggers on this data set.
Tagger Accuracy%
Our new tagger, CRF with B+C1+C2 configuration 90.1
Ritter et al. (Ritter et al., 2011), CRF tagger 88.3
Owoputi et al. (Owoputi et al., 2013), MEMM tagger 90? 0.5
Table 7: Evaluation on Twitter data with PTB tags
In addition, we evaluated the tagger performance on another English social media data: NPS chat
(?Chat with PTB tags? (Forsythand and Martell, 2007)). Due to the large number of tokens (50 K), we
trained and tested our tagger with a 5-fold cross validation setup. Our new tagger performance as well
as the other taggers results are given in Table 8.
Tagger Accuracy%
Our new tagger, CRF with B+C1+C2 configuration 92.7
Forsythand and Martell (Forsythand and Martell, 2007), HMM tagger 90.8
Owoputi et al. (Owoputi et al., 2013), MEMM tagger 93.4? 0.3
Table 8: Evaluation on Chat data with PTB tags
8 Conclusion and Future Work
In this paper, we have presented an innovative work on POS tagging for French social media noisy
input. Because of the specific phenomena encountered in such data and also because of the lack of large
training corpus, we proposed a discriminative sequence labeling model (CRF) enhanced with several type
of features. After experimenting different configurations of features, we achieved 91.9% token accuracy
on target corpus. Moreover, experiments on English social media contents show that our model obtains
further improvement over previous works on these data and could be reproduced for other languages. In
the future, we plan to pursue this work in two main directions: (a) Integrate the new tagger with a robust
syntactic parser and investigate its impact on dependency parsing applied to social media and (b) evaluate
the impact of POS tagging on opinion mining on micro-blogs, since this parser is the core component of
an opinion mining system applied in different social-media analytics projects.
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18:467?479.
Marie Candito and Beno??t Crabb?e. 2009. Improving generative statistical parsing with semi-supervised word
clustering. In Proceedings of the 11th International Conference on Parsing Technologies, IWPT ?09, pages
138?141, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pascal Denis and Beno??t Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art pos tagging.
Language Resources and Evaluation, 46:721?736.
6
http://illocutioninc.com/site/products-data.html
1770
Jacob Eisenstein. 2013. What to do about bad language on the internet. In proc. of NAACL.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Anno-
tating named entities in twitter data with crowdsourcing. Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 80?88.
Jenny R. Finkel, Trond Grenager, and Manning Christopher. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363?370.
Eric N. Forsythand and Craig H. Martell. 2007. Lexical and discourse analysis of online chat dialog. In Proceed-
ings of the International Conference on Semantic Computing, ICSC ?07, pages 19?26, Washington, DC, USA.
IEEE Computer Society.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Joseph
Van Genabith. 2011. From news to comment: Resources and benchmarks for parsing the language of web 2.0.
In Proceedings of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter:
Annotation, features, and experiments. Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies: short papers, 2:42?47.
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: makn sens a #twitter. Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-
nologies, 1:365?378.
Beaux Sharifi Mark-Anthony Hutton and Jugal Kalita. 2010. Summarizing microblogs automatically. In Proceed-
ings of NAACL.
Reinhard Kneser and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language
modeling. In In Proceedings of the European Conference on Speech Communication and Technology (Eu-
rospeech).
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. Proceedings of the Eighteenth International Conference on
Machine Learning, pages 282?289.
J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon. 2010. Practical very large scale CRFs. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504?513. Association for
Computational Linguistics, July.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Diana Maynard, Kalina Bontcheva, and Dominic Rout. 2012. Challenges in developing opinion mining tools
for social media. In Proceedings of @NLP can u tag #usergeneratedcontent?! Workshop at International
Conference on Language Resources and Evaluation, LREC 2012, 26 May 2012, Istanbul, Turkey.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Proceedings of HLT, pages 337?342.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, N. Schneider, and N.A. Smith. 2013. Improved part-of-speech
tagging for online conversational text with word clusters. In Proceedings of NAACL.
Lawrence Philips. 1990. Hanging on the metaphone. Computer Language, 7:12.
Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proceedings
of NAACL.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental
study. ACL.
1771
Djam?e Seddah, Beno??t Sagot, Marie Candito, Virginie Mouilleron, and Vanessa Combet. 2012. The French Social
Media Bank: a Treebank of Noisy User Generated Content. In COLING 2012 - 24th International Conference
on Computational Linguistics, Mumbai, India, Dec. Kay, Martin and Boitet, Christian.
Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP/VLC 2000, pages 63?70.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. IN PROCEEDINGS OF HLT-NAACL, pages 252?259.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for
l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ?09, pages 477?485, Stroudsburg, PA, USA. Association for Computational
Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-
supervised learning. In ACL.
William E Winkler. 2006. Overview of record linkage and current research directions. Technical report, BUREAU
OF THE CENSUS.
Liu Xiaohua, Zhang Shaodian, Wei Furu, and Zhou Ming. 2011. Recognizing named entities in tweets. In
Proceedings of ACL.
1772
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 838?842,
Dublin, Ireland, August 23-24, 2014.
XRCE: Hybrid Classification for Aspect-based Sentiment Analysis
Caroline Brun, Diana Nicoleta Popa, Claude Roux
Xerox Research Centre Europe
6, chemin de Maupertuis
38240 Meylan, France
{caroline.brun, diana.popa, claude.roux}@xrce.xerox.com
Abstract
In this paper, we present the system we
have developed for the SemEval-2014
Task 4 dedicated to Aspect-Based Senti-
ment Analysis. The system is based on
a robust parser that provides information
to feed different classifiers with linguis-
tic features dedicated to aspect categories
and aspect categories polarity classifica-
tion. We mainly present the work which
has been done on the restaurant domain
1
for the four subtasks, aspect term and cat-
egory detection and aspect term and cate-
gory polarity.
1 Introduction
Aspect Based Sentiment Analysis aims at discov-
ering the opinions or sentiments expressed by a
user on the different aspects of a given entity ((Hu
and Liu, 2004); (Liu, 2012)). A wide range of
methods and techniques have been proposed to ad-
dress this task, among which systems that use syn-
tactic dependencies to link source and target of the
opinion, such as in (Kim and Hovy, 2004), (Bloom
et al., 2007), or (Wu et al., 2009). We have devel-
oped a system that belongs to this family, (Brun,
2011), as we believe that syntactic processing of
complex phenomena (negation, comparison, ...)
is a crucial step to perform aspect-based opinion
mining. In this paper, we describe the adaptations
we have made to this system for SemEval, and the
way it is applied to category and polarity classifi-
cation.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
We have not performed any domain adapation for the
laptop corpus and only submitted a run for the subtask 1, term
detection.
2 Description of the System
In this section, we describe the different compo-
nents of the system.
2.1 Existing System
In order to tackle the Semeval?14 Task 4, (Pon-
tiki et al., 2014), we used our existing aspect-
based opinion detection system. The opinion de-
tection system we built relies on a robust deep
syntactic parser, (Ait-Mokhtar et al., 2001), as a
fundamental component, from which semantic re-
lations of opinion are calculated. Parsing here
includes tokenization, morpho-syntactic analysis,
tagging which is performed via a combination of
hand-written rules and HMM, Named Entity De-
tection, chunking and finally, extraction of depen-
dency relations between lexical nodes. These re-
lations are labeled with deep syntactic functions.
More precisely, a predicate (verbal or nominal) is
linked with what we call its deep subject (SUBJ-
N), its deep object (OBJ-N), and modifiers. In
addition, the parser calculates more sophisticated
and complex relations using derivational morpho-
logic properties, deep syntactic properties (subject
and object of infinitives in the context of control
verbs), and some limited lexical semantic coding.
Syntactic relations already extracted by a
general dependency grammar, lexical information
about word polarities, sub categorization informa-
tion and syntactic dependencies are all combined
within our robust parser to extract the semantic
relations. The polarity lexicon has been built
using existing resources and also by applying
classification techniques over large corpora, while
the semantic extraction rules are handcrafted, see
(Brun, 2011) and (Brun, 2012) for the complete
description of these different components. The
system outputs a semantic dependency called
SENTIMENT which can be binary, i.e. linking
opinionated terms and their targets, or unary,
i.e. just the polar term in case the target of the
838
opinion hasn?t been detected. For example, when
parsing I was highly disappointed by their service
and food., the systems outputs the following
dependencies:
SUBJ N(disappointed,food)
SUBJ N(disappointed,service)
OBJ N(disappointed,I)
MANNER PRE(disappointed,highly)
SENTIMENT NEGATIVE(disappointed,service)
SENTIMENT NEGATIVE(disappointed,food)
In this system, aspects terms are not explic-
itly extracted, however all non-polar arguments of
the SENTIMENT dependency are potential aspect
terms. Moreover, this system considers only posi-
tive and negative opinions, but does not cover the
neutral and conflict polarities.
2.2 System Adaptation
The opinion detection system described in the
previous section has been adapted for the Se-
mEval2014 Task4, in two ways: some lexical ac-
quisition has been performed in order to detect the
terms of the domain, and some rules have been de-
veloped to detect multi-word terms and to output
semantic dependencies associating their polarity
to terms and categories.
2.2.1 Lexical Enrichment and Term
Detection
As said before, the existing system encodes a rea-
sonable amount of polar vocabulary. However, as
the task implies domain knowledge to detect the
terms, we have first extracted the terms from the
training corpus and encoded their words into our
lexicons, assigning to them the semantic features
food, service, ambiance and price. We have then
extended the list with Wordnet synonyms. To im-
prove coverage, we have also extracted and fil-
tered food term lists from Wikipedia pages and en-
coded them. More precisely, the list of food terms
has been extracted from the Wikipedia ?Food Por-
tal?, from the category ?Lists of foods?
2
. At the
end of this process, our lexicon has the following
coverage: Polar words: 1265 negative, 1082 posi-
tive and Domain words: 761 food words, 31 price
words, 105 ambiance words, 42 service words.
In order to detect the terms, some local grammar
rules (based on regular expressions) have been de-
veloped taking into account the lexical semantic
2
http://en.wikipedia.org/wiki/Category:Lists of foods
information encoded in the previous step. These
rules detect the multi-words terms, e.g. pas-
trami sandwiches, group them under the appropri-
ate syntactic category (noun, verb) and associate
them with the corresponding lexical semantic fea-
ture, food, service, ambiance, price. In addition to
this, in order to prepare the aspect category clas-
sification (c.f. section 2.3.3), a layer of semantic
dependencies has been added to the grammar: If
a domain term is detected in a sentence, a unary
dependency corresponding to its category (FOOD,
SERVICE, PRICE, AMBIANCE) is built.
2.2.2 Grammar Adaptation for Polarity
Detection
The English grammar, which had been previously
developed to detect sentiments, has also been
adapted in order to extract the opinions associated
to the terms and categories detected at the previous
step.
If an aspect term is the second argument of a
SENTIMENT relation, 2 dependencies, one for the
term (OPINION ON TERM) and one for the corre-
sponding category (OPINION ON CATEGORY) are
built. They inherit the polarity (positive or nega-
tive) of the SENTIMENT dependency. If these de-
pendencies target the same term and category and
if they have opposite polarity, they are modified in
order to bear the feature ?conflict?.
Then, if a sentence contains a term and
if no SENTIMENT dependency has been de-
tected, the OPINION ON TERM and OPIN-
ION ON CATEGORY are created with the polarity
?neutral?. Finally, if no terms have been de-
tected in a sentence, there are two cases: (1)
a SENTIMENT dependency has been detected
somewhere in the sentence, the dependency
OPINION ON CATEGORY(anecdote/misc), is
created with the corresponding polarity (positive
or negative); (2) no SENTIMENT dependency
has been detected, the dependency OPIN-
ION ON CATEGORY(anecdote/misc), is created
with polarity ?neutral?.
The dependency OPINION ON TERM links the
terms to their polarities in the sentences and serves
as input for the subtasks 1 and 3.
2.3 Classification
2.3.1 KiF (Knowledge in Frame)
The whole system, training and prediction, has
been implemented in KiF (Knowledge in Frame),
a script language that has been implemented into
839
the very fabric of the rule-based Xerox Incremen-
tal Parser (XIP). KiF offers a very simple way to
hybridize a rule-based parser with machine learn-
ing technique. For instance, a KiF function, which
evaluates a set a features to predict a class, can be
called from a rule, which could then be fired along
the output of that function. KiF is a multi-threaded
programming language, which is available for all
platforms (Windows, Mac OS, Linux). It pro-
vides all the necessary objects (strings, containers
or classes) and many encapsulations of dynamic
libraries from different C programs such as classi-
fiers (liblinear and libsvm), database (SQLite), or
XML (libxml2), which can be loaded on the fly.
All internal XIP linguistic structures are wrapped
up into KiF objects. For example, linguistic fea-
tures are available as maps, which can be modi-
fied and re-injected into their own syntactic nodes.
The language syntax is a mix between Java (types
are static) and Python (in the way containers are
handled), but provides many implicit conversions
to avoid code overloading with too many func-
tions. KiF allows for an efficient integration of
all aspects of linguistic analysis into a very sim-
ple framework, where XML documents can be an-
alyzed and modified both with linguistic parsing
and classifiers into a few hundred lines of code.
2.3.2 General Methodology
We focus on four main tasks: detecting the as-
pect terms and aspect categories and their corre-
sponding polarities. While the detection of aspect
terms and their corresponding polarities occurs at
the grammar level, for the detection of aspect cate-
gories and their corresponding polarities we make
use of the liblinear library (Fan et al., 2008) to
train our models. We train one classifier for detect-
ing the categories and further, for each category
we train a separate classifier for detecting the po-
larities corresponding to that particular category.
For both settings, we use 10-fold cross-validation.
The two modules for aspect category classification
and aspect category polarity classification are de-
scribed in details further.
2.3.3 Aspect Category Classification
The sentence classification module is used to as-
sign aspect categories to sentences. For each sen-
tence, the module takes as input features the bag
of words in the sentence as well as the information
provided by the syntactic parser. The output con-
sists of a list of categories corresponding to each
sentence.
In the pre-processing stage stop words are re-
moved (determinants, conjunctions). Further, we
use the L2-regularized logistic regression solver
from the liblinear library to train a model. The
features considered are the word lemmas from the
sentence along with their frequencies (term fre-
quency). Apart from this, the information pro-
vided by the rule based component is also taken
into account to increase the term frequency for
terms belonging to the detected categories.
Such information can consist of: dependencies
denoting the category to which a detected aspect
term belongs (Food, Service, Price, Ambiance)
and dependencies denoting the opinions on the
detected aspect terms and categories (OPIN-
ION ON CATEGORY, OPINION ON TERM). For
example for the following sentence: ?Fab-
ulous service, fantastic food, and a chilled
out atmosphere and environment?, the salient
dependencies produced by the syntactic parser are:
FOOD(food), AMBIANCE(atmosphere),
SERVICE(service), AMBIANCE(environment),
OPINION ON CATEGORY POSITIVE(food),
OPINION ON CATEGORY POSITIVE(service),
OPINION ON CATEGORY POSITIVE(ambiance),
OPINION ON TERM POSITIVE(food),
OPINION ON TERM POSITIVE(service),
OPINION ON TERM POSITIVE(atmosphere).
This yields the following features having an
increase in their frequencies: food (+3), service
(+3), atmosphere (+2), environment (+1), am-
biance (+1).
Once the logistic regression is performed, each
category is predicted with a certain probability.
Since in one sentence there may be entities that re-
fer to different categories, we set a threshold with
respect to the probability values to be taken into
account. We have tried different approaches to set
this threshold. The best results on the training and
trial data were obtained with a threshold of 0.25,
(i.e. we kept only the categories with a probability
over 0.25).
2.3.4 Aspect Category Polarity Classification
The approach to predict the polarity for each cate-
gory is similar to the one predicting the categories
for each sentence, with some differences as will
be further detailed. The classification uses for fea-
tures, the bag of words (term frequency), but also
840
the polarity provided by XIP by the following de-
pendencies: OPINION ON CATEGORY and SEN-
TIMENT. Whenever these dependencies are de-
tected, a feature is added to the classification of
the form polarity category. Thus for the previ-
ous example sentence: Fabulous service, fantastic
food, and a chilled out atmosphere and environ-
ment, the additional dependencies considered are
SENTIMENT POSITIVE(atmosphere, chilled out),
SENTIMENT POSITIVE(food, fantastic), SENTI-
MENT POSITIVE(service, Fabulous). After map-
ping back the terms to their corresponding cate-
gories, the added features are: positive ambiance,
positive food and positive service. Since the de-
pendency OPINION ON CATEGORY is also de-
tected by the parser for these categories, each
of the above mentioned features will have a fre-
quency of 2 in this case. Moreover, the polarity
alone is also added as a feature. The training is
performed using the L2-regularized L2-loss sup-
port vector classification solver from the same li-
brary (liblinear) and a model is generated for each
category. Thus, depending on the categories de-
tected within a certain sentence, the correspond-
ing model is used to make the prediction regarding
their polarities. The classifier?s output represents
the predicted polarity for one given category.
3 Evaluation
The corpus used for evaluating the system con-
tains 800 sentences, 1134 aspect term occurrences,
1025 aspect category occurrences, 5 different as-
pect categories and 555 distinct aspect terms. All
these belong to the restaurant domain.
3.1 Terms and Category Detection
When evaluating aspect terms and aspect cate-
gories detection, three measures were taken into
account: precision, recall and the f1-measure.
For both aspect term extraction and aspect cat-
egory detection, the baseline methodologies are
presented in (Pontiki et al., 2014). Table 1 shows
the results obtained using our approach as com-
pared to the baseline for aspect term detection,
whereas Table 2 outlines the results regarding as-
pect category detection in terms of the previously
mentioned measures.
Furthermore, it is interesting to notice the in-
crease in performance obtained by combining the
bag-of-words features with the output of the parser
as opposed to just using the bag-of words. These
Method Precision Recall F-Measure
Baseline 0.627329 0.376866 0.470862
XRCE 0.862453 0.818342 0.839818
Table 1: Aspect term detection.
Method Precision Recall F-Measure
Baseline 0.637500 0.483412 0.549865
BOW 0.77337 0.799024 0.785988
XRCE 0.832335 0.813658 0.822890
Table 2: Aspect category detection.
differences are outlined for aspect category detec-
tion in Table 2, where BOW denotes the system
using the same settings, but just the bag-of-words
features and XRCE denotes the submitted system
where the bag-of-words features are augmented
with parser output features.
For both tasks of aspect term and aspect cate-
gory detection, our system clearly outperforms the
baseline, resulting in being ranked among the first
3 in the competition for the restaurant corpus.
3.2 Terms and Category Polarity Detection
Similarly, Table 3 shows the results in terms of
accuracy on aspect term polarity detection and on
aspect category polarity detection. Here, baseline
methodologies are similar to the ones used for as-
pect category detection and also described in (Pon-
tiki et al., 2014). Again, our system ranks high in
the competition, achieving an overall accuracy of
0.77 for aspect term polarity detection and 0.78 for
aspect category polarity detection. Furthermore, a
comparison is also made between the current sys-
tem and one that, using the same settings, would
not take into account the features provided by the
parser (BOW). The results emphasize the impor-
tance of using the merged version.
Method Task Accuracy
Baseline Term polarity 0.552239
XRCE Term polarity 0.776895
Baseline Category polarity 0.563981
BOW Category polarity 0.681951
XRCE Category polarity 0.781463
Table 3: Aspect term and aspect category polarity.
841
Label Precision Recall F-measure
conflict NaN 0 NaN
negative 0.7857 0.7296 0.7566
neutral 0.5833 0.3214 0.4145
positive 0.7998 0.9272 0.8588
Table 4: Aspect term polarity (2).
Label Precision Recall F-measure
conflict 0.5333 0.1538 0.2388
negative 0.726 0.6802 0.7023
neutral 0.5119 0.4574 0.4831
positive 0.8343 0.9117 0.8713
Table 5: Aspect category polarity (2).
3.3 Error Analysis
The results obtained with our system are unar-
guably competitive, but some remarks can be
made regarding the most frequent causes of er-
rors. In the task of aspect category classification,
the choice of the threshold (0.25) may have con-
stituted a factor impacting the performance. In
the task of aspect term detection, the lexical cov-
erage is one of the factors to explain the difference
in performance between training/trial data and test
data.
Table 4 contains the results obtained in terms
of precision, recall and F-measure for each of the
possible polarities for terms (positive, negative,
neutral and conflict) and similarly does Table 5
for category polarities. In both cases we notice a
clear decrease for these measures when predicting
the conflict and neutral classes, with a higher de-
crease in the case of aspect term polarity detection.
This can be explained by the fact that the syntactic
parser was primarily customized to detect the neg-
ative and positive labels. This obviously had an
impact on the final results as the information from
the parser constituted some of the input features
for the classification.
4 Conclusion
The combination of a symbolic parser, customized
with specialized lexicons, with SVM classifiers
proved to be an interesting platform to implement
a category/polarity detection system. The sym-
bolic parser on the one hand provides a versatile
architecture to add lexical and multi-words infor-
mation, augmented with specific rules, in order to
feed classifiers with high quality features. How-
ever, some work will be needed to improve per-
formances on the neutral and conflict polarities,
which rely less on specific words, than on a more
global interpretation of the content.
Acknowledgements
We would like to thank the Semeval task 4
organizers, as well as our colleague, Vassilina
Nikoulina, for her help on this project.
References
Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2001. A multi-input dependency parser. In
IWPT.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In In HLT-
NAACL 2007, pages 308?315.
Caroline Brun. 2011. Detecting opinions using deep
syntactic analysis. In RANLP, pages 392?398.
Caroline Brun. 2012. Learning opinionated patterns
for contextual opinion detection. In COLING, pages
165?174.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In International
Workshop on Semantic Evaluation (SemEval).
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
842
