Coling 2010: Poster Volume, pages 445?453,
Beijing, August 2010
Morphological analysis can improve a CCG parser for English
Matthew Honnibal, Jonathan K. Kummerfeld and James R. Curran
School of Information Technologies
University of Sydney
{mhonn,jono,james}@it.usyd.edu.au
Abstract
Because English is a low morphology lan-
guage, current statistical parsers tend to
ignore morphology and accept some level
of redundancy. This paper investigates
how costly such redundancy is for a lex-
icalised grammar such as CCG.
We use morphological analysis to split
verb inflectional suffixes into separate to-
kens, so that they can receive their own
lexical categories. We find that this im-
proves accuracy when the splits are based
on correct POS tags, but that errors in gold
standard or automatically assigned POS
tags are costly for the system. This shows
that the parser can benefit from morpho-
logical analysis, so long as the analysis is
correct.
1 Introduction
English is a configurational language, so gram-
matical functions are mostly expressed through
word order and function words, rather than with
inflectional morphology. Most English verbs have
four forms, and none have more than five. Most of
the world?s languages have far richer inflectional
morphology, some with millions of possible in-
flection combinations.
There has been much work on addressing the
sparse data problems rich morphology creates, but
morphology has received little attention in the En-
glish statistical parsing literature. We suggest that
English morphology may prove to be an under-
utilised aspect of linguistic structure that can im-
prove the performance of an English parser. En-
glish also has a rich set of resources available, so
an experiment that is difficult to perform with an-
other language may be easier to conduct in En-
glish, and a technique that makes good use of En-
glish morphology may transfer well to a morpho-
logically rich language. under-exploited in En-
glish natural language
In this paper, we show how morphological
information can improve an English statistical
parser based on a lexicalised formalism, Com-
binatory Categorial Grammar (CCG, Steedman,
2000), using a technique suggested for Turkish
(Bozsahin, 2002) and Korean (Cha et al, 2002).
They describe how a morphologically rich lan-
guage can be analysed efficiently with CCG by
splitting off inflectional affixes as morphological
tokens. This allows the affix to receive a cate-
gory that performs the feature coercion. For in-
stance, sleeping would ordinarily be assigned the
category S [ng ]\NP : a sentence with the [ng ] fea-
ture requiring a leftward NP argument. We split
the word into two tokens:
sleep -ing
S [b]\NP (S [ng ]\NP)\(S [b]\NP)
The additional token creates a separate space
for inflectional information, factoring it away
from the argument structure information.
Even with only 5 verb forms in English, we
found that accurate morphological analysis im-
proved parser accuracy. However, the system had
trouble recovering from analysis errors caused by
incorrect POS tags.
We then tested how inflection categories in-
teracted with hat categories, a linguistically-
motivated extension to the formalism, proposed
by Honnibal and Curran (2009), that introduces
some sparse data problems but improves parser
effiency. The parser?s accuracy improved by 0.8%
when gold standard POS tags were used, but not
with automatic POS tags. Our method addresses
problems caused by even low morphology, and
future work will make the system more robust to
POS tagging errors.
445
2 Combinatory Categorial Grammar
Combinatory Categorial Grammar (CCG, Steed-
man, 2000) is a lexicalised grammar, which means
that each word in the sentence is associated with
a category that specifies its argument structure
and the type and features of the constituent that
it heads. For instance, in might head a PP -typed
constituent with one NP -typed argument, written
as PP/NP . The / operator denotes an argument
to the right; \ denotes an argument to the left.
For example, a transitive verb is a function from
a rightward NP to and a leftward NP to a sen-
tence, (S\NP)/NP . The grammar consists of a
few schematic rules to combine the categories:
X /Y Y ?> X
Y X \Y ?< X
X /Y Y /Z ?>B X /Z
Y \Z X \Y ?<B X \Z
Y /Z X \Y ?<B? X /Z
CCGbank (Hockenmaier and Steedman, 2007)
extends this grammar with a set of type-changing
rules, designed to strike a better balance between
sparsity in the category set and ambiguity in the
grammar. We mark such productions TC.
In wide-coverage descriptions, categories are
generally modelled as typed feature structures
(Shieber, 1986), rather than atomic symbols. This
allows the grammar to include head indices, and
to unify under-specified features. In our nota-
tion features are annotated in square-brackets, e.g.
S [dcl ]. Head-finding indices are annotated on
categories as subscripts, e.g. (NPy\NPy)/NPz .
We occasionally abbreviate S\NP as VP , and
S [adj ]\NP as ADJ .
2.1 Statistical CCG parsing and morphology
In CCGbank, there are five features that are
largely governed by the inflection of the verb:
writes/wrote (S [dcl ]\NP)/NP
(was) written (S [pss]\NP)/NP
(has) written (S [pt ]\NP)/NP
(is) writing (S [ng ]\NP)/NP
(to) write (S [b]\NP)/NP
The features are necessary for satisfactory anal-
yses. Without inflectional features, there is no
way to block over-generation like has running or
was ran. However, the inflectional features also
create a level of redundancy if the different in-
flected forms are treated as individual lexical en-
tries. The different inflected forms of a verb will
all share the same set of potential argument struc-
tures, so some way of grouping the entries to-
gether is desirable.
Systems like the PET HPSG parser (Oepen et al,
2004) and the XLE LFG parser (Butt et al, 2006)
use a set of lexical rules that match morphologi-
cal operations with transformations on the lexical
categories. For example, a lexical rule is used to
ensure that an intransitive verb like sleeping re-
ceives the same argument structure as the base
form sleep, but with the appropriate inflectional
feature. This scheme works well for rule-based
parsers, but it is less well suited for statistical
parsers, as the rules propose categories but do not
help the model estimate their likelihood or assign
them feature weights.
Statistical parsers for lexicalised formalisms
such as CCG are very sensitive to the number of
categories in the lexicon and the complexity of
the mapping between words and categories. The
sub-task of assigning lexical categories, supertag-
ging (Bangalore and Joshi, 1999), is most of the
parsing task. Supertaggers mitigate sparse data
problems by using a label frequency threshold to
prune rare categories from the search space. Clark
and Curran (2007) employ a tag dictionary that re-
stricts the model to assigning word/category pairs
seen in the training data for frequent words.
The tag dictionary causes some level of under-
generation, because not all valid word/category
pairs will occur in the limited training data avail-
able. The morphological tokens we introduce help
to mitigate this, by bringing together what were
distinct verbs and argument structures, using lem-
matisation and factoring inflection away from ar-
gument structures. The tag dictionaries for the in-
flectional morphemes will have very high cover-
age, because there are only a few inflectional cat-
egories and a few inflectional types.
3 Inflectional Categories
We implement the morphemic categories that
have been discussed in the CCG literature
446
be ?ing good and do ?ing good
(S [b]\NP)/ADJ (S [ng ]\NP)\(S [b]\NP) ADJ conj (S [b]\NP)/NP (S [ng ]\NP)\(S [b]\NP) NP
<B? <B?
(S [ng ]\NP)/ADJ (S [ng ]\NP)/NP
> >
S [ng ]\NP S [ng ]\NP
<?>
(S [ng ]\NP)\(S [ng ]\NP)
<
S [ng ]\NP
Figure 1: A single inflection category (in bold) can serve many different argument structures.
(Bozsahin, 2002; Cha et al, 2002). The inflected
form is broken into two morphemes, and each is
assigned a category. The category for the inflec-
tional suffix is a function from a category with the
bare-form feature [b] to a category that has an in-
flectional feature. This prevents verbal categories
from having to express their inflectional features
directly. Instead, their categories only have to ex-
press their argument structure.
The CCG combinators allow multiple argument
structures to share a single inflectional category.
For instance, the (S [ng ]\NP)\(S [b]\NP) cate-
gory can supply the [ng ] feature to all categories
that have one leftward NP argument and any
number of rightward arguments, via the gener-
alised backward composition combinator. Fig-
ure 1 shows this category transforming two dif-
ferent argument structures, using the backward
crossed composition rule (<B?).
Table 1 shows the most frequent inflection cat-
egories we introduce. The majority of inflected
verbs in the corpus have a subject and some num-
ber of rightward arguments, so we can almost
assign one category per feature. The most fre-
quent exceptions are participles that function as
pre-nominal modifiers and verbs of speech.
Table 2 shows the inflectional token types we
introduce and which features they correspond to.
Our scheme largely follows the Penn Treebank
tag set (Bies et al, 1995), except we avoided dis-
tinguishing past participles from past tense (-en
vs -ed), because this distinction was a significant
source of errors for our morphological analysis
process, which relies on the part-of-speech tag.
3.1 Creating Training Data
We prepared a version of CCGbank (Hocken-
maier and Steedman, 2007) with inflectional to-
kens. This involved the following steps:
Correcting POS tags: Our morphological anal-
Freq. Category Example
32.964 (S [dcl ]\NP)\(S [b]\NP) He ran
11,431 (S [pss]\NP)\(S [b]\NP) He was run down
11,324 (S [ng ]\NP)\(S [b]\NP) He was running
4,343 (S [pt ]\NP)\(S [b]\NP) He has run
3,457 (N /N )\(S [b]\NP) the running man
2,011 S [dcl ]\S ?..?, he says
1,604 (S [dcl ]\S)\(S [b]\S) ?..?, said the boy
169 (S [dcl ]\ADJ )\(S [b]\ADJ ) Here ?s the deal
55 (S [dcl ]\PP)\(S [b]\PP) On it was a bee
Table 1: The inflectional categories introduced.
Token POS Feat Example
-es VBZ dcl He write -es letters
-e VBP dcl They write -e letters
-ed VBD dcl They write -ed letters
-ed VBN pt They have write -ed letters
-ed VBN pss Letters were write -ed
-ing VBG ng They are write -ing letters
Table 2: The inflectional token types introduced.
ysis relies on the part-of-speech tags provided
with CCGbank. We identified and corrected
words whose POS tags were inconsistent with their
lexical category, as discussed in Section 3.2.
Lemmatising verbs and removing features:
We used the morphy WordNet lemmatiser imple-
mented in NLTK1 to recover the lemma of the in-
flected verbs, identified by their POS tag (VBP,
VBG, VBN or VBZ). The verb?s categories were
updated by switching their features to [b].
Deriving inflectional categories: The gener-
alised backward composition rules allow a func-
tor to generalise over some sequence of ar-
gument categories, so long as they all share
the same directionality. For instance, a func-
tor (S\NP)\(S\NP) could backward cross-
compose into a category ((S\NP)/NP)/PP to
its left, generalising over the two rightward ar-
guments that were not specified by the functor?s
argument. It could not, however, compose into
a category like ((S\NP)\NP)/PP , because the
two arguments (NP and PP ) have differing direc-
1http://www.nltk.org
447
Freq. From To Examples
1056 VBG IN including, according, following
379 VBN JJ involved, related, concerned
351 VBN IN compared, based, given
274 VBG NN trading, spending, restructuring
140 VBZ NN is, ?s, has
102 VB VBP sell, let, have
53 VBZ MD does, is, has
45 VBG JJ pending, missing, misleading
41 VBP MD do, are, have
40 VBD MD did, were, was
334 All others
2,815 Total
Table 3: The most frequent POS tag conversions.
tionalities (leftward and rightward).
Without this restriction, we would only require
one inflection category per feature, using inflec-
tional categories like S [ng ]\S [b]. Instead, our in-
flectional categories must subcategorise for every
argument except the outermost directionally con-
sistent sequence. We discard this outermost con-
sistent sequence, remove all features, and use the
resulting category as the argument and result. We
then restore the result?s feature, and set the argu-
ment?s feature to [b].
Inserting inflectional tokens: Finally, the in-
flectional token is inserted after the verb, with a
new node introduced to preserve binarisation.
3.2 POS tag corrections
Hockenmaier and Steedman (2007) corrected sev-
eral classes of POS tag errors in the Penn Treebank
when creating CCGbank. We follow Clark and
Curran (2007) in using their corrected POS labels,
but found that there were still some words with in-
consistent POS tags and lexical categories, such as
building|NN|(S[dcl]\NP)/NP.
In order to make our morphological anal-
ysis more consistent, we identify and correct
such POS tagging errors as follows. We
use two regular expressions to identify ver-
bal lexical categories and verbal POS tags:
?\(*S\[(dcl|pss|ng|pt|b)\] and
AUX|MD|V.. respectively. If a word has a
verbal lexical category and non-verbal POS, we
correct its POS tag with reference to its suffix and
its category?s inflectional feature. If a word has a
verbal POS tag and a non-verbal lexical category,
we select the POS tag that occurs most frequently
with its lexical category.
The only exception are verbs functioning as
nominal modifiers, such as running in the running
man, which are generally POS tagged VBG but re-
ceive a lexical category of N /N . We leave these
POS tagged as verbs, and instead analyse their
suffixes as performing a form-function transfor-
mation that turns them from S [b]\NP verbs into
N /N adjectives ? (N /N )\(S [b]\NP).
Table 3 lists the most common before-and-
after POS tag pairs from our corrections, and the
words that most frequently exemplified the pair.
When compiling the table some clear errors came
to light, such as the ?correction? of is|VBZ to
is|NN. These errors may explain why the POS
tagger?s accuracy drops by 0.1% on the corrected
set, and suggest that the problem of aligning POS
tags and supertags is non-trivial.
In light of these errors, we experimented
with an alternate strategy. Instead of cor-
recting the POS tags, we introduced null
inflectional categories that compensated
for bad morphological tokenisation such as
accord|VBG|(S/S)/PP -ing|VIG|-.
The null inflectional category does not interact
with the rest of the derivation, much like a punc-
tuation symbol. This performed little better than
the baseline, showing that the POS tag corrections
made an important contribution, despite the
problems with our technique.
3.3 Impact on CCGbank Lexicon
Verbal categories in CCGbank (Hockenmaier and
Steedman, 2007) record both the valency and the
inflectional morphology of the verb they are as-
signed to. This means v ? i categories are re-
quired, where v and i are the number of distinct ar-
gument structures and inflectional features in the
grammar respectively.
The inflectional tokens we propose allow in-
flectional morphology to be largely factored away
from the argument structure, so that roughly v+ i
verbal categories are required. A smaller category
set leads to lower category ambiguity, making the
assignment decision easier.
Table 4 summarises the effects of inflection cat-
egories on the lexicon extracted from CCGbank.
Clark and Curran (2007) extract a set of 425 cate-
gories from the training data (Sections 02-21) that
448
consists of all categories that occur at least 10
times. The frequency cut off is used because the
model will not have sufficient evidence to assign
the other 861 categories that occur at least once,
and their distribution is heavy tailed: together,
they only occur 1,426 times. We refer to the fre-
quency filtered set as the lexicon. The parser can-
not assign a category outside its lexicon, so gaps
in it cause under-generation.
The CCGbank lexicon includes 159 verbal cat-
egories. There are 74 distinct argument structures
and 5 distinct features among these verbal cate-
gories. The grammar Clark and Curran (2007)
learn therefore under-generates, because 211 of
the 370 (5 ? 74) argument structure and feature
combinations are rare or unattested in the training
data. For instance, there is a (S [dcl ]\NP)/PP
category, but no corresponding (S [b]\NP)/PP ,
making it impossible for the grammar to generate
a sentence like I want to talk to you, as the cor-
rect category for talk in this context is missing. It
would be trivial to add the missing categories to
the lexicon, but a statistical model would be un-
able to reproduce them. There are 8 occurrences
of such missing categories in Section 00, the de-
velopment data.
The reduction in data sparsity brought by the in-
flection categories causes 22 additional argument
structures to cross the frequency threshold into
the lexicon. A grammar induced from this cor-
pus is thus able to generate 480 (96?5) argument
structure and feature combinations, three times as
many as could be generated before.
We introduce 15 inflectional categories in the
corpus. The ten most frequent are shown in Table
1. The combinatory rules allow these 15 inflection
categories to serve 96 argument structures, reduc-
ing the number of verbal categories in the lexicon
from 159 to 89 (74 + 15).
The statistics at frequency 1 are less reliable,
because many of the categories may be linguisti-
cally spurious: they may be artefacts caused by
annotation noise in the Penn Treebank, or the
conversion heuristics used by Hockenmaier and
Steedman (2007).
? CCGbank +Inflect
Inflection categories 10 0 15
Argument structures 10 74 96
Verb categories generated 10 159 480
All categories 10 425 375
Inflection categories 1 0 31
Argument structures 1 283 283
Verbs categories generated 1 498 1415
All categories 1 1285 1120
Table 4: Effect of inflection tokens on the category set for
categories with frequency ? 10 and ? 1
3.4 Configuration of parsing experiments
We conducted two sets of parsing experiments,
comparing the impact of inflectional tokens on
CCGbank (Hockenmaier and Steedman, 2007)
and hat CCGbank (Honnibal and Curran, 2009).
The experiments allow us to gauge the impact of
inflectional tokens on versions of CCGbank with
differing numbers of verbal categories.
We used revision 1319 of the C&C parser2
(Clark and Curran, 2007), using the best-
performing configuration they describe, which
used the hybrid dependency model. The most
important hyper-parameters in their configuration
are the ? and K values, which control the work-
flow between the supertagger and parser. We use
the Honnibal and Curran (2009) values of these
parameters in our hat category experiments, de-
scribed in Section 5.
Accuracy was evaluated using labelled depen-
dency F -scores (LF ). CCG dependencies are la-
belled by the head?s lexical category and the ar-
gument slot that the dependency fills. We evalu-
ated the baseline and inflection parsers on the un-
modified dependencies, to allow direct compari-
son. For the inflection parsers, we pre-processed
the POS-tagged input to introduce inflection to-
kens, and post-processed it to remove them.
We follow Clark and Curran (2007) in not
evaluating accuracy over sentences for which the
parser returned no analysis. The percentage of
sentences analysed is described as the parser?s
coverage (C). Speed (S) figures refer to sentences
parsed per second (including failures) on a dual-
CPU Pentium 4 Xeon with 4GB of RAM.
2http://trac.ask.it.usyd.edu.au/candc
449
4 Parsing Results on CCGbank
Table 5 compares the performance of the parser
on Sections 00 and 23 with and without inflection
tokens. Section 00 was used for development ex-
periments to test different approaches, and Section
23 is the test data. Similar effects were observed
on both evaluation sections.
The inflection tokens had no significant impact
on speed or coverage, but did improve accuracy
by 0.49% F -measure when gold standard POS
tags were used, compared to the baseline. How-
ever, some of the accuracy improvement can be
attributed to the POS tag corrections described in
Section 3.2, so the improvement from the inflec-
tion tokens alone was 0.39%.
The POS tag corrections caused a large drop in
performance when automatic POS tags were used.
We attribute this to the imperfections in our cor-
rection strategy. The inflection tokens improved
the accuracy by 0.39%, but this was not large
enough to correct for the drop in accuracy caused
by the POS changes.
Another possibility is that our morphological
analysis makes POS tagger errors harder to re-
cover from. Instead of an incorrect feature value,
POS tag errors can now induce poor morphologi-
cal splits such as starl|VBG -ing|VIG. POS
tagging errors are already problematic for the C&C
parser, because only the highest ranked tag is
forwarded to the supertagger as a feature. Our
morphological analysis strategy seems to exacer-
bate this error propagation problem. Curran et al
(2006) showed that using a beam of POS tags as
features in the supertagger and parser mitigated
the loss of accuracy from POS tagging errors. Un-
fortunately, with our morphological analysis strat-
egy, POS tag variations change the tokenisation
of a sentence, making parsing more complicated.
Perhaps the best solution would be to address the
tagging errors in the treebank more thoroughly,
and reform the annotation scheme to deal with
particularly persistant error cases. This might im-
prove POS tag accuracy to a level where errors are
rare enough to be unproblematic.
Despite the limited morphology in English, the
inflectional tokens improved the parser?s accuracy
when gold standard POS tags were supplied. We
Gold POS Auto POS
LF S C LF S C
Baseline 00 87.19 22 99.22 85.28 24 99.11
+POS 00 87.46 24 99.16 85.04 23 99.05
+Inflect 00 87.81 24 99.11 85.33 23 98.95
Baseline 23 87.69 36 99.63 85.50 36 99.58
+POS 23 87.79 36 99.63 85.06 36 99.50
+Inflect 23 88.18 36 99.58 85.42 33 99.34
Table 5: Effect of POS changes and inflection tokens on
accuracy (LF ), speed (S) and coverage (C) on 00 and 23.
attribute the increase in accuracy to the more ef-
ficient word-to-category mapping caused by re-
placing inflected forms with lemmas, and feature-
bearing verb categories with ones that only refer to
the argument structure. We examined this hypoth-
esis by performing a further experiment, to inves-
tigate how inflection tokens interact with hat cat-
egories, which introduce additional verbal cate-
gories that represent form-function discrepancies.
5 Inflection Tokens and Hat Categories
Honnibal and Curran (2009) introduce an exten-
sion to the CCG formalism, hat categories, as an
alternative way to solve the modifier category pro-
liferation (MCP) problem. MCP is caused when
a modifier is itself modified by another modi-
fier. For instance, in the sentence he was in-
jured running with scissors, with modifies run-
ning, which modifies injured. This produces the
category ((VP\VP)\(VP\VP))/NP for with, a
rare category that is sensitive to too much of the
sentence?s structure.
Hockenmaier and Steedman (2007) address
MCP by adding type-changing rules to CCGbank.
These type-changing rules transform specific cat-
egories. They are specific to the analyses in the
corpus, unlike the standard combinators, which
are schematic and language universal. Honnibal
and Curran?s (2009) contribution is to extend the
formalism to allow these type-changing rules to
be lexically specified, restoring universality to the
grammar ? but at the cost of sparse data problems
in the lexicon. Figure 2 shows how a reduced rel-
ative clause is analysed using hat categories. The
hat category (S [pss]\NP)NP\NP is subject to the
unhat rule, which unarily replaces it with its hat,
NP\NP , allowing it to function as a modifier.
Hat categories have a practical advantage for a
parser that uses a supertagging phase (Bangalore
450
The company bought by Google last year is profitable
NP/N N (S [pss]\NP)NP\NP (VP\VP)/NP NP NPVP\VP/N N (S [dcl ]\NP)/ADJ ADJ
> > > >
NP VP\VP NPVP\VP S [dcl ]\NP
<
(S [pss]\NP)NP\NP
<
(S [pss]\NP)NP\NP
H
NP\NP
<NP
<
S [dcl ]
Figure 2: CCG derivation showing hat categories and the unhat rule.
The company buy ?ed by Google last year
NP/N N S [b]\NP (S [pss]\NP)NP\NP\(S [b]\NP) (VP\VP)/NP NP NPVP\VP/N N
> < > >
NP (S [pss]\NP)NP\NP VP\VP NPVP\VP
< H
(S [pss]\NP)NP\NP VP\VP
<
(S [pss]\NP)NP\NP
H
NP\NP
<NP
Figure 3: CCG derivation showing how inflectional tokens interact with hat categories.
and Joshi, 1999), such as the C&C system (Clark
and Curran, 2007). By replacing type-changing
rules with additional lexical categories, more of
the work is shifted to the supertagger. The su-
pertagging phase is much more efficient than the
chart parsing stage, so redistribution of labour
makes the parser considerably faster.
Honnibal and Curran (2009) found that the
parser was 37% faster on the test set, at a cost
of 0.5% accuracy. They attribute the drop in ac-
curacy to sparse data problems for the supertag-
ger, due to the increase in the number of lexical
categories. We hypothesised that inflectional cate-
gories could address this problem, as the two anal-
yses interact well.
5.1 Analyses with inflectional hat categories
Using hat categories to lexicalise type-changing
rules offers attractive formal properties, and some
practical advantages. However, it also misses
some generalisations. A type-changing operation
such as S [ng ]\NP ? NP\NP must be avail-
able to any VP. If we encounter a new word, The
company is blagging its employees, we can gen-
eralise to the reduced relative form, She works for
that company blagging its employees with no ad-
ditional information.
This property could be preserved with some
form of lexical rule, but a novel word-category
pair is difficult for a statistical model to assign.
Inflection tokens offer an attractive solution to this
problem, as shown in Figure 3. Assigning the hat
category to the suffix makes it available to any
verb the suffix follows ? it is just another func-
tion the inflectional suffix can perform. This gen-
erality also makes it much easier to learn, because
it does not matter whether the training data hap-
pens to contain examples of a given verb perfom-
ing that grammatical function.
We prepared a version of the Honnibal and
Curran (2009) hat CCGbank, moving hats on to
inflectional categories wherever possible. The
hat CCGbank?s lexicon contained 105 hat cate-
gories, of which 77 were assigned to inflected
verbs. We introduced 33 inflection hat cate-
gories in their place, reducing the number of
hat categories by 27.9%. Fewer hat categories
were required because different argument struc-
tures could be served by the same inflection cat-
egory. For instance, the (S [ng ]\NP)NP\NP and
(S [ng ]\NP)NP\NP/NP categories were both re-
placed by the (S [ng ]\NP)NP\NP\(S [b]\NP)
category. Table 6 lists the most frequent inflection
hat categories we introduce.
451
Freq. Category
3332 (S [pss]\NP)NP\NP\(S [b]\NP)
1518 (S [ng ]\NP)NP\NP\(S [b]\NP)
1231 (S [ng ]\NP)(S\NP)\(S\NP)\(S [b]\NP)
360 ((S [dcl ]\NP)/NP)NP\NP\((S [b]\NP)/NP)
316 (S [ng ]\NP)NP\(S [b]\NP)
234 ((S [dcl ]\NP)/S)S/S\((S [b]\NP)/S)
209 (S [ng ]\NP)S/S\(S [b]\NP)
162 (S [dcl ]NP\NP\NP)\(S [b]\NP)
157 ((S [dcl ]\NP)/S)VP/VP\((S [b]\NP)/S)
128 (S [pss]\NP)S/S\(S [b]\NP)
Table 6: The most frequent inflection hat categories.
5.2 Parsing results
Table 7 shows the hat parser?s performance with
and without inflectional categories. We used the
values for the ? and K hyper-parameters de-
scribed by Honnibal and Curran (2009). These
hyper-parameters were tuned on Section 00, and
some over-fitting seems apparent. We also fol-
lowed their dependency conversion procedure, to
allow evaluation over the original CCGbank de-
pendencies and thus direct comparison with Table
5. We also merged the parser changes they de-
scribed into the development version of the C&C
parser we are using, for parse speed comparison.
Interestingly, incorporating the hat changes into
the current version has increased the advantage
of the hat categories. Honnibal and Curran re-
port a 37% improvement in speed for the hybrid
model (which we are using) on Section 23, using
gold standard POS tags. With our version of the
parser, the improvement is 86% (36 vs. 67 sen-
tences parsed per second).
With gold standard POS tags, the inflection to-
kens improved the hat parser?s accuracy by 0.8%,
but decreased its speed by 24%. We attribute
the decrease in speed to the increase in sentence
length coupled with the new uncertainty on the
inflectional tokens. Coverage increased slightly
with gold standard POS tags, but decreased with
automatic POS tags. We attribute this to the fact
that POS tagging errors lead to morphological
analysis errors.
The accuracy improvement on the hat corpus
was more robust to POS tagging errors than the
CCGbank results, however. This may be be-
cause POS tagging errors are already quite prob-
lematic for the hat category parser. POS tag fea-
Gold POS Auto POS
LF S C LF S C
Hat baseline 00 87.08 32 99.53 84.67 34 99.32
Hat inflect 00 87.85 37 99.63 84.99 30 98.95
Hat baseline 23 87.26 67 99.50 84.93 53 99.58
Hat inflect 23 88.06 54 99.63 85.25 43 99.38
Table 7: Effect of inflection tokens on accuracy (LF ),
speed (S) and coverage (C) on Sections 00 and 23.
tures are more important for the supertagger than
the parser, and the supertagger performs more of
the work for the hat parser.
6 Conclusion
Lexicalised formalisms like CCG (Steedman,
2000) and HPSG (Pollard and Sag, 1994) have
led to high-performance statistical parsers of En-
glish, such as the C&C CCG parser (Clark and
Curran, 2007) and the ENJU HPSG (Miyao and
Tsuji, 2008) parser. The performance of these
parsers can be partially attributed to their theoret-
ical foundations. This is particularly true of the
C&C parser, which exploits CCG?s lexicalisation
to divide the parsing task between two integrated
models (Clark and Curran, 2004).
We have followed this formalism-driven ap-
proach by exploiting morphology for English syn-
tactic parsing, using a strategy designed for mor-
phologically rich languages. Combining our tech-
nique with hat categories leads to a 20% improve-
ment in efficiency, with a 0.25% loss of accuracy.
If the POS tag error problem were addressed, the
two strategies combined would improve efficiency
by 50%, and improve accuracy by 0.37%. These
results illustrate that linguistically motivated solu-
tions can produce substantial practical advantages
for language technologies.
Acknowledgments
We would like to thank the anonymous reviewers
for their feedback, and the members of the CCG-
technicians mailing list for discussion about some
of our analyses. Matthew Honnibal was supported
by Australian Research Council (ARC) Discovery
Grant DP0665973. James Curran was supported
by ARC Discovery grant DP1097291 and the Cap-
ital Markets Cooperative Research Centre.
452
References
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237?265.
Ann Bies, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing guidelines
for Treebank II style Penn Treebank project.
Technical report, MS-CIS-95-06, University of
Pennsylvania, Philadelphia, PA, USA.
Cem Bozsahin. 2002. The combinatory mor-
phemic lexicon. Computational Linguistics,
28(2):145?186.
Miriam Butt, Mary Dalrymple, and Tracy H.
King, editors. 2006. CSLI Publications, Stan-
ford, CA.
Jeongwon Cha, Geunbae Lee, and Jonghyeok
Lee. 2002. Korean Combinatory Categorial
Grammar and statistical parsing. Computers
and the Humanities, 36(4):431?453.
Stephen Clark and James R. Curran. 2004. The
importance of supertagging for wide-coverage
CCG parsing. In Proceedings of 20th Interna-
tional Conference on Computational Linguis-
tics, pages 282?288. Geneva, Switzerland.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguis-
tics, 33(4):493?552.
James R. Curran, Stephen Clark, and David
Vadas. 2006. Multi-tagging for lexicalized-
grammar parsing. In Proceedings of the Joint
Conference of the International Committee on
Computational Linguistics and the Association
for Computational Linguistics, pages 697?704.
Sydney, Austrailia.
Julia Hockenmaier and Mark Steedman. 2007.
CCGbank: a corpus of CCG derivations
and dependency structures extracted from the
Penn Treebank. Computational Linguistics,
33(3):355?396.
Matthew Honnibal and James R. Curran. 2009.
Fully lexicalising CCGbank with hat cate-
gories. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 1212?1221. Singapore.
Yusuke Miyao and Jun?ichi Tsuji. 2008. Feature
forest models for probabilistic HPSG parsing.
Computational Linguistics, 34(1):35?80.
Stepan Oepen, Daniel Flickenger, Kristina
Toutanova, and Christopher D. Manning. 2004.
LinGO Redwoods. a rich and dynamic treebank
for HPSG. Research on Language and Compu-
tation, 2(4):575?596.
Carl Pollard and Ivan Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago.
Stuart M. Shieber. 1986. An Introduction to
Unification-Based Approaches to Grammar,
volume 4 of CSLI Lecture Notes. CSLI Pub-
lications, Stanford, CA.
Mark Steedman. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA, USA.
453
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1048?1059, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Parser Showdown at the Wall Street Corral:
An Empirical Investigation of Error Types in Parser Output
Jonathan K. Kummerfeld? David Hall? James R. Curran? Dan Klein?
?Computer Science Division ? e-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,dlwh,klein}@cs.berkeley.edu james@it.usyd.edu.au
Abstract
Constituency parser performance is primarily
interpreted through a single metric, F-score
on WSJ section 23, that conveys no linguis-
tic information regarding the remaining errors.
We classify errors within a set of linguisti-
cally meaningful types using tree transforma-
tions that repair groups of errors together. We
use this analysis to answer a range of ques-
tions about parser behaviour, including what
linguistic constructions are difficult for state-
of-the-art parsers, what types of errors are be-
ing resolved by rerankers, and what types are
introduced when parsing out-of-domain text.
1 Introduction
Parsing has been a major area of research within
computational linguistics for decades, and con-
stituent parser F-scores on WSJ section 23 have ex-
ceeded 90% (Petrov and Klein, 2007), and 92%
when using self-training and reranking (McClosky
et al 2006; Charniak and Johnson, 2005). While
these results give a useful measure of overall per-
formance, they provide no information about the na-
ture, or relative importance, of the remaining errors.
Broad investigations of parser errors beyond the
PARSEVAL metric (Abney et al 1991) have either
focused on specific parsers, e.g. Collins (2003), or
have involved conversion to dependencies (Carroll
et al 1998; King et al 2003). In all of these cases,
the analysis has not taken into consideration how a
set of errors can have a common cause, e.g. a single
mis-attachment can create multiple node errors.
We propose a new method of error classifica-
tion using tree transformations. Errors in the parse
tree are repaired using subtree movement, node cre-
ation, and node deletion. Each step in the process is
then associated with a linguistically meaningful er-
ror type, based on factors such as the node that is
moved, its siblings, and parents.
Using our method we analyse the output of thir-
teen constituency parsers on newswire. Some of
the frequent error types that we identify are widely
recognised as challenging, such as prepositional
phrase (PP) attachment. However, other significant
types have not received as much attention, such as
clause attachment and modifier attachment.
Our method also enables us to investigate where
reranking and self-training improve parsing. Pre-
viously, these developments were analysed only in
terms of their impact on F-score. Similarly, the chal-
lenge of out-of-domain parsing has only been ex-
pressed in terms of this single objective. We are able
to decompose the drop in performance and show that
a disproportionate number of the extra errors are due
to coordination and clause attachment.
This work presents a comprehensive investigation
of parser behaviour in terms of linguistically mean-
ingful errors. By applying our method to multiple
parsers and domains we are able to answer questions
about parser behaviour that were previously only ap-
proachable through approximate measures, such as
counts of node errors. We show which errors have
been reduced over the past fifteen years of parsing
research; where rerankers are making their gains and
where they are not exploiting the full potential of k-
best lists; and what types of errors arise when mov-
ing out-of-domain. We have released our system1 to
enable future work to apply our methodology.
1http://code.google.com/p/berkeley-parser-analyser/
1048
2 Background
Most attempts to understand the behaviour of con-
stituency parsers have focused on overall evaluation
metrics. The three main methods are intrinsic eval-
uation with PARSEVAL, evaluation on dependencies
extracted from the constituency parse, and evalua-
tion on downstream tasks that rely on parsing.
Intrinsic evaluation with PARSEVAL, which calcu-
lates precision and recall over labeled tree nodes, is
a useful indicator of overall performance, but does
not pinpoint which structures the parser has most
difficulty with. Even when the breakdown for par-
ticular node types is presented (e.g. Collins, 2003),
the interaction between node errors is not taken into
account. For example, a VP node could be missing
because of incorrect PP attachment, a coordination
error, or a unary production mistake. There has been
some work that addresses these issues by analysing
the output of constituency parsers on linguistically
motivated error types, but only by hand on sets of
around 100 sentences (Hara et al 2007; Yu et al
2011). By automatically classifying parse errors we
are able to consider the output of multiple parsers on
thousands of sentences.
The second major parser evaluation method in-
volves extraction of grammatical relations (King et
al., 2003; Briscoe and Carroll, 2006) or dependen-
cies (Lin, 1998; Briscoe et al 2002). These met-
rics have been argued to be more informative and
generally applicable (Carroll et al 1998), and have
the advantage that the breakdown over dependency
types is more informative than over node types.
There have been comparisons of multiple parsers
(Foster and van Genabith, 2008; Nivre et al 2010;
Cer et al 2010), as well as work on finding rela-
tions between errors (Hara et al 2009), and break-
ing down errors by a range of factors (McDonald and
Nivre, 2007). However, one challenge is that results
for constituency parsers are strongly influenced by
the dependency scheme being used and how easy it
is to extract the dependencies from a given parser?s
output (Clark and Hockenmaier, 2002). Our ap-
proach does not have this disadvantage, as we anal-
yse parser output directly.
The third major approach involves extrinsic eval-
uation, where the parser?s output is used in a down-
stream task, such as machine translation (Quirk
and Corston-Oliver, 2006), information extraction
(Miyao et al 2008), textual entailment (Yuret et
al., 2010), or semantic dependencies (Dridan and
Oepen, 2011). While some of these approaches give
a better sense of the impact of parse errors, they re-
quire integration into a larger system, making it less
clear where a given error originates.
The work we present here differs from existing
approaches by directly and automatically classifying
errors into meaningful types. This enables the first
very broad, yet detailed, study of parser behaviour,
evaluating the output of thirteen parsers over thou-
sands of sentences.
3 Parsers
Our evaluation is over a wide range of PTB con-
stituency parsers and their variants from the past fif-
teen years. For all parsers we used the publicly avail-
able version, with the standard parameter settings.
Berkeley (Petrov et al 2006; Petrov and Klein,
2007). An unlexicalised parser with a grammar
constructed with automatic state splitting.
Bikel (2004) implementation of Collins (1997).
BUBS (Dunlop et al 2011; Bodenstab et al
2011). A ?grammar-agnostic constituent
parser,? which uses a Berkeley Parser grammar,
but parses with various pruning techniques to
improve speed, at the cost of accuracy.
Charniak (2000). A generative parser with a max-
imum entropy-inspired model. We also use the
reranker (Charniak and Johnson, 2005), and the
self-trained model (McClosky et al 2006).
Collins (1997). A generative lexicalised parser,
with three models, a base model, a model that
uses subcategorisation frames for head words,
and a model that takes into account traces.
SSN (Henderson, 2003; Henderson, 2004). A sta-
tistical left-corner parser, with probabilities es-
timated by a neural network.
Stanford (Klein and Manning, 2003a; Klein and
Manning, 2003b). We consider both the un-
lexicalised PCFG parser (-U) and the factored
parser (-F), which combines the PCFG parser
with a lexicalised dependency parser.
1049
System F P R Exact Speed
ENHANCED TRAINING / SYSTEMS
Charniak-SR 92.07 92.44 91.70 44.87 1.8
Charniak-R 91.41 91.78 91.04 44.04 1.8
Charniak-S 91.02 91.16 90.89 40.77 1.8
STANDARD PARSERS
Berkeley 90.06 90.30 89.81 36.59 4.2
Charniak 89.71 89.88 89.55 37.25 1.8
SSN 89.42 89.96 88.89 32.74 1.8
BUBS 88.50 88.57 88.43 31.62 27.6
Bikel 88.16 88.23 88.10 32.33 0.8
Collins-3 87.66 87.82 87.50 32.22 2.0
Collins-2 87.62 87.77 87.48 32.51 2.2
Collins-1 87.09 87.29 86.90 30.35 3.3
Stanford-L 86.42 86.35 86.49 27.65 0.7
Stanford-U 85.78 86.48 85.09 28.35 2.7
Table 1: PARSEVAL results on WSJ section 23 for the
parsers we consider. The columns are F-score, precision,
recall, exact sentence match, and speed (sents/sec). Cov-
erage was left out as it was above 99.8% for all parsers.
In the ENHANCED TRAINING / SYSTEMS section we in-
clude the Charniak parser with reranking (R), with a self-
trained model (S), and both (SR).
Table 1 shows the standard performance metrics,
measured on section 23 of the WSJ, using all sen-
tences. Speeds were measured using a Quad-Core
Xeon CPU (2.33GHz 4MB L2 cache) with 16GB
of RAM. These results clearly show the variation in
parsing performance, but they do not show which
constructions are the source of those variations.
4 Error Classification
While the statistics in Table 1 give a sense of over-
all parser performance they do not provide linguisti-
cally meaningful intuition for the source of remain-
ing errors. Breaking down the remaining errors by
node type is not particularly informative, as a sin-
gle attachment error can cause multiple node errors,
many of which are for unrelated node types. For
example, in Figure 1 there is a PP attachment error
that causes seven bracket errors (extra S, NP, PP, and
NP, missing S, NP, and PP). Determining that these
correspond to a PP attachment error from just the la-
bels of the missing and extra nodes is difficult. In
contrast, the approach we describe below takes into
consideration the relations between errors, grouping
them into linguistically meaningful sets.
We classify node errors in two phases. First, we
S
VP
VP
S
NP
PP
NP
PP
in 1986
NP
NNP
Applied
IN
of
NP
chief executive officer
VBN
named
VBD
was
NP
PRP
He
(a) Parser output
S
VP
VP
PP
in 1986
S
NP
PP
NP
NNP
Applied
IN
of
NP
chief executive officer
VBN
named
VBD
was
NP
PRP
He
(b) Gold tree
Figure 1: Grouping errors by node type is of limited use-
fulness. In this figure and those that follow the top tree
is the incorrect parse and the bottom tree is the correct
parse. Bold, boxed nodes are either extra (marked in the
incorrect tree) or missing (marked in the correct tree).
This is an example of PP Attachment (in 1986 is too
low), but that is not at all clear from the set of incorrect
nodes (extra S, NP, PP, and NP, missing S, NP, and PP).
find a set of tree transformations that convert the out-
put tree into the gold tree. Second, the transforma-
tion are classified into error types such as PP attach-
ment and coordination. Pseudocode for our method
is shown in Algorithm 1. The tree transformation
stage corresponds to the main loop, while the sec-
ond stage corresponds to the final loop.
4.1 Tree Transformation
The core of our transformation process is a set of op-
erations that move subtrees, create nodes, and delete
nodes. Searching for the shortest path to transform
one tree into another is prohibitively slow.2 We find
2We implemented various search procedures and found sim-
ilar results on the sentences that could be processed in a reason-
1050
Algorithm 1 Tree transformation error classification
U = initial set of node errors
Sort U by the depth of the error in the tree, deepest first
G = ?
repeat
for all errors e ? U do
if e fits an environment template t then
g = new error group
Correct e as specified by t
for all errors f that t corrects do
Remove f from U
Insert f into g
end for
Add g to G
end if
end for
until unable to correct any further errors
for all remaining errors e ? U do
Insert a group into G containing e
end for
for all groups g ? G do
Classify g based on properties of the group
end for
a path by applying a greedy bottom?up approach,
iterating through the errors in order of tree depth.
We match each error with a template based on
nearby tree structure and errors. For example, in
Figure 1 there are four extra nodes that all cover
spans ending at Applied in 1986: S, NP, PP, NP.
There are also three missing nodes with spans end-
ing between Applied and in: PP, NP, and S. Figure 2
depicts these errors as spans, showing that this case
fits three criteria: (1) there are a set of extra spans all
ending at the same point, (2) there are a set of miss-
ing spans all ending at the same point, and (3) the ex-
tra spans cross the missing spans, extending beyond
their end-point. This indicates that the node start-
ing after Applied is attaching too low and should be
moved up, outside all of the extra nodes. Together,
the criteria and transformation form a template.
Once a suitable template is identified we correct
the error by moving subtrees, adding nodes and re-
moving nodes. In the example this is done by mov-
ing the node spanning in 1986 up in the tree until it
is outside of all the extra spans. Since moving the PP
leaves a unary production from an NP to an NP, we
also collapse that level. In total this corrects seven
able amount of time.
named chief executive officer of Applied in 1986
Figure 2: Templates are defined in terms of extra and
missing spans, shown here with unbroken lines above and
dashed lines below, respectively. This is an example of a
set of extra spans that cross a set of missing spans (which
in both cases all end at the same position). If the last two
words are moved, two of the extra spans will match the
two missing spans. The other extra span is deleted during
the move as it creates an NP?NP unary production.
errors, as there are three cases in which an extra node
is present that matches a missing node once the PP
is moved. All of these errors are placed in a single
group and information about the nearby tree struc-
ture before and after the transformation is recorded.
We continue to make passes through the list until
no errors are corrected on a pass. For each remaining
node error an individual error group is created.
The templates were constructed by hand based on
manual analysis of parser output. They cover a range
of combinations of extra and missing spans, with
further variation for whether crossing is occurring
and if so whether the crossing bracket starts or ends
in the middle of the correct bracket. Errors that do
not match any of our templates are left uncorrected.
4.2 Transformation Classification
We began with a large set of node errors, in the first
stage they were placed into groups, one group per
tree transformation used to get from the test tree to
the gold tree. Next we classify each group as one of
the error types below.
PP Attachment Any case in which the transforma-
tion involved moving a Prepositional Phrase, or
the incorrect bracket is over a PP, e.g.
He was (VP named chief executive officer of
fill(NP Applied (PP in 1986)))
where (PP in 1986) should modify the entire
VP, rather than just Applied.
NP Attachment Several cases in which NPs had to
be moved, particularly for mistakes in appos-
itive constructions and incorrect attachments
within a verb phrase, e.g.
The bonds (VP go (PP on sale (NP Oct. 19)))
where Oct. 19 should be an argument of go.
1051
VP
NP
NN
today
NP
VBG
appearing
NP
NN
ad
JJ
new
DT
another
VBD
wrote
(a) Parser output
VP
NP
VP
NP
NN
today
VBG
appearing
NP
NN
ad
JJ
new
DT
another
VBD
wrote
(b) Gold tree
Figure 3: NP Attachment: today is too high, it should
be the argument of appearing, rather than wrote. This
causes three node errors (extra NP, missing NP and VP).
VP
ADVP
ahead of time
S
VP
VP
PP
about it
VB
think
TO
to
VBD
had
(a) Parser output
VP
S
VP
VP
ADVP
ahead of time
PP
about it
VB
think
TO
to
VBD
had
(b) Gold tree
Figure 4: Modifier Attachment: ahead of time is too
high, it should modify think, not had. This causes six
node errors (extra S, VP, and VP, missing S, VP, and VP).
Modifier Attachment Cases involving incorrectly
placed adjectives and adverbs, including errors
corrected by subtree movement and errors re-
quiring only creation of a node, e.g.
(NP (ADVP even more) severe setbacks)
where there should be an extra ADVP node
over even more severe.
Clause Attachment Any group that involves move-
ment of some form of S node.
VP
S
VP
VP
SBAR
unless the agency . . .
NP
the RTC to . . .
VB
restrict
TO
to
VBZ
intends
(a) Parser output
VP
SBAR
unless the agency . . .
S
VP
VP
NP
the RTC to . . .
VB
restrict
TO
to
VBZ
intends
(b) Gold tree
Figure 5: Clause Attachment: unless the agency re-
ceives specific congressional authorization is attaching
too low. This causes six node errors (extra S, VP, and
VP, missing S, VP and VP).
SINV
NP
PP
of major market activity
NP
a breakdown
VBZ
is
VP
VBG
Following
(a) Parser output
SINV
NP
NP
PP
of major market activity
NP
a breakdown
VBZ
is
S
VP
VBG
Following
(b) Gold tree
SINV
:
:
NP-SBJ-1
NP
PP
of major market activity
NP
a breakdown
VBZ
is
S-ADV
VP
VBG
Following
NP-SBJ
-NONE-
*-1
(c) Gold tree with traces and function tags
Figure 6: Two Unary errors, a missing S and a missing
NP. The third tree is the PTB tree before traces and func-
tion tags are removed. Note that the missing NP is over
another NP, a production that does occur widely in the
treebank, particularly over the word it.
1052
NP
PP
NP
NP
Dresdner AG?s 10% decline
CC
and
NP
Mannesmann AG
IN
for
NP
A 16% drop
(a) Parser output
NP
NP
Dresdner AG?s 10% decline
CC
and
NP
PP
NP
Mannesmann AG
IN
for
NP
A 16% drop
(b) Gold tree
Figure 7: Coordination: and Dresdner AG?s 10% de-
cline is too low. This causes four node errors (extra PP
and NP, missing NP and PP).
Unary Mistakes involving unary productions that
are not linked to a nearby error such as a match-
ing extra or missing node. We do not include a
breakdown by unary type, though we did find
that clause labeling (S, SINV, etc) accounted
for a large proportion of the errors.
Coordination Cases in which a conjunction is an
immediate sibling of the nodes being moved, or
is the leftmost or rightmost node being moved.
NP Internal Structure While most NP structure is
not annotated in the PTB, there is some use of
ADJP, NX, NAC and QP nodes. We form a
single group for each NP that has one or more
errors involving these types of nodes.
Different label In many cases a node is present in
the tree that spans the correct set of words, but
has the wrong label, in which case we group the
two node errors, (one extra, one missing), as a
single error.
Single word phrase A range of node errors that
span a single word, with checks to ensure this
is not linked to another error (e.g. one part of a
set of internal noun phrase errors).
Other There is a long tail of other errors. Some
could be placed within the categories above,
but would require far more specific rules.
For many of these error types it would be diffi-
cult to extract a meaningful understanding from only
NP
PP
NP
NNP
Baker
NNP
State
IN
of
NNP
Secretary
(a) Parser output
NP
NNP
Baker
PP
NP
NNP
State
IN
of
NNP
Secretary
(b) Gold tree
Figure 8: NP Internal Structure: Baker is too low, caus-
ing four errors (extra PP and NP, missing PP and NP).
the list of node errors involved. Even for error types
that can be measured by counting node errors or rule
production errors, our approach has the advantage
that we identify groups of errors with a single cause.
For example, a missing unary production may corre-
spond to an extra bracket that contains a subtree that
attached incorrectly.
4.3 Methodology
We used sections 00 and 24 as development data
while constructing the tree transformation and error
group classification methods. All of our examples
in text come from these sections as well, but for all
tables of results we ran our system on section 23.
We chose to run our analysis on section 23 as it is
the only section we are sure was not used in the de-
velopment of any of the parsers, either for tuning or
feature development. Our evaluation is entirely fo-
cused on the errors of the parsers, so unless there is
a particular construction that is unusually prevalent
in section 23, we are not revealing any information
about the test set that could bias future work.
5 Results
Our system enables us to answer questions about
parser behaviour that could previously only be
probed indirectly. We demonstrate its usefulness by
applying it to a range of parsers (here), to reranked
K-best lists of various lengths, and to output for out-
of-domain parsing (following sections).
In Table 2 we consider the breakdown of parser
1053
PP Clause Diff Mod NP 1-Word NP
Parser F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.60 0.38 0.31 0.25 0.25 0.23 0.20 0.14 0.14 0.50
Charniak-RS 92.07
Charniak-R 91.41
Charniak-S 91.02
Berkeley 90.06
Charniak 89.71
SSN 89.42
BUBS 88.63
Bikel 88.16
Collins-3 87.66
Collins-2 87.62
Collins-1 87.09
Stanford-F 86.42
Stanford-U 85.78
Worst 1.12 0.61 0.51 0.39 0.45 0.40 0.42 0.27 0.27 1.13
Table 2: Average number of bracket errors per sentence due to the top ten error types. For instance, Stanford-U
produces output that has, on average, 1.12 bracket errors per sentence that are due to PP attachment. The scale for
each column is indicated by the Best and Worst values.
Nodes
Error Type Occurrences Involved Ratio
PP Attachment 846 1455 1.7
Single word phrase 490 490 1.0
Clause Attachment 385 913 2.4
Modifier Attachment 383 599 1.6
Different Label 377 754 2.0
Unary 347 349 1.0
NP Attachment 321 597 1.9
NP Internal Structure 299 352 1.2
Coordination 209 557 2.7
Unary Clause Label 185 200 1.1
VP Attachment 64 159 2.5
Parenthetical Attachment 31 74 2.4
Missing Parenthetical 12 17 1.4
Unclassified 655 734 1.1
Table 3: Breakdown of errors on section 23 for the Char-
niak parser with self-trained model and reranker. Errors
are sorted by the number of times they occur. Ratio is the
average number of node errors caused by each error we
identify (i.e. Nodes Involved / Occurrences).
errors on WSJ section 23. The shaded area of
each bar indicates the frequency of parse errors (i.e.
empty means fewest errors). The area filled in is
determined by the expected number of node errors
per sentence that are attributed to that type of error.
The average number of node errors per sentence for
a completely full bar is indicated by the Worst row,
and the value for a completely empty bar is indicated
by the Best row. Exact error counts are available at
http://code.google.com/p/berkeley-parser-analyser/.
We use counts of node errors to make the con-
tributions of each type of error more interpretable.
As Table 3 shows, some errors typically cause only
a single node error, where as others, such as co-
ordination, generally cause several. This means
that considering counts of error groups would over-
emphasise some error types, e.g. single word phrase
errors are second most important by number of
groups (in Table 3), but seventh by total number of
node errors (in Table 2).
As expected, PP attachment is the largest contrib-
utor to errors, across all parsers. Interestingly, coor-
dination is sixth on the list, though that is partly due
to the fact that there are fewer coordination decisions
to be made in the treebank.3
By looking at the performance of the Collins
parser we can see the development over the past
fifteen years. There has been improvement across
the board, but in some cases, e.g. clause attach-
ment errors and different label errors, the change has
been more limited (24% and 29% reductions respec-
tively). We investigated the breakdown of the differ-
ent label errors by label, but no particular cases of la-
3This is indicated by the frequency of CCs and PPs in sec-
tions 02?21 of the treebank, 16,844 and 95,581 respectively.
These counts are only an indicator of the number of decisions
as the nodes can be used in ways that do not involve a decision,
such as sentences that start with a conjunction.
1054
PP Clause Diff Mod NP 1-Word NP
System K F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.08 0.04 0.08 0.05 0.06 0.04 0.08 0.04 0.04 0.11
1000 98.30
100 97.54
50 97.18
Oracle 20 96.40
10 95.66
5 94.61
2 92.59
1000 92.07
100 92.08
50 92.07
Charniak 20 92.05
10 92.16
5 91.94
2 91.56
1 91.02
Worst 0.66 0.43 0.33 0.26 0.28 0.26 0.23 0.16 0.19 0.60
Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parser
with reranking and the self-trained model. The oracle results are determined by taking the parse in each K-best list
with the highest F-score.
bel confusion stand out, and we found that the most
common cases remained the same between Collins
and the top results.
It is also interesting to compare pairs of parsers
that share aspects of their architecture. One such
pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalised
dependency parser. The main sources of the 0.64
gain in F-score are PP attachment and coordination.
Another interesting pair is the Berkeley parser and
the BUBS parser, which uses a Berkeley grammar,
but improves speed by pruning. The pruning meth-
ods used in BUBS are particularly damaging for PP
attachment errors and unary errors.
Various comparisons can be made between Char-
niak parser variants. We discuss the reranker be-
low. For the self-trained model McClosky et al
(2006) performed some error analysis, considering
variations in F-score depending on the frequency of
tags such as PP, IN and CC in sentences. Here we
see gains on all error types, though particularly for
clause attachment, modifier attachment and coordi-
nation, which fits with their observations.
5.1 Reranking
The standard dynamic programming approach to
parsing limits the range of features that can be em-
ployed. One way to deal with this issue is to mod-
ify the parser to produce the top K parses, rather
than just the 1-best, then use a model with more so-
phisticated features to choose the best parse from
this list (Collins, 2000). While re-ranking has led to
gains in performance (Charniak and Johnson, 2005),
there has been limited analysis of how effectively
rerankers are using the set of available options. Re-
cent work has explored this question in more depth,
but focusing on how variation in the parameters
impacts performance on standard metrics (Huang,
2008; Ng et al 2010; Auli and Lopez, 2011; Ng
and Curran, 2012).
In Table 4 we present a breakdown over error
types for the Charniak parser, using the self-trained
model and reranker. The oracle results use the parse
in each K-best list with the highest F-score. While
this may not give the true oracle result, as F-score
does not factor over sentences, it gives a close ap-
proximation. The table has the same columns as Ta-
ble 2, but the ranges on the bars now reflect the min
and max for these sets.
While there is improvement on all errors when us-
ing the reranker, there is very little additional gain
beyond the first 5-10 parses. Even for the oracle
results, most of the improvement occurs within the
first 5-10 parses. The limited utility of extra parses
1055
PP Clause Diff Mod NP 1-Word NP
Corpus F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021
WSJ 23 92.07
Brown-F 85.91
Brown-G 84.56
Brown-K 84.09
Brown-L 83.95
Brown-M 84.65
Brown-N 85.20
Brown-P 84.09
Brown-R 83.60
G-Web Blogs 84.15
G-Web Email 81.18
Worst 0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073
Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and
the self-trained model. We use per word error rates here rather than per sentence as there is great variation in average
sentence length across the domains, skewing the per sentence results.
for the reranker may be due to the importance of
the base parser output probability feature (which, by
definition, decreases within the K-best list).
Interestingly, the oracle performance improves
across all error types, even at the 2-best level. This
indicates that the base parser model is not particu-
larly biased against a single error. Focusing on the
rows for K = 2 we can also see two interesting out-
liers. The PP attachment improvement of the ora-
cle is considerably higher than that of the reranker,
particularly compared to the differences for other er-
rors, suggesting that the reranker lacks the features
necessary to make the decision better than the parser.
The other interesting outlier is NP internal structure,
which continues to make improvements for longer
lists, unlike the other error types.
5.2 Out-of-Domain
Parsing performance drops considerably when shift-
ing outside of the domain a parser was trained on
(Gildea, 2001). Clegg and Shepherd (2005) evalu-
ated parsers qualitatively on node types and rule pro-
ductions. Bender et al(2011) designed a Wikipedia
test set to evaluate parsers on dependencies repre-
senting ten specific linguistic phenomena.
To provide a deeper understanding of the er-
rors arising when parsing outside of the newswire
domain, we analyse performance of the Charniak
parser with reranker and self-trained model on the
eight parts of the Brown corpus (Marcus et al
Corpus Description Sentences Av. Length
WSJ 23 Newswire 2416 23.5
Brown F Popular 3164 23.4
Brown G Biographies 3279 25.5
Brown K General 3881 17.2
Brown L Mystery 3714 15.7
Brown M Science 881 16.6
Brown N Adventure 4415 16.0
Brown P Romance 3942 17.4
Brown R Humour 967 22.7
G-Web Blogs Blogs 1016 23.6
G-Web Email E-mail 2450 11.9
Table 6: Variation in size and contents of the domains we
consider. The variation in average sentence lengths skews
the results for errors per sentences, and so in Table 5 we
consider errors per word.
1993), and two parts of the Google Web corpus
(Petrov and McDonald, 2012). Table 6 shows statis-
tics for the corpora. The variation in average sen-
tence lengths skew the results for errors per sen-
tence. To handle this we divide by the number of
words to determine the results in Table 5, rather than
by the number of sentences, as in previous figures.
There are several interesting features in the table.
First, on the Brown datasets, while the general trend
is towards worse performance on all errors, NP in-
ternal structure is a notable exception and in some
cases PP attachment and unaries are as well.
In the other errors we see similar patterns across
the corpora, except humour (Brown R), on which the
parser is particularly bad at coordination and clause
1056
attachment. This makes sense, as the colloquial na-
ture of the text includes more unusual uses of con-
junctions, for example:
She was a living doll and no mistake ? the ...
Comparing the Brown corpora and the Google
Web corpora, there are much larger divergences. We
see a particularly large decrease in NP internal struc-
ture. Looking at some of the instances of this error, it
appears to be largely caused by incorrect handling of
structures such as URLs and phone numbers, which
do not appear in the PTB. There are also some more
difficult cases, for example:
... going up for sale in the next month or do .
where or do is a QP. This typographical error is ex-
tremely difficult to handle for a parser trained only
on well-formed text.
For e-mail there is a substantial drop on single
word phrases. Breaking the errors down by label we
found that the majority of the new errors are miss-
ing or extra NPs over single words. Here the main
problem appears to be temporal expressions, though
there also appear to be a substantial number of errors
that are also at the POS level, such as when NNP is
assigned to ta in this case:
... let you know that I ?m out ta here !
Some of these issues, such as URL handling,
could be resolved with suitable training data. Other
issues, such as ungrammatical language and uncon-
ventional use of words, pose a greater challenge.
6 Conclusion
The single F-score objective over brackets or depen-
dencies obscures important differences between sta-
tistical parsers. For instance, a single attachment er-
ror can lead to one or many mismatched brackets.
We have created a novel tree-transformation
methodology for evaluating parsers that categorises
errors into linguistically meaningful types. Using
this approach, we presented the first detailed exam-
ination of the errors produced by a wide range of
constituency parsers for English. We found that PP
attachment and clause attachment are the most chal-
lenging constructions, while coordination turns out
to be less problematic than previously thought. We
also noted interesting variations in error types for
parsers variants.
We investigated the errors resolved in reranking,
and introduced by changing domains. We found that
the Charniak rerankers improved most error types,
but made little headway on improving PP attach-
ment. Changing domain has an impact on all error
types, except NP internal structure.
We have released our system so that future con-
stituent parsers can be evaluated using our method-
ology. Our analysis provides new insight into the
development of parsers over the past fifteen years,
and the challenges that remain.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was par-
tially supported by a General Sir John Monash Fel-
lowship to the first author, the Office of Naval Re-
search under MURI Grant No. N000140911081, an
NSF Fellowship to the second author, ARC Discov-
ery grant DP1097291, the Capital Markets CRC, and
the NSF under grant 0643742.
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306?311, Pacific Grove,
California, USA, February.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings of ACL, pages 470?480, Portland, Oregon, USA,
June.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Proceed-
ings of EMNLP, pages 397?408, Edinburgh, United
Kingdom, July.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of ACL, pages
440?449, Portland, Oregon, USA, June.
1057
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of ACL, pages 41?
48, Sydney, Australia, July.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake, 2002. Relational Evaluation Schemes,
pages 4?8. Las Palmas, Canary Islands, Spain, May.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal.
In Proceedings of LREC, pages 447?454, Granada,
Spain, May.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC, Valletta, Malta,
May.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, USA, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139,
Seattle, Washington, USA, April.
Stephen Clark and Julia Hockenmaier. 2002. Evaluat-
ing a wide-coverage ccg parser. In Proceedings of the
LREC Beyond Parseval Workshop, Las Palmas, Ca-
nary Islands, Spain, May.
Andrew B. Clegg and Adrian J. Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedical
corpus. In Proceedings of the ACL Workshop on Soft-
ware, pages 14?33, Ann Arbor, Michigan, USA, June.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL,
pages 16?23, Madrid, Spain, July.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML, pages
175?182, Palo Alto, California, USA, June.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Rebecca Dridan and Stephan Oepen. 2011. Parser evalu-
ation using elementary dependency matching. In Pro-
ceedings of IWPT, pages 225?230, Dublin, Ireland,
October.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2011. Efficient matrix-encoded grammars and low la-
tency parallelization strategies for cyk. In Proceedings
of IWPT, pages 163?174, Dublin, Ireland, October.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings of LREC, Mar-
rakech, Morocco, May.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of EMNLP, pages 167?202,
Pittsburgh, Pennsylvania, USA, June.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of IWPT, pages 11?22, Prague,
Czech Republic, June.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2009. Descriptive and empirical approaches to captur-
ing underlying dependencies among parsing errors. In
Proceedings of EMNLP, pages 1162?1171, Singapore,
August.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of NAACL, pages 24?31, Edmonton, Canada,
May.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
ACL, pages 95?102, Barcelona, Spain, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Columbus, Ohio, USA, June.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Cor-
pora at EACL, Budapest, Hungary, April.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of ACL,
pages 423?430, Sapporo, Japan, July.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In Proceedings of NIPS, pages 3?10,
Vancouver, British Columbia, Canada, December.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97?114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of NAACL, pages 152?159, New York, New York,
USA, June.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of EMNLP, pages 122?131,
Prague, Czech Republic, June.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
1058
Proceedings of ACL, pages 46?54, Columbus, Ohio,
USA, June.
Dominick Ng and James R. Curran. 2012. N-best CCG
parsing and reranking. In Proceedings of ACL, Jeju,
South Korea, July.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage ccg parser. In Pro-
ceedings of ALTA, pages 90?98, Melbourne, Australia,
December.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez-Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of Coling, pages 833?841, Beijing, China, August.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411, Rochester, New York, USA, April.
Slav Petrov and Ryan McDonald. 2012. SANCL Shared
Task. LDC2012E43. Linguistic Data Consortium.
Philadelphia, Philadelphia, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL, pages
433?440, Sydney, Australia, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP,
pages 62?69, Sydney, Australia, July.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
IWPT, pages 48?57, Dublin, Ireland, October.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using textual
entailments. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 51?56, Up-
psala, Sweden, July.
1059
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 265?277,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Error-Driven Analysis of Challenges in Coreference Resolution
Jonathan K. Kummerfeld and Dan Klein
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720, USA
{jkk,klein}@cs.berkeley.edu
Abstract
Coreference resolution metrics quantify errors
but do not analyze them. Here, we consider
an automated method of categorizing errors in
the output of a coreference system into intu-
itive underlying error types. Using this tool,
we first compare the error distributions across
a large set of systems, then analyze common
errors across the top ten systems, empirically
characterizing the major unsolved challenges
of the coreference resolution task.
1 Introduction
Metrics produce measurements that concisely sum-
marize performance on the full range of error types,
and for coreference resolution there has been ex-
tensive work on developing effective metrics (Luo,
2005; Recasens and Hovy, 2011). However, it is also
valuable to tease apart the errors to understand their
relative importance.
Previous investigations of coreference errors have
focused on quantifying the importance of subtasks
such as named entity recognition and anaphoricity
detection, typically by measuring accuracy improve-
ments when partial gold annotations are provided
(Stoyanov et al, 2009; Pradhan et al, 2011; Prad-
han et al, 2012). For coreference resolution the
drawback of this approach is that decisions are often
interdependent, and so even partial gold information
is extremely informative. Also, previous work only
considered errors by counting links, which does not
capture certain errors in a natural way, e.g. when
a system incorrectly divides a large entity into two
parts, each with multiple mentions. Recent work has
considered some of these issues, but only with small
scale manual analysis (Holen, 2013).
We present a new tool that automatically classifies
errors in the standard output of any coreference res-
olution system. Our approach is to identify changes
that convert the system output into the gold annota-
tions, and map the steps in the conversion onto lin-
guistically intuitive error types. Since our tool uses
only system output, we are able to classify errors
made by systems of any architecture, including both
systems that use link-based inference and systems
that use global inference methods.
Using our tool we perform two studies to un-
derstand similarities and differences between sys-
tems. First, we compare the error distributions on
coreference resolution of all of the systems from the
CoNLL 2011 shared task plus several publicly avail-
able systems. This comparison adds to the analy-
sis from the shared task by illustrating the substan-
tial variation in the types of errors different systems
make. Second, we investigate the aggregate behav-
ior of ten state-of-the-art systems, providing a de-
tailed characterization of each error type. This in-
vestigation identifies key outstanding challenges and
presents the impact that solving each of them would
have in terms of changes in the standard coreference
resolution metrics.
We find that the best systems are not best across
all error types, that a large proportion of span errors
are due to superficial parse differences, and that the
biggest performance loss is on missed entities that
contain a small number of mentions.
This work presents a comprehensive investiga-
tion of common errors in coreference resolution,
identifying particular issues worth focusing on in
future research. Our analysis tool is available at
code.google.com/p/berkeley-coreference-analyser/.
265
2 Background
Most coreference work focuses on accuracy im-
provements, as measured by metrics such as MUC
(Vilain et al, 1995), B3 (Bagga and Baldwin, 1998),
CEAF (Luo, 2005), and BLANC (Recasens and
Hovy, 2011). The only common forms of further
analysis are results for anaphoricity detection and
scores for each mention type (nominal, pronoun,
proper). Two exceptions are: the detailed analysis of
the Reconcile system by Stoyanov et al (2009), and
the multi-system comparisons in the CoNLL shared
task reports (Pradhan et al, 2011, 2012).
A common approach to performance analysis is to
calculate scores for nominals, pronouns and proper
names separately, but this is a very coarse division
(Ng and Cardie, 2002; Haghighi and Klein, 2009).
More fine consideration of some subtasks does oc-
cur, for example, anaphoricity detection, which has
been recognized as a key challenge in coreference
resolution for decades and regularly has separate re-
sults reported (Paice and Husk, 1987; Sobha et al,
2011; Yuan et al, 2012; Bjo?rkelund and Farkas,
2012; Zhekova et al, 2012). Some work has also
included anecdotal discussion of specific error types
or manual classification of a small set of errors, but
these approaches do not effectively quantify the rel-
ative impact of different errors (Chen and Ng, 2012;
Martschat et al, 2012; Haghighi and Klein, 2009).
In a recent paper, Holen (2013) presented a detailed
manual analysis that considered a more comprehen-
sive set of error types, but their focus was on explor-
ing the shortcomings of current metrics, rather than
understanding the behavior of current systems.
The detailed investigation presented by Stoyanov
et al (2009) is the closest to the work we present
here. First, they measured accuracy improvements
when their system was given gold annotations for
three subtasks of coreference resolution: mention
detection, named entity recognition, and anaphoric-
ity detection. To isolate other types of errors they de-
fined resolution classes, based on both the type of a
mention, and properties of possible antecedents (for
example, nominals that have a possible antecedent
that is an exact string match). For each resolution
class they measured performance while giving the
system gold annotations for all other classes. While
this approach is effective at characterizing variations
President Clinton1 is questioning the legitimacy
of George W. Bush?s election victory. Speaking
last night to Democratic supporters in Chicago,
he said Bush won the election only because Re-
publicans stopped the vote-counting in Florida,
and Mr. Clinton1 praised Al Gore?s campaign
manager, Bill Daley, for the way he handled the
election. ?I2 want to thank Bill Daley for his ex-
emplary service as Secretary of Commerce. He
was brilliant. I2 think he did a brilliant job in
leading Vice President Gore to victory myself2.?
Figure 1: Two coreference errors. Mentions are under-
lined and subscripts indicate entities. One error is a men-
tion missing from the system output, he. The other is the
division of references to Bill Clinton into two entities.
between the nine classes they defined, it misses the
cascade effect of errors that only occur when all
mentions are being resolved at once.
The only multi-system comparisons are the
CoNLL task reports (Pradhan et al, 2011, 2012),
which explored the impact of mention detection and
anaphoricity detection through subtasks with differ-
ent types of gold annotation. With a large set of sys-
tems, and well controlled experimental conditions,
the tasks provided a great snapshot of progress in the
field, which we aim to supplement by characterizing
the major outstanding sources of error.
This work adds to previous investigations by pro-
viding a comprehensive and detailed analysis of er-
rors. Our tool can automatically analyze any sys-
tem?s output, giving a reliable estimate of the rela-
tive importance of different error types.
3 Error Classification
When inspecting the output of coreference resolu-
tion systems, several types of errors become imme-
diately apparent: entities that have been divided into
pieces, spurious entities, non-referential pronouns
that have been assigned antecedents, and so on. Our
goal in this work is to automatically assign intuitive
labels like these to errors in system output.
A simple approach, refining results by measur-
ing the accuracy of subsets of the mentions, can be
misleading. For example, in Figure 1, we can in-
tuitively see two pronoun related mistakes: a miss-
ing mention (he), and a divided entity where the two
pieces are the blue pronouns (I2, I2, myself2) and the
red proper names (President Clinton1, Mr. Clinton1).
266
Simply counting the number of incorrect pronoun
links would miss the distinction between the two
types of mistakes present.
One question in designing an error analysis tool
like ours is whether to operate on just system output,
or to also consider intermediate system decisions.
We focused on using system output because other
methods cannot uniformly apply to the full range of
coreference resolution decoding methods, from link
based methods to global inference methods.
Our overall approach is to transform the sys-
tem output into the gold annotations, then map
the changes made in the conversion process to er-
rors. The transformation process is presented in Sec-
tion 3.1 and Figure 2, and the mapping process is
described in Section 3.2 and Figure 3.
3.1 Transformations
The first part of our error classification process de-
termines the changes needed to transform the system
output into the gold annotations. This five stage pro-
cess is described below, and an abstract example is
presented in Figure 2.
1. Alter Span transforms an incorrect system
mention into a gold mention that has the same
head token. In Figure 2 this stage is demon-
strated by a mention in the leftmost entity,
which has its span altered, indicated by the
change from an X to a light blue circle.
2. Split breaks the system entities into pieces,
each containing mentions from a single gold
entity. In Figure 2 there are three changes in
this stage: the leftmost entity is split into a red
piece and a light blue piece, the middle entity
is split into a dark red piece and an X, and the
rightmost entity is split into singletons.
3. Remove deletes every mention that is not
present in the gold annotations. In Figure 2 this
means the four singleton X?s are removed.
4. Introduce creates a singleton entity for each
mention that is missing from the system output.
In Figure 2 this stage involves the introduction
of a light blue mention and two white mentions.
5. Merge combines entities to form the final,
completely correct, set of entities. In Figure 2
the two red entities are merged, the singleton
Mentions
Spurious mention
Entity
1. Alter Span
2. Split
3. Remove
4. Introduce
5. Merge
X
X
X
XX
X X
X
X
X X
X
X
Gold entities indicated using common shading
X
Key
System
Output
Gold
Entities
Figure 2: Abstract example of the transformation process
that converts system output (at the top) to gold annota-
tions (at the bottom).
blue entity is merged with the rest of the blue
entity, and the two white mentions are merged.
267
Operation(s) Error System Gold
i) Alter Span Span error Gorbachev Soviet leader Gorbachev
ii)
Multiple Introduces
Missing Entity
- the pills
and Merges - the tranquilizing pills
iii)
Multiple Splits
Extra Entity
human rights -
and Removes Human Rights -
Introduce
and Merge
the Arab region the Arab region
iv) Missing Mention the region the region
- it
Split and
Remove
her story her story
v) Extra Mention this this
it -
vi) Merge Divided Entity
Iraq1 Iraq1
this nation2 this nation1
the nation2 the nation1
its1 its1
vii) Split Conflated Entities
Mohammed Rashid1 Mohammed Rashid1
the Rashid case1 the Rashid case2
Rashid1 Rashid1
the case1 the case2
Figure 3: Examples of the error types. In examples (i) - (iv) and (vi) the system output contains a single entity. When
multiple entities are involved, they are marked with subscripts. Mentions are in the order in which they appear in the
text. All examples are from system output on the dev set of the CoNLL task.
One subtle point in the split stage is how to record
an entity being split into several pieces. This could
either be a single operation, one entity being split
into N pieces, or N ?1 operations, each involving a
single piece being split off from the rest of the entity.
We use the second approach, as it fits more naturally
with the error mapping we describe in the follow-
ing section. Similarly, for the merge operation, we
record N entities being merged as N?1 operations.
3.2 Mapping
The operations in Section 3.1 are mapped onto seven
error types. In some cases, a single change maps
onto a single error, while in others a single error rep-
resents several closely related operations from adja-
cent stages in the error correction process. The map-
ping is described below and in Figure 3.
1. Span Error. Each Alter Span operation is
mapped to a Span Error, e.g. in Figure 3(i), the
system mention Gorbachev is replaced by the
annotated mention Soviet leader Gorbachev.
2. Missing Entity. A set of Introduce and Merge
operations that forms an entirely new entity,
e.g. the white entity in Figure 2, and the pills
in Figure 3(ii). This error is still assigned if
the new entity includes pronouns that were al-
ready present in the system output. The rea-
soning for this is that most pronouns in the cor-
pus are coreferent, so including just the pro-
nouns from an entity is not meaningfully dif-
ferent from missing the entity entirely.
3. Extra Entity. A set of Split and Remove oper-
ations that completely remove an entity, e.g. the
rightmost entity in Figure 2, and Figure 3(iii).
As for the Missing Entity error type, this error
is still assigned if the original entity contained
pronouns that were valid.
4. Missing Mention. An Introduce and a Merge
that apply to the same mention, e.g. it in Fig-
ure 3(iv), and the blue mention in Figure 2.
5. Extra Mention. A Split and a Remove that ap-
ply to the same mention, e.g. it in Figure 3(v),
and the X in the red entity in Figure 2.
6. Divided Entity. Each remaining Merge oper-
ation is mapped to a Divided Entity error, e.g.
Figure 3(vi), and the red entity in Figure 2.
7. Conflated Entities. Each remaining Split op-
eration is mapped to a Conflated Entity error,
e.g. Figure 3(vii), and the blue and red entities
in Figure 2.
268
4 Methodology
Our tool processes the CoNLL task output, with no
other information required. During development,
and when choosing examples for this paper, we
used the development set of the CoNLL shared task
(Hovy et al, 2006; Pradhan et al, 2007; Pradhan et
al., 2011). The results we present in the rest of the
paper are all for the test set. Using the development
set would have been misleading, as the entrants in
the shared task used it to tune their systems.
4.1 Systems
We analyzed all of the 2011 CoNLL task systems, as
well as several publicly available systems. For the
shared task systems we used the output data from
the task itself, provided by the organizers. For the
publicly available systems we used the default con-
figurations. Finally, we included another run of the
Stanford system, with their OntoNotes-tuned param-
eters (STANFORD-T).
The publicly available systems we used are:
BERKELEY (Durrett and Klein, 2013), IMS
(Bjo?rkelund and Farkas, 2012), STANFORD (Lee
et al, 2013), RECONCILE (Stoyanov et al, 2010),
BART (Versley et al, 2008), UIUC (Bengtson and
Roth, 2008), and CHERRYPICKER (Rahman and
Ng, 2009). The systems from the shared task are
listed in Table 1 and in the references.
5 Broad System Comparison
Table 1 presents the frequency of errors for each sys-
tem and F-Scores for standard metrics1 on the test
set of the 2011 CoNLL shared task. Each bar is
filled in proportion to the number of errors the sys-
tem made, with a full bar corresponding to the num-
ber of errors listed in the bottom row.
The metrics provide an effective overall rank-
ing, as the systems with high scores generally make
fewer errors. However, the metrics do not convey
the significant variation in the types of errors sys-
tems make. For example, YANG and CHARTON are
assigned almost the same scores, but YANG makes
more than twice as many Extra Mention errors.
1CEAF and BLANC are not included as the most recent ver-
sion of the CoNLL scorer (v5) is incorrect, and there are no
standard implementations available.
The most frequent error across all systems is Di-
vided Entity. Unlike parsing errors (Kummerfeld et
al., 2012), improvements are not monotonic, with
better systems often making more errors of one type
when decreasing the frequency of another type.
One outlier is the Irwin et al (2011) system,
which makes very few mistakes in five categories,
but many in the last two. This reflects a high pre-
cision, low recall approach, where clusters are only
formed when there is high confidence.
The third section of Table 1 shows results for sys-
tems that were run with gold noun phrase span in-
formation. This reduces all errors slightly, though
most noticeably Extra Mention, Missing Mention,
and Span Error. On inspection of the remaining
Span Errors we found that many are due to incon-
sistencies regarding the inclusion of the possessive.
The final section of the table shows results for sys-
tems that were provided with the set of mentions that
are coreferent. In this setting, three of the error types
are not present, but there are still Missing Mentions
and Missing Entities because systems do not always
choose an antecedent, leaving a mention as a single-
ton, which is then ignored.
While this broad comparison gives a complete
view of the range of errors present, it is still a coarse
representation. In the next section, we characterize
the common errors on a finer level by breaking down
each error type by a range of properties.
6 Common Errors
To investigate the aggregate state of the art, in this
section we consider results averaged over the top
ten systems: CAI, CHANG, IMS, NUGUES, SAN-
TOS, SAPENA, SONG, STANFORD-T, STOYANOV,
URYUPINA-OPEN.2 These systems represent a broad
range of approaches, all of which are effective.
In each section below, we focus on one or two
error types, characterizing the mistakes by a range
of properties. We then consider a few questions that
apply across multiple error types.
6.1 Span Errors
To characterize the Span Errors, we considered the
text that is in the gold mention, but not the system
2For systems that occur multiple times in Table 1, we only
use the best instance. The BERKELEY system was not included
as it had not been published at submission time.
269
Metric F-Scores Span Conflated Extra Extra Divided Missing Missing
System Mention MUC B3 Error Entities Mention Entity Entity Mention Entity
PUBLICLY AVAILABLE SYSTEMS
BERKELEY 75.57 66.43 66.17
IMS 72.96 64.71 64.73
STANFORD-T 71.21 61.40 63.06
STANFORD 58.56 48.37 56.42
RECONCILE 46.45 49.40 54.90
BART 56.61 46.00 52.56
UIUC 50.60 45.21 52.88
CHERRYPICKER 41.10 40.71 51.39
CONLL, PREDICTED MENTIONS
LEE-OPEN 70.94 61.03 62.96
LEE 70.70 59.56 61.88
SAPENA 43.20 59.54 61.28
SONG 67.26 59.95 60.08
CHANG 64.86 57.13 61.75
CAI-OPEN 67.45 57.86 60.89
NUGUES 68.96 58.61 59.75
URYUPINA-OPEN 68.39 57.63 58.74
SANTOS 65.45 56.65 59.48
STOYANOV 67.78 58.43 57.35
HAO 64.30 54.46 55.82
YANG 63.93 52.31 55.85
CHARTON 64.36 52.49 55.61
KLENNER-OPEN 62.28 49.86 55.62
SOBHA 64.83 50.48 54.85
ZHOU 62.31 48.96 53.42
KOBDANI 61.03 48.62 53.00
ZHANG 61.13 47.88 52.76
XINXIN 61.92 46.62 51.50
KUMMERFELD 62.72 42.70 50.05
IRWIN-OPEN 35.27 27.21 44.29
ZHEKOVA 48.29 24.08 41.42
IRWIN 26.67 19.98 42.73
CONLL, GOLD NP SPANS
LEE-OPEN 75.39 65.39 65.88
LEE 75.16 63.90 64.70
NUGUES 72.42 62.12 61.67
CHANG 67.91 59.77 62.97
SANTOS 67.80 59.52 61.35
STOYANOV 70.29 61.53 59.07
SONG 66.68 55.48 58.04
KOBDANI 66.08 53.94 55.82
ZHANG 64.89 51.64 54.77
ZHEKOVA 62.67 35.22 45.80
CONLL, GOLD MENTIONS
LEE-OPEN 90.93 81.56 75.95
CHANG 99.97 82.52 73.68
Most Errors 2410 3849 2744 5290 4789 2026 3237
Table 1: Counts for each error type on the test set of the 2011 CoNLL task. Bars indicate the number of errors, with
white as zero and fully filled as the number in the Most Errors row. -OPEN indicates a system using external resources.
270
Type Missing Extra
NP 65.8 45.0
POS 12.4 96.9
, 71.2 22.4
SBAR 55.9 1.9
PP 46.2 10.3
DT 17.0 35.9
Total 271.1 224.6
Table 2: Counts of Span Errors grouped by the label over
the extra/missing part of the mention.
mention (missing text), and vice versa (extra text).
We then found nodes in the gold parse that cov-
ered just this extra/missing text, e.g. in Figure 3(i)
we would consider the node over Soviet leader. In
Table 2 we show the most frequent parse nodes.
Some of these differences are superficial, such as
the possessive and the punctuation. Others, such as
the missing PP and SBAR cases, may be due to parse
errors. Of the system mentions involved in span er-
rors, 27.0% do not correspond to a node in the gold
parse. The frequency of punctuation errors could
also be parse related, because punctuation is not con-
sidered in the standard parser evaluation.
Overall it seems that span errors can best be dealt
with by improving parsing, though it is not possi-
ble to completely eliminate these errors because of
inconsistent annotations.
6.2 Extra Mention and Missing Mention
We consider Extra and Missing Mentions together
as they mirror each other, forming a precision-recall
tradeoff, where a high precision system will have
fewer Extra Mentions and more Missing Mentions,
and a high recall system will have the opposite.
Table 3 divides these errors by the type of men-
tion involved and presents some of the most fre-
quent Extra Mentions and Missing Mentions. For
the corpus statistics we count as mentions all NP
spans in the gold parse plus any word tagged with
PRP, WP, WDT, or WRB (following the definition
of gold mention boundaries for the CoNLL tasks).
The mentions it and you are the most common
errors, matching observations from several of the
papers cited in Section 2. However, there is a sur-
prising imbalance between Extra and Missing cases,
e.g. it accounts for a third of the extra errors, but
only 12% of the Missing errors. This imbalance may
Av. Errors Corpus Stats
Mention Extra Missing Count % Coref.
Proper Name 281.6 297.7 6915 59.0
Nominal 484.2 516.5 33328 15.9
Pronoun 390.7 323.3 9926 69.7
it 130.4 38.9 1211 57.1
you 85.2 55.9 1028 44.9
we 39.6 19.6 691 64.7
us 23.2 3.2 242 23.6
that 13.8 13.4 2010 11.5
they 9.6 39.5 738 94.3
their 8.6 21.5 410 95.1
Total 1156.5 1137.5 50169 32.5
Table 3: Counts of Missing and Extra Mention errors by
mention type, and the most common mentions.
Proper Name Nominal
Extra Missing Extra Missing
Text match 145.2 163.6 171.2 96.1
Head match 56.8 70.7 149.6 166.0
Other 79.6 63.4 163.4 254.4
NER Matches 143.4 174.4 23.0 32.0
NER Differs 6.6 6.1 2.4 0.0
NER Unknown 131.6 117.2 458.8 484.5
Total 281.6 297.7 484.2 516.5
Table 4: Counts of Extra and Missing Mentions, grouped
by properties of the mention and the entity it is in.
be the result of systems being tuned to the metrics,
which seem to penalize Missing Mentions more than
Extra Mentions (shown in Section 6.7).
In Table 4 we consider the Extra Mention er-
rors and Missing Mention errors involving proper
names and nominals. The top section counts errors
in which the mention involved in the error has an
exact string match with a mention in the cluster, or
whether it has just a head match. The second sec-
tion of the table considers the named entity anno-
tations in OntoNotes, counting how often the men-
tion?s type matches the type of the cluster.
In all cases shown in the table it appears that sys-
tems are striking a balance between these two types
of errors. One exception may be the use of exact
string matching for nominals, which seems to be bi-
ased towards Extra Mentions.
For these two error types, our observations agree
with previous work: the most common specific error
is the identification of pleonastic pronouns, named
entity types are of limited use, and head matching is
already being used about as effectively as it can be.
271
Composition Av. Errors
Name Nom Pro Extra Missing
0 1 1 70.7 271.6
1 0 1 13.2 28.1
1 1 0 26.6 86.2
2 0 0 61.3 89.3
0 2 0 512.0 347.9
0 0 2 110.9 13.6
3+ 0 0 14.7 14.4
0 3+ 0 154.8 65.9
0 0 3+ 91.0 18.1
Other 51.8 216.4
Total 1107.0 1151.5
Table 5: Counts of Extra and Missing Entity errors,
grouped by the composition of the entity (Names, Nomi-
nals, Pronouns).
Match Type Extra Missing
Proper Name 51.4 42.2
Exact Nominal 338.3 49.5
Pronoun 141.9 10.3
Head
Proper Name 14.4 27.3
Nominal 234.7 129.0
Proper Name 10.2 34.2
None Nominal 92.8 235.3
Pronoun 60.0 21.4
Table 6: Counts of Extra and Missing Entity errors
grouped by properties of the mentions in the entity.
6.3 Extra Entities and Missing Entities
In this section, we consider the errors that involve an
entire entity that was either missing from the system
output or does not exist in the annotations.
Table 5 counts these errors based on the compo-
sition of the entity. There are several noticeable dif-
ferences between the two error types, e.g. for entities
containing one nominal and one pronoun (row 0 1 1)
there are far more Missing errors than Extra errors,
while entities containing two pronouns (row 0 0 2)
have the opposite trend.
It is clear that entities consisting of a single type
of mention are the primary source of these errors,
accounting for 85.3% of the Extra Entity errors,
and 47.7% of Missing Entity errors. Table 6 shows
counts for these cases divided into three groups:
when all mentions are identical, when all mentions
have the same head, and the rest.
Nominals are the most frequent type in Table 6,
and have the greatest variation across the three sec-
Mention Extra Missing
that 6.9 99.7
it 47.7 47.8
this 0.9 36.2
they 3.8 29.1
their 2.1 23.5
them 0.9 13.8
Any pronoun 83.9 299.7
Table 7: Counts of common Missing and Extra Entity
errors where the entity has just two mentions: a pronoun
and either a nominal or a proper name.
tions of the table. For the Extra column, Exact match
cases are a major challenge, accounting for over half
of the nominal errors. These errors include cases
like the example below, where two mentions are not
considered coreferent because they are generic:
everybody tends to mistake the part for the whole.
Here, mistaking the part for the whole is ...
For missing entities we see the opposite trend,
with Exact match cases accounting for less than 12%
of nominal errors. Instead, cases with no match are
the greatest challenge, such as this example, which
requires semantic knowledge to correctly resolve:
The charges related to her sale of ImClone stock.
She sold the share a day before ...
The other common case in Table 5 is an entity
containing a pronoun and a nominal. In Table 7 we
present the most frequent pronouns for this case and
the similar case involving a pronoun and a name.
One way of interpreting these errors is from
the perspective of the pronoun, which is either
incorrectly coreferent (Extra), or incorrectly non-
coreferent (Missing). From this perspective, these
errors are similar in nature to those described by Ta-
ble 3. However, the distribution of errors is quite dif-
ferent, with it being balanced here where previously
it skewed heavily towards extra mentions, while that
was balanced in Table 3 but is skewed towards being
part of Missing Entities here.
Extra Entity errors and Missing Entity errors are
particularly challenging because they are dominated
by entities that are either just nominals, or a nominal
and a pronoun, and for these cases the string match-
ing features are often misleading. This implies that
reducing Extra Entity and Missing Entity errors will
require the use of discourse, context, and semantics.
272
Incorrect Part Rest of Entity Av. Errors
Na No Pr Na No Pr Conflated Divided
- - 1+ - - 1+ 312.7 69.9
- - 1+ - 1+ 1+ 238.5 179.8
- - 1+ - 1+ - 189.6 549.3
- 1+ - - 1+ - 181.5 156.5
- - 1+ 1+ 1+ 1+ 143.6 181.5
- - 1+ 1+ - 1+ 109.7 150.5
- - 1+ 1+ - - 60.0 136.5
Other 454.8 657.7
Total 1690.4 2081.7
Table 8: Counts of Conflated and Divided entities errors
grouped by the Name / Nominal / Pronoun composition
of the parts involved.
6.4 Conflated Entities and Divided Entities
Table 8 breaks down the Conflated Entities errors
and Divided Entity errors by the composition of the
part being split/merged and the rest of the entity in-
volved. Each 1+ indicates that at least one mention
of that type is present (Name / Nominal / Pronoun).
Clearly pronouns being placed incorrectly is the
biggest issue here, with almost all of the common
errors involving a part with just pronouns. It is also
clear that not having proper names in the rest of
the entity presents a challenge. One particularly no-
ticeable issue involves entities composed entirely of
pronouns, which are often created by systems con-
flating the pronouns of two entities together.
Table 8 aggregates errors by the presence of dif-
ferent types of mentions. Aggregating instead by the
exact composition of the incorrect part being con-
flated or divided we found that instances with a part
containing a single pronoun account for 38.9% of
conflated cases and 35.8% of divided cases.
Finally, it is worth noting that in many cases a part
is both conflated with the wrong entity, and divided
from its true entity. Only 12.6% of Conflated Entity
errors led to a complete gold entity with no other er-
rors, and only 21.3% of Divided Entity errors came
from parts that were not involved in another error.
Conflated Entities and Divided Entities are domi-
nated by pronoun link errors: cases where a pronoun
was placed in the wrong entity. Finding finer charac-
terizations of these errors is difficult, as almost any
division produces sparse counts, reflecting the long
tail of mistakes that make up these two error types.
Gold System Decision Count
Cataphoric
Same referent 10.6
Different referent 13.4
Not cataphoric 208.2
Not present 42.8
Not cataphoric Cataphoric 46.2
Not present Cataphoric 186.8
Table 9: Occurrence of mistakes involving cataphora.
6.5 Cataphora
Cataphora (when an anaphor precedes its an-
tecedent) is a pronoun-specific problem that does
not fit easily in the common left-to-right coreference
resolution approach. In the CoNLL test set, 2.8% of
the pronouns are cataphoric. In Table 9 we show
how well systems handle this challenge by counting
mentions based on whether they are cataphoric in
the annotations, are cataphoric in the system output,
and whether the antecedents match.
Systems handle cataphora poorly, missing almost
all of the true instances, and introducing a large
number of extra cases. However, this issue is a fairly
small part of the task, with limited metric impact.
6.6 Entity Properties
Gender, number, person, and named entity type are
properties commonly used in coreference resolution
systems. In some cases, two mentions with differ-
ent properties are placed in the same entity. Some
of these cases are correct, such as variation in per-
son between mentions inside and outside of quotes.
However, many of these cases are errors. In Table 11
we present the percentage of entities that contain
mentions with properties of more than one type. For
named entity types we considered the annotations in
OntoNotes; for the other properties we derive them
from the pronouns in each cluster.
For all of the properties, there are many entities
that we could not assign a value to, either because
no named entity information was available, or be-
cause no pronouns with an unambiguous value for
the property were present. For named entity infor-
mation, OntoNotes only has annotations for 68% of
gold entities, suggesting that named entity taggers
are of limited usefulness, matching observations on
the MUC and ACE corpora (Stoyanov et al, 2009).
The results in the ?Gold? column of Table 11 in-
273
Mentions MUC B3
Error type P R F P R F P R F
Span Error 2.8 2.8 2.7 2.8 2.8 2.8 1.0 2.0 1.6
Conflated Entities 1.7 0.0 0.8 9.9 0.0 4.5 15.9 0.0 6.2
Extra Mention 5.5 0.0 2.6 6.4 0.0 3.0 5.3 0.0 2.2
Extra Entity 15.3 0.0 7.0 11.4 0.0 5.2 6.1 0.0 2.4
Divided Entity 1.8 6.8 4.3 5.7 16.8 10.9 -10.0 21.6 4.5
Missing Mention 1.8 7.0 4.4 3.2 9.2 6.1 -1.3 7.3 3.4
Missing Entity 3.8 16.2 9.8 5.3 13.7 9.3 1.7 11.4 7.0
Table 10: Average accuracy improvement if all errors of a particular type are corrected. Each row in the lower section
is calculated independently, relative to the change after the span errors have been corrected. Some values are negative
because the merge operations involved in fixing the errors are applying to clusters that contain mentions from more
than one gold entity.
Property System Gold
Named Entity 1.7% 0.7%
Gender 0.8% 0.1%
Number 2.1% 0.8%
Person 6.4% 5.1%
Table 11: Percentage of entities that contain mentions
with properties that disagree.
dicate possible errors in the annotations, e.g. in the
0.7% of entities with a mixture of named entity types
there may be mistakes in the coreference annota-
tions, or mistakes in the named entity annotations.3
However, even after taking into consideration cases
where the mixture is valid and cases of annotation
errors, current systems are placing mentions with
different properties in the same clusters.
6.7 Impact of Errors on Metric Scores
Table 10 shows the performance impact of correct-
ing errors of each type. The Span Error row gives
improvements over the original scores, while all
other rows are relative to the scores after Span Er-
rors are corrected.4 By fixing each of the other error
types in isolation, we can get a sense of the gain if
just that error type is addressed. However, it also
means some mentions are incorrectly placed in the
same cluster, causing some negative scores.
Interaction between the error types and the way
the metrics are defined means that the deltas do not
3This kind of cross-annotation analysis may be a useful way
of detecting annotation errors.
4This difference was necessary as the later errors make
changes relative to the state of the entities after the Span Errors
are corrected, e.g. in Figure 2 a blue and red entity is split that
previously contained an X instead of one of the blue mentions.
add up to the overall average gap in performance, but
it is still clear that every error type has a noticeable
impact. Missing Entity errors have the most sub-
stantial impact, which reflects the precision oriented
nature of many coreference resolution systems.
7 Conclusion
While the improvement of metrics and the organiza-
tion of shared tasks have been crucial for progress
in coreference resolution, there is much insight to be
gained by performing a close analysis of errors.
We have presented a new means of automatically
classifying coreference errors that provides an ex-
haustive view of error types. Using our tool we have
analyzed the output of a large set of coreference res-
olution systems and investigated the common chal-
lenges across state-of-the-art systems.
We find that there is considerable variability in
the distribution of errors, and the best systems are
not best across all error types. No single source
of errors stands out as the most substantial chal-
lenge today. However, it is worth noting that
while confidence measures can be used to reduce
precision-related errors, no system has been able to
effectively address the recall-related errors, such as
Missed Entities. Our analysis tool is available at
code.google.com/p/berkeley-coreference-analyser/.
Acknowledgments
We would like to thank the CoNLL task organizers
for providing us with system outputs. This work was
supported by a General Sir John Monash fellowship
to the first author and by BBN under DARPA con-
tract HR0011-12-C-0014.
274
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of The First International Conference on Language
Resources and Evaluation Workshop on Linguistics
Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 294?303.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 49?55.
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50.
Jie Cai, Eva Mujdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 56?60.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Nick Rizzolo, Mark Sammons, and Dan Roth. 2011.
Inference protocols for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 40?44.
Eric Charton and Michel Gagnon. 2011. Poly-co: a mul-
tilayer perceptron approach for coreference detection.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 97?101.
Chen Chen and Vincent Ng. 2012. Combining the best of
two worlds: A hybrid approach to multilingual coref-
erence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 56?63.
Weipeng Chen, Muyu Zhang, and Bing Qin. 2011.
Coreference resolution system using maximum en-
tropy classifier. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 127?130.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods in
Natural Language Processing.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161.
Gordana Ilic Holen. 2013. Critical reflections on evalu-
ation practices in coreference resolution. In Proceed-
ings of the 2013 NAACL HLT Student Research Work-
shop, pages 1?7.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92.
Manfred Klenner and Don Tuggener. 2011. An incre-
mental model for coreference resolution with restric-
tive antecedent accessibility. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 81?85.
Hamidreza Kobdani and Hinrich Schuetze. 2011. Super-
vised coreference resolution with sucre. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 71?
75.
Jonathan K. Kummerfeld, Mohit Bansal, David Burkett,
and Dan Klein. 2011. Mention detection: Heuristics
for the ontonotes annotations. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 102?106.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of er-
ror types in parser output. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1048?1059.
Sobha Lalitha Devi, Pattabhi Rao, Vijay Sundar Ram R,
M. C S, and A. A. 2011. Hybrid approach for corefer-
ence resolution. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 93?96.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanfords multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Xinxin Li, Xuan Wang, and Shuhan Qi. 2011. Coref-
erence resolution with loose transitivity constraints.
275
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 107?111.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 100?106.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 104?111.
Cicero Nogueira dos Santos and Davi Lopes Carvalho.
2011. Rule and tree ensembles for unrestricted
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 51?55.
C. D. Paice and G. D. Husk. 1987. Towards the auto-
matic recognition of anaphoric features in english text:
the impersonal pronoun ?it?. Computer Speech & Lan-
guage, 2(2):109?132.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in ontonotes. In Proceedings of the International Con-
ference on Semantic Computing, pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the
15th Conference on Computational Natural Language
Learning (CoNLL 2011), pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-2012
shared task: Modeling multilingual unrestricted coref-
erence in ontonotes. In Joint Conference on EMNLP
and CoNLL - Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
M. Recasens and E. Hovy. 2011. BLANC: Implement-
ing the rand index for coreference evaluation. Natural
Language Engineering, 17:485?510, 9.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39.
Lalitha Devi. Sobha, RK. Rao. Pattabhi, R. Vijay Sundar
Ram, CS. Malarkodi, and A. Akilandeswari. 2011.
Hybrid approach for coreference resolution. In Pro-
ceedings of the Fifteenth Conference on Computa-
tional Natural Language Learning: Shared Task,
pages 93?96.
Yang Song, Houfeng Wang, and Jing Jiang. 2011. Link
type based pre-cluster pair model for coreference reso-
lution. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 131?135.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In Proceedings of the
ACL 2010 Conference Short Papers, pages 156?161.
Veselin Stoyanov, Uday Babbar, Pracheer Gupta, and
Claire Cardie. 2011. Reconciling ontonotes: Unre-
stricted coreference resolution in ontonotes with rec-
oncile. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 122?126.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. Bart:
a modular toolkit for coreference resolution. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technologies: Demo Session, pages 9?12.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Uunderstanding Conference,
pages 45?52.
Hao Xiong, Linfeng Song, Fandong Meng, Yang Liu,
Qun Liu, and Yajuan Lv. 2011. Ets: An error tolera-
ble system for coreference resolution. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 76?80.
Yaqin Yang, Nianwen Xue, and Peter Anick. 2011. A
machine learning-based coreference detection system
276
for ontonotes. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 117?121.
Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang,
Liping Ge, Zengjian Liu, Meng Liao, and Xianbo
Si. 2012. A mixed deterministic model for corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 76?82.
Desislava Zhekova and Sandra Ku?bler. 2011. Ubiu: A
robust system for resolving unrestricted coreference.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 112?116.
Desislava Zhekova, Sandra Ku?bler, Joshua Bonner,
Marwa Ragheb, and Yu-Yin Hsu. 2012. Ubiu for mul-
tilingual coreference resolution in ontonotes. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 88?94.
Huiwei Zhou, Yao Li, Degen Huang, Yan Zhang, Chun-
long Wu, and Yuansheng Yang. 2011. Combin-
ing syntactic and semantic features by svm for unre-
stricted coreference resolution. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 66?70.
277
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345?355,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Faster Parsing by Supertagger Adaptation
Jonathan K. Kummerfelda Jessika Roesner b Tim Dawborna James Haggertya
James R. Currana? Stephen Clark c?
School of Information Technologiesa Department of Computer Scienceb Computer Laboratoryc
University of Sydney University of Texas at Austin University of Cambridge
NSW 2006, Australia Austin, TX, USA Cambridge CB3 0FD, UK
james@it.usyd.edu.aua? stephen.clark@cl.cam.ac.uk c?
Abstract
We propose a novel self-training method
for a parser which uses a lexicalised gram-
mar and supertagger, focusing on increas-
ing the speed of the parser rather than
its accuracy. The idea is to train the su-
pertagger on large amounts of parser out-
put, so that the supertagger can learn to
supply the supertags that the parser will
eventually choose as part of the highest-
scoring derivation. Since the supertag-
ger supplies fewer supertags overall, the
parsing speed is increased. We demon-
strate the effectiveness of the method us-
ing a CCG supertagger and parser, obtain-
ing significant speed increases on newspa-
per text with no loss in accuracy. We also
show that the method can be used to adapt
the CCG parser to new domains, obtain-
ing accuracy and speed improvements for
Wikipedia and biomedical text.
1 Introduction
In many NLP tasks and applications, e.g. distribu-
tional similarity (Curran, 2004) and question an-
swering (Dumais et al, 2002), large volumes of
text and detailed syntactic information are both
critical for high performance. To avoid a trade-
off between these two, we need to increase parsing
speed, but without losing accuracy.
Parsing with lexicalised grammar formalisms,
such as Lexicalised Tree Adjoining Grammar and
Combinatory Categorial Grammar (CCG; Steed-
man, 2000), can be made more efficient using a
supertagger. Bangalore and Joshi (1999) call su-
pertagging almost parsing because of the signifi-
cant reduction in ambiguity which occurs once the
supertags have been assigned.
In this paper, we focus on the CCG parser and
supertagger described in Clark and Curran (2007).
Since the CCG lexical category set used by the su-
pertagger is much larger than the Penn Treebank
POS tag set, the accuracy of supertagging is much
lower than POS tagging; hence the CCG supertag-
ger assigns multiple supertags1 to a word, when
the local context does not provide enough infor-
mation to decide on the correct supertag.
The supertagger feeds lexical categories to the
parser, and the two interact, sometimes using mul-
tiple passes over a sentence. If a spanning analy-
sis cannot be found by the parser, the number of
lexical categories supplied by the supertagger is
increased. The supertagger-parser interaction in-
fluences speed in two ways: first, the larger the
lexical ambiguity, the more derivations the parser
must consider; second, each further pass is as
costly as parsing a whole extra sentence.
Our goal is to increase parsing speed without
loss of accuracy. The technique we use is a form
of self-training, in which the output of the parser is
used to train the supertagger component. The ex-
isting literature on self-training reports mixed re-
sults. Clark et al (2003) were unable to improve
the accuracy of POS tagging using self-training.
In contrast, McClosky et al (2006a) report im-
proved accuracy through self-training for a two-
stage parser and re-ranker.
Here our goal is not to improve accuracy, only
to maintain it, which we achieve through an adap-
tive supertagger. The adaptive supertagger pro-
duces lexical categories that the parser would have
used in the final derivation when using the base-
line model. However, it does so with much lower
ambiguity levels, and potentially during an ear-
lier pass, which means sentences are parsed faster.
By increasing the ambiguity level of the adaptive
models to match the baseline system, we can also
slightly increase supertagging accuracy, which can
lead to higher parsing accuracy.
1We use supertag and lexical category interchangeably.
345
Using the parser to generate training data also
has the advantage that it is not a domain specific
process. Previous work has shown that parsers
typically perform poorly outside of their train-
ing domain (Gildea, 2001). Using a newspaper-
trained parser, we constructed new training sets for
Wikipedia and biomedical text. These were used
to create new supertagging models adapted to the
different domains.
The self-training method of adapting the su-
pertagger to suit the parser increased parsing speed
by more than 50% across all three domains, with-
out loss of accuracy. Using an adapted supertagger
with ambiguity levels tuned to match the baseline
system, we were also able to increase F-score on
labelled grammatical relations by 0.75%.
2 Background
Many statistical parsers use two stages: a tag-
ging stage that labels each word with its gram-
matical role, and a parsing stage that uses the tags
to form a parse tree. Lexicalised grammars typ-
ically contain a much smaller set of rules than
phrase-structure grammars, relying on tags (su-
pertags) that contain a more detailed description
of each word?s role in the sentence. This leads to
much larger tag sets, and shifts a large proportion
of the search for an optimal derivation to the tag-
ging component of the parser.
Figure 1 gives two sentences and their CCG
derivations, showing how some of the syntactic
ambiguity is transferred to the supertagging com-
ponent in a lexicalised grammar. Note that the
lexical category assigned to with is different in
each case, reflecting the fact that the prepositional
phrase attaches differently. Either we need a tag-
ging model that can resolve this ambiguity, or both
lexical categories must be supplied to the parser
which can then attempt to resolve the ambiguity
by eventually selecting between them.
2.1 Supertagging
Supertaggers typically use standard linear-time
tagging algorithms, and only consider words in the
local context when assigning a supertag. The C&C
supertagger is similar to the Ratnaparkhi (1996)
tagger, using features based on words and POS
tags in a five-word window surrounding the target
word, and defining a local probability distribution
over supertags for each word in the sentence, given
the previous two supertags. The Viterbi algorithm
I ate pizza with cutlery
NP (S\NP)/NP NP ((S\NP)\(S\NP))/NP NP
> >
S\NP (S\NP)\(S\NP)
<
S\NP
<
S
I ate pizza with anchovies
NP (S\NP)/NP NP (NP\NP)/NP NP
>
NP\NP
<
NP
>
S\NP
<
S
Figure 1: Two CCG derivations with PP ambiguity.
can be used to find the most probable supertag se-
quence. Alternatively the Forward-Backward al-
gorithm can be used to efficiently sum over all se-
quences, giving a probability distribution over su-
pertags for each word which is conditional only on
the input sentence.
Supertaggers can be made accurate enough for
wide coverage parsing using multi-tagging (Chen
et al, 1999), in which more than one supertag
can be assigned to a word; however, as more su-
pertags are supplied by the supertagger, parsing
efficiency decreases (Chen et al, 2002), demon-
strating the influence of lexical ambiguity on pars-
ing complexity (Sarkar et al, 2000).
Clark and Curran (2004) applied supertagging
to CCG, using a flexible multi-tagging approach.
The supertagger assigns to a word all lexical cate-
gories whose probabilities are within some factor,
?, of the most probable category for that word.
When the supertagger is integrated with the C&C
parser, several progressively lower ? values are
considered. If a sentence is not parsed on one
pass then the parser attempts to parse the sentence
again with a lower ? value, using a larger set of
categories from the supertagger. Since most sen-
tences are parsed at the first level (in which the av-
erage number of supertags assigned to each word
is only slightly greater than one), this provides
some of the speed benefit of single tagging, but
without loss of coverage (Clark and Curran, 2004).
Supertagging has since been effectively applied
to other formalisms, such as HPSG (Blunsom and
Baldwin, 2006; Zhang et al, 2009), and as an in-
formation source for tasks such as Statistical Ma-
chine Translation (Hassan et al, 2007). The use
of parser output for supertagger training has been
explored for LTAG by Sarkar (2007). However, the
focus of that work was on improving parser and
supertagger accuracy rather than speed.
346
Previously , watch imports were denied such duty-free treatment
S/S , N /N N (S [dcl ]\NP)/(S [pss]\NP) (S [pss]\NP)/NP NP/NP N/N N
N N (S[dcl]\NP)/NP S [pss]\NP (N /N )/(N /N )
S [adj ]\NP (S [dcl ]\NP)/(S [adj ]\NP) (S [pss]\NP)/NP N /N
(S [pt ]\NP)/NP
(S[dcl]\NP)/NP
Figure 2: An example sentence and the sets of categories assigned by the supertagger. The first category
in each column is correct and the categories used by the parser are marked in bold. The correct category
for watch is included here, for expository purposes, but in fact was not provided by the supertagger.
2.2 Semi-supervised training
Previous exploration of semi-supervised training
in NLP has focused on improving accuracy, often
for the case where only small amounts of manually
labelled training data are available. One approach
is co-training, in which two models with indepen-
dent views of the data iteratively inform each other
by labelling extra training data. Sarkar (2001) ap-
plied co-training to LTAG parsing, in which the su-
pertagger and parser provide the two views. Steed-
man et al (2003) extended the method to a variety
of parser pairs.
Another method is to use a re-ranker (Collins
and Koo, 2002) on the output of a system to gener-
ate new training data. Like co-training, this takes
advantage of a different view of the data, but the
two views are not independent as the re-ranker is
limited to the set of options produced by the sys-
tem. This method has been used effectively to
improve parsing performance on newspaper text
(McClosky et al, 2006a), as well as adapting a
Penn Treebank parser to a new domain (McClosky
et al, 2006b).
As well as using independent views of data to
generate extra training data, multiple views can be
used to provide constraints at test time. Holling-
shead and Roark (2007) improved the accuracy
of a parsing pipeline by using the output of later
stages to constrain earlier stages.
The only work we are aware of that uses self-
training to improve the efficiency of parsers is van
Noord (2009), who adopts a similar idea to the
one in this paper for improving the efficiency of
a Dutch parser based on a manually constructed
HPSG grammar.
3 Adaptive Supertagging
The purpose of the supertagger is to cut down the
search space for the parser by reducing the set of
categories that must be considered for each word.
A perfect supertagger would assign the correct cat-
egory to every word. CCG supertaggers are about
92% accurate when assigning a single lexical cate-
gory to each word (Clark and Curran, 2004). This
is not accurate enough for wide coverage parsing
and so a multi-tagging approach is used instead.
In the final derivation, the parser uses one category
from each set, and it is important to note that hav-
ing the correct category in the set does not guaran-
tee that the parser will use it.
Figure 2 gives an example sentence and the sets
of lexical categories supplied by the supertagger,
for a particular value of ?.2 The usual target of
the supertagging task is to produce the top row of
categories in Figure 2, the correct categories. We
propose a new task that instead aims for the cat-
egories the parser will use, which are marked in
bold for this case. The purpose of this new task is
to improve speed.
The reason speed will be improved is that we
can construct models that will constrain the set of
possible derivations more than the baseline model.
We can construct these models because we can
obtain much more of our target output, parser-
annotated sentences, than we could for the gold-
standard supertagging task.
The new target data will contain tagging errors,
and so supertagging accuracy measured against
the correct categories may decrease. If we ob-
tained perfect accuracy on our new task then we
would be removing all of the categories not cho-
sen by the parser. However, parsing accuracy will
not decrease since the parser will still receive the
categories it would have used, and will therefore
be able to form the same highest-scoring deriva-
tion (and hence will choose it).
To test this idea we parsed millions of sentences
2Two of the categories for such have been left out for
reasons of space, and the correct category for watch has been
included for expository reasons. The fact that the supertagger
does not supply this category is the reason that the parser does
not analyse the sentence correctly.
347
in three domains, producing new data annotated
with the categories that the parser used with the
baseline model. We constructed new supertagging
models that are adapted to suit the parser by train-
ing on the combination of these sets and the stan-
dard training corpora. We applied standard evalu-
ation metrics for speed and accuracy, and explored
the source of the changes in parsing performance.
4 Data
In this work, we consider three domains: news-
wire, Wikipedia text and biomedical text.
4.1 Training and accuracy evaluation
We have used Sections 02-21 of CCGbank (Hock-
enmaier and Steedman, 2007), the CCG version of
the Penn Treebank (Marcus et al, 1993), as train-
ing data for the newspaper domain. Sections 00
and 23 were used for development and test eval-
uation. A further 113,346,430 tokens (4,566,241
sentences) of raw data from the Wall Street Jour-
nal section of the North American News Corpus
(Graff, 1995) were parsed to produce the training
data for adaptation. This text was tokenised us-
ing the C&C tools tokeniser and parsed using our
baseline models. For the smaller training sets, sen-
tences from 1988 were used as they would be most
similar in style to the evaluation corpus. In all ex-
periments the sentences from 1989 were excluded
to ensure no overlap occurred with CCGbank.
As Wikipedia text we have used 794,024,397
tokens (51,673,069 sentences) from Wikipedia ar-
ticles. This text was processed in the same way as
the NANC data to produce parser-annotated train-
ing data. For supertagger evaluation, one thousand
sentences were manually annotated with CCG lex-
ical categories and POS tags. For parser evalua-
tion, three hundred of these sentences were man-
ually annotated with DepBank grammatical rela-
tions (King et al, 2003) in the style of Briscoe
and Carroll (2006). Both sets of annotations were
produced by manually correcting the output of the
baseline system. The annotation was performed
by Stephen Clark and Laura Rimell.
For the biomedical domain we have used sev-
eral different resources. As gold standard data for
supertagger evaluation we have used supertagged
GENIA data (Kim et al, 2003), annotated by
Rimell and Clark (2008). For parsing evalua-
tion, grammatical relations from the BioInfer cor-
pus were used (Pyysalo et al, 2007), with the
Source Sentence Length Corpus %
Range Average Variance
0-4 3.26 0.64 1.2
5-20 14.04 17.41 39.2
News 21-40 28.76 29.27 49.4
41-250 49.73 86.73 10.2
All 24.83 152.15 100.0
0-4 2.81 0.60 22.4
5-20 11.64 21.56 48.9
Wiki 21-40 28.02 28.48 24.3
41-250 49.69 77.70 4.5
All 15.33 154.57 100.0
0-4 2.98 0.75 0.9
5-20 14.54 15.14 41.3
Bio 21-40 28.49 29.34 48.0
41-250 49.17 68.34 9.8
All 24.53 139.35 100.0
Table 1: Statistics for sentences in the supertagger
training data. Sentences containing more than 250
tokens were not included in our data sets.
same post-processing process as Rimell and Clark
(2009) to convert the C&C parser output to Stan-
ford format grammatical relations (de Marneffe
et al, 2006). For adaptive training we have
used 1,900,618,859 tokens (76,739,723 sentences)
from the MEDLINE abstracts tokenised by McIn-
tosh and Curran (2008). These sentences were
POS-tagged and parsed twice, once as for the
newswire and Wikipedia data, and then again, us-
ing the bio-specific models developed by Rimell
and Clark (2009). Statistics for the sentences in
the training sets are given in Table 1.
4.2 Speed evaluation data
For speed evaluation we held out three sets of sen-
tences from each domain-specific corpus. Specif-
ically, we used 30,000, 4,000 and 2,000 unique
sentences of length 5-20, 21-40 and 41-250 tokens
respectively. Speeds on these length controlled
sets were combined to calculate an overall pars-
ing speed for the text in each domain. Note that
more than 20% of the Wikipedia sentences were
less than five words in length and the overall dis-
tribution is skewed towards shorter sentences com-
pared to the other corpora.
5 Evaluation
We used the hybrid parsing model described in
Clark and Curran (2007), and the Viterbi decoder
to find the highest-scoring derivation. The multi-
pass supertagger-parser interaction was also used.
The test data was excluded from training data
for the supertagger for all of the newswire and
Wikipedia models. For the biomedical models ten-
348
fold cross validation was used. The accuracy of
supertagging is measured by multi-tagging at the
first ? level and considering a word correct if the
correct tag is amongst any of the assigned tags.
For the biomedical parser evaluation we have
used the parsing model and grammatical relation
conversion script from Rimell and Clark (2009).
Our timing measurements are calculated in two
ways. Overall times were measured using the C&C
parser?s timers. Individual sentence measurements
were made using the Intel timing registers, since
standard methods are not accurate enough for the
short time it takes to parse a single sentence.
To check whether changes were statistically sig-
nificant we applied the test described by Chinchor
(1995). This measures the probability that two sets
of responses are drawn from the same distribution,
where a score below 0.05 is considered significant.
Models were trained on an Intel Core2Duo
3GHz with 4GB of RAM. The evaluation was per-
formed on a dual quad-core Intel Xeon 2.27GHz
with 16GB of RAM.
5.1 Tagging ambiguity optimisation
The number of lexical categories assigned to a
word by the CCG supertagger depends on the prob-
abilities calculated for each category and the ?
level being used. Each lexical category with a
probability within a factor of ? of the most prob-
able category is included. This means that the
choice of ? level determines the tagging ambigu-
ity, and so has great influence on parsing speed, ac-
curacy and coverage. Also, the tagging ambiguity
produced by a ? level will vary between models.
A more confident model will have a more peaked
distribution of category probabilities for a word,
and therefore need a smaller ? value to assign the
same number of categories.
Additionally, the C&C parser uses multiple ?
levels. The first pass over a sentence is at a high ?
level, resulting in a low tagging ambiguity. If the
categories assigned are too restrictive to enable a
spanning analysis, the system makes another pass
with a lower ? level, resulting in a higher tagging
ambiguity. A maximum of five passes are made,
with the ? levels varying from 0.075 to 0.001.
We have taken two approaches to choosing ?
levels. When the aim of an experiment is to im-
prove speed, we use the system?s default ? levels.
While this choice means a more confident model
will assign fewer tags, this simply reflects the fact
that the model is more confident. It should pro-
duce similar accuracy results, but with lower am-
biguity, which will lead to higher speed.
For accuracy optimisation experiments we tune
the ? levels to produce the same average tagging
ambiguity as the baseline model on Section 00 of
CCGbank. Accuracy depends heavily on the num-
ber of categories supplied, so the new models are
at an accuracy disadvantage if they propose fewer
categories. By matching the ambiguity of the de-
fault model, we can increase accuracy at the cost
of some of the speed improvements the new mod-
els obtain.
6 Results
We have performed four primary sets of exper-
iments to explore the ability of an adaptive su-
pertagger to improve parsing speed or accuracy. In
the first two experiments, we explore performance
on the newswire domain, which is the source of
training data for the parsing model and the base-
line supertagging model. In the second set of ex-
periments, we train on a mixture of gold standard
newswire data and parser-annotated data from the
target domain.
In both cases we perform two experiments. The
first aimed to improve speed, keeping the ? levels
the same. This should lead to an increase in speed
as the extra training data means the models are
more confident and so have lower ambiguity than
the baseline model for a given ? value. The second
experiment aimed to improve accuracy, tuning the
? levels as described in the previous section.
6.1 Newswire speed improvement
In our first experiment, we trained supertagger
models using Generalised Iterative Scaling (GIS)
(Darroch and Ratcliff, 1972), the limited mem-
ory BFGS method (BFGS) (Nocedal and Wright,
1999), the averaged perceptron (Collins, 2002),
and the margin infused relaxed algorithm (MIRA)
(Crammer and Singer, 2003). Note that these
are all alternative methods for estimating the lo-
cal log-linear probability distributions used by the
Ratnaparkhi-style tagger. We do not use global
tagging models as in Lafferty et al (2001) or
Collins (2002). The training data consisted of Sec-
tions 02?21 of CCGbank and progressively larger
quantities of parser-annotated NANC data ? from
zero to four million extra sentences. The results of
these tests are presented in Table 2.
349
Ambiguity (%) Tagging Accuracy (%) F-score Speed (sents / sec)
Data 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 1.27 96.34 85.46 39.6
BFGS 1.27 1.23 1.19 1.18 96.33 96.18 95.95 95.93 85.45 85.51 85.57 85.68 39.8 49.6 71.8 60.0
GIS 1.28 1.24 1.21 1.20 96.44 96.27 96.09 96.11 85.44 85.46 85.58 85.62 37.4 44.1 51.3 54.1
MIRA 1.30 1.24 1.17 1.13 96.44 96.14 95.56 95.18 85.44 85.40 85.38 85.42 34.1 44.8 60.2 73.3
Table 2: Speed improvements on newswire, using various amounts of parser-annotated NANC data.
Sentences Av. Time Change (ms) Total Time Change (s)
Sentence length 5-20 21-40 41-250 5-20 21-40 41-250 5-20 21-40 41-250
Lower tag amb. 1166 333 281 -7.54 -71.42 -183.23 -1.1 -29 -26
Earlier pass Same tag amb. 248 38 8 -2.94 -27.08 -108.28 -0.095 -1.3 -0.44
Higher tag amb. 530 33 14 -5.84 -32.25 -44.10 -0.40 -1.3 -0.31
Lower tag amb. 19288 3120 1533 -1.13 -5.18 -38.05 -2.8 -20 -30
Same pass Same tag amb. 7285 259 35 -0.29 0.94 24.57 -0.28 0.30 0.44
Higher tag amb. 1133 101 24 -0.25 2.70 8.09 -0.037 0.34 0.099
Lower tag amb. 334 114 104 0.90 7.60 -46.34 0.039 1.1 -2.5
Later pass Same tag amb. 14 1 0 1.06 4.26 n/a 0.0019 0.0053 0.0
Higher tag amb. 2 1 1 -0.13 26.43 308.03 -3.4e-05 0.033 0.16
Table 3: Breakdown of the source of changes in speed. The test sentences are divided into nine sets
based on the change in parsing behaviour between the baseline model and a model trained using MIRA,
Sections 02-21 of CCGbank and 4,000,000 NANC sentences.
Using the default ? levels we found that the
perceptron-trained models lost accuracy, disqual-
ifying them from this test. The BFGS, GIS and
MIRA models produced mixed results, but no
statistically significant decrease in accuracy, and
as the amount of parser-annotated data was in-
creased, parsing speed increased by up to 85%.
To determine the source of the speed improve-
ment we considered the times recorded by the tim-
ing registers. In Table 3, we have aggregated these
measurements based on the change in the pass at
which the sentence is parsed, and how the tag-
ging ambiguity changes on that pass. For sen-
tences parsed on two different passes the ambigu-
ity comparison is at the earlier pass. The ?Total
Time Change? section of the table is the change in
parsing time for sentences of that type when pars-
ing ten thousand sentences from the corpus. This
takes into consideration the actual distribution of
sentence lengths in the corpus.
Several effects can be observed in these re-
sults. 72% of sentences are parsed on the same
pass, but with lower tag ambiguity (5th row in Ta-
ble 3). This provides 44% of the speed improve-
ment. Three to six times as many sentences are
parsed on an earlier pass than are parsed on a later
pass. This means the sentences parsed later have
very little effect on the overall speed. At the same
time, the average gain for sentences parsed earlier
is almost always larger than the average cost for
sentences parsed later. These effects combine to
produce a particularly large improvement for the
sentences parsed at an earlier pass. In fact, despite
making up only 7% of sentences in the set, those
parsed earlier with lower ambiguity provide 50%
of the speed improvement.
It is also interesting to note the changes for sen-
tences parsed on the same pass, with the same
ambiguity. We may expect these sentences to be
parsed in approximately the same amount of time,
and this is the case for the short set, but not for the
two larger sets, where we see an increase in pars-
ing time. This suggests that the categories being
supplied are more productive, leading to a larger
set of possible derivations.
6.2 Newswire accuracy optimised
Any decrease in tagging ambiguity will generally
lead to a decrease in accuracy. The parser uses a
more sophisticated algorithm with global knowl-
edge of the sentence and so we would expect it
to be better at choosing categories than the su-
pertagger. Unlike the supertagger it will exclude
categories that cannot be used in a derivation. In
the previous section, we saw that training the su-
pertagger on parser output allowed us to develop
models that produced the same categories, despite
lower tagging ambiguity. Since they were trained
on the categories the parser was able to use in
derivations, these models should also now be pro-
viding categories that are more likely to be useful.
This leads us to our second experiment, opti-
350
Tagging Accuracy (%) F-score Speed (sents / sec)
NANC sents 0k 40k 400k 4m 0k 40k 400k 4m 0k 40k 400k 4m
Baseline 96.34 85.46 39.6
BFGS 96.33 96.42 96.42 96.66 85.45 85.55 85.64 85.98 39.5 43.7 43.9 42.7
GIS 96.34 96.43 96.53 96.62 85.36 85.47 85.84 85.87 39.1 41.4 41.7 42.6
Perceptron 95.82 95.99 96.30 - 85.28 85.39 85.64 - 45.9 48.0 45.2 -
MIRA 96.23 96.29 96.46 96.63 85.47 85.45 85.55 85.84 37.7 41.4 41.4 42.9
Table 4: Accuracy optimisation on newswire, using various amounts of parser-annotated NANC data.
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
News Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.267 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.126 1.151 1.130 95.18 93.56 90.07 85.42 81.2 75.2 73.3 83.9 60.3
Wiki 1.147 1.154 1.129 95.06 93.52 90.03 84.70 81.4 75.5 62.4 73.9 58.7
Bio 1.134 1.146 1.114 94.66 93.15 89.88 84.23 80.7 75.9 66.2 90.4 59.3
Table 5: Cross-corpus speed improvement, models trained with MIRA and 4,000,000 sentences. The
highlighted values are the top speed for each evaluation set and results that are statistically indistinguish-
able from it.
mising accuracy on newswire. We used the same
models as in the previous experiment, but tuned
the ? levels as described in Section 5.1.
Comparing Tables 2 and 4 we can see the in-
fluence of ? level choice, and therefore tagging
ambiguity. When the default ? values were used
ambiguity dropped consistently as more parser-
annotated data was used, and category accuracy
dropped in the same way. Tuning the ? levels to
match ambiguity produces the opposite trend.
Interestingly, while the decrease in supertag ac-
curacy in the previous experiment did not translate
into a decrease in F-score, the increase in tag accu-
racy here does translate into an increase in F-score.
This indicates that the supertagger is adapting to
suit the parser. In the previous experiment, the
supertagger was still providing the categories the
parser would have used with the baseline supertag-
ging model, but it provided fewer other categories.
Since the parser is not a perfect supertagger these
other categories may in fact have been incorrect,
and so supertagger accuracy goes down, without
changing parsing results. Here we have allowed
the supertagger to assign extra categories, which
will only increase its accuracy.
The increase in F-score has two sources. First,
our supertagger is more accurate, and so the parser
is more likely to receive category sets that can be
combined into the correct derivation. Also, the su-
pertagger has been trained on categories that the
parser is able to use in derivations, which means
they are more productive.
As Table 6 shows, this change translates into an
improvement of up to 0.75% in F-score on Section
Model Tag. Acc. F-score Speed
(%) (%) (sents/sec)
Baseline 96.51 85.20 39.6
GIS, 4,000k NANC 96.83 85.95 42.6
BFGS, 4,000k NANC 96.91 85.90 42.7
MIRA, 4,000k NANC 96.84 85.79 42.9
Table 6: Evaluation of top models on Section 23 of
CCGbank. All changes in F-score are statistically
significant.
23 of CCGbank. All of the new models in the table
make a statistically significant improvement over
the baseline.
It is also interesting to note that the results in
Tables 2, 4 and 6, are similar for all of the train-
ing algorithms. However, the training times differ
considerably. For all four algorithms the training
time is proportional to the amount of data, but the
GIS and BFGS models trained on only CCGbank
took 4,500 and 4,200 seconds to train, while the
equivalent perceptron and MIRA models took 90
and 95 seconds to train.
6.3 Annotation method comparison
To determine whether these improvements were
dependent on the annotations being produced
by the parser we performed a set of tests with
supertagger, rather than parser, annotated data.
Three extra training sets were created by annotat-
ing newswire sentences with supertags using the
baseline supertagging model. One set used the
one-best tagger, and two were produced using the
most probable tag for each word out of the set sup-
plied by the multi-tagger, with variations in the ?
value and dictionary cutoff for the two sets.
351
Train Corpus Ambiguity Tag. Acc. F-score Speed (sents / sec)
Wiki Bio News Wiki Bio News Wiki Bio News Wiki Bio
Baseline 1.317 1.281 96.34 94.52 90.70 85.46 80.8 75.0 39.6 50.9 35.1
News 1.331 1.322 96.53 94.86 91.32 85.84 80.1 75.2 41.8 32.6 31.4
Wiki 1.293 1.251 96.28 94.79 91.08 85.02 81.7 75.8 40.4 37.2 37.2
Bio 1.287 1.195 96.15 94.28 91.03 84.95 80.6 76.1 39.2 52.9 26.2
Table 7: Cross-corpus accuracy optimisation, models trained with GIS and 400,000 sentences.
Annotation method Tag. Acc. F-score
Baseline 96.34 85.46
Parser 96.46 85.55
One-best super 95.94 85.24
Multi-tagger a 95.91 84.98
Multi-tagger b 96.00 84.99
Table 8: Comparison of annotation methods for
extra data. The multi-taggers used ? values 0.075
and 0.001, and dictionary cutoffs 20 and 150, for
taggers a and b respectively.
Corpus Speed (sents / sec)
Sent length 5-20 21-40 41-250
News 242 44.8 8.24
Wiki 224 42.0 6.10
Bio 268 41.5 6.48
Table 9: Cross-corpus speed for the baseline
model on data sets balanced on sentence length.
As Table 8 shows, in all cases the use of
supertagger-annotated data led to poorer perfor-
mance than the baseline system, while the use of
parser-annotated data led to an improvement in F-
score. The parser has access to a range of infor-
mation that the supertagger does not, producing a
different view of the data that the supertagger can
productively learn from.
6.4 Cross-domain speed improvement
When applying parsers out of domain they are typ-
ically slower and less accurate (Gildea, 2001). In
this experiment, we attempt to increase speed on
out-of-domain data. Note that for some of the re-
sults presented here it may appear that the C&C
parser does not lose speed when out of domain,
since the Wikipedia and biomedical corpora con-
tain shorter sentences on average than the news
corpus. However, by testing on balanced sets it
is clear that speed does decrease, particularly for
longer sentences, as shown in Table 9.
For our domain adaptation development ex-
periments, we considered a collection of differ-
ent models; here we only present results for the
best set of models. For speed improvement these
were MIRA models trained on 4,000,000 parser-
annotated sentences from the target domain.
As Table 5 shows, this training method pro-
duces models adapted to the new domain. In par-
ticular, note that models trained on Wikipedia or
the biomedical data produce lower F-scores3 than
the baseline on newswire. Meanwhile, on the
target domain they are adapted to, these models
achieve a higher F-score and parse sentences at
least 45% faster than the baseline.
The changes in tagging ambiguity and accuracy
also show that adaptation has occurred. In all
cases, the new models have lower tagging ambi-
guity, and lower supertag accuracy. However, on
the corpus of the extra data, the performance of
the adapted models is comparable to the baseline
model, which means the parser is probably still be
receiving the same categories that it used from the
sets provided by the baseline system.
6.5 Cross-domain accuracy optimised
The ambiguity tuning method used to improve ac-
curacy on the newspaper domain can also be ap-
plied to the models trained on other domains. In
Table 7, we have tested models trained using GIS
and 400,000 sentences of parsed target-domain
text, with ? levels tuned to match ambiguity with
the baseline.
As for the newspaper domain, we observe in-
creased supertag accuracy and F-score. Also, in
almost every case the new models perform worse
than the baseline on domains other than the one
they were trained on.
In some cases the models in Table 7 are less ac-
curate than those in Table 5. This is because as
well as optimising the ? levels we have changed
training methods. All of the training methods were
tried, but only the method with the best results in
newswire is included here, which for F-score when
trained on 400,000 sentences was GIS.
The accuracy presented so far for the biomedi-
3Note that the F-scores for Wikipedia and biomedical text
are reported to only three significant figures as only 300 and
500 sentences respectively were available for parser evalua-
tion.
352
Train Corpus F-score
Rimell and Clark (2009) 81.5
Baseline 80.7
CCGbank + Genia 81.5
+ Newswire 81.9
+ Wikipedia 82.2
+ Biomedical 81.7
+ R&C annotated Bio 82.3
Table 10: Performance comparison for models us-
ing extra gold standard biomedical data. Models
were trained with GIS and 4,000,000 extra sen-
tences, and are tested using a POS-tagger trained
on biomedical data.
cal model is considerably lower than that reported
by Rimell and Clark (2009). This is because no
gold standard biomedical training data was used
in our experiments. Table 10 shows the results of
adding Rimell and Clark?s gold standard biomedi-
cal supertag data and using their biomedical POS-
tagger. The table also shows how accuracy can be
further improved by adding our parser-annotated
data from the biomedical domain as well as the
additional gold standard data.
7 Conclusion
This work has demonstrated that an adapted su-
pertagger can improve parsing speed and accu-
racy. The purpose of the supertagger is to re-
duce the search space for the parser. By train-
ing the supertagger on parser output, we allow the
parser to reach the derivation it would have found,
sooner. This approach also enables domain adap-
tation, improving speed and accuracy outside the
original domain of the parser.
The perceptron-based algorithms used in this
work are also able to function online, modifying
the model weights after each sentence is parsed.
This could be used to construct a system that con-
tinuously adapts to the domain it is parsing.
By training on parser-annotated NANC data
we constructed models that were adapted to the
newspaper-trained parser. The fastest model
parsed sentences 1.85 times as fast and was as
accurate as the baseline system. Adaptive train-
ing is also an effective method of improving per-
formance on other domains. Models trained on
parser-annotated Wikipedia text and MEDLINE
text had improved performance on these target do-
mains, in terms of both speed and accuracy. Op-
timising for speed or accuracy can be achieved by
modifying the ? levels used by the supertagger,
which controls the lexical category ambiguity at
each level used by the parser.
The result is an accurate and efficient wide-
coverage CCG parser that can be easily adapted for
NLP applications in new domains without manu-
ally annotating data.
Acknowledgements
We thank the reviewers for their helpful feed-
back. This work was supported by Australian Re-
search Council Discovery grants DP0665973 and
DP1097291, the Capital Markets Cooperative Re-
search Centre, and a University of Sydney Merit
Scholarship. Part of the work was completed at the
Johns Hopkins University Summer Workshop and
(partially) supported by National Science Founda-
tion Grant Number IIS-0833652.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Phil Blunsom and Timothy Baldwin. 2006. Multi-
lingual deep lexical acquisition for HPSGs via su-
pertagging. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 164?171, Sydney, Australia.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the Poster Ses-
sion of the 21st International Conference on Com-
putational Linguistics, Sydney, Australia.
John Chen, Srinivas Bangalore, and Vijay K. Shanker.
1999. New models for improving supertag disam-
biguation. In Proceedings of the Ninth Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 188?195, Bergen, Nor-
way.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram su-
pertagger. In Proceedings of the 6th International
Workshop on Tree Adjoining Grammars and Related
Frameworks, pages 259?268, Venice, Italy.
Nancy Chinchor. 1995. Statistical significance
of MUC-6 results. In Proceedings of the Sixth
Message Understanding Conference, pages 39?43,
Columbia, MD, USA.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of the 20th International Con-
ference on Computational Linguistics, pages 282?
288, Geneva, Switzerland.
353
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabelled
data. In Proceedings of the seventh Conference on
Natural Language Learning, pages 49?55, Edmon-
ton, Canada.
Michael Collins and Terry Koo. 2002. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
John N. Darroch and David Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470?1480.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?54,
Genoa, Italy.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: Is
more always better? In Proceedings of the 25th In-
ternational ACMSIGIR Conference on Research and
Development, Tampere, Finland.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing, Pittsburgh, PA, USA.
David Graff. 1995. North American News Text Cor-
pus. LDC95T21. Linguistic Data Consortium.
Philadelphia, PA, USA.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
288?295, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Kristy Hollingshead and Brian Roark. 2007. Pipeline
iteration. In Proceedings of the 45th Meeting of the
Association for Computational Linguistics, pages
952?959, Prague, Czech Republic.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(1):180?182.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of
the 4th International Workshop on Linguistically In-
terpreted Corpora, Budapest, Hungary.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289, San Francisco, CA, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, Brook-
lyn, NY, USA.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 337?344, Sydney, Aus-
tralia.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain inde-
pendent lexicon and template acquisition. In Pro-
ceedings of the Australasian Language Technology
Workshop, Hobart, Australia.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal Optimization. Springer.
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the unification of syntactic annotations
under the Stanford dependency scheme: a case study
on bioinfer and GENIA. In Proceedings of the ACL
workshop on biological, translational, and clinical
language processing, pages 25?32, Prague, Czech
Republic.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of the 1996 Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142, Philadelphia, PA, USA.
354
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
475?484, Honolulu, HI, USA.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852?865.
Anoop Sarkar, Fel Xia, and Aravind K. Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proceedings of
the COLING Workshop on Efficiency in Large-scale
Parsing Systems, pages 37?42, Luxembourg.
Anoop Sarkar. 2001. Applying co-training methods
to statistical parsing. In Proceedings of the Second
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 1?8,
Pittsburgh, PA, USA.
Anoop Sarkar. 2007. Combining supertagging and
lexicalized tree-adjoining grammar parsing. In
Srinivas Bangalore and Aravind Joshi, editors, Com-
plexity of Lexical Descriptions and its Relevance to
Natural Language Processing: A Supertagging Ap-
proach. MIT Press, Boston, MA, USA.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Stephen Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the 10th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 331?338, Budapest, Hun-
gary.
Geertjan van Noord. 2009. Learning efficient parsing.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817?825. Association for Com-
putational Linguistics.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi
Tsujii. 2009. HPSG supertagging: A sequence la-
beling view. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages
210?213, Paris, France.
355
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105?109,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Robust Conversion of CCG Derivations to Phrase Structure Trees
Jonathan K. Kummerfeld? Dan Klein? James R. Curran?
?Computer Science Division ? e-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,klein}@cs.berkeley.edu james@it.usyd.edu.au
Abstract
We propose an improved, bottom-up method
for converting CCG derivations into PTB-style
phrase structure trees. In contrast with past
work (Clark and Curran, 2009), which used
simple transductions on category pairs, our ap-
proach uses richer transductions attached to
single categories. Our conversion preserves
more sentences under round-trip conversion
(51.1% vs. 39.6%) and is more robust. In par-
ticular, unlike past methods, ours does not re-
quire ad-hoc rules over non-local features, and
so can be easily integrated into a parser.
1 Introduction
Converting the Penn Treebank (PTB, Marcus et al,
1993) to other formalisms, such as HPSG (Miyao
et al, 2004), LFG (Cahill et al, 2008), LTAG (Xia,
1999), and CCG (Hockenmaier, 2003), is a com-
plex process that renders linguistic phenomena in
formalism-specific ways. Tools for reversing these
conversions are desirable for downstream parser use
and parser comparison. However, reversing conver-
sions is difficult, as corpus conversions may lose in-
formation or smooth over PTB inconsistencies.
Clark and Curran (2009) developed a CCG to PTB
conversion that treats the CCG derivation as a phrase
structure tree and applies hand-crafted rules to ev-
ery pair of categories that combine in the derivation.
Because their approach does not exploit the gener-
alisations inherent in the CCG formalism, they must
resort to ad-hoc rules over non-local features of the
CCG constituents being combined (when a fixed pair
of CCG categories correspond to multiple PTB struc-
tures). Even with such rules, they correctly convert
only 39.6% of gold CCGbank derivations.
Our conversion assigns a set of bracket instruc-
tions to each word based on its CCG category, then
follows the CCG derivation, applying and combin-
ing instructions at each combinatory step to build a
phrase structure tree. This requires specific instruc-
tions for each category (not all pairs), and generic
operations for each combinator. We cover all cate-
gories in the development set and correctly convert
51.1% of sentences. Unlike Clark and Curran?s ap-
proach, we require no rules that consider non-local
features of constituents, which enables the possibil-
ity of simple integration with a CKY-based parser.
The most common errors our approach makes in-
volve nodes for clauses and rare spans such as QPs,
NXs, and NACs. Many of these errors are inconsis-
tencies in the original PTB annotations that are not
recoverable. These issues make evaluating parser
output difficult, but our method does enable an im-
proved comparison of CCG and PTB parsers.
2 Background
There has been extensive work on converting parser
output for evaluation, e.g. Lin (1998) and Briscoe et
al. (2002) proposed using underlying dependencies
for evaluation. There has also been work on conver-
sion to phrase structure, from dependencies (Xia and
Palmer, 2001; Xia et al, 2009) and from lexicalised
formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008)
and TAG (Chiang, 2000; Sarkar, 2001). Our focus is
on CCG to PTB conversion (Clark and Curran, 2009).
2.1 Combinatory Categorial Grammar (CCG)
The lower half of Figure 1 shows a CCG derivation
(Steedman, 2000) in which each word is assigned a
category, and combinatory rules are applied to ad-
jacent categories until only one remains. Categories
105
JJ NNS
PRP$ NN DT NN
NP NP
VBD S
NP VP
S
Italian magistrates labeled his death a suicide
N /N N ((S [dcl ]\NP)/NP)/NP NP [nb]/N N NP [nb]/N N
> > >
N NP NP
>
NP (S [dcl ]\NP)/NP
>
S [dcl ]\NP
<
S [dcl ]
Figure 1: A crossing constituents example: his . . . suicide
(PTB) crosses labeled . . . death (CCGbank).
Categories Schema
N create an NP
((S [dcl ]\NP)/NP)/NP create a VP
N /N + N place left under right
NP [nb]/N + N place left under right
((S [dcl ]\NP)/NP)/NP + NP place right under left
(S [dcl ]\NP)/NP + NP place right under left
NP + S [dcl ]\NP place both under S
Table 1: Example C&C-CONV lexical and rule schemas.
can be atomic, e.g. the N assigned to magistrates,
or complex functions of the form result / arg, where
result and arg are categories and the slash indicates
the argument?s directionality. Combinators define
how adjacent categories can combine. Figure 1 uses
function application, where a complex category con-
sumes an adjacent argument to form its result, e.g.
S [dcl ]\NP combines with the NP to its left to form
an S [dcl ]. More powerful combinators allow cate-
gories to combine with greater flexibility.
We cannot form a PTB tree by simply relabeling
the categories in a CCG derivation because the map-
ping to phrase labels is many-to-many, CCG deriva-
tions contain extra brackets due to binarisation, and
there are cases where the constituents in the PTB tree
and the CCG derivation cross (e.g. in Figure 1).
2.2 Clark and Curran (2009)
Clark and Curran (2009), hereafter C&C-CONV, as-
sign a schema to each leaf (lexical category) and rule
(pair of combining categories) in the CCG derivation.
The PTB tree is constructed from the CCG bottom-
up, creating leaves with lexical schemas, then merg-
ing/adding sub-trees using rule schemas at each step.
The schemas for Figure 1 are shown in Table 1.
These apply to create NPs over magistrates, death,
and suicide, and a VP over labeled, and then com-
bine the trees by placing one under the other at each
step, and finally create an S node at the root.
C&C-CONV has sparsity problems, requiring
schemas for all valid pairs of categories ? at a
minimum, the 2853 unique category combinations
found in CCGbank. Clark and Curran (2009) create
schemas for only 776 of these, handling the remain-
der with approximate catch-all rules.
C&C-CONV only specifies one simple schema for
each rule (pair of categories). This appears reason-
able at first, but frequently causes problems, e.g.:
(N /N )/(N /N ) + N /N
?more than? + ?30? (1)
?relatively? + ?small? (2)
Here either a QP bracket (1) or an ADJP bracket
(2) should be created. Since both examples involve
the same rule schema, C&C-CONV would incorrectly
process them in the same way. To combat the most
glaring errors, C&C-CONV manipulates the PTB tree
with ad-hoc rules based on non-local features over
the CCG nodes being combined ? an approach that
cannot be easily integrated into a parser.
These disadvantages are a consequence of failing
to exploit the generalisations that CCG combinators
define. We return to this example below to show how
our approach handles both cases correctly.
3 Our Approach
Our conversion assigns a set of instructions to each
lexical category and defines generic operations for
each combinator that combine instructions. Figure 2
shows a typical instruction, which specifies the node
to create and where to place the PTB trees associated
with the two categories combining. More complex
operations are shown in Table 2. Categories with
multiple arguments are assigned one instruction per
argument, e.g. labeled has three. These are applied
one at a time, as each combinatory step occurs.
For the example from the previous section we be-
gin by assigning the instructions shown in Table 3.
Some of these can apply immediately as they do not
involve an argument, e.g. magistrates has (NP f).
One of the more complex cases in the example is
Italian, which is assigned (NP f {a}). This creates
a new bracket, inserts the functor?s tree, and flattens
and inserts the argument?s tree, producing:
(NP (JJ Italian) (NNS magistrates))
106
((S\NP)/NP)/NP NP
f a
(S\NP)/NP
f a
VP
Figure 2: An example function application. Top row:
CCG rule. Bottom row: applying instruction (VP f a).
Symbol Meaning Example
(X f a) Add an X bracket around (VP f a)
functor and argument
{ } Flatten enclosed node (N f {a})
X* Use same label as arg. (S* f {a})
or default to X
fi Place subtrees (PP f0 (S f1..k a))
Table 2: Types of operations in instructions.
For the complete example the final tree is almost
correct but omits the S bracket around the final two
NPs. To fix our example we could have modified our
instructions to use the final symbol in Table 2. The
subscripts indicate which subtrees to place where.
However, for this particular construction the PTB an-
notations are inconsistent, and so we cannot recover
without introducing more errors elsewhere.
For combinators other than function application,
we combine the instructions in various ways. Ad-
ditionally, we vary the instructions assigned based
on the POS tag in 32 cases, and for the word not,
to recover distinctions not captured by CCGbank
categories alone. In 52 cases the later instruc-
tions depend on the structure of the argument being
picked up. We have sixteen special cases for non-
combinatory binary rules and twelve special cases
for non-combinatory unary rules.
Our approach naturally handles our QP vs. ADJP
example because the two cases have different lexical
categories: ((N /N )/(N /N ))\(S [adj ]\NP) on than
and (N /N )/(N /N ) on relatively. This lexical dif-
ference means we can assign different instructions to
correctly recover the QP and ADJP nodes, whereas
C&C-CONV applies the same schema in both cases
as the categories combining are the same.
4 Evaluation
Using sections 00-21 of the treebanks, we hand-
crafted instructions for 527 lexical categories, a pro-
cess that took under 100 hours, and includes all the
categories used by the C&C parser. There are 647
further categories and 35 non-combinatory binary
rules in sections 00-21 that we did not annotate. For
Category Instruction set
N (NP f)
N /N1 (NP f {a})
NP [nb]/N1 (NP f {a})
((S [dcl ]\NP3 )/NP2 )/NP1 (VP f a)
(VP {f} a)
(S a f)
Table 3: Instruction sets for the categories in Figure 1.
System Data P R F Sent.
00 (all) 95.37 93.67 94.51 39.6
C&C 00 (len ? 40) 95.85 94.39 95.12 42.1
CONV 23 (all) 95.33 93.95 94.64 39.7
23 (len ? 40) 95.44 94.04 94.73 41.9
00 (all) 96.69 96.58 96.63 51.1
This 00 (len ? 40) 96.98 96.77 96.87 53.6
Work 23 (all) 96.49 96.11 96.30 51.4
23 (len ? 40) 96.57 96.21 96.39 53.8
Table 4: PARSEVAL Precision, Recall, F-Score, and exact
sentence match for converted gold CCG derivations.
unannotated categories, we use the instructions of
the result category with an added instruction.
Table 4 compares our approach with C&C-CONV
on gold CCG derivations. The results shown are as
reported by EVALB (Abney et al, 1991) using the
Collins (1997) parameters. Our approach leads to in-
creases on all metrics of at least 1.1%, and increases
exact sentence match by over 11% (both absolute).
Many of the remaining errors relate to missing
and extra clause nodes and a range of rare structures,
such as QPs, NACs, and NXs. The only other promi-
nent errors are single word spans, e.g. extra or miss-
ing ADVPs. Many of these errors are unrecover-
able from CCGbank, either because inconsistencies
in the PTB have been smoothed over or because they
are genuine but rare constructions that were lost.
4.1 Parser Comparison
When we convert the output of a CCG parser, the PTB
trees that are produced will contain errors created by
our conversion as well as by the parser. In this sec-
tion we are interested in comparing parsers, so we
need to factor out errors created by our conversion.
One way to do this is to calculate a projected score
(PROJ), as the parser result over the oracle result, but
this is a very rough approximation. Another way is
to evaluate only on the 51% of sentences for which
our conversion from gold CCG derivations is perfect
(CLEAN). However, even on this set our conversion
107
0
20
40
60
80
100
0 20 40 60 80 100C
o
nv
er
te
d
C&
C,
EV
A
LB
Converted Gold, EVALB
0
20
40
60
80
100
0 20 40 60 80 100
N
at
iv
e
C&
C,
ld
ep
s
Converted Gold, EVALB
Figure 3: For each sentence in the treebank, we plot
the converted parser output against gold conversion (left),
and the original parser evaluation against gold conversion
(right). Left: Most points lie below the diagonal, indicat-
ing that the quality of converted parser output (y) is upper
bounded by the quality of conversion on gold parses (x).
Right: No clear correlation is present, indicating that the
set of sentences that are converted best (on the far right),
are not necessarily easy to parse.
introduces errors, as the parser output may contain
categories that are harder to convert.
Parser F-scores are generally higher on CLEAN,
which could mean that this set is easier to parse, or it
could mean that these sentences don?t contain anno-
tation inconsistencies, and so the parsers aren?t in-
correct for returning the true parse (as opposed to
the one in the PTB). To test this distinction we look
for correlation between conversion quality and parse
difficulty on another metric. In particular, Figure 3
(right) shows CCG labeled dependency performance
for the C&C parser vs. CCGbank conversion PARSE-
VAL scores. The lack of a strong correlation, and the
spread on the line x = 100, supports the theory that
these sentences are not necessarily easier to parse,
but rather have fewer annotation inconsistencies.
In the left plot, the y-axis is PARSEVAL on con-
verted C&C parser output. Conversion quality essen-
tially bounds the performance of the parser. The few
points above the diagonal are mostly short sentences
on which the C&C parser uses categories that lead
to one extra correct node. The main constructions
on which parse errors occur, e.g. PP attachment, are
rarely converted incorrectly, and so we expect the
number of errors to be cumulative. Some sentences
are higher in the right plot than the left because there
are distinctions in CCG that are not always present in
the PTB, e.g. the argument-adjunct distinction.
Table 5 presents F-scores for three PTB parsers
and three CCG parsers (with their output converted
by our method). One interesting comparison is be-
tween the PTB parser of Petrov and Klein (2007) and
Sentences CLEAN ALL PROJ
Converted gold CCG
CCGbank 100.0 96.3 ?
Converted CCG
Clark and Curran (2007) 90.9 85.5 88.8
Fowler and Penn (2010) 90.9 86.0 89.3
Auli and Lopez (2011) 91.7 86.2 89.5
Native PTB
Klein and Manning (2003) 89.8 85.8 ?
Petrov and Klein (2007) 93.6 90.1 ?
Charniak and Johnson (2005) 94.8 91.5 ?
Table 5: F-scores on section 23 for PTB parsers and
CCG parsers with their output converted by our method.
CLEAN is only on sentences that are converted perfectly
from gold CCG (51%). ALL is over all sentences. PROJ is
a projected F-score (ALL result / CCGbank ALL result).
the CCG parser of Fowler and Penn (2010), which
use the same underlying parser. The performance
gap is partly due to structures in the PTB that are not
recoverable from CCGbank, but probably also indi-
cates that the split-merge model is less effective in
CCG, which has far more symbols than the PTB.
It is difficult to make conclusive claims about
the performance of the parsers. As shown earlier,
CLEAN does not completely factor out the errors in-
troduced by our conversion, as the parser output may
be more difficult to convert, and the calculation of
PROJ only roughly factors out the errors. However,
the results do suggest that the performance of the
CCG parsers is approaching that of the Petrov parser.
5 Conclusion
By exploiting the generalised combinators of the
CCG formalism, we have developed a new method
of converting CCG derivations into PTB-style trees.
Our system, which is publicly available1 , is more
effective than previous work, increasing exact sen-
tence match by more than 11% (absolute), and can
be directly integrated with a CCG parser.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was
supported by a General Sir John Monash Fellow-
ship, the Office of Naval Research under MURI
Grant No. N000140911081, ARC Discovery grant
DP1097291, and the Capital Markets CRC.
1http://code.google.com/p/berkeley-ccg2pst/
108
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306?311.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings of ACL, pages 470?480.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes. In
Proceedings of the Beyond PARSEVAL Workshop at
LREC, pages 4?8.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of ACL, pages 456?463.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of CCG and penn treebank parsers. In
Proceedings of ACL, pages 53?56.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL,
pages 16?23.
Timothy A. D. Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory categorial
grammar. In Proceedings of ACL, pages 335?344.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, School of Informatics, The University of
Edinburgh.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97?114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Takuya Matsuzaki and Jun?ichi Tsujii. 2008. Com-
parative parser performance analysis across grammar
frameworks through automatic tree conversion using
synchronous grammars. In Proceedings of Coling,
pages 545?552.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of IJCNLP, pages
684?693.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL, pages
1?8.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proceedings
of HLT, pages 1?5.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories, pages 159?170.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the Natural Lan-
guage Processing Pacific Rim Symposium, pages 398?
403.
109
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98?103,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Examination of Challenges in Chinese Parsing
Jonathan K. Kummerfeld
?
Daniel Tse
?
James R. Curran
?
Dan Klein
?
?
Computer Science Division
?
School of Information Technology
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,klein}@cs.berkeley.edu {dtse6695,james}@it.usyd.edu.au
Abstract
Aspects of Chinese syntax result in a dis-
tinctive mix of parsing challenges. How-
ever, the contribution of individual sources
of error to overall difficulty is not well un-
derstood. We conduct  a  comprehensive
automatic analysis of error types made by
Chinese parsers, covering a broad range of
error types for large sets of sentences, en-
abling the first empirical ranking of Chi-
nese error types by their performance im-
pact. We also investigate which error types
are resolved by using gold part-of-speech
tags, showing that improving Chinese tag-
ging  only  addresses  certain  error  types,
leaving substantial outstanding challenges.
1 Introduction
A decade of Chinese parsing research, enabled
by the Penn Chinese Treebank (PCTB; Xue et al,
2005), has seen Chinese parsing performance im-
prove from 76.7 F1 (Bikel and Chiang, 2000) to
84.1 F1 (Qian and Liu, 2012). While recent ad-
vances have focused on understanding and reduc-
ing the errors that occur in segmentation and part-
of-speech tagging (Qian and Liu, 2012; Jiang et al,
2009; Forst and Fang, 2009), a range of substantial
issues remain that are purely syntactic.
Early work by Levy and Manning (2003) pre-
sented modifications to a parser motivated by a
manual investigation of parsing errors. They noted
substantial differences between Chinese and En-
glish parsing, attributing some of the differences to
treebank annotation decisions and others to mean-
ingful differences in syntax. Based on this analysis
they considered how to modify their parser to cap-
ture the information necessary to model the syn-
tax within the PCTB. However, their manual ana-
lysis was limited in scope, covering only part of
the parser output, and was unable to characterize
the relative impact of the issues they uncovered.
This paper presents a more comprehensive ana-
lysis of errors in Chinese parsing, building on the
technique presented in Kummerfeld et al (2012),
which characterized the error behavior of English
parsers by quantifying how often they make er-
rors such as PP attachment and coordination scope.
To accommodate error classes that are absent in
English, we  augment  the  system  to  recognize
Chinese-specific parse errors.
1
We use the modi-
fied system to show the relative impact of different
error types across a range of Chinese parsers.
To understand the impact of tagging errors on
different  error  types, we  performed  a  part-of-
speech ablation experiment, in  which particular
confusions are introduced in isolation. By analyz-
ing the distribution of errors in the system output
with and without gold part-of-speech tags, we are
able to isolate and quantify the error types that can
be resolved by improvements in tagging accuracy.
Our analysis shows that improvements in tag-
ging accuracy can only address a subset of the chal-
lenges of Chinese syntax. Further improvement in
Chinese parsing performance will require research
addressing other challenges, in particular, deter-
mining coordination scope.
2 Background
The closest previous work is the detailed manual
analysis performed by Levy and Manning (2003).
While their focus was on issues faced by their fac-
tored PCFG parser (Klein and Manning, 2003b),
the error types they identified are general issues
presented by Chinese syntax in the PCTB. They
presented several Chinese error types that are rare
or absent in English, including noun/verb ambigu-
ity, NP-internal structure and coordination ambi-
guity due to pro-drop, suggesting that closing the
English-Chinese parsing gap demands techniques
1
The system described  in  this  paper  is  available  from
http://code.google.com/p/berkeley-parser-analyser/
98
beyond those currently used for English. How-
ever, as noted in their final section, their manual
analysis of parse errors in 100 sentences only cov-
ered a portion of a single parser?s output, limiting
the conclusions they could reach regarding the dis-
tribution of errors in Chinese parsing.
2.1 Automatic Error Analysis
Our  analysis  builds  on  Kummerfeld  et al
(2012), which presented a system that automati-
cally classifies English parse errors using a two
stage process. First, the system finds the shortest
path from the system output to the gold annota-
tions, where each step in the path is a tree transfor-
mation, fixing at least one bracket error. Second,
each transformation step is classified into one of
several error types.
When directly applied to Chinese parser output,
the system placed over 27% of the errors in the
catch-all ?Other? type. Many of these errors clearly
fall into one of a small set of error types, motivat-
ing an adaptation to Chinese syntax.
3 Adapting error analysis to Chinese
To adapt the Kummerfeld et al (2012) system to
Chinese, we developed a new version of the second
stage of the system, which assigns an error cate-
gory to each tree transformation step.
To characterize the errors the original system
placed in the ?Other? category, we looked through
one  hundred  sentences, identifying  error  types
generated by Chinese syntax that the existing sys-
tem did not account for. With these observations
we were able to implement new rules to catch the
previously missed cases, leading to the set shown
in Table 1. To ensure the accuracy of our classifica-
tions, we alternated between refining the classifica-
tion code and looking at affected classifications to
identify issues. We also periodically changed the
sentences from the development set we manually
checked, to avoid over-fitting.
Where necessary, we also expanded the infor-
mation available during classification. For exam-
ple, we use the structure of the final gold standard
tree when classifying errors that are a byproduct of
sense disambiguation errors.
4 Chinese parsing errors
Table 1 presents the errors made by the Berkeley
parser. Below we describe the error types that are
Error Type Brackets % of total
NP-internal* 6019 22.70%
Coordination 2781 10.49%
Verb taking wrong args* 2310 8.71%
Unary 2262 8.53%
Modifier Attachment 1900 7.17%
One Word Span 1560 5.88%
Different label 1418 5.35%
Unary A-over-A 1208 4.56%
Wrong sense/bad attach* 1018 3.84%
Noun boundary error* 685 2.58%
VP Attachment 626 2.36%
Clause Attachment 542 2.04%
PP Attachment 514 1.94%
Split Verb Compound* 232 0.88%
Scope error* 143 0.54%
NP Attachment 109 0.41%
Other 3186 12.02%
Table 1: Errors made when parsing Chinese. Values are the
number of bracket errors attributed to that error type. The
values shown are for the Berkeley parser, evaluated on the
development set. * indicates error types that were added or
substantially changed as part of this work.
either new in this analysis, have had their definition
altered, or have an interesting distribution.
2
In all of our results we follow Kummerfeld et al
(2012), presenting the number of bracket errors
(missing or extra)  attributed to each error type.
Bracket counts are more informative than a direct
count of each error type, because the impact on
EVALB F-score varies between errors, e.g. a sin-
gle attachment error can cause 20 bracket errors,
while a unary error causes only one.
NP-internal. (Figure 1a). Unlike  the  Penn
Treebank (Marcus et al, 1993), the PCTB anno-
tates some NP-internal structure. We assign this
error type when a transformation involves words
whose parts of speech in the gold tree are one of:
CC, CD, DEG, ETC, JJ, NN, NR, NT and OD.
We investigated the errors that fall into the NP-
internal category and found that 49% of the errors
involved the creation or deletion of a single pre-
termianl phrasal bracket. These errors arise when
a parser proposes a tree in which POS tags (for in-
stance, JJ or NN) occur as siblings of phrasal tags
(such as NP), a configuration used by the PCTB
bracketing guidelines to indicate complementation
as opposed to adjunction (Xue et al, 2005).
2
For an explanation of the English error types, see Kum-
merfeld et al (2012).
99
Verb taking wrong args. (Figure 1b). This
error type arises when a verb (e.g.?? reverse)
is  hypothesized  to  take  an  incorrect  argument
(?? Bush instead of ?? position). Note that
this also covers some of the errors that Kummer-
feld  et al (2012) classified  as  NP Attachment,
changing the distribution for that type.
Unary. For mis-application of unary rules we
separate out instances in which the two brackets in
the production have the the same label (A-over-A).
This cases is created when traces are eliminated, a
standard step in evaluation. More than a third of
unary errors made by the Berkeley parser are of the
A-over-A type. This can be attributed to two fac-
tors: (i) the PCTB annotates non-local dependen-
cies using traces, and (ii) Chinese syntax generates
more traces than English syntax (Guo et al, 2007).
However, for parsers that do not return traces they
are a benign error.
Modifier attachment. (Figure 1c). Incorrect
modifier scope caused by modifier phrase attach-
ment level. This is less frequent in Chinese than
in English: while English VP modifiers occur in
pre- and post-verbal positions, Chinese only al-
lows pre-verbal modification.
Wrong sense/bad attach. (Figure 1d). This ap-
plies when the head word of a phrase receives the
wrong POS, leading to an attachment error. This
error type is common in Chinese because of POS
fluidity, e.g. the well-known Chinese verb/noun
ambiguity often causes mis-attachments that are
classified as this error type.
In  Figure 1d, the  word ?? invest has
both  noun  and  verb  senses. While  the  gold
standard  interpretation  is  the  relative  clause
firms that Macau invests in, the parser returned an
NP interpretation Macau investment firms.
Noun boundary error. In this error type, a span
is moved to a position where the POS tags of its
new siblings all belong to the list of NP-internal
structure tags which we identified above, reflecting
the inclusion of additional material into an NP.
Split  verb  compound. The  PCTB annota-
tions recognize several Chinese verb compound-
ing strategies, such as  the serial  verb construc-
tion (???? plan [and] build) and the resulta-
tive construction (?? cook [until] done), which
join a bare verb to another lexical item. We in-
troduce an error type specific to Chinese, in which
such verb compounds are split, with the two halves
of the compound placed in different phrases.
..NP
.
.NN .
??
coach
.
.NN .
??
soccer
.
.NN .
??
nat'l
.NP
.
.NP
.
.NP.NN
.
.NP.NN
.
.NP.NN
(a) NP-internal structure errors
..VP
.
.NP
.
.NP .
??
position
.
.DNP
.
.DEG .?
.
.NP .
??
Bush
.
.VV .
??
reverse
.CP
.
.IP
.
.VP
.
.VV
.
.NP
.
.DEC
.
.NP
(b) Verb taking wrong arguments
..VP
.
.VP .
????
win gold
.
.QP
.
.QP .
???
3rd time
.
.ADVP .
??
in a row
.VP
.
.ADVP
.
.QP
.
.QP
.VP
(c) Modifier attachment ambiguity
..CP
.
.NP .
??
firm
.
.IP
.
.VP .
??
invest
.
.NP .
??
Macau
.NP
.
.NP
.
.NP
.
.NP
.NP
(d) Sense confusion
Figure 1: Prominent error types in Chinese parsing. The left
tree is the gold structure; the right is the parser hypothesis.
Scope error. These are cases in which a new
span must be added to more closely bind a modifier
phrase (ADVP, ADJP, and PP).
PP attachment. This error type is rare in Chi-
nese, as adjunct PPs are pre-verbal. It does oc-
cur near coordinated VPs, where ambiguity arises
about  which of  the conjuncts  the PP has scope
over. Whether this particular case is PP attachment
or coordination is debatable; we follow Kummer-
feld et al (2012) and label it PP attachment.
4.1 Chinese-English comparison
It is difficult to directly compare error analysis
results for Chinese and English parsing because
of substantial changes in the classification method,
and differences in treebank annotations.
As described in the previous section, the set of
error categories considered for Chinese is very dif-
ferent to the set of categories for English. Even
for some of the categories that were not substan-
tially changed, errors may be classified differently
because of cross-over between categories between
100
NP Verb Mod. 1-Word Diff Wrong Noun VP Clause PP
System F1 Int. Coord Args Unary Attach Span Label Sense Edge Attach Attach Attach Other
Best 1.54 1.25 1.01 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87
Berk-G 86.8
Berk-2 81.8
Berk-1 81.1
ZPAR 78.1
Bikel 76.1
Stan-F 76.0
Stan-P 70.0
Worst 3.94 1.75 1.73 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11
Table 2: Error breakdown for the development set of PCTB 6. The area filled in for each bar indicates the average number of
bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in
the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with
two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel),
the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P).
two categories (e.g. between Verb taking wrong
args and NP Attachment).
Differences in treebank annotations also present
a challenge for cross-language error comparison.
The  most  common  error  type  in  Chinese, NP-
internal structure, is rare in the results of Kummer-
feld et al (2012), but the datasets are not compara-
ble because the PTB has very limited NP-internal
structure annotated. Further characterization of the
impact of annotation differences on errors is be-
yond the scope of this paper.
Three conclusions that can be made are that (i)
coordination is a major issue in both languages,
(ii) PP attachment is a much greater problem in
English, and  (iii)  a  higher  frequency  of  trace-
generating syntax in Chinese compared to English
poses substantial challenges.
5 Cross-parser analysis
The previous section described the error types
and their distribution for a single Chinese parser.
Here we confirm that these are general trends, by
showing that the same pattern is observed for sev-
eral  different  parsers  on  the  PCTB 6 dev  set.
3
We include results  for  a  transition-based parser
(ZPAR; Zhang  and  Clark, 2009), a  split-merge
PCFG parser (Petrov et al, 2006; Petrov and Klein,
2007; Petrov, 2010), a lexicalized parser (Bikel
and Chiang, 2000), and a factored PCFG and de-
pendency parser (Levy and Manning, 2003; Klein
and Manning, 2003a,b).
4
Comparing the two Stanford parsers in Table 2,
the factored model provides clear improvements
3
We use the standard data split suggested by the PCTB 6
file manifest. As a result, our results differ from those previ-
ously reported on other splits. All analysis is on the dev set,
to avoid revealing specific information about the test set.
4
These parsers represent a variety of parsing methods,
though exclude some recently developed parsers that are not
publicly available (Qian and Liu, 2012; Xiong et al, 2005).
on  sense  disambiguation, but  performs  slightly
worse on coordination.
The Berkeley product parser we include uses
only two grammars because we found, in contrast
to the English results (Petrov, 2010), that further
grammars provided limited benefits. Comparing
the performance with the standard Berkeley parser
it seems that the diversity in the grammars only as-
sists certain error types, with most of the improve-
ment  occurring in  four  of  the categories, while
there is no improvement, or a slight decrease, in
five categories.
6 Tagging Error Impact
The challenge of accurate POS tagging in Chi-
nese has been a major part of several recent papers
(Qian and Liu, 2012; Jiang et al, 2009; Forst and
Fang, 2009). The Berk-G row of Table 2 shows
the performance of the Berkeley parser when given
gold POS tags.
5
While the F1 improvement is un-
surprising, for the first time we can clearly show
that the gains are only in a subset of the error types.
In particular, tagging improvement will not help
for two of the most significant challenges: coordi-
nation scope errors, and verb argument selection.
To see which tagging confusions contribute to
which error reductions, we adapt the POS ablation
approach of Tse and Curran (2012). We consider
the POS tag pairs shown in Table 3. To isolate the
effects of each confusion we start from the gold
tags and introduce the output of the Stanford tag-
ger whenever it returns one of the two tags being
considered.
6
We then feed these ?semi-gold? tags
5
We used the Berkeley parser as it was the best of the
parsers we considered. Note that the Berkeley parser occa-
sionally prunes all of the parses that use the gold POS tags,
and so returns the best available alternative. This leads to a
POS accuracy of 99.35%, which is still well above the parser?s
standard POS accuracy of 93.66%.
6
We introduce errors to gold tags, rather than removing er-
101
Confused tags Errors ? F1
VV NN 1055 -2.72
DEC DEG 526 -1.72
JJ NN 297 -0.57
NR NN 320 -0.05
Table 3: The most frequently confused POS tag pairs. Each
? F1 is relative to Berk-G.
to the Berkeley parser, and run the fine-grained er-
ror analysis on its output.
VV/NN. This confusion has been consistently
shown to be a major contributor to parsing errors
(Levy and Manning, 2003; Tse and Curran, 2012;
Qian and Liu, 2012), and we find a drop of over 2.7
F1 when the output of the tagger is introduced. We
found that while most error types have contribu-
tions from a range of POS confusions, verb/noun
confusion was responsible for virtually all of the
noun boundary errors corrected by using gold tags.
DEG/DEC. This confusion between the rela-
tivizer and subordinator senses of the particle ?
de is the primary source of improvements on mod-
ifier attachment when using gold tags.
NR/NN and JJ/NN. Despite  their  frequency,
these confusions have little effect on parsing per-
formance. Even within the NP-internal error type
their impact is limited, and almost all of the errors
do not change the logical form.
7 Conclusion
We have  quantified  the  relative  impacts  of  a
comprehensive set of error types in Chinese pars-
ing. Our analysis has also shown that while im-
provements in Chinese POS tagging can make a
substantial difference for some error types, it will
not address two high-frequency error types: in-
correct verb argument attachment and coordina-
tion scope. The frequency of these two error types
is also unimproved by the use of products of la-
tent variable grammars. These observations sug-
gest that resolving the core challenges of Chinese
parsing will require new developments that suit the
distinctive properties of Chinese syntax.
Acknowledgments
We extend our thanks to Yue Zhang for helping
us train new ZPAR models. We would also like
to thank the anonymous reviewers for their help-
ful suggestions. This research was supported by
a General Sir John Monash Fellowship to the first
rors from automatic tags, isolating the effect of a single con-
fusion by eliminating interaction between tagging decisions.
author, the Capital Markets CRC under ARC Dis-
covery grant DP1097291, and the NSF under grant
0643742.
References
Daniel M. Bikel and David Chiang. 2000. Two
Statistical Parsing Models Applied to the Chi-
nese Treebank. In Proceedings of the Second
Chinese Language Processing Workshop, pages
1?6. Hong Kong, China.
Martin Forst and Ji Fang. 2009. TBL-improved
non-deterministic  segmentation  and  POS tag-
ging for a Chinese parser. In Proceedings of the
12th Conference of the European Chapter of the
ACL, pages 264?272. Athens, Greece.
Yuqing Guo, Haifeng Wang, and Josef van Gen-
abith. 2007. Recovering Non-Local Dependen-
cies for Chinese. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 257?266. Prague, Czech Republic.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging
? A Case Study. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
volume 1, pages 522?530. Suntec, Singapore.
Dan Klein and Christopher D. Manning. 2003a.
Accurate Unlexicalized Parsing. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 423?430.
Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b.
Fast Exact Inference with a Factored Model for
Natural Language Parsing. In Advances in Neu-
ral Information Processing Systems 15, pages
3?10. MIT Press, Cambridge, MA.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser Show-
down at the Wall Street Corral: An Empirical
Investigation of Error Types in Parser Output.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 1048?1059. Jeju Island, South
Korea.
102
Roger Levy and Christopher Manning. 2003. Is
it harder to parse Chinese, or the Chinese Tree-
bank? In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguis-
tics, pages 439?446. Sapporo, Japan.
Mitchell P.  Marcus, Beatrice  Santorini, and
Mary Ann  Marcinkiewicz.  1993. Building
a  Large  Annotated  Corpus  of  English: The
Penn  Treebank. Computational  Linguistics,
19(2):313?330.
Slav Petrov. 2010. Products of Random Latent
Variable Grammars. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational  Linguistics, pages  19?27.  Los
Angeles, California.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein.  2006. Learning  Accurate, Com-
pact, and Interpretable Tree Annotation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual
Meeting of the Association for Computational
Linguistics, pages 433?440. Sydney, Australia.
Slav Petrov and Dan Klein. 2007. Improved In-
ference for Unlexicalized Parsing. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics; Proceed-
ings of the Main Conference, pages 404?411.
Rochester, New York, USA.
Xian Qian and Yang Liu.  2012. Joint Chinese
word segmentation, POS tagging and parsing.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 501?511. Jeju Island, Korea.
Daniel  Tse  and  James R.  Curran.  2012. The
Challenges of Parsing Chinese with Combina-
tory Categorial Grammar. In Proceedings of the
2012 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, pages
295?304. Montre?al, Canada.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun
Lin, and Yueliang Qian. 2005. Parsing the Penn
Chinese Treebank with semantic knowledge. In
Proceedings of  the Second international  joint
conference  on  Natural  Language  Processing,
pages 70?81. Jeju Island, Korea.
Nianwen  Xue, Fei  Xia, Fu-Dong  Chiou, and
Martha  Palmer.  2005. The  Penn  Chinese
TreeBank: Phrase  structure  annotation  of  a
large corpus. Natural Language Engineering,
11(2):207?238.
Yue Zhang and Stephen Clark. 2009. Transition-
Based Parsing of the Chinese Treebank using a
Global Discriminative Model. In Proceedings
of the 11th International Conference on Parsing
Technologies (IWPT?09), pages 162?171. Paris,
France.
103
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 102?106,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Mention Detection: Heuristics for the OntoNotes annotations
Jonathan K. Kummerfeld, Mohit Bansal, David Burkett and Dan Klein
Computer Science Division
University of California at Berkeley
{jkk,mbansal,dburkett,klein}@cs.berkeley.edu
Abstract
Our submission was a reduced version of
the system described in Haghighi and Klein
(2010), with extensions to improve mention
detection to suit the OntoNotes annotation
scheme. Including exact matching mention
detection in this shared task added a new and
challenging dimension to the problem, partic-
ularly for our system, which previously used
a very permissive detection method. We im-
proved this aspect of the system by adding
filters based on the annotation scheme for
OntoNotes and analysis of system behavior on
the development set. These changes led to im-
provements in coreference F-score of 10.06,
5.71, 6.78, 6.63 and 3.09 on the MUC, B3,
Ceaf-e, Ceaf-m and Blanc, metrics, respec-
tively, and a final task score of 47.10.
1 Introduction
Coreference resolution is concerned with identifying
mentions of entities in text and determining which
mentions are referring to the same entity. Previously
the focus in the field has been on the latter task.
Typically, mentions were considered correct if their
span was within the true span of a gold mention, and
contained the head word. This task (Pradhan et al,
2011) has set a harder challenge by only considering
exact matches to be correct.
Our system uses an unsupervised approach based
on a generative model. Unlike previous work, we
did not use the Bllip or Wikipedia data described in
Haghighi and Klein (2010). This was necessary for
the system to be eligible for the closed task.
The system detects mentions by finding the max-
imal projection of every noun and pronoun. For the
OntoNotes corpus this approach posed several prob-
lems. First, the annotation scheme explicitly rejects
noun phrases in certain constructions. And second,
it includes coreference for events as well as things.
In preliminary experiments on the development set,
we found that spurious mentions were our primary
source of error. Using an oracle to exclude all spu-
rious mentions at evaluation time yielded improve-
ments ranging from five to thirty percent across the
various metrics used in this task. Thus, we decided
to focus our efforts on methods for detecting and fil-
tering spurious mentions.
To improve mention detection, we filtered men-
tions both before and after coreference resolution.
Filters prior to coreference resolution were con-
structed based on the annotation scheme and partic-
ular cases that should never be mentions (e.g. single
word spans with the EX tag). Filters after corefer-
ence resolution were constructed based on analysis
of common errors on the development set.
These changes led to considerable improvement
in mention detection precision. The heuristics used
in post-resolution filtering had a significant negative
impact on recall, but this cost was out-weighed by
the improvements in precision. Overall, the use of
these filters led to a significant improvement in F1
across all the coreference resolution evaluation met-
rics considered in the task.
2 Core System
We use a generative approach that is mainly un-
supervised, as described in detail in Haghighi and
102
Klein (2010), and briefly below.
2.1 Model
The system uses all three of the standard abstrac-
tions in coreference resolution; mentions, entities
and types. A mention is a span in the text, the en-
tity is the actual object or event the mention refers
to, and each type is a group of entities. For example,
?the Mountain View based search giant? is a men-
tion that refers to the entity Google, which is of type
organization.
At each level we define a set of properties (e.g.
proper-head). For mentions, these properties are
linked directly to words from the span. For enti-
ties, each property corresponds to a list of words,
instances of which are seen in specific mentions of
that entity. At the type level, we assign a pair of
multinomials to each property. The first of these
multinomials is a distribution over words, reflecting
their occurrence for this property for entities of this
type. The second is a distribution over non-negative
integers, representing the length of word lists for this
property in entities of this type.
The only form of supervision used in the system
is at the type level. The set of types is defined and
lists of prototype words for each property of each
type are provided. We also include a small number
of extra types with no prototype words, for entities
that do not fit well in any of the specified types.
These abstractions are used to form a generative
model with three components; a semantic module, a
discourse module and a mention module. In addi-
tion to the properties and corresponding parameters
described above, the model is specified by a multi-
nomial prior over types (?), log-linear parameters
over discourse choices (pi), and a small number of
hyperparameters (?).
Entities are generated by the semantic module by
drawing a type t according to ?, and then using that
type?s multinomials to populate word lists for each
property.
The assignment of entities to mentions is handled
by the discourse module. Affinities between men-
tions are defined by a log-linear model with param-
eters pi for a range of standard features.
Finally, the mention module generates the ac-
tual words in the span. Words are drawn for each
property from the lists for the relevant entity, with
a hyper-parameter for interpolation between a uni-
form distribution over the words for the entity and
the underlying distribution for the type. This allows
the model to capture the fact that some properties
use words that are very specific to the entity (e.g.
proper names) while others are not at all specific
(e.g. pronouns).
2.2 Learning and Inference
The learning procedure finds parameters that are
likely under the model?s posterior distribution. This
is achieved with a variational approximation that
factors over the parameters of the model. Each set
of parameters is optimized in turn, while the rest are
held fixed. The specific update methods vary for
each set of parameters; for details see Section 4 of
Haghighi and Klein (2010).
3 Mention detection extensions
The system described in Haghighi and Klein (2010)
includes every NP span as a mention. When run on
the OntoNotes data this leads to a large number of
spurious mentions, even when ignoring singletons.
One challenge when working with the OntoNotes
data is that singleton mentions are not annotated.
This makes it difficult to untangle errors in coref-
erence resolution and errors in mention detection. A
mention produced by the system might not be in the
gold set for one of two reasons; either because it is
a spurious mention, or because it is not co-referent.
Without manually annotating the singletons in the
data, these two cases cannot be easily separated.
3.1 Baseline mention detection
The standard approach used in the system to detect
mentions is to consider each word and its maximal
projection, accepting it only if the span is an NP or
the word is a pronoun. This approach will intro-
duce spurious mentions if the parser makes a mis-
take, or if the NP is not considered a mention in the
OntoNotes corpus. In this work, we considered the
provided parses and parses produced by the Berke-
ley parser (Petrov et al, 2006) trained on the pro-
vided training data. We added a set of filters based
on the annotation scheme described by Pradhan et al
(2007). Some filters are applied before coreference
resolution and others afterward, as described below.
103
Data Set Filters P R F
Dev
None 37.59 76.93 50.50
Pre 39.49 76.83 52.17
Post 59.05 68.08 63.24
All 58.69 67.98 63.00
Test All 56.97 69.77 62.72
Table 1: Mention detection performance with various
subsets of the filters.
3.2 Before Coreference Resolution
The pre-resolution filters were based on three reli-
able features of spurious mentions:
? Appositive constructions
? Attributes signaled by copular verbs
? Single word mentions with a POS tag in the set:
EX, IN, WRB, WP
To detect appositive constructions we searched
for the following pattern:
NP
NP , NP . . .
And to detect attributes signaled by copular struc-
tures we searched for this pattern:
VP
cop verb NP
where we used the fairly conservative set of cop-
ular verbs: {is, are, was, ?m}. In both
cases, any mention whose maximal NP projection
appeared as the bold node in a subtree matching the
pattern was excluded.
In all three cases, errors from the parser (or POS
tagger) may lead to the deletion of valid mentions.
However, we found the impact of this was small and
was outweighed by the number of spurious mentions
removed.
3.3 After Coreference Resolution
To construct the post-coreference filters we analyzed
system output on the development set, and tuned
Filters MUC B3 Ceaf-e Blanc
None 25.24 45.89 50.32 59.12
Pre 27.06 47.71 50.15 60.17
Post 42.08 62.53 43.88 66.54
All 42.03 62.42 43.56 66.60
Table 2: Precision for coreference resolution on the dev
set.
Filters MUC B3 Ceaf-e Blanc
None 50.54 78.54 26.17 62.77
Pre 51.20 77.73 27.23 62.97
Post 45.93 64.72 39.84 61.20
All 46.21 64.96 39.24 61.28
Table 3: Recall for coreference resolution on the dev set.
based on MUC and B3 performance. The final set
of filters used were:
? Filter if the head word is in a gazetteer, which
we constructed based on behavior on the devel-
opment set (head words found using the Collins
(1999) rules)
? Filter if the POS tag is one of WDT, NNS, RB,
JJ, ADJP
? Filter if the mention is a specific case of you
or it that is more often generic (you know,
you can, it is)
? Filter if the mention is any cardinal other than
a year
A few other more specific filters were also in-
cluded (e.g. ?s when tagged as PRP) and one type
of exception (if all words are capitalized, the men-
tion is kept).
4 Other modifications
The parses in the OntoNotes data include the addi-
tion of structure within noun phrases. Our system
was not designed to handle the NML tag, so we
removed such nodes, reverting to the standard flat-
tened NP structures found in the Penn Treebank.
We also trained the Berkeley parser on the pro-
vided training data, and used it to label the develop-
ment and test sets.1 We found that performance was
1In a small number of cases, the Berkeley parser failed, and
we used the provided parse tree instead.
104
Filters MUC B3 Ceaf-e Ceaf-m Blanc
None 33.67 57.93 34.43 42.72 60.60
Pre 35.40 59.13 35.29 43.72 61.38
Post 43.92 63.61 41.76 49.74 63.26
All 44.02 63.66 41.29 49.46 63.34
Table 4: F1 scores for coreference resolution on the dev
set.
slightly improved by the use of these parses instead
of the provided parses.
5 Results
Since our focus when extending our system for this
task was on mention detection, we present results
with variations in the sets of mention filters used. In
particular, we have included results for our baseline
system (None), when only the filters before coref-
erence resolution are used (Pre), when only the fil-
ters after coreference resolution are used (Post), and
when all filters are used (All).
The main approach behind the pre-coreference fil-
ters was to consider the parse to catch cases that are
almost never mentions. In particular, these filters
target cases that are explicitly excluded by the an-
notation scheme. As Table 1 shows, this led to a
1.90% increase in mention detection precision and
0.13% decrease in recall, which is probably a result
of parse errors.
For the post-coreference filters, the approach was
quite different. Each filter was introduced based on
analysis of the errors in the mention sets produced
by our system on the development set. Most of the
filters constructed in this way catch some true men-
tions as well as spurious mentions, leading to signif-
icant improvements in precision at the cost of recall.
Specifically an increase of 21.46% in precision and
decrease of 8.85% in recall, but an overall increase
of 12.74% in F1-score.
As Tables 2 and 3 show, these changes in mention
detection performance generally lead to improve-
ments in precision at the expense of recall, with the
exception of Ceaf-e where the trends are reversed.
However, as shown in Table 4, there is an overall
improvement in F1 in all cases.
In general the change from only post-coreference
filters to all filters is slightly negative. The final sys-
Metric R P F1
MUC 46.39 39.56 42.70
B3 63.60 57.30 60.29
Ceaf-m 45.35 45.35 45.35
Ceaf-e 35.05 42.26 38.32
Blanc 58.74 61.58 59.91
Table 5: Complete results on the test set
tem used all of the filters because the process used to
create the post-coreference filters was more suscep-
tible to over-fitting, and the pre-coreference filters
provided such an unambiguously positive contribu-
tion to mention detection.
6 Conclusion
We modified the coreference system of Haghighi
and Klein (2010) to improve mention detection per-
formance. We focused on tuning using the MUC and
B3 metrics, but found considerable improvements
across all metrics.
One important difference between the system de-
scribed here and previous work was the data avail-
able. Unlike Haghighi and Klein (2010), no extra
data from Wikipedia or Bllip was used, a restriction
that was necessary to be eligible for the closed part
of the task.
By implementing heuristics based on the annota-
tion scheme for the OntoNotes data set and our own
analysis of system behavior on the development set
we were able to achieve the results shown in Table 5,
giving a final task score of 47.10.
7 Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
ported by the Office of Naval Research under MURI
Grant No. N000140911081, and a General Sir John
Monash Fellowship.
References
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
105
ings of NAACL, pages 385?393, Los Angeles, Califor-
nia, June. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
ontonotes. In Proceedings of the International Confer-
ence on Semantic Computing, pages 446?453, Wash-
ington, DC, USA. IEEE Computer Society.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
106
