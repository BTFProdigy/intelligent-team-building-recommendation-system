Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 66?69,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH Machine Translation System for WMT 2009
Maja Popovic?, David Vilar, Daniel Stein, Evgeny Matusov and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the shared transla-
tion task of the Fourth Workshop of Sta-
tistical Machine Translation (WMT 2009)
with the German-English, French-English
and Spanish-English pair in each transla-
tion direction. The submissions were gen-
erated using a phrase-based and a hierar-
chical statistical machine translation sys-
tems with appropriate morpho-syntactic
enhancements. POS-based reorderings of
the source language for the phrase-based
systems and splitting of German com-
pounds for both systems were applied. For
some tasks, a system combination was
used to generate a final hypothesis. An ad-
ditional English hypothesis was produced
by combining all three final systems for
translation into English.
1 Introduction
For the WMT 2009 shared task, RWTH submit-
ted translations for the German-English, French-
English and Spanish-English language pair in both
directions. A phrase-based translation system en-
hanced with appropriate morpho-syntactic trans-
formations was used for all translation direc-
tions. Local POS-based word reorderings were ap-
plied for the Spanish-English and French-English
pair, and long range reorderings for the German-
English pair. For this language pair splitting
of German compounds was also applied. Spe-
cial efforts were made for the French-English and
German-English translation, where a hierarchi-
cal system was also used and the final submis-
sions are the result of a system combination. For
translation into English, an additional hypothesis
was produced as a result of combination of the
final German-to-English, French-to-English and
Spanish-to-English systems.
2 Translation models
2.1 Phrase-based model
We used a standard phrase-based system similar to
the one described in (Zens et al, 2002). The pairs
of source and corresponding target phrases are ex-
tracted from the word-aligned bilingual training
corpus. Phrases are defined as non-empty contigu-
ous sequences of words. The phrase translation
probabilities are estimated using relative frequen-
cies. In order to obtain a more symmetric model,
the phrase-based model is used in both directions.
2.2 Hierarchical model
The hierarchical phrase-based approach can be
considered as an extension of the standard phrase-
based model. In this model we allow the phrases
to have ?gaps?, i.e. we allow non-contiguous parts
of the source sentence to be translated into pos-
sibly non-contiguous parts of the target sentence.
The model can be formalized as a synchronous
context-free grammar (Chiang, 2007). The model
also included some additional heuristics which
have shown to be helpful for improving translation
quality, as proposed in (Vilar et al, 2008).
The first step in the hierarchical phrase extrac-
tion is the same as for the phrased-based model.
Having a set of initial phrases, we search for
phrases which contain other smaller sub-phrases
and produce a new phrase with gaps. In our sys-
tem, we restricted the number of non-terminals for
each hierarchical phrase to a maximum of two,
which were also not allowed to be adjacent. The
scores of the phrases are again computed as rela-
tive frequencies.
2.3 Common models
For both translation models, phrase-based and hi-
erarchical, additional common models were used:
word-based lexicon model, phrase penalty, word
penalty and target language model.
66
The target language model was a standard n-
gram language model trained by the SRI language
modeling toolkit (Stolcke, 2002). The smooth-
ing technique we apply was the modified Kneser-
Ney discounting with interpolation. In our case we
used a 4-gram language model.
3 Morpho-syntactic transformations
3.1 POS-based word reorderings
For the phrase-based systems, the local and
long range POS-based reordering rules described
in (Popovic? and Ney, 2006) were applied on the
training and test corpora as a preprocessing step.
Local reorderings were used for the Spanish-
English and French-English language pairs in or-
der to handle differences between the positions of
nouns and adjectives in the two languages. Adjec-
tives in Spanish and French, as in most Romanic
languages, are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, for these language pairs local
reorderings of nouns and adjective groups in the
source language were applied. The following se-
quences of words are considered to be an adjective
group: a single adjective, two or more consecutive
adjectives, a sequence of adjectives and coordinate
conjunctions, as well as an adjective along with its
corresponding adverb. If the source language is
Spanish or French, each noun is moved behind the
corresponding adjective group. If the source lan-
guage is English, each adjective group is moved
behind the corresponding noun.
Long range reorderings were applied on the
verb groups for the German-English language pair.
Verbs in the German language can often be placed
at the end of a clause. This is mostly the case
with infinitives and past participles, but there are
many cases when other verb forms also occur at
the clause end. For the translation from German
into English, following verb types were moved to-
wards the beginning of a clause: infinitives, infini-
tives+zu, finite verbs, past participles and negative
particles. For the translation from English to Ger-
man, infinitives and past participles were moved
to the end of a clause, where punctuation marks,
subordinate conjunctions and finite verbs are con-
sidered as the beginning of the next clause.
3.2 German compound words
For the translation from German into English, Ger-
man compounds were split using the frequency-
based method described in (Koehn and Knight,
2003). For the other translation direction, the En-
glish text was first translated into the modified
German language with split compounds. The gen-
erated output was then postprocessed, i.e. the
components were merged using the method de-
scribed in (Popovic? et al, 2006): a list of com-
pounds and a list of components are extracted from
the original German training corpus. If the word
in the generated output is in the component list,
check if this word merged with the next word is in
the compound list. If it is, merge the two words.
4 System combination
For system combination we used the approach de-
scribed in (Matusov et al, 2006). The method is
based on the generation of a consensus transla-
tion out of the output of different translation sys-
tems. The core of the method consists in building
a confusion network for each sentence by align-
ing and combining the (single-best) translation hy-
pothesis from one MT system with the translations
produced by the other MT systems (and the other
translations from the same system, if n-best lists
are used in combination). For each sentence, each
MT system is selected once as ?primary? system,
and the other hypotheses are aligned to this hy-
pothesis. The resulting confusion networks are
combined into a signle word graph, which is then
weighted with system-specific factors, similar to
the approach of (Rosti et al, 2007), and a trigram
LM trained on the MT hypotheses. The translation
with the best total score within this word graph is
selected as consensus translation. The scaling fac-
tors of these models are optimized using the Con-
dor toolkit (Berghen and Bersini, 2005) to achieve
optimal BLEU score on the dev set.
5 Experimental results
5.1 Experimental settings
For all translation directions, we used the provided
EuroParl and News parallel corpora to train the
translation models and the News monolingual cor-
pora to train the language models. All systems
were optimised for the BLEU score on the develop-
ment data (the ?dev-a? part of the 2008 evaluation
data). The other part of the 2008 evaluation set
(?dev-b?) is used as a blind test set. The results re-
ported in the next section will be referring to this
test set. For the tasks including a system combi-
nation, the parameters for the system combination
67
were also trained on the ?dev-b? set. The reported
evaluation metrics are the BLEU score and two
syntax-oriented metrics which have shown a high
correlation with human evaluations: the PBLEU
score (BLEU calculated on POS sequences) and
the POS-F-score PF (similar to the BLEU score but
based on the F-measure instead of precision and
on arithmetic mean instead of geometric mean).
The POS tags used for reorderings and for syn-
tactic evaluation metrics for the English and the
German corpora were generated using the statisti-
cal n-gram-based TnT-tagger (Brants, 2000). The
Spanish corpora are annotated using the FreeLing
analyser (Carreras et al, 2004), and the French
texts using the TreeTagger1.
5.2 Translation results
Table 1 presents the results for the German-
English language pair. For translation from Ger-
man into English, results for the phrase-based sys-
tem with and without verb reordering and com-
pound splitting are shown. The hierarchical sys-
tem was trained with split German compounds.
The final submission was produced by combining
those five systems. The improvement obtained by
system combination on the unseen test data 2009
is similar, i.e. from the systems with BLEU scores
of 17.0%, 17.2%, 17.5%, 17.6% and 17.7% to the
final system with 18.5%.
German?English BLEU PBLEU PF
phrase-based 17.8 31.6 39.7
+reorder verbs 18.2 32.6 40.3
+split compounds 18.0 31.9 40.0
+reord+split 18.4 33.1 40.7
hierarchical+split 18.5 33.5 40.1
system combination 19.2 33.8 40.9
English?German BLEU PBLEU PF
phrase-based 13.6 31.6 39.7
+reorder verbs 13.7 32.4 40.2
+split compounds 13.7 32.3 40.1
+reord+split 13.7 32.3 40.1
system combination 14.0 32.7 40.3
Table 1: Translation results [%] for the German-
English language pair, News2008 dev-b.
The other translation direction is more difficult
and improvements from morpho-syntactic trans-
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
formations are smaller. No hierarchical system
was trained for this translation direction. The com-
bination of the four phrase-based systems leads
to further improvements (on the unseen test set
as well: contrastive hypotheses have the BLEU
scores in the range from 12.7% to 13.0%, and the
final BLEU score is 13.2%).
The results for the French-English language
pair are shown in Table 2. For the French-to-
English system, we submitted the result of the
combination of three systems: a phrase-based with
and without local reorderings and a hierarchical
system. For the unseen test set, the BLEU score of
the system combination output is 24.4%, whereas
the contrastive hypotheses have 23.2%, 23.4% and
24.1%. For the other translation direction we did
not use the system combination, the submission is
produced by the phrase-based system with local
adjective reorderings.
French?English BLEU PBLEU PF
phrase-based 20.9 37.1 43.2
+reorder adjectives 21.3 38.2 43.6
hierarchical 20.3 36.7 42.6
system combination 21.7 38.5 43.8
English?French BLEU PBLEU PF
phrase-based 20.2 39.5 45.9
+reorder adjectives 20.7 40.6 46.4
Table 2: Translation results [%] for the French-
English language pair, News2008 dev-b.
Table 3 presents the results for the Spanish-
English language pair. As in the English-to-
French translation, the phrase-based system with
adjective reorderings is used to produce the sub-
mitted hypothesis for both translation directions.
Spanish?English BLEU PBLEU PF
phrase-based 22.1 38.5 44.1
+reorder adjectives 22.5 39.2 44.6
English?Spanish BLEU PBLEU PF
phrase-based 20.6 29.3 35.7
+reorder adjectives 21.1 29.7 35.9
Table 3: Translation results [%] for the Spanish-
English language pair, News2008 dev-b.
68
The result of the additional experiment, i.e. for
the multisource translation int English is presented
in Table 4. The English hypothesis is produced by
the combination of the three best systems for each
language pair, and it can be seen that the transla-
tion performance increases in all measures. This
suggests that each language pair poses different
difficulties for the translation task, and the com-
bination of all three can improve performance.
F+S+G?English BLEU PBLEU PF
system combination 25.1 41.0 46.4
Table 4: Multisource translation results [%]:
the English hypothesis is obtained as result of
a system combination of all language pairs,
News2008 dev-b.
6 Conclusions
The RWTH system submitted to the WMT 2009
shared translation task used a phrase-based sys-
tem and a hierarchical system with appropriate
morpho-syntactic extensions, i.e. POS based word
reorderings and splitting of German compounds
were used. System combination produced gains
in BLEU score over phrasal-system baselines in
the German-to-English, English-to-German and
French-to-English tasks.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
Powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, (33):201?228.
Philipp Koehn and Kevin Knight. 2003. Empiri-
cal methods for compound splitting. In Proceed-
ings 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 347?354, Budapest, Hungary, April.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proceedings of EACL
2006 (11th Conference of the European Chapter
of the Association for Computational Linguistics),
pages 33?40, Trento, Italy, April.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Trans-
lation. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC), pages 1278?1283, Genoa, Italy, May.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th International Con-
ference on Natural Language Processing (FinTAL),
pages 616?624, Turku, Finland, August. Lecture
Notes in Computer Science, Springer Verlag.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
25th German Conference on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artifi-
cial Intelligence (LNAI), pages 18?32, Aachen, Ger-
many, September. Springer Verlag.
69
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93?97,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2010
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,
Saab Mansour, Daniel Stein and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
In this paper we describe the statisti-
cal machine translation system of the
RWTH Aachen University developed for
the translation task of the Fifth Workshop
on Statistical Machine Translation. State-
of-the-art phrase-based and hierarchical
statistical MT systems are augmented
with appropriate morpho-syntactic en-
hancements, as well as alternative phrase
training methods and extended lexicon
models. For some tasks, a system combi-
nation of the best systems was used to gen-
erate a final hypothesis. We participated
in the constrained condition of German-
English and French-English in each trans-
lation direction.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT 2010 shared
translation task. We used it as an opportunity to in-
corporate novel methods which have been investi-
gated at RWTH over the last year and which have
proven to be successful in other evaluations.
For all tasks we used standard alignment and
training tools as well as our in-house phrase-
based and hierarchical statistical MT decoders.
When German was involved, morpho-syntactic
preprocessing was applied. An alternative phrase-
training method and additional models were tested
and investigated with respect to their effect for the
different language pairs. For two of the language
pairs we could improve performance by system
combination.
An overview of the systems and models will fol-
low in Section 2 and 3, which describe the base-
line architecture, followed by descriptions of the
additional system components. Morpho-syntactic
analysis and other preprocessing issues are cov-
ered by Section 4. Finally, translation results for
the different languages and system variants are
presented in Section 5.
2 Translation Systems
For the WMT 2010 Evaluation we used stan-
dard phrase-based and hierarchical translation sys-
tems. Alignments were trained with a variant of
GIZA++. Target language models are 4-gram lan-
guage models trained with the SRI toolkit, using
Kneser-Ney discounting with interpolation.
2.1 Phrase-Based System
Our phrase-based translation system is similar to
the one described in (Zens and Ney, 2008). Phrase
pairs are extracted from a word-aligned bilingual
corpus and their translation probability in both di-
rections is estimated by relative frequencies. Ad-
ditional models include a standard n-gram lan-
guage model, phrase-level IBM1, word-, phrase-
and distortion-penalties and a discriminative re-
ordering model as described in (Zens and Ney,
2006).
2.2 Hierarchical System
Our hierarchical phrase-based system is similar to
the one described in (Chiang, 2007). It allows for
gaps in the phrases by employing a context-free
grammar and a CYK-like parsing during the de-
coding step. It has similar features as the phrase-
based system mentioned above. For some sys-
tems, we only allowed the non-terminals in hierar-
chical phrases to be substituted with initial phrases
as in (Iglesias et al, 2009), which gave better re-
sults on some language pairs. We will refer to this
as ?shallow rules?.
2.3 System Combination
The RWTH approach to MT system combination
of the French?English systems as well as the
German?English systems is a refined version of
the ROVER approach in ASR (Fiscus, 1997) with
93
German?English French?English English?French
BLEU # Phrases BLEU # Phrases BLEU # Phrases
Standard 19.7 128M 25.5 225M 23.7 261M
FA 20.0 12M 25.9 35M 24.0 33M
Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For
German?English and English?French phrase table interpolation was applied.
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. Alignments between the sys-
tems are learned by GIZA++, a one-to-one align-
ment is generated from the learned state occupa-
tion probabilities.
From these alignments, a confusion network
(CN) is then built using one of the hypotheses as
?skeleton? or ?primary? hypothesis. We do not
make a hard decision on which of the hypothe-
ses to use for that, but instead combine all pos-
sible CNs into a single lattice. Majority voting on
the generated lattice is performed using the prior
probabilities for each system as well as other sta-
tistical models such as a special trigram language
model. This language model is also learned on
the input hypotheses. The intention is to favor
longer phrases contained in individual hypotheses.
The translation with the best total score within this
lattice is selected as consensus translation. Scal-
ing factors of these models are optimized similar
to MERT using the Downhill Simplex algorithm.
As the objective function for this optimization, we
selected a linear combination of BLEU and TER
with a weight of 2 on the former; a combination
that has proven to deliver stable results on sev-
eral MT evaluation measures in preceding experi-
ments.
In contrast to previous years, we now include a
separate consensus true casing step to exploit the
true casing capabilities of some of the input sys-
tems: After generating a (lower cased) consensus
translation from the CN, we sum up the counts of
different casing variants of each word in a sen-
tence over the input hypotheses, and use the ma-
jority casing over those. In previous experiments,
this showed to work significantly better than us-
ing a fixed non-consensus true caser, and main-
tains flexibility on the input systems.
3 New Additional Models
3.1 Forced Alignment
For the German?English, French?English and
English?French language tasks we applied a
forced alignment procedure to train the phrase
translation model with the EM algorithm, sim-
ilar to the one described in (DeNero et al,
2006). Here, the phrase translation probabil-
ities are estimated from their relative frequen-
cies in the phrase-aligned training data. The
phrase alignment is produced by a modified
version of the translation decoder. In addi-
tion to providing a statistically well-founded
phrase model, this has the benefit of produc-
ing smaller phrase tables and thus allowing
more rapid experiments. For the language pairs
German?English and English?French the best
results were achieved by log-linear interpolation
of the standard phrase table with the generative
model. For French?English we directly used the
model trained by forced alignment. A detailed
description of the training procedure is given in
(Wuebker et al, 2010). Table 1 shows the system
performances and phrase table sizes with the stan-
dard phrase table and the one trained with forced
alignment after the first EM iteration. We can see
that the generative model reduces the phrase table
size by 85-90% while increasing performance by
0.3% to 0.4% BLEU.
3.2 Extended Lexicon Models
In previous work, RWTH was able to show the
positive impact of extended lexicon models that
cope with lexical context beyond the limited hori-
zon of phrase pairs and n-gram language models.
Mauser et al (2009) report improvements of
up to +1% in BLEU on large-scale systems for
Chinese?English and Arabic?English by incor-
porating discriminative and trigger-based lexicon
models into a state-of-the-art phrase-based de-
coder. They discuss how the two types of lexicon
94
models help to select content words by capturing
long-distance effects.
The triplet model is a straightforward extension
of the IBM model 1 with a second trigger, and like
the former is trained iteratively using the EM al-
gorithm. In search, the triggers are usually on the
source side, i.e., p(e|f, f ?) is modeled. The path-
constrained triplet model restricts the first source
trigger to the aligned target word, whereas the sec-
ond trigger can move along the whole source sen-
tence. See (Hasan et al, 2008) for a detailed de-
scription and variants of the model and its training.
For the WMT 2010 evaluation, triplets mod-
eling p(e|f, f ?) were trained and applied di-
rectly in search for all relevant language pairs.
Path-constrained models were trained on the in-
domain news-commentary data only and on the
news-commentary plus the Europarl data. Al-
though experience from similar setups indicates
that triplet lexicon models can be beneficial for
machine translation between the languages En-
glish, French, and German, on this year?s WMT
translation tasks slight improvements on the devel-
opment sets did not or only partially carry over to
the held-out test sets. Nevertheless, systems with
triplets were used for system combination, as ex-
tended lexicon models often help to predict con-
tent words and to capture long-range dependen-
cies. Thus they can help to find a strong consensus
hypothesis.
3.3 Unsupervised Training
Due to the small size of the English?German re-
sources available for language modeling as well as
for lexicon extraction, we decided to apply the un-
supervised adaptation suggested in (Schwenk and
Senellart, 2009). We use a baseline SMT system to
translate in-domain monolingual source data, fil-
ter the translations according to a decoder score
normalized by sentence length, add this synthetic
bilingual data to the original one and rebuild the
SMT system from scratch.
The motivation behind the method is that the
phrase table will adapt to the genre, and thus
let phrases which are domain related have higher
probabilities. Two phenomena are observed from
phrase tables and the corresponding translations:
? Phrase translation probabilities are changed,
making the system choose better phrase
translation candidates.
Running Words
English German
Bilingual 44.3M 43.4M
Dict. 1.4M 1.2M
AFP 610.7M
AFP unsup. 152.0M 157.3M
Table 2: Overview on data for unsupervised train-
ing.
BLEU
Dev Test
baseline 15.0 14.7
+dict. 15.1 14.6
+unsup.+dict 15.4 14.9
Table 3: Results for unsupervised training method.
? Phrases which appear repeatedly in the do-
main get higher probabilities, so that the de-
coder can better segment the sentence.
To implement this idea, we translate the AFP part
of the English LDC Gigaword v4.0 and obtain the
synthetic data.
To decrease the number of OOV words, we use
dictionaries from the stardict directory as addi-
tional bilingual data to translate the AFP corpus.
We filter sentences with OOV words and sentences
longer than 100 tokens. A summary of the addi-
tional data used is shown in Table 2.
We tried to use the best 10%, 20% and 40% of
the synthetic data, where the 40% option worked
best. A summary of the results is given in Table 3.
Although this is our best result for the
English?German task, it was not submitted, be-
cause the use of the dictionary is not allowed in
the constrained track.
4 Preprocessing
4.1 Large Parallel Data
In addition to the provided parallel Europarl and
news-commentary corpora, also the large French-
English news corpus (about 22.5 Mio. sentence
pairs) and the French-English UN corpus (about
7.2 Mio. sentence pairs) were available. Since
model training and tuning with such large cor-
pora takes a very long time, we extracted about
2 Mio. sentence pairs of both of these corpora. We
filter sentences with the following properties:
95
? Only sentences of minimum length of 4 to-
kens were considered.
? At least 92% of the vocabulary of each sen-
tence occur in the development set.
? The ratio of the vocabulary size of a sen-
tence and the number of its tokens is mini-
mum 80%.
4.2 Morpho-Syntactic Analysis
German, as a flexible and morphologically rich
language, raises a couple of problems in machine
translation. We picked two major problems and
tackled them with morpho-syntactic pre- and post-
processing: compound splitting and long-range
verb reordering.
For the translation from German into English,
German compound words were split using the
frequency-based method described in (Koehn and
Knight, 2003). Thereby, we forbid certain words
and syllables to be split. For the other trans-
lation direction, the English text was first trans-
lated into the modified German language with
split compounds. The generated output was then
postprocessed by re-merging the previously gen-
erated components using the method described in
(Popovic? et al, 2006).
Additionally, for the German?English phrase-
based system, the long-range POS-based reorder-
ing rules described in (Popovic? and Ney, 2006)
were applied on the training and test corpora as a
preprocessing step. Thereby, German verbs which
occur at the end of a clause, like infinitives and
past participles, are moved towards the beginning
of that clause. With this, we improved our baseline
phrase-based system by 0.6% BLEU.
5 Experimental Results
For all translation directions, we used the provided
parallel corpora (Europarl, news) to train the trans-
lation models and the monolingual corpora to train
BLEU
Dev Test
phrase-based baseline 19.9 19.2
phrase-based (+POS+mero+giga) 21.0 20.3
hierarchical baseline 20.2 19.6
hierarchical (+giga) 20.5 20.1
system combination 21.4 20.4
Table 4: Results for the German?English task.
the language models. We improved the French-
English systems by enriching the data with parts of
the large addional data, extracted with the method
described in Section 4.1. Depending on the sys-
tem this gave an improvement of 0.2-0.7% BLEU.
We also made use of the large giga-news as well
as the LDC Gigaword corpora for the French and
English language models. All systems were opti-
mized for BLEU score on the development data,
newstest2008. The newstest2009 data is
used as a blind test set.
In the following, we will give the BLEU scores
for all language tasks of the baseline system and
the best setup for both, the phrase-based and the
hierarchical system. We will use the following
notations to indicate the several methods we used:
(+POS) POS-based verb reordering
(+mero) maximum entropy reordering
(+giga) including giga-news and
LDC Gigaword in LM
(fa) trained by forced alignment
(shallow) allow only shallow rules
We applied system combination of up to 6 sys-
tems with several setups. The submitted systems
are marked in tables 4-7.
6 Conclusion
For the participation in the WMT 2010 shared
translation task, RWTH used state-of-the-art
phrase-based and hierarchical translation systems.
To deal with the rich morphology and word or-
der differences in German, compound splitting
and long range verb reordering were applied in a
preprocessing step. For the French-English lan-
guage pairs, RWTH extracted parts of the large
news corpus and the UN corpus as additional
training data. Further, training the phrase trans-
lation model with forced alignment yielded im-
provements in BLEU. To obtain the final hypothe-
sis for the French?English and German?English
BLEU
Dev Test
phrase-based baseline 14.8 14.5
phrase-based (+mero) 15.0 14.7
hierarchical baseline 14.2 13.9
hierarchical (shallow) 14.5 14.3
Table 5: Results for the English?German task.
96
BLEU
Dev Test
phrase-based baseline 21.8 25.1
phrase-based (fa+giga) 23.0 26.1
hierarchical baseline 21.9 25.0
hierarchical (shallow+giga) 22.7 25.6
system combination 23.1 26.1
Table 6: Results for the French?English task.
BLEU
Dev Test
phrase-based baseline 20.9 23.2
phrase-based (fa+mero+giga) 23.0 24.6
hierarchical baseline 20.6 22.5
hierarchical (shallow,+giga) 22.4 24.3
Table 7: Results for the English?French task.
language pairs, RWTH applied system combina-
tion. Altogether, by application of these meth-
ods RWTH was able to increase performance in
BLEU by 0.8% for German?English, 0.2% for
English?German, 1.0% for French?English and
1.4% for English?French on the test set over the
respective baseline systems.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Sur-
face Heuristics. In Proceedings of the Workshop on
Statistical Machine Translation, pages 31?38.
J.G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-
Ferrer. 2008. Triplet Lexicon Models for Statisti-
cal Machine Translation. In Proceedings of Emperi-
cal Methods of Natural Language Processing, pages
372?381.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extend-
ing Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 210?217.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
H. Schwenk and J. Senellart. 2009. Translation Model
Adaptation for an Arabic/French News Translation
System by Lightly-Supervised Training. In MT
Summit XII.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. To ap-
pear.
R. Zens and H. Ney. 2006. Discriminative Reorder-
ing Models for Statistical Machine Translation. In
Workshop on Statistical Machine Translation, pages
55?63.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statis-
tical Machine Translation. In International Work-
shop on Spoken Language Translation.
97
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 262?270,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Jane: Open Source Hierarchical Translation, Extended with Reordering
and Lexicon Models
David Vilar, Daniel Stein, Matthias Huck and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
{vilar,stein,huck,ney}@cs.rwth-aachen.de
Abstract
We present Jane, RWTH?s hierarchical
phrase-based translation system, which
has been open sourced for the scientific
community. This system has been in de-
velopment at RWTH for the last two years
and has been successfully applied in dif-
ferent machine translation evaluations. It
includes extensions to the hierarchical ap-
proach developed by RWTH as well as
other research institutions. In this paper
we give an overview of its main features.
We also introduce a novel reordering
model for the hierarchical phrase-based
approach which further enhances transla-
tion performance, and analyze the effect
some recent extended lexicon models have
on the performance of the system.
1 Introduction
We present a new open source toolkit for hi-
erarchical phrase-based translation, as described
in (Chiang, 2007). The hierarchical phrase model
is an extension of the standard phrase model,
where the phrases are allowed to have ?gaps?. In
this way, long-distance dependencies and reorder-
ings can be modelled in a consistent way. As in
nearly all current statistical approaches to machine
translation, this model is embedded in a log-linear
model combination.
RWTH has been developing this tool during
the last two years and it was used success-
fully in numerous machine translation evalua-
tions. It is developed in C++ with special at-
tention to clean code, extensibility and efficiency.
The toolkit is available under an open source
non-commercial license and downloadable from
http://www.hltpr.rwth-aachen.de/jane.
In this paper we give an overview of the main
features of the toolkit and introduce two new ex-
tensions to the hierarchical model. The first one
is an additional reordering model inspired by the
reordering widely used in phrase-based transla-
tion systems and the second one comprises two
extended lexicon models which further improve
translation performance.
2 Related Work
Jane implements many features presented in pre-
vious work developed both at RWTH and other
groups. As we go over the features of the system
we will provide the corresponding references.
Jane is not the first system of its kind, al-
though it provides some unique features. There
are other open source hierarchical decoders avail-
able. These include
? SAMT (Zollmann and Venugopal, 2006):
The original version is not maintained any
more and we had problems working on big
corpora. A new version which requires
Hadoop has just been released, however the
documentation is still missing.
? Joshua (Li et al, 2009): A decoder written
in Java by the John Hopkins University. This
project is the most similar to our own, how-
ever both were developed independently and
each one has some unique features. A brief
comparison between these two systems is in-
cluded in Section 5.1.
? Moses (Koehn et al, 2007): The de-facto
standard phrase-based translation decoder
has now been extended to support hierarchi-
cal translation. This is still in an experimental
branch, however.
3 Features
In this section we will only give a brief overview
of the features implemented in Jane. For de-
tailed explanation of previously published algo-
262
rithms and methods, we refer to the given litera-
ture.
3.1 Search Algorithms
The search for the best translation proceeds in two
steps. First, a monolingual parsing of the input
sentence is carried out using the CYK+ algorithm
(Chappelier and Rajman, 1998), a generalization
of the CYK algorithm which relaxes the require-
ment for the grammar to be in Chomsky normal
form. From the CYK+ chart we extract a hyper-
graph representing the parsing space.
In a second step the translations are generated,
computing the language model scores in an inte-
grated fashion. Both the cube pruning and cube
growing algorithms (Huang and Chiang, 2007) are
implemented. For the latter case, the extensions
concerning the language model heuristics similar
to (Vilar and Ney, 2009) have also been included.
3.2 Language Models
Jane supports four formats for n-gram language
models:
? The ARPA format for language models. We
use the SRI toolkit (Stolcke, 2002) to support
this format.
? The binary language model format supported
by the SRI toolkit. This format allows for a
more efficient language model storage, which
reduces loading times. In order to reduce
memory consumption, the language model
can be reloaded for every sentence, filtering
the n-grams that will be needed for scoring
the possible translations. This format is spe-
cially useful for this case.
? Randomized LMs as described in (Talbot and
Osborne, 2007), using the open source im-
plementation made available by the authors
of the paper. This approach uses a space ef-
ficient but approximate representation of the
set of n-grams in the language model. In
particular the probability for unseen n-grams
may be overestimated.
? An in-house, exact representation format
with on-demand loading of n-grams, using
the internal prefix-tree implementation which
is also used for phrase storage (see also Sec-
tion 3.9).
Several language models (also of mixed formats)
can be used during search. Their scores are com-
bined in the log-linear framework.
3.3 Syntactic Features
Soft syntactic features comparable to (Vilar et al,
2008) are implemented in the extraction step of
the toolkit. In search, they are considered as ad-
ditional feature functions of the translation rules.
The decoder is able to handle an arbitrary num-
ber of non-terminal symbols. The extraction has
been extended so that the extraction of SAMT-
rules is included (Zollmann and Venugopal, 2006)
but this approach is not fully supported (there
may be empty parses due to the extended num-
ber of non-terminals). We instead opted to sup-
port the generalization presented in (Venugopal et
al., 2009), where the information about the new
non-terminals is included as an additional feature
in the log-linear model.
In addition, dependency information in the
spirit of (Shen et al, 2008) is included. Jane fea-
tures models for string-to-dependency language
models and computes various scores based on the
well-formedness of the resulting dependency tree.
Jane supports the Stanford parsing format,1 but
can be easily extended to other parsers.
3.4 Additional Reordering Models
In the standard formulation of the hierarchical
phrase-based translation model two additional
rules are added:
S ? ?S?0X?1, S?0X?1?
S ? ?X?0, X?0?
(1)
This allows for a monotonic concatenation of
phrases, very much in the way monotonic phrase-
based translation is carried out.
It is a well-known fact that for phrase-based
translation, the use of additional reordering mod-
els is a key component, essential for achieving
good translation quality. In the hierarchical model,
the reordering is already integrated in the transla-
tion formalism, but there are still cases where the
required reorderings are not captured by the hier-
archical phrases alone.
The flexibility of the grammar formalism allows
us to add additional reordering models without the
need to explicitely modify the code for supporting
them. The most straightforward example would
1http://nlp.stanford.edu/software/lex-parser.shtml
263
be to include the ITG-Reorderings (Wu, 1997), by
adding following rule
S ? ?S?0S?1, S?1S?0? (2)
We can also model other reordering constraints.
As an example, phrase-level IBM reordering con-
straints with a window length of 1 can be included
substituting the rules in Equation (1) with follow-
ing rules
S ? ?M?0,M?0?
S ? ?M?0S?1,M?0S?1?
S ? ?B?0M?1,M?1B?0?
M ? ?X?0, X?0?
M ? ?M?0X?1,M?0X?1?
B ? ?X?0, X?0?
B ? ?B?0X?1, X?1B?0?
(3)
In these rules we have added two additional non-
terminals. The M non-terminal denotes a mono-
tonic block and the B non-terminal a back jump.
Actually both of them represent monotonic trans-
lations and the grammar could be simplified by
using only one of them. Separating them allows
for more flexibility, e.g. when restricting the jump
width, where we only have to restrict the maxi-
mum span width of the non-terminal B. These
rules can be generalized for other reordering con-
straints or window lengths.
Additionally distance-based costs can be com-
puted for these reorderings. To the best of our
knowledge, this is the first time such additional
reorderings have been applied to the hierarchical
phrase-based approach.
3.5 Extended Lexicon Models
We enriched Jane with the ability to score hy-
potheses with discriminative and trigger-based
lexicon models that use global source sentence
context and are capable of predicting context-
specific target words. This approach has recently
been shown to improve the translation results of
conventional phrase-based systems. In this sec-
tion, we briefly review the basic aspects of these
extended lexicon models. They are similar to
(Mauser et al, 2009), and we refer there for a more
detailed exposition on the training procedures and
results in conventional phrase-based decoding.
Note that the training for these models is not
distributed together with Jane.
3.5.1 Discriminative Word Lexicon
The first of the two lexicon models is denoted as
discriminative word lexicon (DWL) and acts as a
statistical classifier that decides whether a word
from the target vocabulary should be included in
a translation hypothesis. For that purpose, it con-
siders all the words from the source sentence, but
does not take any position information into ac-
count, i.e. it operates on sets, not on sequences or
even trees. The probability of a word being part
of the target sentence, given a set of source words,
are decomposed into binary features, one for each
source vocabulary entry. These binary features are
combined in a log-linear fashion with correspond-
ing feature weights. The discriminative word lex-
icon is trained independently for each target word
using the L-BFGS (Byrd et al, 1995) algorithm.
For regularization, Gaussian priors are utilized.
DWL model probabilities are computed as
p(e|f) =
?
e?VE
p(e?|f) ?
?
e?e
p(e+|f)
p(e?|f)
(4)
with VE being the target vocabulary, e the set of
target words in a sentence, and f the set of source
words, respectively. Here, the event e+ is used
when the target word e is included in the target
sentence and e? if not. As the left part of the prod-
uct in Equation (4) is constant given a source sen-
tence, it can be dropped, which enables us to score
partial hypotheses during search.
3.5.2 Triplet Lexicon
The second lexicon model we employ in Jane,
the triplet lexicon model, is in many aspects re-
lated to IBM model 1 (Brown et al, 1993), but
extends it with an additional word in the con-
ditioning part of the lexical probabilities. This
introduces a means for an improved representa-
tion of long-range dependencies in the data. Like
IBM model 1, the triplets are trained iteratively
with the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). Jane implements
the so-called inverse triplet model p(e|f, f ?).
The triplet lexicon model score t(?) of the ap-
plication of a rule X ? ??, ?? where (?, ?) is
a bilingual phrase pair that may contain symbols
from the non-terminal set is computed as
t(?, ?, fJ0 ) = (5)
?
?
e
log
?
?
2
J ? (J + 1)
?
j
?
j?>j
p(e|fj , fj?)
?
?
264
with e ranging over all terminal symbols in the tar-
get part ? of the rule. The second sum selects all
words from the source sentence fJ0 (including the
empty word that is denoted as f0 here). The third
sum incorporates the rest of the source sentence
right of the first triggering word. The order of
the triggers is not relevant because per definition
p(e|f, f ?) = p(e|f ?, f), i.e. the model is symmet-
ric. Non-terminals in ? have to be skipped when
the rule is scored.
In Jane, we also implemented scoring for a vari-
ant of the triplet lexicon model called the path-
constrained (or path-aligned) triplet model. The
characteristic of path-constrained triplets is that
the first trigger f is restricted to the aligned target
word e. The second trigger f ? is allowed to move
along the whole remaining source sentence. For
the training of the model, we use word alignment
information obtained by GIZA++ (Och and Ney,
2003). To be able to apply the model in search,
Jane has to be run with a phrase table that con-
tains word alignment for each phrase, too, with the
exception of phrases which are composed purely
of non-terminals. Jane?s phrase extraction can op-
tionally supply this information from the training
data.
(Hasan et al, 2008) and (Hasan and Ney, 2009)
employ similar techniques and provide some more
discussion on the path-aligned variant of the
model and other possible restrictions.
3.6 Forced Alignments
Jane has also preliminary support for forced align-
ments between a given source and target sentence.
Given a sentence in the source language and its
translation in the target language, we find the best
way the source sentence can be translated into
the given target sentence, using the available in-
ventory of phrases. This is needed for more ad-
vanced training approaches like the ones presented
in (Blunsom et al, 2008) or (Cmejrek et al, 2009).
As reported in these papers, due to the restrictions
in the phrase extraction process, not all sentences
in the training corpus can be aligned in this way.
3.7 Optimization Methods
Two method based on n-best for minimum error
rate training (MERT) of the parameters of the log-
linear model are included in Jane. The first one
is the procedure described in (Och, 2003), which
has become a standard in the machine translation
community. We use an in-house implementation
of the method.
The second one is the MIRA algorithm, first
applied for machine translation in (Chiang et al,
2009). This algorithm is more adequate when the
number of parameters to optimize is large.
If the Numerical Recipes library (Press et al,
2002) is available, an additional general purpose
optimization tool is also compiled. Using this
tool a single-best optimization procedure based on
the downhill simplex method (Nelder and Mead,
1965) is included. This method, however, can be
considered deprecated in favour of the above men-
tioned methods.
3.8 Parallelized operation
If the Sun Grid Engine2 is available, all operations
of Jane can be parallelized. For the extraction pro-
cess, the corpus is split into chunks (the granular-
ity being user-controlled) which are distributed in
the computer cluster. Count collection, marginal
computation and count normalization all happens
in an automatic and parallel manner.
For the translation process a batch job is started
on a number of computers. A server distributes the
sentences to translate to the computers that have
been made available to the translation job.
The optimization process also benefits from
the parallelized optimization. Additionally, for
the minimum error rate training methods, random
restarts may be performed on different computers
in a parallel fashion.
The same client-server infrastructure used for
parallel translation may also be reused for inter-
active systems. Although no code in this direction
is provided, one would only need to implement a
corresponding frontend which communicates with
the translation server (which may be located on an-
other machine).
3.9 Extensibility
One of the goals when implementing the toolkit
was to make it easy to extend it with new features.
For this, an abstract class was created which we
called secondary model. New models need only to
derive from this class and implement the abstract
methods for data reading and costs computation.
This allows for an encapsulation of the computa-
tions, which can be activated and deactivated on
demand. The models described in Sections 3.3
2http://www.sun.com/software/sge/
265
through 3.5 are implemented in this way. We thus
try to achieve loose coupling in the implementa-
tion.
In addition a flexible prefix tree implementation
with on-demand loading capabilities is included as
part of the code. This class has been used for im-
plementing on-demand loading of phrases in the
spirit of (Zens and Ney, 2007) and the on-demand
n-gram format described in Section 3.2, in addi-
tion to some intermediate steps in the phrase ex-
traction process. The code may also be reused in
other, independent projects.
3.10 Code
The main core of Jane has been implemented in
C++. Our guideline was to write code that was
correct, maintainable and efficient. We tried to
achieve correctness by means of unit tests inte-
grated in the source as well as regression tests. We
also defined a set of coding guidelines, which we
try to enforce in order to have readable and main-
tainable code. Examples include using descriptive
variable names, appending an underscore to pri-
vate members of classes or having each class name
start with an uppercase letter while variable names
start with lowercase letters.
The code is documented at great length using
the doxygen system,3 and the filling up of the
missing parts is an ongoing effort. Every tool
comes with an extensive help functionality, and
the main tools also have their own man pages.
As for efficiency we always try to speed up the
code and reduce memory consumption by imple-
menting better algorithms. We try to avoid ?dark
magic programming methods? and hard to follow
optimizations are only applied in critical parts of
the code. We try to document every such occur-
rence.
4 Experimental Results
In this section we will present some experimental
results obtained using Jane. We will pay special
attention to the performance of the new reordering
and lexicon models presented in this paper. We
will present results on three different large-scale
tasks and language pairs.
Additionally RWTH participated in this year?s
WMT evaluation, where Jane was one of the sub-
mitted systems. We refer to the system description
for supplementary experimental results.
3http://www.doxygen.org
dev test
System BLEU TER BLEU TER
Jane baseline 24.2 59.5 25.4 57.4
+ reordering 25.2 58.2 26.5 56.1
Table 1: Results for Europarl German-English
data. BLEU and TER results are in percentage.
4.1 Europarl Data
The first task is the Europarl as defined in the
Quaero project. The main part of the corpus in
this task consists of the Europarl corpus as used in
the WMT evaluation (Callison-Burch et al, 2009),
with some additional data collected in the scope of
the project.
We tried the reordering approach presented in
Section 3.4 on the German-English language pair.
The results are shown in Table 1. As can be seen
from these results, the additional reorderings ob-
tain nearly 1% improvement both in BLEU and
TER scores. Regrettably for this corpus the ex-
tended lexicon models did not bring any improve-
ments.
Table 2 shows the results for the French-English
language pair of the Europarl task. On this task
the extended lexicon models yield an improve-
ment over the baseline system of 0.9% in BLEU
and 0.9% in TER on the test set.
4.2 NIST Arabic-English
We also show results on the Arabic-English
NIST?08 task, using the NIST?06 set as develop-
ment set. It has been reported in other work that
the hierarchical system is not competitive with a
phrase-based system for this language pair (Birch
et al, 2009). We report the figures of our state-
of-the-art phrase-based system as comparison (de-
noted as PBT).
As can be seen from Table 3, the baseline
Jane system is in fact 0.6% worse in BLEU and
1.0% worse in TER than the baseline PBT sys-
tem. When we include the extended lexicon mod-
els we see that the difference in performance is re-
duced. For Jane the extended lexicon models give
an improvement of up to 1.9% in BLEU and 1.7%
in TER, respectively, bringing the system on par
with the PBT system extended with the same lex-
icon models, and obtaining an even slightly better
BLEU score.
266
dev test
BLEU TER BLEU TER
Baseline 30.0 52.6 31.1 50.0
DWL 30.4 52.2 31.4 49.6
Triplets 30.4 52.0 31.7 49.4
path-constrained Triplets 30.3 52.1 31.6 49.3
DWL + Triplets 30.7 52.0 32.0 49.1
DWL + path-constrained Triplets 30.8 51.7 31.6 49.3
Table 2: Results for the French-English task. BLEU and TER results are in percentage.
dev (MT?06) test (MT?08)
Jane PBT Jane PBT
BLEU TER BLEU TER BLEU TER BLEU TER
Baseline 43.2 50.8 44.1 49.4 44.1 50.1 44.7 49.1
DWL 45.3 48.7 45.1 48.4 45.6 48.4 45.6 48.4
Triplets 44.4 49.1 44.6 49.2 45.3 48.8 44.9 49.0
path-constrained Triplets 44.3 49.4 44.7 49.1 44.9 49.3 45.3 48.7
DWL + Triplets 45.0 48.9 45.1 48.5 45.3 48.6 45.5 48.5
DWL + path-constrained Triplets 45.2 48.8 45.1 48.6 46.0 48.5 45.8 48.3
Table 3: Results for the Arabic-English task. BLEU and TER results are in percentage.
5 Discussion
We feel that the hierarchical phrase-based transla-
tion approach still shares some shortcomings con-
cerning lexical selection with conventional phrase-
based translation. Bilingual lexical context be-
yond the phrase boundaries is barely taken into
account by the base model. In particular, if only
one generic non-terminal is used, the selection of
a sub-phrase that fills the gap of a hierarchical
phrase is not affected by the words composing the
phrase it is embedded in ? except for the language
model score. This shortcoming is one of the issues
syntactically motivated models try to address.
The extended lexicon models analyzed in this
work also try to address this issue. One can con-
sider that they complement the efforts that are be-
ing made on a deep structural level within the hi-
erarchical approach. Though they are trained on
surface forms only, without any syntactic informa-
tion, they still operate at a scope that exceeds the
capability of common feature sets of standard hi-
erarchical phrase-based SMT systems.
As the experiments in Section 4 show, the ef-
fect of these extended lexicon models is more im-
portant for the hierarchical phrase-based approach
than for the phrase-based approach. In our opinion
this is probably mainly due to the higher flexibil-
ity of the hierarchical system, both because of its
intrinsic nature and because of the higher number
of phrases extracted by the system. The scoring
of the phrases is still carried out by simple relative
frequencies, which seem to be insufficient. The
additional lexicon models seem to help in this re-
spect.
5.1 Short Comparison with Joshua
As mentioned in Section 2, Joshua is the most
similar decoder to our own. It was developed in
parallel at the Johns Hopkins University and it is
267
System words/sec
Joshua 11.6
Jane cube prune 15.9
Jane cube grow 60.3
Table 4: Speed comparison Jane vs. Joshua. We
measure the translated words per second.
currently used by a number of groups around the
world.
Jane was started separately and independently.
In their basic working mode, both systems imple-
ment parsing using a synchronous grammar and
include language model information. Each of the
projects then progressed independently, most of
the features described in Section 3 being only
available in Jane.
Efficiency is one of the points where we think
Jane outperforms Joshua. One of the reasons can
well be the fact that it is written in C++ while
Joshua is written in Java. In order to compare run-
ning times we converted a grammar extracted by
Jane to Joshua?s format and adapted the parame-
ters accordingly. To the best of our knowledge we
configured both decoders to perform the same task
(cube pruning, 300-best generation, same pruning
parameters). Except for some minor differences4
the results were equal.
We tried this setup on the IWSLT?08 Arabic to
English translation task. The speed results (mea-
sured in translated words per second) can be seen
in Table 4. Jane operating with cube prune is
nearly 50% faster than Joshua, at the same level
of translation performance. If we switch to cube
grow, the speed difference is even bigger, with
a speedup of nearly 4 times. However this usu-
ally comes with a penalty in BLEU score (nor-
mally under 0.5% BLEU in our experience). This
increased speed can be specially interesting for
applications like interactive machine translation
or online translation services, where the response
time is critical and sometimes even more impor-
tant than a small (and often hardly noticeable) loss
in translation quality.
Another important point concerning efficiency
is the startup time. Thanks to the binary format
described in Section 3.9, there is virtually no delay
4E.g. the OOVs seem to be handled in a slightly different
way, as the placement was sometimes different.
in the loading of the phrase table in Jane.5 In fact
Joshua?s long phrase table loading times were the
main reason the performance measures were done
on a small corpus like IWSLT instead of one of the
large tasks described in Section 4.
We want to make clear that we did not go into
great depth in the workings of Joshua, just stayed
at the basic level described in the manual. This
tool is used also for large-scale evaluations and
hence there certainly are settings for dealing with
these big tasks. Therefore this comparison has to
be taken with a grain of salt.
We also want to stress that we explicitly chose
to leave translation results out of this comparison.
Several different components have great impact
on translation quality, including phrase extraction,
minimum error training and additional parameter
settings of the decoder. As we pointed out we
do not have the expertise in Joshua to perform all
these tasks in an optimal way, and for that reason
we did not include such a comparison. However,
both JHU and RWTH participated in this year?s
WMT evaluation, where the systems, applied by
their respective authors, can be directly compared.
And in no way do we see Joshua and Jane as
?competing? systems. Having different systems
is always enriching, and particularly as system
combination shows great improvements in trans-
lation quality, having several alternative systems
can only be considered a positive situation.
6 Licensing
Jane is distributed under a custom open source
license. This includes free usage for non-
commercial purposes as long as any changes made
to the original software are published under the
terms of the same license. The exact formulation
is available at the download page for Jane.
7 Conclusion
With Jane, we release a state-of-the-art hi-
erarchical toolkit to the scientific community
and hope to provide a good starting point for
fellow researchers, allowing them to have a
solid system even if the research field is new
to them. It is available for download from
http://www.hltpr.rwth-aachen.de/jane. The
system in its current state is stable and efficient
enough to handle even large-scale tasks such as
5There is, however, still some delay when loading the lan-
guage model for some of the supported formats.
268
the WMT and NIST evaluations, while producing
highly competitive results.
Moreover, we presented additional reordering
and lexicon models that further enhance the per-
formance of the system.
And in case you are wondering, Jane is Just an
Acronym, Nothing Else. The name comes from
the character in the Ender?s Game series (Card,
1986).
Acknowledgments
Special thanks to the people who have contributed
code to Jane: Markus Freitag, Stephan Peitz, Car-
men Heger, Arne Mauser and Niklas Hoppe.
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR001-06-C-0023. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the DARPA.
References
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A Quantitative Analysis of Reordering Phe-
nomena. In Proc. of the Workshop on Statistical Ma-
chine Translation, pages 197?205, Athens, Greece,
March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A Discriminative Latent Variable Model for Statis-
tical Machine Translation. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 200?208, Columbus, Ohio,
June.
Peter F. Brown, Stephan A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A Limited Memory Algorithm
for Bound Constrained Optimization. SIAM Journal
on Scientific Computing, 16(5):1190?1208.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation, pages 1?28, Athens, Greece, March.
Orson Scott Card. 1986. Speaker for the Dead. Tor
Books.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133?137,
Paris, France, April.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new Features for Statistical Machine Trans-
lation. In Proc. of the Human Language Technology
Conference / North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 218?226, Boulder, Colorado, June.
David Chiang. 2007. Hierarchical Phrase-based
Translation. Computational Linguistics, 33(2):201?
228, June.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient
Bilingual Chart Parsing. In Proc. of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 136?143, Tokyo, Japan.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal
Statistical Society Series B, 39(1):1?22.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
Extended Lexicon Models in Search and Rescoring
for SMT. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume short papers, pages 17?20, Boulder, CO, USA,
June.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet Lexicon Mod-
els for Statistical Machine Translation. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP), pages 372?381.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
144?151, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 177?180,
Prague, Czech Republic, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An Open Source Toolkit for Parsing-Based Machine
Translation. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 135?139, Athens,
Greece, March.
269
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conference on Empirical Methods
for Natural Language Processing (EMNLP), pages
210?218, Singapore, August.
John A. Nelder and Roger Mead. 1965. The Downhill
Simplex Method. Computer Journal, 7:308.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio, June.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 3, pages 901?904, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom Filter Language Models: Tera-scale LMs on
the Cheap. In Proc. of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476, Prague, Czech
Republic, June.
Ashish Venugopal, Andreas Zollmann, N.A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Human
Language Technology Conference / North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 236?244, Boulder,
Colorado, June.
David Vilar and Hermann Ney. 2009. On LM Heuris-
tics for the Cube Growing Algorithm. In Proc. of
the Annual Conference of the European Association
for Machine Translation (EAMT), pages 242?249,
Barcelona, Spain, May.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing Soft Syntax Features and Heuristics for
Hierarchical Phrase Based Machine Translation.
In Proc. of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 190?197,
Waikiki, Hawaii, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2007. Efficient
Phrase-Table Representation for Machine Transla-
tion with Applications to Online MT and Speech
Translation. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 492?499, Rochester, New York, April.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart Pars-
ing. In Proc. of the Human Language Technology
Conference / North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 138?141, New York, June.
270
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405?412,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2011
Matthias Huck, Joern Wuebker, Christoph Schmidt, Markus Freitag, Stephan Peitz,
Daniel Stein, Arnaud Dagnelies, Saab Mansour, Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
This paper describes the statistical machine
translation (SMT) systems developed by
RWTH Aachen University for the translation
task of the EMNLP 2011 Sixth Workshop on
Statistical Machine Translation. Both phrase-
based and hierarchical SMT systems were
trained for the constrained German-English
and French-English tasks in all directions. Ex-
periments were conducted to compare differ-
ent training data sets, training methods and op-
timization criteria, as well as additional mod-
els on dependency structure and phrase re-
ordering. Further, we applied a system com-
bination technique to create a consensus hy-
pothesis from several different systems.
1 Overview
We sketch the baseline architecture of RWTH?s se-
tups for the WMT 2011 shared translation task by
providing an overview of our translation systems in
Section 2. In addition to the baseline features, we
adopted several novel methods, which will be pre-
sented in Section 3. Details on the respective se-
tups and translation results for the French-English
and German-English language pairs (in both trans-
lation directions) are given in Sections 4 and 5. We
finally conclude the paper in Section 6.
2 Translation Systems
For the WMT 2011 evaluation we utilized RWTH?s
state-of-the-art phrase-based and hierarchical trans-
lation systems as well as our in-house system com-
bination framework. GIZA++ (Och and Ney, 2003)
was employed to train word alignments, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
2.1 Phrase-Based System
We applied a phrase-based translation (PBT) system
similar to the one described in (Zens and Ney, 2008).
Phrase pairs are extracted from a word-aligned bilin-
gual corpus and their translation probability in both
directions is estimated by relative frequencies. The
standard feature set moreover includes an n-gram
language model, phrase-level single-word lexicons
and word-, phrase- and distortion-penalties. To lexi-
calize reordering, a discriminative reordering model
(Zens and Ney, 2006a) is used. Parameters are opti-
mized with the Downhill-Simplex algorithm (Nelder
and Mead, 1965) on the word graph.
2.2 Hierarchical System
For the hierarchical setups described in this paper,
the open source Jane toolkit (Vilar et al, 2010) was
employed. Jane has been developed at RWTH and
implements the hierarchical approach as introduced
by Chiang (2007) with some state-of-the-art exten-
sions. In hierarchical phrase-based translation, a
weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The standard models integrated into
our Jane systems are: phrase translation probabil-
ities and lexical translation probabilities on phrase
level, each for both translation directions, length
405
penalties on word and phrase level, three binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, source-
to-target and target-to-source phrase length ratios,
four binary count features and an n-gram language
model. The model weights are optimized with stan-
dard MERT (Och, 2003) on 100-best lists.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
3 Translation Modeling
We incorporated several novel methods into our sys-
tems for the WMT 2011 evaluation. This section
provides a short survey of three of the methods
which we suppose to be of particular interest.
3.1 Language Model Data Selection
For the English and German language models,
we applied the data selection method proposed in
(Moore and Lewis, 2010). Each sentence is scored
by the difference in cross-entropy between a lan-
guage model trained from in-domain data and a lan-
guage model trained from a similar-sized sample of
the out-of-domain data. As in-domain data we used
the news-commentary corpus. The out-of-domain
data from which the data was selected are the news
crawl corpus for both languages and for English the
109 corpus and the LDC Gigaword data. We used a
3-gram trained with the SRI toolkit to compute the
cross-entropy. For the news crawl corpus, only 1/8
of the sentences were discarded. Of the 109 corpus
we retained 1/2 and of the LDC Gigaword data we
retained 1/4 of the sentences to train the language
models.
3.2 Phrase Model Training
For the German?English and French?English
translation tasks we applied a forced alignment pro-
cedure to train the phrase translation model with the
EM algorithm, similar to the one described in (DeN-
ero et al, 2006). Here, the phrase translation prob-
abilities are estimated from their relative frequen-
cies in the phrase-aligned training data. The phrase
alignment is produced by a modified version of the
translation decoder. In addition to providing a statis-
tically well-founded phrase model, this has the ben-
efit of producing smaller phrase tables and thus al-
lowing more rapid experiments. A detailed descrip-
tion of the training procedure is given in (Wuebker
et al, 2010).
3.3 Soft String-to-Dependency
Given a dependency tree of the target language,
we are able to introduce language models that span
over longer distances than the usual n-grams, as in
(Shen et al, 2008). To obtain dependency structures,
we apply the Stanford parser (Klein and Manning,
2003) on the target side of the training material.
RWTH?s open source hierarchical translation toolkit
Jane has been extended to include dependency infor-
mation in the phrase table and to build dependency
trees on the output hypotheses at decoding time from
this information.
Shen et al (2008) use only phrases that meet cer-
tain restrictions. The first possibility is what the au-
thors call a fixed dependency structure. With the
exception of one word within this phrase, called
the head, no outside word may have a dependency
within this phrase. Also, all inner words may only
depend on each other or on the head. For a second
structure, called a floating dependency structure, the
head dependency word may also exist outside the
phrase. If the dependency structure of a phrase con-
forms to these restrictions, it is denoted as valid.
In our phrase table, we mark those phrases that
possess a valid dependency structure with a binary
feature, but all phrases are retained as translation op-
tions. In addition to storing the dependency informa-
tion, we also memorize for all hierarchical phrases
if the content of gaps has been dependent on the left
or on the right side. We utilize the dependency in-
formation during the search process by adding three
406
French English
Sentences 3 710 985
Running Words 98 352 916 87 689 253
Vocabulary 179 548 216 765
Table 1: Corpus statistics of the preprocessed high-
quality training data (Europarl, news-commentary, and
selected parts of the 109 and UN corpora) for the
RWTH systems for the WMT 2011 French?English and
English?French translation tasks. Numerical quantities
are replaced by a single category symbol.
features to the log-linear model: merging errors to
the left, merging errors to the right, and the ratio of
valid vs. non-valid dependency structures. The de-
coder computes the corresponding costs when it tries
to construct a dependency tree of a (partial) hypothe-
sis on-the-fly by merging the dependency structures
of the used phrase pairs.
In an n-best reranking step, we compute depen-
dency language model scores on the dependencies
which were assembled on the hypotheses by the
search procedure. We apply one language model
for left-side dependencies and one for right-side de-
pendencies. For head structures, we also compute
their scores by exploiting a simple unigram language
model. We furthermore include a language count
feature that is incremented each time we compute
a dependency language model score. As trees with
few dependencies have less individual costs to be
computed, they tend to obtain lower overall costs
than trees with more complex structures in other
sentences. The intention behind this feature is thus
comparable to the word penalty in combination with
a normal n-gram language model.
4 French-English Setups
We set up both hierarchical and standard phrase-
based systems for the constrained condition of the
WMT 2011 French?English and English?French
translation tasks. The English?French RWTH pri-
mary submission was produced with a single hierar-
chical system, while a system combination of three
systems was used to generate a final hypothesis for
the French?English primary submission.
Besides the Europarl and news-commentary cor-
pora, the provided parallel data also comprehends
French English
Sentences 29 996 228
Running Words 916 347 538 778 544 843
Vocabulary 1 568 089 1 585 093
Table 2: Corpus statistics of the preprocessed full training
data for the RWTH primary system for the WMT 2011
English?French translation task. Numerical quantities
are replaced by a single category symbol.
the large French-English 109 corpus and the French-
English UN corpus. Since model training with
such a huge amount of data requires a consider-
able computational effort, RWTH decided to select
a high-quality part of altogether about 2 Mio. sen-
tence pairs from the latter two corpora. The selec-
tion of parallel sentences was carried out according
to three criteria: (1) Only sentences of minimum
length of 4 tokens are considered, (2) at least 92%
of the vocabulary of each sentence occurs in new-
stest2008, and (3) the ratio of the vocabulary size
of a sentence and the number of its tokens is mini-
mum 80%. Word alignments in both directions were
trained with GIZA++ and symmetrized according to
the refined method that was proposed in (Och and
Ney, 2003). The phrase tables of the translation
systems are extracted from the Europarl and news-
commentary parallel training data as well as the se-
lected high-quality parts the 109 and UN corpora
only. The only exception is the hierarchical system
used for the English?French RWTH primary sub-
mission which comprehends a second phrase table
with lexical (i.e. non-hierarchical) phrases extracted
from the full parallel data (approximately 30 Mio.
sentence pairs).
Detailed statistics of the high-quality parallel
training data (Europarl, news-commentary, and the
selected parts of the 109 and UN corpora) are given
in Table 1, the corpus statistics of the full parallel
data from which the second phrase table with lexi-
cal phrases for the English?French RWTH primary
system was created are presented in Table 2.
The translation systems use large 4-gram lan-
guage models with modified Kneser-Ney smooth-
ing. The French language model was trained on
most of the provided French data including the
monolingual LDC Gigaword corpora, the English
407
newstest2009 newstest2010
French?English BLEU TER BLEU TER
System combination of ? systems (primary) 26.7 56.0 27.4 54.9
PBT with triplet lexicon, no forced alignment (contrastive) ? 26.2 56.7 27.2 55.3
Jane as below + improved LM (contrastive) 26.3 57.4 26.7 56.2
Jane with parse match + syntactic labels + dependency ? 26.2 57.5 26.5 56.4
PBT with forced alignment phrase training ? 26.0 57.1 26.3 56.0
Table 3: RWTH systems for the WMT 2011 French?English translation task (truecase). BLEU and TER results are
in percentage.
newstest2009 newstest2010
English?French BLEU TER BLEU TER
Jane shallow + in-domain TM + lexical phrases from full data 25.3 60.1 27.1 57.2
Jane shallow + in-domain TM + triplets + DWL + parse match 24.8 60.5 26.6 57.5
PBT with triplets, DWL, sentence-level word lexicon, discrim. reord. 24.8 60.1 26.5 57.3
Table 4: RWTH systems for the WMT 2011 English?French translation task (truecase). BLEU and TER results are
in percentage.
language model was trained on automatically se-
lected English data (cf. Section 3.1) from the pro-
vided resources including the 109 corpus and LDC
Gigaword.
The scaling factors of the log-linear model com-
bination are optimized towards BLEU on new-
stest2009, newstest2010 is used as an unseen test set.
4.1 Experimental Results French?English
The results for the French?English task are given in
Table 3. RWTH?s three submissions ? one primary
and two contrastive ? are labeled accordingly in the
table. The first contrastive submission is a phrase-
based system with a standard feature set plus an ad-
ditional triplet lexicon model (Mauser et al, 2009).
The triplet lexicon model was trained on in-domain
news commentary data only. The second contrastive
submission is a hierarchical Jane system with three
syntax-based extensions: A parse match model (Vi-
lar et al, 2008), soft syntactic labels (Stein et al,
2010), and the soft string-to-dependency extension
as described in Section 3.3. The primary submis-
sion combines the phrase-based contrastive system,
a hierarchical system that is very similar to the Jane
contrastive submission but with a slightly worse lan-
guage model, and an additional PBT system that has
been trained with forced alignment (Wuebker et al,
2010) on WMT 2010 data only.
4.2 Experimental Results English?French
The results for the English?French task are given
in Table 4. We likewise submitted two contrastive
systems for this translation direction. The first con-
trastive submission is a phrase-based system, en-
hanced with a triplet lexicon model and a discrim-
inative word lexicon model (Mauser et al, 2009) ?
both trained on in-domain news commentary data
only ? as well as a sentence-level single-word lex-
icon model and a discriminative reordering model
(Zens and Ney, 2006a). The second contrastive sub-
mission is a hierarchical Jane system with shallow
rules (Iglesias et al, 2009), a triplet lexicon model, a
discriminative word lexicon, the parse match model,
and a second phrase table extracted from in-domain
data only. Our primary submission is very similar
to the latter Jane setup. It does not comprise the ex-
tended lexicon models and the parse match exten-
sion, but instead includes lexical phrases from the
full 30 Mio. sentence corpus as described above.
5 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. The corpus statis-
408
German English
Sentences 1 857 745
Running Words 48 449 977 50 559 217
Vocabulary 387 593 123 470
Table 5: Corpus statistics of the preprocessed train-
ing data for the WMT 2011 German?English and
English?German translation tasks. Numerical quantities
are replaced by a single category symbol.
tics can be found in Table 5. Word alignments were
generated with GIZA++ and symmetrized as for the
French-English setups.
The language models are 4-grams trained on the
bilingual data as well as the provided News crawl
corpus. For the English language model the 109
French-English and LDC Gigaword corpora were
used additionally. For the 109 French-English and
LDC Gigaword corpora RWTH applied the data se-
lection technique described in Section 3.1. We ex-
amined two different language models, one with
LDC data and one without.
Systems were optimized on the newstest2009 data
set, newstest2008 was used as test set. The scores
for newstest2010 are included for completeness.
5.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the source side
was preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we performed the long-range
part-of-speech based reordering rules proposed by
(Popovic? et al, 2006). For additional experiments
we used the TreeTagger (Schmid, 1995) to produce
a lemmatized version of the German source.
5.2 Optimization Criterion
We studied the impact of different optimization cri-
teria on tranlsation performance. The usual prac-
tice is to optimize the scaling factors to maximize
BLEU. We also experimented with two different
combinations of BLEU and Translation Edit Rate
(TER): TER?BLEU and TER?4BLEU. The first
denotes the equally weighted combination, while for
the latter BLEU is weighted 4 times as strong as
TER.
5.3 Experimental Results German?English
For the German?English task we conducted ex-
periments comparing the standard phrase extraction
with the phrase training technique described in Sec-
tion 3.2. For the latter we applied log-linear phrase-
table interpolation as proposed in (Wuebker et al,
2010). Further experiments included the use of addi-
tional language model training data, reranking of n-
best lists generated by the phrase-based system, and
different optimization criteria. We also carried out
a system combination of several systems, including
phrase-based systems on lemmatized German and
on source data without compound splitting and two
hierarchical systems optimized for different criteria.
The results are given in Table 6.
A considerable increase in translation quality can
be achieved by application of German compound
splitting. The system that operates on German
surface forms without compound splitting (SUR)
clearly underperforms the baseline system with mor-
phological preprocessing. The system on lemma-
tized German (LEM) is at about the same level as
the system on surface forms.
In comparison to the standard heuristic phrase ex-
traction technique, performing phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006b),
sentence length model, a 6-gram LM and single-
word lexicon models in both normal and inverse di-
rection. These models are combined in a log-linear
fashion and the scaling factors are tuned in the same
manner as the baseline system (using TER?4BLEU
on newstest2009).
The table includes three identical Jane systems
which are optimized for different criteria. The one
optimized for TER?4BLEU offers the best balance
between BLEU and TER, but was not finished in
time for submission. As primary submission we
chose the reranked PBT system, as secondary the
system combination.
409
newstest2008 newstest2009 newstest2010
German?English opt criterion BLEU TER BLEU TER BLEU TER
Syscombi of ? (secondary) TER?BLEU 21.1 62.1 20.8 61.2 23.7 59.2
Jane +GW ? BLEU 21.5 63.9 21.0 63.3 22.9 61.7
Jane +GW TER?4BLEU 21.4 62.6 21.1 62.0 23.5 60.3
PBT (FA) rerank +GW (primary) ? TER?4BLEU 21.4 62.8 21.1 61.9 23.4 60.1
PBT (FA) +GW ? TER?4BLEU 21.1 63.0 21.1 62.2 23.3 60.3
Jane +GW ? TER?BLEU 20.9 61.1 20.4 60.5 23.4 58.3
PBT (FA) TER?4BLEU 21.1 63.2 20.6 62.4 23.2 60.4
PBT TER?4BLEU 20.6 62.7 20.3 61.9 23.3 59.7
PBT (SUR) ? TER?4BLEU 19.5 66.5 18.9 65.8 21.0 64.9
PBT (LEM) ? TER?4BLEU 19.2 66.1 18.9 65.4 21.0 63.5
Table 6: RWTH systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results
are in percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model.
SUR and LEM denote the systems without compound splitting and on the lemmatized source, respectively. The three
hierarchical Jane systems are identical, but used different parameter optimization criterea.
newstest2008 newstest2009 newstest2010
English?German opt criterion BLEU TER BLEU TER BLEU TER
PBT + discrim. reord. (primary) TER?4BLEU 15.3 70.2 15.1 69.8 16.2 65.6
PBT + discrim. reord. BLEU 15.2 70.6 15.2 70.1 16.2 66.0
PBT TER?4BLEU 15.2 70.7 15.2 70.2 16.2 66.1
Jane BLEU 15.1 72.1 15.4 71.2 16.4 67.4
Jane TER?4BLEU 15.1 68.4 14.6 69.5 14.6 65.9
Table 7: RWTH systems for the WMT 2011 English?German translation task (truecase). BLEU and TER results are
in percentage.
5.4 Experimental Results English?German
We likewise studied the effect of using BLEU only
versus using TER?4BLEU as optimization crite-
rion in the English?German translation direction.
Moreover, we tested the impact of the discriminative
reordering model (Zens and Ney, 2006a). The re-
sults can be found in Table 7. For the phrase-based
system, optimizing towards TER?4BLEU leads to
slightly better results both in BLEU and TER than
optimizing towards BLEU. Using the discriminative
reordering model yields some improvements both on
newstest2008 and newstest2010. In the case of the
hierarchical system, the effect of the optimization
criterion is more pronounced than for the phrase-
based system. However, in this case it clearly leads
to a tradeoff between BLEU and TER, as the choice
of TER?4BLEU harms the translation results of
test2010 with respect to BLEU.
6 Conclusion
For the participation in the WMT 2011 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. We used
all bilingual and monolingual data provided for the
constrained track. To limit the size of the lan-
guage model, a data selection technique was applied.
Several techniques yielded improvements over the
baseline, including three syntactic models, extended
lexicon models, a discriminative reordering model,
forced alignment training, reranking methods and
different optimization criteria.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
410
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the Workshop on Statis-
tical Machine Translation, pages 31?38.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
D. Klein and C.D. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 423?430.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending Sta-
tistical Machine Translation with Discriminative and
Trigger-Based Lexicon Models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In Proceedings
of ACL-08: HLT. Association for Computational Lin-
guistics, pages 577?585, June.
D. Stein, S. Peitz, D. Vilar, and H. Ney. 2010. A Cocktail
of Deep Syntactic Features for Hierarchical Machine
Translation. In Conference of the Association for Ma-
chine Translation in the Americas 2010, page 9, Den-
ver, USA, October.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
D. Vilar, D. Stein, and H. Ney. 2008. Analysing Soft
Syntax Features and Heuristics for Hierarchical Phrase
Based Machine Translation. In Proc. of the Int. Work-
shop on Spoken Language Translation (IWSLT), pages
190?197, Waikiki, Hawaii, October.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006a. Discriminative Reordering
Models for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens and H. Ney. 2006b. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
411
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
412
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 91?96,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lightly-Supervised Training for Hierarchical Phrase-Based Machine
Translation
Matthias Huck1 and David Vilar1,2 and Daniel Stein1 and Hermann Ney1
1 Human Language Technology and Pattern 2 DFKI GmbH
Recognition Group, RWTH Aachen University Berlin, Germany
<surname>@cs.rwth-aachen.de david.vilar@dfki.de
Abstract
In this paper we apply lightly-supervised
training to a hierarchical phrase-based statis-
tical machine translation system. We employ
bitexts that have been built by automatically
translating large amounts of monolingual data
as additional parallel training corpora. We ex-
plore different ways of using this additional
data to improve our system.
Our results show that integrating a second
translation model with only non-hierarchical
phrases extracted from the automatically gen-
erated bitexts is a reasonable approach. The
translation performance matches the result we
achieve with a joint extraction on all train-
ing bitexts while the system is kept smaller
due to a considerably lower overall number of
phrases.
1 Introduction
We investigate the impact of an employment of large
amounts of unsupervised parallel data as training
data for a statistical machine translation (SMT) sys-
tem. The unsupervised parallel data is created by au-
tomatically translating monolingual source language
corpora. This approach is called lightly-supervised
training in the literature and has been introduced by
Schwenk (2008). In contrast to Schwenk, we do not
apply lightly-supervised training to a conventional
phrase-based system (Och et al, 1999; Koehn et al,
2003) but to a hierarchical phrase-based translation
(HPBT) system.
In hierarchical phrase-based translation (Chiang,
2005) a weighted synchronous context-free gram-
mar is induced from parallel text, the search is
based on CYK+ parsing (Chappelier and Rajman,
1998) and typically carried out using the cube prun-
ing algorithm (Huang and Chiang, 2007). In addi-
tion to the contiguous lexical phrases as in standard
phrase-based translation, the hierarchical phrase-
based paradigm also allows for phrases with gaps
which are called hierarchical phrases. A generic
non-terminal symbol serves as a placeholder that
marks the gaps.
In this paper we study several different ways
of incorporating unsupervised training data into
a hierarchical system. The basic techniques we
employ are the use of multiple translation mod-
els and a distinction of the hierarchical and the
non-hierarchical (i.e. lexical) part of the transla-
tion model. We report experimental results on
the large-scale NIST Arabic-English translation task
and show that lightly-supervised training yields sig-
nificant gains over the baseline.
2 Related Work
Large-scale lightly-supervised training for SMT as
we define it in this paper has been first carried out
by Schwenk (2008). Schwenk translates a large
amount of monolingual French data with an initial
Moses (Koehn et al, 2007) baseline system into En-
glish. In Schwenk?s original work, an additional
bilingual dictionary is added to the baseline. With
lightly-supervised training, Schwenk achieves im-
provements of around one BLEU point over the
baseline. In a later work (Schwenk and Senellart,
2009) he applies the same method for translation
model adaptation on an Arabic-French task with
91
gains of up to 3.5 points BLEU. 1
Hierarchical phrase-based translation has been pi-
oneered by David Chiang (Chiang, 2005; Chiang,
2007) with his Hiero system. The hierarchical
paradigm has been implemented and extended by
several groups since, some have published their soft-
ware as open source (Li et al, 2009; Hoang et al,
2009; Vilar et al, 2010).
Combining multiple translation models has been
investigated for domain adaptation by Foster and
Kuhn (2007) and Koehn and Schroeder (2007) be-
fore. Heger et al (2010) exploit the distinction be-
tween hierarchical and lexical phrases in a similar
way as we do. They train phrase translation proba-
bilities with forced alignment using a conventional
phrase-based system (Wuebker et al, 2010) and em-
ploy them for the lexical phrases while the hierarchi-
cal phrases stay untouched.
3 Using the Unsupervised Data
The most straightforward way of trying to improve
the baseline with lightly-supervised training would
be to concatenate the human-generated parallel data
and the unsupervised data and to jointly extract
phrases from the unified parallel data (after having
trained word alignments for the unsupervised bitexts
as well). This method is simple and expected to
be effective usually. There may however be two
drawbacks: First, the reliability and the amount of
parallel sentences may differ between the human-
generated and the unsupervised part of the training
data. It might be desirable to run separate extrac-
tions on the two corpora in order to be able to dis-
tinguish and weight phrases (or rather their scores)
according to their origin during decoding. Second, if
we incorporate large amounts of additional unsuper-
vised data, the amount of phrases that are extracted
may become much larger. We would want to avoid
blowing up our phrase table sizes without an appro-
1Schwenk names the method lightly-supervised training be-
cause the topics that are covered in the monolingual source lan-
guage data that is being translated may potentially also be cov-
ered by parts of the language model training data of the system
which is used to translate them. This can be considered as a
form of light supervision. We loosely apply the term lightly-
supervised training if we mean the process of utilizing a ma-
chine translation system to produce additional bitexts that are
used as training data, but still refer to the automatically pro-
duced bilingual corpora as unsupervised data.
Arabic English
Sentences 2 514 413
Running words 54 324 372 55 348 390
Vocabulary 264 528 207 780
Singletons 115 171 91 390
Table 1: Data statistics for the preprocessed Arabic-
English parallel training corpus. In the corpus, numer-
ical quantities have been replaced by a special category
symbol.
dev (MT06) test (MT08)
Sentences 1 797 1 360
Running words 49 677 45 095
Vocabulary 9 274 9 387
OOV [%] 0.5 0.4
Table 2: Data statistics for the preprocessed Arabic part
of the dev and test corpora. In the corpus, numerical
quantities have been replaced by a special category sym-
bol.
priate effect on translation quality. This holds in par-
ticular in the case of hierarchical phrases. Phrase-
based machine translation systems are usually able
to correctly handle local context dependencies, but
often have problems in producing a fluent sentence
structure across long distances. It is thus an intuitive
supposition that using hierarchical phrases extracted
from unsupervised data in addition to the hierar-
chical phrases extracted from the presumably more
reliable human-generated bitexts does not increase
translation quality. We will compare a joint extrac-
tion to the usage of two separate translation mod-
els (either without separate weighting, with a binary
feature, or as a log-linear mixture). We will further
check if including hierarchical phrases from the un-
supervised data is beneficial or not.
4 Experiments
We use the open source Jane toolkit (Vilar et al,
2010) for our experiments, a hierarchical phrase-
based translation software written in C++.
4.1 Baseline System
The baseline system has been trained using a
human-generated parallel corpus of 2.5M Arabic-
English sentence pairs. Word alignments in both
92
directions were produced with GIZA++ and sym-
metrized according to the refined method that was
suggested by Och and Ney (2003).
The models integrated into our baseline system
are: phrase translation probabilities and lexical
translation probabilities for both translation direc-
tions, length penalties on word and phrase level,
three binary features marking hierarchical phrases,
glue rule, and rules with non-terminals at the bound-
aries, four simple additional count- and length-
based binary features, and a large 4-gram language
model with modified Kneser-Ney smoothing that
was trained with the SRILM toolkit (Stolcke, 2002).
We ran the cube pruning algorithm, the depth of
the hierarchical recursion was restricted to one by
using shallow rules as proposed by Iglesias et al
(2009).
The scaling factors of the log-linear model com-
bination have been optimized towards BLEU with
MERT (Och, 2003) on the MT06 NIST test corpus.
MT08 was employed as held-out test data. Detailed
statistics for the parallel training data are given in
Table 1, for the development and the test corpus in
Table 2.
4.2 Unsupervised Data
The unsupervised data that we integrate has been
created by automatic translations of parts of the
Arabic LDC Gigaword corpus (mostly from the
HYT collection) with a standard phrase-based sys-
tem (Koehn et al, 2003). We thus in fact conduct a
cross-system and cross-paradigm variant of lightly-
supervised training. Translating the monolingual
Arabic data has been performed by LIUM, Le Mans,
France. We thank Holger Schwenk for kindly pro-
viding the translations.
The score computed by the decoder for each
translation has been normalized with respect to the
sentence length and used to select the most reliable
sentence pairs. Word alignments for the unsuper-
vised data have been produced in the same way as
for the baseline bilingual training data. We report
the statistics of the unsupervised data in Table 3.
4.3 Translation Models
We extracted three different phrase tables, one from
the baseline human-generated parallel data only,
one from the unsupervised data only, and one joint
Arabic English
Sentences 4 743 763
Running words 121 478 207 134 227 697
Vocabulary 306 152 237 645
Singletons 130 981 102 251
Table 3: Data statistics for the Arabic-English unsuper-
vised training corpus after selection of the most reliable
sentence pairs. In the corpus, numerical quantities have
been replaced by a special category symbol.
phrase table from the concatenation of the baseline
data and the unsupervised data. We will denote the
different extractions as baseline, unsupervised, and
joint, respectively.
The conventional restrictions have been applied
for phrase extraction in all conditions, i.e. a maxi-
mum length of ten words on source and target side
for lexical phrases, a length limit of five (including
non-terminal symbols) on source side and ten on tar-
get side for hierarchical phrases, and at most two
non-terminals per rule which are not allowed to be
adjacent on the source side. To limit the number of
hierarchical phrases, a minimum count cutoff of one
and an extraction pruning threshold of 0.1 have been
applied to them. Note that we did not prune lexical
phrases.
Statistics on the phrase table sizes are presented
in Table 4.2 In total the joint extraction results in
almost three times as many phrases as the baseline
extraction. The extraction from the unsupervised
data exclusively results in more than two times as
many hierarchical phrases as from the baseline data.
The sum of the number of hierarchical phrases from
baseline and unsupervised extraction is very close
to the number of hierarchical phrases from the joint
extraction. If we discard the hierarchical phrases ex-
tracted from the unsupervised data and use the lex-
ical part of the unsupervised phrase table (27.3M
phrases) as a second translation model in addition to
the baseline phrase table (67.0M phrases), the over-
all number of phrases is increased by only 41% com-
pared to the baseline system.
2The phrase tables have been filtered towards the phrases
needed for the translation of a given collection of test corpora.
93
number of phrases
lexical hierarchical total
extraction from baseline data 19.8M 47.2M 67.0M
extraction from unsupervised data 27.3M 115.6M 142.9M
phrases present in both tables 15.0M 40.1M 55.1M
joint extraction baseline + unsupervised 32.1M 166.5M 198.6M
Table 4: Phrase table sizes. The phrase tables have been filtered towards a larger set of test corpora containing a total
of 2.3 million running words.
dev (MT06) test (MT08)
BLEU TER BLEU TER
[%] [%] [%] [%]
HPBT baseline 44.1 49.9 44.4?0.9 49.4?0.8
HPBT unsupervised only 45.3 48.8 45.2 49.1
joint extraction baseline + unsupervised 45.6 48.7 45.4?0.9 49.1?0.8
baseline hierarchical phrases + unsupervised lexical phrases 45.1 49.1 45.2 49.2
baseline hierarchical phrases + joint extraction lexical phrases 45.3 48.7 45.3 49.1
baseline + unsupervised lexical phrases 45.3 48.9 45.3 49.0
baseline + unsupervised lexical phrases (with binary feature) 45.3 48.8 45.4 49.0
baseline + unsupervised lexical phrases (separate scaling factors) 45.3 48.9 45.0 49.3
baseline + unsupervised full table 45.6 48.6 45.1 48.9
baseline + unsupervised full table (with binary feature) 45.5 48.6 45.2 48.8
baseline + unsupervised full table (separate scaling factors) 45.5 48.7 45.3 49.0
Table 5: Results for the NIST Arabic-English translation task (truecase). The 90% confidence interval is given for the
baseline system as well as for the system with joint phrase extraction. Results in bold are significantly better than the
baseline.
4.4 Experimental Results
The empirical evaluation of all our systems on the
two standard metrics BLEU (Papineni et al, 2002)
and TER (Snover et al, 2006) is presented in Ta-
ble 5. We have also checked the results for statistical
significance over the baseline. The confidence in-
tervals have been computed using bootstrapping for
BLEU and Cochran?s approximate ratio variance for
TER (Leusch and Ney, 2009).
When we combine the full baseline phrase ta-
ble with the unsupervised phrase table or the lexi-
cal part of it, we either use common scaling factors
for their source-to-target and target-to-source trans-
lation costs, or we use common scaling factors but
mark entries from the unsupervised table with a bi-
nary feature, or we optimize the four translation fea-
tures separately for each of the two tables as part of
the log-linear model combination.
Including the unsupervised data leads to a sub-
stantial gain on the unseen test set of up to +1.0%
BLEU absolute. The different ways of combining
the manually produced data with the unsupervised
have little impact on translation quality. This holds
specifically for the combination with only the lexical
phrases, which, when marked with a binary feature,
is able to obtain the same results as the full (joint
extraction) system but with much less phrases. We
compared the decoding speed of these two setups
and observed that the system with less phrases is
clearly faster (5.5 vs. 2.6 words per second, mea-
sured on MT08). The memory requirements of the
systems do not differ greatly as we are using a bi-
narized representation of the phrase table with on-
demand loading. All setups consume slightly less
than 16 gigabytes of RAM.
94
5 Conclusion
We presented several approaches of applying
lightly-supervised training to hierarchical phrase-
based machine translation. Using the additional au-
tomatically produced bitexts we have been able to
obtain considerable gains compared to the baseline
on the large-scale NIST Arabic-to-English transla-
tion task. We showed that a joint phrase extraction
from human-generated and automatically generated
parallel training data is not required to achieve sig-
nificant improvements. The same translation qual-
ity can be achieved by adding a second translation
model with only lexical phrases extracted from the
automatically created bitexts. The overall amount of
phrases can thus be kept much smaller.
Acknowledgments
The authors would like to thank Holger Schwenk
from LIUM, Le Mans, France, for making the au-
tomatic translations of the Arabic LDC Gigaword
corpus available. This work was partly supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-08-C-0110.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the DARPA.
References
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In Proc. of the First Workshop on Tabulation
in Parsing and Deduction, pages 133?137, April.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Arbor,
MI, June.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Carmen Heger, Joern Wuebker, David Vilar, and Her-
mann Ney. 2010. A Combination of Hierarchical Sys-
tems with Forced Alignments from Phrase-Based Sys-
tems. In Proc. of the Int. Workshop on Spoken Lan-
guage Translation (IWSLT), Paris, France, December.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchical,
and Syntax-Based Statistical Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144?151, Prague,
Czech Republic, June.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule Filtering by Pattern
for Efficient Hierarchical Translation. In Proc. of the
12th Conf. of the Europ. Chapter of the Assoc. for
Computational Linguistics (EACL), pages 380?388,
Athens, Greece, March.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 224?227, Prague, Czech
Republic, June.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc. of
the Human Language Technology Conf. / North Amer-
ican Chapter of the Assoc. for Computational Lin-
guistics (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 177?180, Prague, Czech Re-
public, June.
Gregor Leusch and Hermann Ney. 2009. Edit distances
with block movements and error rate confidence esti-
mates. Machine Translation, December.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An Open
Source Toolkit for Parsing-Based Machine Transla-
tion. In Proc. of the Workshop on Statistical Machine
Translation, pages 135?139, Athens, Greece, March.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora (EMNLP99), pages 20?
28, University of Maryland, College Park, MD, June.
95
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of the 40th
Annual Meeting of the Assoc. for Computational Lin-
guistics (ACL), pages 311?318, Philadelphia, PA, July.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/French News Trans-
lation System by Lightly-Supervised Training. In MT
Summit XII, Ottawa, Ontario, Canada, August.
Holger Schwenk. 2008. Investigations on Large-Scale
Lightly-Supervised Training for Statistical Machine
Translation. In Proc. of the Int. Workshop on Spo-
ken Language Translation (IWSLT), pages 182?189,
Waikiki, Hawaii, October.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 223?231, Cambridge,
MA, August.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, pages 262?270,
Uppsala, Sweden, July.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training Phrase Translation Models with Leaving-
One-Out. In Proc. of the Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 475?
484, Uppsala, Sweden, July.
96
