Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 692?700,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Framework of Feature Selection Methods for Text Categorization 
 
 
Shoushan Li1  Rui Xia2  Chengqing Zong2  Chu-Ren Huang1 
 
1
 Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang} 
@gmail.com 
 
2
 National Laboratory of Pattern 
Recognition 
 Institute of Automation 
 Chinese Academy of Sciences  
{rxia,cqzong}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In text categorization, feature selection (FS) is 
a strategy that aims at making text classifiers 
more efficient and accurate. However, when 
dealing with a new task, it is still difficult to 
quickly select a suitable one from various FS 
methods provided by many previous studies. 
In this paper, we propose a theoretic 
framework of FS methods based on two basic 
measurements: frequency measurement and 
ratio measurement. Then six popular FS 
methods are in detail discussed under this 
framework. Moreover, with the guidance of 
our theoretical analysis, we propose a novel 
method called weighed frequency and odds 
(WFO) that combines the two measurements 
with trained weights. The experimental results 
on data sets from both topic-based and 
sentiment classification tasks show that this 
new method is robust across different tasks 
and numbers of selected features.  
1 Introduction 
With the rapid growth of online information, text 
classification, the task of assigning text 
documents to one or more predefined categories, 
has become one of the key tools for 
automatically handling and organizing text 
information. 
The problems of text classification normally 
involve the difficulty of extremely high 
dimensional feature space which sometimes 
makes learning algorithms intractable. A 
standard procedure to reduce the feature 
dimensionality is called feature selection (FS). 
Various FS methods, such as document 
frequency (DF), information gain (IG), mutual 
information (MI), 2? -test (CHI), Bi-Normal 
Separation (BNS), and weighted log-likelihood 
ratio (WLLR), have been proposed for the tasks 
(Yang and Pedersen, 1997; Nigam et al, 2000; 
Forman, 2003) and make text classification more 
efficient and accurate. 
However, comparing these FS methods 
appears to be difficult because they are usually 
based on different theories or measurements. For 
example, MI and IG are based on information 
theory, while CHI is mainly based on the 
measurements of statistic independence. 
Previous comparisons of these methods have 
mainly depended on empirical studies that are 
heavily affected by the experimental sets. As a 
result, conclusions from those studies are 
sometimes inconsistent. In order to better 
understand the relationship between these 
methods, building a general theoretical 
framework provides a fascinating perspective. 
Furthermore, in real applications, selecting an 
appropriate FS method remains hard for a new 
task because too many FS methods are available 
due to the long history of FS studies. For 
example, merely in an early survey paper 
(Sebastiani, 2002), eight methods are mentioned. 
These methods are provided by previous work 
for dealing with different text classification tasks 
but none of them is shown to be robust across 
different classification applications. 
In this paper, we propose a framework with 
two basic measurements for theoretical 
comparison of six FS methods which are widely 
used in text classification. Moreover, a novel 
method is set forth that combines the two 
measurements and tunes their influences 
considering different application domains and 
numbers of selected features. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work on 
692
feature selection for text classification. Section 3 
theoretically analyzes six FS methods and 
proposes a new FS approach. Experimental 
results are presented and analyzed in Section 4. 
Finally, Section 5 draws our conclusions and 
outlines the future work. 
2 Related Work 
FS is a basic problem in pattern recognition and 
has been a fertile field of research and 
development since the 1970s. It has been proven 
to be effective on removing irrelevant and 
redundant features, increasing efficiency in 
learning tasks, and improving learning 
performance. 
FS methods fall into two broad categories, the 
filter model and the wrapper model (John et al, 
1994). The wrapper model requires one 
predetermined learning algorithm in feature 
selection and uses its performance to evaluate 
and determine which features are selected. And 
the filter model relies on general characteristics 
of the training data to select some features 
without involving any specific learning 
algorithm. There is evidence that wrapper 
methods often perform better on small scale 
problems (John et al 1994), but on large scale 
problems, such as text classification, wrapper 
methods are shown to be impractical because of 
its high computational cost. Therefore, in text 
classification, filter methods using feature 
scoring metrics are popularly used. Below we 
review some recent studies of feature selection 
on both topic-based and sentiment classification. 
In the past decade, FS studies mainly focus on 
topic-based classification where the classification 
categories are related to the subject content, e.g., 
sport or education. Yang and Pedersen (1997) 
investigate five FS metrics and report that good 
FS methods improve the categorization accuracy 
with an aggressive feature removal using DF, IG, 
and CHI. More recently, Forman (2003) 
empirically compares twelve FS methods on 229 
text classification problem instances and 
proposes a new method called 'Bi-Normal 
Separation' (BNS). Their experimental results 
show that BNS can perform very well in the 
evaluation metrics of recall rate and F-measure. 
But for the metric of precision, it often loses to 
IG. Besides these two comparison studies, many 
others contribute to this topic (Yang and Liu, 
1999; Brank et al, 2002; Gabrilovich and 
Markovitch, 2004) and more and more new FS 
methods are generated, such as, Gini index 
(Shang et al, 2007), Distance to Transition Point 
(DTP) (Moyotl-Hernandez and Jimenez-Salazar, 
2005), Strong Class Information Words (SCIW) 
(Li and Zong, 2005) and parameter tuning based 
FS for Rocchio classifier (Moschitti, 2003). 
Recently, sentiment classification has become 
popular because of its wide applications (Pang et 
al., 2002). Its criterion of classification is the 
attitude expressed in the text (e.g., recommended 
or not recommended, positive or negative) rather 
than some facts (e.g., sport or education). To our 
best knowledge, yet no related work has focused 
on comparison studies of FS methods on this 
special task. There are only some scattered 
reports in their experimental studies. Riloff et al 
(2006) report that the traditional FS method 
(only using IG method) performs worse than the 
baseline in some cases. However, Cui et al 
(2006) present the experiments on the sentiment 
classification for large-scale online product 
reviews to show that using the FS method of CHI 
does not degrade the performance but can 
significantly reduce the dimension of the feature 
vector. 
Moreover, Ng et al (2006) examine the FS of 
the weighted log-likelihood ratio (WLLR) on the 
movie review dataset and achieves an accuracy 
of 87.1%, which is higher than the result reported 
by Pang and Lee (2004) with the same dataset. 
From the analysis above, we believe that the 
performance of the sentiment classification 
system is also dramatically affected by FS. 
3 Our Framework 
In the selection process, each feature (term, or 
single word) is assigned with a score according 
to a score-computing function. Then those with 
higher scores are selected. These mathematical 
definitions of the score-computing functions are 
often defined by some probabilities which are 
estimated by some statistic information in the 
documents across different categories. For the 
convenience of description, we give some 
notations of these probabilities below. 
( )P t : the probability that a document x  contains 
term t ; 
( )iP c : the probability that a document x  does 
not belong to category ic ; 
( , )iP t c : the joint probability that a document x  
contains term t  and also belongs to category ic ; 
( | )iP c t : the probability that a document x belongs 
to category ic ?under the condition that it contains  
term t. 
693
( | )iP t c : the probability that, a document x does 
not contain term t with the condition that x belongs to 
category ic ; 
Some other probabilities, such as ( )P t , ( )iP c , 
( | )iP t c , ( | )iP t c , ( | )iP c t ,  and ( | )iP c t , are 
similarly defined. 
In order to estimate these probabilities, 
statistical information from the training data is 
needed, and notations about the training data are 
given as follows: 
1{ }mi ic = : the set of categories; 
iA : the number of the documents that contain the 
term t  and also belong to category ic ; 
iB : the number of the documents that contain the 
term t  but do not belong to category ic ; 
iN : the total number of the documents that belong 
to category ic ; 
allN : the total number of all documents from the 
training data. 
iC : the number of the documents that do not 
contain the term t  but belong to category ic , i.e., 
i iN A?  
iD : the number of the documents that neither 
contain the term t  nor belong to category ic , i.e., 
all i iN N B? ? ; 
In this section, we would analyze theoretically 
six popular methods, namely DF, MI, IG, CHI, 
BNS, and WLLR. Although these six FS 
methods are defined differently with different 
scoring measurements, we believe that they are 
strongly related. In order to connect them, we 
define two basic measurements which are 
discussed as follows. 
The first measurement is to compute the 
document frequency in one category, i.e., iA .  
The second measurement is the ratio between 
the document frequencies in one category and 
the other categories, i.e., /i iA B . The terms with 
a high ratio are often referred to as the terms with 
high category information. 
These two measurements form the basis for all 
the measurements that are used by the FS 
methods throughout this paper. In particular, we 
show that DF and MI are using the first and 
second measurement respectively. Other 
complicated FS methods are combinations of 
these two measurements. Thus, we regard the 
two measurements as basic, which are referred to 
as the frequency measurement and ratio 
measurement. 
3.1 Document Frequency (DF) 
DF is the number of documents in which a term 
occurs. It is defined as 
1
( )m iiDF A==?  
The terms with low or high document 
frequency are often referred to as rare or 
common terms, respectively. It is easy to see that 
this FS method is based on the first basic 
measurement. It assumes that the terms with 
higher document frequency are more informative 
for classification. But sometimes this assumption 
does not make any sense, for example, the stop 
words (e.g., the, a, an) hold very high DF scores, 
but they seldom contribute to classification. In 
general, this simple method performs very well 
in some topic-based classification tasks (Yang 
and Pedersen, 1997). 
3.2 Mutual Information (MI) 
The mutual information between term t  and 
class ic  is defined as 
( | )( , ) log ( )
i
i
P t cI t c
P t
=  
And it is estimated as 
log ( )( )
i all
i i i i
A NMI
A C A B
?
=
+ +
 
Let us consider the following formula (using 
Bayes theorem) 
( | ) ( | )( , ) log log( ) ( )
i i
i
i
P t c P c tI t c
P t P c
= =  
Therefore, 
( , )= log ( | ) log ( )i i iI t c P c t P c?  
And it is estimated as 
log log
      log log
1
      log(1 ) log
/
i i
i i all
i i i
i all
i
i i all
A NMI
A B N
A B N
A N
N
A B N
= ?
+
+
= ? ?
= ? + ?
 
From this formula, we can see that the MI score 
is based on the second basic measurement. This 
method assumes that the term with higher 
category ratio is more effective for classification. 
It is reported that this method is biased 
towards low frequency terms and the bias 
becomes extreme when ( )P t  is near zero. It can 
be seen in the following formula (Yang and 
Pedersen, 1997)  
( , ) log( ( | )) log( ( ))i iI t c P t c P t= ?  
694
Therefore, this method might perform badly 
when common terms are informative for 
classification. 
Taking into account mutual information of all 
categories, two types of MI score are commonly 
used: the maximum score ( )
max
I t  and the 
average score ( )avgI t , i.e.,  
1( ) max { ( , )}mmax i iI t I t c== , 
1
( ) ( ) ( , )mavg i iiI t P c I t c== ?? .  
We choose the maximum score since it performs 
better than the average score (Yang and Pedersen, 
1997). It is worth noting that the same choice is 
made for other methods, including CHI, BNS, 
and WLLR in this paper. 
3.3 Information Gain (IG) 
IG measures the number of bits of information 
obtained for category prediction by recognizing 
the presence or absence of a term in a document 
(Yang and Pedersen, 1997). The function is 
1
1
1
( ) { ( ) log ( )}
            +{ ( )[ ( | ) log ( | )]
           ( )[ ( | ) log ( | )]}
m
i ii
m
i ii
m
i ii
G t P c P c
P t P c t P c t
P t P c t P c t
=
=
=
= ?
+
?
?
?
 
And it is estimated as 
1
1 1
1 1
{ log }
    +( / )[ log ]
  ( / )[ log ]
m i i
i
all all
m m i i
i alli i
i i i i
m m i i
i alli i
i i i i
N NIG
N N
A AA N
A B A B
C CC N
C D C D
=
= =
= =
= ?
+ +
+
+ +
?
? ?
? ?
From the definition, we know that the 
information gain is the weighted average of the 
mutual information ( , )iI t c and ( , )iI t c  where 
the weights are the joint probabilities ( , )iP t c and 
( , )iP t c : 
1 1
( ) ( , ) ( , ) ( , ) ( , )m mi i i ii iG t P t c I t c P t c I t c= == +? ?  
Since ( , )iP t c is closely related to the 
document frequency iA  and the mutual 
information ( , )iI t c  is shown to be based on the 
second measurement, we can say that the IG 
score is influenced by the two basic 
measurements. 
3.4 2?  Statistic (CHI) 
The CHI measurement (Yang and Pedersen, 
1997) is defined as 
2( )
( ) ( ) ( ) ( )
all i i i i
i i i i i i i i
N A D C BCHI
A C B D A B C D
? ?
=
+ ? + ? + ? +
 
In order to get the relationship between CHI 
and the two measurements, the above formula is 
rewritten as follows 
2[ ( ) ( ) ]
( ) ( ) [ ( )]
all i all i i i i i
i all i i i all i i
N A N N B N A BCHI
N N N A B N A B
? ? ? ? ?
=
? ? ? + ? ? +
  
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same 
( 2
all iN N= ). The CHI score then can be written 
as
 
2
2
2 ( )
( ) [2 ( )]
2 ( / 1)
      2( / 1) [ / ( / 1)]
i i i
i i i i i
i i i
i
i i i i i i
i
N A BCHI
A B N A B
N A B
NA B A B A B
A
?
=
+ ? ? +
?
=
+ ? ? ? +
 
From the above formula, we see that the CHI 
score is related to both the frequency 
measurement iA
 
and ratio measurement 
/i iA B . Also, when keeping the same ratio value, 
the terms with higher document frequencies will 
yield higher CHI scores. 
3.5 Bi-Normal Separation (BNS) 
BNS method is originally proposed by Forman 
(2003) and it is defined as 
1 1( , ) ( ( | )) ( ( | )i i iBNS t c F P t c F P t c? ?= ?  
It is calculated using the following formula 
1 1( ) ( )i i
i all i
A B
BNS F F
N N N
? ?
= ?
?
 
where ( )F x  is the cumulative probability 
function of standard normal distribution. 
For simplicity, we assume that there are two 
categories and the numbers of the training 
documents in the two categories are the same, 
i.e., 2
all iN N=  and we also assume that i iA B> . 
It should be noted that this assumption is only to 
allow easier analysis but will not be applied in 
our experiment implementation. In addition, we 
only consider the case when / 0.5i iA N ? . In 
fact, most terms take the document frequency 
iA which is less than half of iN .  
Under these conditions, the BNS score can be 
shown in Figure 1 where the area of the shadow 
part represents ( / / )i i i iA N B N?  and the length 
of the projection to the x  axis represents the 
BNS score. 
695
From Figure 1, we can easily draw the two 
following conclusions: 
1) Given the same value of iA , the BNS score 
increases with the increase of i iA B? . 
2) Given the same value of i iA B? , BNS score 
increase with the decrease of iA . 
 
Figure 1. View of BNS using the normal probability 
distribution. Both the left and right graphs have 
shadowed areas of the same size. 
 
And the value of i iA B?  can be rewritten as 
the following 
1(1 )
/
i i
i i i i
i i i
A BA B A A
A A B
?
? = ? = ? ?  
The above analysis gives the following 
conclusions regarding the relationship between 
BNS and the two basic measurements: 
1) Given the same iA , the BNS score increases 
with the increase of /i iA B . 
2) Given the same /i iA B , when iA  increases, 
i iA B?  also increase. It seems that the BNS 
score does not show a clear relationship with 
iA . 
In summary, the BNS FS method is biased 
towards the terms with the high category ratio 
but cannot be said to be sensitive to document 
frequency. 
3.6 Weighted Log Likelihood Ratio 
(WLLR) 
WLLR method (Nigam et al, 2000) is defined as 
( | )( , ) ( | ) log ( | )
i
i i
i
P t cWLLR t c P t c
P t c
=  
And it is estimated as 
( )logi i all i
i i i
A A N NWLLR
N B N
? ?
=
?
 
The formula shows WLLR is proportional to 
the frequency measurement and the logarithm of 
the ratio measurement. Clearly, WLLR is biased 
towards the terms with both high category ratio 
and high document frequency and the frequency 
measurement seems to take a more important 
place than the ratio measurement. 
3.7 Weighed Frequency and Odds (WFO)  
So far in this section, we have shown that the 
two basic measurements constitute the six FS 
methods. The class prior probabilities, 
( ),  1,2,...,iP c i m= , are also related to the 
selection methods except for the two basic 
measurements. Since they are often estimated 
according to the distribution of the documents in 
the training data and are identical for all the 
terms in a class, we ignore the discussion of their 
influence on the selection measurements. In the 
experiment, we consider the case when training 
data have equal class prior probabilities. When 
training data are unbalanced, we need to change 
the forms of the two basic measurements to 
/i iA N  and ( ) / ( )i all i i iA N N B N? ? ? . 
Because some methods are expressed in 
complex forms, it is difficult to explain their 
relationship with the two basic measurements, 
for example, which one prefers the category ratio 
most. Instead, we will give the preference 
analysis in the experiment by analyzing the 
features in real applications. But the following 
two conclusions are drawn without doubt 
according to the theoretical analysis given above. 
1) Good features are features with high 
document frequency; 
2) Good features are features with high 
category ratio. 
These two conclusions are consistent with the 
original intuition. However, using any single one 
does not provide competence in selecting the 
best set of features. For example, stop words, 
such as ?a?, ?the? and ?as?, have very high 
document frequency but are useless for the 
classification. In real applications, we need to 
mix these two measurements to select good 
features. Because of different distribution of 
features in different domains, the importance of 
each measurement may differ a lot in different 
applications. Moreover, even in a given domain, 
when different numbers of features are to be 
selected, different combinations of the two 
measurements are required to provide the best 
performance. 
Although a great number of FS methods is 
available, none of them can appropriately change 
the preference of the two measurements. A better 
way is to tune the importance according to the 
application rather than to use a predetermined 
combination. Therefore, we propose a new FS 
method called Weighed Frequency and Odds 
(WFO), which is defined as 
696
 ( | ) / ( | ) 1i iwhen P t c P t c >  
1( | )( , ) ( | ) [log ]( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
                 ( , ) 0i
else
WFO t c =
 
And it is estimated as 
1( )( ) (log )i i all i
i i i
A A N NWFO
N B N
? ??? ?
=
?
 
where ?
 
is the parameter for tuning the weight 
between frequency and odds. The value of ?
 
varies from 0 to 1. By assigning different value 
to ?  we can adjust the preference of each 
measurement. Specially, when 0? = , the 
algorithm prefers the category ratio that is 
equivalent to the MI method; when 1? = , the 
algorithm is similar to DF; when 0.5? = , the 
algorithm is exactly the WLLR method. In real 
applications, a suitable parameter ?  needs to be 
learned by using training data. 
4 Experimental Studies  
4.1 Experimental Setup 
Data Set:  The experiments are carried out on 
both topic-based and sentiment text classification 
datasets. In topic-based text classification, we 
use two popular data sets: one subset of 
Reuters-21578 referred to as R2 and the 20 
Newsgroup dataset referred to as 20NG. In detail, 
R2 consist of about 2,000 2-category documents 
from standard corpus of Reuters-21578. And 
20NG is a collection of approximately 20,000 
20-category documents 1 . In sentiment text 
classification, we also use two data sets: one is 
the widely used Cornell movie-review dataset2 
(Pang and Lee, 2004) and one dataset from 
product reviews of domain DVD3 (Blitzer et al, 
2007). Both of them are 2-category tasks and 
each consists of 2,000 reviews. In our 
experiments, the document numbers of all data 
sets are (nearly) equally distributed cross all 
categories. 
Classification Algorithm: Many 
classification algorithms are available for text 
classification, such as Na?ve Bayes, Maximum 
Entropy, k-NN, and SVM. Among these methods, 
SVM is shown to perform better than other 
methods (Yang and Pedersen, 1997; Pang et al, 
                                                      
1
 http://people.csail.mit.edu/~jrennie/20Newsgroups/ 
2 http://www.cs.cornell.edu/People/pabo/movie-review-data/ 
3
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
 
2002). Hence we apply SVM algorithm with the 
help of the LIBSVM 4  tool. Almost all 
parameters are set to their default values except 
the kernel function which is changed from a 
polynomial kernel function to a linear one 
because the linear one usually performs better for 
text classification tasks. 
Experiment Implementation: In the 
experiments, each dataset is randomly and 
evenly split into two subsets: 90% documents as 
the training data and the remaining 10% as 
testing data. The training data are used for 
training SVM classifiers, learning parameters in 
WFO method and selecting "good" features for 
each FS method. The features are single words 
with a bool weight (0 or 1), representing the 
presence or absence of a feature. In addition to 
the ?principled? FS methods, terms occurring in 
less than three documents ( 3DF ? ) in the 
training set are removed. 
4.2 Relationship between FS Methods and 
the Two Basic Measurements 
To help understand the relationship between FS 
methods and the two basic measurements, the 
empirical study is presented as follows. 
Since the methods of DF and MI only utilize 
the document frequency and category 
information respectively, we use the DF scores 
and MI scores to represent the information of the 
two basic measurements. Thus we would select 
the top-2% terms with each method and then 
investigate the distribution of their DF and MI 
scores.  
First of all, for clear comparison, we 
normalize the scores coming from all the 
methods using Min-Max normalization method 
which is designed to map a score s  to 's  in 
the range [0, 1] by computing 
'
s Min
s
Max Min
?
=
?
 
where
 
Min
 
and Max
 
denote the minimum 
and maximum values respectively in all terms? 
scores using one FS method. 
Table 1 shows the mean values of all top-2% 
terms? MI scores and DF scores of all the six FS 
methods in each domain. From this table, we can 
apparently see the relationship between each 
method and the two basic measurements. For 
instance, BNS most distinctly prefers the terms 
with high MI scores and low DF scores. 
According to the degree of this preference, we 
                                                      
4
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
697
FS 
Methods 
Domain 
20NG R2 Movie DVD 
DF score MI score DF score MI score DF score MI score DF score MI score 
MI 0.004 0.870 0.047 0.959 0.003 0.888 0.004 0.881 
BNS 0.005 0.864 0.117 0.922 0.008 0.881 0.006 0.880 
CHI 0.015 0.814 0.211 0.748 0.092 0.572 0.055 0.676 
IG 0.087 0.525 0.209 0.792 0.095 0.559 0.066 0.669 
WLLR 0.026 0.764 0.206 0.805 0.168 0.414 0.127 0.481 
DF 0.122 0.252 0.268 0.562 0.419 0.09 0.321 0.111 
 
Table 1. The mean values of all top-2% terms? MI and DF scores using six FS methods in each domain 
 
can rank these six methods as 
MI, BNS IG, CHI, WLLR DFf f , where x yf
 
means method x
 
prefers the terms with  
higher MI scores (higher category information) 
and lower DF scores (lower document frequency) 
than method y. This empirical discovery is in 
agreement with the finding that WLLR is biased 
towards the high frequency terms and also with 
the finding that BNS is biased towards high 
category information (cf. Section 3 theoretical 
analysis). Also, we can find that CHI and IG 
share a similar preference of these two 
measurements in 2-category domains, i.e., R2, 
movie, and DVD. This gives a good explanation 
that CHI and IG are two similar-performed 
methods for 2-category tasks, which have been 
found by Forman (2003) in their experimental 
studies. 
According to the preference, we roughly 
cluster FS methods into three groups. The first 
group includes the methods which dramatically 
prefer the category information, e.g., MI and 
BNS; the second one includes those which prefer 
both kinds of information, e.g., CHI, IG, and 
WLLR; and the third one includes those which 
strongly prefer frequency information, e.g., DF. 
4.3 Performances of Different FS Methods 
It is worth noting that learning parameters in 
WFO is very important for its good performance. 
We use 9-fold cross validation to help learning 
the parameter ?  so as to avoid over-fitting. 
Specifically, we run nine times by using every 8 
fold documents as a new training data set and the 
remaining one fold documents as a development 
data set. In each running with one fixed feature 
number m, we get the best 
,i m best? ? (i=1,..., 9) 
value through varying 
,i m?  from 0 to 1 with the 
step of 0.1 to get the best performance in the 
development data set. The average value 
m best? ? , 
i.e., 
9
,1
( ) / 9m best i m besti? ?? ?== ?  
is used for further testing. 
Figure 2 shows the experimental results when 
using all FS methods with different selected 
feature numbers. The red line with star tags 
represents the results of WFO. At the first glance, 
in R2 domain, the differences of performances 
across all are very noisy when the feature 
number is larger than 1,000, which makes the 
comparison meaningless. We think that this is 
because the performances themselves in this task 
are very high (nearly 98%) and the differences 
between two FS methods cannot be very large 
(less than one percent). Even this, WFO method 
do never get the worst performance and can also 
achieve the top performance in about half times, 
e.g., when feature numbers are 20, 50, 100, 500, 
3000. 
Let us pay more attention to the other three 
domains and discuss the results in the following 
two cases. 
In the first case when the feature number is 
low (about less than 1,000), the FS methods in 
the second group including IG, CHI, WLLR,  
always perform better than those in the other two 
groups. WFO can also perform well because its 
parameters 
m best? ?  are successfully learned to be 
around 0.5, which makes it consistently belong 
to the second group. Take 500 feature number 
for instance, the parameters 500 best? ?  are 0.42, 
0.50, and 0.34 in these three domains 
respectively. 
In the second case when the feature number is 
large, among the six traditional methods, MI and 
BNS take the leads in the domains of 20NG and 
Movie while IG and CHI seem to be better and 
more stable than others in the domain of DVD. 
As for WFO, its performances are excellent cross 
all these three domains and different feature 
numbers. In each domain, it performs similarly 
as or better than the top methods due to its 
well-learned parameters. For example, in 20NG, 
the parameters 
m best? ?  are 0.28, 0.20, 0.08, and 
0.01 when feature numbers are 10,000, 15,000, 
20,000, and 30,000. These values are close to 0 
698
(WFO equals MI when 0? = ) while MI is the 
top one in this domain. 
10 20 50 100 200 500 1000 2000 3000 4227
0.88
0.9
0.92
0.94
0.96
0.98
1
feature number
a
cc
u
ra
cy
Topic - R2
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
200 500 1000 2000 5000 10000 15000 20000 30000 32091
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
feature number
ac
cu
ra
cy
Topic - 20NG
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
50 200 500 1000 4000 7000 10000 13000 15176
0.55
0.6
0.65
0.7
0.75
0.8
0.85
feature number
ac
cu
ra
cy
Sentiment - Movie
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
20 50 100 500 1000 1500 2000 3000 4000 5824
0.5
0.55
0.6
0.65
0.7
0.75
0.8
feature number
ac
cu
ra
cy
Sentiment - DVD
 
 
DF
MI
IG
BNS
CHI
WLLR
WFO
 
Figure 2. The classification accuracies of the four domains 
using seven different FS methods while increasing the 
number of selected features. 
 
From Figure 2, we can also find that FS does 
help sentiment classification. At least, it can 
dramatically decrease the feature numbers 
without losing classification accuracies (see 
Movie domain, using only 500-4000 features is 
as good as using all 15176 features). 
5 Conclusion and Future Work 
In this paper, we propose a framework with two 
basic measurements and use it to theoretically 
analyze six FS methods. The differences among 
them mainly lie in how they use these two 
measurements. Moreover, with the guidance of 
the analysis, a novel method called WFO is 
proposed, which combine these two 
measurements with trained weights. The 
experimental results show that our framework 
helps us to better understand and compare 
different FS methods. Furthermore, the novel 
method WFO generated from the framework, can 
perform robustly across different domains and 
feature numbers. 
In our study, we use four data sets to test our 
new method. There are much more data sets on 
text categorization which can be used. In 
additional, we only focus on using balanced 
samples in each category to do the experiments. 
It is also necessary to compare the FS methods 
on some unbalanced data sets, which are 
common in real-life applications (Forman, 2003; 
Mladeni and Marko, 1999). These matters will 
be dealt with in the future work. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in the 
Hong Kong Polytechnic University. 
References  
J. Blitzer, M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain adaptation for sentiment 
classification. In Proceedings of ACL-07, the 45th 
Meeting of the Association for Computational 
Linguistics. 
J. Brank, M. Grobelnik, N. Milic-Frayling, and D. 
Mladenic. 2002. Interaction of feature selection 
methods and linear classification models. In 
Workshop on Text Learning held at ICML. 
H. Cui, V. Mittal, and M. Datar. 2006. Comparative 
experiments on sentiment classification for online 
product reviews. In Proceedings of AAAI-06, the 
21st National Conference on Artificial Intelligence. 
G. Forman. 2003. An extensive empirical study of 
feature selection metrics for text classification. The 
Journal of Machine Learning Research, 3(1): 
1289-1305. 
699
E. Gabrilovich and S. Markovitch. 2004. Text 
categorization with many redundant features: using 
aggressive feature selection to make SVMs 
competitive with C4.5. In Proceedings of the ICML, 
the 21st International Conference on Machine 
Learning. 
G. John, K. Ron, and K. Pfleger. 1994. Irrelevant 
features and the subset selection problem. In 
Proceedings of ICML-94, the 11st International 
Conference on Machine Learning.  
S. Li and C. Zong. 2005. A new approach to feature 
selection for text categorization. In Proceedings of 
the IEEE International Conference on Natural 
Language Processing and Knowledge Engineering 
(NLP-KE). 
D. Mladeni and G. Marko. 1999. Feature selection for 
unbalanced class distribution and naive bayes. In 
Proceedings of ICML-99, the 16th International 
Conference on Machine Learning. 
A. Moschitti. 2003. A study on optimal parameter 
tuning for Rocchio text classifier. In Proceedings 
of ECIR, Lecture Notes in Computer Science, 
vol. 2633, pp. 420-435. 
E. Moyotl-Hernandez and H. Jimenez-Salazar. 2005. 
Enhancement of DTP feature selection method for 
text categorization. In Proceedings of CICLing, 
Lecture Notes in Computer Science, vol.3406, 
pp.719-722.  
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. 
Examining the role of linguistic knowledge sources 
in the automatic identification and classification of 
reviews. In Proceedings of the COLING/ACL Main 
Conference Poster Sessions. 
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 
2000. Text classification from labeled and 
unlabeled documents using EM. Machine Learning, 
39(2/3): 103-134. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP-02, 
the Conference on Empirical Methods in Natural 
Language Processing. 
B. Pang and L. Lee. 2004. A sentimental education: 
Sentiment analysis using subjectivity 
summarization based on minimum cuts. In 
Proceedings of ACL-04, the 42nd Meeting of the 
Association for Computational Linguistics. 
E. Riloff, S. Patwardhan, and J. Wiebe. 2006. Feature 
subsumption for opinion analysis. In Proceedings 
of EMNLP-06, the Conference on Empirical 
Methods in Natural Language Processing,. 
F. Sebastiani. 2002. Machine learning in automated 
text categorization. ACM Computing Surveys, 
34(1): 1-47. 
W. Shang, H. Huang, H. Zhu, Y. Lin, Y. Qu, and Z. 
Wang. 2007. A novel feature selection algorithm 
for text categorization. The Journal of Expert 
System with Applications, 33:1-5. 
Y. Yang and J. Pedersen. 1997. A comparative study 
on feature selection in text categorization. In 
Proceedings of ICML-97, the 14th International 
Conference on Machine Learning. 
Y. Yang and X. Liu. 1999. A re-examination of text 
categorization methods. In Proceedings of 
SIGIR-99, the 22nd annual international ACM 
Conference on Research and Development in 
Information Retrieval.  
700
Coling 2010: Poster Volume, pages 1336?1344,
Beijing, August 2010
Exploring the Use of Word Relation Features
for Sentiment Classification 
Rui Xia and Chengqing Zong 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
{rxia, cqzong}@nlpr.ia.ac.cn 
Abstract
Word relation features, which encode 
relation information between words, are 
supposed to be effective features for 
sentiment classification. However, the 
use of word relation features suffers 
from two issues. One is the sparse-data 
problem and the lack of generalization 
performance; the other is the limitation 
of using word relations as additional 
features to unigrams. To address the two 
issues, we propose a generalized word 
relation feature extraction method and 
an ensemble model to efficiently inte-
grate unigrams and different type of 
word relation features. Furthermore, 
aimed at reducing the computation 
complexity, we propose two fast feature 
selection methods that are specially de-
signed for word relation features. A 
range of experiments are conducted to 
evaluate the effectiveness and efficiency 
of our approaches. 
1 Introduction 
The task of text sentiment classification has be-
come a hotspot in the field of natural language 
processing in recent years (Pang and Lee, 2008). 
The dominating text representation method in 
sentiment classification is known as the bag-of-
words (BOW) model. Although BOW is quite 
simple and efficient, a great deal of the informa-
tion from original text is discarded, word order 
is disrupted and syntactic structures are broken. 
Therefore, more sophisticated features with a 
deeper understanding of the text are required for 
sentiment classification tasks. 
With the attempt to capture the word relation 
information behind the text, word relation (WR) 
features, such as higher-order n-grams and word 
dependency relations, have been employed in 
text representation for sentiment classification 
(Dave et al, 2003; Gamon, 2004; Joshi and 
Penstein-Ros?, 2009). 
However, in most of the literature, the per-
formance of individual WR feature set was poor, 
even inferior to the traditional unigrams. For 
this reason, WR features were commonly used 
as additional features to supplement unigrams, 
to encode more word order and word relation 
information. Even so, the performance of joint 
features was still far from satisfactory (Dave et 
al., 2003; Gamon, 2004; Joshi and Penstein-
Ros?, 2009).  
We speculate that the poor performance is 
possibly due to the following two reasons: 1) in 
WR features, the data are sparse and the fea-
tures lack generalization capability; 2) the use 
of joint features of unigrams and WR features 
has its limitation.  
On one hand, there were attempts at finding 
better generalized WR (GWR) features. Gamon 
(2004) back off words in n-grams (and semantic 
relations) to their respective POS tags (e.g., 
great-movie to adjective-noun); Joshi and Ros? 
(2009) propose a method by only backing off 
the head word in dependency relation pairs to its 
POS tag (e.g., great-movie to great-noun), 
which are supposed to be more generalized than 
word pairs. Based on Joshi and Ros??s method, 
we back off the word in each word relation pairs 
to its corresponding POS cluster, making the 
feature space smarter and more effective. 
On the other hand, we find that from uni-
grams to WR features, relevance between fea-
tures is reduced and the independence is in-
1336
creased. Although the discriminative model 
(e.g., SVM) is proven to be more effective on 
unigrams (Pang et al, 2002) for its ability of 
capturing the complexity of more relevant fea-
tures, WR features are more inclined to work 
better in the generative model (e.g., NB) since 
the feature independence assumption holds well 
in this case. 
Based on this finding, we therefore intuitively 
seek, instead of jointly using unigrams and 
GWR features, to efficiently integrate them to 
synthesize a more accurate classification proce-
dure. We use the ensemble model to fuse differ-
ent types of features under distinct classification 
models, with an attempt to overcome individual 
drawbacks and benefit from each other?s merit, 
and finally to enhance the overall performance. 
Furthermore, feature reduction is another im-
portant issue of using WR features. Due to the 
huge dimension of WR feature space, traditional 
feature selection methods in text classification 
perform inefficiently. However, to our knowl-
edge, no related work has focused on feature 
selection specially designed for WR features. 
Taking this point into consideration, we pro-
pose two fast feature selection methods (FMI 
and FIG) for GWR features with a theoretical 
proof. FMI and FIG regard the importance of a 
GWR feature as two component parts, and take 
the sum of two scores as the final score. FMI 
and FIG remain a close approximation to MI 
and IG, but speed up the computation by at most 
10 times. Finally, we apply FMI and FIG to the 
ensemble model, reducing the computation 
complexity to a great extent. 
The remainder of this paper is organized as 
follows. In Section 2, we introduce the approach 
to extracting GWR features. In Section 3, we 
present the ensemble model for integrating dif-
ferent types of features. In Section 4, the fast 
feature selection methods for WR features are 
proposed. Experimental results are reported in 
Section 5. Section 6 draws conclusions and out-
lines directions for future work. 
2 Generalized Word Relation Features 
A straightforward method for extracting WR 
features is to simply map word pairs into the 
feature vector. However, due to the sparse-data 
problem and the lack of generalization ability, 
the performance of WR is discounted. Consider 
the following two pieces of text: 
1) Avatar is a great movie. I definitely rec-
ommend it. 
2) I definitely recommend this book. It is great.
We lay the emphasis on the following word 
pairs: great-movie, great-it, it-recommend, and 
book-recommend. Although these features are 
good indicators of sentiment, due to the sparse-
data problem, they may not contribute as impor-
tantly as we have expected in machine learning 
algorithms. Moreover, the effects of those fea-
tures would be greatly reduced when they are 
not captured in the test dataset (for example, a 
new feature great-song in the test set would 
never benefit from great-movie and great-it).
Joshi and Ros? (2009) back off the head word 
in each of the relation pairs to its POS tag. Tak-
ing great-movie for example, the back-off fea-
ture will be great-noun. With such a transforma-
tion, original features like great-movie, great-
book and other great-noun pairs are regarded as 
one feature, hence, the learning algorithms 
could learn a weight for a more general feature 
that has stronger evidence of association with 
the class, and any new test sentence that con-
tains an unseen noun in a similar relationship 
with the adjective great (e.g., great-song) will 
receive some weight in favor of the class label. 
With the attempt to make a further generali-
zation, we conduct a POS clustering. Consider-
ing the effect of different POS tags in both uni-
grams and word relations, the POS tags are 
categorized as shown in Table 1. 
POS-cluster Contained POS tags 
J JJ, JJS, JJR 
R RB, RBS, RBR 
V VB, VBZ, VBD, VBN, VBG, VBP
N NN, NNS, NNP, NNPS, PRP 
O The other POS tags 
Table 1: POS Clustering (the Penn Corpus Style) 
Since adjectives and adverbs have the highest 
correlation with sentiment, and some verbs and 
nouns are also strong indicators of sentiment, 
we therefore put them into separate clusters. All 
the other tags are categorized to one cluster be-
cause they contain a lot of noise rather than use-
ful information. In addition, we assign pronouns 
to POS-cluster N, aimed at capturing the gener-
ality in WR features like great-movie and great-
it, or book-recommend and it-recommend.
1337
Taking ?Avatar is a great movie? for example, 
different types of WR features are presented in 
Table 2, where Uni denotes unigrams; WR-Bi 
indicates traditional bigrams; WR-Dp indicates 
word pairs of dependency relation; GWR-Bi 
and GWR-Dp respectively denote generalized 
bigrams and dependency relations. 
WR types WR features 
WR-Bi Avatar-is, is-a, a-great, great-movie 
WR-Dp 
Avatar-is, a-movie, great-movie, 
movie-is 
GWR-Bi 
Avatar-V, is-O, a-J, great-N, 
N-is, V-a, O-great, J-movie
GWR-Dp 
Avatar-V, a-N, great-N, movie-V, 
N-is, O-movie, J-movie
Table 2: Different types of WR features 
3 An Ensemble Model for Integrating 
WR Features 
3.1 Joint Features, Good Enough? 
Although the unigram feature space is simple, 
and the WR features are more sophisticated, the 
latter was mostly used as extra features in addi-
tion to the former, rather than to substitute it. 
Even so, in most of the literature, the improve-
ments of joint features are still not as good as 
we had expected. For example, Dave et al 
(2003) try to extract a refined subset of WR 
pairs (adjective-noun, subject-verb, and verb-
object pairs) as additional features to traditional 
unigrams, but do not get significant improve-
ments. In the experiments of Joshi and Ros? 
(2009), the improvements of unigrams together 
with WR features (even generalized WR 
features) are also not remarkable (sometimes 
even worse) compared to simple unigrams. 
One possible explanation might be that dif-
ferent types of features have distinct distribu-
tions, and therefore would probably yield vary 
performance on different machine learning al-
gorithms. For example, the generative model is 
optimal if the distribution is well estimated; 
otherwise the performance will drop signifi-
cantly (for instance, NB performs poorly unless 
the feature independence assumption holds 
well). While on the contrary, the discriminative 
model such as SVM is good at representing the 
complexity of relevant features. 
Let us review the results reported by Pang 
and Lee (2002) that compare different classifi-
cation algorithms: SVM performs significantly 
better than NB on unigrams; while the outcome 
is the opposite on bigrams. It is possibly due to 
that from unigrams to bigrams, the relevance 
between features is reduced (bigrams cover 
some relevance of unigram pairs), and the inde-
pendence between features increases. 
Since GWR features are less relevant and 
more independent in comparison, it is reason-
able for us to infer that these features would 
work better on NB than on SVM. We therefore 
intuitively seek to employ the ensemble model 
for sentiment classification tasks, with an at-
tempt to efficiently integrate different types of 
features under distinct classification models. 
3.2 Model Formulation 
The ensemble model (Kittler, 1998), which 
combines the outputs of several base classifiers 
to form an integrated output, has become an 
effective classification method for many do-
mains.
For our ensemble task, we train six base clas-
sifiers (the NB and SVM model respectively on 
the Uni, GWR-Bi and GWR-Dp features). By 
mapping the probabilistic outputs (for C  classes) 
of D base classifiers into the meta-vector 
11 1 1? [ , , , , , , ],C kj D DCo o o o ox ! ! ! !  (1) 
the weighted ensemble is formulized by 
1 1
? ?( ) ,
D D
j j k kj k k
k k
O g o xX X q 
 
  ? ?x D j  (2) 
where  is the weight assigned to the -th
ss
mization 
, we use descent 
 defined as 
k
X k
base cla ifier. 
3.3 Weight Opti
Inspired by linear regression
methods to seek optimization according to cer-
tain criteria. We employ two criteria, namely 
the perceptron criterion and the minimum clas-
sification error (MCE) criterion. 
The perceptron cost function is
1, ,
1
1 N
? ?max ( ) ( ) .
ip j i y ij C
i
J g g
N 
? ? ? ?? ?? x x!  (3) 
The minimization of 
p
J is approximately equal 
sc
 1992) 
is
function is given by 
to seek a minimum mi lassification rate. 
The MCE criterion (Juang and Katagiri,
supposed to be more relevant to the classifica-
tion error. A short version of MCE criterion 
1338
1 1
1
? ?( ) ( ( ) max ( ))
N C
mce i
i j
J I y j g g
N
E
 
   ?? x x  (4) j k
k jv
where  is the sigmoid function. 
For both criteria, stochastic gradient descent 
. SGD uses 
( )E <
(SGD) is utilized for optimization
approximate gradients estimated from subsets of 
the training data and updates the parameters in 
an online manner: 
( 1) ( ) ( )
h h
h
J
k k kX X I s   . (5) 
Xs
functions are respectively
The gradients of perceptron and MCE cost 
p
1
1
? ?( )
N
h
ih
J
x x
NX q 
s
 
s ?  (6) i iD s h D yq 
where
i
, and 
1, ,
?arg max ( )
i j
j C
s g

 x
!
MC ( ))
y i
Js E
1
1
? ? ? ?( )(1 ( )
i i i i
N
y i h D s h D y
ih
l l x x
NX q  q 
  
s ? x x  (7) 
where x?  and 
As for perceptron criterion, we employ the 
average perceptron (AvgP) (Freund and 
Sc
In the past decade, feature selection (FS) studies 
n.
pose a 
fast feature selection method that is specially 
designed for GWR features. In our method, the 
re  (e.g., great-
? ?( ) ( ( ) max ( ))
ij i y i k ih j
l g gE
v
  x x
, ;
?arg max ( )
i
i j i
C j y
s g
v
x
!
.
1,j

hapire, 1999), a variation of perceptron model 
that averages the weights of all iteration loops, 
to improve the generalization performance. 
4 Feature Selection for WR Features 
mainly focus on topical text classificatio
(Yang and Pedersen, 1997) investigate five FS 
metrics and reported that good FS methods 
(such as IG and CHI) can improve the categori-
zation accuracy with an aggressive feature re-
moval. In sentiment classification tasks, tradi-
tional FS methods were also proven to be effec-
tive (Ng et al, 2006; Li et al, 2009). 
With regard to WR features, since the dimen-
sion of feature space has sharply increased, the 
amount of computation is considerably large 
when employing traditional FS methods. 
4.1 Fast MI and Fast IG 
In order to address this problem, we pro
importance of a GWR featu ws
movie) is considered as two component parts: 
the non-back-off word w  (great) and the POS 
pairs s  (J-N). We calculate the score of w  and 
s  respectively using existing FS methods, and 
take the sum of them as the final score. By as-
suming the two parts are mutually independent, 
the im ortance of a relation feature can be taken 
separately. We now give a theoretical support. 
First, the mutual information between a rela-
tion feature ws  and class 
k
c  is defined as 
p
( , )
( , ) log .
( ) ( )
k
k
k
P ws c
I ws c
P ws P c
  (8) 
If w  and s  are independent, they are condi-
tionally inde ndent. Thus e have pe w
( | )
( , ) log
( )
( | ) ( | )
log
( ) ( )
k k
P w c P s c
P w P s
x
( | ) ( | )
log log
( ) ( )
( , ) ( , ).
k
k
k k
k k
P ws c
I ws c
P ws
P w c P s c
P w P s
I w c I s c

 
 
 (9) 
ula (9) indicates that under the assum
tion that two component parts  and 
Form p-
w s  of a 
relation feature  are mutually
the mutual in
ws
formation of the relation feature 
independent,
( , )
k
I ws c  equals the sum of two component 
parts ( , )
k
I w c  and ( , )
k
I s c .
Since the aver  mutual information across 
all classes ( )
age
I ws  is the probabilistic sum of 
ss, it can be written as: each cla
.( ) ( ) ( )I ws I w I sx   (10) 
d Pe
 weighted average of 
Yang an dersen (1997) show that the in-
formation gain ( )G t  is the
( , )
k
I t c  and ( , )
k
I t c
can cons
. Therefore, with the sa
ider the infor
ula
t IG (FIG) respectively. Now let 
 of the independ-
ence assumption. In fact in a rel
tw
me 
mation gain of reason, we 
a relation feature ( )G ws  as the sum of two com-
ponent parts: 
( ) ( )G w G sx   (11) 
We refer to Form (10) and (11) as fast MI 
(FMI) and fas
us look back at the rationality
( )G ws
ation feature, 
o component parts are hardly independent 
since they are ?related?. Nonetheless, if we con-
1339
sider a GWR feature as a combination of the 
non-back-off word and the POS pairs, the as-
sumption will be easier to satisfy. Taking great-
movie (great-N) for example, compared to great
and N, great and J-N are more independent (J-N 
covers some relation information), therefore it is 
more feasible to take ( ) (J-N)G great G  as an 
approximation of ( -N)G great .
Laying aside the assumption, we place em-
phasis on the advantage of FIG (FMI) in com-
putational efficiency. A ension 
of the unigrams feature sp
ssuming the dim
ace is , and ignor-
in
escribed in section 3.2. In 
-
-
on
ense
 the effective-
lection. 
, 2004) is used in 
ment-level polarit
positive and 1,000 
N
N
g the data-sparse problem, the dimension of 
the GWR feature space is 2 5 Nq q  (backing off 
head/modifier word to 5 POS-cluster). Tradi-
tional IG (MI) feature selection needs to calcu-
late the score of all 10 Nq  features, while FIG 
(FMI) only needs to comp  words and 
25 POS pairs. That is to say, FIG (FMI) can 
speed up the computation of traditional IG (MI) 
by at most 10 times. 
4.2 Integration with the Ensemble Model 
We now present how FMI (FIG) is applied to 
the ensemble model d
ute for 
each of the six base-classifiers described in Sec
tion 3.2, feature selection is performed (tradi
tional IG on unigrams, FIG on GWR features).  
Note that when performing FIG on individual 
GWR feature sets, the computation of non-
back-off word ( )G w , is taken care of by having 
already computed IG on unigrams. Thus, we
ly need to compute the score of 25 POS pairs. 
From this point of view, FIG (FMI) is quite 
suitable for the mble model. 
5 Experiments
We first present the performance of system per-
formance, and then demonstrate
ness of fast feature se
5.1 Experimental Setup 
Datasets: The Cornell movie-review dataset 1
introduced by (Pang and Lee
our experiments. It is a docu
dataset that contains 1,000 
y
negative processed reviews. 
1 http://www.cs.cornell.edu/people/pabo/movie-review-data/
We also use the dataset2 introduced in (Joshi 
and Penstein-Ros?, 2009) for comparison. It is a 
su
t and the E-product dataset is 
at
l (McCallum 
ts are 
d ?enko, 2004) is employed. 
Ta
e accuracy. 
gan-
per-
bset (200 sentences each for 11 different 
products) of the product review dataset released 
by (Hu and Liu, 2004). We will refer to it E-
product dataset. 
The Movie dataset is a domain-specific docu-
ment-level datase
sentence-level and cross-domain. We conduct 
experiments on both of them to evaluate our 
approach in a wide range of tasks. 
Classifier: We implement the NB classifier 
based on a multinomial event mode
and Nigam, 1998) with Laplace smoothing. The 
tool LIBSVM3 is chosen as the SVM classifier. 
Setting of kernel function is linear kernel, the 
penalty parameter is set to one, and the Platt?s 
probabilistic output for SVM is applied to ap-
proximate the posterior probabilities. Term 
presence is used as the feature weighting. 
Implementation: The Movie dataset is evenly 
divided into 5 folds, and all the experimen
conducted with a 5-fold cross validation. Fol-
lowing the settings by Joshi and Ros?, an 11-
fold cross validation is applied to E-product 
dataset, where each test fold contains all the 
sentences for one of the 11 products, and the 
sentences for the remaining 10 products are 
used for training. 
For ensemble learning, the stacking frame-
work (D?eroski an
king the Movie dataset for example, in each 
loop of the 5-fold cross validation, the probabil-
istic outputs of the test fold are considered as 
test samples for ensemble leaning; and an inner 
4-fold leave-one-out procedure is applied to the 
training data, where samples in each fold are 
trained on the remaining three folds to obtain 
the probabilistic outputs which serve as training 
samples for ensemble learning. 
All the performance in the remaining tables 
and figures is in terms of averag
5.2 Results of Classification Accuracy 
The results of classification accuracy are or
ized in three parts. We first compare the 
formance of individual WR and GWR; secondly 
we compare joint features and the ensemble 
2 http://www.cs.cmu.edu/~maheshj/datasets/acl09short.html
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
1340
model; thirdly we compare different ensemble 
strategies; finally we make a comparison with 
some related work. 
5.2.1 WR vs. GWR 
Table 3 presents the re
feature sets. Four types 
sults of individual WR 
of WR features, includ-
ing WR-Bi, WR-Dp, GWR-Bi and GWR-Dp, 
are examined under two classification models 
on two datasets. For each of the results, we re-
port the best accuracy under feature selection. 
Model WR Feature Movie E-product
WR-Bi 83.05 63.27 
GWR-Bi 85.55 65.17 
WR-Dp 82.15 65.14 
SVM
GWR-Dp 83.40 67.09 
WR-Bi 84.60 66.86 
GWR-Bi 85.45 67.50 
WR-Dp 83.90 65.68 
NB
GWR-Dp 83.65 67.41 
Table 3: Acc ) of I al WR e 
Sets
formance of individua R and WR. With the 
SV
uracies (% ndividu  Featur
At first, we place the emphasis on the per-
l GW
M model, the performance of GWR features 
is remarkable compared to traditional WR pairs. 
Specifically, on the Movie dataset, GWR-Bi 
outperforms WR-Bi by 2.50%, and GWR-Dp 
outperforms WR-Dp by 1.35%; on the E-
product dataset, the improvements are 1.90% 
and 1.95%. Under the NB model, on the Movie 
dataset, GWR-Bi outperforms WR-Bi by 0.85%; 
on the E-product dataset, GWR-Bi outperforms 
WR-Bi by 0.64% and GWR-Dp outperforms 
WR-Dp by 1.73%. One exception is GWR-Dp 
on the Movie dataset, but the decline is slight 
(0.25%).
WR Feature Movie E-product 
WR-Bi 386k 21k 
GWR-Bi 152k 16k 
WR-Dp 455k 24k 
GWR-Dp 151k 16k 
Table ion of ual Fe Space 
fe -
ag
nt
in 
 s4: Dimen  Individ ature 
Secondly, we compare the dimensions of dif-
rent feature space. Table 4 presents the aver
e size of different types of feature spaces on 
two datasets. On the Movie dataset, the size of 
GWR feature space has been significantly re-
duced (386k vs. 152k in Bi; 455k vs. 151k in 
Dp). On the E-product dataset, since the training 
set are made up by 10 different domains, data 
are quite sparse, therefore, the extent of dimen-
sion reduction is not as sound as that on Movie 
dataset, but still considerable (21k vs. 16k in Bi; 
24k vs. 16k in Dp). 
5.2.2 Joint Features vs. Ensemble Model 
The performance of individual feature sets, joi
feature set and ensemble model is reported 
Table 5. Uni, GWR-Bi and GWR-Dp are used 
as individual features sets in the ensemble 
model, and Joint Features denote the union of 
three individual sets. For feature selection, IG is 
used in Joint Features, and FIG is used in the 
ensemble model. The reported results are in 
terms of the best accuracy under feature selec-
tion.
Feature and Model Movie E-product
SVM 85.20 67.77 
Uni
NB 84.10 66.18 
SVM 85.55 65.17 
GWR-Bi 
NB 85.45 67.50 
SVM 83.40 67.09 
GWR-Dp 
NB 83.65 67.41 
SVM 86.10 66.55 
Joint Features 
NB 85.20 67.64 
AvgP 88.60 70.14 
E
M  
nsemble Model
CE 88.55 70.18 
Table 5: Accuracies  Co t Fe
Joint Feature nsem odel
-
vidual emon-
str
els on different feature 
se
 (%) of mponen atures, 
s and E ble M
To begin with, we observe the results of indi
feature sets. Although we have d
ated that GWR features are more effective 
than WR, it is a pity that they do not show sig-
nificant superiority (sometimes even worse) 
compared to unigrams. That is to say, although 
GWR features encode more generalized word 
relation information than WR features, the role 
of unigrams still can not be replaced. This is in 
accordance with that, WR (GWR) features are 
used as additional features to assist unigrams in 
most of the literature.  
Secondly, we focus on the performance of 
two classification mod
ts. SVM seems to work better than NB on 
unigrams (more than 1%); while on GWR-Bi 
and GWR-Dp feature sets, NB tends to be over-
all effective. This has confirmed our speculation 
that WR features perform better under NB than 
under SVM (since independence between fea-
tures increases) and strengthened the confidence 
1341
of our motivation to ensemble different types of 
features under distinct classification models. 
Finally, we make a comparison of Joint Fea-
tures and Ensemble model. Observing the re-
su
he result of Joint Features is even 
w
ifferent
lts on the Movie dataset, Joint Features ex-
ceed individual feature sets, but the improve-
ments are not remarkable (less than 1 percent-
age compared to the best individual score). 
While the results of the ensemble model, as we 
have expected, are fairly good. AvgP and MCE 
respectively get the scores of 0.886 and 0.8855, 
robustly higher than that of Joint Features 
(0.8610 and 0.8520 respectively under SVM 
and NB). 
On the E-product dataset, it is quite surpris-
ing that t
orse than some of the individual features sets. 
This also confirms that Joint Features are some-
times not so effective at exploring different 
types of features. With regard to the ensemble 
model, AvgP gets an accuracy of 0.7014 and 
MCE achieves the best score (0.7018), consis-
tently superior to the results of Joint Features. 
5.2.3 Different Ensemble Strategies 
We also examine the performance of d
strategies. In Table 6, three ensemble strategies 
are compared, where  ?(Uni & Bi & Dp ) @ 
SVM? denotes ensemble of three kinds of fea-
ture sets with the fixed SVM classifier,  ?Uni @ 
(NB & SVM)? denotes ensemble of two classi-
fiers on fixed unigram features, and ?(Uni & Bi 
& Dp ) @ (NB & SVM)? denotes ensemble of 
both classifiers and feature sets. 
Ensemble Strategy Movie E-product
AveP 86.60 69.50 (Uni & Bi & Dp )  
E@ SVM MC 86.60 69.59 
AveP 87.75 68.95 Uni
@ ( M) NB & SV MCE 87.80 69.14 
AveP 88.60 70.14 (Uni &  Dp ) 
@ (NB & SVM) 
 Bi &
MCE 88.55 70.18 
Table 6: Accuracies  Di Ens
Strategies.
f 
ensemble of either s or classifiers is 
ro
rams
ovie 
e-
lin
t result (0.679) on joint features of 
un
 for GWR 
of MI and 
-
se
 (%) of fferent emble 
Seen from Table 5 and 6, the performance o
 feature set
bustly better than any individual classifier, as 
well as the joint features on both datasets. With 
regard to ensemble of both feature sets and clas-
sification algorithms, it is the most effective 
compared to the above two ensemble strategies. 
This is in accordance with our motivation de-
scribed in Section 3.1. 
5.2.4 Comparison with Related Work 
We take the performance of SVM on unig
as the baseline for comparison. On the M
dataset, Pang and Lee (2004) and Ng et al 
(2006) reported the baseline accuracy of 0.871. 
But our baseline is 2 percentages lower (0.852). 
It is mainly due to that: 1) 0.871 was obtained 
by a 10-fold cross validation, and our result is 
get by 5-fold cross validation; 2) the result of 
the tool LibSVM is inferior of SVMlight by al-
most 1-2 percentages, since the penalty parame-
ter in LibSVM is fixed, while in SVMlight, the 
value is automatically adapted; 3) the baseline 
in Ng et al (2006) is obtained with length nor-
malization which play a role in performance. 
Ng et al reported the state of art best per-
formance (0.905), which outperforms the bas
e (0.871) by 3.4%. Our best result of ensem-
ble model (0.886) gets a comparable improve-
ment (3.40%) compared to our obtained base-
line (0.852).  
On the E-product dataset, Joshi and Ros? re-
ported the bes
igrams and their proposed GWR features. 
This is in accordance with our result of Joint 
Features (0.6655 by SVM and 0.6764 by NB). 
The superiority of our ensemble result is quite 
significant (0.7014 by AvgP and 0.7018 by 
MCE).
5.3 Results of Feature Selection 
In this part, we examine FMI and FIG
feature selection. The performance 
IG are also presented for comparison. The re-
sults on the Movie and E-product datasets are 
displayed in Figures 1 and 2 respectively. Due 
to space limit, we only report the results of 
GWR-Bi features for Movie and GWR-Dp fea-
tures for E-product. In each of the figures, the 
results under NB and SVM are both presented. 
At first, we observe the results of feature se-
lection for GWR-Bi features on the Movie data
t. At first glance, IG and FIG have roughly the 
same performance. IG-based methods are 
shown to be quite effective in GWR feature re-
duction. For example under the NB model, top 
2.5% (4000) GWR-Bi features ranked by IG 
and FIG achieve accuracies of 0.849 and 0.842 
1342
respectively, even better than the score with all 
features (0.8415).
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 150,000
0.6
0.65
0.7
0.75
0.8
0.85
Movie: Bi-wpc @ SVM
Feature number
Ac
cu
ra
cy
IG
FIG
MI
FMI
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 150,000
0.6
0.65
0.7
0.75
0.8
0.85
Movie: Bi-wpc @ NB
Feature number
Ac
cu
ra
cy
IG
FIG
MI
FMI
Figure 1: Feature Selection for GWR-Bi Features on 
the Movie Dataset 
0 2000 4000 6000 8000 10000 12000 14000 16000
0.45
0.5
0.55
0.6
0.65
0.7
E-product: Dp-wpc @ SVM
Feature number
Ac
cu
ra
cy
IG
FIG
MI
FMI
0 2000 4000 6000 8000 10000 12000 14000 16000
0.45
0.5
0.55
0.6
0.65
0.7
E-product: Dp-wpc @ NB
Feature number
Ac
cu
ra
cy WR features for sentiment classification. We 
have proposed a GWR feature extraction ap-
proach and an ensemble model to efficiently 
integrate different types of features. Moreover, 
we have proposed two fast feature selection 
methods (FMI and FIG) for GWR features. 
Individual GWR features outperform tr
IG
FIG
MI
FMI
Figure 2: Feature Selection for GWR-Dp features on 
We then ob  finer granu-
la
er-
fo
 size of E-
pr
omparisons are made ac-
co
he compu
ble model, when per-
o
6 Conclusions and Future Work 
adi-
tio
proved to be a good solution for se-
lecting GWR features. It is also worthy noting 
stu
the E-product dataset
serve IG vs. FIG in a
rity. When the selected features are few (less 
than 5%), IG performs significantly better than 
FIG, while the latter gradually approaches the 
former when the feature number increases: as it 
comes to 10-15%, their performance is quite 
close. From then on, FIG is consistently compa-
rable to IG, even sometimes slightly better.  
With regard to MI and FMI, although the p
rmance compared to IG and FIG is rather poor 
(the reason has been intensively studied by 
Yang and Pedersen, 1997). Our focus is the 
ability of FMI for approximating MI. From this 
point of view, FMI is by contrast effective, es-
pecially with more than 1/3 features. 
Compared to the Movie dataset, the
oduct dataset is much smaller, and the data 
are much sparser. Nevertheless, IG and FIG are 
still effective. On one hand, top 1.25% (2000) 
features ranked by IG yield a result better than 
(or comparable to) that with all features. On the 
other hand, FIG is still competent to be a good 
approximation to IG. 
All of the above c
rding to accuracies, and we now pay attention 
to computational efficiency. Taking the Movie 
dataset for example, IG needs to compute scores 
of information gain for all 152k  features, while 
FIG only needs to comput 5 5k q  scores, 
saving more than 70% of t tational 
load; on the E-product dataset, although the data 
are sparse, the rate of computation reduction is 
still significant (62.5%). 
Note that in the ensem
e 42
f rming FIG for individual GWR feature set, 
part of its inherent complexity is already taken 
care of by having already computed IG on Uni 
feature set, and we only need to compute the 
scores for 25 POS pairs. From this perspective, 
FIG is even more attractive in the ensemble 
model. 
The focus of this paper is exploring the use of 
nal WR features significantly, but they still 
can not totally substitute unigrams. The ensem-
ble model is quite effective at integrating uni-
grams and different types of WR feature, and 
the performance is significantly better than joint 
features.
FIG is 
that FIG is a general feature selection method 
for bigram features, even outside the scope of 
sentiment classification and text classification.  
In the future, we plan to make an in-depth
dy about why individual WR features are 
inferior to unigrams, and how to make the joint 
features more effective. We also plan to extend 
the use of GWR features to the task of transfer 
learning, which we think is a promising direc-
tion for future work. 
1343
Acknowledgment 
We thank Yufeng Chen, Shoushan Li, Ping Jian 
and the anonymous reviewers for valuable com-
ments and helpful suggestions. The research 
work has been partially funded by the Natural 
Science Foundation of China under Grant No. 
60975053, 90820303 and 60736014, the Na-
tional Key Technology R&D Program under 
Grant No. 2006BAH03B02, the Hi-Tech Re-
search and Development Program (?863? Pro-
gram) of China under Grant No. 
2006AA010108-4, and also supported by the 
China-Singapore Institute of Digital Media 
(CSIDM) project under grant No. CSIDM-
200804. 
References
Kushal Dave, Steve Lawrence and David M. Pen-
nock, 2003. Mining the Peanut Gallery: Opinion 
Extraction and Semantic Classification of Product 
Reviews. In Proceedings of the international 
World Wide Web Conference (WWW), pages 
519-528. 
Sa?o D?eroski and Bernard ?enko, 2004. Is combin-
ing classifiers with stacking better than selecting 
the best one? Machine Learning, 54 (3). pages 
255-273. 
Yoav Freund and Robert E. Schapire, 1999. Large 
margin classification using the perceptron algo-
rithm. Machine Learning, 37 (3). pages 277-296. 
Michael Gamon, 2004. Sentiment classification on 
customer feedback data: noisy data, large feature 
vectors, and the role of linguistic analysis. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING). pages 841-847. 
Minqing Hu and Bing Liu, 2004. Mining and sum-
marizing customer reviews. In Proceedings of the 
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168-177. 
Mahesh Joshi and Carolyn Penstein-Ros?, 2009. 
Generalizing dependency features for opinion 
mining. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 313-316. 
Biing-Hwang Juang and Shigeru Katagiri, 1992. 
Discriminative learning for minimum error classi-
fication. IEEE Transactions on Signal Processing, 
40 (12). pages 3043-3054. 
J Kittler, 1998. Combining classifiers: A theoretical 
framework. Pattern Analysis and Applications, 1 
(1). pages 18-27. 
Shoushan Li, Rui Xia, Chengqing Zong and Chu-
Ren Huang, 2009. A framework of feature selec-
tion methods for text categorization. In Proceed-
ings of the Joint Conference of the 47th Annual 
Meeting of the Association for Computational 
Linguistics (ACL), pages 692-700. 
Andrew McCallum and Kamal Nigam, 1998. A com-
parison of event models for naive bayes text clas-
sification. In Proceedings of the AAAI workshop 
on learning for text categorization. 
Vincent Ng, Sajib Dasgupta and S. M. Niaz Arifin, 
2006. Examining the Role of Linguistic Knowl-
edge Sources in the Automatic Identification and 
Classification of Reviews. In Proceedings of the 
COLING/ACL, pages 611-618. 
Bo Pang and Lillian Lee, 2004. A Sentimental Edu-
cation: Sentiment Analysis Using Subjectivity 
Summarization Based on Minimum Cuts. In Pro-
ceedings of the Association for Computational 
Linguistics (ACL), pages 271-278. 
Bo Pang and Lillian Lee, 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2 (1-2). pages 1-135. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan, 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 79-86. 
Yiming Yang and Jan O. Pedersen, 1997. A com-
parative study on feature selection in text catego-
rization. In Proceedings of the 14th International 
Conference on Machine Learning (ICML), pages 
412-420. 
1344
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 521?525,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
 
Dual Training and Dual Prediction for Polarity Classification 
 
Rui Xia, Tao Wang, Xuelei Hu 
Department of Computer Science 
Nanjing University of  
Science and Technology 
rxia@njust.edu.cn,  
linclonwang@163.com, 
xlhu@njust.edu.cn 
Shoushan Li 
NLP Lab 
Department of  
Computer Science 
Soochow University 
shoushan.li 
@gmail.com 
Chengqing Zong 
National Lab of  
Pattern Recognition 
Institute of Automation 
CAS 
cqzong 
@nlpr.ia.ac.cn 
 
 
Abstract 
Bag-of-words (BOW) is now the most popular 
way to model text in machine learning based 
sentiment classification. However, the perfor-
mance of such approach sometimes remains 
rather limited due to some fundamental defi-
ciencies of the BOW model. In this paper, we 
focus on the polarity shift problem, and pro-
pose a novel approach, called dual training and 
dual prediction (DTDP), to address it. The 
basic idea of DTDP is to first generate artifi-
cial samples that are polarity-opposite to the 
original samples by polarity reversion, and 
then leverage both the original and opposite 
samples for (dual) training and (dual) predic-
tion. Experimental results on four datasets 
demonstrate the effectiveness of the proposed 
approach for polarity classification.  
1 Introduction 
The most popular text representation model in 
machine learning based sentiment classification 
is known as the bag-of-words (BOW) model, 
where a piece of text is represented by an unor-
dered collection of words, based on which stand-
ard machine learning algorithms are employed as 
classifiers. Although the BOW model is simple 
and has achieved great successes in topic-based 
text classification, it disrupts word order, breaks 
the syntactic structures and discards some kinds 
of semantic information that are possibly very 
important for sentiment classification. Such dis-
advantages sometimes limit the performance of 
sentiment classification systems.  
A lot of subsequent work focused on feature 
engineering that aims to find a set of effective 
features based on the BOW representation. How-
ever, there still remain some problems that are 
not well addressed. Out of them, the polarity 
shift problem is the biggest one. 
We refer to ?polarity shift? as a linguistic phe-
nomenon that the sentiment orientation of a text 
is reversed (from positive to negative or vice ver-
sa) because of some particular expressions called 
polarity shifters. Negation words (e.g., ?no?, ?not? 
and ?don?t?) are the most important type of po-
larity shifter. For example, by adding a negation 
word ?don?t? to a positive text ?I like this book? 
in front of ?like?, the orientation of the text is 
reversed from positive to negative.  
Naturally, handling polarity shift is very im-
portant for sentiment classification. However, the 
BOW representations of two polarity-opposite 
texts, e.g., ?I like this book? and ?I don?t like this 
book?, are considered to be very similar by most 
of machine learning algorithms. Although some 
methods have been proposed in the literature to 
address the polarity shift problem (Das and Chen, 
2001; Pang et al, 2002; Na et al, 2004; Kenndey 
and Inkpen, 2006; Ikeda et al, 2008; Li and 
Huang, 2009; Li et al, 2010), the state-of-the-art 
results are still far from satisfactory. For example, 
the improvements are less than 2% after consid-
ering polarity shift in Li et al (2010). 
In this work, we propose a novel approach, 
called dual training and dual prediction (DTDP), 
to address the polarity shift problem. By taking 
advantage of the unique nature of polarity classi-
fication, DTDP is motivated by first generating 
artificial samples that are polarity-opposite to the 
original ones. For example, given the original 
sample ?I don?t like this book. It is boring,? its 
polarity-opposite version, ?I like this book. It is 
interesting?, is artificially generated. Second, the 
original and opposite training samples are used 
together for training a sentiment classifier (called 
dual training), and the original and opposite test 
samples are used together for prediction (called 
dual prediction). Experimental results prove that 
the procedure of DTDP is very effective at cor-
recting the training and prediction errors caused 
521
 by polarity shift, and it beats other alternative 
methods of considering polarity shift. 
2 Related Work 
The lexicon-based sentiment classification sys-
tems can be easily modified to include polarity 
shift. One common way is to directly reverse the 
sentiment orientation of polarity-shifted words, 
and then sum up the orientations word by word 
(Hu and Liu, 2004; Kim and Hovy, 2004; Po-
lanyi and Zaenen, 2004; Kennedy and Inkpen, 
2006). Wilson et al (2005) discussed other com-
plex negation effects by using conjunctive and 
dependency relations among polarity words. Alt-
hough handling polarity shift is easy and effec-
tive in term-counting systems, they rarely outper-
form the baselines of machine learning methods 
(Kennedy, 2006). 
The machine learning methods are generally 
more effective for sentiment classification. How-
ever, it is difficult to handle polarity shift based 
on the BOW model. Das and Chen (2001) pro-
posed a method by simply attaching ?NOT? to 
words in the scope of negation, so that in the text 
?I don?t like book?, the word ?like? is changed to 
a new word ?like-NOT?. There were also some 
attempts to model polarity shift by using more 
complex linguistic features (Na et al, 2004; 
Kennedy and Inkpen, 2006). But the improve-
ments upon the baselines of machine learning 
systems are very slight (less than 1%). 
Ikeda et al (2008) proposed a machine learn-
ing method, to model polarity-shifters for both 
word-wise and sentence-wise sentiment classifi-
cation, based on a dictionary extracted from 
General Inquirer. Li and Huang (2009) proposed 
a method first to classify each sentence in a text 
into a polarity-unshifted part and a polarity-
shifted part according to certain rules, then to 
represent them as two bag-of-words for senti-
ment classification. Li et al (2010) further pro-
posed a method to separate the shifted and un-
shifted text based on training a binary detector. 
Classification models are then trained based on 
each of the two parts. An ensemble of two com-
ponent parts is used at last to get the final polari-
ty of the whole text. 
3 The Proposed Approach 
We first present the method for generating artifi-
cial polarity-opposite samples, and then intro-
duce the algorithm of dual training and dual pre-
diction (DTDP). 
3.1 Generating Artificial Polarity-Opposite 
Samples 
Given an original sample and an antonym dic-
tionary (e.g., WordNet 1 ), a polarity-opposite 
sample is generated artificially according to the 
following rules: 
1) Sentiment word reversion: All sentiment 
words out of the scope of negation are re-
versed to their antonyms; 
2) Handling negation: If there is a negation 
expression, we first detect the scope of nega-
tion, and then remove the negation words 
(e.g., ?no?, ?not?, and ?don?t?). The senti-
ment words in the scope of negation are not 
reversed; 
3) Label reversion: The class label of the la-
beled sample is also reversed to its opposite 
(i.e., Positive to Negative, or vice versa) as 
the class label of newly generated samples 
(called polarity-opposite samples). 
Let us use a simple example to explain the 
generation process. Given the original sample: 
The original sample 
Text:   I don?t like this book. It is boring. 
Label: Negative 
According to Rule 1, ?boring? is reversed to 
its antonym ?interesting?; According to Rule 2, 
the negation word ?don?t? is removed, and ?like? 
is not reversed; According to Rule 3, the class 
label Negative is reversed to Positive. Finally, an 
artificial polarity-opposite sample is generated: 
The generated opposite sample 
Text:   I like this book. It is interesting. 
Label: Positive 
All samples in the training and test set are re-
versed to their polarity-opposite versions. We 
refer to them as ?opposite training set? and ?op-
posite test set?, respectively. 
3.2 Dual Training and Dual Prediction 
In this part, we introduce how to make use of the 
original and opposite training/test data together 
for dual training and dual prediction (DTDP). 
Dual Training: Let D = f(xi; yi)gNi=1 and 
~D = f(~xi; ~yi)gNi=1 be the original and opposite 
training set respectively, where x  denotes the 
feature vector, y  denotes the class label, and N  
denotes the size of training set. In dual training, 
D [ ~D  are used together as training data to learn 
                                                 
1 http://wordnet.princeton.edu/ 
522
 a classification model. The size of training data 
is doubled in dual training. 
Suppose the example in Section 3.1 is used as 
one training sample. As far as only the original 
sample (?I don?t like this book. It is boring.?) is 
considered, the feature ?like? will be improperly 
recognized as a negative indicator (since the 
class label is Negative), ignoring the expression 
of negation. Nevertheless, if the generated oppo-
site sample (?I like this book. It is interesting.?) 
is also used for training, ?like? will be learned 
correctly, due to the removal of negation in sam-
ple reversion. Therefore, the procedure of dual 
training can correct some learning errors caused 
by polarity shift. 
Dual Prediction: Given an already-trained 
classification model, in dual prediction, the orig-
inal and opposite test samples are used together 
for prediction. In dual prediction, when we pre-
dict the positive degree of a test sample, we 
measure not only how positive the original test 
sample is, but also how negative the opposite 
sample is.  
Let x  and ~x  denote the feature vector of the 
original and opposite test samples respectively; 
let pd(cjx)  and pd(cj~x)  denote the predictions of 
the original and opposite test sample, based on 
the dual training model. The dual predicting 
function is defined as: 
pd(+jx; ~x) = (1?a)pd(+jx)+apd(?j~x), 
pd(?jx; ~x) = (1?a)pd(?jx)+apd(+j~x), 
where a  (06 a6 1 ) is the weight of the oppo-
site prediction.  
Now suppose the example in Section 3.1 is a 
test sample. As far as only the original test sam-
ple (?I don?t like this book. It is boring.?) is used 
for prediction, it is very likely that it is falsely 
predicted as Positive, since ?like? is a strong pos-
itive feature, despite that it is in the scope of ne-
gation. While in dual prediction, we still measure 
the ?sentiment-opposite? degree of the opposite 
test sample (?I like this book. It is interesting.?). 
Since negation is removed, it is very likely that 
the opposite test sample is assigned with a high 
positive score, which could compensate the pre-
diction errors of the original test sample. 
Final Output: It should be noted that alt-
hough the artificially generated training and test-
ing data are helpful in most cases, they still pro-
duce some noises (e.g., some poorly generated 
samples may violate the quality of the original 
data set). Therefore, instead of using all dual 
predictions as the final output, we use the origi-
nal prediction po(cjx)  as an alternate, in case that 
the dual prediction pd(cjx; ~x)  is not enough con-
fident, according to a confidence threshold t . The 
final output is defined as: 
pf(cjx) =
? pd(cjx; ~x); if?p > tpo(cjx); if?p < t
 
where ?p= pd(cjx; ~x)?po(cjx). 
4 Experimental Study 
4.1 Datasets 
The Multi-Domain Sentiment Datasets2 are used 
for evaluations. They consist of product reviews 
collected from four different domains: Book, 
DVD, Electronics and Kitchen. Each of them 
contains 1,000 positive and 1,000 negative re-
views. Each of the datasets is randomly spit into 
5 folds, with four folds serving as training data, 
and the remaining one fold serving as test data. 
All of the following results are reported in terms 
of an average of 5-fold cross validation. 
4.2 Evaluated Systems 
We evaluate four machine learning systems that 
are proposed to address polarity shift in docu-
ment-level polarity classification: 
1) Baseline: standard machine learning meth-
ods based on the BOW model, without han-
dling polarity shift;  
2) Das-2001: the method proposed by Das and 
Chen (2001), where ?NOT? is attached to the 
words in the scope of negation as a prepro-
cessing step; 
3) Li-2010: the approach proposed by Li et al 
(2010). The details of the algorithm is intro-
duced in related work; 
4) DTDP: our approach proposed in Section 3. 
The WordNet dictionary is used for sample 
reversion. The empirical value of the param-
eter a  and t  are used in the evaluation.  
4.3 Comparison of the Evaluated Systems 
In table 1, we report the classification accuracy 
of four evaluated systems using unigram features. 
We consider two widely-used classification algo-
rithms: SVM and Na?ve Bayes. For SVM, the 
LibSVM toolkit3 is used with a linear kernel and 
the default penalty parameter. For Na?ve Bayes, 
the OpenPR-NB toolkit4 is used. 
                                                 
2 http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
4 http://www.openpr.org.cn  
523
 Dataset 
SVM Na?ve Bayes 
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP 
Book 0.745 0.763 0.760 0.800 0.779 0.783 0.792 0.814 
DVD 0.764 0.771 0.795 0.823 0.795 0.793 0.810 0.820 
Electronics 0.796 0.813 0.812 0.828 0.815 0.827 0.824 0.841 
Kitchen 0.822 0.820 0.844 0.849 0.830 0.847 0.840 0.859 
Avg. 0.782 0.792 0.803 0.825 0.804 0.813 0.817 0.834 
Table 1: Classification accuracy of different systems using unigram features 
Dataset 
SVM Na?ve Bayes 
Baseline Das-2001 Li-2010 DTDP Baseline Das-2001 Li-2010 DTDP 
Book 0.775 0.777 0.788 0.818 0.811 0.815 0.822 0.840 
DVD 0.790 0.793 0.809 0.828 0.824 0.826 0.837 0.868 
Electronics 0.818 0.834 0.841 0.848 0.841 0.857 0.852 0.866 
Kitchen 0.847 0.844 0.870 0.878 0.878 0.879 0.883 0.896 
Avg. 0.808 0.812 0.827 0.843 0.839 0.844 0.849 0.868 
Table 2: Classification accuracy of different systems using both unigram and bigram features 
Compared to the Baseline system, the Das-
2001 approach achieves very slight improve-
ments (less than 1%). The performance of Li-
2010 is relatively effective: it improves the aver-
age score by 0.21% and 0.13% on SVM and Na-
?ve Bayes, respectively. Yet, the improvements 
are still not satisfactory. 
As for our approach (DTDP), the improve-
ments are remarkable. Compared to the Baseline 
system, the average improvements are 4.3% and 
3.0% on SVM and Na?ve Bayes, respectively. In 
comparison with the state-of-the-art (Li-2010), 
the average improvement is 2.2% and 1.7% on 
SVM and Na?ve Bayes, respectively. 
We also report the classification accuracy of 
four systems using both unigrams and bigrams 
features for classification in Table 2. From this 
table, we can see that the performance of each 
system is improved compared to that using uni-
grams. It is now relatively difficult to show im-
provements by incorporating polarity shift, be-
cause using bigrams already captured a part of 
negations (e.g., ?don?t like?).  
The Das-2001 approach still shows very lim-
ited improvements (less than 0.5%), which 
agrees with the reports in Pang et al (2002). The 
improvements of Li-2010 are also reduced: 1.9% 
and 1% on SVM and Na?ve Bayes, respectively.  
Although the improvements of the previous 
two systems are both limited, the performance of 
our approach (DTDP) is still sound. It improves 
the Baseline system by 3.7% and 2.9% on SVM 
and Na?ve Bayes, respectively, and outperforms 
the state-of-the-art (Li-2010) by 1.6% and 1.9% 
on SVM and Na?ve Bayes, respectively. 
5 Conclusions 
In this work, we propose a method, called dual 
training and dual prediction (DTDP), to address 
the polarity shift problem in sentiment classifica-
tion. The basic idea of DTDP is to generate arti-
ficial samples that are polarity-opposite to the 
original samples, and to make use of both the 
original and opposite samples for dual training 
and dual prediction. Experimental studies show 
that our DTDP algorithm is very effective for 
sentiment classification and it beats other alterna-
tive methods of considering polarity shift.  
One limitation of current work is that the tun-
ing of parameters in DTDP (such as a  and t ) is 
not well discussed. We will leave this issue to an 
extended version. 
Acknowledgments 
The research work is supported by the Jiangsu 
Provincial Natural Science Foundation of China 
(BK2012396), the Research Fund for the Doc-
toral Program of Higher Education of China 
(20123219120025), and the Open Project Pro-
gram of the National Laboratory of Pattern 
Recognition (NLPR). This work is also partly 
supported by the Hi-Tech Research and Devel-
opment Program of China (2012AA011102 and 
2012AA011101), the Program of Introducing 
Talents of Discipline to Universities (B13022), 
and the Open Project Program of the Jiangsu Key 
Laboratory of Image and Video Understanding 
for Social Safety (30920130122006).  
524
 References  
S. Das and M. Chen. 2001. Yahoo! for Amazon: 
Extracting market sentiment from stock mes-
sage boards. In Proceedings of the Asia Pacif-
ic Finance Association Annual Conference. 
M. Hu and B. Liu. 2004. Mining opinion features 
in customer reviews. In Proceedings of the 
National Conference on Artificial Intelligence 
(AAAI). 
D. Ikeda, H. Takamura L. Ratinov M. Okumura. 
2008. Learning to Shift the Polarity of Words 
for Sentiment Classification. In Proceedings 
of the International Joint Conference on Natu-
ral Language Processing (IJCNLP).  
S. Kim and E. Hovy. 2004. Determining the sen-
timent of opinions. In Proceeding of the Inter-
national Conference on Computational Lin-
guistics (COLING). 
A. Kennedy and D. Inkpen. 2006. Sentiment 
classification of movie reviews using contex-
tual valence shifters. Computational Intelli-
gence, 22:110?125. 
S. Li and C. Huang. 2009. Sentiment classifica-
tion considering negation and contrast transi-
tion. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Com-
putation (PACLIC). 
S. Li, S. Lee, Y. Chen, C. Huang and G. Zhou. 
2010. Sentiment Classification and Polarity 
Shifting. In Proceeding of the International 
Conference on Computational Linguistics 
(COLING). 
J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 
2004. Effectiveness of simple linguistic pro-
cessing in automatic sentiment classification 
of product reviews. In Proceeding of the Con-
ference of the International Society for 
Knowledge Organization. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. 
Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings 
of the Conference on Empirical Methods in 
Natural Language Processing (EMNLP). 
L. Polanyi and A. Zaenen. 2004. Contextual lex-
ical valence shifters. In Proceedings of the 
AAAI Spring Symposium on Exploring Attitude 
and Affect in Text, AAAI technical report. 
P. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceeding of the 
Annual Meeting of the Association for Compu-
tational Linguistics (ACL). 
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. 
Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of 
the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). 
 
525
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 842?847,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bilingual Event Extraction: a Case Study on Trigger Type Determina-
tion 
Zhu Zhu?  Shoushan Li?*  Guodong Zhou?  Rui Xia? 
 
?Natural Language Processing Lab 
Soochow University, China 
{zhuzhu0020, 
shoushan.li}@gmail.com, 
gdzhou@suda.edu.cn 
 
?Department of Computer Science 
 Nanjing University of Science and 
Technology 
rxia@njust.edu.cn 
 
 
Abstract 
Event extraction generally suffers from the 
data sparseness problem. In this paper, we 
address this problem by utilizing the labeled 
data from two different languages. As a pre-
liminary study, we mainly focus on the sub-
task of trigger type determination in event 
extraction. To make the training data in dif-
ferent languages help each other, we pro-
pose a uniform text representation with bi-
lingual features to represent the samples and 
handle the difficulty of locating the triggers 
in the translated text from both monolingual 
and bilingual perspectives. Empirical studies 
demonstrate the effectiveness of the pro-
posed approach to bilingual classification on 
trigger type determination. ? 
1 Introduction 
Event extraction is an increasingly hot and chal-
lenging research topic in the natural language 
processing (NLP) community (Ahn, 2006; Saun 
et al 2006; Zhao et al 2008). It aims to automat-
ically extract certain types of events with the ar-
guments to present the texts under a structured 
form. In event extraction, there are four primary 
subtasks, named trigger identification, trigger 
type determination, argument identification, and 
argument role determination (Chen and NG, 
2012). As an important technology in infor-
mation extraction, event extraction could be ap-
plied to many fields such as information retrieval, 
summarization, text mining, and question an-
swering. 
Recently, the dominative approach to event 
extraction is based on supervised learning where 
a set of labeled samples are exploited to train a 
model to extract the events. However, the availa-
                                                 
? *  Corresponding author 
ble labeled data are rather sparse due to various 
kinds of event categories. For example, the event 
taxonomy in ACE 2005 1  (Automatic Content 
Extraction) includes 8 types of events, with 33 
subtypes, such as ?Marry/Life? (subtype/type), 
and ?Transport/Movement?. Moreover, some 
subtypes such as ?Nominate/Personnel? and 
?Convict/Justice? contain less than 10 labeled 
samples in the English and Chinese corpus re-
spectively. Apparently, such a small scale of 
training data is difficult to yield a satisfying per-
formance. 
One possible way to alleviate the data sparse-
ness problem in event extraction is to conduct 
bilingual event extraction with training data from 
two different languages. This is motivated by the 
fact that labeled data from a language is highly 
possible to convey similar information in another 
language. For example, E1 is an event sample 
from the English corpus and E2 is another one in 
the Chinese corpus. Apparently, E1 and the Eng-
lish translation text of E2, share some important 
clues such as meet and Iraq which highly indi-
cates the event type of ?Meet/Contact?.  
 
E1: Bush arrived in Saint Petersburg on Sat-
urday, when he also briefly met German chancel-
lor Gerhard Schroeder, whose opposition to the 
Iraq war had soured his relationship with Wash-
ington, at a dinner hosted by Putin. 
E2: ?????????????????
???? ???????????????
????(U.S. president George W. Bush   will 
visit Germany in February and meet with   
Schroeder, Iran and Iraq will be the focus of the   
talks the two sides.) 
 
In this paper, we address the data sparseness 
problem in event extraction with a bilingual pro-
                                                 
1http://www.nist.gov/speech/tests/ace/2005 
842
cessing approach which aims to exploit bilingual 
training data to enhance the extraction perfor-
mance in each language. As a preliminary work, 
we mainly focus on the subtask of trigger type 
determination. Accordingly, our goal is to design 
a classifier which is trained with labeled data 
from two different languages and is capable of 
classifying the test data from both languages. 
Generally, this task possesses two main chal-
lenges.  
The first challenge is text representation, 
namely, how to eliminate the language gap be-
tween the two languages. To tackle this, we first 
employ Google Translate2, a state-of-the-art ma-
chine translation system, to gain the translation 
of an event instance, similar to what has been 
widely done by previous studies in bilingual 
classification tasks e.g., Wan (2008); Then, we 
uniformly represent each text with bilingual 
word features. That is, we augment each original 
feature vector into a novel one which contains 
the translated features.  
The second challenge is the translation for 
some specific features. It is well-known that 
some specific features, such as the triggers and 
their context features, are extremely important 
for determining the event types. For example, in 
E3, both trigger ?left? and named entity ?Sad-
dam? are important features to tell the event type, 
i.e., "Transport/Movement". When it is translated 
to Chinese, it is also required to know trigger ??
??(left) and named entity ????? (Saddam) 
in E4, the Chinese translation of E3.  
 
E3: Saddam's clan is said to have left for a 
small village in the desert. 
E4: Chinese translation: ? ?  ? ? ?
(Saddam) ?? ?? ??(left) ?? ? ? ?? 
? ??? 
 
However, it is normally difficult to know 
which words are the triggers and surrounding 
entities in the translated sentence. To tackle this 
issue, we propose to locate the trigger from both 
monolingual and bilingual perspectives in the 
translation text. Empirical studies demonstrate 
that adding the translation of these specific fea-
tures substantially improves the classification 
performance.  
The remainder of this paper is organized as 
follows. Section 2 overviews the related work on 
event extraction. Section 3 proposes our ap-
                                                 
2 www.google.com 
proach to bilingual event extraction. Section 4 
gives the experimental studies. In Section 5, we 
conclude our work and give some future work. 
2 Related Work  
In the NLP community, event extraction has 
been mainly studied in both English and Chinese. 
In English, various supervised learning ap-
proaches have been explored recently. Bethard 
and Martin (2006) formulate the event identifica-
tion as a classification problem in a word-
chunking paradigm, introducing a variety of lin-
guistically motivated features. Ahn (2006) pro-
poses a trigger-based method. It first identifies 
the trigger in an event, and then uses a multi-
classifier to implement trigger type determina-
tion. Ji and Grishman (2008) employ an ap-
proach to propagate consistent event arguments 
across sentences and documents. Liao and 
Grishman (2010) apply document level infor-
mation to improve the performance of event ex-
traction. Hong et al (2011) leverage cross-entity 
information to improve traditional event extrac-
tion, regarding entity type consistency as a key 
feature. More recently, Li et al (2013) propose a 
joint framework based on structured prediction 
which extracts triggers and arguments together. 
In Chinese, relevant studies in event extraction 
are in a relatively primary stage with focus on 
more special characteristics and challenges. Tan 
et al (2008) employ local feature selection and 
explicit discrimination of positive and negative 
features to ensure the performance of trigger type 
determination. Chen and Ji (2009) apply lexical, 
syntactic and semantic features in trigger label-
ing and argument labeling to improve the per-
formance. More recently, Li et al (2012) and Li 
et al (2013) introduce two inference mechanisms 
to infer unknown triggers and recover trigger 
mentions respectively with morphological struc-
tures.  
In comparison with above studies, we focus on 
bilingual event extraction. Although bilingual 
classification has been paid lots of attention in 
other fields (Wan 2008; Haghighi et al, 2008; 
Ismail et al, 2010; Lu et al, 2011?Li et al, 
2013), there is few related work in event extrac-
tion. The only one related work we find is Ji 
(2009) which proposes an inductive learning ap-
proach to exploit cross-lingual predicate clusters 
to improve the event extraction task with the 
main goal to get the event taggers from extra re-
sources, i.e., an English and Chinese parallel 
corpus. Differently, our goal is to make the la-
843
beled data from two languages help each other 
without any other extra resources, which is origi-
nal in the study of event extraction. 
3 The Proposed Approach 
Trigger type determination aims to determine the 
event type of a trigger given the trigger and its 
context (e.g., a sentence). Existing approaches to 
trigger type determination mainly focus on mon-
olingual classification. Figure 1 illustrates the 
framework for Chinese and English. 
In comparison, our approach exploits the cor-
pora from two different languages. Figure 2 illus-
trates the framework. As shown in the figure, we 
first get the translated corpora of Chinese and 
English origin corpora through machine transla-
tion. Then, we represent each text with bilingual 
features, which enables us to merge the training 
data from both languages so as to make them 
help each other. 
 
Figure 1: The framework of monolingual classifi-
cation for trigger type determination 
 
Figure 2: The framework of bilingual classification 
for trigger type determination 
3.1 Text Representation  
In a supervised learning approach, labeled data is 
trained to obtain a classifier. In this approach, the 
extracted features are the key components to 
make a successful classifier. Table 1 shows some 
typical kinds of features in a monolingual classi-
fication task for trigger type determination. To 
better understand these features, the real feature 
examples in E3 are given in the table. 
Given the feature definition, a monolingual 
sample x  is represented as the combination of all 
the features, i.e.,  
1 2, , , , _ , _ ,
_ , , _ , _
ne e e Tri POS Tri Tri conx POS con Ent Ent type Ent subtype
? ?? ? ?? ?
  (1) 
Features Feature examples in E3 
All words 
(
1 2, , ne e e ) 
Saddam, clan, is, ... , 
desert 
Trigger (Tri) left 
POS of the trigger 
(POS_Tri) 
VBN 
Trigger's context 
words (Tri_con) 
...,have, for,... 
POS of trigger's 
context words 
(POS_con) 
...,VB,IN,? 
Entities around trig-
ger (Ent) 
Saddam 
Entity type 
(Ent_type) 
PER 
Entity subtype 
(Ent_subtype) 
individual 
Table 1: The features and some feature examples for 
trigger type determination 
 
In bilingual classification, we represent a sam-
ple with bilingual features, which makes it possi-
ble to train with the data from two languages. To 
achieve this goal, we employ a single feature 
augmentation strategy to augment the monolin-
gual features into bilingual features, i.e.,  
,Chinese Englishx x x?
                      (2) 
Specifically, a sample x  is represented as fol-
lows: 
1 2
1 2
, , , , _ , _ ,
_ , , _ , _
, , , , , _ , _ ,
_ , , _ , _
m c c c
c c c
n e e e
e e e
c c c Tri POS Tri Tri con
POS con Ent Ent type Ent subtype
x
e e e Tri POS Tri Tri con
POS con Ent Ent type Ent subtype
? ?? ?
? ?? ?
? ?? ?? ? ?? ?? ?? ?? ?? ?? ?
  (3) 
Where the tokens with the ?c?/?e? subscript mean 
the features generated from the Chinese/English 
text. From the features, we can see that some 
Classifier Results 
Chinese event 
corpus 
Machine trans-
lation 
Translated 
samples 
Text representation 
Translated 
samples 
English event 
corpus 
Machine trans-
lation 
Text representation 
Samples with 
bilingual features 
Samples with 
bilingual features 
Trigger type determination 
for Chinese 
Trigger type determination 
for English 
Chinese event 
corpus 
Classifier 
English 
event corpus 
Classifier 
Results Results 
844
features, such as Tri_con and Ent, depend on the 
location of the trigger word. Therefore, locating 
the trigger in the translated text becomes crucial.  
3.2 Locating Translated Trigger 
Without loss of generality, we consider the case 
of translating a Chinese event sample into an 
English one. Formally, the word sequence of a 
Chinese event sample is denoted as 
1 2( , , , )c ns c c c? , while the sequence of the 
translated one is denoted as
1 2( , , )e ms e e e? . 
Then, the objective is to get the English trigger 
eTri  in es , given the Chinese trigger word  
cTri in cs . The objective function is given as fol-
lows:  
? ?_1 ,argmax k l ek l m P e Tri? ? ?
                 (4) 
Where 
_k le
 denotes the substring 
1( , , )k k le e e?  
in 
es  and 1 ,k l m? ? . 
In this paper, the above function could be 
solved in two perspectives: monolingual and bi-
lingual ones. The former uses the English train-
ing data alone to locate the trigger while the lat-
ter exploit the bilingual information to get the 
translated counterpart of the Chinese trigger. 
The monolingual perspective: The objective 
is to locate the trigger with the monolingual in-
formation. That is,  
? ?_1 ,argmax | ,k l e e ek l m P e Tri s R? ? ?
           (5) 
Where 
eR  denotes the training resource in Eng-
lish. In fact, this task is exactly the first subtask 
in event extraction named trigger identification, 
as mentioned in Introduction. For a simplified 
implementation, we first estimate the probabili-
ties of ? ?_k l eP e Tri?  in eR  with maximum like-
lihood estimation when 
_k l ee s?
.  
The bilingual perspective: The objective is to 
locate the trigger with the bilingual information. 
That is, 
? ?_1 ,argmax | , ,k l e e c ck l m P e Tri s s Tri? ? ?
        (6) 
Where 
cTri  is the trigger word in Chinese and es  
is the translated text towards 
cs . More generally, 
this can be solved from a standard word align-
ment model in machine translation (Och et al 
1999; Koehn et al 2003). However, training a 
word alignment requires a huge parallel corpus 
which is not available here.  
 For a simplified implementation, we first get 
the 
cTri ?s translation? denoted as cTritrans
?
with Google Translate. Then, we estimate 
? ?_k l eP e Tri?  as follows:  
? ? __ 0.9 ck l Trik l e if e transP e Tri others?
???? ? ???
    (7) 
Where 0.9 is an empirical value which makes the 
translation probability become a dominative fac-
tor when the translation of the trigger is found in 
the translated sentence. ?  is a small value which 
makes the sum of all probabilities equals 1.   
The final decision is made according to both 
the monolingual and bilingual perspectives, i.e., 
? ?
? ?
_
1 ,
_
arg max  | ,
              | , ,
k l e e e
k l m
k l e e c c
P e Tri s R
P e Tri s s Tri
? ?
?
? ?
        (8) 
Note that we reduce the computational cost by 
make the word length of the trigger less than 3, 
i.e., 3l k? ? . 
4 Experimentation 
4.1 Experimental Setting  
Data sets: The Chinese and English corpus for 
even extraction are from ACE2005, which in-
volves 8 types and 33 subtypes. All our experi-
ments are conducted on the subtype case. Due to 
the space limit, we only report the statistics for 
each type, as shown in Table 2. For each subtype, 
80% samples are used as training data while the 
rest are as test data. 
 
# Chinese English total 
Life 389 902 1291 
Movement 593 679 1272 
Transaction 147 379 526 
Business 144 137 281 
Conflict 514 1629 2143 
Contact 263 373 636 
Personnel 203 514 717 
Justice 457 672 1129 
total 2710 5285 7995 
Table 2: Statistics in each event type in both Chinese 
and English data sets 
 
Features: The features have been illustrated in 
Table 1 in Section 3.2.  
845
Classification algorithm: The maximum en-
tropy (ME) classifier is implemented with the 
public tool, Mallet Toolkits3 . 
Evaluation metric: The performance of event 
type recognition is evaluated with F-score. 
4.2 Experimental Results  
In this section, we evaluate the performance of 
our approach to bilingual classification on trigger 
type determination. For comparison, following 
approaches are implemented: 
? Monolingual: perform monolingual classi-
fication on the Chinese and English corpus 
individually, as shown in Figure 1. 
? Bilingual: perform bilingual classification 
with partial bilingual features, ignoring the 
context features (e.g., context words, con-
text entities) under the assumption that the 
trigger location task is not done. 
? Bilingual_location: perform bilingual clas-
sification by translating each sample into 
another language and using a uniform repre-
sentation with all bilingual features as 
shown in Section 3.2. This is exactly our 
approach. The number of the context words 
and entities before or after the trigger words 
is set as 3. 
0.658
0.706
0.677
0.679
0.678
0.734
0.62
0.64
0.66
0.68
0.7
0.72
0.74
Chinese Test Data English Test Data
F
-
s
c
o
r
e
Monolingual Bilingual Bilingual_location
 
Figure 3: Performance comparison of the three ap-
proaches on the Chinese and English test data 
 
Figure 3 shows the classification results of the 
three approaches on the Chinese and English test 
data. From this figure, we can see that Bilin-
gual_location apparently outperform Monolin-
gual, which verifies the effectiveness of using 
bilingual corpus. Specifically, the improvement 
by our approach in Chinese is impressive, reach-
ing 7.6%. The results also demonstrate the im-
portance of the operation of the trigger location, 
                                                 
3 http://mallet.cs.umass.edu/   
without which, bilingual classification can only 
slightly improve the performance, as shown in 
the English test data.  
The results demonstrate that our bilingual 
classification approaches are more effective for 
the Chinese data. This is understandable because 
the size of English data is much larger than that 
of Chinese data, 5285 vs. 2710, as shown in Ta-
ble 2. Specifically, after checking the results in 
each subtype, we find that some subtypes in Chi-
nese have very few samples while corresponding 
subtypes in English have a certain number sam-
ples. For example, the subtype of 
?Elect/Personnel? only contains 30 samples in 
the Chinese data while 161 samples can be found 
in the English data, which leads a very high im-
provement (15.4%) for the Chinese test data. In 
summary, our bilingual classification approach 
provides an effective way to handle the data 
sparseness problem in even extraction. 
5 Conclusion and Future Work 
This paper addresses the data sparseness problem 
in event extraction by proposing a bilingual clas-
sification approach. In this approach, we use a 
uniform text representation with bilingual fea-
tures and merge the training samples from both 
languages to enlarge the size of the labeled data. 
Furthermore, we handle the difficulty of locating 
the trigger from both the monolingual and bilin-
gual perspectives. Empirical studies show that 
our approach is effective in using bilingual cor-
pus to improve monolingual classification in 
trigger type determination.  
Bilingual event extraction is still in its early 
stage and many related research issues need to be 
investigated in the future work. For example, it is 
required to propose novel approaches to the bi-
lingual processing tasks in other subtasks of 
event extraction. Moreover, it is rather challeng-
ing to consider a whole bilingual processing 
framework when all these subtasks are involved 
together.  
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61375073, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) project No.543810 and one Early Career 
Scheme (ECS) project No.559313 sponsored by 
the Research Grants Council of Hong Kong, the 
NSF grant of Zhejiang Province No.Z1110551. 
846
References  
Ahn D. 2006. The Stages of Event Extraction. In Pro-
ceedings of the Workshop on Annotating and Rea-
soning about Time and Events, pp.1~8. 
Bethard S. and J. Martin. 2006. Identification of 
Event Mentions and Their Semantic Class. In Pro-
ceedings of EMNLP-2006, pp.146-154. 
Chen C. and V. NG. 2012. Joint Modeling for Chi-
nese Event Extraction with Rich Linguistic Fea-
tures. In Proceedings of COLING-2012, pp. 529-
544. 
Chen Z. and H. Ji. 2009. Language Specific Issue and 
Feature Exploration in Chinese Event Extraction. 
In Proceedings of NAACL-2009, pp. 209-212. 
Haghighi A., P. Liang, T. Berg-Kirkpatrick and D. 
Klein. 2008. Learning Bilingual Lexicons from 
Monolingual Corpora. In Proceedings of ACL-
2008, pp. 771-779. 
Hong Y., J. Zhang., B. Ma., J. Yao., and G. Zhou. 
2011. Using Cross-Entity Inference to Improve 
Event Extraction. In Proceedings of ACL-2011, pp. 
1127?1136. 
Ismail A., and S. Manandhar. 2010. Bilingual Lexicon 
Extraction from Comparable Corpora Using In-
domain Terms. In Proceedings of COLING-2010, 
pp.481-489. 
Ji H. 2009. Cross-lingual Predicate Cluster Acquisi-
tion to Improve Bilingual Event Extraction by In-
ductive Learning. In Proceedings of the Workshop 
on Unsupervised and Minimally Supervised Learn-
ing of Lexical Semantics, pp. 27-35. 
Ji H, and R. Grishman. 2008. Refining Event Extrac-
tion through Cross-Document Inference. In Pro-
ceedings of ACL-2008, pp. 254-262. 
Koehn P., F. Och, and D. Marcu. 2003. Statistical 
Phrase-based Translation. In Proceedings of HTL-
NAACL-2003, pp. 127-133. 
Li P., and G. Zhou. 2012. Employing Morphological 
Structures and Sememes for Chinese Event Extrac-
tion. In Proceedings of COLING-2012, pp. 1619-
1634. 
Li P., Q. Zhu and G. Zhou. 2013. Using Composition-
al Semantics and Discourse Consistency to Im-
prove Chinese Trigger Identification. In Proceed-
ings of COLING-2013, pp. 399-415. 
Li Q, H Ji, and H. Liang. 2013. Joint Event Extraction 
via Structured Prediction with Global Features. In 
Proceedings of ACL-2013, pp. 73-82. 
Li S, R Wang, H Liu, and CR Huang. 2013. Active 
Learning for Cross-Lingual Sentiment Classifica-
tion. In Proceedings of Natural Language Pro-
cessing and Chinese Computing, pp. 236-246. 
Liao S and R. Grishman. 2010. Using Document Lev-
el Cross-event Inference to Improve Event Extrac-
tion. In Proceedings of ACL-2010, pp. 789-797. 
Lu B., C. Tan, C. Cardie and B. K. Tsou. 2011. Joint 
Bilingual Sentiment Classification with Unlabeled 
Parallel Corpora. In Proceedings of ACL-2011, pp. 
320-330.  
Och F., C. Tillmann, and H. Ney. 1999. Improved 
Alignment Models for Statistical Machine Transla-
tion. In Proceedings of EMNLP-1999, pp.20-28. 
Tan H., T. Zhao, and J. Zheng. 2008. Identification of 
Chinese Event and Their Argument Roles. In Pro-
ceedings of  CITWORKSHOPS-2008,  pp. 14-19. 
Wan X. 2008. Using Bilingual Knowledge and En-
semble Techniques for Unsupervised Chinese Sen-
timent Analysis. In  Proceedings of EMNLP-2008, 
pp. 553-561. 
Zhao Y., Y. Wang, B. Qin, et al 2008. Research on 
Chinese Event Extraction. In Proceedings of Jour-
nal of  Chinese Information, 22(01), pp. 3-8. 
847
