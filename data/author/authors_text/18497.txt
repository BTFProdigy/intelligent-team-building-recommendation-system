Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857?862,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Word Level Language Identification in Online Multilingual Communication
Dong Nguyen1 A. Seza Dog?ruo?z23
(1) Human Media Interaction, University of Twente, Enschede, The Netherlands
(2) Tilburg School of Humanities, Tilburg University, Tilburg, The Netherlands
(3) Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA
dong.p.ng@gmail.com, a.s.dogruoz@gmail.com
Abstract
Multilingual speakers switch between lan-
guages in online and spoken communication.
Analyses of large scale multilingual data re-
quire automatic language identification at the
word level. For our experiments with mul-
tilingual online discussions, we first tag the
language of individual words using language
models and dictionaries. Secondly, we incor-
porate context to improve the performance.
We achieve an accuracy of 98%. Besides word
level accuracy, we use two new metrics to
evaluate this task.
1 Introduction
There are more multilingual speakers in the world
than monolingual speakers (Auer and Wei, 2007).
Multilingual speakers switch across languages in
daily communication (Auer, 1999). With the in-
creasing use of social media, multilingual speakers
also communicate with each other in online environ-
ments (Paolillo, 2011). Data from such resources
can be used to study code switching patterns and lan-
guage preferences in online multilingual conversa-
tions. Although most studies on multilingual online
communication rely on manual identification of lan-
guages in relatively small datasets (Danet and Her-
ring, 2007; Androutsopoulos, 2007), there is a grow-
ing demand for automatic language identification in
larger datasets. Such a system would also be useful
for selecting the right parsers to process multilingual
documents and to build language resources for mi-
nority languages (King and Abney, 2013).
In this paper, we identify Dutch (NL) en Turkish
(TR) at the word level in a large online forum for
Turkish-Dutch speakers living in the Netherlands.
The users in the forum frequently switch languages
within posts, for example:
<TR> Sariyi ver </TR>
<NL> Wel mooi doelpunt </NL>
So far, language identification has mostly been mod-
eled as a document classification problem. Most ap-
proaches rely on character or byte n-grams, by com-
paring n-gram profiles (Cavnar and Trenkle, 1994),
or using various machine learning classifiers. While
McNamee (2005) argues that language identification
is a solved problem, classification on a more fine-
grained level (instead of document level) remains a
challenge (Hughes et al, 2006). Furthermore, lan-
guage identification is more difficult for short texts
(Baldwin and Lui, 2010; Vatanen et al, 2010), such
as queries and tweets (Bergsma et al, 2012; Carter
et al, 2012; Ceylan and Kim, 2009). Tagging in-
dividual words (without context) has been done us-
ing dictionaries, affix statistics and classifiers us-
ing character n-grams (Hammarstro?m, 2007; Got-
tron and Lipka, 2010). Although Yamaguchi and
Tanaka-Ishii (2012) segmented text by language,
their data was artificially created by randomly sam-
pling and concatenating text segments (40-160 char-
acters) from monolingual texts. Therefore, the lan-
guage switches do not reflect realistic switches as
they occur in natural texts. Most related to ours is
the work by King and Abney (2013) who labeled
languages of words in multilingual web pages, but
evaluated the task only using word level accuracy.
857
Our paper makes the following contributions: 1)
We explore two new ways to evaluate the task for an-
alyzing multilingual communication and show that
only word accuracy gives a limited view 2) We are
the first to apply this task on a conversational and
larger dataset 3) We show that features using the
context improve the performance 4) We present a
new public dataset to support research on language
identification.
In the rest of the paper, we first discuss the related
work and describe our dataset. Secondly, we present
our experiments. We finally conclude with a sum-
mary and suggestions for future work.
2 Corpus
Our data1 comes from one of the largest online
communities in The Netherlands for Turkish-Dutch
speakers. All posts from May 2006 until October
2012 were crawled. Although Dutch and Turkish
dominate the forum, English fixed phrases (e.g. no
comment, come on) are also occasionally observed.
Users switch between languages within and across
posts. Examples 1 and 2 illustrate switches between
Dutch and Turkish within the same post. Example 1
is a switch at sentence level, example 2 is a switch
at word level.
Example 1:
<NL>Mijn dag kan niet stuk :) </NL>
<TR> Cok guzel bir haber aldim </TR>
Translation: <NL> This made my day:)
</NL><TR> I received good news
</TR>
Example 2:
<TR>kahvalti</TR><NL>met
vriendinnen by my thuis </NL>
Translation: <TR>breakfast </TR>
<NL> with my girlfriends at my home
</NL>
The data is highly informal with misspellings,
lengthening of characters (e.g. hotttt), replacement
of Turkish characters (kahvalti instead of kahvalt?)
and spelling variations (tankyu instead of thank
you). Dutch and Turkish sometimes share common
spellings (e.g. ben is am in Dutch and I in Turkish),
making this a challenging task.
1Available at http://www.dongnguyen.nl/data-langid-
emnlp2013.html
Annotation
For this research, we classify words as either Turkish
or Dutch. Since Dutch and English are typologically
more similar to each other than Turkish, the English
phrases (less than 1%) are classified as Dutch. Posts
were randomly sampled and annotated by a native
Turkish speaker who is also fluent in Dutch. A na-
tive Dutch speaker annotated a random set of 100
posts (Cohen?s kappa = 0.98). The following tokens
were ignored for language identification:
? Smileys (as part of the forum markup, as well
as textual smileys such as ?:)? ).
? Numeric tokens and punctuation.
? Forum tags (e.g. [u] to underline text).
? Links, images, embedded videos etc.
? Turkish and Dutch first names and place
names2.
? Usernames when indicated with special forum
markup.
? Chat words, such as hahaha, ooooh and lol rec-
ognized using regular expressions.
Posts for which all tokens are ignored, are not
included in the corpus.
Statistics
The dataset was randomly divided into a training,
development and test set. The statistics are listed
in Table 1. The statistics show that Dutch is the
majority language, although the difference between
Turkish and Dutch is not large. We also find that the
documents (i.e. posts) are short, with on average 18
tokens per document. The data represents realistic
texts found in online multilingual communication.
Compared to previously used datasets (Yamaguchi
and Tanaka-Ishii, 2012; King and Abney, 2013), the
data is noisier and the documents are much shorter.
#NL tokens #TR tokens #Posts/(BL%)
Train 14900 (54%) 12737 (46%) 1603 (15%)
Dev 8590 (51%) 8140 (49%) 728 (19%)
Test 5895 (53%) 5293 (47%) 735 (17%)
Table 1: Number of tokens and posts for Dutch (NL) and
Turkish (TR), including % of bilingual (BL) posts
2Based on online name lists and Wikipedia pages
858
3 Experimental Setup
3.1 Training Corpora
We used the following corpora to extract dictionaries
and language models.
? GenCor: Turkish web pages (Sak et al, 2008).
? NLCOW2012: Dutch web pages (Scha?fer and
Bildhauer, 2012).
? Blog authorship corpus: English blogs (Schler
et al, 2006).
Each corpus was chunked into large segments
which were then selected randomly until 5M tokens
were obtained for each language. We tokenized the
text and kept the punctuation.
3.2 Baselines
As baselines, we use langid.py3 (Lui and Bald-
win, 2012) and van Noord?s TextCat implementa-
tion4 of the algorithm by Cavnar and Trenkle (1994).
TextCat is based on the comparison of n-gram pro-
files and langid.py on Naive Bayes with n-gram fea-
tures. For both baselines, words were entered indi-
vidually to each program. Words for which no lan-
guage could be determined were assigned to Dutch.
These models were developed to identify the lan-
guages of the documents instead of words and we
did not retrain them. Therefore, these models are
not expected to perform well on this task.
3.3 Models
We start with models that assign languages based on
only the current word. Next, we explore models and
features that can exploit the context (the other words
in the post). Words with the highest probability for
English were assigned to Dutch for evaluation.
Dictionary lookup (DICT)
We extract dictionaries with word frequencies from
the training corpora. This approach looks up the
words in the dictionaries and chooses the language
for which the word has the highest probability. If
the word does not occur in the dictionaries, Dutch is
chosen as the language.
3https://github.com/saffsd/langid.py
4http://www.let.rug.nl/?vannoord/TextCat/
Language model (LM)
We build a character n-gram language model for
each language (max. n-gram length is 5). We use
Witten-Bell smoothing and include word boundaries
for calculating the probabilities.
Dictionary + Language model (DICT+LM)
We first use the dictionary lookup approach (DICT).
If the word does not occur in dictionaries, a decision
is made using the language models (LM).
Logistic Regression (LR)
We use a logistic regression model that incorporates
context with the following features:
? (Individual word) Label assigned by the
DICT+LM model.
? (Context) The results of the LM model based on
previous + current token, and current token +
next token (e.g. the sequence ?ben thuis? (am
home) as a whole if ben is the current token).
This gives the language model more context for
estimation. We compare the use of the assigned
labels (LAB) with the use of the log probability
values (PROB) as feature values.
Conditional Random Fields (CRF)
We treat the task as a sequence labeling problem and
experiment with linear-chain Conditional Random
Fields (Lafferty et al, 2001) in three settings:
? (Individual word) A CRF with only the tags as-
signed by the DICT+LM to the individual to-
kens as a feature (BASE).
? (Context). CRFs using the LAB or PROB as ad-
ditional features (same features as in the logis-
tic regression model) to capture additional con-
text.
3.4 Implementation
Language identification was not performed for texts
within quotes. To handle the alphabetical length-
ening (e.g. lolllll), words are normalized by trim-
ming same character sequences of three characters
or more. We use the Lingpipe5 and Scikit-learn (Pe-
dregosa et al, 2011) toolkits for our experiments.
5http://alias-i.com/lingpipe/
859
Word classification Fraction Post classification
TR NL MAE
Run P R P R Acc. ? All Mono. BL F1 Acc.
Textcat 0.872 0.647 0.743 0.915 0.788 0.739 0.251 0.264 0.188 0.386 0.396
LangIDPy 0.954 0.387 0.641 0.983 0.701 0.615 0.364 0.371 0.333 0.413 0.475
DICT 0.955 0.733 0.802 0.969 0.858 0.827 0.196 0.200 0.175 0.511 0.531
LM 0.950 0.930 0.938 0.956 0.944 0.926 0.074 0.076 0.065 0.699 0.703
DICT + LM 0.951 0.934 0.942 0.957 0.946 0.943 0.067 0.067 0.063 0.711 0.717
LR + LAB 0.965 0.952 0.958 0.969 0.961 0.917 0.066 0.066 0.068 0.791 0.808
LR + PROB 0.956 0.976 0.978 0.959 0.967 0.945 0.048 0.044 0.064 0.826 0.849
CRF + BASE 0.973 0.974 0.977 0.976 0.975 0.940 0.043 0.027 0.119 0.858 0.898
CRF + LAB 0.964 0.977 0.979 0.967 0.972 0.933 0.046 0.033 0.111 0.855 0.891
CRF + PROB 0.970 0.980 0.982 0.973 0.976 0.946 0.039 0.025 0.103 0.853 0.895
Table 2: Results of language identification experiments.
3.5 Evaluation
The assigned labels can be used for computational
analysis of multilingual data in different ways. For
example, these labels can be used to analyze lan-
guage preferences in multilingual communication or
the direction of the switches (from Turkish to Dutch
or the other way around). Therefore, we evaluate the
methods from different perspectives.
The evaluation at word and post levels is done
with the following metrics:
? Word classification precision (P), recall (R) and
accuracy. Although this is the most straightfor-
ward approach to evaluate the task, it ignores
the document boundaries.
? Fraction of language in a post: Pearson?s cor-
relation (?) and Mean Absolute Error (MAE) of
proportion of Turkish in a post. This evaluates
the measured proportion of languages in a post
when the actual tags for individual words are
not needed. For example, such information is
useful for analyzing the language preferences
of users in the online forum. Besides report-
ing the MAE over all posts, we also separate
the performance over monolingual and bilin-
gual posts (BL).
? Post classification: Durham (2003) analyzed
the switch between languages in terms of the
amount of monolingual and bilingual posts.
Our posts are classified as NL, TR or bilingual
(BL) if all words are tagged in the particular
language or both. We report F1 and accuracy.
4 Results
The results are presented in Table 2. Significance
tests were done by comparing the results of the word
and post classification measures using McNemar?s
test, and comparing the MAEs using paired t-tests.
All runs were significantly different from each other
based on these tests (p < 0.05), except the MAEs of
the DICT+LM and LR+LAB runs and the MAEs and
post classification metrics between the CRFs runs.
The difficulty of the task is illustrated by exam-
ining the coverage of the tokens by the dictionaries.
24.6% of the tokens (dev + test set) appear in both
dictionaries, 31.1% only in the Turkish dictionary,
30.5% only in the Dutch dictionary and 13.9% in
none of the dictionaries.
The baselines do not perform well. This confirms
that language identification at the word level needs
different approaches than identification at the docu-
ment level. Using language models result in a bet-
ter performance than dictionaries. They can han-
dle unseen words and are more robust against the
noisy spellings. The combination of language mod-
els and dictionaries is more effective than the indi-
vidual models. The results improve when context
was added using a logistic regression model, espe-
cially with the probability values as feature values.
CRFs improve the results but the improvement
on the correlation and MAE is less. More specifi-
cally, CRFs improve the performance on monolin-
gual posts, especially when a single word is tagged
in the wrong language. However, when the influence
of the context is too high, CRFs reduce the perfor-
mance in bilingual posts.
860
This is also illustrated with the results of the post
classification. The LR+PROB run has a high recall
(0.905), but a low precision (0.559) for bilingual
posts, while the CRF+PROB approach has a low re-
call (0.611) and a high precision (0.828).
The fraction of Dutch and Turkish in posts varies
widely, providing additional challenges to the use of
CRFs for this task. Classifying posts first as mono-
lingual/bilingual and tagging individual words after-
wards for bilingual posts might improve the perfor-
mance.
The evaluation metrics highlight different aspects
of the task whereas word level accuracy gives a
limited view. We suggest using multiple metrics to
evaluate this task for future research.
Dictionaries versus Language Models
The results reported in Table 2 were obtained by
sampling 5M tokens of each language. To study the
effect of the number of tokens on the performance
of the DICT and LM runs, we vary the amount of
data. The performance of both methods increases
consistently with more data (Figure 1). We also
find that language models achieve good performance
with only a limited amount of data, and consistently
outperform the approach using dictionaries. This is
probably due to the highly informal and noisy nature
of our data.
Num. sampled tokens
Accu
racy
0.6
0.7
0.8
0.9
1.0
0 2 ? 10
6
4 ? 10
6
LM
DICT
Figure 1: Effect of sampling size
Post classification
We experimented with classifying posts into TR, NL
and bilingual using the results of the word level lan-
guage identification (Table 2: post classification).
Posts were classified as a particular language if all
words were tagged as belonging to that language,
and bilingual otherwise. Runs using CRFs achieved
the best performance.
We now experiment with allowing a margin (e.g.
a margin of 0.10 classifies posts as TR if at least
90% of the words are classified as TR). Allowing
a small margin already increases the results of sim-
pler approaches (such as the LR-PROB run, Table 3)
by making it more robust against errors. However,
allowing a margin reduces the performance of the
CRF runs.
Margin 0.0 0.05 0.10 0.15 0.20
Accuracy 0.849 0.873 0.876 0.878 0.865
Table 3: Effect of margin on post classification
(LR-PROB run)
Error analysis
The manual analysis of the results revealed three
main challenges: 1) Our data is highly informal
with many spelling variations (e.g. moimoimoi,
goooooooooooolllll) and noise (e.g. asdfghjfgsha-
haha) 2) Words sharing spelling in Dutch and Turk-
ish are difficult to identify especially when there
is no context available (e.g. a post with only one
word). These words are annotated based on their
context. For example, the word super in ?Seyma,
super? is annotated as Turkish since Seyma is also a
Turkish word. 3) Named entity recognition is neces-
sary to improve the performance of the system and
decrease the noise in evaluation. Based on precom-
piled lists, our system ignores named entities. How-
ever, some names still remain undetected (e.g. user-
names).
5 Conclusion
We presented experiments on identifying the lan-
guage of individual words in multilingual conversa-
tional data. Our results reveal that language models
are more robust than dictionaries and adding context
improves the performance. We evaluate our methods
from different perspectives based on how language
identification at word level can be used to analyze
multilingual data. The highly informal spelling in
online environments and the occurrences of named
entities pose challenges.
Future work could focus on cases with more than
two languages, and languages that are typologically
less distinct from each other or dialects (Trieschnigg
et al, 2012).
861
6 Acknowledgements
The first author was supported by the Netherlands
Organization for Scientific Research (NWO) grant
640.005.002 (FACT) and the second author through
a postdoctoral research grant in E-Humanities (Digi-
tal Humanities) by Tilburg University (NL). The au-
thors would like to thank Marie?t Theune and Dolf
Trieschnigg for feedback.
References
J. Androutsopoulos, 2007. The multilingual internet.
Language, Culture and communication online, chapter
Language choice and code-switching in German-based
diasporic web-forums., pages 340?361. Oxford: Ox-
ford University Press.
P. Auer and L. Wei. 2007. Introduction: Multilingual-
ism as a problem? Monolingualism as a problem? In
Handbook of Multilingualism and Multilingual Com-
munication, volume 5 of Handbooks of Applied Lin-
guistics, pages 1?14. Mouton de Gruyter.
P. Auer. 1999. From codeswitching via language mix-
ing to fused lects toward a dynamic typology of bilin-
gual speech. International Journal of Bilingualism,
3(4):309?332.
T. Baldwin and M. Lui. 2010. Language identification:
the long and the short of the matter. In Proceedings of
NAACL 2010.
S. Bergsma, P. McNamee, M. Bagdouri, C. Fink, and
T. Wilson. 2012. Language identification for creat-
ing language-specific twitter collections. In Proceed-
ings of the Second Workshop on Language in Social
Media.
S. Carter, W. Weerkamp, and M. Tsagkias. 2012. Mi-
croblog language identification: Overcoming the limi-
tations of short, unedited and idiomatic text. Language
Resources and Evaluation, pages 1?21.
W.B. Cavnar and J. M. Trenkle. 1994. N-gram-based
text categorization. In Proceedings of Third Annual
Symposium on Document Analysis and Information
Retrieval.
H. Ceylan and Y. Kim. 2009. Language identification of
search engine queries. In Proceedings of ACL 2009.
B. Danet and S. C. Herring. 2007. The multilingual In-
ternet: Language, culture, and communication online.
Oxford University Press Oxford.
M. Durham. 2003. Language choice on a Swiss mailing
list. Journal of Computer-Mediated Communication,
9(1).
T. Gottron and N. Lipka. 2010. A comparison of lan-
guage identification approaches on short, query-style
texts. In Proceedings of ECIR 2010.
H. Hammarstro?m. 2007. A fine-grained model for lan-
guage identification. In Proceedings of iNEWS-07
Workshop at SIGIR 2007.
B. Hughes, T. Baldwin, S. Bird, J. Nicholson, and
A. Mackinlay. 2006. Reconsidering language identifi-
cation for written language resources. In Proceedings
of LREC 2006.
B. King and S. Abney. 2013. Labeling the languages
of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL 2013.
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML 2001.
M. Lui and T. Baldwin. 2012. langid.py: an off-the-shelf
language identification tool. In Proceedings of ACL
2012.
P. McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. Jour-
nal of Computing Sciences in Colleges, 20(3):94?101.
J.C. Paolillo. 2011. ?Conversational? codeswitching on
Usenet and Internet Relay Chat. Language@Internet,
8(3).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: machine learning in Python.
Journal of Machine Learning Research, 12:2825?
2830.
H. Sak, T. Gu?ngo?r, and M. Sarac?lar. 2008. Turkish lan-
guage resources: Morphological parser, morphologi-
cal disambiguator and web corpus. In GoTAL 2008,
volume 5221 of LNCS, pages 417?427. Springer.
R. Scha?fer and F. Bildhauer. 2012. Building large cor-
pora from the web using a new efficient tool chain. In
Proceedings of LREC 2012.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. In Pro-
ceedings of 2006 AAAI Spring Symposium on Compu-
tational Approaches for Analyzing Weblogs.
D. Trieschnigg, D. Hiemstra, M. Theune, F. Jong, and
T. Meder. 2012. An exploration of language identifi-
cation techniques for the Dutch folktale database. In
Adaptation of Language Resources and Tools for Pro-
cessing Cultural Heritage workshop (LREC 2012).
T. Vatanen, J. J. Va?yrynen, and S. Virpioja. 2010. Lan-
guage identification of short text segments with n-
gram models. In Proceedings of LREC 2010.
H. Yamaguchi and K. Tanaka-Ishii. 2012. Text segmen-
tation by language using minimum description length.
In Proceedings of ACL 2012.
862
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1391?1395,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Predicting Dialect Variation in Immigrant Contexts
Using Light Verb Constructions
A. Seza Do
?
gru
?
oz
Netherlands Institute for Advanced Study
Wassenaar, Netherlands
a.s.dogruoz@gmail.com
Preslav Nakov
Qatar Computing Research Institute
Tornado Tower
floor 10, P.O. Box 5825, Doha, Qatar
pnakov@qf.org.qa
Abstract
Languages spoken by immigrants change
due to contact with the local languages.
Capturing these changes is problematic for
current language technologies, which are
typically developed for speakers of the
standard dialect only. Even when dialec-
tal variants are available for such technolo-
gies, we still need to predict which di-
alect is being used. In this study, we dis-
tinguish between the immigrant and the
standard dialect of Turkish by focusing on
Light Verb Constructions. We experiment
with a number of grammatical and contex-
tual features, achieving over 84% accuracy
(56% baseline).
1 Introduction
Human languages are in constant evolution, driven
in part by contact with other languages (Uriel,
1953; Thomason, 2008). In immigrant contexts,
bilingual and multilingual speakers act as agents
of change by transmitting borrowed words and ex-
pressions across languages (Grosjean, 2014). De-
pending on social factors such as duration and in-
tensity of contact with the local languages, large-
scale spread of borrowed elements could lead to
differences between the contact and non-contact
dialects of the same language (Winford, 2005).
For example, Spanish spoken by immigrants in
USA sounds different in comparison to Spanish
spoken in South America (Corval?an, 2003).
In this study, we focus on the immigrant di-
alect of Turkish as spoken in the Netherlands
(NL-Turkish), which differs from Turkish spo-
ken in Turkey (TR-Turkish). In contact situa-
tions, it is common for verbs to be borrowed
across languages and integrated as nominal com-
plements of Light Verb Constructions (LVCs) (Ed-
wards and Gardner-Chloros, 2007; Butt, 2010).
NL-Turkish LVCs are changing due to Dutch in-
fluence (Do?gru?oz and Backus, 2007; Do?gru?oz and
Backus, 2009; Do?gru?oz and Gries, 2012). How-
ever, assessing Dutch influence is not always easy
since NL-Turkish LVCs still co-exist with the TR-
Turkish LVCs. This study aims to automatically
identify the features that can distinguish between
NL-Turkish and TR-Turkish LVCs.
Our study would benefit Machine Translation
systems targeting dialectal variation. It differs
from studies concerning the well-established di-
alectal variations of Arabic, e.g., Levantine, Gulf,
Egyptian, Maghrebi (Salloum and Habash, 2012),
EU vs. Brazilian Portuguese (Marujo et al., 2011)
or Turkish vs. Tatar (Altintas and Cicekli, 2002).
In contrast, we are interested in developing lan-
guage technologies for immigrant dialects, which
are often understudied and lack written resources
due to their unofficial status. When immigrant
speakers face communication difficulties (e.g., bu-
reaucratic affairs with the local officials, teacher-
parent meetings, doctor-patient conversations) in
the local languages (e.g., Dutch) of the host coun-
try, they are often provided with translation equiv-
alents in the standard dialect (e.g., TR-Turkish)
of their native languages. However, these trans-
lations ignore the evolution of the immigrant di-
alect.
1
By identifying the differences between two
dialects of the same variety, we aim to improve
Machine Translation systems targeting immigrant
speakers. Our contributions are the following:
? We are the first to predict on-going dialect
variation in immigrant contexts as opposed to
studying established dialect variations.
? We are also the first to compare bilingual
LVCs with the monolingual ones across two
dialects of the same language.
1
One of the authors failed the driving test in the Nether-
lands due to the dialect variation in the Turkish translation.
1391
? Our comparison of grammatical versus con-
textual features reveals context to be much
more important.
? We experiment with LVCs extracted from
natural spoken data rather than relying on iso-
lated occurences, out of context.
2 Method
We follow Baldwin and Kim (2010) and Butt
(2010) in their definitions of LVCs, which state
that there is a unity between the nominal and the
verbal complements, but the meaning of the verb
is somewhat bleached. In this study, we focus
on Turkish LVCs with the verbal complements of
yapmak/etmek, which both can be translated as
?make/do?. LVCs with these verbal complements
are undergoing change in NL-Turkish (Do?gru?oz
and Backus, 2009).
We experiment with the following features to
predict NL-Turkish vs. TR-Turkish LVCs.
2.1 Nominal Features
In addition to traditional LVCs (e.g. [?ut?u yap-
mak] ?iron do? (to iron) with both complements
of Turkish origins), there is also foreign influ-
ence on Turkish LVCs. Section 2.1.1 describes
the foreign influence on both NL-Turkish and TR-
Turkish nominal complements based on their ety-
mological origins.
2.1.1 Influence on Nominal Complements
Dutch Influence In example (1), the Dutch verb
overplaats is nominalized through the infinitive
marker (-en) and precedes the Turkish verb yap-
mak to form a Turkish-Dutch bilingual LVC.
Example 1:
O arkadas? [overplaats-en yap-?l-acak-t?.]
That friend [replace-inf
2
do-pass-fut-past].
That friend would have been replaced.
In addition to borrowing nominalized Dutch
verbs to form bilingual LVCs, Dutch LVCs are
also translated as a chunk into NL-Turkish. These
translated LVCs sound unconventional to TR-
Turkish speakers (Do?gru?oz and Gries, 2012). In
example (2), the LVC [s?nav yapmak] ?exam do?
is a literal translation of the Dutch [examen doen]
?exam-pl do?, which is used to describe how stu-
dents take high school exams to graduate.
2
acc: accusative, fut:future, inf:infinitive, past:past tense,
part: participle, pres: present tense, pl: plural, poss: poss-
esive, prog:progressive tense, sg: singular
In a similar context, TR-Turkish speakers would
have used [s?nav-a girmek] ?exam enter? instead.
These LVCs are also classified as having their ori-
gins in another language.
Example 2:
?
Uc? g?und?ur [s?nav yap-?yor-uz].
Three day [exam do-prog-1pl].
We are having exams for the last three days.
Other Foreign Influences Although Dutch in-
fluence is clearly present in NL-Turkish LVCs,
TR-Turkish LVCs are also not free of foreign in-
fluence. We have come across Arabic, Persian,
French and English influences on Turkish LVCs
with nominalized foreign verbs or literally trans-
lated LVCs as chunks. Example (3) illustrates how
a borrowed Arabic verb (hitap, ?address?) is in-
tegrated as a nominal complement into a Turkish
LVC [hitap etmek] ?address do?.
Example 3:
Hoca-m diye [hitap edi-yo-z] biz.
Teacher-poss.1sg like [address do-prog-1pl]
we.
We address (him) as the teacher.
Example (4) illustrates how an English LVC [do
sports] is borrowed into Turkish as a chunk [spor
yapmak] ?sports do?.
Example 4:
Yaz?n [spor yap-?yo-z].
summer spor do-prog-1pl
We do sports in summer.
We have identified the etymological origins of
LVCs in both corpora using an online etymolog-
ical dictionary.
3
Although LVCs of Dutch origin
only occur in NL-Turkish, LVCs borrowed from
other languages (e.g., Arabic, English, French) oc-
cur both in NL-Turkish and in TR-Turkish.
2.1.2 Case Marking
We also came across Turkish [N V] constructions
with ?yapmak? and ?etmek? where the nominal
complement acts as the object of the verb.
Turkish marks the direct objects with accusative
case marking if they are definite (Enc?, 1991). In
example (5), the nominal element is the object of
the verb, and thus it has the accusative marker.
Example 5:
Ben kendi [is?-im-i yap-?yor-um.]
I own [work-poss.1sg-acc do-prog-1sg].
I do my own work.
3
http://www.nisanyansozluk.com/
1392
However, indefinite objects of the verb are left
unmarked for case. In example (6), yapmak takes
an indefinite object (food) as the complement. The
boundary between [N V] constructions with in-
definite nominal objects and LVCs are somewhat
blurry. In both cases, the meaning of the verbal
complement is bleached out and the nominal com-
plement weighs heavier than the verbal one. We
will not dwell further on this subtle distinction, but
we plan future work on this topic following Cook
et al. (2007) and Vincze et al. (2013).
Example 6:
Bazen [yemek yap-ar-d?-m]
Sometimes [food do-pres-past-1sg]
I used to sometimes prepare food.
Since Dutch does not mark objects of the verb
morphologically, NL-Turkish speakers have diffi-
culty (e.g., unnecessary addition or omission of
case markers) in determining the definiteness of
the nominal complements in [N V] constructions
(Do?gru?oz and Backus, 2009). Therefore, we ex-
pect this feature to differentiate well between NL-
Turkish and TR-Turkish [N V] constructions and
LVCs with yapmak/etmek as verbal complements.
2.2 Verbal Complements
2.2.1 Finiteness
The verbs in LVCs are assumed to be flexible for
inflection (Baldwin and Kim, 2010). However, we
know little about how fineteness contributes to the
formation of LVCs. To the best of our knowledge,
finiteness has not been tested as a feature for iden-
tifying LVCs earlier. Therefore, we encoded the
finiteness on yapmak/etmek as a binary (yes/no)
feature in both data sets. Example (7) illustrates a
non-finite LVC where the verb stem (et) is accom-
panied with an infinitive marker (-mek).
Example 7:
Misafir-ler-e [ikram et-mek] ic?in al-d?-k
Guest-pl-dat [serve do-inf.] for buy-past-1pl
We bought (it) to serve the guests.
2.2.2 Type
NL-Turkish speakers could use other light verbs
than TR-Turkish speakers for the same LVC con-
struction. In example (8), the NL-Turkish speaker
uses [do?gum etmek] ?birth do? instead of [do?gum
yapmak] ?birth do?, which is commonly preferred
by TR-Turkish speakers. To capture this differ-
ence between the two dialects, we include the verb
type as a feature as well.
Example 8:
Orda kad?n [do
?
gum et-ti].
There lady [birth do-past].
The lady gave birth there.
2.3 Word Order in LVCs
To the best of our knowledge, the influence of
word order in LVCs has not been investigated as
a feature. Although Turkish has a relatively flexi-
ble constituent order, object-verb (OV) is the most
frequent word order for both NL-Turkish and TR-
Turkish (Do?gru?oz and Backus, 2007). NL-Turkish
speakers have adopted Dutch word order verb-
object (VO) for some syntactic constructions, but
we know little about the word order variation for
LVCs. Encoding the word order of LVCs as a
binary feature (OV vs. VO) could give us clues
about differences or similarities of LVC use in NL-
Turkish and in TR-Turkish. In example (9), the
nominal complement (one thing) follows the ver-
bal complement instead of preceding it as seen in
earlier examples.
Example 9:
[Yap-acak bir s?ey] yok.
[Do-part. one thing] exist.not
There is nothing to do.
2.4 Context
So far, most studies were carried out ignoring
the context of LVCs but focusing on their inher-
ent grammatical features (e.g., lexical, syntactic,
semantic or morphological). However, the con-
text of an utterance could potentially provide addi-
tional useful cues. Since our data comes from nat-
ural conversations, we also experimented with the
contextual information (words surrounding LVCs)
as a feature for both data sets.
3 Data
Our data comes from spoken NL-Turkish (46
speakers from the Netherlands, 74,461 words)
and TR-Turkish (22 speakers from Turkey, 28,731
words) corpora collected by one of the authors.
LVC?s are automatically extracted from the data
using their stem forms (?yap-?, ?et-? without the
infinitive -mEk). Table 1 illustrates the frequency
of [N V] constructions with etmek and yapmak in
both data sets.
# etmek # yapmak # Total
NL-Turkish 449 543 992
TR-Turkish 527 755 1282
Total 976 1298
Table 1: Distribution of etmek and yapmak.
1393
4 Experiments
Our aim is to build a classifier that can determine
whether a particular utterance containing an LVC
(with the verbs yapmak or etmek) is uttered by an
NL-Turkish or a TR-Turkish speaker.
We make use the following features in our
classifier: (1) words from the context of the
LVCs, (2) type of the light verb (yapmak or
etmek), (3) the nominal complements, (4) finite-
ness of the verb (finite/non-finite), (5) case
marking on the nominal complement (yes/no),
(6) word order (VO/OV), and (7) etymolog-
ical origins of the nominal complement (Ara-
bic/Dutch/French/English/Persian/Turkish/mixed).
For the contextual features, we experiment with
two models: (a) we distinguish between a word
extracted from the context to the left or to the right
of the verb (yapmak or etmek) in the feature space,
and (b) we do not make a distinction in terms of
context. The reason to experiment with option
(a) is due to the potential importance of the word
order. While the word order variation is already
modeled through feature (6), we also include the
context as an additional feature to test its effect.
On the down side, adding context doubles the fea-
ture space size and could lead to data sparseness
issues. For the context words, we did not filter out
stopwords since they are part of natural speech.
For our experiments, we used an SVM classifier
as implemented in LibSVM. We used a linear ker-
nel; more complex kernels did not help. We report
results for a 5-fold cross-validation.
5 Results
Table 2 illustrates the results of our experiments.
All models outperform the majority class base-
line of always predicting TR-Turkish (which is
56.38% accuracy) by a sizable margin. Further-
more, splitting the context into left/right yields ap-
proximately 1.5% absolute drop in accuracy.
Split the Context?
Features Left vs. Right No Split
Baseline 56.38
Full model 82.81 84.30
no context 70.67
no nominal complements 82.19 83.64
no info about etymol. origin 82.10 83.99
no finiteness 83.03 84.35
no case marking info 82.76 84.43
no word order info 82.89 84.43
no verb type 82.94 84.39
Table 2: Cross-validation accuracy (5 folds).
The lower part of the table shows the results
when turning off each of the feature types. The
context seems to be the most important feature
since its exclusion leads to a drop from low-to-
mid eighties to about 70% accuracy. Except the
nominal complements and the information about
etymological origins, most other features seem to
have marginal impact on accuracy. Excluding the
two features (nominal complements and etymo-
logical origins) lead to approximately 0.5% ab-
solute drop in accuracy. The impact of the last
four features in the table is tiny; excluding some
of them even leads to a tiny improvement.
Overall, we can conclude that by far the most
important features are the context features (with-
out the left/right context split). The other use-
ful features are the nominal complements and the
information about the etymological origin of the
borrowed LVCs. The remaining four linguistic
features seem to be largely irrelevant.
6 Conclusion and Future Work
Language technologies are usually developed for
standard dialects, ignoring the linguistic differ-
ences in other dialects such as those in immigrant
contexts. One of the reasons for this is the dif-
ficulty of assessing and predicting linguistic dif-
ferences across dialects. This is similar to ef-
forts to translate well-established Arabic dialects
(Bakr et al., 2008; Sawaf, 2010), or to adapt be-
tween Brazilian and European Portuguese (Marujo
et al., 2011), Czech?Slovak (Haji?c et al., 2000),
Spanish?Portuguese (Nakov and Ng, 2009; Nakov
and Ng, 2012), Turkish?Crimean Tatar (Altintas
and Cicekli, 2002), Irish?Scottish Gaelic (Scan-
nell, 2006), Bulgarian?Macedonian (Nakov and
Tiedemann, 2012), Malay?Indonesian (Wang et
al., 2012) or Mandarin?Cantonese (Zhang, 1998).
In this work, we have built a classifier that uses
LVCs to differentiate between two different Turk-
ish dialects: standard and immigrant. The results
indicate that contextual features are most useful
for this task. Although this requires further inves-
tigation, we can explain it by the thousands of fea-
tures context generates: each contextual word is a
feature. Thus, it is very hard for our grammatical
features to compete against contextual features but
they do have an impact.
We are planning to extend our study to dialects
in other immigrant settings (e.g., Turkish in Ger-
many) and to other types of multiword expressions
(e.g., [N N] compounds).
1394
References
Kemal Altintas and Ilyas Cicekli. 2002. A machine
translation system between a pair of closely related
languages. In Proceedings of the 17th International
Symposium on Computer and Information Sciences,
ISCIS ?02, pages 192?196, Orlando, FL, USA.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting
written Egyptian colloquial dialect into diacritized
Arabic. In Proceedings of the 6th International
Conference on Informatics and Systems, INFOS ?08,
Cairo, Egypt.
Timothy Baldwin and Su Nam Kim, 2010. In Nitin
Indurkhya and Fred J. Damerau (eds.), Handbook
of Natural Language Processing, chapter Multiword
expressions, pages 267?292. CRC Press, Boca Ra-
ton, USA, second edition.
Miriam Butt, 2010. In Mengistu Amberber, Brett
Baker, and Mark Harvey (eds.), Complex predi-
cates: cross-linguistic perspectives on event struc-
ture, chapter The light verb jungle: still hacking
away, pages 48?78. Cambridge University Press.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of MWE ?07,
pages 41?48, Prague, Czech Republic.
Carmen Silva Corval?an. 2003. Otra mirada a la ex-
presi?on del sujeto como variable sint?actica. Lengua,
variaci?on y contexto: Estudios dedicados a Hum-
berto L?opez Morales, 2:849?860.
A. Seza Do?gru?oz and Ad Backus. 2007. Postverbal el-
ements in immigrant Turkish: Evidence of change?
International Journal of Bilingualism, 11(2):185?
220.
A. Seza Do?gru?oz and Ad Backus. 2009. Innova-
tive constructions in Dutch Turkish: An assessment
of ongoing contact-induced change. Bilingualism:
Language and Cognition, 12(01):41?63.
A. Seza Do?gru?oz and Stefan Gries. 2012. Spread of
on-going changes in an immigrant language: Turk-
ish in the Netherlands. Review of Cognitive Linguis-
tics, 10(2).
Malcolm Edwards and Penelope Gardner-Chloros.
2007. Compound verbs in codeswitching: Bilin-
guals making do? International Journal of Bilin-
gualism, 11(1):73?91.
M?urvet Enc?. 1991. The semantics of specificity. Lin-
guistic Inquiry, 22(1):1?25.
Franc?ois Grosjean. 2014. Bicultural bilinguals. Inter-
national Journal of Bilingualism, pages 1?15.
Jan Haji?c, Jan Hric, and Vladislav Kubo?n. 2000. Ma-
chine translation of very close languages. In Pro-
ceedings of ANLP ?00, pages 7?12, Seattle, WA,
USA.
Lu??s Marujo, Nuno Grazina, Tiago Lu??s, Wang Ling,
Lu??sa Coheur, and Isabel Trancoso. 2011. BP2EP -
adaptation of Brazilian Portuguese texts to European
Portuguese. In Proceedings of EAMT ?11, pages
129?136, Leuven, Belgium.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of EMNLP ?09, pages 1358?1367, Sin-
gapore.
Preslav Nakov and Hwee Tou Ng. 2012. Improving
statistical machine translation for a resource-poor
language using related resource-rich languages. J.
Artif. Intell. Res. (JAIR), 44:179?222.
Preslav Nakov and J?org Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proceedings of ACL ?12, Jeju Island, Korea.
Wael Salloum and Nizar Habash. 2012. Elissa: A di-
alectal to standard Arabic machine translation sys-
tem. In Proceedings of COLING ?12, pages 385?
392, Mumbai, India.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of AMTA ?10,
Denver, Colorado.
Kevin Scannell. 2006. Machine translation for
closely related language pairs. In Proceedings of the
LREC 2006 Workshop on Strategies for developing
machine translation for minority languages, pages
103?107, Genoa, Italy.
Sarah Thomason. 2008. Social and linguistic factors
as predictors of contact-induced change. Journal of
language contact, 2(1):42?56.
Weinreich Uriel. 1953. Languages in contact: Find-
ings and problems. Publications of the Linguistic
Circle of New York, vol. 1.
Veronika Vincze, Istv?an Nagy, and Rich?ard Farkas.
2013. Identifying English and Hungarian light verb
constructions: A contrastive approach. In Proceed-
ings of ACL ?13, pages 255?261, Sofia, Bulgaria.
Pidong Wang, Preslav Nakov, and Hwee Tou Ng.
2012. Source language adaptation for resource-poor
machine translation. In Proceedings of EMNLP-
CoNLL ?12, pages 286?296, Jeju Island, Korea.
Donald Winford. 2005. Contact-induced changes:
Classification and processes. Diachronica,
22(2):373?427.
Xiaoheng Zhang. 1998. Dialect MT: a case study be-
tween Cantonese and Mandarin. In Proceedings of
the COLING ?98, pages 1460?1464, Montreal, Que-
bec, Canada.
1395
